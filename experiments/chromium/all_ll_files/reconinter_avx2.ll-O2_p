; ModuleID = '../../third_party/libaom/source/libaom/av1/common/x86/reconinter_avx2.c'
source_filename = "../../third_party/libaom/source/libaom/av1/common/x86/reconinter_avx2.c"
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

module asm ".symver exp, exp@GLIBC_2.2.5"
module asm ".symver exp2, exp2@GLIBC_2.2.5"
module asm ".symver exp2f, exp2f@GLIBC_2.2.5"
module asm ".symver expf, expf@GLIBC_2.2.5"
module asm ".symver lgamma, lgamma@GLIBC_2.2.5"
module asm ".symver lgammaf, lgammaf@GLIBC_2.2.5"
module asm ".symver lgammal, lgammal@GLIBC_2.2.5"
module asm ".symver log, log@GLIBC_2.2.5"
module asm ".symver log2, log2@GLIBC_2.2.5"
module asm ".symver log2f, log2f@GLIBC_2.2.5"
module asm ".symver logf, logf@GLIBC_2.2.5"
module asm ".symver pow, pow@GLIBC_2.2.5"
module asm ".symver powf, powf@GLIBC_2.2.5"

%struct.ConvolveParams = type { i32, i16*, i32, i32, i32, i32, i32, i32, i32, i32 }

; Function Attrs: nofree nounwind ssp uwtable
define hidden void @av1_build_compound_diffwtd_mask_avx2(i8* nocapture, i8 zeroext, i8* nocapture readonly, i32, i8* nocapture readonly, i32, i32, i32) local_unnamed_addr #0 {
  %9 = icmp eq i8 %1, 1
  %10 = select i1 %9, i16 -26, i16 38
  %11 = insertelement <16 x i16> undef, i16 %10, i32 0
  %12 = shufflevector <16 x i16> %11, <16 x i16> undef, <16 x i32> zeroinitializer
  switch i32 %7, label %50 [
    i32 4, label %35
    i32 8, label %20
    i32 16, label %13
  ]

13:                                               ; preds = %8
  %14 = sext i32 %3 to i64
  %15 = sext i32 %5 to i64
  %16 = shl i32 %3, 1
  %17 = sext i32 %16 to i64
  %18 = shl i32 %5, 1
  %19 = sext i32 %18 to i64
  br label %185

20:                                               ; preds = %8
  %21 = sext i32 %3 to i64
  %22 = shl nsw i32 %3, 1
  %23 = sext i32 %22 to i64
  %24 = mul nsw i32 %3, 3
  %25 = sext i32 %24 to i64
  %26 = sext i32 %5 to i64
  %27 = shl nsw i32 %5, 1
  %28 = sext i32 %27 to i64
  %29 = mul nsw i32 %5, 3
  %30 = sext i32 %29 to i64
  %31 = shl i32 %3, 2
  %32 = sext i32 %31 to i64
  %33 = shl i32 %5, 2
  %34 = sext i32 %33 to i64
  br label %117

35:                                               ; preds = %8
  %36 = sext i32 %3 to i64
  %37 = shl nsw i32 %3, 1
  %38 = sext i32 %37 to i64
  %39 = mul nsw i32 %3, 3
  %40 = sext i32 %39 to i64
  %41 = sext i32 %5 to i64
  %42 = shl nsw i32 %5, 1
  %43 = sext i32 %42 to i64
  %44 = mul nsw i32 %5, 3
  %45 = sext i32 %44 to i64
  %46 = shl i32 %3, 2
  %47 = sext i32 %46 to i64
  %48 = shl i32 %5, 2
  %49 = sext i32 %48 to i64
  br label %54

50:                                               ; preds = %8
  %51 = sext i32 %3 to i64
  %52 = sext i32 %5 to i64
  %53 = sext i32 %7 to i64
  br label %231

54:                                               ; preds = %35, %54
  %55 = phi i32 [ %115, %54 ], [ 0, %35 ]
  %56 = phi i8* [ %113, %54 ], [ %4, %35 ]
  %57 = phi i8* [ %112, %54 ], [ %2, %35 ]
  %58 = phi i8* [ %114, %54 ], [ %0, %35 ]
  %59 = bitcast i8* %57 to i32*
  %60 = load i32, i32* %59, align 1
  %61 = insertelement <4 x i32> undef, i32 %60, i32 0
  %62 = getelementptr inbounds i8, i8* %57, i64 %36
  %63 = bitcast i8* %62 to i32*
  %64 = load i32, i32* %63, align 1
  %65 = getelementptr inbounds i8, i8* %57, i64 %38
  %66 = bitcast i8* %65 to i32*
  %67 = load i32, i32* %66, align 1
  %68 = insertelement <4 x i32> undef, i32 %67, i32 0
  %69 = getelementptr inbounds i8, i8* %57, i64 %40
  %70 = bitcast i8* %69 to i32*
  %71 = load i32, i32* %70, align 1
  %72 = insertelement <4 x i32> %61, i32 %64, i32 1
  %73 = bitcast <4 x i32> %72 to <2 x i64>
  %74 = insertelement <4 x i32> %68, i32 %71, i32 1
  %75 = bitcast <4 x i32> %74 to <2 x i64>
  %76 = shufflevector <2 x i64> %73, <2 x i64> %75, <2 x i32> <i32 0, i32 2>
  %77 = bitcast <2 x i64> %76 to <16 x i8>
  %78 = zext <16 x i8> %77 to <16 x i16>
  %79 = bitcast i8* %56 to i32*
  %80 = load i32, i32* %79, align 1
  %81 = insertelement <4 x i32> undef, i32 %80, i32 0
  %82 = getelementptr inbounds i8, i8* %56, i64 %41
  %83 = bitcast i8* %82 to i32*
  %84 = load i32, i32* %83, align 1
  %85 = getelementptr inbounds i8, i8* %56, i64 %43
  %86 = bitcast i8* %85 to i32*
  %87 = load i32, i32* %86, align 1
  %88 = insertelement <4 x i32> undef, i32 %87, i32 0
  %89 = getelementptr inbounds i8, i8* %56, i64 %45
  %90 = bitcast i8* %89 to i32*
  %91 = load i32, i32* %90, align 1
  %92 = insertelement <4 x i32> %81, i32 %84, i32 1
  %93 = bitcast <4 x i32> %92 to <2 x i64>
  %94 = insertelement <4 x i32> %88, i32 %91, i32 1
  %95 = bitcast <4 x i32> %94 to <2 x i64>
  %96 = shufflevector <2 x i64> %93, <2 x i64> %95, <2 x i32> <i32 0, i32 2>
  %97 = bitcast <2 x i64> %96 to <16 x i8>
  %98 = zext <16 x i8> %97 to <16 x i16>
  %99 = sub nsw <16 x i16> %78, %98
  %100 = sub nsw <16 x i16> zeroinitializer, %99
  %101 = icmp slt <16 x i16> %99, zeroinitializer
  %102 = select <16 x i1> %101, <16 x i16> %100, <16 x i16> %99
  %103 = lshr <16 x i16> %102, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %104 = add <16 x i16> %103, %12
  %105 = sub <16 x i16> zeroinitializer, %104
  %106 = icmp slt <16 x i16> %104, zeroinitializer
  %107 = select <16 x i1> %106, <16 x i16> %105, <16 x i16> %104
  %108 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %107, <16 x i16> undef) #5
  %109 = bitcast <32 x i8> %108 to <4 x i64>
  %110 = shufflevector <4 x i64> %109, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  %111 = bitcast i8* %58 to <2 x i64>*
  store <2 x i64> %110, <2 x i64>* %111, align 1
  %112 = getelementptr inbounds i8, i8* %57, i64 %47
  %113 = getelementptr inbounds i8, i8* %56, i64 %49
  %114 = getelementptr inbounds i8, i8* %58, i64 16
  %115 = add nuw nsw i32 %55, 4
  %116 = icmp slt i32 %115, %6
  br i1 %116, label %54, label %287

117:                                              ; preds = %20, %117
  %118 = phi i32 [ %183, %117 ], [ 0, %20 ]
  %119 = phi i8* [ %181, %117 ], [ %4, %20 ]
  %120 = phi i8* [ %180, %117 ], [ %2, %20 ]
  %121 = phi i8* [ %182, %117 ], [ %0, %20 ]
  %122 = bitcast i8* %120 to i64*
  %123 = load i64, i64* %122, align 1
  %124 = insertelement <2 x i64> undef, i64 %123, i32 0
  %125 = getelementptr inbounds i8, i8* %120, i64 %21
  %126 = bitcast i8* %125 to i64*
  %127 = load i64, i64* %126, align 1
  %128 = insertelement <2 x i64> undef, i64 %127, i32 0
  %129 = getelementptr inbounds i8, i8* %120, i64 %23
  %130 = bitcast i8* %129 to i64*
  %131 = load i64, i64* %130, align 1
  %132 = getelementptr inbounds i8, i8* %120, i64 %25
  %133 = bitcast i8* %132 to i64*
  %134 = load i64, i64* %133, align 1
  %135 = insertelement <2 x i64> %124, i64 %131, i32 1
  %136 = bitcast <2 x i64> %135 to <16 x i8>
  %137 = zext <16 x i8> %136 to <16 x i16>
  %138 = insertelement <2 x i64> %128, i64 %134, i32 1
  %139 = bitcast <2 x i64> %138 to <16 x i8>
  %140 = zext <16 x i8> %139 to <16 x i16>
  %141 = bitcast i8* %119 to i64*
  %142 = load i64, i64* %141, align 1
  %143 = insertelement <2 x i64> undef, i64 %142, i32 0
  %144 = getelementptr inbounds i8, i8* %119, i64 %26
  %145 = bitcast i8* %144 to i64*
  %146 = load i64, i64* %145, align 1
  %147 = insertelement <2 x i64> undef, i64 %146, i32 0
  %148 = getelementptr inbounds i8, i8* %119, i64 %28
  %149 = bitcast i8* %148 to i64*
  %150 = load i64, i64* %149, align 1
  %151 = getelementptr inbounds i8, i8* %119, i64 %30
  %152 = bitcast i8* %151 to i64*
  %153 = load i64, i64* %152, align 1
  %154 = insertelement <2 x i64> %143, i64 %150, i32 1
  %155 = bitcast <2 x i64> %154 to <16 x i8>
  %156 = zext <16 x i8> %155 to <16 x i16>
  %157 = insertelement <2 x i64> %147, i64 %153, i32 1
  %158 = bitcast <2 x i64> %157 to <16 x i8>
  %159 = zext <16 x i8> %158 to <16 x i16>
  %160 = sub nsw <16 x i16> %137, %156
  %161 = sub nsw <16 x i16> zeroinitializer, %160
  %162 = icmp slt <16 x i16> %160, zeroinitializer
  %163 = select <16 x i1> %162, <16 x i16> %161, <16 x i16> %160
  %164 = lshr <16 x i16> %163, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %165 = add <16 x i16> %164, %12
  %166 = sub <16 x i16> zeroinitializer, %165
  %167 = icmp slt <16 x i16> %165, zeroinitializer
  %168 = select <16 x i1> %167, <16 x i16> %166, <16 x i16> %165
  %169 = sub nsw <16 x i16> %140, %159
  %170 = sub nsw <16 x i16> zeroinitializer, %169
  %171 = icmp slt <16 x i16> %169, zeroinitializer
  %172 = select <16 x i1> %171, <16 x i16> %170, <16 x i16> %169
  %173 = lshr <16 x i16> %172, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %174 = add <16 x i16> %173, %12
  %175 = sub <16 x i16> zeroinitializer, %174
  %176 = icmp slt <16 x i16> %174, zeroinitializer
  %177 = select <16 x i1> %176, <16 x i16> %175, <16 x i16> %174
  %178 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %168, <16 x i16> %177) #5
  %179 = bitcast i8* %121 to <32 x i8>*
  store <32 x i8> %178, <32 x i8>* %179, align 1
  %180 = getelementptr inbounds i8, i8* %120, i64 %32
  %181 = getelementptr inbounds i8, i8* %119, i64 %34
  %182 = getelementptr inbounds i8, i8* %121, i64 32
  %183 = add nuw nsw i32 %118, 4
  %184 = icmp slt i32 %183, %6
  br i1 %184, label %117, label %287

185:                                              ; preds = %13, %185
  %186 = phi i32 [ %229, %185 ], [ 0, %13 ]
  %187 = phi i8* [ %227, %185 ], [ %4, %13 ]
  %188 = phi i8* [ %226, %185 ], [ %2, %13 ]
  %189 = phi i8* [ %228, %185 ], [ %0, %13 ]
  %190 = bitcast i8* %188 to <16 x i8>*
  %191 = load <16 x i8>, <16 x i8>* %190, align 16
  %192 = getelementptr inbounds i8, i8* %188, i64 %14
  %193 = bitcast i8* %192 to <16 x i8>*
  %194 = load <16 x i8>, <16 x i8>* %193, align 16
  %195 = bitcast i8* %187 to <16 x i8>*
  %196 = load <16 x i8>, <16 x i8>* %195, align 16
  %197 = getelementptr inbounds i8, i8* %187, i64 %15
  %198 = bitcast i8* %197 to <16 x i8>*
  %199 = load <16 x i8>, <16 x i8>* %198, align 16
  %200 = zext <16 x i8> %191 to <16 x i16>
  %201 = zext <16 x i8> %194 to <16 x i16>
  %202 = zext <16 x i8> %196 to <16 x i16>
  %203 = zext <16 x i8> %199 to <16 x i16>
  %204 = sub nsw <16 x i16> %200, %202
  %205 = sub nsw <16 x i16> zeroinitializer, %204
  %206 = icmp slt <16 x i16> %204, zeroinitializer
  %207 = select <16 x i1> %206, <16 x i16> %205, <16 x i16> %204
  %208 = lshr <16 x i16> %207, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %209 = add <16 x i16> %208, %12
  %210 = sub <16 x i16> zeroinitializer, %209
  %211 = icmp slt <16 x i16> %209, zeroinitializer
  %212 = select <16 x i1> %211, <16 x i16> %210, <16 x i16> %209
  %213 = sub nsw <16 x i16> %201, %203
  %214 = sub nsw <16 x i16> zeroinitializer, %213
  %215 = icmp slt <16 x i16> %213, zeroinitializer
  %216 = select <16 x i1> %215, <16 x i16> %214, <16 x i16> %213
  %217 = lshr <16 x i16> %216, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %218 = add <16 x i16> %217, %12
  %219 = sub <16 x i16> zeroinitializer, %218
  %220 = icmp slt <16 x i16> %218, zeroinitializer
  %221 = select <16 x i1> %220, <16 x i16> %219, <16 x i16> %218
  %222 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %212, <16 x i16> %221) #5
  %223 = bitcast <32 x i8> %222 to <4 x i64>
  %224 = shufflevector <4 x i64> %223, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %225 = bitcast i8* %189 to <4 x i64>*
  store <4 x i64> %224, <4 x i64>* %225, align 1
  %226 = getelementptr inbounds i8, i8* %188, i64 %17
  %227 = getelementptr inbounds i8, i8* %187, i64 %19
  %228 = getelementptr inbounds i8, i8* %189, i64 32
  %229 = add nuw nsw i32 %186, 2
  %230 = icmp slt i32 %229, %6
  br i1 %230, label %185, label %287

231:                                              ; preds = %50, %281
  %232 = phi i32 [ %285, %281 ], [ 0, %50 ]
  %233 = phi i8* [ %283, %281 ], [ %4, %50 ]
  %234 = phi i8* [ %282, %281 ], [ %2, %50 ]
  %235 = phi i8* [ %284, %281 ], [ %0, %50 ]
  br label %236

236:                                              ; preds = %236, %231
  %237 = phi i64 [ %279, %236 ], [ 0, %231 ]
  %238 = getelementptr inbounds i8, i8* %234, i64 %237
  %239 = bitcast i8* %238 to <4 x i64>*
  %240 = load <4 x i64>, <4 x i64>* %239, align 1
  %241 = getelementptr inbounds i8, i8* %233, i64 %237
  %242 = bitcast i8* %241 to <4 x i64>*
  %243 = load <4 x i64>, <4 x i64>* %242, align 1
  %244 = shufflevector <4 x i64> %240, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %245 = bitcast <2 x i64> %244 to <16 x i8>
  %246 = zext <16 x i8> %245 to <16 x i16>
  %247 = shufflevector <4 x i64> %243, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %248 = bitcast <2 x i64> %247 to <16 x i8>
  %249 = zext <16 x i8> %248 to <16 x i16>
  %250 = shufflevector <4 x i64> %240, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %251 = bitcast <2 x i64> %250 to <16 x i8>
  %252 = zext <16 x i8> %251 to <16 x i16>
  %253 = shufflevector <4 x i64> %243, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %254 = bitcast <2 x i64> %253 to <16 x i8>
  %255 = zext <16 x i8> %254 to <16 x i16>
  %256 = sub nsw <16 x i16> %246, %249
  %257 = sub nsw <16 x i16> zeroinitializer, %256
  %258 = icmp slt <16 x i16> %256, zeroinitializer
  %259 = select <16 x i1> %258, <16 x i16> %257, <16 x i16> %256
  %260 = lshr <16 x i16> %259, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %261 = add <16 x i16> %260, %12
  %262 = sub <16 x i16> zeroinitializer, %261
  %263 = icmp slt <16 x i16> %261, zeroinitializer
  %264 = select <16 x i1> %263, <16 x i16> %262, <16 x i16> %261
  %265 = sub nsw <16 x i16> %252, %255
  %266 = sub nsw <16 x i16> zeroinitializer, %265
  %267 = icmp slt <16 x i16> %265, zeroinitializer
  %268 = select <16 x i1> %267, <16 x i16> %266, <16 x i16> %265
  %269 = lshr <16 x i16> %268, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %270 = add <16 x i16> %269, %12
  %271 = sub <16 x i16> zeroinitializer, %270
  %272 = icmp slt <16 x i16> %270, zeroinitializer
  %273 = select <16 x i1> %272, <16 x i16> %271, <16 x i16> %270
  %274 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %264, <16 x i16> %273) #5
  %275 = bitcast <32 x i8> %274 to <4 x i64>
  %276 = shufflevector <4 x i64> %275, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %277 = getelementptr inbounds i8, i8* %235, i64 %237
  %278 = bitcast i8* %277 to <4 x i64>*
  store <4 x i64> %276, <4 x i64>* %278, align 1
  %279 = add nuw nsw i64 %237, 32
  %280 = icmp slt i64 %279, %53
  br i1 %280, label %236, label %281

281:                                              ; preds = %236
  %282 = getelementptr inbounds i8, i8* %234, i64 %51
  %283 = getelementptr inbounds i8, i8* %233, i64 %52
  %284 = getelementptr inbounds i8, i8* %235, i64 %53
  %285 = add nuw nsw i32 %232, 1
  %286 = icmp slt i32 %285, %6
  br i1 %286, label %231, label %287

287:                                              ; preds = %185, %117, %54, %281
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define hidden void @av1_build_compound_diffwtd_mask_d16_avx2(i8* nocapture, i8 zeroext, i16* nocapture readonly, i32, i16* nocapture readonly, i32, i32, i32, %struct.ConvolveParams* nocapture readonly, i32) local_unnamed_addr #0 {
  %11 = getelementptr inbounds %struct.ConvolveParams, %struct.ConvolveParams* %8, i64 0, i32 3
  %12 = load i32, i32* %11, align 4
  %13 = getelementptr inbounds %struct.ConvolveParams, %struct.ConvolveParams* %8, i64 0, i32 4
  %14 = load i32, i32* %13, align 8
  %15 = add i32 %9, 6
  %16 = sub i32 %15, %12
  %17 = sub i32 %16, %14
  %18 = icmp eq i8 %1, 0
  %19 = shl i32 1, %17
  %20 = lshr i32 %19, 1
  %21 = trunc i32 %20 to i16
  %22 = insertelement <16 x i16> undef, i16 %21, i32 0
  %23 = shufflevector <16 x i16> %22, <16 x i16> undef, <16 x i32> zeroinitializer
  br i1 %18, label %24, label %535

24:                                               ; preds = %10
  switch i32 %7, label %68 [
    i32 4, label %53
    i32 8, label %38
    i32 16, label %31
    i32 32, label %28
    i32 64, label %25
  ]

25:                                               ; preds = %24
  %26 = sext i32 %3 to i64
  %27 = sext i32 %5 to i64
  br label %299

28:                                               ; preds = %24
  %29 = sext i32 %3 to i64
  %30 = sext i32 %5 to i64
  br label %255

31:                                               ; preds = %24
  %32 = sext i32 %3 to i64
  %33 = sext i32 %5 to i64
  %34 = shl i32 %3, 1
  %35 = sext i32 %34 to i64
  %36 = shl i32 %5, 1
  %37 = sext i32 %36 to i64
  br label %211

38:                                               ; preds = %24
  %39 = sext i32 %3 to i64
  %40 = mul nsw i32 %3, 3
  %41 = sext i32 %40 to i64
  %42 = shl nsw i32 %3, 1
  %43 = sext i32 %42 to i64
  %44 = sext i32 %5 to i64
  %45 = mul nsw i32 %5, 3
  %46 = sext i32 %45 to i64
  %47 = shl nsw i32 %5, 1
  %48 = sext i32 %47 to i64
  %49 = shl i32 %3, 2
  %50 = sext i32 %49 to i64
  %51 = shl i32 %5, 2
  %52 = sext i32 %51 to i64
  br label %135

53:                                               ; preds = %24
  %54 = sext i32 %3 to i64
  %55 = shl nsw i32 %3, 1
  %56 = sext i32 %55 to i64
  %57 = mul nsw i32 %3, 3
  %58 = sext i32 %57 to i64
  %59 = sext i32 %5 to i64
  %60 = shl nsw i32 %5, 1
  %61 = sext i32 %60 to i64
  %62 = mul nsw i32 %5, 3
  %63 = sext i32 %62 to i64
  %64 = shl i32 %3, 2
  %65 = sext i32 %64 to i64
  %66 = shl i32 %5, 2
  %67 = sext i32 %66 to i64
  br label %71

68:                                               ; preds = %24
  %69 = sext i32 %3 to i64
  %70 = sext i32 %5 to i64
  br label %380

71:                                               ; preds = %71, %53
  %72 = phi i32 [ %133, %71 ], [ 0, %53 ]
  %73 = phi i16* [ %131, %71 ], [ %4, %53 ]
  %74 = phi i16* [ %130, %71 ], [ %2, %53 ]
  %75 = phi i8* [ %132, %71 ], [ %0, %53 ]
  %76 = bitcast i16* %74 to i64*
  %77 = load i64, i64* %76, align 1
  %78 = insertelement <4 x i64> undef, i64 %77, i32 0
  %79 = getelementptr inbounds i16, i16* %74, i64 %54
  %80 = bitcast i16* %79 to i64*
  %81 = load i64, i64* %80, align 1
  %82 = getelementptr inbounds i16, i16* %74, i64 %56
  %83 = bitcast i16* %82 to i64*
  %84 = load i64, i64* %83, align 1
  %85 = insertelement <2 x i64> undef, i64 %84, i32 0
  %86 = getelementptr inbounds i16, i16* %74, i64 %58
  %87 = bitcast i16* %86 to i64*
  %88 = load i64, i64* %87, align 1
  %89 = bitcast i16* %73 to i64*
  %90 = load i64, i64* %89, align 1
  %91 = insertelement <4 x i64> undef, i64 %90, i32 0
  %92 = getelementptr inbounds i16, i16* %73, i64 %59
  %93 = bitcast i16* %92 to i64*
  %94 = load i64, i64* %93, align 1
  %95 = getelementptr inbounds i16, i16* %73, i64 %61
  %96 = bitcast i16* %95 to i64*
  %97 = load i64, i64* %96, align 1
  %98 = insertelement <2 x i64> undef, i64 %97, i32 0
  %99 = getelementptr inbounds i16, i16* %73, i64 %63
  %100 = bitcast i16* %99 to i64*
  %101 = load i64, i64* %100, align 1
  %102 = insertelement <2 x i64> %85, i64 %88, i32 1
  %103 = insertelement <4 x i64> %78, i64 %81, i32 1
  %104 = bitcast <4 x i64> %103 to <8 x i32>
  %105 = bitcast <2 x i64> %102 to <4 x i32>
  %106 = shufflevector <4 x i32> %105, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %107 = shufflevector <8 x i32> %104, <8 x i32> %106, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %108 = insertelement <2 x i64> %98, i64 %101, i32 1
  %109 = insertelement <4 x i64> %91, i64 %94, i32 1
  %110 = bitcast <4 x i64> %109 to <8 x i32>
  %111 = bitcast <2 x i64> %108 to <4 x i32>
  %112 = shufflevector <4 x i32> %111, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %113 = shufflevector <8 x i32> %110, <8 x i32> %112, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %114 = bitcast <8 x i32> %107 to <16 x i16>
  %115 = bitcast <8 x i32> %113 to <16 x i16>
  %116 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %114, <16 x i16> %115) #5
  %117 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %115, <16 x i16> %114) #5
  %118 = icmp ugt <16 x i16> %116, %117
  %119 = select <16 x i1> %118, <16 x i16> %116, <16 x i16> %117
  %120 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %119, <16 x i16> %23) #5
  %121 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %120, i32 %17) #5
  %122 = lshr <16 x i16> %121, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %123 = icmp ult <16 x i16> %122, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %124 = select <16 x i1> %123, <16 x i16> %122, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %125 = add nuw nsw <16 x i16> %124, <i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38>
  %126 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %125, <16 x i16> undef) #5
  %127 = bitcast <32 x i8> %126 to <4 x i64>
  %128 = shufflevector <4 x i64> %127, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  %129 = bitcast i8* %75 to <2 x i64>*
  store <2 x i64> %128, <2 x i64>* %129, align 1
  %130 = getelementptr inbounds i16, i16* %74, i64 %65
  %131 = getelementptr inbounds i16, i16* %73, i64 %67
  %132 = getelementptr inbounds i8, i8* %75, i64 16
  %133 = add nuw nsw i32 %72, 4
  %134 = icmp slt i32 %133, %6
  br i1 %134, label %71, label %1046

135:                                              ; preds = %135, %38
  %136 = phi i32 [ %209, %135 ], [ 0, %38 ]
  %137 = phi i16* [ %207, %135 ], [ %4, %38 ]
  %138 = phi i16* [ %206, %135 ], [ %2, %38 ]
  %139 = phi i8* [ %208, %135 ], [ %0, %38 ]
  %140 = getelementptr inbounds i16, i16* %138, i64 %39
  %141 = bitcast i16* %140 to <4 x i32>*
  %142 = load <4 x i32>, <4 x i32>* %141, align 1
  %143 = bitcast i16* %138 to <2 x i64>*
  %144 = load <2 x i64>, <2 x i64>* %143, align 1
  %145 = shufflevector <2 x i64> %144, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %146 = bitcast <4 x i64> %145 to <8 x i32>
  %147 = shufflevector <4 x i32> %142, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %148 = shufflevector <8 x i32> %146, <8 x i32> %147, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %149 = getelementptr inbounds i16, i16* %138, i64 %41
  %150 = getelementptr inbounds i16, i16* %138, i64 %43
  %151 = bitcast i16* %149 to <4 x i32>*
  %152 = load <4 x i32>, <4 x i32>* %151, align 1
  %153 = bitcast i16* %150 to <2 x i64>*
  %154 = load <2 x i64>, <2 x i64>* %153, align 1
  %155 = shufflevector <2 x i64> %154, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %156 = bitcast <4 x i64> %155 to <8 x i32>
  %157 = shufflevector <4 x i32> %152, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %158 = shufflevector <8 x i32> %156, <8 x i32> %157, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %159 = getelementptr inbounds i16, i16* %137, i64 %44
  %160 = bitcast i16* %159 to <4 x i32>*
  %161 = load <4 x i32>, <4 x i32>* %160, align 1
  %162 = bitcast i16* %137 to <2 x i64>*
  %163 = load <2 x i64>, <2 x i64>* %162, align 1
  %164 = shufflevector <2 x i64> %163, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %165 = bitcast <4 x i64> %164 to <8 x i32>
  %166 = shufflevector <4 x i32> %161, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %167 = shufflevector <8 x i32> %165, <8 x i32> %166, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %168 = getelementptr inbounds i16, i16* %137, i64 %46
  %169 = getelementptr inbounds i16, i16* %137, i64 %48
  %170 = bitcast i16* %168 to <4 x i32>*
  %171 = load <4 x i32>, <4 x i32>* %170, align 1
  %172 = bitcast i16* %169 to <2 x i64>*
  %173 = load <2 x i64>, <2 x i64>* %172, align 1
  %174 = shufflevector <2 x i64> %173, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %175 = bitcast <4 x i64> %174 to <8 x i32>
  %176 = shufflevector <4 x i32> %171, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %177 = shufflevector <8 x i32> %175, <8 x i32> %176, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %178 = bitcast <8 x i32> %148 to <16 x i16>
  %179 = bitcast <8 x i32> %167 to <16 x i16>
  %180 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %178, <16 x i16> %179) #5
  %181 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %179, <16 x i16> %178) #5
  %182 = icmp ugt <16 x i16> %180, %181
  %183 = select <16 x i1> %182, <16 x i16> %180, <16 x i16> %181
  %184 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %183, <16 x i16> %23) #5
  %185 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %184, i32 %17) #5
  %186 = lshr <16 x i16> %185, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %187 = icmp ult <16 x i16> %186, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %188 = select <16 x i1> %187, <16 x i16> %186, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %189 = add nuw nsw <16 x i16> %188, <i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38>
  %190 = bitcast <8 x i32> %158 to <16 x i16>
  %191 = bitcast <8 x i32> %177 to <16 x i16>
  %192 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %190, <16 x i16> %191) #5
  %193 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %191, <16 x i16> %190) #5
  %194 = icmp ugt <16 x i16> %192, %193
  %195 = select <16 x i1> %194, <16 x i16> %192, <16 x i16> %193
  %196 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %195, <16 x i16> %23) #5
  %197 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %196, i32 %17) #5
  %198 = lshr <16 x i16> %197, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %199 = icmp ult <16 x i16> %198, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %200 = select <16 x i1> %199, <16 x i16> %198, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %201 = add nuw nsw <16 x i16> %200, <i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38>
  %202 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %189, <16 x i16> %201) #5
  %203 = bitcast <32 x i8> %202 to <4 x i64>
  %204 = shufflevector <4 x i64> %203, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %205 = bitcast i8* %139 to <4 x i64>*
  store <4 x i64> %204, <4 x i64>* %205, align 1
  %206 = getelementptr inbounds i16, i16* %138, i64 %50
  %207 = getelementptr inbounds i16, i16* %137, i64 %52
  %208 = getelementptr inbounds i8, i8* %139, i64 32
  %209 = add nuw nsw i32 %136, 4
  %210 = icmp slt i32 %209, %6
  br i1 %210, label %135, label %1046

211:                                              ; preds = %211, %31
  %212 = phi i32 [ %253, %211 ], [ 0, %31 ]
  %213 = phi i16* [ %251, %211 ], [ %4, %31 ]
  %214 = phi i16* [ %250, %211 ], [ %2, %31 ]
  %215 = phi i8* [ %252, %211 ], [ %0, %31 ]
  %216 = bitcast i16* %214 to <16 x i16>*
  %217 = load <16 x i16>, <16 x i16>* %216, align 1
  %218 = getelementptr inbounds i16, i16* %214, i64 %32
  %219 = bitcast i16* %218 to <16 x i16>*
  %220 = load <16 x i16>, <16 x i16>* %219, align 1
  %221 = bitcast i16* %213 to <16 x i16>*
  %222 = load <16 x i16>, <16 x i16>* %221, align 1
  %223 = getelementptr inbounds i16, i16* %213, i64 %33
  %224 = bitcast i16* %223 to <16 x i16>*
  %225 = load <16 x i16>, <16 x i16>* %224, align 1
  %226 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %217, <16 x i16> %222) #5
  %227 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %222, <16 x i16> %217) #5
  %228 = icmp ugt <16 x i16> %226, %227
  %229 = select <16 x i1> %228, <16 x i16> %226, <16 x i16> %227
  %230 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %229, <16 x i16> %23) #5
  %231 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %230, i32 %17) #5
  %232 = lshr <16 x i16> %231, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %233 = icmp ult <16 x i16> %232, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %234 = select <16 x i1> %233, <16 x i16> %232, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %235 = add nuw nsw <16 x i16> %234, <i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38>
  %236 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %220, <16 x i16> %225) #5
  %237 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %225, <16 x i16> %220) #5
  %238 = icmp ugt <16 x i16> %236, %237
  %239 = select <16 x i1> %238, <16 x i16> %236, <16 x i16> %237
  %240 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %239, <16 x i16> %23) #5
  %241 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %240, i32 %17) #5
  %242 = lshr <16 x i16> %241, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %243 = icmp ult <16 x i16> %242, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %244 = select <16 x i1> %243, <16 x i16> %242, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %245 = add nuw nsw <16 x i16> %244, <i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38>
  %246 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %235, <16 x i16> %245) #5
  %247 = bitcast <32 x i8> %246 to <4 x i64>
  %248 = shufflevector <4 x i64> %247, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %249 = bitcast i8* %215 to <4 x i64>*
  store <4 x i64> %248, <4 x i64>* %249, align 1
  %250 = getelementptr inbounds i16, i16* %214, i64 %35
  %251 = getelementptr inbounds i16, i16* %213, i64 %37
  %252 = getelementptr inbounds i8, i8* %215, i64 32
  %253 = add nuw nsw i32 %212, 2
  %254 = icmp slt i32 %253, %6
  br i1 %254, label %211, label %1046

255:                                              ; preds = %255, %28
  %256 = phi i32 [ %297, %255 ], [ 0, %28 ]
  %257 = phi i16* [ %295, %255 ], [ %4, %28 ]
  %258 = phi i16* [ %294, %255 ], [ %2, %28 ]
  %259 = phi i8* [ %296, %255 ], [ %0, %28 ]
  %260 = bitcast i16* %258 to <16 x i16>*
  %261 = load <16 x i16>, <16 x i16>* %260, align 1
  %262 = getelementptr inbounds i16, i16* %258, i64 16
  %263 = bitcast i16* %262 to <16 x i16>*
  %264 = load <16 x i16>, <16 x i16>* %263, align 1
  %265 = bitcast i16* %257 to <16 x i16>*
  %266 = load <16 x i16>, <16 x i16>* %265, align 1
  %267 = getelementptr inbounds i16, i16* %257, i64 16
  %268 = bitcast i16* %267 to <16 x i16>*
  %269 = load <16 x i16>, <16 x i16>* %268, align 1
  %270 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %261, <16 x i16> %266) #5
  %271 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %266, <16 x i16> %261) #5
  %272 = icmp ugt <16 x i16> %270, %271
  %273 = select <16 x i1> %272, <16 x i16> %270, <16 x i16> %271
  %274 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %273, <16 x i16> %23) #5
  %275 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %274, i32 %17) #5
  %276 = lshr <16 x i16> %275, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %277 = icmp ult <16 x i16> %276, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %278 = select <16 x i1> %277, <16 x i16> %276, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %279 = add nuw nsw <16 x i16> %278, <i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38>
  %280 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %264, <16 x i16> %269) #5
  %281 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %269, <16 x i16> %264) #5
  %282 = icmp ugt <16 x i16> %280, %281
  %283 = select <16 x i1> %282, <16 x i16> %280, <16 x i16> %281
  %284 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %283, <16 x i16> %23) #5
  %285 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %284, i32 %17) #5
  %286 = lshr <16 x i16> %285, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %287 = icmp ult <16 x i16> %286, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %288 = select <16 x i1> %287, <16 x i16> %286, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %289 = add nuw nsw <16 x i16> %288, <i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38>
  %290 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %279, <16 x i16> %289) #5
  %291 = bitcast <32 x i8> %290 to <4 x i64>
  %292 = shufflevector <4 x i64> %291, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %293 = bitcast i8* %259 to <4 x i64>*
  store <4 x i64> %292, <4 x i64>* %293, align 1
  %294 = getelementptr inbounds i16, i16* %258, i64 %29
  %295 = getelementptr inbounds i16, i16* %257, i64 %30
  %296 = getelementptr inbounds i8, i8* %259, i64 32
  %297 = add nuw nsw i32 %256, 1
  %298 = icmp slt i32 %297, %6
  br i1 %298, label %255, label %1046

299:                                              ; preds = %299, %25
  %300 = phi i32 [ %378, %299 ], [ 0, %25 ]
  %301 = phi i16* [ %376, %299 ], [ %4, %25 ]
  %302 = phi i16* [ %375, %299 ], [ %2, %25 ]
  %303 = phi i8* [ %377, %299 ], [ %0, %25 ]
  %304 = bitcast i16* %302 to <16 x i16>*
  %305 = load <16 x i16>, <16 x i16>* %304, align 1
  %306 = getelementptr inbounds i16, i16* %302, i64 16
  %307 = bitcast i16* %306 to <16 x i16>*
  %308 = load <16 x i16>, <16 x i16>* %307, align 1
  %309 = getelementptr inbounds i16, i16* %302, i64 32
  %310 = bitcast i16* %309 to <16 x i16>*
  %311 = load <16 x i16>, <16 x i16>* %310, align 1
  %312 = getelementptr inbounds i16, i16* %302, i64 48
  %313 = bitcast i16* %312 to <16 x i16>*
  %314 = load <16 x i16>, <16 x i16>* %313, align 1
  %315 = bitcast i16* %301 to <16 x i16>*
  %316 = load <16 x i16>, <16 x i16>* %315, align 1
  %317 = getelementptr inbounds i16, i16* %301, i64 16
  %318 = bitcast i16* %317 to <16 x i16>*
  %319 = load <16 x i16>, <16 x i16>* %318, align 1
  %320 = getelementptr inbounds i16, i16* %301, i64 32
  %321 = bitcast i16* %320 to <16 x i16>*
  %322 = load <16 x i16>, <16 x i16>* %321, align 1
  %323 = getelementptr inbounds i16, i16* %301, i64 48
  %324 = bitcast i16* %323 to <16 x i16>*
  %325 = load <16 x i16>, <16 x i16>* %324, align 1
  %326 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %305, <16 x i16> %316) #5
  %327 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %316, <16 x i16> %305) #5
  %328 = icmp ugt <16 x i16> %326, %327
  %329 = select <16 x i1> %328, <16 x i16> %326, <16 x i16> %327
  %330 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %329, <16 x i16> %23) #5
  %331 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %330, i32 %17) #5
  %332 = lshr <16 x i16> %331, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %333 = icmp ult <16 x i16> %332, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %334 = select <16 x i1> %333, <16 x i16> %332, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %335 = add nuw nsw <16 x i16> %334, <i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38>
  %336 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %308, <16 x i16> %319) #5
  %337 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %319, <16 x i16> %308) #5
  %338 = icmp ugt <16 x i16> %336, %337
  %339 = select <16 x i1> %338, <16 x i16> %336, <16 x i16> %337
  %340 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %339, <16 x i16> %23) #5
  %341 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %340, i32 %17) #5
  %342 = lshr <16 x i16> %341, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %343 = icmp ult <16 x i16> %342, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %344 = select <16 x i1> %343, <16 x i16> %342, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %345 = add nuw nsw <16 x i16> %344, <i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38>
  %346 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %311, <16 x i16> %322) #5
  %347 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %322, <16 x i16> %311) #5
  %348 = icmp ugt <16 x i16> %346, %347
  %349 = select <16 x i1> %348, <16 x i16> %346, <16 x i16> %347
  %350 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %349, <16 x i16> %23) #5
  %351 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %350, i32 %17) #5
  %352 = lshr <16 x i16> %351, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %353 = icmp ult <16 x i16> %352, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %354 = select <16 x i1> %353, <16 x i16> %352, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %355 = add nuw nsw <16 x i16> %354, <i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38>
  %356 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %314, <16 x i16> %325) #5
  %357 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %325, <16 x i16> %314) #5
  %358 = icmp ugt <16 x i16> %356, %357
  %359 = select <16 x i1> %358, <16 x i16> %356, <16 x i16> %357
  %360 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %359, <16 x i16> %23) #5
  %361 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %360, i32 %17) #5
  %362 = lshr <16 x i16> %361, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %363 = icmp ult <16 x i16> %362, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %364 = select <16 x i1> %363, <16 x i16> %362, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %365 = add nuw nsw <16 x i16> %364, <i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38>
  %366 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %335, <16 x i16> %345) #5
  %367 = bitcast <32 x i8> %366 to <4 x i64>
  %368 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %355, <16 x i16> %365) #5
  %369 = bitcast <32 x i8> %368 to <4 x i64>
  %370 = shufflevector <4 x i64> %367, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %371 = bitcast i8* %303 to <4 x i64>*
  store <4 x i64> %370, <4 x i64>* %371, align 1
  %372 = getelementptr inbounds i8, i8* %303, i64 32
  %373 = shufflevector <4 x i64> %369, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %374 = bitcast i8* %372 to <4 x i64>*
  store <4 x i64> %373, <4 x i64>* %374, align 1
  %375 = getelementptr inbounds i16, i16* %302, i64 %26
  %376 = getelementptr inbounds i16, i16* %301, i64 %27
  %377 = getelementptr inbounds i8, i8* %303, i64 64
  %378 = add nuw nsw i32 %300, 1
  %379 = icmp slt i32 %378, %6
  br i1 %379, label %299, label %1046

380:                                              ; preds = %380, %68
  %381 = phi i32 [ %533, %380 ], [ 0, %68 ]
  %382 = phi i16* [ %531, %380 ], [ %4, %68 ]
  %383 = phi i16* [ %530, %380 ], [ %2, %68 ]
  %384 = phi i8* [ %532, %380 ], [ %0, %68 ]
  %385 = bitcast i16* %383 to <16 x i16>*
  %386 = load <16 x i16>, <16 x i16>* %385, align 1
  %387 = getelementptr inbounds i16, i16* %383, i64 16
  %388 = bitcast i16* %387 to <16 x i16>*
  %389 = load <16 x i16>, <16 x i16>* %388, align 1
  %390 = getelementptr inbounds i16, i16* %383, i64 32
  %391 = bitcast i16* %390 to <16 x i16>*
  %392 = load <16 x i16>, <16 x i16>* %391, align 1
  %393 = getelementptr inbounds i16, i16* %383, i64 48
  %394 = bitcast i16* %393 to <16 x i16>*
  %395 = load <16 x i16>, <16 x i16>* %394, align 1
  %396 = getelementptr inbounds i16, i16* %383, i64 64
  %397 = bitcast i16* %396 to <16 x i16>*
  %398 = load <16 x i16>, <16 x i16>* %397, align 1
  %399 = getelementptr inbounds i16, i16* %383, i64 80
  %400 = bitcast i16* %399 to <16 x i16>*
  %401 = load <16 x i16>, <16 x i16>* %400, align 1
  %402 = getelementptr inbounds i16, i16* %383, i64 96
  %403 = bitcast i16* %402 to <16 x i16>*
  %404 = load <16 x i16>, <16 x i16>* %403, align 1
  %405 = getelementptr inbounds i16, i16* %383, i64 112
  %406 = bitcast i16* %405 to <16 x i16>*
  %407 = load <16 x i16>, <16 x i16>* %406, align 1
  %408 = bitcast i16* %382 to <16 x i16>*
  %409 = load <16 x i16>, <16 x i16>* %408, align 1
  %410 = getelementptr inbounds i16, i16* %382, i64 16
  %411 = bitcast i16* %410 to <16 x i16>*
  %412 = load <16 x i16>, <16 x i16>* %411, align 1
  %413 = getelementptr inbounds i16, i16* %382, i64 32
  %414 = bitcast i16* %413 to <16 x i16>*
  %415 = load <16 x i16>, <16 x i16>* %414, align 1
  %416 = getelementptr inbounds i16, i16* %382, i64 48
  %417 = bitcast i16* %416 to <16 x i16>*
  %418 = load <16 x i16>, <16 x i16>* %417, align 1
  %419 = getelementptr inbounds i16, i16* %382, i64 64
  %420 = bitcast i16* %419 to <16 x i16>*
  %421 = load <16 x i16>, <16 x i16>* %420, align 1
  %422 = getelementptr inbounds i16, i16* %382, i64 80
  %423 = bitcast i16* %422 to <16 x i16>*
  %424 = load <16 x i16>, <16 x i16>* %423, align 1
  %425 = getelementptr inbounds i16, i16* %382, i64 96
  %426 = bitcast i16* %425 to <16 x i16>*
  %427 = load <16 x i16>, <16 x i16>* %426, align 1
  %428 = getelementptr inbounds i16, i16* %382, i64 112
  %429 = bitcast i16* %428 to <16 x i16>*
  %430 = load <16 x i16>, <16 x i16>* %429, align 1
  %431 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %386, <16 x i16> %409) #5
  %432 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %409, <16 x i16> %386) #5
  %433 = icmp ugt <16 x i16> %431, %432
  %434 = select <16 x i1> %433, <16 x i16> %431, <16 x i16> %432
  %435 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %434, <16 x i16> %23) #5
  %436 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %435, i32 %17) #5
  %437 = lshr <16 x i16> %436, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %438 = icmp ult <16 x i16> %437, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %439 = select <16 x i1> %438, <16 x i16> %437, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %440 = add nuw nsw <16 x i16> %439, <i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38>
  %441 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %389, <16 x i16> %412) #5
  %442 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %412, <16 x i16> %389) #5
  %443 = icmp ugt <16 x i16> %441, %442
  %444 = select <16 x i1> %443, <16 x i16> %441, <16 x i16> %442
  %445 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %444, <16 x i16> %23) #5
  %446 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %445, i32 %17) #5
  %447 = lshr <16 x i16> %446, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %448 = icmp ult <16 x i16> %447, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %449 = select <16 x i1> %448, <16 x i16> %447, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %450 = add nuw nsw <16 x i16> %449, <i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38>
  %451 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %392, <16 x i16> %415) #5
  %452 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %415, <16 x i16> %392) #5
  %453 = icmp ugt <16 x i16> %451, %452
  %454 = select <16 x i1> %453, <16 x i16> %451, <16 x i16> %452
  %455 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %454, <16 x i16> %23) #5
  %456 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %455, i32 %17) #5
  %457 = lshr <16 x i16> %456, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %458 = icmp ult <16 x i16> %457, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %459 = select <16 x i1> %458, <16 x i16> %457, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %460 = add nuw nsw <16 x i16> %459, <i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38>
  %461 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %395, <16 x i16> %418) #5
  %462 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %418, <16 x i16> %395) #5
  %463 = icmp ugt <16 x i16> %461, %462
  %464 = select <16 x i1> %463, <16 x i16> %461, <16 x i16> %462
  %465 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %464, <16 x i16> %23) #5
  %466 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %465, i32 %17) #5
  %467 = lshr <16 x i16> %466, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %468 = icmp ult <16 x i16> %467, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %469 = select <16 x i1> %468, <16 x i16> %467, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %470 = add nuw nsw <16 x i16> %469, <i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38>
  %471 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %398, <16 x i16> %421) #5
  %472 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %421, <16 x i16> %398) #5
  %473 = icmp ugt <16 x i16> %471, %472
  %474 = select <16 x i1> %473, <16 x i16> %471, <16 x i16> %472
  %475 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %474, <16 x i16> %23) #5
  %476 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %475, i32 %17) #5
  %477 = lshr <16 x i16> %476, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %478 = icmp ult <16 x i16> %477, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %479 = select <16 x i1> %478, <16 x i16> %477, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %480 = add nuw nsw <16 x i16> %479, <i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38>
  %481 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %401, <16 x i16> %424) #5
  %482 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %424, <16 x i16> %401) #5
  %483 = icmp ugt <16 x i16> %481, %482
  %484 = select <16 x i1> %483, <16 x i16> %481, <16 x i16> %482
  %485 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %484, <16 x i16> %23) #5
  %486 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %485, i32 %17) #5
  %487 = lshr <16 x i16> %486, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %488 = icmp ult <16 x i16> %487, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %489 = select <16 x i1> %488, <16 x i16> %487, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %490 = add nuw nsw <16 x i16> %489, <i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38>
  %491 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %404, <16 x i16> %427) #5
  %492 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %427, <16 x i16> %404) #5
  %493 = icmp ugt <16 x i16> %491, %492
  %494 = select <16 x i1> %493, <16 x i16> %491, <16 x i16> %492
  %495 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %494, <16 x i16> %23) #5
  %496 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %495, i32 %17) #5
  %497 = lshr <16 x i16> %496, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %498 = icmp ult <16 x i16> %497, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %499 = select <16 x i1> %498, <16 x i16> %497, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %500 = add nuw nsw <16 x i16> %499, <i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38>
  %501 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %407, <16 x i16> %430) #5
  %502 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %430, <16 x i16> %407) #5
  %503 = icmp ugt <16 x i16> %501, %502
  %504 = select <16 x i1> %503, <16 x i16> %501, <16 x i16> %502
  %505 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %504, <16 x i16> %23) #5
  %506 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %505, i32 %17) #5
  %507 = lshr <16 x i16> %506, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %508 = icmp ult <16 x i16> %507, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %509 = select <16 x i1> %508, <16 x i16> %507, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %510 = add nuw nsw <16 x i16> %509, <i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38>
  %511 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %440, <16 x i16> %450) #5
  %512 = bitcast <32 x i8> %511 to <4 x i64>
  %513 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %460, <16 x i16> %470) #5
  %514 = bitcast <32 x i8> %513 to <4 x i64>
  %515 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %480, <16 x i16> %490) #5
  %516 = bitcast <32 x i8> %515 to <4 x i64>
  %517 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %500, <16 x i16> %510) #5
  %518 = bitcast <32 x i8> %517 to <4 x i64>
  %519 = shufflevector <4 x i64> %512, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %520 = bitcast i8* %384 to <4 x i64>*
  store <4 x i64> %519, <4 x i64>* %520, align 1
  %521 = getelementptr inbounds i8, i8* %384, i64 32
  %522 = shufflevector <4 x i64> %514, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %523 = bitcast i8* %521 to <4 x i64>*
  store <4 x i64> %522, <4 x i64>* %523, align 1
  %524 = getelementptr inbounds i8, i8* %384, i64 64
  %525 = shufflevector <4 x i64> %516, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %526 = bitcast i8* %524 to <4 x i64>*
  store <4 x i64> %525, <4 x i64>* %526, align 1
  %527 = getelementptr inbounds i8, i8* %384, i64 96
  %528 = shufflevector <4 x i64> %518, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %529 = bitcast i8* %527 to <4 x i64>*
  store <4 x i64> %528, <4 x i64>* %529, align 1
  %530 = getelementptr inbounds i16, i16* %383, i64 %69
  %531 = getelementptr inbounds i16, i16* %382, i64 %70
  %532 = getelementptr inbounds i8, i8* %384, i64 128
  %533 = add nuw nsw i32 %381, 1
  %534 = icmp slt i32 %533, %6
  br i1 %534, label %380, label %1046

535:                                              ; preds = %10
  switch i32 %7, label %579 [
    i32 4, label %564
    i32 8, label %549
    i32 16, label %542
    i32 32, label %539
    i32 64, label %536
  ]

536:                                              ; preds = %535
  %537 = sext i32 %3 to i64
  %538 = sext i32 %5 to i64
  br label %810

539:                                              ; preds = %535
  %540 = sext i32 %3 to i64
  %541 = sext i32 %5 to i64
  br label %766

542:                                              ; preds = %535
  %543 = sext i32 %3 to i64
  %544 = sext i32 %5 to i64
  %545 = shl i32 %3, 1
  %546 = sext i32 %545 to i64
  %547 = shl i32 %5, 1
  %548 = sext i32 %547 to i64
  br label %722

549:                                              ; preds = %535
  %550 = sext i32 %3 to i64
  %551 = mul nsw i32 %3, 3
  %552 = sext i32 %551 to i64
  %553 = shl nsw i32 %3, 1
  %554 = sext i32 %553 to i64
  %555 = sext i32 %5 to i64
  %556 = mul nsw i32 %5, 3
  %557 = sext i32 %556 to i64
  %558 = shl nsw i32 %5, 1
  %559 = sext i32 %558 to i64
  %560 = shl i32 %3, 2
  %561 = sext i32 %560 to i64
  %562 = shl i32 %5, 2
  %563 = sext i32 %562 to i64
  br label %646

564:                                              ; preds = %535
  %565 = sext i32 %3 to i64
  %566 = shl nsw i32 %3, 1
  %567 = sext i32 %566 to i64
  %568 = mul nsw i32 %3, 3
  %569 = sext i32 %568 to i64
  %570 = sext i32 %5 to i64
  %571 = shl nsw i32 %5, 1
  %572 = sext i32 %571 to i64
  %573 = mul nsw i32 %5, 3
  %574 = sext i32 %573 to i64
  %575 = shl i32 %3, 2
  %576 = sext i32 %575 to i64
  %577 = shl i32 %5, 2
  %578 = sext i32 %577 to i64
  br label %582

579:                                              ; preds = %535
  %580 = sext i32 %3 to i64
  %581 = sext i32 %5 to i64
  br label %891

582:                                              ; preds = %582, %564
  %583 = phi i32 [ %644, %582 ], [ 0, %564 ]
  %584 = phi i16* [ %642, %582 ], [ %4, %564 ]
  %585 = phi i16* [ %641, %582 ], [ %2, %564 ]
  %586 = phi i8* [ %643, %582 ], [ %0, %564 ]
  %587 = bitcast i16* %585 to i64*
  %588 = load i64, i64* %587, align 1
  %589 = insertelement <4 x i64> undef, i64 %588, i32 0
  %590 = getelementptr inbounds i16, i16* %585, i64 %565
  %591 = bitcast i16* %590 to i64*
  %592 = load i64, i64* %591, align 1
  %593 = getelementptr inbounds i16, i16* %585, i64 %567
  %594 = bitcast i16* %593 to i64*
  %595 = load i64, i64* %594, align 1
  %596 = insertelement <2 x i64> undef, i64 %595, i32 0
  %597 = getelementptr inbounds i16, i16* %585, i64 %569
  %598 = bitcast i16* %597 to i64*
  %599 = load i64, i64* %598, align 1
  %600 = bitcast i16* %584 to i64*
  %601 = load i64, i64* %600, align 1
  %602 = insertelement <4 x i64> undef, i64 %601, i32 0
  %603 = getelementptr inbounds i16, i16* %584, i64 %570
  %604 = bitcast i16* %603 to i64*
  %605 = load i64, i64* %604, align 1
  %606 = getelementptr inbounds i16, i16* %584, i64 %572
  %607 = bitcast i16* %606 to i64*
  %608 = load i64, i64* %607, align 1
  %609 = insertelement <2 x i64> undef, i64 %608, i32 0
  %610 = getelementptr inbounds i16, i16* %584, i64 %574
  %611 = bitcast i16* %610 to i64*
  %612 = load i64, i64* %611, align 1
  %613 = insertelement <2 x i64> %596, i64 %599, i32 1
  %614 = insertelement <4 x i64> %589, i64 %592, i32 1
  %615 = bitcast <4 x i64> %614 to <8 x i32>
  %616 = bitcast <2 x i64> %613 to <4 x i32>
  %617 = shufflevector <4 x i32> %616, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %618 = shufflevector <8 x i32> %615, <8 x i32> %617, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %619 = insertelement <2 x i64> %609, i64 %612, i32 1
  %620 = insertelement <4 x i64> %602, i64 %605, i32 1
  %621 = bitcast <4 x i64> %620 to <8 x i32>
  %622 = bitcast <2 x i64> %619 to <4 x i32>
  %623 = shufflevector <4 x i32> %622, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %624 = shufflevector <8 x i32> %621, <8 x i32> %623, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %625 = bitcast <8 x i32> %618 to <16 x i16>
  %626 = bitcast <8 x i32> %624 to <16 x i16>
  %627 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %625, <16 x i16> %626) #5
  %628 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %626, <16 x i16> %625) #5
  %629 = icmp ugt <16 x i16> %627, %628
  %630 = select <16 x i1> %629, <16 x i16> %627, <16 x i16> %628
  %631 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %630, <16 x i16> %23) #5
  %632 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %631, i32 %17) #5
  %633 = lshr <16 x i16> %632, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %634 = icmp ult <16 x i16> %633, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %635 = select <16 x i1> %634, <16 x i16> %633, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %636 = sub nuw nsw <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>, %635
  %637 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %636, <16 x i16> undef) #5
  %638 = bitcast <32 x i8> %637 to <4 x i64>
  %639 = shufflevector <4 x i64> %638, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  %640 = bitcast i8* %586 to <2 x i64>*
  store <2 x i64> %639, <2 x i64>* %640, align 1
  %641 = getelementptr inbounds i16, i16* %585, i64 %576
  %642 = getelementptr inbounds i16, i16* %584, i64 %578
  %643 = getelementptr inbounds i8, i8* %586, i64 16
  %644 = add nuw nsw i32 %583, 4
  %645 = icmp slt i32 %644, %6
  br i1 %645, label %582, label %1046

646:                                              ; preds = %646, %549
  %647 = phi i32 [ %720, %646 ], [ 0, %549 ]
  %648 = phi i16* [ %718, %646 ], [ %4, %549 ]
  %649 = phi i16* [ %717, %646 ], [ %2, %549 ]
  %650 = phi i8* [ %719, %646 ], [ %0, %549 ]
  %651 = getelementptr inbounds i16, i16* %649, i64 %550
  %652 = bitcast i16* %651 to <4 x i32>*
  %653 = load <4 x i32>, <4 x i32>* %652, align 1
  %654 = bitcast i16* %649 to <2 x i64>*
  %655 = load <2 x i64>, <2 x i64>* %654, align 1
  %656 = shufflevector <2 x i64> %655, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %657 = bitcast <4 x i64> %656 to <8 x i32>
  %658 = shufflevector <4 x i32> %653, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %659 = shufflevector <8 x i32> %657, <8 x i32> %658, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %660 = getelementptr inbounds i16, i16* %649, i64 %552
  %661 = getelementptr inbounds i16, i16* %649, i64 %554
  %662 = bitcast i16* %660 to <4 x i32>*
  %663 = load <4 x i32>, <4 x i32>* %662, align 1
  %664 = bitcast i16* %661 to <2 x i64>*
  %665 = load <2 x i64>, <2 x i64>* %664, align 1
  %666 = shufflevector <2 x i64> %665, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %667 = bitcast <4 x i64> %666 to <8 x i32>
  %668 = shufflevector <4 x i32> %663, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %669 = shufflevector <8 x i32> %667, <8 x i32> %668, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %670 = getelementptr inbounds i16, i16* %648, i64 %555
  %671 = bitcast i16* %670 to <4 x i32>*
  %672 = load <4 x i32>, <4 x i32>* %671, align 1
  %673 = bitcast i16* %648 to <2 x i64>*
  %674 = load <2 x i64>, <2 x i64>* %673, align 1
  %675 = shufflevector <2 x i64> %674, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %676 = bitcast <4 x i64> %675 to <8 x i32>
  %677 = shufflevector <4 x i32> %672, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %678 = shufflevector <8 x i32> %676, <8 x i32> %677, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %679 = getelementptr inbounds i16, i16* %648, i64 %557
  %680 = getelementptr inbounds i16, i16* %648, i64 %559
  %681 = bitcast i16* %679 to <4 x i32>*
  %682 = load <4 x i32>, <4 x i32>* %681, align 1
  %683 = bitcast i16* %680 to <2 x i64>*
  %684 = load <2 x i64>, <2 x i64>* %683, align 1
  %685 = shufflevector <2 x i64> %684, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %686 = bitcast <4 x i64> %685 to <8 x i32>
  %687 = shufflevector <4 x i32> %682, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %688 = shufflevector <8 x i32> %686, <8 x i32> %687, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %689 = bitcast <8 x i32> %659 to <16 x i16>
  %690 = bitcast <8 x i32> %678 to <16 x i16>
  %691 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %689, <16 x i16> %690) #5
  %692 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %690, <16 x i16> %689) #5
  %693 = icmp ugt <16 x i16> %691, %692
  %694 = select <16 x i1> %693, <16 x i16> %691, <16 x i16> %692
  %695 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %694, <16 x i16> %23) #5
  %696 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %695, i32 %17) #5
  %697 = lshr <16 x i16> %696, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %698 = icmp ult <16 x i16> %697, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %699 = select <16 x i1> %698, <16 x i16> %697, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %700 = sub nuw nsw <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>, %699
  %701 = bitcast <8 x i32> %669 to <16 x i16>
  %702 = bitcast <8 x i32> %688 to <16 x i16>
  %703 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %701, <16 x i16> %702) #5
  %704 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %702, <16 x i16> %701) #5
  %705 = icmp ugt <16 x i16> %703, %704
  %706 = select <16 x i1> %705, <16 x i16> %703, <16 x i16> %704
  %707 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %706, <16 x i16> %23) #5
  %708 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %707, i32 %17) #5
  %709 = lshr <16 x i16> %708, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %710 = icmp ult <16 x i16> %709, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %711 = select <16 x i1> %710, <16 x i16> %709, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %712 = sub nuw nsw <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>, %711
  %713 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %700, <16 x i16> %712) #5
  %714 = bitcast <32 x i8> %713 to <4 x i64>
  %715 = shufflevector <4 x i64> %714, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %716 = bitcast i8* %650 to <4 x i64>*
  store <4 x i64> %715, <4 x i64>* %716, align 1
  %717 = getelementptr inbounds i16, i16* %649, i64 %561
  %718 = getelementptr inbounds i16, i16* %648, i64 %563
  %719 = getelementptr inbounds i8, i8* %650, i64 32
  %720 = add nuw nsw i32 %647, 4
  %721 = icmp slt i32 %720, %6
  br i1 %721, label %646, label %1046

722:                                              ; preds = %722, %542
  %723 = phi i32 [ %764, %722 ], [ 0, %542 ]
  %724 = phi i16* [ %762, %722 ], [ %4, %542 ]
  %725 = phi i16* [ %761, %722 ], [ %2, %542 ]
  %726 = phi i8* [ %763, %722 ], [ %0, %542 ]
  %727 = bitcast i16* %725 to <16 x i16>*
  %728 = load <16 x i16>, <16 x i16>* %727, align 1
  %729 = getelementptr inbounds i16, i16* %725, i64 %543
  %730 = bitcast i16* %729 to <16 x i16>*
  %731 = load <16 x i16>, <16 x i16>* %730, align 1
  %732 = bitcast i16* %724 to <16 x i16>*
  %733 = load <16 x i16>, <16 x i16>* %732, align 1
  %734 = getelementptr inbounds i16, i16* %724, i64 %544
  %735 = bitcast i16* %734 to <16 x i16>*
  %736 = load <16 x i16>, <16 x i16>* %735, align 1
  %737 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %728, <16 x i16> %733) #5
  %738 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %733, <16 x i16> %728) #5
  %739 = icmp ugt <16 x i16> %737, %738
  %740 = select <16 x i1> %739, <16 x i16> %737, <16 x i16> %738
  %741 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %740, <16 x i16> %23) #5
  %742 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %741, i32 %17) #5
  %743 = lshr <16 x i16> %742, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %744 = icmp ult <16 x i16> %743, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %745 = select <16 x i1> %744, <16 x i16> %743, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %746 = sub nuw nsw <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>, %745
  %747 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %731, <16 x i16> %736) #5
  %748 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %736, <16 x i16> %731) #5
  %749 = icmp ugt <16 x i16> %747, %748
  %750 = select <16 x i1> %749, <16 x i16> %747, <16 x i16> %748
  %751 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %750, <16 x i16> %23) #5
  %752 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %751, i32 %17) #5
  %753 = lshr <16 x i16> %752, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %754 = icmp ult <16 x i16> %753, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %755 = select <16 x i1> %754, <16 x i16> %753, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %756 = sub nuw nsw <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>, %755
  %757 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %746, <16 x i16> %756) #5
  %758 = bitcast <32 x i8> %757 to <4 x i64>
  %759 = shufflevector <4 x i64> %758, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %760 = bitcast i8* %726 to <4 x i64>*
  store <4 x i64> %759, <4 x i64>* %760, align 1
  %761 = getelementptr inbounds i16, i16* %725, i64 %546
  %762 = getelementptr inbounds i16, i16* %724, i64 %548
  %763 = getelementptr inbounds i8, i8* %726, i64 32
  %764 = add nuw nsw i32 %723, 2
  %765 = icmp slt i32 %764, %6
  br i1 %765, label %722, label %1046

766:                                              ; preds = %766, %539
  %767 = phi i32 [ %808, %766 ], [ 0, %539 ]
  %768 = phi i16* [ %806, %766 ], [ %4, %539 ]
  %769 = phi i16* [ %805, %766 ], [ %2, %539 ]
  %770 = phi i8* [ %807, %766 ], [ %0, %539 ]
  %771 = bitcast i16* %769 to <16 x i16>*
  %772 = load <16 x i16>, <16 x i16>* %771, align 1
  %773 = getelementptr inbounds i16, i16* %769, i64 16
  %774 = bitcast i16* %773 to <16 x i16>*
  %775 = load <16 x i16>, <16 x i16>* %774, align 1
  %776 = bitcast i16* %768 to <16 x i16>*
  %777 = load <16 x i16>, <16 x i16>* %776, align 1
  %778 = getelementptr inbounds i16, i16* %768, i64 16
  %779 = bitcast i16* %778 to <16 x i16>*
  %780 = load <16 x i16>, <16 x i16>* %779, align 1
  %781 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %772, <16 x i16> %777) #5
  %782 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %777, <16 x i16> %772) #5
  %783 = icmp ugt <16 x i16> %781, %782
  %784 = select <16 x i1> %783, <16 x i16> %781, <16 x i16> %782
  %785 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %784, <16 x i16> %23) #5
  %786 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %785, i32 %17) #5
  %787 = lshr <16 x i16> %786, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %788 = icmp ult <16 x i16> %787, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %789 = select <16 x i1> %788, <16 x i16> %787, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %790 = sub nuw nsw <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>, %789
  %791 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %775, <16 x i16> %780) #5
  %792 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %780, <16 x i16> %775) #5
  %793 = icmp ugt <16 x i16> %791, %792
  %794 = select <16 x i1> %793, <16 x i16> %791, <16 x i16> %792
  %795 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %794, <16 x i16> %23) #5
  %796 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %795, i32 %17) #5
  %797 = lshr <16 x i16> %796, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %798 = icmp ult <16 x i16> %797, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %799 = select <16 x i1> %798, <16 x i16> %797, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %800 = sub nuw nsw <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>, %799
  %801 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %790, <16 x i16> %800) #5
  %802 = bitcast <32 x i8> %801 to <4 x i64>
  %803 = shufflevector <4 x i64> %802, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %804 = bitcast i8* %770 to <4 x i64>*
  store <4 x i64> %803, <4 x i64>* %804, align 1
  %805 = getelementptr inbounds i16, i16* %769, i64 %540
  %806 = getelementptr inbounds i16, i16* %768, i64 %541
  %807 = getelementptr inbounds i8, i8* %770, i64 32
  %808 = add nuw nsw i32 %767, 1
  %809 = icmp slt i32 %808, %6
  br i1 %809, label %766, label %1046

810:                                              ; preds = %810, %536
  %811 = phi i32 [ %889, %810 ], [ 0, %536 ]
  %812 = phi i16* [ %887, %810 ], [ %4, %536 ]
  %813 = phi i16* [ %886, %810 ], [ %2, %536 ]
  %814 = phi i8* [ %888, %810 ], [ %0, %536 ]
  %815 = bitcast i16* %813 to <16 x i16>*
  %816 = load <16 x i16>, <16 x i16>* %815, align 1
  %817 = getelementptr inbounds i16, i16* %813, i64 16
  %818 = bitcast i16* %817 to <16 x i16>*
  %819 = load <16 x i16>, <16 x i16>* %818, align 1
  %820 = getelementptr inbounds i16, i16* %813, i64 32
  %821 = bitcast i16* %820 to <16 x i16>*
  %822 = load <16 x i16>, <16 x i16>* %821, align 1
  %823 = getelementptr inbounds i16, i16* %813, i64 48
  %824 = bitcast i16* %823 to <16 x i16>*
  %825 = load <16 x i16>, <16 x i16>* %824, align 1
  %826 = bitcast i16* %812 to <16 x i16>*
  %827 = load <16 x i16>, <16 x i16>* %826, align 1
  %828 = getelementptr inbounds i16, i16* %812, i64 16
  %829 = bitcast i16* %828 to <16 x i16>*
  %830 = load <16 x i16>, <16 x i16>* %829, align 1
  %831 = getelementptr inbounds i16, i16* %812, i64 32
  %832 = bitcast i16* %831 to <16 x i16>*
  %833 = load <16 x i16>, <16 x i16>* %832, align 1
  %834 = getelementptr inbounds i16, i16* %812, i64 48
  %835 = bitcast i16* %834 to <16 x i16>*
  %836 = load <16 x i16>, <16 x i16>* %835, align 1
  %837 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %816, <16 x i16> %827) #5
  %838 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %827, <16 x i16> %816) #5
  %839 = icmp ugt <16 x i16> %837, %838
  %840 = select <16 x i1> %839, <16 x i16> %837, <16 x i16> %838
  %841 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %840, <16 x i16> %23) #5
  %842 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %841, i32 %17) #5
  %843 = lshr <16 x i16> %842, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %844 = icmp ult <16 x i16> %843, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %845 = select <16 x i1> %844, <16 x i16> %843, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %846 = sub nuw nsw <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>, %845
  %847 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %819, <16 x i16> %830) #5
  %848 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %830, <16 x i16> %819) #5
  %849 = icmp ugt <16 x i16> %847, %848
  %850 = select <16 x i1> %849, <16 x i16> %847, <16 x i16> %848
  %851 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %850, <16 x i16> %23) #5
  %852 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %851, i32 %17) #5
  %853 = lshr <16 x i16> %852, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %854 = icmp ult <16 x i16> %853, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %855 = select <16 x i1> %854, <16 x i16> %853, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %856 = sub nuw nsw <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>, %855
  %857 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %822, <16 x i16> %833) #5
  %858 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %833, <16 x i16> %822) #5
  %859 = icmp ugt <16 x i16> %857, %858
  %860 = select <16 x i1> %859, <16 x i16> %857, <16 x i16> %858
  %861 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %860, <16 x i16> %23) #5
  %862 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %861, i32 %17) #5
  %863 = lshr <16 x i16> %862, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %864 = icmp ult <16 x i16> %863, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %865 = select <16 x i1> %864, <16 x i16> %863, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %866 = sub nuw nsw <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>, %865
  %867 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %825, <16 x i16> %836) #5
  %868 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %836, <16 x i16> %825) #5
  %869 = icmp ugt <16 x i16> %867, %868
  %870 = select <16 x i1> %869, <16 x i16> %867, <16 x i16> %868
  %871 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %870, <16 x i16> %23) #5
  %872 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %871, i32 %17) #5
  %873 = lshr <16 x i16> %872, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %874 = icmp ult <16 x i16> %873, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %875 = select <16 x i1> %874, <16 x i16> %873, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %876 = sub nuw nsw <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>, %875
  %877 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %846, <16 x i16> %856) #5
  %878 = bitcast <32 x i8> %877 to <4 x i64>
  %879 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %866, <16 x i16> %876) #5
  %880 = bitcast <32 x i8> %879 to <4 x i64>
  %881 = shufflevector <4 x i64> %878, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %882 = bitcast i8* %814 to <4 x i64>*
  store <4 x i64> %881, <4 x i64>* %882, align 1
  %883 = getelementptr inbounds i8, i8* %814, i64 32
  %884 = shufflevector <4 x i64> %880, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %885 = bitcast i8* %883 to <4 x i64>*
  store <4 x i64> %884, <4 x i64>* %885, align 1
  %886 = getelementptr inbounds i16, i16* %813, i64 %537
  %887 = getelementptr inbounds i16, i16* %812, i64 %538
  %888 = getelementptr inbounds i8, i8* %814, i64 64
  %889 = add nuw nsw i32 %811, 1
  %890 = icmp slt i32 %889, %6
  br i1 %890, label %810, label %1046

891:                                              ; preds = %891, %579
  %892 = phi i32 [ %1044, %891 ], [ 0, %579 ]
  %893 = phi i16* [ %1042, %891 ], [ %4, %579 ]
  %894 = phi i16* [ %1041, %891 ], [ %2, %579 ]
  %895 = phi i8* [ %1043, %891 ], [ %0, %579 ]
  %896 = bitcast i16* %894 to <16 x i16>*
  %897 = load <16 x i16>, <16 x i16>* %896, align 1
  %898 = getelementptr inbounds i16, i16* %894, i64 16
  %899 = bitcast i16* %898 to <16 x i16>*
  %900 = load <16 x i16>, <16 x i16>* %899, align 1
  %901 = getelementptr inbounds i16, i16* %894, i64 32
  %902 = bitcast i16* %901 to <16 x i16>*
  %903 = load <16 x i16>, <16 x i16>* %902, align 1
  %904 = getelementptr inbounds i16, i16* %894, i64 48
  %905 = bitcast i16* %904 to <16 x i16>*
  %906 = load <16 x i16>, <16 x i16>* %905, align 1
  %907 = getelementptr inbounds i16, i16* %894, i64 64
  %908 = bitcast i16* %907 to <16 x i16>*
  %909 = load <16 x i16>, <16 x i16>* %908, align 1
  %910 = getelementptr inbounds i16, i16* %894, i64 80
  %911 = bitcast i16* %910 to <16 x i16>*
  %912 = load <16 x i16>, <16 x i16>* %911, align 1
  %913 = getelementptr inbounds i16, i16* %894, i64 96
  %914 = bitcast i16* %913 to <16 x i16>*
  %915 = load <16 x i16>, <16 x i16>* %914, align 1
  %916 = getelementptr inbounds i16, i16* %894, i64 112
  %917 = bitcast i16* %916 to <16 x i16>*
  %918 = load <16 x i16>, <16 x i16>* %917, align 1
  %919 = bitcast i16* %893 to <16 x i16>*
  %920 = load <16 x i16>, <16 x i16>* %919, align 1
  %921 = getelementptr inbounds i16, i16* %893, i64 16
  %922 = bitcast i16* %921 to <16 x i16>*
  %923 = load <16 x i16>, <16 x i16>* %922, align 1
  %924 = getelementptr inbounds i16, i16* %893, i64 32
  %925 = bitcast i16* %924 to <16 x i16>*
  %926 = load <16 x i16>, <16 x i16>* %925, align 1
  %927 = getelementptr inbounds i16, i16* %893, i64 48
  %928 = bitcast i16* %927 to <16 x i16>*
  %929 = load <16 x i16>, <16 x i16>* %928, align 1
  %930 = getelementptr inbounds i16, i16* %893, i64 64
  %931 = bitcast i16* %930 to <16 x i16>*
  %932 = load <16 x i16>, <16 x i16>* %931, align 1
  %933 = getelementptr inbounds i16, i16* %893, i64 80
  %934 = bitcast i16* %933 to <16 x i16>*
  %935 = load <16 x i16>, <16 x i16>* %934, align 1
  %936 = getelementptr inbounds i16, i16* %893, i64 96
  %937 = bitcast i16* %936 to <16 x i16>*
  %938 = load <16 x i16>, <16 x i16>* %937, align 1
  %939 = getelementptr inbounds i16, i16* %893, i64 112
  %940 = bitcast i16* %939 to <16 x i16>*
  %941 = load <16 x i16>, <16 x i16>* %940, align 1
  %942 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %897, <16 x i16> %920) #5
  %943 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %920, <16 x i16> %897) #5
  %944 = icmp ugt <16 x i16> %942, %943
  %945 = select <16 x i1> %944, <16 x i16> %942, <16 x i16> %943
  %946 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %945, <16 x i16> %23) #5
  %947 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %946, i32 %17) #5
  %948 = lshr <16 x i16> %947, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %949 = icmp ult <16 x i16> %948, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %950 = select <16 x i1> %949, <16 x i16> %948, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %951 = sub nuw nsw <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>, %950
  %952 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %900, <16 x i16> %923) #5
  %953 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %923, <16 x i16> %900) #5
  %954 = icmp ugt <16 x i16> %952, %953
  %955 = select <16 x i1> %954, <16 x i16> %952, <16 x i16> %953
  %956 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %955, <16 x i16> %23) #5
  %957 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %956, i32 %17) #5
  %958 = lshr <16 x i16> %957, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %959 = icmp ult <16 x i16> %958, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %960 = select <16 x i1> %959, <16 x i16> %958, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %961 = sub nuw nsw <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>, %960
  %962 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %903, <16 x i16> %926) #5
  %963 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %926, <16 x i16> %903) #5
  %964 = icmp ugt <16 x i16> %962, %963
  %965 = select <16 x i1> %964, <16 x i16> %962, <16 x i16> %963
  %966 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %965, <16 x i16> %23) #5
  %967 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %966, i32 %17) #5
  %968 = lshr <16 x i16> %967, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %969 = icmp ult <16 x i16> %968, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %970 = select <16 x i1> %969, <16 x i16> %968, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %971 = sub nuw nsw <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>, %970
  %972 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %906, <16 x i16> %929) #5
  %973 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %929, <16 x i16> %906) #5
  %974 = icmp ugt <16 x i16> %972, %973
  %975 = select <16 x i1> %974, <16 x i16> %972, <16 x i16> %973
  %976 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %975, <16 x i16> %23) #5
  %977 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %976, i32 %17) #5
  %978 = lshr <16 x i16> %977, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %979 = icmp ult <16 x i16> %978, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %980 = select <16 x i1> %979, <16 x i16> %978, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %981 = sub nuw nsw <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>, %980
  %982 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %909, <16 x i16> %932) #5
  %983 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %932, <16 x i16> %909) #5
  %984 = icmp ugt <16 x i16> %982, %983
  %985 = select <16 x i1> %984, <16 x i16> %982, <16 x i16> %983
  %986 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %985, <16 x i16> %23) #5
  %987 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %986, i32 %17) #5
  %988 = lshr <16 x i16> %987, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %989 = icmp ult <16 x i16> %988, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %990 = select <16 x i1> %989, <16 x i16> %988, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %991 = sub nuw nsw <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>, %990
  %992 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %912, <16 x i16> %935) #5
  %993 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %935, <16 x i16> %912) #5
  %994 = icmp ugt <16 x i16> %992, %993
  %995 = select <16 x i1> %994, <16 x i16> %992, <16 x i16> %993
  %996 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %995, <16 x i16> %23) #5
  %997 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %996, i32 %17) #5
  %998 = lshr <16 x i16> %997, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %999 = icmp ult <16 x i16> %998, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %1000 = select <16 x i1> %999, <16 x i16> %998, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %1001 = sub nuw nsw <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>, %1000
  %1002 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %915, <16 x i16> %938) #5
  %1003 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %938, <16 x i16> %915) #5
  %1004 = icmp ugt <16 x i16> %1002, %1003
  %1005 = select <16 x i1> %1004, <16 x i16> %1002, <16 x i16> %1003
  %1006 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %1005, <16 x i16> %23) #5
  %1007 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %1006, i32 %17) #5
  %1008 = lshr <16 x i16> %1007, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %1009 = icmp ult <16 x i16> %1008, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %1010 = select <16 x i1> %1009, <16 x i16> %1008, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %1011 = sub nuw nsw <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>, %1010
  %1012 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %918, <16 x i16> %941) #5
  %1013 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %941, <16 x i16> %918) #5
  %1014 = icmp ugt <16 x i16> %1012, %1013
  %1015 = select <16 x i1> %1014, <16 x i16> %1012, <16 x i16> %1013
  %1016 = tail call <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16> %1015, <16 x i16> %23) #5
  %1017 = tail call <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16> %1016, i32 %17) #5
  %1018 = lshr <16 x i16> %1017, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %1019 = icmp ult <16 x i16> %1018, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %1020 = select <16 x i1> %1019, <16 x i16> %1018, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %1021 = sub nuw nsw <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>, %1020
  %1022 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %951, <16 x i16> %961) #5
  %1023 = bitcast <32 x i8> %1022 to <4 x i64>
  %1024 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %971, <16 x i16> %981) #5
  %1025 = bitcast <32 x i8> %1024 to <4 x i64>
  %1026 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %991, <16 x i16> %1001) #5
  %1027 = bitcast <32 x i8> %1026 to <4 x i64>
  %1028 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %1011, <16 x i16> %1021) #5
  %1029 = bitcast <32 x i8> %1028 to <4 x i64>
  %1030 = shufflevector <4 x i64> %1023, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %1031 = bitcast i8* %895 to <4 x i64>*
  store <4 x i64> %1030, <4 x i64>* %1031, align 1
  %1032 = getelementptr inbounds i8, i8* %895, i64 32
  %1033 = shufflevector <4 x i64> %1025, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %1034 = bitcast i8* %1032 to <4 x i64>*
  store <4 x i64> %1033, <4 x i64>* %1034, align 1
  %1035 = getelementptr inbounds i8, i8* %895, i64 64
  %1036 = shufflevector <4 x i64> %1027, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %1037 = bitcast i8* %1035 to <4 x i64>*
  store <4 x i64> %1036, <4 x i64>* %1037, align 1
  %1038 = getelementptr inbounds i8, i8* %895, i64 96
  %1039 = shufflevector <4 x i64> %1029, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %1040 = bitcast i8* %1038 to <4 x i64>*
  store <4 x i64> %1039, <4 x i64>* %1040, align 1
  %1041 = getelementptr inbounds i16, i16* %894, i64 %580
  %1042 = getelementptr inbounds i16, i16* %893, i64 %581
  %1043 = getelementptr inbounds i8, i8* %895, i64 128
  %1044 = add nuw nsw i32 %892, 1
  %1045 = icmp slt i32 %1044, %6
  br i1 %1045, label %891, label %1046

1046:                                             ; preds = %810, %766, %722, %646, %582, %891, %299, %255, %211, %135, %71, %380
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @av1_build_compound_diffwtd_mask_highbd_avx2(i8*, i8 zeroext, i8*, i32, i8*, i32, i32, i32, i32) local_unnamed_addr #1 {
  %10 = icmp slt i32 %7, 16
  br i1 %10, label %11, label %12

11:                                               ; preds = %9
  tail call void @av1_build_compound_diffwtd_mask_highbd_ssse3(i8* %0, i8 zeroext %1, i8* %2, i32 %3, i8* %4, i32 %5, i32 %6, i32 %7, i32 %8) #5
  br label %197

12:                                               ; preds = %9
  %13 = ptrtoint i8* %2 to i64
  %14 = shl i64 %13, 1
  %15 = inttoptr i64 %14 to i16*
  %16 = ptrtoint i8* %4 to i64
  %17 = shl i64 %16, 1
  %18 = inttoptr i64 %17 to i16*
  %19 = icmp eq i32 %8, 8
  br i1 %19, label %20, label %105

20:                                               ; preds = %12
  %21 = icmp eq i8 %1, 1
  %22 = icmp sgt i32 %6, 0
  br i1 %21, label %28, label %23

23:                                               ; preds = %20
  br i1 %22, label %24, label %197

24:                                               ; preds = %23
  %25 = sext i32 %3 to i64
  %26 = sext i32 %5 to i64
  %27 = sext i32 %7 to i64
  br label %69

28:                                               ; preds = %20
  br i1 %22, label %29, label %197

29:                                               ; preds = %28
  %30 = sext i32 %3 to i64
  %31 = sext i32 %5 to i64
  %32 = sext i32 %7 to i64
  br label %33

33:                                               ; preds = %38, %29
  %34 = phi i8* [ %0, %29 ], [ %41, %38 ]
  %35 = phi i16* [ %15, %29 ], [ %39, %38 ]
  %36 = phi i16* [ %18, %29 ], [ %40, %38 ]
  %37 = phi i32 [ 0, %29 ], [ %42, %38 ]
  br label %44

38:                                               ; preds = %44
  %39 = getelementptr inbounds i16, i16* %35, i64 %30
  %40 = getelementptr inbounds i16, i16* %36, i64 %31
  %41 = getelementptr inbounds i8, i8* %34, i64 %32
  %42 = add nuw nsw i32 %37, 1
  %43 = icmp eq i32 %42, %6
  br i1 %43, label %197, label %33

44:                                               ; preds = %33, %44
  %45 = phi i64 [ 0, %33 ], [ %67, %44 ]
  %46 = getelementptr inbounds i16, i16* %35, i64 %45
  %47 = bitcast i16* %46 to <16 x i16>*
  %48 = load <16 x i16>, <16 x i16>* %47, align 1
  %49 = getelementptr inbounds i16, i16* %36, i64 %45
  %50 = bitcast i16* %49 to <16 x i16>*
  %51 = load <16 x i16>, <16 x i16>* %50, align 1
  %52 = sub <16 x i16> %48, %51
  %53 = sub <16 x i16> zeroinitializer, %52
  %54 = icmp slt <16 x i16> %52, zeroinitializer
  %55 = select <16 x i1> %54, <16 x i16> %53, <16 x i16> %52
  %56 = ashr <16 x i16> %55, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %57 = icmp sgt <16 x i16> %56, <i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38>
  %58 = select <16 x i1> %57, <16 x i16> %56, <16 x i16> <i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38>
  %59 = icmp slt <16 x i16> %58, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %60 = select <16 x i1> %59, <16 x i16> %58, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %61 = sub nsw <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>, %60
  %62 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %61, <16 x i16> undef) #5
  %63 = bitcast <32 x i8> %62 to <4 x i64>
  %64 = shufflevector <4 x i64> %63, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  %65 = getelementptr inbounds i8, i8* %34, i64 %45
  %66 = bitcast i8* %65 to <2 x i64>*
  store <2 x i64> %64, <2 x i64>* %66, align 1
  %67 = add nuw nsw i64 %45, 16
  %68 = icmp slt i64 %67, %32
  br i1 %68, label %44, label %38

69:                                               ; preds = %74, %24
  %70 = phi i8* [ %0, %24 ], [ %77, %74 ]
  %71 = phi i16* [ %15, %24 ], [ %75, %74 ]
  %72 = phi i16* [ %18, %24 ], [ %76, %74 ]
  %73 = phi i32 [ 0, %24 ], [ %78, %74 ]
  br label %80

74:                                               ; preds = %80
  %75 = getelementptr inbounds i16, i16* %71, i64 %25
  %76 = getelementptr inbounds i16, i16* %72, i64 %26
  %77 = getelementptr inbounds i8, i8* %70, i64 %27
  %78 = add nuw nsw i32 %73, 1
  %79 = icmp eq i32 %78, %6
  br i1 %79, label %197, label %69

80:                                               ; preds = %69, %80
  %81 = phi i64 [ 0, %69 ], [ %103, %80 ]
  %82 = getelementptr inbounds i16, i16* %71, i64 %81
  %83 = bitcast i16* %82 to <16 x i16>*
  %84 = load <16 x i16>, <16 x i16>* %83, align 1
  %85 = getelementptr inbounds i16, i16* %72, i64 %81
  %86 = bitcast i16* %85 to <16 x i16>*
  %87 = load <16 x i16>, <16 x i16>* %86, align 1
  %88 = sub <16 x i16> %84, %87
  %89 = sub <16 x i16> zeroinitializer, %88
  %90 = icmp slt <16 x i16> %88, zeroinitializer
  %91 = select <16 x i1> %90, <16 x i16> %89, <16 x i16> %88
  %92 = ashr <16 x i16> %91, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %93 = icmp sgt <16 x i16> %92, <i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38>
  %94 = select <16 x i1> %93, <16 x i16> %92, <16 x i16> <i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38, i16 -38>
  %95 = icmp slt <16 x i16> %94, <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %96 = select <16 x i1> %95, <16 x i16> %94, <16 x i16> <i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26, i16 26>
  %97 = add nsw <16 x i16> %96, <i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38>
  %98 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %97, <16 x i16> undef) #5
  %99 = bitcast <32 x i8> %98 to <4 x i64>
  %100 = shufflevector <4 x i64> %99, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  %101 = getelementptr inbounds i8, i8* %70, i64 %81
  %102 = bitcast i8* %101 to <2 x i64>*
  store <2 x i64> %100, <2 x i64>* %102, align 1
  %103 = add nuw nsw i64 %81, 16
  %104 = icmp slt i64 %103, %27
  br i1 %104, label %80, label %74

105:                                              ; preds = %12
  %106 = add nsw i32 %8, -4
  %107 = zext i32 %106 to i64
  %108 = insertelement <2 x i64> undef, i64 %107, i32 0
  %109 = shufflevector <2 x i64> %108, <2 x i64> undef, <2 x i32> zeroinitializer
  %110 = icmp eq i8 %1, 1
  %111 = icmp sgt i32 %6, 0
  br i1 %110, label %118, label %112

112:                                              ; preds = %105
  br i1 %111, label %113, label %197

113:                                              ; preds = %112
  %114 = bitcast <2 x i64> %109 to <8 x i16>
  %115 = sext i32 %3 to i64
  %116 = sext i32 %5 to i64
  %117 = sext i32 %7 to i64
  br label %161

118:                                              ; preds = %105
  br i1 %111, label %119, label %197

119:                                              ; preds = %118
  %120 = bitcast <2 x i64> %109 to <8 x i16>
  %121 = sext i32 %3 to i64
  %122 = sext i32 %5 to i64
  %123 = sext i32 %7 to i64
  br label %124

124:                                              ; preds = %129, %119
  %125 = phi i8* [ %0, %119 ], [ %132, %129 ]
  %126 = phi i16* [ %15, %119 ], [ %130, %129 ]
  %127 = phi i16* [ %18, %119 ], [ %131, %129 ]
  %128 = phi i32 [ 0, %119 ], [ %133, %129 ]
  br label %135

129:                                              ; preds = %135
  %130 = getelementptr inbounds i16, i16* %126, i64 %121
  %131 = getelementptr inbounds i16, i16* %127, i64 %122
  %132 = getelementptr inbounds i8, i8* %125, i64 %123
  %133 = add nuw nsw i32 %128, 1
  %134 = icmp eq i32 %133, %6
  br i1 %134, label %197, label %124

135:                                              ; preds = %124, %135
  %136 = phi i64 [ 0, %124 ], [ %159, %135 ]
  %137 = getelementptr inbounds i16, i16* %126, i64 %136
  %138 = bitcast i16* %137 to <16 x i16>*
  %139 = load <16 x i16>, <16 x i16>* %138, align 1
  %140 = getelementptr inbounds i16, i16* %127, i64 %136
  %141 = bitcast i16* %140 to <16 x i16>*
  %142 = load <16 x i16>, <16 x i16>* %141, align 1
  %143 = sub <16 x i16> %139, %142
  %144 = sub <16 x i16> zeroinitializer, %143
  %145 = icmp slt <16 x i16> %143, zeroinitializer
  %146 = select <16 x i1> %145, <16 x i16> %144, <16 x i16> %143
  %147 = tail call <16 x i16> @llvm.x86.avx2.psra.w(<16 x i16> %146, <8 x i16> %120) #5
  %148 = add <16 x i16> %147, <i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38>
  %149 = icmp sgt <16 x i16> %148, zeroinitializer
  %150 = select <16 x i1> %149, <16 x i16> %148, <16 x i16> zeroinitializer
  %151 = icmp slt <16 x i16> %150, <i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64>
  %152 = select <16 x i1> %151, <16 x i16> %150, <16 x i16> <i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64>
  %153 = sub nuw nsw <16 x i16> <i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64>, %152
  %154 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %153, <16 x i16> undef) #5
  %155 = bitcast <32 x i8> %154 to <4 x i64>
  %156 = shufflevector <4 x i64> %155, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  %157 = getelementptr inbounds i8, i8* %125, i64 %136
  %158 = bitcast i8* %157 to <2 x i64>*
  store <2 x i64> %156, <2 x i64>* %158, align 1
  %159 = add nuw nsw i64 %136, 16
  %160 = icmp slt i64 %159, %123
  br i1 %160, label %135, label %129

161:                                              ; preds = %166, %113
  %162 = phi i8* [ %0, %113 ], [ %169, %166 ]
  %163 = phi i16* [ %15, %113 ], [ %167, %166 ]
  %164 = phi i32 [ 0, %113 ], [ %170, %166 ]
  %165 = phi i16* [ %18, %113 ], [ %168, %166 ]
  br label %172

166:                                              ; preds = %172
  %167 = getelementptr inbounds i16, i16* %163, i64 %115
  %168 = getelementptr inbounds i16, i16* %165, i64 %116
  %169 = getelementptr inbounds i8, i8* %162, i64 %117
  %170 = add nuw nsw i32 %164, 1
  %171 = icmp eq i32 %170, %6
  br i1 %171, label %197, label %161

172:                                              ; preds = %161, %172
  %173 = phi i64 [ 0, %161 ], [ %195, %172 ]
  %174 = getelementptr inbounds i16, i16* %163, i64 %173
  %175 = bitcast i16* %174 to <16 x i16>*
  %176 = load <16 x i16>, <16 x i16>* %175, align 1
  %177 = getelementptr inbounds i16, i16* %165, i64 %173
  %178 = bitcast i16* %177 to <16 x i16>*
  %179 = load <16 x i16>, <16 x i16>* %178, align 1
  %180 = sub <16 x i16> %176, %179
  %181 = sub <16 x i16> zeroinitializer, %180
  %182 = icmp slt <16 x i16> %180, zeroinitializer
  %183 = select <16 x i1> %182, <16 x i16> %181, <16 x i16> %180
  %184 = tail call <16 x i16> @llvm.x86.avx2.psra.w(<16 x i16> %183, <8 x i16> %114) #5
  %185 = add <16 x i16> %184, <i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38, i16 38>
  %186 = icmp sgt <16 x i16> %185, zeroinitializer
  %187 = select <16 x i1> %186, <16 x i16> %185, <16 x i16> zeroinitializer
  %188 = icmp slt <16 x i16> %187, <i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64>
  %189 = select <16 x i1> %188, <16 x i16> %187, <16 x i16> <i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64, i16 64>
  %190 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %189, <16 x i16> undef) #5
  %191 = bitcast <32 x i8> %190 to <4 x i64>
  %192 = shufflevector <4 x i64> %191, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  %193 = getelementptr inbounds i8, i8* %162, i64 %173
  %194 = bitcast i8* %193 to <2 x i64>*
  store <2 x i64> %192, <2 x i64>* %194, align 1
  %195 = add nuw nsw i64 %173, 16
  %196 = icmp slt i64 %195, %117
  br i1 %196, label %172, label %166

197:                                              ; preds = %166, %129, %74, %38, %112, %118, %23, %28, %11
  ret void
}

declare void @av1_build_compound_diffwtd_mask_highbd_ssse3(i8*, i8 zeroext, i8*, i32, i8*, i32, i32, i32, i32) local_unnamed_addr #2

; Function Attrs: nounwind readnone
declare <16 x i16> @llvm.x86.avx2.psrli.w(<16 x i16>, i32) #3

; Function Attrs: nounwind readnone
declare <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16>, <16 x i16>) #3

; Function Attrs: nounwind readnone speculatable
declare <16 x i16> @llvm.usub.sat.v16i16(<16 x i16>, <16 x i16>) #4

; Function Attrs: nounwind readnone speculatable
declare <16 x i16> @llvm.uadd.sat.v16i16(<16 x i16>, <16 x i16>) #4

; Function Attrs: nounwind readnone
declare <16 x i16> @llvm.x86.avx2.psra.w(<16 x i16>, <8 x i16>) #3

attributes #0 = { nofree nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="256" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+avx,+avx2,+cx8,+fxsr,+mmx,+popcnt,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="256" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+avx,+avx2,+cx8,+fxsr,+mmx,+popcnt,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #2 = { "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+avx,+avx2,+cx8,+fxsr,+mmx,+popcnt,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #3 = { nounwind readnone }
attributes #4 = { nounwind readnone speculatable }
attributes #5 = { nounwind }

!llvm.module.flags = !{!0, !1}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 2}
