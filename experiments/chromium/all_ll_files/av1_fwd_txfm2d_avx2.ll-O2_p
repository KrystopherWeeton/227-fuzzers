; ModuleID = '../../third_party/libaom/source/libaom/av1/encoder/x86/av1_fwd_txfm2d_avx2.c'
source_filename = "../../third_party/libaom/source/libaom/av1/encoder/x86/av1_fwd_txfm2d_avx2.c"
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

module asm ".symver exp, exp@GLIBC_2.2.5"
module asm ".symver exp2, exp2@GLIBC_2.2.5"
module asm ".symver exp2f, exp2f@GLIBC_2.2.5"
module asm ".symver expf, expf@GLIBC_2.2.5"
module asm ".symver lgamma, lgamma@GLIBC_2.2.5"
module asm ".symver lgammaf, lgammaf@GLIBC_2.2.5"
module asm ".symver lgammal, lgammal@GLIBC_2.2.5"
module asm ".symver log, log@GLIBC_2.2.5"
module asm ".symver log2, log2@GLIBC_2.2.5"
module asm ".symver log2f, log2f@GLIBC_2.2.5"
module asm ".symver logf, logf@GLIBC_2.2.5"
module asm ".symver pow, pow@GLIBC_2.2.5"
module asm ".symver powf, powf@GLIBC_2.2.5"

%struct.txfm_param = type { i8, i8, i32, i32, i32, i8, i32 }

@fwd_txfm2d_func_ls = internal unnamed_addr constant [19 x void (i16*, i32*, i32, i8, i32)*] [void (i16*, i32*, i32, i8, i32)* @av1_lowbd_fwd_txfm2d_4x4_sse2, void (i16*, i32*, i32, i8, i32)* @av1_lowbd_fwd_txfm2d_8x8_sse2, void (i16*, i32*, i32, i8, i32)* @lowbd_fwd_txfm2d_16x16_avx2, void (i16*, i32*, i32, i8, i32)* @lowbd_fwd_txfm2d_32x32_avx2, void (i16*, i32*, i32, i8, i32)* @lowbd_fwd_txfm2d_64x64_avx2, void (i16*, i32*, i32, i8, i32)* @av1_lowbd_fwd_txfm2d_4x8_sse2, void (i16*, i32*, i32, i8, i32)* @av1_lowbd_fwd_txfm2d_8x4_sse2, void (i16*, i32*, i32, i8, i32)* @lowbd_fwd_txfm2d_8x16_avx2, void (i16*, i32*, i32, i8, i32)* @lowbd_fwd_txfm2d_16x8_avx2, void (i16*, i32*, i32, i8, i32)* @lowbd_fwd_txfm2d_16x32_avx2, void (i16*, i32*, i32, i8, i32)* @lowbd_fwd_txfm2d_32x16_avx2, void (i16*, i32*, i32, i8, i32)* @lowbd_fwd_txfm2d_32x64_avx2, void (i16*, i32*, i32, i8, i32)* @lowbd_fwd_txfm2d_64x32_avx2, void (i16*, i32*, i32, i8, i32)* @av1_lowbd_fwd_txfm2d_4x16_sse2, void (i16*, i32*, i32, i8, i32)* @av1_lowbd_fwd_txfm2d_16x4_sse2, void (i16*, i32*, i32, i8, i32)* @av1_lowbd_fwd_txfm2d_8x32_sse2, void (i16*, i32*, i32, i8, i32)* @av1_lowbd_fwd_txfm2d_32x8_sse2, void (i16*, i32*, i32, i8, i32)* @lowbd_fwd_txfm2d_16x64_avx2, void (i16*, i32*, i32, i8, i32)* @lowbd_fwd_txfm2d_64x16_avx2], align 16
@av1_fwd_txfm_shift_ls = external local_unnamed_addr global [19 x i8*], align 16
@av1_fwd_cos_bit_col = external local_unnamed_addr constant [5 x [5 x i8]], align 16
@av1_fwd_cos_bit_row = external local_unnamed_addr constant [5 x [5 x i8]], align 16
@col_txfm16x16_arr = internal unnamed_addr constant [16 x void (<4 x i64>*, <4 x i64>*, i8)*] [void (<4 x i64>*, <4 x i64>*, i8)* @fdct16x16_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fadst16x16_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fdct16x16_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fadst16x16_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fadst16x16_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fdct16x16_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fadst16x16_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fadst16x16_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fadst16x16_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fidentity16x16_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fdct16x16_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fidentity16x16_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fadst16x16_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fidentity16x16_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fadst16x16_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fidentity16x16_new_avx2], align 16
@row_txfm16x16_arr = internal unnamed_addr constant [16 x void (<4 x i64>*, <4 x i64>*, i8)*] [void (<4 x i64>*, <4 x i64>*, i8)* @fdct16x16_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fdct16x16_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fadst16x16_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fadst16x16_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fdct16x16_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fadst16x16_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fadst16x16_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fadst16x16_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fadst16x16_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fidentity16x16_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fidentity16x16_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fdct16x16_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fidentity16x16_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fadst16x16_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fidentity16x16_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fadst16x16_new_avx2], align 16
@av1_cospi_arr_data = external local_unnamed_addr constant [7 x [64 x i32]], align 16
@col_txfm16x32_arr = internal unnamed_addr constant [16 x void (<4 x i64>*, <4 x i64>*, i8)*] [void (<4 x i64>*, <4 x i64>*, i8)* @fdct16x32_avx2, void (<4 x i64>*, <4 x i64>*, i8)* null, void (<4 x i64>*, <4 x i64>*, i8)* null, void (<4 x i64>*, <4 x i64>*, i8)* null, void (<4 x i64>*, <4 x i64>*, i8)* null, void (<4 x i64>*, <4 x i64>*, i8)* null, void (<4 x i64>*, <4 x i64>*, i8)* null, void (<4 x i64>*, <4 x i64>*, i8)* null, void (<4 x i64>*, <4 x i64>*, i8)* null, void (<4 x i64>*, <4 x i64>*, i8)* @fidentity16x32_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fdct16x32_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fidentity16x32_avx2, void (<4 x i64>*, <4 x i64>*, i8)* null, void (<4 x i64>*, <4 x i64>*, i8)* null, void (<4 x i64>*, <4 x i64>*, i8)* null, void (<4 x i64>*, <4 x i64>*, i8)* null], align 16
@row_txfm16x32_arr = internal unnamed_addr constant [16 x void (<4 x i64>*, <4 x i64>*, i8)*] [void (<4 x i64>*, <4 x i64>*, i8)* @fdct16x32_avx2, void (<4 x i64>*, <4 x i64>*, i8)* null, void (<4 x i64>*, <4 x i64>*, i8)* null, void (<4 x i64>*, <4 x i64>*, i8)* null, void (<4 x i64>*, <4 x i64>*, i8)* null, void (<4 x i64>*, <4 x i64>*, i8)* null, void (<4 x i64>*, <4 x i64>*, i8)* null, void (<4 x i64>*, <4 x i64>*, i8)* null, void (<4 x i64>*, <4 x i64>*, i8)* null, void (<4 x i64>*, <4 x i64>*, i8)* @fidentity16x32_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fidentity16x32_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fdct16x32_avx2, void (<4 x i64>*, <4 x i64>*, i8)* null, void (<4 x i64>*, <4 x i64>*, i8)* null, void (<4 x i64>*, <4 x i64>*, i8)* null, void (<4 x i64>*, <4 x i64>*, i8)* null], align 16
@col_txfm8x16_arr = internal unnamed_addr constant [16 x void (<2 x i64>*, <2 x i64>*, i8)*] [void (<2 x i64>*, <2 x i64>*, i8)* @fdct8x16_new_avx2, void (<2 x i64>*, <2 x i64>*, i8)* @fadst8x16_new_avx2, void (<2 x i64>*, <2 x i64>*, i8)* @fdct8x16_new_avx2, void (<2 x i64>*, <2 x i64>*, i8)* @fadst8x16_new_avx2, void (<2 x i64>*, <2 x i64>*, i8)* @fadst8x16_new_avx2, void (<2 x i64>*, <2 x i64>*, i8)* @fdct8x16_new_avx2, void (<2 x i64>*, <2 x i64>*, i8)* @fadst8x16_new_avx2, void (<2 x i64>*, <2 x i64>*, i8)* @fadst8x16_new_avx2, void (<2 x i64>*, <2 x i64>*, i8)* @fadst8x16_new_avx2, void (<2 x i64>*, <2 x i64>*, i8)* @fidentity8x16_new_avx2, void (<2 x i64>*, <2 x i64>*, i8)* @fdct8x16_new_avx2, void (<2 x i64>*, <2 x i64>*, i8)* @fidentity8x16_new_avx2, void (<2 x i64>*, <2 x i64>*, i8)* @fadst8x16_new_avx2, void (<2 x i64>*, <2 x i64>*, i8)* @fidentity8x16_new_avx2, void (<2 x i64>*, <2 x i64>*, i8)* @fadst8x16_new_avx2, void (<2 x i64>*, <2 x i64>*, i8)* @fidentity8x16_new_avx2], align 16
@row_txfm8x16_arr = internal unnamed_addr constant [16 x void (<4 x i64>*, <4 x i64>*, i8)*] [void (<4 x i64>*, <4 x i64>*, i8)* @fdct8x8_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fdct8x8_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fadst8x8_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fadst8x8_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fdct8x8_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fadst8x8_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fadst8x8_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fadst8x8_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fadst8x8_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fidentity8x8_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fidentity8x8_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fdct8x8_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fidentity8x8_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fadst8x8_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fidentity8x8_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fadst8x8_new_avx2], align 16
@col_txfm16x8_arr = internal unnamed_addr constant [16 x void (<4 x i64>*, <4 x i64>*, i8)*] [void (<4 x i64>*, <4 x i64>*, i8)* @fdct8x8_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fadst8x8_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fdct8x8_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fadst8x8_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fadst8x8_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fdct8x8_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fadst8x8_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fadst8x8_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fadst8x8_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fidentity8x8_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fdct8x8_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fidentity8x8_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fadst8x8_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fidentity8x8_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fadst8x8_new_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @fidentity8x8_new_avx2], align 16
@row_txfm16x8_arr = internal unnamed_addr constant [16 x void (<2 x i64>*, <2 x i64>*, i8)*] [void (<2 x i64>*, <2 x i64>*, i8)* @fdct8x16_new_avx2, void (<2 x i64>*, <2 x i64>*, i8)* @fdct8x16_new_avx2, void (<2 x i64>*, <2 x i64>*, i8)* @fadst8x16_new_avx2, void (<2 x i64>*, <2 x i64>*, i8)* @fadst8x16_new_avx2, void (<2 x i64>*, <2 x i64>*, i8)* @fdct8x16_new_avx2, void (<2 x i64>*, <2 x i64>*, i8)* @fadst8x16_new_avx2, void (<2 x i64>*, <2 x i64>*, i8)* @fadst8x16_new_avx2, void (<2 x i64>*, <2 x i64>*, i8)* @fadst8x16_new_avx2, void (<2 x i64>*, <2 x i64>*, i8)* @fadst8x16_new_avx2, void (<2 x i64>*, <2 x i64>*, i8)* @fidentity8x16_new_avx2, void (<2 x i64>*, <2 x i64>*, i8)* @fidentity8x16_new_avx2, void (<2 x i64>*, <2 x i64>*, i8)* @fdct8x16_new_avx2, void (<2 x i64>*, <2 x i64>*, i8)* @fidentity8x16_new_avx2, void (<2 x i64>*, <2 x i64>*, i8)* @fadst8x16_new_avx2, void (<2 x i64>*, <2 x i64>*, i8)* @fidentity8x16_new_avx2, void (<2 x i64>*, <2 x i64>*, i8)* @fadst8x16_new_avx2], align 16

; Function Attrs: nounwind ssp uwtable
define hidden void @av1_lowbd_fwd_txfm_avx2(i16*, i32*, i32, %struct.txfm_param*) local_unnamed_addr #0 {
  %5 = getelementptr inbounds %struct.txfm_param, %struct.txfm_param* %3, i64 0, i32 1
  %6 = load i8, i8* %5, align 1
  %7 = getelementptr inbounds %struct.txfm_param, %struct.txfm_param* %3, i64 0, i32 2
  %8 = load i32, i32* %7, align 4
  %9 = icmp ne i32 %8, 0
  %10 = icmp eq i8 %6, 0
  %11 = and i1 %10, %9
  br i1 %11, label %12, label %13

12:                                               ; preds = %4
  tail call void @av1_lowbd_fwd_txfm_c(i16* %0, i32* %1, i32 %2, %struct.txfm_param* %3) #8
  br label %21

13:                                               ; preds = %4
  %14 = zext i8 %6 to i64
  %15 = getelementptr inbounds [19 x void (i16*, i32*, i32, i8, i32)*], [19 x void (i16*, i32*, i32, i8, i32)*]* @fwd_txfm2d_func_ls, i64 0, i64 %14
  %16 = load void (i16*, i32*, i32, i8, i32)*, void (i16*, i32*, i32, i8, i32)** %15, align 8
  %17 = getelementptr inbounds %struct.txfm_param, %struct.txfm_param* %3, i64 0, i32 0
  %18 = load i8, i8* %17, align 4
  %19 = getelementptr inbounds %struct.txfm_param, %struct.txfm_param* %3, i64 0, i32 3
  %20 = load i32, i32* %19, align 4
  tail call void %16(i16* %0, i32* %1, i32 %2, i8 zeroext %18, i32 %20) #8
  br label %21

21:                                               ; preds = %13, %12
  ret void
}

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #1

declare void @av1_lowbd_fwd_txfm_c(i16*, i32*, i32, %struct.txfm_param*) local_unnamed_addr #2

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #1

declare void @av1_lowbd_fwd_txfm2d_4x4_sse2(i16*, i32*, i32, i8 zeroext, i32) #2

declare void @av1_lowbd_fwd_txfm2d_8x8_sse2(i16*, i32*, i32, i8 zeroext, i32) #2

; Function Attrs: nounwind ssp uwtable
define internal void @lowbd_fwd_txfm2d_16x16_avx2(i16* nocapture readonly, i32* nocapture, i32, i8 zeroext, i32) #3 {
  %6 = alloca [16 x <4 x i64>], align 32
  %7 = alloca [16 x <4 x i64>], align 32
  %8 = bitcast [16 x <4 x i64>]* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 512, i8* nonnull %8) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %8, i8 -86, i64 512, i1 false)
  %9 = bitcast [16 x <4 x i64>]* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 512, i8* nonnull %9) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %9, i8 -86, i64 512, i1 false)
  %10 = load i8*, i8** getelementptr inbounds ([19 x i8*], [19 x i8*]* @av1_fwd_txfm_shift_ls, i64 0, i64 2), align 16
  %11 = load i8, i8* getelementptr inbounds ([5 x [5 x i8]], [5 x [5 x i8]]* @av1_fwd_cos_bit_col, i64 0, i64 2, i64 2), align 2
  %12 = load i8, i8* getelementptr inbounds ([5 x [5 x i8]], [5 x [5 x i8]]* @av1_fwd_cos_bit_row, i64 0, i64 2, i64 2), align 2
  %13 = zext i8 %3 to i64
  %14 = getelementptr inbounds [16 x void (<4 x i64>*, <4 x i64>*, i8)*], [16 x void (<4 x i64>*, <4 x i64>*, i8)*]* @col_txfm16x16_arr, i64 0, i64 %13
  %15 = load void (<4 x i64>*, <4 x i64>*, i8)*, void (<4 x i64>*, <4 x i64>*, i8)** %14, align 8
  %16 = getelementptr inbounds [16 x void (<4 x i64>*, <4 x i64>*, i8)*], [16 x void (<4 x i64>*, <4 x i64>*, i8)*]* @row_txfm16x16_arr, i64 0, i64 %13
  %17 = load void (<4 x i64>*, <4 x i64>*, i8)*, void (<4 x i64>*, <4 x i64>*, i8)** %16, align 8
  switch i8 %3, label %100 [
    i8 6, label %19
    i8 15, label %18
    i8 7, label %18
    i8 5, label %18
    i8 14, label %20
    i8 8, label %20
    i8 4, label %20
  ]

18:                                               ; preds = %5, %5, %5
  br label %100

19:                                               ; preds = %5
  br label %20

20:                                               ; preds = %5, %5, %5, %19
  %21 = phi i32 [ 1, %19 ], [ 0, %5 ], [ 0, %5 ], [ 0, %5 ]
  %22 = sext i32 %2 to i64
  %23 = bitcast i16* %0 to <4 x i64>*
  %24 = load <4 x i64>, <4 x i64>* %23, align 32
  %25 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 15
  store <4 x i64> %24, <4 x i64>* %25, align 32
  %26 = getelementptr inbounds i16, i16* %0, i64 %22
  %27 = bitcast i16* %26 to <4 x i64>*
  %28 = load <4 x i64>, <4 x i64>* %27, align 32
  %29 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 14
  store <4 x i64> %28, <4 x i64>* %29, align 32
  %30 = shl nsw i64 %22, 1
  %31 = getelementptr inbounds i16, i16* %0, i64 %30
  %32 = bitcast i16* %31 to <4 x i64>*
  %33 = load <4 x i64>, <4 x i64>* %32, align 32
  %34 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 13
  store <4 x i64> %33, <4 x i64>* %34, align 32
  %35 = mul nsw i64 %22, 3
  %36 = getelementptr inbounds i16, i16* %0, i64 %35
  %37 = bitcast i16* %36 to <4 x i64>*
  %38 = load <4 x i64>, <4 x i64>* %37, align 32
  %39 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 12
  store <4 x i64> %38, <4 x i64>* %39, align 32
  %40 = shl nsw i64 %22, 2
  %41 = getelementptr inbounds i16, i16* %0, i64 %40
  %42 = bitcast i16* %41 to <4 x i64>*
  %43 = load <4 x i64>, <4 x i64>* %42, align 32
  %44 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 11
  store <4 x i64> %43, <4 x i64>* %44, align 32
  %45 = mul nsw i64 %22, 5
  %46 = getelementptr inbounds i16, i16* %0, i64 %45
  %47 = bitcast i16* %46 to <4 x i64>*
  %48 = load <4 x i64>, <4 x i64>* %47, align 32
  %49 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 10
  store <4 x i64> %48, <4 x i64>* %49, align 32
  %50 = mul nsw i64 %22, 6
  %51 = getelementptr inbounds i16, i16* %0, i64 %50
  %52 = bitcast i16* %51 to <4 x i64>*
  %53 = load <4 x i64>, <4 x i64>* %52, align 32
  %54 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 9
  store <4 x i64> %53, <4 x i64>* %54, align 32
  %55 = mul nsw i64 %22, 7
  %56 = getelementptr inbounds i16, i16* %0, i64 %55
  %57 = bitcast i16* %56 to <4 x i64>*
  %58 = load <4 x i64>, <4 x i64>* %57, align 32
  %59 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 8
  store <4 x i64> %58, <4 x i64>* %59, align 32
  %60 = shl nsw i64 %22, 3
  %61 = getelementptr inbounds i16, i16* %0, i64 %60
  %62 = bitcast i16* %61 to <4 x i64>*
  %63 = load <4 x i64>, <4 x i64>* %62, align 32
  %64 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 7
  store <4 x i64> %63, <4 x i64>* %64, align 32
  %65 = mul nsw i64 %22, 9
  %66 = getelementptr inbounds i16, i16* %0, i64 %65
  %67 = bitcast i16* %66 to <4 x i64>*
  %68 = load <4 x i64>, <4 x i64>* %67, align 32
  %69 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 6
  store <4 x i64> %68, <4 x i64>* %69, align 32
  %70 = mul nsw i64 %22, 10
  %71 = getelementptr inbounds i16, i16* %0, i64 %70
  %72 = bitcast i16* %71 to <4 x i64>*
  %73 = load <4 x i64>, <4 x i64>* %72, align 32
  %74 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 5
  store <4 x i64> %73, <4 x i64>* %74, align 32
  %75 = mul nsw i64 %22, 11
  %76 = getelementptr inbounds i16, i16* %0, i64 %75
  %77 = bitcast i16* %76 to <4 x i64>*
  %78 = load <4 x i64>, <4 x i64>* %77, align 32
  %79 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 4
  store <4 x i64> %78, <4 x i64>* %79, align 32
  %80 = mul nsw i64 %22, 12
  %81 = getelementptr inbounds i16, i16* %0, i64 %80
  %82 = bitcast i16* %81 to <4 x i64>*
  %83 = load <4 x i64>, <4 x i64>* %82, align 32
  %84 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 3
  store <4 x i64> %83, <4 x i64>* %84, align 32
  %85 = mul nsw i64 %22, 13
  %86 = getelementptr inbounds i16, i16* %0, i64 %85
  %87 = bitcast i16* %86 to <4 x i64>*
  %88 = load <4 x i64>, <4 x i64>* %87, align 32
  %89 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 2
  store <4 x i64> %88, <4 x i64>* %89, align 32
  %90 = mul nsw i64 %22, 14
  %91 = getelementptr inbounds i16, i16* %0, i64 %90
  %92 = bitcast i16* %91 to <4 x i64>*
  %93 = load <4 x i64>, <4 x i64>* %92, align 32
  %94 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 1
  store <4 x i64> %93, <4 x i64>* %94, align 32
  %95 = mul nsw i64 %22, 15
  %96 = getelementptr inbounds i16, i16* %0, i64 %95
  %97 = bitcast i16* %96 to <4 x i64>*
  %98 = load <4 x i64>, <4 x i64>* %97, align 32
  %99 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 0
  store <4 x i64> %98, <4 x i64>* %99, align 32
  br label %180

100:                                              ; preds = %18, %5
  %101 = phi i32 [ 0, %5 ], [ 1, %18 ]
  %102 = sext i32 %2 to i64
  %103 = bitcast i16* %0 to <4 x i64>*
  %104 = load <4 x i64>, <4 x i64>* %103, align 32
  %105 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 0
  store <4 x i64> %104, <4 x i64>* %105, align 32
  %106 = getelementptr inbounds i16, i16* %0, i64 %102
  %107 = bitcast i16* %106 to <4 x i64>*
  %108 = load <4 x i64>, <4 x i64>* %107, align 32
  %109 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 1
  store <4 x i64> %108, <4 x i64>* %109, align 32
  %110 = shl nsw i64 %102, 1
  %111 = getelementptr inbounds i16, i16* %0, i64 %110
  %112 = bitcast i16* %111 to <4 x i64>*
  %113 = load <4 x i64>, <4 x i64>* %112, align 32
  %114 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 2
  store <4 x i64> %113, <4 x i64>* %114, align 32
  %115 = mul nsw i64 %102, 3
  %116 = getelementptr inbounds i16, i16* %0, i64 %115
  %117 = bitcast i16* %116 to <4 x i64>*
  %118 = load <4 x i64>, <4 x i64>* %117, align 32
  %119 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 3
  store <4 x i64> %118, <4 x i64>* %119, align 32
  %120 = shl nsw i64 %102, 2
  %121 = getelementptr inbounds i16, i16* %0, i64 %120
  %122 = bitcast i16* %121 to <4 x i64>*
  %123 = load <4 x i64>, <4 x i64>* %122, align 32
  %124 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 4
  store <4 x i64> %123, <4 x i64>* %124, align 32
  %125 = mul nsw i64 %102, 5
  %126 = getelementptr inbounds i16, i16* %0, i64 %125
  %127 = bitcast i16* %126 to <4 x i64>*
  %128 = load <4 x i64>, <4 x i64>* %127, align 32
  %129 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 5
  store <4 x i64> %128, <4 x i64>* %129, align 32
  %130 = mul nsw i64 %102, 6
  %131 = getelementptr inbounds i16, i16* %0, i64 %130
  %132 = bitcast i16* %131 to <4 x i64>*
  %133 = load <4 x i64>, <4 x i64>* %132, align 32
  %134 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 6
  store <4 x i64> %133, <4 x i64>* %134, align 32
  %135 = mul nsw i64 %102, 7
  %136 = getelementptr inbounds i16, i16* %0, i64 %135
  %137 = bitcast i16* %136 to <4 x i64>*
  %138 = load <4 x i64>, <4 x i64>* %137, align 32
  %139 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 7
  store <4 x i64> %138, <4 x i64>* %139, align 32
  %140 = shl nsw i64 %102, 3
  %141 = getelementptr inbounds i16, i16* %0, i64 %140
  %142 = bitcast i16* %141 to <4 x i64>*
  %143 = load <4 x i64>, <4 x i64>* %142, align 32
  %144 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 8
  store <4 x i64> %143, <4 x i64>* %144, align 32
  %145 = mul nsw i64 %102, 9
  %146 = getelementptr inbounds i16, i16* %0, i64 %145
  %147 = bitcast i16* %146 to <4 x i64>*
  %148 = load <4 x i64>, <4 x i64>* %147, align 32
  %149 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 9
  store <4 x i64> %148, <4 x i64>* %149, align 32
  %150 = mul nsw i64 %102, 10
  %151 = getelementptr inbounds i16, i16* %0, i64 %150
  %152 = bitcast i16* %151 to <4 x i64>*
  %153 = load <4 x i64>, <4 x i64>* %152, align 32
  %154 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 10
  store <4 x i64> %153, <4 x i64>* %154, align 32
  %155 = mul nsw i64 %102, 11
  %156 = getelementptr inbounds i16, i16* %0, i64 %155
  %157 = bitcast i16* %156 to <4 x i64>*
  %158 = load <4 x i64>, <4 x i64>* %157, align 32
  %159 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 11
  store <4 x i64> %158, <4 x i64>* %159, align 32
  %160 = mul nsw i64 %102, 12
  %161 = getelementptr inbounds i16, i16* %0, i64 %160
  %162 = bitcast i16* %161 to <4 x i64>*
  %163 = load <4 x i64>, <4 x i64>* %162, align 32
  %164 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 12
  store <4 x i64> %163, <4 x i64>* %164, align 32
  %165 = mul nsw i64 %102, 13
  %166 = getelementptr inbounds i16, i16* %0, i64 %165
  %167 = bitcast i16* %166 to <4 x i64>*
  %168 = load <4 x i64>, <4 x i64>* %167, align 32
  %169 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 13
  store <4 x i64> %168, <4 x i64>* %169, align 32
  %170 = mul nsw i64 %102, 14
  %171 = getelementptr inbounds i16, i16* %0, i64 %170
  %172 = bitcast i16* %171 to <4 x i64>*
  %173 = load <4 x i64>, <4 x i64>* %172, align 32
  %174 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 14
  store <4 x i64> %173, <4 x i64>* %174, align 32
  %175 = mul nsw i64 %102, 15
  %176 = getelementptr inbounds i16, i16* %0, i64 %175
  %177 = bitcast i16* %176 to <4 x i64>*
  %178 = load <4 x i64>, <4 x i64>* %177, align 32
  %179 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 15
  store <4 x i64> %178, <4 x i64>* %179, align 32
  br label %180

180:                                              ; preds = %20, %100
  %181 = phi <4 x i64> [ %148, %100 ], [ %53, %20 ]
  %182 = phi <4 x i64> [ %143, %100 ], [ %58, %20 ]
  %183 = phi <4 x i64> [ %138, %100 ], [ %63, %20 ]
  %184 = phi <4 x i64> [ %133, %100 ], [ %68, %20 ]
  %185 = phi <4 x i64> [ %128, %100 ], [ %73, %20 ]
  %186 = phi <4 x i64> [ %123, %100 ], [ %78, %20 ]
  %187 = phi <4 x i64> [ %163, %100 ], [ %38, %20 ]
  %188 = phi <4 x i64> [ %158, %100 ], [ %43, %20 ]
  %189 = phi <4 x i64> [ %153, %100 ], [ %48, %20 ]
  %190 = phi <4 x i64> [ %118, %100 ], [ %83, %20 ]
  %191 = phi <4 x i64> [ %113, %100 ], [ %88, %20 ]
  %192 = phi <4 x i64> [ %108, %100 ], [ %93, %20 ]
  %193 = phi <4 x i64> [ %104, %100 ], [ %98, %20 ]
  %194 = phi <4 x i64>* [ %105, %100 ], [ %99, %20 ]
  %195 = phi i32 [ %101, %100 ], [ %21, %20 ]
  %196 = bitcast <4 x i64> %186 to <16 x i16>
  %197 = bitcast <4 x i64> %185 to <16 x i16>
  %198 = bitcast <4 x i64> %184 to <16 x i16>
  %199 = bitcast <4 x i64> %183 to <16 x i16>
  %200 = bitcast <4 x i64> %182 to <16 x i16>
  %201 = bitcast <4 x i64> %181 to <16 x i16>
  %202 = bitcast <4 x i64> %187 to <16 x i16>
  %203 = bitcast <4 x i64> %188 to <16 x i16>
  %204 = bitcast <4 x i64> %189 to <16 x i16>
  %205 = bitcast <4 x i64> %190 to <16 x i16>
  %206 = bitcast <4 x i64> %191 to <16 x i16>
  %207 = bitcast <4 x i64> %192 to <16 x i16>
  %208 = bitcast <4 x i64> %193 to <16 x i16>
  %209 = load i8, i8* %10, align 1
  %210 = sext i8 %209 to i32
  %211 = icmp slt i8 %209, 0
  br i1 %211, label %212, label %292

212:                                              ; preds = %180
  %213 = sub nsw i32 0, %210
  %214 = xor i32 %210, -1
  %215 = shl i32 1, %214
  %216 = trunc i32 %215 to i16
  %217 = insertelement <16 x i16> undef, i16 %216, i32 0
  %218 = shufflevector <16 x i16> %217, <16 x i16> undef, <16 x i32> zeroinitializer
  %219 = bitcast [16 x <4 x i64>]* %6 to <16 x i16>*
  %220 = load <16 x i16>, <16 x i16>* %219, align 32
  %221 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %220, <16 x i16> %218) #8
  %222 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %221, i32 %213) #8
  store <16 x i16> %222, <16 x i16>* %219, align 32
  %223 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 1
  %224 = bitcast <4 x i64>* %223 to <16 x i16>*
  %225 = load <16 x i16>, <16 x i16>* %224, align 32
  %226 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %225, <16 x i16> %218) #8
  %227 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %226, i32 %213) #8
  store <16 x i16> %227, <16 x i16>* %224, align 32
  %228 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 2
  %229 = bitcast <4 x i64>* %228 to <16 x i16>*
  %230 = load <16 x i16>, <16 x i16>* %229, align 32
  %231 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %230, <16 x i16> %218) #8
  %232 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %231, i32 %213) #8
  store <16 x i16> %232, <16 x i16>* %229, align 32
  %233 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 3
  %234 = bitcast <4 x i64>* %233 to <16 x i16>*
  %235 = load <16 x i16>, <16 x i16>* %234, align 32
  %236 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %235, <16 x i16> %218) #8
  %237 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %236, i32 %213) #8
  store <16 x i16> %237, <16 x i16>* %234, align 32
  %238 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 4
  %239 = bitcast <4 x i64>* %238 to <16 x i16>*
  %240 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %196, <16 x i16> %218) #8
  %241 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %240, i32 %213) #8
  store <16 x i16> %241, <16 x i16>* %239, align 32
  %242 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 5
  %243 = bitcast <4 x i64>* %242 to <16 x i16>*
  %244 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %197, <16 x i16> %218) #8
  %245 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %244, i32 %213) #8
  store <16 x i16> %245, <16 x i16>* %243, align 32
  %246 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 6
  %247 = bitcast <4 x i64>* %246 to <16 x i16>*
  %248 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %198, <16 x i16> %218) #8
  %249 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %248, i32 %213) #8
  store <16 x i16> %249, <16 x i16>* %247, align 32
  %250 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 7
  %251 = bitcast <4 x i64>* %250 to <16 x i16>*
  %252 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %199, <16 x i16> %218) #8
  %253 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %252, i32 %213) #8
  store <16 x i16> %253, <16 x i16>* %251, align 32
  %254 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 8
  %255 = bitcast <4 x i64>* %254 to <16 x i16>*
  %256 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %200, <16 x i16> %218) #8
  %257 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %256, i32 %213) #8
  store <16 x i16> %257, <16 x i16>* %255, align 32
  %258 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 9
  %259 = bitcast <4 x i64>* %258 to <16 x i16>*
  %260 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %201, <16 x i16> %218) #8
  %261 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %260, i32 %213) #8
  store <16 x i16> %261, <16 x i16>* %259, align 32
  %262 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 10
  %263 = bitcast <4 x i64>* %262 to <16 x i16>*
  %264 = load <16 x i16>, <16 x i16>* %263, align 32
  %265 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %264, <16 x i16> %218) #8
  %266 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %265, i32 %213) #8
  store <16 x i16> %266, <16 x i16>* %263, align 32
  %267 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 11
  %268 = bitcast <4 x i64>* %267 to <16 x i16>*
  %269 = load <16 x i16>, <16 x i16>* %268, align 32
  %270 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %269, <16 x i16> %218) #8
  %271 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %270, i32 %213) #8
  store <16 x i16> %271, <16 x i16>* %268, align 32
  %272 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 12
  %273 = bitcast <4 x i64>* %272 to <16 x i16>*
  %274 = load <16 x i16>, <16 x i16>* %273, align 32
  %275 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %274, <16 x i16> %218) #8
  %276 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %275, i32 %213) #8
  store <16 x i16> %276, <16 x i16>* %273, align 32
  %277 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 13
  %278 = bitcast <4 x i64>* %277 to <16 x i16>*
  %279 = load <16 x i16>, <16 x i16>* %278, align 32
  %280 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %279, <16 x i16> %218) #8
  %281 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %280, i32 %213) #8
  store <16 x i16> %281, <16 x i16>* %278, align 32
  %282 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 14
  %283 = bitcast <4 x i64>* %282 to <16 x i16>*
  %284 = load <16 x i16>, <16 x i16>* %283, align 32
  %285 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %284, <16 x i16> %218) #8
  %286 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %285, i32 %213) #8
  store <16 x i16> %286, <16 x i16>* %283, align 32
  %287 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 15
  %288 = bitcast <4 x i64>* %287 to <16 x i16>*
  %289 = load <16 x i16>, <16 x i16>* %288, align 32
  %290 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %289, <16 x i16> %218) #8
  %291 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %290, i32 %213) #8
  store <16 x i16> %291, <16 x i16>* %288, align 32
  br label %345

292:                                              ; preds = %180
  %293 = icmp eq i8 %209, 0
  br i1 %293, label %345, label %294

294:                                              ; preds = %292
  %295 = bitcast [16 x <4 x i64>]* %6 to <16 x i16>*
  %296 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %208, i32 %210) #8
  store <16 x i16> %296, <16 x i16>* %295, align 32
  %297 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 1
  %298 = bitcast <4 x i64>* %297 to <16 x i16>*
  %299 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %207, i32 %210) #8
  store <16 x i16> %299, <16 x i16>* %298, align 32
  %300 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 2
  %301 = bitcast <4 x i64>* %300 to <16 x i16>*
  %302 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %206, i32 %210) #8
  store <16 x i16> %302, <16 x i16>* %301, align 32
  %303 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 3
  %304 = bitcast <4 x i64>* %303 to <16 x i16>*
  %305 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %205, i32 %210) #8
  store <16 x i16> %305, <16 x i16>* %304, align 32
  %306 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 4
  %307 = bitcast <4 x i64>* %306 to <16 x i16>*
  %308 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %196, i32 %210) #8
  store <16 x i16> %308, <16 x i16>* %307, align 32
  %309 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 5
  %310 = bitcast <4 x i64>* %309 to <16 x i16>*
  %311 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %197, i32 %210) #8
  store <16 x i16> %311, <16 x i16>* %310, align 32
  %312 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 6
  %313 = bitcast <4 x i64>* %312 to <16 x i16>*
  %314 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %198, i32 %210) #8
  store <16 x i16> %314, <16 x i16>* %313, align 32
  %315 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 7
  %316 = bitcast <4 x i64>* %315 to <16 x i16>*
  %317 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %199, i32 %210) #8
  store <16 x i16> %317, <16 x i16>* %316, align 32
  %318 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 8
  %319 = bitcast <4 x i64>* %318 to <16 x i16>*
  %320 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %200, i32 %210) #8
  store <16 x i16> %320, <16 x i16>* %319, align 32
  %321 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 9
  %322 = bitcast <4 x i64>* %321 to <16 x i16>*
  %323 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %201, i32 %210) #8
  store <16 x i16> %323, <16 x i16>* %322, align 32
  %324 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 10
  %325 = bitcast <4 x i64>* %324 to <16 x i16>*
  %326 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %204, i32 %210) #8
  store <16 x i16> %326, <16 x i16>* %325, align 32
  %327 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 11
  %328 = bitcast <4 x i64>* %327 to <16 x i16>*
  %329 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %203, i32 %210) #8
  store <16 x i16> %329, <16 x i16>* %328, align 32
  %330 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 12
  %331 = bitcast <4 x i64>* %330 to <16 x i16>*
  %332 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %202, i32 %210) #8
  store <16 x i16> %332, <16 x i16>* %331, align 32
  %333 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 13
  %334 = bitcast <4 x i64>* %333 to <16 x i16>*
  %335 = load <16 x i16>, <16 x i16>* %334, align 32
  %336 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %335, i32 %210) #8
  store <16 x i16> %336, <16 x i16>* %334, align 32
  %337 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 14
  %338 = bitcast <4 x i64>* %337 to <16 x i16>*
  %339 = load <16 x i16>, <16 x i16>* %338, align 32
  %340 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %339, i32 %210) #8
  store <16 x i16> %340, <16 x i16>* %338, align 32
  %341 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 15
  %342 = bitcast <4 x i64>* %341 to <16 x i16>*
  %343 = load <16 x i16>, <16 x i16>* %342, align 32
  %344 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %343, i32 %210) #8
  store <16 x i16> %344, <16 x i16>* %342, align 32
  br label %345

345:                                              ; preds = %294, %212, %292
  call void %15(<4 x i64>* %194, <4 x i64>* %194, i8 signext %11) #8
  %346 = getelementptr inbounds i8, i8* %10, i64 1
  %347 = load i8, i8* %346, align 1
  %348 = sext i8 %347 to i32
  %349 = icmp slt i8 %347, 0
  br i1 %349, label %350, label %508

350:                                              ; preds = %345
  %351 = sub nsw i32 0, %348
  %352 = xor i32 %348, -1
  %353 = shl i32 1, %352
  %354 = trunc i32 %353 to i16
  %355 = insertelement <16 x i16> undef, i16 %354, i32 0
  %356 = shufflevector <16 x i16> %355, <16 x i16> undef, <16 x i32> zeroinitializer
  %357 = bitcast [16 x <4 x i64>]* %6 to <16 x i16>*
  %358 = load <16 x i16>, <16 x i16>* %357, align 32
  %359 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %358, <16 x i16> %356) #8
  %360 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %359, i32 %351) #8
  store <16 x i16> %360, <16 x i16>* %357, align 32
  %361 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 1
  %362 = bitcast <4 x i64>* %361 to <16 x i16>*
  %363 = load <16 x i16>, <16 x i16>* %362, align 32
  %364 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %363, <16 x i16> %356) #8
  %365 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %364, i32 %351) #8
  store <16 x i16> %365, <16 x i16>* %362, align 32
  %366 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 2
  %367 = bitcast <4 x i64>* %366 to <16 x i16>*
  %368 = load <16 x i16>, <16 x i16>* %367, align 32
  %369 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %368, <16 x i16> %356) #8
  %370 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %369, i32 %351) #8
  store <16 x i16> %370, <16 x i16>* %367, align 32
  %371 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 3
  %372 = bitcast <4 x i64>* %371 to <16 x i16>*
  %373 = load <16 x i16>, <16 x i16>* %372, align 32
  %374 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %373, <16 x i16> %356) #8
  %375 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %374, i32 %351) #8
  store <16 x i16> %375, <16 x i16>* %372, align 32
  %376 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 4
  %377 = bitcast <4 x i64>* %376 to <16 x i16>*
  %378 = load <16 x i16>, <16 x i16>* %377, align 32
  %379 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %378, <16 x i16> %356) #8
  %380 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %379, i32 %351) #8
  store <16 x i16> %380, <16 x i16>* %377, align 32
  %381 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 5
  %382 = bitcast <4 x i64>* %381 to <16 x i16>*
  %383 = load <16 x i16>, <16 x i16>* %382, align 32
  %384 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %383, <16 x i16> %356) #8
  %385 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %384, i32 %351) #8
  store <16 x i16> %385, <16 x i16>* %382, align 32
  %386 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 6
  %387 = bitcast <4 x i64>* %386 to <16 x i16>*
  %388 = load <16 x i16>, <16 x i16>* %387, align 32
  %389 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %388, <16 x i16> %356) #8
  %390 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %389, i32 %351) #8
  store <16 x i16> %390, <16 x i16>* %387, align 32
  %391 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 7
  %392 = bitcast <4 x i64>* %391 to <16 x i16>*
  %393 = load <16 x i16>, <16 x i16>* %392, align 32
  %394 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %393, <16 x i16> %356) #8
  %395 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %394, i32 %351) #8
  store <16 x i16> %395, <16 x i16>* %392, align 32
  %396 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 8
  %397 = bitcast <4 x i64>* %396 to <16 x i16>*
  %398 = load <16 x i16>, <16 x i16>* %397, align 32
  %399 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %398, <16 x i16> %356) #8
  %400 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %399, i32 %351) #8
  store <16 x i16> %400, <16 x i16>* %397, align 32
  %401 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 9
  %402 = bitcast <4 x i64>* %401 to <16 x i16>*
  %403 = load <16 x i16>, <16 x i16>* %402, align 32
  %404 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %403, <16 x i16> %356) #8
  %405 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %404, i32 %351) #8
  store <16 x i16> %405, <16 x i16>* %402, align 32
  %406 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 10
  %407 = bitcast <4 x i64>* %406 to <16 x i16>*
  %408 = load <16 x i16>, <16 x i16>* %407, align 32
  %409 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %408, <16 x i16> %356) #8
  %410 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %409, i32 %351) #8
  store <16 x i16> %410, <16 x i16>* %407, align 32
  %411 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 11
  %412 = bitcast <4 x i64>* %411 to <16 x i16>*
  %413 = load <16 x i16>, <16 x i16>* %412, align 32
  %414 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %413, <16 x i16> %356) #8
  %415 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %414, i32 %351) #8
  store <16 x i16> %415, <16 x i16>* %412, align 32
  %416 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 12
  %417 = bitcast <4 x i64>* %416 to <16 x i16>*
  %418 = load <16 x i16>, <16 x i16>* %417, align 32
  %419 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %418, <16 x i16> %356) #8
  %420 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %419, i32 %351) #8
  store <16 x i16> %420, <16 x i16>* %417, align 32
  %421 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 13
  %422 = bitcast <4 x i64>* %421 to <16 x i16>*
  %423 = load <16 x i16>, <16 x i16>* %422, align 32
  %424 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %423, <16 x i16> %356) #8
  %425 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %424, i32 %351) #8
  store <16 x i16> %425, <16 x i16>* %422, align 32
  %426 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 14
  %427 = bitcast <4 x i64>* %426 to <16 x i16>*
  %428 = load <16 x i16>, <16 x i16>* %427, align 32
  %429 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %428, <16 x i16> %356) #8
  %430 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %429, i32 %351) #8
  store <16 x i16> %430, <16 x i16>* %427, align 32
  %431 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 15
  %432 = bitcast <4 x i64>* %431 to <16 x i16>*
  %433 = load <16 x i16>, <16 x i16>* %432, align 32
  %434 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %433, <16 x i16> %356) #8
  %435 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %434, i32 %351) #8
  store <16 x i16> %435, <16 x i16>* %432, align 32
  %436 = bitcast <16 x i16> %360 to <2 x i128>
  %437 = extractelement <2 x i128> %436, i32 0
  %438 = bitcast i128 %437 to <2 x i64>
  %439 = bitcast <16 x i16> %400 to <2 x i128>
  %440 = extractelement <2 x i128> %439, i32 0
  %441 = bitcast i128 %440 to <2 x i64>
  %442 = bitcast <16 x i16> %365 to <2 x i128>
  %443 = extractelement <2 x i128> %442, i32 0
  %444 = bitcast i128 %443 to <2 x i64>
  %445 = bitcast <16 x i16> %405 to <2 x i128>
  %446 = extractelement <2 x i128> %445, i32 0
  %447 = bitcast i128 %446 to <2 x i64>
  %448 = bitcast <16 x i16> %370 to <2 x i128>
  %449 = extractelement <2 x i128> %448, i32 0
  %450 = bitcast i128 %449 to <2 x i64>
  %451 = bitcast <16 x i16> %410 to <2 x i128>
  %452 = extractelement <2 x i128> %451, i32 0
  %453 = bitcast i128 %452 to <2 x i64>
  %454 = bitcast <16 x i16> %375 to <2 x i128>
  %455 = extractelement <2 x i128> %454, i32 0
  %456 = bitcast i128 %455 to <2 x i64>
  %457 = bitcast <16 x i16> %415 to <2 x i128>
  %458 = extractelement <2 x i128> %457, i32 0
  %459 = bitcast i128 %458 to <2 x i64>
  %460 = bitcast <16 x i16> %380 to <2 x i128>
  %461 = extractelement <2 x i128> %460, i32 0
  %462 = bitcast i128 %461 to <2 x i64>
  %463 = bitcast <16 x i16> %420 to <2 x i128>
  %464 = extractelement <2 x i128> %463, i32 0
  %465 = bitcast i128 %464 to <2 x i64>
  %466 = bitcast <16 x i16> %385 to <2 x i128>
  %467 = extractelement <2 x i128> %466, i32 0
  %468 = bitcast i128 %467 to <2 x i64>
  %469 = bitcast <16 x i16> %425 to <2 x i128>
  %470 = extractelement <2 x i128> %469, i32 0
  %471 = bitcast i128 %470 to <2 x i64>
  %472 = bitcast <16 x i16> %390 to <2 x i128>
  %473 = extractelement <2 x i128> %472, i32 0
  %474 = bitcast i128 %473 to <2 x i64>
  %475 = bitcast <16 x i16> %430 to <2 x i128>
  %476 = extractelement <2 x i128> %475, i32 0
  %477 = bitcast i128 %476 to <2 x i64>
  %478 = bitcast <16 x i16> %395 to <2 x i128>
  %479 = extractelement <2 x i128> %478, i32 0
  %480 = bitcast i128 %479 to <2 x i64>
  %481 = bitcast <16 x i16> %435 to <2 x i128>
  %482 = extractelement <2 x i128> %481, i32 0
  %483 = bitcast i128 %482 to <2 x i64>
  %484 = bitcast <16 x i16> %400 to <2 x i128>
  %485 = extractelement <2 x i128> %484, i32 1
  %486 = bitcast i128 %485 to <2 x i64>
  %487 = bitcast <16 x i16> %405 to <2 x i128>
  %488 = extractelement <2 x i128> %487, i32 1
  %489 = bitcast i128 %488 to <2 x i64>
  %490 = bitcast <16 x i16> %410 to <2 x i128>
  %491 = extractelement <2 x i128> %490, i32 1
  %492 = bitcast i128 %491 to <2 x i64>
  %493 = bitcast <16 x i16> %415 to <2 x i128>
  %494 = extractelement <2 x i128> %493, i32 1
  %495 = bitcast i128 %494 to <2 x i64>
  %496 = bitcast <16 x i16> %420 to <2 x i128>
  %497 = extractelement <2 x i128> %496, i32 1
  %498 = bitcast i128 %497 to <2 x i64>
  %499 = bitcast <16 x i16> %425 to <2 x i128>
  %500 = extractelement <2 x i128> %499, i32 1
  %501 = bitcast i128 %500 to <2 x i64>
  %502 = bitcast <16 x i16> %430 to <2 x i128>
  %503 = extractelement <2 x i128> %502, i32 1
  %504 = bitcast i128 %503 to <2 x i64>
  %505 = bitcast <16 x i16> %435 to <2 x i128>
  %506 = extractelement <2 x i128> %505, i32 1
  %507 = bitcast i128 %506 to <2 x i64>
  br label %710

508:                                              ; preds = %345
  %509 = icmp eq i8 %347, 0
  br i1 %509, label %510, label %574

510:                                              ; preds = %508
  %511 = bitcast <4 x i64>* %194 to <2 x i64>*
  %512 = load <2 x i64>, <2 x i64>* %511, align 16
  %513 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 8
  %514 = bitcast <4 x i64>* %513 to <2 x i64>*
  %515 = load <2 x i64>, <2 x i64>* %514, align 32
  %516 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 1
  %517 = bitcast <4 x i64>* %516 to <2 x i64>*
  %518 = load <2 x i64>, <2 x i64>* %517, align 32
  %519 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 9
  %520 = bitcast <4 x i64>* %519 to <2 x i64>*
  %521 = load <2 x i64>, <2 x i64>* %520, align 32
  %522 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 2
  %523 = bitcast <4 x i64>* %522 to <2 x i64>*
  %524 = load <2 x i64>, <2 x i64>* %523, align 32
  %525 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 10
  %526 = bitcast <4 x i64>* %525 to <2 x i64>*
  %527 = load <2 x i64>, <2 x i64>* %526, align 32
  %528 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 3
  %529 = bitcast <4 x i64>* %528 to <2 x i64>*
  %530 = load <2 x i64>, <2 x i64>* %529, align 32
  %531 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 11
  %532 = bitcast <4 x i64>* %531 to <2 x i64>*
  %533 = load <2 x i64>, <2 x i64>* %532, align 32
  %534 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 4
  %535 = bitcast <4 x i64>* %534 to <2 x i64>*
  %536 = load <2 x i64>, <2 x i64>* %535, align 32
  %537 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 12
  %538 = bitcast <4 x i64>* %537 to <2 x i64>*
  %539 = load <2 x i64>, <2 x i64>* %538, align 32
  %540 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 5
  %541 = bitcast <4 x i64>* %540 to <2 x i64>*
  %542 = load <2 x i64>, <2 x i64>* %541, align 32
  %543 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 13
  %544 = bitcast <4 x i64>* %543 to <2 x i64>*
  %545 = load <2 x i64>, <2 x i64>* %544, align 32
  %546 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 6
  %547 = bitcast <4 x i64>* %546 to <2 x i64>*
  %548 = load <2 x i64>, <2 x i64>* %547, align 32
  %549 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 14
  %550 = bitcast <4 x i64>* %549 to <2 x i64>*
  %551 = load <2 x i64>, <2 x i64>* %550, align 32
  %552 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 7
  %553 = bitcast <4 x i64>* %552 to <2 x i64>*
  %554 = load <2 x i64>, <2 x i64>* %553, align 32
  %555 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 15
  %556 = bitcast <4 x i64>* %555 to <2 x i64>*
  %557 = load <2 x i64>, <2 x i64>* %556, align 32
  %558 = getelementptr inbounds <2 x i64>, <2 x i64>* %514, i64 1
  %559 = load <2 x i64>, <2 x i64>* %558, align 16
  %560 = getelementptr inbounds <2 x i64>, <2 x i64>* %520, i64 1
  %561 = load <2 x i64>, <2 x i64>* %560, align 16
  %562 = getelementptr inbounds <2 x i64>, <2 x i64>* %526, i64 1
  %563 = load <2 x i64>, <2 x i64>* %562, align 16
  %564 = getelementptr inbounds <2 x i64>, <2 x i64>* %532, i64 1
  %565 = load <2 x i64>, <2 x i64>* %564, align 16
  %566 = getelementptr inbounds <2 x i64>, <2 x i64>* %538, i64 1
  %567 = load <2 x i64>, <2 x i64>* %566, align 16
  %568 = getelementptr inbounds <2 x i64>, <2 x i64>* %544, i64 1
  %569 = load <2 x i64>, <2 x i64>* %568, align 16
  %570 = getelementptr inbounds <2 x i64>, <2 x i64>* %550, i64 1
  %571 = load <2 x i64>, <2 x i64>* %570, align 16
  %572 = getelementptr inbounds <2 x i64>, <2 x i64>* %556, i64 1
  %573 = load <2 x i64>, <2 x i64>* %572, align 16
  br label %710

574:                                              ; preds = %508
  %575 = bitcast [16 x <4 x i64>]* %6 to <16 x i16>*
  %576 = load <16 x i16>, <16 x i16>* %575, align 32
  %577 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %576, i32 %348) #8
  store <16 x i16> %577, <16 x i16>* %575, align 32
  %578 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 1
  %579 = bitcast <4 x i64>* %578 to <16 x i16>*
  %580 = load <16 x i16>, <16 x i16>* %579, align 32
  %581 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %580, i32 %348) #8
  store <16 x i16> %581, <16 x i16>* %579, align 32
  %582 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 2
  %583 = bitcast <4 x i64>* %582 to <16 x i16>*
  %584 = load <16 x i16>, <16 x i16>* %583, align 32
  %585 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %584, i32 %348) #8
  store <16 x i16> %585, <16 x i16>* %583, align 32
  %586 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 3
  %587 = bitcast <4 x i64>* %586 to <16 x i16>*
  %588 = load <16 x i16>, <16 x i16>* %587, align 32
  %589 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %588, i32 %348) #8
  store <16 x i16> %589, <16 x i16>* %587, align 32
  %590 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 4
  %591 = bitcast <4 x i64>* %590 to <16 x i16>*
  %592 = load <16 x i16>, <16 x i16>* %591, align 32
  %593 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %592, i32 %348) #8
  store <16 x i16> %593, <16 x i16>* %591, align 32
  %594 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 5
  %595 = bitcast <4 x i64>* %594 to <16 x i16>*
  %596 = load <16 x i16>, <16 x i16>* %595, align 32
  %597 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %596, i32 %348) #8
  store <16 x i16> %597, <16 x i16>* %595, align 32
  %598 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 6
  %599 = bitcast <4 x i64>* %598 to <16 x i16>*
  %600 = load <16 x i16>, <16 x i16>* %599, align 32
  %601 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %600, i32 %348) #8
  store <16 x i16> %601, <16 x i16>* %599, align 32
  %602 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 7
  %603 = bitcast <4 x i64>* %602 to <16 x i16>*
  %604 = load <16 x i16>, <16 x i16>* %603, align 32
  %605 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %604, i32 %348) #8
  store <16 x i16> %605, <16 x i16>* %603, align 32
  %606 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 8
  %607 = bitcast <4 x i64>* %606 to <16 x i16>*
  %608 = load <16 x i16>, <16 x i16>* %607, align 32
  %609 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %608, i32 %348) #8
  store <16 x i16> %609, <16 x i16>* %607, align 32
  %610 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 9
  %611 = bitcast <4 x i64>* %610 to <16 x i16>*
  %612 = load <16 x i16>, <16 x i16>* %611, align 32
  %613 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %612, i32 %348) #8
  store <16 x i16> %613, <16 x i16>* %611, align 32
  %614 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 10
  %615 = bitcast <4 x i64>* %614 to <16 x i16>*
  %616 = load <16 x i16>, <16 x i16>* %615, align 32
  %617 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %616, i32 %348) #8
  store <16 x i16> %617, <16 x i16>* %615, align 32
  %618 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 11
  %619 = bitcast <4 x i64>* %618 to <16 x i16>*
  %620 = load <16 x i16>, <16 x i16>* %619, align 32
  %621 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %620, i32 %348) #8
  store <16 x i16> %621, <16 x i16>* %619, align 32
  %622 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 12
  %623 = bitcast <4 x i64>* %622 to <16 x i16>*
  %624 = load <16 x i16>, <16 x i16>* %623, align 32
  %625 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %624, i32 %348) #8
  store <16 x i16> %625, <16 x i16>* %623, align 32
  %626 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 13
  %627 = bitcast <4 x i64>* %626 to <16 x i16>*
  %628 = load <16 x i16>, <16 x i16>* %627, align 32
  %629 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %628, i32 %348) #8
  store <16 x i16> %629, <16 x i16>* %627, align 32
  %630 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 14
  %631 = bitcast <4 x i64>* %630 to <16 x i16>*
  %632 = load <16 x i16>, <16 x i16>* %631, align 32
  %633 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %632, i32 %348) #8
  store <16 x i16> %633, <16 x i16>* %631, align 32
  %634 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 15
  %635 = bitcast <4 x i64>* %634 to <16 x i16>*
  %636 = load <16 x i16>, <16 x i16>* %635, align 32
  %637 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %636, i32 %348) #8
  store <16 x i16> %637, <16 x i16>* %635, align 32
  %638 = bitcast <16 x i16> %577 to <2 x i128>
  %639 = extractelement <2 x i128> %638, i32 0
  %640 = bitcast i128 %639 to <2 x i64>
  %641 = bitcast <16 x i16> %609 to <2 x i128>
  %642 = extractelement <2 x i128> %641, i32 0
  %643 = bitcast i128 %642 to <2 x i64>
  %644 = bitcast <16 x i16> %581 to <2 x i128>
  %645 = extractelement <2 x i128> %644, i32 0
  %646 = bitcast i128 %645 to <2 x i64>
  %647 = bitcast <16 x i16> %613 to <2 x i128>
  %648 = extractelement <2 x i128> %647, i32 0
  %649 = bitcast i128 %648 to <2 x i64>
  %650 = bitcast <16 x i16> %585 to <2 x i128>
  %651 = extractelement <2 x i128> %650, i32 0
  %652 = bitcast i128 %651 to <2 x i64>
  %653 = bitcast <16 x i16> %617 to <2 x i128>
  %654 = extractelement <2 x i128> %653, i32 0
  %655 = bitcast i128 %654 to <2 x i64>
  %656 = bitcast <16 x i16> %589 to <2 x i128>
  %657 = extractelement <2 x i128> %656, i32 0
  %658 = bitcast i128 %657 to <2 x i64>
  %659 = bitcast <16 x i16> %621 to <2 x i128>
  %660 = extractelement <2 x i128> %659, i32 0
  %661 = bitcast i128 %660 to <2 x i64>
  %662 = bitcast <16 x i16> %593 to <2 x i128>
  %663 = extractelement <2 x i128> %662, i32 0
  %664 = bitcast i128 %663 to <2 x i64>
  %665 = bitcast <16 x i16> %625 to <2 x i128>
  %666 = extractelement <2 x i128> %665, i32 0
  %667 = bitcast i128 %666 to <2 x i64>
  %668 = bitcast <16 x i16> %597 to <2 x i128>
  %669 = extractelement <2 x i128> %668, i32 0
  %670 = bitcast i128 %669 to <2 x i64>
  %671 = bitcast <16 x i16> %629 to <2 x i128>
  %672 = extractelement <2 x i128> %671, i32 0
  %673 = bitcast i128 %672 to <2 x i64>
  %674 = bitcast <16 x i16> %601 to <2 x i128>
  %675 = extractelement <2 x i128> %674, i32 0
  %676 = bitcast i128 %675 to <2 x i64>
  %677 = bitcast <16 x i16> %633 to <2 x i128>
  %678 = extractelement <2 x i128> %677, i32 0
  %679 = bitcast i128 %678 to <2 x i64>
  %680 = bitcast <16 x i16> %605 to <2 x i128>
  %681 = extractelement <2 x i128> %680, i32 0
  %682 = bitcast i128 %681 to <2 x i64>
  %683 = bitcast <16 x i16> %637 to <2 x i128>
  %684 = extractelement <2 x i128> %683, i32 0
  %685 = bitcast i128 %684 to <2 x i64>
  %686 = bitcast <16 x i16> %609 to <2 x i128>
  %687 = extractelement <2 x i128> %686, i32 1
  %688 = bitcast i128 %687 to <2 x i64>
  %689 = bitcast <16 x i16> %613 to <2 x i128>
  %690 = extractelement <2 x i128> %689, i32 1
  %691 = bitcast i128 %690 to <2 x i64>
  %692 = bitcast <16 x i16> %617 to <2 x i128>
  %693 = extractelement <2 x i128> %692, i32 1
  %694 = bitcast i128 %693 to <2 x i64>
  %695 = bitcast <16 x i16> %621 to <2 x i128>
  %696 = extractelement <2 x i128> %695, i32 1
  %697 = bitcast i128 %696 to <2 x i64>
  %698 = bitcast <16 x i16> %625 to <2 x i128>
  %699 = extractelement <2 x i128> %698, i32 1
  %700 = bitcast i128 %699 to <2 x i64>
  %701 = bitcast <16 x i16> %629 to <2 x i128>
  %702 = extractelement <2 x i128> %701, i32 1
  %703 = bitcast i128 %702 to <2 x i64>
  %704 = bitcast <16 x i16> %633 to <2 x i128>
  %705 = extractelement <2 x i128> %704, i32 1
  %706 = bitcast i128 %705 to <2 x i64>
  %707 = bitcast <16 x i16> %637 to <2 x i128>
  %708 = extractelement <2 x i128> %707, i32 1
  %709 = bitcast i128 %708 to <2 x i64>
  br label %710

710:                                              ; preds = %510, %574, %350
  %711 = phi <2 x i64> [ %573, %510 ], [ %709, %574 ], [ %507, %350 ]
  %712 = phi <2 x i64> [ %571, %510 ], [ %706, %574 ], [ %504, %350 ]
  %713 = phi <2 x i64> [ %569, %510 ], [ %703, %574 ], [ %501, %350 ]
  %714 = phi <2 x i64> [ %567, %510 ], [ %700, %574 ], [ %498, %350 ]
  %715 = phi <2 x i64> [ %565, %510 ], [ %697, %574 ], [ %495, %350 ]
  %716 = phi <2 x i64> [ %563, %510 ], [ %694, %574 ], [ %492, %350 ]
  %717 = phi <2 x i64> [ %561, %510 ], [ %691, %574 ], [ %489, %350 ]
  %718 = phi <2 x i64> [ %559, %510 ], [ %688, %574 ], [ %486, %350 ]
  %719 = phi <2 x i64> [ %557, %510 ], [ %685, %574 ], [ %483, %350 ]
  %720 = phi <2 x i64> [ %554, %510 ], [ %682, %574 ], [ %480, %350 ]
  %721 = phi <2 x i64> [ %551, %510 ], [ %679, %574 ], [ %477, %350 ]
  %722 = phi <2 x i64> [ %548, %510 ], [ %676, %574 ], [ %474, %350 ]
  %723 = phi <2 x i64> [ %545, %510 ], [ %673, %574 ], [ %471, %350 ]
  %724 = phi <2 x i64> [ %542, %510 ], [ %670, %574 ], [ %468, %350 ]
  %725 = phi <2 x i64> [ %539, %510 ], [ %667, %574 ], [ %465, %350 ]
  %726 = phi <2 x i64> [ %536, %510 ], [ %664, %574 ], [ %462, %350 ]
  %727 = phi <2 x i64> [ %533, %510 ], [ %661, %574 ], [ %459, %350 ]
  %728 = phi <2 x i64> [ %530, %510 ], [ %658, %574 ], [ %456, %350 ]
  %729 = phi <2 x i64> [ %527, %510 ], [ %655, %574 ], [ %453, %350 ]
  %730 = phi <2 x i64> [ %524, %510 ], [ %652, %574 ], [ %450, %350 ]
  %731 = phi <2 x i64> [ %521, %510 ], [ %649, %574 ], [ %447, %350 ]
  %732 = phi <2 x i64> [ %518, %510 ], [ %646, %574 ], [ %444, %350 ]
  %733 = phi <2 x i64> [ %515, %510 ], [ %643, %574 ], [ %441, %350 ]
  %734 = phi <2 x i64> [ %512, %510 ], [ %640, %574 ], [ %438, %350 ]
  %735 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %7, i64 0, i64 0
  %736 = bitcast <4 x i64>* %194 to <2 x i64>*
  %737 = shufflevector <2 x i64> %734, <2 x i64> %733, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %738 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 1
  %739 = bitcast <4 x i64>* %738 to <2 x i64>*
  %740 = shufflevector <2 x i64> %732, <2 x i64> %731, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %741 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 2
  %742 = bitcast <4 x i64>* %741 to <2 x i64>*
  %743 = shufflevector <2 x i64> %730, <2 x i64> %729, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %744 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 3
  %745 = bitcast <4 x i64>* %744 to <2 x i64>*
  %746 = shufflevector <2 x i64> %728, <2 x i64> %727, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %747 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 4
  %748 = bitcast <4 x i64>* %747 to <2 x i64>*
  %749 = shufflevector <2 x i64> %726, <2 x i64> %725, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %750 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 5
  %751 = bitcast <4 x i64>* %750 to <2 x i64>*
  %752 = shufflevector <2 x i64> %724, <2 x i64> %723, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %753 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 6
  %754 = bitcast <4 x i64>* %753 to <2 x i64>*
  %755 = shufflevector <2 x i64> %722, <2 x i64> %721, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %756 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 7
  %757 = bitcast <4 x i64>* %756 to <2 x i64>*
  %758 = shufflevector <2 x i64> %720, <2 x i64> %719, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %759 = getelementptr inbounds <2 x i64>, <2 x i64>* %736, i64 1
  %760 = load <2 x i64>, <2 x i64>* %759, align 16
  %761 = shufflevector <2 x i64> %760, <2 x i64> %718, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %762 = getelementptr inbounds <2 x i64>, <2 x i64>* %739, i64 1
  %763 = load <2 x i64>, <2 x i64>* %762, align 16
  %764 = shufflevector <2 x i64> %763, <2 x i64> %717, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %765 = getelementptr inbounds <2 x i64>, <2 x i64>* %742, i64 1
  %766 = load <2 x i64>, <2 x i64>* %765, align 16
  %767 = shufflevector <2 x i64> %766, <2 x i64> %716, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %768 = getelementptr inbounds <2 x i64>, <2 x i64>* %745, i64 1
  %769 = load <2 x i64>, <2 x i64>* %768, align 16
  %770 = shufflevector <2 x i64> %769, <2 x i64> %715, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %771 = getelementptr inbounds <2 x i64>, <2 x i64>* %748, i64 1
  %772 = load <2 x i64>, <2 x i64>* %771, align 16
  %773 = shufflevector <2 x i64> %772, <2 x i64> %714, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %774 = getelementptr inbounds <2 x i64>, <2 x i64>* %751, i64 1
  %775 = load <2 x i64>, <2 x i64>* %774, align 16
  %776 = shufflevector <2 x i64> %775, <2 x i64> %713, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %777 = getelementptr inbounds <2 x i64>, <2 x i64>* %754, i64 1
  %778 = load <2 x i64>, <2 x i64>* %777, align 16
  %779 = shufflevector <2 x i64> %778, <2 x i64> %712, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %780 = getelementptr inbounds <2 x i64>, <2 x i64>* %757, i64 1
  %781 = load <2 x i64>, <2 x i64>* %780, align 16
  %782 = shufflevector <2 x i64> %781, <2 x i64> %711, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %783 = bitcast <4 x i64> %737 to <16 x i16>
  %784 = bitcast <4 x i64> %740 to <16 x i16>
  %785 = shufflevector <16 x i16> %783, <16 x i16> %784, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %786 = shufflevector <16 x i16> %783, <16 x i16> %784, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %787 = bitcast <4 x i64> %743 to <16 x i16>
  %788 = bitcast <4 x i64> %746 to <16 x i16>
  %789 = shufflevector <16 x i16> %787, <16 x i16> %788, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %790 = shufflevector <16 x i16> %787, <16 x i16> %788, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %791 = bitcast <4 x i64> %749 to <16 x i16>
  %792 = bitcast <4 x i64> %752 to <16 x i16>
  %793 = shufflevector <16 x i16> %791, <16 x i16> %792, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %794 = shufflevector <16 x i16> %791, <16 x i16> %792, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %795 = bitcast <4 x i64> %755 to <16 x i16>
  %796 = bitcast <4 x i64> %758 to <16 x i16>
  %797 = shufflevector <16 x i16> %795, <16 x i16> %796, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %798 = shufflevector <16 x i16> %795, <16 x i16> %796, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %799 = bitcast <16 x i16> %785 to <8 x i32>
  %800 = bitcast <16 x i16> %789 to <8 x i32>
  %801 = shufflevector <8 x i32> %799, <8 x i32> %800, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %802 = shufflevector <8 x i32> %799, <8 x i32> %800, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %803 = bitcast <16 x i16> %793 to <8 x i32>
  %804 = bitcast <16 x i16> %797 to <8 x i32>
  %805 = shufflevector <8 x i32> %803, <8 x i32> %804, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %806 = shufflevector <8 x i32> %803, <8 x i32> %804, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %807 = bitcast <16 x i16> %786 to <8 x i32>
  %808 = bitcast <16 x i16> %790 to <8 x i32>
  %809 = shufflevector <8 x i32> %807, <8 x i32> %808, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %810 = shufflevector <8 x i32> %807, <8 x i32> %808, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %811 = bitcast <16 x i16> %794 to <8 x i32>
  %812 = bitcast <16 x i16> %798 to <8 x i32>
  %813 = shufflevector <8 x i32> %811, <8 x i32> %812, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %814 = shufflevector <8 x i32> %811, <8 x i32> %812, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %815 = bitcast <8 x i32> %801 to <4 x i64>
  %816 = bitcast <8 x i32> %805 to <4 x i64>
  %817 = shufflevector <4 x i64> %815, <4 x i64> %816, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %817, <4 x i64>* %735, align 32
  %818 = shufflevector <4 x i64> %815, <4 x i64> %816, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %819 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %7, i64 0, i64 1
  store <4 x i64> %818, <4 x i64>* %819, align 32
  %820 = bitcast <8 x i32> %809 to <4 x i64>
  %821 = bitcast <8 x i32> %813 to <4 x i64>
  %822 = shufflevector <4 x i64> %820, <4 x i64> %821, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %823 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %7, i64 0, i64 4
  store <4 x i64> %822, <4 x i64>* %823, align 32
  %824 = shufflevector <4 x i64> %820, <4 x i64> %821, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %825 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %7, i64 0, i64 5
  store <4 x i64> %824, <4 x i64>* %825, align 32
  %826 = bitcast <8 x i32> %802 to <4 x i64>
  %827 = bitcast <8 x i32> %806 to <4 x i64>
  %828 = shufflevector <4 x i64> %826, <4 x i64> %827, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %829 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %7, i64 0, i64 2
  store <4 x i64> %828, <4 x i64>* %829, align 32
  %830 = shufflevector <4 x i64> %826, <4 x i64> %827, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %831 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %7, i64 0, i64 3
  store <4 x i64> %830, <4 x i64>* %831, align 32
  %832 = bitcast <8 x i32> %810 to <4 x i64>
  %833 = bitcast <8 x i32> %814 to <4 x i64>
  %834 = shufflevector <4 x i64> %832, <4 x i64> %833, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %835 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %7, i64 0, i64 6
  store <4 x i64> %834, <4 x i64>* %835, align 32
  %836 = shufflevector <4 x i64> %832, <4 x i64> %833, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %837 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %7, i64 0, i64 7
  store <4 x i64> %836, <4 x i64>* %837, align 32
  %838 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %7, i64 0, i64 8
  %839 = bitcast <4 x i64> %761 to <16 x i16>
  %840 = bitcast <4 x i64> %764 to <16 x i16>
  %841 = shufflevector <16 x i16> %839, <16 x i16> %840, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %842 = shufflevector <16 x i16> %839, <16 x i16> %840, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %843 = bitcast <4 x i64> %767 to <16 x i16>
  %844 = bitcast <4 x i64> %770 to <16 x i16>
  %845 = shufflevector <16 x i16> %843, <16 x i16> %844, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %846 = shufflevector <16 x i16> %843, <16 x i16> %844, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %847 = bitcast <4 x i64> %773 to <16 x i16>
  %848 = bitcast <4 x i64> %776 to <16 x i16>
  %849 = shufflevector <16 x i16> %847, <16 x i16> %848, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %850 = shufflevector <16 x i16> %847, <16 x i16> %848, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %851 = bitcast <4 x i64> %779 to <16 x i16>
  %852 = bitcast <4 x i64> %782 to <16 x i16>
  %853 = shufflevector <16 x i16> %851, <16 x i16> %852, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %854 = shufflevector <16 x i16> %851, <16 x i16> %852, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %855 = bitcast <16 x i16> %841 to <8 x i32>
  %856 = bitcast <16 x i16> %845 to <8 x i32>
  %857 = shufflevector <8 x i32> %855, <8 x i32> %856, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %858 = shufflevector <8 x i32> %855, <8 x i32> %856, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %859 = bitcast <16 x i16> %849 to <8 x i32>
  %860 = bitcast <16 x i16> %853 to <8 x i32>
  %861 = shufflevector <8 x i32> %859, <8 x i32> %860, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %862 = shufflevector <8 x i32> %859, <8 x i32> %860, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %863 = bitcast <16 x i16> %842 to <8 x i32>
  %864 = bitcast <16 x i16> %846 to <8 x i32>
  %865 = shufflevector <8 x i32> %863, <8 x i32> %864, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %866 = shufflevector <8 x i32> %863, <8 x i32> %864, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %867 = bitcast <16 x i16> %850 to <8 x i32>
  %868 = bitcast <16 x i16> %854 to <8 x i32>
  %869 = shufflevector <8 x i32> %867, <8 x i32> %868, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %870 = shufflevector <8 x i32> %867, <8 x i32> %868, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %871 = bitcast <8 x i32> %857 to <4 x i64>
  %872 = bitcast <8 x i32> %861 to <4 x i64>
  %873 = shufflevector <4 x i64> %871, <4 x i64> %872, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %873, <4 x i64>* %838, align 32
  %874 = shufflevector <4 x i64> %871, <4 x i64> %872, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %875 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %7, i64 0, i64 9
  store <4 x i64> %874, <4 x i64>* %875, align 32
  %876 = bitcast <8 x i32> %865 to <4 x i64>
  %877 = bitcast <8 x i32> %869 to <4 x i64>
  %878 = shufflevector <4 x i64> %876, <4 x i64> %877, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %879 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %7, i64 0, i64 12
  store <4 x i64> %878, <4 x i64>* %879, align 32
  %880 = shufflevector <4 x i64> %876, <4 x i64> %877, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %881 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %7, i64 0, i64 13
  store <4 x i64> %880, <4 x i64>* %881, align 32
  %882 = bitcast <8 x i32> %858 to <4 x i64>
  %883 = bitcast <8 x i32> %862 to <4 x i64>
  %884 = shufflevector <4 x i64> %882, <4 x i64> %883, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %885 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %7, i64 0, i64 10
  store <4 x i64> %884, <4 x i64>* %885, align 32
  %886 = shufflevector <4 x i64> %882, <4 x i64> %883, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %887 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %7, i64 0, i64 11
  store <4 x i64> %886, <4 x i64>* %887, align 32
  %888 = bitcast <8 x i32> %866 to <4 x i64>
  %889 = bitcast <8 x i32> %870 to <4 x i64>
  %890 = shufflevector <4 x i64> %888, <4 x i64> %889, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %891 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %7, i64 0, i64 14
  store <4 x i64> %890, <4 x i64>* %891, align 32
  %892 = shufflevector <4 x i64> %888, <4 x i64> %889, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %893 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %7, i64 0, i64 15
  store <4 x i64> %892, <4 x i64>* %893, align 32
  %894 = icmp eq i32 %195, 0
  br i1 %894, label %905, label %895

895:                                              ; preds = %710
  %896 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 15
  %897 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 14
  %898 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 13
  %899 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 12
  %900 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 11
  %901 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 10
  %902 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 9
  %903 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 8
  store <4 x i64> %817, <4 x i64>* %896, align 32
  store <4 x i64> %818, <4 x i64>* %897, align 32
  store <4 x i64> %828, <4 x i64>* %898, align 32
  store <4 x i64> %830, <4 x i64>* %899, align 32
  store <4 x i64> %822, <4 x i64>* %900, align 32
  store <4 x i64> %824, <4 x i64>* %901, align 32
  store <4 x i64> %834, <4 x i64>* %902, align 32
  store <4 x i64> %836, <4 x i64>* %903, align 32
  store <4 x i64> %873, <4 x i64>* %756, align 32
  store <4 x i64> %874, <4 x i64>* %753, align 32
  store <4 x i64> %884, <4 x i64>* %750, align 32
  store <4 x i64> %886, <4 x i64>* %747, align 32
  store <4 x i64> %878, <4 x i64>* %744, align 32
  store <4 x i64> %880, <4 x i64>* %741, align 32
  store <4 x i64> %890, <4 x i64>* %738, align 32
  %904 = getelementptr inbounds [16 x <4 x i64>], [16 x <4 x i64>]* %6, i64 0, i64 0
  store <4 x i64> %892, <4 x i64>* %904, align 32
  br label %905

905:                                              ; preds = %895, %710
  %906 = phi <4 x i64>* [ %735, %710 ], [ %194, %895 ]
  call void %17(<4 x i64>* %906, <4 x i64>* %906, i8 signext %12) #8
  %907 = getelementptr inbounds i8, i8* %10, i64 2
  %908 = load i8, i8* %907, align 1
  %909 = sext i8 %908 to i32
  %910 = icmp slt i8 %908, 0
  br i1 %910, label %911, label %1069

911:                                              ; preds = %905
  %912 = sub nsw i32 0, %909
  %913 = xor i32 %909, -1
  %914 = shl i32 1, %913
  %915 = trunc i32 %914 to i16
  %916 = insertelement <16 x i16> undef, i16 %915, i32 0
  %917 = shufflevector <16 x i16> %916, <16 x i16> undef, <16 x i32> zeroinitializer
  %918 = bitcast <4 x i64>* %906 to <16 x i16>*
  %919 = load <16 x i16>, <16 x i16>* %918, align 32
  %920 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %919, <16 x i16> %917) #8
  %921 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %920, i32 %912) #8
  store <16 x i16> %921, <16 x i16>* %918, align 32
  %922 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 1
  %923 = bitcast <4 x i64>* %922 to <16 x i16>*
  %924 = load <16 x i16>, <16 x i16>* %923, align 32
  %925 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %924, <16 x i16> %917) #8
  %926 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %925, i32 %912) #8
  store <16 x i16> %926, <16 x i16>* %923, align 32
  %927 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 2
  %928 = bitcast <4 x i64>* %927 to <16 x i16>*
  %929 = load <16 x i16>, <16 x i16>* %928, align 32
  %930 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %929, <16 x i16> %917) #8
  %931 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %930, i32 %912) #8
  store <16 x i16> %931, <16 x i16>* %928, align 32
  %932 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 3
  %933 = bitcast <4 x i64>* %932 to <16 x i16>*
  %934 = load <16 x i16>, <16 x i16>* %933, align 32
  %935 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %934, <16 x i16> %917) #8
  %936 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %935, i32 %912) #8
  store <16 x i16> %936, <16 x i16>* %933, align 32
  %937 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 4
  %938 = bitcast <4 x i64>* %937 to <16 x i16>*
  %939 = load <16 x i16>, <16 x i16>* %938, align 32
  %940 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %939, <16 x i16> %917) #8
  %941 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %940, i32 %912) #8
  store <16 x i16> %941, <16 x i16>* %938, align 32
  %942 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 5
  %943 = bitcast <4 x i64>* %942 to <16 x i16>*
  %944 = load <16 x i16>, <16 x i16>* %943, align 32
  %945 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %944, <16 x i16> %917) #8
  %946 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %945, i32 %912) #8
  store <16 x i16> %946, <16 x i16>* %943, align 32
  %947 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 6
  %948 = bitcast <4 x i64>* %947 to <16 x i16>*
  %949 = load <16 x i16>, <16 x i16>* %948, align 32
  %950 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %949, <16 x i16> %917) #8
  %951 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %950, i32 %912) #8
  store <16 x i16> %951, <16 x i16>* %948, align 32
  %952 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 7
  %953 = bitcast <4 x i64>* %952 to <16 x i16>*
  %954 = load <16 x i16>, <16 x i16>* %953, align 32
  %955 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %954, <16 x i16> %917) #8
  %956 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %955, i32 %912) #8
  store <16 x i16> %956, <16 x i16>* %953, align 32
  %957 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 8
  %958 = bitcast <4 x i64>* %957 to <16 x i16>*
  %959 = load <16 x i16>, <16 x i16>* %958, align 32
  %960 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %959, <16 x i16> %917) #8
  %961 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %960, i32 %912) #8
  store <16 x i16> %961, <16 x i16>* %958, align 32
  %962 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 9
  %963 = bitcast <4 x i64>* %962 to <16 x i16>*
  %964 = load <16 x i16>, <16 x i16>* %963, align 32
  %965 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %964, <16 x i16> %917) #8
  %966 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %965, i32 %912) #8
  store <16 x i16> %966, <16 x i16>* %963, align 32
  %967 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 10
  %968 = bitcast <4 x i64>* %967 to <16 x i16>*
  %969 = load <16 x i16>, <16 x i16>* %968, align 32
  %970 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %969, <16 x i16> %917) #8
  %971 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %970, i32 %912) #8
  store <16 x i16> %971, <16 x i16>* %968, align 32
  %972 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 11
  %973 = bitcast <4 x i64>* %972 to <16 x i16>*
  %974 = load <16 x i16>, <16 x i16>* %973, align 32
  %975 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %974, <16 x i16> %917) #8
  %976 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %975, i32 %912) #8
  store <16 x i16> %976, <16 x i16>* %973, align 32
  %977 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 12
  %978 = bitcast <4 x i64>* %977 to <16 x i16>*
  %979 = load <16 x i16>, <16 x i16>* %978, align 32
  %980 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %979, <16 x i16> %917) #8
  %981 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %980, i32 %912) #8
  store <16 x i16> %981, <16 x i16>* %978, align 32
  %982 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 13
  %983 = bitcast <4 x i64>* %982 to <16 x i16>*
  %984 = load <16 x i16>, <16 x i16>* %983, align 32
  %985 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %984, <16 x i16> %917) #8
  %986 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %985, i32 %912) #8
  store <16 x i16> %986, <16 x i16>* %983, align 32
  %987 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 14
  %988 = bitcast <4 x i64>* %987 to <16 x i16>*
  %989 = load <16 x i16>, <16 x i16>* %988, align 32
  %990 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %989, <16 x i16> %917) #8
  %991 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %990, i32 %912) #8
  store <16 x i16> %991, <16 x i16>* %988, align 32
  %992 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 15
  %993 = bitcast <4 x i64>* %992 to <16 x i16>*
  %994 = load <16 x i16>, <16 x i16>* %993, align 32
  %995 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %994, <16 x i16> %917) #8
  %996 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %995, i32 %912) #8
  store <16 x i16> %996, <16 x i16>* %993, align 32
  %997 = bitcast <16 x i16> %921 to <2 x i128>
  %998 = extractelement <2 x i128> %997, i32 0
  %999 = bitcast i128 %998 to <2 x i64>
  %1000 = bitcast <16 x i16> %961 to <2 x i128>
  %1001 = extractelement <2 x i128> %1000, i32 0
  %1002 = bitcast i128 %1001 to <2 x i64>
  %1003 = bitcast <16 x i16> %926 to <2 x i128>
  %1004 = extractelement <2 x i128> %1003, i32 0
  %1005 = bitcast i128 %1004 to <2 x i64>
  %1006 = bitcast <16 x i16> %966 to <2 x i128>
  %1007 = extractelement <2 x i128> %1006, i32 0
  %1008 = bitcast i128 %1007 to <2 x i64>
  %1009 = bitcast <16 x i16> %931 to <2 x i128>
  %1010 = extractelement <2 x i128> %1009, i32 0
  %1011 = bitcast i128 %1010 to <2 x i64>
  %1012 = bitcast <16 x i16> %971 to <2 x i128>
  %1013 = extractelement <2 x i128> %1012, i32 0
  %1014 = bitcast i128 %1013 to <2 x i64>
  %1015 = bitcast <16 x i16> %936 to <2 x i128>
  %1016 = extractelement <2 x i128> %1015, i32 0
  %1017 = bitcast i128 %1016 to <2 x i64>
  %1018 = bitcast <16 x i16> %976 to <2 x i128>
  %1019 = extractelement <2 x i128> %1018, i32 0
  %1020 = bitcast i128 %1019 to <2 x i64>
  %1021 = bitcast <16 x i16> %941 to <2 x i128>
  %1022 = extractelement <2 x i128> %1021, i32 0
  %1023 = bitcast i128 %1022 to <2 x i64>
  %1024 = bitcast <16 x i16> %981 to <2 x i128>
  %1025 = extractelement <2 x i128> %1024, i32 0
  %1026 = bitcast i128 %1025 to <2 x i64>
  %1027 = bitcast <16 x i16> %946 to <2 x i128>
  %1028 = extractelement <2 x i128> %1027, i32 0
  %1029 = bitcast i128 %1028 to <2 x i64>
  %1030 = bitcast <16 x i16> %986 to <2 x i128>
  %1031 = extractelement <2 x i128> %1030, i32 0
  %1032 = bitcast i128 %1031 to <2 x i64>
  %1033 = bitcast <16 x i16> %951 to <2 x i128>
  %1034 = extractelement <2 x i128> %1033, i32 0
  %1035 = bitcast i128 %1034 to <2 x i64>
  %1036 = bitcast <16 x i16> %991 to <2 x i128>
  %1037 = extractelement <2 x i128> %1036, i32 0
  %1038 = bitcast i128 %1037 to <2 x i64>
  %1039 = bitcast <16 x i16> %956 to <2 x i128>
  %1040 = extractelement <2 x i128> %1039, i32 0
  %1041 = bitcast i128 %1040 to <2 x i64>
  %1042 = bitcast <16 x i16> %996 to <2 x i128>
  %1043 = extractelement <2 x i128> %1042, i32 0
  %1044 = bitcast i128 %1043 to <2 x i64>
  %1045 = bitcast <16 x i16> %961 to <2 x i128>
  %1046 = extractelement <2 x i128> %1045, i32 1
  %1047 = bitcast i128 %1046 to <2 x i64>
  %1048 = bitcast <16 x i16> %966 to <2 x i128>
  %1049 = extractelement <2 x i128> %1048, i32 1
  %1050 = bitcast i128 %1049 to <2 x i64>
  %1051 = bitcast <16 x i16> %971 to <2 x i128>
  %1052 = extractelement <2 x i128> %1051, i32 1
  %1053 = bitcast i128 %1052 to <2 x i64>
  %1054 = bitcast <16 x i16> %976 to <2 x i128>
  %1055 = extractelement <2 x i128> %1054, i32 1
  %1056 = bitcast i128 %1055 to <2 x i64>
  %1057 = bitcast <16 x i16> %981 to <2 x i128>
  %1058 = extractelement <2 x i128> %1057, i32 1
  %1059 = bitcast i128 %1058 to <2 x i64>
  %1060 = bitcast <16 x i16> %986 to <2 x i128>
  %1061 = extractelement <2 x i128> %1060, i32 1
  %1062 = bitcast i128 %1061 to <2 x i64>
  %1063 = bitcast <16 x i16> %991 to <2 x i128>
  %1064 = extractelement <2 x i128> %1063, i32 1
  %1065 = bitcast i128 %1064 to <2 x i64>
  %1066 = bitcast <16 x i16> %996 to <2 x i128>
  %1067 = extractelement <2 x i128> %1066, i32 1
  %1068 = bitcast i128 %1067 to <2 x i64>
  br label %1271

1069:                                             ; preds = %905
  %1070 = icmp eq i8 %908, 0
  br i1 %1070, label %1071, label %1135

1071:                                             ; preds = %1069
  %1072 = bitcast <4 x i64>* %906 to <2 x i64>*
  %1073 = load <2 x i64>, <2 x i64>* %1072, align 16
  %1074 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 8
  %1075 = bitcast <4 x i64>* %1074 to <2 x i64>*
  %1076 = load <2 x i64>, <2 x i64>* %1075, align 16
  %1077 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 1
  %1078 = bitcast <4 x i64>* %1077 to <2 x i64>*
  %1079 = load <2 x i64>, <2 x i64>* %1078, align 16
  %1080 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 9
  %1081 = bitcast <4 x i64>* %1080 to <2 x i64>*
  %1082 = load <2 x i64>, <2 x i64>* %1081, align 16
  %1083 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 2
  %1084 = bitcast <4 x i64>* %1083 to <2 x i64>*
  %1085 = load <2 x i64>, <2 x i64>* %1084, align 16
  %1086 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 10
  %1087 = bitcast <4 x i64>* %1086 to <2 x i64>*
  %1088 = load <2 x i64>, <2 x i64>* %1087, align 16
  %1089 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 3
  %1090 = bitcast <4 x i64>* %1089 to <2 x i64>*
  %1091 = load <2 x i64>, <2 x i64>* %1090, align 16
  %1092 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 11
  %1093 = bitcast <4 x i64>* %1092 to <2 x i64>*
  %1094 = load <2 x i64>, <2 x i64>* %1093, align 16
  %1095 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 4
  %1096 = bitcast <4 x i64>* %1095 to <2 x i64>*
  %1097 = load <2 x i64>, <2 x i64>* %1096, align 16
  %1098 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 12
  %1099 = bitcast <4 x i64>* %1098 to <2 x i64>*
  %1100 = load <2 x i64>, <2 x i64>* %1099, align 16
  %1101 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 5
  %1102 = bitcast <4 x i64>* %1101 to <2 x i64>*
  %1103 = load <2 x i64>, <2 x i64>* %1102, align 16
  %1104 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 13
  %1105 = bitcast <4 x i64>* %1104 to <2 x i64>*
  %1106 = load <2 x i64>, <2 x i64>* %1105, align 16
  %1107 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 6
  %1108 = bitcast <4 x i64>* %1107 to <2 x i64>*
  %1109 = load <2 x i64>, <2 x i64>* %1108, align 16
  %1110 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 14
  %1111 = bitcast <4 x i64>* %1110 to <2 x i64>*
  %1112 = load <2 x i64>, <2 x i64>* %1111, align 16
  %1113 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 7
  %1114 = bitcast <4 x i64>* %1113 to <2 x i64>*
  %1115 = load <2 x i64>, <2 x i64>* %1114, align 16
  %1116 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 15
  %1117 = bitcast <4 x i64>* %1116 to <2 x i64>*
  %1118 = load <2 x i64>, <2 x i64>* %1117, align 16
  %1119 = getelementptr inbounds <2 x i64>, <2 x i64>* %1075, i64 1
  %1120 = load <2 x i64>, <2 x i64>* %1119, align 16
  %1121 = getelementptr inbounds <2 x i64>, <2 x i64>* %1081, i64 1
  %1122 = load <2 x i64>, <2 x i64>* %1121, align 16
  %1123 = getelementptr inbounds <2 x i64>, <2 x i64>* %1087, i64 1
  %1124 = load <2 x i64>, <2 x i64>* %1123, align 16
  %1125 = getelementptr inbounds <2 x i64>, <2 x i64>* %1093, i64 1
  %1126 = load <2 x i64>, <2 x i64>* %1125, align 16
  %1127 = getelementptr inbounds <2 x i64>, <2 x i64>* %1099, i64 1
  %1128 = load <2 x i64>, <2 x i64>* %1127, align 16
  %1129 = getelementptr inbounds <2 x i64>, <2 x i64>* %1105, i64 1
  %1130 = load <2 x i64>, <2 x i64>* %1129, align 16
  %1131 = getelementptr inbounds <2 x i64>, <2 x i64>* %1111, i64 1
  %1132 = load <2 x i64>, <2 x i64>* %1131, align 16
  %1133 = getelementptr inbounds <2 x i64>, <2 x i64>* %1117, i64 1
  %1134 = load <2 x i64>, <2 x i64>* %1133, align 16
  br label %1271

1135:                                             ; preds = %1069
  %1136 = bitcast <4 x i64>* %906 to <16 x i16>*
  %1137 = load <16 x i16>, <16 x i16>* %1136, align 32
  %1138 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %1137, i32 %909) #8
  store <16 x i16> %1138, <16 x i16>* %1136, align 32
  %1139 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 1
  %1140 = bitcast <4 x i64>* %1139 to <16 x i16>*
  %1141 = load <16 x i16>, <16 x i16>* %1140, align 32
  %1142 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %1141, i32 %909) #8
  store <16 x i16> %1142, <16 x i16>* %1140, align 32
  %1143 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 2
  %1144 = bitcast <4 x i64>* %1143 to <16 x i16>*
  %1145 = load <16 x i16>, <16 x i16>* %1144, align 32
  %1146 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %1145, i32 %909) #8
  store <16 x i16> %1146, <16 x i16>* %1144, align 32
  %1147 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 3
  %1148 = bitcast <4 x i64>* %1147 to <16 x i16>*
  %1149 = load <16 x i16>, <16 x i16>* %1148, align 32
  %1150 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %1149, i32 %909) #8
  store <16 x i16> %1150, <16 x i16>* %1148, align 32
  %1151 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 4
  %1152 = bitcast <4 x i64>* %1151 to <16 x i16>*
  %1153 = load <16 x i16>, <16 x i16>* %1152, align 32
  %1154 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %1153, i32 %909) #8
  store <16 x i16> %1154, <16 x i16>* %1152, align 32
  %1155 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 5
  %1156 = bitcast <4 x i64>* %1155 to <16 x i16>*
  %1157 = load <16 x i16>, <16 x i16>* %1156, align 32
  %1158 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %1157, i32 %909) #8
  store <16 x i16> %1158, <16 x i16>* %1156, align 32
  %1159 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 6
  %1160 = bitcast <4 x i64>* %1159 to <16 x i16>*
  %1161 = load <16 x i16>, <16 x i16>* %1160, align 32
  %1162 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %1161, i32 %909) #8
  store <16 x i16> %1162, <16 x i16>* %1160, align 32
  %1163 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 7
  %1164 = bitcast <4 x i64>* %1163 to <16 x i16>*
  %1165 = load <16 x i16>, <16 x i16>* %1164, align 32
  %1166 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %1165, i32 %909) #8
  store <16 x i16> %1166, <16 x i16>* %1164, align 32
  %1167 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 8
  %1168 = bitcast <4 x i64>* %1167 to <16 x i16>*
  %1169 = load <16 x i16>, <16 x i16>* %1168, align 32
  %1170 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %1169, i32 %909) #8
  store <16 x i16> %1170, <16 x i16>* %1168, align 32
  %1171 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 9
  %1172 = bitcast <4 x i64>* %1171 to <16 x i16>*
  %1173 = load <16 x i16>, <16 x i16>* %1172, align 32
  %1174 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %1173, i32 %909) #8
  store <16 x i16> %1174, <16 x i16>* %1172, align 32
  %1175 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 10
  %1176 = bitcast <4 x i64>* %1175 to <16 x i16>*
  %1177 = load <16 x i16>, <16 x i16>* %1176, align 32
  %1178 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %1177, i32 %909) #8
  store <16 x i16> %1178, <16 x i16>* %1176, align 32
  %1179 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 11
  %1180 = bitcast <4 x i64>* %1179 to <16 x i16>*
  %1181 = load <16 x i16>, <16 x i16>* %1180, align 32
  %1182 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %1181, i32 %909) #8
  store <16 x i16> %1182, <16 x i16>* %1180, align 32
  %1183 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 12
  %1184 = bitcast <4 x i64>* %1183 to <16 x i16>*
  %1185 = load <16 x i16>, <16 x i16>* %1184, align 32
  %1186 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %1185, i32 %909) #8
  store <16 x i16> %1186, <16 x i16>* %1184, align 32
  %1187 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 13
  %1188 = bitcast <4 x i64>* %1187 to <16 x i16>*
  %1189 = load <16 x i16>, <16 x i16>* %1188, align 32
  %1190 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %1189, i32 %909) #8
  store <16 x i16> %1190, <16 x i16>* %1188, align 32
  %1191 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 14
  %1192 = bitcast <4 x i64>* %1191 to <16 x i16>*
  %1193 = load <16 x i16>, <16 x i16>* %1192, align 32
  %1194 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %1193, i32 %909) #8
  store <16 x i16> %1194, <16 x i16>* %1192, align 32
  %1195 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 15
  %1196 = bitcast <4 x i64>* %1195 to <16 x i16>*
  %1197 = load <16 x i16>, <16 x i16>* %1196, align 32
  %1198 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %1197, i32 %909) #8
  store <16 x i16> %1198, <16 x i16>* %1196, align 32
  %1199 = bitcast <16 x i16> %1138 to <2 x i128>
  %1200 = extractelement <2 x i128> %1199, i32 0
  %1201 = bitcast i128 %1200 to <2 x i64>
  %1202 = bitcast <16 x i16> %1170 to <2 x i128>
  %1203 = extractelement <2 x i128> %1202, i32 0
  %1204 = bitcast i128 %1203 to <2 x i64>
  %1205 = bitcast <16 x i16> %1142 to <2 x i128>
  %1206 = extractelement <2 x i128> %1205, i32 0
  %1207 = bitcast i128 %1206 to <2 x i64>
  %1208 = bitcast <16 x i16> %1174 to <2 x i128>
  %1209 = extractelement <2 x i128> %1208, i32 0
  %1210 = bitcast i128 %1209 to <2 x i64>
  %1211 = bitcast <16 x i16> %1146 to <2 x i128>
  %1212 = extractelement <2 x i128> %1211, i32 0
  %1213 = bitcast i128 %1212 to <2 x i64>
  %1214 = bitcast <16 x i16> %1178 to <2 x i128>
  %1215 = extractelement <2 x i128> %1214, i32 0
  %1216 = bitcast i128 %1215 to <2 x i64>
  %1217 = bitcast <16 x i16> %1150 to <2 x i128>
  %1218 = extractelement <2 x i128> %1217, i32 0
  %1219 = bitcast i128 %1218 to <2 x i64>
  %1220 = bitcast <16 x i16> %1182 to <2 x i128>
  %1221 = extractelement <2 x i128> %1220, i32 0
  %1222 = bitcast i128 %1221 to <2 x i64>
  %1223 = bitcast <16 x i16> %1154 to <2 x i128>
  %1224 = extractelement <2 x i128> %1223, i32 0
  %1225 = bitcast i128 %1224 to <2 x i64>
  %1226 = bitcast <16 x i16> %1186 to <2 x i128>
  %1227 = extractelement <2 x i128> %1226, i32 0
  %1228 = bitcast i128 %1227 to <2 x i64>
  %1229 = bitcast <16 x i16> %1158 to <2 x i128>
  %1230 = extractelement <2 x i128> %1229, i32 0
  %1231 = bitcast i128 %1230 to <2 x i64>
  %1232 = bitcast <16 x i16> %1190 to <2 x i128>
  %1233 = extractelement <2 x i128> %1232, i32 0
  %1234 = bitcast i128 %1233 to <2 x i64>
  %1235 = bitcast <16 x i16> %1162 to <2 x i128>
  %1236 = extractelement <2 x i128> %1235, i32 0
  %1237 = bitcast i128 %1236 to <2 x i64>
  %1238 = bitcast <16 x i16> %1194 to <2 x i128>
  %1239 = extractelement <2 x i128> %1238, i32 0
  %1240 = bitcast i128 %1239 to <2 x i64>
  %1241 = bitcast <16 x i16> %1166 to <2 x i128>
  %1242 = extractelement <2 x i128> %1241, i32 0
  %1243 = bitcast i128 %1242 to <2 x i64>
  %1244 = bitcast <16 x i16> %1198 to <2 x i128>
  %1245 = extractelement <2 x i128> %1244, i32 0
  %1246 = bitcast i128 %1245 to <2 x i64>
  %1247 = bitcast <16 x i16> %1170 to <2 x i128>
  %1248 = extractelement <2 x i128> %1247, i32 1
  %1249 = bitcast i128 %1248 to <2 x i64>
  %1250 = bitcast <16 x i16> %1174 to <2 x i128>
  %1251 = extractelement <2 x i128> %1250, i32 1
  %1252 = bitcast i128 %1251 to <2 x i64>
  %1253 = bitcast <16 x i16> %1178 to <2 x i128>
  %1254 = extractelement <2 x i128> %1253, i32 1
  %1255 = bitcast i128 %1254 to <2 x i64>
  %1256 = bitcast <16 x i16> %1182 to <2 x i128>
  %1257 = extractelement <2 x i128> %1256, i32 1
  %1258 = bitcast i128 %1257 to <2 x i64>
  %1259 = bitcast <16 x i16> %1186 to <2 x i128>
  %1260 = extractelement <2 x i128> %1259, i32 1
  %1261 = bitcast i128 %1260 to <2 x i64>
  %1262 = bitcast <16 x i16> %1190 to <2 x i128>
  %1263 = extractelement <2 x i128> %1262, i32 1
  %1264 = bitcast i128 %1263 to <2 x i64>
  %1265 = bitcast <16 x i16> %1194 to <2 x i128>
  %1266 = extractelement <2 x i128> %1265, i32 1
  %1267 = bitcast i128 %1266 to <2 x i64>
  %1268 = bitcast <16 x i16> %1198 to <2 x i128>
  %1269 = extractelement <2 x i128> %1268, i32 1
  %1270 = bitcast i128 %1269 to <2 x i64>
  br label %1271

1271:                                             ; preds = %1071, %1135, %911
  %1272 = phi <2 x i64> [ %1134, %1071 ], [ %1270, %1135 ], [ %1068, %911 ]
  %1273 = phi <2 x i64> [ %1132, %1071 ], [ %1267, %1135 ], [ %1065, %911 ]
  %1274 = phi <2 x i64> [ %1130, %1071 ], [ %1264, %1135 ], [ %1062, %911 ]
  %1275 = phi <2 x i64> [ %1128, %1071 ], [ %1261, %1135 ], [ %1059, %911 ]
  %1276 = phi <2 x i64> [ %1126, %1071 ], [ %1258, %1135 ], [ %1056, %911 ]
  %1277 = phi <2 x i64> [ %1124, %1071 ], [ %1255, %1135 ], [ %1053, %911 ]
  %1278 = phi <2 x i64> [ %1122, %1071 ], [ %1252, %1135 ], [ %1050, %911 ]
  %1279 = phi <2 x i64> [ %1120, %1071 ], [ %1249, %1135 ], [ %1047, %911 ]
  %1280 = phi <2 x i64> [ %1118, %1071 ], [ %1246, %1135 ], [ %1044, %911 ]
  %1281 = phi <2 x i64> [ %1115, %1071 ], [ %1243, %1135 ], [ %1041, %911 ]
  %1282 = phi <2 x i64> [ %1112, %1071 ], [ %1240, %1135 ], [ %1038, %911 ]
  %1283 = phi <2 x i64> [ %1109, %1071 ], [ %1237, %1135 ], [ %1035, %911 ]
  %1284 = phi <2 x i64> [ %1106, %1071 ], [ %1234, %1135 ], [ %1032, %911 ]
  %1285 = phi <2 x i64> [ %1103, %1071 ], [ %1231, %1135 ], [ %1029, %911 ]
  %1286 = phi <2 x i64> [ %1100, %1071 ], [ %1228, %1135 ], [ %1026, %911 ]
  %1287 = phi <2 x i64> [ %1097, %1071 ], [ %1225, %1135 ], [ %1023, %911 ]
  %1288 = phi <2 x i64> [ %1094, %1071 ], [ %1222, %1135 ], [ %1020, %911 ]
  %1289 = phi <2 x i64> [ %1091, %1071 ], [ %1219, %1135 ], [ %1017, %911 ]
  %1290 = phi <2 x i64> [ %1088, %1071 ], [ %1216, %1135 ], [ %1014, %911 ]
  %1291 = phi <2 x i64> [ %1085, %1071 ], [ %1213, %1135 ], [ %1011, %911 ]
  %1292 = phi <2 x i64> [ %1082, %1071 ], [ %1210, %1135 ], [ %1008, %911 ]
  %1293 = phi <2 x i64> [ %1079, %1071 ], [ %1207, %1135 ], [ %1005, %911 ]
  %1294 = phi <2 x i64> [ %1076, %1071 ], [ %1204, %1135 ], [ %1002, %911 ]
  %1295 = phi <2 x i64> [ %1073, %1071 ], [ %1201, %1135 ], [ %999, %911 ]
  %1296 = bitcast <4 x i64>* %906 to <2 x i64>*
  %1297 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 8
  %1298 = shufflevector <2 x i64> %1295, <2 x i64> %1294, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1299 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 1
  %1300 = bitcast <4 x i64>* %1299 to <2 x i64>*
  %1301 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 9
  %1302 = shufflevector <2 x i64> %1293, <2 x i64> %1292, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1303 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 2
  %1304 = bitcast <4 x i64>* %1303 to <2 x i64>*
  %1305 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 10
  %1306 = shufflevector <2 x i64> %1291, <2 x i64> %1290, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1307 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 3
  %1308 = bitcast <4 x i64>* %1307 to <2 x i64>*
  %1309 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 11
  %1310 = shufflevector <2 x i64> %1289, <2 x i64> %1288, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1311 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 4
  %1312 = bitcast <4 x i64>* %1311 to <2 x i64>*
  %1313 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 12
  %1314 = shufflevector <2 x i64> %1287, <2 x i64> %1286, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1315 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 5
  %1316 = bitcast <4 x i64>* %1315 to <2 x i64>*
  %1317 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 13
  %1318 = shufflevector <2 x i64> %1285, <2 x i64> %1284, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1319 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 6
  %1320 = bitcast <4 x i64>* %1319 to <2 x i64>*
  %1321 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 14
  %1322 = shufflevector <2 x i64> %1283, <2 x i64> %1282, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1323 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 7
  %1324 = bitcast <4 x i64>* %1323 to <2 x i64>*
  %1325 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 15
  %1326 = shufflevector <2 x i64> %1281, <2 x i64> %1280, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1327 = getelementptr inbounds <2 x i64>, <2 x i64>* %1296, i64 1
  %1328 = load <2 x i64>, <2 x i64>* %1327, align 16
  %1329 = shufflevector <2 x i64> %1328, <2 x i64> %1279, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1330 = getelementptr inbounds <2 x i64>, <2 x i64>* %1300, i64 1
  %1331 = load <2 x i64>, <2 x i64>* %1330, align 16
  %1332 = shufflevector <2 x i64> %1331, <2 x i64> %1278, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1333 = getelementptr inbounds <2 x i64>, <2 x i64>* %1304, i64 1
  %1334 = load <2 x i64>, <2 x i64>* %1333, align 16
  %1335 = shufflevector <2 x i64> %1334, <2 x i64> %1277, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1336 = getelementptr inbounds <2 x i64>, <2 x i64>* %1308, i64 1
  %1337 = load <2 x i64>, <2 x i64>* %1336, align 16
  %1338 = shufflevector <2 x i64> %1337, <2 x i64> %1276, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1339 = getelementptr inbounds <2 x i64>, <2 x i64>* %1312, i64 1
  %1340 = load <2 x i64>, <2 x i64>* %1339, align 16
  %1341 = shufflevector <2 x i64> %1340, <2 x i64> %1275, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1342 = getelementptr inbounds <2 x i64>, <2 x i64>* %1316, i64 1
  %1343 = load <2 x i64>, <2 x i64>* %1342, align 16
  %1344 = shufflevector <2 x i64> %1343, <2 x i64> %1274, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1345 = getelementptr inbounds <2 x i64>, <2 x i64>* %1320, i64 1
  %1346 = load <2 x i64>, <2 x i64>* %1345, align 16
  %1347 = shufflevector <2 x i64> %1346, <2 x i64> %1273, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1348 = getelementptr inbounds <2 x i64>, <2 x i64>* %1324, i64 1
  %1349 = load <2 x i64>, <2 x i64>* %1348, align 16
  %1350 = shufflevector <2 x i64> %1349, <2 x i64> %1272, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1351 = bitcast <4 x i64> %1298 to <16 x i16>
  %1352 = bitcast <4 x i64> %1302 to <16 x i16>
  %1353 = shufflevector <16 x i16> %1351, <16 x i16> %1352, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1354 = shufflevector <16 x i16> %1351, <16 x i16> %1352, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1355 = bitcast <4 x i64> %1306 to <16 x i16>
  %1356 = bitcast <4 x i64> %1310 to <16 x i16>
  %1357 = shufflevector <16 x i16> %1355, <16 x i16> %1356, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1358 = shufflevector <16 x i16> %1355, <16 x i16> %1356, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1359 = bitcast <4 x i64> %1314 to <16 x i16>
  %1360 = bitcast <4 x i64> %1318 to <16 x i16>
  %1361 = shufflevector <16 x i16> %1359, <16 x i16> %1360, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1362 = shufflevector <16 x i16> %1359, <16 x i16> %1360, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1363 = bitcast <4 x i64> %1322 to <16 x i16>
  %1364 = bitcast <4 x i64> %1326 to <16 x i16>
  %1365 = shufflevector <16 x i16> %1363, <16 x i16> %1364, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1366 = shufflevector <16 x i16> %1363, <16 x i16> %1364, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1367 = bitcast <16 x i16> %1353 to <8 x i32>
  %1368 = bitcast <16 x i16> %1357 to <8 x i32>
  %1369 = shufflevector <8 x i32> %1367, <8 x i32> %1368, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1370 = shufflevector <8 x i32> %1367, <8 x i32> %1368, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1371 = bitcast <16 x i16> %1361 to <8 x i32>
  %1372 = bitcast <16 x i16> %1365 to <8 x i32>
  %1373 = shufflevector <8 x i32> %1371, <8 x i32> %1372, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1374 = shufflevector <8 x i32> %1371, <8 x i32> %1372, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1375 = bitcast <16 x i16> %1354 to <8 x i32>
  %1376 = bitcast <16 x i16> %1358 to <8 x i32>
  %1377 = shufflevector <8 x i32> %1375, <8 x i32> %1376, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1378 = shufflevector <8 x i32> %1375, <8 x i32> %1376, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1379 = bitcast <16 x i16> %1362 to <8 x i32>
  %1380 = bitcast <16 x i16> %1366 to <8 x i32>
  %1381 = shufflevector <8 x i32> %1379, <8 x i32> %1380, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1382 = shufflevector <8 x i32> %1379, <8 x i32> %1380, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1383 = bitcast <8 x i32> %1369 to <4 x i64>
  %1384 = bitcast <8 x i32> %1373 to <4 x i64>
  %1385 = shufflevector <4 x i64> %1383, <4 x i64> %1384, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1385, <4 x i64>* %906, align 32
  %1386 = shufflevector <4 x i64> %1383, <4 x i64> %1384, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1386, <4 x i64>* %1299, align 32
  %1387 = bitcast <8 x i32> %1377 to <4 x i64>
  %1388 = bitcast <8 x i32> %1381 to <4 x i64>
  %1389 = shufflevector <4 x i64> %1387, <4 x i64> %1388, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1389, <4 x i64>* %1311, align 32
  %1390 = shufflevector <4 x i64> %1387, <4 x i64> %1388, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1390, <4 x i64>* %1315, align 32
  %1391 = bitcast <8 x i32> %1370 to <4 x i64>
  %1392 = bitcast <8 x i32> %1374 to <4 x i64>
  %1393 = shufflevector <4 x i64> %1391, <4 x i64> %1392, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1393, <4 x i64>* %1303, align 32
  %1394 = shufflevector <4 x i64> %1391, <4 x i64> %1392, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1394, <4 x i64>* %1307, align 32
  %1395 = bitcast <8 x i32> %1378 to <4 x i64>
  %1396 = bitcast <8 x i32> %1382 to <4 x i64>
  %1397 = shufflevector <4 x i64> %1395, <4 x i64> %1396, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1397, <4 x i64>* %1319, align 32
  %1398 = shufflevector <4 x i64> %1395, <4 x i64> %1396, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1398, <4 x i64>* %1323, align 32
  %1399 = bitcast <4 x i64> %1329 to <16 x i16>
  %1400 = bitcast <4 x i64> %1332 to <16 x i16>
  %1401 = shufflevector <16 x i16> %1399, <16 x i16> %1400, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1402 = shufflevector <16 x i16> %1399, <16 x i16> %1400, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1403 = bitcast <4 x i64> %1335 to <16 x i16>
  %1404 = bitcast <4 x i64> %1338 to <16 x i16>
  %1405 = shufflevector <16 x i16> %1403, <16 x i16> %1404, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1406 = shufflevector <16 x i16> %1403, <16 x i16> %1404, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1407 = bitcast <4 x i64> %1341 to <16 x i16>
  %1408 = bitcast <4 x i64> %1344 to <16 x i16>
  %1409 = shufflevector <16 x i16> %1407, <16 x i16> %1408, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1410 = shufflevector <16 x i16> %1407, <16 x i16> %1408, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1411 = bitcast <4 x i64> %1347 to <16 x i16>
  %1412 = bitcast <4 x i64> %1350 to <16 x i16>
  %1413 = shufflevector <16 x i16> %1411, <16 x i16> %1412, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1414 = shufflevector <16 x i16> %1411, <16 x i16> %1412, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1415 = bitcast <16 x i16> %1401 to <8 x i32>
  %1416 = bitcast <16 x i16> %1405 to <8 x i32>
  %1417 = shufflevector <8 x i32> %1415, <8 x i32> %1416, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1418 = shufflevector <8 x i32> %1415, <8 x i32> %1416, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1419 = bitcast <16 x i16> %1409 to <8 x i32>
  %1420 = bitcast <16 x i16> %1413 to <8 x i32>
  %1421 = shufflevector <8 x i32> %1419, <8 x i32> %1420, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1422 = shufflevector <8 x i32> %1419, <8 x i32> %1420, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1423 = bitcast <16 x i16> %1402 to <8 x i32>
  %1424 = bitcast <16 x i16> %1406 to <8 x i32>
  %1425 = shufflevector <8 x i32> %1423, <8 x i32> %1424, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1426 = shufflevector <8 x i32> %1423, <8 x i32> %1424, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1427 = bitcast <16 x i16> %1410 to <8 x i32>
  %1428 = bitcast <16 x i16> %1414 to <8 x i32>
  %1429 = shufflevector <8 x i32> %1427, <8 x i32> %1428, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1430 = shufflevector <8 x i32> %1427, <8 x i32> %1428, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1431 = bitcast <8 x i32> %1417 to <4 x i64>
  %1432 = bitcast <8 x i32> %1421 to <4 x i64>
  %1433 = shufflevector <4 x i64> %1431, <4 x i64> %1432, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1433, <4 x i64>* %1297, align 32
  %1434 = shufflevector <4 x i64> %1431, <4 x i64> %1432, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1434, <4 x i64>* %1301, align 32
  %1435 = bitcast <8 x i32> %1425 to <4 x i64>
  %1436 = bitcast <8 x i32> %1429 to <4 x i64>
  %1437 = shufflevector <4 x i64> %1435, <4 x i64> %1436, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1437, <4 x i64>* %1313, align 32
  %1438 = shufflevector <4 x i64> %1435, <4 x i64> %1436, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1438, <4 x i64>* %1317, align 32
  %1439 = bitcast <8 x i32> %1418 to <4 x i64>
  %1440 = bitcast <8 x i32> %1422 to <4 x i64>
  %1441 = shufflevector <4 x i64> %1439, <4 x i64> %1440, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1441, <4 x i64>* %1305, align 32
  %1442 = shufflevector <4 x i64> %1439, <4 x i64> %1440, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1442, <4 x i64>* %1309, align 32
  %1443 = bitcast <8 x i32> %1426 to <4 x i64>
  %1444 = bitcast <8 x i32> %1430 to <4 x i64>
  %1445 = shufflevector <4 x i64> %1443, <4 x i64> %1444, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1445, <4 x i64>* %1321, align 32
  %1446 = shufflevector <4 x i64> %1443, <4 x i64> %1444, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1446, <4 x i64>* %1325, align 32
  %1447 = shufflevector <4 x i64> %1385, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %1448 = bitcast <2 x i64> %1447 to <8 x i16>
  %1449 = sext <8 x i16> %1448 to <8 x i32>
  %1450 = bitcast i32* %1 to <8 x i32>*
  store <8 x i32> %1449, <8 x i32>* %1450, align 32
  %1451 = getelementptr inbounds i32, i32* %1, i64 8
  %1452 = load <4 x i64>, <4 x i64>* %906, align 32
  %1453 = shufflevector <4 x i64> %1452, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %1454 = bitcast <2 x i64> %1453 to <8 x i16>
  %1455 = sext <8 x i16> %1454 to <8 x i32>
  %1456 = bitcast i32* %1451 to <8 x i32>*
  store <8 x i32> %1455, <8 x i32>* %1456, align 32
  br label %1457

1457:                                             ; preds = %1477, %1271
  %1458 = phi i64 [ 1, %1271 ], [ %1492, %1477 ]
  %1459 = phi i32* [ %1, %1271 ], [ %1478, %1477 ]
  %1460 = getelementptr inbounds i32, i32* %1459, i64 16
  %1461 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 %1458
  %1462 = load <4 x i64>, <4 x i64>* %1461, align 32
  %1463 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 %1458
  %1464 = shufflevector <4 x i64> %1462, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %1465 = bitcast <2 x i64> %1464 to <8 x i16>
  %1466 = sext <8 x i16> %1465 to <8 x i32>
  %1467 = bitcast i32* %1460 to <8 x i32>*
  store <8 x i32> %1466, <8 x i32>* %1467, align 32
  %1468 = getelementptr inbounds i32, i32* %1459, i64 24
  %1469 = load <4 x i64>, <4 x i64>* %1463, align 32
  %1470 = shufflevector <4 x i64> %1469, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %1471 = bitcast <2 x i64> %1470 to <8 x i16>
  %1472 = sext <8 x i16> %1471 to <8 x i32>
  %1473 = bitcast i32* %1468 to <8 x i32>*
  store <8 x i32> %1472, <8 x i32>* %1473, align 32
  %1474 = add nuw nsw i64 %1458, 1
  %1475 = icmp eq i64 %1474, 16
  br i1 %1475, label %1476, label %1477

1476:                                             ; preds = %1457
  call void @llvm.lifetime.end.p0i8(i64 512, i8* nonnull %9) #8
  call void @llvm.lifetime.end.p0i8(i64 512, i8* nonnull %8) #8
  ret void

1477:                                             ; preds = %1457
  %1478 = getelementptr inbounds i32, i32* %1459, i64 32
  %1479 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 %1474
  %1480 = load <4 x i64>, <4 x i64>* %1479, align 32
  %1481 = getelementptr inbounds <4 x i64>, <4 x i64>* %906, i64 %1474
  %1482 = shufflevector <4 x i64> %1480, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %1483 = bitcast <2 x i64> %1482 to <8 x i16>
  %1484 = sext <8 x i16> %1483 to <8 x i32>
  %1485 = bitcast i32* %1478 to <8 x i32>*
  store <8 x i32> %1484, <8 x i32>* %1485, align 32
  %1486 = getelementptr inbounds i32, i32* %1459, i64 40
  %1487 = load <4 x i64>, <4 x i64>* %1481, align 32
  %1488 = shufflevector <4 x i64> %1487, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %1489 = bitcast <2 x i64> %1488 to <8 x i16>
  %1490 = sext <8 x i16> %1489 to <8 x i32>
  %1491 = bitcast i32* %1486 to <8 x i32>*
  store <8 x i32> %1490, <8 x i32>* %1491, align 32
  %1492 = add nuw nsw i64 %1458, 2
  br label %1457
}

; Function Attrs: nounwind ssp uwtable
define internal void @lowbd_fwd_txfm2d_32x32_avx2(i16* nocapture readonly, i32* nocapture, i32, i8 zeroext, i32) #3 {
  %6 = alloca [32 x <4 x i64>], align 32
  %7 = alloca [128 x <4 x i64>], align 32
  %8 = bitcast [32 x <4 x i64>]* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 1024, i8* nonnull %8) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %8, i8 -86, i64 1024, i1 false)
  %9 = bitcast [128 x <4 x i64>]* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 4096, i8* nonnull %9) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %9, i8 -86, i64 4096, i1 false)
  %10 = load i8*, i8** getelementptr inbounds ([19 x i8*], [19 x i8*]* @av1_fwd_txfm_shift_ls, i64 0, i64 3), align 8
  %11 = load i8, i8* getelementptr inbounds ([5 x [5 x i8]], [5 x [5 x i8]]* @av1_fwd_cos_bit_col, i64 0, i64 3, i64 3), align 1
  %12 = load i8, i8* getelementptr inbounds ([5 x [5 x i8]], [5 x [5 x i8]]* @av1_fwd_cos_bit_row, i64 0, i64 3, i64 3), align 1
  %13 = zext i8 %3 to i64
  %14 = getelementptr inbounds [16 x void (<4 x i64>*, <4 x i64>*, i8)*], [16 x void (<4 x i64>*, <4 x i64>*, i8)*]* @col_txfm16x32_arr, i64 0, i64 %13
  %15 = load void (<4 x i64>*, <4 x i64>*, i8)*, void (<4 x i64>*, <4 x i64>*, i8)** %14, align 8
  %16 = getelementptr inbounds [16 x void (<4 x i64>*, <4 x i64>*, i8)*], [16 x void (<4 x i64>*, <4 x i64>*, i8)*]* @row_txfm16x32_arr, i64 0, i64 %13
  %17 = load void (<4 x i64>*, <4 x i64>*, i8)*, void (<4 x i64>*, <4 x i64>*, i8)** %16, align 8
  switch i8 %3, label %21 [
    i8 6, label %20
    i8 15, label %19
    i8 7, label %19
    i8 5, label %19
    i8 14, label %18
    i8 8, label %18
    i8 4, label %18
  ]

18:                                               ; preds = %5, %5, %5
  br label %21

19:                                               ; preds = %5, %5, %5
  br label %21

20:                                               ; preds = %5
  br label %21

21:                                               ; preds = %5, %18, %19, %20
  %22 = phi i1 [ false, %20 ], [ true, %19 ], [ false, %18 ], [ true, %5 ]
  %23 = phi i32 [ 1, %20 ], [ 1, %19 ], [ 0, %18 ], [ 0, %5 ]
  %24 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 0
  %25 = sext i32 %2 to i64
  %26 = getelementptr inbounds i8, i8* %10, i64 1
  %27 = bitcast [32 x <4 x i64>]* %6 to <2 x i64>*
  %28 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 8
  %29 = bitcast <4 x i64>* %28 to <2 x i64>*
  %30 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 1
  %31 = bitcast <4 x i64>* %30 to <2 x i64>*
  %32 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 9
  %33 = bitcast <4 x i64>* %32 to <2 x i64>*
  %34 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 2
  %35 = bitcast <4 x i64>* %34 to <2 x i64>*
  %36 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 10
  %37 = bitcast <4 x i64>* %36 to <2 x i64>*
  %38 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 3
  %39 = bitcast <4 x i64>* %38 to <2 x i64>*
  %40 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 11
  %41 = bitcast <4 x i64>* %40 to <2 x i64>*
  %42 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 4
  %43 = bitcast <4 x i64>* %42 to <2 x i64>*
  %44 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 12
  %45 = bitcast <4 x i64>* %44 to <2 x i64>*
  %46 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 5
  %47 = bitcast <4 x i64>* %46 to <2 x i64>*
  %48 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 13
  %49 = bitcast <4 x i64>* %48 to <2 x i64>*
  %50 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 6
  %51 = bitcast <4 x i64>* %50 to <2 x i64>*
  %52 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 14
  %53 = bitcast <4 x i64>* %52 to <2 x i64>*
  %54 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 7
  %55 = bitcast <4 x i64>* %54 to <2 x i64>*
  %56 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 15
  %57 = bitcast <4 x i64>* %56 to <2 x i64>*
  %58 = getelementptr inbounds <2 x i64>, <2 x i64>* %27, i64 1
  %59 = getelementptr inbounds <2 x i64>, <2 x i64>* %29, i64 1
  %60 = getelementptr inbounds <2 x i64>, <2 x i64>* %31, i64 1
  %61 = getelementptr inbounds <2 x i64>, <2 x i64>* %33, i64 1
  %62 = getelementptr inbounds <2 x i64>, <2 x i64>* %35, i64 1
  %63 = getelementptr inbounds <2 x i64>, <2 x i64>* %37, i64 1
  %64 = getelementptr inbounds <2 x i64>, <2 x i64>* %39, i64 1
  %65 = getelementptr inbounds <2 x i64>, <2 x i64>* %41, i64 1
  %66 = getelementptr inbounds <2 x i64>, <2 x i64>* %43, i64 1
  %67 = getelementptr inbounds <2 x i64>, <2 x i64>* %45, i64 1
  %68 = getelementptr inbounds <2 x i64>, <2 x i64>* %47, i64 1
  %69 = getelementptr inbounds <2 x i64>, <2 x i64>* %49, i64 1
  %70 = getelementptr inbounds <2 x i64>, <2 x i64>* %51, i64 1
  %71 = getelementptr inbounds <2 x i64>, <2 x i64>* %53, i64 1
  %72 = getelementptr inbounds <2 x i64>, <2 x i64>* %55, i64 1
  %73 = getelementptr inbounds <2 x i64>, <2 x i64>* %57, i64 1
  %74 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 16
  %75 = getelementptr inbounds [128 x <4 x i64>], [128 x <4 x i64>]* %7, i64 0, i64 32
  %76 = bitcast <4 x i64>* %74 to <2 x i64>*
  %77 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 24
  %78 = bitcast <4 x i64>* %77 to <2 x i64>*
  %79 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 17
  %80 = bitcast <4 x i64>* %79 to <2 x i64>*
  %81 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 25
  %82 = bitcast <4 x i64>* %81 to <2 x i64>*
  %83 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 18
  %84 = bitcast <4 x i64>* %83 to <2 x i64>*
  %85 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 26
  %86 = bitcast <4 x i64>* %85 to <2 x i64>*
  %87 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 19
  %88 = bitcast <4 x i64>* %87 to <2 x i64>*
  %89 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 27
  %90 = bitcast <4 x i64>* %89 to <2 x i64>*
  %91 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 20
  %92 = bitcast <4 x i64>* %91 to <2 x i64>*
  %93 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 28
  %94 = bitcast <4 x i64>* %93 to <2 x i64>*
  %95 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 21
  %96 = bitcast <4 x i64>* %95 to <2 x i64>*
  %97 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 29
  %98 = bitcast <4 x i64>* %97 to <2 x i64>*
  %99 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 22
  %100 = bitcast <4 x i64>* %99 to <2 x i64>*
  %101 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 30
  %102 = bitcast <4 x i64>* %101 to <2 x i64>*
  %103 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 23
  %104 = bitcast <4 x i64>* %103 to <2 x i64>*
  %105 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 31
  %106 = bitcast <4 x i64>* %105 to <2 x i64>*
  %107 = getelementptr inbounds <2 x i64>, <2 x i64>* %76, i64 1
  %108 = getelementptr inbounds <2 x i64>, <2 x i64>* %78, i64 1
  %109 = getelementptr inbounds <2 x i64>, <2 x i64>* %80, i64 1
  %110 = getelementptr inbounds <2 x i64>, <2 x i64>* %82, i64 1
  %111 = getelementptr inbounds <2 x i64>, <2 x i64>* %84, i64 1
  %112 = getelementptr inbounds <2 x i64>, <2 x i64>* %86, i64 1
  %113 = getelementptr inbounds <2 x i64>, <2 x i64>* %88, i64 1
  %114 = getelementptr inbounds <2 x i64>, <2 x i64>* %90, i64 1
  %115 = getelementptr inbounds <2 x i64>, <2 x i64>* %92, i64 1
  %116 = getelementptr inbounds <2 x i64>, <2 x i64>* %94, i64 1
  %117 = getelementptr inbounds <2 x i64>, <2 x i64>* %96, i64 1
  %118 = getelementptr inbounds <2 x i64>, <2 x i64>* %98, i64 1
  %119 = getelementptr inbounds <2 x i64>, <2 x i64>* %100, i64 1
  %120 = getelementptr inbounds <2 x i64>, <2 x i64>* %102, i64 1
  %121 = getelementptr inbounds <2 x i64>, <2 x i64>* %104, i64 1
  %122 = getelementptr inbounds <2 x i64>, <2 x i64>* %106, i64 1
  br label %126

123:                                              ; preds = %304
  %124 = icmp eq i32 %23, 0
  %125 = getelementptr inbounds i8, i8* %10, i64 2
  br label %628

126:                                              ; preds = %304, %21
  %127 = phi i64 [ 0, %21 ], [ %625, %304 ]
  %128 = shl nsw i64 %127, 4
  %129 = getelementptr inbounds i16, i16* %0, i64 %128
  br i1 %22, label %151, label %130

130:                                              ; preds = %126, %130
  %131 = phi i64 [ %149, %130 ], [ 0, %126 ]
  %132 = mul nsw i64 %131, %25
  %133 = getelementptr inbounds i16, i16* %129, i64 %132
  %134 = bitcast i16* %133 to <4 x i64>*
  %135 = load <4 x i64>, <4 x i64>* %134, align 32
  %136 = shl i64 %131, 32
  %137 = sub nuw nsw i64 133143986176, %136
  %138 = ashr exact i64 %137, 32
  %139 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %138
  store <4 x i64> %135, <4 x i64>* %139, align 32
  %140 = or i64 %131, 1
  %141 = mul nsw i64 %140, %25
  %142 = getelementptr inbounds i16, i16* %129, i64 %141
  %143 = bitcast i16* %142 to <4 x i64>*
  %144 = load <4 x i64>, <4 x i64>* %143, align 32
  %145 = shl i64 %140, 32
  %146 = sub nuw nsw i64 133143986176, %145
  %147 = ashr exact i64 %146, 32
  %148 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %147
  store <4 x i64> %144, <4 x i64>* %148, align 32
  %149 = add nuw nsw i64 %131, 2
  %150 = icmp eq i64 %149, 32
  br i1 %150, label %178, label %130

151:                                              ; preds = %126, %151
  %152 = phi i64 [ %176, %151 ], [ 0, %126 ]
  %153 = mul nsw i64 %152, %25
  %154 = getelementptr inbounds i16, i16* %129, i64 %153
  %155 = bitcast i16* %154 to <4 x i64>*
  %156 = load <4 x i64>, <4 x i64>* %155, align 32
  %157 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %152
  store <4 x i64> %156, <4 x i64>* %157, align 32
  %158 = or i64 %152, 1
  %159 = mul nsw i64 %158, %25
  %160 = getelementptr inbounds i16, i16* %129, i64 %159
  %161 = bitcast i16* %160 to <4 x i64>*
  %162 = load <4 x i64>, <4 x i64>* %161, align 32
  %163 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %158
  store <4 x i64> %162, <4 x i64>* %163, align 32
  %164 = or i64 %152, 2
  %165 = mul nsw i64 %164, %25
  %166 = getelementptr inbounds i16, i16* %129, i64 %165
  %167 = bitcast i16* %166 to <4 x i64>*
  %168 = load <4 x i64>, <4 x i64>* %167, align 32
  %169 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %164
  store <4 x i64> %168, <4 x i64>* %169, align 32
  %170 = or i64 %152, 3
  %171 = mul nsw i64 %170, %25
  %172 = getelementptr inbounds i16, i16* %129, i64 %171
  %173 = bitcast i16* %172 to <4 x i64>*
  %174 = load <4 x i64>, <4 x i64>* %173, align 32
  %175 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %170
  store <4 x i64> %174, <4 x i64>* %175, align 32
  %176 = add nuw nsw i64 %152, 4
  %177 = icmp eq i64 %176, 32
  br i1 %177, label %178, label %151

178:                                              ; preds = %130, %151
  %179 = load i8, i8* %10, align 1
  %180 = sext i8 %179 to i32
  %181 = icmp slt i8 %179, 0
  br i1 %181, label %182, label %216

182:                                              ; preds = %178
  %183 = sub nsw i32 0, %180
  %184 = xor i32 %180, -1
  %185 = shl i32 1, %184
  %186 = trunc i32 %185 to i16
  %187 = insertelement <16 x i16> undef, i16 %186, i32 0
  %188 = shufflevector <16 x i16> %187, <16 x i16> undef, <16 x i32> zeroinitializer
  br label %189

189:                                              ; preds = %189, %182
  %190 = phi i64 [ 0, %182 ], [ %214, %189 ]
  %191 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %190
  %192 = bitcast <4 x i64>* %191 to <16 x i16>*
  %193 = load <16 x i16>, <16 x i16>* %192, align 32
  %194 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %193, <16 x i16> %188) #8
  %195 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %194, i32 %183) #8
  store <16 x i16> %195, <16 x i16>* %192, align 32
  %196 = or i64 %190, 1
  %197 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %196
  %198 = bitcast <4 x i64>* %197 to <16 x i16>*
  %199 = load <16 x i16>, <16 x i16>* %198, align 32
  %200 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %199, <16 x i16> %188) #8
  %201 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %200, i32 %183) #8
  store <16 x i16> %201, <16 x i16>* %198, align 32
  %202 = or i64 %190, 2
  %203 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %202
  %204 = bitcast <4 x i64>* %203 to <16 x i16>*
  %205 = load <16 x i16>, <16 x i16>* %204, align 32
  %206 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %205, <16 x i16> %188) #8
  %207 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %206, i32 %183) #8
  store <16 x i16> %207, <16 x i16>* %204, align 32
  %208 = or i64 %190, 3
  %209 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %208
  %210 = bitcast <4 x i64>* %209 to <16 x i16>*
  %211 = load <16 x i16>, <16 x i16>* %210, align 32
  %212 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %211, <16 x i16> %188) #8
  %213 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %212, i32 %183) #8
  store <16 x i16> %213, <16 x i16>* %210, align 32
  %214 = add nuw nsw i64 %190, 4
  %215 = icmp eq i64 %214, 32
  br i1 %215, label %241, label %189

216:                                              ; preds = %178
  %217 = icmp eq i8 %179, 0
  br i1 %217, label %241, label %218

218:                                              ; preds = %216, %218
  %219 = phi i64 [ %239, %218 ], [ 0, %216 ]
  %220 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %219
  %221 = bitcast <4 x i64>* %220 to <16 x i16>*
  %222 = load <16 x i16>, <16 x i16>* %221, align 32
  %223 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %222, i32 %180) #8
  store <16 x i16> %223, <16 x i16>* %221, align 32
  %224 = or i64 %219, 1
  %225 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %224
  %226 = bitcast <4 x i64>* %225 to <16 x i16>*
  %227 = load <16 x i16>, <16 x i16>* %226, align 32
  %228 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %227, i32 %180) #8
  store <16 x i16> %228, <16 x i16>* %226, align 32
  %229 = or i64 %219, 2
  %230 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %229
  %231 = bitcast <4 x i64>* %230 to <16 x i16>*
  %232 = load <16 x i16>, <16 x i16>* %231, align 32
  %233 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %232, i32 %180) #8
  store <16 x i16> %233, <16 x i16>* %231, align 32
  %234 = or i64 %219, 3
  %235 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %234
  %236 = bitcast <4 x i64>* %235 to <16 x i16>*
  %237 = load <16 x i16>, <16 x i16>* %236, align 32
  %238 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %237, i32 %180) #8
  store <16 x i16> %238, <16 x i16>* %236, align 32
  %239 = add nuw nsw i64 %219, 4
  %240 = icmp eq i64 %239, 32
  br i1 %240, label %241, label %218

241:                                              ; preds = %218, %189, %216
  call void %15(<4 x i64>* nonnull %24, <4 x i64>* nonnull %24, i8 signext %11) #8
  %242 = load i8, i8* %26, align 1
  %243 = sext i8 %242 to i32
  %244 = icmp slt i8 %242, 0
  br i1 %244, label %245, label %279

245:                                              ; preds = %241
  %246 = sub nsw i32 0, %243
  %247 = xor i32 %243, -1
  %248 = shl i32 1, %247
  %249 = trunc i32 %248 to i16
  %250 = insertelement <16 x i16> undef, i16 %249, i32 0
  %251 = shufflevector <16 x i16> %250, <16 x i16> undef, <16 x i32> zeroinitializer
  br label %252

252:                                              ; preds = %252, %245
  %253 = phi i64 [ 0, %245 ], [ %277, %252 ]
  %254 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %253
  %255 = bitcast <4 x i64>* %254 to <16 x i16>*
  %256 = load <16 x i16>, <16 x i16>* %255, align 32
  %257 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %256, <16 x i16> %251) #8
  %258 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %257, i32 %246) #8
  store <16 x i16> %258, <16 x i16>* %255, align 32
  %259 = or i64 %253, 1
  %260 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %259
  %261 = bitcast <4 x i64>* %260 to <16 x i16>*
  %262 = load <16 x i16>, <16 x i16>* %261, align 32
  %263 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %262, <16 x i16> %251) #8
  %264 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %263, i32 %246) #8
  store <16 x i16> %264, <16 x i16>* %261, align 32
  %265 = or i64 %253, 2
  %266 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %265
  %267 = bitcast <4 x i64>* %266 to <16 x i16>*
  %268 = load <16 x i16>, <16 x i16>* %267, align 32
  %269 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %268, <16 x i16> %251) #8
  %270 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %269, i32 %246) #8
  store <16 x i16> %270, <16 x i16>* %267, align 32
  %271 = or i64 %253, 3
  %272 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %271
  %273 = bitcast <4 x i64>* %272 to <16 x i16>*
  %274 = load <16 x i16>, <16 x i16>* %273, align 32
  %275 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %274, <16 x i16> %251) #8
  %276 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %275, i32 %246) #8
  store <16 x i16> %276, <16 x i16>* %273, align 32
  %277 = add nuw nsw i64 %253, 4
  %278 = icmp eq i64 %277, 32
  br i1 %278, label %304, label %252

279:                                              ; preds = %241
  %280 = icmp eq i8 %242, 0
  br i1 %280, label %304, label %281

281:                                              ; preds = %279, %281
  %282 = phi i64 [ %302, %281 ], [ 0, %279 ]
  %283 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %282
  %284 = bitcast <4 x i64>* %283 to <16 x i16>*
  %285 = load <16 x i16>, <16 x i16>* %284, align 32
  %286 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %285, i32 %243) #8
  store <16 x i16> %286, <16 x i16>* %284, align 32
  %287 = or i64 %282, 1
  %288 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %287
  %289 = bitcast <4 x i64>* %288 to <16 x i16>*
  %290 = load <16 x i16>, <16 x i16>* %289, align 32
  %291 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %290, i32 %243) #8
  store <16 x i16> %291, <16 x i16>* %289, align 32
  %292 = or i64 %282, 2
  %293 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %292
  %294 = bitcast <4 x i64>* %293 to <16 x i16>*
  %295 = load <16 x i16>, <16 x i16>* %294, align 32
  %296 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %295, i32 %243) #8
  store <16 x i16> %296, <16 x i16>* %294, align 32
  %297 = or i64 %282, 3
  %298 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %297
  %299 = bitcast <4 x i64>* %298 to <16 x i16>*
  %300 = load <16 x i16>, <16 x i16>* %299, align 32
  %301 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %300, i32 %243) #8
  store <16 x i16> %301, <16 x i16>* %299, align 32
  %302 = add nuw nsw i64 %282, 4
  %303 = icmp eq i64 %302, 32
  br i1 %303, label %304, label %281

304:                                              ; preds = %281, %252, %279
  %305 = getelementptr inbounds [128 x <4 x i64>], [128 x <4 x i64>]* %7, i64 0, i64 %128
  %306 = load <2 x i64>, <2 x i64>* %27, align 32
  %307 = load <2 x i64>, <2 x i64>* %29, align 32
  %308 = shufflevector <2 x i64> %306, <2 x i64> %307, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %309 = load <2 x i64>, <2 x i64>* %31, align 32
  %310 = load <2 x i64>, <2 x i64>* %33, align 32
  %311 = shufflevector <2 x i64> %309, <2 x i64> %310, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %312 = load <2 x i64>, <2 x i64>* %35, align 32
  %313 = load <2 x i64>, <2 x i64>* %37, align 32
  %314 = shufflevector <2 x i64> %312, <2 x i64> %313, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %315 = load <2 x i64>, <2 x i64>* %39, align 32
  %316 = load <2 x i64>, <2 x i64>* %41, align 32
  %317 = shufflevector <2 x i64> %315, <2 x i64> %316, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %318 = load <2 x i64>, <2 x i64>* %43, align 32
  %319 = load <2 x i64>, <2 x i64>* %45, align 32
  %320 = shufflevector <2 x i64> %318, <2 x i64> %319, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %321 = load <2 x i64>, <2 x i64>* %47, align 32
  %322 = load <2 x i64>, <2 x i64>* %49, align 32
  %323 = shufflevector <2 x i64> %321, <2 x i64> %322, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %324 = load <2 x i64>, <2 x i64>* %51, align 32
  %325 = load <2 x i64>, <2 x i64>* %53, align 32
  %326 = shufflevector <2 x i64> %324, <2 x i64> %325, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %327 = load <2 x i64>, <2 x i64>* %55, align 32
  %328 = load <2 x i64>, <2 x i64>* %57, align 32
  %329 = shufflevector <2 x i64> %327, <2 x i64> %328, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %330 = load <2 x i64>, <2 x i64>* %58, align 16
  %331 = load <2 x i64>, <2 x i64>* %59, align 16
  %332 = shufflevector <2 x i64> %330, <2 x i64> %331, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %333 = load <2 x i64>, <2 x i64>* %60, align 16
  %334 = load <2 x i64>, <2 x i64>* %61, align 16
  %335 = shufflevector <2 x i64> %333, <2 x i64> %334, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %336 = load <2 x i64>, <2 x i64>* %62, align 16
  %337 = load <2 x i64>, <2 x i64>* %63, align 16
  %338 = shufflevector <2 x i64> %336, <2 x i64> %337, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %339 = load <2 x i64>, <2 x i64>* %64, align 16
  %340 = load <2 x i64>, <2 x i64>* %65, align 16
  %341 = shufflevector <2 x i64> %339, <2 x i64> %340, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %342 = load <2 x i64>, <2 x i64>* %66, align 16
  %343 = load <2 x i64>, <2 x i64>* %67, align 16
  %344 = shufflevector <2 x i64> %342, <2 x i64> %343, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %345 = load <2 x i64>, <2 x i64>* %68, align 16
  %346 = load <2 x i64>, <2 x i64>* %69, align 16
  %347 = shufflevector <2 x i64> %345, <2 x i64> %346, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %348 = load <2 x i64>, <2 x i64>* %70, align 16
  %349 = load <2 x i64>, <2 x i64>* %71, align 16
  %350 = shufflevector <2 x i64> %348, <2 x i64> %349, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %351 = load <2 x i64>, <2 x i64>* %72, align 16
  %352 = load <2 x i64>, <2 x i64>* %73, align 16
  %353 = shufflevector <2 x i64> %351, <2 x i64> %352, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %354 = bitcast <4 x i64> %308 to <16 x i16>
  %355 = bitcast <4 x i64> %311 to <16 x i16>
  %356 = shufflevector <16 x i16> %354, <16 x i16> %355, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %357 = shufflevector <16 x i16> %354, <16 x i16> %355, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %358 = bitcast <4 x i64> %314 to <16 x i16>
  %359 = bitcast <4 x i64> %317 to <16 x i16>
  %360 = shufflevector <16 x i16> %358, <16 x i16> %359, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %361 = shufflevector <16 x i16> %358, <16 x i16> %359, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %362 = bitcast <4 x i64> %320 to <16 x i16>
  %363 = bitcast <4 x i64> %323 to <16 x i16>
  %364 = shufflevector <16 x i16> %362, <16 x i16> %363, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %365 = shufflevector <16 x i16> %362, <16 x i16> %363, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %366 = bitcast <4 x i64> %326 to <16 x i16>
  %367 = bitcast <4 x i64> %329 to <16 x i16>
  %368 = shufflevector <16 x i16> %366, <16 x i16> %367, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %369 = shufflevector <16 x i16> %366, <16 x i16> %367, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %370 = bitcast <16 x i16> %356 to <8 x i32>
  %371 = bitcast <16 x i16> %360 to <8 x i32>
  %372 = shufflevector <8 x i32> %370, <8 x i32> %371, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %373 = shufflevector <8 x i32> %370, <8 x i32> %371, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %374 = bitcast <16 x i16> %364 to <8 x i32>
  %375 = bitcast <16 x i16> %368 to <8 x i32>
  %376 = shufflevector <8 x i32> %374, <8 x i32> %375, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %377 = shufflevector <8 x i32> %374, <8 x i32> %375, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %378 = bitcast <16 x i16> %357 to <8 x i32>
  %379 = bitcast <16 x i16> %361 to <8 x i32>
  %380 = shufflevector <8 x i32> %378, <8 x i32> %379, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %381 = shufflevector <8 x i32> %378, <8 x i32> %379, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %382 = bitcast <16 x i16> %365 to <8 x i32>
  %383 = bitcast <16 x i16> %369 to <8 x i32>
  %384 = shufflevector <8 x i32> %382, <8 x i32> %383, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %385 = shufflevector <8 x i32> %382, <8 x i32> %383, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %386 = bitcast <8 x i32> %372 to <4 x i64>
  %387 = bitcast <8 x i32> %376 to <4 x i64>
  %388 = shufflevector <4 x i64> %386, <4 x i64> %387, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %388, <4 x i64>* %305, align 32
  %389 = shufflevector <4 x i64> %386, <4 x i64> %387, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %390 = getelementptr inbounds <4 x i64>, <4 x i64>* %305, i64 1
  store <4 x i64> %389, <4 x i64>* %390, align 32
  %391 = bitcast <8 x i32> %380 to <4 x i64>
  %392 = bitcast <8 x i32> %384 to <4 x i64>
  %393 = shufflevector <4 x i64> %391, <4 x i64> %392, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %394 = getelementptr inbounds <4 x i64>, <4 x i64>* %305, i64 4
  store <4 x i64> %393, <4 x i64>* %394, align 32
  %395 = shufflevector <4 x i64> %391, <4 x i64> %392, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %396 = getelementptr inbounds <4 x i64>, <4 x i64>* %305, i64 5
  store <4 x i64> %395, <4 x i64>* %396, align 32
  %397 = bitcast <8 x i32> %373 to <4 x i64>
  %398 = bitcast <8 x i32> %377 to <4 x i64>
  %399 = shufflevector <4 x i64> %397, <4 x i64> %398, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %400 = getelementptr inbounds <4 x i64>, <4 x i64>* %305, i64 2
  store <4 x i64> %399, <4 x i64>* %400, align 32
  %401 = shufflevector <4 x i64> %397, <4 x i64> %398, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %402 = getelementptr inbounds <4 x i64>, <4 x i64>* %305, i64 3
  store <4 x i64> %401, <4 x i64>* %402, align 32
  %403 = bitcast <8 x i32> %381 to <4 x i64>
  %404 = bitcast <8 x i32> %385 to <4 x i64>
  %405 = shufflevector <4 x i64> %403, <4 x i64> %404, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %406 = getelementptr inbounds <4 x i64>, <4 x i64>* %305, i64 6
  store <4 x i64> %405, <4 x i64>* %406, align 32
  %407 = shufflevector <4 x i64> %403, <4 x i64> %404, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %408 = getelementptr inbounds <4 x i64>, <4 x i64>* %305, i64 7
  store <4 x i64> %407, <4 x i64>* %408, align 32
  %409 = getelementptr inbounds <4 x i64>, <4 x i64>* %305, i64 8
  %410 = bitcast <4 x i64> %332 to <16 x i16>
  %411 = bitcast <4 x i64> %335 to <16 x i16>
  %412 = shufflevector <16 x i16> %410, <16 x i16> %411, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %413 = shufflevector <16 x i16> %410, <16 x i16> %411, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %414 = bitcast <4 x i64> %338 to <16 x i16>
  %415 = bitcast <4 x i64> %341 to <16 x i16>
  %416 = shufflevector <16 x i16> %414, <16 x i16> %415, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %417 = shufflevector <16 x i16> %414, <16 x i16> %415, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %418 = bitcast <4 x i64> %344 to <16 x i16>
  %419 = bitcast <4 x i64> %347 to <16 x i16>
  %420 = shufflevector <16 x i16> %418, <16 x i16> %419, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %421 = shufflevector <16 x i16> %418, <16 x i16> %419, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %422 = bitcast <4 x i64> %350 to <16 x i16>
  %423 = bitcast <4 x i64> %353 to <16 x i16>
  %424 = shufflevector <16 x i16> %422, <16 x i16> %423, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %425 = shufflevector <16 x i16> %422, <16 x i16> %423, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %426 = bitcast <16 x i16> %412 to <8 x i32>
  %427 = bitcast <16 x i16> %416 to <8 x i32>
  %428 = shufflevector <8 x i32> %426, <8 x i32> %427, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %429 = shufflevector <8 x i32> %426, <8 x i32> %427, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %430 = bitcast <16 x i16> %420 to <8 x i32>
  %431 = bitcast <16 x i16> %424 to <8 x i32>
  %432 = shufflevector <8 x i32> %430, <8 x i32> %431, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %433 = shufflevector <8 x i32> %430, <8 x i32> %431, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %434 = bitcast <16 x i16> %413 to <8 x i32>
  %435 = bitcast <16 x i16> %417 to <8 x i32>
  %436 = shufflevector <8 x i32> %434, <8 x i32> %435, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %437 = shufflevector <8 x i32> %434, <8 x i32> %435, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %438 = bitcast <16 x i16> %421 to <8 x i32>
  %439 = bitcast <16 x i16> %425 to <8 x i32>
  %440 = shufflevector <8 x i32> %438, <8 x i32> %439, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %441 = shufflevector <8 x i32> %438, <8 x i32> %439, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %442 = bitcast <8 x i32> %428 to <4 x i64>
  %443 = bitcast <8 x i32> %432 to <4 x i64>
  %444 = shufflevector <4 x i64> %442, <4 x i64> %443, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %444, <4 x i64>* %409, align 32
  %445 = shufflevector <4 x i64> %442, <4 x i64> %443, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %446 = getelementptr inbounds <4 x i64>, <4 x i64>* %305, i64 9
  store <4 x i64> %445, <4 x i64>* %446, align 32
  %447 = bitcast <8 x i32> %436 to <4 x i64>
  %448 = bitcast <8 x i32> %440 to <4 x i64>
  %449 = shufflevector <4 x i64> %447, <4 x i64> %448, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %450 = getelementptr inbounds <4 x i64>, <4 x i64>* %305, i64 12
  store <4 x i64> %449, <4 x i64>* %450, align 32
  %451 = shufflevector <4 x i64> %447, <4 x i64> %448, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %452 = getelementptr inbounds <4 x i64>, <4 x i64>* %305, i64 13
  store <4 x i64> %451, <4 x i64>* %452, align 32
  %453 = bitcast <8 x i32> %429 to <4 x i64>
  %454 = bitcast <8 x i32> %433 to <4 x i64>
  %455 = shufflevector <4 x i64> %453, <4 x i64> %454, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %456 = getelementptr inbounds <4 x i64>, <4 x i64>* %305, i64 10
  store <4 x i64> %455, <4 x i64>* %456, align 32
  %457 = shufflevector <4 x i64> %453, <4 x i64> %454, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %458 = getelementptr inbounds <4 x i64>, <4 x i64>* %305, i64 11
  store <4 x i64> %457, <4 x i64>* %458, align 32
  %459 = bitcast <8 x i32> %437 to <4 x i64>
  %460 = bitcast <8 x i32> %441 to <4 x i64>
  %461 = shufflevector <4 x i64> %459, <4 x i64> %460, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %462 = getelementptr inbounds <4 x i64>, <4 x i64>* %305, i64 14
  store <4 x i64> %461, <4 x i64>* %462, align 32
  %463 = shufflevector <4 x i64> %459, <4 x i64> %460, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %464 = getelementptr inbounds <4 x i64>, <4 x i64>* %305, i64 15
  store <4 x i64> %463, <4 x i64>* %464, align 32
  %465 = getelementptr inbounds <4 x i64>, <4 x i64>* %75, i64 %128
  %466 = load <2 x i64>, <2 x i64>* %76, align 32
  %467 = load <2 x i64>, <2 x i64>* %78, align 32
  %468 = shufflevector <2 x i64> %466, <2 x i64> %467, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %469 = load <2 x i64>, <2 x i64>* %80, align 32
  %470 = load <2 x i64>, <2 x i64>* %82, align 32
  %471 = shufflevector <2 x i64> %469, <2 x i64> %470, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %472 = load <2 x i64>, <2 x i64>* %84, align 32
  %473 = load <2 x i64>, <2 x i64>* %86, align 32
  %474 = shufflevector <2 x i64> %472, <2 x i64> %473, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %475 = load <2 x i64>, <2 x i64>* %88, align 32
  %476 = load <2 x i64>, <2 x i64>* %90, align 32
  %477 = shufflevector <2 x i64> %475, <2 x i64> %476, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %478 = load <2 x i64>, <2 x i64>* %92, align 32
  %479 = load <2 x i64>, <2 x i64>* %94, align 32
  %480 = shufflevector <2 x i64> %478, <2 x i64> %479, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %481 = load <2 x i64>, <2 x i64>* %96, align 32
  %482 = load <2 x i64>, <2 x i64>* %98, align 32
  %483 = shufflevector <2 x i64> %481, <2 x i64> %482, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %484 = load <2 x i64>, <2 x i64>* %100, align 32
  %485 = load <2 x i64>, <2 x i64>* %102, align 32
  %486 = shufflevector <2 x i64> %484, <2 x i64> %485, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %487 = load <2 x i64>, <2 x i64>* %104, align 32
  %488 = load <2 x i64>, <2 x i64>* %106, align 32
  %489 = shufflevector <2 x i64> %487, <2 x i64> %488, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %490 = load <2 x i64>, <2 x i64>* %107, align 16
  %491 = load <2 x i64>, <2 x i64>* %108, align 16
  %492 = shufflevector <2 x i64> %490, <2 x i64> %491, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %493 = load <2 x i64>, <2 x i64>* %109, align 16
  %494 = load <2 x i64>, <2 x i64>* %110, align 16
  %495 = shufflevector <2 x i64> %493, <2 x i64> %494, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %496 = load <2 x i64>, <2 x i64>* %111, align 16
  %497 = load <2 x i64>, <2 x i64>* %112, align 16
  %498 = shufflevector <2 x i64> %496, <2 x i64> %497, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %499 = load <2 x i64>, <2 x i64>* %113, align 16
  %500 = load <2 x i64>, <2 x i64>* %114, align 16
  %501 = shufflevector <2 x i64> %499, <2 x i64> %500, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %502 = load <2 x i64>, <2 x i64>* %115, align 16
  %503 = load <2 x i64>, <2 x i64>* %116, align 16
  %504 = shufflevector <2 x i64> %502, <2 x i64> %503, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %505 = load <2 x i64>, <2 x i64>* %117, align 16
  %506 = load <2 x i64>, <2 x i64>* %118, align 16
  %507 = shufflevector <2 x i64> %505, <2 x i64> %506, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %508 = load <2 x i64>, <2 x i64>* %119, align 16
  %509 = load <2 x i64>, <2 x i64>* %120, align 16
  %510 = shufflevector <2 x i64> %508, <2 x i64> %509, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %511 = load <2 x i64>, <2 x i64>* %121, align 16
  %512 = load <2 x i64>, <2 x i64>* %122, align 16
  %513 = shufflevector <2 x i64> %511, <2 x i64> %512, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %514 = bitcast <4 x i64> %468 to <16 x i16>
  %515 = bitcast <4 x i64> %471 to <16 x i16>
  %516 = shufflevector <16 x i16> %514, <16 x i16> %515, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %517 = shufflevector <16 x i16> %514, <16 x i16> %515, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %518 = bitcast <4 x i64> %474 to <16 x i16>
  %519 = bitcast <4 x i64> %477 to <16 x i16>
  %520 = shufflevector <16 x i16> %518, <16 x i16> %519, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %521 = shufflevector <16 x i16> %518, <16 x i16> %519, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %522 = bitcast <4 x i64> %480 to <16 x i16>
  %523 = bitcast <4 x i64> %483 to <16 x i16>
  %524 = shufflevector <16 x i16> %522, <16 x i16> %523, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %525 = shufflevector <16 x i16> %522, <16 x i16> %523, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %526 = bitcast <4 x i64> %486 to <16 x i16>
  %527 = bitcast <4 x i64> %489 to <16 x i16>
  %528 = shufflevector <16 x i16> %526, <16 x i16> %527, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %529 = shufflevector <16 x i16> %526, <16 x i16> %527, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %530 = bitcast <16 x i16> %516 to <8 x i32>
  %531 = bitcast <16 x i16> %520 to <8 x i32>
  %532 = shufflevector <8 x i32> %530, <8 x i32> %531, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %533 = shufflevector <8 x i32> %530, <8 x i32> %531, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %534 = bitcast <16 x i16> %524 to <8 x i32>
  %535 = bitcast <16 x i16> %528 to <8 x i32>
  %536 = shufflevector <8 x i32> %534, <8 x i32> %535, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %537 = shufflevector <8 x i32> %534, <8 x i32> %535, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %538 = bitcast <16 x i16> %517 to <8 x i32>
  %539 = bitcast <16 x i16> %521 to <8 x i32>
  %540 = shufflevector <8 x i32> %538, <8 x i32> %539, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %541 = shufflevector <8 x i32> %538, <8 x i32> %539, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %542 = bitcast <16 x i16> %525 to <8 x i32>
  %543 = bitcast <16 x i16> %529 to <8 x i32>
  %544 = shufflevector <8 x i32> %542, <8 x i32> %543, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %545 = shufflevector <8 x i32> %542, <8 x i32> %543, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %546 = bitcast <8 x i32> %532 to <4 x i64>
  %547 = bitcast <8 x i32> %536 to <4 x i64>
  %548 = shufflevector <4 x i64> %546, <4 x i64> %547, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %548, <4 x i64>* %465, align 32
  %549 = shufflevector <4 x i64> %546, <4 x i64> %547, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %550 = getelementptr inbounds <4 x i64>, <4 x i64>* %465, i64 1
  store <4 x i64> %549, <4 x i64>* %550, align 32
  %551 = bitcast <8 x i32> %540 to <4 x i64>
  %552 = bitcast <8 x i32> %544 to <4 x i64>
  %553 = shufflevector <4 x i64> %551, <4 x i64> %552, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %554 = getelementptr inbounds <4 x i64>, <4 x i64>* %465, i64 4
  store <4 x i64> %553, <4 x i64>* %554, align 32
  %555 = shufflevector <4 x i64> %551, <4 x i64> %552, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %556 = getelementptr inbounds <4 x i64>, <4 x i64>* %465, i64 5
  store <4 x i64> %555, <4 x i64>* %556, align 32
  %557 = bitcast <8 x i32> %533 to <4 x i64>
  %558 = bitcast <8 x i32> %537 to <4 x i64>
  %559 = shufflevector <4 x i64> %557, <4 x i64> %558, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %560 = getelementptr inbounds <4 x i64>, <4 x i64>* %465, i64 2
  store <4 x i64> %559, <4 x i64>* %560, align 32
  %561 = shufflevector <4 x i64> %557, <4 x i64> %558, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %562 = getelementptr inbounds <4 x i64>, <4 x i64>* %465, i64 3
  store <4 x i64> %561, <4 x i64>* %562, align 32
  %563 = bitcast <8 x i32> %541 to <4 x i64>
  %564 = bitcast <8 x i32> %545 to <4 x i64>
  %565 = shufflevector <4 x i64> %563, <4 x i64> %564, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %566 = getelementptr inbounds <4 x i64>, <4 x i64>* %465, i64 6
  store <4 x i64> %565, <4 x i64>* %566, align 32
  %567 = shufflevector <4 x i64> %563, <4 x i64> %564, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %568 = getelementptr inbounds <4 x i64>, <4 x i64>* %465, i64 7
  store <4 x i64> %567, <4 x i64>* %568, align 32
  %569 = getelementptr inbounds <4 x i64>, <4 x i64>* %465, i64 8
  %570 = bitcast <4 x i64> %492 to <16 x i16>
  %571 = bitcast <4 x i64> %495 to <16 x i16>
  %572 = shufflevector <16 x i16> %570, <16 x i16> %571, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %573 = shufflevector <16 x i16> %570, <16 x i16> %571, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %574 = bitcast <4 x i64> %498 to <16 x i16>
  %575 = bitcast <4 x i64> %501 to <16 x i16>
  %576 = shufflevector <16 x i16> %574, <16 x i16> %575, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %577 = shufflevector <16 x i16> %574, <16 x i16> %575, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %578 = bitcast <4 x i64> %504 to <16 x i16>
  %579 = bitcast <4 x i64> %507 to <16 x i16>
  %580 = shufflevector <16 x i16> %578, <16 x i16> %579, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %581 = shufflevector <16 x i16> %578, <16 x i16> %579, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %582 = bitcast <4 x i64> %510 to <16 x i16>
  %583 = bitcast <4 x i64> %513 to <16 x i16>
  %584 = shufflevector <16 x i16> %582, <16 x i16> %583, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %585 = shufflevector <16 x i16> %582, <16 x i16> %583, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %586 = bitcast <16 x i16> %572 to <8 x i32>
  %587 = bitcast <16 x i16> %576 to <8 x i32>
  %588 = shufflevector <8 x i32> %586, <8 x i32> %587, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %589 = shufflevector <8 x i32> %586, <8 x i32> %587, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %590 = bitcast <16 x i16> %580 to <8 x i32>
  %591 = bitcast <16 x i16> %584 to <8 x i32>
  %592 = shufflevector <8 x i32> %590, <8 x i32> %591, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %593 = shufflevector <8 x i32> %590, <8 x i32> %591, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %594 = bitcast <16 x i16> %573 to <8 x i32>
  %595 = bitcast <16 x i16> %577 to <8 x i32>
  %596 = shufflevector <8 x i32> %594, <8 x i32> %595, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %597 = shufflevector <8 x i32> %594, <8 x i32> %595, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %598 = bitcast <16 x i16> %581 to <8 x i32>
  %599 = bitcast <16 x i16> %585 to <8 x i32>
  %600 = shufflevector <8 x i32> %598, <8 x i32> %599, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %601 = shufflevector <8 x i32> %598, <8 x i32> %599, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %602 = bitcast <8 x i32> %588 to <4 x i64>
  %603 = bitcast <8 x i32> %592 to <4 x i64>
  %604 = shufflevector <4 x i64> %602, <4 x i64> %603, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %604, <4 x i64>* %569, align 32
  %605 = shufflevector <4 x i64> %602, <4 x i64> %603, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %606 = getelementptr inbounds <4 x i64>, <4 x i64>* %465, i64 9
  store <4 x i64> %605, <4 x i64>* %606, align 32
  %607 = bitcast <8 x i32> %596 to <4 x i64>
  %608 = bitcast <8 x i32> %600 to <4 x i64>
  %609 = shufflevector <4 x i64> %607, <4 x i64> %608, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %610 = getelementptr inbounds <4 x i64>, <4 x i64>* %465, i64 12
  store <4 x i64> %609, <4 x i64>* %610, align 32
  %611 = shufflevector <4 x i64> %607, <4 x i64> %608, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %612 = getelementptr inbounds <4 x i64>, <4 x i64>* %465, i64 13
  store <4 x i64> %611, <4 x i64>* %612, align 32
  %613 = bitcast <8 x i32> %589 to <4 x i64>
  %614 = bitcast <8 x i32> %593 to <4 x i64>
  %615 = shufflevector <4 x i64> %613, <4 x i64> %614, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %616 = getelementptr inbounds <4 x i64>, <4 x i64>* %465, i64 10
  store <4 x i64> %615, <4 x i64>* %616, align 32
  %617 = shufflevector <4 x i64> %613, <4 x i64> %614, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %618 = getelementptr inbounds <4 x i64>, <4 x i64>* %465, i64 11
  store <4 x i64> %617, <4 x i64>* %618, align 32
  %619 = bitcast <8 x i32> %597 to <4 x i64>
  %620 = bitcast <8 x i32> %601 to <4 x i64>
  %621 = shufflevector <4 x i64> %619, <4 x i64> %620, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %622 = getelementptr inbounds <4 x i64>, <4 x i64>* %465, i64 14
  store <4 x i64> %621, <4 x i64>* %622, align 32
  %623 = shufflevector <4 x i64> %619, <4 x i64> %620, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %624 = getelementptr inbounds <4 x i64>, <4 x i64>* %465, i64 15
  store <4 x i64> %623, <4 x i64>* %624, align 32
  %625 = add nuw nsw i64 %127, 1
  %626 = icmp eq i64 %625, 2
  br i1 %626, label %123, label %126

627:                                              ; preds = %1159
  call void @llvm.lifetime.end.p0i8(i64 4096, i8* nonnull %9) #8
  call void @llvm.lifetime.end.p0i8(i64 1024, i8* nonnull %8) #8
  ret void

628:                                              ; preds = %1159, %123
  %629 = phi i64 [ 0, %123 ], [ %1160, %1159 ]
  %630 = shl nsw i64 %629, 5
  %631 = getelementptr inbounds [128 x <4 x i64>], [128 x <4 x i64>]* %7, i64 0, i64 %630
  br i1 %124, label %649, label %632

632:                                              ; preds = %628, %632
  %633 = phi i64 [ %647, %632 ], [ 0, %628 ]
  %634 = getelementptr inbounds <4 x i64>, <4 x i64>* %631, i64 %633
  %635 = load <4 x i64>, <4 x i64>* %634, align 32
  %636 = shl i64 %633, 32
  %637 = sub nuw nsw i64 133143986176, %636
  %638 = ashr exact i64 %637, 32
  %639 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %638
  store <4 x i64> %635, <4 x i64>* %639, align 32
  %640 = or i64 %633, 1
  %641 = getelementptr inbounds <4 x i64>, <4 x i64>* %631, i64 %640
  %642 = load <4 x i64>, <4 x i64>* %641, align 32
  %643 = shl i64 %640, 32
  %644 = sub nuw nsw i64 133143986176, %643
  %645 = ashr exact i64 %644, 32
  %646 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %645
  store <4 x i64> %642, <4 x i64>* %646, align 32
  %647 = add nuw nsw i64 %633, 2
  %648 = icmp eq i64 %647, 32
  br i1 %648, label %649, label %632

649:                                              ; preds = %632, %628
  %650 = phi <4 x i64>* [ %631, %628 ], [ %24, %632 ]
  call void %17(<4 x i64>* %650, <4 x i64>* %650, i8 signext %12) #8
  %651 = load i8, i8* %125, align 1
  %652 = sext i8 %651 to i32
  %653 = icmp slt i8 %651, 0
  br i1 %653, label %654, label %688

654:                                              ; preds = %649
  %655 = sub nsw i32 0, %652
  %656 = xor i32 %652, -1
  %657 = shl i32 1, %656
  %658 = trunc i32 %657 to i16
  %659 = insertelement <16 x i16> undef, i16 %658, i32 0
  %660 = shufflevector <16 x i16> %659, <16 x i16> undef, <16 x i32> zeroinitializer
  br label %661

661:                                              ; preds = %661, %654
  %662 = phi i64 [ 0, %654 ], [ %686, %661 ]
  %663 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 %662
  %664 = bitcast <4 x i64>* %663 to <16 x i16>*
  %665 = load <16 x i16>, <16 x i16>* %664, align 32
  %666 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %665, <16 x i16> %660) #8
  %667 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %666, i32 %655) #8
  store <16 x i16> %667, <16 x i16>* %664, align 32
  %668 = or i64 %662, 1
  %669 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 %668
  %670 = bitcast <4 x i64>* %669 to <16 x i16>*
  %671 = load <16 x i16>, <16 x i16>* %670, align 32
  %672 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %671, <16 x i16> %660) #8
  %673 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %672, i32 %655) #8
  store <16 x i16> %673, <16 x i16>* %670, align 32
  %674 = or i64 %662, 2
  %675 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 %674
  %676 = bitcast <4 x i64>* %675 to <16 x i16>*
  %677 = load <16 x i16>, <16 x i16>* %676, align 32
  %678 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %677, <16 x i16> %660) #8
  %679 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %678, i32 %655) #8
  store <16 x i16> %679, <16 x i16>* %676, align 32
  %680 = or i64 %662, 3
  %681 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 %680
  %682 = bitcast <4 x i64>* %681 to <16 x i16>*
  %683 = load <16 x i16>, <16 x i16>* %682, align 32
  %684 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %683, <16 x i16> %660) #8
  %685 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %684, i32 %655) #8
  store <16 x i16> %685, <16 x i16>* %682, align 32
  %686 = add nuw nsw i64 %662, 4
  %687 = icmp eq i64 %686, 32
  br i1 %687, label %713, label %661

688:                                              ; preds = %649
  %689 = icmp eq i8 %651, 0
  br i1 %689, label %713, label %690

690:                                              ; preds = %688, %690
  %691 = phi i64 [ %711, %690 ], [ 0, %688 ]
  %692 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 %691
  %693 = bitcast <4 x i64>* %692 to <16 x i16>*
  %694 = load <16 x i16>, <16 x i16>* %693, align 32
  %695 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %694, i32 %652) #8
  store <16 x i16> %695, <16 x i16>* %693, align 32
  %696 = or i64 %691, 1
  %697 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 %696
  %698 = bitcast <4 x i64>* %697 to <16 x i16>*
  %699 = load <16 x i16>, <16 x i16>* %698, align 32
  %700 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %699, i32 %652) #8
  store <16 x i16> %700, <16 x i16>* %698, align 32
  %701 = or i64 %691, 2
  %702 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 %701
  %703 = bitcast <4 x i64>* %702 to <16 x i16>*
  %704 = load <16 x i16>, <16 x i16>* %703, align 32
  %705 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %704, i32 %652) #8
  store <16 x i16> %705, <16 x i16>* %703, align 32
  %706 = or i64 %691, 3
  %707 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 %706
  %708 = bitcast <4 x i64>* %707 to <16 x i16>*
  %709 = load <16 x i16>, <16 x i16>* %708, align 32
  %710 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %709, i32 %652) #8
  store <16 x i16> %710, <16 x i16>* %708, align 32
  %711 = add nuw nsw i64 %691, 4
  %712 = icmp eq i64 %711, 32
  br i1 %712, label %713, label %690

713:                                              ; preds = %690, %661, %688
  %714 = bitcast <4 x i64>* %650 to <2 x i64>*
  %715 = load <2 x i64>, <2 x i64>* %714, align 16
  %716 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 8
  %717 = bitcast <4 x i64>* %716 to <2 x i64>*
  %718 = load <2 x i64>, <2 x i64>* %717, align 16
  %719 = shufflevector <2 x i64> %715, <2 x i64> %718, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %720 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 1
  %721 = bitcast <4 x i64>* %720 to <2 x i64>*
  %722 = load <2 x i64>, <2 x i64>* %721, align 16
  %723 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 9
  %724 = bitcast <4 x i64>* %723 to <2 x i64>*
  %725 = load <2 x i64>, <2 x i64>* %724, align 16
  %726 = shufflevector <2 x i64> %722, <2 x i64> %725, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %727 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 2
  %728 = bitcast <4 x i64>* %727 to <2 x i64>*
  %729 = load <2 x i64>, <2 x i64>* %728, align 16
  %730 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 10
  %731 = bitcast <4 x i64>* %730 to <2 x i64>*
  %732 = load <2 x i64>, <2 x i64>* %731, align 16
  %733 = shufflevector <2 x i64> %729, <2 x i64> %732, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %734 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 3
  %735 = bitcast <4 x i64>* %734 to <2 x i64>*
  %736 = load <2 x i64>, <2 x i64>* %735, align 16
  %737 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 11
  %738 = bitcast <4 x i64>* %737 to <2 x i64>*
  %739 = load <2 x i64>, <2 x i64>* %738, align 16
  %740 = shufflevector <2 x i64> %736, <2 x i64> %739, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %741 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 4
  %742 = bitcast <4 x i64>* %741 to <2 x i64>*
  %743 = load <2 x i64>, <2 x i64>* %742, align 16
  %744 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 12
  %745 = bitcast <4 x i64>* %744 to <2 x i64>*
  %746 = load <2 x i64>, <2 x i64>* %745, align 16
  %747 = shufflevector <2 x i64> %743, <2 x i64> %746, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %748 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 5
  %749 = bitcast <4 x i64>* %748 to <2 x i64>*
  %750 = load <2 x i64>, <2 x i64>* %749, align 16
  %751 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 13
  %752 = bitcast <4 x i64>* %751 to <2 x i64>*
  %753 = load <2 x i64>, <2 x i64>* %752, align 16
  %754 = shufflevector <2 x i64> %750, <2 x i64> %753, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %755 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 6
  %756 = bitcast <4 x i64>* %755 to <2 x i64>*
  %757 = load <2 x i64>, <2 x i64>* %756, align 16
  %758 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 14
  %759 = bitcast <4 x i64>* %758 to <2 x i64>*
  %760 = load <2 x i64>, <2 x i64>* %759, align 16
  %761 = shufflevector <2 x i64> %757, <2 x i64> %760, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %762 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 7
  %763 = bitcast <4 x i64>* %762 to <2 x i64>*
  %764 = load <2 x i64>, <2 x i64>* %763, align 16
  %765 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 15
  %766 = bitcast <4 x i64>* %765 to <2 x i64>*
  %767 = load <2 x i64>, <2 x i64>* %766, align 16
  %768 = shufflevector <2 x i64> %764, <2 x i64> %767, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %769 = getelementptr inbounds <2 x i64>, <2 x i64>* %714, i64 1
  %770 = load <2 x i64>, <2 x i64>* %769, align 16
  %771 = getelementptr inbounds <2 x i64>, <2 x i64>* %717, i64 1
  %772 = load <2 x i64>, <2 x i64>* %771, align 16
  %773 = shufflevector <2 x i64> %770, <2 x i64> %772, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %774 = getelementptr inbounds <2 x i64>, <2 x i64>* %721, i64 1
  %775 = load <2 x i64>, <2 x i64>* %774, align 16
  %776 = getelementptr inbounds <2 x i64>, <2 x i64>* %724, i64 1
  %777 = load <2 x i64>, <2 x i64>* %776, align 16
  %778 = shufflevector <2 x i64> %775, <2 x i64> %777, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %779 = getelementptr inbounds <2 x i64>, <2 x i64>* %728, i64 1
  %780 = load <2 x i64>, <2 x i64>* %779, align 16
  %781 = getelementptr inbounds <2 x i64>, <2 x i64>* %731, i64 1
  %782 = load <2 x i64>, <2 x i64>* %781, align 16
  %783 = shufflevector <2 x i64> %780, <2 x i64> %782, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %784 = getelementptr inbounds <2 x i64>, <2 x i64>* %735, i64 1
  %785 = load <2 x i64>, <2 x i64>* %784, align 16
  %786 = getelementptr inbounds <2 x i64>, <2 x i64>* %738, i64 1
  %787 = load <2 x i64>, <2 x i64>* %786, align 16
  %788 = shufflevector <2 x i64> %785, <2 x i64> %787, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %789 = getelementptr inbounds <2 x i64>, <2 x i64>* %742, i64 1
  %790 = load <2 x i64>, <2 x i64>* %789, align 16
  %791 = getelementptr inbounds <2 x i64>, <2 x i64>* %745, i64 1
  %792 = load <2 x i64>, <2 x i64>* %791, align 16
  %793 = shufflevector <2 x i64> %790, <2 x i64> %792, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %794 = getelementptr inbounds <2 x i64>, <2 x i64>* %749, i64 1
  %795 = load <2 x i64>, <2 x i64>* %794, align 16
  %796 = getelementptr inbounds <2 x i64>, <2 x i64>* %752, i64 1
  %797 = load <2 x i64>, <2 x i64>* %796, align 16
  %798 = shufflevector <2 x i64> %795, <2 x i64> %797, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %799 = getelementptr inbounds <2 x i64>, <2 x i64>* %756, i64 1
  %800 = load <2 x i64>, <2 x i64>* %799, align 16
  %801 = getelementptr inbounds <2 x i64>, <2 x i64>* %759, i64 1
  %802 = load <2 x i64>, <2 x i64>* %801, align 16
  %803 = shufflevector <2 x i64> %800, <2 x i64> %802, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %804 = getelementptr inbounds <2 x i64>, <2 x i64>* %763, i64 1
  %805 = load <2 x i64>, <2 x i64>* %804, align 16
  %806 = getelementptr inbounds <2 x i64>, <2 x i64>* %766, i64 1
  %807 = load <2 x i64>, <2 x i64>* %806, align 16
  %808 = shufflevector <2 x i64> %805, <2 x i64> %807, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %809 = bitcast <4 x i64> %719 to <16 x i16>
  %810 = bitcast <4 x i64> %726 to <16 x i16>
  %811 = shufflevector <16 x i16> %809, <16 x i16> %810, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %812 = shufflevector <16 x i16> %809, <16 x i16> %810, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %813 = bitcast <4 x i64> %733 to <16 x i16>
  %814 = bitcast <4 x i64> %740 to <16 x i16>
  %815 = shufflevector <16 x i16> %813, <16 x i16> %814, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %816 = shufflevector <16 x i16> %813, <16 x i16> %814, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %817 = bitcast <4 x i64> %747 to <16 x i16>
  %818 = bitcast <4 x i64> %754 to <16 x i16>
  %819 = shufflevector <16 x i16> %817, <16 x i16> %818, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %820 = shufflevector <16 x i16> %817, <16 x i16> %818, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %821 = bitcast <4 x i64> %761 to <16 x i16>
  %822 = bitcast <4 x i64> %768 to <16 x i16>
  %823 = shufflevector <16 x i16> %821, <16 x i16> %822, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %824 = shufflevector <16 x i16> %821, <16 x i16> %822, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %825 = bitcast <16 x i16> %811 to <8 x i32>
  %826 = bitcast <16 x i16> %815 to <8 x i32>
  %827 = shufflevector <8 x i32> %825, <8 x i32> %826, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %828 = shufflevector <8 x i32> %825, <8 x i32> %826, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %829 = bitcast <16 x i16> %819 to <8 x i32>
  %830 = bitcast <16 x i16> %823 to <8 x i32>
  %831 = shufflevector <8 x i32> %829, <8 x i32> %830, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %832 = shufflevector <8 x i32> %829, <8 x i32> %830, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %833 = bitcast <16 x i16> %812 to <8 x i32>
  %834 = bitcast <16 x i16> %816 to <8 x i32>
  %835 = shufflevector <8 x i32> %833, <8 x i32> %834, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %836 = shufflevector <8 x i32> %833, <8 x i32> %834, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %837 = bitcast <16 x i16> %820 to <8 x i32>
  %838 = bitcast <16 x i16> %824 to <8 x i32>
  %839 = shufflevector <8 x i32> %837, <8 x i32> %838, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %840 = shufflevector <8 x i32> %837, <8 x i32> %838, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %841 = bitcast <8 x i32> %827 to <4 x i64>
  %842 = bitcast <8 x i32> %831 to <4 x i64>
  %843 = shufflevector <4 x i64> %841, <4 x i64> %842, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %843, <4 x i64>* %650, align 32
  %844 = shufflevector <4 x i64> %841, <4 x i64> %842, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %844, <4 x i64>* %720, align 32
  %845 = bitcast <8 x i32> %835 to <4 x i64>
  %846 = bitcast <8 x i32> %839 to <4 x i64>
  %847 = shufflevector <4 x i64> %845, <4 x i64> %846, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %847, <4 x i64>* %741, align 32
  %848 = shufflevector <4 x i64> %845, <4 x i64> %846, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %848, <4 x i64>* %748, align 32
  %849 = bitcast <8 x i32> %828 to <4 x i64>
  %850 = bitcast <8 x i32> %832 to <4 x i64>
  %851 = shufflevector <4 x i64> %849, <4 x i64> %850, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %851, <4 x i64>* %727, align 32
  %852 = shufflevector <4 x i64> %849, <4 x i64> %850, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %852, <4 x i64>* %734, align 32
  %853 = bitcast <8 x i32> %836 to <4 x i64>
  %854 = bitcast <8 x i32> %840 to <4 x i64>
  %855 = shufflevector <4 x i64> %853, <4 x i64> %854, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %855, <4 x i64>* %755, align 32
  %856 = shufflevector <4 x i64> %853, <4 x i64> %854, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %856, <4 x i64>* %762, align 32
  %857 = bitcast <4 x i64> %773 to <16 x i16>
  %858 = bitcast <4 x i64> %778 to <16 x i16>
  %859 = shufflevector <16 x i16> %857, <16 x i16> %858, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %860 = shufflevector <16 x i16> %857, <16 x i16> %858, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %861 = bitcast <4 x i64> %783 to <16 x i16>
  %862 = bitcast <4 x i64> %788 to <16 x i16>
  %863 = shufflevector <16 x i16> %861, <16 x i16> %862, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %864 = shufflevector <16 x i16> %861, <16 x i16> %862, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %865 = bitcast <4 x i64> %793 to <16 x i16>
  %866 = bitcast <4 x i64> %798 to <16 x i16>
  %867 = shufflevector <16 x i16> %865, <16 x i16> %866, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %868 = shufflevector <16 x i16> %865, <16 x i16> %866, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %869 = bitcast <4 x i64> %803 to <16 x i16>
  %870 = bitcast <4 x i64> %808 to <16 x i16>
  %871 = shufflevector <16 x i16> %869, <16 x i16> %870, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %872 = shufflevector <16 x i16> %869, <16 x i16> %870, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %873 = bitcast <16 x i16> %859 to <8 x i32>
  %874 = bitcast <16 x i16> %863 to <8 x i32>
  %875 = shufflevector <8 x i32> %873, <8 x i32> %874, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %876 = shufflevector <8 x i32> %873, <8 x i32> %874, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %877 = bitcast <16 x i16> %867 to <8 x i32>
  %878 = bitcast <16 x i16> %871 to <8 x i32>
  %879 = shufflevector <8 x i32> %877, <8 x i32> %878, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %880 = shufflevector <8 x i32> %877, <8 x i32> %878, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %881 = bitcast <16 x i16> %860 to <8 x i32>
  %882 = bitcast <16 x i16> %864 to <8 x i32>
  %883 = shufflevector <8 x i32> %881, <8 x i32> %882, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %884 = shufflevector <8 x i32> %881, <8 x i32> %882, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %885 = bitcast <16 x i16> %868 to <8 x i32>
  %886 = bitcast <16 x i16> %872 to <8 x i32>
  %887 = shufflevector <8 x i32> %885, <8 x i32> %886, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %888 = shufflevector <8 x i32> %885, <8 x i32> %886, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %889 = bitcast <8 x i32> %875 to <4 x i64>
  %890 = bitcast <8 x i32> %879 to <4 x i64>
  %891 = shufflevector <4 x i64> %889, <4 x i64> %890, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %891, <4 x i64>* %716, align 32
  %892 = shufflevector <4 x i64> %889, <4 x i64> %890, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %892, <4 x i64>* %723, align 32
  %893 = bitcast <8 x i32> %883 to <4 x i64>
  %894 = bitcast <8 x i32> %887 to <4 x i64>
  %895 = shufflevector <4 x i64> %893, <4 x i64> %894, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %895, <4 x i64>* %744, align 32
  %896 = shufflevector <4 x i64> %893, <4 x i64> %894, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %896, <4 x i64>* %751, align 32
  %897 = bitcast <8 x i32> %876 to <4 x i64>
  %898 = bitcast <8 x i32> %880 to <4 x i64>
  %899 = shufflevector <4 x i64> %897, <4 x i64> %898, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %899, <4 x i64>* %730, align 32
  %900 = shufflevector <4 x i64> %897, <4 x i64> %898, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %900, <4 x i64>* %737, align 32
  %901 = bitcast <8 x i32> %884 to <4 x i64>
  %902 = bitcast <8 x i32> %888 to <4 x i64>
  %903 = shufflevector <4 x i64> %901, <4 x i64> %902, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %903, <4 x i64>* %758, align 32
  %904 = shufflevector <4 x i64> %901, <4 x i64> %902, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %904, <4 x i64>* %765, align 32
  %905 = shl nsw i64 %629, 9
  %906 = getelementptr inbounds i32, i32* %1, i64 %905
  %907 = shufflevector <4 x i64> %843, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %908 = bitcast <2 x i64> %907 to <8 x i16>
  %909 = sext <8 x i16> %908 to <8 x i32>
  %910 = bitcast i32* %906 to <8 x i32>*
  store <8 x i32> %909, <8 x i32>* %910, align 32
  %911 = getelementptr inbounds i32, i32* %906, i64 8
  %912 = load <4 x i64>, <4 x i64>* %650, align 32
  %913 = shufflevector <4 x i64> %912, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %914 = bitcast <2 x i64> %913 to <8 x i16>
  %915 = sext <8 x i16> %914 to <8 x i32>
  %916 = bitcast i32* %911 to <8 x i32>*
  store <8 x i32> %915, <8 x i32>* %916, align 32
  br label %917

917:                                              ; preds = %1162, %713
  %918 = phi i64 [ 1, %713 ], [ %1177, %1162 ]
  %919 = phi i32* [ %906, %713 ], [ %1163, %1162 ]
  %920 = getelementptr inbounds i32, i32* %919, i64 32
  %921 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 %918
  %922 = load <4 x i64>, <4 x i64>* %921, align 32
  %923 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 %918
  %924 = shufflevector <4 x i64> %922, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %925 = bitcast <2 x i64> %924 to <8 x i16>
  %926 = sext <8 x i16> %925 to <8 x i32>
  %927 = bitcast i32* %920 to <8 x i32>*
  store <8 x i32> %926, <8 x i32>* %927, align 32
  %928 = getelementptr inbounds i32, i32* %919, i64 40
  %929 = load <4 x i64>, <4 x i64>* %923, align 32
  %930 = shufflevector <4 x i64> %929, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %931 = bitcast <2 x i64> %930 to <8 x i16>
  %932 = sext <8 x i16> %931 to <8 x i32>
  %933 = bitcast i32* %928 to <8 x i32>*
  store <8 x i32> %932, <8 x i32>* %933, align 32
  %934 = add nuw nsw i64 %918, 1
  %935 = icmp eq i64 %934, 16
  br i1 %935, label %936, label %1162

936:                                              ; preds = %917
  %937 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 16
  %938 = bitcast <4 x i64>* %937 to <2 x i64>*
  %939 = load <2 x i64>, <2 x i64>* %938, align 16
  %940 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 24
  %941 = bitcast <4 x i64>* %940 to <2 x i64>*
  %942 = load <2 x i64>, <2 x i64>* %941, align 16
  %943 = shufflevector <2 x i64> %939, <2 x i64> %942, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %944 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 17
  %945 = bitcast <4 x i64>* %944 to <2 x i64>*
  %946 = load <2 x i64>, <2 x i64>* %945, align 16
  %947 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 25
  %948 = bitcast <4 x i64>* %947 to <2 x i64>*
  %949 = load <2 x i64>, <2 x i64>* %948, align 16
  %950 = shufflevector <2 x i64> %946, <2 x i64> %949, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %951 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 18
  %952 = bitcast <4 x i64>* %951 to <2 x i64>*
  %953 = load <2 x i64>, <2 x i64>* %952, align 16
  %954 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 26
  %955 = bitcast <4 x i64>* %954 to <2 x i64>*
  %956 = load <2 x i64>, <2 x i64>* %955, align 16
  %957 = shufflevector <2 x i64> %953, <2 x i64> %956, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %958 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 19
  %959 = bitcast <4 x i64>* %958 to <2 x i64>*
  %960 = load <2 x i64>, <2 x i64>* %959, align 16
  %961 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 27
  %962 = bitcast <4 x i64>* %961 to <2 x i64>*
  %963 = load <2 x i64>, <2 x i64>* %962, align 16
  %964 = shufflevector <2 x i64> %960, <2 x i64> %963, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %965 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 20
  %966 = bitcast <4 x i64>* %965 to <2 x i64>*
  %967 = load <2 x i64>, <2 x i64>* %966, align 16
  %968 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 28
  %969 = bitcast <4 x i64>* %968 to <2 x i64>*
  %970 = load <2 x i64>, <2 x i64>* %969, align 16
  %971 = shufflevector <2 x i64> %967, <2 x i64> %970, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %972 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 21
  %973 = bitcast <4 x i64>* %972 to <2 x i64>*
  %974 = load <2 x i64>, <2 x i64>* %973, align 16
  %975 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 29
  %976 = bitcast <4 x i64>* %975 to <2 x i64>*
  %977 = load <2 x i64>, <2 x i64>* %976, align 16
  %978 = shufflevector <2 x i64> %974, <2 x i64> %977, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %979 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 22
  %980 = bitcast <4 x i64>* %979 to <2 x i64>*
  %981 = load <2 x i64>, <2 x i64>* %980, align 16
  %982 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 30
  %983 = bitcast <4 x i64>* %982 to <2 x i64>*
  %984 = load <2 x i64>, <2 x i64>* %983, align 16
  %985 = shufflevector <2 x i64> %981, <2 x i64> %984, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %986 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 23
  %987 = bitcast <4 x i64>* %986 to <2 x i64>*
  %988 = load <2 x i64>, <2 x i64>* %987, align 16
  %989 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 31
  %990 = bitcast <4 x i64>* %989 to <2 x i64>*
  %991 = load <2 x i64>, <2 x i64>* %990, align 16
  %992 = shufflevector <2 x i64> %988, <2 x i64> %991, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %993 = getelementptr inbounds <2 x i64>, <2 x i64>* %938, i64 1
  %994 = load <2 x i64>, <2 x i64>* %993, align 16
  %995 = getelementptr inbounds <2 x i64>, <2 x i64>* %941, i64 1
  %996 = load <2 x i64>, <2 x i64>* %995, align 16
  %997 = shufflevector <2 x i64> %994, <2 x i64> %996, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %998 = getelementptr inbounds <2 x i64>, <2 x i64>* %945, i64 1
  %999 = load <2 x i64>, <2 x i64>* %998, align 16
  %1000 = getelementptr inbounds <2 x i64>, <2 x i64>* %948, i64 1
  %1001 = load <2 x i64>, <2 x i64>* %1000, align 16
  %1002 = shufflevector <2 x i64> %999, <2 x i64> %1001, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1003 = getelementptr inbounds <2 x i64>, <2 x i64>* %952, i64 1
  %1004 = load <2 x i64>, <2 x i64>* %1003, align 16
  %1005 = getelementptr inbounds <2 x i64>, <2 x i64>* %955, i64 1
  %1006 = load <2 x i64>, <2 x i64>* %1005, align 16
  %1007 = shufflevector <2 x i64> %1004, <2 x i64> %1006, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1008 = getelementptr inbounds <2 x i64>, <2 x i64>* %959, i64 1
  %1009 = load <2 x i64>, <2 x i64>* %1008, align 16
  %1010 = getelementptr inbounds <2 x i64>, <2 x i64>* %962, i64 1
  %1011 = load <2 x i64>, <2 x i64>* %1010, align 16
  %1012 = shufflevector <2 x i64> %1009, <2 x i64> %1011, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1013 = getelementptr inbounds <2 x i64>, <2 x i64>* %966, i64 1
  %1014 = load <2 x i64>, <2 x i64>* %1013, align 16
  %1015 = getelementptr inbounds <2 x i64>, <2 x i64>* %969, i64 1
  %1016 = load <2 x i64>, <2 x i64>* %1015, align 16
  %1017 = shufflevector <2 x i64> %1014, <2 x i64> %1016, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1018 = getelementptr inbounds <2 x i64>, <2 x i64>* %973, i64 1
  %1019 = load <2 x i64>, <2 x i64>* %1018, align 16
  %1020 = getelementptr inbounds <2 x i64>, <2 x i64>* %976, i64 1
  %1021 = load <2 x i64>, <2 x i64>* %1020, align 16
  %1022 = shufflevector <2 x i64> %1019, <2 x i64> %1021, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1023 = getelementptr inbounds <2 x i64>, <2 x i64>* %980, i64 1
  %1024 = load <2 x i64>, <2 x i64>* %1023, align 16
  %1025 = getelementptr inbounds <2 x i64>, <2 x i64>* %983, i64 1
  %1026 = load <2 x i64>, <2 x i64>* %1025, align 16
  %1027 = shufflevector <2 x i64> %1024, <2 x i64> %1026, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1028 = getelementptr inbounds <2 x i64>, <2 x i64>* %987, i64 1
  %1029 = load <2 x i64>, <2 x i64>* %1028, align 16
  %1030 = getelementptr inbounds <2 x i64>, <2 x i64>* %990, i64 1
  %1031 = load <2 x i64>, <2 x i64>* %1030, align 16
  %1032 = shufflevector <2 x i64> %1029, <2 x i64> %1031, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1033 = bitcast <4 x i64> %943 to <16 x i16>
  %1034 = bitcast <4 x i64> %950 to <16 x i16>
  %1035 = shufflevector <16 x i16> %1033, <16 x i16> %1034, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1036 = shufflevector <16 x i16> %1033, <16 x i16> %1034, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1037 = bitcast <4 x i64> %957 to <16 x i16>
  %1038 = bitcast <4 x i64> %964 to <16 x i16>
  %1039 = shufflevector <16 x i16> %1037, <16 x i16> %1038, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1040 = shufflevector <16 x i16> %1037, <16 x i16> %1038, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1041 = bitcast <4 x i64> %971 to <16 x i16>
  %1042 = bitcast <4 x i64> %978 to <16 x i16>
  %1043 = shufflevector <16 x i16> %1041, <16 x i16> %1042, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1044 = shufflevector <16 x i16> %1041, <16 x i16> %1042, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1045 = bitcast <4 x i64> %985 to <16 x i16>
  %1046 = bitcast <4 x i64> %992 to <16 x i16>
  %1047 = shufflevector <16 x i16> %1045, <16 x i16> %1046, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1048 = shufflevector <16 x i16> %1045, <16 x i16> %1046, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1049 = bitcast <16 x i16> %1035 to <8 x i32>
  %1050 = bitcast <16 x i16> %1039 to <8 x i32>
  %1051 = shufflevector <8 x i32> %1049, <8 x i32> %1050, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1052 = shufflevector <8 x i32> %1049, <8 x i32> %1050, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1053 = bitcast <16 x i16> %1043 to <8 x i32>
  %1054 = bitcast <16 x i16> %1047 to <8 x i32>
  %1055 = shufflevector <8 x i32> %1053, <8 x i32> %1054, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1056 = shufflevector <8 x i32> %1053, <8 x i32> %1054, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1057 = bitcast <16 x i16> %1036 to <8 x i32>
  %1058 = bitcast <16 x i16> %1040 to <8 x i32>
  %1059 = shufflevector <8 x i32> %1057, <8 x i32> %1058, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1060 = shufflevector <8 x i32> %1057, <8 x i32> %1058, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1061 = bitcast <16 x i16> %1044 to <8 x i32>
  %1062 = bitcast <16 x i16> %1048 to <8 x i32>
  %1063 = shufflevector <8 x i32> %1061, <8 x i32> %1062, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1064 = shufflevector <8 x i32> %1061, <8 x i32> %1062, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1065 = bitcast <8 x i32> %1051 to <4 x i64>
  %1066 = bitcast <8 x i32> %1055 to <4 x i64>
  %1067 = shufflevector <4 x i64> %1065, <4 x i64> %1066, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1067, <4 x i64>* %937, align 32
  %1068 = shufflevector <4 x i64> %1065, <4 x i64> %1066, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1068, <4 x i64>* %944, align 32
  %1069 = bitcast <8 x i32> %1059 to <4 x i64>
  %1070 = bitcast <8 x i32> %1063 to <4 x i64>
  %1071 = shufflevector <4 x i64> %1069, <4 x i64> %1070, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1071, <4 x i64>* %965, align 32
  %1072 = shufflevector <4 x i64> %1069, <4 x i64> %1070, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1072, <4 x i64>* %972, align 32
  %1073 = bitcast <8 x i32> %1052 to <4 x i64>
  %1074 = bitcast <8 x i32> %1056 to <4 x i64>
  %1075 = shufflevector <4 x i64> %1073, <4 x i64> %1074, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1075, <4 x i64>* %951, align 32
  %1076 = shufflevector <4 x i64> %1073, <4 x i64> %1074, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1076, <4 x i64>* %958, align 32
  %1077 = bitcast <8 x i32> %1060 to <4 x i64>
  %1078 = bitcast <8 x i32> %1064 to <4 x i64>
  %1079 = shufflevector <4 x i64> %1077, <4 x i64> %1078, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1079, <4 x i64>* %979, align 32
  %1080 = shufflevector <4 x i64> %1077, <4 x i64> %1078, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1080, <4 x i64>* %986, align 32
  %1081 = bitcast <4 x i64> %997 to <16 x i16>
  %1082 = bitcast <4 x i64> %1002 to <16 x i16>
  %1083 = shufflevector <16 x i16> %1081, <16 x i16> %1082, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1084 = shufflevector <16 x i16> %1081, <16 x i16> %1082, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1085 = bitcast <4 x i64> %1007 to <16 x i16>
  %1086 = bitcast <4 x i64> %1012 to <16 x i16>
  %1087 = shufflevector <16 x i16> %1085, <16 x i16> %1086, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1088 = shufflevector <16 x i16> %1085, <16 x i16> %1086, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1089 = bitcast <4 x i64> %1017 to <16 x i16>
  %1090 = bitcast <4 x i64> %1022 to <16 x i16>
  %1091 = shufflevector <16 x i16> %1089, <16 x i16> %1090, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1092 = shufflevector <16 x i16> %1089, <16 x i16> %1090, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1093 = bitcast <4 x i64> %1027 to <16 x i16>
  %1094 = bitcast <4 x i64> %1032 to <16 x i16>
  %1095 = shufflevector <16 x i16> %1093, <16 x i16> %1094, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1096 = shufflevector <16 x i16> %1093, <16 x i16> %1094, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1097 = bitcast <16 x i16> %1083 to <8 x i32>
  %1098 = bitcast <16 x i16> %1087 to <8 x i32>
  %1099 = shufflevector <8 x i32> %1097, <8 x i32> %1098, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1100 = shufflevector <8 x i32> %1097, <8 x i32> %1098, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1101 = bitcast <16 x i16> %1091 to <8 x i32>
  %1102 = bitcast <16 x i16> %1095 to <8 x i32>
  %1103 = shufflevector <8 x i32> %1101, <8 x i32> %1102, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1104 = shufflevector <8 x i32> %1101, <8 x i32> %1102, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1105 = bitcast <16 x i16> %1084 to <8 x i32>
  %1106 = bitcast <16 x i16> %1088 to <8 x i32>
  %1107 = shufflevector <8 x i32> %1105, <8 x i32> %1106, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1108 = shufflevector <8 x i32> %1105, <8 x i32> %1106, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1109 = bitcast <16 x i16> %1092 to <8 x i32>
  %1110 = bitcast <16 x i16> %1096 to <8 x i32>
  %1111 = shufflevector <8 x i32> %1109, <8 x i32> %1110, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1112 = shufflevector <8 x i32> %1109, <8 x i32> %1110, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1113 = bitcast <8 x i32> %1099 to <4 x i64>
  %1114 = bitcast <8 x i32> %1103 to <4 x i64>
  %1115 = shufflevector <4 x i64> %1113, <4 x i64> %1114, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1115, <4 x i64>* %940, align 32
  %1116 = shufflevector <4 x i64> %1113, <4 x i64> %1114, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1116, <4 x i64>* %947, align 32
  %1117 = bitcast <8 x i32> %1107 to <4 x i64>
  %1118 = bitcast <8 x i32> %1111 to <4 x i64>
  %1119 = shufflevector <4 x i64> %1117, <4 x i64> %1118, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1119, <4 x i64>* %968, align 32
  %1120 = shufflevector <4 x i64> %1117, <4 x i64> %1118, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1120, <4 x i64>* %975, align 32
  %1121 = bitcast <8 x i32> %1100 to <4 x i64>
  %1122 = bitcast <8 x i32> %1104 to <4 x i64>
  %1123 = shufflevector <4 x i64> %1121, <4 x i64> %1122, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1123, <4 x i64>* %954, align 32
  %1124 = shufflevector <4 x i64> %1121, <4 x i64> %1122, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1124, <4 x i64>* %961, align 32
  %1125 = bitcast <8 x i32> %1108 to <4 x i64>
  %1126 = bitcast <8 x i32> %1112 to <4 x i64>
  %1127 = shufflevector <4 x i64> %1125, <4 x i64> %1126, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1127, <4 x i64>* %982, align 32
  %1128 = shufflevector <4 x i64> %1125, <4 x i64> %1126, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1128, <4 x i64>* %989, align 32
  %1129 = getelementptr inbounds i32, i32* %906, i64 16
  %1130 = shufflevector <4 x i64> %1067, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %1131 = bitcast <2 x i64> %1130 to <8 x i16>
  %1132 = sext <8 x i16> %1131 to <8 x i32>
  %1133 = bitcast i32* %1129 to <8 x i32>*
  store <8 x i32> %1132, <8 x i32>* %1133, align 32
  %1134 = getelementptr inbounds i32, i32* %1129, i64 8
  %1135 = load <4 x i64>, <4 x i64>* %937, align 32
  %1136 = shufflevector <4 x i64> %1135, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %1137 = bitcast <2 x i64> %1136 to <8 x i16>
  %1138 = sext <8 x i16> %1137 to <8 x i32>
  %1139 = bitcast i32* %1134 to <8 x i32>*
  store <8 x i32> %1138, <8 x i32>* %1139, align 32
  br label %1140

1140:                                             ; preds = %1178, %936
  %1141 = phi i64 [ 1, %936 ], [ %1193, %1178 ]
  %1142 = phi i32* [ %1129, %936 ], [ %1179, %1178 ]
  %1143 = getelementptr inbounds i32, i32* %1142, i64 32
  %1144 = getelementptr inbounds <4 x i64>, <4 x i64>* %937, i64 %1141
  %1145 = load <4 x i64>, <4 x i64>* %1144, align 32
  %1146 = getelementptr inbounds <4 x i64>, <4 x i64>* %937, i64 %1141
  %1147 = shufflevector <4 x i64> %1145, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %1148 = bitcast <2 x i64> %1147 to <8 x i16>
  %1149 = sext <8 x i16> %1148 to <8 x i32>
  %1150 = bitcast i32* %1143 to <8 x i32>*
  store <8 x i32> %1149, <8 x i32>* %1150, align 32
  %1151 = getelementptr inbounds i32, i32* %1142, i64 40
  %1152 = load <4 x i64>, <4 x i64>* %1146, align 32
  %1153 = shufflevector <4 x i64> %1152, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %1154 = bitcast <2 x i64> %1153 to <8 x i16>
  %1155 = sext <8 x i16> %1154 to <8 x i32>
  %1156 = bitcast i32* %1151 to <8 x i32>*
  store <8 x i32> %1155, <8 x i32>* %1156, align 32
  %1157 = add nuw nsw i64 %1141, 1
  %1158 = icmp eq i64 %1157, 16
  br i1 %1158, label %1159, label %1178

1159:                                             ; preds = %1140
  %1160 = add nuw nsw i64 %629, 1
  %1161 = icmp eq i64 %1160, 2
  br i1 %1161, label %627, label %628

1162:                                             ; preds = %917
  %1163 = getelementptr inbounds i32, i32* %919, i64 64
  %1164 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 %934
  %1165 = load <4 x i64>, <4 x i64>* %1164, align 32
  %1166 = getelementptr inbounds <4 x i64>, <4 x i64>* %650, i64 %934
  %1167 = shufflevector <4 x i64> %1165, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %1168 = bitcast <2 x i64> %1167 to <8 x i16>
  %1169 = sext <8 x i16> %1168 to <8 x i32>
  %1170 = bitcast i32* %1163 to <8 x i32>*
  store <8 x i32> %1169, <8 x i32>* %1170, align 32
  %1171 = getelementptr inbounds i32, i32* %919, i64 72
  %1172 = load <4 x i64>, <4 x i64>* %1166, align 32
  %1173 = shufflevector <4 x i64> %1172, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %1174 = bitcast <2 x i64> %1173 to <8 x i16>
  %1175 = sext <8 x i16> %1174 to <8 x i32>
  %1176 = bitcast i32* %1171 to <8 x i32>*
  store <8 x i32> %1175, <8 x i32>* %1176, align 32
  %1177 = add nuw nsw i64 %918, 2
  br label %917

1178:                                             ; preds = %1140
  %1179 = getelementptr inbounds i32, i32* %1142, i64 64
  %1180 = getelementptr inbounds <4 x i64>, <4 x i64>* %937, i64 %1157
  %1181 = load <4 x i64>, <4 x i64>* %1180, align 32
  %1182 = getelementptr inbounds <4 x i64>, <4 x i64>* %937, i64 %1157
  %1183 = shufflevector <4 x i64> %1181, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %1184 = bitcast <2 x i64> %1183 to <8 x i16>
  %1185 = sext <8 x i16> %1184 to <8 x i32>
  %1186 = bitcast i32* %1179 to <8 x i32>*
  store <8 x i32> %1185, <8 x i32>* %1186, align 32
  %1187 = getelementptr inbounds i32, i32* %1142, i64 72
  %1188 = load <4 x i64>, <4 x i64>* %1182, align 32
  %1189 = shufflevector <4 x i64> %1188, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %1190 = bitcast <2 x i64> %1189 to <8 x i16>
  %1191 = sext <8 x i16> %1190 to <8 x i32>
  %1192 = bitcast i32* %1187 to <8 x i32>*
  store <8 x i32> %1191, <8 x i32>* %1192, align 32
  %1193 = add nuw nsw i64 %1141, 2
  br label %1140
}

; Function Attrs: nounwind ssp uwtable
define internal void @lowbd_fwd_txfm2d_64x64_avx2(i16* nocapture readonly, i32* nocapture, i32, i8 zeroext, i32) #3 {
  %6 = alloca [64 x <4 x i64>], align 32
  %7 = alloca [256 x <4 x i64>], align 32
  %8 = alloca [64 x <4 x i64>], align 32
  %9 = alloca [64 x <4 x i64>], align 32
  %10 = bitcast [64 x <4 x i64>]* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 2048, i8* nonnull %10) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %10, i8 -86, i64 2048, i1 false)
  %11 = bitcast [256 x <4 x i64>]* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 8192, i8* nonnull %11) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %11, i8 -86, i64 8192, i1 false)
  %12 = load i8*, i8** getelementptr inbounds ([19 x i8*], [19 x i8*]* @av1_fwd_txfm_shift_ls, i64 0, i64 4), align 16
  %13 = load i8, i8* getelementptr inbounds ([5 x [5 x i8]], [5 x [5 x i8]]* @av1_fwd_cos_bit_col, i64 0, i64 4, i64 4), align 4
  %14 = load i8, i8* getelementptr inbounds ([5 x [5 x i8]], [5 x [5 x i8]]* @av1_fwd_cos_bit_row, i64 0, i64 4, i64 4), align 4
  %15 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 0
  %16 = sext i32 %2 to i64
  %17 = getelementptr inbounds i8, i8* %12, i64 1
  br label %24

18:                                               ; preds = %182
  %19 = bitcast [64 x <4 x i64>]* %8 to i8*
  %20 = bitcast [64 x <4 x i64>]* %9 to i8*
  %21 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 0
  %22 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %9, i64 0, i64 0
  %23 = getelementptr inbounds i8, i8* %12, i64 2
  br label %401

24:                                               ; preds = %182, %5
  %25 = phi i64 [ 0, %5 ], [ %183, %182 ]
  %26 = shl nsw i64 %25, 4
  %27 = getelementptr inbounds i16, i16* %0, i64 %26
  br label %28

28:                                               ; preds = %28, %24
  %29 = phi i64 [ 0, %24 ], [ %53, %28 ]
  %30 = mul nsw i64 %29, %16
  %31 = getelementptr inbounds i16, i16* %27, i64 %30
  %32 = bitcast i16* %31 to <4 x i64>*
  %33 = load <4 x i64>, <4 x i64>* %32, align 32
  %34 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %29
  store <4 x i64> %33, <4 x i64>* %34, align 32
  %35 = or i64 %29, 1
  %36 = mul nsw i64 %35, %16
  %37 = getelementptr inbounds i16, i16* %27, i64 %36
  %38 = bitcast i16* %37 to <4 x i64>*
  %39 = load <4 x i64>, <4 x i64>* %38, align 32
  %40 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %35
  store <4 x i64> %39, <4 x i64>* %40, align 32
  %41 = or i64 %29, 2
  %42 = mul nsw i64 %41, %16
  %43 = getelementptr inbounds i16, i16* %27, i64 %42
  %44 = bitcast i16* %43 to <4 x i64>*
  %45 = load <4 x i64>, <4 x i64>* %44, align 32
  %46 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %41
  store <4 x i64> %45, <4 x i64>* %46, align 32
  %47 = or i64 %29, 3
  %48 = mul nsw i64 %47, %16
  %49 = getelementptr inbounds i16, i16* %27, i64 %48
  %50 = bitcast i16* %49 to <4 x i64>*
  %51 = load <4 x i64>, <4 x i64>* %50, align 32
  %52 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %47
  store <4 x i64> %51, <4 x i64>* %52, align 32
  %53 = add nuw nsw i64 %29, 4
  %54 = icmp eq i64 %53, 64
  br i1 %54, label %55, label %28

55:                                               ; preds = %28
  %56 = load i8, i8* %12, align 1
  %57 = sext i8 %56 to i32
  %58 = icmp slt i8 %56, 0
  br i1 %58, label %59, label %93

59:                                               ; preds = %55
  %60 = sub nsw i32 0, %57
  %61 = xor i32 %57, -1
  %62 = shl i32 1, %61
  %63 = trunc i32 %62 to i16
  %64 = insertelement <16 x i16> undef, i16 %63, i32 0
  %65 = shufflevector <16 x i16> %64, <16 x i16> undef, <16 x i32> zeroinitializer
  br label %66

66:                                               ; preds = %66, %59
  %67 = phi i64 [ 0, %59 ], [ %91, %66 ]
  %68 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %67
  %69 = bitcast <4 x i64>* %68 to <16 x i16>*
  %70 = load <16 x i16>, <16 x i16>* %69, align 32
  %71 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %70, <16 x i16> %65) #8
  %72 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %71, i32 %60) #8
  store <16 x i16> %72, <16 x i16>* %69, align 32
  %73 = or i64 %67, 1
  %74 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %73
  %75 = bitcast <4 x i64>* %74 to <16 x i16>*
  %76 = load <16 x i16>, <16 x i16>* %75, align 32
  %77 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %76, <16 x i16> %65) #8
  %78 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %77, i32 %60) #8
  store <16 x i16> %78, <16 x i16>* %75, align 32
  %79 = or i64 %67, 2
  %80 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %79
  %81 = bitcast <4 x i64>* %80 to <16 x i16>*
  %82 = load <16 x i16>, <16 x i16>* %81, align 32
  %83 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %82, <16 x i16> %65) #8
  %84 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %83, i32 %60) #8
  store <16 x i16> %84, <16 x i16>* %81, align 32
  %85 = or i64 %67, 3
  %86 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %85
  %87 = bitcast <4 x i64>* %86 to <16 x i16>*
  %88 = load <16 x i16>, <16 x i16>* %87, align 32
  %89 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %88, <16 x i16> %65) #8
  %90 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %89, i32 %60) #8
  store <16 x i16> %90, <16 x i16>* %87, align 32
  %91 = add nuw nsw i64 %67, 4
  %92 = icmp eq i64 %91, 64
  br i1 %92, label %118, label %66

93:                                               ; preds = %55
  %94 = icmp eq i8 %56, 0
  br i1 %94, label %118, label %95

95:                                               ; preds = %93, %95
  %96 = phi i64 [ %116, %95 ], [ 0, %93 ]
  %97 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %96
  %98 = bitcast <4 x i64>* %97 to <16 x i16>*
  %99 = load <16 x i16>, <16 x i16>* %98, align 32
  %100 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %99, i32 %57) #8
  store <16 x i16> %100, <16 x i16>* %98, align 32
  %101 = or i64 %96, 1
  %102 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %101
  %103 = bitcast <4 x i64>* %102 to <16 x i16>*
  %104 = load <16 x i16>, <16 x i16>* %103, align 32
  %105 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %104, i32 %57) #8
  store <16 x i16> %105, <16 x i16>* %103, align 32
  %106 = or i64 %96, 2
  %107 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %106
  %108 = bitcast <4 x i64>* %107 to <16 x i16>*
  %109 = load <16 x i16>, <16 x i16>* %108, align 32
  %110 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %109, i32 %57) #8
  store <16 x i16> %110, <16 x i16>* %108, align 32
  %111 = or i64 %96, 3
  %112 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %111
  %113 = bitcast <4 x i64>* %112 to <16 x i16>*
  %114 = load <16 x i16>, <16 x i16>* %113, align 32
  %115 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %114, i32 %57) #8
  store <16 x i16> %115, <16 x i16>* %113, align 32
  %116 = add nuw nsw i64 %96, 4
  %117 = icmp eq i64 %116, 64
  br i1 %117, label %118, label %95

118:                                              ; preds = %95, %66, %93
  call fastcc void @fdct16x64_new_avx2(<4 x i64>* nonnull %15, <4 x i64>* nonnull %15, i8 signext %13)
  %119 = load i8, i8* %17, align 1
  %120 = sext i8 %119 to i32
  %121 = icmp slt i8 %119, 0
  br i1 %121, label %122, label %156

122:                                              ; preds = %118
  %123 = sub nsw i32 0, %120
  %124 = xor i32 %120, -1
  %125 = shl i32 1, %124
  %126 = trunc i32 %125 to i16
  %127 = insertelement <16 x i16> undef, i16 %126, i32 0
  %128 = shufflevector <16 x i16> %127, <16 x i16> undef, <16 x i32> zeroinitializer
  br label %129

129:                                              ; preds = %129, %122
  %130 = phi i64 [ 0, %122 ], [ %154, %129 ]
  %131 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %130
  %132 = bitcast <4 x i64>* %131 to <16 x i16>*
  %133 = load <16 x i16>, <16 x i16>* %132, align 32
  %134 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %133, <16 x i16> %128) #8
  %135 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %134, i32 %123) #8
  store <16 x i16> %135, <16 x i16>* %132, align 32
  %136 = or i64 %130, 1
  %137 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %136
  %138 = bitcast <4 x i64>* %137 to <16 x i16>*
  %139 = load <16 x i16>, <16 x i16>* %138, align 32
  %140 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %139, <16 x i16> %128) #8
  %141 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %140, i32 %123) #8
  store <16 x i16> %141, <16 x i16>* %138, align 32
  %142 = or i64 %130, 2
  %143 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %142
  %144 = bitcast <4 x i64>* %143 to <16 x i16>*
  %145 = load <16 x i16>, <16 x i16>* %144, align 32
  %146 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %145, <16 x i16> %128) #8
  %147 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %146, i32 %123) #8
  store <16 x i16> %147, <16 x i16>* %144, align 32
  %148 = or i64 %130, 3
  %149 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %148
  %150 = bitcast <4 x i64>* %149 to <16 x i16>*
  %151 = load <16 x i16>, <16 x i16>* %150, align 32
  %152 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %151, <16 x i16> %128) #8
  %153 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %152, i32 %123) #8
  store <16 x i16> %153, <16 x i16>* %150, align 32
  %154 = add nuw nsw i64 %130, 4
  %155 = icmp eq i64 %154, 64
  br i1 %155, label %158, label %129

156:                                              ; preds = %118
  %157 = icmp eq i8 %119, 0
  br i1 %157, label %158, label %159

158:                                              ; preds = %159, %129, %156
  br label %185

159:                                              ; preds = %156, %159
  %160 = phi i64 [ %180, %159 ], [ 0, %156 ]
  %161 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %160
  %162 = bitcast <4 x i64>* %161 to <16 x i16>*
  %163 = load <16 x i16>, <16 x i16>* %162, align 32
  %164 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %163, i32 %120) #8
  store <16 x i16> %164, <16 x i16>* %162, align 32
  %165 = or i64 %160, 1
  %166 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %165
  %167 = bitcast <4 x i64>* %166 to <16 x i16>*
  %168 = load <16 x i16>, <16 x i16>* %167, align 32
  %169 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %168, i32 %120) #8
  store <16 x i16> %169, <16 x i16>* %167, align 32
  %170 = or i64 %160, 2
  %171 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %170
  %172 = bitcast <4 x i64>* %171 to <16 x i16>*
  %173 = load <16 x i16>, <16 x i16>* %172, align 32
  %174 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %173, i32 %120) #8
  store <16 x i16> %174, <16 x i16>* %172, align 32
  %175 = or i64 %160, 3
  %176 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %175
  %177 = bitcast <4 x i64>* %176 to <16 x i16>*
  %178 = load <16 x i16>, <16 x i16>* %177, align 32
  %179 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %178, i32 %120) #8
  store <16 x i16> %179, <16 x i16>* %177, align 32
  %180 = add nuw nsw i64 %160, 4
  %181 = icmp eq i64 %180, 64
  br i1 %181, label %158, label %159

182:                                              ; preds = %185
  %183 = add nuw nsw i64 %25, 1
  %184 = icmp eq i64 %183, 4
  br i1 %184, label %18, label %24

185:                                              ; preds = %158, %185
  %186 = phi i64 [ %398, %185 ], [ 0, %158 ]
  %187 = shl nsw i64 %186, 4
  %188 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %187
  %189 = shl nsw i64 %186, 6
  %190 = getelementptr inbounds [256 x <4 x i64>], [256 x <4 x i64>]* %7, i64 0, i64 %189
  %191 = getelementptr inbounds <4 x i64>, <4 x i64>* %190, i64 %26
  %192 = bitcast <4 x i64>* %188 to <2 x i64>*
  %193 = load <2 x i64>, <2 x i64>* %192, align 32
  %194 = getelementptr inbounds <4 x i64>, <4 x i64>* %188, i64 8
  %195 = bitcast <4 x i64>* %194 to <2 x i64>*
  %196 = load <2 x i64>, <2 x i64>* %195, align 32
  %197 = shufflevector <2 x i64> %193, <2 x i64> %196, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %198 = getelementptr inbounds <4 x i64>, <4 x i64>* %188, i64 1
  %199 = bitcast <4 x i64>* %198 to <2 x i64>*
  %200 = load <2 x i64>, <2 x i64>* %199, align 32
  %201 = getelementptr inbounds <4 x i64>, <4 x i64>* %188, i64 9
  %202 = bitcast <4 x i64>* %201 to <2 x i64>*
  %203 = load <2 x i64>, <2 x i64>* %202, align 32
  %204 = shufflevector <2 x i64> %200, <2 x i64> %203, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %205 = getelementptr inbounds <4 x i64>, <4 x i64>* %188, i64 2
  %206 = bitcast <4 x i64>* %205 to <2 x i64>*
  %207 = load <2 x i64>, <2 x i64>* %206, align 32
  %208 = getelementptr inbounds <4 x i64>, <4 x i64>* %188, i64 10
  %209 = bitcast <4 x i64>* %208 to <2 x i64>*
  %210 = load <2 x i64>, <2 x i64>* %209, align 32
  %211 = shufflevector <2 x i64> %207, <2 x i64> %210, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %212 = getelementptr inbounds <4 x i64>, <4 x i64>* %188, i64 3
  %213 = bitcast <4 x i64>* %212 to <2 x i64>*
  %214 = load <2 x i64>, <2 x i64>* %213, align 32
  %215 = getelementptr inbounds <4 x i64>, <4 x i64>* %188, i64 11
  %216 = bitcast <4 x i64>* %215 to <2 x i64>*
  %217 = load <2 x i64>, <2 x i64>* %216, align 32
  %218 = shufflevector <2 x i64> %214, <2 x i64> %217, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %219 = getelementptr inbounds <4 x i64>, <4 x i64>* %188, i64 4
  %220 = bitcast <4 x i64>* %219 to <2 x i64>*
  %221 = load <2 x i64>, <2 x i64>* %220, align 32
  %222 = getelementptr inbounds <4 x i64>, <4 x i64>* %188, i64 12
  %223 = bitcast <4 x i64>* %222 to <2 x i64>*
  %224 = load <2 x i64>, <2 x i64>* %223, align 32
  %225 = shufflevector <2 x i64> %221, <2 x i64> %224, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %226 = getelementptr inbounds <4 x i64>, <4 x i64>* %188, i64 5
  %227 = bitcast <4 x i64>* %226 to <2 x i64>*
  %228 = load <2 x i64>, <2 x i64>* %227, align 32
  %229 = getelementptr inbounds <4 x i64>, <4 x i64>* %188, i64 13
  %230 = bitcast <4 x i64>* %229 to <2 x i64>*
  %231 = load <2 x i64>, <2 x i64>* %230, align 32
  %232 = shufflevector <2 x i64> %228, <2 x i64> %231, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %233 = getelementptr inbounds <4 x i64>, <4 x i64>* %188, i64 6
  %234 = bitcast <4 x i64>* %233 to <2 x i64>*
  %235 = load <2 x i64>, <2 x i64>* %234, align 32
  %236 = getelementptr inbounds <4 x i64>, <4 x i64>* %188, i64 14
  %237 = bitcast <4 x i64>* %236 to <2 x i64>*
  %238 = load <2 x i64>, <2 x i64>* %237, align 32
  %239 = shufflevector <2 x i64> %235, <2 x i64> %238, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %240 = getelementptr inbounds <4 x i64>, <4 x i64>* %188, i64 7
  %241 = bitcast <4 x i64>* %240 to <2 x i64>*
  %242 = load <2 x i64>, <2 x i64>* %241, align 32
  %243 = getelementptr inbounds <4 x i64>, <4 x i64>* %188, i64 15
  %244 = bitcast <4 x i64>* %243 to <2 x i64>*
  %245 = load <2 x i64>, <2 x i64>* %244, align 32
  %246 = shufflevector <2 x i64> %242, <2 x i64> %245, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %247 = getelementptr inbounds <2 x i64>, <2 x i64>* %192, i64 1
  %248 = load <2 x i64>, <2 x i64>* %247, align 16
  %249 = getelementptr inbounds <2 x i64>, <2 x i64>* %195, i64 1
  %250 = load <2 x i64>, <2 x i64>* %249, align 16
  %251 = shufflevector <2 x i64> %248, <2 x i64> %250, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %252 = getelementptr inbounds <2 x i64>, <2 x i64>* %199, i64 1
  %253 = load <2 x i64>, <2 x i64>* %252, align 16
  %254 = getelementptr inbounds <2 x i64>, <2 x i64>* %202, i64 1
  %255 = load <2 x i64>, <2 x i64>* %254, align 16
  %256 = shufflevector <2 x i64> %253, <2 x i64> %255, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %257 = getelementptr inbounds <2 x i64>, <2 x i64>* %206, i64 1
  %258 = load <2 x i64>, <2 x i64>* %257, align 16
  %259 = getelementptr inbounds <2 x i64>, <2 x i64>* %209, i64 1
  %260 = load <2 x i64>, <2 x i64>* %259, align 16
  %261 = shufflevector <2 x i64> %258, <2 x i64> %260, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %262 = getelementptr inbounds <2 x i64>, <2 x i64>* %213, i64 1
  %263 = load <2 x i64>, <2 x i64>* %262, align 16
  %264 = getelementptr inbounds <2 x i64>, <2 x i64>* %216, i64 1
  %265 = load <2 x i64>, <2 x i64>* %264, align 16
  %266 = shufflevector <2 x i64> %263, <2 x i64> %265, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %267 = getelementptr inbounds <2 x i64>, <2 x i64>* %220, i64 1
  %268 = load <2 x i64>, <2 x i64>* %267, align 16
  %269 = getelementptr inbounds <2 x i64>, <2 x i64>* %223, i64 1
  %270 = load <2 x i64>, <2 x i64>* %269, align 16
  %271 = shufflevector <2 x i64> %268, <2 x i64> %270, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %272 = getelementptr inbounds <2 x i64>, <2 x i64>* %227, i64 1
  %273 = load <2 x i64>, <2 x i64>* %272, align 16
  %274 = getelementptr inbounds <2 x i64>, <2 x i64>* %230, i64 1
  %275 = load <2 x i64>, <2 x i64>* %274, align 16
  %276 = shufflevector <2 x i64> %273, <2 x i64> %275, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %277 = getelementptr inbounds <2 x i64>, <2 x i64>* %234, i64 1
  %278 = load <2 x i64>, <2 x i64>* %277, align 16
  %279 = getelementptr inbounds <2 x i64>, <2 x i64>* %237, i64 1
  %280 = load <2 x i64>, <2 x i64>* %279, align 16
  %281 = shufflevector <2 x i64> %278, <2 x i64> %280, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %282 = getelementptr inbounds <2 x i64>, <2 x i64>* %241, i64 1
  %283 = load <2 x i64>, <2 x i64>* %282, align 16
  %284 = getelementptr inbounds <2 x i64>, <2 x i64>* %244, i64 1
  %285 = load <2 x i64>, <2 x i64>* %284, align 16
  %286 = shufflevector <2 x i64> %283, <2 x i64> %285, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %287 = bitcast <4 x i64> %197 to <16 x i16>
  %288 = bitcast <4 x i64> %204 to <16 x i16>
  %289 = shufflevector <16 x i16> %287, <16 x i16> %288, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %290 = shufflevector <16 x i16> %287, <16 x i16> %288, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %291 = bitcast <4 x i64> %211 to <16 x i16>
  %292 = bitcast <4 x i64> %218 to <16 x i16>
  %293 = shufflevector <16 x i16> %291, <16 x i16> %292, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %294 = shufflevector <16 x i16> %291, <16 x i16> %292, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %295 = bitcast <4 x i64> %225 to <16 x i16>
  %296 = bitcast <4 x i64> %232 to <16 x i16>
  %297 = shufflevector <16 x i16> %295, <16 x i16> %296, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %298 = shufflevector <16 x i16> %295, <16 x i16> %296, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %299 = bitcast <4 x i64> %239 to <16 x i16>
  %300 = bitcast <4 x i64> %246 to <16 x i16>
  %301 = shufflevector <16 x i16> %299, <16 x i16> %300, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %302 = shufflevector <16 x i16> %299, <16 x i16> %300, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %303 = bitcast <16 x i16> %289 to <8 x i32>
  %304 = bitcast <16 x i16> %293 to <8 x i32>
  %305 = shufflevector <8 x i32> %303, <8 x i32> %304, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %306 = shufflevector <8 x i32> %303, <8 x i32> %304, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %307 = bitcast <16 x i16> %297 to <8 x i32>
  %308 = bitcast <16 x i16> %301 to <8 x i32>
  %309 = shufflevector <8 x i32> %307, <8 x i32> %308, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %310 = shufflevector <8 x i32> %307, <8 x i32> %308, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %311 = bitcast <16 x i16> %290 to <8 x i32>
  %312 = bitcast <16 x i16> %294 to <8 x i32>
  %313 = shufflevector <8 x i32> %311, <8 x i32> %312, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %314 = shufflevector <8 x i32> %311, <8 x i32> %312, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %315 = bitcast <16 x i16> %298 to <8 x i32>
  %316 = bitcast <16 x i16> %302 to <8 x i32>
  %317 = shufflevector <8 x i32> %315, <8 x i32> %316, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %318 = shufflevector <8 x i32> %315, <8 x i32> %316, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %319 = bitcast <8 x i32> %305 to <4 x i64>
  %320 = bitcast <8 x i32> %309 to <4 x i64>
  %321 = shufflevector <4 x i64> %319, <4 x i64> %320, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %321, <4 x i64>* %191, align 32
  %322 = shufflevector <4 x i64> %319, <4 x i64> %320, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %323 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 1
  store <4 x i64> %322, <4 x i64>* %323, align 32
  %324 = bitcast <8 x i32> %313 to <4 x i64>
  %325 = bitcast <8 x i32> %317 to <4 x i64>
  %326 = shufflevector <4 x i64> %324, <4 x i64> %325, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %327 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 4
  store <4 x i64> %326, <4 x i64>* %327, align 32
  %328 = shufflevector <4 x i64> %324, <4 x i64> %325, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %329 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 5
  store <4 x i64> %328, <4 x i64>* %329, align 32
  %330 = bitcast <8 x i32> %306 to <4 x i64>
  %331 = bitcast <8 x i32> %310 to <4 x i64>
  %332 = shufflevector <4 x i64> %330, <4 x i64> %331, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %333 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 2
  store <4 x i64> %332, <4 x i64>* %333, align 32
  %334 = shufflevector <4 x i64> %330, <4 x i64> %331, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %335 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 3
  store <4 x i64> %334, <4 x i64>* %335, align 32
  %336 = bitcast <8 x i32> %314 to <4 x i64>
  %337 = bitcast <8 x i32> %318 to <4 x i64>
  %338 = shufflevector <4 x i64> %336, <4 x i64> %337, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %339 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 6
  store <4 x i64> %338, <4 x i64>* %339, align 32
  %340 = shufflevector <4 x i64> %336, <4 x i64> %337, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %341 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 7
  store <4 x i64> %340, <4 x i64>* %341, align 32
  %342 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 8
  %343 = bitcast <4 x i64> %251 to <16 x i16>
  %344 = bitcast <4 x i64> %256 to <16 x i16>
  %345 = shufflevector <16 x i16> %343, <16 x i16> %344, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %346 = shufflevector <16 x i16> %343, <16 x i16> %344, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %347 = bitcast <4 x i64> %261 to <16 x i16>
  %348 = bitcast <4 x i64> %266 to <16 x i16>
  %349 = shufflevector <16 x i16> %347, <16 x i16> %348, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %350 = shufflevector <16 x i16> %347, <16 x i16> %348, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %351 = bitcast <4 x i64> %271 to <16 x i16>
  %352 = bitcast <4 x i64> %276 to <16 x i16>
  %353 = shufflevector <16 x i16> %351, <16 x i16> %352, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %354 = shufflevector <16 x i16> %351, <16 x i16> %352, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %355 = bitcast <4 x i64> %281 to <16 x i16>
  %356 = bitcast <4 x i64> %286 to <16 x i16>
  %357 = shufflevector <16 x i16> %355, <16 x i16> %356, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %358 = shufflevector <16 x i16> %355, <16 x i16> %356, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %359 = bitcast <16 x i16> %345 to <8 x i32>
  %360 = bitcast <16 x i16> %349 to <8 x i32>
  %361 = shufflevector <8 x i32> %359, <8 x i32> %360, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %362 = shufflevector <8 x i32> %359, <8 x i32> %360, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %363 = bitcast <16 x i16> %353 to <8 x i32>
  %364 = bitcast <16 x i16> %357 to <8 x i32>
  %365 = shufflevector <8 x i32> %363, <8 x i32> %364, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %366 = shufflevector <8 x i32> %363, <8 x i32> %364, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %367 = bitcast <16 x i16> %346 to <8 x i32>
  %368 = bitcast <16 x i16> %350 to <8 x i32>
  %369 = shufflevector <8 x i32> %367, <8 x i32> %368, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %370 = shufflevector <8 x i32> %367, <8 x i32> %368, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %371 = bitcast <16 x i16> %354 to <8 x i32>
  %372 = bitcast <16 x i16> %358 to <8 x i32>
  %373 = shufflevector <8 x i32> %371, <8 x i32> %372, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %374 = shufflevector <8 x i32> %371, <8 x i32> %372, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %375 = bitcast <8 x i32> %361 to <4 x i64>
  %376 = bitcast <8 x i32> %365 to <4 x i64>
  %377 = shufflevector <4 x i64> %375, <4 x i64> %376, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %377, <4 x i64>* %342, align 32
  %378 = shufflevector <4 x i64> %375, <4 x i64> %376, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %379 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 9
  store <4 x i64> %378, <4 x i64>* %379, align 32
  %380 = bitcast <8 x i32> %369 to <4 x i64>
  %381 = bitcast <8 x i32> %373 to <4 x i64>
  %382 = shufflevector <4 x i64> %380, <4 x i64> %381, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %383 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 12
  store <4 x i64> %382, <4 x i64>* %383, align 32
  %384 = shufflevector <4 x i64> %380, <4 x i64> %381, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %385 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 13
  store <4 x i64> %384, <4 x i64>* %385, align 32
  %386 = bitcast <8 x i32> %362 to <4 x i64>
  %387 = bitcast <8 x i32> %366 to <4 x i64>
  %388 = shufflevector <4 x i64> %386, <4 x i64> %387, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %389 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 10
  store <4 x i64> %388, <4 x i64>* %389, align 32
  %390 = shufflevector <4 x i64> %386, <4 x i64> %387, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %391 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 11
  store <4 x i64> %390, <4 x i64>* %391, align 32
  %392 = bitcast <8 x i32> %370 to <4 x i64>
  %393 = bitcast <8 x i32> %374 to <4 x i64>
  %394 = shufflevector <4 x i64> %392, <4 x i64> %393, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %395 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 14
  store <4 x i64> %394, <4 x i64>* %395, align 32
  %396 = shufflevector <4 x i64> %392, <4 x i64> %393, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %397 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 15
  store <4 x i64> %396, <4 x i64>* %397, align 32
  %398 = add nuw nsw i64 %186, 1
  %399 = icmp eq i64 %398, 2
  br i1 %399, label %182, label %185

400:                                              ; preds = %562
  call void @llvm.lifetime.end.p0i8(i64 8192, i8* nonnull %11) #8
  call void @llvm.lifetime.end.p0i8(i64 2048, i8* nonnull %10) #8
  ret void

401:                                              ; preds = %562, %18
  %402 = phi i64 [ 0, %18 ], [ %563, %562 ]
  call void @llvm.lifetime.start.p0i8(i64 2048, i8* nonnull %19) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %19, i8 -86, i64 2048, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 2048, i8* nonnull %20) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %20, i8 -86, i64 2048, i1 false)
  %403 = shl nsw i64 %402, 6
  %404 = getelementptr inbounds [256 x <4 x i64>], [256 x <4 x i64>]* %7, i64 0, i64 %403
  %405 = bitcast <4 x i64>* %404 to <2 x i64>*
  br label %529

406:                                              ; preds = %529
  call fastcc void @fdct64_new_avx2(<4 x i64>* nonnull %21, <4 x i64>* nonnull %21, i8 signext %14)
  call fastcc void @fdct64_new_avx2(<4 x i64>* nonnull %22, <4 x i64>* nonnull %22, i8 signext %14)
  %407 = load i8, i8* %23, align 1
  %408 = sext i8 %407 to i32
  %409 = sub nsw i32 0, %408
  %410 = icmp slt i8 %407, 0
  br i1 %410, label %411, label %443

411:                                              ; preds = %406
  %412 = xor i32 %408, -1
  %413 = shl i32 1, %412
  %414 = insertelement <8 x i32> undef, i32 %413, i32 0
  %415 = shufflevector <8 x i32> %414, <8 x i32> undef, <8 x i32> zeroinitializer
  br label %416

416:                                              ; preds = %416, %411
  %417 = phi i64 [ 0, %411 ], [ %441, %416 ]
  %418 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %417
  %419 = bitcast <4 x i64>* %418 to <8 x i32>*
  %420 = load <8 x i32>, <8 x i32>* %419, align 32
  %421 = add <8 x i32> %420, %415
  %422 = call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %421, i32 %409) #8
  store <8 x i32> %422, <8 x i32>* %419, align 32
  %423 = or i64 %417, 1
  %424 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %423
  %425 = bitcast <4 x i64>* %424 to <8 x i32>*
  %426 = load <8 x i32>, <8 x i32>* %425, align 32
  %427 = add <8 x i32> %426, %415
  %428 = call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %427, i32 %409) #8
  store <8 x i32> %428, <8 x i32>* %425, align 32
  %429 = or i64 %417, 2
  %430 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %429
  %431 = bitcast <4 x i64>* %430 to <8 x i32>*
  %432 = load <8 x i32>, <8 x i32>* %431, align 32
  %433 = add <8 x i32> %432, %415
  %434 = call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %433, i32 %409) #8
  store <8 x i32> %434, <8 x i32>* %431, align 32
  %435 = or i64 %417, 3
  %436 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %435
  %437 = bitcast <4 x i64>* %436 to <8 x i32>*
  %438 = load <8 x i32>, <8 x i32>* %437, align 32
  %439 = add <8 x i32> %438, %415
  %440 = call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %439, i32 %409) #8
  store <8 x i32> %440, <8 x i32>* %437, align 32
  %441 = add nuw nsw i64 %417, 4
  %442 = icmp eq i64 %441, 32
  br i1 %442, label %466, label %416

443:                                              ; preds = %406, %443
  %444 = phi i64 [ %464, %443 ], [ 0, %406 ]
  %445 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %444
  %446 = bitcast <4 x i64>* %445 to <8 x i32>*
  %447 = load <8 x i32>, <8 x i32>* %446, align 32
  %448 = call <8 x i32> @llvm.x86.avx2.pslli.d(<8 x i32> %447, i32 %408) #8
  store <8 x i32> %448, <8 x i32>* %446, align 32
  %449 = or i64 %444, 1
  %450 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %449
  %451 = bitcast <4 x i64>* %450 to <8 x i32>*
  %452 = load <8 x i32>, <8 x i32>* %451, align 32
  %453 = call <8 x i32> @llvm.x86.avx2.pslli.d(<8 x i32> %452, i32 %408) #8
  store <8 x i32> %453, <8 x i32>* %451, align 32
  %454 = or i64 %444, 2
  %455 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %454
  %456 = bitcast <4 x i64>* %455 to <8 x i32>*
  %457 = load <8 x i32>, <8 x i32>* %456, align 32
  %458 = call <8 x i32> @llvm.x86.avx2.pslli.d(<8 x i32> %457, i32 %408) #8
  store <8 x i32> %458, <8 x i32>* %456, align 32
  %459 = or i64 %444, 3
  %460 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %459
  %461 = bitcast <4 x i64>* %460 to <8 x i32>*
  %462 = load <8 x i32>, <8 x i32>* %461, align 32
  %463 = call <8 x i32> @llvm.x86.avx2.pslli.d(<8 x i32> %462, i32 %408) #8
  store <8 x i32> %463, <8 x i32>* %461, align 32
  %464 = add nuw nsw i64 %444, 4
  %465 = icmp eq i64 %464, 32
  br i1 %465, label %466, label %443

466:                                              ; preds = %443, %416
  %467 = load i8, i8* %23, align 1
  %468 = sext i8 %467 to i32
  %469 = sub nsw i32 0, %468
  %470 = icmp slt i8 %467, 0
  br i1 %470, label %471, label %503

471:                                              ; preds = %466
  %472 = xor i32 %468, -1
  %473 = shl i32 1, %472
  %474 = insertelement <8 x i32> undef, i32 %473, i32 0
  %475 = shufflevector <8 x i32> %474, <8 x i32> undef, <8 x i32> zeroinitializer
  br label %476

476:                                              ; preds = %476, %471
  %477 = phi i64 [ 0, %471 ], [ %501, %476 ]
  %478 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %9, i64 0, i64 %477
  %479 = bitcast <4 x i64>* %478 to <8 x i32>*
  %480 = load <8 x i32>, <8 x i32>* %479, align 32
  %481 = add <8 x i32> %480, %475
  %482 = call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %481, i32 %469) #8
  store <8 x i32> %482, <8 x i32>* %479, align 32
  %483 = or i64 %477, 1
  %484 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %9, i64 0, i64 %483
  %485 = bitcast <4 x i64>* %484 to <8 x i32>*
  %486 = load <8 x i32>, <8 x i32>* %485, align 32
  %487 = add <8 x i32> %486, %475
  %488 = call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %487, i32 %469) #8
  store <8 x i32> %488, <8 x i32>* %485, align 32
  %489 = or i64 %477, 2
  %490 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %9, i64 0, i64 %489
  %491 = bitcast <4 x i64>* %490 to <8 x i32>*
  %492 = load <8 x i32>, <8 x i32>* %491, align 32
  %493 = add <8 x i32> %492, %475
  %494 = call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %493, i32 %469) #8
  store <8 x i32> %494, <8 x i32>* %491, align 32
  %495 = or i64 %477, 3
  %496 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %9, i64 0, i64 %495
  %497 = bitcast <4 x i64>* %496 to <8 x i32>*
  %498 = load <8 x i32>, <8 x i32>* %497, align 32
  %499 = add <8 x i32> %498, %475
  %500 = call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %499, i32 %469) #8
  store <8 x i32> %500, <8 x i32>* %497, align 32
  %501 = add nuw nsw i64 %477, 4
  %502 = icmp eq i64 %501, 32
  br i1 %502, label %526, label %476

503:                                              ; preds = %466, %503
  %504 = phi i64 [ %524, %503 ], [ 0, %466 ]
  %505 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %9, i64 0, i64 %504
  %506 = bitcast <4 x i64>* %505 to <8 x i32>*
  %507 = load <8 x i32>, <8 x i32>* %506, align 32
  %508 = call <8 x i32> @llvm.x86.avx2.pslli.d(<8 x i32> %507, i32 %468) #8
  store <8 x i32> %508, <8 x i32>* %506, align 32
  %509 = or i64 %504, 1
  %510 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %9, i64 0, i64 %509
  %511 = bitcast <4 x i64>* %510 to <8 x i32>*
  %512 = load <8 x i32>, <8 x i32>* %511, align 32
  %513 = call <8 x i32> @llvm.x86.avx2.pslli.d(<8 x i32> %512, i32 %468) #8
  store <8 x i32> %513, <8 x i32>* %511, align 32
  %514 = or i64 %504, 2
  %515 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %9, i64 0, i64 %514
  %516 = bitcast <4 x i64>* %515 to <8 x i32>*
  %517 = load <8 x i32>, <8 x i32>* %516, align 32
  %518 = call <8 x i32> @llvm.x86.avx2.pslli.d(<8 x i32> %517, i32 %468) #8
  store <8 x i32> %518, <8 x i32>* %516, align 32
  %519 = or i64 %504, 3
  %520 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %9, i64 0, i64 %519
  %521 = bitcast <4 x i64>* %520 to <8 x i32>*
  %522 = load <8 x i32>, <8 x i32>* %521, align 32
  %523 = call <8 x i32> @llvm.x86.avx2.pslli.d(<8 x i32> %522, i32 %468) #8
  store <8 x i32> %523, <8 x i32>* %521, align 32
  %524 = add nuw nsw i64 %504, 4
  %525 = icmp eq i64 %524, 32
  br i1 %525, label %526, label %503

526:                                              ; preds = %503, %476
  %527 = shl nsw i64 %402, 9
  %528 = getelementptr inbounds i32, i32* %1, i64 %527
  br label %565

529:                                              ; preds = %529, %401
  %530 = phi i64 [ 0, %401 ], [ %560, %529 ]
  %531 = shl nuw nsw i64 %530, 1
  %532 = getelementptr inbounds <4 x i64>, <4 x i64>* %404, i64 %530
  %533 = bitcast <4 x i64>* %532 to <8 x i16>*
  %534 = load <8 x i16>, <8 x i16>* %533, align 32
  %535 = sext <8 x i16> %534 to <8 x i32>
  %536 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %530
  %537 = bitcast <4 x i64>* %536 to <8 x i32>*
  store <8 x i32> %535, <8 x i32>* %537, align 32
  %538 = or i64 %531, 1
  %539 = getelementptr inbounds <2 x i64>, <2 x i64>* %405, i64 %538
  %540 = bitcast <2 x i64>* %539 to <8 x i16>*
  %541 = load <8 x i16>, <8 x i16>* %540, align 16
  %542 = sext <8 x i16> %541 to <8 x i32>
  %543 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %9, i64 0, i64 %530
  %544 = bitcast <4 x i64>* %543 to <8 x i32>*
  store <8 x i32> %542, <8 x i32>* %544, align 32
  %545 = or i64 %530, 1
  %546 = shl nuw nsw i64 %545, 1
  %547 = getelementptr inbounds <4 x i64>, <4 x i64>* %404, i64 %545
  %548 = bitcast <4 x i64>* %547 to <8 x i16>*
  %549 = load <8 x i16>, <8 x i16>* %548, align 32
  %550 = sext <8 x i16> %549 to <8 x i32>
  %551 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %545
  %552 = bitcast <4 x i64>* %551 to <8 x i32>*
  store <8 x i32> %550, <8 x i32>* %552, align 32
  %553 = or i64 %546, 1
  %554 = getelementptr inbounds <2 x i64>, <2 x i64>* %405, i64 %553
  %555 = bitcast <2 x i64>* %554 to <8 x i16>*
  %556 = load <8 x i16>, <8 x i16>* %555, align 16
  %557 = sext <8 x i16> %556 to <8 x i32>
  %558 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %9, i64 0, i64 %545
  %559 = bitcast <4 x i64>* %558 to <8 x i32>*
  store <8 x i32> %557, <8 x i32>* %559, align 32
  %560 = add nuw nsw i64 %530, 2
  %561 = icmp eq i64 %560, 64
  br i1 %561, label %406, label %529

562:                                              ; preds = %565
  call void @llvm.lifetime.end.p0i8(i64 2048, i8* nonnull %20) #8
  call void @llvm.lifetime.end.p0i8(i64 2048, i8* nonnull %19) #8
  %563 = add nuw nsw i64 %402, 1
  %564 = icmp eq i64 %563, 2
  br i1 %564, label %400, label %401

565:                                              ; preds = %565, %526
  %566 = phi i64 [ 0, %526 ], [ %712, %565 ]
  %567 = shl nsw i64 %566, 3
  %568 = getelementptr inbounds i32, i32* %528, i64 %567
  %569 = bitcast i32* %568 to <4 x i64>*
  %570 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %567
  %571 = bitcast <4 x i64>* %570 to <8 x i32>*
  %572 = load <8 x i32>, <8 x i32>* %571, align 32
  %573 = getelementptr inbounds <4 x i64>, <4 x i64>* %570, i64 2
  %574 = bitcast <4 x i64>* %573 to <8 x i32>*
  %575 = load <8 x i32>, <8 x i32>* %574, align 32
  %576 = shufflevector <8 x i32> %572, <8 x i32> %575, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %577 = shufflevector <8 x i32> %572, <8 x i32> %575, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %578 = getelementptr inbounds <4 x i64>, <4 x i64>* %570, i64 1
  %579 = bitcast <4 x i64>* %578 to <8 x i32>*
  %580 = load <8 x i32>, <8 x i32>* %579, align 32
  %581 = getelementptr inbounds <4 x i64>, <4 x i64>* %570, i64 3
  %582 = bitcast <4 x i64>* %581 to <8 x i32>*
  %583 = load <8 x i32>, <8 x i32>* %582, align 32
  %584 = shufflevector <8 x i32> %580, <8 x i32> %583, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %585 = shufflevector <8 x i32> %580, <8 x i32> %583, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %586 = getelementptr inbounds <4 x i64>, <4 x i64>* %570, i64 4
  %587 = bitcast <4 x i64>* %586 to <8 x i32>*
  %588 = load <8 x i32>, <8 x i32>* %587, align 32
  %589 = getelementptr inbounds <4 x i64>, <4 x i64>* %570, i64 6
  %590 = bitcast <4 x i64>* %589 to <8 x i32>*
  %591 = load <8 x i32>, <8 x i32>* %590, align 32
  %592 = shufflevector <8 x i32> %588, <8 x i32> %591, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %593 = shufflevector <8 x i32> %588, <8 x i32> %591, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %594 = getelementptr inbounds <4 x i64>, <4 x i64>* %570, i64 5
  %595 = bitcast <4 x i64>* %594 to <8 x i32>*
  %596 = load <8 x i32>, <8 x i32>* %595, align 32
  %597 = getelementptr inbounds <4 x i64>, <4 x i64>* %570, i64 7
  %598 = bitcast <4 x i64>* %597 to <8 x i32>*
  %599 = load <8 x i32>, <8 x i32>* %598, align 32
  %600 = shufflevector <8 x i32> %596, <8 x i32> %599, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %601 = shufflevector <8 x i32> %596, <8 x i32> %599, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %602 = shufflevector <8 x i32> %576, <8 x i32> %584, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %603 = bitcast <8 x i32> %602 to <4 x i64>
  %604 = shufflevector <8 x i32> %576, <8 x i32> %584, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %605 = bitcast <8 x i32> %604 to <4 x i64>
  %606 = shufflevector <8 x i32> %577, <8 x i32> %585, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %607 = bitcast <8 x i32> %606 to <4 x i64>
  %608 = shufflevector <8 x i32> %577, <8 x i32> %585, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %609 = bitcast <8 x i32> %608 to <4 x i64>
  %610 = shufflevector <8 x i32> %592, <8 x i32> %600, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %611 = bitcast <8 x i32> %610 to <4 x i64>
  %612 = shufflevector <8 x i32> %592, <8 x i32> %600, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %613 = bitcast <8 x i32> %612 to <4 x i64>
  %614 = shufflevector <8 x i32> %593, <8 x i32> %601, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %615 = bitcast <8 x i32> %614 to <4 x i64>
  %616 = shufflevector <8 x i32> %593, <8 x i32> %601, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %617 = bitcast <8 x i32> %616 to <4 x i64>
  %618 = shufflevector <4 x i64> %603, <4 x i64> %611, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
  store <4 x i64> %618, <4 x i64>* %569, align 32
  %619 = shufflevector <4 x i64> %605, <4 x i64> %613, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
  %620 = getelementptr inbounds i32, i32* %568, i64 32
  %621 = bitcast i32* %620 to <4 x i64>*
  store <4 x i64> %619, <4 x i64>* %621, align 32
  %622 = shufflevector <4 x i64> %607, <4 x i64> %615, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
  %623 = getelementptr inbounds i32, i32* %568, i64 64
  %624 = bitcast i32* %623 to <4 x i64>*
  store <4 x i64> %622, <4 x i64>* %624, align 32
  %625 = shufflevector <4 x i64> %609, <4 x i64> %617, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
  %626 = getelementptr inbounds i32, i32* %568, i64 96
  %627 = bitcast i32* %626 to <4 x i64>*
  store <4 x i64> %625, <4 x i64>* %627, align 32
  %628 = shufflevector <4 x i64> %603, <4 x i64> %611, <4 x i32> <i32 2, i32 3, i32 6, i32 7>
  %629 = getelementptr inbounds i32, i32* %568, i64 128
  %630 = bitcast i32* %629 to <4 x i64>*
  store <4 x i64> %628, <4 x i64>* %630, align 32
  %631 = shufflevector <4 x i64> %605, <4 x i64> %613, <4 x i32> <i32 2, i32 3, i32 6, i32 7>
  %632 = getelementptr inbounds i32, i32* %568, i64 160
  %633 = bitcast i32* %632 to <4 x i64>*
  store <4 x i64> %631, <4 x i64>* %633, align 32
  %634 = shufflevector <4 x i64> %607, <4 x i64> %615, <4 x i32> <i32 2, i32 3, i32 6, i32 7>
  %635 = getelementptr inbounds i32, i32* %568, i64 192
  %636 = bitcast i32* %635 to <4 x i64>*
  store <4 x i64> %634, <4 x i64>* %636, align 32
  %637 = shufflevector <4 x i64> %609, <4 x i64> %617, <4 x i32> <i32 2, i32 3, i32 6, i32 7>
  %638 = getelementptr inbounds i32, i32* %568, i64 224
  %639 = bitcast i32* %638 to <4 x i64>*
  store <4 x i64> %637, <4 x i64>* %639, align 32
  %640 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %9, i64 0, i64 %567
  %641 = getelementptr inbounds i32, i32* %568, i64 256
  %642 = bitcast i32* %641 to <4 x i64>*
  %643 = bitcast <4 x i64>* %640 to <8 x i32>*
  %644 = load <8 x i32>, <8 x i32>* %643, align 32
  %645 = getelementptr inbounds <4 x i64>, <4 x i64>* %640, i64 2
  %646 = bitcast <4 x i64>* %645 to <8 x i32>*
  %647 = load <8 x i32>, <8 x i32>* %646, align 32
  %648 = shufflevector <8 x i32> %644, <8 x i32> %647, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %649 = shufflevector <8 x i32> %644, <8 x i32> %647, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %650 = getelementptr inbounds <4 x i64>, <4 x i64>* %640, i64 1
  %651 = bitcast <4 x i64>* %650 to <8 x i32>*
  %652 = load <8 x i32>, <8 x i32>* %651, align 32
  %653 = getelementptr inbounds <4 x i64>, <4 x i64>* %640, i64 3
  %654 = bitcast <4 x i64>* %653 to <8 x i32>*
  %655 = load <8 x i32>, <8 x i32>* %654, align 32
  %656 = shufflevector <8 x i32> %652, <8 x i32> %655, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %657 = shufflevector <8 x i32> %652, <8 x i32> %655, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %658 = getelementptr inbounds <4 x i64>, <4 x i64>* %640, i64 4
  %659 = bitcast <4 x i64>* %658 to <8 x i32>*
  %660 = load <8 x i32>, <8 x i32>* %659, align 32
  %661 = getelementptr inbounds <4 x i64>, <4 x i64>* %640, i64 6
  %662 = bitcast <4 x i64>* %661 to <8 x i32>*
  %663 = load <8 x i32>, <8 x i32>* %662, align 32
  %664 = shufflevector <8 x i32> %660, <8 x i32> %663, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %665 = shufflevector <8 x i32> %660, <8 x i32> %663, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %666 = getelementptr inbounds <4 x i64>, <4 x i64>* %640, i64 5
  %667 = bitcast <4 x i64>* %666 to <8 x i32>*
  %668 = load <8 x i32>, <8 x i32>* %667, align 32
  %669 = getelementptr inbounds <4 x i64>, <4 x i64>* %640, i64 7
  %670 = bitcast <4 x i64>* %669 to <8 x i32>*
  %671 = load <8 x i32>, <8 x i32>* %670, align 32
  %672 = shufflevector <8 x i32> %668, <8 x i32> %671, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %673 = shufflevector <8 x i32> %668, <8 x i32> %671, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %674 = shufflevector <8 x i32> %648, <8 x i32> %656, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %675 = bitcast <8 x i32> %674 to <4 x i64>
  %676 = shufflevector <8 x i32> %648, <8 x i32> %656, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %677 = bitcast <8 x i32> %676 to <4 x i64>
  %678 = shufflevector <8 x i32> %649, <8 x i32> %657, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %679 = bitcast <8 x i32> %678 to <4 x i64>
  %680 = shufflevector <8 x i32> %649, <8 x i32> %657, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %681 = bitcast <8 x i32> %680 to <4 x i64>
  %682 = shufflevector <8 x i32> %664, <8 x i32> %672, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %683 = bitcast <8 x i32> %682 to <4 x i64>
  %684 = shufflevector <8 x i32> %664, <8 x i32> %672, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %685 = bitcast <8 x i32> %684 to <4 x i64>
  %686 = shufflevector <8 x i32> %665, <8 x i32> %673, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %687 = bitcast <8 x i32> %686 to <4 x i64>
  %688 = shufflevector <8 x i32> %665, <8 x i32> %673, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %689 = bitcast <8 x i32> %688 to <4 x i64>
  %690 = shufflevector <4 x i64> %675, <4 x i64> %683, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
  store <4 x i64> %690, <4 x i64>* %642, align 32
  %691 = shufflevector <4 x i64> %677, <4 x i64> %685, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
  %692 = getelementptr inbounds i32, i32* %641, i64 32
  %693 = bitcast i32* %692 to <4 x i64>*
  store <4 x i64> %691, <4 x i64>* %693, align 32
  %694 = shufflevector <4 x i64> %679, <4 x i64> %687, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
  %695 = getelementptr inbounds i32, i32* %641, i64 64
  %696 = bitcast i32* %695 to <4 x i64>*
  store <4 x i64> %694, <4 x i64>* %696, align 32
  %697 = shufflevector <4 x i64> %681, <4 x i64> %689, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
  %698 = getelementptr inbounds i32, i32* %641, i64 96
  %699 = bitcast i32* %698 to <4 x i64>*
  store <4 x i64> %697, <4 x i64>* %699, align 32
  %700 = shufflevector <4 x i64> %675, <4 x i64> %683, <4 x i32> <i32 2, i32 3, i32 6, i32 7>
  %701 = getelementptr inbounds i32, i32* %641, i64 128
  %702 = bitcast i32* %701 to <4 x i64>*
  store <4 x i64> %700, <4 x i64>* %702, align 32
  %703 = shufflevector <4 x i64> %677, <4 x i64> %685, <4 x i32> <i32 2, i32 3, i32 6, i32 7>
  %704 = getelementptr inbounds i32, i32* %641, i64 160
  %705 = bitcast i32* %704 to <4 x i64>*
  store <4 x i64> %703, <4 x i64>* %705, align 32
  %706 = shufflevector <4 x i64> %679, <4 x i64> %687, <4 x i32> <i32 2, i32 3, i32 6, i32 7>
  %707 = getelementptr inbounds i32, i32* %641, i64 192
  %708 = bitcast i32* %707 to <4 x i64>*
  store <4 x i64> %706, <4 x i64>* %708, align 32
  %709 = shufflevector <4 x i64> %681, <4 x i64> %689, <4 x i32> <i32 2, i32 3, i32 6, i32 7>
  %710 = getelementptr inbounds i32, i32* %641, i64 224
  %711 = bitcast i32* %710 to <4 x i64>*
  store <4 x i64> %709, <4 x i64>* %711, align 32
  %712 = add nuw nsw i64 %566, 1
  %713 = icmp eq i64 %712, 4
  br i1 %713, label %562, label %565
}

declare void @av1_lowbd_fwd_txfm2d_4x8_sse2(i16*, i32*, i32, i8 zeroext, i32) #2

declare void @av1_lowbd_fwd_txfm2d_8x4_sse2(i16*, i32*, i32, i8 zeroext, i32) #2

; Function Attrs: nounwind ssp uwtable
define internal void @lowbd_fwd_txfm2d_8x16_avx2(i16* nocapture readonly, i32*, i32, i8 zeroext, i32) #3 {
  %6 = alloca [16 x <2 x i64>], align 16
  %7 = alloca [16 x <2 x i64>], align 16
  %8 = alloca [8 x <4 x i64>], align 32
  %9 = bitcast [16 x <2 x i64>]* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %9) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %9, i8 -86, i64 256, i1 false)
  %10 = bitcast [16 x <2 x i64>]* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %10) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %10, i8 -86, i64 256, i1 false)
  %11 = bitcast [8 x <4 x i64>]* %8 to i8*
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %11) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %11, i8 -86, i64 256, i1 false)
  %12 = load i8*, i8** getelementptr inbounds ([19 x i8*], [19 x i8*]* @av1_fwd_txfm_shift_ls, i64 0, i64 7), align 8
  %13 = load i8, i8* getelementptr inbounds ([5 x [5 x i8]], [5 x [5 x i8]]* @av1_fwd_cos_bit_col, i64 0, i64 1, i64 2), align 1
  %14 = load i8, i8* getelementptr inbounds ([5 x [5 x i8]], [5 x [5 x i8]]* @av1_fwd_cos_bit_row, i64 0, i64 1, i64 2), align 1
  %15 = zext i8 %3 to i64
  %16 = getelementptr inbounds [16 x void (<2 x i64>*, <2 x i64>*, i8)*], [16 x void (<2 x i64>*, <2 x i64>*, i8)*]* @col_txfm8x16_arr, i64 0, i64 %15
  %17 = load void (<2 x i64>*, <2 x i64>*, i8)*, void (<2 x i64>*, <2 x i64>*, i8)** %16, align 8
  %18 = getelementptr inbounds [16 x void (<4 x i64>*, <4 x i64>*, i8)*], [16 x void (<4 x i64>*, <4 x i64>*, i8)*]* @row_txfm8x16_arr, i64 0, i64 %15
  %19 = load void (<4 x i64>*, <4 x i64>*, i8)*, void (<4 x i64>*, <4 x i64>*, i8)** %18, align 8
  switch i8 %3, label %102 [
    i8 6, label %21
    i8 15, label %20
    i8 7, label %20
    i8 5, label %20
    i8 14, label %22
    i8 8, label %22
    i8 4, label %22
  ]

20:                                               ; preds = %5, %5, %5
  br label %102

21:                                               ; preds = %5
  br label %22

22:                                               ; preds = %5, %5, %5, %21
  %23 = phi i32 [ 1, %21 ], [ 0, %5 ], [ 0, %5 ], [ 0, %5 ]
  %24 = sext i32 %2 to i64
  %25 = bitcast i16* %0 to <2 x i64>*
  %26 = load <2 x i64>, <2 x i64>* %25, align 16
  %27 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 15
  store <2 x i64> %26, <2 x i64>* %27, align 16
  %28 = getelementptr inbounds i16, i16* %0, i64 %24
  %29 = bitcast i16* %28 to <2 x i64>*
  %30 = load <2 x i64>, <2 x i64>* %29, align 16
  %31 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 14
  store <2 x i64> %30, <2 x i64>* %31, align 16
  %32 = shl nsw i64 %24, 1
  %33 = getelementptr inbounds i16, i16* %0, i64 %32
  %34 = bitcast i16* %33 to <2 x i64>*
  %35 = load <2 x i64>, <2 x i64>* %34, align 16
  %36 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 13
  store <2 x i64> %35, <2 x i64>* %36, align 16
  %37 = mul nsw i64 %24, 3
  %38 = getelementptr inbounds i16, i16* %0, i64 %37
  %39 = bitcast i16* %38 to <2 x i64>*
  %40 = load <2 x i64>, <2 x i64>* %39, align 16
  %41 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 12
  store <2 x i64> %40, <2 x i64>* %41, align 16
  %42 = shl nsw i64 %24, 2
  %43 = getelementptr inbounds i16, i16* %0, i64 %42
  %44 = bitcast i16* %43 to <2 x i64>*
  %45 = load <2 x i64>, <2 x i64>* %44, align 16
  %46 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 11
  store <2 x i64> %45, <2 x i64>* %46, align 16
  %47 = mul nsw i64 %24, 5
  %48 = getelementptr inbounds i16, i16* %0, i64 %47
  %49 = bitcast i16* %48 to <2 x i64>*
  %50 = load <2 x i64>, <2 x i64>* %49, align 16
  %51 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 10
  store <2 x i64> %50, <2 x i64>* %51, align 16
  %52 = mul nsw i64 %24, 6
  %53 = getelementptr inbounds i16, i16* %0, i64 %52
  %54 = bitcast i16* %53 to <2 x i64>*
  %55 = load <2 x i64>, <2 x i64>* %54, align 16
  %56 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 9
  store <2 x i64> %55, <2 x i64>* %56, align 16
  %57 = mul nsw i64 %24, 7
  %58 = getelementptr inbounds i16, i16* %0, i64 %57
  %59 = bitcast i16* %58 to <2 x i64>*
  %60 = load <2 x i64>, <2 x i64>* %59, align 16
  %61 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 8
  store <2 x i64> %60, <2 x i64>* %61, align 16
  %62 = shl nsw i64 %24, 3
  %63 = getelementptr inbounds i16, i16* %0, i64 %62
  %64 = bitcast i16* %63 to <2 x i64>*
  %65 = load <2 x i64>, <2 x i64>* %64, align 16
  %66 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 7
  store <2 x i64> %65, <2 x i64>* %66, align 16
  %67 = mul nsw i64 %24, 9
  %68 = getelementptr inbounds i16, i16* %0, i64 %67
  %69 = bitcast i16* %68 to <2 x i64>*
  %70 = load <2 x i64>, <2 x i64>* %69, align 16
  %71 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 6
  store <2 x i64> %70, <2 x i64>* %71, align 16
  %72 = mul nsw i64 %24, 10
  %73 = getelementptr inbounds i16, i16* %0, i64 %72
  %74 = bitcast i16* %73 to <2 x i64>*
  %75 = load <2 x i64>, <2 x i64>* %74, align 16
  %76 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 5
  store <2 x i64> %75, <2 x i64>* %76, align 16
  %77 = mul nsw i64 %24, 11
  %78 = getelementptr inbounds i16, i16* %0, i64 %77
  %79 = bitcast i16* %78 to <2 x i64>*
  %80 = load <2 x i64>, <2 x i64>* %79, align 16
  %81 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 4
  store <2 x i64> %80, <2 x i64>* %81, align 16
  %82 = mul nsw i64 %24, 12
  %83 = getelementptr inbounds i16, i16* %0, i64 %82
  %84 = bitcast i16* %83 to <2 x i64>*
  %85 = load <2 x i64>, <2 x i64>* %84, align 16
  %86 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 3
  store <2 x i64> %85, <2 x i64>* %86, align 16
  %87 = mul nsw i64 %24, 13
  %88 = getelementptr inbounds i16, i16* %0, i64 %87
  %89 = bitcast i16* %88 to <2 x i64>*
  %90 = load <2 x i64>, <2 x i64>* %89, align 16
  %91 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 2
  store <2 x i64> %90, <2 x i64>* %91, align 16
  %92 = mul nsw i64 %24, 14
  %93 = getelementptr inbounds i16, i16* %0, i64 %92
  %94 = bitcast i16* %93 to <2 x i64>*
  %95 = load <2 x i64>, <2 x i64>* %94, align 16
  %96 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 1
  store <2 x i64> %95, <2 x i64>* %96, align 16
  %97 = mul nsw i64 %24, 15
  %98 = getelementptr inbounds i16, i16* %0, i64 %97
  %99 = bitcast i16* %98 to <2 x i64>*
  %100 = load <2 x i64>, <2 x i64>* %99, align 16
  %101 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 0
  store <2 x i64> %100, <2 x i64>* %101, align 16
  br label %182

102:                                              ; preds = %20, %5
  %103 = phi i32 [ 0, %5 ], [ 1, %20 ]
  %104 = sext i32 %2 to i64
  %105 = bitcast i16* %0 to <2 x i64>*
  %106 = load <2 x i64>, <2 x i64>* %105, align 16
  %107 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 0
  store <2 x i64> %106, <2 x i64>* %107, align 16
  %108 = getelementptr inbounds i16, i16* %0, i64 %104
  %109 = bitcast i16* %108 to <2 x i64>*
  %110 = load <2 x i64>, <2 x i64>* %109, align 16
  %111 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 1
  store <2 x i64> %110, <2 x i64>* %111, align 16
  %112 = shl nsw i64 %104, 1
  %113 = getelementptr inbounds i16, i16* %0, i64 %112
  %114 = bitcast i16* %113 to <2 x i64>*
  %115 = load <2 x i64>, <2 x i64>* %114, align 16
  %116 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 2
  store <2 x i64> %115, <2 x i64>* %116, align 16
  %117 = mul nsw i64 %104, 3
  %118 = getelementptr inbounds i16, i16* %0, i64 %117
  %119 = bitcast i16* %118 to <2 x i64>*
  %120 = load <2 x i64>, <2 x i64>* %119, align 16
  %121 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 3
  store <2 x i64> %120, <2 x i64>* %121, align 16
  %122 = shl nsw i64 %104, 2
  %123 = getelementptr inbounds i16, i16* %0, i64 %122
  %124 = bitcast i16* %123 to <2 x i64>*
  %125 = load <2 x i64>, <2 x i64>* %124, align 16
  %126 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 4
  store <2 x i64> %125, <2 x i64>* %126, align 16
  %127 = mul nsw i64 %104, 5
  %128 = getelementptr inbounds i16, i16* %0, i64 %127
  %129 = bitcast i16* %128 to <2 x i64>*
  %130 = load <2 x i64>, <2 x i64>* %129, align 16
  %131 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 5
  store <2 x i64> %130, <2 x i64>* %131, align 16
  %132 = mul nsw i64 %104, 6
  %133 = getelementptr inbounds i16, i16* %0, i64 %132
  %134 = bitcast i16* %133 to <2 x i64>*
  %135 = load <2 x i64>, <2 x i64>* %134, align 16
  %136 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 6
  store <2 x i64> %135, <2 x i64>* %136, align 16
  %137 = mul nsw i64 %104, 7
  %138 = getelementptr inbounds i16, i16* %0, i64 %137
  %139 = bitcast i16* %138 to <2 x i64>*
  %140 = load <2 x i64>, <2 x i64>* %139, align 16
  %141 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 7
  store <2 x i64> %140, <2 x i64>* %141, align 16
  %142 = shl nsw i64 %104, 3
  %143 = getelementptr inbounds i16, i16* %0, i64 %142
  %144 = bitcast i16* %143 to <2 x i64>*
  %145 = load <2 x i64>, <2 x i64>* %144, align 16
  %146 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 8
  store <2 x i64> %145, <2 x i64>* %146, align 16
  %147 = mul nsw i64 %104, 9
  %148 = getelementptr inbounds i16, i16* %0, i64 %147
  %149 = bitcast i16* %148 to <2 x i64>*
  %150 = load <2 x i64>, <2 x i64>* %149, align 16
  %151 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 9
  store <2 x i64> %150, <2 x i64>* %151, align 16
  %152 = mul nsw i64 %104, 10
  %153 = getelementptr inbounds i16, i16* %0, i64 %152
  %154 = bitcast i16* %153 to <2 x i64>*
  %155 = load <2 x i64>, <2 x i64>* %154, align 16
  %156 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 10
  store <2 x i64> %155, <2 x i64>* %156, align 16
  %157 = mul nsw i64 %104, 11
  %158 = getelementptr inbounds i16, i16* %0, i64 %157
  %159 = bitcast i16* %158 to <2 x i64>*
  %160 = load <2 x i64>, <2 x i64>* %159, align 16
  %161 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 11
  store <2 x i64> %160, <2 x i64>* %161, align 16
  %162 = mul nsw i64 %104, 12
  %163 = getelementptr inbounds i16, i16* %0, i64 %162
  %164 = bitcast i16* %163 to <2 x i64>*
  %165 = load <2 x i64>, <2 x i64>* %164, align 16
  %166 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 12
  store <2 x i64> %165, <2 x i64>* %166, align 16
  %167 = mul nsw i64 %104, 13
  %168 = getelementptr inbounds i16, i16* %0, i64 %167
  %169 = bitcast i16* %168 to <2 x i64>*
  %170 = load <2 x i64>, <2 x i64>* %169, align 16
  %171 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 13
  store <2 x i64> %170, <2 x i64>* %171, align 16
  %172 = mul nsw i64 %104, 14
  %173 = getelementptr inbounds i16, i16* %0, i64 %172
  %174 = bitcast i16* %173 to <2 x i64>*
  %175 = load <2 x i64>, <2 x i64>* %174, align 16
  %176 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 14
  store <2 x i64> %175, <2 x i64>* %176, align 16
  %177 = mul nsw i64 %104, 15
  %178 = getelementptr inbounds i16, i16* %0, i64 %177
  %179 = bitcast i16* %178 to <2 x i64>*
  %180 = load <2 x i64>, <2 x i64>* %179, align 16
  %181 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 15
  store <2 x i64> %180, <2 x i64>* %181, align 16
  br label %182

182:                                              ; preds = %22, %102
  %183 = phi <2 x i64> [ %150, %102 ], [ %55, %22 ]
  %184 = phi <2 x i64> [ %145, %102 ], [ %60, %22 ]
  %185 = phi <2 x i64> [ %140, %102 ], [ %65, %22 ]
  %186 = phi <2 x i64> [ %135, %102 ], [ %70, %22 ]
  %187 = phi <2 x i64> [ %130, %102 ], [ %75, %22 ]
  %188 = phi <2 x i64> [ %125, %102 ], [ %80, %22 ]
  %189 = phi <2 x i64> [ %120, %102 ], [ %85, %22 ]
  %190 = phi <2 x i64> [ %165, %102 ], [ %40, %22 ]
  %191 = phi <2 x i64> [ %160, %102 ], [ %45, %22 ]
  %192 = phi <2 x i64> [ %155, %102 ], [ %50, %22 ]
  %193 = phi <2 x i64> [ %115, %102 ], [ %90, %22 ]
  %194 = phi <2 x i64> [ %110, %102 ], [ %95, %22 ]
  %195 = phi <2 x i64>* [ %107, %102 ], [ %101, %22 ]
  %196 = phi i32 [ %103, %102 ], [ %23, %22 ]
  %197 = bitcast <2 x i64> %189 to <8 x i16>
  %198 = bitcast <2 x i64> %188 to <8 x i16>
  %199 = bitcast <2 x i64> %187 to <8 x i16>
  %200 = bitcast <2 x i64> %186 to <8 x i16>
  %201 = bitcast <2 x i64> %185 to <8 x i16>
  %202 = bitcast <2 x i64> %184 to <8 x i16>
  %203 = bitcast <2 x i64> %183 to <8 x i16>
  %204 = bitcast <2 x i64> %190 to <8 x i16>
  %205 = bitcast <2 x i64> %191 to <8 x i16>
  %206 = bitcast <2 x i64> %192 to <8 x i16>
  %207 = bitcast <2 x i64> %193 to <8 x i16>
  %208 = bitcast <2 x i64> %194 to <8 x i16>
  %209 = load i8, i8* %12, align 1
  %210 = sext i8 %209 to i32
  %211 = icmp slt i8 %209, 0
  br i1 %211, label %212, label %291

212:                                              ; preds = %182
  %213 = sub nsw i32 0, %210
  %214 = xor i32 %210, -1
  %215 = shl i32 1, %214
  %216 = trunc i32 %215 to i16
  %217 = insertelement <8 x i16> undef, i16 %216, i32 0
  %218 = shufflevector <8 x i16> %217, <8 x i16> undef, <8 x i32> zeroinitializer
  %219 = bitcast <2 x i64>* %195 to <8 x i16>*
  %220 = load <8 x i16>, <8 x i16>* %219, align 16
  %221 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %220, <8 x i16> %218) #8
  %222 = tail call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %221, i32 %213) #8
  store <8 x i16> %222, <8 x i16>* %219, align 16
  %223 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 1
  %224 = bitcast <2 x i64>* %223 to <8 x i16>*
  %225 = load <8 x i16>, <8 x i16>* %224, align 16
  %226 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %225, <8 x i16> %218) #8
  %227 = tail call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %226, i32 %213) #8
  store <8 x i16> %227, <8 x i16>* %224, align 16
  %228 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 2
  %229 = bitcast <2 x i64>* %228 to <8 x i16>*
  %230 = load <8 x i16>, <8 x i16>* %229, align 16
  %231 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %230, <8 x i16> %218) #8
  %232 = tail call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %231, i32 %213) #8
  store <8 x i16> %232, <8 x i16>* %229, align 16
  %233 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 3
  %234 = bitcast <2 x i64>* %233 to <8 x i16>*
  %235 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %197, <8 x i16> %218) #8
  %236 = tail call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %235, i32 %213) #8
  store <8 x i16> %236, <8 x i16>* %234, align 16
  %237 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 4
  %238 = bitcast <2 x i64>* %237 to <8 x i16>*
  %239 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %198, <8 x i16> %218) #8
  %240 = tail call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %239, i32 %213) #8
  store <8 x i16> %240, <8 x i16>* %238, align 16
  %241 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 5
  %242 = bitcast <2 x i64>* %241 to <8 x i16>*
  %243 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %199, <8 x i16> %218) #8
  %244 = tail call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %243, i32 %213) #8
  store <8 x i16> %244, <8 x i16>* %242, align 16
  %245 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 6
  %246 = bitcast <2 x i64>* %245 to <8 x i16>*
  %247 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %200, <8 x i16> %218) #8
  %248 = tail call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %247, i32 %213) #8
  store <8 x i16> %248, <8 x i16>* %246, align 16
  %249 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 7
  %250 = bitcast <2 x i64>* %249 to <8 x i16>*
  %251 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %201, <8 x i16> %218) #8
  %252 = tail call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %251, i32 %213) #8
  store <8 x i16> %252, <8 x i16>* %250, align 16
  %253 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 8
  %254 = bitcast <2 x i64>* %253 to <8 x i16>*
  %255 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %202, <8 x i16> %218) #8
  %256 = tail call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %255, i32 %213) #8
  store <8 x i16> %256, <8 x i16>* %254, align 16
  %257 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 9
  %258 = bitcast <2 x i64>* %257 to <8 x i16>*
  %259 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %203, <8 x i16> %218) #8
  %260 = tail call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %259, i32 %213) #8
  store <8 x i16> %260, <8 x i16>* %258, align 16
  %261 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 10
  %262 = bitcast <2 x i64>* %261 to <8 x i16>*
  %263 = load <8 x i16>, <8 x i16>* %262, align 16
  %264 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %263, <8 x i16> %218) #8
  %265 = tail call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %264, i32 %213) #8
  store <8 x i16> %265, <8 x i16>* %262, align 16
  %266 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 11
  %267 = bitcast <2 x i64>* %266 to <8 x i16>*
  %268 = load <8 x i16>, <8 x i16>* %267, align 16
  %269 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %268, <8 x i16> %218) #8
  %270 = tail call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %269, i32 %213) #8
  store <8 x i16> %270, <8 x i16>* %267, align 16
  %271 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 12
  %272 = bitcast <2 x i64>* %271 to <8 x i16>*
  %273 = load <8 x i16>, <8 x i16>* %272, align 16
  %274 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %273, <8 x i16> %218) #8
  %275 = tail call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %274, i32 %213) #8
  store <8 x i16> %275, <8 x i16>* %272, align 16
  %276 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 13
  %277 = bitcast <2 x i64>* %276 to <8 x i16>*
  %278 = load <8 x i16>, <8 x i16>* %277, align 16
  %279 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %278, <8 x i16> %218) #8
  %280 = tail call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %279, i32 %213) #8
  store <8 x i16> %280, <8 x i16>* %277, align 16
  %281 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 14
  %282 = bitcast <2 x i64>* %281 to <8 x i16>*
  %283 = load <8 x i16>, <8 x i16>* %282, align 16
  %284 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %283, <8 x i16> %218) #8
  %285 = tail call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %284, i32 %213) #8
  store <8 x i16> %285, <8 x i16>* %282, align 16
  %286 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 15
  %287 = bitcast <2 x i64>* %286 to <8 x i16>*
  %288 = load <8 x i16>, <8 x i16>* %287, align 16
  %289 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %288, <8 x i16> %218) #8
  %290 = tail call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %289, i32 %213) #8
  store <8 x i16> %290, <8 x i16>* %287, align 16
  br label %345

291:                                              ; preds = %182
  %292 = icmp eq i8 %209, 0
  br i1 %292, label %345, label %293

293:                                              ; preds = %291
  %294 = bitcast <2 x i64>* %195 to <8 x i16>*
  %295 = load <8 x i16>, <8 x i16>* %294, align 16
  %296 = tail call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %295, i32 %210) #8
  store <8 x i16> %296, <8 x i16>* %294, align 16
  %297 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 1
  %298 = bitcast <2 x i64>* %297 to <8 x i16>*
  %299 = tail call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %208, i32 %210) #8
  store <8 x i16> %299, <8 x i16>* %298, align 16
  %300 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 2
  %301 = bitcast <2 x i64>* %300 to <8 x i16>*
  %302 = tail call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %207, i32 %210) #8
  store <8 x i16> %302, <8 x i16>* %301, align 16
  %303 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 3
  %304 = bitcast <2 x i64>* %303 to <8 x i16>*
  %305 = tail call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %197, i32 %210) #8
  store <8 x i16> %305, <8 x i16>* %304, align 16
  %306 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 4
  %307 = bitcast <2 x i64>* %306 to <8 x i16>*
  %308 = tail call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %198, i32 %210) #8
  store <8 x i16> %308, <8 x i16>* %307, align 16
  %309 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 5
  %310 = bitcast <2 x i64>* %309 to <8 x i16>*
  %311 = tail call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %199, i32 %210) #8
  store <8 x i16> %311, <8 x i16>* %310, align 16
  %312 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 6
  %313 = bitcast <2 x i64>* %312 to <8 x i16>*
  %314 = tail call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %200, i32 %210) #8
  store <8 x i16> %314, <8 x i16>* %313, align 16
  %315 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 7
  %316 = bitcast <2 x i64>* %315 to <8 x i16>*
  %317 = tail call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %201, i32 %210) #8
  store <8 x i16> %317, <8 x i16>* %316, align 16
  %318 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 8
  %319 = bitcast <2 x i64>* %318 to <8 x i16>*
  %320 = tail call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %202, i32 %210) #8
  store <8 x i16> %320, <8 x i16>* %319, align 16
  %321 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 9
  %322 = bitcast <2 x i64>* %321 to <8 x i16>*
  %323 = tail call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %203, i32 %210) #8
  store <8 x i16> %323, <8 x i16>* %322, align 16
  %324 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 10
  %325 = bitcast <2 x i64>* %324 to <8 x i16>*
  %326 = tail call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %206, i32 %210) #8
  store <8 x i16> %326, <8 x i16>* %325, align 16
  %327 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 11
  %328 = bitcast <2 x i64>* %327 to <8 x i16>*
  %329 = tail call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %205, i32 %210) #8
  store <8 x i16> %329, <8 x i16>* %328, align 16
  %330 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 12
  %331 = bitcast <2 x i64>* %330 to <8 x i16>*
  %332 = tail call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %204, i32 %210) #8
  store <8 x i16> %332, <8 x i16>* %331, align 16
  %333 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 13
  %334 = bitcast <2 x i64>* %333 to <8 x i16>*
  %335 = load <8 x i16>, <8 x i16>* %334, align 16
  %336 = tail call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %335, i32 %210) #8
  store <8 x i16> %336, <8 x i16>* %334, align 16
  %337 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 14
  %338 = bitcast <2 x i64>* %337 to <8 x i16>*
  %339 = load <8 x i16>, <8 x i16>* %338, align 16
  %340 = tail call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %339, i32 %210) #8
  store <8 x i16> %340, <8 x i16>* %338, align 16
  %341 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 15
  %342 = bitcast <2 x i64>* %341 to <8 x i16>*
  %343 = load <8 x i16>, <8 x i16>* %342, align 16
  %344 = tail call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %343, i32 %210) #8
  store <8 x i16> %344, <8 x i16>* %342, align 16
  br label %345

345:                                              ; preds = %212, %291, %293
  call void %17(<2 x i64>* %195, <2 x i64>* %195, i8 signext %13) #8
  %346 = getelementptr inbounds i8, i8* %12, i64 1
  %347 = load i8, i8* %346, align 1
  %348 = sext i8 %347 to i32
  %349 = icmp slt i8 %347, 0
  br i1 %349, label %350, label %436

350:                                              ; preds = %345
  %351 = sub nsw i32 0, %348
  %352 = xor i32 %348, -1
  %353 = shl i32 1, %352
  %354 = trunc i32 %353 to i16
  %355 = insertelement <8 x i16> undef, i16 %354, i32 0
  %356 = shufflevector <8 x i16> %355, <8 x i16> undef, <8 x i32> zeroinitializer
  %357 = bitcast <2 x i64>* %195 to <8 x i16>*
  %358 = load <8 x i16>, <8 x i16>* %357, align 16
  %359 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %358, <8 x i16> %356) #8
  %360 = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %359, i32 %351) #8
  store <8 x i16> %360, <8 x i16>* %357, align 16
  %361 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 1
  %362 = bitcast <2 x i64>* %361 to <8 x i16>*
  %363 = load <8 x i16>, <8 x i16>* %362, align 16
  %364 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %363, <8 x i16> %356) #8
  %365 = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %364, i32 %351) #8
  store <8 x i16> %365, <8 x i16>* %362, align 16
  %366 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 2
  %367 = bitcast <2 x i64>* %366 to <8 x i16>*
  %368 = load <8 x i16>, <8 x i16>* %367, align 16
  %369 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %368, <8 x i16> %356) #8
  %370 = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %369, i32 %351) #8
  store <8 x i16> %370, <8 x i16>* %367, align 16
  %371 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 3
  %372 = bitcast <2 x i64>* %371 to <8 x i16>*
  %373 = load <8 x i16>, <8 x i16>* %372, align 16
  %374 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %373, <8 x i16> %356) #8
  %375 = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %374, i32 %351) #8
  store <8 x i16> %375, <8 x i16>* %372, align 16
  %376 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 4
  %377 = bitcast <2 x i64>* %376 to <8 x i16>*
  %378 = load <8 x i16>, <8 x i16>* %377, align 16
  %379 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %378, <8 x i16> %356) #8
  %380 = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %379, i32 %351) #8
  store <8 x i16> %380, <8 x i16>* %377, align 16
  %381 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 5
  %382 = bitcast <2 x i64>* %381 to <8 x i16>*
  %383 = load <8 x i16>, <8 x i16>* %382, align 16
  %384 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %383, <8 x i16> %356) #8
  %385 = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %384, i32 %351) #8
  store <8 x i16> %385, <8 x i16>* %382, align 16
  %386 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 6
  %387 = bitcast <2 x i64>* %386 to <8 x i16>*
  %388 = load <8 x i16>, <8 x i16>* %387, align 16
  %389 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %388, <8 x i16> %356) #8
  %390 = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %389, i32 %351) #8
  store <8 x i16> %390, <8 x i16>* %387, align 16
  %391 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 7
  %392 = bitcast <2 x i64>* %391 to <8 x i16>*
  %393 = load <8 x i16>, <8 x i16>* %392, align 16
  %394 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %393, <8 x i16> %356) #8
  %395 = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %394, i32 %351) #8
  store <8 x i16> %395, <8 x i16>* %392, align 16
  %396 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 8
  %397 = bitcast <2 x i64>* %396 to <8 x i16>*
  %398 = load <8 x i16>, <8 x i16>* %397, align 16
  %399 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %398, <8 x i16> %356) #8
  %400 = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %399, i32 %351) #8
  store <8 x i16> %400, <8 x i16>* %397, align 16
  %401 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 9
  %402 = bitcast <2 x i64>* %401 to <8 x i16>*
  %403 = load <8 x i16>, <8 x i16>* %402, align 16
  %404 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %403, <8 x i16> %356) #8
  %405 = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %404, i32 %351) #8
  store <8 x i16> %405, <8 x i16>* %402, align 16
  %406 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 10
  %407 = bitcast <2 x i64>* %406 to <8 x i16>*
  %408 = load <8 x i16>, <8 x i16>* %407, align 16
  %409 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %408, <8 x i16> %356) #8
  %410 = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %409, i32 %351) #8
  store <8 x i16> %410, <8 x i16>* %407, align 16
  %411 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 11
  %412 = bitcast <2 x i64>* %411 to <8 x i16>*
  %413 = load <8 x i16>, <8 x i16>* %412, align 16
  %414 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %413, <8 x i16> %356) #8
  %415 = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %414, i32 %351) #8
  store <8 x i16> %415, <8 x i16>* %412, align 16
  %416 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 12
  %417 = bitcast <2 x i64>* %416 to <8 x i16>*
  %418 = load <8 x i16>, <8 x i16>* %417, align 16
  %419 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %418, <8 x i16> %356) #8
  %420 = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %419, i32 %351) #8
  store <8 x i16> %420, <8 x i16>* %417, align 16
  %421 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 13
  %422 = bitcast <2 x i64>* %421 to <8 x i16>*
  %423 = load <8 x i16>, <8 x i16>* %422, align 16
  %424 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %423, <8 x i16> %356) #8
  %425 = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %424, i32 %351) #8
  store <8 x i16> %425, <8 x i16>* %422, align 16
  %426 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 14
  %427 = bitcast <2 x i64>* %426 to <8 x i16>*
  %428 = load <8 x i16>, <8 x i16>* %427, align 16
  %429 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %428, <8 x i16> %356) #8
  %430 = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %429, i32 %351) #8
  store <8 x i16> %430, <8 x i16>* %427, align 16
  %431 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 15
  %432 = bitcast <2 x i64>* %431 to <8 x i16>*
  %433 = load <8 x i16>, <8 x i16>* %432, align 16
  %434 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %433, <8 x i16> %356) #8
  %435 = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %434, i32 %351) #8
  store <8 x i16> %435, <8 x i16>* %432, align 16
  br label %542

436:                                              ; preds = %345
  %437 = icmp eq i8 %347, 0
  %438 = bitcast <2 x i64>* %195 to <8 x i16>*
  %439 = load <8 x i16>, <8 x i16>* %438, align 16
  br i1 %437, label %440, label %480

440:                                              ; preds = %436
  %441 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 1
  %442 = bitcast <2 x i64>* %441 to <8 x i16>*
  %443 = load <8 x i16>, <8 x i16>* %442, align 16
  %444 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 2
  %445 = bitcast <2 x i64>* %444 to <8 x i16>*
  %446 = load <8 x i16>, <8 x i16>* %445, align 16
  %447 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 3
  %448 = bitcast <2 x i64>* %447 to <8 x i16>*
  %449 = load <8 x i16>, <8 x i16>* %448, align 16
  %450 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 4
  %451 = bitcast <2 x i64>* %450 to <8 x i16>*
  %452 = load <8 x i16>, <8 x i16>* %451, align 16
  %453 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 5
  %454 = bitcast <2 x i64>* %453 to <8 x i16>*
  %455 = load <8 x i16>, <8 x i16>* %454, align 16
  %456 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 6
  %457 = bitcast <2 x i64>* %456 to <8 x i16>*
  %458 = load <8 x i16>, <8 x i16>* %457, align 16
  %459 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 7
  %460 = bitcast <2 x i64>* %459 to <8 x i16>*
  %461 = load <8 x i16>, <8 x i16>* %460, align 16
  %462 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 8
  %463 = bitcast <2 x i64>* %462 to <8 x i16>*
  %464 = load <8 x i16>, <8 x i16>* %463, align 16
  %465 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 9
  %466 = bitcast <2 x i64>* %465 to <8 x i16>*
  %467 = load <8 x i16>, <8 x i16>* %466, align 16
  %468 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 10
  %469 = bitcast <2 x i64>* %468 to <8 x i16>*
  %470 = load <8 x i16>, <8 x i16>* %469, align 16
  %471 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 11
  %472 = bitcast <2 x i64>* %471 to <8 x i16>*
  %473 = load <8 x i16>, <8 x i16>* %472, align 16
  %474 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 12
  %475 = bitcast <2 x i64>* %474 to <8 x i16>*
  %476 = load <8 x i16>, <8 x i16>* %475, align 16
  %477 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 13
  %478 = bitcast <2 x i64>* %477 to <8 x i16>*
  %479 = load <8 x i16>, <8 x i16>* %478, align 16
  br label %542

480:                                              ; preds = %436
  %481 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %439, i32 %348) #8
  store <8 x i16> %481, <8 x i16>* %438, align 16
  %482 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 1
  %483 = bitcast <2 x i64>* %482 to <8 x i16>*
  %484 = load <8 x i16>, <8 x i16>* %483, align 16
  %485 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %484, i32 %348) #8
  store <8 x i16> %485, <8 x i16>* %483, align 16
  %486 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 2
  %487 = bitcast <2 x i64>* %486 to <8 x i16>*
  %488 = load <8 x i16>, <8 x i16>* %487, align 16
  %489 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %488, i32 %348) #8
  store <8 x i16> %489, <8 x i16>* %487, align 16
  %490 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 3
  %491 = bitcast <2 x i64>* %490 to <8 x i16>*
  %492 = load <8 x i16>, <8 x i16>* %491, align 16
  %493 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %492, i32 %348) #8
  store <8 x i16> %493, <8 x i16>* %491, align 16
  %494 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 4
  %495 = bitcast <2 x i64>* %494 to <8 x i16>*
  %496 = load <8 x i16>, <8 x i16>* %495, align 16
  %497 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %496, i32 %348) #8
  store <8 x i16> %497, <8 x i16>* %495, align 16
  %498 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 5
  %499 = bitcast <2 x i64>* %498 to <8 x i16>*
  %500 = load <8 x i16>, <8 x i16>* %499, align 16
  %501 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %500, i32 %348) #8
  store <8 x i16> %501, <8 x i16>* %499, align 16
  %502 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 6
  %503 = bitcast <2 x i64>* %502 to <8 x i16>*
  %504 = load <8 x i16>, <8 x i16>* %503, align 16
  %505 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %504, i32 %348) #8
  store <8 x i16> %505, <8 x i16>* %503, align 16
  %506 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 7
  %507 = bitcast <2 x i64>* %506 to <8 x i16>*
  %508 = load <8 x i16>, <8 x i16>* %507, align 16
  %509 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %508, i32 %348) #8
  store <8 x i16> %509, <8 x i16>* %507, align 16
  %510 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 8
  %511 = bitcast <2 x i64>* %510 to <8 x i16>*
  %512 = load <8 x i16>, <8 x i16>* %511, align 16
  %513 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %512, i32 %348) #8
  store <8 x i16> %513, <8 x i16>* %511, align 16
  %514 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 9
  %515 = bitcast <2 x i64>* %514 to <8 x i16>*
  %516 = load <8 x i16>, <8 x i16>* %515, align 16
  %517 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %516, i32 %348) #8
  store <8 x i16> %517, <8 x i16>* %515, align 16
  %518 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 10
  %519 = bitcast <2 x i64>* %518 to <8 x i16>*
  %520 = load <8 x i16>, <8 x i16>* %519, align 16
  %521 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %520, i32 %348) #8
  store <8 x i16> %521, <8 x i16>* %519, align 16
  %522 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 11
  %523 = bitcast <2 x i64>* %522 to <8 x i16>*
  %524 = load <8 x i16>, <8 x i16>* %523, align 16
  %525 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %524, i32 %348) #8
  store <8 x i16> %525, <8 x i16>* %523, align 16
  %526 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 12
  %527 = bitcast <2 x i64>* %526 to <8 x i16>*
  %528 = load <8 x i16>, <8 x i16>* %527, align 16
  %529 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %528, i32 %348) #8
  store <8 x i16> %529, <8 x i16>* %527, align 16
  %530 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 13
  %531 = bitcast <2 x i64>* %530 to <8 x i16>*
  %532 = load <8 x i16>, <8 x i16>* %531, align 16
  %533 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %532, i32 %348) #8
  store <8 x i16> %533, <8 x i16>* %531, align 16
  %534 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 14
  %535 = bitcast <2 x i64>* %534 to <8 x i16>*
  %536 = load <8 x i16>, <8 x i16>* %535, align 16
  %537 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %536, i32 %348) #8
  store <8 x i16> %537, <8 x i16>* %535, align 16
  %538 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 15
  %539 = bitcast <2 x i64>* %538 to <8 x i16>*
  %540 = load <8 x i16>, <8 x i16>* %539, align 16
  %541 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %540, i32 %348) #8
  store <8 x i16> %541, <8 x i16>* %539, align 16
  br label %542

542:                                              ; preds = %440, %350, %480
  %543 = phi <8 x i16> [ %479, %440 ], [ %425, %350 ], [ %533, %480 ]
  %544 = phi <8 x i16> [ %476, %440 ], [ %420, %350 ], [ %529, %480 ]
  %545 = phi <8 x i16> [ %473, %440 ], [ %415, %350 ], [ %525, %480 ]
  %546 = phi <8 x i16> [ %470, %440 ], [ %410, %350 ], [ %521, %480 ]
  %547 = phi <8 x i16> [ %467, %440 ], [ %405, %350 ], [ %517, %480 ]
  %548 = phi <8 x i16> [ %464, %440 ], [ %400, %350 ], [ %513, %480 ]
  %549 = phi <8 x i16> [ %461, %440 ], [ %395, %350 ], [ %509, %480 ]
  %550 = phi <8 x i16> [ %458, %440 ], [ %390, %350 ], [ %505, %480 ]
  %551 = phi <8 x i16> [ %455, %440 ], [ %385, %350 ], [ %501, %480 ]
  %552 = phi <8 x i16> [ %452, %440 ], [ %380, %350 ], [ %497, %480 ]
  %553 = phi <8 x i16> [ %449, %440 ], [ %375, %350 ], [ %493, %480 ]
  %554 = phi <8 x i16> [ %446, %440 ], [ %370, %350 ], [ %489, %480 ]
  %555 = phi <8 x i16> [ %443, %440 ], [ %365, %350 ], [ %485, %480 ]
  %556 = phi <8 x i16> [ %439, %440 ], [ %360, %350 ], [ %481, %480 ]
  %557 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %7, i64 0, i64 0
  %558 = shufflevector <8 x i16> %556, <8 x i16> %555, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %559 = shufflevector <8 x i16> %554, <8 x i16> %553, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %560 = shufflevector <8 x i16> %552, <8 x i16> %551, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %561 = shufflevector <8 x i16> %550, <8 x i16> %549, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %562 = shufflevector <8 x i16> %556, <8 x i16> %555, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %563 = shufflevector <8 x i16> %554, <8 x i16> %553, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %564 = shufflevector <8 x i16> %552, <8 x i16> %551, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %565 = shufflevector <8 x i16> %550, <8 x i16> %549, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %566 = bitcast <8 x i16> %558 to <4 x i32>
  %567 = bitcast <8 x i16> %559 to <4 x i32>
  %568 = shufflevector <4 x i32> %566, <4 x i32> %567, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %569 = bitcast <4 x i32> %568 to <2 x i64>
  %570 = bitcast <8 x i16> %560 to <4 x i32>
  %571 = bitcast <8 x i16> %561 to <4 x i32>
  %572 = shufflevector <4 x i32> %570, <4 x i32> %571, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %573 = bitcast <4 x i32> %572 to <2 x i64>
  %574 = bitcast <8 x i16> %562 to <4 x i32>
  %575 = bitcast <8 x i16> %563 to <4 x i32>
  %576 = shufflevector <4 x i32> %574, <4 x i32> %575, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %577 = bitcast <4 x i32> %576 to <2 x i64>
  %578 = bitcast <8 x i16> %564 to <4 x i32>
  %579 = bitcast <8 x i16> %565 to <4 x i32>
  %580 = shufflevector <4 x i32> %578, <4 x i32> %579, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %581 = bitcast <4 x i32> %580 to <2 x i64>
  %582 = shufflevector <4 x i32> %566, <4 x i32> %567, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %583 = bitcast <4 x i32> %582 to <2 x i64>
  %584 = shufflevector <4 x i32> %570, <4 x i32> %571, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %585 = bitcast <4 x i32> %584 to <2 x i64>
  %586 = shufflevector <4 x i32> %574, <4 x i32> %575, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %587 = bitcast <4 x i32> %586 to <2 x i64>
  %588 = shufflevector <4 x i32> %578, <4 x i32> %579, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %589 = bitcast <4 x i32> %588 to <2 x i64>
  %590 = shufflevector <2 x i64> %569, <2 x i64> %573, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %590, <2 x i64>* %557, align 16
  %591 = shufflevector <2 x i64> %569, <2 x i64> %573, <2 x i32> <i32 1, i32 3>
  %592 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %7, i64 0, i64 1
  store <2 x i64> %591, <2 x i64>* %592, align 16
  %593 = shufflevector <2 x i64> %583, <2 x i64> %585, <2 x i32> <i32 0, i32 2>
  %594 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %7, i64 0, i64 2
  store <2 x i64> %593, <2 x i64>* %594, align 16
  %595 = shufflevector <2 x i64> %583, <2 x i64> %585, <2 x i32> <i32 1, i32 3>
  %596 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %7, i64 0, i64 3
  store <2 x i64> %595, <2 x i64>* %596, align 16
  %597 = shufflevector <2 x i64> %577, <2 x i64> %581, <2 x i32> <i32 0, i32 2>
  %598 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %7, i64 0, i64 4
  store <2 x i64> %597, <2 x i64>* %598, align 16
  %599 = shufflevector <2 x i64> %577, <2 x i64> %581, <2 x i32> <i32 1, i32 3>
  %600 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %7, i64 0, i64 5
  store <2 x i64> %599, <2 x i64>* %600, align 16
  %601 = shufflevector <2 x i64> %587, <2 x i64> %589, <2 x i32> <i32 0, i32 2>
  %602 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %7, i64 0, i64 6
  store <2 x i64> %601, <2 x i64>* %602, align 16
  %603 = shufflevector <2 x i64> %587, <2 x i64> %589, <2 x i32> <i32 1, i32 3>
  %604 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %7, i64 0, i64 7
  store <2 x i64> %603, <2 x i64>* %604, align 16
  %605 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 8
  %606 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %7, i64 0, i64 8
  %607 = shufflevector <8 x i16> %548, <8 x i16> %547, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %608 = shufflevector <8 x i16> %546, <8 x i16> %545, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %609 = shufflevector <8 x i16> %544, <8 x i16> %543, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %610 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 14
  %611 = bitcast <2 x i64>* %610 to <8 x i16>*
  %612 = load <8 x i16>, <8 x i16>* %611, align 16
  %613 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 15
  %614 = bitcast <2 x i64>* %613 to <8 x i16>*
  %615 = load <8 x i16>, <8 x i16>* %614, align 16
  %616 = shufflevector <8 x i16> %612, <8 x i16> %615, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %617 = shufflevector <8 x i16> %548, <8 x i16> %547, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %618 = shufflevector <8 x i16> %546, <8 x i16> %545, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %619 = shufflevector <8 x i16> %544, <8 x i16> %543, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %620 = shufflevector <8 x i16> %612, <8 x i16> %615, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %621 = bitcast <8 x i16> %607 to <4 x i32>
  %622 = bitcast <8 x i16> %608 to <4 x i32>
  %623 = shufflevector <4 x i32> %621, <4 x i32> %622, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %624 = bitcast <4 x i32> %623 to <2 x i64>
  %625 = bitcast <8 x i16> %609 to <4 x i32>
  %626 = bitcast <8 x i16> %616 to <4 x i32>
  %627 = shufflevector <4 x i32> %625, <4 x i32> %626, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %628 = bitcast <4 x i32> %627 to <2 x i64>
  %629 = bitcast <8 x i16> %617 to <4 x i32>
  %630 = bitcast <8 x i16> %618 to <4 x i32>
  %631 = shufflevector <4 x i32> %629, <4 x i32> %630, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %632 = bitcast <4 x i32> %631 to <2 x i64>
  %633 = bitcast <8 x i16> %619 to <4 x i32>
  %634 = bitcast <8 x i16> %620 to <4 x i32>
  %635 = shufflevector <4 x i32> %633, <4 x i32> %634, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %636 = bitcast <4 x i32> %635 to <2 x i64>
  %637 = shufflevector <4 x i32> %621, <4 x i32> %622, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %638 = bitcast <4 x i32> %637 to <2 x i64>
  %639 = shufflevector <4 x i32> %625, <4 x i32> %626, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %640 = bitcast <4 x i32> %639 to <2 x i64>
  %641 = shufflevector <4 x i32> %629, <4 x i32> %630, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %642 = bitcast <4 x i32> %641 to <2 x i64>
  %643 = shufflevector <4 x i32> %633, <4 x i32> %634, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %644 = bitcast <4 x i32> %643 to <2 x i64>
  %645 = shufflevector <2 x i64> %624, <2 x i64> %628, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %645, <2 x i64>* %606, align 16
  %646 = shufflevector <2 x i64> %624, <2 x i64> %628, <2 x i32> <i32 1, i32 3>
  %647 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %7, i64 0, i64 9
  store <2 x i64> %646, <2 x i64>* %647, align 16
  %648 = shufflevector <2 x i64> %638, <2 x i64> %640, <2 x i32> <i32 0, i32 2>
  %649 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %7, i64 0, i64 10
  store <2 x i64> %648, <2 x i64>* %649, align 16
  %650 = shufflevector <2 x i64> %638, <2 x i64> %640, <2 x i32> <i32 1, i32 3>
  %651 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %7, i64 0, i64 11
  store <2 x i64> %650, <2 x i64>* %651, align 16
  %652 = shufflevector <2 x i64> %632, <2 x i64> %636, <2 x i32> <i32 0, i32 2>
  %653 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %7, i64 0, i64 12
  store <2 x i64> %652, <2 x i64>* %653, align 16
  %654 = shufflevector <2 x i64> %632, <2 x i64> %636, <2 x i32> <i32 1, i32 3>
  %655 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %7, i64 0, i64 13
  store <2 x i64> %654, <2 x i64>* %655, align 16
  %656 = shufflevector <2 x i64> %642, <2 x i64> %644, <2 x i32> <i32 0, i32 2>
  %657 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %7, i64 0, i64 14
  store <2 x i64> %656, <2 x i64>* %657, align 16
  %658 = shufflevector <2 x i64> %642, <2 x i64> %644, <2 x i32> <i32 1, i32 3>
  %659 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %7, i64 0, i64 15
  store <2 x i64> %658, <2 x i64>* %659, align 16
  %660 = icmp eq i32 %196, 0
  br i1 %660, label %677, label %661

661:                                              ; preds = %542
  %662 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 7
  %663 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 6
  %664 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 5
  %665 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 4
  %666 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 3
  %667 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 2
  %668 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 1
  store <2 x i64> %590, <2 x i64>* %662, align 16
  store <2 x i64> %591, <2 x i64>* %663, align 16
  store <2 x i64> %593, <2 x i64>* %664, align 16
  store <2 x i64> %595, <2 x i64>* %665, align 16
  store <2 x i64> %597, <2 x i64>* %666, align 16
  store <2 x i64> %599, <2 x i64>* %667, align 16
  store <2 x i64> %601, <2 x i64>* %668, align 16
  %669 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 0
  store <2 x i64> %603, <2 x i64>* %669, align 16
  %670 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 15
  store <2 x i64> %645, <2 x i64>* %670, align 16
  %671 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 14
  store <2 x i64> %646, <2 x i64>* %671, align 16
  %672 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 13
  store <2 x i64> %648, <2 x i64>* %672, align 16
  %673 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 12
  store <2 x i64> %650, <2 x i64>* %673, align 16
  %674 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 11
  store <2 x i64> %652, <2 x i64>* %674, align 16
  %675 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 10
  store <2 x i64> %654, <2 x i64>* %675, align 16
  %676 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 9
  store <2 x i64> %656, <2 x i64>* %676, align 16
  store <2 x i64> %658, <2 x i64>* %605, align 16
  br label %677

677:                                              ; preds = %661, %542
  %678 = phi <2 x i64> [ %590, %542 ], [ %603, %661 ]
  %679 = phi <2 x i64>* [ %557, %542 ], [ %195, %661 ]
  %680 = phi <2 x i64>* [ %606, %542 ], [ %605, %661 ]
  %681 = getelementptr inbounds [8 x <4 x i64>], [8 x <4 x i64>]* %8, i64 0, i64 0
  %682 = shufflevector <2 x i64> %678, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %683 = bitcast <4 x i64> %682 to <8 x i32>
  %684 = bitcast <2 x i64>* %680 to <4 x i32>*
  %685 = load <4 x i32>, <4 x i32>* %684, align 16
  %686 = shufflevector <4 x i32> %685, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %687 = shufflevector <8 x i32> %683, <8 x i32> %686, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %688 = bitcast [8 x <4 x i64>]* %8 to <8 x i32>*
  store <8 x i32> %687, <8 x i32>* %688, align 32
  %689 = getelementptr inbounds <2 x i64>, <2 x i64>* %679, i64 1
  %690 = load <2 x i64>, <2 x i64>* %689, align 16
  %691 = shufflevector <2 x i64> %690, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %692 = bitcast <4 x i64> %691 to <8 x i32>
  %693 = getelementptr inbounds <2 x i64>, <2 x i64>* %680, i64 1
  %694 = bitcast <2 x i64>* %693 to <4 x i32>*
  %695 = load <4 x i32>, <4 x i32>* %694, align 16
  %696 = shufflevector <4 x i32> %695, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %697 = shufflevector <8 x i32> %692, <8 x i32> %696, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %698 = getelementptr inbounds [8 x <4 x i64>], [8 x <4 x i64>]* %8, i64 0, i64 1
  %699 = bitcast <4 x i64>* %698 to <8 x i32>*
  store <8 x i32> %697, <8 x i32>* %699, align 32
  %700 = getelementptr inbounds <2 x i64>, <2 x i64>* %679, i64 2
  %701 = load <2 x i64>, <2 x i64>* %700, align 16
  %702 = shufflevector <2 x i64> %701, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %703 = bitcast <4 x i64> %702 to <8 x i32>
  %704 = getelementptr inbounds <2 x i64>, <2 x i64>* %680, i64 2
  %705 = bitcast <2 x i64>* %704 to <4 x i32>*
  %706 = load <4 x i32>, <4 x i32>* %705, align 16
  %707 = shufflevector <4 x i32> %706, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %708 = shufflevector <8 x i32> %703, <8 x i32> %707, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %709 = getelementptr inbounds [8 x <4 x i64>], [8 x <4 x i64>]* %8, i64 0, i64 2
  %710 = bitcast <4 x i64>* %709 to <8 x i32>*
  store <8 x i32> %708, <8 x i32>* %710, align 32
  %711 = getelementptr inbounds <2 x i64>, <2 x i64>* %679, i64 3
  %712 = load <2 x i64>, <2 x i64>* %711, align 16
  %713 = shufflevector <2 x i64> %712, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %714 = bitcast <4 x i64> %713 to <8 x i32>
  %715 = getelementptr inbounds <2 x i64>, <2 x i64>* %680, i64 3
  %716 = bitcast <2 x i64>* %715 to <4 x i32>*
  %717 = load <4 x i32>, <4 x i32>* %716, align 16
  %718 = shufflevector <4 x i32> %717, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %719 = shufflevector <8 x i32> %714, <8 x i32> %718, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %720 = getelementptr inbounds [8 x <4 x i64>], [8 x <4 x i64>]* %8, i64 0, i64 3
  %721 = bitcast <4 x i64>* %720 to <8 x i32>*
  store <8 x i32> %719, <8 x i32>* %721, align 32
  %722 = getelementptr inbounds <2 x i64>, <2 x i64>* %679, i64 4
  %723 = load <2 x i64>, <2 x i64>* %722, align 16
  %724 = shufflevector <2 x i64> %723, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %725 = bitcast <4 x i64> %724 to <8 x i32>
  %726 = getelementptr inbounds <2 x i64>, <2 x i64>* %680, i64 4
  %727 = bitcast <2 x i64>* %726 to <4 x i32>*
  %728 = load <4 x i32>, <4 x i32>* %727, align 16
  %729 = shufflevector <4 x i32> %728, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %730 = shufflevector <8 x i32> %725, <8 x i32> %729, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %731 = getelementptr inbounds [8 x <4 x i64>], [8 x <4 x i64>]* %8, i64 0, i64 4
  %732 = bitcast <4 x i64>* %731 to <8 x i32>*
  store <8 x i32> %730, <8 x i32>* %732, align 32
  %733 = getelementptr inbounds <2 x i64>, <2 x i64>* %679, i64 5
  %734 = load <2 x i64>, <2 x i64>* %733, align 16
  %735 = shufflevector <2 x i64> %734, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %736 = bitcast <4 x i64> %735 to <8 x i32>
  %737 = getelementptr inbounds <2 x i64>, <2 x i64>* %680, i64 5
  %738 = bitcast <2 x i64>* %737 to <4 x i32>*
  %739 = load <4 x i32>, <4 x i32>* %738, align 16
  %740 = shufflevector <4 x i32> %739, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %741 = shufflevector <8 x i32> %736, <8 x i32> %740, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %742 = getelementptr inbounds [8 x <4 x i64>], [8 x <4 x i64>]* %8, i64 0, i64 5
  %743 = bitcast <4 x i64>* %742 to <8 x i32>*
  store <8 x i32> %741, <8 x i32>* %743, align 32
  %744 = getelementptr inbounds <2 x i64>, <2 x i64>* %679, i64 6
  %745 = load <2 x i64>, <2 x i64>* %744, align 16
  %746 = shufflevector <2 x i64> %745, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %747 = bitcast <4 x i64> %746 to <8 x i32>
  %748 = getelementptr inbounds <2 x i64>, <2 x i64>* %680, i64 6
  %749 = bitcast <2 x i64>* %748 to <4 x i32>*
  %750 = load <4 x i32>, <4 x i32>* %749, align 16
  %751 = shufflevector <4 x i32> %750, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %752 = shufflevector <8 x i32> %747, <8 x i32> %751, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %753 = getelementptr inbounds [8 x <4 x i64>], [8 x <4 x i64>]* %8, i64 0, i64 6
  %754 = bitcast <4 x i64>* %753 to <8 x i32>*
  store <8 x i32> %752, <8 x i32>* %754, align 32
  %755 = getelementptr inbounds <2 x i64>, <2 x i64>* %679, i64 7
  %756 = load <2 x i64>, <2 x i64>* %755, align 16
  %757 = shufflevector <2 x i64> %756, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %758 = bitcast <4 x i64> %757 to <8 x i32>
  %759 = getelementptr inbounds <2 x i64>, <2 x i64>* %680, i64 7
  %760 = bitcast <2 x i64>* %759 to <4 x i32>*
  %761 = load <4 x i32>, <4 x i32>* %760, align 16
  %762 = shufflevector <4 x i32> %761, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %763 = shufflevector <8 x i32> %758, <8 x i32> %762, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %764 = getelementptr inbounds [8 x <4 x i64>], [8 x <4 x i64>]* %8, i64 0, i64 7
  %765 = bitcast <4 x i64>* %764 to <8 x i32>*
  store <8 x i32> %763, <8 x i32>* %765, align 32
  call void %19(<4 x i64>* nonnull %681, <4 x i64>* nonnull %681, i8 signext %14) #8
  %766 = getelementptr inbounds i8, i8* %12, i64 2
  %767 = load i8, i8* %766, align 1
  %768 = sext i8 %767 to i32
  %769 = icmp slt i8 %767, 0
  br i1 %769, label %770, label %809

770:                                              ; preds = %677
  %771 = sub nsw i32 0, %768
  %772 = xor i32 %768, -1
  %773 = shl i32 1, %772
  %774 = trunc i32 %773 to i16
  %775 = insertelement <16 x i16> undef, i16 %774, i32 0
  %776 = shufflevector <16 x i16> %775, <16 x i16> undef, <16 x i32> zeroinitializer
  %777 = bitcast [8 x <4 x i64>]* %8 to <16 x i16>*
  %778 = load <16 x i16>, <16 x i16>* %777, align 32
  %779 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %778, <16 x i16> %776) #8
  %780 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %779, i32 %771) #8
  store <16 x i16> %780, <16 x i16>* %777, align 32
  %781 = bitcast <4 x i64>* %698 to <16 x i16>*
  %782 = load <16 x i16>, <16 x i16>* %781, align 32
  %783 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %782, <16 x i16> %776) #8
  %784 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %783, i32 %771) #8
  store <16 x i16> %784, <16 x i16>* %781, align 32
  %785 = bitcast <4 x i64>* %709 to <16 x i16>*
  %786 = load <16 x i16>, <16 x i16>* %785, align 32
  %787 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %786, <16 x i16> %776) #8
  %788 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %787, i32 %771) #8
  store <16 x i16> %788, <16 x i16>* %785, align 32
  %789 = bitcast <4 x i64>* %720 to <16 x i16>*
  %790 = load <16 x i16>, <16 x i16>* %789, align 32
  %791 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %790, <16 x i16> %776) #8
  %792 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %791, i32 %771) #8
  store <16 x i16> %792, <16 x i16>* %789, align 32
  %793 = bitcast <4 x i64>* %731 to <16 x i16>*
  %794 = load <16 x i16>, <16 x i16>* %793, align 32
  %795 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %794, <16 x i16> %776) #8
  %796 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %795, i32 %771) #8
  store <16 x i16> %796, <16 x i16>* %793, align 32
  %797 = bitcast <4 x i64>* %742 to <16 x i16>*
  %798 = load <16 x i16>, <16 x i16>* %797, align 32
  %799 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %798, <16 x i16> %776) #8
  %800 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %799, i32 %771) #8
  store <16 x i16> %800, <16 x i16>* %797, align 32
  %801 = bitcast <4 x i64>* %753 to <16 x i16>*
  %802 = load <16 x i16>, <16 x i16>* %801, align 32
  %803 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %802, <16 x i16> %776) #8
  %804 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %803, i32 %771) #8
  store <16 x i16> %804, <16 x i16>* %801, align 32
  %805 = bitcast <4 x i64>* %764 to <16 x i16>*
  %806 = load <16 x i16>, <16 x i16>* %805, align 32
  %807 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %806, <16 x i16> %776) #8
  %808 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %807, i32 %771) #8
  store <16 x i16> %808, <16 x i16>* %805, align 32
  br label %851

809:                                              ; preds = %677
  %810 = icmp eq i8 %767, 0
  %811 = bitcast [8 x <4 x i64>]* %8 to <16 x i16>*
  %812 = load <16 x i16>, <16 x i16>* %811, align 32
  br i1 %810, label %813, label %828

813:                                              ; preds = %809
  %814 = bitcast <4 x i64>* %698 to <16 x i16>*
  %815 = load <16 x i16>, <16 x i16>* %814, align 32
  %816 = bitcast <4 x i64>* %709 to <16 x i16>*
  %817 = load <16 x i16>, <16 x i16>* %816, align 32
  %818 = bitcast <4 x i64>* %720 to <16 x i16>*
  %819 = load <16 x i16>, <16 x i16>* %818, align 32
  %820 = bitcast <4 x i64>* %731 to <16 x i16>*
  %821 = load <16 x i16>, <16 x i16>* %820, align 32
  %822 = bitcast <4 x i64>* %742 to <16 x i16>*
  %823 = load <16 x i16>, <16 x i16>* %822, align 32
  %824 = bitcast <4 x i64>* %753 to <16 x i16>*
  %825 = load <16 x i16>, <16 x i16>* %824, align 32
  %826 = bitcast <4 x i64>* %764 to <16 x i16>*
  %827 = load <16 x i16>, <16 x i16>* %826, align 32
  br label %851

828:                                              ; preds = %809
  %829 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %812, i32 %768) #8
  store <16 x i16> %829, <16 x i16>* %811, align 32
  %830 = bitcast <4 x i64>* %698 to <16 x i16>*
  %831 = load <16 x i16>, <16 x i16>* %830, align 32
  %832 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %831, i32 %768) #8
  store <16 x i16> %832, <16 x i16>* %830, align 32
  %833 = bitcast <4 x i64>* %709 to <16 x i16>*
  %834 = load <16 x i16>, <16 x i16>* %833, align 32
  %835 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %834, i32 %768) #8
  store <16 x i16> %835, <16 x i16>* %833, align 32
  %836 = bitcast <4 x i64>* %720 to <16 x i16>*
  %837 = load <16 x i16>, <16 x i16>* %836, align 32
  %838 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %837, i32 %768) #8
  store <16 x i16> %838, <16 x i16>* %836, align 32
  %839 = bitcast <4 x i64>* %731 to <16 x i16>*
  %840 = load <16 x i16>, <16 x i16>* %839, align 32
  %841 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %840, i32 %768) #8
  store <16 x i16> %841, <16 x i16>* %839, align 32
  %842 = bitcast <4 x i64>* %742 to <16 x i16>*
  %843 = load <16 x i16>, <16 x i16>* %842, align 32
  %844 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %843, i32 %768) #8
  store <16 x i16> %844, <16 x i16>* %842, align 32
  %845 = bitcast <4 x i64>* %753 to <16 x i16>*
  %846 = load <16 x i16>, <16 x i16>* %845, align 32
  %847 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %846, i32 %768) #8
  store <16 x i16> %847, <16 x i16>* %845, align 32
  %848 = bitcast <4 x i64>* %764 to <16 x i16>*
  %849 = load <16 x i16>, <16 x i16>* %848, align 32
  %850 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %849, i32 %768) #8
  store <16 x i16> %850, <16 x i16>* %848, align 32
  br label %851

851:                                              ; preds = %813, %828, %770
  %852 = phi <16 x i16>* [ %826, %813 ], [ %848, %828 ], [ %805, %770 ]
  %853 = phi <16 x i16>* [ %824, %813 ], [ %845, %828 ], [ %801, %770 ]
  %854 = phi <16 x i16>* [ %822, %813 ], [ %842, %828 ], [ %797, %770 ]
  %855 = phi <16 x i16> [ %827, %813 ], [ %850, %828 ], [ %808, %770 ]
  %856 = phi <16 x i16> [ %825, %813 ], [ %847, %828 ], [ %804, %770 ]
  %857 = phi <16 x i16> [ %823, %813 ], [ %844, %828 ], [ %800, %770 ]
  %858 = phi <16 x i16> [ %821, %813 ], [ %841, %828 ], [ %796, %770 ]
  %859 = phi <16 x i16> [ %819, %813 ], [ %838, %828 ], [ %792, %770 ]
  %860 = phi <16 x i16> [ %817, %813 ], [ %835, %828 ], [ %788, %770 ]
  %861 = phi <16 x i16> [ %815, %813 ], [ %832, %828 ], [ %784, %770 ]
  %862 = phi <16 x i16> [ %812, %813 ], [ %829, %828 ], [ %780, %770 ]
  %863 = shufflevector <16 x i16> %862, <16 x i16> %861, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %864 = shufflevector <16 x i16> %860, <16 x i16> %859, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %865 = shufflevector <16 x i16> %858, <16 x i16> %857, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %866 = shufflevector <16 x i16> %856, <16 x i16> %855, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %867 = shufflevector <16 x i16> %862, <16 x i16> %861, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %868 = shufflevector <16 x i16> %860, <16 x i16> %859, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %869 = shufflevector <16 x i16> %858, <16 x i16> %857, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %870 = shufflevector <16 x i16> %856, <16 x i16> %855, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %871 = bitcast <16 x i16> %863 to <8 x i32>
  %872 = bitcast <16 x i16> %864 to <8 x i32>
  %873 = shufflevector <8 x i32> %871, <8 x i32> %872, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %874 = bitcast <8 x i32> %873 to <4 x i64>
  %875 = bitcast <16 x i16> %865 to <8 x i32>
  %876 = bitcast <16 x i16> %866 to <8 x i32>
  %877 = shufflevector <8 x i32> %875, <8 x i32> %876, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %878 = bitcast <8 x i32> %877 to <4 x i64>
  %879 = bitcast <16 x i16> %867 to <8 x i32>
  %880 = bitcast <16 x i16> %868 to <8 x i32>
  %881 = shufflevector <8 x i32> %879, <8 x i32> %880, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %882 = bitcast <8 x i32> %881 to <4 x i64>
  %883 = bitcast <16 x i16> %869 to <8 x i32>
  %884 = bitcast <16 x i16> %870 to <8 x i32>
  %885 = shufflevector <8 x i32> %883, <8 x i32> %884, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %886 = bitcast <8 x i32> %885 to <4 x i64>
  %887 = shufflevector <8 x i32> %871, <8 x i32> %872, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %888 = bitcast <8 x i32> %887 to <4 x i64>
  %889 = shufflevector <8 x i32> %875, <8 x i32> %876, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %890 = bitcast <8 x i32> %889 to <4 x i64>
  %891 = shufflevector <8 x i32> %879, <8 x i32> %880, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %892 = bitcast <8 x i32> %891 to <4 x i64>
  %893 = shufflevector <8 x i32> %883, <8 x i32> %884, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %894 = bitcast <8 x i32> %893 to <4 x i64>
  %895 = shufflevector <4 x i64> %874, <4 x i64> %878, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %895, <4 x i64>* %681, align 32
  %896 = shufflevector <4 x i64> %874, <4 x i64> %878, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %896, <4 x i64>* %698, align 32
  %897 = shufflevector <4 x i64> %888, <4 x i64> %890, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %897, <4 x i64>* %709, align 32
  %898 = shufflevector <4 x i64> %888, <4 x i64> %890, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %898, <4 x i64>* %720, align 32
  %899 = shufflevector <4 x i64> %882, <4 x i64> %886, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %899, <4 x i64>* %731, align 32
  %900 = shufflevector <4 x i64> %882, <4 x i64> %886, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %900, <4 x i64>* %742, align 32
  %901 = shufflevector <4 x i64> %892, <4 x i64> %894, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %901, <4 x i64>* %753, align 32
  %902 = shufflevector <4 x i64> %892, <4 x i64> %894, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %902, <4 x i64>* %764, align 32
  %903 = bitcast <4 x i64> %895 to <16 x i16>
  %904 = shufflevector <16 x i16> %903, <16 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %905 = shufflevector <16 x i16> %903, <16 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %906 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %904, <16 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %907 = ashr <8 x i32> %906, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %908 = bitcast <8 x i32> %907 to <4 x i64>
  %909 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %905, <16 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %910 = ashr <8 x i32> %909, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %911 = bitcast <8 x i32> %910 to <4 x i64>
  %912 = shufflevector <8 x i32> %907, <8 x i32> %910, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
  %913 = bitcast i32* %1 to <2 x i64>*
  %914 = shufflevector <4 x i64> %908, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %914, <2 x i64>* %913, align 16
  %915 = getelementptr inbounds i32, i32* %1, i64 4
  %916 = bitcast i32* %915 to <2 x i64>*
  %917 = shufflevector <4 x i64> %911, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %917, <2 x i64>* %916, align 16
  %918 = getelementptr inbounds i32, i32* %1, i64 64
  %919 = bitcast i32* %918 to <8 x i32>*
  store <8 x i32> %912, <8 x i32>* %919, align 32
  %920 = bitcast <4 x i64> %896 to <16 x i16>
  %921 = getelementptr inbounds i32, i32* %1, i64 8
  %922 = shufflevector <16 x i16> %920, <16 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %923 = shufflevector <16 x i16> %920, <16 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %924 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %922, <16 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %925 = ashr <8 x i32> %924, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %926 = bitcast <8 x i32> %925 to <4 x i64>
  %927 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %923, <16 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %928 = ashr <8 x i32> %927, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %929 = bitcast <8 x i32> %928 to <4 x i64>
  %930 = shufflevector <8 x i32> %925, <8 x i32> %928, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
  %931 = bitcast i32* %921 to <2 x i64>*
  %932 = shufflevector <4 x i64> %926, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %932, <2 x i64>* %931, align 16
  %933 = getelementptr inbounds i32, i32* %1, i64 12
  %934 = bitcast i32* %933 to <2 x i64>*
  %935 = shufflevector <4 x i64> %929, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %935, <2 x i64>* %934, align 16
  %936 = getelementptr inbounds i32, i32* %1, i64 72
  %937 = bitcast i32* %936 to <8 x i32>*
  store <8 x i32> %930, <8 x i32>* %937, align 32
  %938 = bitcast <4 x i64> %897 to <16 x i16>
  %939 = getelementptr inbounds i32, i32* %1, i64 16
  %940 = shufflevector <16 x i16> %938, <16 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %941 = shufflevector <16 x i16> %938, <16 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %942 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %940, <16 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %943 = ashr <8 x i32> %942, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %944 = bitcast <8 x i32> %943 to <4 x i64>
  %945 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %941, <16 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %946 = ashr <8 x i32> %945, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %947 = bitcast <8 x i32> %946 to <4 x i64>
  %948 = shufflevector <8 x i32> %943, <8 x i32> %946, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
  %949 = bitcast i32* %939 to <2 x i64>*
  %950 = shufflevector <4 x i64> %944, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %950, <2 x i64>* %949, align 16
  %951 = getelementptr inbounds i32, i32* %1, i64 20
  %952 = bitcast i32* %951 to <2 x i64>*
  %953 = shufflevector <4 x i64> %947, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %953, <2 x i64>* %952, align 16
  %954 = getelementptr inbounds i32, i32* %1, i64 80
  %955 = bitcast i32* %954 to <8 x i32>*
  store <8 x i32> %948, <8 x i32>* %955, align 32
  %956 = bitcast <4 x i64> %898 to <16 x i16>
  %957 = getelementptr inbounds i32, i32* %1, i64 24
  %958 = shufflevector <16 x i16> %956, <16 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %959 = shufflevector <16 x i16> %956, <16 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %960 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %958, <16 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %961 = ashr <8 x i32> %960, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %962 = bitcast <8 x i32> %961 to <4 x i64>
  %963 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %959, <16 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %964 = ashr <8 x i32> %963, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %965 = bitcast <8 x i32> %964 to <4 x i64>
  %966 = shufflevector <8 x i32> %961, <8 x i32> %964, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
  %967 = bitcast i32* %957 to <2 x i64>*
  %968 = shufflevector <4 x i64> %962, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %968, <2 x i64>* %967, align 16
  %969 = getelementptr inbounds i32, i32* %1, i64 28
  %970 = bitcast i32* %969 to <2 x i64>*
  %971 = shufflevector <4 x i64> %965, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %971, <2 x i64>* %970, align 16
  %972 = getelementptr inbounds i32, i32* %1, i64 88
  %973 = bitcast i32* %972 to <8 x i32>*
  store <8 x i32> %966, <8 x i32>* %973, align 32
  %974 = bitcast <4 x i64> %899 to <16 x i16>
  %975 = getelementptr inbounds i32, i32* %1, i64 32
  %976 = shufflevector <16 x i16> %974, <16 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %977 = shufflevector <16 x i16> %974, <16 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %978 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %976, <16 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %979 = ashr <8 x i32> %978, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %980 = bitcast <8 x i32> %979 to <4 x i64>
  %981 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %977, <16 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %982 = ashr <8 x i32> %981, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %983 = bitcast <8 x i32> %982 to <4 x i64>
  %984 = shufflevector <8 x i32> %979, <8 x i32> %982, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
  %985 = bitcast i32* %975 to <2 x i64>*
  %986 = shufflevector <4 x i64> %980, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %986, <2 x i64>* %985, align 16
  %987 = getelementptr inbounds i32, i32* %1, i64 36
  %988 = bitcast i32* %987 to <2 x i64>*
  %989 = shufflevector <4 x i64> %983, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %989, <2 x i64>* %988, align 16
  %990 = getelementptr inbounds i32, i32* %1, i64 96
  %991 = bitcast i32* %990 to <8 x i32>*
  store <8 x i32> %984, <8 x i32>* %991, align 32
  %992 = load <16 x i16>, <16 x i16>* %854, align 32
  %993 = getelementptr inbounds i32, i32* %1, i64 40
  %994 = shufflevector <16 x i16> %992, <16 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %995 = shufflevector <16 x i16> %992, <16 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %996 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %994, <16 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %997 = ashr <8 x i32> %996, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %998 = bitcast <8 x i32> %997 to <4 x i64>
  %999 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %995, <16 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %1000 = ashr <8 x i32> %999, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %1001 = bitcast <8 x i32> %1000 to <4 x i64>
  %1002 = shufflevector <8 x i32> %997, <8 x i32> %1000, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
  %1003 = bitcast i32* %993 to <2 x i64>*
  %1004 = shufflevector <4 x i64> %998, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %1004, <2 x i64>* %1003, align 16
  %1005 = getelementptr inbounds i32, i32* %1, i64 44
  %1006 = bitcast i32* %1005 to <2 x i64>*
  %1007 = shufflevector <4 x i64> %1001, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %1007, <2 x i64>* %1006, align 16
  %1008 = getelementptr inbounds i32, i32* %1, i64 104
  %1009 = bitcast i32* %1008 to <8 x i32>*
  store <8 x i32> %1002, <8 x i32>* %1009, align 32
  %1010 = load <16 x i16>, <16 x i16>* %853, align 32
  %1011 = getelementptr inbounds i32, i32* %1, i64 48
  %1012 = shufflevector <16 x i16> %1010, <16 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1013 = shufflevector <16 x i16> %1010, <16 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1014 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1012, <16 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %1015 = ashr <8 x i32> %1014, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %1016 = bitcast <8 x i32> %1015 to <4 x i64>
  %1017 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1013, <16 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %1018 = ashr <8 x i32> %1017, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %1019 = bitcast <8 x i32> %1018 to <4 x i64>
  %1020 = shufflevector <8 x i32> %1015, <8 x i32> %1018, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
  %1021 = bitcast i32* %1011 to <2 x i64>*
  %1022 = shufflevector <4 x i64> %1016, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %1022, <2 x i64>* %1021, align 16
  %1023 = getelementptr inbounds i32, i32* %1, i64 52
  %1024 = bitcast i32* %1023 to <2 x i64>*
  %1025 = shufflevector <4 x i64> %1019, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %1025, <2 x i64>* %1024, align 16
  %1026 = getelementptr inbounds i32, i32* %1, i64 112
  %1027 = bitcast i32* %1026 to <8 x i32>*
  store <8 x i32> %1020, <8 x i32>* %1027, align 32
  %1028 = load <16 x i16>, <16 x i16>* %852, align 32
  %1029 = getelementptr inbounds i32, i32* %1, i64 56
  %1030 = shufflevector <16 x i16> %1028, <16 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1031 = shufflevector <16 x i16> %1028, <16 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1032 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1030, <16 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %1033 = ashr <8 x i32> %1032, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %1034 = bitcast <8 x i32> %1033 to <4 x i64>
  %1035 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1031, <16 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %1036 = ashr <8 x i32> %1035, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %1037 = bitcast <8 x i32> %1036 to <4 x i64>
  %1038 = shufflevector <8 x i32> %1033, <8 x i32> %1036, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
  %1039 = bitcast i32* %1029 to <2 x i64>*
  %1040 = shufflevector <4 x i64> %1034, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %1040, <2 x i64>* %1039, align 16
  %1041 = getelementptr inbounds i32, i32* %1, i64 60
  %1042 = bitcast i32* %1041 to <2 x i64>*
  %1043 = shufflevector <4 x i64> %1037, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %1043, <2 x i64>* %1042, align 16
  %1044 = getelementptr inbounds i32, i32* %1, i64 120
  %1045 = bitcast i32* %1044 to <8 x i32>*
  store <8 x i32> %1038, <8 x i32>* %1045, align 32
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %11) #8
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %10) #8
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %9) #8
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @lowbd_fwd_txfm2d_16x8_avx2(i16* nocapture readonly, i32* nocapture, i32, i8 zeroext, i32) #3 {
  %6 = alloca [16 x <2 x i64>], align 16
  %7 = alloca [16 x <2 x i64>], align 16
  %8 = alloca [8 x <4 x i64>], align 32
  %9 = bitcast [16 x <2 x i64>]* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %9) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %9, i8 -86, i64 256, i1 false)
  %10 = bitcast [16 x <2 x i64>]* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %10) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %10, i8 -86, i64 256, i1 false)
  %11 = bitcast [8 x <4 x i64>]* %8 to i8*
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %11) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %11, i8 -86, i64 256, i1 false)
  %12 = load i8*, i8** getelementptr inbounds ([19 x i8*], [19 x i8*]* @av1_fwd_txfm_shift_ls, i64 0, i64 8), align 16
  %13 = load i8, i8* getelementptr inbounds ([5 x [5 x i8]], [5 x [5 x i8]]* @av1_fwd_cos_bit_col, i64 0, i64 2, i64 1), align 1
  %14 = load i8, i8* getelementptr inbounds ([5 x [5 x i8]], [5 x [5 x i8]]* @av1_fwd_cos_bit_row, i64 0, i64 2, i64 1), align 1
  %15 = zext i8 %3 to i64
  %16 = getelementptr inbounds [16 x void (<4 x i64>*, <4 x i64>*, i8)*], [16 x void (<4 x i64>*, <4 x i64>*, i8)*]* @col_txfm16x8_arr, i64 0, i64 %15
  %17 = load void (<4 x i64>*, <4 x i64>*, i8)*, void (<4 x i64>*, <4 x i64>*, i8)** %16, align 8
  %18 = getelementptr inbounds [16 x void (<2 x i64>*, <2 x i64>*, i8)*], [16 x void (<2 x i64>*, <2 x i64>*, i8)*]* @row_txfm16x8_arr, i64 0, i64 %15
  %19 = load void (<2 x i64>*, <2 x i64>*, i8)*, void (<2 x i64>*, <2 x i64>*, i8)** %18, align 8
  switch i8 %3, label %94 [
    i8 6, label %21
    i8 15, label %20
    i8 7, label %20
    i8 5, label %20
    i8 14, label %22
    i8 8, label %22
    i8 4, label %22
  ]

20:                                               ; preds = %5, %5, %5
  br label %94

21:                                               ; preds = %5
  br label %22

22:                                               ; preds = %5, %5, %5, %21
  %23 = phi i32 [ 1, %21 ], [ 0, %5 ], [ 0, %5 ], [ 0, %5 ]
  %24 = sext i32 %2 to i64
  %25 = bitcast i16* %0 to <2 x i64>*
  %26 = load <2 x i64>, <2 x i64>* %25, align 16
  %27 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 7
  store <2 x i64> %26, <2 x i64>* %27, align 16
  %28 = getelementptr inbounds i16, i16* %0, i64 %24
  %29 = bitcast i16* %28 to <2 x i64>*
  %30 = load <2 x i64>, <2 x i64>* %29, align 16
  %31 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 6
  store <2 x i64> %30, <2 x i64>* %31, align 16
  %32 = shl nsw i64 %24, 1
  %33 = getelementptr inbounds i16, i16* %0, i64 %32
  %34 = bitcast i16* %33 to <2 x i64>*
  %35 = load <2 x i64>, <2 x i64>* %34, align 16
  %36 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 5
  store <2 x i64> %35, <2 x i64>* %36, align 16
  %37 = mul nsw i64 %24, 3
  %38 = getelementptr inbounds i16, i16* %0, i64 %37
  %39 = bitcast i16* %38 to <2 x i64>*
  %40 = load <2 x i64>, <2 x i64>* %39, align 16
  %41 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 4
  store <2 x i64> %40, <2 x i64>* %41, align 16
  %42 = shl nsw i64 %24, 2
  %43 = getelementptr inbounds i16, i16* %0, i64 %42
  %44 = bitcast i16* %43 to <2 x i64>*
  %45 = load <2 x i64>, <2 x i64>* %44, align 16
  %46 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 3
  store <2 x i64> %45, <2 x i64>* %46, align 16
  %47 = mul nsw i64 %24, 5
  %48 = getelementptr inbounds i16, i16* %0, i64 %47
  %49 = bitcast i16* %48 to <2 x i64>*
  %50 = load <2 x i64>, <2 x i64>* %49, align 16
  %51 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 2
  store <2 x i64> %50, <2 x i64>* %51, align 16
  %52 = mul nsw i64 %24, 6
  %53 = getelementptr inbounds i16, i16* %0, i64 %52
  %54 = bitcast i16* %53 to <2 x i64>*
  %55 = load <2 x i64>, <2 x i64>* %54, align 16
  %56 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 1
  store <2 x i64> %55, <2 x i64>* %56, align 16
  %57 = mul nsw i64 %24, 7
  %58 = getelementptr inbounds i16, i16* %0, i64 %57
  %59 = bitcast i16* %58 to <2 x i64>*
  %60 = load <2 x i64>, <2 x i64>* %59, align 16
  %61 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 0
  store <2 x i64> %60, <2 x i64>* %61, align 16
  %62 = getelementptr inbounds i16, i16* %0, i64 8
  %63 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 8
  %64 = bitcast i16* %62 to <2 x i64>*
  %65 = load <2 x i64>, <2 x i64>* %64, align 16
  %66 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 15
  store <2 x i64> %65, <2 x i64>* %66, align 16
  %67 = getelementptr inbounds i16, i16* %62, i64 %24
  %68 = bitcast i16* %67 to <2 x i64>*
  %69 = load <2 x i64>, <2 x i64>* %68, align 16
  %70 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 14
  store <2 x i64> %69, <2 x i64>* %70, align 16
  %71 = getelementptr inbounds i16, i16* %62, i64 %32
  %72 = bitcast i16* %71 to <2 x i64>*
  %73 = load <2 x i64>, <2 x i64>* %72, align 16
  %74 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 13
  store <2 x i64> %73, <2 x i64>* %74, align 16
  %75 = getelementptr inbounds i16, i16* %62, i64 %37
  %76 = bitcast i16* %75 to <2 x i64>*
  %77 = load <2 x i64>, <2 x i64>* %76, align 16
  %78 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 12
  store <2 x i64> %77, <2 x i64>* %78, align 16
  %79 = getelementptr inbounds i16, i16* %62, i64 %42
  %80 = bitcast i16* %79 to <2 x i64>*
  %81 = load <2 x i64>, <2 x i64>* %80, align 16
  %82 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 11
  store <2 x i64> %81, <2 x i64>* %82, align 16
  %83 = getelementptr inbounds i16, i16* %62, i64 %47
  %84 = bitcast i16* %83 to <2 x i64>*
  %85 = load <2 x i64>, <2 x i64>* %84, align 16
  %86 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 10
  store <2 x i64> %85, <2 x i64>* %86, align 16
  %87 = getelementptr inbounds i16, i16* %62, i64 %52
  %88 = bitcast i16* %87 to <2 x i64>*
  %89 = load <2 x i64>, <2 x i64>* %88, align 16
  %90 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 9
  store <2 x i64> %89, <2 x i64>* %90, align 16
  %91 = getelementptr inbounds i16, i16* %62, i64 %57
  %92 = bitcast i16* %91 to <2 x i64>*
  %93 = load <2 x i64>, <2 x i64>* %92, align 16
  store <2 x i64> %93, <2 x i64>* %63, align 16
  br label %166

94:                                               ; preds = %20, %5
  %95 = phi i32 [ 0, %5 ], [ 1, %20 ]
  %96 = sext i32 %2 to i64
  %97 = bitcast i16* %0 to <2 x i64>*
  %98 = load <2 x i64>, <2 x i64>* %97, align 16
  %99 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 0
  store <2 x i64> %98, <2 x i64>* %99, align 16
  %100 = getelementptr inbounds i16, i16* %0, i64 %96
  %101 = bitcast i16* %100 to <2 x i64>*
  %102 = load <2 x i64>, <2 x i64>* %101, align 16
  %103 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 1
  store <2 x i64> %102, <2 x i64>* %103, align 16
  %104 = shl nsw i64 %96, 1
  %105 = getelementptr inbounds i16, i16* %0, i64 %104
  %106 = bitcast i16* %105 to <2 x i64>*
  %107 = load <2 x i64>, <2 x i64>* %106, align 16
  %108 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 2
  store <2 x i64> %107, <2 x i64>* %108, align 16
  %109 = mul nsw i64 %96, 3
  %110 = getelementptr inbounds i16, i16* %0, i64 %109
  %111 = bitcast i16* %110 to <2 x i64>*
  %112 = load <2 x i64>, <2 x i64>* %111, align 16
  %113 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 3
  store <2 x i64> %112, <2 x i64>* %113, align 16
  %114 = shl nsw i64 %96, 2
  %115 = getelementptr inbounds i16, i16* %0, i64 %114
  %116 = bitcast i16* %115 to <2 x i64>*
  %117 = load <2 x i64>, <2 x i64>* %116, align 16
  %118 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 4
  store <2 x i64> %117, <2 x i64>* %118, align 16
  %119 = mul nsw i64 %96, 5
  %120 = getelementptr inbounds i16, i16* %0, i64 %119
  %121 = bitcast i16* %120 to <2 x i64>*
  %122 = load <2 x i64>, <2 x i64>* %121, align 16
  %123 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 5
  store <2 x i64> %122, <2 x i64>* %123, align 16
  %124 = mul nsw i64 %96, 6
  %125 = getelementptr inbounds i16, i16* %0, i64 %124
  %126 = bitcast i16* %125 to <2 x i64>*
  %127 = load <2 x i64>, <2 x i64>* %126, align 16
  %128 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 6
  store <2 x i64> %127, <2 x i64>* %128, align 16
  %129 = mul nsw i64 %96, 7
  %130 = getelementptr inbounds i16, i16* %0, i64 %129
  %131 = bitcast i16* %130 to <2 x i64>*
  %132 = load <2 x i64>, <2 x i64>* %131, align 16
  %133 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 7
  store <2 x i64> %132, <2 x i64>* %133, align 16
  %134 = getelementptr inbounds i16, i16* %0, i64 8
  %135 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 8
  %136 = bitcast i16* %134 to <2 x i64>*
  %137 = load <2 x i64>, <2 x i64>* %136, align 16
  store <2 x i64> %137, <2 x i64>* %135, align 16
  %138 = getelementptr inbounds i16, i16* %134, i64 %96
  %139 = bitcast i16* %138 to <2 x i64>*
  %140 = load <2 x i64>, <2 x i64>* %139, align 16
  %141 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 9
  store <2 x i64> %140, <2 x i64>* %141, align 16
  %142 = getelementptr inbounds i16, i16* %134, i64 %104
  %143 = bitcast i16* %142 to <2 x i64>*
  %144 = load <2 x i64>, <2 x i64>* %143, align 16
  %145 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 10
  store <2 x i64> %144, <2 x i64>* %145, align 16
  %146 = getelementptr inbounds i16, i16* %134, i64 %109
  %147 = bitcast i16* %146 to <2 x i64>*
  %148 = load <2 x i64>, <2 x i64>* %147, align 16
  %149 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 11
  store <2 x i64> %148, <2 x i64>* %149, align 16
  %150 = getelementptr inbounds i16, i16* %134, i64 %114
  %151 = bitcast i16* %150 to <2 x i64>*
  %152 = load <2 x i64>, <2 x i64>* %151, align 16
  %153 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 12
  store <2 x i64> %152, <2 x i64>* %153, align 16
  %154 = getelementptr inbounds i16, i16* %134, i64 %119
  %155 = bitcast i16* %154 to <2 x i64>*
  %156 = load <2 x i64>, <2 x i64>* %155, align 16
  %157 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 13
  store <2 x i64> %156, <2 x i64>* %157, align 16
  %158 = getelementptr inbounds i16, i16* %134, i64 %124
  %159 = bitcast i16* %158 to <2 x i64>*
  %160 = load <2 x i64>, <2 x i64>* %159, align 16
  %161 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 14
  store <2 x i64> %160, <2 x i64>* %161, align 16
  %162 = getelementptr inbounds i16, i16* %134, i64 %129
  %163 = bitcast i16* %162 to <2 x i64>*
  %164 = load <2 x i64>, <2 x i64>* %163, align 16
  %165 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 15
  store <2 x i64> %164, <2 x i64>* %165, align 16
  br label %166

166:                                              ; preds = %22, %94
  %167 = phi <2 x i64> [ %164, %94 ], [ %65, %22 ]
  %168 = phi <2 x i64> [ %132, %94 ], [ %26, %22 ]
  %169 = phi <2 x i64> [ %160, %94 ], [ %69, %22 ]
  %170 = phi <2 x i64> [ %127, %94 ], [ %30, %22 ]
  %171 = phi <2 x i64> [ %156, %94 ], [ %73, %22 ]
  %172 = phi <2 x i64> [ %122, %94 ], [ %35, %22 ]
  %173 = phi <2 x i64> [ %152, %94 ], [ %77, %22 ]
  %174 = phi <2 x i64> [ %117, %94 ], [ %40, %22 ]
  %175 = phi <2 x i64> [ %148, %94 ], [ %81, %22 ]
  %176 = phi <2 x i64> [ %112, %94 ], [ %45, %22 ]
  %177 = phi <2 x i64> [ %144, %94 ], [ %85, %22 ]
  %178 = phi <2 x i64> [ %107, %94 ], [ %50, %22 ]
  %179 = phi <2 x i64> [ %140, %94 ], [ %89, %22 ]
  %180 = phi <2 x i64> [ %102, %94 ], [ %55, %22 ]
  %181 = phi <2 x i64> [ %137, %94 ], [ %93, %22 ]
  %182 = phi <2 x i64> [ %98, %94 ], [ %60, %22 ]
  %183 = phi <2 x i64>* [ %99, %94 ], [ %61, %22 ]
  %184 = phi i32 [ %95, %94 ], [ %23, %22 ]
  %185 = bitcast <2 x i64> %167 to <4 x i32>
  %186 = bitcast <2 x i64> %169 to <4 x i32>
  %187 = bitcast <2 x i64> %171 to <4 x i32>
  %188 = bitcast <2 x i64> %173 to <4 x i32>
  %189 = bitcast <2 x i64> %175 to <4 x i32>
  %190 = bitcast <2 x i64> %177 to <4 x i32>
  %191 = bitcast <2 x i64> %179 to <4 x i32>
  %192 = bitcast <2 x i64> %181 to <4 x i32>
  %193 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 8
  %194 = getelementptr inbounds [8 x <4 x i64>], [8 x <4 x i64>]* %8, i64 0, i64 0
  %195 = shufflevector <2 x i64> %182, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %196 = bitcast <4 x i64> %195 to <8 x i32>
  %197 = shufflevector <4 x i32> %192, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %198 = shufflevector <8 x i32> %196, <8 x i32> %197, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %199 = bitcast [8 x <4 x i64>]* %8 to <8 x i32>*
  store <8 x i32> %198, <8 x i32>* %199, align 32
  %200 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 1
  %201 = shufflevector <2 x i64> %180, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %202 = bitcast <4 x i64> %201 to <8 x i32>
  %203 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 9
  %204 = shufflevector <4 x i32> %191, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %205 = shufflevector <8 x i32> %202, <8 x i32> %204, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %206 = getelementptr inbounds [8 x <4 x i64>], [8 x <4 x i64>]* %8, i64 0, i64 1
  %207 = bitcast <4 x i64>* %206 to <8 x i32>*
  store <8 x i32> %205, <8 x i32>* %207, align 32
  %208 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 2
  %209 = shufflevector <2 x i64> %178, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %210 = bitcast <4 x i64> %209 to <8 x i32>
  %211 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 10
  %212 = shufflevector <4 x i32> %190, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %213 = shufflevector <8 x i32> %210, <8 x i32> %212, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %214 = getelementptr inbounds [8 x <4 x i64>], [8 x <4 x i64>]* %8, i64 0, i64 2
  %215 = bitcast <4 x i64>* %214 to <8 x i32>*
  store <8 x i32> %213, <8 x i32>* %215, align 32
  %216 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 3
  %217 = shufflevector <2 x i64> %176, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %218 = bitcast <4 x i64> %217 to <8 x i32>
  %219 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 11
  %220 = shufflevector <4 x i32> %189, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %221 = shufflevector <8 x i32> %218, <8 x i32> %220, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %222 = getelementptr inbounds [8 x <4 x i64>], [8 x <4 x i64>]* %8, i64 0, i64 3
  %223 = bitcast <4 x i64>* %222 to <8 x i32>*
  store <8 x i32> %221, <8 x i32>* %223, align 32
  %224 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 4
  %225 = shufflevector <2 x i64> %174, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %226 = bitcast <4 x i64> %225 to <8 x i32>
  %227 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 12
  %228 = shufflevector <4 x i32> %188, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %229 = shufflevector <8 x i32> %226, <8 x i32> %228, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %230 = getelementptr inbounds [8 x <4 x i64>], [8 x <4 x i64>]* %8, i64 0, i64 4
  %231 = bitcast <4 x i64>* %230 to <8 x i32>*
  store <8 x i32> %229, <8 x i32>* %231, align 32
  %232 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 5
  %233 = shufflevector <2 x i64> %172, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %234 = bitcast <4 x i64> %233 to <8 x i32>
  %235 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 13
  %236 = shufflevector <4 x i32> %187, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %237 = shufflevector <8 x i32> %234, <8 x i32> %236, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %238 = getelementptr inbounds [8 x <4 x i64>], [8 x <4 x i64>]* %8, i64 0, i64 5
  %239 = bitcast <4 x i64>* %238 to <8 x i32>*
  store <8 x i32> %237, <8 x i32>* %239, align 32
  %240 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 6
  %241 = shufflevector <2 x i64> %170, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %242 = bitcast <4 x i64> %241 to <8 x i32>
  %243 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 14
  %244 = shufflevector <4 x i32> %186, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %245 = shufflevector <8 x i32> %242, <8 x i32> %244, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %246 = getelementptr inbounds [8 x <4 x i64>], [8 x <4 x i64>]* %8, i64 0, i64 6
  %247 = bitcast <4 x i64>* %246 to <8 x i32>*
  store <8 x i32> %245, <8 x i32>* %247, align 32
  %248 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 7
  %249 = shufflevector <2 x i64> %168, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %250 = bitcast <4 x i64> %249 to <8 x i32>
  %251 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 15
  %252 = shufflevector <4 x i32> %185, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %253 = shufflevector <8 x i32> %250, <8 x i32> %252, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %254 = getelementptr inbounds [8 x <4 x i64>], [8 x <4 x i64>]* %8, i64 0, i64 7
  %255 = bitcast <4 x i64>* %254 to <8 x i32>*
  store <8 x i32> %253, <8 x i32>* %255, align 32
  %256 = load i8, i8* %12, align 1
  %257 = sext i8 %256 to i32
  %258 = icmp slt i8 %256, 0
  %259 = bitcast <8 x i32> %198 to <16 x i16>
  %260 = bitcast <8 x i32> %205 to <16 x i16>
  %261 = bitcast <8 x i32> %213 to <16 x i16>
  %262 = bitcast <8 x i32> %221 to <16 x i16>
  %263 = bitcast <8 x i32> %229 to <16 x i16>
  %264 = bitcast <8 x i32> %237 to <16 x i16>
  %265 = bitcast <8 x i32> %245 to <16 x i16>
  %266 = bitcast <8 x i32> %253 to <16 x i16>
  br i1 %258, label %267, label %298

267:                                              ; preds = %166
  %268 = sub nsw i32 0, %257
  %269 = xor i32 %257, -1
  %270 = shl i32 1, %269
  %271 = trunc i32 %270 to i16
  %272 = insertelement <16 x i16> undef, i16 %271, i32 0
  %273 = shufflevector <16 x i16> %272, <16 x i16> undef, <16 x i32> zeroinitializer
  %274 = bitcast [8 x <4 x i64>]* %8 to <16 x i16>*
  %275 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %259, <16 x i16> %273) #8
  %276 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %275, i32 %268) #8
  store <16 x i16> %276, <16 x i16>* %274, align 32
  %277 = bitcast <4 x i64>* %206 to <16 x i16>*
  %278 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %260, <16 x i16> %273) #8
  %279 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %278, i32 %268) #8
  store <16 x i16> %279, <16 x i16>* %277, align 32
  %280 = bitcast <4 x i64>* %214 to <16 x i16>*
  %281 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %261, <16 x i16> %273) #8
  %282 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %281, i32 %268) #8
  store <16 x i16> %282, <16 x i16>* %280, align 32
  %283 = bitcast <4 x i64>* %222 to <16 x i16>*
  %284 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %262, <16 x i16> %273) #8
  %285 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %284, i32 %268) #8
  store <16 x i16> %285, <16 x i16>* %283, align 32
  %286 = bitcast <4 x i64>* %230 to <16 x i16>*
  %287 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %263, <16 x i16> %273) #8
  %288 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %287, i32 %268) #8
  store <16 x i16> %288, <16 x i16>* %286, align 32
  %289 = bitcast <4 x i64>* %238 to <16 x i16>*
  %290 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %264, <16 x i16> %273) #8
  %291 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %290, i32 %268) #8
  store <16 x i16> %291, <16 x i16>* %289, align 32
  %292 = bitcast <4 x i64>* %246 to <16 x i16>*
  %293 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %265, <16 x i16> %273) #8
  %294 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %293, i32 %268) #8
  store <16 x i16> %294, <16 x i16>* %292, align 32
  %295 = bitcast <4 x i64>* %254 to <16 x i16>*
  %296 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %266, <16 x i16> %273) #8
  %297 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %296, i32 %268) #8
  store <16 x i16> %297, <16 x i16>* %295, align 32
  br label %317

298:                                              ; preds = %166
  %299 = icmp eq i8 %256, 0
  br i1 %299, label %317, label %300

300:                                              ; preds = %298
  %301 = bitcast [8 x <4 x i64>]* %8 to <16 x i16>*
  %302 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %259, i32 %257) #8
  store <16 x i16> %302, <16 x i16>* %301, align 32
  %303 = bitcast <4 x i64>* %206 to <16 x i16>*
  %304 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %260, i32 %257) #8
  store <16 x i16> %304, <16 x i16>* %303, align 32
  %305 = bitcast <4 x i64>* %214 to <16 x i16>*
  %306 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %261, i32 %257) #8
  store <16 x i16> %306, <16 x i16>* %305, align 32
  %307 = bitcast <4 x i64>* %222 to <16 x i16>*
  %308 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %262, i32 %257) #8
  store <16 x i16> %308, <16 x i16>* %307, align 32
  %309 = bitcast <4 x i64>* %230 to <16 x i16>*
  %310 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %263, i32 %257) #8
  store <16 x i16> %310, <16 x i16>* %309, align 32
  %311 = bitcast <4 x i64>* %238 to <16 x i16>*
  %312 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %264, i32 %257) #8
  store <16 x i16> %312, <16 x i16>* %311, align 32
  %313 = bitcast <4 x i64>* %246 to <16 x i16>*
  %314 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %265, i32 %257) #8
  store <16 x i16> %314, <16 x i16>* %313, align 32
  %315 = bitcast <4 x i64>* %254 to <16 x i16>*
  %316 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %266, i32 %257) #8
  store <16 x i16> %316, <16 x i16>* %315, align 32
  br label %317

317:                                              ; preds = %300, %267, %298
  call void %17(<4 x i64>* nonnull %194, <4 x i64>* nonnull %194, i8 signext %13) #8
  %318 = getelementptr inbounds i8, i8* %12, i64 1
  %319 = load i8, i8* %318, align 1
  %320 = sext i8 %319 to i32
  %321 = icmp slt i8 %319, 0
  br i1 %321, label %322, label %361

322:                                              ; preds = %317
  %323 = sub nsw i32 0, %320
  %324 = xor i32 %320, -1
  %325 = shl i32 1, %324
  %326 = trunc i32 %325 to i16
  %327 = insertelement <16 x i16> undef, i16 %326, i32 0
  %328 = shufflevector <16 x i16> %327, <16 x i16> undef, <16 x i32> zeroinitializer
  %329 = bitcast [8 x <4 x i64>]* %8 to <16 x i16>*
  %330 = load <16 x i16>, <16 x i16>* %329, align 32
  %331 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %330, <16 x i16> %328) #8
  %332 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %331, i32 %323) #8
  store <16 x i16> %332, <16 x i16>* %329, align 32
  %333 = bitcast <4 x i64>* %206 to <16 x i16>*
  %334 = load <16 x i16>, <16 x i16>* %333, align 32
  %335 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %334, <16 x i16> %328) #8
  %336 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %335, i32 %323) #8
  store <16 x i16> %336, <16 x i16>* %333, align 32
  %337 = bitcast <4 x i64>* %214 to <16 x i16>*
  %338 = load <16 x i16>, <16 x i16>* %337, align 32
  %339 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %338, <16 x i16> %328) #8
  %340 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %339, i32 %323) #8
  store <16 x i16> %340, <16 x i16>* %337, align 32
  %341 = bitcast <4 x i64>* %222 to <16 x i16>*
  %342 = load <16 x i16>, <16 x i16>* %341, align 32
  %343 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %342, <16 x i16> %328) #8
  %344 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %343, i32 %323) #8
  store <16 x i16> %344, <16 x i16>* %341, align 32
  %345 = bitcast <4 x i64>* %230 to <16 x i16>*
  %346 = load <16 x i16>, <16 x i16>* %345, align 32
  %347 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %346, <16 x i16> %328) #8
  %348 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %347, i32 %323) #8
  store <16 x i16> %348, <16 x i16>* %345, align 32
  %349 = bitcast <4 x i64>* %238 to <16 x i16>*
  %350 = load <16 x i16>, <16 x i16>* %349, align 32
  %351 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %350, <16 x i16> %328) #8
  %352 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %351, i32 %323) #8
  store <16 x i16> %352, <16 x i16>* %349, align 32
  %353 = bitcast <4 x i64>* %246 to <16 x i16>*
  %354 = load <16 x i16>, <16 x i16>* %353, align 32
  %355 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %354, <16 x i16> %328) #8
  %356 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %355, i32 %323) #8
  store <16 x i16> %356, <16 x i16>* %353, align 32
  %357 = bitcast <4 x i64>* %254 to <16 x i16>*
  %358 = load <16 x i16>, <16 x i16>* %357, align 32
  %359 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %358, <16 x i16> %328) #8
  %360 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %359, i32 %323) #8
  store <16 x i16> %360, <16 x i16>* %357, align 32
  br label %403

361:                                              ; preds = %317
  %362 = icmp eq i8 %319, 0
  %363 = bitcast [8 x <4 x i64>]* %8 to <16 x i16>*
  %364 = load <16 x i16>, <16 x i16>* %363, align 32
  br i1 %362, label %365, label %380

365:                                              ; preds = %361
  %366 = bitcast <4 x i64>* %206 to <16 x i16>*
  %367 = load <16 x i16>, <16 x i16>* %366, align 32
  %368 = bitcast <4 x i64>* %214 to <16 x i16>*
  %369 = load <16 x i16>, <16 x i16>* %368, align 32
  %370 = bitcast <4 x i64>* %222 to <16 x i16>*
  %371 = load <16 x i16>, <16 x i16>* %370, align 32
  %372 = bitcast <4 x i64>* %230 to <16 x i16>*
  %373 = load <16 x i16>, <16 x i16>* %372, align 32
  %374 = bitcast <4 x i64>* %238 to <16 x i16>*
  %375 = load <16 x i16>, <16 x i16>* %374, align 32
  %376 = bitcast <4 x i64>* %246 to <16 x i16>*
  %377 = load <16 x i16>, <16 x i16>* %376, align 32
  %378 = bitcast <4 x i64>* %254 to <16 x i16>*
  %379 = load <16 x i16>, <16 x i16>* %378, align 32
  br label %403

380:                                              ; preds = %361
  %381 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %364, i32 %320) #8
  store <16 x i16> %381, <16 x i16>* %363, align 32
  %382 = bitcast <4 x i64>* %206 to <16 x i16>*
  %383 = load <16 x i16>, <16 x i16>* %382, align 32
  %384 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %383, i32 %320) #8
  store <16 x i16> %384, <16 x i16>* %382, align 32
  %385 = bitcast <4 x i64>* %214 to <16 x i16>*
  %386 = load <16 x i16>, <16 x i16>* %385, align 32
  %387 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %386, i32 %320) #8
  store <16 x i16> %387, <16 x i16>* %385, align 32
  %388 = bitcast <4 x i64>* %222 to <16 x i16>*
  %389 = load <16 x i16>, <16 x i16>* %388, align 32
  %390 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %389, i32 %320) #8
  store <16 x i16> %390, <16 x i16>* %388, align 32
  %391 = bitcast <4 x i64>* %230 to <16 x i16>*
  %392 = load <16 x i16>, <16 x i16>* %391, align 32
  %393 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %392, i32 %320) #8
  store <16 x i16> %393, <16 x i16>* %391, align 32
  %394 = bitcast <4 x i64>* %238 to <16 x i16>*
  %395 = load <16 x i16>, <16 x i16>* %394, align 32
  %396 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %395, i32 %320) #8
  store <16 x i16> %396, <16 x i16>* %394, align 32
  %397 = bitcast <4 x i64>* %246 to <16 x i16>*
  %398 = load <16 x i16>, <16 x i16>* %397, align 32
  %399 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %398, i32 %320) #8
  store <16 x i16> %399, <16 x i16>* %397, align 32
  %400 = bitcast <4 x i64>* %254 to <16 x i16>*
  %401 = load <16 x i16>, <16 x i16>* %400, align 32
  %402 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %401, i32 %320) #8
  store <16 x i16> %402, <16 x i16>* %400, align 32
  br label %403

403:                                              ; preds = %365, %380, %322
  %404 = phi <16 x i16> [ %379, %365 ], [ %402, %380 ], [ %360, %322 ]
  %405 = phi <16 x i16> [ %377, %365 ], [ %399, %380 ], [ %356, %322 ]
  %406 = phi <16 x i16> [ %375, %365 ], [ %396, %380 ], [ %352, %322 ]
  %407 = phi <16 x i16> [ %373, %365 ], [ %393, %380 ], [ %348, %322 ]
  %408 = phi <16 x i16> [ %371, %365 ], [ %390, %380 ], [ %344, %322 ]
  %409 = phi <16 x i16> [ %369, %365 ], [ %387, %380 ], [ %340, %322 ]
  %410 = phi <16 x i16> [ %367, %365 ], [ %384, %380 ], [ %336, %322 ]
  %411 = phi <16 x i16> [ %364, %365 ], [ %381, %380 ], [ %332, %322 ]
  %412 = shufflevector <16 x i16> %411, <16 x i16> %410, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %413 = shufflevector <16 x i16> %409, <16 x i16> %408, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %414 = shufflevector <16 x i16> %407, <16 x i16> %406, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %415 = shufflevector <16 x i16> %405, <16 x i16> %404, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %416 = shufflevector <16 x i16> %411, <16 x i16> %410, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %417 = shufflevector <16 x i16> %409, <16 x i16> %408, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %418 = shufflevector <16 x i16> %407, <16 x i16> %406, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %419 = shufflevector <16 x i16> %405, <16 x i16> %404, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %420 = bitcast <16 x i16> %412 to <8 x i32>
  %421 = bitcast <16 x i16> %413 to <8 x i32>
  %422 = shufflevector <8 x i32> %420, <8 x i32> %421, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %423 = bitcast <8 x i32> %422 to <4 x i64>
  %424 = bitcast <16 x i16> %414 to <8 x i32>
  %425 = bitcast <16 x i16> %415 to <8 x i32>
  %426 = shufflevector <8 x i32> %424, <8 x i32> %425, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %427 = bitcast <8 x i32> %426 to <4 x i64>
  %428 = bitcast <16 x i16> %416 to <8 x i32>
  %429 = bitcast <16 x i16> %417 to <8 x i32>
  %430 = shufflevector <8 x i32> %428, <8 x i32> %429, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %431 = bitcast <8 x i32> %430 to <4 x i64>
  %432 = bitcast <16 x i16> %418 to <8 x i32>
  %433 = bitcast <16 x i16> %419 to <8 x i32>
  %434 = shufflevector <8 x i32> %432, <8 x i32> %433, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %435 = bitcast <8 x i32> %434 to <4 x i64>
  %436 = shufflevector <8 x i32> %420, <8 x i32> %421, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %437 = bitcast <8 x i32> %436 to <4 x i64>
  %438 = shufflevector <8 x i32> %424, <8 x i32> %425, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %439 = bitcast <8 x i32> %438 to <4 x i64>
  %440 = shufflevector <8 x i32> %428, <8 x i32> %429, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %441 = bitcast <8 x i32> %440 to <4 x i64>
  %442 = shufflevector <8 x i32> %432, <8 x i32> %433, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %443 = bitcast <8 x i32> %442 to <4 x i64>
  %444 = shufflevector <4 x i64> %423, <4 x i64> %427, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %444, <4 x i64>* %194, align 32
  %445 = shufflevector <4 x i64> %423, <4 x i64> %427, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %445, <4 x i64>* %206, align 32
  %446 = shufflevector <4 x i64> %437, <4 x i64> %439, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %446, <4 x i64>* %214, align 32
  %447 = shufflevector <4 x i64> %437, <4 x i64> %439, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %447, <4 x i64>* %222, align 32
  %448 = shufflevector <4 x i64> %431, <4 x i64> %435, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %448, <4 x i64>* %230, align 32
  %449 = shufflevector <4 x i64> %431, <4 x i64> %435, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %449, <4 x i64>* %238, align 32
  %450 = shufflevector <4 x i64> %441, <4 x i64> %443, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %450, <4 x i64>* %246, align 32
  %451 = shufflevector <4 x i64> %441, <4 x i64> %443, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %451, <4 x i64>* %254, align 32
  %452 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %7, i64 0, i64 0
  %453 = shufflevector <4 x i64> %444, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %453, <2 x i64>* %452, align 16
  %454 = shufflevector <4 x i64> %445, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %455 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %7, i64 0, i64 1
  store <2 x i64> %454, <2 x i64>* %455, align 16
  %456 = shufflevector <4 x i64> %446, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %457 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %7, i64 0, i64 2
  store <2 x i64> %456, <2 x i64>* %457, align 16
  %458 = shufflevector <4 x i64> %447, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %459 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %7, i64 0, i64 3
  store <2 x i64> %458, <2 x i64>* %459, align 16
  %460 = shufflevector <4 x i64> %448, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %461 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %7, i64 0, i64 4
  store <2 x i64> %460, <2 x i64>* %461, align 16
  %462 = shufflevector <4 x i64> %449, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %463 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %7, i64 0, i64 5
  store <2 x i64> %462, <2 x i64>* %463, align 16
  %464 = shufflevector <4 x i64> %450, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %465 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %7, i64 0, i64 6
  store <2 x i64> %464, <2 x i64>* %465, align 16
  %466 = shufflevector <4 x i64> %451, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %467 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %7, i64 0, i64 7
  store <2 x i64> %466, <2 x i64>* %467, align 16
  %468 = shufflevector <4 x i64> %444, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %469 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %7, i64 0, i64 8
  store <2 x i64> %468, <2 x i64>* %469, align 16
  %470 = shufflevector <4 x i64> %445, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %471 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %7, i64 0, i64 9
  store <2 x i64> %470, <2 x i64>* %471, align 16
  %472 = shufflevector <4 x i64> %446, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %473 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %7, i64 0, i64 10
  store <2 x i64> %472, <2 x i64>* %473, align 16
  %474 = shufflevector <4 x i64> %447, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %475 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %7, i64 0, i64 11
  store <2 x i64> %474, <2 x i64>* %475, align 16
  %476 = shufflevector <4 x i64> %448, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %477 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %7, i64 0, i64 12
  store <2 x i64> %476, <2 x i64>* %477, align 16
  %478 = shufflevector <4 x i64> %449, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %479 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %7, i64 0, i64 13
  store <2 x i64> %478, <2 x i64>* %479, align 16
  %480 = shufflevector <4 x i64> %450, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %481 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %7, i64 0, i64 14
  store <2 x i64> %480, <2 x i64>* %481, align 16
  %482 = shufflevector <4 x i64> %451, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %483 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %7, i64 0, i64 15
  store <2 x i64> %482, <2 x i64>* %483, align 16
  %484 = icmp eq i32 %184, 0
  br i1 %484, label %487, label %485

485:                                              ; preds = %403
  store <2 x i64> %453, <2 x i64>* %251, align 16
  store <2 x i64> %454, <2 x i64>* %243, align 16
  store <2 x i64> %456, <2 x i64>* %235, align 16
  store <2 x i64> %458, <2 x i64>* %227, align 16
  store <2 x i64> %460, <2 x i64>* %219, align 16
  store <2 x i64> %462, <2 x i64>* %211, align 16
  store <2 x i64> %464, <2 x i64>* %203, align 16
  store <2 x i64> %466, <2 x i64>* %193, align 16
  store <2 x i64> %468, <2 x i64>* %248, align 16
  store <2 x i64> %470, <2 x i64>* %240, align 16
  store <2 x i64> %472, <2 x i64>* %232, align 16
  store <2 x i64> %474, <2 x i64>* %224, align 16
  store <2 x i64> %476, <2 x i64>* %216, align 16
  store <2 x i64> %478, <2 x i64>* %208, align 16
  store <2 x i64> %480, <2 x i64>* %200, align 16
  %486 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 0
  store <2 x i64> %482, <2 x i64>* %486, align 16
  br label %487

487:                                              ; preds = %485, %403
  %488 = phi <2 x i64>* [ %452, %403 ], [ %183, %485 ]
  call void %19(<2 x i64>* %488, <2 x i64>* %488, i8 signext %14) #8
  %489 = getelementptr inbounds i8, i8* %12, i64 2
  %490 = load i8, i8* %489, align 1
  %491 = sext i8 %490 to i32
  %492 = icmp slt i8 %490, 0
  br i1 %492, label %493, label %579

493:                                              ; preds = %487
  %494 = sub nsw i32 0, %491
  %495 = xor i32 %491, -1
  %496 = shl i32 1, %495
  %497 = trunc i32 %496 to i16
  %498 = insertelement <8 x i16> undef, i16 %497, i32 0
  %499 = shufflevector <8 x i16> %498, <8 x i16> undef, <8 x i32> zeroinitializer
  %500 = bitcast <2 x i64>* %488 to <8 x i16>*
  %501 = load <8 x i16>, <8 x i16>* %500, align 16
  %502 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %501, <8 x i16> %499) #8
  %503 = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %502, i32 %494) #8
  store <8 x i16> %503, <8 x i16>* %500, align 16
  %504 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 1
  %505 = bitcast <2 x i64>* %504 to <8 x i16>*
  %506 = load <8 x i16>, <8 x i16>* %505, align 16
  %507 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %506, <8 x i16> %499) #8
  %508 = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %507, i32 %494) #8
  store <8 x i16> %508, <8 x i16>* %505, align 16
  %509 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 2
  %510 = bitcast <2 x i64>* %509 to <8 x i16>*
  %511 = load <8 x i16>, <8 x i16>* %510, align 16
  %512 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %511, <8 x i16> %499) #8
  %513 = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %512, i32 %494) #8
  store <8 x i16> %513, <8 x i16>* %510, align 16
  %514 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 3
  %515 = bitcast <2 x i64>* %514 to <8 x i16>*
  %516 = load <8 x i16>, <8 x i16>* %515, align 16
  %517 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %516, <8 x i16> %499) #8
  %518 = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %517, i32 %494) #8
  store <8 x i16> %518, <8 x i16>* %515, align 16
  %519 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 4
  %520 = bitcast <2 x i64>* %519 to <8 x i16>*
  %521 = load <8 x i16>, <8 x i16>* %520, align 16
  %522 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %521, <8 x i16> %499) #8
  %523 = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %522, i32 %494) #8
  store <8 x i16> %523, <8 x i16>* %520, align 16
  %524 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 5
  %525 = bitcast <2 x i64>* %524 to <8 x i16>*
  %526 = load <8 x i16>, <8 x i16>* %525, align 16
  %527 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %526, <8 x i16> %499) #8
  %528 = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %527, i32 %494) #8
  store <8 x i16> %528, <8 x i16>* %525, align 16
  %529 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 6
  %530 = bitcast <2 x i64>* %529 to <8 x i16>*
  %531 = load <8 x i16>, <8 x i16>* %530, align 16
  %532 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %531, <8 x i16> %499) #8
  %533 = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %532, i32 %494) #8
  store <8 x i16> %533, <8 x i16>* %530, align 16
  %534 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 7
  %535 = bitcast <2 x i64>* %534 to <8 x i16>*
  %536 = load <8 x i16>, <8 x i16>* %535, align 16
  %537 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %536, <8 x i16> %499) #8
  %538 = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %537, i32 %494) #8
  store <8 x i16> %538, <8 x i16>* %535, align 16
  %539 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 8
  %540 = bitcast <2 x i64>* %539 to <8 x i16>*
  %541 = load <8 x i16>, <8 x i16>* %540, align 16
  %542 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %541, <8 x i16> %499) #8
  %543 = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %542, i32 %494) #8
  store <8 x i16> %543, <8 x i16>* %540, align 16
  %544 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 9
  %545 = bitcast <2 x i64>* %544 to <8 x i16>*
  %546 = load <8 x i16>, <8 x i16>* %545, align 16
  %547 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %546, <8 x i16> %499) #8
  %548 = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %547, i32 %494) #8
  store <8 x i16> %548, <8 x i16>* %545, align 16
  %549 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 10
  %550 = bitcast <2 x i64>* %549 to <8 x i16>*
  %551 = load <8 x i16>, <8 x i16>* %550, align 16
  %552 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %551, <8 x i16> %499) #8
  %553 = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %552, i32 %494) #8
  store <8 x i16> %553, <8 x i16>* %550, align 16
  %554 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 11
  %555 = bitcast <2 x i64>* %554 to <8 x i16>*
  %556 = load <8 x i16>, <8 x i16>* %555, align 16
  %557 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %556, <8 x i16> %499) #8
  %558 = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %557, i32 %494) #8
  store <8 x i16> %558, <8 x i16>* %555, align 16
  %559 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 12
  %560 = bitcast <2 x i64>* %559 to <8 x i16>*
  %561 = load <8 x i16>, <8 x i16>* %560, align 16
  %562 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %561, <8 x i16> %499) #8
  %563 = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %562, i32 %494) #8
  store <8 x i16> %563, <8 x i16>* %560, align 16
  %564 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 13
  %565 = bitcast <2 x i64>* %564 to <8 x i16>*
  %566 = load <8 x i16>, <8 x i16>* %565, align 16
  %567 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %566, <8 x i16> %499) #8
  %568 = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %567, i32 %494) #8
  store <8 x i16> %568, <8 x i16>* %565, align 16
  %569 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 14
  %570 = bitcast <2 x i64>* %569 to <8 x i16>*
  %571 = load <8 x i16>, <8 x i16>* %570, align 16
  %572 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %571, <8 x i16> %499) #8
  %573 = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %572, i32 %494) #8
  store <8 x i16> %573, <8 x i16>* %570, align 16
  %574 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 15
  %575 = bitcast <2 x i64>* %574 to <8 x i16>*
  %576 = load <8 x i16>, <8 x i16>* %575, align 16
  %577 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %576, <8 x i16> %499) #8
  %578 = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %577, i32 %494) #8
  store <8 x i16> %578, <8 x i16>* %575, align 16
  br label %667

579:                                              ; preds = %487
  %580 = icmp eq i8 %490, 0
  %581 = bitcast <2 x i64>* %488 to <8 x i16>*
  %582 = load <8 x i16>, <8 x i16>* %581, align 16
  br i1 %580, label %583, label %605

583:                                              ; preds = %579
  %584 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 1
  %585 = bitcast <2 x i64>* %584 to <8 x i16>*
  %586 = load <8 x i16>, <8 x i16>* %585, align 16
  %587 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 2
  %588 = bitcast <2 x i64>* %587 to <8 x i16>*
  %589 = load <8 x i16>, <8 x i16>* %588, align 16
  %590 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 3
  %591 = bitcast <2 x i64>* %590 to <8 x i16>*
  %592 = load <8 x i16>, <8 x i16>* %591, align 16
  %593 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 4
  %594 = bitcast <2 x i64>* %593 to <8 x i16>*
  %595 = load <8 x i16>, <8 x i16>* %594, align 16
  %596 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 5
  %597 = bitcast <2 x i64>* %596 to <8 x i16>*
  %598 = load <8 x i16>, <8 x i16>* %597, align 16
  %599 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 6
  %600 = bitcast <2 x i64>* %599 to <8 x i16>*
  %601 = load <8 x i16>, <8 x i16>* %600, align 16
  %602 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 7
  %603 = bitcast <2 x i64>* %602 to <8 x i16>*
  %604 = load <8 x i16>, <8 x i16>* %603, align 16
  br label %667

605:                                              ; preds = %579
  %606 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %582, i32 %491) #8
  store <8 x i16> %606, <8 x i16>* %581, align 16
  %607 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 1
  %608 = bitcast <2 x i64>* %607 to <8 x i16>*
  %609 = load <8 x i16>, <8 x i16>* %608, align 16
  %610 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %609, i32 %491) #8
  store <8 x i16> %610, <8 x i16>* %608, align 16
  %611 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 2
  %612 = bitcast <2 x i64>* %611 to <8 x i16>*
  %613 = load <8 x i16>, <8 x i16>* %612, align 16
  %614 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %613, i32 %491) #8
  store <8 x i16> %614, <8 x i16>* %612, align 16
  %615 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 3
  %616 = bitcast <2 x i64>* %615 to <8 x i16>*
  %617 = load <8 x i16>, <8 x i16>* %616, align 16
  %618 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %617, i32 %491) #8
  store <8 x i16> %618, <8 x i16>* %616, align 16
  %619 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 4
  %620 = bitcast <2 x i64>* %619 to <8 x i16>*
  %621 = load <8 x i16>, <8 x i16>* %620, align 16
  %622 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %621, i32 %491) #8
  store <8 x i16> %622, <8 x i16>* %620, align 16
  %623 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 5
  %624 = bitcast <2 x i64>* %623 to <8 x i16>*
  %625 = load <8 x i16>, <8 x i16>* %624, align 16
  %626 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %625, i32 %491) #8
  store <8 x i16> %626, <8 x i16>* %624, align 16
  %627 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 6
  %628 = bitcast <2 x i64>* %627 to <8 x i16>*
  %629 = load <8 x i16>, <8 x i16>* %628, align 16
  %630 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %629, i32 %491) #8
  store <8 x i16> %630, <8 x i16>* %628, align 16
  %631 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 7
  %632 = bitcast <2 x i64>* %631 to <8 x i16>*
  %633 = load <8 x i16>, <8 x i16>* %632, align 16
  %634 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %633, i32 %491) #8
  store <8 x i16> %634, <8 x i16>* %632, align 16
  %635 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 8
  %636 = bitcast <2 x i64>* %635 to <8 x i16>*
  %637 = load <8 x i16>, <8 x i16>* %636, align 16
  %638 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %637, i32 %491) #8
  store <8 x i16> %638, <8 x i16>* %636, align 16
  %639 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 9
  %640 = bitcast <2 x i64>* %639 to <8 x i16>*
  %641 = load <8 x i16>, <8 x i16>* %640, align 16
  %642 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %641, i32 %491) #8
  store <8 x i16> %642, <8 x i16>* %640, align 16
  %643 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 10
  %644 = bitcast <2 x i64>* %643 to <8 x i16>*
  %645 = load <8 x i16>, <8 x i16>* %644, align 16
  %646 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %645, i32 %491) #8
  store <8 x i16> %646, <8 x i16>* %644, align 16
  %647 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 11
  %648 = bitcast <2 x i64>* %647 to <8 x i16>*
  %649 = load <8 x i16>, <8 x i16>* %648, align 16
  %650 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %649, i32 %491) #8
  store <8 x i16> %650, <8 x i16>* %648, align 16
  %651 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 12
  %652 = bitcast <2 x i64>* %651 to <8 x i16>*
  %653 = load <8 x i16>, <8 x i16>* %652, align 16
  %654 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %653, i32 %491) #8
  store <8 x i16> %654, <8 x i16>* %652, align 16
  %655 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 13
  %656 = bitcast <2 x i64>* %655 to <8 x i16>*
  %657 = load <8 x i16>, <8 x i16>* %656, align 16
  %658 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %657, i32 %491) #8
  store <8 x i16> %658, <8 x i16>* %656, align 16
  %659 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 14
  %660 = bitcast <2 x i64>* %659 to <8 x i16>*
  %661 = load <8 x i16>, <8 x i16>* %660, align 16
  %662 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %661, i32 %491) #8
  store <8 x i16> %662, <8 x i16>* %660, align 16
  %663 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 15
  %664 = bitcast <2 x i64>* %663 to <8 x i16>*
  %665 = load <8 x i16>, <8 x i16>* %664, align 16
  %666 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %665, i32 %491) #8
  store <8 x i16> %666, <8 x i16>* %664, align 16
  br label %667

667:                                              ; preds = %583, %493, %605
  %668 = phi <8 x i16> [ %604, %583 ], [ %538, %493 ], [ %634, %605 ]
  %669 = phi <8 x i16> [ %601, %583 ], [ %533, %493 ], [ %630, %605 ]
  %670 = phi <8 x i16> [ %598, %583 ], [ %528, %493 ], [ %626, %605 ]
  %671 = phi <8 x i16> [ %595, %583 ], [ %523, %493 ], [ %622, %605 ]
  %672 = phi <8 x i16> [ %592, %583 ], [ %518, %493 ], [ %618, %605 ]
  %673 = phi <8 x i16> [ %589, %583 ], [ %513, %493 ], [ %614, %605 ]
  %674 = phi <8 x i16> [ %586, %583 ], [ %508, %493 ], [ %610, %605 ]
  %675 = phi <8 x i16> [ %582, %583 ], [ %503, %493 ], [ %606, %605 ]
  %676 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 1
  %677 = shufflevector <8 x i16> %675, <8 x i16> %674, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %678 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 2
  %679 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 3
  %680 = shufflevector <8 x i16> %673, <8 x i16> %672, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %681 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 4
  %682 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 5
  %683 = shufflevector <8 x i16> %671, <8 x i16> %670, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %684 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 6
  %685 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 7
  %686 = shufflevector <8 x i16> %669, <8 x i16> %668, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %687 = shufflevector <8 x i16> %675, <8 x i16> %674, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %688 = shufflevector <8 x i16> %673, <8 x i16> %672, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %689 = shufflevector <8 x i16> %671, <8 x i16> %670, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %690 = shufflevector <8 x i16> %669, <8 x i16> %668, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %691 = bitcast <8 x i16> %677 to <4 x i32>
  %692 = bitcast <8 x i16> %680 to <4 x i32>
  %693 = shufflevector <4 x i32> %691, <4 x i32> %692, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %694 = bitcast <4 x i32> %693 to <2 x i64>
  %695 = bitcast <8 x i16> %683 to <4 x i32>
  %696 = bitcast <8 x i16> %686 to <4 x i32>
  %697 = shufflevector <4 x i32> %695, <4 x i32> %696, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %698 = bitcast <4 x i32> %697 to <2 x i64>
  %699 = bitcast <8 x i16> %687 to <4 x i32>
  %700 = bitcast <8 x i16> %688 to <4 x i32>
  %701 = shufflevector <4 x i32> %699, <4 x i32> %700, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %702 = bitcast <4 x i32> %701 to <2 x i64>
  %703 = bitcast <8 x i16> %689 to <4 x i32>
  %704 = bitcast <8 x i16> %690 to <4 x i32>
  %705 = shufflevector <4 x i32> %703, <4 x i32> %704, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %706 = bitcast <4 x i32> %705 to <2 x i64>
  %707 = shufflevector <4 x i32> %691, <4 x i32> %692, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %708 = bitcast <4 x i32> %707 to <2 x i64>
  %709 = shufflevector <4 x i32> %695, <4 x i32> %696, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %710 = bitcast <4 x i32> %709 to <2 x i64>
  %711 = shufflevector <4 x i32> %699, <4 x i32> %700, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %712 = bitcast <4 x i32> %711 to <2 x i64>
  %713 = shufflevector <4 x i32> %703, <4 x i32> %704, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %714 = bitcast <4 x i32> %713 to <2 x i64>
  %715 = shufflevector <2 x i64> %694, <2 x i64> %698, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %715, <2 x i64>* %488, align 16
  %716 = shufflevector <2 x i64> %694, <2 x i64> %698, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %716, <2 x i64>* %676, align 16
  %717 = shufflevector <2 x i64> %708, <2 x i64> %710, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %717, <2 x i64>* %678, align 16
  %718 = shufflevector <2 x i64> %708, <2 x i64> %710, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %718, <2 x i64>* %679, align 16
  %719 = shufflevector <2 x i64> %702, <2 x i64> %706, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %719, <2 x i64>* %681, align 16
  %720 = shufflevector <2 x i64> %702, <2 x i64> %706, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %720, <2 x i64>* %682, align 16
  %721 = shufflevector <2 x i64> %712, <2 x i64> %714, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %721, <2 x i64>* %684, align 16
  %722 = shufflevector <2 x i64> %712, <2 x i64> %714, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %722, <2 x i64>* %685, align 16
  %723 = bitcast <2 x i64> %715 to <8 x i16>
  %724 = shufflevector <8 x i16> %723, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %725 = shufflevector <8 x i16> %723, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %726 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %724, <8 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %727 = ashr <4 x i32> %726, <i32 12, i32 12, i32 12, i32 12>
  %728 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %725, <8 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %729 = ashr <4 x i32> %728, <i32 12, i32 12, i32 12, i32 12>
  %730 = bitcast i32* %1 to <4 x i32>*
  store <4 x i32> %727, <4 x i32>* %730, align 16
  %731 = getelementptr inbounds i32, i32* %1, i64 4
  %732 = bitcast i32* %731 to <4 x i32>*
  store <4 x i32> %729, <4 x i32>* %732, align 16
  %733 = bitcast <2 x i64> %716 to <8 x i16>
  %734 = getelementptr inbounds i32, i32* %1, i64 16
  %735 = shufflevector <8 x i16> %733, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %736 = shufflevector <8 x i16> %733, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %737 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %735, <8 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %738 = ashr <4 x i32> %737, <i32 12, i32 12, i32 12, i32 12>
  %739 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %736, <8 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %740 = ashr <4 x i32> %739, <i32 12, i32 12, i32 12, i32 12>
  %741 = bitcast i32* %734 to <4 x i32>*
  store <4 x i32> %738, <4 x i32>* %741, align 16
  %742 = getelementptr inbounds i32, i32* %1, i64 20
  %743 = bitcast i32* %742 to <4 x i32>*
  store <4 x i32> %740, <4 x i32>* %743, align 16
  %744 = bitcast <2 x i64> %717 to <8 x i16>
  %745 = getelementptr inbounds i32, i32* %1, i64 32
  %746 = shufflevector <8 x i16> %744, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %747 = shufflevector <8 x i16> %744, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %748 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %746, <8 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %749 = ashr <4 x i32> %748, <i32 12, i32 12, i32 12, i32 12>
  %750 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %747, <8 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %751 = ashr <4 x i32> %750, <i32 12, i32 12, i32 12, i32 12>
  %752 = bitcast i32* %745 to <4 x i32>*
  store <4 x i32> %749, <4 x i32>* %752, align 16
  %753 = getelementptr inbounds i32, i32* %1, i64 36
  %754 = bitcast i32* %753 to <4 x i32>*
  store <4 x i32> %751, <4 x i32>* %754, align 16
  %755 = bitcast <2 x i64> %718 to <8 x i16>
  %756 = getelementptr inbounds i32, i32* %1, i64 48
  %757 = shufflevector <8 x i16> %755, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %758 = shufflevector <8 x i16> %755, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %759 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %757, <8 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %760 = ashr <4 x i32> %759, <i32 12, i32 12, i32 12, i32 12>
  %761 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %758, <8 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %762 = ashr <4 x i32> %761, <i32 12, i32 12, i32 12, i32 12>
  %763 = bitcast i32* %756 to <4 x i32>*
  store <4 x i32> %760, <4 x i32>* %763, align 16
  %764 = getelementptr inbounds i32, i32* %1, i64 52
  %765 = bitcast i32* %764 to <4 x i32>*
  store <4 x i32> %762, <4 x i32>* %765, align 16
  %766 = bitcast <2 x i64> %719 to <8 x i16>
  %767 = getelementptr inbounds i32, i32* %1, i64 64
  %768 = shufflevector <8 x i16> %766, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %769 = shufflevector <8 x i16> %766, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %770 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %768, <8 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %771 = ashr <4 x i32> %770, <i32 12, i32 12, i32 12, i32 12>
  %772 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %769, <8 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %773 = ashr <4 x i32> %772, <i32 12, i32 12, i32 12, i32 12>
  %774 = bitcast i32* %767 to <4 x i32>*
  store <4 x i32> %771, <4 x i32>* %774, align 16
  %775 = getelementptr inbounds i32, i32* %1, i64 68
  %776 = bitcast i32* %775 to <4 x i32>*
  store <4 x i32> %773, <4 x i32>* %776, align 16
  %777 = bitcast <2 x i64> %720 to <8 x i16>
  %778 = getelementptr inbounds i32, i32* %1, i64 80
  %779 = shufflevector <8 x i16> %777, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %780 = shufflevector <8 x i16> %777, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %781 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %779, <8 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %782 = ashr <4 x i32> %781, <i32 12, i32 12, i32 12, i32 12>
  %783 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %780, <8 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %784 = ashr <4 x i32> %783, <i32 12, i32 12, i32 12, i32 12>
  %785 = bitcast i32* %778 to <4 x i32>*
  store <4 x i32> %782, <4 x i32>* %785, align 16
  %786 = getelementptr inbounds i32, i32* %1, i64 84
  %787 = bitcast i32* %786 to <4 x i32>*
  store <4 x i32> %784, <4 x i32>* %787, align 16
  %788 = bitcast <2 x i64> %721 to <8 x i16>
  %789 = getelementptr inbounds i32, i32* %1, i64 96
  %790 = shufflevector <8 x i16> %788, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %791 = shufflevector <8 x i16> %788, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %792 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %790, <8 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %793 = ashr <4 x i32> %792, <i32 12, i32 12, i32 12, i32 12>
  %794 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %791, <8 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %795 = ashr <4 x i32> %794, <i32 12, i32 12, i32 12, i32 12>
  %796 = bitcast i32* %789 to <4 x i32>*
  store <4 x i32> %793, <4 x i32>* %796, align 16
  %797 = getelementptr inbounds i32, i32* %1, i64 100
  %798 = bitcast i32* %797 to <4 x i32>*
  store <4 x i32> %795, <4 x i32>* %798, align 16
  %799 = bitcast <2 x i64> %722 to <8 x i16>
  %800 = getelementptr inbounds i32, i32* %1, i64 112
  %801 = shufflevector <8 x i16> %799, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %802 = shufflevector <8 x i16> %799, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %803 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %801, <8 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %804 = ashr <4 x i32> %803, <i32 12, i32 12, i32 12, i32 12>
  %805 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %802, <8 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %806 = ashr <4 x i32> %805, <i32 12, i32 12, i32 12, i32 12>
  %807 = bitcast i32* %800 to <4 x i32>*
  store <4 x i32> %804, <4 x i32>* %807, align 16
  %808 = getelementptr inbounds i32, i32* %1, i64 116
  %809 = bitcast i32* %808 to <4 x i32>*
  store <4 x i32> %806, <4 x i32>* %809, align 16
  %810 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 8
  %811 = bitcast <2 x i64>* %810 to <8 x i16>*
  %812 = load <8 x i16>, <8 x i16>* %811, align 16
  %813 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 9
  %814 = bitcast <2 x i64>* %813 to <8 x i16>*
  %815 = load <8 x i16>, <8 x i16>* %814, align 16
  %816 = shufflevector <8 x i16> %812, <8 x i16> %815, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %817 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 10
  %818 = bitcast <2 x i64>* %817 to <8 x i16>*
  %819 = load <8 x i16>, <8 x i16>* %818, align 16
  %820 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 11
  %821 = bitcast <2 x i64>* %820 to <8 x i16>*
  %822 = load <8 x i16>, <8 x i16>* %821, align 16
  %823 = shufflevector <8 x i16> %819, <8 x i16> %822, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %824 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 12
  %825 = bitcast <2 x i64>* %824 to <8 x i16>*
  %826 = load <8 x i16>, <8 x i16>* %825, align 16
  %827 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 13
  %828 = bitcast <2 x i64>* %827 to <8 x i16>*
  %829 = load <8 x i16>, <8 x i16>* %828, align 16
  %830 = shufflevector <8 x i16> %826, <8 x i16> %829, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %831 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 14
  %832 = bitcast <2 x i64>* %831 to <8 x i16>*
  %833 = load <8 x i16>, <8 x i16>* %832, align 16
  %834 = getelementptr inbounds <2 x i64>, <2 x i64>* %488, i64 15
  %835 = bitcast <2 x i64>* %834 to <8 x i16>*
  %836 = load <8 x i16>, <8 x i16>* %835, align 16
  %837 = shufflevector <8 x i16> %833, <8 x i16> %836, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %838 = shufflevector <8 x i16> %812, <8 x i16> %815, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %839 = shufflevector <8 x i16> %819, <8 x i16> %822, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %840 = shufflevector <8 x i16> %826, <8 x i16> %829, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %841 = shufflevector <8 x i16> %833, <8 x i16> %836, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %842 = bitcast <8 x i16> %816 to <4 x i32>
  %843 = bitcast <8 x i16> %823 to <4 x i32>
  %844 = shufflevector <4 x i32> %842, <4 x i32> %843, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %845 = bitcast <4 x i32> %844 to <2 x i64>
  %846 = bitcast <8 x i16> %830 to <4 x i32>
  %847 = bitcast <8 x i16> %837 to <4 x i32>
  %848 = shufflevector <4 x i32> %846, <4 x i32> %847, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %849 = bitcast <4 x i32> %848 to <2 x i64>
  %850 = bitcast <8 x i16> %838 to <4 x i32>
  %851 = bitcast <8 x i16> %839 to <4 x i32>
  %852 = shufflevector <4 x i32> %850, <4 x i32> %851, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %853 = bitcast <4 x i32> %852 to <2 x i64>
  %854 = bitcast <8 x i16> %840 to <4 x i32>
  %855 = bitcast <8 x i16> %841 to <4 x i32>
  %856 = shufflevector <4 x i32> %854, <4 x i32> %855, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %857 = bitcast <4 x i32> %856 to <2 x i64>
  %858 = shufflevector <4 x i32> %842, <4 x i32> %843, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %859 = bitcast <4 x i32> %858 to <2 x i64>
  %860 = shufflevector <4 x i32> %846, <4 x i32> %847, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %861 = bitcast <4 x i32> %860 to <2 x i64>
  %862 = shufflevector <4 x i32> %850, <4 x i32> %851, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %863 = bitcast <4 x i32> %862 to <2 x i64>
  %864 = shufflevector <4 x i32> %854, <4 x i32> %855, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %865 = bitcast <4 x i32> %864 to <2 x i64>
  %866 = shufflevector <2 x i64> %845, <2 x i64> %849, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %866, <2 x i64>* %810, align 16
  %867 = shufflevector <2 x i64> %845, <2 x i64> %849, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %867, <2 x i64>* %813, align 16
  %868 = shufflevector <2 x i64> %859, <2 x i64> %861, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %868, <2 x i64>* %817, align 16
  %869 = shufflevector <2 x i64> %859, <2 x i64> %861, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %869, <2 x i64>* %820, align 16
  %870 = shufflevector <2 x i64> %853, <2 x i64> %857, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %870, <2 x i64>* %824, align 16
  %871 = shufflevector <2 x i64> %853, <2 x i64> %857, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %871, <2 x i64>* %827, align 16
  %872 = shufflevector <2 x i64> %863, <2 x i64> %865, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %872, <2 x i64>* %831, align 16
  %873 = shufflevector <2 x i64> %863, <2 x i64> %865, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %873, <2 x i64>* %834, align 16
  %874 = getelementptr inbounds i32, i32* %1, i64 8
  %875 = bitcast <2 x i64> %866 to <8 x i16>
  %876 = shufflevector <8 x i16> %875, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %877 = shufflevector <8 x i16> %875, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %878 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %876, <8 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %879 = ashr <4 x i32> %878, <i32 12, i32 12, i32 12, i32 12>
  %880 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %877, <8 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %881 = ashr <4 x i32> %880, <i32 12, i32 12, i32 12, i32 12>
  %882 = bitcast i32* %874 to <4 x i32>*
  store <4 x i32> %879, <4 x i32>* %882, align 16
  %883 = getelementptr inbounds i32, i32* %1, i64 12
  %884 = bitcast i32* %883 to <4 x i32>*
  store <4 x i32> %881, <4 x i32>* %884, align 16
  %885 = bitcast <2 x i64> %867 to <8 x i16>
  %886 = getelementptr inbounds i32, i32* %1, i64 24
  %887 = shufflevector <8 x i16> %885, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %888 = shufflevector <8 x i16> %885, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %889 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %887, <8 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %890 = ashr <4 x i32> %889, <i32 12, i32 12, i32 12, i32 12>
  %891 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %888, <8 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %892 = ashr <4 x i32> %891, <i32 12, i32 12, i32 12, i32 12>
  %893 = bitcast i32* %886 to <4 x i32>*
  store <4 x i32> %890, <4 x i32>* %893, align 16
  %894 = getelementptr inbounds i32, i32* %1, i64 28
  %895 = bitcast i32* %894 to <4 x i32>*
  store <4 x i32> %892, <4 x i32>* %895, align 16
  %896 = bitcast <2 x i64> %868 to <8 x i16>
  %897 = getelementptr inbounds i32, i32* %1, i64 40
  %898 = shufflevector <8 x i16> %896, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %899 = shufflevector <8 x i16> %896, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %900 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %898, <8 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %901 = ashr <4 x i32> %900, <i32 12, i32 12, i32 12, i32 12>
  %902 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %899, <8 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %903 = ashr <4 x i32> %902, <i32 12, i32 12, i32 12, i32 12>
  %904 = bitcast i32* %897 to <4 x i32>*
  store <4 x i32> %901, <4 x i32>* %904, align 16
  %905 = getelementptr inbounds i32, i32* %1, i64 44
  %906 = bitcast i32* %905 to <4 x i32>*
  store <4 x i32> %903, <4 x i32>* %906, align 16
  %907 = bitcast <2 x i64> %869 to <8 x i16>
  %908 = getelementptr inbounds i32, i32* %1, i64 56
  %909 = shufflevector <8 x i16> %907, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %910 = shufflevector <8 x i16> %907, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %911 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %909, <8 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %912 = ashr <4 x i32> %911, <i32 12, i32 12, i32 12, i32 12>
  %913 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %910, <8 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %914 = ashr <4 x i32> %913, <i32 12, i32 12, i32 12, i32 12>
  %915 = bitcast i32* %908 to <4 x i32>*
  store <4 x i32> %912, <4 x i32>* %915, align 16
  %916 = getelementptr inbounds i32, i32* %1, i64 60
  %917 = bitcast i32* %916 to <4 x i32>*
  store <4 x i32> %914, <4 x i32>* %917, align 16
  %918 = bitcast <2 x i64> %870 to <8 x i16>
  %919 = getelementptr inbounds i32, i32* %1, i64 72
  %920 = shufflevector <8 x i16> %918, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %921 = shufflevector <8 x i16> %918, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %922 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %920, <8 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %923 = ashr <4 x i32> %922, <i32 12, i32 12, i32 12, i32 12>
  %924 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %921, <8 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %925 = ashr <4 x i32> %924, <i32 12, i32 12, i32 12, i32 12>
  %926 = bitcast i32* %919 to <4 x i32>*
  store <4 x i32> %923, <4 x i32>* %926, align 16
  %927 = getelementptr inbounds i32, i32* %1, i64 76
  %928 = bitcast i32* %927 to <4 x i32>*
  store <4 x i32> %925, <4 x i32>* %928, align 16
  %929 = bitcast <2 x i64> %871 to <8 x i16>
  %930 = getelementptr inbounds i32, i32* %1, i64 88
  %931 = shufflevector <8 x i16> %929, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %932 = shufflevector <8 x i16> %929, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %933 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %931, <8 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %934 = ashr <4 x i32> %933, <i32 12, i32 12, i32 12, i32 12>
  %935 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %932, <8 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %936 = ashr <4 x i32> %935, <i32 12, i32 12, i32 12, i32 12>
  %937 = bitcast i32* %930 to <4 x i32>*
  store <4 x i32> %934, <4 x i32>* %937, align 16
  %938 = getelementptr inbounds i32, i32* %1, i64 92
  %939 = bitcast i32* %938 to <4 x i32>*
  store <4 x i32> %936, <4 x i32>* %939, align 16
  %940 = bitcast <2 x i64> %872 to <8 x i16>
  %941 = getelementptr inbounds i32, i32* %1, i64 104
  %942 = shufflevector <8 x i16> %940, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %943 = shufflevector <8 x i16> %940, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %944 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %942, <8 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %945 = ashr <4 x i32> %944, <i32 12, i32 12, i32 12, i32 12>
  %946 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %943, <8 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %947 = ashr <4 x i32> %946, <i32 12, i32 12, i32 12, i32 12>
  %948 = bitcast i32* %941 to <4 x i32>*
  store <4 x i32> %945, <4 x i32>* %948, align 16
  %949 = getelementptr inbounds i32, i32* %1, i64 108
  %950 = bitcast i32* %949 to <4 x i32>*
  store <4 x i32> %947, <4 x i32>* %950, align 16
  %951 = bitcast <2 x i64> %873 to <8 x i16>
  %952 = getelementptr inbounds i32, i32* %1, i64 120
  %953 = shufflevector <8 x i16> %951, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %954 = shufflevector <8 x i16> %951, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %955 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %953, <8 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %956 = ashr <4 x i32> %955, <i32 12, i32 12, i32 12, i32 12>
  %957 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %954, <8 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %958 = ashr <4 x i32> %957, <i32 12, i32 12, i32 12, i32 12>
  %959 = bitcast i32* %952 to <4 x i32>*
  store <4 x i32> %956, <4 x i32>* %959, align 16
  %960 = getelementptr inbounds i32, i32* %1, i64 124
  %961 = bitcast i32* %960 to <4 x i32>*
  store <4 x i32> %958, <4 x i32>* %961, align 16
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %11) #8
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %10) #8
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %9) #8
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @lowbd_fwd_txfm2d_16x32_avx2(i16* nocapture readonly, i32* nocapture, i32, i8 zeroext, i32) #3 {
  %6 = alloca [32 x <4 x i64>], align 32
  %7 = alloca [32 x <4 x i64>], align 32
  %8 = bitcast [32 x <4 x i64>]* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 1024, i8* nonnull %8) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %8, i8 -86, i64 1024, i1 false)
  %9 = bitcast [32 x <4 x i64>]* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 1024, i8* nonnull %9) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %9, i8 -86, i64 1024, i1 false)
  %10 = load i8*, i8** getelementptr inbounds ([19 x i8*], [19 x i8*]* @av1_fwd_txfm_shift_ls, i64 0, i64 9), align 8
  %11 = load i8, i8* getelementptr inbounds ([5 x [5 x i8]], [5 x [5 x i8]]* @av1_fwd_cos_bit_col, i64 0, i64 2, i64 3), align 1
  %12 = load i8, i8* getelementptr inbounds ([5 x [5 x i8]], [5 x [5 x i8]]* @av1_fwd_cos_bit_row, i64 0, i64 2, i64 3), align 1
  %13 = zext i8 %3 to i64
  %14 = getelementptr inbounds [16 x void (<4 x i64>*, <4 x i64>*, i8)*], [16 x void (<4 x i64>*, <4 x i64>*, i8)*]* @col_txfm16x32_arr, i64 0, i64 %13
  %15 = load void (<4 x i64>*, <4 x i64>*, i8)*, void (<4 x i64>*, <4 x i64>*, i8)** %14, align 8
  %16 = getelementptr inbounds [16 x void (<4 x i64>*, <4 x i64>*, i8)*], [16 x void (<4 x i64>*, <4 x i64>*, i8)*]* @row_txfm16x16_arr, i64 0, i64 %13
  %17 = load void (<4 x i64>*, <4 x i64>*, i8)*, void (<4 x i64>*, <4 x i64>*, i8)** %16, align 8
  switch i8 %3, label %44 [
    i8 6, label %19
    i8 15, label %18
    i8 7, label %18
    i8 5, label %18
    i8 14, label %20
    i8 8, label %20
    i8 4, label %20
  ]

18:                                               ; preds = %5, %5, %5
  br label %44

19:                                               ; preds = %5
  br label %20

20:                                               ; preds = %5, %5, %5, %19
  %21 = phi i32 [ 1, %19 ], [ 0, %5 ], [ 0, %5 ], [ 0, %5 ]
  %22 = sext i32 %2 to i64
  br label %23

23:                                               ; preds = %23, %20
  %24 = phi i64 [ 0, %20 ], [ %42, %23 ]
  %25 = mul nsw i64 %24, %22
  %26 = getelementptr inbounds i16, i16* %0, i64 %25
  %27 = bitcast i16* %26 to <4 x i64>*
  %28 = load <4 x i64>, <4 x i64>* %27, align 32
  %29 = shl i64 %24, 32
  %30 = sub nuw nsw i64 133143986176, %29
  %31 = ashr exact i64 %30, 32
  %32 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %31
  store <4 x i64> %28, <4 x i64>* %32, align 32
  %33 = or i64 %24, 1
  %34 = mul nsw i64 %33, %22
  %35 = getelementptr inbounds i16, i16* %0, i64 %34
  %36 = bitcast i16* %35 to <4 x i64>*
  %37 = load <4 x i64>, <4 x i64>* %36, align 32
  %38 = shl i64 %33, 32
  %39 = sub nuw nsw i64 133143986176, %38
  %40 = ashr exact i64 %39, 32
  %41 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %40
  store <4 x i64> %37, <4 x i64>* %41, align 32
  %42 = add nuw nsw i64 %24, 2
  %43 = icmp eq i64 %42, 32
  br i1 %43, label %74, label %23

44:                                               ; preds = %18, %5
  %45 = phi i32 [ 0, %5 ], [ 1, %18 ]
  %46 = sext i32 %2 to i64
  br label %47

47:                                               ; preds = %47, %44
  %48 = phi i64 [ 0, %44 ], [ %72, %47 ]
  %49 = mul nsw i64 %48, %46
  %50 = getelementptr inbounds i16, i16* %0, i64 %49
  %51 = bitcast i16* %50 to <4 x i64>*
  %52 = load <4 x i64>, <4 x i64>* %51, align 32
  %53 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %48
  store <4 x i64> %52, <4 x i64>* %53, align 32
  %54 = or i64 %48, 1
  %55 = mul nsw i64 %54, %46
  %56 = getelementptr inbounds i16, i16* %0, i64 %55
  %57 = bitcast i16* %56 to <4 x i64>*
  %58 = load <4 x i64>, <4 x i64>* %57, align 32
  %59 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %54
  store <4 x i64> %58, <4 x i64>* %59, align 32
  %60 = or i64 %48, 2
  %61 = mul nsw i64 %60, %46
  %62 = getelementptr inbounds i16, i16* %0, i64 %61
  %63 = bitcast i16* %62 to <4 x i64>*
  %64 = load <4 x i64>, <4 x i64>* %63, align 32
  %65 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %60
  store <4 x i64> %64, <4 x i64>* %65, align 32
  %66 = or i64 %48, 3
  %67 = mul nsw i64 %66, %46
  %68 = getelementptr inbounds i16, i16* %0, i64 %67
  %69 = bitcast i16* %68 to <4 x i64>*
  %70 = load <4 x i64>, <4 x i64>* %69, align 32
  %71 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %66
  store <4 x i64> %70, <4 x i64>* %71, align 32
  %72 = add nuw nsw i64 %48, 4
  %73 = icmp eq i64 %72, 32
  br i1 %73, label %74, label %47

74:                                               ; preds = %23, %47
  %75 = phi i32 [ %45, %47 ], [ %21, %23 ]
  %76 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 0
  %77 = load i8, i8* %10, align 1
  %78 = sext i8 %77 to i32
  %79 = icmp slt i8 %77, 0
  br i1 %79, label %80, label %114

80:                                               ; preds = %74
  %81 = sub nsw i32 0, %78
  %82 = xor i32 %78, -1
  %83 = shl i32 1, %82
  %84 = trunc i32 %83 to i16
  %85 = insertelement <16 x i16> undef, i16 %84, i32 0
  %86 = shufflevector <16 x i16> %85, <16 x i16> undef, <16 x i32> zeroinitializer
  br label %87

87:                                               ; preds = %87, %80
  %88 = phi i64 [ 0, %80 ], [ %112, %87 ]
  %89 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %88
  %90 = bitcast <4 x i64>* %89 to <16 x i16>*
  %91 = load <16 x i16>, <16 x i16>* %90, align 32
  %92 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %91, <16 x i16> %86) #8
  %93 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %92, i32 %81) #8
  store <16 x i16> %93, <16 x i16>* %90, align 32
  %94 = or i64 %88, 1
  %95 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %94
  %96 = bitcast <4 x i64>* %95 to <16 x i16>*
  %97 = load <16 x i16>, <16 x i16>* %96, align 32
  %98 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %97, <16 x i16> %86) #8
  %99 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %98, i32 %81) #8
  store <16 x i16> %99, <16 x i16>* %96, align 32
  %100 = or i64 %88, 2
  %101 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %100
  %102 = bitcast <4 x i64>* %101 to <16 x i16>*
  %103 = load <16 x i16>, <16 x i16>* %102, align 32
  %104 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %103, <16 x i16> %86) #8
  %105 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %104, i32 %81) #8
  store <16 x i16> %105, <16 x i16>* %102, align 32
  %106 = or i64 %88, 3
  %107 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %106
  %108 = bitcast <4 x i64>* %107 to <16 x i16>*
  %109 = load <16 x i16>, <16 x i16>* %108, align 32
  %110 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %109, <16 x i16> %86) #8
  %111 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %110, i32 %81) #8
  store <16 x i16> %111, <16 x i16>* %108, align 32
  %112 = add nuw nsw i64 %88, 4
  %113 = icmp eq i64 %112, 32
  br i1 %113, label %139, label %87

114:                                              ; preds = %74
  %115 = icmp eq i8 %77, 0
  br i1 %115, label %139, label %116

116:                                              ; preds = %114, %116
  %117 = phi i64 [ %137, %116 ], [ 0, %114 ]
  %118 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %117
  %119 = bitcast <4 x i64>* %118 to <16 x i16>*
  %120 = load <16 x i16>, <16 x i16>* %119, align 32
  %121 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %120, i32 %78) #8
  store <16 x i16> %121, <16 x i16>* %119, align 32
  %122 = or i64 %117, 1
  %123 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %122
  %124 = bitcast <4 x i64>* %123 to <16 x i16>*
  %125 = load <16 x i16>, <16 x i16>* %124, align 32
  %126 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %125, i32 %78) #8
  store <16 x i16> %126, <16 x i16>* %124, align 32
  %127 = or i64 %117, 2
  %128 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %127
  %129 = bitcast <4 x i64>* %128 to <16 x i16>*
  %130 = load <16 x i16>, <16 x i16>* %129, align 32
  %131 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %130, i32 %78) #8
  store <16 x i16> %131, <16 x i16>* %129, align 32
  %132 = or i64 %117, 3
  %133 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %132
  %134 = bitcast <4 x i64>* %133 to <16 x i16>*
  %135 = load <16 x i16>, <16 x i16>* %134, align 32
  %136 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %135, i32 %78) #8
  store <16 x i16> %136, <16 x i16>* %134, align 32
  %137 = add nuw nsw i64 %117, 4
  %138 = icmp eq i64 %137, 32
  br i1 %138, label %139, label %116

139:                                              ; preds = %116, %87, %114
  call void %15(<4 x i64>* %76, <4 x i64>* %76, i8 signext %11) #8
  %140 = getelementptr inbounds i8, i8* %10, i64 1
  %141 = load i8, i8* %140, align 1
  %142 = sext i8 %141 to i32
  %143 = icmp slt i8 %141, 0
  br i1 %143, label %144, label %178

144:                                              ; preds = %139
  %145 = sub nsw i32 0, %142
  %146 = xor i32 %142, -1
  %147 = shl i32 1, %146
  %148 = trunc i32 %147 to i16
  %149 = insertelement <16 x i16> undef, i16 %148, i32 0
  %150 = shufflevector <16 x i16> %149, <16 x i16> undef, <16 x i32> zeroinitializer
  br label %151

151:                                              ; preds = %151, %144
  %152 = phi i64 [ 0, %144 ], [ %176, %151 ]
  %153 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %152
  %154 = bitcast <4 x i64>* %153 to <16 x i16>*
  %155 = load <16 x i16>, <16 x i16>* %154, align 32
  %156 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %155, <16 x i16> %150) #8
  %157 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %156, i32 %145) #8
  store <16 x i16> %157, <16 x i16>* %154, align 32
  %158 = or i64 %152, 1
  %159 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %158
  %160 = bitcast <4 x i64>* %159 to <16 x i16>*
  %161 = load <16 x i16>, <16 x i16>* %160, align 32
  %162 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %161, <16 x i16> %150) #8
  %163 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %162, i32 %145) #8
  store <16 x i16> %163, <16 x i16>* %160, align 32
  %164 = or i64 %152, 2
  %165 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %164
  %166 = bitcast <4 x i64>* %165 to <16 x i16>*
  %167 = load <16 x i16>, <16 x i16>* %166, align 32
  %168 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %167, <16 x i16> %150) #8
  %169 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %168, i32 %145) #8
  store <16 x i16> %169, <16 x i16>* %166, align 32
  %170 = or i64 %152, 3
  %171 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %170
  %172 = bitcast <4 x i64>* %171 to <16 x i16>*
  %173 = load <16 x i16>, <16 x i16>* %172, align 32
  %174 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %173, <16 x i16> %150) #8
  %175 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %174, i32 %145) #8
  store <16 x i16> %175, <16 x i16>* %172, align 32
  %176 = add nuw nsw i64 %152, 4
  %177 = icmp eq i64 %176, 32
  br i1 %177, label %203, label %151

178:                                              ; preds = %139
  %179 = icmp eq i8 %141, 0
  br i1 %179, label %203, label %180

180:                                              ; preds = %178, %180
  %181 = phi i64 [ %201, %180 ], [ 0, %178 ]
  %182 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %181
  %183 = bitcast <4 x i64>* %182 to <16 x i16>*
  %184 = load <16 x i16>, <16 x i16>* %183, align 32
  %185 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %184, i32 %142) #8
  store <16 x i16> %185, <16 x i16>* %183, align 32
  %186 = or i64 %181, 1
  %187 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %186
  %188 = bitcast <4 x i64>* %187 to <16 x i16>*
  %189 = load <16 x i16>, <16 x i16>* %188, align 32
  %190 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %189, i32 %142) #8
  store <16 x i16> %190, <16 x i16>* %188, align 32
  %191 = or i64 %181, 2
  %192 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %191
  %193 = bitcast <4 x i64>* %192 to <16 x i16>*
  %194 = load <16 x i16>, <16 x i16>* %193, align 32
  %195 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %194, i32 %142) #8
  store <16 x i16> %195, <16 x i16>* %193, align 32
  %196 = or i64 %181, 3
  %197 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %196
  %198 = bitcast <4 x i64>* %197 to <16 x i16>*
  %199 = load <16 x i16>, <16 x i16>* %198, align 32
  %200 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %199, i32 %142) #8
  store <16 x i16> %200, <16 x i16>* %198, align 32
  %201 = add nuw nsw i64 %181, 4
  %202 = icmp eq i64 %201, 32
  br i1 %202, label %203, label %180

203:                                              ; preds = %180, %151, %178
  %204 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %7, i64 0, i64 0
  %205 = bitcast [32 x <4 x i64>]* %6 to <2 x i64>*
  %206 = load <2 x i64>, <2 x i64>* %205, align 32
  %207 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 8
  %208 = bitcast <4 x i64>* %207 to <2 x i64>*
  %209 = load <2 x i64>, <2 x i64>* %208, align 32
  %210 = shufflevector <2 x i64> %206, <2 x i64> %209, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %211 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 1
  %212 = bitcast <4 x i64>* %211 to <2 x i64>*
  %213 = load <2 x i64>, <2 x i64>* %212, align 32
  %214 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 9
  %215 = bitcast <4 x i64>* %214 to <2 x i64>*
  %216 = load <2 x i64>, <2 x i64>* %215, align 32
  %217 = shufflevector <2 x i64> %213, <2 x i64> %216, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %218 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 2
  %219 = bitcast <4 x i64>* %218 to <2 x i64>*
  %220 = load <2 x i64>, <2 x i64>* %219, align 32
  %221 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 10
  %222 = bitcast <4 x i64>* %221 to <2 x i64>*
  %223 = load <2 x i64>, <2 x i64>* %222, align 32
  %224 = shufflevector <2 x i64> %220, <2 x i64> %223, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %225 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 3
  %226 = bitcast <4 x i64>* %225 to <2 x i64>*
  %227 = load <2 x i64>, <2 x i64>* %226, align 32
  %228 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 11
  %229 = bitcast <4 x i64>* %228 to <2 x i64>*
  %230 = load <2 x i64>, <2 x i64>* %229, align 32
  %231 = shufflevector <2 x i64> %227, <2 x i64> %230, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %232 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 4
  %233 = bitcast <4 x i64>* %232 to <2 x i64>*
  %234 = load <2 x i64>, <2 x i64>* %233, align 32
  %235 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 12
  %236 = bitcast <4 x i64>* %235 to <2 x i64>*
  %237 = load <2 x i64>, <2 x i64>* %236, align 32
  %238 = shufflevector <2 x i64> %234, <2 x i64> %237, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %239 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 5
  %240 = bitcast <4 x i64>* %239 to <2 x i64>*
  %241 = load <2 x i64>, <2 x i64>* %240, align 32
  %242 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 13
  %243 = bitcast <4 x i64>* %242 to <2 x i64>*
  %244 = load <2 x i64>, <2 x i64>* %243, align 32
  %245 = shufflevector <2 x i64> %241, <2 x i64> %244, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %246 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 6
  %247 = bitcast <4 x i64>* %246 to <2 x i64>*
  %248 = load <2 x i64>, <2 x i64>* %247, align 32
  %249 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 14
  %250 = bitcast <4 x i64>* %249 to <2 x i64>*
  %251 = load <2 x i64>, <2 x i64>* %250, align 32
  %252 = shufflevector <2 x i64> %248, <2 x i64> %251, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %253 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 7
  %254 = bitcast <4 x i64>* %253 to <2 x i64>*
  %255 = load <2 x i64>, <2 x i64>* %254, align 32
  %256 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 15
  %257 = bitcast <4 x i64>* %256 to <2 x i64>*
  %258 = load <2 x i64>, <2 x i64>* %257, align 32
  %259 = shufflevector <2 x i64> %255, <2 x i64> %258, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %260 = getelementptr inbounds <2 x i64>, <2 x i64>* %205, i64 1
  %261 = load <2 x i64>, <2 x i64>* %260, align 16
  %262 = getelementptr inbounds <2 x i64>, <2 x i64>* %208, i64 1
  %263 = load <2 x i64>, <2 x i64>* %262, align 16
  %264 = shufflevector <2 x i64> %261, <2 x i64> %263, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %265 = getelementptr inbounds <2 x i64>, <2 x i64>* %212, i64 1
  %266 = load <2 x i64>, <2 x i64>* %265, align 16
  %267 = getelementptr inbounds <2 x i64>, <2 x i64>* %215, i64 1
  %268 = load <2 x i64>, <2 x i64>* %267, align 16
  %269 = shufflevector <2 x i64> %266, <2 x i64> %268, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %270 = getelementptr inbounds <2 x i64>, <2 x i64>* %219, i64 1
  %271 = load <2 x i64>, <2 x i64>* %270, align 16
  %272 = getelementptr inbounds <2 x i64>, <2 x i64>* %222, i64 1
  %273 = load <2 x i64>, <2 x i64>* %272, align 16
  %274 = shufflevector <2 x i64> %271, <2 x i64> %273, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %275 = getelementptr inbounds <2 x i64>, <2 x i64>* %226, i64 1
  %276 = load <2 x i64>, <2 x i64>* %275, align 16
  %277 = getelementptr inbounds <2 x i64>, <2 x i64>* %229, i64 1
  %278 = load <2 x i64>, <2 x i64>* %277, align 16
  %279 = shufflevector <2 x i64> %276, <2 x i64> %278, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %280 = getelementptr inbounds <2 x i64>, <2 x i64>* %233, i64 1
  %281 = load <2 x i64>, <2 x i64>* %280, align 16
  %282 = getelementptr inbounds <2 x i64>, <2 x i64>* %236, i64 1
  %283 = load <2 x i64>, <2 x i64>* %282, align 16
  %284 = shufflevector <2 x i64> %281, <2 x i64> %283, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %285 = getelementptr inbounds <2 x i64>, <2 x i64>* %240, i64 1
  %286 = load <2 x i64>, <2 x i64>* %285, align 16
  %287 = getelementptr inbounds <2 x i64>, <2 x i64>* %243, i64 1
  %288 = load <2 x i64>, <2 x i64>* %287, align 16
  %289 = shufflevector <2 x i64> %286, <2 x i64> %288, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %290 = getelementptr inbounds <2 x i64>, <2 x i64>* %247, i64 1
  %291 = load <2 x i64>, <2 x i64>* %290, align 16
  %292 = getelementptr inbounds <2 x i64>, <2 x i64>* %250, i64 1
  %293 = load <2 x i64>, <2 x i64>* %292, align 16
  %294 = shufflevector <2 x i64> %291, <2 x i64> %293, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %295 = getelementptr inbounds <2 x i64>, <2 x i64>* %254, i64 1
  %296 = load <2 x i64>, <2 x i64>* %295, align 16
  %297 = getelementptr inbounds <2 x i64>, <2 x i64>* %257, i64 1
  %298 = load <2 x i64>, <2 x i64>* %297, align 16
  %299 = shufflevector <2 x i64> %296, <2 x i64> %298, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %300 = bitcast <4 x i64> %210 to <16 x i16>
  %301 = bitcast <4 x i64> %217 to <16 x i16>
  %302 = shufflevector <16 x i16> %300, <16 x i16> %301, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %303 = shufflevector <16 x i16> %300, <16 x i16> %301, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %304 = bitcast <4 x i64> %224 to <16 x i16>
  %305 = bitcast <4 x i64> %231 to <16 x i16>
  %306 = shufflevector <16 x i16> %304, <16 x i16> %305, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %307 = shufflevector <16 x i16> %304, <16 x i16> %305, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %308 = bitcast <4 x i64> %238 to <16 x i16>
  %309 = bitcast <4 x i64> %245 to <16 x i16>
  %310 = shufflevector <16 x i16> %308, <16 x i16> %309, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %311 = shufflevector <16 x i16> %308, <16 x i16> %309, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %312 = bitcast <4 x i64> %252 to <16 x i16>
  %313 = bitcast <4 x i64> %259 to <16 x i16>
  %314 = shufflevector <16 x i16> %312, <16 x i16> %313, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %315 = shufflevector <16 x i16> %312, <16 x i16> %313, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %316 = bitcast <16 x i16> %302 to <8 x i32>
  %317 = bitcast <16 x i16> %306 to <8 x i32>
  %318 = shufflevector <8 x i32> %316, <8 x i32> %317, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %319 = shufflevector <8 x i32> %316, <8 x i32> %317, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %320 = bitcast <16 x i16> %310 to <8 x i32>
  %321 = bitcast <16 x i16> %314 to <8 x i32>
  %322 = shufflevector <8 x i32> %320, <8 x i32> %321, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %323 = shufflevector <8 x i32> %320, <8 x i32> %321, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %324 = bitcast <16 x i16> %303 to <8 x i32>
  %325 = bitcast <16 x i16> %307 to <8 x i32>
  %326 = shufflevector <8 x i32> %324, <8 x i32> %325, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %327 = shufflevector <8 x i32> %324, <8 x i32> %325, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %328 = bitcast <16 x i16> %311 to <8 x i32>
  %329 = bitcast <16 x i16> %315 to <8 x i32>
  %330 = shufflevector <8 x i32> %328, <8 x i32> %329, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %331 = shufflevector <8 x i32> %328, <8 x i32> %329, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %332 = bitcast <8 x i32> %318 to <4 x i64>
  %333 = bitcast <8 x i32> %322 to <4 x i64>
  %334 = shufflevector <4 x i64> %332, <4 x i64> %333, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %334, <4 x i64>* %204, align 32
  %335 = shufflevector <4 x i64> %332, <4 x i64> %333, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %336 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %7, i64 0, i64 1
  store <4 x i64> %335, <4 x i64>* %336, align 32
  %337 = bitcast <8 x i32> %326 to <4 x i64>
  %338 = bitcast <8 x i32> %330 to <4 x i64>
  %339 = shufflevector <4 x i64> %337, <4 x i64> %338, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %340 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %7, i64 0, i64 4
  store <4 x i64> %339, <4 x i64>* %340, align 32
  %341 = shufflevector <4 x i64> %337, <4 x i64> %338, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %342 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %7, i64 0, i64 5
  store <4 x i64> %341, <4 x i64>* %342, align 32
  %343 = bitcast <8 x i32> %319 to <4 x i64>
  %344 = bitcast <8 x i32> %323 to <4 x i64>
  %345 = shufflevector <4 x i64> %343, <4 x i64> %344, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %346 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %7, i64 0, i64 2
  store <4 x i64> %345, <4 x i64>* %346, align 32
  %347 = shufflevector <4 x i64> %343, <4 x i64> %344, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %348 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %7, i64 0, i64 3
  store <4 x i64> %347, <4 x i64>* %348, align 32
  %349 = bitcast <8 x i32> %327 to <4 x i64>
  %350 = bitcast <8 x i32> %331 to <4 x i64>
  %351 = shufflevector <4 x i64> %349, <4 x i64> %350, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %352 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %7, i64 0, i64 6
  store <4 x i64> %351, <4 x i64>* %352, align 32
  %353 = shufflevector <4 x i64> %349, <4 x i64> %350, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %354 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %7, i64 0, i64 7
  store <4 x i64> %353, <4 x i64>* %354, align 32
  %355 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %7, i64 0, i64 8
  %356 = bitcast <4 x i64> %264 to <16 x i16>
  %357 = bitcast <4 x i64> %269 to <16 x i16>
  %358 = shufflevector <16 x i16> %356, <16 x i16> %357, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %359 = shufflevector <16 x i16> %356, <16 x i16> %357, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %360 = bitcast <4 x i64> %274 to <16 x i16>
  %361 = bitcast <4 x i64> %279 to <16 x i16>
  %362 = shufflevector <16 x i16> %360, <16 x i16> %361, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %363 = shufflevector <16 x i16> %360, <16 x i16> %361, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %364 = bitcast <4 x i64> %284 to <16 x i16>
  %365 = bitcast <4 x i64> %289 to <16 x i16>
  %366 = shufflevector <16 x i16> %364, <16 x i16> %365, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %367 = shufflevector <16 x i16> %364, <16 x i16> %365, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %368 = bitcast <4 x i64> %294 to <16 x i16>
  %369 = bitcast <4 x i64> %299 to <16 x i16>
  %370 = shufflevector <16 x i16> %368, <16 x i16> %369, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %371 = shufflevector <16 x i16> %368, <16 x i16> %369, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %372 = bitcast <16 x i16> %358 to <8 x i32>
  %373 = bitcast <16 x i16> %362 to <8 x i32>
  %374 = shufflevector <8 x i32> %372, <8 x i32> %373, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %375 = shufflevector <8 x i32> %372, <8 x i32> %373, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %376 = bitcast <16 x i16> %366 to <8 x i32>
  %377 = bitcast <16 x i16> %370 to <8 x i32>
  %378 = shufflevector <8 x i32> %376, <8 x i32> %377, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %379 = shufflevector <8 x i32> %376, <8 x i32> %377, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %380 = bitcast <16 x i16> %359 to <8 x i32>
  %381 = bitcast <16 x i16> %363 to <8 x i32>
  %382 = shufflevector <8 x i32> %380, <8 x i32> %381, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %383 = shufflevector <8 x i32> %380, <8 x i32> %381, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %384 = bitcast <16 x i16> %367 to <8 x i32>
  %385 = bitcast <16 x i16> %371 to <8 x i32>
  %386 = shufflevector <8 x i32> %384, <8 x i32> %385, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %387 = shufflevector <8 x i32> %384, <8 x i32> %385, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %388 = bitcast <8 x i32> %374 to <4 x i64>
  %389 = bitcast <8 x i32> %378 to <4 x i64>
  %390 = shufflevector <4 x i64> %388, <4 x i64> %389, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %390, <4 x i64>* %355, align 32
  %391 = shufflevector <4 x i64> %388, <4 x i64> %389, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %392 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %7, i64 0, i64 9
  store <4 x i64> %391, <4 x i64>* %392, align 32
  %393 = bitcast <8 x i32> %382 to <4 x i64>
  %394 = bitcast <8 x i32> %386 to <4 x i64>
  %395 = shufflevector <4 x i64> %393, <4 x i64> %394, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %396 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %7, i64 0, i64 12
  store <4 x i64> %395, <4 x i64>* %396, align 32
  %397 = shufflevector <4 x i64> %393, <4 x i64> %394, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %398 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %7, i64 0, i64 13
  store <4 x i64> %397, <4 x i64>* %398, align 32
  %399 = bitcast <8 x i32> %375 to <4 x i64>
  %400 = bitcast <8 x i32> %379 to <4 x i64>
  %401 = shufflevector <4 x i64> %399, <4 x i64> %400, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %402 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %7, i64 0, i64 10
  store <4 x i64> %401, <4 x i64>* %402, align 32
  %403 = shufflevector <4 x i64> %399, <4 x i64> %400, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %404 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %7, i64 0, i64 11
  store <4 x i64> %403, <4 x i64>* %404, align 32
  %405 = bitcast <8 x i32> %383 to <4 x i64>
  %406 = bitcast <8 x i32> %387 to <4 x i64>
  %407 = shufflevector <4 x i64> %405, <4 x i64> %406, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %408 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %7, i64 0, i64 14
  store <4 x i64> %407, <4 x i64>* %408, align 32
  %409 = shufflevector <4 x i64> %405, <4 x i64> %406, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %410 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %7, i64 0, i64 15
  store <4 x i64> %409, <4 x i64>* %410, align 32
  %411 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 16
  %412 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %7, i64 0, i64 16
  %413 = bitcast <4 x i64>* %411 to <2 x i64>*
  %414 = load <2 x i64>, <2 x i64>* %413, align 32
  %415 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 24
  %416 = bitcast <4 x i64>* %415 to <2 x i64>*
  %417 = load <2 x i64>, <2 x i64>* %416, align 32
  %418 = shufflevector <2 x i64> %414, <2 x i64> %417, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %419 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 17
  %420 = bitcast <4 x i64>* %419 to <2 x i64>*
  %421 = load <2 x i64>, <2 x i64>* %420, align 32
  %422 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 25
  %423 = bitcast <4 x i64>* %422 to <2 x i64>*
  %424 = load <2 x i64>, <2 x i64>* %423, align 32
  %425 = shufflevector <2 x i64> %421, <2 x i64> %424, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %426 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 18
  %427 = bitcast <4 x i64>* %426 to <2 x i64>*
  %428 = load <2 x i64>, <2 x i64>* %427, align 32
  %429 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 26
  %430 = bitcast <4 x i64>* %429 to <2 x i64>*
  %431 = load <2 x i64>, <2 x i64>* %430, align 32
  %432 = shufflevector <2 x i64> %428, <2 x i64> %431, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %433 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 19
  %434 = bitcast <4 x i64>* %433 to <2 x i64>*
  %435 = load <2 x i64>, <2 x i64>* %434, align 32
  %436 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 27
  %437 = bitcast <4 x i64>* %436 to <2 x i64>*
  %438 = load <2 x i64>, <2 x i64>* %437, align 32
  %439 = shufflevector <2 x i64> %435, <2 x i64> %438, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %440 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 20
  %441 = bitcast <4 x i64>* %440 to <2 x i64>*
  %442 = load <2 x i64>, <2 x i64>* %441, align 32
  %443 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 28
  %444 = bitcast <4 x i64>* %443 to <2 x i64>*
  %445 = load <2 x i64>, <2 x i64>* %444, align 32
  %446 = shufflevector <2 x i64> %442, <2 x i64> %445, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %447 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 21
  %448 = bitcast <4 x i64>* %447 to <2 x i64>*
  %449 = load <2 x i64>, <2 x i64>* %448, align 32
  %450 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 29
  %451 = bitcast <4 x i64>* %450 to <2 x i64>*
  %452 = load <2 x i64>, <2 x i64>* %451, align 32
  %453 = shufflevector <2 x i64> %449, <2 x i64> %452, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %454 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 22
  %455 = bitcast <4 x i64>* %454 to <2 x i64>*
  %456 = load <2 x i64>, <2 x i64>* %455, align 32
  %457 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 30
  %458 = bitcast <4 x i64>* %457 to <2 x i64>*
  %459 = load <2 x i64>, <2 x i64>* %458, align 32
  %460 = shufflevector <2 x i64> %456, <2 x i64> %459, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %461 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 23
  %462 = bitcast <4 x i64>* %461 to <2 x i64>*
  %463 = load <2 x i64>, <2 x i64>* %462, align 32
  %464 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 31
  %465 = bitcast <4 x i64>* %464 to <2 x i64>*
  %466 = load <2 x i64>, <2 x i64>* %465, align 32
  %467 = shufflevector <2 x i64> %463, <2 x i64> %466, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %468 = getelementptr inbounds <2 x i64>, <2 x i64>* %413, i64 1
  %469 = load <2 x i64>, <2 x i64>* %468, align 16
  %470 = getelementptr inbounds <2 x i64>, <2 x i64>* %416, i64 1
  %471 = load <2 x i64>, <2 x i64>* %470, align 16
  %472 = shufflevector <2 x i64> %469, <2 x i64> %471, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %473 = getelementptr inbounds <2 x i64>, <2 x i64>* %420, i64 1
  %474 = load <2 x i64>, <2 x i64>* %473, align 16
  %475 = getelementptr inbounds <2 x i64>, <2 x i64>* %423, i64 1
  %476 = load <2 x i64>, <2 x i64>* %475, align 16
  %477 = shufflevector <2 x i64> %474, <2 x i64> %476, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %478 = getelementptr inbounds <2 x i64>, <2 x i64>* %427, i64 1
  %479 = load <2 x i64>, <2 x i64>* %478, align 16
  %480 = getelementptr inbounds <2 x i64>, <2 x i64>* %430, i64 1
  %481 = load <2 x i64>, <2 x i64>* %480, align 16
  %482 = shufflevector <2 x i64> %479, <2 x i64> %481, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %483 = getelementptr inbounds <2 x i64>, <2 x i64>* %434, i64 1
  %484 = load <2 x i64>, <2 x i64>* %483, align 16
  %485 = getelementptr inbounds <2 x i64>, <2 x i64>* %437, i64 1
  %486 = load <2 x i64>, <2 x i64>* %485, align 16
  %487 = shufflevector <2 x i64> %484, <2 x i64> %486, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %488 = getelementptr inbounds <2 x i64>, <2 x i64>* %441, i64 1
  %489 = load <2 x i64>, <2 x i64>* %488, align 16
  %490 = getelementptr inbounds <2 x i64>, <2 x i64>* %444, i64 1
  %491 = load <2 x i64>, <2 x i64>* %490, align 16
  %492 = shufflevector <2 x i64> %489, <2 x i64> %491, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %493 = getelementptr inbounds <2 x i64>, <2 x i64>* %448, i64 1
  %494 = load <2 x i64>, <2 x i64>* %493, align 16
  %495 = getelementptr inbounds <2 x i64>, <2 x i64>* %451, i64 1
  %496 = load <2 x i64>, <2 x i64>* %495, align 16
  %497 = shufflevector <2 x i64> %494, <2 x i64> %496, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %498 = getelementptr inbounds <2 x i64>, <2 x i64>* %455, i64 1
  %499 = load <2 x i64>, <2 x i64>* %498, align 16
  %500 = getelementptr inbounds <2 x i64>, <2 x i64>* %458, i64 1
  %501 = load <2 x i64>, <2 x i64>* %500, align 16
  %502 = shufflevector <2 x i64> %499, <2 x i64> %501, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %503 = getelementptr inbounds <2 x i64>, <2 x i64>* %462, i64 1
  %504 = load <2 x i64>, <2 x i64>* %503, align 16
  %505 = getelementptr inbounds <2 x i64>, <2 x i64>* %465, i64 1
  %506 = load <2 x i64>, <2 x i64>* %505, align 16
  %507 = shufflevector <2 x i64> %504, <2 x i64> %506, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %508 = bitcast <4 x i64> %418 to <16 x i16>
  %509 = bitcast <4 x i64> %425 to <16 x i16>
  %510 = shufflevector <16 x i16> %508, <16 x i16> %509, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %511 = shufflevector <16 x i16> %508, <16 x i16> %509, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %512 = bitcast <4 x i64> %432 to <16 x i16>
  %513 = bitcast <4 x i64> %439 to <16 x i16>
  %514 = shufflevector <16 x i16> %512, <16 x i16> %513, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %515 = shufflevector <16 x i16> %512, <16 x i16> %513, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %516 = bitcast <4 x i64> %446 to <16 x i16>
  %517 = bitcast <4 x i64> %453 to <16 x i16>
  %518 = shufflevector <16 x i16> %516, <16 x i16> %517, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %519 = shufflevector <16 x i16> %516, <16 x i16> %517, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %520 = bitcast <4 x i64> %460 to <16 x i16>
  %521 = bitcast <4 x i64> %467 to <16 x i16>
  %522 = shufflevector <16 x i16> %520, <16 x i16> %521, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %523 = shufflevector <16 x i16> %520, <16 x i16> %521, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %524 = bitcast <16 x i16> %510 to <8 x i32>
  %525 = bitcast <16 x i16> %514 to <8 x i32>
  %526 = shufflevector <8 x i32> %524, <8 x i32> %525, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %527 = shufflevector <8 x i32> %524, <8 x i32> %525, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %528 = bitcast <16 x i16> %518 to <8 x i32>
  %529 = bitcast <16 x i16> %522 to <8 x i32>
  %530 = shufflevector <8 x i32> %528, <8 x i32> %529, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %531 = shufflevector <8 x i32> %528, <8 x i32> %529, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %532 = bitcast <16 x i16> %511 to <8 x i32>
  %533 = bitcast <16 x i16> %515 to <8 x i32>
  %534 = shufflevector <8 x i32> %532, <8 x i32> %533, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %535 = shufflevector <8 x i32> %532, <8 x i32> %533, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %536 = bitcast <16 x i16> %519 to <8 x i32>
  %537 = bitcast <16 x i16> %523 to <8 x i32>
  %538 = shufflevector <8 x i32> %536, <8 x i32> %537, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %539 = shufflevector <8 x i32> %536, <8 x i32> %537, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %540 = bitcast <8 x i32> %526 to <4 x i64>
  %541 = bitcast <8 x i32> %530 to <4 x i64>
  %542 = shufflevector <4 x i64> %540, <4 x i64> %541, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %542, <4 x i64>* %412, align 32
  %543 = shufflevector <4 x i64> %540, <4 x i64> %541, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %544 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %7, i64 0, i64 17
  store <4 x i64> %543, <4 x i64>* %544, align 32
  %545 = bitcast <8 x i32> %534 to <4 x i64>
  %546 = bitcast <8 x i32> %538 to <4 x i64>
  %547 = shufflevector <4 x i64> %545, <4 x i64> %546, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %548 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %7, i64 0, i64 20
  store <4 x i64> %547, <4 x i64>* %548, align 32
  %549 = shufflevector <4 x i64> %545, <4 x i64> %546, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %550 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %7, i64 0, i64 21
  store <4 x i64> %549, <4 x i64>* %550, align 32
  %551 = bitcast <8 x i32> %527 to <4 x i64>
  %552 = bitcast <8 x i32> %531 to <4 x i64>
  %553 = shufflevector <4 x i64> %551, <4 x i64> %552, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %554 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %7, i64 0, i64 18
  store <4 x i64> %553, <4 x i64>* %554, align 32
  %555 = shufflevector <4 x i64> %551, <4 x i64> %552, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %556 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %7, i64 0, i64 19
  store <4 x i64> %555, <4 x i64>* %556, align 32
  %557 = bitcast <8 x i32> %535 to <4 x i64>
  %558 = bitcast <8 x i32> %539 to <4 x i64>
  %559 = shufflevector <4 x i64> %557, <4 x i64> %558, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %560 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %7, i64 0, i64 22
  store <4 x i64> %559, <4 x i64>* %560, align 32
  %561 = shufflevector <4 x i64> %557, <4 x i64> %558, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %562 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %7, i64 0, i64 23
  store <4 x i64> %561, <4 x i64>* %562, align 32
  %563 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %7, i64 0, i64 24
  %564 = bitcast <4 x i64> %472 to <16 x i16>
  %565 = bitcast <4 x i64> %477 to <16 x i16>
  %566 = shufflevector <16 x i16> %564, <16 x i16> %565, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %567 = shufflevector <16 x i16> %564, <16 x i16> %565, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %568 = bitcast <4 x i64> %482 to <16 x i16>
  %569 = bitcast <4 x i64> %487 to <16 x i16>
  %570 = shufflevector <16 x i16> %568, <16 x i16> %569, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %571 = shufflevector <16 x i16> %568, <16 x i16> %569, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %572 = bitcast <4 x i64> %492 to <16 x i16>
  %573 = bitcast <4 x i64> %497 to <16 x i16>
  %574 = shufflevector <16 x i16> %572, <16 x i16> %573, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %575 = shufflevector <16 x i16> %572, <16 x i16> %573, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %576 = bitcast <4 x i64> %502 to <16 x i16>
  %577 = bitcast <4 x i64> %507 to <16 x i16>
  %578 = shufflevector <16 x i16> %576, <16 x i16> %577, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %579 = shufflevector <16 x i16> %576, <16 x i16> %577, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %580 = bitcast <16 x i16> %566 to <8 x i32>
  %581 = bitcast <16 x i16> %570 to <8 x i32>
  %582 = shufflevector <8 x i32> %580, <8 x i32> %581, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %583 = shufflevector <8 x i32> %580, <8 x i32> %581, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %584 = bitcast <16 x i16> %574 to <8 x i32>
  %585 = bitcast <16 x i16> %578 to <8 x i32>
  %586 = shufflevector <8 x i32> %584, <8 x i32> %585, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %587 = shufflevector <8 x i32> %584, <8 x i32> %585, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %588 = bitcast <16 x i16> %567 to <8 x i32>
  %589 = bitcast <16 x i16> %571 to <8 x i32>
  %590 = shufflevector <8 x i32> %588, <8 x i32> %589, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %591 = shufflevector <8 x i32> %588, <8 x i32> %589, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %592 = bitcast <16 x i16> %575 to <8 x i32>
  %593 = bitcast <16 x i16> %579 to <8 x i32>
  %594 = shufflevector <8 x i32> %592, <8 x i32> %593, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %595 = shufflevector <8 x i32> %592, <8 x i32> %593, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %596 = bitcast <8 x i32> %582 to <4 x i64>
  %597 = bitcast <8 x i32> %586 to <4 x i64>
  %598 = shufflevector <4 x i64> %596, <4 x i64> %597, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %598, <4 x i64>* %563, align 32
  %599 = shufflevector <4 x i64> %596, <4 x i64> %597, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %600 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %7, i64 0, i64 25
  store <4 x i64> %599, <4 x i64>* %600, align 32
  %601 = bitcast <8 x i32> %590 to <4 x i64>
  %602 = bitcast <8 x i32> %594 to <4 x i64>
  %603 = shufflevector <4 x i64> %601, <4 x i64> %602, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %604 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %7, i64 0, i64 28
  store <4 x i64> %603, <4 x i64>* %604, align 32
  %605 = shufflevector <4 x i64> %601, <4 x i64> %602, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %606 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %7, i64 0, i64 29
  store <4 x i64> %605, <4 x i64>* %606, align 32
  %607 = bitcast <8 x i32> %583 to <4 x i64>
  %608 = bitcast <8 x i32> %587 to <4 x i64>
  %609 = shufflevector <4 x i64> %607, <4 x i64> %608, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %610 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %7, i64 0, i64 26
  store <4 x i64> %609, <4 x i64>* %610, align 32
  %611 = shufflevector <4 x i64> %607, <4 x i64> %608, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %612 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %7, i64 0, i64 27
  store <4 x i64> %611, <4 x i64>* %612, align 32
  %613 = bitcast <8 x i32> %591 to <4 x i64>
  %614 = bitcast <8 x i32> %595 to <4 x i64>
  %615 = shufflevector <4 x i64> %613, <4 x i64> %614, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %616 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %7, i64 0, i64 30
  store <4 x i64> %615, <4 x i64>* %616, align 32
  %617 = shufflevector <4 x i64> %613, <4 x i64> %614, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %618 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %7, i64 0, i64 31
  store <4 x i64> %617, <4 x i64>* %618, align 32
  %619 = icmp eq i32 %75, 0
  %620 = getelementptr inbounds i8, i8* %10, i64 2
  %621 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 0
  br label %623

622:                                              ; preds = %1232
  call void @llvm.lifetime.end.p0i8(i64 1024, i8* nonnull %9) #8
  call void @llvm.lifetime.end.p0i8(i64 1024, i8* nonnull %8) #8
  ret void

623:                                              ; preds = %1232, %203
  %624 = phi i64 [ 0, %203 ], [ %1233, %1232 ]
  %625 = shl nsw i64 %624, 4
  %626 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %7, i64 0, i64 %625
  br i1 %619, label %659, label %627

627:                                              ; preds = %623
  %628 = load <4 x i64>, <4 x i64>* %626, align 32
  store <4 x i64> %628, <4 x i64>* %256, align 32
  %629 = getelementptr inbounds <4 x i64>, <4 x i64>* %626, i64 1
  %630 = load <4 x i64>, <4 x i64>* %629, align 32
  store <4 x i64> %630, <4 x i64>* %249, align 32
  %631 = getelementptr inbounds <4 x i64>, <4 x i64>* %626, i64 2
  %632 = load <4 x i64>, <4 x i64>* %631, align 32
  store <4 x i64> %632, <4 x i64>* %242, align 32
  %633 = getelementptr inbounds <4 x i64>, <4 x i64>* %626, i64 3
  %634 = load <4 x i64>, <4 x i64>* %633, align 32
  store <4 x i64> %634, <4 x i64>* %235, align 32
  %635 = getelementptr inbounds <4 x i64>, <4 x i64>* %626, i64 4
  %636 = load <4 x i64>, <4 x i64>* %635, align 32
  store <4 x i64> %636, <4 x i64>* %228, align 32
  %637 = getelementptr inbounds <4 x i64>, <4 x i64>* %626, i64 5
  %638 = load <4 x i64>, <4 x i64>* %637, align 32
  store <4 x i64> %638, <4 x i64>* %221, align 32
  %639 = getelementptr inbounds <4 x i64>, <4 x i64>* %626, i64 6
  %640 = load <4 x i64>, <4 x i64>* %639, align 32
  store <4 x i64> %640, <4 x i64>* %214, align 32
  %641 = getelementptr inbounds <4 x i64>, <4 x i64>* %626, i64 7
  %642 = load <4 x i64>, <4 x i64>* %641, align 32
  store <4 x i64> %642, <4 x i64>* %207, align 32
  %643 = getelementptr inbounds <4 x i64>, <4 x i64>* %626, i64 8
  %644 = load <4 x i64>, <4 x i64>* %643, align 32
  store <4 x i64> %644, <4 x i64>* %253, align 32
  %645 = getelementptr inbounds <4 x i64>, <4 x i64>* %626, i64 9
  %646 = load <4 x i64>, <4 x i64>* %645, align 32
  store <4 x i64> %646, <4 x i64>* %246, align 32
  %647 = getelementptr inbounds <4 x i64>, <4 x i64>* %626, i64 10
  %648 = load <4 x i64>, <4 x i64>* %647, align 32
  store <4 x i64> %648, <4 x i64>* %239, align 32
  %649 = getelementptr inbounds <4 x i64>, <4 x i64>* %626, i64 11
  %650 = load <4 x i64>, <4 x i64>* %649, align 32
  store <4 x i64> %650, <4 x i64>* %232, align 32
  %651 = getelementptr inbounds <4 x i64>, <4 x i64>* %626, i64 12
  %652 = load <4 x i64>, <4 x i64>* %651, align 32
  store <4 x i64> %652, <4 x i64>* %225, align 32
  %653 = getelementptr inbounds <4 x i64>, <4 x i64>* %626, i64 13
  %654 = load <4 x i64>, <4 x i64>* %653, align 32
  store <4 x i64> %654, <4 x i64>* %218, align 32
  %655 = getelementptr inbounds <4 x i64>, <4 x i64>* %626, i64 14
  %656 = load <4 x i64>, <4 x i64>* %655, align 32
  store <4 x i64> %656, <4 x i64>* %211, align 32
  %657 = getelementptr inbounds <4 x i64>, <4 x i64>* %626, i64 15
  %658 = load <4 x i64>, <4 x i64>* %657, align 32
  store <4 x i64> %658, <4 x i64>* %621, align 32
  br label %659

659:                                              ; preds = %627, %623
  %660 = phi <4 x i64>* [ %626, %623 ], [ %76, %627 ]
  call void %17(<4 x i64>* %660, <4 x i64>* %660, i8 signext %12) #8
  %661 = load i8, i8* %620, align 1
  %662 = sext i8 %661 to i32
  %663 = icmp slt i8 %661, 0
  br i1 %663, label %664, label %822

664:                                              ; preds = %659
  %665 = sub nsw i32 0, %662
  %666 = xor i32 %662, -1
  %667 = shl i32 1, %666
  %668 = trunc i32 %667 to i16
  %669 = insertelement <16 x i16> undef, i16 %668, i32 0
  %670 = shufflevector <16 x i16> %669, <16 x i16> undef, <16 x i32> zeroinitializer
  %671 = bitcast <4 x i64>* %660 to <16 x i16>*
  %672 = load <16 x i16>, <16 x i16>* %671, align 32
  %673 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %672, <16 x i16> %670) #8
  %674 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %673, i32 %665) #8
  store <16 x i16> %674, <16 x i16>* %671, align 32
  %675 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 1
  %676 = bitcast <4 x i64>* %675 to <16 x i16>*
  %677 = load <16 x i16>, <16 x i16>* %676, align 32
  %678 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %677, <16 x i16> %670) #8
  %679 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %678, i32 %665) #8
  store <16 x i16> %679, <16 x i16>* %676, align 32
  %680 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 2
  %681 = bitcast <4 x i64>* %680 to <16 x i16>*
  %682 = load <16 x i16>, <16 x i16>* %681, align 32
  %683 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %682, <16 x i16> %670) #8
  %684 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %683, i32 %665) #8
  store <16 x i16> %684, <16 x i16>* %681, align 32
  %685 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 3
  %686 = bitcast <4 x i64>* %685 to <16 x i16>*
  %687 = load <16 x i16>, <16 x i16>* %686, align 32
  %688 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %687, <16 x i16> %670) #8
  %689 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %688, i32 %665) #8
  store <16 x i16> %689, <16 x i16>* %686, align 32
  %690 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 4
  %691 = bitcast <4 x i64>* %690 to <16 x i16>*
  %692 = load <16 x i16>, <16 x i16>* %691, align 32
  %693 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %692, <16 x i16> %670) #8
  %694 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %693, i32 %665) #8
  store <16 x i16> %694, <16 x i16>* %691, align 32
  %695 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 5
  %696 = bitcast <4 x i64>* %695 to <16 x i16>*
  %697 = load <16 x i16>, <16 x i16>* %696, align 32
  %698 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %697, <16 x i16> %670) #8
  %699 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %698, i32 %665) #8
  store <16 x i16> %699, <16 x i16>* %696, align 32
  %700 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 6
  %701 = bitcast <4 x i64>* %700 to <16 x i16>*
  %702 = load <16 x i16>, <16 x i16>* %701, align 32
  %703 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %702, <16 x i16> %670) #8
  %704 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %703, i32 %665) #8
  store <16 x i16> %704, <16 x i16>* %701, align 32
  %705 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 7
  %706 = bitcast <4 x i64>* %705 to <16 x i16>*
  %707 = load <16 x i16>, <16 x i16>* %706, align 32
  %708 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %707, <16 x i16> %670) #8
  %709 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %708, i32 %665) #8
  store <16 x i16> %709, <16 x i16>* %706, align 32
  %710 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 8
  %711 = bitcast <4 x i64>* %710 to <16 x i16>*
  %712 = load <16 x i16>, <16 x i16>* %711, align 32
  %713 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %712, <16 x i16> %670) #8
  %714 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %713, i32 %665) #8
  store <16 x i16> %714, <16 x i16>* %711, align 32
  %715 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 9
  %716 = bitcast <4 x i64>* %715 to <16 x i16>*
  %717 = load <16 x i16>, <16 x i16>* %716, align 32
  %718 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %717, <16 x i16> %670) #8
  %719 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %718, i32 %665) #8
  store <16 x i16> %719, <16 x i16>* %716, align 32
  %720 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 10
  %721 = bitcast <4 x i64>* %720 to <16 x i16>*
  %722 = load <16 x i16>, <16 x i16>* %721, align 32
  %723 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %722, <16 x i16> %670) #8
  %724 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %723, i32 %665) #8
  store <16 x i16> %724, <16 x i16>* %721, align 32
  %725 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 11
  %726 = bitcast <4 x i64>* %725 to <16 x i16>*
  %727 = load <16 x i16>, <16 x i16>* %726, align 32
  %728 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %727, <16 x i16> %670) #8
  %729 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %728, i32 %665) #8
  store <16 x i16> %729, <16 x i16>* %726, align 32
  %730 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 12
  %731 = bitcast <4 x i64>* %730 to <16 x i16>*
  %732 = load <16 x i16>, <16 x i16>* %731, align 32
  %733 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %732, <16 x i16> %670) #8
  %734 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %733, i32 %665) #8
  store <16 x i16> %734, <16 x i16>* %731, align 32
  %735 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 13
  %736 = bitcast <4 x i64>* %735 to <16 x i16>*
  %737 = load <16 x i16>, <16 x i16>* %736, align 32
  %738 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %737, <16 x i16> %670) #8
  %739 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %738, i32 %665) #8
  store <16 x i16> %739, <16 x i16>* %736, align 32
  %740 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 14
  %741 = bitcast <4 x i64>* %740 to <16 x i16>*
  %742 = load <16 x i16>, <16 x i16>* %741, align 32
  %743 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %742, <16 x i16> %670) #8
  %744 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %743, i32 %665) #8
  store <16 x i16> %744, <16 x i16>* %741, align 32
  %745 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 15
  %746 = bitcast <4 x i64>* %745 to <16 x i16>*
  %747 = load <16 x i16>, <16 x i16>* %746, align 32
  %748 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %747, <16 x i16> %670) #8
  %749 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %748, i32 %665) #8
  store <16 x i16> %749, <16 x i16>* %746, align 32
  %750 = bitcast <16 x i16> %674 to <2 x i128>
  %751 = extractelement <2 x i128> %750, i32 0
  %752 = bitcast i128 %751 to <2 x i64>
  %753 = bitcast <16 x i16> %714 to <2 x i128>
  %754 = extractelement <2 x i128> %753, i32 0
  %755 = bitcast i128 %754 to <2 x i64>
  %756 = bitcast <16 x i16> %679 to <2 x i128>
  %757 = extractelement <2 x i128> %756, i32 0
  %758 = bitcast i128 %757 to <2 x i64>
  %759 = bitcast <16 x i16> %719 to <2 x i128>
  %760 = extractelement <2 x i128> %759, i32 0
  %761 = bitcast i128 %760 to <2 x i64>
  %762 = bitcast <16 x i16> %684 to <2 x i128>
  %763 = extractelement <2 x i128> %762, i32 0
  %764 = bitcast i128 %763 to <2 x i64>
  %765 = bitcast <16 x i16> %724 to <2 x i128>
  %766 = extractelement <2 x i128> %765, i32 0
  %767 = bitcast i128 %766 to <2 x i64>
  %768 = bitcast <16 x i16> %689 to <2 x i128>
  %769 = extractelement <2 x i128> %768, i32 0
  %770 = bitcast i128 %769 to <2 x i64>
  %771 = bitcast <16 x i16> %729 to <2 x i128>
  %772 = extractelement <2 x i128> %771, i32 0
  %773 = bitcast i128 %772 to <2 x i64>
  %774 = bitcast <16 x i16> %694 to <2 x i128>
  %775 = extractelement <2 x i128> %774, i32 0
  %776 = bitcast i128 %775 to <2 x i64>
  %777 = bitcast <16 x i16> %734 to <2 x i128>
  %778 = extractelement <2 x i128> %777, i32 0
  %779 = bitcast i128 %778 to <2 x i64>
  %780 = bitcast <16 x i16> %699 to <2 x i128>
  %781 = extractelement <2 x i128> %780, i32 0
  %782 = bitcast i128 %781 to <2 x i64>
  %783 = bitcast <16 x i16> %739 to <2 x i128>
  %784 = extractelement <2 x i128> %783, i32 0
  %785 = bitcast i128 %784 to <2 x i64>
  %786 = bitcast <16 x i16> %704 to <2 x i128>
  %787 = extractelement <2 x i128> %786, i32 0
  %788 = bitcast i128 %787 to <2 x i64>
  %789 = bitcast <16 x i16> %744 to <2 x i128>
  %790 = extractelement <2 x i128> %789, i32 0
  %791 = bitcast i128 %790 to <2 x i64>
  %792 = bitcast <16 x i16> %709 to <2 x i128>
  %793 = extractelement <2 x i128> %792, i32 0
  %794 = bitcast i128 %793 to <2 x i64>
  %795 = bitcast <16 x i16> %749 to <2 x i128>
  %796 = extractelement <2 x i128> %795, i32 0
  %797 = bitcast i128 %796 to <2 x i64>
  %798 = bitcast <16 x i16> %714 to <2 x i128>
  %799 = extractelement <2 x i128> %798, i32 1
  %800 = bitcast i128 %799 to <2 x i64>
  %801 = bitcast <16 x i16> %719 to <2 x i128>
  %802 = extractelement <2 x i128> %801, i32 1
  %803 = bitcast i128 %802 to <2 x i64>
  %804 = bitcast <16 x i16> %724 to <2 x i128>
  %805 = extractelement <2 x i128> %804, i32 1
  %806 = bitcast i128 %805 to <2 x i64>
  %807 = bitcast <16 x i16> %729 to <2 x i128>
  %808 = extractelement <2 x i128> %807, i32 1
  %809 = bitcast i128 %808 to <2 x i64>
  %810 = bitcast <16 x i16> %734 to <2 x i128>
  %811 = extractelement <2 x i128> %810, i32 1
  %812 = bitcast i128 %811 to <2 x i64>
  %813 = bitcast <16 x i16> %739 to <2 x i128>
  %814 = extractelement <2 x i128> %813, i32 1
  %815 = bitcast i128 %814 to <2 x i64>
  %816 = bitcast <16 x i16> %744 to <2 x i128>
  %817 = extractelement <2 x i128> %816, i32 1
  %818 = bitcast i128 %817 to <2 x i64>
  %819 = bitcast <16 x i16> %749 to <2 x i128>
  %820 = extractelement <2 x i128> %819, i32 1
  %821 = bitcast i128 %820 to <2 x i64>
  br label %1024

822:                                              ; preds = %659
  %823 = icmp eq i8 %661, 0
  br i1 %823, label %824, label %888

824:                                              ; preds = %822
  %825 = bitcast <4 x i64>* %660 to <2 x i64>*
  %826 = load <2 x i64>, <2 x i64>* %825, align 16
  %827 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 8
  %828 = bitcast <4 x i64>* %827 to <2 x i64>*
  %829 = load <2 x i64>, <2 x i64>* %828, align 16
  %830 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 1
  %831 = bitcast <4 x i64>* %830 to <2 x i64>*
  %832 = load <2 x i64>, <2 x i64>* %831, align 16
  %833 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 9
  %834 = bitcast <4 x i64>* %833 to <2 x i64>*
  %835 = load <2 x i64>, <2 x i64>* %834, align 16
  %836 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 2
  %837 = bitcast <4 x i64>* %836 to <2 x i64>*
  %838 = load <2 x i64>, <2 x i64>* %837, align 16
  %839 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 10
  %840 = bitcast <4 x i64>* %839 to <2 x i64>*
  %841 = load <2 x i64>, <2 x i64>* %840, align 16
  %842 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 3
  %843 = bitcast <4 x i64>* %842 to <2 x i64>*
  %844 = load <2 x i64>, <2 x i64>* %843, align 16
  %845 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 11
  %846 = bitcast <4 x i64>* %845 to <2 x i64>*
  %847 = load <2 x i64>, <2 x i64>* %846, align 16
  %848 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 4
  %849 = bitcast <4 x i64>* %848 to <2 x i64>*
  %850 = load <2 x i64>, <2 x i64>* %849, align 16
  %851 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 12
  %852 = bitcast <4 x i64>* %851 to <2 x i64>*
  %853 = load <2 x i64>, <2 x i64>* %852, align 16
  %854 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 5
  %855 = bitcast <4 x i64>* %854 to <2 x i64>*
  %856 = load <2 x i64>, <2 x i64>* %855, align 16
  %857 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 13
  %858 = bitcast <4 x i64>* %857 to <2 x i64>*
  %859 = load <2 x i64>, <2 x i64>* %858, align 16
  %860 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 6
  %861 = bitcast <4 x i64>* %860 to <2 x i64>*
  %862 = load <2 x i64>, <2 x i64>* %861, align 16
  %863 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 14
  %864 = bitcast <4 x i64>* %863 to <2 x i64>*
  %865 = load <2 x i64>, <2 x i64>* %864, align 16
  %866 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 7
  %867 = bitcast <4 x i64>* %866 to <2 x i64>*
  %868 = load <2 x i64>, <2 x i64>* %867, align 16
  %869 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 15
  %870 = bitcast <4 x i64>* %869 to <2 x i64>*
  %871 = load <2 x i64>, <2 x i64>* %870, align 16
  %872 = getelementptr inbounds <2 x i64>, <2 x i64>* %828, i64 1
  %873 = load <2 x i64>, <2 x i64>* %872, align 16
  %874 = getelementptr inbounds <2 x i64>, <2 x i64>* %834, i64 1
  %875 = load <2 x i64>, <2 x i64>* %874, align 16
  %876 = getelementptr inbounds <2 x i64>, <2 x i64>* %840, i64 1
  %877 = load <2 x i64>, <2 x i64>* %876, align 16
  %878 = getelementptr inbounds <2 x i64>, <2 x i64>* %846, i64 1
  %879 = load <2 x i64>, <2 x i64>* %878, align 16
  %880 = getelementptr inbounds <2 x i64>, <2 x i64>* %852, i64 1
  %881 = load <2 x i64>, <2 x i64>* %880, align 16
  %882 = getelementptr inbounds <2 x i64>, <2 x i64>* %858, i64 1
  %883 = load <2 x i64>, <2 x i64>* %882, align 16
  %884 = getelementptr inbounds <2 x i64>, <2 x i64>* %864, i64 1
  %885 = load <2 x i64>, <2 x i64>* %884, align 16
  %886 = getelementptr inbounds <2 x i64>, <2 x i64>* %870, i64 1
  %887 = load <2 x i64>, <2 x i64>* %886, align 16
  br label %1024

888:                                              ; preds = %822
  %889 = bitcast <4 x i64>* %660 to <16 x i16>*
  %890 = load <16 x i16>, <16 x i16>* %889, align 32
  %891 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %890, i32 %662) #8
  store <16 x i16> %891, <16 x i16>* %889, align 32
  %892 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 1
  %893 = bitcast <4 x i64>* %892 to <16 x i16>*
  %894 = load <16 x i16>, <16 x i16>* %893, align 32
  %895 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %894, i32 %662) #8
  store <16 x i16> %895, <16 x i16>* %893, align 32
  %896 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 2
  %897 = bitcast <4 x i64>* %896 to <16 x i16>*
  %898 = load <16 x i16>, <16 x i16>* %897, align 32
  %899 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %898, i32 %662) #8
  store <16 x i16> %899, <16 x i16>* %897, align 32
  %900 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 3
  %901 = bitcast <4 x i64>* %900 to <16 x i16>*
  %902 = load <16 x i16>, <16 x i16>* %901, align 32
  %903 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %902, i32 %662) #8
  store <16 x i16> %903, <16 x i16>* %901, align 32
  %904 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 4
  %905 = bitcast <4 x i64>* %904 to <16 x i16>*
  %906 = load <16 x i16>, <16 x i16>* %905, align 32
  %907 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %906, i32 %662) #8
  store <16 x i16> %907, <16 x i16>* %905, align 32
  %908 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 5
  %909 = bitcast <4 x i64>* %908 to <16 x i16>*
  %910 = load <16 x i16>, <16 x i16>* %909, align 32
  %911 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %910, i32 %662) #8
  store <16 x i16> %911, <16 x i16>* %909, align 32
  %912 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 6
  %913 = bitcast <4 x i64>* %912 to <16 x i16>*
  %914 = load <16 x i16>, <16 x i16>* %913, align 32
  %915 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %914, i32 %662) #8
  store <16 x i16> %915, <16 x i16>* %913, align 32
  %916 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 7
  %917 = bitcast <4 x i64>* %916 to <16 x i16>*
  %918 = load <16 x i16>, <16 x i16>* %917, align 32
  %919 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %918, i32 %662) #8
  store <16 x i16> %919, <16 x i16>* %917, align 32
  %920 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 8
  %921 = bitcast <4 x i64>* %920 to <16 x i16>*
  %922 = load <16 x i16>, <16 x i16>* %921, align 32
  %923 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %922, i32 %662) #8
  store <16 x i16> %923, <16 x i16>* %921, align 32
  %924 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 9
  %925 = bitcast <4 x i64>* %924 to <16 x i16>*
  %926 = load <16 x i16>, <16 x i16>* %925, align 32
  %927 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %926, i32 %662) #8
  store <16 x i16> %927, <16 x i16>* %925, align 32
  %928 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 10
  %929 = bitcast <4 x i64>* %928 to <16 x i16>*
  %930 = load <16 x i16>, <16 x i16>* %929, align 32
  %931 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %930, i32 %662) #8
  store <16 x i16> %931, <16 x i16>* %929, align 32
  %932 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 11
  %933 = bitcast <4 x i64>* %932 to <16 x i16>*
  %934 = load <16 x i16>, <16 x i16>* %933, align 32
  %935 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %934, i32 %662) #8
  store <16 x i16> %935, <16 x i16>* %933, align 32
  %936 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 12
  %937 = bitcast <4 x i64>* %936 to <16 x i16>*
  %938 = load <16 x i16>, <16 x i16>* %937, align 32
  %939 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %938, i32 %662) #8
  store <16 x i16> %939, <16 x i16>* %937, align 32
  %940 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 13
  %941 = bitcast <4 x i64>* %940 to <16 x i16>*
  %942 = load <16 x i16>, <16 x i16>* %941, align 32
  %943 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %942, i32 %662) #8
  store <16 x i16> %943, <16 x i16>* %941, align 32
  %944 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 14
  %945 = bitcast <4 x i64>* %944 to <16 x i16>*
  %946 = load <16 x i16>, <16 x i16>* %945, align 32
  %947 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %946, i32 %662) #8
  store <16 x i16> %947, <16 x i16>* %945, align 32
  %948 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 15
  %949 = bitcast <4 x i64>* %948 to <16 x i16>*
  %950 = load <16 x i16>, <16 x i16>* %949, align 32
  %951 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %950, i32 %662) #8
  store <16 x i16> %951, <16 x i16>* %949, align 32
  %952 = bitcast <16 x i16> %891 to <2 x i128>
  %953 = extractelement <2 x i128> %952, i32 0
  %954 = bitcast i128 %953 to <2 x i64>
  %955 = bitcast <16 x i16> %923 to <2 x i128>
  %956 = extractelement <2 x i128> %955, i32 0
  %957 = bitcast i128 %956 to <2 x i64>
  %958 = bitcast <16 x i16> %895 to <2 x i128>
  %959 = extractelement <2 x i128> %958, i32 0
  %960 = bitcast i128 %959 to <2 x i64>
  %961 = bitcast <16 x i16> %927 to <2 x i128>
  %962 = extractelement <2 x i128> %961, i32 0
  %963 = bitcast i128 %962 to <2 x i64>
  %964 = bitcast <16 x i16> %899 to <2 x i128>
  %965 = extractelement <2 x i128> %964, i32 0
  %966 = bitcast i128 %965 to <2 x i64>
  %967 = bitcast <16 x i16> %931 to <2 x i128>
  %968 = extractelement <2 x i128> %967, i32 0
  %969 = bitcast i128 %968 to <2 x i64>
  %970 = bitcast <16 x i16> %903 to <2 x i128>
  %971 = extractelement <2 x i128> %970, i32 0
  %972 = bitcast i128 %971 to <2 x i64>
  %973 = bitcast <16 x i16> %935 to <2 x i128>
  %974 = extractelement <2 x i128> %973, i32 0
  %975 = bitcast i128 %974 to <2 x i64>
  %976 = bitcast <16 x i16> %907 to <2 x i128>
  %977 = extractelement <2 x i128> %976, i32 0
  %978 = bitcast i128 %977 to <2 x i64>
  %979 = bitcast <16 x i16> %939 to <2 x i128>
  %980 = extractelement <2 x i128> %979, i32 0
  %981 = bitcast i128 %980 to <2 x i64>
  %982 = bitcast <16 x i16> %911 to <2 x i128>
  %983 = extractelement <2 x i128> %982, i32 0
  %984 = bitcast i128 %983 to <2 x i64>
  %985 = bitcast <16 x i16> %943 to <2 x i128>
  %986 = extractelement <2 x i128> %985, i32 0
  %987 = bitcast i128 %986 to <2 x i64>
  %988 = bitcast <16 x i16> %915 to <2 x i128>
  %989 = extractelement <2 x i128> %988, i32 0
  %990 = bitcast i128 %989 to <2 x i64>
  %991 = bitcast <16 x i16> %947 to <2 x i128>
  %992 = extractelement <2 x i128> %991, i32 0
  %993 = bitcast i128 %992 to <2 x i64>
  %994 = bitcast <16 x i16> %919 to <2 x i128>
  %995 = extractelement <2 x i128> %994, i32 0
  %996 = bitcast i128 %995 to <2 x i64>
  %997 = bitcast <16 x i16> %951 to <2 x i128>
  %998 = extractelement <2 x i128> %997, i32 0
  %999 = bitcast i128 %998 to <2 x i64>
  %1000 = bitcast <16 x i16> %923 to <2 x i128>
  %1001 = extractelement <2 x i128> %1000, i32 1
  %1002 = bitcast i128 %1001 to <2 x i64>
  %1003 = bitcast <16 x i16> %927 to <2 x i128>
  %1004 = extractelement <2 x i128> %1003, i32 1
  %1005 = bitcast i128 %1004 to <2 x i64>
  %1006 = bitcast <16 x i16> %931 to <2 x i128>
  %1007 = extractelement <2 x i128> %1006, i32 1
  %1008 = bitcast i128 %1007 to <2 x i64>
  %1009 = bitcast <16 x i16> %935 to <2 x i128>
  %1010 = extractelement <2 x i128> %1009, i32 1
  %1011 = bitcast i128 %1010 to <2 x i64>
  %1012 = bitcast <16 x i16> %939 to <2 x i128>
  %1013 = extractelement <2 x i128> %1012, i32 1
  %1014 = bitcast i128 %1013 to <2 x i64>
  %1015 = bitcast <16 x i16> %943 to <2 x i128>
  %1016 = extractelement <2 x i128> %1015, i32 1
  %1017 = bitcast i128 %1016 to <2 x i64>
  %1018 = bitcast <16 x i16> %947 to <2 x i128>
  %1019 = extractelement <2 x i128> %1018, i32 1
  %1020 = bitcast i128 %1019 to <2 x i64>
  %1021 = bitcast <16 x i16> %951 to <2 x i128>
  %1022 = extractelement <2 x i128> %1021, i32 1
  %1023 = bitcast i128 %1022 to <2 x i64>
  br label %1024

1024:                                             ; preds = %824, %888, %664
  %1025 = phi <2 x i64> [ %887, %824 ], [ %1023, %888 ], [ %821, %664 ]
  %1026 = phi <2 x i64> [ %885, %824 ], [ %1020, %888 ], [ %818, %664 ]
  %1027 = phi <2 x i64> [ %883, %824 ], [ %1017, %888 ], [ %815, %664 ]
  %1028 = phi <2 x i64> [ %881, %824 ], [ %1014, %888 ], [ %812, %664 ]
  %1029 = phi <2 x i64> [ %879, %824 ], [ %1011, %888 ], [ %809, %664 ]
  %1030 = phi <2 x i64> [ %877, %824 ], [ %1008, %888 ], [ %806, %664 ]
  %1031 = phi <2 x i64> [ %875, %824 ], [ %1005, %888 ], [ %803, %664 ]
  %1032 = phi <2 x i64> [ %873, %824 ], [ %1002, %888 ], [ %800, %664 ]
  %1033 = phi <2 x i64> [ %871, %824 ], [ %999, %888 ], [ %797, %664 ]
  %1034 = phi <2 x i64> [ %868, %824 ], [ %996, %888 ], [ %794, %664 ]
  %1035 = phi <2 x i64> [ %865, %824 ], [ %993, %888 ], [ %791, %664 ]
  %1036 = phi <2 x i64> [ %862, %824 ], [ %990, %888 ], [ %788, %664 ]
  %1037 = phi <2 x i64> [ %859, %824 ], [ %987, %888 ], [ %785, %664 ]
  %1038 = phi <2 x i64> [ %856, %824 ], [ %984, %888 ], [ %782, %664 ]
  %1039 = phi <2 x i64> [ %853, %824 ], [ %981, %888 ], [ %779, %664 ]
  %1040 = phi <2 x i64> [ %850, %824 ], [ %978, %888 ], [ %776, %664 ]
  %1041 = phi <2 x i64> [ %847, %824 ], [ %975, %888 ], [ %773, %664 ]
  %1042 = phi <2 x i64> [ %844, %824 ], [ %972, %888 ], [ %770, %664 ]
  %1043 = phi <2 x i64> [ %841, %824 ], [ %969, %888 ], [ %767, %664 ]
  %1044 = phi <2 x i64> [ %838, %824 ], [ %966, %888 ], [ %764, %664 ]
  %1045 = phi <2 x i64> [ %835, %824 ], [ %963, %888 ], [ %761, %664 ]
  %1046 = phi <2 x i64> [ %832, %824 ], [ %960, %888 ], [ %758, %664 ]
  %1047 = phi <2 x i64> [ %829, %824 ], [ %957, %888 ], [ %755, %664 ]
  %1048 = phi <2 x i64> [ %826, %824 ], [ %954, %888 ], [ %752, %664 ]
  %1049 = bitcast <4 x i64>* %660 to <2 x i64>*
  %1050 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 8
  %1051 = shufflevector <2 x i64> %1048, <2 x i64> %1047, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1052 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 1
  %1053 = bitcast <4 x i64>* %1052 to <2 x i64>*
  %1054 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 9
  %1055 = shufflevector <2 x i64> %1046, <2 x i64> %1045, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1056 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 2
  %1057 = bitcast <4 x i64>* %1056 to <2 x i64>*
  %1058 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 10
  %1059 = shufflevector <2 x i64> %1044, <2 x i64> %1043, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1060 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 3
  %1061 = bitcast <4 x i64>* %1060 to <2 x i64>*
  %1062 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 11
  %1063 = shufflevector <2 x i64> %1042, <2 x i64> %1041, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1064 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 4
  %1065 = bitcast <4 x i64>* %1064 to <2 x i64>*
  %1066 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 12
  %1067 = shufflevector <2 x i64> %1040, <2 x i64> %1039, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1068 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 5
  %1069 = bitcast <4 x i64>* %1068 to <2 x i64>*
  %1070 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 13
  %1071 = shufflevector <2 x i64> %1038, <2 x i64> %1037, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1072 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 6
  %1073 = bitcast <4 x i64>* %1072 to <2 x i64>*
  %1074 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 14
  %1075 = shufflevector <2 x i64> %1036, <2 x i64> %1035, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1076 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 7
  %1077 = bitcast <4 x i64>* %1076 to <2 x i64>*
  %1078 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 15
  %1079 = shufflevector <2 x i64> %1034, <2 x i64> %1033, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1080 = getelementptr inbounds <2 x i64>, <2 x i64>* %1049, i64 1
  %1081 = load <2 x i64>, <2 x i64>* %1080, align 16
  %1082 = shufflevector <2 x i64> %1081, <2 x i64> %1032, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1083 = getelementptr inbounds <2 x i64>, <2 x i64>* %1053, i64 1
  %1084 = load <2 x i64>, <2 x i64>* %1083, align 16
  %1085 = shufflevector <2 x i64> %1084, <2 x i64> %1031, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1086 = getelementptr inbounds <2 x i64>, <2 x i64>* %1057, i64 1
  %1087 = load <2 x i64>, <2 x i64>* %1086, align 16
  %1088 = shufflevector <2 x i64> %1087, <2 x i64> %1030, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1089 = getelementptr inbounds <2 x i64>, <2 x i64>* %1061, i64 1
  %1090 = load <2 x i64>, <2 x i64>* %1089, align 16
  %1091 = shufflevector <2 x i64> %1090, <2 x i64> %1029, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1092 = getelementptr inbounds <2 x i64>, <2 x i64>* %1065, i64 1
  %1093 = load <2 x i64>, <2 x i64>* %1092, align 16
  %1094 = shufflevector <2 x i64> %1093, <2 x i64> %1028, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1095 = getelementptr inbounds <2 x i64>, <2 x i64>* %1069, i64 1
  %1096 = load <2 x i64>, <2 x i64>* %1095, align 16
  %1097 = shufflevector <2 x i64> %1096, <2 x i64> %1027, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1098 = getelementptr inbounds <2 x i64>, <2 x i64>* %1073, i64 1
  %1099 = load <2 x i64>, <2 x i64>* %1098, align 16
  %1100 = shufflevector <2 x i64> %1099, <2 x i64> %1026, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1101 = getelementptr inbounds <2 x i64>, <2 x i64>* %1077, i64 1
  %1102 = load <2 x i64>, <2 x i64>* %1101, align 16
  %1103 = shufflevector <2 x i64> %1102, <2 x i64> %1025, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1104 = bitcast <4 x i64> %1051 to <16 x i16>
  %1105 = bitcast <4 x i64> %1055 to <16 x i16>
  %1106 = shufflevector <16 x i16> %1104, <16 x i16> %1105, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1107 = shufflevector <16 x i16> %1104, <16 x i16> %1105, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1108 = bitcast <4 x i64> %1059 to <16 x i16>
  %1109 = bitcast <4 x i64> %1063 to <16 x i16>
  %1110 = shufflevector <16 x i16> %1108, <16 x i16> %1109, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1111 = shufflevector <16 x i16> %1108, <16 x i16> %1109, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1112 = bitcast <4 x i64> %1067 to <16 x i16>
  %1113 = bitcast <4 x i64> %1071 to <16 x i16>
  %1114 = shufflevector <16 x i16> %1112, <16 x i16> %1113, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1115 = shufflevector <16 x i16> %1112, <16 x i16> %1113, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1116 = bitcast <4 x i64> %1075 to <16 x i16>
  %1117 = bitcast <4 x i64> %1079 to <16 x i16>
  %1118 = shufflevector <16 x i16> %1116, <16 x i16> %1117, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1119 = shufflevector <16 x i16> %1116, <16 x i16> %1117, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1120 = bitcast <16 x i16> %1106 to <8 x i32>
  %1121 = bitcast <16 x i16> %1110 to <8 x i32>
  %1122 = shufflevector <8 x i32> %1120, <8 x i32> %1121, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1123 = shufflevector <8 x i32> %1120, <8 x i32> %1121, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1124 = bitcast <16 x i16> %1114 to <8 x i32>
  %1125 = bitcast <16 x i16> %1118 to <8 x i32>
  %1126 = shufflevector <8 x i32> %1124, <8 x i32> %1125, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1127 = shufflevector <8 x i32> %1124, <8 x i32> %1125, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1128 = bitcast <16 x i16> %1107 to <8 x i32>
  %1129 = bitcast <16 x i16> %1111 to <8 x i32>
  %1130 = shufflevector <8 x i32> %1128, <8 x i32> %1129, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1131 = shufflevector <8 x i32> %1128, <8 x i32> %1129, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1132 = bitcast <16 x i16> %1115 to <8 x i32>
  %1133 = bitcast <16 x i16> %1119 to <8 x i32>
  %1134 = shufflevector <8 x i32> %1132, <8 x i32> %1133, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1135 = shufflevector <8 x i32> %1132, <8 x i32> %1133, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1136 = bitcast <8 x i32> %1122 to <4 x i64>
  %1137 = bitcast <8 x i32> %1126 to <4 x i64>
  %1138 = shufflevector <4 x i64> %1136, <4 x i64> %1137, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1138, <4 x i64>* %660, align 32
  %1139 = shufflevector <4 x i64> %1136, <4 x i64> %1137, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1139, <4 x i64>* %1052, align 32
  %1140 = bitcast <8 x i32> %1130 to <4 x i64>
  %1141 = bitcast <8 x i32> %1134 to <4 x i64>
  %1142 = shufflevector <4 x i64> %1140, <4 x i64> %1141, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1142, <4 x i64>* %1064, align 32
  %1143 = shufflevector <4 x i64> %1140, <4 x i64> %1141, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1143, <4 x i64>* %1068, align 32
  %1144 = bitcast <8 x i32> %1123 to <4 x i64>
  %1145 = bitcast <8 x i32> %1127 to <4 x i64>
  %1146 = shufflevector <4 x i64> %1144, <4 x i64> %1145, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1146, <4 x i64>* %1056, align 32
  %1147 = shufflevector <4 x i64> %1144, <4 x i64> %1145, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1147, <4 x i64>* %1060, align 32
  %1148 = bitcast <8 x i32> %1131 to <4 x i64>
  %1149 = bitcast <8 x i32> %1135 to <4 x i64>
  %1150 = shufflevector <4 x i64> %1148, <4 x i64> %1149, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1150, <4 x i64>* %1072, align 32
  %1151 = shufflevector <4 x i64> %1148, <4 x i64> %1149, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1151, <4 x i64>* %1076, align 32
  %1152 = bitcast <4 x i64> %1082 to <16 x i16>
  %1153 = bitcast <4 x i64> %1085 to <16 x i16>
  %1154 = shufflevector <16 x i16> %1152, <16 x i16> %1153, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1155 = shufflevector <16 x i16> %1152, <16 x i16> %1153, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1156 = bitcast <4 x i64> %1088 to <16 x i16>
  %1157 = bitcast <4 x i64> %1091 to <16 x i16>
  %1158 = shufflevector <16 x i16> %1156, <16 x i16> %1157, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1159 = shufflevector <16 x i16> %1156, <16 x i16> %1157, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1160 = bitcast <4 x i64> %1094 to <16 x i16>
  %1161 = bitcast <4 x i64> %1097 to <16 x i16>
  %1162 = shufflevector <16 x i16> %1160, <16 x i16> %1161, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1163 = shufflevector <16 x i16> %1160, <16 x i16> %1161, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1164 = bitcast <4 x i64> %1100 to <16 x i16>
  %1165 = bitcast <4 x i64> %1103 to <16 x i16>
  %1166 = shufflevector <16 x i16> %1164, <16 x i16> %1165, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1167 = shufflevector <16 x i16> %1164, <16 x i16> %1165, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1168 = bitcast <16 x i16> %1154 to <8 x i32>
  %1169 = bitcast <16 x i16> %1158 to <8 x i32>
  %1170 = shufflevector <8 x i32> %1168, <8 x i32> %1169, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1171 = shufflevector <8 x i32> %1168, <8 x i32> %1169, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1172 = bitcast <16 x i16> %1162 to <8 x i32>
  %1173 = bitcast <16 x i16> %1166 to <8 x i32>
  %1174 = shufflevector <8 x i32> %1172, <8 x i32> %1173, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1175 = shufflevector <8 x i32> %1172, <8 x i32> %1173, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1176 = bitcast <16 x i16> %1155 to <8 x i32>
  %1177 = bitcast <16 x i16> %1159 to <8 x i32>
  %1178 = shufflevector <8 x i32> %1176, <8 x i32> %1177, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1179 = shufflevector <8 x i32> %1176, <8 x i32> %1177, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1180 = bitcast <16 x i16> %1163 to <8 x i32>
  %1181 = bitcast <16 x i16> %1167 to <8 x i32>
  %1182 = shufflevector <8 x i32> %1180, <8 x i32> %1181, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1183 = shufflevector <8 x i32> %1180, <8 x i32> %1181, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1184 = bitcast <8 x i32> %1170 to <4 x i64>
  %1185 = bitcast <8 x i32> %1174 to <4 x i64>
  %1186 = shufflevector <4 x i64> %1184, <4 x i64> %1185, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1186, <4 x i64>* %1050, align 32
  %1187 = shufflevector <4 x i64> %1184, <4 x i64> %1185, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1187, <4 x i64>* %1054, align 32
  %1188 = bitcast <8 x i32> %1178 to <4 x i64>
  %1189 = bitcast <8 x i32> %1182 to <4 x i64>
  %1190 = shufflevector <4 x i64> %1188, <4 x i64> %1189, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1190, <4 x i64>* %1066, align 32
  %1191 = shufflevector <4 x i64> %1188, <4 x i64> %1189, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1191, <4 x i64>* %1070, align 32
  %1192 = bitcast <8 x i32> %1171 to <4 x i64>
  %1193 = bitcast <8 x i32> %1175 to <4 x i64>
  %1194 = shufflevector <4 x i64> %1192, <4 x i64> %1193, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1194, <4 x i64>* %1058, align 32
  %1195 = shufflevector <4 x i64> %1192, <4 x i64> %1193, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1195, <4 x i64>* %1062, align 32
  %1196 = bitcast <8 x i32> %1179 to <4 x i64>
  %1197 = bitcast <8 x i32> %1183 to <4 x i64>
  %1198 = shufflevector <4 x i64> %1196, <4 x i64> %1197, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1198, <4 x i64>* %1074, align 32
  %1199 = shufflevector <4 x i64> %1196, <4 x i64> %1197, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1199, <4 x i64>* %1078, align 32
  %1200 = shl nsw i64 %624, 8
  %1201 = getelementptr inbounds i32, i32* %1, i64 %1200
  %1202 = shufflevector <4 x i64> %1138, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %1203 = bitcast <4 x i64> %1202 to <16 x i16>
  %1204 = shufflevector <16 x i16> %1203, <16 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1205 = shufflevector <16 x i16> %1203, <16 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1206 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1204, <16 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %1207 = ashr <8 x i32> %1206, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %1208 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1205, <16 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %1209 = ashr <8 x i32> %1208, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %1210 = bitcast i32* %1201 to <8 x i32>*
  store <8 x i32> %1207, <8 x i32>* %1210, align 32
  %1211 = getelementptr inbounds i32, i32* %1201, i64 8
  %1212 = bitcast i32* %1211 to <8 x i32>*
  store <8 x i32> %1209, <8 x i32>* %1212, align 32
  br label %1213

1213:                                             ; preds = %1024, %1213
  %1214 = phi i64 [ 1, %1024 ], [ %1230, %1213 ]
  %1215 = getelementptr inbounds <4 x i64>, <4 x i64>* %660, i64 %1214
  %1216 = load <4 x i64>, <4 x i64>* %1215, align 32
  %1217 = shl nsw i64 %1214, 4
  %1218 = getelementptr inbounds i32, i32* %1201, i64 %1217
  %1219 = shufflevector <4 x i64> %1216, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %1220 = bitcast <4 x i64> %1219 to <16 x i16>
  %1221 = shufflevector <16 x i16> %1220, <16 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1222 = shufflevector <16 x i16> %1220, <16 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1223 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1221, <16 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %1224 = ashr <8 x i32> %1223, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %1225 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1222, <16 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %1226 = ashr <8 x i32> %1225, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %1227 = bitcast i32* %1218 to <8 x i32>*
  store <8 x i32> %1224, <8 x i32>* %1227, align 32
  %1228 = getelementptr inbounds i32, i32* %1218, i64 8
  %1229 = bitcast i32* %1228 to <8 x i32>*
  store <8 x i32> %1226, <8 x i32>* %1229, align 32
  %1230 = add nuw nsw i64 %1214, 1
  %1231 = icmp eq i64 %1230, 16
  br i1 %1231, label %1232, label %1213

1232:                                             ; preds = %1213
  %1233 = add nuw nsw i64 %624, 1
  %1234 = icmp eq i64 %1233, 2
  br i1 %1234, label %622, label %623
}

; Function Attrs: nounwind ssp uwtable
define internal void @lowbd_fwd_txfm2d_32x16_avx2(i16* nocapture readonly, i32* nocapture, i32, i8 zeroext, i32) #3 {
  %6 = alloca [32 x <4 x i64>], align 32
  %7 = alloca [64 x <4 x i64>], align 32
  %8 = bitcast [32 x <4 x i64>]* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 1024, i8* nonnull %8) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %8, i8 -86, i64 1024, i1 false)
  %9 = bitcast [64 x <4 x i64>]* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 2048, i8* nonnull %9) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %9, i8 -86, i64 2048, i1 false)
  %10 = load i8*, i8** getelementptr inbounds ([19 x i8*], [19 x i8*]* @av1_fwd_txfm_shift_ls, i64 0, i64 10), align 16
  %11 = load i8, i8* getelementptr inbounds ([5 x [5 x i8]], [5 x [5 x i8]]* @av1_fwd_cos_bit_col, i64 0, i64 3, i64 2), align 1
  %12 = load i8, i8* getelementptr inbounds ([5 x [5 x i8]], [5 x [5 x i8]]* @av1_fwd_cos_bit_row, i64 0, i64 3, i64 2), align 1
  %13 = zext i8 %3 to i64
  %14 = getelementptr inbounds [16 x void (<4 x i64>*, <4 x i64>*, i8)*], [16 x void (<4 x i64>*, <4 x i64>*, i8)*]* @col_txfm16x16_arr, i64 0, i64 %13
  %15 = load void (<4 x i64>*, <4 x i64>*, i8)*, void (<4 x i64>*, <4 x i64>*, i8)** %14, align 8
  %16 = getelementptr inbounds [16 x void (<4 x i64>*, <4 x i64>*, i8)*], [16 x void (<4 x i64>*, <4 x i64>*, i8)*]* @row_txfm16x32_arr, i64 0, i64 %13
  %17 = load void (<4 x i64>*, <4 x i64>*, i8)*, void (<4 x i64>*, <4 x i64>*, i8)** %16, align 8
  switch i8 %3, label %21 [
    i8 6, label %20
    i8 15, label %19
    i8 7, label %19
    i8 5, label %19
    i8 14, label %18
    i8 8, label %18
    i8 4, label %18
  ]

18:                                               ; preds = %5, %5, %5
  br label %21

19:                                               ; preds = %5, %5, %5
  br label %21

20:                                               ; preds = %5
  br label %21

21:                                               ; preds = %5, %18, %19, %20
  %22 = phi i1 [ false, %20 ], [ true, %19 ], [ false, %18 ], [ true, %5 ]
  %23 = phi i32 [ 1, %20 ], [ 1, %19 ], [ 0, %18 ], [ 0, %5 ]
  %24 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 0
  %25 = sext i32 %2 to i64
  %26 = getelementptr inbounds i8, i8* %10, i64 1
  %27 = bitcast [32 x <4 x i64>]* %6 to <2 x i64>*
  %28 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 8
  %29 = bitcast <4 x i64>* %28 to <2 x i64>*
  %30 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 1
  %31 = bitcast <4 x i64>* %30 to <2 x i64>*
  %32 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 9
  %33 = bitcast <4 x i64>* %32 to <2 x i64>*
  %34 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 2
  %35 = bitcast <4 x i64>* %34 to <2 x i64>*
  %36 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 10
  %37 = bitcast <4 x i64>* %36 to <2 x i64>*
  %38 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 3
  %39 = bitcast <4 x i64>* %38 to <2 x i64>*
  %40 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 11
  %41 = bitcast <4 x i64>* %40 to <2 x i64>*
  %42 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 4
  %43 = bitcast <4 x i64>* %42 to <2 x i64>*
  %44 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 12
  %45 = bitcast <4 x i64>* %44 to <2 x i64>*
  %46 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 5
  %47 = bitcast <4 x i64>* %46 to <2 x i64>*
  %48 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 13
  %49 = bitcast <4 x i64>* %48 to <2 x i64>*
  %50 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 6
  %51 = bitcast <4 x i64>* %50 to <2 x i64>*
  %52 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 14
  %53 = bitcast <4 x i64>* %52 to <2 x i64>*
  %54 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 7
  %55 = bitcast <4 x i64>* %54 to <2 x i64>*
  %56 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 15
  %57 = bitcast <4 x i64>* %56 to <2 x i64>*
  %58 = getelementptr inbounds <2 x i64>, <2 x i64>* %27, i64 1
  %59 = getelementptr inbounds <2 x i64>, <2 x i64>* %29, i64 1
  %60 = getelementptr inbounds <2 x i64>, <2 x i64>* %31, i64 1
  %61 = getelementptr inbounds <2 x i64>, <2 x i64>* %33, i64 1
  %62 = getelementptr inbounds <2 x i64>, <2 x i64>* %35, i64 1
  %63 = getelementptr inbounds <2 x i64>, <2 x i64>* %37, i64 1
  %64 = getelementptr inbounds <2 x i64>, <2 x i64>* %39, i64 1
  %65 = getelementptr inbounds <2 x i64>, <2 x i64>* %41, i64 1
  %66 = getelementptr inbounds <2 x i64>, <2 x i64>* %43, i64 1
  %67 = getelementptr inbounds <2 x i64>, <2 x i64>* %45, i64 1
  %68 = getelementptr inbounds <2 x i64>, <2 x i64>* %47, i64 1
  %69 = getelementptr inbounds <2 x i64>, <2 x i64>* %49, i64 1
  %70 = getelementptr inbounds <2 x i64>, <2 x i64>* %51, i64 1
  %71 = getelementptr inbounds <2 x i64>, <2 x i64>* %53, i64 1
  %72 = getelementptr inbounds <2 x i64>, <2 x i64>* %55, i64 1
  %73 = getelementptr inbounds <2 x i64>, <2 x i64>* %57, i64 1
  %74 = shl nsw i64 %25, 1
  %75 = mul nsw i64 %25, 3
  %76 = shl nsw i64 %25, 2
  %77 = mul nsw i64 %25, 5
  %78 = mul nsw i64 %25, 6
  %79 = mul nsw i64 %25, 7
  %80 = shl nsw i64 %25, 3
  %81 = mul nsw i64 %25, 9
  %82 = mul nsw i64 %25, 10
  %83 = mul nsw i64 %25, 11
  %84 = mul nsw i64 %25, 12
  %85 = mul nsw i64 %25, 13
  %86 = mul nsw i64 %25, 14
  %87 = mul nsw i64 %25, 15
  %88 = shl nsw i64 %25, 1
  %89 = mul nsw i64 %25, 3
  %90 = shl nsw i64 %25, 2
  %91 = mul nsw i64 %25, 5
  %92 = mul nsw i64 %25, 6
  %93 = mul nsw i64 %25, 7
  %94 = shl nsw i64 %25, 3
  %95 = mul nsw i64 %25, 9
  %96 = mul nsw i64 %25, 10
  %97 = mul nsw i64 %25, 11
  %98 = mul nsw i64 %25, 12
  %99 = mul nsw i64 %25, 13
  %100 = mul nsw i64 %25, 14
  %101 = mul nsw i64 %25, 15
  %102 = bitcast [32 x <4 x i64>]* %6 to <16 x i16>*
  %103 = bitcast <4 x i64>* %30 to <16 x i16>*
  %104 = bitcast <4 x i64>* %34 to <16 x i16>*
  %105 = bitcast <4 x i64>* %38 to <16 x i16>*
  %106 = bitcast <4 x i64>* %42 to <16 x i16>*
  %107 = bitcast <4 x i64>* %46 to <16 x i16>*
  %108 = bitcast <4 x i64>* %50 to <16 x i16>*
  %109 = bitcast <4 x i64>* %54 to <16 x i16>*
  %110 = bitcast <4 x i64>* %28 to <16 x i16>*
  %111 = bitcast <4 x i64>* %32 to <16 x i16>*
  %112 = bitcast <4 x i64>* %36 to <16 x i16>*
  %113 = bitcast <4 x i64>* %40 to <16 x i16>*
  %114 = bitcast <4 x i64>* %44 to <16 x i16>*
  %115 = bitcast <4 x i64>* %48 to <16 x i16>*
  %116 = bitcast <4 x i64>* %52 to <16 x i16>*
  %117 = bitcast <4 x i64>* %56 to <16 x i16>*
  %118 = bitcast [32 x <4 x i64>]* %6 to <16 x i16>*
  %119 = bitcast <4 x i64>* %30 to <16 x i16>*
  %120 = bitcast <4 x i64>* %34 to <16 x i16>*
  %121 = bitcast <4 x i64>* %38 to <16 x i16>*
  %122 = bitcast <4 x i64>* %42 to <16 x i16>*
  %123 = bitcast <4 x i64>* %46 to <16 x i16>*
  %124 = bitcast <4 x i64>* %50 to <16 x i16>*
  %125 = bitcast <4 x i64>* %54 to <16 x i16>*
  %126 = bitcast <4 x i64>* %28 to <16 x i16>*
  %127 = bitcast <4 x i64>* %32 to <16 x i16>*
  %128 = bitcast <4 x i64>* %36 to <16 x i16>*
  %129 = bitcast <4 x i64>* %40 to <16 x i16>*
  %130 = bitcast <4 x i64>* %44 to <16 x i16>*
  %131 = bitcast <4 x i64>* %48 to <16 x i16>*
  %132 = bitcast <4 x i64>* %52 to <16 x i16>*
  %133 = bitcast <4 x i64>* %56 to <16 x i16>*
  %134 = bitcast [32 x <4 x i64>]* %6 to <16 x i16>*
  %135 = bitcast <4 x i64>* %30 to <16 x i16>*
  %136 = bitcast <4 x i64>* %34 to <16 x i16>*
  %137 = bitcast <4 x i64>* %38 to <16 x i16>*
  %138 = bitcast <4 x i64>* %42 to <16 x i16>*
  %139 = bitcast <4 x i64>* %46 to <16 x i16>*
  %140 = bitcast <4 x i64>* %50 to <16 x i16>*
  %141 = bitcast <4 x i64>* %54 to <16 x i16>*
  %142 = bitcast <4 x i64>* %28 to <16 x i16>*
  %143 = bitcast <4 x i64>* %32 to <16 x i16>*
  %144 = bitcast <4 x i64>* %36 to <16 x i16>*
  %145 = bitcast <4 x i64>* %40 to <16 x i16>*
  %146 = bitcast <4 x i64>* %44 to <16 x i16>*
  %147 = bitcast <4 x i64>* %48 to <16 x i16>*
  %148 = bitcast <4 x i64>* %52 to <16 x i16>*
  %149 = bitcast <4 x i64>* %56 to <16 x i16>*
  %150 = bitcast [32 x <4 x i64>]* %6 to <16 x i16>*
  %151 = bitcast <4 x i64>* %30 to <16 x i16>*
  %152 = bitcast <4 x i64>* %34 to <16 x i16>*
  %153 = bitcast <4 x i64>* %38 to <16 x i16>*
  %154 = bitcast <4 x i64>* %42 to <16 x i16>*
  %155 = bitcast <4 x i64>* %46 to <16 x i16>*
  %156 = bitcast <4 x i64>* %50 to <16 x i16>*
  %157 = bitcast <4 x i64>* %54 to <16 x i16>*
  %158 = bitcast <4 x i64>* %28 to <16 x i16>*
  %159 = bitcast <4 x i64>* %32 to <16 x i16>*
  %160 = bitcast <4 x i64>* %36 to <16 x i16>*
  %161 = bitcast <4 x i64>* %40 to <16 x i16>*
  %162 = bitcast <4 x i64>* %44 to <16 x i16>*
  %163 = bitcast <4 x i64>* %48 to <16 x i16>*
  %164 = bitcast <4 x i64>* %52 to <16 x i16>*
  %165 = bitcast <4 x i64>* %56 to <16 x i16>*
  br label %168

166:                                              ; preds = %628
  %167 = icmp eq i32 %23, 0
  br i1 %167, label %808, label %791

168:                                              ; preds = %628, %21
  %169 = phi i64 [ 0, %21 ], [ %789, %628 ]
  %170 = shl nsw i64 %169, 4
  %171 = getelementptr inbounds i16, i16* %0, i64 %170
  %172 = bitcast i16* %171 to <4 x i64>*
  %173 = load <4 x i64>, <4 x i64>* %172, align 32
  br i1 %22, label %220, label %174

174:                                              ; preds = %168
  store <4 x i64> %173, <4 x i64>* %56, align 32
  %175 = getelementptr inbounds i16, i16* %171, i64 %25
  %176 = bitcast i16* %175 to <4 x i64>*
  %177 = load <4 x i64>, <4 x i64>* %176, align 32
  store <4 x i64> %177, <4 x i64>* %52, align 32
  %178 = getelementptr inbounds i16, i16* %171, i64 %74
  %179 = bitcast i16* %178 to <4 x i64>*
  %180 = load <4 x i64>, <4 x i64>* %179, align 32
  store <4 x i64> %180, <4 x i64>* %48, align 32
  %181 = getelementptr inbounds i16, i16* %171, i64 %75
  %182 = bitcast i16* %181 to <4 x i64>*
  %183 = load <4 x i64>, <4 x i64>* %182, align 32
  store <4 x i64> %183, <4 x i64>* %44, align 32
  %184 = getelementptr inbounds i16, i16* %171, i64 %76
  %185 = bitcast i16* %184 to <4 x i64>*
  %186 = load <4 x i64>, <4 x i64>* %185, align 32
  store <4 x i64> %186, <4 x i64>* %40, align 32
  %187 = getelementptr inbounds i16, i16* %171, i64 %77
  %188 = bitcast i16* %187 to <4 x i64>*
  %189 = load <4 x i64>, <4 x i64>* %188, align 32
  store <4 x i64> %189, <4 x i64>* %36, align 32
  %190 = getelementptr inbounds i16, i16* %171, i64 %78
  %191 = bitcast i16* %190 to <4 x i64>*
  %192 = load <4 x i64>, <4 x i64>* %191, align 32
  store <4 x i64> %192, <4 x i64>* %32, align 32
  %193 = getelementptr inbounds i16, i16* %171, i64 %79
  %194 = bitcast i16* %193 to <4 x i64>*
  %195 = load <4 x i64>, <4 x i64>* %194, align 32
  store <4 x i64> %195, <4 x i64>* %28, align 32
  %196 = getelementptr inbounds i16, i16* %171, i64 %80
  %197 = bitcast i16* %196 to <4 x i64>*
  %198 = load <4 x i64>, <4 x i64>* %197, align 32
  store <4 x i64> %198, <4 x i64>* %54, align 32
  %199 = getelementptr inbounds i16, i16* %171, i64 %81
  %200 = bitcast i16* %199 to <4 x i64>*
  %201 = load <4 x i64>, <4 x i64>* %200, align 32
  store <4 x i64> %201, <4 x i64>* %50, align 32
  %202 = getelementptr inbounds i16, i16* %171, i64 %82
  %203 = bitcast i16* %202 to <4 x i64>*
  %204 = load <4 x i64>, <4 x i64>* %203, align 32
  store <4 x i64> %204, <4 x i64>* %46, align 32
  %205 = getelementptr inbounds i16, i16* %171, i64 %83
  %206 = bitcast i16* %205 to <4 x i64>*
  %207 = load <4 x i64>, <4 x i64>* %206, align 32
  store <4 x i64> %207, <4 x i64>* %42, align 32
  %208 = getelementptr inbounds i16, i16* %171, i64 %84
  %209 = bitcast i16* %208 to <4 x i64>*
  %210 = load <4 x i64>, <4 x i64>* %209, align 32
  store <4 x i64> %210, <4 x i64>* %38, align 32
  %211 = getelementptr inbounds i16, i16* %171, i64 %85
  %212 = bitcast i16* %211 to <4 x i64>*
  %213 = load <4 x i64>, <4 x i64>* %212, align 32
  store <4 x i64> %213, <4 x i64>* %34, align 32
  %214 = getelementptr inbounds i16, i16* %171, i64 %86
  %215 = bitcast i16* %214 to <4 x i64>*
  %216 = load <4 x i64>, <4 x i64>* %215, align 32
  store <4 x i64> %216, <4 x i64>* %30, align 32
  %217 = getelementptr inbounds i16, i16* %171, i64 %87
  %218 = bitcast i16* %217 to <4 x i64>*
  %219 = load <4 x i64>, <4 x i64>* %218, align 32
  store <4 x i64> %219, <4 x i64>* %24, align 32
  br label %266

220:                                              ; preds = %168
  store <4 x i64> %173, <4 x i64>* %24, align 32
  %221 = getelementptr inbounds i16, i16* %171, i64 %25
  %222 = bitcast i16* %221 to <4 x i64>*
  %223 = load <4 x i64>, <4 x i64>* %222, align 32
  store <4 x i64> %223, <4 x i64>* %30, align 32
  %224 = getelementptr inbounds i16, i16* %171, i64 %88
  %225 = bitcast i16* %224 to <4 x i64>*
  %226 = load <4 x i64>, <4 x i64>* %225, align 32
  store <4 x i64> %226, <4 x i64>* %34, align 32
  %227 = getelementptr inbounds i16, i16* %171, i64 %89
  %228 = bitcast i16* %227 to <4 x i64>*
  %229 = load <4 x i64>, <4 x i64>* %228, align 32
  store <4 x i64> %229, <4 x i64>* %38, align 32
  %230 = getelementptr inbounds i16, i16* %171, i64 %90
  %231 = bitcast i16* %230 to <4 x i64>*
  %232 = load <4 x i64>, <4 x i64>* %231, align 32
  store <4 x i64> %232, <4 x i64>* %42, align 32
  %233 = getelementptr inbounds i16, i16* %171, i64 %91
  %234 = bitcast i16* %233 to <4 x i64>*
  %235 = load <4 x i64>, <4 x i64>* %234, align 32
  store <4 x i64> %235, <4 x i64>* %46, align 32
  %236 = getelementptr inbounds i16, i16* %171, i64 %92
  %237 = bitcast i16* %236 to <4 x i64>*
  %238 = load <4 x i64>, <4 x i64>* %237, align 32
  store <4 x i64> %238, <4 x i64>* %50, align 32
  %239 = getelementptr inbounds i16, i16* %171, i64 %93
  %240 = bitcast i16* %239 to <4 x i64>*
  %241 = load <4 x i64>, <4 x i64>* %240, align 32
  store <4 x i64> %241, <4 x i64>* %54, align 32
  %242 = getelementptr inbounds i16, i16* %171, i64 %94
  %243 = bitcast i16* %242 to <4 x i64>*
  %244 = load <4 x i64>, <4 x i64>* %243, align 32
  store <4 x i64> %244, <4 x i64>* %28, align 32
  %245 = getelementptr inbounds i16, i16* %171, i64 %95
  %246 = bitcast i16* %245 to <4 x i64>*
  %247 = load <4 x i64>, <4 x i64>* %246, align 32
  store <4 x i64> %247, <4 x i64>* %32, align 32
  %248 = getelementptr inbounds i16, i16* %171, i64 %96
  %249 = bitcast i16* %248 to <4 x i64>*
  %250 = load <4 x i64>, <4 x i64>* %249, align 32
  store <4 x i64> %250, <4 x i64>* %36, align 32
  %251 = getelementptr inbounds i16, i16* %171, i64 %97
  %252 = bitcast i16* %251 to <4 x i64>*
  %253 = load <4 x i64>, <4 x i64>* %252, align 32
  store <4 x i64> %253, <4 x i64>* %40, align 32
  %254 = getelementptr inbounds i16, i16* %171, i64 %98
  %255 = bitcast i16* %254 to <4 x i64>*
  %256 = load <4 x i64>, <4 x i64>* %255, align 32
  store <4 x i64> %256, <4 x i64>* %44, align 32
  %257 = getelementptr inbounds i16, i16* %171, i64 %99
  %258 = bitcast i16* %257 to <4 x i64>*
  %259 = load <4 x i64>, <4 x i64>* %258, align 32
  store <4 x i64> %259, <4 x i64>* %48, align 32
  %260 = getelementptr inbounds i16, i16* %171, i64 %100
  %261 = bitcast i16* %260 to <4 x i64>*
  %262 = load <4 x i64>, <4 x i64>* %261, align 32
  store <4 x i64> %262, <4 x i64>* %52, align 32
  %263 = getelementptr inbounds i16, i16* %171, i64 %101
  %264 = bitcast i16* %263 to <4 x i64>*
  %265 = load <4 x i64>, <4 x i64>* %264, align 32
  store <4 x i64> %265, <4 x i64>* %56, align 32
  br label %266

266:                                              ; preds = %174, %220
  %267 = phi <4 x i64> [ %189, %174 ], [ %250, %220 ]
  %268 = phi <4 x i64> [ %192, %174 ], [ %247, %220 ]
  %269 = phi <4 x i64> [ %195, %174 ], [ %244, %220 ]
  %270 = phi <4 x i64> [ %198, %174 ], [ %241, %220 ]
  %271 = phi <4 x i64> [ %201, %174 ], [ %238, %220 ]
  %272 = phi <4 x i64> [ %204, %174 ], [ %235, %220 ]
  %273 = phi <4 x i64> [ %207, %174 ], [ %232, %220 ]
  %274 = phi <4 x i64> [ %210, %174 ], [ %229, %220 ]
  %275 = phi <4 x i64> [ %213, %174 ], [ %226, %220 ]
  %276 = phi <4 x i64> [ %180, %174 ], [ %259, %220 ]
  %277 = phi <4 x i64> [ %183, %174 ], [ %256, %220 ]
  %278 = phi <4 x i64> [ %186, %174 ], [ %253, %220 ]
  %279 = phi <4 x i64> [ %216, %174 ], [ %223, %220 ]
  %280 = phi <4 x i64> [ %219, %174 ], [ %173, %220 ]
  %281 = bitcast <4 x i64> %275 to <16 x i16>
  %282 = bitcast <4 x i64> %274 to <16 x i16>
  %283 = bitcast <4 x i64> %273 to <16 x i16>
  %284 = bitcast <4 x i64> %272 to <16 x i16>
  %285 = bitcast <4 x i64> %271 to <16 x i16>
  %286 = bitcast <4 x i64> %270 to <16 x i16>
  %287 = bitcast <4 x i64> %269 to <16 x i16>
  %288 = bitcast <4 x i64> %268 to <16 x i16>
  %289 = bitcast <4 x i64> %267 to <16 x i16>
  %290 = bitcast <4 x i64> %276 to <16 x i16>
  %291 = bitcast <4 x i64> %277 to <16 x i16>
  %292 = bitcast <4 x i64> %278 to <16 x i16>
  %293 = bitcast <4 x i64> %279 to <16 x i16>
  %294 = bitcast <4 x i64> %280 to <16 x i16>
  %295 = load i8, i8* %10, align 1
  %296 = sext i8 %295 to i32
  %297 = icmp slt i8 %295, 0
  br i1 %297, label %298, label %344

298:                                              ; preds = %266
  %299 = sub nsw i32 0, %296
  %300 = xor i32 %296, -1
  %301 = shl i32 1, %300
  %302 = trunc i32 %301 to i16
  %303 = insertelement <16 x i16> undef, i16 %302, i32 0
  %304 = shufflevector <16 x i16> %303, <16 x i16> undef, <16 x i32> zeroinitializer
  %305 = load <16 x i16>, <16 x i16>* %118, align 32
  %306 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %305, <16 x i16> %304) #8
  %307 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %306, i32 %299) #8
  store <16 x i16> %307, <16 x i16>* %118, align 32
  %308 = load <16 x i16>, <16 x i16>* %119, align 32
  %309 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %308, <16 x i16> %304) #8
  %310 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %309, i32 %299) #8
  store <16 x i16> %310, <16 x i16>* %119, align 32
  %311 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %281, <16 x i16> %304) #8
  %312 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %311, i32 %299) #8
  store <16 x i16> %312, <16 x i16>* %120, align 32
  %313 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %282, <16 x i16> %304) #8
  %314 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %313, i32 %299) #8
  store <16 x i16> %314, <16 x i16>* %121, align 32
  %315 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %283, <16 x i16> %304) #8
  %316 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %315, i32 %299) #8
  store <16 x i16> %316, <16 x i16>* %122, align 32
  %317 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %284, <16 x i16> %304) #8
  %318 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %317, i32 %299) #8
  store <16 x i16> %318, <16 x i16>* %123, align 32
  %319 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %285, <16 x i16> %304) #8
  %320 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %319, i32 %299) #8
  store <16 x i16> %320, <16 x i16>* %124, align 32
  %321 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %286, <16 x i16> %304) #8
  %322 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %321, i32 %299) #8
  store <16 x i16> %322, <16 x i16>* %125, align 32
  %323 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %287, <16 x i16> %304) #8
  %324 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %323, i32 %299) #8
  store <16 x i16> %324, <16 x i16>* %126, align 32
  %325 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %288, <16 x i16> %304) #8
  %326 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %325, i32 %299) #8
  store <16 x i16> %326, <16 x i16>* %127, align 32
  %327 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %289, <16 x i16> %304) #8
  %328 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %327, i32 %299) #8
  store <16 x i16> %328, <16 x i16>* %128, align 32
  %329 = load <16 x i16>, <16 x i16>* %129, align 32
  %330 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %329, <16 x i16> %304) #8
  %331 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %330, i32 %299) #8
  store <16 x i16> %331, <16 x i16>* %129, align 32
  %332 = load <16 x i16>, <16 x i16>* %130, align 32
  %333 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %332, <16 x i16> %304) #8
  %334 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %333, i32 %299) #8
  store <16 x i16> %334, <16 x i16>* %130, align 32
  %335 = load <16 x i16>, <16 x i16>* %131, align 32
  %336 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %335, <16 x i16> %304) #8
  %337 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %336, i32 %299) #8
  store <16 x i16> %337, <16 x i16>* %131, align 32
  %338 = load <16 x i16>, <16 x i16>* %132, align 32
  %339 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %338, <16 x i16> %304) #8
  %340 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %339, i32 %299) #8
  store <16 x i16> %340, <16 x i16>* %132, align 32
  %341 = load <16 x i16>, <16 x i16>* %133, align 32
  %342 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %341, <16 x i16> %304) #8
  %343 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %342, i32 %299) #8
  store <16 x i16> %343, <16 x i16>* %133, align 32
  br label %365

344:                                              ; preds = %266
  %345 = icmp eq i8 %295, 0
  br i1 %345, label %365, label %346

346:                                              ; preds = %344
  %347 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %294, i32 %296) #8
  store <16 x i16> %347, <16 x i16>* %102, align 32
  %348 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %293, i32 %296) #8
  store <16 x i16> %348, <16 x i16>* %103, align 32
  %349 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %281, i32 %296) #8
  store <16 x i16> %349, <16 x i16>* %104, align 32
  %350 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %282, i32 %296) #8
  store <16 x i16> %350, <16 x i16>* %105, align 32
  %351 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %283, i32 %296) #8
  store <16 x i16> %351, <16 x i16>* %106, align 32
  %352 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %284, i32 %296) #8
  store <16 x i16> %352, <16 x i16>* %107, align 32
  %353 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %285, i32 %296) #8
  store <16 x i16> %353, <16 x i16>* %108, align 32
  %354 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %286, i32 %296) #8
  store <16 x i16> %354, <16 x i16>* %109, align 32
  %355 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %287, i32 %296) #8
  store <16 x i16> %355, <16 x i16>* %110, align 32
  %356 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %288, i32 %296) #8
  store <16 x i16> %356, <16 x i16>* %111, align 32
  %357 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %289, i32 %296) #8
  store <16 x i16> %357, <16 x i16>* %112, align 32
  %358 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %292, i32 %296) #8
  store <16 x i16> %358, <16 x i16>* %113, align 32
  %359 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %291, i32 %296) #8
  store <16 x i16> %359, <16 x i16>* %114, align 32
  %360 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %290, i32 %296) #8
  store <16 x i16> %360, <16 x i16>* %115, align 32
  %361 = load <16 x i16>, <16 x i16>* %116, align 32
  %362 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %361, i32 %296) #8
  store <16 x i16> %362, <16 x i16>* %116, align 32
  %363 = load <16 x i16>, <16 x i16>* %117, align 32
  %364 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %363, i32 %296) #8
  store <16 x i16> %364, <16 x i16>* %117, align 32
  br label %365

365:                                              ; preds = %346, %298, %344
  call void %15(<4 x i64>* nonnull %24, <4 x i64>* nonnull %24, i8 signext %11) #8
  %366 = load i8, i8* %26, align 1
  %367 = sext i8 %366 to i32
  %368 = icmp slt i8 %366, 0
  br i1 %368, label %369, label %496

369:                                              ; preds = %365
  %370 = sub nsw i32 0, %367
  %371 = xor i32 %367, -1
  %372 = shl i32 1, %371
  %373 = trunc i32 %372 to i16
  %374 = insertelement <16 x i16> undef, i16 %373, i32 0
  %375 = shufflevector <16 x i16> %374, <16 x i16> undef, <16 x i32> zeroinitializer
  %376 = load <16 x i16>, <16 x i16>* %150, align 32
  %377 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %376, <16 x i16> %375) #8
  %378 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %377, i32 %370) #8
  store <16 x i16> %378, <16 x i16>* %150, align 32
  %379 = load <16 x i16>, <16 x i16>* %151, align 32
  %380 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %379, <16 x i16> %375) #8
  %381 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %380, i32 %370) #8
  store <16 x i16> %381, <16 x i16>* %151, align 32
  %382 = load <16 x i16>, <16 x i16>* %152, align 32
  %383 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %382, <16 x i16> %375) #8
  %384 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %383, i32 %370) #8
  store <16 x i16> %384, <16 x i16>* %152, align 32
  %385 = load <16 x i16>, <16 x i16>* %153, align 32
  %386 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %385, <16 x i16> %375) #8
  %387 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %386, i32 %370) #8
  store <16 x i16> %387, <16 x i16>* %153, align 32
  %388 = load <16 x i16>, <16 x i16>* %154, align 32
  %389 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %388, <16 x i16> %375) #8
  %390 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %389, i32 %370) #8
  store <16 x i16> %390, <16 x i16>* %154, align 32
  %391 = load <16 x i16>, <16 x i16>* %155, align 32
  %392 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %391, <16 x i16> %375) #8
  %393 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %392, i32 %370) #8
  store <16 x i16> %393, <16 x i16>* %155, align 32
  %394 = load <16 x i16>, <16 x i16>* %156, align 32
  %395 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %394, <16 x i16> %375) #8
  %396 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %395, i32 %370) #8
  store <16 x i16> %396, <16 x i16>* %156, align 32
  %397 = load <16 x i16>, <16 x i16>* %157, align 32
  %398 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %397, <16 x i16> %375) #8
  %399 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %398, i32 %370) #8
  store <16 x i16> %399, <16 x i16>* %157, align 32
  %400 = load <16 x i16>, <16 x i16>* %158, align 32
  %401 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %400, <16 x i16> %375) #8
  %402 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %401, i32 %370) #8
  store <16 x i16> %402, <16 x i16>* %158, align 32
  %403 = load <16 x i16>, <16 x i16>* %159, align 32
  %404 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %403, <16 x i16> %375) #8
  %405 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %404, i32 %370) #8
  store <16 x i16> %405, <16 x i16>* %159, align 32
  %406 = load <16 x i16>, <16 x i16>* %160, align 32
  %407 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %406, <16 x i16> %375) #8
  %408 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %407, i32 %370) #8
  store <16 x i16> %408, <16 x i16>* %160, align 32
  %409 = load <16 x i16>, <16 x i16>* %161, align 32
  %410 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %409, <16 x i16> %375) #8
  %411 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %410, i32 %370) #8
  store <16 x i16> %411, <16 x i16>* %161, align 32
  %412 = load <16 x i16>, <16 x i16>* %162, align 32
  %413 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %412, <16 x i16> %375) #8
  %414 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %413, i32 %370) #8
  store <16 x i16> %414, <16 x i16>* %162, align 32
  %415 = load <16 x i16>, <16 x i16>* %163, align 32
  %416 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %415, <16 x i16> %375) #8
  %417 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %416, i32 %370) #8
  store <16 x i16> %417, <16 x i16>* %163, align 32
  %418 = load <16 x i16>, <16 x i16>* %164, align 32
  %419 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %418, <16 x i16> %375) #8
  %420 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %419, i32 %370) #8
  store <16 x i16> %420, <16 x i16>* %164, align 32
  %421 = load <16 x i16>, <16 x i16>* %165, align 32
  %422 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %421, <16 x i16> %375) #8
  %423 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %422, i32 %370) #8
  store <16 x i16> %423, <16 x i16>* %165, align 32
  %424 = bitcast <16 x i16> %378 to <2 x i128>
  %425 = extractelement <2 x i128> %424, i32 0
  %426 = bitcast i128 %425 to <2 x i64>
  %427 = bitcast <16 x i16> %402 to <2 x i128>
  %428 = extractelement <2 x i128> %427, i32 0
  %429 = bitcast i128 %428 to <2 x i64>
  %430 = bitcast <16 x i16> %381 to <2 x i128>
  %431 = extractelement <2 x i128> %430, i32 0
  %432 = bitcast i128 %431 to <2 x i64>
  %433 = bitcast <16 x i16> %405 to <2 x i128>
  %434 = extractelement <2 x i128> %433, i32 0
  %435 = bitcast i128 %434 to <2 x i64>
  %436 = bitcast <16 x i16> %384 to <2 x i128>
  %437 = extractelement <2 x i128> %436, i32 0
  %438 = bitcast i128 %437 to <2 x i64>
  %439 = bitcast <16 x i16> %408 to <2 x i128>
  %440 = extractelement <2 x i128> %439, i32 0
  %441 = bitcast i128 %440 to <2 x i64>
  %442 = bitcast <16 x i16> %387 to <2 x i128>
  %443 = extractelement <2 x i128> %442, i32 0
  %444 = bitcast i128 %443 to <2 x i64>
  %445 = bitcast <16 x i16> %411 to <2 x i128>
  %446 = extractelement <2 x i128> %445, i32 0
  %447 = bitcast i128 %446 to <2 x i64>
  %448 = bitcast <16 x i16> %390 to <2 x i128>
  %449 = extractelement <2 x i128> %448, i32 0
  %450 = bitcast i128 %449 to <2 x i64>
  %451 = bitcast <16 x i16> %414 to <2 x i128>
  %452 = extractelement <2 x i128> %451, i32 0
  %453 = bitcast i128 %452 to <2 x i64>
  %454 = bitcast <16 x i16> %393 to <2 x i128>
  %455 = extractelement <2 x i128> %454, i32 0
  %456 = bitcast i128 %455 to <2 x i64>
  %457 = bitcast <16 x i16> %417 to <2 x i128>
  %458 = extractelement <2 x i128> %457, i32 0
  %459 = bitcast i128 %458 to <2 x i64>
  %460 = bitcast <16 x i16> %396 to <2 x i128>
  %461 = extractelement <2 x i128> %460, i32 0
  %462 = bitcast i128 %461 to <2 x i64>
  %463 = bitcast <16 x i16> %420 to <2 x i128>
  %464 = extractelement <2 x i128> %463, i32 0
  %465 = bitcast i128 %464 to <2 x i64>
  %466 = bitcast <16 x i16> %399 to <2 x i128>
  %467 = extractelement <2 x i128> %466, i32 0
  %468 = bitcast i128 %467 to <2 x i64>
  %469 = bitcast <16 x i16> %423 to <2 x i128>
  %470 = extractelement <2 x i128> %469, i32 0
  %471 = bitcast i128 %470 to <2 x i64>
  %472 = bitcast <16 x i16> %402 to <2 x i128>
  %473 = extractelement <2 x i128> %472, i32 1
  %474 = bitcast i128 %473 to <2 x i64>
  %475 = bitcast <16 x i16> %405 to <2 x i128>
  %476 = extractelement <2 x i128> %475, i32 1
  %477 = bitcast i128 %476 to <2 x i64>
  %478 = bitcast <16 x i16> %408 to <2 x i128>
  %479 = extractelement <2 x i128> %478, i32 1
  %480 = bitcast i128 %479 to <2 x i64>
  %481 = bitcast <16 x i16> %411 to <2 x i128>
  %482 = extractelement <2 x i128> %481, i32 1
  %483 = bitcast i128 %482 to <2 x i64>
  %484 = bitcast <16 x i16> %414 to <2 x i128>
  %485 = extractelement <2 x i128> %484, i32 1
  %486 = bitcast i128 %485 to <2 x i64>
  %487 = bitcast <16 x i16> %417 to <2 x i128>
  %488 = extractelement <2 x i128> %487, i32 1
  %489 = bitcast i128 %488 to <2 x i64>
  %490 = bitcast <16 x i16> %420 to <2 x i128>
  %491 = extractelement <2 x i128> %490, i32 1
  %492 = bitcast i128 %491 to <2 x i64>
  %493 = bitcast <16 x i16> %423 to <2 x i128>
  %494 = extractelement <2 x i128> %493, i32 1
  %495 = bitcast i128 %494 to <2 x i64>
  br label %628

496:                                              ; preds = %365
  %497 = icmp eq i8 %366, 0
  br i1 %497, label %498, label %523

498:                                              ; preds = %496
  %499 = load <2 x i64>, <2 x i64>* %27, align 32
  %500 = load <2 x i64>, <2 x i64>* %29, align 32
  %501 = load <2 x i64>, <2 x i64>* %31, align 32
  %502 = load <2 x i64>, <2 x i64>* %33, align 32
  %503 = load <2 x i64>, <2 x i64>* %35, align 32
  %504 = load <2 x i64>, <2 x i64>* %37, align 32
  %505 = load <2 x i64>, <2 x i64>* %39, align 32
  %506 = load <2 x i64>, <2 x i64>* %41, align 32
  %507 = load <2 x i64>, <2 x i64>* %43, align 32
  %508 = load <2 x i64>, <2 x i64>* %45, align 32
  %509 = load <2 x i64>, <2 x i64>* %47, align 32
  %510 = load <2 x i64>, <2 x i64>* %49, align 32
  %511 = load <2 x i64>, <2 x i64>* %51, align 32
  %512 = load <2 x i64>, <2 x i64>* %53, align 32
  %513 = load <2 x i64>, <2 x i64>* %55, align 32
  %514 = load <2 x i64>, <2 x i64>* %57, align 32
  %515 = load <2 x i64>, <2 x i64>* %59, align 16
  %516 = load <2 x i64>, <2 x i64>* %61, align 16
  %517 = load <2 x i64>, <2 x i64>* %63, align 16
  %518 = load <2 x i64>, <2 x i64>* %65, align 16
  %519 = load <2 x i64>, <2 x i64>* %67, align 16
  %520 = load <2 x i64>, <2 x i64>* %69, align 16
  %521 = load <2 x i64>, <2 x i64>* %71, align 16
  %522 = load <2 x i64>, <2 x i64>* %73, align 16
  br label %628

523:                                              ; preds = %496
  %524 = load <16 x i16>, <16 x i16>* %134, align 32
  %525 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %524, i32 %367) #8
  store <16 x i16> %525, <16 x i16>* %134, align 32
  %526 = load <16 x i16>, <16 x i16>* %135, align 32
  %527 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %526, i32 %367) #8
  store <16 x i16> %527, <16 x i16>* %135, align 32
  %528 = load <16 x i16>, <16 x i16>* %136, align 32
  %529 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %528, i32 %367) #8
  store <16 x i16> %529, <16 x i16>* %136, align 32
  %530 = load <16 x i16>, <16 x i16>* %137, align 32
  %531 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %530, i32 %367) #8
  store <16 x i16> %531, <16 x i16>* %137, align 32
  %532 = load <16 x i16>, <16 x i16>* %138, align 32
  %533 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %532, i32 %367) #8
  store <16 x i16> %533, <16 x i16>* %138, align 32
  %534 = load <16 x i16>, <16 x i16>* %139, align 32
  %535 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %534, i32 %367) #8
  store <16 x i16> %535, <16 x i16>* %139, align 32
  %536 = load <16 x i16>, <16 x i16>* %140, align 32
  %537 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %536, i32 %367) #8
  store <16 x i16> %537, <16 x i16>* %140, align 32
  %538 = load <16 x i16>, <16 x i16>* %141, align 32
  %539 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %538, i32 %367) #8
  store <16 x i16> %539, <16 x i16>* %141, align 32
  %540 = load <16 x i16>, <16 x i16>* %142, align 32
  %541 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %540, i32 %367) #8
  store <16 x i16> %541, <16 x i16>* %142, align 32
  %542 = load <16 x i16>, <16 x i16>* %143, align 32
  %543 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %542, i32 %367) #8
  store <16 x i16> %543, <16 x i16>* %143, align 32
  %544 = load <16 x i16>, <16 x i16>* %144, align 32
  %545 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %544, i32 %367) #8
  store <16 x i16> %545, <16 x i16>* %144, align 32
  %546 = load <16 x i16>, <16 x i16>* %145, align 32
  %547 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %546, i32 %367) #8
  store <16 x i16> %547, <16 x i16>* %145, align 32
  %548 = load <16 x i16>, <16 x i16>* %146, align 32
  %549 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %548, i32 %367) #8
  store <16 x i16> %549, <16 x i16>* %146, align 32
  %550 = load <16 x i16>, <16 x i16>* %147, align 32
  %551 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %550, i32 %367) #8
  store <16 x i16> %551, <16 x i16>* %147, align 32
  %552 = load <16 x i16>, <16 x i16>* %148, align 32
  %553 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %552, i32 %367) #8
  store <16 x i16> %553, <16 x i16>* %148, align 32
  %554 = load <16 x i16>, <16 x i16>* %149, align 32
  %555 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %554, i32 %367) #8
  store <16 x i16> %555, <16 x i16>* %149, align 32
  %556 = bitcast <16 x i16> %525 to <2 x i128>
  %557 = extractelement <2 x i128> %556, i32 0
  %558 = bitcast i128 %557 to <2 x i64>
  %559 = bitcast <16 x i16> %541 to <2 x i128>
  %560 = extractelement <2 x i128> %559, i32 0
  %561 = bitcast i128 %560 to <2 x i64>
  %562 = bitcast <16 x i16> %527 to <2 x i128>
  %563 = extractelement <2 x i128> %562, i32 0
  %564 = bitcast i128 %563 to <2 x i64>
  %565 = bitcast <16 x i16> %543 to <2 x i128>
  %566 = extractelement <2 x i128> %565, i32 0
  %567 = bitcast i128 %566 to <2 x i64>
  %568 = bitcast <16 x i16> %529 to <2 x i128>
  %569 = extractelement <2 x i128> %568, i32 0
  %570 = bitcast i128 %569 to <2 x i64>
  %571 = bitcast <16 x i16> %545 to <2 x i128>
  %572 = extractelement <2 x i128> %571, i32 0
  %573 = bitcast i128 %572 to <2 x i64>
  %574 = bitcast <16 x i16> %531 to <2 x i128>
  %575 = extractelement <2 x i128> %574, i32 0
  %576 = bitcast i128 %575 to <2 x i64>
  %577 = bitcast <16 x i16> %547 to <2 x i128>
  %578 = extractelement <2 x i128> %577, i32 0
  %579 = bitcast i128 %578 to <2 x i64>
  %580 = bitcast <16 x i16> %533 to <2 x i128>
  %581 = extractelement <2 x i128> %580, i32 0
  %582 = bitcast i128 %581 to <2 x i64>
  %583 = bitcast <16 x i16> %549 to <2 x i128>
  %584 = extractelement <2 x i128> %583, i32 0
  %585 = bitcast i128 %584 to <2 x i64>
  %586 = bitcast <16 x i16> %535 to <2 x i128>
  %587 = extractelement <2 x i128> %586, i32 0
  %588 = bitcast i128 %587 to <2 x i64>
  %589 = bitcast <16 x i16> %551 to <2 x i128>
  %590 = extractelement <2 x i128> %589, i32 0
  %591 = bitcast i128 %590 to <2 x i64>
  %592 = bitcast <16 x i16> %537 to <2 x i128>
  %593 = extractelement <2 x i128> %592, i32 0
  %594 = bitcast i128 %593 to <2 x i64>
  %595 = bitcast <16 x i16> %553 to <2 x i128>
  %596 = extractelement <2 x i128> %595, i32 0
  %597 = bitcast i128 %596 to <2 x i64>
  %598 = bitcast <16 x i16> %539 to <2 x i128>
  %599 = extractelement <2 x i128> %598, i32 0
  %600 = bitcast i128 %599 to <2 x i64>
  %601 = bitcast <16 x i16> %555 to <2 x i128>
  %602 = extractelement <2 x i128> %601, i32 0
  %603 = bitcast i128 %602 to <2 x i64>
  %604 = bitcast <16 x i16> %541 to <2 x i128>
  %605 = extractelement <2 x i128> %604, i32 1
  %606 = bitcast i128 %605 to <2 x i64>
  %607 = bitcast <16 x i16> %543 to <2 x i128>
  %608 = extractelement <2 x i128> %607, i32 1
  %609 = bitcast i128 %608 to <2 x i64>
  %610 = bitcast <16 x i16> %545 to <2 x i128>
  %611 = extractelement <2 x i128> %610, i32 1
  %612 = bitcast i128 %611 to <2 x i64>
  %613 = bitcast <16 x i16> %547 to <2 x i128>
  %614 = extractelement <2 x i128> %613, i32 1
  %615 = bitcast i128 %614 to <2 x i64>
  %616 = bitcast <16 x i16> %549 to <2 x i128>
  %617 = extractelement <2 x i128> %616, i32 1
  %618 = bitcast i128 %617 to <2 x i64>
  %619 = bitcast <16 x i16> %551 to <2 x i128>
  %620 = extractelement <2 x i128> %619, i32 1
  %621 = bitcast i128 %620 to <2 x i64>
  %622 = bitcast <16 x i16> %553 to <2 x i128>
  %623 = extractelement <2 x i128> %622, i32 1
  %624 = bitcast i128 %623 to <2 x i64>
  %625 = bitcast <16 x i16> %555 to <2 x i128>
  %626 = extractelement <2 x i128> %625, i32 1
  %627 = bitcast i128 %626 to <2 x i64>
  br label %628

628:                                              ; preds = %498, %523, %369
  %629 = phi <2 x i64> [ %522, %498 ], [ %627, %523 ], [ %495, %369 ]
  %630 = phi <2 x i64> [ %521, %498 ], [ %624, %523 ], [ %492, %369 ]
  %631 = phi <2 x i64> [ %520, %498 ], [ %621, %523 ], [ %489, %369 ]
  %632 = phi <2 x i64> [ %519, %498 ], [ %618, %523 ], [ %486, %369 ]
  %633 = phi <2 x i64> [ %518, %498 ], [ %615, %523 ], [ %483, %369 ]
  %634 = phi <2 x i64> [ %517, %498 ], [ %612, %523 ], [ %480, %369 ]
  %635 = phi <2 x i64> [ %516, %498 ], [ %609, %523 ], [ %477, %369 ]
  %636 = phi <2 x i64> [ %515, %498 ], [ %606, %523 ], [ %474, %369 ]
  %637 = phi <2 x i64> [ %514, %498 ], [ %603, %523 ], [ %471, %369 ]
  %638 = phi <2 x i64> [ %513, %498 ], [ %600, %523 ], [ %468, %369 ]
  %639 = phi <2 x i64> [ %512, %498 ], [ %597, %523 ], [ %465, %369 ]
  %640 = phi <2 x i64> [ %511, %498 ], [ %594, %523 ], [ %462, %369 ]
  %641 = phi <2 x i64> [ %510, %498 ], [ %591, %523 ], [ %459, %369 ]
  %642 = phi <2 x i64> [ %509, %498 ], [ %588, %523 ], [ %456, %369 ]
  %643 = phi <2 x i64> [ %508, %498 ], [ %585, %523 ], [ %453, %369 ]
  %644 = phi <2 x i64> [ %507, %498 ], [ %582, %523 ], [ %450, %369 ]
  %645 = phi <2 x i64> [ %506, %498 ], [ %579, %523 ], [ %447, %369 ]
  %646 = phi <2 x i64> [ %505, %498 ], [ %576, %523 ], [ %444, %369 ]
  %647 = phi <2 x i64> [ %504, %498 ], [ %573, %523 ], [ %441, %369 ]
  %648 = phi <2 x i64> [ %503, %498 ], [ %570, %523 ], [ %438, %369 ]
  %649 = phi <2 x i64> [ %502, %498 ], [ %567, %523 ], [ %435, %369 ]
  %650 = phi <2 x i64> [ %501, %498 ], [ %564, %523 ], [ %432, %369 ]
  %651 = phi <2 x i64> [ %500, %498 ], [ %561, %523 ], [ %429, %369 ]
  %652 = phi <2 x i64> [ %499, %498 ], [ %558, %523 ], [ %426, %369 ]
  %653 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %170
  %654 = shufflevector <2 x i64> %652, <2 x i64> %651, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %655 = shufflevector <2 x i64> %650, <2 x i64> %649, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %656 = shufflevector <2 x i64> %648, <2 x i64> %647, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %657 = shufflevector <2 x i64> %646, <2 x i64> %645, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %658 = shufflevector <2 x i64> %644, <2 x i64> %643, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %659 = shufflevector <2 x i64> %642, <2 x i64> %641, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %660 = shufflevector <2 x i64> %640, <2 x i64> %639, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %661 = shufflevector <2 x i64> %638, <2 x i64> %637, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %662 = load <2 x i64>, <2 x i64>* %58, align 16
  %663 = shufflevector <2 x i64> %662, <2 x i64> %636, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %664 = load <2 x i64>, <2 x i64>* %60, align 16
  %665 = shufflevector <2 x i64> %664, <2 x i64> %635, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %666 = load <2 x i64>, <2 x i64>* %62, align 16
  %667 = shufflevector <2 x i64> %666, <2 x i64> %634, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %668 = load <2 x i64>, <2 x i64>* %64, align 16
  %669 = shufflevector <2 x i64> %668, <2 x i64> %633, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %670 = load <2 x i64>, <2 x i64>* %66, align 16
  %671 = shufflevector <2 x i64> %670, <2 x i64> %632, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %672 = load <2 x i64>, <2 x i64>* %68, align 16
  %673 = shufflevector <2 x i64> %672, <2 x i64> %631, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %674 = load <2 x i64>, <2 x i64>* %70, align 16
  %675 = shufflevector <2 x i64> %674, <2 x i64> %630, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %676 = load <2 x i64>, <2 x i64>* %72, align 16
  %677 = shufflevector <2 x i64> %676, <2 x i64> %629, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %678 = bitcast <4 x i64> %654 to <16 x i16>
  %679 = bitcast <4 x i64> %655 to <16 x i16>
  %680 = shufflevector <16 x i16> %678, <16 x i16> %679, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %681 = shufflevector <16 x i16> %678, <16 x i16> %679, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %682 = bitcast <4 x i64> %656 to <16 x i16>
  %683 = bitcast <4 x i64> %657 to <16 x i16>
  %684 = shufflevector <16 x i16> %682, <16 x i16> %683, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %685 = shufflevector <16 x i16> %682, <16 x i16> %683, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %686 = bitcast <4 x i64> %658 to <16 x i16>
  %687 = bitcast <4 x i64> %659 to <16 x i16>
  %688 = shufflevector <16 x i16> %686, <16 x i16> %687, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %689 = shufflevector <16 x i16> %686, <16 x i16> %687, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %690 = bitcast <4 x i64> %660 to <16 x i16>
  %691 = bitcast <4 x i64> %661 to <16 x i16>
  %692 = shufflevector <16 x i16> %690, <16 x i16> %691, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %693 = shufflevector <16 x i16> %690, <16 x i16> %691, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %694 = bitcast <16 x i16> %680 to <8 x i32>
  %695 = bitcast <16 x i16> %684 to <8 x i32>
  %696 = shufflevector <8 x i32> %694, <8 x i32> %695, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %697 = shufflevector <8 x i32> %694, <8 x i32> %695, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %698 = bitcast <16 x i16> %688 to <8 x i32>
  %699 = bitcast <16 x i16> %692 to <8 x i32>
  %700 = shufflevector <8 x i32> %698, <8 x i32> %699, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %701 = shufflevector <8 x i32> %698, <8 x i32> %699, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %702 = bitcast <16 x i16> %681 to <8 x i32>
  %703 = bitcast <16 x i16> %685 to <8 x i32>
  %704 = shufflevector <8 x i32> %702, <8 x i32> %703, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %705 = shufflevector <8 x i32> %702, <8 x i32> %703, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %706 = bitcast <16 x i16> %689 to <8 x i32>
  %707 = bitcast <16 x i16> %693 to <8 x i32>
  %708 = shufflevector <8 x i32> %706, <8 x i32> %707, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %709 = shufflevector <8 x i32> %706, <8 x i32> %707, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %710 = bitcast <8 x i32> %696 to <4 x i64>
  %711 = bitcast <8 x i32> %700 to <4 x i64>
  %712 = shufflevector <4 x i64> %710, <4 x i64> %711, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %712, <4 x i64>* %653, align 32
  %713 = shufflevector <4 x i64> %710, <4 x i64> %711, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %714 = getelementptr inbounds <4 x i64>, <4 x i64>* %653, i64 1
  store <4 x i64> %713, <4 x i64>* %714, align 32
  %715 = bitcast <8 x i32> %704 to <4 x i64>
  %716 = bitcast <8 x i32> %708 to <4 x i64>
  %717 = shufflevector <4 x i64> %715, <4 x i64> %716, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %718 = getelementptr inbounds <4 x i64>, <4 x i64>* %653, i64 4
  store <4 x i64> %717, <4 x i64>* %718, align 32
  %719 = shufflevector <4 x i64> %715, <4 x i64> %716, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %720 = getelementptr inbounds <4 x i64>, <4 x i64>* %653, i64 5
  store <4 x i64> %719, <4 x i64>* %720, align 32
  %721 = bitcast <8 x i32> %697 to <4 x i64>
  %722 = bitcast <8 x i32> %701 to <4 x i64>
  %723 = shufflevector <4 x i64> %721, <4 x i64> %722, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %724 = getelementptr inbounds <4 x i64>, <4 x i64>* %653, i64 2
  store <4 x i64> %723, <4 x i64>* %724, align 32
  %725 = shufflevector <4 x i64> %721, <4 x i64> %722, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %726 = getelementptr inbounds <4 x i64>, <4 x i64>* %653, i64 3
  store <4 x i64> %725, <4 x i64>* %726, align 32
  %727 = bitcast <8 x i32> %705 to <4 x i64>
  %728 = bitcast <8 x i32> %709 to <4 x i64>
  %729 = shufflevector <4 x i64> %727, <4 x i64> %728, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %730 = getelementptr inbounds <4 x i64>, <4 x i64>* %653, i64 6
  store <4 x i64> %729, <4 x i64>* %730, align 32
  %731 = shufflevector <4 x i64> %727, <4 x i64> %728, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %732 = getelementptr inbounds <4 x i64>, <4 x i64>* %653, i64 7
  store <4 x i64> %731, <4 x i64>* %732, align 32
  %733 = getelementptr inbounds <4 x i64>, <4 x i64>* %653, i64 8
  %734 = bitcast <4 x i64> %663 to <16 x i16>
  %735 = bitcast <4 x i64> %665 to <16 x i16>
  %736 = shufflevector <16 x i16> %734, <16 x i16> %735, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %737 = shufflevector <16 x i16> %734, <16 x i16> %735, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %738 = bitcast <4 x i64> %667 to <16 x i16>
  %739 = bitcast <4 x i64> %669 to <16 x i16>
  %740 = shufflevector <16 x i16> %738, <16 x i16> %739, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %741 = shufflevector <16 x i16> %738, <16 x i16> %739, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %742 = bitcast <4 x i64> %671 to <16 x i16>
  %743 = bitcast <4 x i64> %673 to <16 x i16>
  %744 = shufflevector <16 x i16> %742, <16 x i16> %743, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %745 = shufflevector <16 x i16> %742, <16 x i16> %743, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %746 = bitcast <4 x i64> %675 to <16 x i16>
  %747 = bitcast <4 x i64> %677 to <16 x i16>
  %748 = shufflevector <16 x i16> %746, <16 x i16> %747, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %749 = shufflevector <16 x i16> %746, <16 x i16> %747, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %750 = bitcast <16 x i16> %736 to <8 x i32>
  %751 = bitcast <16 x i16> %740 to <8 x i32>
  %752 = shufflevector <8 x i32> %750, <8 x i32> %751, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %753 = shufflevector <8 x i32> %750, <8 x i32> %751, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %754 = bitcast <16 x i16> %744 to <8 x i32>
  %755 = bitcast <16 x i16> %748 to <8 x i32>
  %756 = shufflevector <8 x i32> %754, <8 x i32> %755, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %757 = shufflevector <8 x i32> %754, <8 x i32> %755, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %758 = bitcast <16 x i16> %737 to <8 x i32>
  %759 = bitcast <16 x i16> %741 to <8 x i32>
  %760 = shufflevector <8 x i32> %758, <8 x i32> %759, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %761 = shufflevector <8 x i32> %758, <8 x i32> %759, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %762 = bitcast <16 x i16> %745 to <8 x i32>
  %763 = bitcast <16 x i16> %749 to <8 x i32>
  %764 = shufflevector <8 x i32> %762, <8 x i32> %763, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %765 = shufflevector <8 x i32> %762, <8 x i32> %763, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %766 = bitcast <8 x i32> %752 to <4 x i64>
  %767 = bitcast <8 x i32> %756 to <4 x i64>
  %768 = shufflevector <4 x i64> %766, <4 x i64> %767, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %768, <4 x i64>* %733, align 32
  %769 = shufflevector <4 x i64> %766, <4 x i64> %767, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %770 = getelementptr inbounds <4 x i64>, <4 x i64>* %653, i64 9
  store <4 x i64> %769, <4 x i64>* %770, align 32
  %771 = bitcast <8 x i32> %760 to <4 x i64>
  %772 = bitcast <8 x i32> %764 to <4 x i64>
  %773 = shufflevector <4 x i64> %771, <4 x i64> %772, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %774 = getelementptr inbounds <4 x i64>, <4 x i64>* %653, i64 12
  store <4 x i64> %773, <4 x i64>* %774, align 32
  %775 = shufflevector <4 x i64> %771, <4 x i64> %772, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %776 = getelementptr inbounds <4 x i64>, <4 x i64>* %653, i64 13
  store <4 x i64> %775, <4 x i64>* %776, align 32
  %777 = bitcast <8 x i32> %753 to <4 x i64>
  %778 = bitcast <8 x i32> %757 to <4 x i64>
  %779 = shufflevector <4 x i64> %777, <4 x i64> %778, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %780 = getelementptr inbounds <4 x i64>, <4 x i64>* %653, i64 10
  store <4 x i64> %779, <4 x i64>* %780, align 32
  %781 = shufflevector <4 x i64> %777, <4 x i64> %778, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %782 = getelementptr inbounds <4 x i64>, <4 x i64>* %653, i64 11
  store <4 x i64> %781, <4 x i64>* %782, align 32
  %783 = bitcast <8 x i32> %761 to <4 x i64>
  %784 = bitcast <8 x i32> %765 to <4 x i64>
  %785 = shufflevector <4 x i64> %783, <4 x i64> %784, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %786 = getelementptr inbounds <4 x i64>, <4 x i64>* %653, i64 14
  store <4 x i64> %785, <4 x i64>* %786, align 32
  %787 = shufflevector <4 x i64> %783, <4 x i64> %784, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %788 = getelementptr inbounds <4 x i64>, <4 x i64>* %653, i64 15
  store <4 x i64> %787, <4 x i64>* %788, align 32
  %789 = add nuw nsw i64 %169, 1
  %790 = icmp eq i64 %789, 2
  br i1 %790, label %166, label %168

791:                                              ; preds = %166, %791
  %792 = phi i64 [ %806, %791 ], [ 0, %166 ]
  %793 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %792
  %794 = load <4 x i64>, <4 x i64>* %793, align 32
  %795 = shl i64 %792, 32
  %796 = sub nuw nsw i64 133143986176, %795
  %797 = ashr exact i64 %796, 32
  %798 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %797
  store <4 x i64> %794, <4 x i64>* %798, align 32
  %799 = or i64 %792, 1
  %800 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %799
  %801 = load <4 x i64>, <4 x i64>* %800, align 32
  %802 = shl i64 %799, 32
  %803 = sub nuw nsw i64 133143986176, %802
  %804 = ashr exact i64 %803, 32
  %805 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %6, i64 0, i64 %804
  store <4 x i64> %801, <4 x i64>* %805, align 32
  %806 = add nuw nsw i64 %792, 2
  %807 = icmp eq i64 %806, 32
  br i1 %807, label %810, label %791

808:                                              ; preds = %166
  %809 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 0
  br label %810

810:                                              ; preds = %791, %808
  %811 = phi <4 x i64>* [ %809, %808 ], [ %24, %791 ]
  call void %17(<4 x i64>* %811, <4 x i64>* %811, i8 signext %12) #8
  %812 = getelementptr inbounds i8, i8* %10, i64 2
  %813 = load i8, i8* %812, align 1
  %814 = sext i8 %813 to i32
  %815 = icmp slt i8 %813, 0
  br i1 %815, label %816, label %850

816:                                              ; preds = %810
  %817 = sub nsw i32 0, %814
  %818 = xor i32 %814, -1
  %819 = shl i32 1, %818
  %820 = trunc i32 %819 to i16
  %821 = insertelement <16 x i16> undef, i16 %820, i32 0
  %822 = shufflevector <16 x i16> %821, <16 x i16> undef, <16 x i32> zeroinitializer
  br label %823

823:                                              ; preds = %823, %816
  %824 = phi i64 [ 0, %816 ], [ %848, %823 ]
  %825 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 %824
  %826 = bitcast <4 x i64>* %825 to <16 x i16>*
  %827 = load <16 x i16>, <16 x i16>* %826, align 32
  %828 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %827, <16 x i16> %822) #8
  %829 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %828, i32 %817) #8
  store <16 x i16> %829, <16 x i16>* %826, align 32
  %830 = or i64 %824, 1
  %831 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 %830
  %832 = bitcast <4 x i64>* %831 to <16 x i16>*
  %833 = load <16 x i16>, <16 x i16>* %832, align 32
  %834 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %833, <16 x i16> %822) #8
  %835 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %834, i32 %817) #8
  store <16 x i16> %835, <16 x i16>* %832, align 32
  %836 = or i64 %824, 2
  %837 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 %836
  %838 = bitcast <4 x i64>* %837 to <16 x i16>*
  %839 = load <16 x i16>, <16 x i16>* %838, align 32
  %840 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %839, <16 x i16> %822) #8
  %841 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %840, i32 %817) #8
  store <16 x i16> %841, <16 x i16>* %838, align 32
  %842 = or i64 %824, 3
  %843 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 %842
  %844 = bitcast <4 x i64>* %843 to <16 x i16>*
  %845 = load <16 x i16>, <16 x i16>* %844, align 32
  %846 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %845, <16 x i16> %822) #8
  %847 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %846, i32 %817) #8
  store <16 x i16> %847, <16 x i16>* %844, align 32
  %848 = add nuw nsw i64 %824, 4
  %849 = icmp eq i64 %848, 32
  br i1 %849, label %875, label %823

850:                                              ; preds = %810
  %851 = icmp eq i8 %813, 0
  br i1 %851, label %875, label %852

852:                                              ; preds = %850, %852
  %853 = phi i64 [ %873, %852 ], [ 0, %850 ]
  %854 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 %853
  %855 = bitcast <4 x i64>* %854 to <16 x i16>*
  %856 = load <16 x i16>, <16 x i16>* %855, align 32
  %857 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %856, i32 %814) #8
  store <16 x i16> %857, <16 x i16>* %855, align 32
  %858 = or i64 %853, 1
  %859 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 %858
  %860 = bitcast <4 x i64>* %859 to <16 x i16>*
  %861 = load <16 x i16>, <16 x i16>* %860, align 32
  %862 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %861, i32 %814) #8
  store <16 x i16> %862, <16 x i16>* %860, align 32
  %863 = or i64 %853, 2
  %864 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 %863
  %865 = bitcast <4 x i64>* %864 to <16 x i16>*
  %866 = load <16 x i16>, <16 x i16>* %865, align 32
  %867 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %866, i32 %814) #8
  store <16 x i16> %867, <16 x i16>* %865, align 32
  %868 = or i64 %853, 3
  %869 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 %868
  %870 = bitcast <4 x i64>* %869 to <16 x i16>*
  %871 = load <16 x i16>, <16 x i16>* %870, align 32
  %872 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %871, i32 %814) #8
  store <16 x i16> %872, <16 x i16>* %870, align 32
  %873 = add nuw nsw i64 %853, 4
  %874 = icmp eq i64 %873, 32
  br i1 %874, label %875, label %852

875:                                              ; preds = %852, %823, %850
  %876 = bitcast <4 x i64>* %811 to <2 x i64>*
  %877 = load <2 x i64>, <2 x i64>* %876, align 16
  %878 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 8
  %879 = bitcast <4 x i64>* %878 to <2 x i64>*
  %880 = load <2 x i64>, <2 x i64>* %879, align 16
  %881 = shufflevector <2 x i64> %877, <2 x i64> %880, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %882 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 1
  %883 = bitcast <4 x i64>* %882 to <2 x i64>*
  %884 = load <2 x i64>, <2 x i64>* %883, align 16
  %885 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 9
  %886 = bitcast <4 x i64>* %885 to <2 x i64>*
  %887 = load <2 x i64>, <2 x i64>* %886, align 16
  %888 = shufflevector <2 x i64> %884, <2 x i64> %887, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %889 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 2
  %890 = bitcast <4 x i64>* %889 to <2 x i64>*
  %891 = load <2 x i64>, <2 x i64>* %890, align 16
  %892 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 10
  %893 = bitcast <4 x i64>* %892 to <2 x i64>*
  %894 = load <2 x i64>, <2 x i64>* %893, align 16
  %895 = shufflevector <2 x i64> %891, <2 x i64> %894, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %896 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 3
  %897 = bitcast <4 x i64>* %896 to <2 x i64>*
  %898 = load <2 x i64>, <2 x i64>* %897, align 16
  %899 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 11
  %900 = bitcast <4 x i64>* %899 to <2 x i64>*
  %901 = load <2 x i64>, <2 x i64>* %900, align 16
  %902 = shufflevector <2 x i64> %898, <2 x i64> %901, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %903 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 4
  %904 = bitcast <4 x i64>* %903 to <2 x i64>*
  %905 = load <2 x i64>, <2 x i64>* %904, align 16
  %906 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 12
  %907 = bitcast <4 x i64>* %906 to <2 x i64>*
  %908 = load <2 x i64>, <2 x i64>* %907, align 16
  %909 = shufflevector <2 x i64> %905, <2 x i64> %908, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %910 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 5
  %911 = bitcast <4 x i64>* %910 to <2 x i64>*
  %912 = load <2 x i64>, <2 x i64>* %911, align 16
  %913 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 13
  %914 = bitcast <4 x i64>* %913 to <2 x i64>*
  %915 = load <2 x i64>, <2 x i64>* %914, align 16
  %916 = shufflevector <2 x i64> %912, <2 x i64> %915, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %917 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 6
  %918 = bitcast <4 x i64>* %917 to <2 x i64>*
  %919 = load <2 x i64>, <2 x i64>* %918, align 16
  %920 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 14
  %921 = bitcast <4 x i64>* %920 to <2 x i64>*
  %922 = load <2 x i64>, <2 x i64>* %921, align 16
  %923 = shufflevector <2 x i64> %919, <2 x i64> %922, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %924 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 7
  %925 = bitcast <4 x i64>* %924 to <2 x i64>*
  %926 = load <2 x i64>, <2 x i64>* %925, align 16
  %927 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 15
  %928 = bitcast <4 x i64>* %927 to <2 x i64>*
  %929 = load <2 x i64>, <2 x i64>* %928, align 16
  %930 = shufflevector <2 x i64> %926, <2 x i64> %929, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %931 = getelementptr inbounds <2 x i64>, <2 x i64>* %876, i64 1
  %932 = load <2 x i64>, <2 x i64>* %931, align 16
  %933 = getelementptr inbounds <2 x i64>, <2 x i64>* %879, i64 1
  %934 = load <2 x i64>, <2 x i64>* %933, align 16
  %935 = shufflevector <2 x i64> %932, <2 x i64> %934, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %936 = getelementptr inbounds <2 x i64>, <2 x i64>* %883, i64 1
  %937 = load <2 x i64>, <2 x i64>* %936, align 16
  %938 = getelementptr inbounds <2 x i64>, <2 x i64>* %886, i64 1
  %939 = load <2 x i64>, <2 x i64>* %938, align 16
  %940 = shufflevector <2 x i64> %937, <2 x i64> %939, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %941 = getelementptr inbounds <2 x i64>, <2 x i64>* %890, i64 1
  %942 = load <2 x i64>, <2 x i64>* %941, align 16
  %943 = getelementptr inbounds <2 x i64>, <2 x i64>* %893, i64 1
  %944 = load <2 x i64>, <2 x i64>* %943, align 16
  %945 = shufflevector <2 x i64> %942, <2 x i64> %944, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %946 = getelementptr inbounds <2 x i64>, <2 x i64>* %897, i64 1
  %947 = load <2 x i64>, <2 x i64>* %946, align 16
  %948 = getelementptr inbounds <2 x i64>, <2 x i64>* %900, i64 1
  %949 = load <2 x i64>, <2 x i64>* %948, align 16
  %950 = shufflevector <2 x i64> %947, <2 x i64> %949, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %951 = getelementptr inbounds <2 x i64>, <2 x i64>* %904, i64 1
  %952 = load <2 x i64>, <2 x i64>* %951, align 16
  %953 = getelementptr inbounds <2 x i64>, <2 x i64>* %907, i64 1
  %954 = load <2 x i64>, <2 x i64>* %953, align 16
  %955 = shufflevector <2 x i64> %952, <2 x i64> %954, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %956 = getelementptr inbounds <2 x i64>, <2 x i64>* %911, i64 1
  %957 = load <2 x i64>, <2 x i64>* %956, align 16
  %958 = getelementptr inbounds <2 x i64>, <2 x i64>* %914, i64 1
  %959 = load <2 x i64>, <2 x i64>* %958, align 16
  %960 = shufflevector <2 x i64> %957, <2 x i64> %959, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %961 = getelementptr inbounds <2 x i64>, <2 x i64>* %918, i64 1
  %962 = load <2 x i64>, <2 x i64>* %961, align 16
  %963 = getelementptr inbounds <2 x i64>, <2 x i64>* %921, i64 1
  %964 = load <2 x i64>, <2 x i64>* %963, align 16
  %965 = shufflevector <2 x i64> %962, <2 x i64> %964, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %966 = getelementptr inbounds <2 x i64>, <2 x i64>* %925, i64 1
  %967 = load <2 x i64>, <2 x i64>* %966, align 16
  %968 = getelementptr inbounds <2 x i64>, <2 x i64>* %928, i64 1
  %969 = load <2 x i64>, <2 x i64>* %968, align 16
  %970 = shufflevector <2 x i64> %967, <2 x i64> %969, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %971 = bitcast <4 x i64> %881 to <16 x i16>
  %972 = bitcast <4 x i64> %888 to <16 x i16>
  %973 = shufflevector <16 x i16> %971, <16 x i16> %972, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %974 = shufflevector <16 x i16> %971, <16 x i16> %972, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %975 = bitcast <4 x i64> %895 to <16 x i16>
  %976 = bitcast <4 x i64> %902 to <16 x i16>
  %977 = shufflevector <16 x i16> %975, <16 x i16> %976, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %978 = shufflevector <16 x i16> %975, <16 x i16> %976, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %979 = bitcast <4 x i64> %909 to <16 x i16>
  %980 = bitcast <4 x i64> %916 to <16 x i16>
  %981 = shufflevector <16 x i16> %979, <16 x i16> %980, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %982 = shufflevector <16 x i16> %979, <16 x i16> %980, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %983 = bitcast <4 x i64> %923 to <16 x i16>
  %984 = bitcast <4 x i64> %930 to <16 x i16>
  %985 = shufflevector <16 x i16> %983, <16 x i16> %984, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %986 = shufflevector <16 x i16> %983, <16 x i16> %984, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %987 = bitcast <16 x i16> %973 to <8 x i32>
  %988 = bitcast <16 x i16> %977 to <8 x i32>
  %989 = shufflevector <8 x i32> %987, <8 x i32> %988, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %990 = shufflevector <8 x i32> %987, <8 x i32> %988, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %991 = bitcast <16 x i16> %981 to <8 x i32>
  %992 = bitcast <16 x i16> %985 to <8 x i32>
  %993 = shufflevector <8 x i32> %991, <8 x i32> %992, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %994 = shufflevector <8 x i32> %991, <8 x i32> %992, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %995 = bitcast <16 x i16> %974 to <8 x i32>
  %996 = bitcast <16 x i16> %978 to <8 x i32>
  %997 = shufflevector <8 x i32> %995, <8 x i32> %996, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %998 = shufflevector <8 x i32> %995, <8 x i32> %996, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %999 = bitcast <16 x i16> %982 to <8 x i32>
  %1000 = bitcast <16 x i16> %986 to <8 x i32>
  %1001 = shufflevector <8 x i32> %999, <8 x i32> %1000, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1002 = shufflevector <8 x i32> %999, <8 x i32> %1000, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1003 = bitcast <8 x i32> %989 to <4 x i64>
  %1004 = bitcast <8 x i32> %993 to <4 x i64>
  %1005 = shufflevector <4 x i64> %1003, <4 x i64> %1004, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1005, <4 x i64>* %811, align 32
  %1006 = shufflevector <4 x i64> %1003, <4 x i64> %1004, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1006, <4 x i64>* %882, align 32
  %1007 = bitcast <8 x i32> %997 to <4 x i64>
  %1008 = bitcast <8 x i32> %1001 to <4 x i64>
  %1009 = shufflevector <4 x i64> %1007, <4 x i64> %1008, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1009, <4 x i64>* %903, align 32
  %1010 = shufflevector <4 x i64> %1007, <4 x i64> %1008, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1010, <4 x i64>* %910, align 32
  %1011 = bitcast <8 x i32> %990 to <4 x i64>
  %1012 = bitcast <8 x i32> %994 to <4 x i64>
  %1013 = shufflevector <4 x i64> %1011, <4 x i64> %1012, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1013, <4 x i64>* %889, align 32
  %1014 = shufflevector <4 x i64> %1011, <4 x i64> %1012, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1014, <4 x i64>* %896, align 32
  %1015 = bitcast <8 x i32> %998 to <4 x i64>
  %1016 = bitcast <8 x i32> %1002 to <4 x i64>
  %1017 = shufflevector <4 x i64> %1015, <4 x i64> %1016, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1017, <4 x i64>* %917, align 32
  %1018 = shufflevector <4 x i64> %1015, <4 x i64> %1016, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1018, <4 x i64>* %924, align 32
  %1019 = bitcast <4 x i64> %935 to <16 x i16>
  %1020 = bitcast <4 x i64> %940 to <16 x i16>
  %1021 = shufflevector <16 x i16> %1019, <16 x i16> %1020, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1022 = shufflevector <16 x i16> %1019, <16 x i16> %1020, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1023 = bitcast <4 x i64> %945 to <16 x i16>
  %1024 = bitcast <4 x i64> %950 to <16 x i16>
  %1025 = shufflevector <16 x i16> %1023, <16 x i16> %1024, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1026 = shufflevector <16 x i16> %1023, <16 x i16> %1024, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1027 = bitcast <4 x i64> %955 to <16 x i16>
  %1028 = bitcast <4 x i64> %960 to <16 x i16>
  %1029 = shufflevector <16 x i16> %1027, <16 x i16> %1028, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1030 = shufflevector <16 x i16> %1027, <16 x i16> %1028, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1031 = bitcast <4 x i64> %965 to <16 x i16>
  %1032 = bitcast <4 x i64> %970 to <16 x i16>
  %1033 = shufflevector <16 x i16> %1031, <16 x i16> %1032, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1034 = shufflevector <16 x i16> %1031, <16 x i16> %1032, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1035 = bitcast <16 x i16> %1021 to <8 x i32>
  %1036 = bitcast <16 x i16> %1025 to <8 x i32>
  %1037 = shufflevector <8 x i32> %1035, <8 x i32> %1036, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1038 = shufflevector <8 x i32> %1035, <8 x i32> %1036, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1039 = bitcast <16 x i16> %1029 to <8 x i32>
  %1040 = bitcast <16 x i16> %1033 to <8 x i32>
  %1041 = shufflevector <8 x i32> %1039, <8 x i32> %1040, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1042 = shufflevector <8 x i32> %1039, <8 x i32> %1040, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1043 = bitcast <16 x i16> %1022 to <8 x i32>
  %1044 = bitcast <16 x i16> %1026 to <8 x i32>
  %1045 = shufflevector <8 x i32> %1043, <8 x i32> %1044, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1046 = shufflevector <8 x i32> %1043, <8 x i32> %1044, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1047 = bitcast <16 x i16> %1030 to <8 x i32>
  %1048 = bitcast <16 x i16> %1034 to <8 x i32>
  %1049 = shufflevector <8 x i32> %1047, <8 x i32> %1048, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1050 = shufflevector <8 x i32> %1047, <8 x i32> %1048, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1051 = bitcast <8 x i32> %1037 to <4 x i64>
  %1052 = bitcast <8 x i32> %1041 to <4 x i64>
  %1053 = shufflevector <4 x i64> %1051, <4 x i64> %1052, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1053, <4 x i64>* %878, align 32
  %1054 = shufflevector <4 x i64> %1051, <4 x i64> %1052, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1054, <4 x i64>* %885, align 32
  %1055 = bitcast <8 x i32> %1045 to <4 x i64>
  %1056 = bitcast <8 x i32> %1049 to <4 x i64>
  %1057 = shufflevector <4 x i64> %1055, <4 x i64> %1056, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1057, <4 x i64>* %906, align 32
  %1058 = shufflevector <4 x i64> %1055, <4 x i64> %1056, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1058, <4 x i64>* %913, align 32
  %1059 = bitcast <8 x i32> %1038 to <4 x i64>
  %1060 = bitcast <8 x i32> %1042 to <4 x i64>
  %1061 = shufflevector <4 x i64> %1059, <4 x i64> %1060, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1061, <4 x i64>* %892, align 32
  %1062 = shufflevector <4 x i64> %1059, <4 x i64> %1060, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1062, <4 x i64>* %899, align 32
  %1063 = bitcast <8 x i32> %1046 to <4 x i64>
  %1064 = bitcast <8 x i32> %1050 to <4 x i64>
  %1065 = shufflevector <4 x i64> %1063, <4 x i64> %1064, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1065, <4 x i64>* %920, align 32
  %1066 = shufflevector <4 x i64> %1063, <4 x i64> %1064, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1066, <4 x i64>* %927, align 32
  %1067 = shufflevector <4 x i64> %1005, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %1068 = bitcast <4 x i64> %1067 to <16 x i16>
  %1069 = shufflevector <16 x i16> %1068, <16 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1070 = shufflevector <16 x i16> %1068, <16 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1071 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1069, <16 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %1072 = ashr <8 x i32> %1071, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %1073 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1070, <16 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %1074 = ashr <8 x i32> %1073, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %1075 = bitcast i32* %1 to <8 x i32>*
  store <8 x i32> %1072, <8 x i32>* %1075, align 32
  %1076 = getelementptr inbounds i32, i32* %1, i64 8
  %1077 = bitcast i32* %1076 to <8 x i32>*
  store <8 x i32> %1074, <8 x i32>* %1077, align 32
  br label %1078

1078:                                             ; preds = %875, %1078
  %1079 = phi i64 [ 1, %875 ], [ %1095, %1078 ]
  %1080 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 %1079
  %1081 = load <4 x i64>, <4 x i64>* %1080, align 32
  %1082 = shl nsw i64 %1079, 5
  %1083 = getelementptr inbounds i32, i32* %1, i64 %1082
  %1084 = shufflevector <4 x i64> %1081, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %1085 = bitcast <4 x i64> %1084 to <16 x i16>
  %1086 = shufflevector <16 x i16> %1085, <16 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1087 = shufflevector <16 x i16> %1085, <16 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1088 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1086, <16 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %1089 = ashr <8 x i32> %1088, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %1090 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1087, <16 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %1091 = ashr <8 x i32> %1090, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %1092 = bitcast i32* %1083 to <8 x i32>*
  store <8 x i32> %1089, <8 x i32>* %1092, align 32
  %1093 = getelementptr inbounds i32, i32* %1083, i64 8
  %1094 = bitcast i32* %1093 to <8 x i32>*
  store <8 x i32> %1091, <8 x i32>* %1094, align 32
  %1095 = add nuw nsw i64 %1079, 1
  %1096 = icmp eq i64 %1095, 16
  br i1 %1096, label %1097, label %1078

1097:                                             ; preds = %1078
  %1098 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 16
  %1099 = bitcast <4 x i64>* %1098 to <2 x i64>*
  %1100 = load <2 x i64>, <2 x i64>* %1099, align 16
  %1101 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 24
  %1102 = bitcast <4 x i64>* %1101 to <2 x i64>*
  %1103 = load <2 x i64>, <2 x i64>* %1102, align 16
  %1104 = shufflevector <2 x i64> %1100, <2 x i64> %1103, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1105 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 17
  %1106 = bitcast <4 x i64>* %1105 to <2 x i64>*
  %1107 = load <2 x i64>, <2 x i64>* %1106, align 16
  %1108 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 25
  %1109 = bitcast <4 x i64>* %1108 to <2 x i64>*
  %1110 = load <2 x i64>, <2 x i64>* %1109, align 16
  %1111 = shufflevector <2 x i64> %1107, <2 x i64> %1110, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1112 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 18
  %1113 = bitcast <4 x i64>* %1112 to <2 x i64>*
  %1114 = load <2 x i64>, <2 x i64>* %1113, align 16
  %1115 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 26
  %1116 = bitcast <4 x i64>* %1115 to <2 x i64>*
  %1117 = load <2 x i64>, <2 x i64>* %1116, align 16
  %1118 = shufflevector <2 x i64> %1114, <2 x i64> %1117, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1119 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 19
  %1120 = bitcast <4 x i64>* %1119 to <2 x i64>*
  %1121 = load <2 x i64>, <2 x i64>* %1120, align 16
  %1122 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 27
  %1123 = bitcast <4 x i64>* %1122 to <2 x i64>*
  %1124 = load <2 x i64>, <2 x i64>* %1123, align 16
  %1125 = shufflevector <2 x i64> %1121, <2 x i64> %1124, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1126 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 20
  %1127 = bitcast <4 x i64>* %1126 to <2 x i64>*
  %1128 = load <2 x i64>, <2 x i64>* %1127, align 16
  %1129 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 28
  %1130 = bitcast <4 x i64>* %1129 to <2 x i64>*
  %1131 = load <2 x i64>, <2 x i64>* %1130, align 16
  %1132 = shufflevector <2 x i64> %1128, <2 x i64> %1131, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1133 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 21
  %1134 = bitcast <4 x i64>* %1133 to <2 x i64>*
  %1135 = load <2 x i64>, <2 x i64>* %1134, align 16
  %1136 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 29
  %1137 = bitcast <4 x i64>* %1136 to <2 x i64>*
  %1138 = load <2 x i64>, <2 x i64>* %1137, align 16
  %1139 = shufflevector <2 x i64> %1135, <2 x i64> %1138, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1140 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 22
  %1141 = bitcast <4 x i64>* %1140 to <2 x i64>*
  %1142 = load <2 x i64>, <2 x i64>* %1141, align 16
  %1143 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 30
  %1144 = bitcast <4 x i64>* %1143 to <2 x i64>*
  %1145 = load <2 x i64>, <2 x i64>* %1144, align 16
  %1146 = shufflevector <2 x i64> %1142, <2 x i64> %1145, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1147 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 23
  %1148 = bitcast <4 x i64>* %1147 to <2 x i64>*
  %1149 = load <2 x i64>, <2 x i64>* %1148, align 16
  %1150 = getelementptr inbounds <4 x i64>, <4 x i64>* %811, i64 31
  %1151 = bitcast <4 x i64>* %1150 to <2 x i64>*
  %1152 = load <2 x i64>, <2 x i64>* %1151, align 16
  %1153 = shufflevector <2 x i64> %1149, <2 x i64> %1152, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1154 = getelementptr inbounds <2 x i64>, <2 x i64>* %1099, i64 1
  %1155 = load <2 x i64>, <2 x i64>* %1154, align 16
  %1156 = getelementptr inbounds <2 x i64>, <2 x i64>* %1102, i64 1
  %1157 = load <2 x i64>, <2 x i64>* %1156, align 16
  %1158 = shufflevector <2 x i64> %1155, <2 x i64> %1157, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1159 = getelementptr inbounds <2 x i64>, <2 x i64>* %1106, i64 1
  %1160 = load <2 x i64>, <2 x i64>* %1159, align 16
  %1161 = getelementptr inbounds <2 x i64>, <2 x i64>* %1109, i64 1
  %1162 = load <2 x i64>, <2 x i64>* %1161, align 16
  %1163 = shufflevector <2 x i64> %1160, <2 x i64> %1162, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1164 = getelementptr inbounds <2 x i64>, <2 x i64>* %1113, i64 1
  %1165 = load <2 x i64>, <2 x i64>* %1164, align 16
  %1166 = getelementptr inbounds <2 x i64>, <2 x i64>* %1116, i64 1
  %1167 = load <2 x i64>, <2 x i64>* %1166, align 16
  %1168 = shufflevector <2 x i64> %1165, <2 x i64> %1167, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1169 = getelementptr inbounds <2 x i64>, <2 x i64>* %1120, i64 1
  %1170 = load <2 x i64>, <2 x i64>* %1169, align 16
  %1171 = getelementptr inbounds <2 x i64>, <2 x i64>* %1123, i64 1
  %1172 = load <2 x i64>, <2 x i64>* %1171, align 16
  %1173 = shufflevector <2 x i64> %1170, <2 x i64> %1172, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1174 = getelementptr inbounds <2 x i64>, <2 x i64>* %1127, i64 1
  %1175 = load <2 x i64>, <2 x i64>* %1174, align 16
  %1176 = getelementptr inbounds <2 x i64>, <2 x i64>* %1130, i64 1
  %1177 = load <2 x i64>, <2 x i64>* %1176, align 16
  %1178 = shufflevector <2 x i64> %1175, <2 x i64> %1177, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1179 = getelementptr inbounds <2 x i64>, <2 x i64>* %1134, i64 1
  %1180 = load <2 x i64>, <2 x i64>* %1179, align 16
  %1181 = getelementptr inbounds <2 x i64>, <2 x i64>* %1137, i64 1
  %1182 = load <2 x i64>, <2 x i64>* %1181, align 16
  %1183 = shufflevector <2 x i64> %1180, <2 x i64> %1182, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1184 = getelementptr inbounds <2 x i64>, <2 x i64>* %1141, i64 1
  %1185 = load <2 x i64>, <2 x i64>* %1184, align 16
  %1186 = getelementptr inbounds <2 x i64>, <2 x i64>* %1144, i64 1
  %1187 = load <2 x i64>, <2 x i64>* %1186, align 16
  %1188 = shufflevector <2 x i64> %1185, <2 x i64> %1187, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1189 = getelementptr inbounds <2 x i64>, <2 x i64>* %1148, i64 1
  %1190 = load <2 x i64>, <2 x i64>* %1189, align 16
  %1191 = getelementptr inbounds <2 x i64>, <2 x i64>* %1151, i64 1
  %1192 = load <2 x i64>, <2 x i64>* %1191, align 16
  %1193 = shufflevector <2 x i64> %1190, <2 x i64> %1192, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1194 = bitcast <4 x i64> %1104 to <16 x i16>
  %1195 = bitcast <4 x i64> %1111 to <16 x i16>
  %1196 = shufflevector <16 x i16> %1194, <16 x i16> %1195, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1197 = shufflevector <16 x i16> %1194, <16 x i16> %1195, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1198 = bitcast <4 x i64> %1118 to <16 x i16>
  %1199 = bitcast <4 x i64> %1125 to <16 x i16>
  %1200 = shufflevector <16 x i16> %1198, <16 x i16> %1199, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1201 = shufflevector <16 x i16> %1198, <16 x i16> %1199, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1202 = bitcast <4 x i64> %1132 to <16 x i16>
  %1203 = bitcast <4 x i64> %1139 to <16 x i16>
  %1204 = shufflevector <16 x i16> %1202, <16 x i16> %1203, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1205 = shufflevector <16 x i16> %1202, <16 x i16> %1203, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1206 = bitcast <4 x i64> %1146 to <16 x i16>
  %1207 = bitcast <4 x i64> %1153 to <16 x i16>
  %1208 = shufflevector <16 x i16> %1206, <16 x i16> %1207, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1209 = shufflevector <16 x i16> %1206, <16 x i16> %1207, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1210 = bitcast <16 x i16> %1196 to <8 x i32>
  %1211 = bitcast <16 x i16> %1200 to <8 x i32>
  %1212 = shufflevector <8 x i32> %1210, <8 x i32> %1211, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1213 = shufflevector <8 x i32> %1210, <8 x i32> %1211, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1214 = bitcast <16 x i16> %1204 to <8 x i32>
  %1215 = bitcast <16 x i16> %1208 to <8 x i32>
  %1216 = shufflevector <8 x i32> %1214, <8 x i32> %1215, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1217 = shufflevector <8 x i32> %1214, <8 x i32> %1215, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1218 = bitcast <16 x i16> %1197 to <8 x i32>
  %1219 = bitcast <16 x i16> %1201 to <8 x i32>
  %1220 = shufflevector <8 x i32> %1218, <8 x i32> %1219, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1221 = shufflevector <8 x i32> %1218, <8 x i32> %1219, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1222 = bitcast <16 x i16> %1205 to <8 x i32>
  %1223 = bitcast <16 x i16> %1209 to <8 x i32>
  %1224 = shufflevector <8 x i32> %1222, <8 x i32> %1223, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1225 = shufflevector <8 x i32> %1222, <8 x i32> %1223, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1226 = bitcast <8 x i32> %1212 to <4 x i64>
  %1227 = bitcast <8 x i32> %1216 to <4 x i64>
  %1228 = shufflevector <4 x i64> %1226, <4 x i64> %1227, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1228, <4 x i64>* %1098, align 32
  %1229 = shufflevector <4 x i64> %1226, <4 x i64> %1227, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1229, <4 x i64>* %1105, align 32
  %1230 = bitcast <8 x i32> %1220 to <4 x i64>
  %1231 = bitcast <8 x i32> %1224 to <4 x i64>
  %1232 = shufflevector <4 x i64> %1230, <4 x i64> %1231, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1232, <4 x i64>* %1126, align 32
  %1233 = shufflevector <4 x i64> %1230, <4 x i64> %1231, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1233, <4 x i64>* %1133, align 32
  %1234 = bitcast <8 x i32> %1213 to <4 x i64>
  %1235 = bitcast <8 x i32> %1217 to <4 x i64>
  %1236 = shufflevector <4 x i64> %1234, <4 x i64> %1235, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1236, <4 x i64>* %1112, align 32
  %1237 = shufflevector <4 x i64> %1234, <4 x i64> %1235, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1237, <4 x i64>* %1119, align 32
  %1238 = bitcast <8 x i32> %1221 to <4 x i64>
  %1239 = bitcast <8 x i32> %1225 to <4 x i64>
  %1240 = shufflevector <4 x i64> %1238, <4 x i64> %1239, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1240, <4 x i64>* %1140, align 32
  %1241 = shufflevector <4 x i64> %1238, <4 x i64> %1239, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1241, <4 x i64>* %1147, align 32
  %1242 = bitcast <4 x i64> %1158 to <16 x i16>
  %1243 = bitcast <4 x i64> %1163 to <16 x i16>
  %1244 = shufflevector <16 x i16> %1242, <16 x i16> %1243, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1245 = shufflevector <16 x i16> %1242, <16 x i16> %1243, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1246 = bitcast <4 x i64> %1168 to <16 x i16>
  %1247 = bitcast <4 x i64> %1173 to <16 x i16>
  %1248 = shufflevector <16 x i16> %1246, <16 x i16> %1247, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1249 = shufflevector <16 x i16> %1246, <16 x i16> %1247, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1250 = bitcast <4 x i64> %1178 to <16 x i16>
  %1251 = bitcast <4 x i64> %1183 to <16 x i16>
  %1252 = shufflevector <16 x i16> %1250, <16 x i16> %1251, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1253 = shufflevector <16 x i16> %1250, <16 x i16> %1251, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1254 = bitcast <4 x i64> %1188 to <16 x i16>
  %1255 = bitcast <4 x i64> %1193 to <16 x i16>
  %1256 = shufflevector <16 x i16> %1254, <16 x i16> %1255, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1257 = shufflevector <16 x i16> %1254, <16 x i16> %1255, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1258 = bitcast <16 x i16> %1244 to <8 x i32>
  %1259 = bitcast <16 x i16> %1248 to <8 x i32>
  %1260 = shufflevector <8 x i32> %1258, <8 x i32> %1259, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1261 = shufflevector <8 x i32> %1258, <8 x i32> %1259, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1262 = bitcast <16 x i16> %1252 to <8 x i32>
  %1263 = bitcast <16 x i16> %1256 to <8 x i32>
  %1264 = shufflevector <8 x i32> %1262, <8 x i32> %1263, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1265 = shufflevector <8 x i32> %1262, <8 x i32> %1263, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1266 = bitcast <16 x i16> %1245 to <8 x i32>
  %1267 = bitcast <16 x i16> %1249 to <8 x i32>
  %1268 = shufflevector <8 x i32> %1266, <8 x i32> %1267, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1269 = shufflevector <8 x i32> %1266, <8 x i32> %1267, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1270 = bitcast <16 x i16> %1253 to <8 x i32>
  %1271 = bitcast <16 x i16> %1257 to <8 x i32>
  %1272 = shufflevector <8 x i32> %1270, <8 x i32> %1271, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1273 = shufflevector <8 x i32> %1270, <8 x i32> %1271, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1274 = bitcast <8 x i32> %1260 to <4 x i64>
  %1275 = bitcast <8 x i32> %1264 to <4 x i64>
  %1276 = shufflevector <4 x i64> %1274, <4 x i64> %1275, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1276, <4 x i64>* %1101, align 32
  %1277 = shufflevector <4 x i64> %1274, <4 x i64> %1275, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1277, <4 x i64>* %1108, align 32
  %1278 = bitcast <8 x i32> %1268 to <4 x i64>
  %1279 = bitcast <8 x i32> %1272 to <4 x i64>
  %1280 = shufflevector <4 x i64> %1278, <4 x i64> %1279, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1280, <4 x i64>* %1129, align 32
  %1281 = shufflevector <4 x i64> %1278, <4 x i64> %1279, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1281, <4 x i64>* %1136, align 32
  %1282 = bitcast <8 x i32> %1261 to <4 x i64>
  %1283 = bitcast <8 x i32> %1265 to <4 x i64>
  %1284 = shufflevector <4 x i64> %1282, <4 x i64> %1283, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1284, <4 x i64>* %1115, align 32
  %1285 = shufflevector <4 x i64> %1282, <4 x i64> %1283, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1285, <4 x i64>* %1122, align 32
  %1286 = bitcast <8 x i32> %1269 to <4 x i64>
  %1287 = bitcast <8 x i32> %1273 to <4 x i64>
  %1288 = shufflevector <4 x i64> %1286, <4 x i64> %1287, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1288, <4 x i64>* %1143, align 32
  %1289 = shufflevector <4 x i64> %1286, <4 x i64> %1287, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1289, <4 x i64>* %1150, align 32
  %1290 = getelementptr inbounds i32, i32* %1, i64 16
  %1291 = shufflevector <4 x i64> %1228, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %1292 = bitcast <4 x i64> %1291 to <16 x i16>
  %1293 = shufflevector <16 x i16> %1292, <16 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1294 = shufflevector <16 x i16> %1292, <16 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1295 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1293, <16 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %1296 = ashr <8 x i32> %1295, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %1297 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1294, <16 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %1298 = ashr <8 x i32> %1297, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %1299 = bitcast i32* %1290 to <8 x i32>*
  store <8 x i32> %1296, <8 x i32>* %1299, align 32
  %1300 = getelementptr inbounds i32, i32* %1, i64 24
  %1301 = bitcast i32* %1300 to <8 x i32>*
  store <8 x i32> %1298, <8 x i32>* %1301, align 32
  br label %1302

1302:                                             ; preds = %1097, %1302
  %1303 = phi i64 [ 1, %1097 ], [ %1319, %1302 ]
  %1304 = getelementptr inbounds <4 x i64>, <4 x i64>* %1098, i64 %1303
  %1305 = load <4 x i64>, <4 x i64>* %1304, align 32
  %1306 = shl nsw i64 %1303, 5
  %1307 = getelementptr inbounds i32, i32* %1290, i64 %1306
  %1308 = shufflevector <4 x i64> %1305, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %1309 = bitcast <4 x i64> %1308 to <16 x i16>
  %1310 = shufflevector <16 x i16> %1309, <16 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1311 = shufflevector <16 x i16> %1309, <16 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1312 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1310, <16 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %1313 = ashr <8 x i32> %1312, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %1314 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1311, <16 x i16> <i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048, i16 5793, i16 2048>) #8
  %1315 = ashr <8 x i32> %1314, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %1316 = bitcast i32* %1307 to <8 x i32>*
  store <8 x i32> %1313, <8 x i32>* %1316, align 32
  %1317 = getelementptr inbounds i32, i32* %1307, i64 8
  %1318 = bitcast i32* %1317 to <8 x i32>*
  store <8 x i32> %1315, <8 x i32>* %1318, align 32
  %1319 = add nuw nsw i64 %1303, 1
  %1320 = icmp eq i64 %1319, 16
  br i1 %1320, label %1321, label %1302

1321:                                             ; preds = %1302
  call void @llvm.lifetime.end.p0i8(i64 2048, i8* nonnull %9) #8
  call void @llvm.lifetime.end.p0i8(i64 1024, i8* nonnull %8) #8
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @lowbd_fwd_txfm2d_32x64_avx2(i16* nocapture readonly, i32* nocapture, i32, i8 zeroext, i32) #3 {
  %6 = alloca [64 x <4 x i64>], align 32
  %7 = alloca [256 x <4 x i64>], align 32
  %8 = alloca [32 x <4 x i64>], align 32
  %9 = alloca [32 x <4 x i64>], align 32
  %10 = bitcast [64 x <4 x i64>]* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 2048, i8* nonnull %10) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %10, i8 -86, i64 2048, i1 false)
  %11 = bitcast [256 x <4 x i64>]* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 8192, i8* nonnull %11) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %11, i8 -86, i64 8192, i1 false)
  %12 = load i8*, i8** getelementptr inbounds ([19 x i8*], [19 x i8*]* @av1_fwd_txfm_shift_ls, i64 0, i64 11), align 8
  %13 = load i8, i8* getelementptr inbounds ([5 x [5 x i8]], [5 x [5 x i8]]* @av1_fwd_cos_bit_col, i64 0, i64 3, i64 4), align 1
  %14 = load i8, i8* getelementptr inbounds ([5 x [5 x i8]], [5 x [5 x i8]]* @av1_fwd_cos_bit_row, i64 0, i64 3, i64 4), align 1
  %15 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 0
  %16 = sext i32 %2 to i64
  %17 = getelementptr inbounds i8, i8* %12, i64 1
  br label %24

18:                                               ; preds = %182
  %19 = bitcast [32 x <4 x i64>]* %8 to i8*
  %20 = bitcast [32 x <4 x i64>]* %9 to i8*
  %21 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %8, i64 0, i64 0
  %22 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 0
  %23 = getelementptr inbounds i8, i8* %12, i64 2
  br label %401

24:                                               ; preds = %182, %5
  %25 = phi i64 [ 0, %5 ], [ %183, %182 ]
  %26 = shl nsw i64 %25, 4
  %27 = getelementptr inbounds i16, i16* %0, i64 %26
  br label %28

28:                                               ; preds = %28, %24
  %29 = phi i64 [ 0, %24 ], [ %53, %28 ]
  %30 = mul nsw i64 %29, %16
  %31 = getelementptr inbounds i16, i16* %27, i64 %30
  %32 = bitcast i16* %31 to <4 x i64>*
  %33 = load <4 x i64>, <4 x i64>* %32, align 32
  %34 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %29
  store <4 x i64> %33, <4 x i64>* %34, align 32
  %35 = or i64 %29, 1
  %36 = mul nsw i64 %35, %16
  %37 = getelementptr inbounds i16, i16* %27, i64 %36
  %38 = bitcast i16* %37 to <4 x i64>*
  %39 = load <4 x i64>, <4 x i64>* %38, align 32
  %40 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %35
  store <4 x i64> %39, <4 x i64>* %40, align 32
  %41 = or i64 %29, 2
  %42 = mul nsw i64 %41, %16
  %43 = getelementptr inbounds i16, i16* %27, i64 %42
  %44 = bitcast i16* %43 to <4 x i64>*
  %45 = load <4 x i64>, <4 x i64>* %44, align 32
  %46 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %41
  store <4 x i64> %45, <4 x i64>* %46, align 32
  %47 = or i64 %29, 3
  %48 = mul nsw i64 %47, %16
  %49 = getelementptr inbounds i16, i16* %27, i64 %48
  %50 = bitcast i16* %49 to <4 x i64>*
  %51 = load <4 x i64>, <4 x i64>* %50, align 32
  %52 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %47
  store <4 x i64> %51, <4 x i64>* %52, align 32
  %53 = add nuw nsw i64 %29, 4
  %54 = icmp eq i64 %53, 64
  br i1 %54, label %55, label %28

55:                                               ; preds = %28
  %56 = load i8, i8* %12, align 1
  %57 = sext i8 %56 to i32
  %58 = icmp slt i8 %56, 0
  br i1 %58, label %59, label %93

59:                                               ; preds = %55
  %60 = sub nsw i32 0, %57
  %61 = xor i32 %57, -1
  %62 = shl i32 1, %61
  %63 = trunc i32 %62 to i16
  %64 = insertelement <16 x i16> undef, i16 %63, i32 0
  %65 = shufflevector <16 x i16> %64, <16 x i16> undef, <16 x i32> zeroinitializer
  br label %66

66:                                               ; preds = %66, %59
  %67 = phi i64 [ 0, %59 ], [ %91, %66 ]
  %68 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %67
  %69 = bitcast <4 x i64>* %68 to <16 x i16>*
  %70 = load <16 x i16>, <16 x i16>* %69, align 32
  %71 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %70, <16 x i16> %65) #8
  %72 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %71, i32 %60) #8
  store <16 x i16> %72, <16 x i16>* %69, align 32
  %73 = or i64 %67, 1
  %74 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %73
  %75 = bitcast <4 x i64>* %74 to <16 x i16>*
  %76 = load <16 x i16>, <16 x i16>* %75, align 32
  %77 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %76, <16 x i16> %65) #8
  %78 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %77, i32 %60) #8
  store <16 x i16> %78, <16 x i16>* %75, align 32
  %79 = or i64 %67, 2
  %80 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %79
  %81 = bitcast <4 x i64>* %80 to <16 x i16>*
  %82 = load <16 x i16>, <16 x i16>* %81, align 32
  %83 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %82, <16 x i16> %65) #8
  %84 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %83, i32 %60) #8
  store <16 x i16> %84, <16 x i16>* %81, align 32
  %85 = or i64 %67, 3
  %86 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %85
  %87 = bitcast <4 x i64>* %86 to <16 x i16>*
  %88 = load <16 x i16>, <16 x i16>* %87, align 32
  %89 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %88, <16 x i16> %65) #8
  %90 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %89, i32 %60) #8
  store <16 x i16> %90, <16 x i16>* %87, align 32
  %91 = add nuw nsw i64 %67, 4
  %92 = icmp eq i64 %91, 64
  br i1 %92, label %118, label %66

93:                                               ; preds = %55
  %94 = icmp eq i8 %56, 0
  br i1 %94, label %118, label %95

95:                                               ; preds = %93, %95
  %96 = phi i64 [ %116, %95 ], [ 0, %93 ]
  %97 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %96
  %98 = bitcast <4 x i64>* %97 to <16 x i16>*
  %99 = load <16 x i16>, <16 x i16>* %98, align 32
  %100 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %99, i32 %57) #8
  store <16 x i16> %100, <16 x i16>* %98, align 32
  %101 = or i64 %96, 1
  %102 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %101
  %103 = bitcast <4 x i64>* %102 to <16 x i16>*
  %104 = load <16 x i16>, <16 x i16>* %103, align 32
  %105 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %104, i32 %57) #8
  store <16 x i16> %105, <16 x i16>* %103, align 32
  %106 = or i64 %96, 2
  %107 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %106
  %108 = bitcast <4 x i64>* %107 to <16 x i16>*
  %109 = load <16 x i16>, <16 x i16>* %108, align 32
  %110 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %109, i32 %57) #8
  store <16 x i16> %110, <16 x i16>* %108, align 32
  %111 = or i64 %96, 3
  %112 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %111
  %113 = bitcast <4 x i64>* %112 to <16 x i16>*
  %114 = load <16 x i16>, <16 x i16>* %113, align 32
  %115 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %114, i32 %57) #8
  store <16 x i16> %115, <16 x i16>* %113, align 32
  %116 = add nuw nsw i64 %96, 4
  %117 = icmp eq i64 %116, 64
  br i1 %117, label %118, label %95

118:                                              ; preds = %95, %66, %93
  call fastcc void @fdct16x64_new_avx2(<4 x i64>* nonnull %15, <4 x i64>* nonnull %15, i8 signext %13)
  %119 = load i8, i8* %17, align 1
  %120 = sext i8 %119 to i32
  %121 = icmp slt i8 %119, 0
  br i1 %121, label %122, label %156

122:                                              ; preds = %118
  %123 = sub nsw i32 0, %120
  %124 = xor i32 %120, -1
  %125 = shl i32 1, %124
  %126 = trunc i32 %125 to i16
  %127 = insertelement <16 x i16> undef, i16 %126, i32 0
  %128 = shufflevector <16 x i16> %127, <16 x i16> undef, <16 x i32> zeroinitializer
  br label %129

129:                                              ; preds = %129, %122
  %130 = phi i64 [ 0, %122 ], [ %154, %129 ]
  %131 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %130
  %132 = bitcast <4 x i64>* %131 to <16 x i16>*
  %133 = load <16 x i16>, <16 x i16>* %132, align 32
  %134 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %133, <16 x i16> %128) #8
  %135 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %134, i32 %123) #8
  store <16 x i16> %135, <16 x i16>* %132, align 32
  %136 = or i64 %130, 1
  %137 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %136
  %138 = bitcast <4 x i64>* %137 to <16 x i16>*
  %139 = load <16 x i16>, <16 x i16>* %138, align 32
  %140 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %139, <16 x i16> %128) #8
  %141 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %140, i32 %123) #8
  store <16 x i16> %141, <16 x i16>* %138, align 32
  %142 = or i64 %130, 2
  %143 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %142
  %144 = bitcast <4 x i64>* %143 to <16 x i16>*
  %145 = load <16 x i16>, <16 x i16>* %144, align 32
  %146 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %145, <16 x i16> %128) #8
  %147 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %146, i32 %123) #8
  store <16 x i16> %147, <16 x i16>* %144, align 32
  %148 = or i64 %130, 3
  %149 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %148
  %150 = bitcast <4 x i64>* %149 to <16 x i16>*
  %151 = load <16 x i16>, <16 x i16>* %150, align 32
  %152 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %151, <16 x i16> %128) #8
  %153 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %152, i32 %123) #8
  store <16 x i16> %153, <16 x i16>* %150, align 32
  %154 = add nuw nsw i64 %130, 4
  %155 = icmp eq i64 %154, 64
  br i1 %155, label %158, label %129

156:                                              ; preds = %118
  %157 = icmp eq i8 %119, 0
  br i1 %157, label %158, label %159

158:                                              ; preds = %159, %129, %156
  br label %185

159:                                              ; preds = %156, %159
  %160 = phi i64 [ %180, %159 ], [ 0, %156 ]
  %161 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %160
  %162 = bitcast <4 x i64>* %161 to <16 x i16>*
  %163 = load <16 x i16>, <16 x i16>* %162, align 32
  %164 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %163, i32 %120) #8
  store <16 x i16> %164, <16 x i16>* %162, align 32
  %165 = or i64 %160, 1
  %166 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %165
  %167 = bitcast <4 x i64>* %166 to <16 x i16>*
  %168 = load <16 x i16>, <16 x i16>* %167, align 32
  %169 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %168, i32 %120) #8
  store <16 x i16> %169, <16 x i16>* %167, align 32
  %170 = or i64 %160, 2
  %171 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %170
  %172 = bitcast <4 x i64>* %171 to <16 x i16>*
  %173 = load <16 x i16>, <16 x i16>* %172, align 32
  %174 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %173, i32 %120) #8
  store <16 x i16> %174, <16 x i16>* %172, align 32
  %175 = or i64 %160, 3
  %176 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %175
  %177 = bitcast <4 x i64>* %176 to <16 x i16>*
  %178 = load <16 x i16>, <16 x i16>* %177, align 32
  %179 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %178, i32 %120) #8
  store <16 x i16> %179, <16 x i16>* %177, align 32
  %180 = add nuw nsw i64 %160, 4
  %181 = icmp eq i64 %180, 64
  br i1 %181, label %158, label %159

182:                                              ; preds = %185
  %183 = add nuw nsw i64 %25, 1
  %184 = icmp eq i64 %183, 2
  br i1 %184, label %18, label %24

185:                                              ; preds = %158, %185
  %186 = phi i64 [ %398, %185 ], [ 0, %158 ]
  %187 = shl nsw i64 %186, 4
  %188 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %187
  %189 = shl nsw i64 %186, 5
  %190 = getelementptr inbounds [256 x <4 x i64>], [256 x <4 x i64>]* %7, i64 0, i64 %189
  %191 = getelementptr inbounds <4 x i64>, <4 x i64>* %190, i64 %26
  %192 = bitcast <4 x i64>* %188 to <2 x i64>*
  %193 = load <2 x i64>, <2 x i64>* %192, align 32
  %194 = getelementptr inbounds <4 x i64>, <4 x i64>* %188, i64 8
  %195 = bitcast <4 x i64>* %194 to <2 x i64>*
  %196 = load <2 x i64>, <2 x i64>* %195, align 32
  %197 = shufflevector <2 x i64> %193, <2 x i64> %196, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %198 = getelementptr inbounds <4 x i64>, <4 x i64>* %188, i64 1
  %199 = bitcast <4 x i64>* %198 to <2 x i64>*
  %200 = load <2 x i64>, <2 x i64>* %199, align 32
  %201 = getelementptr inbounds <4 x i64>, <4 x i64>* %188, i64 9
  %202 = bitcast <4 x i64>* %201 to <2 x i64>*
  %203 = load <2 x i64>, <2 x i64>* %202, align 32
  %204 = shufflevector <2 x i64> %200, <2 x i64> %203, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %205 = getelementptr inbounds <4 x i64>, <4 x i64>* %188, i64 2
  %206 = bitcast <4 x i64>* %205 to <2 x i64>*
  %207 = load <2 x i64>, <2 x i64>* %206, align 32
  %208 = getelementptr inbounds <4 x i64>, <4 x i64>* %188, i64 10
  %209 = bitcast <4 x i64>* %208 to <2 x i64>*
  %210 = load <2 x i64>, <2 x i64>* %209, align 32
  %211 = shufflevector <2 x i64> %207, <2 x i64> %210, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %212 = getelementptr inbounds <4 x i64>, <4 x i64>* %188, i64 3
  %213 = bitcast <4 x i64>* %212 to <2 x i64>*
  %214 = load <2 x i64>, <2 x i64>* %213, align 32
  %215 = getelementptr inbounds <4 x i64>, <4 x i64>* %188, i64 11
  %216 = bitcast <4 x i64>* %215 to <2 x i64>*
  %217 = load <2 x i64>, <2 x i64>* %216, align 32
  %218 = shufflevector <2 x i64> %214, <2 x i64> %217, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %219 = getelementptr inbounds <4 x i64>, <4 x i64>* %188, i64 4
  %220 = bitcast <4 x i64>* %219 to <2 x i64>*
  %221 = load <2 x i64>, <2 x i64>* %220, align 32
  %222 = getelementptr inbounds <4 x i64>, <4 x i64>* %188, i64 12
  %223 = bitcast <4 x i64>* %222 to <2 x i64>*
  %224 = load <2 x i64>, <2 x i64>* %223, align 32
  %225 = shufflevector <2 x i64> %221, <2 x i64> %224, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %226 = getelementptr inbounds <4 x i64>, <4 x i64>* %188, i64 5
  %227 = bitcast <4 x i64>* %226 to <2 x i64>*
  %228 = load <2 x i64>, <2 x i64>* %227, align 32
  %229 = getelementptr inbounds <4 x i64>, <4 x i64>* %188, i64 13
  %230 = bitcast <4 x i64>* %229 to <2 x i64>*
  %231 = load <2 x i64>, <2 x i64>* %230, align 32
  %232 = shufflevector <2 x i64> %228, <2 x i64> %231, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %233 = getelementptr inbounds <4 x i64>, <4 x i64>* %188, i64 6
  %234 = bitcast <4 x i64>* %233 to <2 x i64>*
  %235 = load <2 x i64>, <2 x i64>* %234, align 32
  %236 = getelementptr inbounds <4 x i64>, <4 x i64>* %188, i64 14
  %237 = bitcast <4 x i64>* %236 to <2 x i64>*
  %238 = load <2 x i64>, <2 x i64>* %237, align 32
  %239 = shufflevector <2 x i64> %235, <2 x i64> %238, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %240 = getelementptr inbounds <4 x i64>, <4 x i64>* %188, i64 7
  %241 = bitcast <4 x i64>* %240 to <2 x i64>*
  %242 = load <2 x i64>, <2 x i64>* %241, align 32
  %243 = getelementptr inbounds <4 x i64>, <4 x i64>* %188, i64 15
  %244 = bitcast <4 x i64>* %243 to <2 x i64>*
  %245 = load <2 x i64>, <2 x i64>* %244, align 32
  %246 = shufflevector <2 x i64> %242, <2 x i64> %245, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %247 = getelementptr inbounds <2 x i64>, <2 x i64>* %192, i64 1
  %248 = load <2 x i64>, <2 x i64>* %247, align 16
  %249 = getelementptr inbounds <2 x i64>, <2 x i64>* %195, i64 1
  %250 = load <2 x i64>, <2 x i64>* %249, align 16
  %251 = shufflevector <2 x i64> %248, <2 x i64> %250, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %252 = getelementptr inbounds <2 x i64>, <2 x i64>* %199, i64 1
  %253 = load <2 x i64>, <2 x i64>* %252, align 16
  %254 = getelementptr inbounds <2 x i64>, <2 x i64>* %202, i64 1
  %255 = load <2 x i64>, <2 x i64>* %254, align 16
  %256 = shufflevector <2 x i64> %253, <2 x i64> %255, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %257 = getelementptr inbounds <2 x i64>, <2 x i64>* %206, i64 1
  %258 = load <2 x i64>, <2 x i64>* %257, align 16
  %259 = getelementptr inbounds <2 x i64>, <2 x i64>* %209, i64 1
  %260 = load <2 x i64>, <2 x i64>* %259, align 16
  %261 = shufflevector <2 x i64> %258, <2 x i64> %260, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %262 = getelementptr inbounds <2 x i64>, <2 x i64>* %213, i64 1
  %263 = load <2 x i64>, <2 x i64>* %262, align 16
  %264 = getelementptr inbounds <2 x i64>, <2 x i64>* %216, i64 1
  %265 = load <2 x i64>, <2 x i64>* %264, align 16
  %266 = shufflevector <2 x i64> %263, <2 x i64> %265, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %267 = getelementptr inbounds <2 x i64>, <2 x i64>* %220, i64 1
  %268 = load <2 x i64>, <2 x i64>* %267, align 16
  %269 = getelementptr inbounds <2 x i64>, <2 x i64>* %223, i64 1
  %270 = load <2 x i64>, <2 x i64>* %269, align 16
  %271 = shufflevector <2 x i64> %268, <2 x i64> %270, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %272 = getelementptr inbounds <2 x i64>, <2 x i64>* %227, i64 1
  %273 = load <2 x i64>, <2 x i64>* %272, align 16
  %274 = getelementptr inbounds <2 x i64>, <2 x i64>* %230, i64 1
  %275 = load <2 x i64>, <2 x i64>* %274, align 16
  %276 = shufflevector <2 x i64> %273, <2 x i64> %275, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %277 = getelementptr inbounds <2 x i64>, <2 x i64>* %234, i64 1
  %278 = load <2 x i64>, <2 x i64>* %277, align 16
  %279 = getelementptr inbounds <2 x i64>, <2 x i64>* %237, i64 1
  %280 = load <2 x i64>, <2 x i64>* %279, align 16
  %281 = shufflevector <2 x i64> %278, <2 x i64> %280, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %282 = getelementptr inbounds <2 x i64>, <2 x i64>* %241, i64 1
  %283 = load <2 x i64>, <2 x i64>* %282, align 16
  %284 = getelementptr inbounds <2 x i64>, <2 x i64>* %244, i64 1
  %285 = load <2 x i64>, <2 x i64>* %284, align 16
  %286 = shufflevector <2 x i64> %283, <2 x i64> %285, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %287 = bitcast <4 x i64> %197 to <16 x i16>
  %288 = bitcast <4 x i64> %204 to <16 x i16>
  %289 = shufflevector <16 x i16> %287, <16 x i16> %288, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %290 = shufflevector <16 x i16> %287, <16 x i16> %288, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %291 = bitcast <4 x i64> %211 to <16 x i16>
  %292 = bitcast <4 x i64> %218 to <16 x i16>
  %293 = shufflevector <16 x i16> %291, <16 x i16> %292, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %294 = shufflevector <16 x i16> %291, <16 x i16> %292, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %295 = bitcast <4 x i64> %225 to <16 x i16>
  %296 = bitcast <4 x i64> %232 to <16 x i16>
  %297 = shufflevector <16 x i16> %295, <16 x i16> %296, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %298 = shufflevector <16 x i16> %295, <16 x i16> %296, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %299 = bitcast <4 x i64> %239 to <16 x i16>
  %300 = bitcast <4 x i64> %246 to <16 x i16>
  %301 = shufflevector <16 x i16> %299, <16 x i16> %300, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %302 = shufflevector <16 x i16> %299, <16 x i16> %300, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %303 = bitcast <16 x i16> %289 to <8 x i32>
  %304 = bitcast <16 x i16> %293 to <8 x i32>
  %305 = shufflevector <8 x i32> %303, <8 x i32> %304, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %306 = shufflevector <8 x i32> %303, <8 x i32> %304, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %307 = bitcast <16 x i16> %297 to <8 x i32>
  %308 = bitcast <16 x i16> %301 to <8 x i32>
  %309 = shufflevector <8 x i32> %307, <8 x i32> %308, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %310 = shufflevector <8 x i32> %307, <8 x i32> %308, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %311 = bitcast <16 x i16> %290 to <8 x i32>
  %312 = bitcast <16 x i16> %294 to <8 x i32>
  %313 = shufflevector <8 x i32> %311, <8 x i32> %312, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %314 = shufflevector <8 x i32> %311, <8 x i32> %312, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %315 = bitcast <16 x i16> %298 to <8 x i32>
  %316 = bitcast <16 x i16> %302 to <8 x i32>
  %317 = shufflevector <8 x i32> %315, <8 x i32> %316, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %318 = shufflevector <8 x i32> %315, <8 x i32> %316, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %319 = bitcast <8 x i32> %305 to <4 x i64>
  %320 = bitcast <8 x i32> %309 to <4 x i64>
  %321 = shufflevector <4 x i64> %319, <4 x i64> %320, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %321, <4 x i64>* %191, align 32
  %322 = shufflevector <4 x i64> %319, <4 x i64> %320, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %323 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 1
  store <4 x i64> %322, <4 x i64>* %323, align 32
  %324 = bitcast <8 x i32> %313 to <4 x i64>
  %325 = bitcast <8 x i32> %317 to <4 x i64>
  %326 = shufflevector <4 x i64> %324, <4 x i64> %325, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %327 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 4
  store <4 x i64> %326, <4 x i64>* %327, align 32
  %328 = shufflevector <4 x i64> %324, <4 x i64> %325, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %329 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 5
  store <4 x i64> %328, <4 x i64>* %329, align 32
  %330 = bitcast <8 x i32> %306 to <4 x i64>
  %331 = bitcast <8 x i32> %310 to <4 x i64>
  %332 = shufflevector <4 x i64> %330, <4 x i64> %331, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %333 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 2
  store <4 x i64> %332, <4 x i64>* %333, align 32
  %334 = shufflevector <4 x i64> %330, <4 x i64> %331, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %335 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 3
  store <4 x i64> %334, <4 x i64>* %335, align 32
  %336 = bitcast <8 x i32> %314 to <4 x i64>
  %337 = bitcast <8 x i32> %318 to <4 x i64>
  %338 = shufflevector <4 x i64> %336, <4 x i64> %337, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %339 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 6
  store <4 x i64> %338, <4 x i64>* %339, align 32
  %340 = shufflevector <4 x i64> %336, <4 x i64> %337, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %341 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 7
  store <4 x i64> %340, <4 x i64>* %341, align 32
  %342 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 8
  %343 = bitcast <4 x i64> %251 to <16 x i16>
  %344 = bitcast <4 x i64> %256 to <16 x i16>
  %345 = shufflevector <16 x i16> %343, <16 x i16> %344, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %346 = shufflevector <16 x i16> %343, <16 x i16> %344, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %347 = bitcast <4 x i64> %261 to <16 x i16>
  %348 = bitcast <4 x i64> %266 to <16 x i16>
  %349 = shufflevector <16 x i16> %347, <16 x i16> %348, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %350 = shufflevector <16 x i16> %347, <16 x i16> %348, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %351 = bitcast <4 x i64> %271 to <16 x i16>
  %352 = bitcast <4 x i64> %276 to <16 x i16>
  %353 = shufflevector <16 x i16> %351, <16 x i16> %352, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %354 = shufflevector <16 x i16> %351, <16 x i16> %352, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %355 = bitcast <4 x i64> %281 to <16 x i16>
  %356 = bitcast <4 x i64> %286 to <16 x i16>
  %357 = shufflevector <16 x i16> %355, <16 x i16> %356, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %358 = shufflevector <16 x i16> %355, <16 x i16> %356, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %359 = bitcast <16 x i16> %345 to <8 x i32>
  %360 = bitcast <16 x i16> %349 to <8 x i32>
  %361 = shufflevector <8 x i32> %359, <8 x i32> %360, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %362 = shufflevector <8 x i32> %359, <8 x i32> %360, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %363 = bitcast <16 x i16> %353 to <8 x i32>
  %364 = bitcast <16 x i16> %357 to <8 x i32>
  %365 = shufflevector <8 x i32> %363, <8 x i32> %364, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %366 = shufflevector <8 x i32> %363, <8 x i32> %364, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %367 = bitcast <16 x i16> %346 to <8 x i32>
  %368 = bitcast <16 x i16> %350 to <8 x i32>
  %369 = shufflevector <8 x i32> %367, <8 x i32> %368, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %370 = shufflevector <8 x i32> %367, <8 x i32> %368, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %371 = bitcast <16 x i16> %354 to <8 x i32>
  %372 = bitcast <16 x i16> %358 to <8 x i32>
  %373 = shufflevector <8 x i32> %371, <8 x i32> %372, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %374 = shufflevector <8 x i32> %371, <8 x i32> %372, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %375 = bitcast <8 x i32> %361 to <4 x i64>
  %376 = bitcast <8 x i32> %365 to <4 x i64>
  %377 = shufflevector <4 x i64> %375, <4 x i64> %376, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %377, <4 x i64>* %342, align 32
  %378 = shufflevector <4 x i64> %375, <4 x i64> %376, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %379 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 9
  store <4 x i64> %378, <4 x i64>* %379, align 32
  %380 = bitcast <8 x i32> %369 to <4 x i64>
  %381 = bitcast <8 x i32> %373 to <4 x i64>
  %382 = shufflevector <4 x i64> %380, <4 x i64> %381, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %383 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 12
  store <4 x i64> %382, <4 x i64>* %383, align 32
  %384 = shufflevector <4 x i64> %380, <4 x i64> %381, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %385 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 13
  store <4 x i64> %384, <4 x i64>* %385, align 32
  %386 = bitcast <8 x i32> %362 to <4 x i64>
  %387 = bitcast <8 x i32> %366 to <4 x i64>
  %388 = shufflevector <4 x i64> %386, <4 x i64> %387, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %389 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 10
  store <4 x i64> %388, <4 x i64>* %389, align 32
  %390 = shufflevector <4 x i64> %386, <4 x i64> %387, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %391 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 11
  store <4 x i64> %390, <4 x i64>* %391, align 32
  %392 = bitcast <8 x i32> %370 to <4 x i64>
  %393 = bitcast <8 x i32> %374 to <4 x i64>
  %394 = shufflevector <4 x i64> %392, <4 x i64> %393, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %395 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 14
  store <4 x i64> %394, <4 x i64>* %395, align 32
  %396 = shufflevector <4 x i64> %392, <4 x i64> %393, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %397 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 15
  store <4 x i64> %396, <4 x i64>* %397, align 32
  %398 = add nuw nsw i64 %186, 1
  %399 = icmp eq i64 %398, 2
  br i1 %399, label %182, label %185

400:                                              ; preds = %542
  call void @llvm.lifetime.end.p0i8(i64 8192, i8* nonnull %11) #8
  call void @llvm.lifetime.end.p0i8(i64 2048, i8* nonnull %10) #8
  ret void

401:                                              ; preds = %542, %18
  %402 = phi i64 [ 0, %18 ], [ %543, %542 ]
  call void @llvm.lifetime.start.p0i8(i64 1024, i8* nonnull %19) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %19, i8 -86, i64 1024, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 1024, i8* nonnull %20) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %20, i8 -86, i64 1024, i1 false)
  %403 = shl nsw i64 %402, 5
  %404 = getelementptr inbounds [256 x <4 x i64>], [256 x <4 x i64>]* %7, i64 0, i64 %403
  %405 = bitcast <4 x i64>* %404 to <2 x i64>*
  br label %509

406:                                              ; preds = %509
  call fastcc void @fdct32_avx2(<4 x i64>* nonnull %21, <4 x i64>* nonnull %21, i8 signext %14)
  call fastcc void @fdct32_avx2(<4 x i64>* nonnull %22, <4 x i64>* nonnull %22, i8 signext %14)
  %407 = load i8, i8* %23, align 1
  %408 = sext i8 %407 to i32
  %409 = sub nsw i32 0, %408
  %410 = icmp slt i8 %407, 0
  br i1 %410, label %411, label %437

411:                                              ; preds = %406
  %412 = xor i32 %408, -1
  %413 = shl i32 1, %412
  %414 = insertelement <8 x i32> undef, i32 %413, i32 0
  %415 = shufflevector <8 x i32> %414, <8 x i32> undef, <8 x i32> zeroinitializer
  br label %416

416:                                              ; preds = %416, %411
  %417 = phi i64 [ 0, %411 ], [ %435, %416 ]
  %418 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %8, i64 0, i64 %417
  %419 = bitcast <4 x i64>* %418 to <8 x i32>*
  %420 = load <8 x i32>, <8 x i32>* %419, align 32
  %421 = add <8 x i32> %420, %415
  %422 = call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %421, i32 %409) #8
  %423 = mul <8 x i32> %422, <i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793>
  %424 = add <8 x i32> %423, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %425 = ashr <8 x i32> %424, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  store <8 x i32> %425, <8 x i32>* %419, align 32
  %426 = or i64 %417, 1
  %427 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %8, i64 0, i64 %426
  %428 = bitcast <4 x i64>* %427 to <8 x i32>*
  %429 = load <8 x i32>, <8 x i32>* %428, align 32
  %430 = add <8 x i32> %429, %415
  %431 = call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %430, i32 %409) #8
  %432 = mul <8 x i32> %431, <i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793>
  %433 = add <8 x i32> %432, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %434 = ashr <8 x i32> %433, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  store <8 x i32> %434, <8 x i32>* %428, align 32
  %435 = add nuw nsw i64 %417, 2
  %436 = icmp eq i64 %435, 32
  br i1 %436, label %456, label %416

437:                                              ; preds = %406, %437
  %438 = phi i64 [ %454, %437 ], [ 0, %406 ]
  %439 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %8, i64 0, i64 %438
  %440 = bitcast <4 x i64>* %439 to <8 x i32>*
  %441 = load <8 x i32>, <8 x i32>* %440, align 32
  %442 = call <8 x i32> @llvm.x86.avx2.pslli.d(<8 x i32> %441, i32 %408) #8
  %443 = mul <8 x i32> %442, <i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793>
  %444 = add <8 x i32> %443, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %445 = ashr <8 x i32> %444, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  store <8 x i32> %445, <8 x i32>* %440, align 32
  %446 = or i64 %438, 1
  %447 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %8, i64 0, i64 %446
  %448 = bitcast <4 x i64>* %447 to <8 x i32>*
  %449 = load <8 x i32>, <8 x i32>* %448, align 32
  %450 = call <8 x i32> @llvm.x86.avx2.pslli.d(<8 x i32> %449, i32 %408) #8
  %451 = mul <8 x i32> %450, <i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793>
  %452 = add <8 x i32> %451, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %453 = ashr <8 x i32> %452, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  store <8 x i32> %453, <8 x i32>* %448, align 32
  %454 = add nuw nsw i64 %438, 2
  %455 = icmp eq i64 %454, 32
  br i1 %455, label %456, label %437

456:                                              ; preds = %437, %416
  %457 = load i8, i8* %23, align 1
  %458 = sext i8 %457 to i32
  %459 = sub nsw i32 0, %458
  %460 = icmp slt i8 %457, 0
  br i1 %460, label %461, label %487

461:                                              ; preds = %456
  %462 = xor i32 %458, -1
  %463 = shl i32 1, %462
  %464 = insertelement <8 x i32> undef, i32 %463, i32 0
  %465 = shufflevector <8 x i32> %464, <8 x i32> undef, <8 x i32> zeroinitializer
  br label %466

466:                                              ; preds = %466, %461
  %467 = phi i64 [ 0, %461 ], [ %485, %466 ]
  %468 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 %467
  %469 = bitcast <4 x i64>* %468 to <8 x i32>*
  %470 = load <8 x i32>, <8 x i32>* %469, align 32
  %471 = add <8 x i32> %470, %465
  %472 = call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %471, i32 %459) #8
  %473 = mul <8 x i32> %472, <i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793>
  %474 = add <8 x i32> %473, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %475 = ashr <8 x i32> %474, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  store <8 x i32> %475, <8 x i32>* %469, align 32
  %476 = or i64 %467, 1
  %477 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 %476
  %478 = bitcast <4 x i64>* %477 to <8 x i32>*
  %479 = load <8 x i32>, <8 x i32>* %478, align 32
  %480 = add <8 x i32> %479, %465
  %481 = call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %480, i32 %459) #8
  %482 = mul <8 x i32> %481, <i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793>
  %483 = add <8 x i32> %482, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %484 = ashr <8 x i32> %483, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  store <8 x i32> %484, <8 x i32>* %478, align 32
  %485 = add nuw nsw i64 %467, 2
  %486 = icmp eq i64 %485, 32
  br i1 %486, label %506, label %466

487:                                              ; preds = %456, %487
  %488 = phi i64 [ %504, %487 ], [ 0, %456 ]
  %489 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 %488
  %490 = bitcast <4 x i64>* %489 to <8 x i32>*
  %491 = load <8 x i32>, <8 x i32>* %490, align 32
  %492 = call <8 x i32> @llvm.x86.avx2.pslli.d(<8 x i32> %491, i32 %458) #8
  %493 = mul <8 x i32> %492, <i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793>
  %494 = add <8 x i32> %493, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %495 = ashr <8 x i32> %494, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  store <8 x i32> %495, <8 x i32>* %490, align 32
  %496 = or i64 %488, 1
  %497 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 %496
  %498 = bitcast <4 x i64>* %497 to <8 x i32>*
  %499 = load <8 x i32>, <8 x i32>* %498, align 32
  %500 = call <8 x i32> @llvm.x86.avx2.pslli.d(<8 x i32> %499, i32 %458) #8
  %501 = mul <8 x i32> %500, <i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793>
  %502 = add <8 x i32> %501, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %503 = ashr <8 x i32> %502, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  store <8 x i32> %503, <8 x i32>* %498, align 32
  %504 = add nuw nsw i64 %488, 2
  %505 = icmp eq i64 %504, 32
  br i1 %505, label %506, label %487

506:                                              ; preds = %487, %466
  %507 = shl nsw i64 %402, 9
  %508 = getelementptr inbounds i32, i32* %1, i64 %507
  br label %545

509:                                              ; preds = %509, %401
  %510 = phi i64 [ 0, %401 ], [ %540, %509 ]
  %511 = shl nuw nsw i64 %510, 1
  %512 = getelementptr inbounds <4 x i64>, <4 x i64>* %404, i64 %510
  %513 = bitcast <4 x i64>* %512 to <8 x i16>*
  %514 = load <8 x i16>, <8 x i16>* %513, align 32
  %515 = sext <8 x i16> %514 to <8 x i32>
  %516 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %8, i64 0, i64 %510
  %517 = bitcast <4 x i64>* %516 to <8 x i32>*
  store <8 x i32> %515, <8 x i32>* %517, align 32
  %518 = or i64 %511, 1
  %519 = getelementptr inbounds <2 x i64>, <2 x i64>* %405, i64 %518
  %520 = bitcast <2 x i64>* %519 to <8 x i16>*
  %521 = load <8 x i16>, <8 x i16>* %520, align 16
  %522 = sext <8 x i16> %521 to <8 x i32>
  %523 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 %510
  %524 = bitcast <4 x i64>* %523 to <8 x i32>*
  store <8 x i32> %522, <8 x i32>* %524, align 32
  %525 = or i64 %510, 1
  %526 = shl nuw nsw i64 %525, 1
  %527 = getelementptr inbounds <4 x i64>, <4 x i64>* %404, i64 %525
  %528 = bitcast <4 x i64>* %527 to <8 x i16>*
  %529 = load <8 x i16>, <8 x i16>* %528, align 32
  %530 = sext <8 x i16> %529 to <8 x i32>
  %531 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %8, i64 0, i64 %525
  %532 = bitcast <4 x i64>* %531 to <8 x i32>*
  store <8 x i32> %530, <8 x i32>* %532, align 32
  %533 = or i64 %526, 1
  %534 = getelementptr inbounds <2 x i64>, <2 x i64>* %405, i64 %533
  %535 = bitcast <2 x i64>* %534 to <8 x i16>*
  %536 = load <8 x i16>, <8 x i16>* %535, align 16
  %537 = sext <8 x i16> %536 to <8 x i32>
  %538 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 %525
  %539 = bitcast <4 x i64>* %538 to <8 x i32>*
  store <8 x i32> %537, <8 x i32>* %539, align 32
  %540 = add nuw nsw i64 %510, 2
  %541 = icmp eq i64 %540, 32
  br i1 %541, label %406, label %509

542:                                              ; preds = %545
  call void @llvm.lifetime.end.p0i8(i64 1024, i8* nonnull %20) #8
  call void @llvm.lifetime.end.p0i8(i64 1024, i8* nonnull %19) #8
  %543 = add nuw nsw i64 %402, 1
  %544 = icmp eq i64 %543, 2
  br i1 %544, label %400, label %401

545:                                              ; preds = %545, %506
  %546 = phi i64 [ 0, %506 ], [ %692, %545 ]
  %547 = shl nsw i64 %546, 3
  %548 = getelementptr inbounds i32, i32* %508, i64 %547
  %549 = bitcast i32* %548 to <4 x i64>*
  %550 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %8, i64 0, i64 %547
  %551 = bitcast <4 x i64>* %550 to <8 x i32>*
  %552 = load <8 x i32>, <8 x i32>* %551, align 32
  %553 = getelementptr inbounds <4 x i64>, <4 x i64>* %550, i64 2
  %554 = bitcast <4 x i64>* %553 to <8 x i32>*
  %555 = load <8 x i32>, <8 x i32>* %554, align 32
  %556 = shufflevector <8 x i32> %552, <8 x i32> %555, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %557 = shufflevector <8 x i32> %552, <8 x i32> %555, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %558 = getelementptr inbounds <4 x i64>, <4 x i64>* %550, i64 1
  %559 = bitcast <4 x i64>* %558 to <8 x i32>*
  %560 = load <8 x i32>, <8 x i32>* %559, align 32
  %561 = getelementptr inbounds <4 x i64>, <4 x i64>* %550, i64 3
  %562 = bitcast <4 x i64>* %561 to <8 x i32>*
  %563 = load <8 x i32>, <8 x i32>* %562, align 32
  %564 = shufflevector <8 x i32> %560, <8 x i32> %563, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %565 = shufflevector <8 x i32> %560, <8 x i32> %563, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %566 = getelementptr inbounds <4 x i64>, <4 x i64>* %550, i64 4
  %567 = bitcast <4 x i64>* %566 to <8 x i32>*
  %568 = load <8 x i32>, <8 x i32>* %567, align 32
  %569 = getelementptr inbounds <4 x i64>, <4 x i64>* %550, i64 6
  %570 = bitcast <4 x i64>* %569 to <8 x i32>*
  %571 = load <8 x i32>, <8 x i32>* %570, align 32
  %572 = shufflevector <8 x i32> %568, <8 x i32> %571, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %573 = shufflevector <8 x i32> %568, <8 x i32> %571, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %574 = getelementptr inbounds <4 x i64>, <4 x i64>* %550, i64 5
  %575 = bitcast <4 x i64>* %574 to <8 x i32>*
  %576 = load <8 x i32>, <8 x i32>* %575, align 32
  %577 = getelementptr inbounds <4 x i64>, <4 x i64>* %550, i64 7
  %578 = bitcast <4 x i64>* %577 to <8 x i32>*
  %579 = load <8 x i32>, <8 x i32>* %578, align 32
  %580 = shufflevector <8 x i32> %576, <8 x i32> %579, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %581 = shufflevector <8 x i32> %576, <8 x i32> %579, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %582 = shufflevector <8 x i32> %556, <8 x i32> %564, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %583 = bitcast <8 x i32> %582 to <4 x i64>
  %584 = shufflevector <8 x i32> %556, <8 x i32> %564, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %585 = bitcast <8 x i32> %584 to <4 x i64>
  %586 = shufflevector <8 x i32> %557, <8 x i32> %565, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %587 = bitcast <8 x i32> %586 to <4 x i64>
  %588 = shufflevector <8 x i32> %557, <8 x i32> %565, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %589 = bitcast <8 x i32> %588 to <4 x i64>
  %590 = shufflevector <8 x i32> %572, <8 x i32> %580, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %591 = bitcast <8 x i32> %590 to <4 x i64>
  %592 = shufflevector <8 x i32> %572, <8 x i32> %580, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %593 = bitcast <8 x i32> %592 to <4 x i64>
  %594 = shufflevector <8 x i32> %573, <8 x i32> %581, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %595 = bitcast <8 x i32> %594 to <4 x i64>
  %596 = shufflevector <8 x i32> %573, <8 x i32> %581, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %597 = bitcast <8 x i32> %596 to <4 x i64>
  %598 = shufflevector <4 x i64> %583, <4 x i64> %591, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
  store <4 x i64> %598, <4 x i64>* %549, align 32
  %599 = shufflevector <4 x i64> %585, <4 x i64> %593, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
  %600 = getelementptr inbounds i32, i32* %548, i64 32
  %601 = bitcast i32* %600 to <4 x i64>*
  store <4 x i64> %599, <4 x i64>* %601, align 32
  %602 = shufflevector <4 x i64> %587, <4 x i64> %595, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
  %603 = getelementptr inbounds i32, i32* %548, i64 64
  %604 = bitcast i32* %603 to <4 x i64>*
  store <4 x i64> %602, <4 x i64>* %604, align 32
  %605 = shufflevector <4 x i64> %589, <4 x i64> %597, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
  %606 = getelementptr inbounds i32, i32* %548, i64 96
  %607 = bitcast i32* %606 to <4 x i64>*
  store <4 x i64> %605, <4 x i64>* %607, align 32
  %608 = shufflevector <4 x i64> %583, <4 x i64> %591, <4 x i32> <i32 2, i32 3, i32 6, i32 7>
  %609 = getelementptr inbounds i32, i32* %548, i64 128
  %610 = bitcast i32* %609 to <4 x i64>*
  store <4 x i64> %608, <4 x i64>* %610, align 32
  %611 = shufflevector <4 x i64> %585, <4 x i64> %593, <4 x i32> <i32 2, i32 3, i32 6, i32 7>
  %612 = getelementptr inbounds i32, i32* %548, i64 160
  %613 = bitcast i32* %612 to <4 x i64>*
  store <4 x i64> %611, <4 x i64>* %613, align 32
  %614 = shufflevector <4 x i64> %587, <4 x i64> %595, <4 x i32> <i32 2, i32 3, i32 6, i32 7>
  %615 = getelementptr inbounds i32, i32* %548, i64 192
  %616 = bitcast i32* %615 to <4 x i64>*
  store <4 x i64> %614, <4 x i64>* %616, align 32
  %617 = shufflevector <4 x i64> %589, <4 x i64> %597, <4 x i32> <i32 2, i32 3, i32 6, i32 7>
  %618 = getelementptr inbounds i32, i32* %548, i64 224
  %619 = bitcast i32* %618 to <4 x i64>*
  store <4 x i64> %617, <4 x i64>* %619, align 32
  %620 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 %547
  %621 = getelementptr inbounds i32, i32* %548, i64 256
  %622 = bitcast i32* %621 to <4 x i64>*
  %623 = bitcast <4 x i64>* %620 to <8 x i32>*
  %624 = load <8 x i32>, <8 x i32>* %623, align 32
  %625 = getelementptr inbounds <4 x i64>, <4 x i64>* %620, i64 2
  %626 = bitcast <4 x i64>* %625 to <8 x i32>*
  %627 = load <8 x i32>, <8 x i32>* %626, align 32
  %628 = shufflevector <8 x i32> %624, <8 x i32> %627, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %629 = shufflevector <8 x i32> %624, <8 x i32> %627, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %630 = getelementptr inbounds <4 x i64>, <4 x i64>* %620, i64 1
  %631 = bitcast <4 x i64>* %630 to <8 x i32>*
  %632 = load <8 x i32>, <8 x i32>* %631, align 32
  %633 = getelementptr inbounds <4 x i64>, <4 x i64>* %620, i64 3
  %634 = bitcast <4 x i64>* %633 to <8 x i32>*
  %635 = load <8 x i32>, <8 x i32>* %634, align 32
  %636 = shufflevector <8 x i32> %632, <8 x i32> %635, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %637 = shufflevector <8 x i32> %632, <8 x i32> %635, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %638 = getelementptr inbounds <4 x i64>, <4 x i64>* %620, i64 4
  %639 = bitcast <4 x i64>* %638 to <8 x i32>*
  %640 = load <8 x i32>, <8 x i32>* %639, align 32
  %641 = getelementptr inbounds <4 x i64>, <4 x i64>* %620, i64 6
  %642 = bitcast <4 x i64>* %641 to <8 x i32>*
  %643 = load <8 x i32>, <8 x i32>* %642, align 32
  %644 = shufflevector <8 x i32> %640, <8 x i32> %643, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %645 = shufflevector <8 x i32> %640, <8 x i32> %643, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %646 = getelementptr inbounds <4 x i64>, <4 x i64>* %620, i64 5
  %647 = bitcast <4 x i64>* %646 to <8 x i32>*
  %648 = load <8 x i32>, <8 x i32>* %647, align 32
  %649 = getelementptr inbounds <4 x i64>, <4 x i64>* %620, i64 7
  %650 = bitcast <4 x i64>* %649 to <8 x i32>*
  %651 = load <8 x i32>, <8 x i32>* %650, align 32
  %652 = shufflevector <8 x i32> %648, <8 x i32> %651, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %653 = shufflevector <8 x i32> %648, <8 x i32> %651, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %654 = shufflevector <8 x i32> %628, <8 x i32> %636, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %655 = bitcast <8 x i32> %654 to <4 x i64>
  %656 = shufflevector <8 x i32> %628, <8 x i32> %636, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %657 = bitcast <8 x i32> %656 to <4 x i64>
  %658 = shufflevector <8 x i32> %629, <8 x i32> %637, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %659 = bitcast <8 x i32> %658 to <4 x i64>
  %660 = shufflevector <8 x i32> %629, <8 x i32> %637, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %661 = bitcast <8 x i32> %660 to <4 x i64>
  %662 = shufflevector <8 x i32> %644, <8 x i32> %652, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %663 = bitcast <8 x i32> %662 to <4 x i64>
  %664 = shufflevector <8 x i32> %644, <8 x i32> %652, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %665 = bitcast <8 x i32> %664 to <4 x i64>
  %666 = shufflevector <8 x i32> %645, <8 x i32> %653, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %667 = bitcast <8 x i32> %666 to <4 x i64>
  %668 = shufflevector <8 x i32> %645, <8 x i32> %653, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %669 = bitcast <8 x i32> %668 to <4 x i64>
  %670 = shufflevector <4 x i64> %655, <4 x i64> %663, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
  store <4 x i64> %670, <4 x i64>* %622, align 32
  %671 = shufflevector <4 x i64> %657, <4 x i64> %665, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
  %672 = getelementptr inbounds i32, i32* %621, i64 32
  %673 = bitcast i32* %672 to <4 x i64>*
  store <4 x i64> %671, <4 x i64>* %673, align 32
  %674 = shufflevector <4 x i64> %659, <4 x i64> %667, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
  %675 = getelementptr inbounds i32, i32* %621, i64 64
  %676 = bitcast i32* %675 to <4 x i64>*
  store <4 x i64> %674, <4 x i64>* %676, align 32
  %677 = shufflevector <4 x i64> %661, <4 x i64> %669, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
  %678 = getelementptr inbounds i32, i32* %621, i64 96
  %679 = bitcast i32* %678 to <4 x i64>*
  store <4 x i64> %677, <4 x i64>* %679, align 32
  %680 = shufflevector <4 x i64> %655, <4 x i64> %663, <4 x i32> <i32 2, i32 3, i32 6, i32 7>
  %681 = getelementptr inbounds i32, i32* %621, i64 128
  %682 = bitcast i32* %681 to <4 x i64>*
  store <4 x i64> %680, <4 x i64>* %682, align 32
  %683 = shufflevector <4 x i64> %657, <4 x i64> %665, <4 x i32> <i32 2, i32 3, i32 6, i32 7>
  %684 = getelementptr inbounds i32, i32* %621, i64 160
  %685 = bitcast i32* %684 to <4 x i64>*
  store <4 x i64> %683, <4 x i64>* %685, align 32
  %686 = shufflevector <4 x i64> %659, <4 x i64> %667, <4 x i32> <i32 2, i32 3, i32 6, i32 7>
  %687 = getelementptr inbounds i32, i32* %621, i64 192
  %688 = bitcast i32* %687 to <4 x i64>*
  store <4 x i64> %686, <4 x i64>* %688, align 32
  %689 = shufflevector <4 x i64> %661, <4 x i64> %669, <4 x i32> <i32 2, i32 3, i32 6, i32 7>
  %690 = getelementptr inbounds i32, i32* %621, i64 224
  %691 = bitcast i32* %690 to <4 x i64>*
  store <4 x i64> %689, <4 x i64>* %691, align 32
  %692 = add nuw nsw i64 %546, 1
  %693 = icmp eq i64 %692, 4
  br i1 %693, label %542, label %545
}

; Function Attrs: nounwind ssp uwtable
define internal void @lowbd_fwd_txfm2d_64x32_avx2(i16* nocapture readonly, i32* nocapture, i32, i8 zeroext, i32) #3 {
  %6 = alloca [64 x <4 x i64>], align 32
  %7 = alloca [256 x <4 x i64>], align 32
  %8 = alloca [64 x <4 x i64>], align 32
  %9 = alloca [64 x <4 x i64>], align 32
  %10 = bitcast [64 x <4 x i64>]* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 2048, i8* nonnull %10) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %10, i8 -86, i64 2048, i1 false)
  %11 = bitcast [256 x <4 x i64>]* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 8192, i8* nonnull %11) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %11, i8 -86, i64 8192, i1 false)
  %12 = load i8*, i8** getelementptr inbounds ([19 x i8*], [19 x i8*]* @av1_fwd_txfm_shift_ls, i64 0, i64 12), align 16
  %13 = load i8, i8* getelementptr inbounds ([5 x [5 x i8]], [5 x [5 x i8]]* @av1_fwd_cos_bit_col, i64 0, i64 4, i64 3), align 1
  %14 = load i8, i8* getelementptr inbounds ([5 x [5 x i8]], [5 x [5 x i8]]* @av1_fwd_cos_bit_row, i64 0, i64 4, i64 3), align 1
  %15 = zext i8 %3 to i64
  %16 = getelementptr inbounds [16 x void (<4 x i64>*, <4 x i64>*, i8)*], [16 x void (<4 x i64>*, <4 x i64>*, i8)*]* @col_txfm16x32_arr, i64 0, i64 %15
  %17 = load void (<4 x i64>*, <4 x i64>*, i8)*, void (<4 x i64>*, <4 x i64>*, i8)** %16, align 8
  %18 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 0
  %19 = sext i32 %2 to i64
  %20 = getelementptr inbounds i8, i8* %12, i64 1
  br label %27

21:                                               ; preds = %185
  %22 = bitcast [64 x <4 x i64>]* %8 to i8*
  %23 = bitcast [64 x <4 x i64>]* %9 to i8*
  %24 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 0
  %25 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %9, i64 0, i64 0
  %26 = getelementptr inbounds i8, i8* %12, i64 2
  br label %404

27:                                               ; preds = %185, %5
  %28 = phi i64 [ 0, %5 ], [ %186, %185 ]
  %29 = shl nsw i64 %28, 4
  %30 = getelementptr inbounds i16, i16* %0, i64 %29
  br label %31

31:                                               ; preds = %31, %27
  %32 = phi i64 [ 0, %27 ], [ %56, %31 ]
  %33 = mul nsw i64 %32, %19
  %34 = getelementptr inbounds i16, i16* %30, i64 %33
  %35 = bitcast i16* %34 to <4 x i64>*
  %36 = load <4 x i64>, <4 x i64>* %35, align 32
  %37 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %32
  store <4 x i64> %36, <4 x i64>* %37, align 32
  %38 = or i64 %32, 1
  %39 = mul nsw i64 %38, %19
  %40 = getelementptr inbounds i16, i16* %30, i64 %39
  %41 = bitcast i16* %40 to <4 x i64>*
  %42 = load <4 x i64>, <4 x i64>* %41, align 32
  %43 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %38
  store <4 x i64> %42, <4 x i64>* %43, align 32
  %44 = or i64 %32, 2
  %45 = mul nsw i64 %44, %19
  %46 = getelementptr inbounds i16, i16* %30, i64 %45
  %47 = bitcast i16* %46 to <4 x i64>*
  %48 = load <4 x i64>, <4 x i64>* %47, align 32
  %49 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %44
  store <4 x i64> %48, <4 x i64>* %49, align 32
  %50 = or i64 %32, 3
  %51 = mul nsw i64 %50, %19
  %52 = getelementptr inbounds i16, i16* %30, i64 %51
  %53 = bitcast i16* %52 to <4 x i64>*
  %54 = load <4 x i64>, <4 x i64>* %53, align 32
  %55 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %50
  store <4 x i64> %54, <4 x i64>* %55, align 32
  %56 = add nuw nsw i64 %32, 4
  %57 = icmp eq i64 %56, 32
  br i1 %57, label %58, label %31

58:                                               ; preds = %31
  %59 = load i8, i8* %12, align 1
  %60 = sext i8 %59 to i32
  %61 = icmp slt i8 %59, 0
  br i1 %61, label %62, label %96

62:                                               ; preds = %58
  %63 = sub nsw i32 0, %60
  %64 = xor i32 %60, -1
  %65 = shl i32 1, %64
  %66 = trunc i32 %65 to i16
  %67 = insertelement <16 x i16> undef, i16 %66, i32 0
  %68 = shufflevector <16 x i16> %67, <16 x i16> undef, <16 x i32> zeroinitializer
  br label %69

69:                                               ; preds = %69, %62
  %70 = phi i64 [ 0, %62 ], [ %94, %69 ]
  %71 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %70
  %72 = bitcast <4 x i64>* %71 to <16 x i16>*
  %73 = load <16 x i16>, <16 x i16>* %72, align 32
  %74 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %73, <16 x i16> %68) #8
  %75 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %74, i32 %63) #8
  store <16 x i16> %75, <16 x i16>* %72, align 32
  %76 = or i64 %70, 1
  %77 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %76
  %78 = bitcast <4 x i64>* %77 to <16 x i16>*
  %79 = load <16 x i16>, <16 x i16>* %78, align 32
  %80 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %79, <16 x i16> %68) #8
  %81 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %80, i32 %63) #8
  store <16 x i16> %81, <16 x i16>* %78, align 32
  %82 = or i64 %70, 2
  %83 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %82
  %84 = bitcast <4 x i64>* %83 to <16 x i16>*
  %85 = load <16 x i16>, <16 x i16>* %84, align 32
  %86 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %85, <16 x i16> %68) #8
  %87 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %86, i32 %63) #8
  store <16 x i16> %87, <16 x i16>* %84, align 32
  %88 = or i64 %70, 3
  %89 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %88
  %90 = bitcast <4 x i64>* %89 to <16 x i16>*
  %91 = load <16 x i16>, <16 x i16>* %90, align 32
  %92 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %91, <16 x i16> %68) #8
  %93 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %92, i32 %63) #8
  store <16 x i16> %93, <16 x i16>* %90, align 32
  %94 = add nuw nsw i64 %70, 4
  %95 = icmp eq i64 %94, 32
  br i1 %95, label %121, label %69

96:                                               ; preds = %58
  %97 = icmp eq i8 %59, 0
  br i1 %97, label %121, label %98

98:                                               ; preds = %96, %98
  %99 = phi i64 [ %119, %98 ], [ 0, %96 ]
  %100 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %99
  %101 = bitcast <4 x i64>* %100 to <16 x i16>*
  %102 = load <16 x i16>, <16 x i16>* %101, align 32
  %103 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %102, i32 %60) #8
  store <16 x i16> %103, <16 x i16>* %101, align 32
  %104 = or i64 %99, 1
  %105 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %104
  %106 = bitcast <4 x i64>* %105 to <16 x i16>*
  %107 = load <16 x i16>, <16 x i16>* %106, align 32
  %108 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %107, i32 %60) #8
  store <16 x i16> %108, <16 x i16>* %106, align 32
  %109 = or i64 %99, 2
  %110 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %109
  %111 = bitcast <4 x i64>* %110 to <16 x i16>*
  %112 = load <16 x i16>, <16 x i16>* %111, align 32
  %113 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %112, i32 %60) #8
  store <16 x i16> %113, <16 x i16>* %111, align 32
  %114 = or i64 %99, 3
  %115 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %114
  %116 = bitcast <4 x i64>* %115 to <16 x i16>*
  %117 = load <16 x i16>, <16 x i16>* %116, align 32
  %118 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %117, i32 %60) #8
  store <16 x i16> %118, <16 x i16>* %116, align 32
  %119 = add nuw nsw i64 %99, 4
  %120 = icmp eq i64 %119, 32
  br i1 %120, label %121, label %98

121:                                              ; preds = %98, %69, %96
  call void %17(<4 x i64>* nonnull %18, <4 x i64>* nonnull %18, i8 signext %13) #8
  %122 = load i8, i8* %20, align 1
  %123 = sext i8 %122 to i32
  %124 = icmp slt i8 %122, 0
  br i1 %124, label %125, label %159

125:                                              ; preds = %121
  %126 = sub nsw i32 0, %123
  %127 = xor i32 %123, -1
  %128 = shl i32 1, %127
  %129 = trunc i32 %128 to i16
  %130 = insertelement <16 x i16> undef, i16 %129, i32 0
  %131 = shufflevector <16 x i16> %130, <16 x i16> undef, <16 x i32> zeroinitializer
  br label %132

132:                                              ; preds = %132, %125
  %133 = phi i64 [ 0, %125 ], [ %157, %132 ]
  %134 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %133
  %135 = bitcast <4 x i64>* %134 to <16 x i16>*
  %136 = load <16 x i16>, <16 x i16>* %135, align 32
  %137 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %136, <16 x i16> %131) #8
  %138 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %137, i32 %126) #8
  store <16 x i16> %138, <16 x i16>* %135, align 32
  %139 = or i64 %133, 1
  %140 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %139
  %141 = bitcast <4 x i64>* %140 to <16 x i16>*
  %142 = load <16 x i16>, <16 x i16>* %141, align 32
  %143 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %142, <16 x i16> %131) #8
  %144 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %143, i32 %126) #8
  store <16 x i16> %144, <16 x i16>* %141, align 32
  %145 = or i64 %133, 2
  %146 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %145
  %147 = bitcast <4 x i64>* %146 to <16 x i16>*
  %148 = load <16 x i16>, <16 x i16>* %147, align 32
  %149 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %148, <16 x i16> %131) #8
  %150 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %149, i32 %126) #8
  store <16 x i16> %150, <16 x i16>* %147, align 32
  %151 = or i64 %133, 3
  %152 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %151
  %153 = bitcast <4 x i64>* %152 to <16 x i16>*
  %154 = load <16 x i16>, <16 x i16>* %153, align 32
  %155 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %154, <16 x i16> %131) #8
  %156 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %155, i32 %126) #8
  store <16 x i16> %156, <16 x i16>* %153, align 32
  %157 = add nuw nsw i64 %133, 4
  %158 = icmp eq i64 %157, 32
  br i1 %158, label %161, label %132

159:                                              ; preds = %121
  %160 = icmp eq i8 %122, 0
  br i1 %160, label %161, label %162

161:                                              ; preds = %162, %132, %159
  br label %188

162:                                              ; preds = %159, %162
  %163 = phi i64 [ %183, %162 ], [ 0, %159 ]
  %164 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %163
  %165 = bitcast <4 x i64>* %164 to <16 x i16>*
  %166 = load <16 x i16>, <16 x i16>* %165, align 32
  %167 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %166, i32 %123) #8
  store <16 x i16> %167, <16 x i16>* %165, align 32
  %168 = or i64 %163, 1
  %169 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %168
  %170 = bitcast <4 x i64>* %169 to <16 x i16>*
  %171 = load <16 x i16>, <16 x i16>* %170, align 32
  %172 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %171, i32 %123) #8
  store <16 x i16> %172, <16 x i16>* %170, align 32
  %173 = or i64 %163, 2
  %174 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %173
  %175 = bitcast <4 x i64>* %174 to <16 x i16>*
  %176 = load <16 x i16>, <16 x i16>* %175, align 32
  %177 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %176, i32 %123) #8
  store <16 x i16> %177, <16 x i16>* %175, align 32
  %178 = or i64 %163, 3
  %179 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %178
  %180 = bitcast <4 x i64>* %179 to <16 x i16>*
  %181 = load <16 x i16>, <16 x i16>* %180, align 32
  %182 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %181, i32 %123) #8
  store <16 x i16> %182, <16 x i16>* %180, align 32
  %183 = add nuw nsw i64 %163, 4
  %184 = icmp eq i64 %183, 32
  br i1 %184, label %161, label %162

185:                                              ; preds = %188
  %186 = add nuw nsw i64 %28, 1
  %187 = icmp eq i64 %186, 4
  br i1 %187, label %21, label %27

188:                                              ; preds = %161, %188
  %189 = phi i64 [ %401, %188 ], [ 0, %161 ]
  %190 = shl nsw i64 %189, 4
  %191 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %190
  %192 = shl nsw i64 %189, 6
  %193 = getelementptr inbounds [256 x <4 x i64>], [256 x <4 x i64>]* %7, i64 0, i64 %192
  %194 = getelementptr inbounds <4 x i64>, <4 x i64>* %193, i64 %29
  %195 = bitcast <4 x i64>* %191 to <2 x i64>*
  %196 = load <2 x i64>, <2 x i64>* %195, align 32
  %197 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 8
  %198 = bitcast <4 x i64>* %197 to <2 x i64>*
  %199 = load <2 x i64>, <2 x i64>* %198, align 32
  %200 = shufflevector <2 x i64> %196, <2 x i64> %199, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %201 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 1
  %202 = bitcast <4 x i64>* %201 to <2 x i64>*
  %203 = load <2 x i64>, <2 x i64>* %202, align 32
  %204 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 9
  %205 = bitcast <4 x i64>* %204 to <2 x i64>*
  %206 = load <2 x i64>, <2 x i64>* %205, align 32
  %207 = shufflevector <2 x i64> %203, <2 x i64> %206, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %208 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 2
  %209 = bitcast <4 x i64>* %208 to <2 x i64>*
  %210 = load <2 x i64>, <2 x i64>* %209, align 32
  %211 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 10
  %212 = bitcast <4 x i64>* %211 to <2 x i64>*
  %213 = load <2 x i64>, <2 x i64>* %212, align 32
  %214 = shufflevector <2 x i64> %210, <2 x i64> %213, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %215 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 3
  %216 = bitcast <4 x i64>* %215 to <2 x i64>*
  %217 = load <2 x i64>, <2 x i64>* %216, align 32
  %218 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 11
  %219 = bitcast <4 x i64>* %218 to <2 x i64>*
  %220 = load <2 x i64>, <2 x i64>* %219, align 32
  %221 = shufflevector <2 x i64> %217, <2 x i64> %220, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %222 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 4
  %223 = bitcast <4 x i64>* %222 to <2 x i64>*
  %224 = load <2 x i64>, <2 x i64>* %223, align 32
  %225 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 12
  %226 = bitcast <4 x i64>* %225 to <2 x i64>*
  %227 = load <2 x i64>, <2 x i64>* %226, align 32
  %228 = shufflevector <2 x i64> %224, <2 x i64> %227, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %229 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 5
  %230 = bitcast <4 x i64>* %229 to <2 x i64>*
  %231 = load <2 x i64>, <2 x i64>* %230, align 32
  %232 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 13
  %233 = bitcast <4 x i64>* %232 to <2 x i64>*
  %234 = load <2 x i64>, <2 x i64>* %233, align 32
  %235 = shufflevector <2 x i64> %231, <2 x i64> %234, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %236 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 6
  %237 = bitcast <4 x i64>* %236 to <2 x i64>*
  %238 = load <2 x i64>, <2 x i64>* %237, align 32
  %239 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 14
  %240 = bitcast <4 x i64>* %239 to <2 x i64>*
  %241 = load <2 x i64>, <2 x i64>* %240, align 32
  %242 = shufflevector <2 x i64> %238, <2 x i64> %241, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %243 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 7
  %244 = bitcast <4 x i64>* %243 to <2 x i64>*
  %245 = load <2 x i64>, <2 x i64>* %244, align 32
  %246 = getelementptr inbounds <4 x i64>, <4 x i64>* %191, i64 15
  %247 = bitcast <4 x i64>* %246 to <2 x i64>*
  %248 = load <2 x i64>, <2 x i64>* %247, align 32
  %249 = shufflevector <2 x i64> %245, <2 x i64> %248, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %250 = getelementptr inbounds <2 x i64>, <2 x i64>* %195, i64 1
  %251 = load <2 x i64>, <2 x i64>* %250, align 16
  %252 = getelementptr inbounds <2 x i64>, <2 x i64>* %198, i64 1
  %253 = load <2 x i64>, <2 x i64>* %252, align 16
  %254 = shufflevector <2 x i64> %251, <2 x i64> %253, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %255 = getelementptr inbounds <2 x i64>, <2 x i64>* %202, i64 1
  %256 = load <2 x i64>, <2 x i64>* %255, align 16
  %257 = getelementptr inbounds <2 x i64>, <2 x i64>* %205, i64 1
  %258 = load <2 x i64>, <2 x i64>* %257, align 16
  %259 = shufflevector <2 x i64> %256, <2 x i64> %258, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %260 = getelementptr inbounds <2 x i64>, <2 x i64>* %209, i64 1
  %261 = load <2 x i64>, <2 x i64>* %260, align 16
  %262 = getelementptr inbounds <2 x i64>, <2 x i64>* %212, i64 1
  %263 = load <2 x i64>, <2 x i64>* %262, align 16
  %264 = shufflevector <2 x i64> %261, <2 x i64> %263, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %265 = getelementptr inbounds <2 x i64>, <2 x i64>* %216, i64 1
  %266 = load <2 x i64>, <2 x i64>* %265, align 16
  %267 = getelementptr inbounds <2 x i64>, <2 x i64>* %219, i64 1
  %268 = load <2 x i64>, <2 x i64>* %267, align 16
  %269 = shufflevector <2 x i64> %266, <2 x i64> %268, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %270 = getelementptr inbounds <2 x i64>, <2 x i64>* %223, i64 1
  %271 = load <2 x i64>, <2 x i64>* %270, align 16
  %272 = getelementptr inbounds <2 x i64>, <2 x i64>* %226, i64 1
  %273 = load <2 x i64>, <2 x i64>* %272, align 16
  %274 = shufflevector <2 x i64> %271, <2 x i64> %273, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %275 = getelementptr inbounds <2 x i64>, <2 x i64>* %230, i64 1
  %276 = load <2 x i64>, <2 x i64>* %275, align 16
  %277 = getelementptr inbounds <2 x i64>, <2 x i64>* %233, i64 1
  %278 = load <2 x i64>, <2 x i64>* %277, align 16
  %279 = shufflevector <2 x i64> %276, <2 x i64> %278, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %280 = getelementptr inbounds <2 x i64>, <2 x i64>* %237, i64 1
  %281 = load <2 x i64>, <2 x i64>* %280, align 16
  %282 = getelementptr inbounds <2 x i64>, <2 x i64>* %240, i64 1
  %283 = load <2 x i64>, <2 x i64>* %282, align 16
  %284 = shufflevector <2 x i64> %281, <2 x i64> %283, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %285 = getelementptr inbounds <2 x i64>, <2 x i64>* %244, i64 1
  %286 = load <2 x i64>, <2 x i64>* %285, align 16
  %287 = getelementptr inbounds <2 x i64>, <2 x i64>* %247, i64 1
  %288 = load <2 x i64>, <2 x i64>* %287, align 16
  %289 = shufflevector <2 x i64> %286, <2 x i64> %288, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %290 = bitcast <4 x i64> %200 to <16 x i16>
  %291 = bitcast <4 x i64> %207 to <16 x i16>
  %292 = shufflevector <16 x i16> %290, <16 x i16> %291, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %293 = shufflevector <16 x i16> %290, <16 x i16> %291, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %294 = bitcast <4 x i64> %214 to <16 x i16>
  %295 = bitcast <4 x i64> %221 to <16 x i16>
  %296 = shufflevector <16 x i16> %294, <16 x i16> %295, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %297 = shufflevector <16 x i16> %294, <16 x i16> %295, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %298 = bitcast <4 x i64> %228 to <16 x i16>
  %299 = bitcast <4 x i64> %235 to <16 x i16>
  %300 = shufflevector <16 x i16> %298, <16 x i16> %299, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %301 = shufflevector <16 x i16> %298, <16 x i16> %299, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %302 = bitcast <4 x i64> %242 to <16 x i16>
  %303 = bitcast <4 x i64> %249 to <16 x i16>
  %304 = shufflevector <16 x i16> %302, <16 x i16> %303, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %305 = shufflevector <16 x i16> %302, <16 x i16> %303, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %306 = bitcast <16 x i16> %292 to <8 x i32>
  %307 = bitcast <16 x i16> %296 to <8 x i32>
  %308 = shufflevector <8 x i32> %306, <8 x i32> %307, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %309 = shufflevector <8 x i32> %306, <8 x i32> %307, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %310 = bitcast <16 x i16> %300 to <8 x i32>
  %311 = bitcast <16 x i16> %304 to <8 x i32>
  %312 = shufflevector <8 x i32> %310, <8 x i32> %311, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %313 = shufflevector <8 x i32> %310, <8 x i32> %311, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %314 = bitcast <16 x i16> %293 to <8 x i32>
  %315 = bitcast <16 x i16> %297 to <8 x i32>
  %316 = shufflevector <8 x i32> %314, <8 x i32> %315, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %317 = shufflevector <8 x i32> %314, <8 x i32> %315, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %318 = bitcast <16 x i16> %301 to <8 x i32>
  %319 = bitcast <16 x i16> %305 to <8 x i32>
  %320 = shufflevector <8 x i32> %318, <8 x i32> %319, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %321 = shufflevector <8 x i32> %318, <8 x i32> %319, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %322 = bitcast <8 x i32> %308 to <4 x i64>
  %323 = bitcast <8 x i32> %312 to <4 x i64>
  %324 = shufflevector <4 x i64> %322, <4 x i64> %323, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %324, <4 x i64>* %194, align 32
  %325 = shufflevector <4 x i64> %322, <4 x i64> %323, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %326 = getelementptr inbounds <4 x i64>, <4 x i64>* %194, i64 1
  store <4 x i64> %325, <4 x i64>* %326, align 32
  %327 = bitcast <8 x i32> %316 to <4 x i64>
  %328 = bitcast <8 x i32> %320 to <4 x i64>
  %329 = shufflevector <4 x i64> %327, <4 x i64> %328, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %330 = getelementptr inbounds <4 x i64>, <4 x i64>* %194, i64 4
  store <4 x i64> %329, <4 x i64>* %330, align 32
  %331 = shufflevector <4 x i64> %327, <4 x i64> %328, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %332 = getelementptr inbounds <4 x i64>, <4 x i64>* %194, i64 5
  store <4 x i64> %331, <4 x i64>* %332, align 32
  %333 = bitcast <8 x i32> %309 to <4 x i64>
  %334 = bitcast <8 x i32> %313 to <4 x i64>
  %335 = shufflevector <4 x i64> %333, <4 x i64> %334, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %336 = getelementptr inbounds <4 x i64>, <4 x i64>* %194, i64 2
  store <4 x i64> %335, <4 x i64>* %336, align 32
  %337 = shufflevector <4 x i64> %333, <4 x i64> %334, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %338 = getelementptr inbounds <4 x i64>, <4 x i64>* %194, i64 3
  store <4 x i64> %337, <4 x i64>* %338, align 32
  %339 = bitcast <8 x i32> %317 to <4 x i64>
  %340 = bitcast <8 x i32> %321 to <4 x i64>
  %341 = shufflevector <4 x i64> %339, <4 x i64> %340, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %342 = getelementptr inbounds <4 x i64>, <4 x i64>* %194, i64 6
  store <4 x i64> %341, <4 x i64>* %342, align 32
  %343 = shufflevector <4 x i64> %339, <4 x i64> %340, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %344 = getelementptr inbounds <4 x i64>, <4 x i64>* %194, i64 7
  store <4 x i64> %343, <4 x i64>* %344, align 32
  %345 = getelementptr inbounds <4 x i64>, <4 x i64>* %194, i64 8
  %346 = bitcast <4 x i64> %254 to <16 x i16>
  %347 = bitcast <4 x i64> %259 to <16 x i16>
  %348 = shufflevector <16 x i16> %346, <16 x i16> %347, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %349 = shufflevector <16 x i16> %346, <16 x i16> %347, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %350 = bitcast <4 x i64> %264 to <16 x i16>
  %351 = bitcast <4 x i64> %269 to <16 x i16>
  %352 = shufflevector <16 x i16> %350, <16 x i16> %351, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %353 = shufflevector <16 x i16> %350, <16 x i16> %351, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %354 = bitcast <4 x i64> %274 to <16 x i16>
  %355 = bitcast <4 x i64> %279 to <16 x i16>
  %356 = shufflevector <16 x i16> %354, <16 x i16> %355, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %357 = shufflevector <16 x i16> %354, <16 x i16> %355, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %358 = bitcast <4 x i64> %284 to <16 x i16>
  %359 = bitcast <4 x i64> %289 to <16 x i16>
  %360 = shufflevector <16 x i16> %358, <16 x i16> %359, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %361 = shufflevector <16 x i16> %358, <16 x i16> %359, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %362 = bitcast <16 x i16> %348 to <8 x i32>
  %363 = bitcast <16 x i16> %352 to <8 x i32>
  %364 = shufflevector <8 x i32> %362, <8 x i32> %363, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %365 = shufflevector <8 x i32> %362, <8 x i32> %363, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %366 = bitcast <16 x i16> %356 to <8 x i32>
  %367 = bitcast <16 x i16> %360 to <8 x i32>
  %368 = shufflevector <8 x i32> %366, <8 x i32> %367, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %369 = shufflevector <8 x i32> %366, <8 x i32> %367, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %370 = bitcast <16 x i16> %349 to <8 x i32>
  %371 = bitcast <16 x i16> %353 to <8 x i32>
  %372 = shufflevector <8 x i32> %370, <8 x i32> %371, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %373 = shufflevector <8 x i32> %370, <8 x i32> %371, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %374 = bitcast <16 x i16> %357 to <8 x i32>
  %375 = bitcast <16 x i16> %361 to <8 x i32>
  %376 = shufflevector <8 x i32> %374, <8 x i32> %375, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %377 = shufflevector <8 x i32> %374, <8 x i32> %375, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %378 = bitcast <8 x i32> %364 to <4 x i64>
  %379 = bitcast <8 x i32> %368 to <4 x i64>
  %380 = shufflevector <4 x i64> %378, <4 x i64> %379, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %380, <4 x i64>* %345, align 32
  %381 = shufflevector <4 x i64> %378, <4 x i64> %379, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %382 = getelementptr inbounds <4 x i64>, <4 x i64>* %194, i64 9
  store <4 x i64> %381, <4 x i64>* %382, align 32
  %383 = bitcast <8 x i32> %372 to <4 x i64>
  %384 = bitcast <8 x i32> %376 to <4 x i64>
  %385 = shufflevector <4 x i64> %383, <4 x i64> %384, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %386 = getelementptr inbounds <4 x i64>, <4 x i64>* %194, i64 12
  store <4 x i64> %385, <4 x i64>* %386, align 32
  %387 = shufflevector <4 x i64> %383, <4 x i64> %384, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %388 = getelementptr inbounds <4 x i64>, <4 x i64>* %194, i64 13
  store <4 x i64> %387, <4 x i64>* %388, align 32
  %389 = bitcast <8 x i32> %365 to <4 x i64>
  %390 = bitcast <8 x i32> %369 to <4 x i64>
  %391 = shufflevector <4 x i64> %389, <4 x i64> %390, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %392 = getelementptr inbounds <4 x i64>, <4 x i64>* %194, i64 10
  store <4 x i64> %391, <4 x i64>* %392, align 32
  %393 = shufflevector <4 x i64> %389, <4 x i64> %390, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %394 = getelementptr inbounds <4 x i64>, <4 x i64>* %194, i64 11
  store <4 x i64> %393, <4 x i64>* %394, align 32
  %395 = bitcast <8 x i32> %373 to <4 x i64>
  %396 = bitcast <8 x i32> %377 to <4 x i64>
  %397 = shufflevector <4 x i64> %395, <4 x i64> %396, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %398 = getelementptr inbounds <4 x i64>, <4 x i64>* %194, i64 14
  store <4 x i64> %397, <4 x i64>* %398, align 32
  %399 = shufflevector <4 x i64> %395, <4 x i64> %396, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %400 = getelementptr inbounds <4 x i64>, <4 x i64>* %194, i64 15
  store <4 x i64> %399, <4 x i64>* %400, align 32
  %401 = add nuw nsw i64 %189, 1
  %402 = icmp eq i64 %401, 2
  br i1 %402, label %185, label %188

403:                                              ; preds = %545
  call void @llvm.lifetime.end.p0i8(i64 8192, i8* nonnull %11) #8
  call void @llvm.lifetime.end.p0i8(i64 2048, i8* nonnull %10) #8
  ret void

404:                                              ; preds = %545, %21
  %405 = phi i64 [ 0, %21 ], [ %546, %545 ]
  call void @llvm.lifetime.start.p0i8(i64 2048, i8* nonnull %22) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %22, i8 -86, i64 2048, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 2048, i8* nonnull %23) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %23, i8 -86, i64 2048, i1 false)
  %406 = shl nsw i64 %405, 6
  %407 = getelementptr inbounds [256 x <4 x i64>], [256 x <4 x i64>]* %7, i64 0, i64 %406
  %408 = bitcast <4 x i64>* %407 to <2 x i64>*
  br label %512

409:                                              ; preds = %512
  call fastcc void @fdct64_new_avx2(<4 x i64>* nonnull %24, <4 x i64>* nonnull %24, i8 signext %14)
  call fastcc void @fdct64_new_avx2(<4 x i64>* nonnull %25, <4 x i64>* nonnull %25, i8 signext %14)
  %410 = load i8, i8* %26, align 1
  %411 = sext i8 %410 to i32
  %412 = sub nsw i32 0, %411
  %413 = icmp slt i8 %410, 0
  br i1 %413, label %414, label %440

414:                                              ; preds = %409
  %415 = xor i32 %411, -1
  %416 = shl i32 1, %415
  %417 = insertelement <8 x i32> undef, i32 %416, i32 0
  %418 = shufflevector <8 x i32> %417, <8 x i32> undef, <8 x i32> zeroinitializer
  br label %419

419:                                              ; preds = %419, %414
  %420 = phi i64 [ 0, %414 ], [ %438, %419 ]
  %421 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %420
  %422 = bitcast <4 x i64>* %421 to <8 x i32>*
  %423 = load <8 x i32>, <8 x i32>* %422, align 32
  %424 = add <8 x i32> %423, %418
  %425 = call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %424, i32 %412) #8
  %426 = mul <8 x i32> %425, <i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793>
  %427 = add <8 x i32> %426, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %428 = ashr <8 x i32> %427, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  store <8 x i32> %428, <8 x i32>* %422, align 32
  %429 = or i64 %420, 1
  %430 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %429
  %431 = bitcast <4 x i64>* %430 to <8 x i32>*
  %432 = load <8 x i32>, <8 x i32>* %431, align 32
  %433 = add <8 x i32> %432, %418
  %434 = call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %433, i32 %412) #8
  %435 = mul <8 x i32> %434, <i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793>
  %436 = add <8 x i32> %435, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %437 = ashr <8 x i32> %436, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  store <8 x i32> %437, <8 x i32>* %431, align 32
  %438 = add nuw nsw i64 %420, 2
  %439 = icmp eq i64 %438, 32
  br i1 %439, label %459, label %419

440:                                              ; preds = %409, %440
  %441 = phi i64 [ %457, %440 ], [ 0, %409 ]
  %442 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %441
  %443 = bitcast <4 x i64>* %442 to <8 x i32>*
  %444 = load <8 x i32>, <8 x i32>* %443, align 32
  %445 = call <8 x i32> @llvm.x86.avx2.pslli.d(<8 x i32> %444, i32 %411) #8
  %446 = mul <8 x i32> %445, <i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793>
  %447 = add <8 x i32> %446, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %448 = ashr <8 x i32> %447, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  store <8 x i32> %448, <8 x i32>* %443, align 32
  %449 = or i64 %441, 1
  %450 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %449
  %451 = bitcast <4 x i64>* %450 to <8 x i32>*
  %452 = load <8 x i32>, <8 x i32>* %451, align 32
  %453 = call <8 x i32> @llvm.x86.avx2.pslli.d(<8 x i32> %452, i32 %411) #8
  %454 = mul <8 x i32> %453, <i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793>
  %455 = add <8 x i32> %454, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %456 = ashr <8 x i32> %455, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  store <8 x i32> %456, <8 x i32>* %451, align 32
  %457 = add nuw nsw i64 %441, 2
  %458 = icmp eq i64 %457, 32
  br i1 %458, label %459, label %440

459:                                              ; preds = %440, %419
  %460 = load i8, i8* %26, align 1
  %461 = sext i8 %460 to i32
  %462 = sub nsw i32 0, %461
  %463 = icmp slt i8 %460, 0
  br i1 %463, label %464, label %490

464:                                              ; preds = %459
  %465 = xor i32 %461, -1
  %466 = shl i32 1, %465
  %467 = insertelement <8 x i32> undef, i32 %466, i32 0
  %468 = shufflevector <8 x i32> %467, <8 x i32> undef, <8 x i32> zeroinitializer
  br label %469

469:                                              ; preds = %469, %464
  %470 = phi i64 [ 0, %464 ], [ %488, %469 ]
  %471 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %9, i64 0, i64 %470
  %472 = bitcast <4 x i64>* %471 to <8 x i32>*
  %473 = load <8 x i32>, <8 x i32>* %472, align 32
  %474 = add <8 x i32> %473, %468
  %475 = call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %474, i32 %462) #8
  %476 = mul <8 x i32> %475, <i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793>
  %477 = add <8 x i32> %476, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %478 = ashr <8 x i32> %477, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  store <8 x i32> %478, <8 x i32>* %472, align 32
  %479 = or i64 %470, 1
  %480 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %9, i64 0, i64 %479
  %481 = bitcast <4 x i64>* %480 to <8 x i32>*
  %482 = load <8 x i32>, <8 x i32>* %481, align 32
  %483 = add <8 x i32> %482, %468
  %484 = call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %483, i32 %462) #8
  %485 = mul <8 x i32> %484, <i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793>
  %486 = add <8 x i32> %485, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %487 = ashr <8 x i32> %486, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  store <8 x i32> %487, <8 x i32>* %481, align 32
  %488 = add nuw nsw i64 %470, 2
  %489 = icmp eq i64 %488, 32
  br i1 %489, label %509, label %469

490:                                              ; preds = %459, %490
  %491 = phi i64 [ %507, %490 ], [ 0, %459 ]
  %492 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %9, i64 0, i64 %491
  %493 = bitcast <4 x i64>* %492 to <8 x i32>*
  %494 = load <8 x i32>, <8 x i32>* %493, align 32
  %495 = call <8 x i32> @llvm.x86.avx2.pslli.d(<8 x i32> %494, i32 %461) #8
  %496 = mul <8 x i32> %495, <i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793>
  %497 = add <8 x i32> %496, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %498 = ashr <8 x i32> %497, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  store <8 x i32> %498, <8 x i32>* %493, align 32
  %499 = or i64 %491, 1
  %500 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %9, i64 0, i64 %499
  %501 = bitcast <4 x i64>* %500 to <8 x i32>*
  %502 = load <8 x i32>, <8 x i32>* %501, align 32
  %503 = call <8 x i32> @llvm.x86.avx2.pslli.d(<8 x i32> %502, i32 %461) #8
  %504 = mul <8 x i32> %503, <i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793, i32 5793>
  %505 = add <8 x i32> %504, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %506 = ashr <8 x i32> %505, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  store <8 x i32> %506, <8 x i32>* %501, align 32
  %507 = add nuw nsw i64 %491, 2
  %508 = icmp eq i64 %507, 32
  br i1 %508, label %509, label %490

509:                                              ; preds = %490, %469
  %510 = shl nsw i64 %405, 9
  %511 = getelementptr inbounds i32, i32* %1, i64 %510
  br label %548

512:                                              ; preds = %512, %404
  %513 = phi i64 [ 0, %404 ], [ %543, %512 ]
  %514 = shl nuw nsw i64 %513, 1
  %515 = getelementptr inbounds <4 x i64>, <4 x i64>* %407, i64 %513
  %516 = bitcast <4 x i64>* %515 to <8 x i16>*
  %517 = load <8 x i16>, <8 x i16>* %516, align 32
  %518 = sext <8 x i16> %517 to <8 x i32>
  %519 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %513
  %520 = bitcast <4 x i64>* %519 to <8 x i32>*
  store <8 x i32> %518, <8 x i32>* %520, align 32
  %521 = or i64 %514, 1
  %522 = getelementptr inbounds <2 x i64>, <2 x i64>* %408, i64 %521
  %523 = bitcast <2 x i64>* %522 to <8 x i16>*
  %524 = load <8 x i16>, <8 x i16>* %523, align 16
  %525 = sext <8 x i16> %524 to <8 x i32>
  %526 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %9, i64 0, i64 %513
  %527 = bitcast <4 x i64>* %526 to <8 x i32>*
  store <8 x i32> %525, <8 x i32>* %527, align 32
  %528 = or i64 %513, 1
  %529 = shl nuw nsw i64 %528, 1
  %530 = getelementptr inbounds <4 x i64>, <4 x i64>* %407, i64 %528
  %531 = bitcast <4 x i64>* %530 to <8 x i16>*
  %532 = load <8 x i16>, <8 x i16>* %531, align 32
  %533 = sext <8 x i16> %532 to <8 x i32>
  %534 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %528
  %535 = bitcast <4 x i64>* %534 to <8 x i32>*
  store <8 x i32> %533, <8 x i32>* %535, align 32
  %536 = or i64 %529, 1
  %537 = getelementptr inbounds <2 x i64>, <2 x i64>* %408, i64 %536
  %538 = bitcast <2 x i64>* %537 to <8 x i16>*
  %539 = load <8 x i16>, <8 x i16>* %538, align 16
  %540 = sext <8 x i16> %539 to <8 x i32>
  %541 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %9, i64 0, i64 %528
  %542 = bitcast <4 x i64>* %541 to <8 x i32>*
  store <8 x i32> %540, <8 x i32>* %542, align 32
  %543 = add nuw nsw i64 %513, 2
  %544 = icmp eq i64 %543, 64
  br i1 %544, label %409, label %512

545:                                              ; preds = %548
  call void @llvm.lifetime.end.p0i8(i64 2048, i8* nonnull %23) #8
  call void @llvm.lifetime.end.p0i8(i64 2048, i8* nonnull %22) #8
  %546 = add nuw nsw i64 %405, 1
  %547 = icmp eq i64 %546, 2
  br i1 %547, label %403, label %404

548:                                              ; preds = %548, %509
  %549 = phi i64 [ 0, %509 ], [ %695, %548 ]
  %550 = shl nsw i64 %549, 3
  %551 = getelementptr inbounds i32, i32* %511, i64 %550
  %552 = bitcast i32* %551 to <4 x i64>*
  %553 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %550
  %554 = bitcast <4 x i64>* %553 to <8 x i32>*
  %555 = load <8 x i32>, <8 x i32>* %554, align 32
  %556 = getelementptr inbounds <4 x i64>, <4 x i64>* %553, i64 2
  %557 = bitcast <4 x i64>* %556 to <8 x i32>*
  %558 = load <8 x i32>, <8 x i32>* %557, align 32
  %559 = shufflevector <8 x i32> %555, <8 x i32> %558, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %560 = shufflevector <8 x i32> %555, <8 x i32> %558, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %561 = getelementptr inbounds <4 x i64>, <4 x i64>* %553, i64 1
  %562 = bitcast <4 x i64>* %561 to <8 x i32>*
  %563 = load <8 x i32>, <8 x i32>* %562, align 32
  %564 = getelementptr inbounds <4 x i64>, <4 x i64>* %553, i64 3
  %565 = bitcast <4 x i64>* %564 to <8 x i32>*
  %566 = load <8 x i32>, <8 x i32>* %565, align 32
  %567 = shufflevector <8 x i32> %563, <8 x i32> %566, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %568 = shufflevector <8 x i32> %563, <8 x i32> %566, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %569 = getelementptr inbounds <4 x i64>, <4 x i64>* %553, i64 4
  %570 = bitcast <4 x i64>* %569 to <8 x i32>*
  %571 = load <8 x i32>, <8 x i32>* %570, align 32
  %572 = getelementptr inbounds <4 x i64>, <4 x i64>* %553, i64 6
  %573 = bitcast <4 x i64>* %572 to <8 x i32>*
  %574 = load <8 x i32>, <8 x i32>* %573, align 32
  %575 = shufflevector <8 x i32> %571, <8 x i32> %574, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %576 = shufflevector <8 x i32> %571, <8 x i32> %574, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %577 = getelementptr inbounds <4 x i64>, <4 x i64>* %553, i64 5
  %578 = bitcast <4 x i64>* %577 to <8 x i32>*
  %579 = load <8 x i32>, <8 x i32>* %578, align 32
  %580 = getelementptr inbounds <4 x i64>, <4 x i64>* %553, i64 7
  %581 = bitcast <4 x i64>* %580 to <8 x i32>*
  %582 = load <8 x i32>, <8 x i32>* %581, align 32
  %583 = shufflevector <8 x i32> %579, <8 x i32> %582, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %584 = shufflevector <8 x i32> %579, <8 x i32> %582, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %585 = shufflevector <8 x i32> %559, <8 x i32> %567, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %586 = bitcast <8 x i32> %585 to <4 x i64>
  %587 = shufflevector <8 x i32> %559, <8 x i32> %567, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %588 = bitcast <8 x i32> %587 to <4 x i64>
  %589 = shufflevector <8 x i32> %560, <8 x i32> %568, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %590 = bitcast <8 x i32> %589 to <4 x i64>
  %591 = shufflevector <8 x i32> %560, <8 x i32> %568, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %592 = bitcast <8 x i32> %591 to <4 x i64>
  %593 = shufflevector <8 x i32> %575, <8 x i32> %583, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %594 = bitcast <8 x i32> %593 to <4 x i64>
  %595 = shufflevector <8 x i32> %575, <8 x i32> %583, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %596 = bitcast <8 x i32> %595 to <4 x i64>
  %597 = shufflevector <8 x i32> %576, <8 x i32> %584, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %598 = bitcast <8 x i32> %597 to <4 x i64>
  %599 = shufflevector <8 x i32> %576, <8 x i32> %584, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %600 = bitcast <8 x i32> %599 to <4 x i64>
  %601 = shufflevector <4 x i64> %586, <4 x i64> %594, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
  store <4 x i64> %601, <4 x i64>* %552, align 32
  %602 = shufflevector <4 x i64> %588, <4 x i64> %596, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
  %603 = getelementptr inbounds i32, i32* %551, i64 32
  %604 = bitcast i32* %603 to <4 x i64>*
  store <4 x i64> %602, <4 x i64>* %604, align 32
  %605 = shufflevector <4 x i64> %590, <4 x i64> %598, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
  %606 = getelementptr inbounds i32, i32* %551, i64 64
  %607 = bitcast i32* %606 to <4 x i64>*
  store <4 x i64> %605, <4 x i64>* %607, align 32
  %608 = shufflevector <4 x i64> %592, <4 x i64> %600, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
  %609 = getelementptr inbounds i32, i32* %551, i64 96
  %610 = bitcast i32* %609 to <4 x i64>*
  store <4 x i64> %608, <4 x i64>* %610, align 32
  %611 = shufflevector <4 x i64> %586, <4 x i64> %594, <4 x i32> <i32 2, i32 3, i32 6, i32 7>
  %612 = getelementptr inbounds i32, i32* %551, i64 128
  %613 = bitcast i32* %612 to <4 x i64>*
  store <4 x i64> %611, <4 x i64>* %613, align 32
  %614 = shufflevector <4 x i64> %588, <4 x i64> %596, <4 x i32> <i32 2, i32 3, i32 6, i32 7>
  %615 = getelementptr inbounds i32, i32* %551, i64 160
  %616 = bitcast i32* %615 to <4 x i64>*
  store <4 x i64> %614, <4 x i64>* %616, align 32
  %617 = shufflevector <4 x i64> %590, <4 x i64> %598, <4 x i32> <i32 2, i32 3, i32 6, i32 7>
  %618 = getelementptr inbounds i32, i32* %551, i64 192
  %619 = bitcast i32* %618 to <4 x i64>*
  store <4 x i64> %617, <4 x i64>* %619, align 32
  %620 = shufflevector <4 x i64> %592, <4 x i64> %600, <4 x i32> <i32 2, i32 3, i32 6, i32 7>
  %621 = getelementptr inbounds i32, i32* %551, i64 224
  %622 = bitcast i32* %621 to <4 x i64>*
  store <4 x i64> %620, <4 x i64>* %622, align 32
  %623 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %9, i64 0, i64 %550
  %624 = getelementptr inbounds i32, i32* %551, i64 256
  %625 = bitcast i32* %624 to <4 x i64>*
  %626 = bitcast <4 x i64>* %623 to <8 x i32>*
  %627 = load <8 x i32>, <8 x i32>* %626, align 32
  %628 = getelementptr inbounds <4 x i64>, <4 x i64>* %623, i64 2
  %629 = bitcast <4 x i64>* %628 to <8 x i32>*
  %630 = load <8 x i32>, <8 x i32>* %629, align 32
  %631 = shufflevector <8 x i32> %627, <8 x i32> %630, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %632 = shufflevector <8 x i32> %627, <8 x i32> %630, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %633 = getelementptr inbounds <4 x i64>, <4 x i64>* %623, i64 1
  %634 = bitcast <4 x i64>* %633 to <8 x i32>*
  %635 = load <8 x i32>, <8 x i32>* %634, align 32
  %636 = getelementptr inbounds <4 x i64>, <4 x i64>* %623, i64 3
  %637 = bitcast <4 x i64>* %636 to <8 x i32>*
  %638 = load <8 x i32>, <8 x i32>* %637, align 32
  %639 = shufflevector <8 x i32> %635, <8 x i32> %638, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %640 = shufflevector <8 x i32> %635, <8 x i32> %638, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %641 = getelementptr inbounds <4 x i64>, <4 x i64>* %623, i64 4
  %642 = bitcast <4 x i64>* %641 to <8 x i32>*
  %643 = load <8 x i32>, <8 x i32>* %642, align 32
  %644 = getelementptr inbounds <4 x i64>, <4 x i64>* %623, i64 6
  %645 = bitcast <4 x i64>* %644 to <8 x i32>*
  %646 = load <8 x i32>, <8 x i32>* %645, align 32
  %647 = shufflevector <8 x i32> %643, <8 x i32> %646, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %648 = shufflevector <8 x i32> %643, <8 x i32> %646, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %649 = getelementptr inbounds <4 x i64>, <4 x i64>* %623, i64 5
  %650 = bitcast <4 x i64>* %649 to <8 x i32>*
  %651 = load <8 x i32>, <8 x i32>* %650, align 32
  %652 = getelementptr inbounds <4 x i64>, <4 x i64>* %623, i64 7
  %653 = bitcast <4 x i64>* %652 to <8 x i32>*
  %654 = load <8 x i32>, <8 x i32>* %653, align 32
  %655 = shufflevector <8 x i32> %651, <8 x i32> %654, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %656 = shufflevector <8 x i32> %651, <8 x i32> %654, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %657 = shufflevector <8 x i32> %631, <8 x i32> %639, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %658 = bitcast <8 x i32> %657 to <4 x i64>
  %659 = shufflevector <8 x i32> %631, <8 x i32> %639, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %660 = bitcast <8 x i32> %659 to <4 x i64>
  %661 = shufflevector <8 x i32> %632, <8 x i32> %640, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %662 = bitcast <8 x i32> %661 to <4 x i64>
  %663 = shufflevector <8 x i32> %632, <8 x i32> %640, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %664 = bitcast <8 x i32> %663 to <4 x i64>
  %665 = shufflevector <8 x i32> %647, <8 x i32> %655, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %666 = bitcast <8 x i32> %665 to <4 x i64>
  %667 = shufflevector <8 x i32> %647, <8 x i32> %655, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %668 = bitcast <8 x i32> %667 to <4 x i64>
  %669 = shufflevector <8 x i32> %648, <8 x i32> %656, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %670 = bitcast <8 x i32> %669 to <4 x i64>
  %671 = shufflevector <8 x i32> %648, <8 x i32> %656, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %672 = bitcast <8 x i32> %671 to <4 x i64>
  %673 = shufflevector <4 x i64> %658, <4 x i64> %666, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
  store <4 x i64> %673, <4 x i64>* %625, align 32
  %674 = shufflevector <4 x i64> %660, <4 x i64> %668, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
  %675 = getelementptr inbounds i32, i32* %624, i64 32
  %676 = bitcast i32* %675 to <4 x i64>*
  store <4 x i64> %674, <4 x i64>* %676, align 32
  %677 = shufflevector <4 x i64> %662, <4 x i64> %670, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
  %678 = getelementptr inbounds i32, i32* %624, i64 64
  %679 = bitcast i32* %678 to <4 x i64>*
  store <4 x i64> %677, <4 x i64>* %679, align 32
  %680 = shufflevector <4 x i64> %664, <4 x i64> %672, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
  %681 = getelementptr inbounds i32, i32* %624, i64 96
  %682 = bitcast i32* %681 to <4 x i64>*
  store <4 x i64> %680, <4 x i64>* %682, align 32
  %683 = shufflevector <4 x i64> %658, <4 x i64> %666, <4 x i32> <i32 2, i32 3, i32 6, i32 7>
  %684 = getelementptr inbounds i32, i32* %624, i64 128
  %685 = bitcast i32* %684 to <4 x i64>*
  store <4 x i64> %683, <4 x i64>* %685, align 32
  %686 = shufflevector <4 x i64> %660, <4 x i64> %668, <4 x i32> <i32 2, i32 3, i32 6, i32 7>
  %687 = getelementptr inbounds i32, i32* %624, i64 160
  %688 = bitcast i32* %687 to <4 x i64>*
  store <4 x i64> %686, <4 x i64>* %688, align 32
  %689 = shufflevector <4 x i64> %662, <4 x i64> %670, <4 x i32> <i32 2, i32 3, i32 6, i32 7>
  %690 = getelementptr inbounds i32, i32* %624, i64 192
  %691 = bitcast i32* %690 to <4 x i64>*
  store <4 x i64> %689, <4 x i64>* %691, align 32
  %692 = shufflevector <4 x i64> %664, <4 x i64> %672, <4 x i32> <i32 2, i32 3, i32 6, i32 7>
  %693 = getelementptr inbounds i32, i32* %624, i64 224
  %694 = bitcast i32* %693 to <4 x i64>*
  store <4 x i64> %692, <4 x i64>* %694, align 32
  %695 = add nuw nsw i64 %549, 1
  %696 = icmp eq i64 %695, 4
  br i1 %696, label %545, label %548
}

declare void @av1_lowbd_fwd_txfm2d_4x16_sse2(i16*, i32*, i32, i8 zeroext, i32) #2

declare void @av1_lowbd_fwd_txfm2d_16x4_sse2(i16*, i32*, i32, i8 zeroext, i32) #2

declare void @av1_lowbd_fwd_txfm2d_8x32_sse2(i16*, i32*, i32, i8 zeroext, i32) #2

declare void @av1_lowbd_fwd_txfm2d_32x8_sse2(i16*, i32*, i32, i8 zeroext, i32) #2

; Function Attrs: nounwind ssp uwtable
define internal void @lowbd_fwd_txfm2d_16x64_avx2(i16* nocapture readonly, i32* nocapture, i32, i8 zeroext, i32) #3 {
  %6 = alloca [64 x <4 x i64>], align 32
  %7 = alloca [64 x <4 x i64>], align 32
  %8 = bitcast [64 x <4 x i64>]* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 2048, i8* nonnull %8) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %8, i8 -86, i64 2048, i1 false)
  %9 = bitcast [64 x <4 x i64>]* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 2048, i8* nonnull %9) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %9, i8 -86, i64 2048, i1 false)
  %10 = load i8*, i8** getelementptr inbounds ([19 x i8*], [19 x i8*]* @av1_fwd_txfm_shift_ls, i64 0, i64 17), align 8
  %11 = load i8, i8* getelementptr inbounds ([5 x [5 x i8]], [5 x [5 x i8]]* @av1_fwd_cos_bit_col, i64 0, i64 2, i64 4), align 2
  %12 = load i8, i8* getelementptr inbounds ([5 x [5 x i8]], [5 x [5 x i8]]* @av1_fwd_cos_bit_row, i64 0, i64 2, i64 4), align 2
  %13 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 0
  %14 = sext i32 %2 to i64
  %15 = getelementptr inbounds i8, i8* %10, i64 1
  br label %18

16:                                               ; preds = %172
  %17 = getelementptr inbounds i8, i8* %10, i64 2
  br label %388

18:                                               ; preds = %18, %5
  %19 = phi i64 [ 0, %5 ], [ %43, %18 ]
  %20 = mul nsw i64 %19, %14
  %21 = getelementptr inbounds i16, i16* %0, i64 %20
  %22 = bitcast i16* %21 to <4 x i64>*
  %23 = load <4 x i64>, <4 x i64>* %22, align 32
  %24 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %19
  store <4 x i64> %23, <4 x i64>* %24, align 32
  %25 = or i64 %19, 1
  %26 = mul nsw i64 %25, %14
  %27 = getelementptr inbounds i16, i16* %0, i64 %26
  %28 = bitcast i16* %27 to <4 x i64>*
  %29 = load <4 x i64>, <4 x i64>* %28, align 32
  %30 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %25
  store <4 x i64> %29, <4 x i64>* %30, align 32
  %31 = or i64 %19, 2
  %32 = mul nsw i64 %31, %14
  %33 = getelementptr inbounds i16, i16* %0, i64 %32
  %34 = bitcast i16* %33 to <4 x i64>*
  %35 = load <4 x i64>, <4 x i64>* %34, align 32
  %36 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %31
  store <4 x i64> %35, <4 x i64>* %36, align 32
  %37 = or i64 %19, 3
  %38 = mul nsw i64 %37, %14
  %39 = getelementptr inbounds i16, i16* %0, i64 %38
  %40 = bitcast i16* %39 to <4 x i64>*
  %41 = load <4 x i64>, <4 x i64>* %40, align 32
  %42 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %37
  store <4 x i64> %41, <4 x i64>* %42, align 32
  %43 = add nuw nsw i64 %19, 4
  %44 = icmp eq i64 %43, 64
  br i1 %44, label %45, label %18

45:                                               ; preds = %18
  %46 = load i8, i8* %10, align 1
  %47 = sext i8 %46 to i32
  %48 = icmp slt i8 %46, 0
  br i1 %48, label %49, label %83

49:                                               ; preds = %45
  %50 = sub nsw i32 0, %47
  %51 = xor i32 %47, -1
  %52 = shl i32 1, %51
  %53 = trunc i32 %52 to i16
  %54 = insertelement <16 x i16> undef, i16 %53, i32 0
  %55 = shufflevector <16 x i16> %54, <16 x i16> undef, <16 x i32> zeroinitializer
  br label %56

56:                                               ; preds = %56, %49
  %57 = phi i64 [ 0, %49 ], [ %81, %56 ]
  %58 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %57
  %59 = bitcast <4 x i64>* %58 to <16 x i16>*
  %60 = load <16 x i16>, <16 x i16>* %59, align 32
  %61 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %60, <16 x i16> %55) #8
  %62 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %61, i32 %50) #8
  store <16 x i16> %62, <16 x i16>* %59, align 32
  %63 = or i64 %57, 1
  %64 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %63
  %65 = bitcast <4 x i64>* %64 to <16 x i16>*
  %66 = load <16 x i16>, <16 x i16>* %65, align 32
  %67 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %66, <16 x i16> %55) #8
  %68 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %67, i32 %50) #8
  store <16 x i16> %68, <16 x i16>* %65, align 32
  %69 = or i64 %57, 2
  %70 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %69
  %71 = bitcast <4 x i64>* %70 to <16 x i16>*
  %72 = load <16 x i16>, <16 x i16>* %71, align 32
  %73 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %72, <16 x i16> %55) #8
  %74 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %73, i32 %50) #8
  store <16 x i16> %74, <16 x i16>* %71, align 32
  %75 = or i64 %57, 3
  %76 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %75
  %77 = bitcast <4 x i64>* %76 to <16 x i16>*
  %78 = load <16 x i16>, <16 x i16>* %77, align 32
  %79 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %78, <16 x i16> %55) #8
  %80 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %79, i32 %50) #8
  store <16 x i16> %80, <16 x i16>* %77, align 32
  %81 = add nuw nsw i64 %57, 4
  %82 = icmp eq i64 %81, 64
  br i1 %82, label %108, label %56

83:                                               ; preds = %45
  %84 = icmp eq i8 %46, 0
  br i1 %84, label %108, label %85

85:                                               ; preds = %83, %85
  %86 = phi i64 [ %106, %85 ], [ 0, %83 ]
  %87 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %86
  %88 = bitcast <4 x i64>* %87 to <16 x i16>*
  %89 = load <16 x i16>, <16 x i16>* %88, align 32
  %90 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %89, i32 %47) #8
  store <16 x i16> %90, <16 x i16>* %88, align 32
  %91 = or i64 %86, 1
  %92 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %91
  %93 = bitcast <4 x i64>* %92 to <16 x i16>*
  %94 = load <16 x i16>, <16 x i16>* %93, align 32
  %95 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %94, i32 %47) #8
  store <16 x i16> %95, <16 x i16>* %93, align 32
  %96 = or i64 %86, 2
  %97 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %96
  %98 = bitcast <4 x i64>* %97 to <16 x i16>*
  %99 = load <16 x i16>, <16 x i16>* %98, align 32
  %100 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %99, i32 %47) #8
  store <16 x i16> %100, <16 x i16>* %98, align 32
  %101 = or i64 %86, 3
  %102 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %101
  %103 = bitcast <4 x i64>* %102 to <16 x i16>*
  %104 = load <16 x i16>, <16 x i16>* %103, align 32
  %105 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %104, i32 %47) #8
  store <16 x i16> %105, <16 x i16>* %103, align 32
  %106 = add nuw nsw i64 %86, 4
  %107 = icmp eq i64 %106, 64
  br i1 %107, label %108, label %85

108:                                              ; preds = %85, %56, %83
  call fastcc void @fdct16x64_new_avx2(<4 x i64>* nonnull %13, <4 x i64>* nonnull %13, i8 signext %11)
  %109 = load i8, i8* %15, align 1
  %110 = sext i8 %109 to i32
  %111 = icmp slt i8 %109, 0
  br i1 %111, label %112, label %146

112:                                              ; preds = %108
  %113 = sub nsw i32 0, %110
  %114 = xor i32 %110, -1
  %115 = shl i32 1, %114
  %116 = trunc i32 %115 to i16
  %117 = insertelement <16 x i16> undef, i16 %116, i32 0
  %118 = shufflevector <16 x i16> %117, <16 x i16> undef, <16 x i32> zeroinitializer
  br label %119

119:                                              ; preds = %119, %112
  %120 = phi i64 [ 0, %112 ], [ %144, %119 ]
  %121 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %120
  %122 = bitcast <4 x i64>* %121 to <16 x i16>*
  %123 = load <16 x i16>, <16 x i16>* %122, align 32
  %124 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %123, <16 x i16> %118) #8
  %125 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %124, i32 %113) #8
  store <16 x i16> %125, <16 x i16>* %122, align 32
  %126 = or i64 %120, 1
  %127 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %126
  %128 = bitcast <4 x i64>* %127 to <16 x i16>*
  %129 = load <16 x i16>, <16 x i16>* %128, align 32
  %130 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %129, <16 x i16> %118) #8
  %131 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %130, i32 %113) #8
  store <16 x i16> %131, <16 x i16>* %128, align 32
  %132 = or i64 %120, 2
  %133 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %132
  %134 = bitcast <4 x i64>* %133 to <16 x i16>*
  %135 = load <16 x i16>, <16 x i16>* %134, align 32
  %136 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %135, <16 x i16> %118) #8
  %137 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %136, i32 %113) #8
  store <16 x i16> %137, <16 x i16>* %134, align 32
  %138 = or i64 %120, 3
  %139 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %138
  %140 = bitcast <4 x i64>* %139 to <16 x i16>*
  %141 = load <16 x i16>, <16 x i16>* %140, align 32
  %142 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %141, <16 x i16> %118) #8
  %143 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %142, i32 %113) #8
  store <16 x i16> %143, <16 x i16>* %140, align 32
  %144 = add nuw nsw i64 %120, 4
  %145 = icmp eq i64 %144, 64
  br i1 %145, label %171, label %119

146:                                              ; preds = %108
  %147 = icmp eq i8 %109, 0
  br i1 %147, label %171, label %148

148:                                              ; preds = %146, %148
  %149 = phi i64 [ %169, %148 ], [ 0, %146 ]
  %150 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %149
  %151 = bitcast <4 x i64>* %150 to <16 x i16>*
  %152 = load <16 x i16>, <16 x i16>* %151, align 32
  %153 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %152, i32 %110) #8
  store <16 x i16> %153, <16 x i16>* %151, align 32
  %154 = or i64 %149, 1
  %155 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %154
  %156 = bitcast <4 x i64>* %155 to <16 x i16>*
  %157 = load <16 x i16>, <16 x i16>* %156, align 32
  %158 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %157, i32 %110) #8
  store <16 x i16> %158, <16 x i16>* %156, align 32
  %159 = or i64 %149, 2
  %160 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %159
  %161 = bitcast <4 x i64>* %160 to <16 x i16>*
  %162 = load <16 x i16>, <16 x i16>* %161, align 32
  %163 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %162, i32 %110) #8
  store <16 x i16> %163, <16 x i16>* %161, align 32
  %164 = or i64 %149, 3
  %165 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %164
  %166 = bitcast <4 x i64>* %165 to <16 x i16>*
  %167 = load <16 x i16>, <16 x i16>* %166, align 32
  %168 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %167, i32 %110) #8
  store <16 x i16> %168, <16 x i16>* %166, align 32
  %169 = add nuw nsw i64 %149, 4
  %170 = icmp eq i64 %169, 64
  br i1 %170, label %171, label %148

171:                                              ; preds = %148, %119, %146
  br label %172

172:                                              ; preds = %171, %172
  %173 = phi i64 [ %383, %172 ], [ 0, %171 ]
  %174 = shl nsw i64 %173, 4
  %175 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 %174
  %176 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %174
  %177 = bitcast <4 x i64>* %175 to <2 x i64>*
  %178 = load <2 x i64>, <2 x i64>* %177, align 32
  %179 = getelementptr inbounds <4 x i64>, <4 x i64>* %175, i64 8
  %180 = bitcast <4 x i64>* %179 to <2 x i64>*
  %181 = load <2 x i64>, <2 x i64>* %180, align 32
  %182 = shufflevector <2 x i64> %178, <2 x i64> %181, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %183 = getelementptr inbounds <4 x i64>, <4 x i64>* %175, i64 1
  %184 = bitcast <4 x i64>* %183 to <2 x i64>*
  %185 = load <2 x i64>, <2 x i64>* %184, align 32
  %186 = getelementptr inbounds <4 x i64>, <4 x i64>* %175, i64 9
  %187 = bitcast <4 x i64>* %186 to <2 x i64>*
  %188 = load <2 x i64>, <2 x i64>* %187, align 32
  %189 = shufflevector <2 x i64> %185, <2 x i64> %188, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %190 = getelementptr inbounds <4 x i64>, <4 x i64>* %175, i64 2
  %191 = bitcast <4 x i64>* %190 to <2 x i64>*
  %192 = load <2 x i64>, <2 x i64>* %191, align 32
  %193 = getelementptr inbounds <4 x i64>, <4 x i64>* %175, i64 10
  %194 = bitcast <4 x i64>* %193 to <2 x i64>*
  %195 = load <2 x i64>, <2 x i64>* %194, align 32
  %196 = shufflevector <2 x i64> %192, <2 x i64> %195, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %197 = getelementptr inbounds <4 x i64>, <4 x i64>* %175, i64 3
  %198 = bitcast <4 x i64>* %197 to <2 x i64>*
  %199 = load <2 x i64>, <2 x i64>* %198, align 32
  %200 = getelementptr inbounds <4 x i64>, <4 x i64>* %175, i64 11
  %201 = bitcast <4 x i64>* %200 to <2 x i64>*
  %202 = load <2 x i64>, <2 x i64>* %201, align 32
  %203 = shufflevector <2 x i64> %199, <2 x i64> %202, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %204 = getelementptr inbounds <4 x i64>, <4 x i64>* %175, i64 4
  %205 = bitcast <4 x i64>* %204 to <2 x i64>*
  %206 = load <2 x i64>, <2 x i64>* %205, align 32
  %207 = getelementptr inbounds <4 x i64>, <4 x i64>* %175, i64 12
  %208 = bitcast <4 x i64>* %207 to <2 x i64>*
  %209 = load <2 x i64>, <2 x i64>* %208, align 32
  %210 = shufflevector <2 x i64> %206, <2 x i64> %209, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %211 = getelementptr inbounds <4 x i64>, <4 x i64>* %175, i64 5
  %212 = bitcast <4 x i64>* %211 to <2 x i64>*
  %213 = load <2 x i64>, <2 x i64>* %212, align 32
  %214 = getelementptr inbounds <4 x i64>, <4 x i64>* %175, i64 13
  %215 = bitcast <4 x i64>* %214 to <2 x i64>*
  %216 = load <2 x i64>, <2 x i64>* %215, align 32
  %217 = shufflevector <2 x i64> %213, <2 x i64> %216, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %218 = getelementptr inbounds <4 x i64>, <4 x i64>* %175, i64 6
  %219 = bitcast <4 x i64>* %218 to <2 x i64>*
  %220 = load <2 x i64>, <2 x i64>* %219, align 32
  %221 = getelementptr inbounds <4 x i64>, <4 x i64>* %175, i64 14
  %222 = bitcast <4 x i64>* %221 to <2 x i64>*
  %223 = load <2 x i64>, <2 x i64>* %222, align 32
  %224 = shufflevector <2 x i64> %220, <2 x i64> %223, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %225 = getelementptr inbounds <4 x i64>, <4 x i64>* %175, i64 7
  %226 = bitcast <4 x i64>* %225 to <2 x i64>*
  %227 = load <2 x i64>, <2 x i64>* %226, align 32
  %228 = getelementptr inbounds <4 x i64>, <4 x i64>* %175, i64 15
  %229 = bitcast <4 x i64>* %228 to <2 x i64>*
  %230 = load <2 x i64>, <2 x i64>* %229, align 32
  %231 = shufflevector <2 x i64> %227, <2 x i64> %230, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %232 = getelementptr inbounds <2 x i64>, <2 x i64>* %177, i64 1
  %233 = load <2 x i64>, <2 x i64>* %232, align 16
  %234 = getelementptr inbounds <2 x i64>, <2 x i64>* %180, i64 1
  %235 = load <2 x i64>, <2 x i64>* %234, align 16
  %236 = shufflevector <2 x i64> %233, <2 x i64> %235, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %237 = getelementptr inbounds <2 x i64>, <2 x i64>* %184, i64 1
  %238 = load <2 x i64>, <2 x i64>* %237, align 16
  %239 = getelementptr inbounds <2 x i64>, <2 x i64>* %187, i64 1
  %240 = load <2 x i64>, <2 x i64>* %239, align 16
  %241 = shufflevector <2 x i64> %238, <2 x i64> %240, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %242 = getelementptr inbounds <2 x i64>, <2 x i64>* %191, i64 1
  %243 = load <2 x i64>, <2 x i64>* %242, align 16
  %244 = getelementptr inbounds <2 x i64>, <2 x i64>* %194, i64 1
  %245 = load <2 x i64>, <2 x i64>* %244, align 16
  %246 = shufflevector <2 x i64> %243, <2 x i64> %245, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %247 = getelementptr inbounds <2 x i64>, <2 x i64>* %198, i64 1
  %248 = load <2 x i64>, <2 x i64>* %247, align 16
  %249 = getelementptr inbounds <2 x i64>, <2 x i64>* %201, i64 1
  %250 = load <2 x i64>, <2 x i64>* %249, align 16
  %251 = shufflevector <2 x i64> %248, <2 x i64> %250, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %252 = getelementptr inbounds <2 x i64>, <2 x i64>* %205, i64 1
  %253 = load <2 x i64>, <2 x i64>* %252, align 16
  %254 = getelementptr inbounds <2 x i64>, <2 x i64>* %208, i64 1
  %255 = load <2 x i64>, <2 x i64>* %254, align 16
  %256 = shufflevector <2 x i64> %253, <2 x i64> %255, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %257 = getelementptr inbounds <2 x i64>, <2 x i64>* %212, i64 1
  %258 = load <2 x i64>, <2 x i64>* %257, align 16
  %259 = getelementptr inbounds <2 x i64>, <2 x i64>* %215, i64 1
  %260 = load <2 x i64>, <2 x i64>* %259, align 16
  %261 = shufflevector <2 x i64> %258, <2 x i64> %260, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %262 = getelementptr inbounds <2 x i64>, <2 x i64>* %219, i64 1
  %263 = load <2 x i64>, <2 x i64>* %262, align 16
  %264 = getelementptr inbounds <2 x i64>, <2 x i64>* %222, i64 1
  %265 = load <2 x i64>, <2 x i64>* %264, align 16
  %266 = shufflevector <2 x i64> %263, <2 x i64> %265, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %267 = getelementptr inbounds <2 x i64>, <2 x i64>* %226, i64 1
  %268 = load <2 x i64>, <2 x i64>* %267, align 16
  %269 = getelementptr inbounds <2 x i64>, <2 x i64>* %229, i64 1
  %270 = load <2 x i64>, <2 x i64>* %269, align 16
  %271 = shufflevector <2 x i64> %268, <2 x i64> %270, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %272 = bitcast <4 x i64> %182 to <16 x i16>
  %273 = bitcast <4 x i64> %189 to <16 x i16>
  %274 = shufflevector <16 x i16> %272, <16 x i16> %273, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %275 = shufflevector <16 x i16> %272, <16 x i16> %273, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %276 = bitcast <4 x i64> %196 to <16 x i16>
  %277 = bitcast <4 x i64> %203 to <16 x i16>
  %278 = shufflevector <16 x i16> %276, <16 x i16> %277, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %279 = shufflevector <16 x i16> %276, <16 x i16> %277, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %280 = bitcast <4 x i64> %210 to <16 x i16>
  %281 = bitcast <4 x i64> %217 to <16 x i16>
  %282 = shufflevector <16 x i16> %280, <16 x i16> %281, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %283 = shufflevector <16 x i16> %280, <16 x i16> %281, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %284 = bitcast <4 x i64> %224 to <16 x i16>
  %285 = bitcast <4 x i64> %231 to <16 x i16>
  %286 = shufflevector <16 x i16> %284, <16 x i16> %285, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %287 = shufflevector <16 x i16> %284, <16 x i16> %285, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %288 = bitcast <16 x i16> %274 to <8 x i32>
  %289 = bitcast <16 x i16> %278 to <8 x i32>
  %290 = shufflevector <8 x i32> %288, <8 x i32> %289, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %291 = shufflevector <8 x i32> %288, <8 x i32> %289, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %292 = bitcast <16 x i16> %282 to <8 x i32>
  %293 = bitcast <16 x i16> %286 to <8 x i32>
  %294 = shufflevector <8 x i32> %292, <8 x i32> %293, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %295 = shufflevector <8 x i32> %292, <8 x i32> %293, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %296 = bitcast <16 x i16> %275 to <8 x i32>
  %297 = bitcast <16 x i16> %279 to <8 x i32>
  %298 = shufflevector <8 x i32> %296, <8 x i32> %297, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %299 = shufflevector <8 x i32> %296, <8 x i32> %297, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %300 = bitcast <16 x i16> %283 to <8 x i32>
  %301 = bitcast <16 x i16> %287 to <8 x i32>
  %302 = shufflevector <8 x i32> %300, <8 x i32> %301, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %303 = shufflevector <8 x i32> %300, <8 x i32> %301, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %304 = bitcast <8 x i32> %290 to <4 x i64>
  %305 = bitcast <8 x i32> %294 to <4 x i64>
  %306 = shufflevector <4 x i64> %304, <4 x i64> %305, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %306, <4 x i64>* %176, align 32
  %307 = shufflevector <4 x i64> %304, <4 x i64> %305, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %308 = getelementptr inbounds <4 x i64>, <4 x i64>* %176, i64 1
  store <4 x i64> %307, <4 x i64>* %308, align 32
  %309 = bitcast <8 x i32> %298 to <4 x i64>
  %310 = bitcast <8 x i32> %302 to <4 x i64>
  %311 = shufflevector <4 x i64> %309, <4 x i64> %310, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %312 = getelementptr inbounds <4 x i64>, <4 x i64>* %176, i64 4
  store <4 x i64> %311, <4 x i64>* %312, align 32
  %313 = shufflevector <4 x i64> %309, <4 x i64> %310, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %314 = getelementptr inbounds <4 x i64>, <4 x i64>* %176, i64 5
  store <4 x i64> %313, <4 x i64>* %314, align 32
  %315 = bitcast <8 x i32> %291 to <4 x i64>
  %316 = bitcast <8 x i32> %295 to <4 x i64>
  %317 = shufflevector <4 x i64> %315, <4 x i64> %316, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %318 = getelementptr inbounds <4 x i64>, <4 x i64>* %176, i64 2
  store <4 x i64> %317, <4 x i64>* %318, align 32
  %319 = shufflevector <4 x i64> %315, <4 x i64> %316, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %320 = getelementptr inbounds <4 x i64>, <4 x i64>* %176, i64 3
  store <4 x i64> %319, <4 x i64>* %320, align 32
  %321 = bitcast <8 x i32> %299 to <4 x i64>
  %322 = bitcast <8 x i32> %303 to <4 x i64>
  %323 = shufflevector <4 x i64> %321, <4 x i64> %322, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %324 = getelementptr inbounds <4 x i64>, <4 x i64>* %176, i64 6
  store <4 x i64> %323, <4 x i64>* %324, align 32
  %325 = shufflevector <4 x i64> %321, <4 x i64> %322, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %326 = getelementptr inbounds <4 x i64>, <4 x i64>* %176, i64 7
  store <4 x i64> %325, <4 x i64>* %326, align 32
  %327 = getelementptr inbounds <4 x i64>, <4 x i64>* %176, i64 8
  %328 = bitcast <4 x i64> %236 to <16 x i16>
  %329 = bitcast <4 x i64> %241 to <16 x i16>
  %330 = shufflevector <16 x i16> %328, <16 x i16> %329, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %331 = shufflevector <16 x i16> %328, <16 x i16> %329, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %332 = bitcast <4 x i64> %246 to <16 x i16>
  %333 = bitcast <4 x i64> %251 to <16 x i16>
  %334 = shufflevector <16 x i16> %332, <16 x i16> %333, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %335 = shufflevector <16 x i16> %332, <16 x i16> %333, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %336 = bitcast <4 x i64> %256 to <16 x i16>
  %337 = bitcast <4 x i64> %261 to <16 x i16>
  %338 = shufflevector <16 x i16> %336, <16 x i16> %337, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %339 = shufflevector <16 x i16> %336, <16 x i16> %337, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %340 = bitcast <4 x i64> %266 to <16 x i16>
  %341 = bitcast <4 x i64> %271 to <16 x i16>
  %342 = shufflevector <16 x i16> %340, <16 x i16> %341, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %343 = shufflevector <16 x i16> %340, <16 x i16> %341, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %344 = bitcast <16 x i16> %330 to <8 x i32>
  %345 = bitcast <16 x i16> %334 to <8 x i32>
  %346 = shufflevector <8 x i32> %344, <8 x i32> %345, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %347 = shufflevector <8 x i32> %344, <8 x i32> %345, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %348 = bitcast <16 x i16> %338 to <8 x i32>
  %349 = bitcast <16 x i16> %342 to <8 x i32>
  %350 = shufflevector <8 x i32> %348, <8 x i32> %349, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %351 = shufflevector <8 x i32> %348, <8 x i32> %349, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %352 = bitcast <16 x i16> %331 to <8 x i32>
  %353 = bitcast <16 x i16> %335 to <8 x i32>
  %354 = shufflevector <8 x i32> %352, <8 x i32> %353, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %355 = shufflevector <8 x i32> %352, <8 x i32> %353, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %356 = bitcast <16 x i16> %339 to <8 x i32>
  %357 = bitcast <16 x i16> %343 to <8 x i32>
  %358 = shufflevector <8 x i32> %356, <8 x i32> %357, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %359 = shufflevector <8 x i32> %356, <8 x i32> %357, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %360 = bitcast <8 x i32> %346 to <4 x i64>
  %361 = bitcast <8 x i32> %350 to <4 x i64>
  %362 = shufflevector <4 x i64> %360, <4 x i64> %361, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %362, <4 x i64>* %327, align 32
  %363 = shufflevector <4 x i64> %360, <4 x i64> %361, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %364 = getelementptr inbounds <4 x i64>, <4 x i64>* %176, i64 9
  store <4 x i64> %363, <4 x i64>* %364, align 32
  %365 = bitcast <8 x i32> %354 to <4 x i64>
  %366 = bitcast <8 x i32> %358 to <4 x i64>
  %367 = shufflevector <4 x i64> %365, <4 x i64> %366, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %368 = getelementptr inbounds <4 x i64>, <4 x i64>* %176, i64 12
  store <4 x i64> %367, <4 x i64>* %368, align 32
  %369 = shufflevector <4 x i64> %365, <4 x i64> %366, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %370 = getelementptr inbounds <4 x i64>, <4 x i64>* %176, i64 13
  store <4 x i64> %369, <4 x i64>* %370, align 32
  %371 = bitcast <8 x i32> %347 to <4 x i64>
  %372 = bitcast <8 x i32> %351 to <4 x i64>
  %373 = shufflevector <4 x i64> %371, <4 x i64> %372, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %374 = getelementptr inbounds <4 x i64>, <4 x i64>* %176, i64 10
  store <4 x i64> %373, <4 x i64>* %374, align 32
  %375 = shufflevector <4 x i64> %371, <4 x i64> %372, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %376 = getelementptr inbounds <4 x i64>, <4 x i64>* %176, i64 11
  store <4 x i64> %375, <4 x i64>* %376, align 32
  %377 = bitcast <8 x i32> %355 to <4 x i64>
  %378 = bitcast <8 x i32> %359 to <4 x i64>
  %379 = shufflevector <4 x i64> %377, <4 x i64> %378, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %380 = getelementptr inbounds <4 x i64>, <4 x i64>* %176, i64 14
  store <4 x i64> %379, <4 x i64>* %380, align 32
  %381 = shufflevector <4 x i64> %377, <4 x i64> %378, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %382 = getelementptr inbounds <4 x i64>, <4 x i64>* %176, i64 15
  store <4 x i64> %381, <4 x i64>* %382, align 32
  %383 = add nuw nsw i64 %173, 1
  %384 = icmp eq i64 %383, 4
  br i1 %384, label %16, label %172

385:                                              ; preds = %547
  %386 = getelementptr inbounds i32, i32* %1, i64 512
  %387 = bitcast i32* %386 to i8*
  call void @llvm.memset.p0i8.i64(i8* align 4 %387, i8 0, i64 2048, i1 false)
  call void @llvm.lifetime.end.p0i8(i64 2048, i8* nonnull %9) #8
  call void @llvm.lifetime.end.p0i8(i64 2048, i8* nonnull %8) #8
  ret void

388:                                              ; preds = %547, %16
  %389 = phi i64 [ 0, %16 ], [ %548, %547 ]
  %390 = shl nsw i64 %389, 4
  %391 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %390
  call void @fdct16x16_new_avx2(<4 x i64>* %391, <4 x i64>* %391, i8 signext %12)
  %392 = load i8, i8* %17, align 1
  %393 = sext i8 %392 to i32
  %394 = icmp slt i8 %392, 0
  br i1 %394, label %395, label %481

395:                                              ; preds = %388
  %396 = sub nsw i32 0, %393
  %397 = xor i32 %393, -1
  %398 = shl i32 1, %397
  %399 = trunc i32 %398 to i16
  %400 = insertelement <16 x i16> undef, i16 %399, i32 0
  %401 = shufflevector <16 x i16> %400, <16 x i16> undef, <16 x i32> zeroinitializer
  %402 = bitcast <4 x i64>* %391 to <16 x i16>*
  %403 = load <16 x i16>, <16 x i16>* %402, align 32
  %404 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %403, <16 x i16> %401) #8
  %405 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %404, i32 %396) #8
  store <16 x i16> %405, <16 x i16>* %402, align 32
  %406 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 1
  %407 = bitcast <4 x i64>* %406 to <16 x i16>*
  %408 = load <16 x i16>, <16 x i16>* %407, align 32
  %409 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %408, <16 x i16> %401) #8
  %410 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %409, i32 %396) #8
  store <16 x i16> %410, <16 x i16>* %407, align 32
  %411 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 2
  %412 = bitcast <4 x i64>* %411 to <16 x i16>*
  %413 = load <16 x i16>, <16 x i16>* %412, align 32
  %414 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %413, <16 x i16> %401) #8
  %415 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %414, i32 %396) #8
  store <16 x i16> %415, <16 x i16>* %412, align 32
  %416 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 3
  %417 = bitcast <4 x i64>* %416 to <16 x i16>*
  %418 = load <16 x i16>, <16 x i16>* %417, align 32
  %419 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %418, <16 x i16> %401) #8
  %420 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %419, i32 %396) #8
  store <16 x i16> %420, <16 x i16>* %417, align 32
  %421 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 4
  %422 = bitcast <4 x i64>* %421 to <16 x i16>*
  %423 = load <16 x i16>, <16 x i16>* %422, align 32
  %424 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %423, <16 x i16> %401) #8
  %425 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %424, i32 %396) #8
  store <16 x i16> %425, <16 x i16>* %422, align 32
  %426 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 5
  %427 = bitcast <4 x i64>* %426 to <16 x i16>*
  %428 = load <16 x i16>, <16 x i16>* %427, align 32
  %429 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %428, <16 x i16> %401) #8
  %430 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %429, i32 %396) #8
  store <16 x i16> %430, <16 x i16>* %427, align 32
  %431 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 6
  %432 = bitcast <4 x i64>* %431 to <16 x i16>*
  %433 = load <16 x i16>, <16 x i16>* %432, align 32
  %434 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %433, <16 x i16> %401) #8
  %435 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %434, i32 %396) #8
  store <16 x i16> %435, <16 x i16>* %432, align 32
  %436 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 7
  %437 = bitcast <4 x i64>* %436 to <16 x i16>*
  %438 = load <16 x i16>, <16 x i16>* %437, align 32
  %439 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %438, <16 x i16> %401) #8
  %440 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %439, i32 %396) #8
  store <16 x i16> %440, <16 x i16>* %437, align 32
  %441 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 8
  %442 = bitcast <4 x i64>* %441 to <16 x i16>*
  %443 = load <16 x i16>, <16 x i16>* %442, align 32
  %444 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %443, <16 x i16> %401) #8
  %445 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %444, i32 %396) #8
  store <16 x i16> %445, <16 x i16>* %442, align 32
  %446 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 9
  %447 = bitcast <4 x i64>* %446 to <16 x i16>*
  %448 = load <16 x i16>, <16 x i16>* %447, align 32
  %449 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %448, <16 x i16> %401) #8
  %450 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %449, i32 %396) #8
  store <16 x i16> %450, <16 x i16>* %447, align 32
  %451 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 10
  %452 = bitcast <4 x i64>* %451 to <16 x i16>*
  %453 = load <16 x i16>, <16 x i16>* %452, align 32
  %454 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %453, <16 x i16> %401) #8
  %455 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %454, i32 %396) #8
  store <16 x i16> %455, <16 x i16>* %452, align 32
  %456 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 11
  %457 = bitcast <4 x i64>* %456 to <16 x i16>*
  %458 = load <16 x i16>, <16 x i16>* %457, align 32
  %459 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %458, <16 x i16> %401) #8
  %460 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %459, i32 %396) #8
  store <16 x i16> %460, <16 x i16>* %457, align 32
  %461 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 12
  %462 = bitcast <4 x i64>* %461 to <16 x i16>*
  %463 = load <16 x i16>, <16 x i16>* %462, align 32
  %464 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %463, <16 x i16> %401) #8
  %465 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %464, i32 %396) #8
  store <16 x i16> %465, <16 x i16>* %462, align 32
  %466 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 13
  %467 = bitcast <4 x i64>* %466 to <16 x i16>*
  %468 = load <16 x i16>, <16 x i16>* %467, align 32
  %469 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %468, <16 x i16> %401) #8
  %470 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %469, i32 %396) #8
  store <16 x i16> %470, <16 x i16>* %467, align 32
  %471 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 14
  %472 = bitcast <4 x i64>* %471 to <16 x i16>*
  %473 = load <16 x i16>, <16 x i16>* %472, align 32
  %474 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %473, <16 x i16> %401) #8
  %475 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %474, i32 %396) #8
  store <16 x i16> %475, <16 x i16>* %472, align 32
  %476 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 15
  %477 = bitcast <4 x i64>* %476 to <16 x i16>*
  %478 = load <16 x i16>, <16 x i16>* %477, align 32
  %479 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %478, <16 x i16> %401) #8
  %480 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %479, i32 %396) #8
  store <16 x i16> %480, <16 x i16>* %477, align 32
  br label %550

481:                                              ; preds = %388
  %482 = icmp eq i8 %392, 0
  br i1 %482, label %550, label %483

483:                                              ; preds = %481
  %484 = bitcast <4 x i64>* %391 to <16 x i16>*
  %485 = load <16 x i16>, <16 x i16>* %484, align 32
  %486 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %485, i32 %393) #8
  store <16 x i16> %486, <16 x i16>* %484, align 32
  %487 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 1
  %488 = bitcast <4 x i64>* %487 to <16 x i16>*
  %489 = load <16 x i16>, <16 x i16>* %488, align 32
  %490 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %489, i32 %393) #8
  store <16 x i16> %490, <16 x i16>* %488, align 32
  %491 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 2
  %492 = bitcast <4 x i64>* %491 to <16 x i16>*
  %493 = load <16 x i16>, <16 x i16>* %492, align 32
  %494 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %493, i32 %393) #8
  store <16 x i16> %494, <16 x i16>* %492, align 32
  %495 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 3
  %496 = bitcast <4 x i64>* %495 to <16 x i16>*
  %497 = load <16 x i16>, <16 x i16>* %496, align 32
  %498 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %497, i32 %393) #8
  store <16 x i16> %498, <16 x i16>* %496, align 32
  %499 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 4
  %500 = bitcast <4 x i64>* %499 to <16 x i16>*
  %501 = load <16 x i16>, <16 x i16>* %500, align 32
  %502 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %501, i32 %393) #8
  store <16 x i16> %502, <16 x i16>* %500, align 32
  %503 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 5
  %504 = bitcast <4 x i64>* %503 to <16 x i16>*
  %505 = load <16 x i16>, <16 x i16>* %504, align 32
  %506 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %505, i32 %393) #8
  store <16 x i16> %506, <16 x i16>* %504, align 32
  %507 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 6
  %508 = bitcast <4 x i64>* %507 to <16 x i16>*
  %509 = load <16 x i16>, <16 x i16>* %508, align 32
  %510 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %509, i32 %393) #8
  store <16 x i16> %510, <16 x i16>* %508, align 32
  %511 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 7
  %512 = bitcast <4 x i64>* %511 to <16 x i16>*
  %513 = load <16 x i16>, <16 x i16>* %512, align 32
  %514 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %513, i32 %393) #8
  store <16 x i16> %514, <16 x i16>* %512, align 32
  %515 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 8
  %516 = bitcast <4 x i64>* %515 to <16 x i16>*
  %517 = load <16 x i16>, <16 x i16>* %516, align 32
  %518 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %517, i32 %393) #8
  store <16 x i16> %518, <16 x i16>* %516, align 32
  %519 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 9
  %520 = bitcast <4 x i64>* %519 to <16 x i16>*
  %521 = load <16 x i16>, <16 x i16>* %520, align 32
  %522 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %521, i32 %393) #8
  store <16 x i16> %522, <16 x i16>* %520, align 32
  %523 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 10
  %524 = bitcast <4 x i64>* %523 to <16 x i16>*
  %525 = load <16 x i16>, <16 x i16>* %524, align 32
  %526 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %525, i32 %393) #8
  store <16 x i16> %526, <16 x i16>* %524, align 32
  %527 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 11
  %528 = bitcast <4 x i64>* %527 to <16 x i16>*
  %529 = load <16 x i16>, <16 x i16>* %528, align 32
  %530 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %529, i32 %393) #8
  store <16 x i16> %530, <16 x i16>* %528, align 32
  %531 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 12
  %532 = bitcast <4 x i64>* %531 to <16 x i16>*
  %533 = load <16 x i16>, <16 x i16>* %532, align 32
  %534 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %533, i32 %393) #8
  store <16 x i16> %534, <16 x i16>* %532, align 32
  %535 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 13
  %536 = bitcast <4 x i64>* %535 to <16 x i16>*
  %537 = load <16 x i16>, <16 x i16>* %536, align 32
  %538 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %537, i32 %393) #8
  store <16 x i16> %538, <16 x i16>* %536, align 32
  %539 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 14
  %540 = bitcast <4 x i64>* %539 to <16 x i16>*
  %541 = load <16 x i16>, <16 x i16>* %540, align 32
  %542 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %541, i32 %393) #8
  store <16 x i16> %542, <16 x i16>* %540, align 32
  %543 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 15
  %544 = bitcast <4 x i64>* %543 to <16 x i16>*
  %545 = load <16 x i16>, <16 x i16>* %544, align 32
  %546 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %545, i32 %393) #8
  store <16 x i16> %546, <16 x i16>* %544, align 32
  br label %550

547:                                              ; preds = %754
  %548 = add nuw nsw i64 %389, 1
  %549 = icmp eq i64 %548, 4
  br i1 %549, label %385, label %388

550:                                              ; preds = %481, %395, %483
  %551 = shl nsw i64 %389, 8
  %552 = getelementptr inbounds i32, i32* %1, i64 %551
  %553 = bitcast <4 x i64>* %391 to <2 x i64>*
  %554 = load <2 x i64>, <2 x i64>* %553, align 32
  %555 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 8
  %556 = bitcast <4 x i64>* %555 to <2 x i64>*
  %557 = load <2 x i64>, <2 x i64>* %556, align 32
  %558 = shufflevector <2 x i64> %554, <2 x i64> %557, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %559 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 1
  %560 = bitcast <4 x i64>* %559 to <2 x i64>*
  %561 = load <2 x i64>, <2 x i64>* %560, align 32
  %562 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 9
  %563 = bitcast <4 x i64>* %562 to <2 x i64>*
  %564 = load <2 x i64>, <2 x i64>* %563, align 32
  %565 = shufflevector <2 x i64> %561, <2 x i64> %564, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %566 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 2
  %567 = bitcast <4 x i64>* %566 to <2 x i64>*
  %568 = load <2 x i64>, <2 x i64>* %567, align 32
  %569 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 10
  %570 = bitcast <4 x i64>* %569 to <2 x i64>*
  %571 = load <2 x i64>, <2 x i64>* %570, align 32
  %572 = shufflevector <2 x i64> %568, <2 x i64> %571, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %573 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 3
  %574 = bitcast <4 x i64>* %573 to <2 x i64>*
  %575 = load <2 x i64>, <2 x i64>* %574, align 32
  %576 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 11
  %577 = bitcast <4 x i64>* %576 to <2 x i64>*
  %578 = load <2 x i64>, <2 x i64>* %577, align 32
  %579 = shufflevector <2 x i64> %575, <2 x i64> %578, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %580 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 4
  %581 = bitcast <4 x i64>* %580 to <2 x i64>*
  %582 = load <2 x i64>, <2 x i64>* %581, align 32
  %583 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 12
  %584 = bitcast <4 x i64>* %583 to <2 x i64>*
  %585 = load <2 x i64>, <2 x i64>* %584, align 32
  %586 = shufflevector <2 x i64> %582, <2 x i64> %585, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %587 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 5
  %588 = bitcast <4 x i64>* %587 to <2 x i64>*
  %589 = load <2 x i64>, <2 x i64>* %588, align 32
  %590 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 13
  %591 = bitcast <4 x i64>* %590 to <2 x i64>*
  %592 = load <2 x i64>, <2 x i64>* %591, align 32
  %593 = shufflevector <2 x i64> %589, <2 x i64> %592, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %594 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 6
  %595 = bitcast <4 x i64>* %594 to <2 x i64>*
  %596 = load <2 x i64>, <2 x i64>* %595, align 32
  %597 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 14
  %598 = bitcast <4 x i64>* %597 to <2 x i64>*
  %599 = load <2 x i64>, <2 x i64>* %598, align 32
  %600 = shufflevector <2 x i64> %596, <2 x i64> %599, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %601 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 7
  %602 = bitcast <4 x i64>* %601 to <2 x i64>*
  %603 = load <2 x i64>, <2 x i64>* %602, align 32
  %604 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 15
  %605 = bitcast <4 x i64>* %604 to <2 x i64>*
  %606 = load <2 x i64>, <2 x i64>* %605, align 32
  %607 = shufflevector <2 x i64> %603, <2 x i64> %606, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %608 = getelementptr inbounds <2 x i64>, <2 x i64>* %553, i64 1
  %609 = load <2 x i64>, <2 x i64>* %608, align 16
  %610 = getelementptr inbounds <2 x i64>, <2 x i64>* %556, i64 1
  %611 = load <2 x i64>, <2 x i64>* %610, align 16
  %612 = shufflevector <2 x i64> %609, <2 x i64> %611, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %613 = getelementptr inbounds <2 x i64>, <2 x i64>* %560, i64 1
  %614 = load <2 x i64>, <2 x i64>* %613, align 16
  %615 = getelementptr inbounds <2 x i64>, <2 x i64>* %563, i64 1
  %616 = load <2 x i64>, <2 x i64>* %615, align 16
  %617 = shufflevector <2 x i64> %614, <2 x i64> %616, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %618 = getelementptr inbounds <2 x i64>, <2 x i64>* %567, i64 1
  %619 = load <2 x i64>, <2 x i64>* %618, align 16
  %620 = getelementptr inbounds <2 x i64>, <2 x i64>* %570, i64 1
  %621 = load <2 x i64>, <2 x i64>* %620, align 16
  %622 = shufflevector <2 x i64> %619, <2 x i64> %621, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %623 = getelementptr inbounds <2 x i64>, <2 x i64>* %574, i64 1
  %624 = load <2 x i64>, <2 x i64>* %623, align 16
  %625 = getelementptr inbounds <2 x i64>, <2 x i64>* %577, i64 1
  %626 = load <2 x i64>, <2 x i64>* %625, align 16
  %627 = shufflevector <2 x i64> %624, <2 x i64> %626, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %628 = getelementptr inbounds <2 x i64>, <2 x i64>* %581, i64 1
  %629 = load <2 x i64>, <2 x i64>* %628, align 16
  %630 = getelementptr inbounds <2 x i64>, <2 x i64>* %584, i64 1
  %631 = load <2 x i64>, <2 x i64>* %630, align 16
  %632 = shufflevector <2 x i64> %629, <2 x i64> %631, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %633 = getelementptr inbounds <2 x i64>, <2 x i64>* %588, i64 1
  %634 = load <2 x i64>, <2 x i64>* %633, align 16
  %635 = getelementptr inbounds <2 x i64>, <2 x i64>* %591, i64 1
  %636 = load <2 x i64>, <2 x i64>* %635, align 16
  %637 = shufflevector <2 x i64> %634, <2 x i64> %636, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %638 = getelementptr inbounds <2 x i64>, <2 x i64>* %595, i64 1
  %639 = load <2 x i64>, <2 x i64>* %638, align 16
  %640 = getelementptr inbounds <2 x i64>, <2 x i64>* %598, i64 1
  %641 = load <2 x i64>, <2 x i64>* %640, align 16
  %642 = shufflevector <2 x i64> %639, <2 x i64> %641, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %643 = getelementptr inbounds <2 x i64>, <2 x i64>* %602, i64 1
  %644 = load <2 x i64>, <2 x i64>* %643, align 16
  %645 = getelementptr inbounds <2 x i64>, <2 x i64>* %605, i64 1
  %646 = load <2 x i64>, <2 x i64>* %645, align 16
  %647 = shufflevector <2 x i64> %644, <2 x i64> %646, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %648 = bitcast <4 x i64> %558 to <16 x i16>
  %649 = bitcast <4 x i64> %565 to <16 x i16>
  %650 = shufflevector <16 x i16> %648, <16 x i16> %649, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %651 = shufflevector <16 x i16> %648, <16 x i16> %649, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %652 = bitcast <4 x i64> %572 to <16 x i16>
  %653 = bitcast <4 x i64> %579 to <16 x i16>
  %654 = shufflevector <16 x i16> %652, <16 x i16> %653, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %655 = shufflevector <16 x i16> %652, <16 x i16> %653, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %656 = bitcast <4 x i64> %586 to <16 x i16>
  %657 = bitcast <4 x i64> %593 to <16 x i16>
  %658 = shufflevector <16 x i16> %656, <16 x i16> %657, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %659 = shufflevector <16 x i16> %656, <16 x i16> %657, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %660 = bitcast <4 x i64> %600 to <16 x i16>
  %661 = bitcast <4 x i64> %607 to <16 x i16>
  %662 = shufflevector <16 x i16> %660, <16 x i16> %661, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %663 = shufflevector <16 x i16> %660, <16 x i16> %661, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %664 = bitcast <16 x i16> %650 to <8 x i32>
  %665 = bitcast <16 x i16> %654 to <8 x i32>
  %666 = shufflevector <8 x i32> %664, <8 x i32> %665, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %667 = shufflevector <8 x i32> %664, <8 x i32> %665, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %668 = bitcast <16 x i16> %658 to <8 x i32>
  %669 = bitcast <16 x i16> %662 to <8 x i32>
  %670 = shufflevector <8 x i32> %668, <8 x i32> %669, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %671 = shufflevector <8 x i32> %668, <8 x i32> %669, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %672 = bitcast <16 x i16> %651 to <8 x i32>
  %673 = bitcast <16 x i16> %655 to <8 x i32>
  %674 = shufflevector <8 x i32> %672, <8 x i32> %673, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %675 = shufflevector <8 x i32> %672, <8 x i32> %673, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %676 = bitcast <16 x i16> %659 to <8 x i32>
  %677 = bitcast <16 x i16> %663 to <8 x i32>
  %678 = shufflevector <8 x i32> %676, <8 x i32> %677, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %679 = shufflevector <8 x i32> %676, <8 x i32> %677, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %680 = bitcast <8 x i32> %666 to <4 x i64>
  %681 = bitcast <8 x i32> %670 to <4 x i64>
  %682 = shufflevector <4 x i64> %680, <4 x i64> %681, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %682, <4 x i64>* %391, align 32
  %683 = shufflevector <4 x i64> %680, <4 x i64> %681, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %683, <4 x i64>* %559, align 32
  %684 = bitcast <8 x i32> %674 to <4 x i64>
  %685 = bitcast <8 x i32> %678 to <4 x i64>
  %686 = shufflevector <4 x i64> %684, <4 x i64> %685, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %686, <4 x i64>* %580, align 32
  %687 = shufflevector <4 x i64> %684, <4 x i64> %685, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %687, <4 x i64>* %587, align 32
  %688 = bitcast <8 x i32> %667 to <4 x i64>
  %689 = bitcast <8 x i32> %671 to <4 x i64>
  %690 = shufflevector <4 x i64> %688, <4 x i64> %689, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %690, <4 x i64>* %566, align 32
  %691 = shufflevector <4 x i64> %688, <4 x i64> %689, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %691, <4 x i64>* %573, align 32
  %692 = bitcast <8 x i32> %675 to <4 x i64>
  %693 = bitcast <8 x i32> %679 to <4 x i64>
  %694 = shufflevector <4 x i64> %692, <4 x i64> %693, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %694, <4 x i64>* %594, align 32
  %695 = shufflevector <4 x i64> %692, <4 x i64> %693, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %695, <4 x i64>* %601, align 32
  %696 = bitcast <4 x i64> %612 to <16 x i16>
  %697 = bitcast <4 x i64> %617 to <16 x i16>
  %698 = shufflevector <16 x i16> %696, <16 x i16> %697, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %699 = shufflevector <16 x i16> %696, <16 x i16> %697, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %700 = bitcast <4 x i64> %622 to <16 x i16>
  %701 = bitcast <4 x i64> %627 to <16 x i16>
  %702 = shufflevector <16 x i16> %700, <16 x i16> %701, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %703 = shufflevector <16 x i16> %700, <16 x i16> %701, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %704 = bitcast <4 x i64> %632 to <16 x i16>
  %705 = bitcast <4 x i64> %637 to <16 x i16>
  %706 = shufflevector <16 x i16> %704, <16 x i16> %705, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %707 = shufflevector <16 x i16> %704, <16 x i16> %705, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %708 = bitcast <4 x i64> %642 to <16 x i16>
  %709 = bitcast <4 x i64> %647 to <16 x i16>
  %710 = shufflevector <16 x i16> %708, <16 x i16> %709, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %711 = shufflevector <16 x i16> %708, <16 x i16> %709, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %712 = bitcast <16 x i16> %698 to <8 x i32>
  %713 = bitcast <16 x i16> %702 to <8 x i32>
  %714 = shufflevector <8 x i32> %712, <8 x i32> %713, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %715 = shufflevector <8 x i32> %712, <8 x i32> %713, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %716 = bitcast <16 x i16> %706 to <8 x i32>
  %717 = bitcast <16 x i16> %710 to <8 x i32>
  %718 = shufflevector <8 x i32> %716, <8 x i32> %717, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %719 = shufflevector <8 x i32> %716, <8 x i32> %717, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %720 = bitcast <16 x i16> %699 to <8 x i32>
  %721 = bitcast <16 x i16> %703 to <8 x i32>
  %722 = shufflevector <8 x i32> %720, <8 x i32> %721, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %723 = shufflevector <8 x i32> %720, <8 x i32> %721, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %724 = bitcast <16 x i16> %707 to <8 x i32>
  %725 = bitcast <16 x i16> %711 to <8 x i32>
  %726 = shufflevector <8 x i32> %724, <8 x i32> %725, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %727 = shufflevector <8 x i32> %724, <8 x i32> %725, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %728 = bitcast <8 x i32> %714 to <4 x i64>
  %729 = bitcast <8 x i32> %718 to <4 x i64>
  %730 = shufflevector <4 x i64> %728, <4 x i64> %729, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %730, <4 x i64>* %555, align 32
  %731 = shufflevector <4 x i64> %728, <4 x i64> %729, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %731, <4 x i64>* %562, align 32
  %732 = bitcast <8 x i32> %722 to <4 x i64>
  %733 = bitcast <8 x i32> %726 to <4 x i64>
  %734 = shufflevector <4 x i64> %732, <4 x i64> %733, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %734, <4 x i64>* %583, align 32
  %735 = shufflevector <4 x i64> %732, <4 x i64> %733, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %735, <4 x i64>* %590, align 32
  %736 = bitcast <8 x i32> %715 to <4 x i64>
  %737 = bitcast <8 x i32> %719 to <4 x i64>
  %738 = shufflevector <4 x i64> %736, <4 x i64> %737, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %738, <4 x i64>* %569, align 32
  %739 = shufflevector <4 x i64> %736, <4 x i64> %737, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %739, <4 x i64>* %576, align 32
  %740 = bitcast <8 x i32> %723 to <4 x i64>
  %741 = bitcast <8 x i32> %727 to <4 x i64>
  %742 = shufflevector <4 x i64> %740, <4 x i64> %741, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %742, <4 x i64>* %597, align 32
  %743 = shufflevector <4 x i64> %740, <4 x i64> %741, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %743, <4 x i64>* %604, align 32
  %744 = shufflevector <4 x i64> %682, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %745 = bitcast <2 x i64> %744 to <8 x i16>
  %746 = sext <8 x i16> %745 to <8 x i32>
  %747 = bitcast i32* %552 to <8 x i32>*
  store <8 x i32> %746, <8 x i32>* %747, align 32
  %748 = getelementptr inbounds i32, i32* %552, i64 8
  %749 = load <4 x i64>, <4 x i64>* %391, align 32
  %750 = shufflevector <4 x i64> %749, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %751 = bitcast <2 x i64> %750 to <8 x i16>
  %752 = sext <8 x i16> %751 to <8 x i32>
  %753 = bitcast i32* %748 to <8 x i32>*
  store <8 x i32> %752, <8 x i32>* %753, align 32
  br label %754

754:                                              ; preds = %773, %550
  %755 = phi i64 [ 1, %550 ], [ %788, %773 ]
  %756 = phi i32* [ %552, %550 ], [ %774, %773 ]
  %757 = getelementptr inbounds i32, i32* %756, i64 16
  %758 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 %755
  %759 = load <4 x i64>, <4 x i64>* %758, align 32
  %760 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 %755
  %761 = shufflevector <4 x i64> %759, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %762 = bitcast <2 x i64> %761 to <8 x i16>
  %763 = sext <8 x i16> %762 to <8 x i32>
  %764 = bitcast i32* %757 to <8 x i32>*
  store <8 x i32> %763, <8 x i32>* %764, align 32
  %765 = getelementptr inbounds i32, i32* %756, i64 24
  %766 = load <4 x i64>, <4 x i64>* %760, align 32
  %767 = shufflevector <4 x i64> %766, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %768 = bitcast <2 x i64> %767 to <8 x i16>
  %769 = sext <8 x i16> %768 to <8 x i32>
  %770 = bitcast i32* %765 to <8 x i32>*
  store <8 x i32> %769, <8 x i32>* %770, align 32
  %771 = add nuw nsw i64 %755, 1
  %772 = icmp eq i64 %771, 16
  br i1 %772, label %547, label %773

773:                                              ; preds = %754
  %774 = getelementptr inbounds i32, i32* %756, i64 32
  %775 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 %771
  %776 = load <4 x i64>, <4 x i64>* %775, align 32
  %777 = getelementptr inbounds <4 x i64>, <4 x i64>* %391, i64 %771
  %778 = shufflevector <4 x i64> %776, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %779 = bitcast <2 x i64> %778 to <8 x i16>
  %780 = sext <8 x i16> %779 to <8 x i32>
  %781 = bitcast i32* %774 to <8 x i32>*
  store <8 x i32> %780, <8 x i32>* %781, align 32
  %782 = getelementptr inbounds i32, i32* %756, i64 40
  %783 = load <4 x i64>, <4 x i64>* %777, align 32
  %784 = shufflevector <4 x i64> %783, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %785 = bitcast <2 x i64> %784 to <8 x i16>
  %786 = sext <8 x i16> %785 to <8 x i32>
  %787 = bitcast i32* %782 to <8 x i32>*
  store <8 x i32> %786, <8 x i32>* %787, align 32
  %788 = add nuw nsw i64 %755, 2
  br label %754
}

; Function Attrs: nounwind ssp uwtable
define internal void @lowbd_fwd_txfm2d_64x16_avx2(i16* nocapture readonly, i32* nocapture, i32, i8 zeroext, i32) #3 {
  %6 = alloca [64 x <4 x i64>], align 32
  %7 = alloca [64 x <4 x i64>], align 32
  %8 = bitcast [64 x <4 x i64>]* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 2048, i8* nonnull %8) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %8, i8 -86, i64 2048, i1 false)
  %9 = bitcast [64 x <4 x i64>]* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 2048, i8* nonnull %9) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %9, i8 -86, i64 2048, i1 false)
  %10 = load i8*, i8** getelementptr inbounds ([19 x i8*], [19 x i8*]* @av1_fwd_txfm_shift_ls, i64 0, i64 18), align 16
  %11 = load i8, i8* getelementptr inbounds ([5 x [5 x i8]], [5 x [5 x i8]]* @av1_fwd_cos_bit_col, i64 0, i64 4, i64 2), align 2
  %12 = load i8, i8* getelementptr inbounds ([5 x [5 x i8]], [5 x [5 x i8]]* @av1_fwd_cos_bit_row, i64 0, i64 4, i64 2), align 2
  %13 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 0
  %14 = sext i32 %2 to i64
  %15 = getelementptr inbounds i8, i8* %10, i64 1
  %16 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 1
  %17 = shl nsw i64 %14, 1
  %18 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 2
  %19 = mul nsw i64 %14, 3
  %20 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 3
  %21 = shl nsw i64 %14, 2
  %22 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 4
  %23 = mul nsw i64 %14, 5
  %24 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 5
  %25 = mul nsw i64 %14, 6
  %26 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 6
  %27 = mul nsw i64 %14, 7
  %28 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 7
  %29 = shl nsw i64 %14, 3
  %30 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 8
  %31 = mul nsw i64 %14, 9
  %32 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 9
  %33 = mul nsw i64 %14, 10
  %34 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 10
  %35 = mul nsw i64 %14, 11
  %36 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 11
  %37 = mul nsw i64 %14, 12
  %38 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 12
  %39 = mul nsw i64 %14, 13
  %40 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 13
  %41 = mul nsw i64 %14, 14
  %42 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 14
  %43 = mul nsw i64 %14, 15
  %44 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %6, i64 0, i64 15
  %45 = bitcast [64 x <4 x i64>]* %6 to <16 x i16>*
  %46 = bitcast <4 x i64>* %16 to <16 x i16>*
  %47 = bitcast <4 x i64>* %18 to <16 x i16>*
  %48 = bitcast <4 x i64>* %20 to <16 x i16>*
  %49 = bitcast <4 x i64>* %22 to <16 x i16>*
  %50 = bitcast <4 x i64>* %24 to <16 x i16>*
  %51 = bitcast <4 x i64>* %26 to <16 x i16>*
  %52 = bitcast <4 x i64>* %28 to <16 x i16>*
  %53 = bitcast <4 x i64>* %30 to <16 x i16>*
  %54 = bitcast <4 x i64>* %32 to <16 x i16>*
  %55 = bitcast <4 x i64>* %34 to <16 x i16>*
  %56 = bitcast <4 x i64>* %36 to <16 x i16>*
  %57 = bitcast <4 x i64>* %38 to <16 x i16>*
  %58 = bitcast <4 x i64>* %40 to <16 x i16>*
  %59 = bitcast <4 x i64>* %42 to <16 x i16>*
  %60 = bitcast <4 x i64>* %44 to <16 x i16>*
  %61 = bitcast [64 x <4 x i64>]* %6 to <16 x i16>*
  %62 = bitcast <4 x i64>* %16 to <16 x i16>*
  %63 = bitcast <4 x i64>* %18 to <16 x i16>*
  %64 = bitcast <4 x i64>* %20 to <16 x i16>*
  %65 = bitcast <4 x i64>* %22 to <16 x i16>*
  %66 = bitcast <4 x i64>* %24 to <16 x i16>*
  %67 = bitcast <4 x i64>* %26 to <16 x i16>*
  %68 = bitcast <4 x i64>* %28 to <16 x i16>*
  %69 = bitcast <4 x i64>* %30 to <16 x i16>*
  %70 = bitcast <4 x i64>* %32 to <16 x i16>*
  %71 = bitcast <4 x i64>* %34 to <16 x i16>*
  %72 = bitcast <4 x i64>* %36 to <16 x i16>*
  %73 = bitcast <4 x i64>* %38 to <16 x i16>*
  %74 = bitcast <4 x i64>* %40 to <16 x i16>*
  %75 = bitcast <4 x i64>* %42 to <16 x i16>*
  %76 = bitcast <4 x i64>* %44 to <16 x i16>*
  %77 = bitcast [64 x <4 x i64>]* %6 to <16 x i16>*
  %78 = bitcast <4 x i64>* %16 to <16 x i16>*
  %79 = bitcast <4 x i64>* %18 to <16 x i16>*
  %80 = bitcast <4 x i64>* %20 to <16 x i16>*
  %81 = bitcast <4 x i64>* %22 to <16 x i16>*
  %82 = bitcast <4 x i64>* %24 to <16 x i16>*
  %83 = bitcast <4 x i64>* %26 to <16 x i16>*
  %84 = bitcast <4 x i64>* %28 to <16 x i16>*
  %85 = bitcast <4 x i64>* %30 to <16 x i16>*
  %86 = bitcast <4 x i64>* %32 to <16 x i16>*
  %87 = bitcast <4 x i64>* %34 to <16 x i16>*
  %88 = bitcast <4 x i64>* %36 to <16 x i16>*
  %89 = bitcast <4 x i64>* %38 to <16 x i16>*
  %90 = bitcast <4 x i64>* %40 to <16 x i16>*
  %91 = bitcast <4 x i64>* %42 to <16 x i16>*
  %92 = bitcast <4 x i64>* %44 to <16 x i16>*
  %93 = bitcast [64 x <4 x i64>]* %6 to <2 x i64>*
  %94 = bitcast <4 x i64>* %30 to <2 x i64>*
  %95 = bitcast <4 x i64>* %16 to <2 x i64>*
  %96 = bitcast <4 x i64>* %32 to <2 x i64>*
  %97 = bitcast <4 x i64>* %18 to <2 x i64>*
  %98 = bitcast <4 x i64>* %34 to <2 x i64>*
  %99 = bitcast <4 x i64>* %20 to <2 x i64>*
  %100 = bitcast <4 x i64>* %36 to <2 x i64>*
  %101 = bitcast <4 x i64>* %22 to <2 x i64>*
  %102 = bitcast <4 x i64>* %38 to <2 x i64>*
  %103 = bitcast <4 x i64>* %24 to <2 x i64>*
  %104 = bitcast <4 x i64>* %40 to <2 x i64>*
  %105 = bitcast <4 x i64>* %26 to <2 x i64>*
  %106 = bitcast <4 x i64>* %42 to <2 x i64>*
  %107 = bitcast <4 x i64>* %28 to <2 x i64>*
  %108 = bitcast <4 x i64>* %44 to <2 x i64>*
  %109 = getelementptr inbounds <2 x i64>, <2 x i64>* %94, i64 1
  %110 = getelementptr inbounds <2 x i64>, <2 x i64>* %96, i64 1
  %111 = getelementptr inbounds <2 x i64>, <2 x i64>* %98, i64 1
  %112 = getelementptr inbounds <2 x i64>, <2 x i64>* %100, i64 1
  %113 = getelementptr inbounds <2 x i64>, <2 x i64>* %102, i64 1
  %114 = getelementptr inbounds <2 x i64>, <2 x i64>* %104, i64 1
  %115 = getelementptr inbounds <2 x i64>, <2 x i64>* %106, i64 1
  %116 = getelementptr inbounds <2 x i64>, <2 x i64>* %108, i64 1
  %117 = bitcast [64 x <4 x i64>]* %6 to <16 x i16>*
  %118 = bitcast <4 x i64>* %16 to <16 x i16>*
  %119 = bitcast <4 x i64>* %18 to <16 x i16>*
  %120 = bitcast <4 x i64>* %20 to <16 x i16>*
  %121 = bitcast <4 x i64>* %22 to <16 x i16>*
  %122 = bitcast <4 x i64>* %24 to <16 x i16>*
  %123 = bitcast <4 x i64>* %26 to <16 x i16>*
  %124 = bitcast <4 x i64>* %28 to <16 x i16>*
  %125 = bitcast <4 x i64>* %30 to <16 x i16>*
  %126 = bitcast <4 x i64>* %32 to <16 x i16>*
  %127 = bitcast <4 x i64>* %34 to <16 x i16>*
  %128 = bitcast <4 x i64>* %36 to <16 x i16>*
  %129 = bitcast <4 x i64>* %38 to <16 x i16>*
  %130 = bitcast <4 x i64>* %40 to <16 x i16>*
  %131 = bitcast <4 x i64>* %42 to <16 x i16>*
  %132 = bitcast <4 x i64>* %44 to <16 x i16>*
  %133 = bitcast [64 x <4 x i64>]* %6 to <2 x i64>*
  %134 = bitcast <4 x i64>* %16 to <2 x i64>*
  %135 = bitcast <4 x i64>* %18 to <2 x i64>*
  %136 = bitcast <4 x i64>* %20 to <2 x i64>*
  %137 = bitcast <4 x i64>* %22 to <2 x i64>*
  %138 = bitcast <4 x i64>* %24 to <2 x i64>*
  %139 = bitcast <4 x i64>* %26 to <2 x i64>*
  %140 = bitcast <4 x i64>* %28 to <2 x i64>*
  %141 = getelementptr inbounds <2 x i64>, <2 x i64>* %133, i64 1
  %142 = getelementptr inbounds <2 x i64>, <2 x i64>* %134, i64 1
  %143 = getelementptr inbounds <2 x i64>, <2 x i64>* %135, i64 1
  %144 = getelementptr inbounds <2 x i64>, <2 x i64>* %136, i64 1
  %145 = getelementptr inbounds <2 x i64>, <2 x i64>* %137, i64 1
  %146 = getelementptr inbounds <2 x i64>, <2 x i64>* %138, i64 1
  %147 = getelementptr inbounds <2 x i64>, <2 x i64>* %139, i64 1
  %148 = getelementptr inbounds <2 x i64>, <2 x i64>* %140, i64 1
  br label %149

149:                                              ; preds = %545, %5
  %150 = phi i64 [ 0, %5 ], [ %706, %545 ]
  %151 = shl nsw i64 %150, 4
  %152 = getelementptr inbounds i16, i16* %0, i64 %151
  %153 = bitcast i16* %152 to <4 x i64>*
  %154 = load <4 x i64>, <4 x i64>* %153, align 32
  store <4 x i64> %154, <4 x i64>* %13, align 32
  %155 = getelementptr inbounds i16, i16* %152, i64 %14
  %156 = bitcast i16* %155 to <4 x i64>*
  %157 = load <4 x i64>, <4 x i64>* %156, align 32
  store <4 x i64> %157, <4 x i64>* %16, align 32
  %158 = getelementptr inbounds i16, i16* %152, i64 %17
  %159 = bitcast i16* %158 to <4 x i64>*
  %160 = load <4 x i64>, <4 x i64>* %159, align 32
  store <4 x i64> %160, <4 x i64>* %18, align 32
  %161 = getelementptr inbounds i16, i16* %152, i64 %19
  %162 = bitcast i16* %161 to <4 x i64>*
  %163 = load <4 x i64>, <4 x i64>* %162, align 32
  store <4 x i64> %163, <4 x i64>* %20, align 32
  %164 = getelementptr inbounds i16, i16* %152, i64 %21
  %165 = bitcast i16* %164 to <4 x i64>*
  %166 = load <4 x i64>, <4 x i64>* %165, align 32
  store <4 x i64> %166, <4 x i64>* %22, align 32
  %167 = getelementptr inbounds i16, i16* %152, i64 %23
  %168 = bitcast i16* %167 to <4 x i64>*
  %169 = load <4 x i64>, <4 x i64>* %168, align 32
  store <4 x i64> %169, <4 x i64>* %24, align 32
  %170 = getelementptr inbounds i16, i16* %152, i64 %25
  %171 = bitcast i16* %170 to <4 x i64>*
  %172 = load <4 x i64>, <4 x i64>* %171, align 32
  store <4 x i64> %172, <4 x i64>* %26, align 32
  %173 = getelementptr inbounds i16, i16* %152, i64 %27
  %174 = bitcast i16* %173 to <4 x i64>*
  %175 = load <4 x i64>, <4 x i64>* %174, align 32
  store <4 x i64> %175, <4 x i64>* %28, align 32
  %176 = getelementptr inbounds i16, i16* %152, i64 %29
  %177 = bitcast i16* %176 to <4 x i64>*
  %178 = load <4 x i64>, <4 x i64>* %177, align 32
  store <4 x i64> %178, <4 x i64>* %30, align 32
  %179 = getelementptr inbounds i16, i16* %152, i64 %31
  %180 = bitcast i16* %179 to <4 x i64>*
  %181 = load <4 x i64>, <4 x i64>* %180, align 32
  store <4 x i64> %181, <4 x i64>* %32, align 32
  %182 = getelementptr inbounds i16, i16* %152, i64 %33
  %183 = bitcast i16* %182 to <4 x i64>*
  %184 = load <4 x i64>, <4 x i64>* %183, align 32
  store <4 x i64> %184, <4 x i64>* %34, align 32
  %185 = getelementptr inbounds i16, i16* %152, i64 %35
  %186 = bitcast i16* %185 to <4 x i64>*
  %187 = load <4 x i64>, <4 x i64>* %186, align 32
  store <4 x i64> %187, <4 x i64>* %36, align 32
  %188 = getelementptr inbounds i16, i16* %152, i64 %37
  %189 = bitcast i16* %188 to <4 x i64>*
  %190 = load <4 x i64>, <4 x i64>* %189, align 32
  store <4 x i64> %190, <4 x i64>* %38, align 32
  %191 = getelementptr inbounds i16, i16* %152, i64 %39
  %192 = bitcast i16* %191 to <4 x i64>*
  %193 = load <4 x i64>, <4 x i64>* %192, align 32
  store <4 x i64> %193, <4 x i64>* %40, align 32
  %194 = getelementptr inbounds i16, i16* %152, i64 %41
  %195 = bitcast i16* %194 to <4 x i64>*
  %196 = load <4 x i64>, <4 x i64>* %195, align 32
  store <4 x i64> %196, <4 x i64>* %42, align 32
  %197 = getelementptr inbounds i16, i16* %152, i64 %43
  %198 = bitcast i16* %197 to <4 x i64>*
  %199 = load <4 x i64>, <4 x i64>* %198, align 32
  store <4 x i64> %199, <4 x i64>* %44, align 32
  %200 = load i8, i8* %10, align 1
  %201 = sext i8 %200 to i32
  %202 = icmp slt i8 %200, 0
  %203 = bitcast <4 x i64> %154 to <16 x i16>
  %204 = bitcast <4 x i64> %157 to <16 x i16>
  %205 = bitcast <4 x i64> %160 to <16 x i16>
  %206 = bitcast <4 x i64> %163 to <16 x i16>
  %207 = bitcast <4 x i64> %166 to <16 x i16>
  %208 = bitcast <4 x i64> %169 to <16 x i16>
  %209 = bitcast <4 x i64> %172 to <16 x i16>
  %210 = bitcast <4 x i64> %175 to <16 x i16>
  %211 = bitcast <4 x i64> %178 to <16 x i16>
  %212 = bitcast <4 x i64> %181 to <16 x i16>
  %213 = bitcast <4 x i64> %184 to <16 x i16>
  %214 = bitcast <4 x i64> %187 to <16 x i16>
  %215 = bitcast <4 x i64> %190 to <16 x i16>
  %216 = bitcast <4 x i64> %193 to <16 x i16>
  %217 = bitcast <4 x i64> %196 to <16 x i16>
  %218 = bitcast <4 x i64> %199 to <16 x i16>
  br i1 %202, label %219, label %263

219:                                              ; preds = %149
  %220 = sub nsw i32 0, %201
  %221 = xor i32 %201, -1
  %222 = shl i32 1, %221
  %223 = trunc i32 %222 to i16
  %224 = insertelement <16 x i16> undef, i16 %223, i32 0
  %225 = shufflevector <16 x i16> %224, <16 x i16> undef, <16 x i32> zeroinitializer
  %226 = load <16 x i16>, <16 x i16>* %61, align 32
  %227 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %226, <16 x i16> %225) #8
  %228 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %227, i32 %220) #8
  store <16 x i16> %228, <16 x i16>* %61, align 32
  %229 = load <16 x i16>, <16 x i16>* %62, align 32
  %230 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %229, <16 x i16> %225) #8
  %231 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %230, i32 %220) #8
  store <16 x i16> %231, <16 x i16>* %62, align 32
  %232 = load <16 x i16>, <16 x i16>* %63, align 32
  %233 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %232, <16 x i16> %225) #8
  %234 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %233, i32 %220) #8
  store <16 x i16> %234, <16 x i16>* %63, align 32
  %235 = load <16 x i16>, <16 x i16>* %64, align 32
  %236 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %235, <16 x i16> %225) #8
  %237 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %236, i32 %220) #8
  store <16 x i16> %237, <16 x i16>* %64, align 32
  %238 = load <16 x i16>, <16 x i16>* %65, align 32
  %239 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %238, <16 x i16> %225) #8
  %240 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %239, i32 %220) #8
  store <16 x i16> %240, <16 x i16>* %65, align 32
  %241 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %208, <16 x i16> %225) #8
  %242 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %241, i32 %220) #8
  store <16 x i16> %242, <16 x i16>* %66, align 32
  %243 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %209, <16 x i16> %225) #8
  %244 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %243, i32 %220) #8
  store <16 x i16> %244, <16 x i16>* %67, align 32
  %245 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %210, <16 x i16> %225) #8
  %246 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %245, i32 %220) #8
  store <16 x i16> %246, <16 x i16>* %68, align 32
  %247 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %211, <16 x i16> %225) #8
  %248 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %247, i32 %220) #8
  store <16 x i16> %248, <16 x i16>* %69, align 32
  %249 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %212, <16 x i16> %225) #8
  %250 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %249, i32 %220) #8
  store <16 x i16> %250, <16 x i16>* %70, align 32
  %251 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %213, <16 x i16> %225) #8
  %252 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %251, i32 %220) #8
  store <16 x i16> %252, <16 x i16>* %71, align 32
  %253 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %214, <16 x i16> %225) #8
  %254 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %253, i32 %220) #8
  store <16 x i16> %254, <16 x i16>* %72, align 32
  %255 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %215, <16 x i16> %225) #8
  %256 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %255, i32 %220) #8
  store <16 x i16> %256, <16 x i16>* %73, align 32
  %257 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %216, <16 x i16> %225) #8
  %258 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %257, i32 %220) #8
  store <16 x i16> %258, <16 x i16>* %74, align 32
  %259 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %217, <16 x i16> %225) #8
  %260 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %259, i32 %220) #8
  store <16 x i16> %260, <16 x i16>* %75, align 32
  %261 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %218, <16 x i16> %225) #8
  %262 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %261, i32 %220) #8
  store <16 x i16> %262, <16 x i16>* %76, align 32
  br label %282

263:                                              ; preds = %149
  %264 = icmp eq i8 %200, 0
  br i1 %264, label %282, label %265

265:                                              ; preds = %263
  %266 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %203, i32 %201) #8
  store <16 x i16> %266, <16 x i16>* %45, align 32
  %267 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %204, i32 %201) #8
  store <16 x i16> %267, <16 x i16>* %46, align 32
  %268 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %205, i32 %201) #8
  store <16 x i16> %268, <16 x i16>* %47, align 32
  %269 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %206, i32 %201) #8
  store <16 x i16> %269, <16 x i16>* %48, align 32
  %270 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %207, i32 %201) #8
  store <16 x i16> %270, <16 x i16>* %49, align 32
  %271 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %208, i32 %201) #8
  store <16 x i16> %271, <16 x i16>* %50, align 32
  %272 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %209, i32 %201) #8
  store <16 x i16> %272, <16 x i16>* %51, align 32
  %273 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %210, i32 %201) #8
  store <16 x i16> %273, <16 x i16>* %52, align 32
  %274 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %211, i32 %201) #8
  store <16 x i16> %274, <16 x i16>* %53, align 32
  %275 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %212, i32 %201) #8
  store <16 x i16> %275, <16 x i16>* %54, align 32
  %276 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %213, i32 %201) #8
  store <16 x i16> %276, <16 x i16>* %55, align 32
  %277 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %214, i32 %201) #8
  store <16 x i16> %277, <16 x i16>* %56, align 32
  %278 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %215, i32 %201) #8
  store <16 x i16> %278, <16 x i16>* %57, align 32
  %279 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %216, i32 %201) #8
  store <16 x i16> %279, <16 x i16>* %58, align 32
  %280 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %217, i32 %201) #8
  store <16 x i16> %280, <16 x i16>* %59, align 32
  %281 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %218, i32 %201) #8
  store <16 x i16> %281, <16 x i16>* %60, align 32
  br label %282

282:                                              ; preds = %265, %219, %263
  call void @fdct16x16_new_avx2(<4 x i64>* nonnull %13, <4 x i64>* nonnull %13, i8 signext %11)
  %283 = load i8, i8* %15, align 1
  %284 = sext i8 %283 to i32
  %285 = icmp slt i8 %283, 0
  br i1 %285, label %286, label %413

286:                                              ; preds = %282
  %287 = sub nsw i32 0, %284
  %288 = xor i32 %284, -1
  %289 = shl i32 1, %288
  %290 = trunc i32 %289 to i16
  %291 = insertelement <16 x i16> undef, i16 %290, i32 0
  %292 = shufflevector <16 x i16> %291, <16 x i16> undef, <16 x i32> zeroinitializer
  %293 = load <16 x i16>, <16 x i16>* %117, align 32
  %294 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %293, <16 x i16> %292) #8
  %295 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %294, i32 %287) #8
  store <16 x i16> %295, <16 x i16>* %117, align 32
  %296 = load <16 x i16>, <16 x i16>* %118, align 32
  %297 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %296, <16 x i16> %292) #8
  %298 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %297, i32 %287) #8
  store <16 x i16> %298, <16 x i16>* %118, align 32
  %299 = load <16 x i16>, <16 x i16>* %119, align 32
  %300 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %299, <16 x i16> %292) #8
  %301 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %300, i32 %287) #8
  store <16 x i16> %301, <16 x i16>* %119, align 32
  %302 = load <16 x i16>, <16 x i16>* %120, align 32
  %303 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %302, <16 x i16> %292) #8
  %304 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %303, i32 %287) #8
  store <16 x i16> %304, <16 x i16>* %120, align 32
  %305 = load <16 x i16>, <16 x i16>* %121, align 32
  %306 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %305, <16 x i16> %292) #8
  %307 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %306, i32 %287) #8
  store <16 x i16> %307, <16 x i16>* %121, align 32
  %308 = load <16 x i16>, <16 x i16>* %122, align 32
  %309 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %308, <16 x i16> %292) #8
  %310 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %309, i32 %287) #8
  store <16 x i16> %310, <16 x i16>* %122, align 32
  %311 = load <16 x i16>, <16 x i16>* %123, align 32
  %312 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %311, <16 x i16> %292) #8
  %313 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %312, i32 %287) #8
  store <16 x i16> %313, <16 x i16>* %123, align 32
  %314 = load <16 x i16>, <16 x i16>* %124, align 32
  %315 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %314, <16 x i16> %292) #8
  %316 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %315, i32 %287) #8
  store <16 x i16> %316, <16 x i16>* %124, align 32
  %317 = load <16 x i16>, <16 x i16>* %125, align 32
  %318 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %317, <16 x i16> %292) #8
  %319 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %318, i32 %287) #8
  store <16 x i16> %319, <16 x i16>* %125, align 32
  %320 = load <16 x i16>, <16 x i16>* %126, align 32
  %321 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %320, <16 x i16> %292) #8
  %322 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %321, i32 %287) #8
  store <16 x i16> %322, <16 x i16>* %126, align 32
  %323 = load <16 x i16>, <16 x i16>* %127, align 32
  %324 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %323, <16 x i16> %292) #8
  %325 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %324, i32 %287) #8
  store <16 x i16> %325, <16 x i16>* %127, align 32
  %326 = load <16 x i16>, <16 x i16>* %128, align 32
  %327 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %326, <16 x i16> %292) #8
  %328 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %327, i32 %287) #8
  store <16 x i16> %328, <16 x i16>* %128, align 32
  %329 = load <16 x i16>, <16 x i16>* %129, align 32
  %330 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %329, <16 x i16> %292) #8
  %331 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %330, i32 %287) #8
  store <16 x i16> %331, <16 x i16>* %129, align 32
  %332 = load <16 x i16>, <16 x i16>* %130, align 32
  %333 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %332, <16 x i16> %292) #8
  %334 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %333, i32 %287) #8
  store <16 x i16> %334, <16 x i16>* %130, align 32
  %335 = load <16 x i16>, <16 x i16>* %131, align 32
  %336 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %335, <16 x i16> %292) #8
  %337 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %336, i32 %287) #8
  store <16 x i16> %337, <16 x i16>* %131, align 32
  %338 = load <16 x i16>, <16 x i16>* %132, align 32
  %339 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %338, <16 x i16> %292) #8
  %340 = tail call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %339, i32 %287) #8
  store <16 x i16> %340, <16 x i16>* %132, align 32
  %341 = bitcast <16 x i16> %295 to <2 x i128>
  %342 = extractelement <2 x i128> %341, i32 0
  %343 = bitcast i128 %342 to <2 x i64>
  %344 = bitcast <16 x i16> %319 to <2 x i128>
  %345 = extractelement <2 x i128> %344, i32 0
  %346 = bitcast i128 %345 to <2 x i64>
  %347 = bitcast <16 x i16> %298 to <2 x i128>
  %348 = extractelement <2 x i128> %347, i32 0
  %349 = bitcast i128 %348 to <2 x i64>
  %350 = bitcast <16 x i16> %322 to <2 x i128>
  %351 = extractelement <2 x i128> %350, i32 0
  %352 = bitcast i128 %351 to <2 x i64>
  %353 = bitcast <16 x i16> %301 to <2 x i128>
  %354 = extractelement <2 x i128> %353, i32 0
  %355 = bitcast i128 %354 to <2 x i64>
  %356 = bitcast <16 x i16> %325 to <2 x i128>
  %357 = extractelement <2 x i128> %356, i32 0
  %358 = bitcast i128 %357 to <2 x i64>
  %359 = bitcast <16 x i16> %304 to <2 x i128>
  %360 = extractelement <2 x i128> %359, i32 0
  %361 = bitcast i128 %360 to <2 x i64>
  %362 = bitcast <16 x i16> %328 to <2 x i128>
  %363 = extractelement <2 x i128> %362, i32 0
  %364 = bitcast i128 %363 to <2 x i64>
  %365 = bitcast <16 x i16> %307 to <2 x i128>
  %366 = extractelement <2 x i128> %365, i32 0
  %367 = bitcast i128 %366 to <2 x i64>
  %368 = bitcast <16 x i16> %331 to <2 x i128>
  %369 = extractelement <2 x i128> %368, i32 0
  %370 = bitcast i128 %369 to <2 x i64>
  %371 = bitcast <16 x i16> %310 to <2 x i128>
  %372 = extractelement <2 x i128> %371, i32 0
  %373 = bitcast i128 %372 to <2 x i64>
  %374 = bitcast <16 x i16> %334 to <2 x i128>
  %375 = extractelement <2 x i128> %374, i32 0
  %376 = bitcast i128 %375 to <2 x i64>
  %377 = bitcast <16 x i16> %313 to <2 x i128>
  %378 = extractelement <2 x i128> %377, i32 0
  %379 = bitcast i128 %378 to <2 x i64>
  %380 = bitcast <16 x i16> %337 to <2 x i128>
  %381 = extractelement <2 x i128> %380, i32 0
  %382 = bitcast i128 %381 to <2 x i64>
  %383 = bitcast <16 x i16> %316 to <2 x i128>
  %384 = extractelement <2 x i128> %383, i32 0
  %385 = bitcast i128 %384 to <2 x i64>
  %386 = bitcast <16 x i16> %340 to <2 x i128>
  %387 = extractelement <2 x i128> %386, i32 0
  %388 = bitcast i128 %387 to <2 x i64>
  %389 = bitcast <16 x i16> %319 to <2 x i128>
  %390 = extractelement <2 x i128> %389, i32 1
  %391 = bitcast i128 %390 to <2 x i64>
  %392 = bitcast <16 x i16> %322 to <2 x i128>
  %393 = extractelement <2 x i128> %392, i32 1
  %394 = bitcast i128 %393 to <2 x i64>
  %395 = bitcast <16 x i16> %325 to <2 x i128>
  %396 = extractelement <2 x i128> %395, i32 1
  %397 = bitcast i128 %396 to <2 x i64>
  %398 = bitcast <16 x i16> %328 to <2 x i128>
  %399 = extractelement <2 x i128> %398, i32 1
  %400 = bitcast i128 %399 to <2 x i64>
  %401 = bitcast <16 x i16> %331 to <2 x i128>
  %402 = extractelement <2 x i128> %401, i32 1
  %403 = bitcast i128 %402 to <2 x i64>
  %404 = bitcast <16 x i16> %334 to <2 x i128>
  %405 = extractelement <2 x i128> %404, i32 1
  %406 = bitcast i128 %405 to <2 x i64>
  %407 = bitcast <16 x i16> %337 to <2 x i128>
  %408 = extractelement <2 x i128> %407, i32 1
  %409 = bitcast i128 %408 to <2 x i64>
  %410 = bitcast <16 x i16> %340 to <2 x i128>
  %411 = extractelement <2 x i128> %410, i32 1
  %412 = bitcast i128 %411 to <2 x i64>
  br label %545

413:                                              ; preds = %282
  %414 = icmp eq i8 %283, 0
  br i1 %414, label %415, label %440

415:                                              ; preds = %413
  %416 = load <2 x i64>, <2 x i64>* %93, align 32
  %417 = load <2 x i64>, <2 x i64>* %94, align 32
  %418 = load <2 x i64>, <2 x i64>* %95, align 32
  %419 = load <2 x i64>, <2 x i64>* %96, align 32
  %420 = load <2 x i64>, <2 x i64>* %97, align 32
  %421 = load <2 x i64>, <2 x i64>* %98, align 32
  %422 = load <2 x i64>, <2 x i64>* %99, align 32
  %423 = load <2 x i64>, <2 x i64>* %100, align 32
  %424 = load <2 x i64>, <2 x i64>* %101, align 32
  %425 = load <2 x i64>, <2 x i64>* %102, align 32
  %426 = load <2 x i64>, <2 x i64>* %103, align 32
  %427 = load <2 x i64>, <2 x i64>* %104, align 32
  %428 = load <2 x i64>, <2 x i64>* %105, align 32
  %429 = load <2 x i64>, <2 x i64>* %106, align 32
  %430 = load <2 x i64>, <2 x i64>* %107, align 32
  %431 = load <2 x i64>, <2 x i64>* %108, align 32
  %432 = load <2 x i64>, <2 x i64>* %109, align 16
  %433 = load <2 x i64>, <2 x i64>* %110, align 16
  %434 = load <2 x i64>, <2 x i64>* %111, align 16
  %435 = load <2 x i64>, <2 x i64>* %112, align 16
  %436 = load <2 x i64>, <2 x i64>* %113, align 16
  %437 = load <2 x i64>, <2 x i64>* %114, align 16
  %438 = load <2 x i64>, <2 x i64>* %115, align 16
  %439 = load <2 x i64>, <2 x i64>* %116, align 16
  br label %545

440:                                              ; preds = %413
  %441 = load <16 x i16>, <16 x i16>* %77, align 32
  %442 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %441, i32 %284) #8
  store <16 x i16> %442, <16 x i16>* %77, align 32
  %443 = load <16 x i16>, <16 x i16>* %78, align 32
  %444 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %443, i32 %284) #8
  store <16 x i16> %444, <16 x i16>* %78, align 32
  %445 = load <16 x i16>, <16 x i16>* %79, align 32
  %446 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %445, i32 %284) #8
  store <16 x i16> %446, <16 x i16>* %79, align 32
  %447 = load <16 x i16>, <16 x i16>* %80, align 32
  %448 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %447, i32 %284) #8
  store <16 x i16> %448, <16 x i16>* %80, align 32
  %449 = load <16 x i16>, <16 x i16>* %81, align 32
  %450 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %449, i32 %284) #8
  store <16 x i16> %450, <16 x i16>* %81, align 32
  %451 = load <16 x i16>, <16 x i16>* %82, align 32
  %452 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %451, i32 %284) #8
  store <16 x i16> %452, <16 x i16>* %82, align 32
  %453 = load <16 x i16>, <16 x i16>* %83, align 32
  %454 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %453, i32 %284) #8
  store <16 x i16> %454, <16 x i16>* %83, align 32
  %455 = load <16 x i16>, <16 x i16>* %84, align 32
  %456 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %455, i32 %284) #8
  store <16 x i16> %456, <16 x i16>* %84, align 32
  %457 = load <16 x i16>, <16 x i16>* %85, align 32
  %458 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %457, i32 %284) #8
  store <16 x i16> %458, <16 x i16>* %85, align 32
  %459 = load <16 x i16>, <16 x i16>* %86, align 32
  %460 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %459, i32 %284) #8
  store <16 x i16> %460, <16 x i16>* %86, align 32
  %461 = load <16 x i16>, <16 x i16>* %87, align 32
  %462 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %461, i32 %284) #8
  store <16 x i16> %462, <16 x i16>* %87, align 32
  %463 = load <16 x i16>, <16 x i16>* %88, align 32
  %464 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %463, i32 %284) #8
  store <16 x i16> %464, <16 x i16>* %88, align 32
  %465 = load <16 x i16>, <16 x i16>* %89, align 32
  %466 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %465, i32 %284) #8
  store <16 x i16> %466, <16 x i16>* %89, align 32
  %467 = load <16 x i16>, <16 x i16>* %90, align 32
  %468 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %467, i32 %284) #8
  store <16 x i16> %468, <16 x i16>* %90, align 32
  %469 = load <16 x i16>, <16 x i16>* %91, align 32
  %470 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %469, i32 %284) #8
  store <16 x i16> %470, <16 x i16>* %91, align 32
  %471 = load <16 x i16>, <16 x i16>* %92, align 32
  %472 = tail call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %471, i32 %284) #8
  store <16 x i16> %472, <16 x i16>* %92, align 32
  %473 = bitcast <16 x i16> %442 to <2 x i128>
  %474 = extractelement <2 x i128> %473, i32 0
  %475 = bitcast i128 %474 to <2 x i64>
  %476 = bitcast <16 x i16> %458 to <2 x i128>
  %477 = extractelement <2 x i128> %476, i32 0
  %478 = bitcast i128 %477 to <2 x i64>
  %479 = bitcast <16 x i16> %444 to <2 x i128>
  %480 = extractelement <2 x i128> %479, i32 0
  %481 = bitcast i128 %480 to <2 x i64>
  %482 = bitcast <16 x i16> %460 to <2 x i128>
  %483 = extractelement <2 x i128> %482, i32 0
  %484 = bitcast i128 %483 to <2 x i64>
  %485 = bitcast <16 x i16> %446 to <2 x i128>
  %486 = extractelement <2 x i128> %485, i32 0
  %487 = bitcast i128 %486 to <2 x i64>
  %488 = bitcast <16 x i16> %462 to <2 x i128>
  %489 = extractelement <2 x i128> %488, i32 0
  %490 = bitcast i128 %489 to <2 x i64>
  %491 = bitcast <16 x i16> %448 to <2 x i128>
  %492 = extractelement <2 x i128> %491, i32 0
  %493 = bitcast i128 %492 to <2 x i64>
  %494 = bitcast <16 x i16> %464 to <2 x i128>
  %495 = extractelement <2 x i128> %494, i32 0
  %496 = bitcast i128 %495 to <2 x i64>
  %497 = bitcast <16 x i16> %450 to <2 x i128>
  %498 = extractelement <2 x i128> %497, i32 0
  %499 = bitcast i128 %498 to <2 x i64>
  %500 = bitcast <16 x i16> %466 to <2 x i128>
  %501 = extractelement <2 x i128> %500, i32 0
  %502 = bitcast i128 %501 to <2 x i64>
  %503 = bitcast <16 x i16> %452 to <2 x i128>
  %504 = extractelement <2 x i128> %503, i32 0
  %505 = bitcast i128 %504 to <2 x i64>
  %506 = bitcast <16 x i16> %468 to <2 x i128>
  %507 = extractelement <2 x i128> %506, i32 0
  %508 = bitcast i128 %507 to <2 x i64>
  %509 = bitcast <16 x i16> %454 to <2 x i128>
  %510 = extractelement <2 x i128> %509, i32 0
  %511 = bitcast i128 %510 to <2 x i64>
  %512 = bitcast <16 x i16> %470 to <2 x i128>
  %513 = extractelement <2 x i128> %512, i32 0
  %514 = bitcast i128 %513 to <2 x i64>
  %515 = bitcast <16 x i16> %456 to <2 x i128>
  %516 = extractelement <2 x i128> %515, i32 0
  %517 = bitcast i128 %516 to <2 x i64>
  %518 = bitcast <16 x i16> %472 to <2 x i128>
  %519 = extractelement <2 x i128> %518, i32 0
  %520 = bitcast i128 %519 to <2 x i64>
  %521 = bitcast <16 x i16> %458 to <2 x i128>
  %522 = extractelement <2 x i128> %521, i32 1
  %523 = bitcast i128 %522 to <2 x i64>
  %524 = bitcast <16 x i16> %460 to <2 x i128>
  %525 = extractelement <2 x i128> %524, i32 1
  %526 = bitcast i128 %525 to <2 x i64>
  %527 = bitcast <16 x i16> %462 to <2 x i128>
  %528 = extractelement <2 x i128> %527, i32 1
  %529 = bitcast i128 %528 to <2 x i64>
  %530 = bitcast <16 x i16> %464 to <2 x i128>
  %531 = extractelement <2 x i128> %530, i32 1
  %532 = bitcast i128 %531 to <2 x i64>
  %533 = bitcast <16 x i16> %466 to <2 x i128>
  %534 = extractelement <2 x i128> %533, i32 1
  %535 = bitcast i128 %534 to <2 x i64>
  %536 = bitcast <16 x i16> %468 to <2 x i128>
  %537 = extractelement <2 x i128> %536, i32 1
  %538 = bitcast i128 %537 to <2 x i64>
  %539 = bitcast <16 x i16> %470 to <2 x i128>
  %540 = extractelement <2 x i128> %539, i32 1
  %541 = bitcast i128 %540 to <2 x i64>
  %542 = bitcast <16 x i16> %472 to <2 x i128>
  %543 = extractelement <2 x i128> %542, i32 1
  %544 = bitcast i128 %543 to <2 x i64>
  br label %545

545:                                              ; preds = %415, %440, %286
  %546 = phi <2 x i64> [ %439, %415 ], [ %544, %440 ], [ %412, %286 ]
  %547 = phi <2 x i64> [ %438, %415 ], [ %541, %440 ], [ %409, %286 ]
  %548 = phi <2 x i64> [ %437, %415 ], [ %538, %440 ], [ %406, %286 ]
  %549 = phi <2 x i64> [ %436, %415 ], [ %535, %440 ], [ %403, %286 ]
  %550 = phi <2 x i64> [ %435, %415 ], [ %532, %440 ], [ %400, %286 ]
  %551 = phi <2 x i64> [ %434, %415 ], [ %529, %440 ], [ %397, %286 ]
  %552 = phi <2 x i64> [ %433, %415 ], [ %526, %440 ], [ %394, %286 ]
  %553 = phi <2 x i64> [ %432, %415 ], [ %523, %440 ], [ %391, %286 ]
  %554 = phi <2 x i64> [ %431, %415 ], [ %520, %440 ], [ %388, %286 ]
  %555 = phi <2 x i64> [ %430, %415 ], [ %517, %440 ], [ %385, %286 ]
  %556 = phi <2 x i64> [ %429, %415 ], [ %514, %440 ], [ %382, %286 ]
  %557 = phi <2 x i64> [ %428, %415 ], [ %511, %440 ], [ %379, %286 ]
  %558 = phi <2 x i64> [ %427, %415 ], [ %508, %440 ], [ %376, %286 ]
  %559 = phi <2 x i64> [ %426, %415 ], [ %505, %440 ], [ %373, %286 ]
  %560 = phi <2 x i64> [ %425, %415 ], [ %502, %440 ], [ %370, %286 ]
  %561 = phi <2 x i64> [ %424, %415 ], [ %499, %440 ], [ %367, %286 ]
  %562 = phi <2 x i64> [ %423, %415 ], [ %496, %440 ], [ %364, %286 ]
  %563 = phi <2 x i64> [ %422, %415 ], [ %493, %440 ], [ %361, %286 ]
  %564 = phi <2 x i64> [ %421, %415 ], [ %490, %440 ], [ %358, %286 ]
  %565 = phi <2 x i64> [ %420, %415 ], [ %487, %440 ], [ %355, %286 ]
  %566 = phi <2 x i64> [ %419, %415 ], [ %484, %440 ], [ %352, %286 ]
  %567 = phi <2 x i64> [ %418, %415 ], [ %481, %440 ], [ %349, %286 ]
  %568 = phi <2 x i64> [ %417, %415 ], [ %478, %440 ], [ %346, %286 ]
  %569 = phi <2 x i64> [ %416, %415 ], [ %475, %440 ], [ %343, %286 ]
  %570 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %151
  %571 = shufflevector <2 x i64> %569, <2 x i64> %568, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %572 = shufflevector <2 x i64> %567, <2 x i64> %566, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %573 = shufflevector <2 x i64> %565, <2 x i64> %564, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %574 = shufflevector <2 x i64> %563, <2 x i64> %562, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %575 = shufflevector <2 x i64> %561, <2 x i64> %560, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %576 = shufflevector <2 x i64> %559, <2 x i64> %558, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %577 = shufflevector <2 x i64> %557, <2 x i64> %556, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %578 = shufflevector <2 x i64> %555, <2 x i64> %554, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %579 = load <2 x i64>, <2 x i64>* %141, align 16
  %580 = shufflevector <2 x i64> %579, <2 x i64> %553, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %581 = load <2 x i64>, <2 x i64>* %142, align 16
  %582 = shufflevector <2 x i64> %581, <2 x i64> %552, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %583 = load <2 x i64>, <2 x i64>* %143, align 16
  %584 = shufflevector <2 x i64> %583, <2 x i64> %551, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %585 = load <2 x i64>, <2 x i64>* %144, align 16
  %586 = shufflevector <2 x i64> %585, <2 x i64> %550, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %587 = load <2 x i64>, <2 x i64>* %145, align 16
  %588 = shufflevector <2 x i64> %587, <2 x i64> %549, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %589 = load <2 x i64>, <2 x i64>* %146, align 16
  %590 = shufflevector <2 x i64> %589, <2 x i64> %548, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %591 = load <2 x i64>, <2 x i64>* %147, align 16
  %592 = shufflevector <2 x i64> %591, <2 x i64> %547, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %593 = load <2 x i64>, <2 x i64>* %148, align 16
  %594 = shufflevector <2 x i64> %593, <2 x i64> %546, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %595 = bitcast <4 x i64> %571 to <16 x i16>
  %596 = bitcast <4 x i64> %572 to <16 x i16>
  %597 = shufflevector <16 x i16> %595, <16 x i16> %596, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %598 = shufflevector <16 x i16> %595, <16 x i16> %596, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %599 = bitcast <4 x i64> %573 to <16 x i16>
  %600 = bitcast <4 x i64> %574 to <16 x i16>
  %601 = shufflevector <16 x i16> %599, <16 x i16> %600, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %602 = shufflevector <16 x i16> %599, <16 x i16> %600, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %603 = bitcast <4 x i64> %575 to <16 x i16>
  %604 = bitcast <4 x i64> %576 to <16 x i16>
  %605 = shufflevector <16 x i16> %603, <16 x i16> %604, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %606 = shufflevector <16 x i16> %603, <16 x i16> %604, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %607 = bitcast <4 x i64> %577 to <16 x i16>
  %608 = bitcast <4 x i64> %578 to <16 x i16>
  %609 = shufflevector <16 x i16> %607, <16 x i16> %608, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %610 = shufflevector <16 x i16> %607, <16 x i16> %608, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %611 = bitcast <16 x i16> %597 to <8 x i32>
  %612 = bitcast <16 x i16> %601 to <8 x i32>
  %613 = shufflevector <8 x i32> %611, <8 x i32> %612, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %614 = shufflevector <8 x i32> %611, <8 x i32> %612, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %615 = bitcast <16 x i16> %605 to <8 x i32>
  %616 = bitcast <16 x i16> %609 to <8 x i32>
  %617 = shufflevector <8 x i32> %615, <8 x i32> %616, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %618 = shufflevector <8 x i32> %615, <8 x i32> %616, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %619 = bitcast <16 x i16> %598 to <8 x i32>
  %620 = bitcast <16 x i16> %602 to <8 x i32>
  %621 = shufflevector <8 x i32> %619, <8 x i32> %620, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %622 = shufflevector <8 x i32> %619, <8 x i32> %620, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %623 = bitcast <16 x i16> %606 to <8 x i32>
  %624 = bitcast <16 x i16> %610 to <8 x i32>
  %625 = shufflevector <8 x i32> %623, <8 x i32> %624, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %626 = shufflevector <8 x i32> %623, <8 x i32> %624, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %627 = bitcast <8 x i32> %613 to <4 x i64>
  %628 = bitcast <8 x i32> %617 to <4 x i64>
  %629 = shufflevector <4 x i64> %627, <4 x i64> %628, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %629, <4 x i64>* %570, align 32
  %630 = shufflevector <4 x i64> %627, <4 x i64> %628, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %631 = getelementptr inbounds <4 x i64>, <4 x i64>* %570, i64 1
  store <4 x i64> %630, <4 x i64>* %631, align 32
  %632 = bitcast <8 x i32> %621 to <4 x i64>
  %633 = bitcast <8 x i32> %625 to <4 x i64>
  %634 = shufflevector <4 x i64> %632, <4 x i64> %633, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %635 = getelementptr inbounds <4 x i64>, <4 x i64>* %570, i64 4
  store <4 x i64> %634, <4 x i64>* %635, align 32
  %636 = shufflevector <4 x i64> %632, <4 x i64> %633, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %637 = getelementptr inbounds <4 x i64>, <4 x i64>* %570, i64 5
  store <4 x i64> %636, <4 x i64>* %637, align 32
  %638 = bitcast <8 x i32> %614 to <4 x i64>
  %639 = bitcast <8 x i32> %618 to <4 x i64>
  %640 = shufflevector <4 x i64> %638, <4 x i64> %639, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %641 = getelementptr inbounds <4 x i64>, <4 x i64>* %570, i64 2
  store <4 x i64> %640, <4 x i64>* %641, align 32
  %642 = shufflevector <4 x i64> %638, <4 x i64> %639, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %643 = getelementptr inbounds <4 x i64>, <4 x i64>* %570, i64 3
  store <4 x i64> %642, <4 x i64>* %643, align 32
  %644 = bitcast <8 x i32> %622 to <4 x i64>
  %645 = bitcast <8 x i32> %626 to <4 x i64>
  %646 = shufflevector <4 x i64> %644, <4 x i64> %645, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %647 = getelementptr inbounds <4 x i64>, <4 x i64>* %570, i64 6
  store <4 x i64> %646, <4 x i64>* %647, align 32
  %648 = shufflevector <4 x i64> %644, <4 x i64> %645, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %649 = getelementptr inbounds <4 x i64>, <4 x i64>* %570, i64 7
  store <4 x i64> %648, <4 x i64>* %649, align 32
  %650 = getelementptr inbounds <4 x i64>, <4 x i64>* %570, i64 8
  %651 = bitcast <4 x i64> %580 to <16 x i16>
  %652 = bitcast <4 x i64> %582 to <16 x i16>
  %653 = shufflevector <16 x i16> %651, <16 x i16> %652, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %654 = shufflevector <16 x i16> %651, <16 x i16> %652, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %655 = bitcast <4 x i64> %584 to <16 x i16>
  %656 = bitcast <4 x i64> %586 to <16 x i16>
  %657 = shufflevector <16 x i16> %655, <16 x i16> %656, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %658 = shufflevector <16 x i16> %655, <16 x i16> %656, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %659 = bitcast <4 x i64> %588 to <16 x i16>
  %660 = bitcast <4 x i64> %590 to <16 x i16>
  %661 = shufflevector <16 x i16> %659, <16 x i16> %660, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %662 = shufflevector <16 x i16> %659, <16 x i16> %660, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %663 = bitcast <4 x i64> %592 to <16 x i16>
  %664 = bitcast <4 x i64> %594 to <16 x i16>
  %665 = shufflevector <16 x i16> %663, <16 x i16> %664, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %666 = shufflevector <16 x i16> %663, <16 x i16> %664, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %667 = bitcast <16 x i16> %653 to <8 x i32>
  %668 = bitcast <16 x i16> %657 to <8 x i32>
  %669 = shufflevector <8 x i32> %667, <8 x i32> %668, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %670 = shufflevector <8 x i32> %667, <8 x i32> %668, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %671 = bitcast <16 x i16> %661 to <8 x i32>
  %672 = bitcast <16 x i16> %665 to <8 x i32>
  %673 = shufflevector <8 x i32> %671, <8 x i32> %672, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %674 = shufflevector <8 x i32> %671, <8 x i32> %672, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %675 = bitcast <16 x i16> %654 to <8 x i32>
  %676 = bitcast <16 x i16> %658 to <8 x i32>
  %677 = shufflevector <8 x i32> %675, <8 x i32> %676, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %678 = shufflevector <8 x i32> %675, <8 x i32> %676, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %679 = bitcast <16 x i16> %662 to <8 x i32>
  %680 = bitcast <16 x i16> %666 to <8 x i32>
  %681 = shufflevector <8 x i32> %679, <8 x i32> %680, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %682 = shufflevector <8 x i32> %679, <8 x i32> %680, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %683 = bitcast <8 x i32> %669 to <4 x i64>
  %684 = bitcast <8 x i32> %673 to <4 x i64>
  %685 = shufflevector <4 x i64> %683, <4 x i64> %684, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %685, <4 x i64>* %650, align 32
  %686 = shufflevector <4 x i64> %683, <4 x i64> %684, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %687 = getelementptr inbounds <4 x i64>, <4 x i64>* %570, i64 9
  store <4 x i64> %686, <4 x i64>* %687, align 32
  %688 = bitcast <8 x i32> %677 to <4 x i64>
  %689 = bitcast <8 x i32> %681 to <4 x i64>
  %690 = shufflevector <4 x i64> %688, <4 x i64> %689, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %691 = getelementptr inbounds <4 x i64>, <4 x i64>* %570, i64 12
  store <4 x i64> %690, <4 x i64>* %691, align 32
  %692 = shufflevector <4 x i64> %688, <4 x i64> %689, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %693 = getelementptr inbounds <4 x i64>, <4 x i64>* %570, i64 13
  store <4 x i64> %692, <4 x i64>* %693, align 32
  %694 = bitcast <8 x i32> %670 to <4 x i64>
  %695 = bitcast <8 x i32> %674 to <4 x i64>
  %696 = shufflevector <4 x i64> %694, <4 x i64> %695, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %697 = getelementptr inbounds <4 x i64>, <4 x i64>* %570, i64 10
  store <4 x i64> %696, <4 x i64>* %697, align 32
  %698 = shufflevector <4 x i64> %694, <4 x i64> %695, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %699 = getelementptr inbounds <4 x i64>, <4 x i64>* %570, i64 11
  store <4 x i64> %698, <4 x i64>* %699, align 32
  %700 = bitcast <8 x i32> %678 to <4 x i64>
  %701 = bitcast <8 x i32> %682 to <4 x i64>
  %702 = shufflevector <4 x i64> %700, <4 x i64> %701, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %703 = getelementptr inbounds <4 x i64>, <4 x i64>* %570, i64 14
  store <4 x i64> %702, <4 x i64>* %703, align 32
  %704 = shufflevector <4 x i64> %700, <4 x i64> %701, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %705 = getelementptr inbounds <4 x i64>, <4 x i64>* %570, i64 15
  store <4 x i64> %704, <4 x i64>* %705, align 32
  %706 = add nuw nsw i64 %150, 1
  %707 = icmp eq i64 %706, 4
  br i1 %707, label %709, label %149

708:                                              ; preds = %1000
  call void @llvm.lifetime.end.p0i8(i64 2048, i8* nonnull %9) #8
  call void @llvm.lifetime.end.p0i8(i64 2048, i8* nonnull %8) #8
  ret void

709:                                              ; preds = %545
  %710 = getelementptr inbounds i8, i8* %10, i64 2
  %711 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 0
  call fastcc void @fdct16x64_new_avx2(<4 x i64>* nonnull %711, <4 x i64>* nonnull %711, i8 signext %12)
  %712 = load i8, i8* %710, align 1
  %713 = sext i8 %712 to i32
  %714 = icmp slt i8 %712, 0
  br i1 %714, label %715, label %749

715:                                              ; preds = %709
  %716 = sub nsw i32 0, %713
  %717 = xor i32 %713, -1
  %718 = shl i32 1, %717
  %719 = trunc i32 %718 to i16
  %720 = insertelement <16 x i16> undef, i16 %719, i32 0
  %721 = shufflevector <16 x i16> %720, <16 x i16> undef, <16 x i32> zeroinitializer
  br label %722

722:                                              ; preds = %722, %715
  %723 = phi i64 [ 0, %715 ], [ %747, %722 ]
  %724 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %723
  %725 = bitcast <4 x i64>* %724 to <16 x i16>*
  %726 = load <16 x i16>, <16 x i16>* %725, align 32
  %727 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %726, <16 x i16> %721) #8
  %728 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %727, i32 %716) #8
  store <16 x i16> %728, <16 x i16>* %725, align 32
  %729 = or i64 %723, 1
  %730 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %729
  %731 = bitcast <4 x i64>* %730 to <16 x i16>*
  %732 = load <16 x i16>, <16 x i16>* %731, align 32
  %733 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %732, <16 x i16> %721) #8
  %734 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %733, i32 %716) #8
  store <16 x i16> %734, <16 x i16>* %731, align 32
  %735 = or i64 %723, 2
  %736 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %735
  %737 = bitcast <4 x i64>* %736 to <16 x i16>*
  %738 = load <16 x i16>, <16 x i16>* %737, align 32
  %739 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %738, <16 x i16> %721) #8
  %740 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %739, i32 %716) #8
  store <16 x i16> %740, <16 x i16>* %737, align 32
  %741 = or i64 %723, 3
  %742 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %741
  %743 = bitcast <4 x i64>* %742 to <16 x i16>*
  %744 = load <16 x i16>, <16 x i16>* %743, align 32
  %745 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %744, <16 x i16> %721) #8
  %746 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %745, i32 %716) #8
  store <16 x i16> %746, <16 x i16>* %743, align 32
  %747 = add nuw nsw i64 %723, 4
  %748 = icmp eq i64 %747, 64
  br i1 %748, label %774, label %722

749:                                              ; preds = %709
  %750 = icmp eq i8 %712, 0
  br i1 %750, label %774, label %751

751:                                              ; preds = %749, %751
  %752 = phi i64 [ %772, %751 ], [ 0, %749 ]
  %753 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %752
  %754 = bitcast <4 x i64>* %753 to <16 x i16>*
  %755 = load <16 x i16>, <16 x i16>* %754, align 32
  %756 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %755, i32 %713) #8
  store <16 x i16> %756, <16 x i16>* %754, align 32
  %757 = or i64 %752, 1
  %758 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %757
  %759 = bitcast <4 x i64>* %758 to <16 x i16>*
  %760 = load <16 x i16>, <16 x i16>* %759, align 32
  %761 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %760, i32 %713) #8
  store <16 x i16> %761, <16 x i16>* %759, align 32
  %762 = or i64 %752, 2
  %763 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %762
  %764 = bitcast <4 x i64>* %763 to <16 x i16>*
  %765 = load <16 x i16>, <16 x i16>* %764, align 32
  %766 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %765, i32 %713) #8
  store <16 x i16> %766, <16 x i16>* %764, align 32
  %767 = or i64 %752, 3
  %768 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %767
  %769 = bitcast <4 x i64>* %768 to <16 x i16>*
  %770 = load <16 x i16>, <16 x i16>* %769, align 32
  %771 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %770, i32 %713) #8
  store <16 x i16> %771, <16 x i16>* %769, align 32
  %772 = add nuw nsw i64 %752, 4
  %773 = icmp eq i64 %772, 64
  br i1 %773, label %774, label %751

774:                                              ; preds = %751, %722, %749
  br label %775

775:                                              ; preds = %774, %1000
  %776 = phi i64 [ %1001, %1000 ], [ 0, %774 ]
  %777 = shl nsw i64 %776, 4
  %778 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %777
  %779 = bitcast <4 x i64>* %778 to <2 x i64>*
  %780 = load <2 x i64>, <2 x i64>* %779, align 32
  %781 = getelementptr inbounds <4 x i64>, <4 x i64>* %778, i64 8
  %782 = bitcast <4 x i64>* %781 to <2 x i64>*
  %783 = load <2 x i64>, <2 x i64>* %782, align 32
  %784 = shufflevector <2 x i64> %780, <2 x i64> %783, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %785 = getelementptr inbounds <4 x i64>, <4 x i64>* %778, i64 1
  %786 = bitcast <4 x i64>* %785 to <2 x i64>*
  %787 = load <2 x i64>, <2 x i64>* %786, align 32
  %788 = getelementptr inbounds <4 x i64>, <4 x i64>* %778, i64 9
  %789 = bitcast <4 x i64>* %788 to <2 x i64>*
  %790 = load <2 x i64>, <2 x i64>* %789, align 32
  %791 = shufflevector <2 x i64> %787, <2 x i64> %790, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %792 = getelementptr inbounds <4 x i64>, <4 x i64>* %778, i64 2
  %793 = bitcast <4 x i64>* %792 to <2 x i64>*
  %794 = load <2 x i64>, <2 x i64>* %793, align 32
  %795 = getelementptr inbounds <4 x i64>, <4 x i64>* %778, i64 10
  %796 = bitcast <4 x i64>* %795 to <2 x i64>*
  %797 = load <2 x i64>, <2 x i64>* %796, align 32
  %798 = shufflevector <2 x i64> %794, <2 x i64> %797, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %799 = getelementptr inbounds <4 x i64>, <4 x i64>* %778, i64 3
  %800 = bitcast <4 x i64>* %799 to <2 x i64>*
  %801 = load <2 x i64>, <2 x i64>* %800, align 32
  %802 = getelementptr inbounds <4 x i64>, <4 x i64>* %778, i64 11
  %803 = bitcast <4 x i64>* %802 to <2 x i64>*
  %804 = load <2 x i64>, <2 x i64>* %803, align 32
  %805 = shufflevector <2 x i64> %801, <2 x i64> %804, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %806 = getelementptr inbounds <4 x i64>, <4 x i64>* %778, i64 4
  %807 = bitcast <4 x i64>* %806 to <2 x i64>*
  %808 = load <2 x i64>, <2 x i64>* %807, align 32
  %809 = getelementptr inbounds <4 x i64>, <4 x i64>* %778, i64 12
  %810 = bitcast <4 x i64>* %809 to <2 x i64>*
  %811 = load <2 x i64>, <2 x i64>* %810, align 32
  %812 = shufflevector <2 x i64> %808, <2 x i64> %811, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %813 = getelementptr inbounds <4 x i64>, <4 x i64>* %778, i64 5
  %814 = bitcast <4 x i64>* %813 to <2 x i64>*
  %815 = load <2 x i64>, <2 x i64>* %814, align 32
  %816 = getelementptr inbounds <4 x i64>, <4 x i64>* %778, i64 13
  %817 = bitcast <4 x i64>* %816 to <2 x i64>*
  %818 = load <2 x i64>, <2 x i64>* %817, align 32
  %819 = shufflevector <2 x i64> %815, <2 x i64> %818, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %820 = getelementptr inbounds <4 x i64>, <4 x i64>* %778, i64 6
  %821 = bitcast <4 x i64>* %820 to <2 x i64>*
  %822 = load <2 x i64>, <2 x i64>* %821, align 32
  %823 = getelementptr inbounds <4 x i64>, <4 x i64>* %778, i64 14
  %824 = bitcast <4 x i64>* %823 to <2 x i64>*
  %825 = load <2 x i64>, <2 x i64>* %824, align 32
  %826 = shufflevector <2 x i64> %822, <2 x i64> %825, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %827 = getelementptr inbounds <4 x i64>, <4 x i64>* %778, i64 7
  %828 = bitcast <4 x i64>* %827 to <2 x i64>*
  %829 = load <2 x i64>, <2 x i64>* %828, align 32
  %830 = getelementptr inbounds <4 x i64>, <4 x i64>* %778, i64 15
  %831 = bitcast <4 x i64>* %830 to <2 x i64>*
  %832 = load <2 x i64>, <2 x i64>* %831, align 32
  %833 = shufflevector <2 x i64> %829, <2 x i64> %832, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %834 = getelementptr inbounds <2 x i64>, <2 x i64>* %779, i64 1
  %835 = load <2 x i64>, <2 x i64>* %834, align 16
  %836 = getelementptr inbounds <2 x i64>, <2 x i64>* %782, i64 1
  %837 = load <2 x i64>, <2 x i64>* %836, align 16
  %838 = shufflevector <2 x i64> %835, <2 x i64> %837, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %839 = getelementptr inbounds <2 x i64>, <2 x i64>* %786, i64 1
  %840 = load <2 x i64>, <2 x i64>* %839, align 16
  %841 = getelementptr inbounds <2 x i64>, <2 x i64>* %789, i64 1
  %842 = load <2 x i64>, <2 x i64>* %841, align 16
  %843 = shufflevector <2 x i64> %840, <2 x i64> %842, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %844 = getelementptr inbounds <2 x i64>, <2 x i64>* %793, i64 1
  %845 = load <2 x i64>, <2 x i64>* %844, align 16
  %846 = getelementptr inbounds <2 x i64>, <2 x i64>* %796, i64 1
  %847 = load <2 x i64>, <2 x i64>* %846, align 16
  %848 = shufflevector <2 x i64> %845, <2 x i64> %847, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %849 = getelementptr inbounds <2 x i64>, <2 x i64>* %800, i64 1
  %850 = load <2 x i64>, <2 x i64>* %849, align 16
  %851 = getelementptr inbounds <2 x i64>, <2 x i64>* %803, i64 1
  %852 = load <2 x i64>, <2 x i64>* %851, align 16
  %853 = shufflevector <2 x i64> %850, <2 x i64> %852, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %854 = getelementptr inbounds <2 x i64>, <2 x i64>* %807, i64 1
  %855 = load <2 x i64>, <2 x i64>* %854, align 16
  %856 = getelementptr inbounds <2 x i64>, <2 x i64>* %810, i64 1
  %857 = load <2 x i64>, <2 x i64>* %856, align 16
  %858 = shufflevector <2 x i64> %855, <2 x i64> %857, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %859 = getelementptr inbounds <2 x i64>, <2 x i64>* %814, i64 1
  %860 = load <2 x i64>, <2 x i64>* %859, align 16
  %861 = getelementptr inbounds <2 x i64>, <2 x i64>* %817, i64 1
  %862 = load <2 x i64>, <2 x i64>* %861, align 16
  %863 = shufflevector <2 x i64> %860, <2 x i64> %862, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %864 = getelementptr inbounds <2 x i64>, <2 x i64>* %821, i64 1
  %865 = load <2 x i64>, <2 x i64>* %864, align 16
  %866 = getelementptr inbounds <2 x i64>, <2 x i64>* %824, i64 1
  %867 = load <2 x i64>, <2 x i64>* %866, align 16
  %868 = shufflevector <2 x i64> %865, <2 x i64> %867, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %869 = getelementptr inbounds <2 x i64>, <2 x i64>* %828, i64 1
  %870 = load <2 x i64>, <2 x i64>* %869, align 16
  %871 = getelementptr inbounds <2 x i64>, <2 x i64>* %831, i64 1
  %872 = load <2 x i64>, <2 x i64>* %871, align 16
  %873 = shufflevector <2 x i64> %870, <2 x i64> %872, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %874 = bitcast <4 x i64> %784 to <16 x i16>
  %875 = bitcast <4 x i64> %791 to <16 x i16>
  %876 = shufflevector <16 x i16> %874, <16 x i16> %875, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %877 = shufflevector <16 x i16> %874, <16 x i16> %875, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %878 = bitcast <4 x i64> %798 to <16 x i16>
  %879 = bitcast <4 x i64> %805 to <16 x i16>
  %880 = shufflevector <16 x i16> %878, <16 x i16> %879, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %881 = shufflevector <16 x i16> %878, <16 x i16> %879, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %882 = bitcast <4 x i64> %812 to <16 x i16>
  %883 = bitcast <4 x i64> %819 to <16 x i16>
  %884 = shufflevector <16 x i16> %882, <16 x i16> %883, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %885 = shufflevector <16 x i16> %882, <16 x i16> %883, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %886 = bitcast <4 x i64> %826 to <16 x i16>
  %887 = bitcast <4 x i64> %833 to <16 x i16>
  %888 = shufflevector <16 x i16> %886, <16 x i16> %887, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %889 = shufflevector <16 x i16> %886, <16 x i16> %887, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %890 = bitcast <16 x i16> %876 to <8 x i32>
  %891 = bitcast <16 x i16> %880 to <8 x i32>
  %892 = shufflevector <8 x i32> %890, <8 x i32> %891, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %893 = shufflevector <8 x i32> %890, <8 x i32> %891, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %894 = bitcast <16 x i16> %884 to <8 x i32>
  %895 = bitcast <16 x i16> %888 to <8 x i32>
  %896 = shufflevector <8 x i32> %894, <8 x i32> %895, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %897 = shufflevector <8 x i32> %894, <8 x i32> %895, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %898 = bitcast <16 x i16> %877 to <8 x i32>
  %899 = bitcast <16 x i16> %881 to <8 x i32>
  %900 = shufflevector <8 x i32> %898, <8 x i32> %899, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %901 = shufflevector <8 x i32> %898, <8 x i32> %899, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %902 = bitcast <16 x i16> %885 to <8 x i32>
  %903 = bitcast <16 x i16> %889 to <8 x i32>
  %904 = shufflevector <8 x i32> %902, <8 x i32> %903, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %905 = shufflevector <8 x i32> %902, <8 x i32> %903, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %906 = bitcast <8 x i32> %892 to <4 x i64>
  %907 = bitcast <8 x i32> %896 to <4 x i64>
  %908 = shufflevector <4 x i64> %906, <4 x i64> %907, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %908, <4 x i64>* %778, align 32
  %909 = shufflevector <4 x i64> %906, <4 x i64> %907, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %909, <4 x i64>* %785, align 32
  %910 = bitcast <8 x i32> %900 to <4 x i64>
  %911 = bitcast <8 x i32> %904 to <4 x i64>
  %912 = shufflevector <4 x i64> %910, <4 x i64> %911, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %912, <4 x i64>* %806, align 32
  %913 = shufflevector <4 x i64> %910, <4 x i64> %911, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %913, <4 x i64>* %813, align 32
  %914 = bitcast <8 x i32> %893 to <4 x i64>
  %915 = bitcast <8 x i32> %897 to <4 x i64>
  %916 = shufflevector <4 x i64> %914, <4 x i64> %915, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %916, <4 x i64>* %792, align 32
  %917 = shufflevector <4 x i64> %914, <4 x i64> %915, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %917, <4 x i64>* %799, align 32
  %918 = bitcast <8 x i32> %901 to <4 x i64>
  %919 = bitcast <8 x i32> %905 to <4 x i64>
  %920 = shufflevector <4 x i64> %918, <4 x i64> %919, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %920, <4 x i64>* %820, align 32
  %921 = shufflevector <4 x i64> %918, <4 x i64> %919, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %921, <4 x i64>* %827, align 32
  %922 = bitcast <4 x i64> %838 to <16 x i16>
  %923 = bitcast <4 x i64> %843 to <16 x i16>
  %924 = shufflevector <16 x i16> %922, <16 x i16> %923, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %925 = shufflevector <16 x i16> %922, <16 x i16> %923, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %926 = bitcast <4 x i64> %848 to <16 x i16>
  %927 = bitcast <4 x i64> %853 to <16 x i16>
  %928 = shufflevector <16 x i16> %926, <16 x i16> %927, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %929 = shufflevector <16 x i16> %926, <16 x i16> %927, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %930 = bitcast <4 x i64> %858 to <16 x i16>
  %931 = bitcast <4 x i64> %863 to <16 x i16>
  %932 = shufflevector <16 x i16> %930, <16 x i16> %931, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %933 = shufflevector <16 x i16> %930, <16 x i16> %931, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %934 = bitcast <4 x i64> %868 to <16 x i16>
  %935 = bitcast <4 x i64> %873 to <16 x i16>
  %936 = shufflevector <16 x i16> %934, <16 x i16> %935, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %937 = shufflevector <16 x i16> %934, <16 x i16> %935, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %938 = bitcast <16 x i16> %924 to <8 x i32>
  %939 = bitcast <16 x i16> %928 to <8 x i32>
  %940 = shufflevector <8 x i32> %938, <8 x i32> %939, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %941 = shufflevector <8 x i32> %938, <8 x i32> %939, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %942 = bitcast <16 x i16> %932 to <8 x i32>
  %943 = bitcast <16 x i16> %936 to <8 x i32>
  %944 = shufflevector <8 x i32> %942, <8 x i32> %943, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %945 = shufflevector <8 x i32> %942, <8 x i32> %943, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %946 = bitcast <16 x i16> %925 to <8 x i32>
  %947 = bitcast <16 x i16> %929 to <8 x i32>
  %948 = shufflevector <8 x i32> %946, <8 x i32> %947, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %949 = shufflevector <8 x i32> %946, <8 x i32> %947, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %950 = bitcast <16 x i16> %933 to <8 x i32>
  %951 = bitcast <16 x i16> %937 to <8 x i32>
  %952 = shufflevector <8 x i32> %950, <8 x i32> %951, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %953 = shufflevector <8 x i32> %950, <8 x i32> %951, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %954 = bitcast <8 x i32> %940 to <4 x i64>
  %955 = bitcast <8 x i32> %944 to <4 x i64>
  %956 = shufflevector <4 x i64> %954, <4 x i64> %955, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %956, <4 x i64>* %781, align 32
  %957 = shufflevector <4 x i64> %954, <4 x i64> %955, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %957, <4 x i64>* %788, align 32
  %958 = bitcast <8 x i32> %948 to <4 x i64>
  %959 = bitcast <8 x i32> %952 to <4 x i64>
  %960 = shufflevector <4 x i64> %958, <4 x i64> %959, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %960, <4 x i64>* %809, align 32
  %961 = shufflevector <4 x i64> %958, <4 x i64> %959, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %961, <4 x i64>* %816, align 32
  %962 = bitcast <8 x i32> %941 to <4 x i64>
  %963 = bitcast <8 x i32> %945 to <4 x i64>
  %964 = shufflevector <4 x i64> %962, <4 x i64> %963, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %964, <4 x i64>* %795, align 32
  %965 = shufflevector <4 x i64> %962, <4 x i64> %963, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %965, <4 x i64>* %802, align 32
  %966 = bitcast <8 x i32> %949 to <4 x i64>
  %967 = bitcast <8 x i32> %953 to <4 x i64>
  %968 = shufflevector <4 x i64> %966, <4 x i64> %967, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %968, <4 x i64>* %823, align 32
  %969 = shufflevector <4 x i64> %966, <4 x i64> %967, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %969, <4 x i64>* %830, align 32
  %970 = getelementptr inbounds i32, i32* %1, i64 %777
  %971 = shufflevector <4 x i64> %908, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %972 = bitcast <2 x i64> %971 to <8 x i16>
  %973 = sext <8 x i16> %972 to <8 x i32>
  %974 = bitcast i32* %970 to <8 x i32>*
  store <8 x i32> %973, <8 x i32>* %974, align 32
  %975 = getelementptr inbounds i32, i32* %970, i64 8
  %976 = load <4 x i64>, <4 x i64>* %778, align 32
  %977 = shufflevector <4 x i64> %976, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %978 = bitcast <2 x i64> %977 to <8 x i16>
  %979 = sext <8 x i16> %978 to <8 x i32>
  %980 = bitcast i32* %975 to <8 x i32>*
  store <8 x i32> %979, <8 x i32>* %980, align 32
  br label %981

981:                                              ; preds = %1003, %775
  %982 = phi i64 [ 1, %775 ], [ %1018, %1003 ]
  %983 = phi i32* [ %970, %775 ], [ %1004, %1003 ]
  %984 = getelementptr inbounds i32, i32* %983, i64 32
  %985 = getelementptr inbounds <4 x i64>, <4 x i64>* %778, i64 %982
  %986 = load <4 x i64>, <4 x i64>* %985, align 32
  %987 = getelementptr inbounds <4 x i64>, <4 x i64>* %778, i64 %982
  %988 = shufflevector <4 x i64> %986, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %989 = bitcast <2 x i64> %988 to <8 x i16>
  %990 = sext <8 x i16> %989 to <8 x i32>
  %991 = bitcast i32* %984 to <8 x i32>*
  store <8 x i32> %990, <8 x i32>* %991, align 32
  %992 = getelementptr inbounds i32, i32* %983, i64 40
  %993 = load <4 x i64>, <4 x i64>* %987, align 32
  %994 = shufflevector <4 x i64> %993, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %995 = bitcast <2 x i64> %994 to <8 x i16>
  %996 = sext <8 x i16> %995 to <8 x i32>
  %997 = bitcast i32* %992 to <8 x i32>*
  store <8 x i32> %996, <8 x i32>* %997, align 32
  %998 = add nuw nsw i64 %982, 1
  %999 = icmp eq i64 %998, 16
  br i1 %999, label %1000, label %1003

1000:                                             ; preds = %981
  %1001 = add nuw nsw i64 %776, 1
  %1002 = icmp eq i64 %1001, 2
  br i1 %1002, label %708, label %775

1003:                                             ; preds = %981
  %1004 = getelementptr inbounds i32, i32* %983, i64 64
  %1005 = getelementptr inbounds <4 x i64>, <4 x i64>* %778, i64 %998
  %1006 = load <4 x i64>, <4 x i64>* %1005, align 32
  %1007 = getelementptr inbounds <4 x i64>, <4 x i64>* %778, i64 %998
  %1008 = shufflevector <4 x i64> %1006, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %1009 = bitcast <2 x i64> %1008 to <8 x i16>
  %1010 = sext <8 x i16> %1009 to <8 x i32>
  %1011 = bitcast i32* %1004 to <8 x i32>*
  store <8 x i32> %1010, <8 x i32>* %1011, align 32
  %1012 = getelementptr inbounds i32, i32* %983, i64 72
  %1013 = load <4 x i64>, <4 x i64>* %1007, align 32
  %1014 = shufflevector <4 x i64> %1013, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %1015 = bitcast <2 x i64> %1014 to <8 x i16>
  %1016 = sext <8 x i16> %1015 to <8 x i32>
  %1017 = bitcast i32* %1012 to <8 x i32>*
  store <8 x i32> %1016, <8 x i32>* %1017, align 32
  %1018 = add nuw nsw i64 %982, 2
  br label %981
}

; Function Attrs: argmemonly nounwind
declare void @llvm.memset.p0i8.i64(i8* nocapture writeonly, i8, i64, i1 immarg) #1

; Function Attrs: inlinehint nounwind ssp uwtable
define internal void @fdct16x16_new_avx2(<4 x i64>* nocapture readonly, <4 x i64>* nocapture, i8 signext) #4 {
  %4 = sext i8 %2 to i32
  %5 = add nsw i32 %4, -10
  %6 = sext i32 %5 to i64
  %7 = add nsw i32 %4, -1
  %8 = shl i32 1, %7
  %9 = insertelement <8 x i32> undef, i32 %8, i32 0
  %10 = shufflevector <8 x i32> %9, <8 x i32> undef, <8 x i32> zeroinitializer
  %11 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 32
  %12 = load i32, i32* %11, align 16
  %13 = sub i32 0, %12
  %14 = and i32 %13, 65535
  %15 = and i32 %12, 65535
  %16 = shl nuw i32 %15, 16
  %17 = or i32 %16, %14
  %18 = insertelement <8 x i32> undef, i32 %17, i32 0
  %19 = shufflevector <8 x i32> %18, <8 x i32> undef, <8 x i32> zeroinitializer
  %20 = or i32 %16, %15
  %21 = insertelement <8 x i32> undef, i32 %20, i32 0
  %22 = shufflevector <8 x i32> %21, <8 x i32> undef, <8 x i32> zeroinitializer
  %23 = shl nuw i32 %14, 16
  %24 = or i32 %23, %15
  %25 = insertelement <8 x i32> undef, i32 %24, i32 0
  %26 = shufflevector <8 x i32> %25, <8 x i32> undef, <8 x i32> zeroinitializer
  %27 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 48
  %28 = load i32, i32* %27, align 16
  %29 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 16
  %30 = load i32, i32* %29, align 16
  %31 = and i32 %28, 65535
  %32 = shl i32 %30, 16
  %33 = or i32 %32, %31
  %34 = insertelement <8 x i32> undef, i32 %33, i32 0
  %35 = shufflevector <8 x i32> %34, <8 x i32> undef, <8 x i32> zeroinitializer
  %36 = sub i32 0, %30
  %37 = and i32 %36, 65535
  %38 = shl nuw i32 %31, 16
  %39 = or i32 %38, %37
  %40 = insertelement <8 x i32> undef, i32 %39, i32 0
  %41 = shufflevector <8 x i32> %40, <8 x i32> undef, <8 x i32> zeroinitializer
  %42 = sub i32 0, %28
  %43 = and i32 %42, 65535
  %44 = shl nuw i32 %37, 16
  %45 = or i32 %44, %43
  %46 = insertelement <8 x i32> undef, i32 %45, i32 0
  %47 = shufflevector <8 x i32> %46, <8 x i32> undef, <8 x i32> zeroinitializer
  %48 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 56
  %49 = load i32, i32* %48, align 16
  %50 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 8
  %51 = load i32, i32* %50, align 16
  %52 = and i32 %49, 65535
  %53 = shl i32 %51, 16
  %54 = or i32 %53, %52
  %55 = insertelement <8 x i32> undef, i32 %54, i32 0
  %56 = shufflevector <8 x i32> %55, <8 x i32> undef, <8 x i32> zeroinitializer
  %57 = sub i32 0, %51
  %58 = and i32 %57, 65535
  %59 = shl nuw i32 %52, 16
  %60 = or i32 %59, %58
  %61 = insertelement <8 x i32> undef, i32 %60, i32 0
  %62 = shufflevector <8 x i32> %61, <8 x i32> undef, <8 x i32> zeroinitializer
  %63 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 24
  %64 = load i32, i32* %63, align 16
  %65 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 40
  %66 = load i32, i32* %65, align 16
  %67 = and i32 %64, 65535
  %68 = shl i32 %66, 16
  %69 = or i32 %68, %67
  %70 = insertelement <8 x i32> undef, i32 %69, i32 0
  %71 = shufflevector <8 x i32> %70, <8 x i32> undef, <8 x i32> zeroinitializer
  %72 = sub i32 0, %66
  %73 = and i32 %72, 65535
  %74 = shl nuw i32 %67, 16
  %75 = or i32 %74, %73
  %76 = insertelement <8 x i32> undef, i32 %75, i32 0
  %77 = shufflevector <8 x i32> %76, <8 x i32> undef, <8 x i32> zeroinitializer
  %78 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 60
  %79 = load i32, i32* %78, align 16
  %80 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 4
  %81 = load i32, i32* %80, align 16
  %82 = and i32 %79, 65535
  %83 = shl i32 %81, 16
  %84 = or i32 %83, %82
  %85 = insertelement <8 x i32> undef, i32 %84, i32 0
  %86 = shufflevector <8 x i32> %85, <8 x i32> undef, <8 x i32> zeroinitializer
  %87 = sub i32 0, %81
  %88 = and i32 %87, 65535
  %89 = shl nuw i32 %82, 16
  %90 = or i32 %89, %88
  %91 = insertelement <8 x i32> undef, i32 %90, i32 0
  %92 = shufflevector <8 x i32> %91, <8 x i32> undef, <8 x i32> zeroinitializer
  %93 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 28
  %94 = load i32, i32* %93, align 16
  %95 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 36
  %96 = load i32, i32* %95, align 16
  %97 = and i32 %94, 65535
  %98 = shl i32 %96, 16
  %99 = or i32 %98, %97
  %100 = insertelement <8 x i32> undef, i32 %99, i32 0
  %101 = shufflevector <8 x i32> %100, <8 x i32> undef, <8 x i32> zeroinitializer
  %102 = sub i32 0, %96
  %103 = and i32 %102, 65535
  %104 = shl nuw i32 %97, 16
  %105 = or i32 %104, %103
  %106 = insertelement <8 x i32> undef, i32 %105, i32 0
  %107 = shufflevector <8 x i32> %106, <8 x i32> undef, <8 x i32> zeroinitializer
  %108 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 44
  %109 = load i32, i32* %108, align 16
  %110 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 20
  %111 = load i32, i32* %110, align 16
  %112 = and i32 %109, 65535
  %113 = shl i32 %111, 16
  %114 = or i32 %113, %112
  %115 = insertelement <8 x i32> undef, i32 %114, i32 0
  %116 = shufflevector <8 x i32> %115, <8 x i32> undef, <8 x i32> zeroinitializer
  %117 = sub i32 0, %111
  %118 = and i32 %117, 65535
  %119 = shl nuw i32 %112, 16
  %120 = or i32 %119, %118
  %121 = insertelement <8 x i32> undef, i32 %120, i32 0
  %122 = shufflevector <8 x i32> %121, <8 x i32> undef, <8 x i32> zeroinitializer
  %123 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 12
  %124 = load i32, i32* %123, align 16
  %125 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 52
  %126 = load i32, i32* %125, align 16
  %127 = and i32 %124, 65535
  %128 = shl i32 %126, 16
  %129 = or i32 %128, %127
  %130 = insertelement <8 x i32> undef, i32 %129, i32 0
  %131 = shufflevector <8 x i32> %130, <8 x i32> undef, <8 x i32> zeroinitializer
  %132 = sub i32 0, %126
  %133 = and i32 %132, 65535
  %134 = shl nuw i32 %127, 16
  %135 = or i32 %134, %133
  %136 = insertelement <8 x i32> undef, i32 %135, i32 0
  %137 = shufflevector <8 x i32> %136, <8 x i32> undef, <8 x i32> zeroinitializer
  %138 = bitcast <4 x i64>* %0 to <16 x i16>*
  %139 = load <16 x i16>, <16 x i16>* %138, align 32
  %140 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 15
  %141 = bitcast <4 x i64>* %140 to <16 x i16>*
  %142 = load <16 x i16>, <16 x i16>* %141, align 32
  %143 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %139, <16 x i16> %142) #8
  %144 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %139, <16 x i16> %142) #8
  %145 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 1
  %146 = bitcast <4 x i64>* %145 to <16 x i16>*
  %147 = load <16 x i16>, <16 x i16>* %146, align 32
  %148 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 14
  %149 = bitcast <4 x i64>* %148 to <16 x i16>*
  %150 = load <16 x i16>, <16 x i16>* %149, align 32
  %151 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %147, <16 x i16> %150) #8
  %152 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %147, <16 x i16> %150) #8
  %153 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 2
  %154 = bitcast <4 x i64>* %153 to <16 x i16>*
  %155 = load <16 x i16>, <16 x i16>* %154, align 32
  %156 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 13
  %157 = bitcast <4 x i64>* %156 to <16 x i16>*
  %158 = load <16 x i16>, <16 x i16>* %157, align 32
  %159 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %155, <16 x i16> %158) #8
  %160 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %155, <16 x i16> %158) #8
  %161 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 3
  %162 = bitcast <4 x i64>* %161 to <16 x i16>*
  %163 = load <16 x i16>, <16 x i16>* %162, align 32
  %164 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 12
  %165 = bitcast <4 x i64>* %164 to <16 x i16>*
  %166 = load <16 x i16>, <16 x i16>* %165, align 32
  %167 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %163, <16 x i16> %166) #8
  %168 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %163, <16 x i16> %166) #8
  %169 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 4
  %170 = bitcast <4 x i64>* %169 to <16 x i16>*
  %171 = load <16 x i16>, <16 x i16>* %170, align 32
  %172 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 11
  %173 = bitcast <4 x i64>* %172 to <16 x i16>*
  %174 = load <16 x i16>, <16 x i16>* %173, align 32
  %175 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %171, <16 x i16> %174) #8
  %176 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %171, <16 x i16> %174) #8
  %177 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 5
  %178 = bitcast <4 x i64>* %177 to <16 x i16>*
  %179 = load <16 x i16>, <16 x i16>* %178, align 32
  %180 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 10
  %181 = bitcast <4 x i64>* %180 to <16 x i16>*
  %182 = load <16 x i16>, <16 x i16>* %181, align 32
  %183 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %179, <16 x i16> %182) #8
  %184 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %179, <16 x i16> %182) #8
  %185 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 6
  %186 = bitcast <4 x i64>* %185 to <16 x i16>*
  %187 = load <16 x i16>, <16 x i16>* %186, align 32
  %188 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 9
  %189 = bitcast <4 x i64>* %188 to <16 x i16>*
  %190 = load <16 x i16>, <16 x i16>* %189, align 32
  %191 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %187, <16 x i16> %190) #8
  %192 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %187, <16 x i16> %190) #8
  %193 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 7
  %194 = bitcast <4 x i64>* %193 to <16 x i16>*
  %195 = load <16 x i16>, <16 x i16>* %194, align 32
  %196 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 8
  %197 = bitcast <4 x i64>* %196 to <16 x i16>*
  %198 = load <16 x i16>, <16 x i16>* %197, align 32
  %199 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %195, <16 x i16> %198) #8
  %200 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %195, <16 x i16> %198) #8
  %201 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %143, <16 x i16> %199) #8
  %202 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %143, <16 x i16> %199) #8
  %203 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %151, <16 x i16> %191) #8
  %204 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %151, <16 x i16> %191) #8
  %205 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %159, <16 x i16> %183) #8
  %206 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %159, <16 x i16> %183) #8
  %207 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %167, <16 x i16> %175) #8
  %208 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %167, <16 x i16> %175) #8
  %209 = shufflevector <16 x i16> %184, <16 x i16> %160, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %210 = shufflevector <16 x i16> %184, <16 x i16> %160, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %211 = bitcast <8 x i32> %19 to <16 x i16>
  %212 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %209, <16 x i16> %211) #8
  %213 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %210, <16 x i16> %211) #8
  %214 = bitcast <8 x i32> %22 to <16 x i16>
  %215 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %209, <16 x i16> %214) #8
  %216 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %210, <16 x i16> %214) #8
  %217 = add <8 x i32> %212, %10
  %218 = add <8 x i32> %213, %10
  %219 = add <8 x i32> %215, %10
  %220 = add <8 x i32> %216, %10
  %221 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %217, i32 %4) #8
  %222 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %218, i32 %4) #8
  %223 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %219, i32 %4) #8
  %224 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %220, i32 %4) #8
  %225 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %221, <8 x i32> %222) #8
  %226 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %223, <8 x i32> %224) #8
  %227 = shufflevector <16 x i16> %176, <16 x i16> %168, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %228 = shufflevector <16 x i16> %176, <16 x i16> %168, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %229 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %227, <16 x i16> %211) #8
  %230 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %228, <16 x i16> %211) #8
  %231 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %227, <16 x i16> %214) #8
  %232 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %228, <16 x i16> %214) #8
  %233 = add <8 x i32> %229, %10
  %234 = add <8 x i32> %230, %10
  %235 = add <8 x i32> %231, %10
  %236 = add <8 x i32> %232, %10
  %237 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %233, i32 %4) #8
  %238 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %234, i32 %4) #8
  %239 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %235, i32 %4) #8
  %240 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %236, i32 %4) #8
  %241 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %237, <8 x i32> %238) #8
  %242 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %239, <8 x i32> %240) #8
  %243 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %201, <16 x i16> %207) #8
  %244 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %201, <16 x i16> %207) #8
  %245 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %203, <16 x i16> %205) #8
  %246 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %203, <16 x i16> %205) #8
  %247 = shufflevector <16 x i16> %206, <16 x i16> %204, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %248 = shufflevector <16 x i16> %206, <16 x i16> %204, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %249 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %247, <16 x i16> %211) #8
  %250 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %248, <16 x i16> %211) #8
  %251 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %247, <16 x i16> %214) #8
  %252 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %248, <16 x i16> %214) #8
  %253 = add <8 x i32> %249, %10
  %254 = add <8 x i32> %250, %10
  %255 = add <8 x i32> %251, %10
  %256 = add <8 x i32> %252, %10
  %257 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %253, i32 %4) #8
  %258 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %254, i32 %4) #8
  %259 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %255, i32 %4) #8
  %260 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %256, i32 %4) #8
  %261 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %257, <8 x i32> %258) #8
  %262 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %259, <8 x i32> %260) #8
  %263 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %200, <16 x i16> %241) #8
  %264 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %200, <16 x i16> %241) #8
  %265 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %192, <16 x i16> %225) #8
  %266 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %192, <16 x i16> %225) #8
  %267 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %144, <16 x i16> %242) #8
  %268 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %144, <16 x i16> %242) #8
  %269 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %152, <16 x i16> %226) #8
  %270 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %152, <16 x i16> %226) #8
  %271 = shufflevector <16 x i16> %243, <16 x i16> %245, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %272 = shufflevector <16 x i16> %243, <16 x i16> %245, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %273 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %271, <16 x i16> %214) #8
  %274 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %272, <16 x i16> %214) #8
  %275 = bitcast <8 x i32> %26 to <16 x i16>
  %276 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %271, <16 x i16> %275) #8
  %277 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %272, <16 x i16> %275) #8
  %278 = add <8 x i32> %273, %10
  %279 = add <8 x i32> %274, %10
  %280 = add <8 x i32> %276, %10
  %281 = add <8 x i32> %277, %10
  %282 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %278, i32 %4) #8
  %283 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %279, i32 %4) #8
  %284 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %280, i32 %4) #8
  %285 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %281, i32 %4) #8
  %286 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %282, <8 x i32> %283) #8
  %287 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %284, <8 x i32> %285) #8
  %288 = shufflevector <16 x i16> %246, <16 x i16> %244, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %289 = shufflevector <16 x i16> %246, <16 x i16> %244, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %290 = bitcast <8 x i32> %35 to <16 x i16>
  %291 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %288, <16 x i16> %290) #8
  %292 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %289, <16 x i16> %290) #8
  %293 = bitcast <8 x i32> %41 to <16 x i16>
  %294 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %288, <16 x i16> %293) #8
  %295 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %289, <16 x i16> %293) #8
  %296 = add <8 x i32> %291, %10
  %297 = add <8 x i32> %292, %10
  %298 = add <8 x i32> %294, %10
  %299 = add <8 x i32> %295, %10
  %300 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %296, i32 %4) #8
  %301 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %297, i32 %4) #8
  %302 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %298, i32 %4) #8
  %303 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %299, i32 %4) #8
  %304 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %300, <8 x i32> %301) #8
  %305 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %302, <8 x i32> %303) #8
  %306 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %208, <16 x i16> %261) #8
  %307 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %208, <16 x i16> %261) #8
  %308 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %202, <16 x i16> %262) #8
  %309 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %202, <16 x i16> %262) #8
  %310 = shufflevector <16 x i16> %265, <16 x i16> %269, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %311 = shufflevector <16 x i16> %265, <16 x i16> %269, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %312 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %310, <16 x i16> %293) #8
  %313 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %311, <16 x i16> %293) #8
  %314 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %310, <16 x i16> %290) #8
  %315 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %311, <16 x i16> %290) #8
  %316 = add <8 x i32> %312, %10
  %317 = add <8 x i32> %313, %10
  %318 = add <8 x i32> %314, %10
  %319 = add <8 x i32> %315, %10
  %320 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %316, i32 %4) #8
  %321 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %317, i32 %4) #8
  %322 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %318, i32 %4) #8
  %323 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %319, i32 %4) #8
  %324 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %320, <8 x i32> %321) #8
  %325 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %322, <8 x i32> %323) #8
  %326 = shufflevector <16 x i16> %266, <16 x i16> %270, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %327 = shufflevector <16 x i16> %266, <16 x i16> %270, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %328 = bitcast <8 x i32> %47 to <16 x i16>
  %329 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %326, <16 x i16> %328) #8
  %330 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %327, <16 x i16> %328) #8
  %331 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %326, <16 x i16> %293) #8
  %332 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %327, <16 x i16> %293) #8
  %333 = add <8 x i32> %329, %10
  %334 = add <8 x i32> %330, %10
  %335 = add <8 x i32> %331, %10
  %336 = add <8 x i32> %332, %10
  %337 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %333, i32 %4) #8
  %338 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %334, i32 %4) #8
  %339 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %335, i32 %4) #8
  %340 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %336, i32 %4) #8
  %341 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %337, <8 x i32> %338) #8
  %342 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %339, <8 x i32> %340) #8
  %343 = shufflevector <16 x i16> %306, <16 x i16> %308, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %344 = shufflevector <16 x i16> %306, <16 x i16> %308, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %345 = bitcast <8 x i32> %56 to <16 x i16>
  %346 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %343, <16 x i16> %345) #8
  %347 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %344, <16 x i16> %345) #8
  %348 = bitcast <8 x i32> %62 to <16 x i16>
  %349 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %343, <16 x i16> %348) #8
  %350 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %344, <16 x i16> %348) #8
  %351 = add <8 x i32> %346, %10
  %352 = add <8 x i32> %347, %10
  %353 = add <8 x i32> %349, %10
  %354 = add <8 x i32> %350, %10
  %355 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %351, i32 %4) #8
  %356 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %352, i32 %4) #8
  %357 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %353, i32 %4) #8
  %358 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %354, i32 %4) #8
  %359 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %355, <8 x i32> %356) #8
  %360 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %357, <8 x i32> %358) #8
  %361 = shufflevector <16 x i16> %307, <16 x i16> %309, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %362 = shufflevector <16 x i16> %307, <16 x i16> %309, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %363 = bitcast <8 x i32> %71 to <16 x i16>
  %364 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %361, <16 x i16> %363) #8
  %365 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %362, <16 x i16> %363) #8
  %366 = bitcast <8 x i32> %77 to <16 x i16>
  %367 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %361, <16 x i16> %366) #8
  %368 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %362, <16 x i16> %366) #8
  %369 = add <8 x i32> %364, %10
  %370 = add <8 x i32> %365, %10
  %371 = add <8 x i32> %367, %10
  %372 = add <8 x i32> %368, %10
  %373 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %369, i32 %4) #8
  %374 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %370, i32 %4) #8
  %375 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %371, i32 %4) #8
  %376 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %372, i32 %4) #8
  %377 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %373, <8 x i32> %374) #8
  %378 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %375, <8 x i32> %376) #8
  %379 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %263, <16 x i16> %324) #8
  %380 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %263, <16 x i16> %324) #8
  %381 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %264, <16 x i16> %341) #8
  %382 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %264, <16 x i16> %341) #8
  %383 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %268, <16 x i16> %342) #8
  %384 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %268, <16 x i16> %342) #8
  %385 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %267, <16 x i16> %325) #8
  %386 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %267, <16 x i16> %325) #8
  %387 = shufflevector <16 x i16> %379, <16 x i16> %385, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %388 = shufflevector <16 x i16> %379, <16 x i16> %385, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %389 = bitcast <8 x i32> %86 to <16 x i16>
  %390 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %387, <16 x i16> %389) #8
  %391 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %388, <16 x i16> %389) #8
  %392 = bitcast <8 x i32> %92 to <16 x i16>
  %393 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %387, <16 x i16> %392) #8
  %394 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %388, <16 x i16> %392) #8
  %395 = add <8 x i32> %390, %10
  %396 = add <8 x i32> %391, %10
  %397 = add <8 x i32> %393, %10
  %398 = add <8 x i32> %394, %10
  %399 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %395, i32 %4) #8
  %400 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %396, i32 %4) #8
  %401 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %397, i32 %4) #8
  %402 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %398, i32 %4) #8
  %403 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %399, <8 x i32> %400) #8
  %404 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %401, <8 x i32> %402) #8
  %405 = shufflevector <16 x i16> %380, <16 x i16> %386, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %406 = shufflevector <16 x i16> %380, <16 x i16> %386, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %407 = bitcast <8 x i32> %101 to <16 x i16>
  %408 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %405, <16 x i16> %407) #8
  %409 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %406, <16 x i16> %407) #8
  %410 = bitcast <8 x i32> %107 to <16 x i16>
  %411 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %405, <16 x i16> %410) #8
  %412 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %406, <16 x i16> %410) #8
  %413 = add <8 x i32> %408, %10
  %414 = add <8 x i32> %409, %10
  %415 = add <8 x i32> %411, %10
  %416 = add <8 x i32> %412, %10
  %417 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %413, i32 %4) #8
  %418 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %414, i32 %4) #8
  %419 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %415, i32 %4) #8
  %420 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %416, i32 %4) #8
  %421 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %417, <8 x i32> %418) #8
  %422 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %419, <8 x i32> %420) #8
  %423 = shufflevector <16 x i16> %382, <16 x i16> %384, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %424 = shufflevector <16 x i16> %382, <16 x i16> %384, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %425 = bitcast <8 x i32> %116 to <16 x i16>
  %426 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %423, <16 x i16> %425) #8
  %427 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %424, <16 x i16> %425) #8
  %428 = bitcast <8 x i32> %122 to <16 x i16>
  %429 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %423, <16 x i16> %428) #8
  %430 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %424, <16 x i16> %428) #8
  %431 = add <8 x i32> %426, %10
  %432 = add <8 x i32> %427, %10
  %433 = add <8 x i32> %429, %10
  %434 = add <8 x i32> %430, %10
  %435 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %431, i32 %4) #8
  %436 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %432, i32 %4) #8
  %437 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %433, i32 %4) #8
  %438 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %434, i32 %4) #8
  %439 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %435, <8 x i32> %436) #8
  %440 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %437, <8 x i32> %438) #8
  %441 = shufflevector <16 x i16> %381, <16 x i16> %383, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %442 = shufflevector <16 x i16> %381, <16 x i16> %383, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %443 = bitcast <8 x i32> %131 to <16 x i16>
  %444 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %441, <16 x i16> %443) #8
  %445 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %442, <16 x i16> %443) #8
  %446 = bitcast <8 x i32> %137 to <16 x i16>
  %447 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %441, <16 x i16> %446) #8
  %448 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %442, <16 x i16> %446) #8
  %449 = add <8 x i32> %444, %10
  %450 = add <8 x i32> %445, %10
  %451 = add <8 x i32> %447, %10
  %452 = add <8 x i32> %448, %10
  %453 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %449, i32 %4) #8
  %454 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %450, i32 %4) #8
  %455 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %451, i32 %4) #8
  %456 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %452, i32 %4) #8
  %457 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %453, <8 x i32> %454) #8
  %458 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %455, <8 x i32> %456) #8
  %459 = bitcast <4 x i64>* %1 to <16 x i16>*
  store <16 x i16> %286, <16 x i16>* %459, align 32
  %460 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %461 = bitcast <4 x i64>* %460 to <16 x i16>*
  store <16 x i16> %403, <16 x i16>* %461, align 32
  %462 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %463 = bitcast <4 x i64>* %462 to <16 x i16>*
  store <16 x i16> %359, <16 x i16>* %463, align 32
  %464 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %465 = bitcast <4 x i64>* %464 to <16 x i16>*
  store <16 x i16> %458, <16 x i16>* %465, align 32
  %466 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %467 = bitcast <4 x i64>* %466 to <16 x i16>*
  store <16 x i16> %304, <16 x i16>* %467, align 32
  %468 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %469 = bitcast <4 x i64>* %468 to <16 x i16>*
  store <16 x i16> %439, <16 x i16>* %469, align 32
  %470 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %471 = bitcast <4 x i64>* %470 to <16 x i16>*
  store <16 x i16> %378, <16 x i16>* %471, align 32
  %472 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %473 = bitcast <4 x i64>* %472 to <16 x i16>*
  store <16 x i16> %422, <16 x i16>* %473, align 32
  %474 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %475 = bitcast <4 x i64>* %474 to <16 x i16>*
  store <16 x i16> %287, <16 x i16>* %475, align 32
  %476 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %477 = bitcast <4 x i64>* %476 to <16 x i16>*
  store <16 x i16> %421, <16 x i16>* %477, align 32
  %478 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %479 = bitcast <4 x i64>* %478 to <16 x i16>*
  store <16 x i16> %377, <16 x i16>* %479, align 32
  %480 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %481 = bitcast <4 x i64>* %480 to <16 x i16>*
  store <16 x i16> %440, <16 x i16>* %481, align 32
  %482 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %483 = bitcast <4 x i64>* %482 to <16 x i16>*
  store <16 x i16> %305, <16 x i16>* %483, align 32
  %484 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %485 = bitcast <4 x i64>* %484 to <16 x i16>*
  store <16 x i16> %457, <16 x i16>* %485, align 32
  %486 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %487 = bitcast <4 x i64>* %486 to <16 x i16>*
  store <16 x i16> %360, <16 x i16>* %487, align 32
  %488 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 15
  %489 = bitcast <4 x i64>* %488 to <16 x i16>*
  store <16 x i16> %404, <16 x i16>* %489, align 32
  ret void
}

; Function Attrs: inlinehint nounwind ssp uwtable
define internal void @fadst16x16_new_avx2(<4 x i64>* nocapture readonly, <4 x i64>* nocapture, i8 signext) #4 {
  %4 = sext i8 %2 to i32
  %5 = add nsw i32 %4, -10
  %6 = sext i32 %5 to i64
  %7 = add nsw i32 %4, -1
  %8 = shl i32 1, %7
  %9 = insertelement <8 x i32> undef, i32 %8, i32 0
  %10 = shufflevector <8 x i32> %9, <8 x i32> undef, <8 x i32> zeroinitializer
  %11 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 32
  %12 = load i32, i32* %11, align 16
  %13 = and i32 %12, 65535
  %14 = shl nuw i32 %13, 16
  %15 = or i32 %14, %13
  %16 = insertelement <8 x i32> undef, i32 %15, i32 0
  %17 = shufflevector <8 x i32> %16, <8 x i32> undef, <8 x i32> zeroinitializer
  %18 = shl i32 %12, 16
  %19 = sub i32 0, %18
  %20 = or i32 %13, %19
  %21 = insertelement <8 x i32> undef, i32 %20, i32 0
  %22 = shufflevector <8 x i32> %21, <8 x i32> undef, <8 x i32> zeroinitializer
  %23 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 16
  %24 = load i32, i32* %23, align 16
  %25 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 48
  %26 = load i32, i32* %25, align 16
  %27 = and i32 %24, 65535
  %28 = and i32 %26, 65535
  %29 = shl nuw i32 %28, 16
  %30 = or i32 %29, %27
  %31 = insertelement <8 x i32> undef, i32 %30, i32 0
  %32 = shufflevector <8 x i32> %31, <8 x i32> undef, <8 x i32> zeroinitializer
  %33 = shl i32 %24, 16
  %34 = sub i32 0, %33
  %35 = or i32 %28, %34
  %36 = insertelement <8 x i32> undef, i32 %35, i32 0
  %37 = shufflevector <8 x i32> %36, <8 x i32> undef, <8 x i32> zeroinitializer
  %38 = sub i32 0, %26
  %39 = and i32 %38, 65535
  %40 = shl nuw i32 %27, 16
  %41 = or i32 %40, %39
  %42 = insertelement <8 x i32> undef, i32 %41, i32 0
  %43 = shufflevector <8 x i32> %42, <8 x i32> undef, <8 x i32> zeroinitializer
  %44 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 8
  %45 = load i32, i32* %44, align 16
  %46 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 56
  %47 = load i32, i32* %46, align 16
  %48 = and i32 %45, 65535
  %49 = and i32 %47, 65535
  %50 = shl nuw i32 %49, 16
  %51 = or i32 %50, %48
  %52 = insertelement <8 x i32> undef, i32 %51, i32 0
  %53 = shufflevector <8 x i32> %52, <8 x i32> undef, <8 x i32> zeroinitializer
  %54 = shl i32 %45, 16
  %55 = sub i32 0, %54
  %56 = or i32 %49, %55
  %57 = insertelement <8 x i32> undef, i32 %56, i32 0
  %58 = shufflevector <8 x i32> %57, <8 x i32> undef, <8 x i32> zeroinitializer
  %59 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 40
  %60 = load i32, i32* %59, align 16
  %61 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 24
  %62 = load i32, i32* %61, align 16
  %63 = and i32 %60, 65535
  %64 = and i32 %62, 65535
  %65 = shl nuw i32 %64, 16
  %66 = or i32 %65, %63
  %67 = insertelement <8 x i32> undef, i32 %66, i32 0
  %68 = shufflevector <8 x i32> %67, <8 x i32> undef, <8 x i32> zeroinitializer
  %69 = shl i32 %60, 16
  %70 = sub i32 0, %69
  %71 = or i32 %64, %70
  %72 = insertelement <8 x i32> undef, i32 %71, i32 0
  %73 = shufflevector <8 x i32> %72, <8 x i32> undef, <8 x i32> zeroinitializer
  %74 = sub i32 0, %47
  %75 = and i32 %74, 65535
  %76 = shl nuw i32 %48, 16
  %77 = or i32 %76, %75
  %78 = insertelement <8 x i32> undef, i32 %77, i32 0
  %79 = shufflevector <8 x i32> %78, <8 x i32> undef, <8 x i32> zeroinitializer
  %80 = sub i32 0, %62
  %81 = and i32 %80, 65535
  %82 = shl nuw i32 %63, 16
  %83 = or i32 %82, %81
  %84 = insertelement <8 x i32> undef, i32 %83, i32 0
  %85 = shufflevector <8 x i32> %84, <8 x i32> undef, <8 x i32> zeroinitializer
  %86 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 2
  %87 = load i32, i32* %86, align 8
  %88 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 62
  %89 = load i32, i32* %88, align 8
  %90 = and i32 %87, 65535
  %91 = and i32 %89, 65535
  %92 = shl nuw i32 %91, 16
  %93 = or i32 %92, %90
  %94 = insertelement <8 x i32> undef, i32 %93, i32 0
  %95 = shufflevector <8 x i32> %94, <8 x i32> undef, <8 x i32> zeroinitializer
  %96 = shl i32 %87, 16
  %97 = sub i32 0, %96
  %98 = or i32 %91, %97
  %99 = insertelement <8 x i32> undef, i32 %98, i32 0
  %100 = shufflevector <8 x i32> %99, <8 x i32> undef, <8 x i32> zeroinitializer
  %101 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 10
  %102 = load i32, i32* %101, align 8
  %103 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 54
  %104 = load i32, i32* %103, align 8
  %105 = and i32 %102, 65535
  %106 = and i32 %104, 65535
  %107 = shl nuw i32 %106, 16
  %108 = or i32 %107, %105
  %109 = insertelement <8 x i32> undef, i32 %108, i32 0
  %110 = shufflevector <8 x i32> %109, <8 x i32> undef, <8 x i32> zeroinitializer
  %111 = shl i32 %102, 16
  %112 = sub i32 0, %111
  %113 = or i32 %106, %112
  %114 = insertelement <8 x i32> undef, i32 %113, i32 0
  %115 = shufflevector <8 x i32> %114, <8 x i32> undef, <8 x i32> zeroinitializer
  %116 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 18
  %117 = load i32, i32* %116, align 8
  %118 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 46
  %119 = load i32, i32* %118, align 8
  %120 = and i32 %117, 65535
  %121 = and i32 %119, 65535
  %122 = shl nuw i32 %121, 16
  %123 = or i32 %122, %120
  %124 = insertelement <8 x i32> undef, i32 %123, i32 0
  %125 = shufflevector <8 x i32> %124, <8 x i32> undef, <8 x i32> zeroinitializer
  %126 = shl i32 %117, 16
  %127 = sub i32 0, %126
  %128 = or i32 %121, %127
  %129 = insertelement <8 x i32> undef, i32 %128, i32 0
  %130 = shufflevector <8 x i32> %129, <8 x i32> undef, <8 x i32> zeroinitializer
  %131 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 26
  %132 = load i32, i32* %131, align 8
  %133 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 38
  %134 = load i32, i32* %133, align 8
  %135 = and i32 %132, 65535
  %136 = and i32 %134, 65535
  %137 = shl nuw i32 %136, 16
  %138 = or i32 %137, %135
  %139 = insertelement <8 x i32> undef, i32 %138, i32 0
  %140 = shufflevector <8 x i32> %139, <8 x i32> undef, <8 x i32> zeroinitializer
  %141 = shl i32 %132, 16
  %142 = sub i32 0, %141
  %143 = or i32 %136, %142
  %144 = insertelement <8 x i32> undef, i32 %143, i32 0
  %145 = shufflevector <8 x i32> %144, <8 x i32> undef, <8 x i32> zeroinitializer
  %146 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 34
  %147 = load i32, i32* %146, align 8
  %148 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 30
  %149 = load i32, i32* %148, align 8
  %150 = and i32 %147, 65535
  %151 = and i32 %149, 65535
  %152 = shl nuw i32 %151, 16
  %153 = or i32 %152, %150
  %154 = insertelement <8 x i32> undef, i32 %153, i32 0
  %155 = shufflevector <8 x i32> %154, <8 x i32> undef, <8 x i32> zeroinitializer
  %156 = shl i32 %147, 16
  %157 = sub i32 0, %156
  %158 = or i32 %151, %157
  %159 = insertelement <8 x i32> undef, i32 %158, i32 0
  %160 = shufflevector <8 x i32> %159, <8 x i32> undef, <8 x i32> zeroinitializer
  %161 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 42
  %162 = load i32, i32* %161, align 8
  %163 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 22
  %164 = load i32, i32* %163, align 8
  %165 = and i32 %162, 65535
  %166 = and i32 %164, 65535
  %167 = shl nuw i32 %166, 16
  %168 = or i32 %167, %165
  %169 = insertelement <8 x i32> undef, i32 %168, i32 0
  %170 = shufflevector <8 x i32> %169, <8 x i32> undef, <8 x i32> zeroinitializer
  %171 = shl i32 %162, 16
  %172 = sub i32 0, %171
  %173 = or i32 %166, %172
  %174 = insertelement <8 x i32> undef, i32 %173, i32 0
  %175 = shufflevector <8 x i32> %174, <8 x i32> undef, <8 x i32> zeroinitializer
  %176 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 50
  %177 = load i32, i32* %176, align 8
  %178 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 14
  %179 = load i32, i32* %178, align 8
  %180 = and i32 %177, 65535
  %181 = and i32 %179, 65535
  %182 = shl nuw i32 %181, 16
  %183 = or i32 %182, %180
  %184 = insertelement <8 x i32> undef, i32 %183, i32 0
  %185 = shufflevector <8 x i32> %184, <8 x i32> undef, <8 x i32> zeroinitializer
  %186 = shl i32 %177, 16
  %187 = sub i32 0, %186
  %188 = or i32 %181, %187
  %189 = insertelement <8 x i32> undef, i32 %188, i32 0
  %190 = shufflevector <8 x i32> %189, <8 x i32> undef, <8 x i32> zeroinitializer
  %191 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 58
  %192 = load i32, i32* %191, align 8
  %193 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 6
  %194 = load i32, i32* %193, align 8
  %195 = and i32 %192, 65535
  %196 = and i32 %194, 65535
  %197 = shl nuw i32 %196, 16
  %198 = or i32 %197, %195
  %199 = insertelement <8 x i32> undef, i32 %198, i32 0
  %200 = shufflevector <8 x i32> %199, <8 x i32> undef, <8 x i32> zeroinitializer
  %201 = shl i32 %192, 16
  %202 = sub i32 0, %201
  %203 = or i32 %196, %202
  %204 = insertelement <8 x i32> undef, i32 %203, i32 0
  %205 = shufflevector <8 x i32> %204, <8 x i32> undef, <8 x i32> zeroinitializer
  %206 = bitcast <4 x i64>* %0 to <16 x i16>*
  %207 = load <16 x i16>, <16 x i16>* %206, align 32
  %208 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 15
  %209 = bitcast <4 x i64>* %208 to <16 x i16>*
  %210 = load <16 x i16>, <16 x i16>* %209, align 32
  %211 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %210) #8
  %212 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 7
  %213 = bitcast <4 x i64>* %212 to <16 x i16>*
  %214 = load <16 x i16>, <16 x i16>* %213, align 32
  %215 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %214) #8
  %216 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 8
  %217 = bitcast <4 x i64>* %216 to <16 x i16>*
  %218 = load <16 x i16>, <16 x i16>* %217, align 32
  %219 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 3
  %220 = bitcast <4 x i64>* %219 to <16 x i16>*
  %221 = load <16 x i16>, <16 x i16>* %220, align 32
  %222 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %221) #8
  %223 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 12
  %224 = bitcast <4 x i64>* %223 to <16 x i16>*
  %225 = load <16 x i16>, <16 x i16>* %224, align 32
  %226 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 4
  %227 = bitcast <4 x i64>* %226 to <16 x i16>*
  %228 = load <16 x i16>, <16 x i16>* %227, align 32
  %229 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 11
  %230 = bitcast <4 x i64>* %229 to <16 x i16>*
  %231 = load <16 x i16>, <16 x i16>* %230, align 32
  %232 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %231) #8
  %233 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 1
  %234 = bitcast <4 x i64>* %233 to <16 x i16>*
  %235 = load <16 x i16>, <16 x i16>* %234, align 32
  %236 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %235) #8
  %237 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 14
  %238 = bitcast <4 x i64>* %237 to <16 x i16>*
  %239 = load <16 x i16>, <16 x i16>* %238, align 32
  %240 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 6
  %241 = bitcast <4 x i64>* %240 to <16 x i16>*
  %242 = load <16 x i16>, <16 x i16>* %241, align 32
  %243 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 9
  %244 = bitcast <4 x i64>* %243 to <16 x i16>*
  %245 = load <16 x i16>, <16 x i16>* %244, align 32
  %246 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %245) #8
  %247 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 2
  %248 = bitcast <4 x i64>* %247 to <16 x i16>*
  %249 = load <16 x i16>, <16 x i16>* %248, align 32
  %250 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 13
  %251 = bitcast <4 x i64>* %250 to <16 x i16>*
  %252 = load <16 x i16>, <16 x i16>* %251, align 32
  %253 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %252) #8
  %254 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 5
  %255 = bitcast <4 x i64>* %254 to <16 x i16>*
  %256 = load <16 x i16>, <16 x i16>* %255, align 32
  %257 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %256) #8
  %258 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 10
  %259 = bitcast <4 x i64>* %258 to <16 x i16>*
  %260 = load <16 x i16>, <16 x i16>* %259, align 32
  %261 = shufflevector <16 x i16> %215, <16 x i16> %218, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %262 = shufflevector <16 x i16> %215, <16 x i16> %218, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %263 = bitcast <8 x i32> %17 to <16 x i16>
  %264 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %261, <16 x i16> %263) #8
  %265 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %262, <16 x i16> %263) #8
  %266 = bitcast <8 x i32> %22 to <16 x i16>
  %267 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %261, <16 x i16> %266) #8
  %268 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %262, <16 x i16> %266) #8
  %269 = add <8 x i32> %264, %10
  %270 = add <8 x i32> %265, %10
  %271 = add <8 x i32> %267, %10
  %272 = add <8 x i32> %268, %10
  %273 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %269, i32 %4) #8
  %274 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %270, i32 %4) #8
  %275 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %271, i32 %4) #8
  %276 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %272, i32 %4) #8
  %277 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %273, <8 x i32> %274) #8
  %278 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %275, <8 x i32> %276) #8
  %279 = shufflevector <16 x i16> %228, <16 x i16> %232, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %280 = shufflevector <16 x i16> %228, <16 x i16> %232, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %281 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %279, <16 x i16> %263) #8
  %282 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %280, <16 x i16> %263) #8
  %283 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %279, <16 x i16> %266) #8
  %284 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %280, <16 x i16> %266) #8
  %285 = add <8 x i32> %281, %10
  %286 = add <8 x i32> %282, %10
  %287 = add <8 x i32> %283, %10
  %288 = add <8 x i32> %284, %10
  %289 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %285, i32 %4) #8
  %290 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %286, i32 %4) #8
  %291 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %287, i32 %4) #8
  %292 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %288, i32 %4) #8
  %293 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %289, <8 x i32> %290) #8
  %294 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %291, <8 x i32> %292) #8
  %295 = shufflevector <16 x i16> %242, <16 x i16> %246, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %296 = shufflevector <16 x i16> %242, <16 x i16> %246, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %297 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %295, <16 x i16> %263) #8
  %298 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %296, <16 x i16> %263) #8
  %299 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %295, <16 x i16> %266) #8
  %300 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %296, <16 x i16> %266) #8
  %301 = add <8 x i32> %297, %10
  %302 = add <8 x i32> %298, %10
  %303 = add <8 x i32> %299, %10
  %304 = add <8 x i32> %300, %10
  %305 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %301, i32 %4) #8
  %306 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %302, i32 %4) #8
  %307 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %303, i32 %4) #8
  %308 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %304, i32 %4) #8
  %309 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %305, <8 x i32> %306) #8
  %310 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %307, <8 x i32> %308) #8
  %311 = shufflevector <16 x i16> %257, <16 x i16> %260, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %312 = shufflevector <16 x i16> %257, <16 x i16> %260, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %313 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %311, <16 x i16> %263) #8
  %314 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %312, <16 x i16> %263) #8
  %315 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %311, <16 x i16> %266) #8
  %316 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %312, <16 x i16> %266) #8
  %317 = add <8 x i32> %313, %10
  %318 = add <8 x i32> %314, %10
  %319 = add <8 x i32> %315, %10
  %320 = add <8 x i32> %316, %10
  %321 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %317, i32 %4) #8
  %322 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %318, i32 %4) #8
  %323 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %319, i32 %4) #8
  %324 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %320, i32 %4) #8
  %325 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %321, <8 x i32> %322) #8
  %326 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %323, <8 x i32> %324) #8
  %327 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %207, <16 x i16> %277) #8
  %328 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %207, <16 x i16> %277) #8
  %329 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %211, <16 x i16> %278) #8
  %330 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %211, <16 x i16> %278) #8
  %331 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %222, <16 x i16> %293) #8
  %332 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %222, <16 x i16> %293) #8
  %333 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %225, <16 x i16> %294) #8
  %334 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %225, <16 x i16> %294) #8
  %335 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %236, <16 x i16> %309) #8
  %336 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %236, <16 x i16> %309) #8
  %337 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %239, <16 x i16> %310) #8
  %338 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %239, <16 x i16> %310) #8
  %339 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %249, <16 x i16> %325) #8
  %340 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %249, <16 x i16> %325) #8
  %341 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %253, <16 x i16> %326) #8
  %342 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %253, <16 x i16> %326) #8
  %343 = shufflevector <16 x i16> %331, <16 x i16> %333, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %344 = shufflevector <16 x i16> %331, <16 x i16> %333, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %345 = bitcast <8 x i32> %32 to <16 x i16>
  %346 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %343, <16 x i16> %345) #8
  %347 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %344, <16 x i16> %345) #8
  %348 = bitcast <8 x i32> %37 to <16 x i16>
  %349 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %343, <16 x i16> %348) #8
  %350 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %344, <16 x i16> %348) #8
  %351 = add <8 x i32> %346, %10
  %352 = add <8 x i32> %347, %10
  %353 = add <8 x i32> %349, %10
  %354 = add <8 x i32> %350, %10
  %355 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %351, i32 %4) #8
  %356 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %352, i32 %4) #8
  %357 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %353, i32 %4) #8
  %358 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %354, i32 %4) #8
  %359 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %355, <8 x i32> %356) #8
  %360 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %357, <8 x i32> %358) #8
  %361 = shufflevector <16 x i16> %332, <16 x i16> %334, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %362 = shufflevector <16 x i16> %332, <16 x i16> %334, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %363 = bitcast <8 x i32> %43 to <16 x i16>
  %364 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %361, <16 x i16> %363) #8
  %365 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %362, <16 x i16> %363) #8
  %366 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %361, <16 x i16> %345) #8
  %367 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %362, <16 x i16> %345) #8
  %368 = add <8 x i32> %364, %10
  %369 = add <8 x i32> %365, %10
  %370 = add <8 x i32> %366, %10
  %371 = add <8 x i32> %367, %10
  %372 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %368, i32 %4) #8
  %373 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %369, i32 %4) #8
  %374 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %370, i32 %4) #8
  %375 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %371, i32 %4) #8
  %376 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %372, <8 x i32> %373) #8
  %377 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %374, <8 x i32> %375) #8
  %378 = shufflevector <16 x i16> %339, <16 x i16> %341, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %379 = shufflevector <16 x i16> %339, <16 x i16> %341, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %380 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %378, <16 x i16> %345) #8
  %381 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %379, <16 x i16> %345) #8
  %382 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %378, <16 x i16> %348) #8
  %383 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %379, <16 x i16> %348) #8
  %384 = add <8 x i32> %380, %10
  %385 = add <8 x i32> %381, %10
  %386 = add <8 x i32> %382, %10
  %387 = add <8 x i32> %383, %10
  %388 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %384, i32 %4) #8
  %389 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %385, i32 %4) #8
  %390 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %386, i32 %4) #8
  %391 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %387, i32 %4) #8
  %392 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %388, <8 x i32> %389) #8
  %393 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %390, <8 x i32> %391) #8
  %394 = shufflevector <16 x i16> %340, <16 x i16> %342, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %395 = shufflevector <16 x i16> %340, <16 x i16> %342, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %396 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %394, <16 x i16> %363) #8
  %397 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %395, <16 x i16> %363) #8
  %398 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %394, <16 x i16> %345) #8
  %399 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %395, <16 x i16> %345) #8
  %400 = add <8 x i32> %396, %10
  %401 = add <8 x i32> %397, %10
  %402 = add <8 x i32> %398, %10
  %403 = add <8 x i32> %399, %10
  %404 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %400, i32 %4) #8
  %405 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %401, i32 %4) #8
  %406 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %402, i32 %4) #8
  %407 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %403, i32 %4) #8
  %408 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %404, <8 x i32> %405) #8
  %409 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %406, <8 x i32> %407) #8
  %410 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %327, <16 x i16> %359) #8
  %411 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %327, <16 x i16> %359) #8
  %412 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %329, <16 x i16> %360) #8
  %413 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %329, <16 x i16> %360) #8
  %414 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %328, <16 x i16> %376) #8
  %415 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %328, <16 x i16> %376) #8
  %416 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %330, <16 x i16> %377) #8
  %417 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %330, <16 x i16> %377) #8
  %418 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %335, <16 x i16> %392) #8
  %419 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %335, <16 x i16> %392) #8
  %420 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %337, <16 x i16> %393) #8
  %421 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %337, <16 x i16> %393) #8
  %422 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %336, <16 x i16> %408) #8
  %423 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %336, <16 x i16> %408) #8
  %424 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %338, <16 x i16> %409) #8
  %425 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %338, <16 x i16> %409) #8
  %426 = shufflevector <16 x i16> %418, <16 x i16> %420, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %427 = shufflevector <16 x i16> %418, <16 x i16> %420, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %428 = bitcast <8 x i32> %53 to <16 x i16>
  %429 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %426, <16 x i16> %428) #8
  %430 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %427, <16 x i16> %428) #8
  %431 = bitcast <8 x i32> %58 to <16 x i16>
  %432 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %426, <16 x i16> %431) #8
  %433 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %427, <16 x i16> %431) #8
  %434 = add <8 x i32> %429, %10
  %435 = add <8 x i32> %430, %10
  %436 = add <8 x i32> %432, %10
  %437 = add <8 x i32> %433, %10
  %438 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %434, i32 %4) #8
  %439 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %435, i32 %4) #8
  %440 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %436, i32 %4) #8
  %441 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %437, i32 %4) #8
  %442 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %438, <8 x i32> %439) #8
  %443 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %440, <8 x i32> %441) #8
  %444 = shufflevector <16 x i16> %422, <16 x i16> %424, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %445 = shufflevector <16 x i16> %422, <16 x i16> %424, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %446 = bitcast <8 x i32> %68 to <16 x i16>
  %447 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %444, <16 x i16> %446) #8
  %448 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %445, <16 x i16> %446) #8
  %449 = bitcast <8 x i32> %73 to <16 x i16>
  %450 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %444, <16 x i16> %449) #8
  %451 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %445, <16 x i16> %449) #8
  %452 = add <8 x i32> %447, %10
  %453 = add <8 x i32> %448, %10
  %454 = add <8 x i32> %450, %10
  %455 = add <8 x i32> %451, %10
  %456 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %452, i32 %4) #8
  %457 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %453, i32 %4) #8
  %458 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %454, i32 %4) #8
  %459 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %455, i32 %4) #8
  %460 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %456, <8 x i32> %457) #8
  %461 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %458, <8 x i32> %459) #8
  %462 = shufflevector <16 x i16> %419, <16 x i16> %421, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %463 = shufflevector <16 x i16> %419, <16 x i16> %421, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %464 = bitcast <8 x i32> %79 to <16 x i16>
  %465 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %462, <16 x i16> %464) #8
  %466 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %463, <16 x i16> %464) #8
  %467 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %462, <16 x i16> %428) #8
  %468 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %463, <16 x i16> %428) #8
  %469 = add <8 x i32> %465, %10
  %470 = add <8 x i32> %466, %10
  %471 = add <8 x i32> %467, %10
  %472 = add <8 x i32> %468, %10
  %473 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %469, i32 %4) #8
  %474 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %470, i32 %4) #8
  %475 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %471, i32 %4) #8
  %476 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %472, i32 %4) #8
  %477 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %473, <8 x i32> %474) #8
  %478 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %475, <8 x i32> %476) #8
  %479 = shufflevector <16 x i16> %423, <16 x i16> %425, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %480 = shufflevector <16 x i16> %423, <16 x i16> %425, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %481 = bitcast <8 x i32> %85 to <16 x i16>
  %482 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %479, <16 x i16> %481) #8
  %483 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %480, <16 x i16> %481) #8
  %484 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %479, <16 x i16> %446) #8
  %485 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %480, <16 x i16> %446) #8
  %486 = add <8 x i32> %482, %10
  %487 = add <8 x i32> %483, %10
  %488 = add <8 x i32> %484, %10
  %489 = add <8 x i32> %485, %10
  %490 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %486, i32 %4) #8
  %491 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %487, i32 %4) #8
  %492 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %488, i32 %4) #8
  %493 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %489, i32 %4) #8
  %494 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %490, <8 x i32> %491) #8
  %495 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %492, <8 x i32> %493) #8
  %496 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %410, <16 x i16> %442) #8
  %497 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %410, <16 x i16> %442) #8
  %498 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %412, <16 x i16> %443) #8
  %499 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %412, <16 x i16> %443) #8
  %500 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %414, <16 x i16> %460) #8
  %501 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %414, <16 x i16> %460) #8
  %502 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %416, <16 x i16> %461) #8
  %503 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %416, <16 x i16> %461) #8
  %504 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %411, <16 x i16> %477) #8
  %505 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %411, <16 x i16> %477) #8
  %506 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %413, <16 x i16> %478) #8
  %507 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %413, <16 x i16> %478) #8
  %508 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %415, <16 x i16> %494) #8
  %509 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %415, <16 x i16> %494) #8
  %510 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %417, <16 x i16> %495) #8
  %511 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %417, <16 x i16> %495) #8
  %512 = shufflevector <16 x i16> %496, <16 x i16> %498, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %513 = shufflevector <16 x i16> %496, <16 x i16> %498, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %514 = bitcast <8 x i32> %95 to <16 x i16>
  %515 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %512, <16 x i16> %514) #8
  %516 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %513, <16 x i16> %514) #8
  %517 = bitcast <8 x i32> %100 to <16 x i16>
  %518 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %512, <16 x i16> %517) #8
  %519 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %513, <16 x i16> %517) #8
  %520 = add <8 x i32> %515, %10
  %521 = add <8 x i32> %516, %10
  %522 = add <8 x i32> %518, %10
  %523 = add <8 x i32> %519, %10
  %524 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %520, i32 %4) #8
  %525 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %521, i32 %4) #8
  %526 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %522, i32 %4) #8
  %527 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %523, i32 %4) #8
  %528 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %524, <8 x i32> %525) #8
  %529 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %526, <8 x i32> %527) #8
  %530 = shufflevector <16 x i16> %500, <16 x i16> %502, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %531 = shufflevector <16 x i16> %500, <16 x i16> %502, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %532 = bitcast <8 x i32> %110 to <16 x i16>
  %533 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %530, <16 x i16> %532) #8
  %534 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %531, <16 x i16> %532) #8
  %535 = bitcast <8 x i32> %115 to <16 x i16>
  %536 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %530, <16 x i16> %535) #8
  %537 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %531, <16 x i16> %535) #8
  %538 = add <8 x i32> %533, %10
  %539 = add <8 x i32> %534, %10
  %540 = add <8 x i32> %536, %10
  %541 = add <8 x i32> %537, %10
  %542 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %538, i32 %4) #8
  %543 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %539, i32 %4) #8
  %544 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %540, i32 %4) #8
  %545 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %541, i32 %4) #8
  %546 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %542, <8 x i32> %543) #8
  %547 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %544, <8 x i32> %545) #8
  %548 = shufflevector <16 x i16> %504, <16 x i16> %506, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %549 = shufflevector <16 x i16> %504, <16 x i16> %506, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %550 = bitcast <8 x i32> %125 to <16 x i16>
  %551 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %548, <16 x i16> %550) #8
  %552 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %549, <16 x i16> %550) #8
  %553 = bitcast <8 x i32> %130 to <16 x i16>
  %554 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %548, <16 x i16> %553) #8
  %555 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %549, <16 x i16> %553) #8
  %556 = add <8 x i32> %551, %10
  %557 = add <8 x i32> %552, %10
  %558 = add <8 x i32> %554, %10
  %559 = add <8 x i32> %555, %10
  %560 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %556, i32 %4) #8
  %561 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %557, i32 %4) #8
  %562 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %558, i32 %4) #8
  %563 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %559, i32 %4) #8
  %564 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %560, <8 x i32> %561) #8
  %565 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %562, <8 x i32> %563) #8
  %566 = shufflevector <16 x i16> %508, <16 x i16> %510, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %567 = shufflevector <16 x i16> %508, <16 x i16> %510, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %568 = bitcast <8 x i32> %140 to <16 x i16>
  %569 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %566, <16 x i16> %568) #8
  %570 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %567, <16 x i16> %568) #8
  %571 = bitcast <8 x i32> %145 to <16 x i16>
  %572 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %566, <16 x i16> %571) #8
  %573 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %567, <16 x i16> %571) #8
  %574 = add <8 x i32> %569, %10
  %575 = add <8 x i32> %570, %10
  %576 = add <8 x i32> %572, %10
  %577 = add <8 x i32> %573, %10
  %578 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %574, i32 %4) #8
  %579 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %575, i32 %4) #8
  %580 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %576, i32 %4) #8
  %581 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %577, i32 %4) #8
  %582 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %578, <8 x i32> %579) #8
  %583 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %580, <8 x i32> %581) #8
  %584 = shufflevector <16 x i16> %497, <16 x i16> %499, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %585 = shufflevector <16 x i16> %497, <16 x i16> %499, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %586 = bitcast <8 x i32> %155 to <16 x i16>
  %587 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %584, <16 x i16> %586) #8
  %588 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %585, <16 x i16> %586) #8
  %589 = bitcast <8 x i32> %160 to <16 x i16>
  %590 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %584, <16 x i16> %589) #8
  %591 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %585, <16 x i16> %589) #8
  %592 = add <8 x i32> %587, %10
  %593 = add <8 x i32> %588, %10
  %594 = add <8 x i32> %590, %10
  %595 = add <8 x i32> %591, %10
  %596 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %592, i32 %4) #8
  %597 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %593, i32 %4) #8
  %598 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %594, i32 %4) #8
  %599 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %595, i32 %4) #8
  %600 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %596, <8 x i32> %597) #8
  %601 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %598, <8 x i32> %599) #8
  %602 = shufflevector <16 x i16> %501, <16 x i16> %503, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %603 = shufflevector <16 x i16> %501, <16 x i16> %503, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %604 = bitcast <8 x i32> %170 to <16 x i16>
  %605 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %602, <16 x i16> %604) #8
  %606 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %603, <16 x i16> %604) #8
  %607 = bitcast <8 x i32> %175 to <16 x i16>
  %608 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %602, <16 x i16> %607) #8
  %609 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %603, <16 x i16> %607) #8
  %610 = add <8 x i32> %605, %10
  %611 = add <8 x i32> %606, %10
  %612 = add <8 x i32> %608, %10
  %613 = add <8 x i32> %609, %10
  %614 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %610, i32 %4) #8
  %615 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %611, i32 %4) #8
  %616 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %612, i32 %4) #8
  %617 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %613, i32 %4) #8
  %618 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %614, <8 x i32> %615) #8
  %619 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %616, <8 x i32> %617) #8
  %620 = shufflevector <16 x i16> %505, <16 x i16> %507, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %621 = shufflevector <16 x i16> %505, <16 x i16> %507, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %622 = bitcast <8 x i32> %185 to <16 x i16>
  %623 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %620, <16 x i16> %622) #8
  %624 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %621, <16 x i16> %622) #8
  %625 = bitcast <8 x i32> %190 to <16 x i16>
  %626 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %620, <16 x i16> %625) #8
  %627 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %621, <16 x i16> %625) #8
  %628 = add <8 x i32> %623, %10
  %629 = add <8 x i32> %624, %10
  %630 = add <8 x i32> %626, %10
  %631 = add <8 x i32> %627, %10
  %632 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %628, i32 %4) #8
  %633 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %629, i32 %4) #8
  %634 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %630, i32 %4) #8
  %635 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %631, i32 %4) #8
  %636 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %632, <8 x i32> %633) #8
  %637 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %634, <8 x i32> %635) #8
  %638 = shufflevector <16 x i16> %509, <16 x i16> %511, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %639 = shufflevector <16 x i16> %509, <16 x i16> %511, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %640 = bitcast <8 x i32> %200 to <16 x i16>
  %641 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %638, <16 x i16> %640) #8
  %642 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %639, <16 x i16> %640) #8
  %643 = bitcast <8 x i32> %205 to <16 x i16>
  %644 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %638, <16 x i16> %643) #8
  %645 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %639, <16 x i16> %643) #8
  %646 = add <8 x i32> %641, %10
  %647 = add <8 x i32> %642, %10
  %648 = add <8 x i32> %644, %10
  %649 = add <8 x i32> %645, %10
  %650 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %646, i32 %4) #8
  %651 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %647, i32 %4) #8
  %652 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %648, i32 %4) #8
  %653 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %649, i32 %4) #8
  %654 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %650, <8 x i32> %651) #8
  %655 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %652, <8 x i32> %653) #8
  %656 = bitcast <4 x i64>* %1 to <16 x i16>*
  store <16 x i16> %529, <16 x i16>* %656, align 32
  %657 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %658 = bitcast <4 x i64>* %657 to <16 x i16>*
  store <16 x i16> %654, <16 x i16>* %658, align 32
  %659 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %660 = bitcast <4 x i64>* %659 to <16 x i16>*
  store <16 x i16> %547, <16 x i16>* %660, align 32
  %661 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %662 = bitcast <4 x i64>* %661 to <16 x i16>*
  store <16 x i16> %636, <16 x i16>* %662, align 32
  %663 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %664 = bitcast <4 x i64>* %663 to <16 x i16>*
  store <16 x i16> %565, <16 x i16>* %664, align 32
  %665 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %666 = bitcast <4 x i64>* %665 to <16 x i16>*
  store <16 x i16> %618, <16 x i16>* %666, align 32
  %667 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %668 = bitcast <4 x i64>* %667 to <16 x i16>*
  store <16 x i16> %583, <16 x i16>* %668, align 32
  %669 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %670 = bitcast <4 x i64>* %669 to <16 x i16>*
  store <16 x i16> %600, <16 x i16>* %670, align 32
  %671 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %672 = bitcast <4 x i64>* %671 to <16 x i16>*
  store <16 x i16> %601, <16 x i16>* %672, align 32
  %673 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %674 = bitcast <4 x i64>* %673 to <16 x i16>*
  store <16 x i16> %582, <16 x i16>* %674, align 32
  %675 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %676 = bitcast <4 x i64>* %675 to <16 x i16>*
  store <16 x i16> %619, <16 x i16>* %676, align 32
  %677 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %678 = bitcast <4 x i64>* %677 to <16 x i16>*
  store <16 x i16> %564, <16 x i16>* %678, align 32
  %679 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %680 = bitcast <4 x i64>* %679 to <16 x i16>*
  store <16 x i16> %637, <16 x i16>* %680, align 32
  %681 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %682 = bitcast <4 x i64>* %681 to <16 x i16>*
  store <16 x i16> %546, <16 x i16>* %682, align 32
  %683 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %684 = bitcast <4 x i64>* %683 to <16 x i16>*
  store <16 x i16> %655, <16 x i16>* %684, align 32
  %685 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 15
  %686 = bitcast <4 x i64>* %685 to <16 x i16>*
  store <16 x i16> %528, <16 x i16>* %686, align 32
  ret void
}

; Function Attrs: inlinehint nofree nounwind ssp uwtable
define internal void @fidentity16x16_new_avx2(<4 x i64>* nocapture readonly, <4 x i64>* nocapture, i8 signext) #5 {
  br label %5

4:                                                ; preds = %5
  ret void

5:                                                ; preds = %5, %3
  %6 = phi i64 [ 0, %3 ], [ %32, %5 ]
  %7 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 %6
  %8 = bitcast <4 x i64>* %7 to <16 x i16>*
  %9 = load <16 x i16>, <16 x i16>* %8, align 32
  %10 = shufflevector <16 x i16> %9, <16 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %11 = shufflevector <16 x i16> %9, <16 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %12 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %10, <16 x i16> <i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048>) #8
  %13 = ashr <8 x i32> %12, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %14 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %11, <16 x i16> <i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048>) #8
  %15 = ashr <8 x i32> %14, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %16 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %13, <8 x i32> %15) #8
  %17 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 %6
  %18 = bitcast <4 x i64>* %17 to <16 x i16>*
  store <16 x i16> %16, <16 x i16>* %18, align 32
  %19 = or i64 %6, 1
  %20 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 %19
  %21 = bitcast <4 x i64>* %20 to <16 x i16>*
  %22 = load <16 x i16>, <16 x i16>* %21, align 32
  %23 = shufflevector <16 x i16> %22, <16 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %24 = shufflevector <16 x i16> %22, <16 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %25 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %23, <16 x i16> <i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048>) #8
  %26 = ashr <8 x i32> %25, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %27 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %24, <16 x i16> <i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048>) #8
  %28 = ashr <8 x i32> %27, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %29 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %26, <8 x i32> %28) #8
  %30 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 %19
  %31 = bitcast <4 x i64>* %30 to <16 x i16>*
  store <16 x i16> %29, <16 x i16>* %31, align 32
  %32 = add nuw nsw i64 %6, 2
  %33 = icmp eq i64 %32, 16
  br i1 %33, label %4, label %5
}

; Function Attrs: nounwind readnone speculatable
declare <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16>, <16 x i16>) #6

; Function Attrs: nounwind readnone speculatable
declare <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16>, <16 x i16>) #6

; Function Attrs: nounwind readnone
declare <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16>, <16 x i16>) #7

; Function Attrs: nounwind readnone
declare <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32>, i32) #7

; Function Attrs: nounwind readnone
declare <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32>, <8 x i32>) #7

; Function Attrs: nounwind readnone
declare <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16>, i32) #7

; Function Attrs: nounwind readnone
declare <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16>, i32) #7

; Function Attrs: inlinehint nounwind ssp uwtable
define internal void @fdct16x32_avx2(<4 x i64>* readonly, <4 x i64>*, i8 signext) #4 {
  %4 = sext i8 %2 to i32
  %5 = add nsw i32 %4, -10
  %6 = sext i32 %5 to i64
  %7 = add nsw i32 %4, -1
  %8 = shl i32 1, %7
  %9 = insertelement <8 x i32> undef, i32 %8, i32 0
  %10 = shufflevector <8 x i32> %9, <8 x i32> undef, <8 x i32> zeroinitializer
  %11 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 32
  %12 = load i32, i32* %11, align 16
  %13 = sub i32 0, %12
  %14 = and i32 %13, 65535
  %15 = and i32 %12, 65535
  %16 = shl nuw i32 %15, 16
  %17 = or i32 %16, %14
  %18 = insertelement <8 x i32> undef, i32 %17, i32 0
  %19 = shufflevector <8 x i32> %18, <8 x i32> undef, <8 x i32> zeroinitializer
  %20 = or i32 %16, %15
  %21 = insertelement <8 x i32> undef, i32 %20, i32 0
  %22 = shufflevector <8 x i32> %21, <8 x i32> undef, <8 x i32> zeroinitializer
  %23 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 16
  %24 = load i32, i32* %23, align 16
  %25 = sub i32 0, %24
  %26 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 48
  %27 = load i32, i32* %26, align 16
  %28 = and i32 %25, 65535
  %29 = and i32 %27, 65535
  %30 = shl nuw i32 %29, 16
  %31 = or i32 %30, %28
  %32 = insertelement <8 x i32> undef, i32 %31, i32 0
  %33 = shufflevector <8 x i32> %32, <8 x i32> undef, <8 x i32> zeroinitializer
  %34 = shl i32 %24, 16
  %35 = or i32 %29, %34
  %36 = insertelement <8 x i32> undef, i32 %35, i32 0
  %37 = shufflevector <8 x i32> %36, <8 x i32> undef, <8 x i32> zeroinitializer
  %38 = sub i32 0, %27
  %39 = and i32 %38, 65535
  %40 = shl nuw i32 %28, 16
  %41 = or i32 %40, %39
  %42 = insertelement <8 x i32> undef, i32 %41, i32 0
  %43 = shufflevector <8 x i32> %42, <8 x i32> undef, <8 x i32> zeroinitializer
  %44 = shl nuw i32 %14, 16
  %45 = or i32 %44, %15
  %46 = insertelement <8 x i32> undef, i32 %45, i32 0
  %47 = shufflevector <8 x i32> %46, <8 x i32> undef, <8 x i32> zeroinitializer
  %48 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 56
  %49 = load i32, i32* %48, align 16
  %50 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 8
  %51 = load i32, i32* %50, align 16
  %52 = and i32 %49, 65535
  %53 = shl i32 %51, 16
  %54 = or i32 %53, %52
  %55 = insertelement <8 x i32> undef, i32 %54, i32 0
  %56 = shufflevector <8 x i32> %55, <8 x i32> undef, <8 x i32> zeroinitializer
  %57 = sub i32 0, %51
  %58 = and i32 %57, 65535
  %59 = shl nuw i32 %52, 16
  %60 = or i32 %59, %58
  %61 = insertelement <8 x i32> undef, i32 %60, i32 0
  %62 = shufflevector <8 x i32> %61, <8 x i32> undef, <8 x i32> zeroinitializer
  %63 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 24
  %64 = load i32, i32* %63, align 16
  %65 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 40
  %66 = load i32, i32* %65, align 16
  %67 = and i32 %64, 65535
  %68 = shl i32 %66, 16
  %69 = or i32 %68, %67
  %70 = insertelement <8 x i32> undef, i32 %69, i32 0
  %71 = shufflevector <8 x i32> %70, <8 x i32> undef, <8 x i32> zeroinitializer
  %72 = sub i32 0, %66
  %73 = and i32 %72, 65535
  %74 = shl nuw i32 %67, 16
  %75 = or i32 %74, %73
  %76 = insertelement <8 x i32> undef, i32 %75, i32 0
  %77 = shufflevector <8 x i32> %76, <8 x i32> undef, <8 x i32> zeroinitializer
  %78 = sub i32 0, %49
  %79 = and i32 %78, 65535
  %80 = shl nuw i32 %58, 16
  %81 = or i32 %80, %79
  %82 = insertelement <8 x i32> undef, i32 %81, i32 0
  %83 = shufflevector <8 x i32> %82, <8 x i32> undef, <8 x i32> zeroinitializer
  %84 = sub i32 0, %64
  %85 = and i32 %84, 65535
  %86 = shl nuw i32 %73, 16
  %87 = or i32 %86, %85
  %88 = insertelement <8 x i32> undef, i32 %87, i32 0
  %89 = shufflevector <8 x i32> %88, <8 x i32> undef, <8 x i32> zeroinitializer
  %90 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 60
  %91 = load i32, i32* %90, align 16
  %92 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 4
  %93 = load i32, i32* %92, align 16
  %94 = and i32 %91, 65535
  %95 = shl i32 %93, 16
  %96 = or i32 %95, %94
  %97 = insertelement <8 x i32> undef, i32 %96, i32 0
  %98 = shufflevector <8 x i32> %97, <8 x i32> undef, <8 x i32> zeroinitializer
  %99 = sub i32 0, %93
  %100 = and i32 %99, 65535
  %101 = shl nuw i32 %94, 16
  %102 = or i32 %101, %100
  %103 = insertelement <8 x i32> undef, i32 %102, i32 0
  %104 = shufflevector <8 x i32> %103, <8 x i32> undef, <8 x i32> zeroinitializer
  %105 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 28
  %106 = load i32, i32* %105, align 16
  %107 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 36
  %108 = load i32, i32* %107, align 16
  %109 = and i32 %106, 65535
  %110 = shl i32 %108, 16
  %111 = or i32 %110, %109
  %112 = insertelement <8 x i32> undef, i32 %111, i32 0
  %113 = shufflevector <8 x i32> %112, <8 x i32> undef, <8 x i32> zeroinitializer
  %114 = sub i32 0, %108
  %115 = and i32 %114, 65535
  %116 = shl nuw i32 %109, 16
  %117 = or i32 %116, %115
  %118 = insertelement <8 x i32> undef, i32 %117, i32 0
  %119 = shufflevector <8 x i32> %118, <8 x i32> undef, <8 x i32> zeroinitializer
  %120 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 44
  %121 = load i32, i32* %120, align 16
  %122 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 20
  %123 = load i32, i32* %122, align 16
  %124 = and i32 %121, 65535
  %125 = shl i32 %123, 16
  %126 = or i32 %125, %124
  %127 = insertelement <8 x i32> undef, i32 %126, i32 0
  %128 = shufflevector <8 x i32> %127, <8 x i32> undef, <8 x i32> zeroinitializer
  %129 = sub i32 0, %123
  %130 = and i32 %129, 65535
  %131 = shl nuw i32 %124, 16
  %132 = or i32 %131, %130
  %133 = insertelement <8 x i32> undef, i32 %132, i32 0
  %134 = shufflevector <8 x i32> %133, <8 x i32> undef, <8 x i32> zeroinitializer
  %135 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 12
  %136 = load i32, i32* %135, align 16
  %137 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 52
  %138 = load i32, i32* %137, align 16
  %139 = and i32 %136, 65535
  %140 = shl i32 %138, 16
  %141 = or i32 %140, %139
  %142 = insertelement <8 x i32> undef, i32 %141, i32 0
  %143 = shufflevector <8 x i32> %142, <8 x i32> undef, <8 x i32> zeroinitializer
  %144 = sub i32 0, %138
  %145 = and i32 %144, 65535
  %146 = shl nuw i32 %139, 16
  %147 = or i32 %146, %145
  %148 = insertelement <8 x i32> undef, i32 %147, i32 0
  %149 = shufflevector <8 x i32> %148, <8 x i32> undef, <8 x i32> zeroinitializer
  %150 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 62
  %151 = load i32, i32* %150, align 8
  %152 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 2
  %153 = load i32, i32* %152, align 8
  %154 = and i32 %151, 65535
  %155 = shl i32 %153, 16
  %156 = or i32 %155, %154
  %157 = insertelement <8 x i32> undef, i32 %156, i32 0
  %158 = shufflevector <8 x i32> %157, <8 x i32> undef, <8 x i32> zeroinitializer
  %159 = sub i32 0, %153
  %160 = and i32 %159, 65535
  %161 = shl nuw i32 %154, 16
  %162 = or i32 %161, %160
  %163 = insertelement <8 x i32> undef, i32 %162, i32 0
  %164 = shufflevector <8 x i32> %163, <8 x i32> undef, <8 x i32> zeroinitializer
  %165 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 30
  %166 = load i32, i32* %165, align 8
  %167 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 34
  %168 = load i32, i32* %167, align 8
  %169 = and i32 %166, 65535
  %170 = shl i32 %168, 16
  %171 = or i32 %170, %169
  %172 = insertelement <8 x i32> undef, i32 %171, i32 0
  %173 = shufflevector <8 x i32> %172, <8 x i32> undef, <8 x i32> zeroinitializer
  %174 = sub i32 0, %168
  %175 = and i32 %174, 65535
  %176 = shl nuw i32 %169, 16
  %177 = or i32 %176, %175
  %178 = insertelement <8 x i32> undef, i32 %177, i32 0
  %179 = shufflevector <8 x i32> %178, <8 x i32> undef, <8 x i32> zeroinitializer
  %180 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 46
  %181 = load i32, i32* %180, align 8
  %182 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 18
  %183 = load i32, i32* %182, align 8
  %184 = and i32 %181, 65535
  %185 = shl i32 %183, 16
  %186 = or i32 %185, %184
  %187 = insertelement <8 x i32> undef, i32 %186, i32 0
  %188 = shufflevector <8 x i32> %187, <8 x i32> undef, <8 x i32> zeroinitializer
  %189 = sub i32 0, %183
  %190 = and i32 %189, 65535
  %191 = shl nuw i32 %184, 16
  %192 = or i32 %191, %190
  %193 = insertelement <8 x i32> undef, i32 %192, i32 0
  %194 = shufflevector <8 x i32> %193, <8 x i32> undef, <8 x i32> zeroinitializer
  %195 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 14
  %196 = load i32, i32* %195, align 8
  %197 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 50
  %198 = load i32, i32* %197, align 8
  %199 = and i32 %196, 65535
  %200 = shl i32 %198, 16
  %201 = or i32 %200, %199
  %202 = insertelement <8 x i32> undef, i32 %201, i32 0
  %203 = shufflevector <8 x i32> %202, <8 x i32> undef, <8 x i32> zeroinitializer
  %204 = sub i32 0, %198
  %205 = and i32 %204, 65535
  %206 = shl nuw i32 %199, 16
  %207 = or i32 %206, %205
  %208 = insertelement <8 x i32> undef, i32 %207, i32 0
  %209 = shufflevector <8 x i32> %208, <8 x i32> undef, <8 x i32> zeroinitializer
  %210 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 54
  %211 = load i32, i32* %210, align 8
  %212 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 10
  %213 = load i32, i32* %212, align 8
  %214 = and i32 %211, 65535
  %215 = shl i32 %213, 16
  %216 = or i32 %215, %214
  %217 = insertelement <8 x i32> undef, i32 %216, i32 0
  %218 = shufflevector <8 x i32> %217, <8 x i32> undef, <8 x i32> zeroinitializer
  %219 = sub i32 0, %213
  %220 = and i32 %219, 65535
  %221 = shl nuw i32 %214, 16
  %222 = or i32 %221, %220
  %223 = insertelement <8 x i32> undef, i32 %222, i32 0
  %224 = shufflevector <8 x i32> %223, <8 x i32> undef, <8 x i32> zeroinitializer
  %225 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 22
  %226 = load i32, i32* %225, align 8
  %227 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 42
  %228 = load i32, i32* %227, align 8
  %229 = and i32 %226, 65535
  %230 = shl i32 %228, 16
  %231 = or i32 %230, %229
  %232 = insertelement <8 x i32> undef, i32 %231, i32 0
  %233 = shufflevector <8 x i32> %232, <8 x i32> undef, <8 x i32> zeroinitializer
  %234 = sub i32 0, %228
  %235 = and i32 %234, 65535
  %236 = shl nuw i32 %229, 16
  %237 = or i32 %236, %235
  %238 = insertelement <8 x i32> undef, i32 %237, i32 0
  %239 = shufflevector <8 x i32> %238, <8 x i32> undef, <8 x i32> zeroinitializer
  %240 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 38
  %241 = load i32, i32* %240, align 8
  %242 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 26
  %243 = load i32, i32* %242, align 8
  %244 = and i32 %241, 65535
  %245 = shl i32 %243, 16
  %246 = or i32 %245, %244
  %247 = insertelement <8 x i32> undef, i32 %246, i32 0
  %248 = shufflevector <8 x i32> %247, <8 x i32> undef, <8 x i32> zeroinitializer
  %249 = sub i32 0, %243
  %250 = and i32 %249, 65535
  %251 = shl nuw i32 %244, 16
  %252 = or i32 %251, %250
  %253 = insertelement <8 x i32> undef, i32 %252, i32 0
  %254 = shufflevector <8 x i32> %253, <8 x i32> undef, <8 x i32> zeroinitializer
  %255 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 6
  %256 = load i32, i32* %255, align 8
  %257 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 58
  %258 = load i32, i32* %257, align 8
  %259 = and i32 %256, 65535
  %260 = shl i32 %258, 16
  %261 = or i32 %260, %259
  %262 = insertelement <8 x i32> undef, i32 %261, i32 0
  %263 = shufflevector <8 x i32> %262, <8 x i32> undef, <8 x i32> zeroinitializer
  %264 = sub i32 0, %258
  %265 = and i32 %264, 65535
  %266 = shl nuw i32 %259, 16
  %267 = or i32 %266, %265
  %268 = insertelement <8 x i32> undef, i32 %267, i32 0
  %269 = shufflevector <8 x i32> %268, <8 x i32> undef, <8 x i32> zeroinitializer
  %270 = bitcast <4 x i64>* %0 to <16 x i16>*
  %271 = load <16 x i16>, <16 x i16>* %270, align 32
  %272 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 31
  %273 = bitcast <4 x i64>* %272 to <16 x i16>*
  %274 = load <16 x i16>, <16 x i16>* %273, align 32
  %275 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %271, <16 x i16> %274) #8
  %276 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %271, <16 x i16> %274) #8
  %277 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 1
  %278 = bitcast <4 x i64>* %277 to <16 x i16>*
  %279 = load <16 x i16>, <16 x i16>* %278, align 32
  %280 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 30
  %281 = bitcast <4 x i64>* %280 to <16 x i16>*
  %282 = load <16 x i16>, <16 x i16>* %281, align 32
  %283 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %279, <16 x i16> %282) #8
  %284 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %279, <16 x i16> %282) #8
  %285 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 2
  %286 = bitcast <4 x i64>* %285 to <16 x i16>*
  %287 = load <16 x i16>, <16 x i16>* %286, align 32
  %288 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 29
  %289 = bitcast <4 x i64>* %288 to <16 x i16>*
  %290 = load <16 x i16>, <16 x i16>* %289, align 32
  %291 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %287, <16 x i16> %290) #8
  %292 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %287, <16 x i16> %290) #8
  %293 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 3
  %294 = bitcast <4 x i64>* %293 to <16 x i16>*
  %295 = load <16 x i16>, <16 x i16>* %294, align 32
  %296 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 28
  %297 = bitcast <4 x i64>* %296 to <16 x i16>*
  %298 = load <16 x i16>, <16 x i16>* %297, align 32
  %299 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %295, <16 x i16> %298) #8
  %300 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %295, <16 x i16> %298) #8
  %301 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 4
  %302 = bitcast <4 x i64>* %301 to <16 x i16>*
  %303 = load <16 x i16>, <16 x i16>* %302, align 32
  %304 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 27
  %305 = bitcast <4 x i64>* %304 to <16 x i16>*
  %306 = load <16 x i16>, <16 x i16>* %305, align 32
  %307 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %303, <16 x i16> %306) #8
  %308 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %303, <16 x i16> %306) #8
  %309 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 5
  %310 = bitcast <4 x i64>* %309 to <16 x i16>*
  %311 = load <16 x i16>, <16 x i16>* %310, align 32
  %312 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 26
  %313 = bitcast <4 x i64>* %312 to <16 x i16>*
  %314 = load <16 x i16>, <16 x i16>* %313, align 32
  %315 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %311, <16 x i16> %314) #8
  %316 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %311, <16 x i16> %314) #8
  %317 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 6
  %318 = bitcast <4 x i64>* %317 to <16 x i16>*
  %319 = load <16 x i16>, <16 x i16>* %318, align 32
  %320 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 25
  %321 = bitcast <4 x i64>* %320 to <16 x i16>*
  %322 = load <16 x i16>, <16 x i16>* %321, align 32
  %323 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %319, <16 x i16> %322) #8
  %324 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %319, <16 x i16> %322) #8
  %325 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 7
  %326 = bitcast <4 x i64>* %325 to <16 x i16>*
  %327 = load <16 x i16>, <16 x i16>* %326, align 32
  %328 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 24
  %329 = bitcast <4 x i64>* %328 to <16 x i16>*
  %330 = load <16 x i16>, <16 x i16>* %329, align 32
  %331 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %327, <16 x i16> %330) #8
  %332 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %327, <16 x i16> %330) #8
  %333 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 8
  %334 = bitcast <4 x i64>* %333 to <16 x i16>*
  %335 = load <16 x i16>, <16 x i16>* %334, align 32
  %336 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 23
  %337 = bitcast <4 x i64>* %336 to <16 x i16>*
  %338 = load <16 x i16>, <16 x i16>* %337, align 32
  %339 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %335, <16 x i16> %338) #8
  %340 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %335, <16 x i16> %338) #8
  %341 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 9
  %342 = bitcast <4 x i64>* %341 to <16 x i16>*
  %343 = load <16 x i16>, <16 x i16>* %342, align 32
  %344 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 22
  %345 = bitcast <4 x i64>* %344 to <16 x i16>*
  %346 = load <16 x i16>, <16 x i16>* %345, align 32
  %347 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %343, <16 x i16> %346) #8
  %348 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %343, <16 x i16> %346) #8
  %349 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 10
  %350 = bitcast <4 x i64>* %349 to <16 x i16>*
  %351 = load <16 x i16>, <16 x i16>* %350, align 32
  %352 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 21
  %353 = bitcast <4 x i64>* %352 to <16 x i16>*
  %354 = load <16 x i16>, <16 x i16>* %353, align 32
  %355 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %351, <16 x i16> %354) #8
  %356 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %351, <16 x i16> %354) #8
  %357 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 11
  %358 = bitcast <4 x i64>* %357 to <16 x i16>*
  %359 = load <16 x i16>, <16 x i16>* %358, align 32
  %360 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 20
  %361 = bitcast <4 x i64>* %360 to <16 x i16>*
  %362 = load <16 x i16>, <16 x i16>* %361, align 32
  %363 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %359, <16 x i16> %362) #8
  %364 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %359, <16 x i16> %362) #8
  %365 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 12
  %366 = bitcast <4 x i64>* %365 to <16 x i16>*
  %367 = load <16 x i16>, <16 x i16>* %366, align 32
  %368 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 19
  %369 = bitcast <4 x i64>* %368 to <16 x i16>*
  %370 = load <16 x i16>, <16 x i16>* %369, align 32
  %371 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %367, <16 x i16> %370) #8
  %372 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %367, <16 x i16> %370) #8
  %373 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 13
  %374 = bitcast <4 x i64>* %373 to <16 x i16>*
  %375 = load <16 x i16>, <16 x i16>* %374, align 32
  %376 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 18
  %377 = bitcast <4 x i64>* %376 to <16 x i16>*
  %378 = load <16 x i16>, <16 x i16>* %377, align 32
  %379 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %375, <16 x i16> %378) #8
  %380 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %375, <16 x i16> %378) #8
  %381 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 14
  %382 = bitcast <4 x i64>* %381 to <16 x i16>*
  %383 = load <16 x i16>, <16 x i16>* %382, align 32
  %384 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 17
  %385 = bitcast <4 x i64>* %384 to <16 x i16>*
  %386 = load <16 x i16>, <16 x i16>* %385, align 32
  %387 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %383, <16 x i16> %386) #8
  %388 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %383, <16 x i16> %386) #8
  %389 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 15
  %390 = bitcast <4 x i64>* %389 to <16 x i16>*
  %391 = load <16 x i16>, <16 x i16>* %390, align 32
  %392 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 16
  %393 = bitcast <4 x i64>* %392 to <16 x i16>*
  %394 = load <16 x i16>, <16 x i16>* %393, align 32
  %395 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %391, <16 x i16> %394) #8
  %396 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %391, <16 x i16> %394) #8
  %397 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %275, <16 x i16> %395) #8
  %398 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %275, <16 x i16> %395) #8
  %399 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %283, <16 x i16> %387) #8
  %400 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %283, <16 x i16> %387) #8
  %401 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %291, <16 x i16> %379) #8
  %402 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %291, <16 x i16> %379) #8
  %403 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %299, <16 x i16> %371) #8
  %404 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %299, <16 x i16> %371) #8
  %405 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %307, <16 x i16> %363) #8
  %406 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %307, <16 x i16> %363) #8
  %407 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %315, <16 x i16> %355) #8
  %408 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %315, <16 x i16> %355) #8
  %409 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %323, <16 x i16> %347) #8
  %410 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %323, <16 x i16> %347) #8
  %411 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %331, <16 x i16> %339) #8
  %412 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %331, <16 x i16> %339) #8
  %413 = shufflevector <16 x i16> %364, <16 x i16> %308, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %414 = shufflevector <16 x i16> %364, <16 x i16> %308, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %415 = bitcast <8 x i32> %19 to <16 x i16>
  %416 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %413, <16 x i16> %415) #8
  %417 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %414, <16 x i16> %415) #8
  %418 = bitcast <8 x i32> %22 to <16 x i16>
  %419 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %413, <16 x i16> %418) #8
  %420 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %414, <16 x i16> %418) #8
  %421 = add <8 x i32> %416, %10
  %422 = add <8 x i32> %417, %10
  %423 = add <8 x i32> %419, %10
  %424 = add <8 x i32> %420, %10
  %425 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %421, i32 %4) #8
  %426 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %422, i32 %4) #8
  %427 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %423, i32 %4) #8
  %428 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %424, i32 %4) #8
  %429 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %425, <8 x i32> %426) #8
  %430 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %427, <8 x i32> %428) #8
  %431 = shufflevector <16 x i16> %356, <16 x i16> %316, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %432 = shufflevector <16 x i16> %356, <16 x i16> %316, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %433 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %431, <16 x i16> %415) #8
  %434 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %432, <16 x i16> %415) #8
  %435 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %431, <16 x i16> %418) #8
  %436 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %432, <16 x i16> %418) #8
  %437 = add <8 x i32> %433, %10
  %438 = add <8 x i32> %434, %10
  %439 = add <8 x i32> %435, %10
  %440 = add <8 x i32> %436, %10
  %441 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %437, i32 %4) #8
  %442 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %438, i32 %4) #8
  %443 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %439, i32 %4) #8
  %444 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %440, i32 %4) #8
  %445 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %441, <8 x i32> %442) #8
  %446 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %443, <8 x i32> %444) #8
  %447 = shufflevector <16 x i16> %348, <16 x i16> %324, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %448 = shufflevector <16 x i16> %348, <16 x i16> %324, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %449 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %447, <16 x i16> %415) #8
  %450 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %448, <16 x i16> %415) #8
  %451 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %447, <16 x i16> %418) #8
  %452 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %448, <16 x i16> %418) #8
  %453 = add <8 x i32> %449, %10
  %454 = add <8 x i32> %450, %10
  %455 = add <8 x i32> %451, %10
  %456 = add <8 x i32> %452, %10
  %457 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %453, i32 %4) #8
  %458 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %454, i32 %4) #8
  %459 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %455, i32 %4) #8
  %460 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %456, i32 %4) #8
  %461 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %457, <8 x i32> %458) #8
  %462 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %459, <8 x i32> %460) #8
  %463 = shufflevector <16 x i16> %340, <16 x i16> %332, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %464 = shufflevector <16 x i16> %340, <16 x i16> %332, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %465 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %463, <16 x i16> %415) #8
  %466 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %464, <16 x i16> %415) #8
  %467 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %463, <16 x i16> %418) #8
  %468 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %464, <16 x i16> %418) #8
  %469 = add <8 x i32> %465, %10
  %470 = add <8 x i32> %466, %10
  %471 = add <8 x i32> %467, %10
  %472 = add <8 x i32> %468, %10
  %473 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %469, i32 %4) #8
  %474 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %470, i32 %4) #8
  %475 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %471, i32 %4) #8
  %476 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %472, i32 %4) #8
  %477 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %473, <8 x i32> %474) #8
  %478 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %475, <8 x i32> %476) #8
  %479 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %397, <16 x i16> %411) #8
  %480 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %397, <16 x i16> %411) #8
  %481 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %399, <16 x i16> %409) #8
  %482 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %399, <16 x i16> %409) #8
  %483 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %401, <16 x i16> %407) #8
  %484 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %401, <16 x i16> %407) #8
  %485 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %403, <16 x i16> %405) #8
  %486 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %403, <16 x i16> %405) #8
  %487 = shufflevector <16 x i16> %408, <16 x i16> %402, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %488 = shufflevector <16 x i16> %408, <16 x i16> %402, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %489 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %487, <16 x i16> %415) #8
  %490 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %488, <16 x i16> %415) #8
  %491 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %487, <16 x i16> %418) #8
  %492 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %488, <16 x i16> %418) #8
  %493 = add <8 x i32> %489, %10
  %494 = add <8 x i32> %490, %10
  %495 = add <8 x i32> %491, %10
  %496 = add <8 x i32> %492, %10
  %497 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %493, i32 %4) #8
  %498 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %494, i32 %4) #8
  %499 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %495, i32 %4) #8
  %500 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %496, i32 %4) #8
  %501 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %497, <8 x i32> %498) #8
  %502 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %499, <8 x i32> %500) #8
  %503 = shufflevector <16 x i16> %406, <16 x i16> %404, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %504 = shufflevector <16 x i16> %406, <16 x i16> %404, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %505 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %503, <16 x i16> %415) #8
  %506 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %504, <16 x i16> %415) #8
  %507 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %503, <16 x i16> %418) #8
  %508 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %504, <16 x i16> %418) #8
  %509 = add <8 x i32> %505, %10
  %510 = add <8 x i32> %506, %10
  %511 = add <8 x i32> %507, %10
  %512 = add <8 x i32> %508, %10
  %513 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %509, i32 %4) #8
  %514 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %510, i32 %4) #8
  %515 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %511, i32 %4) #8
  %516 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %512, i32 %4) #8
  %517 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %513, <8 x i32> %514) #8
  %518 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %515, <8 x i32> %516) #8
  %519 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %396, <16 x i16> %477) #8
  %520 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %396, <16 x i16> %477) #8
  %521 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %388, <16 x i16> %461) #8
  %522 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %388, <16 x i16> %461) #8
  %523 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %380, <16 x i16> %445) #8
  %524 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %380, <16 x i16> %445) #8
  %525 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %372, <16 x i16> %429) #8
  %526 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %372, <16 x i16> %429) #8
  %527 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %276, <16 x i16> %478) #8
  %528 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %276, <16 x i16> %478) #8
  %529 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %284, <16 x i16> %462) #8
  %530 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %284, <16 x i16> %462) #8
  %531 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %292, <16 x i16> %446) #8
  %532 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %292, <16 x i16> %446) #8
  %533 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %300, <16 x i16> %430) #8
  %534 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %300, <16 x i16> %430) #8
  %535 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %479, <16 x i16> %485) #8
  %536 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %479, <16 x i16> %485) #8
  %537 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %481, <16 x i16> %483) #8
  %538 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %481, <16 x i16> %483) #8
  %539 = shufflevector <16 x i16> %484, <16 x i16> %482, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %540 = shufflevector <16 x i16> %484, <16 x i16> %482, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %541 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %539, <16 x i16> %415) #8
  %542 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %540, <16 x i16> %415) #8
  %543 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %539, <16 x i16> %418) #8
  %544 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %540, <16 x i16> %418) #8
  %545 = add <8 x i32> %541, %10
  %546 = add <8 x i32> %542, %10
  %547 = add <8 x i32> %543, %10
  %548 = add <8 x i32> %544, %10
  %549 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %545, i32 %4) #8
  %550 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %546, i32 %4) #8
  %551 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %547, i32 %4) #8
  %552 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %548, i32 %4) #8
  %553 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %549, <8 x i32> %550) #8
  %554 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %551, <8 x i32> %552) #8
  %555 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %412, <16 x i16> %517) #8
  %556 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %412, <16 x i16> %517) #8
  %557 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %410, <16 x i16> %501) #8
  %558 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %410, <16 x i16> %501) #8
  %559 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %398, <16 x i16> %518) #8
  %560 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %398, <16 x i16> %518) #8
  %561 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %400, <16 x i16> %502) #8
  %562 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %400, <16 x i16> %502) #8
  %563 = shufflevector <16 x i16> %523, <16 x i16> %531, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %564 = shufflevector <16 x i16> %523, <16 x i16> %531, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %565 = bitcast <8 x i32> %33 to <16 x i16>
  %566 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %563, <16 x i16> %565) #8
  %567 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %564, <16 x i16> %565) #8
  %568 = bitcast <8 x i32> %37 to <16 x i16>
  %569 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %563, <16 x i16> %568) #8
  %570 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %564, <16 x i16> %568) #8
  %571 = add <8 x i32> %566, %10
  %572 = add <8 x i32> %567, %10
  %573 = add <8 x i32> %569, %10
  %574 = add <8 x i32> %570, %10
  %575 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %571, i32 %4) #8
  %576 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %572, i32 %4) #8
  %577 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %573, i32 %4) #8
  %578 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %574, i32 %4) #8
  %579 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %575, <8 x i32> %576) #8
  %580 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %577, <8 x i32> %578) #8
  %581 = shufflevector <16 x i16> %525, <16 x i16> %533, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %582 = shufflevector <16 x i16> %525, <16 x i16> %533, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %583 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %581, <16 x i16> %565) #8
  %584 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %582, <16 x i16> %565) #8
  %585 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %581, <16 x i16> %568) #8
  %586 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %582, <16 x i16> %568) #8
  %587 = add <8 x i32> %583, %10
  %588 = add <8 x i32> %584, %10
  %589 = add <8 x i32> %585, %10
  %590 = add <8 x i32> %586, %10
  %591 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %587, i32 %4) #8
  %592 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %588, i32 %4) #8
  %593 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %589, i32 %4) #8
  %594 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %590, i32 %4) #8
  %595 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %591, <8 x i32> %592) #8
  %596 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %593, <8 x i32> %594) #8
  %597 = shufflevector <16 x i16> %526, <16 x i16> %534, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %598 = shufflevector <16 x i16> %526, <16 x i16> %534, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %599 = bitcast <8 x i32> %43 to <16 x i16>
  %600 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %597, <16 x i16> %599) #8
  %601 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %598, <16 x i16> %599) #8
  %602 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %597, <16 x i16> %565) #8
  %603 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %598, <16 x i16> %565) #8
  %604 = add <8 x i32> %600, %10
  %605 = add <8 x i32> %601, %10
  %606 = add <8 x i32> %602, %10
  %607 = add <8 x i32> %603, %10
  %608 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %604, i32 %4) #8
  %609 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %605, i32 %4) #8
  %610 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %606, i32 %4) #8
  %611 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %607, i32 %4) #8
  %612 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %608, <8 x i32> %609) #8
  %613 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %610, <8 x i32> %611) #8
  %614 = shufflevector <16 x i16> %524, <16 x i16> %532, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %615 = shufflevector <16 x i16> %524, <16 x i16> %532, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %616 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %614, <16 x i16> %599) #8
  %617 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %615, <16 x i16> %599) #8
  %618 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %614, <16 x i16> %565) #8
  %619 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %615, <16 x i16> %565) #8
  %620 = add <8 x i32> %616, %10
  %621 = add <8 x i32> %617, %10
  %622 = add <8 x i32> %618, %10
  %623 = add <8 x i32> %619, %10
  %624 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %620, i32 %4) #8
  %625 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %621, i32 %4) #8
  %626 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %622, i32 %4) #8
  %627 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %623, i32 %4) #8
  %628 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %624, <8 x i32> %625) #8
  %629 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %626, <8 x i32> %627) #8
  %630 = shufflevector <16 x i16> %535, <16 x i16> %537, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %631 = shufflevector <16 x i16> %535, <16 x i16> %537, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %632 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %630, <16 x i16> %418) #8
  %633 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %631, <16 x i16> %418) #8
  %634 = bitcast <8 x i32> %47 to <16 x i16>
  %635 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %630, <16 x i16> %634) #8
  %636 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %631, <16 x i16> %634) #8
  %637 = add <8 x i32> %632, %10
  %638 = add <8 x i32> %633, %10
  %639 = add <8 x i32> %635, %10
  %640 = add <8 x i32> %636, %10
  %641 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %637, i32 %4) #8
  %642 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %638, i32 %4) #8
  %643 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %639, i32 %4) #8
  %644 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %640, i32 %4) #8
  %645 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %641, <8 x i32> %642) #8
  %646 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %643, <8 x i32> %644) #8
  %647 = shufflevector <16 x i16> %538, <16 x i16> %536, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %648 = shufflevector <16 x i16> %538, <16 x i16> %536, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %649 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %647, <16 x i16> %568) #8
  %650 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %648, <16 x i16> %568) #8
  %651 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %647, <16 x i16> %565) #8
  %652 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %648, <16 x i16> %565) #8
  %653 = add <8 x i32> %649, %10
  %654 = add <8 x i32> %650, %10
  %655 = add <8 x i32> %651, %10
  %656 = add <8 x i32> %652, %10
  %657 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %653, i32 %4) #8
  %658 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %654, i32 %4) #8
  %659 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %655, i32 %4) #8
  %660 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %656, i32 %4) #8
  %661 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %657, <8 x i32> %658) #8
  %662 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %659, <8 x i32> %660) #8
  %663 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %486, <16 x i16> %553) #8
  %664 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %486, <16 x i16> %553) #8
  %665 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %480, <16 x i16> %554) #8
  %666 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %480, <16 x i16> %554) #8
  %667 = shufflevector <16 x i16> %557, <16 x i16> %561, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %668 = shufflevector <16 x i16> %557, <16 x i16> %561, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %669 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %667, <16 x i16> %565) #8
  %670 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %668, <16 x i16> %565) #8
  %671 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %667, <16 x i16> %568) #8
  %672 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %668, <16 x i16> %568) #8
  %673 = add <8 x i32> %669, %10
  %674 = add <8 x i32> %670, %10
  %675 = add <8 x i32> %671, %10
  %676 = add <8 x i32> %672, %10
  %677 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %673, i32 %4) #8
  %678 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %674, i32 %4) #8
  %679 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %675, i32 %4) #8
  %680 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %676, i32 %4) #8
  %681 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %677, <8 x i32> %678) #8
  %682 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %679, <8 x i32> %680) #8
  %683 = shufflevector <16 x i16> %558, <16 x i16> %562, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %684 = shufflevector <16 x i16> %558, <16 x i16> %562, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %685 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %683, <16 x i16> %599) #8
  %686 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %684, <16 x i16> %599) #8
  %687 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %683, <16 x i16> %565) #8
  %688 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %684, <16 x i16> %565) #8
  %689 = add <8 x i32> %685, %10
  %690 = add <8 x i32> %686, %10
  %691 = add <8 x i32> %687, %10
  %692 = add <8 x i32> %688, %10
  %693 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %689, i32 %4) #8
  %694 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %690, i32 %4) #8
  %695 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %691, i32 %4) #8
  %696 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %692, i32 %4) #8
  %697 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %693, <8 x i32> %694) #8
  %698 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %695, <8 x i32> %696) #8
  %699 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %519, <16 x i16> %595) #8
  %700 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %519, <16 x i16> %595) #8
  %701 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %521, <16 x i16> %579) #8
  %702 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %521, <16 x i16> %579) #8
  %703 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %520, <16 x i16> %612) #8
  %704 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %520, <16 x i16> %612) #8
  %705 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %522, <16 x i16> %628) #8
  %706 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %522, <16 x i16> %628) #8
  %707 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %528, <16 x i16> %613) #8
  %708 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %528, <16 x i16> %613) #8
  %709 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %530, <16 x i16> %629) #8
  %710 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %530, <16 x i16> %629) #8
  %711 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %527, <16 x i16> %596) #8
  %712 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %527, <16 x i16> %596) #8
  %713 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %529, <16 x i16> %580) #8
  %714 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %529, <16 x i16> %580) #8
  %715 = shufflevector <16 x i16> %663, <16 x i16> %665, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %716 = shufflevector <16 x i16> %663, <16 x i16> %665, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %717 = bitcast <8 x i32> %56 to <16 x i16>
  %718 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %715, <16 x i16> %717) #8
  %719 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %716, <16 x i16> %717) #8
  %720 = bitcast <8 x i32> %62 to <16 x i16>
  %721 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %715, <16 x i16> %720) #8
  %722 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %716, <16 x i16> %720) #8
  %723 = add <8 x i32> %718, %10
  %724 = add <8 x i32> %719, %10
  %725 = add <8 x i32> %721, %10
  %726 = add <8 x i32> %722, %10
  %727 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %723, i32 %4) #8
  %728 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %724, i32 %4) #8
  %729 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %725, i32 %4) #8
  %730 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %726, i32 %4) #8
  %731 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %727, <8 x i32> %728) #8
  %732 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %729, <8 x i32> %730) #8
  %733 = shufflevector <16 x i16> %664, <16 x i16> %666, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %734 = shufflevector <16 x i16> %664, <16 x i16> %666, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %735 = bitcast <8 x i32> %71 to <16 x i16>
  %736 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %733, <16 x i16> %735) #8
  %737 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %734, <16 x i16> %735) #8
  %738 = bitcast <8 x i32> %77 to <16 x i16>
  %739 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %733, <16 x i16> %738) #8
  %740 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %734, <16 x i16> %738) #8
  %741 = add <8 x i32> %736, %10
  %742 = add <8 x i32> %737, %10
  %743 = add <8 x i32> %739, %10
  %744 = add <8 x i32> %740, %10
  %745 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %741, i32 %4) #8
  %746 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %742, i32 %4) #8
  %747 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %743, i32 %4) #8
  %748 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %744, i32 %4) #8
  %749 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %745, <8 x i32> %746) #8
  %750 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %747, <8 x i32> %748) #8
  %751 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %555, <16 x i16> %681) #8
  %752 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %555, <16 x i16> %681) #8
  %753 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %556, <16 x i16> %697) #8
  %754 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %556, <16 x i16> %697) #8
  %755 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %560, <16 x i16> %698) #8
  %756 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %560, <16 x i16> %698) #8
  %757 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %559, <16 x i16> %682) #8
  %758 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %559, <16 x i16> %682) #8
  %759 = shufflevector <16 x i16> %701, <16 x i16> %713, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %760 = shufflevector <16 x i16> %701, <16 x i16> %713, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %761 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %759, <16 x i16> %720) #8
  %762 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %760, <16 x i16> %720) #8
  %763 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %759, <16 x i16> %717) #8
  %764 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %760, <16 x i16> %717) #8
  %765 = add <8 x i32> %761, %10
  %766 = add <8 x i32> %762, %10
  %767 = add <8 x i32> %763, %10
  %768 = add <8 x i32> %764, %10
  %769 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %765, i32 %4) #8
  %770 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %766, i32 %4) #8
  %771 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %767, i32 %4) #8
  %772 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %768, i32 %4) #8
  %773 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %769, <8 x i32> %770) #8
  %774 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %771, <8 x i32> %772) #8
  %775 = shufflevector <16 x i16> %702, <16 x i16> %714, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %776 = shufflevector <16 x i16> %702, <16 x i16> %714, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %777 = bitcast <8 x i32> %83 to <16 x i16>
  %778 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %775, <16 x i16> %777) #8
  %779 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %776, <16 x i16> %777) #8
  %780 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %775, <16 x i16> %720) #8
  %781 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %776, <16 x i16> %720) #8
  %782 = add <8 x i32> %778, %10
  %783 = add <8 x i32> %779, %10
  %784 = add <8 x i32> %780, %10
  %785 = add <8 x i32> %781, %10
  %786 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %782, i32 %4) #8
  %787 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %783, i32 %4) #8
  %788 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %784, i32 %4) #8
  %789 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %785, i32 %4) #8
  %790 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %786, <8 x i32> %787) #8
  %791 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %788, <8 x i32> %789) #8
  %792 = shufflevector <16 x i16> %706, <16 x i16> %710, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %793 = shufflevector <16 x i16> %706, <16 x i16> %710, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %794 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %792, <16 x i16> %738) #8
  %795 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %793, <16 x i16> %738) #8
  %796 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %792, <16 x i16> %735) #8
  %797 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %793, <16 x i16> %735) #8
  %798 = add <8 x i32> %794, %10
  %799 = add <8 x i32> %795, %10
  %800 = add <8 x i32> %796, %10
  %801 = add <8 x i32> %797, %10
  %802 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %798, i32 %4) #8
  %803 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %799, i32 %4) #8
  %804 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %800, i32 %4) #8
  %805 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %801, i32 %4) #8
  %806 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %802, <8 x i32> %803) #8
  %807 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %804, <8 x i32> %805) #8
  %808 = shufflevector <16 x i16> %705, <16 x i16> %709, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %809 = shufflevector <16 x i16> %705, <16 x i16> %709, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %810 = bitcast <8 x i32> %89 to <16 x i16>
  %811 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %808, <16 x i16> %810) #8
  %812 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %809, <16 x i16> %810) #8
  %813 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %808, <16 x i16> %738) #8
  %814 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %809, <16 x i16> %738) #8
  %815 = add <8 x i32> %811, %10
  %816 = add <8 x i32> %812, %10
  %817 = add <8 x i32> %813, %10
  %818 = add <8 x i32> %814, %10
  %819 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %815, i32 %4) #8
  %820 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %816, i32 %4) #8
  %821 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %817, i32 %4) #8
  %822 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %818, i32 %4) #8
  %823 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %819, <8 x i32> %820) #8
  %824 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %821, <8 x i32> %822) #8
  %825 = shufflevector <16 x i16> %751, <16 x i16> %757, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %826 = shufflevector <16 x i16> %751, <16 x i16> %757, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %827 = bitcast <8 x i32> %98 to <16 x i16>
  %828 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %825, <16 x i16> %827) #8
  %829 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %826, <16 x i16> %827) #8
  %830 = bitcast <8 x i32> %104 to <16 x i16>
  %831 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %825, <16 x i16> %830) #8
  %832 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %826, <16 x i16> %830) #8
  %833 = add <8 x i32> %828, %10
  %834 = add <8 x i32> %829, %10
  %835 = add <8 x i32> %831, %10
  %836 = add <8 x i32> %832, %10
  %837 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %833, i32 %4) #8
  %838 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %834, i32 %4) #8
  %839 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %835, i32 %4) #8
  %840 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %836, i32 %4) #8
  %841 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %837, <8 x i32> %838) #8
  %842 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %839, <8 x i32> %840) #8
  %843 = shufflevector <16 x i16> %752, <16 x i16> %758, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %844 = shufflevector <16 x i16> %752, <16 x i16> %758, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %845 = bitcast <8 x i32> %113 to <16 x i16>
  %846 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %843, <16 x i16> %845) #8
  %847 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %844, <16 x i16> %845) #8
  %848 = bitcast <8 x i32> %119 to <16 x i16>
  %849 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %843, <16 x i16> %848) #8
  %850 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %844, <16 x i16> %848) #8
  %851 = add <8 x i32> %846, %10
  %852 = add <8 x i32> %847, %10
  %853 = add <8 x i32> %849, %10
  %854 = add <8 x i32> %850, %10
  %855 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %851, i32 %4) #8
  %856 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %852, i32 %4) #8
  %857 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %853, i32 %4) #8
  %858 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %854, i32 %4) #8
  %859 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %855, <8 x i32> %856) #8
  %860 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %857, <8 x i32> %858) #8
  %861 = shufflevector <16 x i16> %754, <16 x i16> %756, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %862 = shufflevector <16 x i16> %754, <16 x i16> %756, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %863 = bitcast <8 x i32> %128 to <16 x i16>
  %864 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %861, <16 x i16> %863) #8
  %865 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %862, <16 x i16> %863) #8
  %866 = bitcast <8 x i32> %134 to <16 x i16>
  %867 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %861, <16 x i16> %866) #8
  %868 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %862, <16 x i16> %866) #8
  %869 = add <8 x i32> %864, %10
  %870 = add <8 x i32> %865, %10
  %871 = add <8 x i32> %867, %10
  %872 = add <8 x i32> %868, %10
  %873 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %869, i32 %4) #8
  %874 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %870, i32 %4) #8
  %875 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %871, i32 %4) #8
  %876 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %872, i32 %4) #8
  %877 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %873, <8 x i32> %874) #8
  %878 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %875, <8 x i32> %876) #8
  %879 = shufflevector <16 x i16> %753, <16 x i16> %755, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %880 = shufflevector <16 x i16> %753, <16 x i16> %755, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %881 = bitcast <8 x i32> %143 to <16 x i16>
  %882 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %879, <16 x i16> %881) #8
  %883 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %880, <16 x i16> %881) #8
  %884 = bitcast <8 x i32> %149 to <16 x i16>
  %885 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %879, <16 x i16> %884) #8
  %886 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %880, <16 x i16> %884) #8
  %887 = add <8 x i32> %882, %10
  %888 = add <8 x i32> %883, %10
  %889 = add <8 x i32> %885, %10
  %890 = add <8 x i32> %886, %10
  %891 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %887, i32 %4) #8
  %892 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %888, i32 %4) #8
  %893 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %889, i32 %4) #8
  %894 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %890, i32 %4) #8
  %895 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %891, <8 x i32> %892) #8
  %896 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %893, <8 x i32> %894) #8
  %897 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %699, <16 x i16> %773) #8
  %898 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %699, <16 x i16> %773) #8
  %899 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %700, <16 x i16> %790) #8
  %900 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %700, <16 x i16> %790) #8
  %901 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %704, <16 x i16> %806) #8
  %902 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %704, <16 x i16> %806) #8
  %903 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %703, <16 x i16> %823) #8
  %904 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %703, <16 x i16> %823) #8
  %905 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %707, <16 x i16> %824) #8
  %906 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %707, <16 x i16> %824) #8
  %907 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %708, <16 x i16> %807) #8
  %908 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %708, <16 x i16> %807) #8
  %909 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %712, <16 x i16> %791) #8
  %910 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %712, <16 x i16> %791) #8
  %911 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %711, <16 x i16> %774) #8
  %912 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %711, <16 x i16> %774) #8
  %913 = shufflevector <16 x i16> %897, <16 x i16> %911, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %914 = shufflevector <16 x i16> %897, <16 x i16> %911, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %915 = bitcast <8 x i32> %158 to <16 x i16>
  %916 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %913, <16 x i16> %915) #8
  %917 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %914, <16 x i16> %915) #8
  %918 = bitcast <8 x i32> %164 to <16 x i16>
  %919 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %913, <16 x i16> %918) #8
  %920 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %914, <16 x i16> %918) #8
  %921 = add <8 x i32> %916, %10
  %922 = add <8 x i32> %917, %10
  %923 = add <8 x i32> %919, %10
  %924 = add <8 x i32> %920, %10
  %925 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %921, i32 %4) #8
  %926 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %922, i32 %4) #8
  %927 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %923, i32 %4) #8
  %928 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %924, i32 %4) #8
  %929 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %925, <8 x i32> %926) #8
  %930 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %927, <8 x i32> %928) #8
  %931 = shufflevector <16 x i16> %898, <16 x i16> %912, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %932 = shufflevector <16 x i16> %898, <16 x i16> %912, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %933 = bitcast <8 x i32> %173 to <16 x i16>
  %934 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %931, <16 x i16> %933) #8
  %935 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %932, <16 x i16> %933) #8
  %936 = bitcast <8 x i32> %179 to <16 x i16>
  %937 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %931, <16 x i16> %936) #8
  %938 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %932, <16 x i16> %936) #8
  %939 = add <8 x i32> %934, %10
  %940 = add <8 x i32> %935, %10
  %941 = add <8 x i32> %937, %10
  %942 = add <8 x i32> %938, %10
  %943 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %939, i32 %4) #8
  %944 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %940, i32 %4) #8
  %945 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %941, i32 %4) #8
  %946 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %942, i32 %4) #8
  %947 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %943, <8 x i32> %944) #8
  %948 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %945, <8 x i32> %946) #8
  %949 = shufflevector <16 x i16> %900, <16 x i16> %910, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %950 = shufflevector <16 x i16> %900, <16 x i16> %910, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %951 = bitcast <8 x i32> %188 to <16 x i16>
  %952 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %949, <16 x i16> %951) #8
  %953 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %950, <16 x i16> %951) #8
  %954 = bitcast <8 x i32> %194 to <16 x i16>
  %955 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %949, <16 x i16> %954) #8
  %956 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %950, <16 x i16> %954) #8
  %957 = add <8 x i32> %952, %10
  %958 = add <8 x i32> %953, %10
  %959 = add <8 x i32> %955, %10
  %960 = add <8 x i32> %956, %10
  %961 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %957, i32 %4) #8
  %962 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %958, i32 %4) #8
  %963 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %959, i32 %4) #8
  %964 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %960, i32 %4) #8
  %965 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %961, <8 x i32> %962) #8
  %966 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %963, <8 x i32> %964) #8
  %967 = shufflevector <16 x i16> %899, <16 x i16> %909, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %968 = shufflevector <16 x i16> %899, <16 x i16> %909, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %969 = bitcast <8 x i32> %203 to <16 x i16>
  %970 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %967, <16 x i16> %969) #8
  %971 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %968, <16 x i16> %969) #8
  %972 = bitcast <8 x i32> %209 to <16 x i16>
  %973 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %967, <16 x i16> %972) #8
  %974 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %968, <16 x i16> %972) #8
  %975 = add <8 x i32> %970, %10
  %976 = add <8 x i32> %971, %10
  %977 = add <8 x i32> %973, %10
  %978 = add <8 x i32> %974, %10
  %979 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %975, i32 %4) #8
  %980 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %976, i32 %4) #8
  %981 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %977, i32 %4) #8
  %982 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %978, i32 %4) #8
  %983 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %979, <8 x i32> %980) #8
  %984 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %981, <8 x i32> %982) #8
  %985 = shufflevector <16 x i16> %901, <16 x i16> %907, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %986 = shufflevector <16 x i16> %901, <16 x i16> %907, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %987 = bitcast <8 x i32> %218 to <16 x i16>
  %988 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %985, <16 x i16> %987) #8
  %989 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %986, <16 x i16> %987) #8
  %990 = bitcast <8 x i32> %224 to <16 x i16>
  %991 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %985, <16 x i16> %990) #8
  %992 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %986, <16 x i16> %990) #8
  %993 = add <8 x i32> %988, %10
  %994 = add <8 x i32> %989, %10
  %995 = add <8 x i32> %991, %10
  %996 = add <8 x i32> %992, %10
  %997 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %993, i32 %4) #8
  %998 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %994, i32 %4) #8
  %999 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %995, i32 %4) #8
  %1000 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %996, i32 %4) #8
  %1001 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %997, <8 x i32> %998) #8
  %1002 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %999, <8 x i32> %1000) #8
  %1003 = shufflevector <16 x i16> %902, <16 x i16> %908, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1004 = shufflevector <16 x i16> %902, <16 x i16> %908, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1005 = bitcast <8 x i32> %233 to <16 x i16>
  %1006 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1003, <16 x i16> %1005) #8
  %1007 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1004, <16 x i16> %1005) #8
  %1008 = bitcast <8 x i32> %239 to <16 x i16>
  %1009 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1003, <16 x i16> %1008) #8
  %1010 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1004, <16 x i16> %1008) #8
  %1011 = add <8 x i32> %1006, %10
  %1012 = add <8 x i32> %1007, %10
  %1013 = add <8 x i32> %1009, %10
  %1014 = add <8 x i32> %1010, %10
  %1015 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1011, i32 %4) #8
  %1016 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1012, i32 %4) #8
  %1017 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1013, i32 %4) #8
  %1018 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1014, i32 %4) #8
  %1019 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1015, <8 x i32> %1016) #8
  %1020 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1017, <8 x i32> %1018) #8
  %1021 = shufflevector <16 x i16> %904, <16 x i16> %906, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1022 = shufflevector <16 x i16> %904, <16 x i16> %906, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1023 = bitcast <8 x i32> %248 to <16 x i16>
  %1024 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1021, <16 x i16> %1023) #8
  %1025 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1022, <16 x i16> %1023) #8
  %1026 = bitcast <8 x i32> %254 to <16 x i16>
  %1027 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1021, <16 x i16> %1026) #8
  %1028 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1022, <16 x i16> %1026) #8
  %1029 = add <8 x i32> %1024, %10
  %1030 = add <8 x i32> %1025, %10
  %1031 = add <8 x i32> %1027, %10
  %1032 = add <8 x i32> %1028, %10
  %1033 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1029, i32 %4) #8
  %1034 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1030, i32 %4) #8
  %1035 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1031, i32 %4) #8
  %1036 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1032, i32 %4) #8
  %1037 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1033, <8 x i32> %1034) #8
  %1038 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1035, <8 x i32> %1036) #8
  %1039 = shufflevector <16 x i16> %903, <16 x i16> %905, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1040 = shufflevector <16 x i16> %903, <16 x i16> %905, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1041 = bitcast <8 x i32> %263 to <16 x i16>
  %1042 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1039, <16 x i16> %1041) #8
  %1043 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1040, <16 x i16> %1041) #8
  %1044 = bitcast <8 x i32> %269 to <16 x i16>
  %1045 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1039, <16 x i16> %1044) #8
  %1046 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1040, <16 x i16> %1044) #8
  %1047 = add <8 x i32> %1042, %10
  %1048 = add <8 x i32> %1043, %10
  %1049 = add <8 x i32> %1045, %10
  %1050 = add <8 x i32> %1046, %10
  %1051 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1047, i32 %4) #8
  %1052 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1048, i32 %4) #8
  %1053 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1049, i32 %4) #8
  %1054 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1050, i32 %4) #8
  %1055 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1051, <8 x i32> %1052) #8
  %1056 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1053, <8 x i32> %1054) #8
  %1057 = bitcast <4 x i64>* %1 to <16 x i16>*
  store <16 x i16> %645, <16 x i16>* %1057, align 32
  %1058 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %1059 = bitcast <4 x i64>* %1058 to <16 x i16>*
  store <16 x i16> %929, <16 x i16>* %1059, align 32
  %1060 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %1061 = bitcast <4 x i64>* %1060 to <16 x i16>*
  store <16 x i16> %841, <16 x i16>* %1061, align 32
  %1062 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %1063 = bitcast <4 x i64>* %1062 to <16 x i16>*
  store <16 x i16> %1056, <16 x i16>* %1063, align 32
  %1064 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %1065 = bitcast <4 x i64>* %1064 to <16 x i16>*
  store <16 x i16> %731, <16 x i16>* %1065, align 32
  %1066 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %1067 = bitcast <4 x i64>* %1066 to <16 x i16>*
  store <16 x i16> %1001, <16 x i16>* %1067, align 32
  %1068 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %1069 = bitcast <4 x i64>* %1068 to <16 x i16>*
  store <16 x i16> %896, <16 x i16>* %1069, align 32
  %1070 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %1071 = bitcast <4 x i64>* %1070 to <16 x i16>*
  store <16 x i16> %984, <16 x i16>* %1071, align 32
  %1072 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %1073 = bitcast <4 x i64>* %1072 to <16 x i16>*
  store <16 x i16> %661, <16 x i16>* %1073, align 32
  %1074 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %1075 = bitcast <4 x i64>* %1074 to <16 x i16>*
  store <16 x i16> %965, <16 x i16>* %1075, align 32
  %1076 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %1077 = bitcast <4 x i64>* %1076 to <16 x i16>*
  store <16 x i16> %877, <16 x i16>* %1077, align 32
  %1078 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %1079 = bitcast <4 x i64>* %1078 to <16 x i16>*
  store <16 x i16> %1020, <16 x i16>* %1079, align 32
  %1080 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %1081 = bitcast <4 x i64>* %1080 to <16 x i16>*
  store <16 x i16> %750, <16 x i16>* %1081, align 32
  %1082 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %1083 = bitcast <4 x i64>* %1082 to <16 x i16>*
  store <16 x i16> %1037, <16 x i16>* %1083, align 32
  %1084 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %1085 = bitcast <4 x i64>* %1084 to <16 x i16>*
  store <16 x i16> %860, <16 x i16>* %1085, align 32
  %1086 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 15
  %1087 = bitcast <4 x i64>* %1086 to <16 x i16>*
  store <16 x i16> %948, <16 x i16>* %1087, align 32
  %1088 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 16
  %1089 = bitcast <4 x i64>* %1088 to <16 x i16>*
  store <16 x i16> %646, <16 x i16>* %1089, align 32
  %1090 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 17
  %1091 = bitcast <4 x i64>* %1090 to <16 x i16>*
  store <16 x i16> %947, <16 x i16>* %1091, align 32
  %1092 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 18
  %1093 = bitcast <4 x i64>* %1092 to <16 x i16>*
  store <16 x i16> %859, <16 x i16>* %1093, align 32
  %1094 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 19
  %1095 = bitcast <4 x i64>* %1094 to <16 x i16>*
  store <16 x i16> %1038, <16 x i16>* %1095, align 32
  %1096 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 20
  %1097 = bitcast <4 x i64>* %1096 to <16 x i16>*
  store <16 x i16> %749, <16 x i16>* %1097, align 32
  %1098 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 21
  %1099 = bitcast <4 x i64>* %1098 to <16 x i16>*
  store <16 x i16> %1019, <16 x i16>* %1099, align 32
  %1100 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 22
  %1101 = bitcast <4 x i64>* %1100 to <16 x i16>*
  store <16 x i16> %878, <16 x i16>* %1101, align 32
  %1102 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 23
  %1103 = bitcast <4 x i64>* %1102 to <16 x i16>*
  store <16 x i16> %966, <16 x i16>* %1103, align 32
  %1104 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 24
  %1105 = bitcast <4 x i64>* %1104 to <16 x i16>*
  store <16 x i16> %662, <16 x i16>* %1105, align 32
  %1106 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 25
  %1107 = bitcast <4 x i64>* %1106 to <16 x i16>*
  store <16 x i16> %983, <16 x i16>* %1107, align 32
  %1108 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 26
  %1109 = bitcast <4 x i64>* %1108 to <16 x i16>*
  store <16 x i16> %895, <16 x i16>* %1109, align 32
  %1110 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 27
  %1111 = bitcast <4 x i64>* %1110 to <16 x i16>*
  store <16 x i16> %1002, <16 x i16>* %1111, align 32
  %1112 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 28
  %1113 = bitcast <4 x i64>* %1112 to <16 x i16>*
  store <16 x i16> %732, <16 x i16>* %1113, align 32
  %1114 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 29
  %1115 = bitcast <4 x i64>* %1114 to <16 x i16>*
  store <16 x i16> %1055, <16 x i16>* %1115, align 32
  %1116 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 30
  %1117 = bitcast <4 x i64>* %1116 to <16 x i16>*
  store <16 x i16> %842, <16 x i16>* %1117, align 32
  %1118 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 31
  %1119 = bitcast <4 x i64>* %1118 to <16 x i16>*
  store <16 x i16> %930, <16 x i16>* %1119, align 32
  ret void
}

; Function Attrs: inlinehint nofree nounwind ssp uwtable
define internal void @fidentity16x32_avx2(<4 x i64>* nocapture readonly, <4 x i64>* nocapture, i8 signext) #5 {
  br label %5

4:                                                ; preds = %5
  ret void

5:                                                ; preds = %5, %3
  %6 = phi i64 [ 0, %3 ], [ %34, %5 ]
  %7 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 %6
  %8 = bitcast <4 x i64>* %7 to <16 x i16>*
  %9 = load <16 x i16>, <16 x i16>* %8, align 32
  %10 = shl <16 x i16> %9, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %11 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 %6
  %12 = bitcast <4 x i64>* %11 to <16 x i16>*
  store <16 x i16> %10, <16 x i16>* %12, align 32
  %13 = or i64 %6, 1
  %14 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 %13
  %15 = bitcast <4 x i64>* %14 to <16 x i16>*
  %16 = load <16 x i16>, <16 x i16>* %15, align 32
  %17 = shl <16 x i16> %16, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %18 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 %13
  %19 = bitcast <4 x i64>* %18 to <16 x i16>*
  store <16 x i16> %17, <16 x i16>* %19, align 32
  %20 = or i64 %6, 2
  %21 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 %20
  %22 = bitcast <4 x i64>* %21 to <16 x i16>*
  %23 = load <16 x i16>, <16 x i16>* %22, align 32
  %24 = shl <16 x i16> %23, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %25 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 %20
  %26 = bitcast <4 x i64>* %25 to <16 x i16>*
  store <16 x i16> %24, <16 x i16>* %26, align 32
  %27 = or i64 %6, 3
  %28 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 %27
  %29 = bitcast <4 x i64>* %28 to <16 x i16>*
  %30 = load <16 x i16>, <16 x i16>* %29, align 32
  %31 = shl <16 x i16> %30, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %32 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 %27
  %33 = bitcast <4 x i64>* %32 to <16 x i16>*
  store <16 x i16> %31, <16 x i16>* %33, align 32
  %34 = add nuw nsw i64 %6, 4
  %35 = icmp eq i64 %34, 32
  br i1 %35, label %4, label %5
}

; Function Attrs: inlinehint nounwind ssp uwtable
define internal fastcc void @fdct16x64_new_avx2(<4 x i64>* readonly, <4 x i64>*, i8 signext) unnamed_addr #4 {
  %4 = sext i8 %2 to i32
  %5 = add nsw i32 %4, -10
  %6 = sext i32 %5 to i64
  %7 = add nsw i32 %4, -1
  %8 = shl i32 1, %7
  %9 = insertelement <8 x i32> undef, i32 %8, i32 0
  %10 = shufflevector <8 x i32> %9, <8 x i32> undef, <8 x i32> zeroinitializer
  %11 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 32
  %12 = load i32, i32* %11, align 16
  %13 = sub i32 0, %12
  %14 = and i32 %13, 65535
  %15 = and i32 %12, 65535
  %16 = shl nuw i32 %15, 16
  %17 = or i32 %16, %14
  %18 = insertelement <8 x i32> undef, i32 %17, i32 0
  %19 = shufflevector <8 x i32> %18, <8 x i32> undef, <8 x i32> zeroinitializer
  %20 = or i32 %16, %15
  %21 = insertelement <8 x i32> undef, i32 %20, i32 0
  %22 = shufflevector <8 x i32> %21, <8 x i32> undef, <8 x i32> zeroinitializer
  %23 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 16
  %24 = load i32, i32* %23, align 16
  %25 = sub i32 0, %24
  %26 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 48
  %27 = load i32, i32* %26, align 16
  %28 = and i32 %25, 65535
  %29 = and i32 %27, 65535
  %30 = shl nuw i32 %29, 16
  %31 = or i32 %30, %28
  %32 = insertelement <8 x i32> undef, i32 %31, i32 0
  %33 = shufflevector <8 x i32> %32, <8 x i32> undef, <8 x i32> zeroinitializer
  %34 = shl i32 %24, 16
  %35 = or i32 %29, %34
  %36 = insertelement <8 x i32> undef, i32 %35, i32 0
  %37 = shufflevector <8 x i32> %36, <8 x i32> undef, <8 x i32> zeroinitializer
  %38 = sub i32 0, %27
  %39 = and i32 %38, 65535
  %40 = shl nuw i32 %28, 16
  %41 = or i32 %40, %39
  %42 = insertelement <8 x i32> undef, i32 %41, i32 0
  %43 = shufflevector <8 x i32> %42, <8 x i32> undef, <8 x i32> zeroinitializer
  %44 = shl nuw i32 %14, 16
  %45 = or i32 %44, %15
  %46 = insertelement <8 x i32> undef, i32 %45, i32 0
  %47 = shufflevector <8 x i32> %46, <8 x i32> undef, <8 x i32> zeroinitializer
  %48 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 8
  %49 = load i32, i32* %48, align 16
  %50 = sub i32 0, %49
  %51 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 56
  %52 = load i32, i32* %51, align 16
  %53 = and i32 %50, 65535
  %54 = and i32 %52, 65535
  %55 = shl nuw i32 %54, 16
  %56 = or i32 %55, %53
  %57 = insertelement <8 x i32> undef, i32 %56, i32 0
  %58 = shufflevector <8 x i32> %57, <8 x i32> undef, <8 x i32> zeroinitializer
  %59 = shl i32 %49, 16
  %60 = or i32 %54, %59
  %61 = insertelement <8 x i32> undef, i32 %60, i32 0
  %62 = shufflevector <8 x i32> %61, <8 x i32> undef, <8 x i32> zeroinitializer
  %63 = sub i32 0, %52
  %64 = and i32 %63, 65535
  %65 = shl nuw i32 %53, 16
  %66 = or i32 %65, %64
  %67 = insertelement <8 x i32> undef, i32 %66, i32 0
  %68 = shufflevector <8 x i32> %67, <8 x i32> undef, <8 x i32> zeroinitializer
  %69 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 40
  %70 = load i32, i32* %69, align 16
  %71 = sub i32 0, %70
  %72 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 24
  %73 = load i32, i32* %72, align 16
  %74 = and i32 %71, 65535
  %75 = and i32 %73, 65535
  %76 = shl nuw i32 %75, 16
  %77 = or i32 %76, %74
  %78 = insertelement <8 x i32> undef, i32 %77, i32 0
  %79 = shufflevector <8 x i32> %78, <8 x i32> undef, <8 x i32> zeroinitializer
  %80 = shl i32 %70, 16
  %81 = or i32 %75, %80
  %82 = insertelement <8 x i32> undef, i32 %81, i32 0
  %83 = shufflevector <8 x i32> %82, <8 x i32> undef, <8 x i32> zeroinitializer
  %84 = sub i32 0, %73
  %85 = and i32 %84, 65535
  %86 = shl nuw i32 %74, 16
  %87 = or i32 %86, %85
  %88 = insertelement <8 x i32> undef, i32 %87, i32 0
  %89 = shufflevector <8 x i32> %88, <8 x i32> undef, <8 x i32> zeroinitializer
  %90 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 60
  %91 = load i32, i32* %90, align 16
  %92 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 4
  %93 = load i32, i32* %92, align 16
  %94 = and i32 %91, 65535
  %95 = shl i32 %93, 16
  %96 = or i32 %95, %94
  %97 = insertelement <8 x i32> undef, i32 %96, i32 0
  %98 = shufflevector <8 x i32> %97, <8 x i32> undef, <8 x i32> zeroinitializer
  %99 = sub i32 0, %93
  %100 = and i32 %99, 65535
  %101 = shl nuw i32 %94, 16
  %102 = or i32 %101, %100
  %103 = insertelement <8 x i32> undef, i32 %102, i32 0
  %104 = shufflevector <8 x i32> %103, <8 x i32> undef, <8 x i32> zeroinitializer
  %105 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 28
  %106 = load i32, i32* %105, align 16
  %107 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 36
  %108 = load i32, i32* %107, align 16
  %109 = and i32 %106, 65535
  %110 = shl i32 %108, 16
  %111 = or i32 %110, %109
  %112 = insertelement <8 x i32> undef, i32 %111, i32 0
  %113 = shufflevector <8 x i32> %112, <8 x i32> undef, <8 x i32> zeroinitializer
  %114 = sub i32 0, %108
  %115 = and i32 %114, 65535
  %116 = shl nuw i32 %109, 16
  %117 = or i32 %116, %115
  %118 = insertelement <8 x i32> undef, i32 %117, i32 0
  %119 = shufflevector <8 x i32> %118, <8 x i32> undef, <8 x i32> zeroinitializer
  %120 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 44
  %121 = load i32, i32* %120, align 16
  %122 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 20
  %123 = load i32, i32* %122, align 16
  %124 = and i32 %121, 65535
  %125 = shl i32 %123, 16
  %126 = or i32 %125, %124
  %127 = insertelement <8 x i32> undef, i32 %126, i32 0
  %128 = shufflevector <8 x i32> %127, <8 x i32> undef, <8 x i32> zeroinitializer
  %129 = sub i32 0, %123
  %130 = and i32 %129, 65535
  %131 = shl nuw i32 %124, 16
  %132 = or i32 %131, %130
  %133 = insertelement <8 x i32> undef, i32 %132, i32 0
  %134 = shufflevector <8 x i32> %133, <8 x i32> undef, <8 x i32> zeroinitializer
  %135 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 12
  %136 = load i32, i32* %135, align 16
  %137 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 52
  %138 = load i32, i32* %137, align 16
  %139 = and i32 %136, 65535
  %140 = shl i32 %138, 16
  %141 = or i32 %140, %139
  %142 = insertelement <8 x i32> undef, i32 %141, i32 0
  %143 = shufflevector <8 x i32> %142, <8 x i32> undef, <8 x i32> zeroinitializer
  %144 = sub i32 0, %138
  %145 = and i32 %144, 65535
  %146 = shl nuw i32 %139, 16
  %147 = or i32 %146, %145
  %148 = insertelement <8 x i32> undef, i32 %147, i32 0
  %149 = shufflevector <8 x i32> %148, <8 x i32> undef, <8 x i32> zeroinitializer
  %150 = sub i32 0, %91
  %151 = and i32 %150, 65535
  %152 = shl nuw i32 %100, 16
  %153 = or i32 %152, %151
  %154 = insertelement <8 x i32> undef, i32 %153, i32 0
  %155 = shufflevector <8 x i32> %154, <8 x i32> undef, <8 x i32> zeroinitializer
  %156 = sub i32 0, %106
  %157 = and i32 %156, 65535
  %158 = shl nuw i32 %115, 16
  %159 = or i32 %158, %157
  %160 = insertelement <8 x i32> undef, i32 %159, i32 0
  %161 = shufflevector <8 x i32> %160, <8 x i32> undef, <8 x i32> zeroinitializer
  %162 = sub i32 0, %121
  %163 = and i32 %162, 65535
  %164 = shl nuw i32 %130, 16
  %165 = or i32 %164, %163
  %166 = insertelement <8 x i32> undef, i32 %165, i32 0
  %167 = shufflevector <8 x i32> %166, <8 x i32> undef, <8 x i32> zeroinitializer
  %168 = sub i32 0, %136
  %169 = and i32 %168, 65535
  %170 = shl nuw i32 %145, 16
  %171 = or i32 %170, %169
  %172 = insertelement <8 x i32> undef, i32 %171, i32 0
  %173 = shufflevector <8 x i32> %172, <8 x i32> undef, <8 x i32> zeroinitializer
  %174 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 62
  %175 = load i32, i32* %174, align 8
  %176 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 2
  %177 = load i32, i32* %176, align 8
  %178 = and i32 %175, 65535
  %179 = shl i32 %177, 16
  %180 = or i32 %179, %178
  %181 = insertelement <8 x i32> undef, i32 %180, i32 0
  %182 = shufflevector <8 x i32> %181, <8 x i32> undef, <8 x i32> zeroinitializer
  %183 = sub i32 0, %177
  %184 = and i32 %183, 65535
  %185 = shl nuw i32 %178, 16
  %186 = or i32 %185, %184
  %187 = insertelement <8 x i32> undef, i32 %186, i32 0
  %188 = shufflevector <8 x i32> %187, <8 x i32> undef, <8 x i32> zeroinitializer
  %189 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 30
  %190 = load i32, i32* %189, align 8
  %191 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 34
  %192 = load i32, i32* %191, align 8
  %193 = and i32 %190, 65535
  %194 = shl i32 %192, 16
  %195 = or i32 %194, %193
  %196 = insertelement <8 x i32> undef, i32 %195, i32 0
  %197 = shufflevector <8 x i32> %196, <8 x i32> undef, <8 x i32> zeroinitializer
  %198 = sub i32 0, %192
  %199 = and i32 %198, 65535
  %200 = shl nuw i32 %193, 16
  %201 = or i32 %200, %199
  %202 = insertelement <8 x i32> undef, i32 %201, i32 0
  %203 = shufflevector <8 x i32> %202, <8 x i32> undef, <8 x i32> zeroinitializer
  %204 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 46
  %205 = load i32, i32* %204, align 8
  %206 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 18
  %207 = load i32, i32* %206, align 8
  %208 = and i32 %205, 65535
  %209 = shl i32 %207, 16
  %210 = or i32 %209, %208
  %211 = insertelement <8 x i32> undef, i32 %210, i32 0
  %212 = shufflevector <8 x i32> %211, <8 x i32> undef, <8 x i32> zeroinitializer
  %213 = sub i32 0, %207
  %214 = and i32 %213, 65535
  %215 = shl nuw i32 %208, 16
  %216 = or i32 %215, %214
  %217 = insertelement <8 x i32> undef, i32 %216, i32 0
  %218 = shufflevector <8 x i32> %217, <8 x i32> undef, <8 x i32> zeroinitializer
  %219 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 14
  %220 = load i32, i32* %219, align 8
  %221 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 50
  %222 = load i32, i32* %221, align 8
  %223 = and i32 %220, 65535
  %224 = shl i32 %222, 16
  %225 = or i32 %224, %223
  %226 = insertelement <8 x i32> undef, i32 %225, i32 0
  %227 = shufflevector <8 x i32> %226, <8 x i32> undef, <8 x i32> zeroinitializer
  %228 = sub i32 0, %222
  %229 = and i32 %228, 65535
  %230 = shl nuw i32 %223, 16
  %231 = or i32 %230, %229
  %232 = insertelement <8 x i32> undef, i32 %231, i32 0
  %233 = shufflevector <8 x i32> %232, <8 x i32> undef, <8 x i32> zeroinitializer
  %234 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 54
  %235 = load i32, i32* %234, align 8
  %236 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 10
  %237 = load i32, i32* %236, align 8
  %238 = and i32 %235, 65535
  %239 = shl i32 %237, 16
  %240 = or i32 %239, %238
  %241 = insertelement <8 x i32> undef, i32 %240, i32 0
  %242 = shufflevector <8 x i32> %241, <8 x i32> undef, <8 x i32> zeroinitializer
  %243 = sub i32 0, %237
  %244 = and i32 %243, 65535
  %245 = shl nuw i32 %238, 16
  %246 = or i32 %245, %244
  %247 = insertelement <8 x i32> undef, i32 %246, i32 0
  %248 = shufflevector <8 x i32> %247, <8 x i32> undef, <8 x i32> zeroinitializer
  %249 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 22
  %250 = load i32, i32* %249, align 8
  %251 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 42
  %252 = load i32, i32* %251, align 8
  %253 = and i32 %250, 65535
  %254 = shl i32 %252, 16
  %255 = or i32 %254, %253
  %256 = insertelement <8 x i32> undef, i32 %255, i32 0
  %257 = shufflevector <8 x i32> %256, <8 x i32> undef, <8 x i32> zeroinitializer
  %258 = sub i32 0, %252
  %259 = and i32 %258, 65535
  %260 = shl nuw i32 %253, 16
  %261 = or i32 %260, %259
  %262 = insertelement <8 x i32> undef, i32 %261, i32 0
  %263 = shufflevector <8 x i32> %262, <8 x i32> undef, <8 x i32> zeroinitializer
  %264 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 38
  %265 = load i32, i32* %264, align 8
  %266 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 26
  %267 = load i32, i32* %266, align 8
  %268 = and i32 %265, 65535
  %269 = shl i32 %267, 16
  %270 = or i32 %269, %268
  %271 = insertelement <8 x i32> undef, i32 %270, i32 0
  %272 = shufflevector <8 x i32> %271, <8 x i32> undef, <8 x i32> zeroinitializer
  %273 = sub i32 0, %267
  %274 = and i32 %273, 65535
  %275 = shl nuw i32 %268, 16
  %276 = or i32 %275, %274
  %277 = insertelement <8 x i32> undef, i32 %276, i32 0
  %278 = shufflevector <8 x i32> %277, <8 x i32> undef, <8 x i32> zeroinitializer
  %279 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 6
  %280 = load i32, i32* %279, align 8
  %281 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 58
  %282 = load i32, i32* %281, align 8
  %283 = and i32 %280, 65535
  %284 = shl i32 %282, 16
  %285 = or i32 %284, %283
  %286 = insertelement <8 x i32> undef, i32 %285, i32 0
  %287 = shufflevector <8 x i32> %286, <8 x i32> undef, <8 x i32> zeroinitializer
  %288 = sub i32 0, %282
  %289 = and i32 %288, 65535
  %290 = shl nuw i32 %283, 16
  %291 = or i32 %290, %289
  %292 = insertelement <8 x i32> undef, i32 %291, i32 0
  %293 = shufflevector <8 x i32> %292, <8 x i32> undef, <8 x i32> zeroinitializer
  %294 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 63
  %295 = load i32, i32* %294, align 4
  %296 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 1
  %297 = load i32, i32* %296, align 4
  %298 = and i32 %295, 65535
  %299 = shl i32 %297, 16
  %300 = or i32 %299, %298
  %301 = insertelement <8 x i32> undef, i32 %300, i32 0
  %302 = shufflevector <8 x i32> %301, <8 x i32> undef, <8 x i32> zeroinitializer
  %303 = sub i32 0, %297
  %304 = and i32 %303, 65535
  %305 = shl nuw i32 %298, 16
  %306 = or i32 %305, %304
  %307 = insertelement <8 x i32> undef, i32 %306, i32 0
  %308 = shufflevector <8 x i32> %307, <8 x i32> undef, <8 x i32> zeroinitializer
  %309 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 31
  %310 = load i32, i32* %309, align 4
  %311 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 33
  %312 = load i32, i32* %311, align 4
  %313 = and i32 %310, 65535
  %314 = shl i32 %312, 16
  %315 = or i32 %314, %313
  %316 = insertelement <8 x i32> undef, i32 %315, i32 0
  %317 = shufflevector <8 x i32> %316, <8 x i32> undef, <8 x i32> zeroinitializer
  %318 = sub i32 0, %312
  %319 = and i32 %318, 65535
  %320 = shl nuw i32 %313, 16
  %321 = or i32 %320, %319
  %322 = insertelement <8 x i32> undef, i32 %321, i32 0
  %323 = shufflevector <8 x i32> %322, <8 x i32> undef, <8 x i32> zeroinitializer
  %324 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 47
  %325 = load i32, i32* %324, align 4
  %326 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 17
  %327 = load i32, i32* %326, align 4
  %328 = and i32 %325, 65535
  %329 = shl i32 %327, 16
  %330 = or i32 %329, %328
  %331 = insertelement <8 x i32> undef, i32 %330, i32 0
  %332 = shufflevector <8 x i32> %331, <8 x i32> undef, <8 x i32> zeroinitializer
  %333 = sub i32 0, %327
  %334 = and i32 %333, 65535
  %335 = shl nuw i32 %328, 16
  %336 = or i32 %335, %334
  %337 = insertelement <8 x i32> undef, i32 %336, i32 0
  %338 = shufflevector <8 x i32> %337, <8 x i32> undef, <8 x i32> zeroinitializer
  %339 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 15
  %340 = load i32, i32* %339, align 4
  %341 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 49
  %342 = load i32, i32* %341, align 4
  %343 = and i32 %340, 65535
  %344 = shl i32 %342, 16
  %345 = or i32 %344, %343
  %346 = insertelement <8 x i32> undef, i32 %345, i32 0
  %347 = shufflevector <8 x i32> %346, <8 x i32> undef, <8 x i32> zeroinitializer
  %348 = sub i32 0, %342
  %349 = and i32 %348, 65535
  %350 = shl nuw i32 %343, 16
  %351 = or i32 %350, %349
  %352 = insertelement <8 x i32> undef, i32 %351, i32 0
  %353 = shufflevector <8 x i32> %352, <8 x i32> undef, <8 x i32> zeroinitializer
  %354 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 55
  %355 = load i32, i32* %354, align 4
  %356 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 9
  %357 = load i32, i32* %356, align 4
  %358 = and i32 %355, 65535
  %359 = shl i32 %357, 16
  %360 = or i32 %359, %358
  %361 = insertelement <8 x i32> undef, i32 %360, i32 0
  %362 = shufflevector <8 x i32> %361, <8 x i32> undef, <8 x i32> zeroinitializer
  %363 = sub i32 0, %357
  %364 = and i32 %363, 65535
  %365 = shl nuw i32 %358, 16
  %366 = or i32 %365, %364
  %367 = insertelement <8 x i32> undef, i32 %366, i32 0
  %368 = shufflevector <8 x i32> %367, <8 x i32> undef, <8 x i32> zeroinitializer
  %369 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 23
  %370 = load i32, i32* %369, align 4
  %371 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 41
  %372 = load i32, i32* %371, align 4
  %373 = and i32 %370, 65535
  %374 = shl i32 %372, 16
  %375 = or i32 %374, %373
  %376 = insertelement <8 x i32> undef, i32 %375, i32 0
  %377 = shufflevector <8 x i32> %376, <8 x i32> undef, <8 x i32> zeroinitializer
  %378 = sub i32 0, %372
  %379 = and i32 %378, 65535
  %380 = shl nuw i32 %373, 16
  %381 = or i32 %380, %379
  %382 = insertelement <8 x i32> undef, i32 %381, i32 0
  %383 = shufflevector <8 x i32> %382, <8 x i32> undef, <8 x i32> zeroinitializer
  %384 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 39
  %385 = load i32, i32* %384, align 4
  %386 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 25
  %387 = load i32, i32* %386, align 4
  %388 = and i32 %385, 65535
  %389 = shl i32 %387, 16
  %390 = or i32 %389, %388
  %391 = insertelement <8 x i32> undef, i32 %390, i32 0
  %392 = shufflevector <8 x i32> %391, <8 x i32> undef, <8 x i32> zeroinitializer
  %393 = sub i32 0, %387
  %394 = and i32 %393, 65535
  %395 = shl nuw i32 %388, 16
  %396 = or i32 %395, %394
  %397 = insertelement <8 x i32> undef, i32 %396, i32 0
  %398 = shufflevector <8 x i32> %397, <8 x i32> undef, <8 x i32> zeroinitializer
  %399 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 7
  %400 = load i32, i32* %399, align 4
  %401 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 57
  %402 = load i32, i32* %401, align 4
  %403 = and i32 %400, 65535
  %404 = shl i32 %402, 16
  %405 = or i32 %404, %403
  %406 = insertelement <8 x i32> undef, i32 %405, i32 0
  %407 = shufflevector <8 x i32> %406, <8 x i32> undef, <8 x i32> zeroinitializer
  %408 = sub i32 0, %402
  %409 = and i32 %408, 65535
  %410 = shl nuw i32 %403, 16
  %411 = or i32 %410, %409
  %412 = insertelement <8 x i32> undef, i32 %411, i32 0
  %413 = shufflevector <8 x i32> %412, <8 x i32> undef, <8 x i32> zeroinitializer
  %414 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 59
  %415 = load i32, i32* %414, align 4
  %416 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 5
  %417 = load i32, i32* %416, align 4
  %418 = and i32 %415, 65535
  %419 = shl i32 %417, 16
  %420 = or i32 %419, %418
  %421 = insertelement <8 x i32> undef, i32 %420, i32 0
  %422 = shufflevector <8 x i32> %421, <8 x i32> undef, <8 x i32> zeroinitializer
  %423 = sub i32 0, %417
  %424 = and i32 %423, 65535
  %425 = shl nuw i32 %418, 16
  %426 = or i32 %425, %424
  %427 = insertelement <8 x i32> undef, i32 %426, i32 0
  %428 = shufflevector <8 x i32> %427, <8 x i32> undef, <8 x i32> zeroinitializer
  %429 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 27
  %430 = load i32, i32* %429, align 4
  %431 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 37
  %432 = load i32, i32* %431, align 4
  %433 = and i32 %430, 65535
  %434 = shl i32 %432, 16
  %435 = or i32 %434, %433
  %436 = insertelement <8 x i32> undef, i32 %435, i32 0
  %437 = shufflevector <8 x i32> %436, <8 x i32> undef, <8 x i32> zeroinitializer
  %438 = sub i32 0, %432
  %439 = and i32 %438, 65535
  %440 = shl nuw i32 %433, 16
  %441 = or i32 %440, %439
  %442 = insertelement <8 x i32> undef, i32 %441, i32 0
  %443 = shufflevector <8 x i32> %442, <8 x i32> undef, <8 x i32> zeroinitializer
  %444 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 43
  %445 = load i32, i32* %444, align 4
  %446 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 21
  %447 = load i32, i32* %446, align 4
  %448 = and i32 %445, 65535
  %449 = shl i32 %447, 16
  %450 = or i32 %449, %448
  %451 = insertelement <8 x i32> undef, i32 %450, i32 0
  %452 = shufflevector <8 x i32> %451, <8 x i32> undef, <8 x i32> zeroinitializer
  %453 = sub i32 0, %447
  %454 = and i32 %453, 65535
  %455 = shl nuw i32 %448, 16
  %456 = or i32 %455, %454
  %457 = insertelement <8 x i32> undef, i32 %456, i32 0
  %458 = shufflevector <8 x i32> %457, <8 x i32> undef, <8 x i32> zeroinitializer
  %459 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 11
  %460 = load i32, i32* %459, align 4
  %461 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 53
  %462 = load i32, i32* %461, align 4
  %463 = and i32 %460, 65535
  %464 = shl i32 %462, 16
  %465 = or i32 %464, %463
  %466 = insertelement <8 x i32> undef, i32 %465, i32 0
  %467 = shufflevector <8 x i32> %466, <8 x i32> undef, <8 x i32> zeroinitializer
  %468 = sub i32 0, %462
  %469 = and i32 %468, 65535
  %470 = shl nuw i32 %463, 16
  %471 = or i32 %470, %469
  %472 = insertelement <8 x i32> undef, i32 %471, i32 0
  %473 = shufflevector <8 x i32> %472, <8 x i32> undef, <8 x i32> zeroinitializer
  %474 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 51
  %475 = load i32, i32* %474, align 4
  %476 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 13
  %477 = load i32, i32* %476, align 4
  %478 = and i32 %475, 65535
  %479 = shl i32 %477, 16
  %480 = or i32 %479, %478
  %481 = insertelement <8 x i32> undef, i32 %480, i32 0
  %482 = shufflevector <8 x i32> %481, <8 x i32> undef, <8 x i32> zeroinitializer
  %483 = sub i32 0, %477
  %484 = and i32 %483, 65535
  %485 = shl nuw i32 %478, 16
  %486 = or i32 %485, %484
  %487 = insertelement <8 x i32> undef, i32 %486, i32 0
  %488 = shufflevector <8 x i32> %487, <8 x i32> undef, <8 x i32> zeroinitializer
  %489 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 19
  %490 = load i32, i32* %489, align 4
  %491 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 45
  %492 = load i32, i32* %491, align 4
  %493 = and i32 %490, 65535
  %494 = shl i32 %492, 16
  %495 = or i32 %494, %493
  %496 = insertelement <8 x i32> undef, i32 %495, i32 0
  %497 = shufflevector <8 x i32> %496, <8 x i32> undef, <8 x i32> zeroinitializer
  %498 = sub i32 0, %492
  %499 = and i32 %498, 65535
  %500 = shl nuw i32 %493, 16
  %501 = or i32 %500, %499
  %502 = insertelement <8 x i32> undef, i32 %501, i32 0
  %503 = shufflevector <8 x i32> %502, <8 x i32> undef, <8 x i32> zeroinitializer
  %504 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 35
  %505 = load i32, i32* %504, align 4
  %506 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 29
  %507 = load i32, i32* %506, align 4
  %508 = and i32 %505, 65535
  %509 = shl i32 %507, 16
  %510 = or i32 %509, %508
  %511 = insertelement <8 x i32> undef, i32 %510, i32 0
  %512 = shufflevector <8 x i32> %511, <8 x i32> undef, <8 x i32> zeroinitializer
  %513 = sub i32 0, %507
  %514 = and i32 %513, 65535
  %515 = shl nuw i32 %508, 16
  %516 = or i32 %515, %514
  %517 = insertelement <8 x i32> undef, i32 %516, i32 0
  %518 = shufflevector <8 x i32> %517, <8 x i32> undef, <8 x i32> zeroinitializer
  %519 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 3
  %520 = load i32, i32* %519, align 4
  %521 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 61
  %522 = load i32, i32* %521, align 4
  %523 = and i32 %520, 65535
  %524 = shl i32 %522, 16
  %525 = or i32 %524, %523
  %526 = insertelement <8 x i32> undef, i32 %525, i32 0
  %527 = shufflevector <8 x i32> %526, <8 x i32> undef, <8 x i32> zeroinitializer
  %528 = sub i32 0, %522
  %529 = and i32 %528, 65535
  %530 = shl nuw i32 %523, 16
  %531 = or i32 %530, %529
  %532 = insertelement <8 x i32> undef, i32 %531, i32 0
  %533 = shufflevector <8 x i32> %532, <8 x i32> undef, <8 x i32> zeroinitializer
  %534 = bitcast <4 x i64>* %0 to <16 x i16>*
  %535 = load <16 x i16>, <16 x i16>* %534, align 32
  %536 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 63
  %537 = bitcast <4 x i64>* %536 to <16 x i16>*
  %538 = load <16 x i16>, <16 x i16>* %537, align 32
  %539 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %535, <16 x i16> %538) #8
  %540 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %535, <16 x i16> %538) #8
  %541 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 1
  %542 = bitcast <4 x i64>* %541 to <16 x i16>*
  %543 = load <16 x i16>, <16 x i16>* %542, align 32
  %544 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 62
  %545 = bitcast <4 x i64>* %544 to <16 x i16>*
  %546 = load <16 x i16>, <16 x i16>* %545, align 32
  %547 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %543, <16 x i16> %546) #8
  %548 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %543, <16 x i16> %546) #8
  %549 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 2
  %550 = bitcast <4 x i64>* %549 to <16 x i16>*
  %551 = load <16 x i16>, <16 x i16>* %550, align 32
  %552 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 61
  %553 = bitcast <4 x i64>* %552 to <16 x i16>*
  %554 = load <16 x i16>, <16 x i16>* %553, align 32
  %555 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %551, <16 x i16> %554) #8
  %556 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %551, <16 x i16> %554) #8
  %557 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 3
  %558 = bitcast <4 x i64>* %557 to <16 x i16>*
  %559 = load <16 x i16>, <16 x i16>* %558, align 32
  %560 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 60
  %561 = bitcast <4 x i64>* %560 to <16 x i16>*
  %562 = load <16 x i16>, <16 x i16>* %561, align 32
  %563 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %559, <16 x i16> %562) #8
  %564 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %559, <16 x i16> %562) #8
  %565 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 4
  %566 = bitcast <4 x i64>* %565 to <16 x i16>*
  %567 = load <16 x i16>, <16 x i16>* %566, align 32
  %568 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 59
  %569 = bitcast <4 x i64>* %568 to <16 x i16>*
  %570 = load <16 x i16>, <16 x i16>* %569, align 32
  %571 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %567, <16 x i16> %570) #8
  %572 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %567, <16 x i16> %570) #8
  %573 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 5
  %574 = bitcast <4 x i64>* %573 to <16 x i16>*
  %575 = load <16 x i16>, <16 x i16>* %574, align 32
  %576 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 58
  %577 = bitcast <4 x i64>* %576 to <16 x i16>*
  %578 = load <16 x i16>, <16 x i16>* %577, align 32
  %579 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %575, <16 x i16> %578) #8
  %580 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %575, <16 x i16> %578) #8
  %581 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 6
  %582 = bitcast <4 x i64>* %581 to <16 x i16>*
  %583 = load <16 x i16>, <16 x i16>* %582, align 32
  %584 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 57
  %585 = bitcast <4 x i64>* %584 to <16 x i16>*
  %586 = load <16 x i16>, <16 x i16>* %585, align 32
  %587 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %583, <16 x i16> %586) #8
  %588 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %583, <16 x i16> %586) #8
  %589 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 7
  %590 = bitcast <4 x i64>* %589 to <16 x i16>*
  %591 = load <16 x i16>, <16 x i16>* %590, align 32
  %592 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 56
  %593 = bitcast <4 x i64>* %592 to <16 x i16>*
  %594 = load <16 x i16>, <16 x i16>* %593, align 32
  %595 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %591, <16 x i16> %594) #8
  %596 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %591, <16 x i16> %594) #8
  %597 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 8
  %598 = bitcast <4 x i64>* %597 to <16 x i16>*
  %599 = load <16 x i16>, <16 x i16>* %598, align 32
  %600 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 55
  %601 = bitcast <4 x i64>* %600 to <16 x i16>*
  %602 = load <16 x i16>, <16 x i16>* %601, align 32
  %603 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %599, <16 x i16> %602) #8
  %604 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %599, <16 x i16> %602) #8
  %605 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 9
  %606 = bitcast <4 x i64>* %605 to <16 x i16>*
  %607 = load <16 x i16>, <16 x i16>* %606, align 32
  %608 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 54
  %609 = bitcast <4 x i64>* %608 to <16 x i16>*
  %610 = load <16 x i16>, <16 x i16>* %609, align 32
  %611 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %607, <16 x i16> %610) #8
  %612 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %607, <16 x i16> %610) #8
  %613 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 10
  %614 = bitcast <4 x i64>* %613 to <16 x i16>*
  %615 = load <16 x i16>, <16 x i16>* %614, align 32
  %616 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 53
  %617 = bitcast <4 x i64>* %616 to <16 x i16>*
  %618 = load <16 x i16>, <16 x i16>* %617, align 32
  %619 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %615, <16 x i16> %618) #8
  %620 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %615, <16 x i16> %618) #8
  %621 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 11
  %622 = bitcast <4 x i64>* %621 to <16 x i16>*
  %623 = load <16 x i16>, <16 x i16>* %622, align 32
  %624 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 52
  %625 = bitcast <4 x i64>* %624 to <16 x i16>*
  %626 = load <16 x i16>, <16 x i16>* %625, align 32
  %627 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %623, <16 x i16> %626) #8
  %628 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %623, <16 x i16> %626) #8
  %629 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 12
  %630 = bitcast <4 x i64>* %629 to <16 x i16>*
  %631 = load <16 x i16>, <16 x i16>* %630, align 32
  %632 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 51
  %633 = bitcast <4 x i64>* %632 to <16 x i16>*
  %634 = load <16 x i16>, <16 x i16>* %633, align 32
  %635 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %631, <16 x i16> %634) #8
  %636 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %631, <16 x i16> %634) #8
  %637 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 13
  %638 = bitcast <4 x i64>* %637 to <16 x i16>*
  %639 = load <16 x i16>, <16 x i16>* %638, align 32
  %640 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 50
  %641 = bitcast <4 x i64>* %640 to <16 x i16>*
  %642 = load <16 x i16>, <16 x i16>* %641, align 32
  %643 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %639, <16 x i16> %642) #8
  %644 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %639, <16 x i16> %642) #8
  %645 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 14
  %646 = bitcast <4 x i64>* %645 to <16 x i16>*
  %647 = load <16 x i16>, <16 x i16>* %646, align 32
  %648 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 49
  %649 = bitcast <4 x i64>* %648 to <16 x i16>*
  %650 = load <16 x i16>, <16 x i16>* %649, align 32
  %651 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %647, <16 x i16> %650) #8
  %652 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %647, <16 x i16> %650) #8
  %653 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 15
  %654 = bitcast <4 x i64>* %653 to <16 x i16>*
  %655 = load <16 x i16>, <16 x i16>* %654, align 32
  %656 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 48
  %657 = bitcast <4 x i64>* %656 to <16 x i16>*
  %658 = load <16 x i16>, <16 x i16>* %657, align 32
  %659 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %655, <16 x i16> %658) #8
  %660 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %655, <16 x i16> %658) #8
  %661 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 16
  %662 = bitcast <4 x i64>* %661 to <16 x i16>*
  %663 = load <16 x i16>, <16 x i16>* %662, align 32
  %664 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 47
  %665 = bitcast <4 x i64>* %664 to <16 x i16>*
  %666 = load <16 x i16>, <16 x i16>* %665, align 32
  %667 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %663, <16 x i16> %666) #8
  %668 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %663, <16 x i16> %666) #8
  %669 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 17
  %670 = bitcast <4 x i64>* %669 to <16 x i16>*
  %671 = load <16 x i16>, <16 x i16>* %670, align 32
  %672 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 46
  %673 = bitcast <4 x i64>* %672 to <16 x i16>*
  %674 = load <16 x i16>, <16 x i16>* %673, align 32
  %675 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %671, <16 x i16> %674) #8
  %676 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %671, <16 x i16> %674) #8
  %677 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 18
  %678 = bitcast <4 x i64>* %677 to <16 x i16>*
  %679 = load <16 x i16>, <16 x i16>* %678, align 32
  %680 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 45
  %681 = bitcast <4 x i64>* %680 to <16 x i16>*
  %682 = load <16 x i16>, <16 x i16>* %681, align 32
  %683 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %679, <16 x i16> %682) #8
  %684 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %679, <16 x i16> %682) #8
  %685 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 19
  %686 = bitcast <4 x i64>* %685 to <16 x i16>*
  %687 = load <16 x i16>, <16 x i16>* %686, align 32
  %688 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 44
  %689 = bitcast <4 x i64>* %688 to <16 x i16>*
  %690 = load <16 x i16>, <16 x i16>* %689, align 32
  %691 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %687, <16 x i16> %690) #8
  %692 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %687, <16 x i16> %690) #8
  %693 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 20
  %694 = bitcast <4 x i64>* %693 to <16 x i16>*
  %695 = load <16 x i16>, <16 x i16>* %694, align 32
  %696 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 43
  %697 = bitcast <4 x i64>* %696 to <16 x i16>*
  %698 = load <16 x i16>, <16 x i16>* %697, align 32
  %699 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %695, <16 x i16> %698) #8
  %700 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %695, <16 x i16> %698) #8
  %701 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 21
  %702 = bitcast <4 x i64>* %701 to <16 x i16>*
  %703 = load <16 x i16>, <16 x i16>* %702, align 32
  %704 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 42
  %705 = bitcast <4 x i64>* %704 to <16 x i16>*
  %706 = load <16 x i16>, <16 x i16>* %705, align 32
  %707 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %703, <16 x i16> %706) #8
  %708 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %703, <16 x i16> %706) #8
  %709 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 22
  %710 = bitcast <4 x i64>* %709 to <16 x i16>*
  %711 = load <16 x i16>, <16 x i16>* %710, align 32
  %712 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 41
  %713 = bitcast <4 x i64>* %712 to <16 x i16>*
  %714 = load <16 x i16>, <16 x i16>* %713, align 32
  %715 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %711, <16 x i16> %714) #8
  %716 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %711, <16 x i16> %714) #8
  %717 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 23
  %718 = bitcast <4 x i64>* %717 to <16 x i16>*
  %719 = load <16 x i16>, <16 x i16>* %718, align 32
  %720 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 40
  %721 = bitcast <4 x i64>* %720 to <16 x i16>*
  %722 = load <16 x i16>, <16 x i16>* %721, align 32
  %723 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %719, <16 x i16> %722) #8
  %724 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %719, <16 x i16> %722) #8
  %725 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 24
  %726 = bitcast <4 x i64>* %725 to <16 x i16>*
  %727 = load <16 x i16>, <16 x i16>* %726, align 32
  %728 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 39
  %729 = bitcast <4 x i64>* %728 to <16 x i16>*
  %730 = load <16 x i16>, <16 x i16>* %729, align 32
  %731 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %727, <16 x i16> %730) #8
  %732 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %727, <16 x i16> %730) #8
  %733 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 25
  %734 = bitcast <4 x i64>* %733 to <16 x i16>*
  %735 = load <16 x i16>, <16 x i16>* %734, align 32
  %736 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 38
  %737 = bitcast <4 x i64>* %736 to <16 x i16>*
  %738 = load <16 x i16>, <16 x i16>* %737, align 32
  %739 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %735, <16 x i16> %738) #8
  %740 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %735, <16 x i16> %738) #8
  %741 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 26
  %742 = bitcast <4 x i64>* %741 to <16 x i16>*
  %743 = load <16 x i16>, <16 x i16>* %742, align 32
  %744 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 37
  %745 = bitcast <4 x i64>* %744 to <16 x i16>*
  %746 = load <16 x i16>, <16 x i16>* %745, align 32
  %747 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %743, <16 x i16> %746) #8
  %748 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %743, <16 x i16> %746) #8
  %749 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 27
  %750 = bitcast <4 x i64>* %749 to <16 x i16>*
  %751 = load <16 x i16>, <16 x i16>* %750, align 32
  %752 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 36
  %753 = bitcast <4 x i64>* %752 to <16 x i16>*
  %754 = load <16 x i16>, <16 x i16>* %753, align 32
  %755 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %751, <16 x i16> %754) #8
  %756 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %751, <16 x i16> %754) #8
  %757 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 28
  %758 = bitcast <4 x i64>* %757 to <16 x i16>*
  %759 = load <16 x i16>, <16 x i16>* %758, align 32
  %760 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 35
  %761 = bitcast <4 x i64>* %760 to <16 x i16>*
  %762 = load <16 x i16>, <16 x i16>* %761, align 32
  %763 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %759, <16 x i16> %762) #8
  %764 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %759, <16 x i16> %762) #8
  %765 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 29
  %766 = bitcast <4 x i64>* %765 to <16 x i16>*
  %767 = load <16 x i16>, <16 x i16>* %766, align 32
  %768 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 34
  %769 = bitcast <4 x i64>* %768 to <16 x i16>*
  %770 = load <16 x i16>, <16 x i16>* %769, align 32
  %771 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %767, <16 x i16> %770) #8
  %772 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %767, <16 x i16> %770) #8
  %773 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 30
  %774 = bitcast <4 x i64>* %773 to <16 x i16>*
  %775 = load <16 x i16>, <16 x i16>* %774, align 32
  %776 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 33
  %777 = bitcast <4 x i64>* %776 to <16 x i16>*
  %778 = load <16 x i16>, <16 x i16>* %777, align 32
  %779 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %775, <16 x i16> %778) #8
  %780 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %775, <16 x i16> %778) #8
  %781 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 31
  %782 = bitcast <4 x i64>* %781 to <16 x i16>*
  %783 = load <16 x i16>, <16 x i16>* %782, align 32
  %784 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 32
  %785 = bitcast <4 x i64>* %784 to <16 x i16>*
  %786 = load <16 x i16>, <16 x i16>* %785, align 32
  %787 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %783, <16 x i16> %786) #8
  %788 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %783, <16 x i16> %786) #8
  %789 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %539, <16 x i16> %787) #8
  %790 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %539, <16 x i16> %787) #8
  %791 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %547, <16 x i16> %779) #8
  %792 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %547, <16 x i16> %779) #8
  %793 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %555, <16 x i16> %771) #8
  %794 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %555, <16 x i16> %771) #8
  %795 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %563, <16 x i16> %763) #8
  %796 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %563, <16 x i16> %763) #8
  %797 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %571, <16 x i16> %755) #8
  %798 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %571, <16 x i16> %755) #8
  %799 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %579, <16 x i16> %747) #8
  %800 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %579, <16 x i16> %747) #8
  %801 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %587, <16 x i16> %739) #8
  %802 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %587, <16 x i16> %739) #8
  %803 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %595, <16 x i16> %731) #8
  %804 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %595, <16 x i16> %731) #8
  %805 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %603, <16 x i16> %723) #8
  %806 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %603, <16 x i16> %723) #8
  %807 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %611, <16 x i16> %715) #8
  %808 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %611, <16 x i16> %715) #8
  %809 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %619, <16 x i16> %707) #8
  %810 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %619, <16 x i16> %707) #8
  %811 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %627, <16 x i16> %699) #8
  %812 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %627, <16 x i16> %699) #8
  %813 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %635, <16 x i16> %691) #8
  %814 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %635, <16 x i16> %691) #8
  %815 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %643, <16 x i16> %683) #8
  %816 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %643, <16 x i16> %683) #8
  %817 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %651, <16 x i16> %675) #8
  %818 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %651, <16 x i16> %675) #8
  %819 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %659, <16 x i16> %667) #8
  %820 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %659, <16 x i16> %667) #8
  %821 = shufflevector <16 x i16> %724, <16 x i16> %604, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %822 = shufflevector <16 x i16> %724, <16 x i16> %604, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %823 = bitcast <8 x i32> %19 to <16 x i16>
  %824 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %821, <16 x i16> %823) #8
  %825 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %822, <16 x i16> %823) #8
  %826 = bitcast <8 x i32> %22 to <16 x i16>
  %827 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %821, <16 x i16> %826) #8
  %828 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %822, <16 x i16> %826) #8
  %829 = add <8 x i32> %824, %10
  %830 = add <8 x i32> %825, %10
  %831 = add <8 x i32> %827, %10
  %832 = add <8 x i32> %828, %10
  %833 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %829, i32 %4) #8
  %834 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %830, i32 %4) #8
  %835 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %831, i32 %4) #8
  %836 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %832, i32 %4) #8
  %837 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %833, <8 x i32> %834) #8
  %838 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %835, <8 x i32> %836) #8
  %839 = shufflevector <16 x i16> %716, <16 x i16> %612, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %840 = shufflevector <16 x i16> %716, <16 x i16> %612, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %841 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %839, <16 x i16> %823) #8
  %842 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %840, <16 x i16> %823) #8
  %843 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %839, <16 x i16> %826) #8
  %844 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %840, <16 x i16> %826) #8
  %845 = add <8 x i32> %841, %10
  %846 = add <8 x i32> %842, %10
  %847 = add <8 x i32> %843, %10
  %848 = add <8 x i32> %844, %10
  %849 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %845, i32 %4) #8
  %850 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %846, i32 %4) #8
  %851 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %847, i32 %4) #8
  %852 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %848, i32 %4) #8
  %853 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %849, <8 x i32> %850) #8
  %854 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %851, <8 x i32> %852) #8
  %855 = shufflevector <16 x i16> %708, <16 x i16> %620, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %856 = shufflevector <16 x i16> %708, <16 x i16> %620, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %857 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %855, <16 x i16> %823) #8
  %858 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %856, <16 x i16> %823) #8
  %859 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %855, <16 x i16> %826) #8
  %860 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %856, <16 x i16> %826) #8
  %861 = add <8 x i32> %857, %10
  %862 = add <8 x i32> %858, %10
  %863 = add <8 x i32> %859, %10
  %864 = add <8 x i32> %860, %10
  %865 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %861, i32 %4) #8
  %866 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %862, i32 %4) #8
  %867 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %863, i32 %4) #8
  %868 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %864, i32 %4) #8
  %869 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %865, <8 x i32> %866) #8
  %870 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %867, <8 x i32> %868) #8
  %871 = shufflevector <16 x i16> %700, <16 x i16> %628, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %872 = shufflevector <16 x i16> %700, <16 x i16> %628, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %873 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %871, <16 x i16> %823) #8
  %874 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %872, <16 x i16> %823) #8
  %875 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %871, <16 x i16> %826) #8
  %876 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %872, <16 x i16> %826) #8
  %877 = add <8 x i32> %873, %10
  %878 = add <8 x i32> %874, %10
  %879 = add <8 x i32> %875, %10
  %880 = add <8 x i32> %876, %10
  %881 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %877, i32 %4) #8
  %882 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %878, i32 %4) #8
  %883 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %879, i32 %4) #8
  %884 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %880, i32 %4) #8
  %885 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %881, <8 x i32> %882) #8
  %886 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %883, <8 x i32> %884) #8
  %887 = shufflevector <16 x i16> %692, <16 x i16> %636, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %888 = shufflevector <16 x i16> %692, <16 x i16> %636, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %889 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %887, <16 x i16> %823) #8
  %890 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %888, <16 x i16> %823) #8
  %891 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %887, <16 x i16> %826) #8
  %892 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %888, <16 x i16> %826) #8
  %893 = add <8 x i32> %889, %10
  %894 = add <8 x i32> %890, %10
  %895 = add <8 x i32> %891, %10
  %896 = add <8 x i32> %892, %10
  %897 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %893, i32 %4) #8
  %898 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %894, i32 %4) #8
  %899 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %895, i32 %4) #8
  %900 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %896, i32 %4) #8
  %901 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %897, <8 x i32> %898) #8
  %902 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %899, <8 x i32> %900) #8
  %903 = shufflevector <16 x i16> %684, <16 x i16> %644, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %904 = shufflevector <16 x i16> %684, <16 x i16> %644, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %905 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %903, <16 x i16> %823) #8
  %906 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %904, <16 x i16> %823) #8
  %907 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %903, <16 x i16> %826) #8
  %908 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %904, <16 x i16> %826) #8
  %909 = add <8 x i32> %905, %10
  %910 = add <8 x i32> %906, %10
  %911 = add <8 x i32> %907, %10
  %912 = add <8 x i32> %908, %10
  %913 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %909, i32 %4) #8
  %914 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %910, i32 %4) #8
  %915 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %911, i32 %4) #8
  %916 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %912, i32 %4) #8
  %917 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %913, <8 x i32> %914) #8
  %918 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %915, <8 x i32> %916) #8
  %919 = shufflevector <16 x i16> %676, <16 x i16> %652, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %920 = shufflevector <16 x i16> %676, <16 x i16> %652, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %921 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %919, <16 x i16> %823) #8
  %922 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %920, <16 x i16> %823) #8
  %923 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %919, <16 x i16> %826) #8
  %924 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %920, <16 x i16> %826) #8
  %925 = add <8 x i32> %921, %10
  %926 = add <8 x i32> %922, %10
  %927 = add <8 x i32> %923, %10
  %928 = add <8 x i32> %924, %10
  %929 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %925, i32 %4) #8
  %930 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %926, i32 %4) #8
  %931 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %927, i32 %4) #8
  %932 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %928, i32 %4) #8
  %933 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %929, <8 x i32> %930) #8
  %934 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %931, <8 x i32> %932) #8
  %935 = shufflevector <16 x i16> %668, <16 x i16> %660, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %936 = shufflevector <16 x i16> %668, <16 x i16> %660, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %937 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %935, <16 x i16> %823) #8
  %938 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %936, <16 x i16> %823) #8
  %939 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %935, <16 x i16> %826) #8
  %940 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %936, <16 x i16> %826) #8
  %941 = add <8 x i32> %937, %10
  %942 = add <8 x i32> %938, %10
  %943 = add <8 x i32> %939, %10
  %944 = add <8 x i32> %940, %10
  %945 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %941, i32 %4) #8
  %946 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %942, i32 %4) #8
  %947 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %943, i32 %4) #8
  %948 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %944, i32 %4) #8
  %949 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %945, <8 x i32> %946) #8
  %950 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %947, <8 x i32> %948) #8
  %951 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %789, <16 x i16> %819) #8
  %952 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %789, <16 x i16> %819) #8
  %953 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %791, <16 x i16> %817) #8
  %954 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %791, <16 x i16> %817) #8
  %955 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %793, <16 x i16> %815) #8
  %956 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %793, <16 x i16> %815) #8
  %957 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %795, <16 x i16> %813) #8
  %958 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %795, <16 x i16> %813) #8
  %959 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %797, <16 x i16> %811) #8
  %960 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %797, <16 x i16> %811) #8
  %961 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %799, <16 x i16> %809) #8
  %962 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %799, <16 x i16> %809) #8
  %963 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %801, <16 x i16> %807) #8
  %964 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %801, <16 x i16> %807) #8
  %965 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %803, <16 x i16> %805) #8
  %966 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %803, <16 x i16> %805) #8
  %967 = shufflevector <16 x i16> %812, <16 x i16> %798, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %968 = shufflevector <16 x i16> %812, <16 x i16> %798, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %969 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %967, <16 x i16> %823) #8
  %970 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %968, <16 x i16> %823) #8
  %971 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %967, <16 x i16> %826) #8
  %972 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %968, <16 x i16> %826) #8
  %973 = add <8 x i32> %969, %10
  %974 = add <8 x i32> %970, %10
  %975 = add <8 x i32> %971, %10
  %976 = add <8 x i32> %972, %10
  %977 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %973, i32 %4) #8
  %978 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %974, i32 %4) #8
  %979 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %975, i32 %4) #8
  %980 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %976, i32 %4) #8
  %981 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %977, <8 x i32> %978) #8
  %982 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %979, <8 x i32> %980) #8
  %983 = shufflevector <16 x i16> %810, <16 x i16> %800, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %984 = shufflevector <16 x i16> %810, <16 x i16> %800, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %985 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %983, <16 x i16> %823) #8
  %986 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %984, <16 x i16> %823) #8
  %987 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %983, <16 x i16> %826) #8
  %988 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %984, <16 x i16> %826) #8
  %989 = add <8 x i32> %985, %10
  %990 = add <8 x i32> %986, %10
  %991 = add <8 x i32> %987, %10
  %992 = add <8 x i32> %988, %10
  %993 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %989, i32 %4) #8
  %994 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %990, i32 %4) #8
  %995 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %991, i32 %4) #8
  %996 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %992, i32 %4) #8
  %997 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %993, <8 x i32> %994) #8
  %998 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %995, <8 x i32> %996) #8
  %999 = shufflevector <16 x i16> %808, <16 x i16> %802, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1000 = shufflevector <16 x i16> %808, <16 x i16> %802, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1001 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %999, <16 x i16> %823) #8
  %1002 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1000, <16 x i16> %823) #8
  %1003 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %999, <16 x i16> %826) #8
  %1004 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1000, <16 x i16> %826) #8
  %1005 = add <8 x i32> %1001, %10
  %1006 = add <8 x i32> %1002, %10
  %1007 = add <8 x i32> %1003, %10
  %1008 = add <8 x i32> %1004, %10
  %1009 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1005, i32 %4) #8
  %1010 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1006, i32 %4) #8
  %1011 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1007, i32 %4) #8
  %1012 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1008, i32 %4) #8
  %1013 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1009, <8 x i32> %1010) #8
  %1014 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1011, <8 x i32> %1012) #8
  %1015 = shufflevector <16 x i16> %806, <16 x i16> %804, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1016 = shufflevector <16 x i16> %806, <16 x i16> %804, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1017 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1015, <16 x i16> %823) #8
  %1018 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1016, <16 x i16> %823) #8
  %1019 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1015, <16 x i16> %826) #8
  %1020 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1016, <16 x i16> %826) #8
  %1021 = add <8 x i32> %1017, %10
  %1022 = add <8 x i32> %1018, %10
  %1023 = add <8 x i32> %1019, %10
  %1024 = add <8 x i32> %1020, %10
  %1025 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1021, i32 %4) #8
  %1026 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1022, i32 %4) #8
  %1027 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1023, i32 %4) #8
  %1028 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1024, i32 %4) #8
  %1029 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1025, <8 x i32> %1026) #8
  %1030 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1027, <8 x i32> %1028) #8
  %1031 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %788, <16 x i16> %949) #8
  %1032 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %788, <16 x i16> %949) #8
  %1033 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %780, <16 x i16> %933) #8
  %1034 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %780, <16 x i16> %933) #8
  %1035 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %772, <16 x i16> %917) #8
  %1036 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %772, <16 x i16> %917) #8
  %1037 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %764, <16 x i16> %901) #8
  %1038 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %764, <16 x i16> %901) #8
  %1039 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %756, <16 x i16> %885) #8
  %1040 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %756, <16 x i16> %885) #8
  %1041 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %748, <16 x i16> %869) #8
  %1042 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %748, <16 x i16> %869) #8
  %1043 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %740, <16 x i16> %853) #8
  %1044 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %740, <16 x i16> %853) #8
  %1045 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %732, <16 x i16> %837) #8
  %1046 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %732, <16 x i16> %837) #8
  %1047 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %540, <16 x i16> %950) #8
  %1048 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %540, <16 x i16> %950) #8
  %1049 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %548, <16 x i16> %934) #8
  %1050 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %548, <16 x i16> %934) #8
  %1051 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %556, <16 x i16> %918) #8
  %1052 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %556, <16 x i16> %918) #8
  %1053 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %564, <16 x i16> %902) #8
  %1054 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %564, <16 x i16> %902) #8
  %1055 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %572, <16 x i16> %886) #8
  %1056 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %572, <16 x i16> %886) #8
  %1057 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %580, <16 x i16> %870) #8
  %1058 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %580, <16 x i16> %870) #8
  %1059 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %588, <16 x i16> %854) #8
  %1060 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %588, <16 x i16> %854) #8
  %1061 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %596, <16 x i16> %838) #8
  %1062 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %596, <16 x i16> %838) #8
  %1063 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %951, <16 x i16> %965) #8
  %1064 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %951, <16 x i16> %965) #8
  %1065 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %953, <16 x i16> %963) #8
  %1066 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %953, <16 x i16> %963) #8
  %1067 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %955, <16 x i16> %961) #8
  %1068 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %955, <16 x i16> %961) #8
  %1069 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %957, <16 x i16> %959) #8
  %1070 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %957, <16 x i16> %959) #8
  %1071 = shufflevector <16 x i16> %962, <16 x i16> %956, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1072 = shufflevector <16 x i16> %962, <16 x i16> %956, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1073 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1071, <16 x i16> %823) #8
  %1074 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1072, <16 x i16> %823) #8
  %1075 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1071, <16 x i16> %826) #8
  %1076 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1072, <16 x i16> %826) #8
  %1077 = add <8 x i32> %1073, %10
  %1078 = add <8 x i32> %1074, %10
  %1079 = add <8 x i32> %1075, %10
  %1080 = add <8 x i32> %1076, %10
  %1081 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1077, i32 %4) #8
  %1082 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1078, i32 %4) #8
  %1083 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1079, i32 %4) #8
  %1084 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1080, i32 %4) #8
  %1085 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1081, <8 x i32> %1082) #8
  %1086 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1083, <8 x i32> %1084) #8
  %1087 = shufflevector <16 x i16> %960, <16 x i16> %958, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1088 = shufflevector <16 x i16> %960, <16 x i16> %958, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1089 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1087, <16 x i16> %823) #8
  %1090 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1088, <16 x i16> %823) #8
  %1091 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1087, <16 x i16> %826) #8
  %1092 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1088, <16 x i16> %826) #8
  %1093 = add <8 x i32> %1089, %10
  %1094 = add <8 x i32> %1090, %10
  %1095 = add <8 x i32> %1091, %10
  %1096 = add <8 x i32> %1092, %10
  %1097 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1093, i32 %4) #8
  %1098 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1094, i32 %4) #8
  %1099 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1095, i32 %4) #8
  %1100 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1096, i32 %4) #8
  %1101 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1097, <8 x i32> %1098) #8
  %1102 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1099, <8 x i32> %1100) #8
  %1103 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %820, <16 x i16> %1029) #8
  %1104 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %820, <16 x i16> %1029) #8
  %1105 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %818, <16 x i16> %1013) #8
  %1106 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %818, <16 x i16> %1013) #8
  %1107 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %816, <16 x i16> %997) #8
  %1108 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %816, <16 x i16> %997) #8
  %1109 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %814, <16 x i16> %981) #8
  %1110 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %814, <16 x i16> %981) #8
  %1111 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %790, <16 x i16> %1030) #8
  %1112 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %790, <16 x i16> %1030) #8
  %1113 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %792, <16 x i16> %1014) #8
  %1114 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %792, <16 x i16> %1014) #8
  %1115 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %794, <16 x i16> %998) #8
  %1116 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %794, <16 x i16> %998) #8
  %1117 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %796, <16 x i16> %982) #8
  %1118 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %796, <16 x i16> %982) #8
  %1119 = shufflevector <16 x i16> %1039, <16 x i16> %1055, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1120 = shufflevector <16 x i16> %1039, <16 x i16> %1055, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1121 = bitcast <8 x i32> %33 to <16 x i16>
  %1122 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1119, <16 x i16> %1121) #8
  %1123 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1120, <16 x i16> %1121) #8
  %1124 = bitcast <8 x i32> %37 to <16 x i16>
  %1125 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1119, <16 x i16> %1124) #8
  %1126 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1120, <16 x i16> %1124) #8
  %1127 = add <8 x i32> %1122, %10
  %1128 = add <8 x i32> %1123, %10
  %1129 = add <8 x i32> %1125, %10
  %1130 = add <8 x i32> %1126, %10
  %1131 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1127, i32 %4) #8
  %1132 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1128, i32 %4) #8
  %1133 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1129, i32 %4) #8
  %1134 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1130, i32 %4) #8
  %1135 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1131, <8 x i32> %1132) #8
  %1136 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1133, <8 x i32> %1134) #8
  %1137 = shufflevector <16 x i16> %1041, <16 x i16> %1057, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1138 = shufflevector <16 x i16> %1041, <16 x i16> %1057, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1139 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1137, <16 x i16> %1121) #8
  %1140 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1138, <16 x i16> %1121) #8
  %1141 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1137, <16 x i16> %1124) #8
  %1142 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1138, <16 x i16> %1124) #8
  %1143 = add <8 x i32> %1139, %10
  %1144 = add <8 x i32> %1140, %10
  %1145 = add <8 x i32> %1141, %10
  %1146 = add <8 x i32> %1142, %10
  %1147 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1143, i32 %4) #8
  %1148 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1144, i32 %4) #8
  %1149 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1145, i32 %4) #8
  %1150 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1146, i32 %4) #8
  %1151 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1147, <8 x i32> %1148) #8
  %1152 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1149, <8 x i32> %1150) #8
  %1153 = shufflevector <16 x i16> %1043, <16 x i16> %1059, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1154 = shufflevector <16 x i16> %1043, <16 x i16> %1059, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1155 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1153, <16 x i16> %1121) #8
  %1156 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1154, <16 x i16> %1121) #8
  %1157 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1153, <16 x i16> %1124) #8
  %1158 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1154, <16 x i16> %1124) #8
  %1159 = add <8 x i32> %1155, %10
  %1160 = add <8 x i32> %1156, %10
  %1161 = add <8 x i32> %1157, %10
  %1162 = add <8 x i32> %1158, %10
  %1163 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1159, i32 %4) #8
  %1164 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1160, i32 %4) #8
  %1165 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1161, i32 %4) #8
  %1166 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1162, i32 %4) #8
  %1167 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1163, <8 x i32> %1164) #8
  %1168 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1165, <8 x i32> %1166) #8
  %1169 = shufflevector <16 x i16> %1045, <16 x i16> %1061, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1170 = shufflevector <16 x i16> %1045, <16 x i16> %1061, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1171 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1169, <16 x i16> %1121) #8
  %1172 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1170, <16 x i16> %1121) #8
  %1173 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1169, <16 x i16> %1124) #8
  %1174 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1170, <16 x i16> %1124) #8
  %1175 = add <8 x i32> %1171, %10
  %1176 = add <8 x i32> %1172, %10
  %1177 = add <8 x i32> %1173, %10
  %1178 = add <8 x i32> %1174, %10
  %1179 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1175, i32 %4) #8
  %1180 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1176, i32 %4) #8
  %1181 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1177, i32 %4) #8
  %1182 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1178, i32 %4) #8
  %1183 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1179, <8 x i32> %1180) #8
  %1184 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1181, <8 x i32> %1182) #8
  %1185 = shufflevector <16 x i16> %1046, <16 x i16> %1062, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1186 = shufflevector <16 x i16> %1046, <16 x i16> %1062, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1187 = bitcast <8 x i32> %43 to <16 x i16>
  %1188 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1185, <16 x i16> %1187) #8
  %1189 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1186, <16 x i16> %1187) #8
  %1190 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1185, <16 x i16> %1121) #8
  %1191 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1186, <16 x i16> %1121) #8
  %1192 = add <8 x i32> %1188, %10
  %1193 = add <8 x i32> %1189, %10
  %1194 = add <8 x i32> %1190, %10
  %1195 = add <8 x i32> %1191, %10
  %1196 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1192, i32 %4) #8
  %1197 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1193, i32 %4) #8
  %1198 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1194, i32 %4) #8
  %1199 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1195, i32 %4) #8
  %1200 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1196, <8 x i32> %1197) #8
  %1201 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1198, <8 x i32> %1199) #8
  %1202 = shufflevector <16 x i16> %1044, <16 x i16> %1060, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1203 = shufflevector <16 x i16> %1044, <16 x i16> %1060, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1204 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1202, <16 x i16> %1187) #8
  %1205 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1203, <16 x i16> %1187) #8
  %1206 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1202, <16 x i16> %1121) #8
  %1207 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1203, <16 x i16> %1121) #8
  %1208 = add <8 x i32> %1204, %10
  %1209 = add <8 x i32> %1205, %10
  %1210 = add <8 x i32> %1206, %10
  %1211 = add <8 x i32> %1207, %10
  %1212 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1208, i32 %4) #8
  %1213 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1209, i32 %4) #8
  %1214 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1210, i32 %4) #8
  %1215 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1211, i32 %4) #8
  %1216 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1212, <8 x i32> %1213) #8
  %1217 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1214, <8 x i32> %1215) #8
  %1218 = shufflevector <16 x i16> %1042, <16 x i16> %1058, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1219 = shufflevector <16 x i16> %1042, <16 x i16> %1058, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1220 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1218, <16 x i16> %1187) #8
  %1221 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1219, <16 x i16> %1187) #8
  %1222 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1218, <16 x i16> %1121) #8
  %1223 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1219, <16 x i16> %1121) #8
  %1224 = add <8 x i32> %1220, %10
  %1225 = add <8 x i32> %1221, %10
  %1226 = add <8 x i32> %1222, %10
  %1227 = add <8 x i32> %1223, %10
  %1228 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1224, i32 %4) #8
  %1229 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1225, i32 %4) #8
  %1230 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1226, i32 %4) #8
  %1231 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1227, i32 %4) #8
  %1232 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1228, <8 x i32> %1229) #8
  %1233 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1230, <8 x i32> %1231) #8
  %1234 = shufflevector <16 x i16> %1040, <16 x i16> %1056, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1235 = shufflevector <16 x i16> %1040, <16 x i16> %1056, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1236 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1234, <16 x i16> %1187) #8
  %1237 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1235, <16 x i16> %1187) #8
  %1238 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1234, <16 x i16> %1121) #8
  %1239 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1235, <16 x i16> %1121) #8
  %1240 = add <8 x i32> %1236, %10
  %1241 = add <8 x i32> %1237, %10
  %1242 = add <8 x i32> %1238, %10
  %1243 = add <8 x i32> %1239, %10
  %1244 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1240, i32 %4) #8
  %1245 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1241, i32 %4) #8
  %1246 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1242, i32 %4) #8
  %1247 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1243, i32 %4) #8
  %1248 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1244, <8 x i32> %1245) #8
  %1249 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1246, <8 x i32> %1247) #8
  %1250 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1063, <16 x i16> %1069) #8
  %1251 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1063, <16 x i16> %1069) #8
  %1252 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1065, <16 x i16> %1067) #8
  %1253 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1065, <16 x i16> %1067) #8
  %1254 = shufflevector <16 x i16> %1068, <16 x i16> %1066, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1255 = shufflevector <16 x i16> %1068, <16 x i16> %1066, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1256 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1254, <16 x i16> %823) #8
  %1257 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1255, <16 x i16> %823) #8
  %1258 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1254, <16 x i16> %826) #8
  %1259 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1255, <16 x i16> %826) #8
  %1260 = add <8 x i32> %1256, %10
  %1261 = add <8 x i32> %1257, %10
  %1262 = add <8 x i32> %1258, %10
  %1263 = add <8 x i32> %1259, %10
  %1264 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1260, i32 %4) #8
  %1265 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1261, i32 %4) #8
  %1266 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1262, i32 %4) #8
  %1267 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1263, i32 %4) #8
  %1268 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1264, <8 x i32> %1265) #8
  %1269 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1266, <8 x i32> %1267) #8
  %1270 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %966, <16 x i16> %1101) #8
  %1271 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %966, <16 x i16> %1101) #8
  %1272 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %964, <16 x i16> %1085) #8
  %1273 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %964, <16 x i16> %1085) #8
  %1274 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %952, <16 x i16> %1102) #8
  %1275 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %952, <16 x i16> %1102) #8
  %1276 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %954, <16 x i16> %1086) #8
  %1277 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %954, <16 x i16> %1086) #8
  %1278 = shufflevector <16 x i16> %1107, <16 x i16> %1115, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1279 = shufflevector <16 x i16> %1107, <16 x i16> %1115, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1280 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1278, <16 x i16> %1121) #8
  %1281 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1279, <16 x i16> %1121) #8
  %1282 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1278, <16 x i16> %1124) #8
  %1283 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1279, <16 x i16> %1124) #8
  %1284 = add <8 x i32> %1280, %10
  %1285 = add <8 x i32> %1281, %10
  %1286 = add <8 x i32> %1282, %10
  %1287 = add <8 x i32> %1283, %10
  %1288 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1284, i32 %4) #8
  %1289 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1285, i32 %4) #8
  %1290 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1286, i32 %4) #8
  %1291 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1287, i32 %4) #8
  %1292 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1288, <8 x i32> %1289) #8
  %1293 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1290, <8 x i32> %1291) #8
  %1294 = shufflevector <16 x i16> %1109, <16 x i16> %1117, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1295 = shufflevector <16 x i16> %1109, <16 x i16> %1117, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1296 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1294, <16 x i16> %1121) #8
  %1297 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1295, <16 x i16> %1121) #8
  %1298 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1294, <16 x i16> %1124) #8
  %1299 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1295, <16 x i16> %1124) #8
  %1300 = add <8 x i32> %1296, %10
  %1301 = add <8 x i32> %1297, %10
  %1302 = add <8 x i32> %1298, %10
  %1303 = add <8 x i32> %1299, %10
  %1304 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1300, i32 %4) #8
  %1305 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1301, i32 %4) #8
  %1306 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1302, i32 %4) #8
  %1307 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1303, i32 %4) #8
  %1308 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1304, <8 x i32> %1305) #8
  %1309 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1306, <8 x i32> %1307) #8
  %1310 = shufflevector <16 x i16> %1110, <16 x i16> %1118, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1311 = shufflevector <16 x i16> %1110, <16 x i16> %1118, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1312 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1310, <16 x i16> %1187) #8
  %1313 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1311, <16 x i16> %1187) #8
  %1314 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1310, <16 x i16> %1121) #8
  %1315 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1311, <16 x i16> %1121) #8
  %1316 = add <8 x i32> %1312, %10
  %1317 = add <8 x i32> %1313, %10
  %1318 = add <8 x i32> %1314, %10
  %1319 = add <8 x i32> %1315, %10
  %1320 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1316, i32 %4) #8
  %1321 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1317, i32 %4) #8
  %1322 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1318, i32 %4) #8
  %1323 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1319, i32 %4) #8
  %1324 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1320, <8 x i32> %1321) #8
  %1325 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1322, <8 x i32> %1323) #8
  %1326 = shufflevector <16 x i16> %1108, <16 x i16> %1116, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1327 = shufflevector <16 x i16> %1108, <16 x i16> %1116, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1328 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1326, <16 x i16> %1187) #8
  %1329 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1327, <16 x i16> %1187) #8
  %1330 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1326, <16 x i16> %1121) #8
  %1331 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1327, <16 x i16> %1121) #8
  %1332 = add <8 x i32> %1328, %10
  %1333 = add <8 x i32> %1329, %10
  %1334 = add <8 x i32> %1330, %10
  %1335 = add <8 x i32> %1331, %10
  %1336 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1332, i32 %4) #8
  %1337 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1333, i32 %4) #8
  %1338 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1334, i32 %4) #8
  %1339 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1335, i32 %4) #8
  %1340 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1336, <8 x i32> %1337) #8
  %1341 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1338, <8 x i32> %1339) #8
  %1342 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1031, <16 x i16> %1183) #8
  %1343 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1031, <16 x i16> %1183) #8
  %1344 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1033, <16 x i16> %1167) #8
  %1345 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1033, <16 x i16> %1167) #8
  %1346 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1035, <16 x i16> %1151) #8
  %1347 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1035, <16 x i16> %1151) #8
  %1348 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1037, <16 x i16> %1135) #8
  %1349 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1037, <16 x i16> %1135) #8
  %1350 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1032, <16 x i16> %1200) #8
  %1351 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1032, <16 x i16> %1200) #8
  %1352 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1034, <16 x i16> %1216) #8
  %1353 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1034, <16 x i16> %1216) #8
  %1354 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1036, <16 x i16> %1232) #8
  %1355 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1036, <16 x i16> %1232) #8
  %1356 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1038, <16 x i16> %1248) #8
  %1357 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1038, <16 x i16> %1248) #8
  %1358 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1048, <16 x i16> %1201) #8
  %1359 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1048, <16 x i16> %1201) #8
  %1360 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1050, <16 x i16> %1217) #8
  %1361 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1050, <16 x i16> %1217) #8
  %1362 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1052, <16 x i16> %1233) #8
  %1363 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1052, <16 x i16> %1233) #8
  %1364 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1054, <16 x i16> %1249) #8
  %1365 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1054, <16 x i16> %1249) #8
  %1366 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1047, <16 x i16> %1184) #8
  %1367 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1047, <16 x i16> %1184) #8
  %1368 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1049, <16 x i16> %1168) #8
  %1369 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1049, <16 x i16> %1168) #8
  %1370 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1051, <16 x i16> %1152) #8
  %1371 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1051, <16 x i16> %1152) #8
  %1372 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1053, <16 x i16> %1136) #8
  %1373 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1053, <16 x i16> %1136) #8
  %1374 = shufflevector <16 x i16> %1250, <16 x i16> %1252, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1375 = shufflevector <16 x i16> %1250, <16 x i16> %1252, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1376 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1374, <16 x i16> %826) #8
  %1377 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1375, <16 x i16> %826) #8
  %1378 = bitcast <8 x i32> %47 to <16 x i16>
  %1379 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1374, <16 x i16> %1378) #8
  %1380 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1375, <16 x i16> %1378) #8
  %1381 = add <8 x i32> %1376, %10
  %1382 = add <8 x i32> %1377, %10
  %1383 = add <8 x i32> %1379, %10
  %1384 = add <8 x i32> %1380, %10
  %1385 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1381, i32 %4) #8
  %1386 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1382, i32 %4) #8
  %1387 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1383, i32 %4) #8
  %1388 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1384, i32 %4) #8
  %1389 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1385, <8 x i32> %1386) #8
  %1390 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1387, <8 x i32> %1388) #8
  %1391 = shufflevector <16 x i16> %1253, <16 x i16> %1251, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1392 = shufflevector <16 x i16> %1253, <16 x i16> %1251, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1393 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1391, <16 x i16> %1124) #8
  %1394 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1392, <16 x i16> %1124) #8
  %1395 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1391, <16 x i16> %1121) #8
  %1396 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1392, <16 x i16> %1121) #8
  %1397 = add <8 x i32> %1393, %10
  %1398 = add <8 x i32> %1394, %10
  %1399 = add <8 x i32> %1395, %10
  %1400 = add <8 x i32> %1396, %10
  %1401 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1397, i32 %4) #8
  %1402 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1398, i32 %4) #8
  %1403 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1399, i32 %4) #8
  %1404 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1400, i32 %4) #8
  %1405 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1401, <8 x i32> %1402) #8
  %1406 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1403, <8 x i32> %1404) #8
  %1407 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1070, <16 x i16> %1268) #8
  %1408 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1070, <16 x i16> %1268) #8
  %1409 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1064, <16 x i16> %1269) #8
  %1410 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1064, <16 x i16> %1269) #8
  %1411 = shufflevector <16 x i16> %1272, <16 x i16> %1276, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1412 = shufflevector <16 x i16> %1272, <16 x i16> %1276, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1413 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1411, <16 x i16> %1121) #8
  %1414 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1412, <16 x i16> %1121) #8
  %1415 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1411, <16 x i16> %1124) #8
  %1416 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1412, <16 x i16> %1124) #8
  %1417 = add <8 x i32> %1413, %10
  %1418 = add <8 x i32> %1414, %10
  %1419 = add <8 x i32> %1415, %10
  %1420 = add <8 x i32> %1416, %10
  %1421 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1417, i32 %4) #8
  %1422 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1418, i32 %4) #8
  %1423 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1419, i32 %4) #8
  %1424 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1420, i32 %4) #8
  %1425 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1421, <8 x i32> %1422) #8
  %1426 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1423, <8 x i32> %1424) #8
  %1427 = shufflevector <16 x i16> %1273, <16 x i16> %1277, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1428 = shufflevector <16 x i16> %1273, <16 x i16> %1277, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1429 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1427, <16 x i16> %1187) #8
  %1430 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1428, <16 x i16> %1187) #8
  %1431 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1427, <16 x i16> %1121) #8
  %1432 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1428, <16 x i16> %1121) #8
  %1433 = add <8 x i32> %1429, %10
  %1434 = add <8 x i32> %1430, %10
  %1435 = add <8 x i32> %1431, %10
  %1436 = add <8 x i32> %1432, %10
  %1437 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1433, i32 %4) #8
  %1438 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1434, i32 %4) #8
  %1439 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1435, i32 %4) #8
  %1440 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1436, i32 %4) #8
  %1441 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1437, <8 x i32> %1438) #8
  %1442 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1439, <8 x i32> %1440) #8
  %1443 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1103, <16 x i16> %1308) #8
  %1444 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1103, <16 x i16> %1308) #8
  %1445 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1105, <16 x i16> %1292) #8
  %1446 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1105, <16 x i16> %1292) #8
  %1447 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1104, <16 x i16> %1324) #8
  %1448 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1104, <16 x i16> %1324) #8
  %1449 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1106, <16 x i16> %1340) #8
  %1450 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1106, <16 x i16> %1340) #8
  %1451 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1112, <16 x i16> %1325) #8
  %1452 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1112, <16 x i16> %1325) #8
  %1453 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1114, <16 x i16> %1341) #8
  %1454 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1114, <16 x i16> %1341) #8
  %1455 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1111, <16 x i16> %1309) #8
  %1456 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1111, <16 x i16> %1309) #8
  %1457 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1113, <16 x i16> %1293) #8
  %1458 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1113, <16 x i16> %1293) #8
  %1459 = shufflevector <16 x i16> %1346, <16 x i16> %1370, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1460 = shufflevector <16 x i16> %1346, <16 x i16> %1370, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1461 = bitcast <8 x i32> %58 to <16 x i16>
  %1462 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1459, <16 x i16> %1461) #8
  %1463 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1460, <16 x i16> %1461) #8
  %1464 = bitcast <8 x i32> %62 to <16 x i16>
  %1465 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1459, <16 x i16> %1464) #8
  %1466 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1460, <16 x i16> %1464) #8
  %1467 = add <8 x i32> %1462, %10
  %1468 = add <8 x i32> %1463, %10
  %1469 = add <8 x i32> %1465, %10
  %1470 = add <8 x i32> %1466, %10
  %1471 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1467, i32 %4) #8
  %1472 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1468, i32 %4) #8
  %1473 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1469, i32 %4) #8
  %1474 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1470, i32 %4) #8
  %1475 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1471, <8 x i32> %1472) #8
  %1476 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1473, <8 x i32> %1474) #8
  %1477 = shufflevector <16 x i16> %1348, <16 x i16> %1372, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1478 = shufflevector <16 x i16> %1348, <16 x i16> %1372, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1479 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1477, <16 x i16> %1461) #8
  %1480 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1478, <16 x i16> %1461) #8
  %1481 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1477, <16 x i16> %1464) #8
  %1482 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1478, <16 x i16> %1464) #8
  %1483 = add <8 x i32> %1479, %10
  %1484 = add <8 x i32> %1480, %10
  %1485 = add <8 x i32> %1481, %10
  %1486 = add <8 x i32> %1482, %10
  %1487 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1483, i32 %4) #8
  %1488 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1484, i32 %4) #8
  %1489 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1485, i32 %4) #8
  %1490 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1486, i32 %4) #8
  %1491 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1487, <8 x i32> %1488) #8
  %1492 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1489, <8 x i32> %1490) #8
  %1493 = shufflevector <16 x i16> %1349, <16 x i16> %1373, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1494 = shufflevector <16 x i16> %1349, <16 x i16> %1373, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1495 = bitcast <8 x i32> %68 to <16 x i16>
  %1496 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1493, <16 x i16> %1495) #8
  %1497 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1494, <16 x i16> %1495) #8
  %1498 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1493, <16 x i16> %1461) #8
  %1499 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1494, <16 x i16> %1461) #8
  %1500 = add <8 x i32> %1496, %10
  %1501 = add <8 x i32> %1497, %10
  %1502 = add <8 x i32> %1498, %10
  %1503 = add <8 x i32> %1499, %10
  %1504 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1500, i32 %4) #8
  %1505 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1501, i32 %4) #8
  %1506 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1502, i32 %4) #8
  %1507 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1503, i32 %4) #8
  %1508 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1504, <8 x i32> %1505) #8
  %1509 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1506, <8 x i32> %1507) #8
  %1510 = shufflevector <16 x i16> %1347, <16 x i16> %1371, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1511 = shufflevector <16 x i16> %1347, <16 x i16> %1371, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1512 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1510, <16 x i16> %1495) #8
  %1513 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1511, <16 x i16> %1495) #8
  %1514 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1510, <16 x i16> %1461) #8
  %1515 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1511, <16 x i16> %1461) #8
  %1516 = add <8 x i32> %1512, %10
  %1517 = add <8 x i32> %1513, %10
  %1518 = add <8 x i32> %1514, %10
  %1519 = add <8 x i32> %1515, %10
  %1520 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1516, i32 %4) #8
  %1521 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1517, i32 %4) #8
  %1522 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1518, i32 %4) #8
  %1523 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1519, i32 %4) #8
  %1524 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1520, <8 x i32> %1521) #8
  %1525 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1522, <8 x i32> %1523) #8
  %1526 = shufflevector <16 x i16> %1355, <16 x i16> %1363, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1527 = shufflevector <16 x i16> %1355, <16 x i16> %1363, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1528 = bitcast <8 x i32> %79 to <16 x i16>
  %1529 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1526, <16 x i16> %1528) #8
  %1530 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1527, <16 x i16> %1528) #8
  %1531 = bitcast <8 x i32> %83 to <16 x i16>
  %1532 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1526, <16 x i16> %1531) #8
  %1533 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1527, <16 x i16> %1531) #8
  %1534 = add <8 x i32> %1529, %10
  %1535 = add <8 x i32> %1530, %10
  %1536 = add <8 x i32> %1532, %10
  %1537 = add <8 x i32> %1533, %10
  %1538 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1534, i32 %4) #8
  %1539 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1535, i32 %4) #8
  %1540 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1536, i32 %4) #8
  %1541 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1537, i32 %4) #8
  %1542 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1538, <8 x i32> %1539) #8
  %1543 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1540, <8 x i32> %1541) #8
  %1544 = shufflevector <16 x i16> %1357, <16 x i16> %1365, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1545 = shufflevector <16 x i16> %1357, <16 x i16> %1365, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1546 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1544, <16 x i16> %1528) #8
  %1547 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1545, <16 x i16> %1528) #8
  %1548 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1544, <16 x i16> %1531) #8
  %1549 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1545, <16 x i16> %1531) #8
  %1550 = add <8 x i32> %1546, %10
  %1551 = add <8 x i32> %1547, %10
  %1552 = add <8 x i32> %1548, %10
  %1553 = add <8 x i32> %1549, %10
  %1554 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1550, i32 %4) #8
  %1555 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1551, i32 %4) #8
  %1556 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1552, i32 %4) #8
  %1557 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1553, i32 %4) #8
  %1558 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1554, <8 x i32> %1555) #8
  %1559 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1556, <8 x i32> %1557) #8
  %1560 = shufflevector <16 x i16> %1356, <16 x i16> %1364, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1561 = shufflevector <16 x i16> %1356, <16 x i16> %1364, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1562 = bitcast <8 x i32> %89 to <16 x i16>
  %1563 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1560, <16 x i16> %1562) #8
  %1564 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1561, <16 x i16> %1562) #8
  %1565 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1560, <16 x i16> %1528) #8
  %1566 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1561, <16 x i16> %1528) #8
  %1567 = add <8 x i32> %1563, %10
  %1568 = add <8 x i32> %1564, %10
  %1569 = add <8 x i32> %1565, %10
  %1570 = add <8 x i32> %1566, %10
  %1571 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1567, i32 %4) #8
  %1572 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1568, i32 %4) #8
  %1573 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1569, i32 %4) #8
  %1574 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1570, i32 %4) #8
  %1575 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1571, <8 x i32> %1572) #8
  %1576 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1573, <8 x i32> %1574) #8
  %1577 = shufflevector <16 x i16> %1354, <16 x i16> %1362, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1578 = shufflevector <16 x i16> %1354, <16 x i16> %1362, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1579 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1577, <16 x i16> %1562) #8
  %1580 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1578, <16 x i16> %1562) #8
  %1581 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1577, <16 x i16> %1528) #8
  %1582 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1578, <16 x i16> %1528) #8
  %1583 = add <8 x i32> %1579, %10
  %1584 = add <8 x i32> %1580, %10
  %1585 = add <8 x i32> %1581, %10
  %1586 = add <8 x i32> %1582, %10
  %1587 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1583, i32 %4) #8
  %1588 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1584, i32 %4) #8
  %1589 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1585, i32 %4) #8
  %1590 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1586, i32 %4) #8
  %1591 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1587, <8 x i32> %1588) #8
  %1592 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1589, <8 x i32> %1590) #8
  %1593 = shufflevector <16 x i16> %1407, <16 x i16> %1409, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1594 = shufflevector <16 x i16> %1407, <16 x i16> %1409, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1595 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1593, <16 x i16> %1464) #8
  %1596 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1594, <16 x i16> %1464) #8
  %1597 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1593, <16 x i16> %1461) #8
  %1598 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1594, <16 x i16> %1461) #8
  %1599 = add <8 x i32> %1595, %10
  %1600 = add <8 x i32> %1596, %10
  %1601 = add <8 x i32> %1597, %10
  %1602 = add <8 x i32> %1598, %10
  %1603 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1599, i32 %4) #8
  %1604 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1600, i32 %4) #8
  %1605 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1601, i32 %4) #8
  %1606 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1602, i32 %4) #8
  %1607 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1603, <8 x i32> %1604) #8
  %1608 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1605, <8 x i32> %1606) #8
  %1609 = shufflevector <16 x i16> %1408, <16 x i16> %1410, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1610 = shufflevector <16 x i16> %1408, <16 x i16> %1410, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1611 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1609, <16 x i16> %1531) #8
  %1612 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1610, <16 x i16> %1531) #8
  %1613 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1609, <16 x i16> %1528) #8
  %1614 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1610, <16 x i16> %1528) #8
  %1615 = add <8 x i32> %1611, %10
  %1616 = add <8 x i32> %1612, %10
  %1617 = add <8 x i32> %1613, %10
  %1618 = add <8 x i32> %1614, %10
  %1619 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1615, i32 %4) #8
  %1620 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1616, i32 %4) #8
  %1621 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1617, i32 %4) #8
  %1622 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1618, i32 %4) #8
  %1623 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1619, <8 x i32> %1620) #8
  %1624 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1621, <8 x i32> %1622) #8
  %1625 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1270, <16 x i16> %1425) #8
  %1626 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1270, <16 x i16> %1425) #8
  %1627 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1271, <16 x i16> %1441) #8
  %1628 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1271, <16 x i16> %1441) #8
  %1629 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1275, <16 x i16> %1442) #8
  %1630 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1275, <16 x i16> %1442) #8
  %1631 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1274, <16 x i16> %1426) #8
  %1632 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1274, <16 x i16> %1426) #8
  %1633 = shufflevector <16 x i16> %1445, <16 x i16> %1457, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1634 = shufflevector <16 x i16> %1445, <16 x i16> %1457, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1635 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1633, <16 x i16> %1461) #8
  %1636 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1634, <16 x i16> %1461) #8
  %1637 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1633, <16 x i16> %1464) #8
  %1638 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1634, <16 x i16> %1464) #8
  %1639 = add <8 x i32> %1635, %10
  %1640 = add <8 x i32> %1636, %10
  %1641 = add <8 x i32> %1637, %10
  %1642 = add <8 x i32> %1638, %10
  %1643 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1639, i32 %4) #8
  %1644 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1640, i32 %4) #8
  %1645 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1641, i32 %4) #8
  %1646 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1642, i32 %4) #8
  %1647 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1643, <8 x i32> %1644) #8
  %1648 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1645, <8 x i32> %1646) #8
  %1649 = shufflevector <16 x i16> %1446, <16 x i16> %1458, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1650 = shufflevector <16 x i16> %1446, <16 x i16> %1458, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1651 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1649, <16 x i16> %1495) #8
  %1652 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1650, <16 x i16> %1495) #8
  %1653 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1649, <16 x i16> %1461) #8
  %1654 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1650, <16 x i16> %1461) #8
  %1655 = add <8 x i32> %1651, %10
  %1656 = add <8 x i32> %1652, %10
  %1657 = add <8 x i32> %1653, %10
  %1658 = add <8 x i32> %1654, %10
  %1659 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1655, i32 %4) #8
  %1660 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1656, i32 %4) #8
  %1661 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1657, i32 %4) #8
  %1662 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1658, i32 %4) #8
  %1663 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1659, <8 x i32> %1660) #8
  %1664 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1661, <8 x i32> %1662) #8
  %1665 = shufflevector <16 x i16> %1450, <16 x i16> %1454, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1666 = shufflevector <16 x i16> %1450, <16 x i16> %1454, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1667 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1665, <16 x i16> %1528) #8
  %1668 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1666, <16 x i16> %1528) #8
  %1669 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1665, <16 x i16> %1531) #8
  %1670 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1666, <16 x i16> %1531) #8
  %1671 = add <8 x i32> %1667, %10
  %1672 = add <8 x i32> %1668, %10
  %1673 = add <8 x i32> %1669, %10
  %1674 = add <8 x i32> %1670, %10
  %1675 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1671, i32 %4) #8
  %1676 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1672, i32 %4) #8
  %1677 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1673, i32 %4) #8
  %1678 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1674, i32 %4) #8
  %1679 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1675, <8 x i32> %1676) #8
  %1680 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1677, <8 x i32> %1678) #8
  %1681 = shufflevector <16 x i16> %1449, <16 x i16> %1453, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1682 = shufflevector <16 x i16> %1449, <16 x i16> %1453, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1683 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1681, <16 x i16> %1562) #8
  %1684 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1682, <16 x i16> %1562) #8
  %1685 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1681, <16 x i16> %1528) #8
  %1686 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1682, <16 x i16> %1528) #8
  %1687 = add <8 x i32> %1683, %10
  %1688 = add <8 x i32> %1684, %10
  %1689 = add <8 x i32> %1685, %10
  %1690 = add <8 x i32> %1686, %10
  %1691 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1687, i32 %4) #8
  %1692 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1688, i32 %4) #8
  %1693 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1689, i32 %4) #8
  %1694 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1690, i32 %4) #8
  %1695 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1691, <8 x i32> %1692) #8
  %1696 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1693, <8 x i32> %1694) #8
  %1697 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1342, <16 x i16> %1491) #8
  %1698 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1342, <16 x i16> %1491) #8
  %1699 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1344, <16 x i16> %1475) #8
  %1700 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1344, <16 x i16> %1475) #8
  %1701 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1343, <16 x i16> %1508) #8
  %1702 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1343, <16 x i16> %1508) #8
  %1703 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1345, <16 x i16> %1524) #8
  %1704 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1345, <16 x i16> %1524) #8
  %1705 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1351, <16 x i16> %1558) #8
  %1706 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1351, <16 x i16> %1558) #8
  %1707 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1353, <16 x i16> %1542) #8
  %1708 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1353, <16 x i16> %1542) #8
  %1709 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1350, <16 x i16> %1575) #8
  %1710 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1350, <16 x i16> %1575) #8
  %1711 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1352, <16 x i16> %1591) #8
  %1712 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1352, <16 x i16> %1591) #8
  %1713 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1358, <16 x i16> %1576) #8
  %1714 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1358, <16 x i16> %1576) #8
  %1715 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1360, <16 x i16> %1592) #8
  %1716 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1360, <16 x i16> %1592) #8
  %1717 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1359, <16 x i16> %1559) #8
  %1718 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1359, <16 x i16> %1559) #8
  %1719 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1361, <16 x i16> %1543) #8
  %1720 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1361, <16 x i16> %1543) #8
  %1721 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1367, <16 x i16> %1509) #8
  %1722 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1367, <16 x i16> %1509) #8
  %1723 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1369, <16 x i16> %1525) #8
  %1724 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1369, <16 x i16> %1525) #8
  %1725 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1366, <16 x i16> %1492) #8
  %1726 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1366, <16 x i16> %1492) #8
  %1727 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1368, <16 x i16> %1476) #8
  %1728 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1368, <16 x i16> %1476) #8
  %1729 = shufflevector <16 x i16> %1625, <16 x i16> %1631, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1730 = shufflevector <16 x i16> %1625, <16 x i16> %1631, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1731 = bitcast <8 x i32> %98 to <16 x i16>
  %1732 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1729, <16 x i16> %1731) #8
  %1733 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1730, <16 x i16> %1731) #8
  %1734 = bitcast <8 x i32> %104 to <16 x i16>
  %1735 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1729, <16 x i16> %1734) #8
  %1736 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1730, <16 x i16> %1734) #8
  %1737 = add <8 x i32> %1732, %10
  %1738 = add <8 x i32> %1733, %10
  %1739 = add <8 x i32> %1735, %10
  %1740 = add <8 x i32> %1736, %10
  %1741 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1737, i32 %4) #8
  %1742 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1738, i32 %4) #8
  %1743 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1739, i32 %4) #8
  %1744 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1740, i32 %4) #8
  %1745 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1741, <8 x i32> %1742) #8
  %1746 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1743, <8 x i32> %1744) #8
  %1747 = shufflevector <16 x i16> %1626, <16 x i16> %1632, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1748 = shufflevector <16 x i16> %1626, <16 x i16> %1632, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1749 = bitcast <8 x i32> %113 to <16 x i16>
  %1750 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1747, <16 x i16> %1749) #8
  %1751 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1748, <16 x i16> %1749) #8
  %1752 = bitcast <8 x i32> %119 to <16 x i16>
  %1753 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1747, <16 x i16> %1752) #8
  %1754 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1748, <16 x i16> %1752) #8
  %1755 = add <8 x i32> %1750, %10
  %1756 = add <8 x i32> %1751, %10
  %1757 = add <8 x i32> %1753, %10
  %1758 = add <8 x i32> %1754, %10
  %1759 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1755, i32 %4) #8
  %1760 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1756, i32 %4) #8
  %1761 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1757, i32 %4) #8
  %1762 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1758, i32 %4) #8
  %1763 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1759, <8 x i32> %1760) #8
  %1764 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1761, <8 x i32> %1762) #8
  %1765 = shufflevector <16 x i16> %1628, <16 x i16> %1630, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1766 = shufflevector <16 x i16> %1628, <16 x i16> %1630, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1767 = bitcast <8 x i32> %128 to <16 x i16>
  %1768 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1765, <16 x i16> %1767) #8
  %1769 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1766, <16 x i16> %1767) #8
  %1770 = bitcast <8 x i32> %134 to <16 x i16>
  %1771 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1765, <16 x i16> %1770) #8
  %1772 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1766, <16 x i16> %1770) #8
  %1773 = add <8 x i32> %1768, %10
  %1774 = add <8 x i32> %1769, %10
  %1775 = add <8 x i32> %1771, %10
  %1776 = add <8 x i32> %1772, %10
  %1777 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1773, i32 %4) #8
  %1778 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1774, i32 %4) #8
  %1779 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1775, i32 %4) #8
  %1780 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1776, i32 %4) #8
  %1781 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1777, <8 x i32> %1778) #8
  %1782 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1779, <8 x i32> %1780) #8
  %1783 = shufflevector <16 x i16> %1627, <16 x i16> %1629, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1784 = shufflevector <16 x i16> %1627, <16 x i16> %1629, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1785 = bitcast <8 x i32> %143 to <16 x i16>
  %1786 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1783, <16 x i16> %1785) #8
  %1787 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1784, <16 x i16> %1785) #8
  %1788 = bitcast <8 x i32> %149 to <16 x i16>
  %1789 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1783, <16 x i16> %1788) #8
  %1790 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1784, <16 x i16> %1788) #8
  %1791 = add <8 x i32> %1786, %10
  %1792 = add <8 x i32> %1787, %10
  %1793 = add <8 x i32> %1789, %10
  %1794 = add <8 x i32> %1790, %10
  %1795 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1791, i32 %4) #8
  %1796 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1792, i32 %4) #8
  %1797 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1793, i32 %4) #8
  %1798 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1794, i32 %4) #8
  %1799 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1795, <8 x i32> %1796) #8
  %1800 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1797, <8 x i32> %1798) #8
  %1801 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1443, <16 x i16> %1647) #8
  %1802 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1443, <16 x i16> %1647) #8
  %1803 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1444, <16 x i16> %1663) #8
  %1804 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1444, <16 x i16> %1663) #8
  %1805 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1448, <16 x i16> %1679) #8
  %1806 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1448, <16 x i16> %1679) #8
  %1807 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1447, <16 x i16> %1695) #8
  %1808 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1447, <16 x i16> %1695) #8
  %1809 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1451, <16 x i16> %1696) #8
  %1810 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1451, <16 x i16> %1696) #8
  %1811 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1452, <16 x i16> %1680) #8
  %1812 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1452, <16 x i16> %1680) #8
  %1813 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1456, <16 x i16> %1664) #8
  %1814 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1456, <16 x i16> %1664) #8
  %1815 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1455, <16 x i16> %1648) #8
  %1816 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1455, <16 x i16> %1648) #8
  %1817 = shufflevector <16 x i16> %1699, <16 x i16> %1727, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1818 = shufflevector <16 x i16> %1699, <16 x i16> %1727, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1819 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1817, <16 x i16> %1734) #8
  %1820 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1818, <16 x i16> %1734) #8
  %1821 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1817, <16 x i16> %1731) #8
  %1822 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1818, <16 x i16> %1731) #8
  %1823 = add <8 x i32> %1819, %10
  %1824 = add <8 x i32> %1820, %10
  %1825 = add <8 x i32> %1821, %10
  %1826 = add <8 x i32> %1822, %10
  %1827 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1823, i32 %4) #8
  %1828 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1824, i32 %4) #8
  %1829 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1825, i32 %4) #8
  %1830 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1826, i32 %4) #8
  %1831 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1827, <8 x i32> %1828) #8
  %1832 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1829, <8 x i32> %1830) #8
  %1833 = shufflevector <16 x i16> %1700, <16 x i16> %1728, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1834 = shufflevector <16 x i16> %1700, <16 x i16> %1728, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1835 = bitcast <8 x i32> %155 to <16 x i16>
  %1836 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1833, <16 x i16> %1835) #8
  %1837 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1834, <16 x i16> %1835) #8
  %1838 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1833, <16 x i16> %1734) #8
  %1839 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1834, <16 x i16> %1734) #8
  %1840 = add <8 x i32> %1836, %10
  %1841 = add <8 x i32> %1837, %10
  %1842 = add <8 x i32> %1838, %10
  %1843 = add <8 x i32> %1839, %10
  %1844 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1840, i32 %4) #8
  %1845 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1841, i32 %4) #8
  %1846 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1842, i32 %4) #8
  %1847 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1843, i32 %4) #8
  %1848 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1844, <8 x i32> %1845) #8
  %1849 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1846, <8 x i32> %1847) #8
  %1850 = shufflevector <16 x i16> %1704, <16 x i16> %1724, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1851 = shufflevector <16 x i16> %1704, <16 x i16> %1724, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1852 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1850, <16 x i16> %1752) #8
  %1853 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1851, <16 x i16> %1752) #8
  %1854 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1850, <16 x i16> %1749) #8
  %1855 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1851, <16 x i16> %1749) #8
  %1856 = add <8 x i32> %1852, %10
  %1857 = add <8 x i32> %1853, %10
  %1858 = add <8 x i32> %1854, %10
  %1859 = add <8 x i32> %1855, %10
  %1860 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1856, i32 %4) #8
  %1861 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1857, i32 %4) #8
  %1862 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1858, i32 %4) #8
  %1863 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1859, i32 %4) #8
  %1864 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1860, <8 x i32> %1861) #8
  %1865 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1862, <8 x i32> %1863) #8
  %1866 = shufflevector <16 x i16> %1703, <16 x i16> %1723, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1867 = shufflevector <16 x i16> %1703, <16 x i16> %1723, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1868 = bitcast <8 x i32> %161 to <16 x i16>
  %1869 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1866, <16 x i16> %1868) #8
  %1870 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1867, <16 x i16> %1868) #8
  %1871 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1866, <16 x i16> %1752) #8
  %1872 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1867, <16 x i16> %1752) #8
  %1873 = add <8 x i32> %1869, %10
  %1874 = add <8 x i32> %1870, %10
  %1875 = add <8 x i32> %1871, %10
  %1876 = add <8 x i32> %1872, %10
  %1877 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1873, i32 %4) #8
  %1878 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1874, i32 %4) #8
  %1879 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1875, i32 %4) #8
  %1880 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1876, i32 %4) #8
  %1881 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1877, <8 x i32> %1878) #8
  %1882 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1879, <8 x i32> %1880) #8
  %1883 = shufflevector <16 x i16> %1707, <16 x i16> %1719, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1884 = shufflevector <16 x i16> %1707, <16 x i16> %1719, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1885 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1883, <16 x i16> %1770) #8
  %1886 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1884, <16 x i16> %1770) #8
  %1887 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1883, <16 x i16> %1767) #8
  %1888 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1884, <16 x i16> %1767) #8
  %1889 = add <8 x i32> %1885, %10
  %1890 = add <8 x i32> %1886, %10
  %1891 = add <8 x i32> %1887, %10
  %1892 = add <8 x i32> %1888, %10
  %1893 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1889, i32 %4) #8
  %1894 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1890, i32 %4) #8
  %1895 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1891, i32 %4) #8
  %1896 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1892, i32 %4) #8
  %1897 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1893, <8 x i32> %1894) #8
  %1898 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1895, <8 x i32> %1896) #8
  %1899 = shufflevector <16 x i16> %1708, <16 x i16> %1720, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1900 = shufflevector <16 x i16> %1708, <16 x i16> %1720, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1901 = bitcast <8 x i32> %167 to <16 x i16>
  %1902 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1899, <16 x i16> %1901) #8
  %1903 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1900, <16 x i16> %1901) #8
  %1904 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1899, <16 x i16> %1770) #8
  %1905 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1900, <16 x i16> %1770) #8
  %1906 = add <8 x i32> %1902, %10
  %1907 = add <8 x i32> %1903, %10
  %1908 = add <8 x i32> %1904, %10
  %1909 = add <8 x i32> %1905, %10
  %1910 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1906, i32 %4) #8
  %1911 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1907, i32 %4) #8
  %1912 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1908, i32 %4) #8
  %1913 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1909, i32 %4) #8
  %1914 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1910, <8 x i32> %1911) #8
  %1915 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1912, <8 x i32> %1913) #8
  %1916 = shufflevector <16 x i16> %1712, <16 x i16> %1716, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1917 = shufflevector <16 x i16> %1712, <16 x i16> %1716, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1918 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1916, <16 x i16> %1788) #8
  %1919 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1917, <16 x i16> %1788) #8
  %1920 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1916, <16 x i16> %1785) #8
  %1921 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1917, <16 x i16> %1785) #8
  %1922 = add <8 x i32> %1918, %10
  %1923 = add <8 x i32> %1919, %10
  %1924 = add <8 x i32> %1920, %10
  %1925 = add <8 x i32> %1921, %10
  %1926 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1922, i32 %4) #8
  %1927 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1923, i32 %4) #8
  %1928 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1924, i32 %4) #8
  %1929 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1925, i32 %4) #8
  %1930 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1926, <8 x i32> %1927) #8
  %1931 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1928, <8 x i32> %1929) #8
  %1932 = shufflevector <16 x i16> %1711, <16 x i16> %1715, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1933 = shufflevector <16 x i16> %1711, <16 x i16> %1715, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1934 = bitcast <8 x i32> %173 to <16 x i16>
  %1935 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1932, <16 x i16> %1934) #8
  %1936 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1933, <16 x i16> %1934) #8
  %1937 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1932, <16 x i16> %1788) #8
  %1938 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1933, <16 x i16> %1788) #8
  %1939 = add <8 x i32> %1935, %10
  %1940 = add <8 x i32> %1936, %10
  %1941 = add <8 x i32> %1937, %10
  %1942 = add <8 x i32> %1938, %10
  %1943 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1939, i32 %4) #8
  %1944 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1940, i32 %4) #8
  %1945 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1941, i32 %4) #8
  %1946 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1942, i32 %4) #8
  %1947 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1943, <8 x i32> %1944) #8
  %1948 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1945, <8 x i32> %1946) #8
  %1949 = shufflevector <16 x i16> %1801, <16 x i16> %1815, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1950 = shufflevector <16 x i16> %1801, <16 x i16> %1815, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1951 = bitcast <8 x i32> %182 to <16 x i16>
  %1952 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1949, <16 x i16> %1951) #8
  %1953 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1950, <16 x i16> %1951) #8
  %1954 = bitcast <8 x i32> %188 to <16 x i16>
  %1955 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1949, <16 x i16> %1954) #8
  %1956 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1950, <16 x i16> %1954) #8
  %1957 = add <8 x i32> %1952, %10
  %1958 = add <8 x i32> %1953, %10
  %1959 = add <8 x i32> %1955, %10
  %1960 = add <8 x i32> %1956, %10
  %1961 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1957, i32 %4) #8
  %1962 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1958, i32 %4) #8
  %1963 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1959, i32 %4) #8
  %1964 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1960, i32 %4) #8
  %1965 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1961, <8 x i32> %1962) #8
  %1966 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1963, <8 x i32> %1964) #8
  %1967 = shufflevector <16 x i16> %1802, <16 x i16> %1816, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1968 = shufflevector <16 x i16> %1802, <16 x i16> %1816, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1969 = bitcast <8 x i32> %197 to <16 x i16>
  %1970 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1967, <16 x i16> %1969) #8
  %1971 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1968, <16 x i16> %1969) #8
  %1972 = bitcast <8 x i32> %203 to <16 x i16>
  %1973 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1967, <16 x i16> %1972) #8
  %1974 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1968, <16 x i16> %1972) #8
  %1975 = add <8 x i32> %1970, %10
  %1976 = add <8 x i32> %1971, %10
  %1977 = add <8 x i32> %1973, %10
  %1978 = add <8 x i32> %1974, %10
  %1979 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1975, i32 %4) #8
  %1980 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1976, i32 %4) #8
  %1981 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1977, i32 %4) #8
  %1982 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1978, i32 %4) #8
  %1983 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1979, <8 x i32> %1980) #8
  %1984 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1981, <8 x i32> %1982) #8
  %1985 = shufflevector <16 x i16> %1804, <16 x i16> %1814, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1986 = shufflevector <16 x i16> %1804, <16 x i16> %1814, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1987 = bitcast <8 x i32> %212 to <16 x i16>
  %1988 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1985, <16 x i16> %1987) #8
  %1989 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1986, <16 x i16> %1987) #8
  %1990 = bitcast <8 x i32> %218 to <16 x i16>
  %1991 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1985, <16 x i16> %1990) #8
  %1992 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1986, <16 x i16> %1990) #8
  %1993 = add <8 x i32> %1988, %10
  %1994 = add <8 x i32> %1989, %10
  %1995 = add <8 x i32> %1991, %10
  %1996 = add <8 x i32> %1992, %10
  %1997 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1993, i32 %4) #8
  %1998 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1994, i32 %4) #8
  %1999 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1995, i32 %4) #8
  %2000 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1996, i32 %4) #8
  %2001 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1997, <8 x i32> %1998) #8
  %2002 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1999, <8 x i32> %2000) #8
  %2003 = shufflevector <16 x i16> %1803, <16 x i16> %1813, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %2004 = shufflevector <16 x i16> %1803, <16 x i16> %1813, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %2005 = bitcast <8 x i32> %227 to <16 x i16>
  %2006 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2003, <16 x i16> %2005) #8
  %2007 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2004, <16 x i16> %2005) #8
  %2008 = bitcast <8 x i32> %233 to <16 x i16>
  %2009 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2003, <16 x i16> %2008) #8
  %2010 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2004, <16 x i16> %2008) #8
  %2011 = add <8 x i32> %2006, %10
  %2012 = add <8 x i32> %2007, %10
  %2013 = add <8 x i32> %2009, %10
  %2014 = add <8 x i32> %2010, %10
  %2015 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2011, i32 %4) #8
  %2016 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2012, i32 %4) #8
  %2017 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2013, i32 %4) #8
  %2018 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2014, i32 %4) #8
  %2019 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2015, <8 x i32> %2016) #8
  %2020 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2017, <8 x i32> %2018) #8
  %2021 = shufflevector <16 x i16> %1805, <16 x i16> %1811, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %2022 = shufflevector <16 x i16> %1805, <16 x i16> %1811, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %2023 = bitcast <8 x i32> %242 to <16 x i16>
  %2024 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2021, <16 x i16> %2023) #8
  %2025 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2022, <16 x i16> %2023) #8
  %2026 = bitcast <8 x i32> %248 to <16 x i16>
  %2027 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2021, <16 x i16> %2026) #8
  %2028 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2022, <16 x i16> %2026) #8
  %2029 = add <8 x i32> %2024, %10
  %2030 = add <8 x i32> %2025, %10
  %2031 = add <8 x i32> %2027, %10
  %2032 = add <8 x i32> %2028, %10
  %2033 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2029, i32 %4) #8
  %2034 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2030, i32 %4) #8
  %2035 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2031, i32 %4) #8
  %2036 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2032, i32 %4) #8
  %2037 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2033, <8 x i32> %2034) #8
  %2038 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2035, <8 x i32> %2036) #8
  %2039 = shufflevector <16 x i16> %1806, <16 x i16> %1812, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %2040 = shufflevector <16 x i16> %1806, <16 x i16> %1812, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %2041 = bitcast <8 x i32> %257 to <16 x i16>
  %2042 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2039, <16 x i16> %2041) #8
  %2043 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2040, <16 x i16> %2041) #8
  %2044 = bitcast <8 x i32> %263 to <16 x i16>
  %2045 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2039, <16 x i16> %2044) #8
  %2046 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2040, <16 x i16> %2044) #8
  %2047 = add <8 x i32> %2042, %10
  %2048 = add <8 x i32> %2043, %10
  %2049 = add <8 x i32> %2045, %10
  %2050 = add <8 x i32> %2046, %10
  %2051 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2047, i32 %4) #8
  %2052 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2048, i32 %4) #8
  %2053 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2049, i32 %4) #8
  %2054 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2050, i32 %4) #8
  %2055 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2051, <8 x i32> %2052) #8
  %2056 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2053, <8 x i32> %2054) #8
  %2057 = shufflevector <16 x i16> %1808, <16 x i16> %1810, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %2058 = shufflevector <16 x i16> %1808, <16 x i16> %1810, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %2059 = bitcast <8 x i32> %272 to <16 x i16>
  %2060 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2057, <16 x i16> %2059) #8
  %2061 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2058, <16 x i16> %2059) #8
  %2062 = bitcast <8 x i32> %278 to <16 x i16>
  %2063 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2057, <16 x i16> %2062) #8
  %2064 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2058, <16 x i16> %2062) #8
  %2065 = add <8 x i32> %2060, %10
  %2066 = add <8 x i32> %2061, %10
  %2067 = add <8 x i32> %2063, %10
  %2068 = add <8 x i32> %2064, %10
  %2069 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2065, i32 %4) #8
  %2070 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2066, i32 %4) #8
  %2071 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2067, i32 %4) #8
  %2072 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2068, i32 %4) #8
  %2073 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2069, <8 x i32> %2070) #8
  %2074 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2071, <8 x i32> %2072) #8
  %2075 = shufflevector <16 x i16> %1807, <16 x i16> %1809, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %2076 = shufflevector <16 x i16> %1807, <16 x i16> %1809, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %2077 = bitcast <8 x i32> %287 to <16 x i16>
  %2078 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2075, <16 x i16> %2077) #8
  %2079 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2076, <16 x i16> %2077) #8
  %2080 = bitcast <8 x i32> %293 to <16 x i16>
  %2081 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2075, <16 x i16> %2080) #8
  %2082 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2076, <16 x i16> %2080) #8
  %2083 = add <8 x i32> %2078, %10
  %2084 = add <8 x i32> %2079, %10
  %2085 = add <8 x i32> %2081, %10
  %2086 = add <8 x i32> %2082, %10
  %2087 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2083, i32 %4) #8
  %2088 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2084, i32 %4) #8
  %2089 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2085, i32 %4) #8
  %2090 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2086, i32 %4) #8
  %2091 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2087, <8 x i32> %2088) #8
  %2092 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2089, <8 x i32> %2090) #8
  %2093 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1697, <16 x i16> %1831) #8
  %2094 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1697, <16 x i16> %1831) #8
  %2095 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1698, <16 x i16> %1848) #8
  %2096 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1698, <16 x i16> %1848) #8
  %2097 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1702, <16 x i16> %1864) #8
  %2098 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1702, <16 x i16> %1864) #8
  %2099 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1701, <16 x i16> %1881) #8
  %2100 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1701, <16 x i16> %1881) #8
  %2101 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1705, <16 x i16> %1897) #8
  %2102 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1705, <16 x i16> %1897) #8
  %2103 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1706, <16 x i16> %1914) #8
  %2104 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1706, <16 x i16> %1914) #8
  %2105 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1710, <16 x i16> %1930) #8
  %2106 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1710, <16 x i16> %1930) #8
  %2107 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1709, <16 x i16> %1947) #8
  %2108 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1709, <16 x i16> %1947) #8
  %2109 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1713, <16 x i16> %1948) #8
  %2110 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1713, <16 x i16> %1948) #8
  %2111 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1714, <16 x i16> %1931) #8
  %2112 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1714, <16 x i16> %1931) #8
  %2113 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1718, <16 x i16> %1915) #8
  %2114 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1718, <16 x i16> %1915) #8
  %2115 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1717, <16 x i16> %1898) #8
  %2116 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1717, <16 x i16> %1898) #8
  %2117 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1721, <16 x i16> %1882) #8
  %2118 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1721, <16 x i16> %1882) #8
  %2119 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1722, <16 x i16> %1865) #8
  %2120 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1722, <16 x i16> %1865) #8
  %2121 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1726, <16 x i16> %1849) #8
  %2122 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1726, <16 x i16> %1849) #8
  %2123 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1725, <16 x i16> %1832) #8
  %2124 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1725, <16 x i16> %1832) #8
  %2125 = shufflevector <16 x i16> %2093, <16 x i16> %2123, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %2126 = shufflevector <16 x i16> %2093, <16 x i16> %2123, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %2127 = bitcast <8 x i32> %302 to <16 x i16>
  %2128 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2125, <16 x i16> %2127) #8
  %2129 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2126, <16 x i16> %2127) #8
  %2130 = bitcast <8 x i32> %308 to <16 x i16>
  %2131 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2125, <16 x i16> %2130) #8
  %2132 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2126, <16 x i16> %2130) #8
  %2133 = add <8 x i32> %2128, %10
  %2134 = add <8 x i32> %2129, %10
  %2135 = add <8 x i32> %2131, %10
  %2136 = add <8 x i32> %2132, %10
  %2137 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2133, i32 %4) #8
  %2138 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2134, i32 %4) #8
  %2139 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2135, i32 %4) #8
  %2140 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2136, i32 %4) #8
  %2141 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2137, <8 x i32> %2138) #8
  %2142 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2139, <8 x i32> %2140) #8
  %2143 = shufflevector <16 x i16> %2094, <16 x i16> %2124, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %2144 = shufflevector <16 x i16> %2094, <16 x i16> %2124, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %2145 = bitcast <8 x i32> %317 to <16 x i16>
  %2146 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2143, <16 x i16> %2145) #8
  %2147 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2144, <16 x i16> %2145) #8
  %2148 = bitcast <8 x i32> %323 to <16 x i16>
  %2149 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2143, <16 x i16> %2148) #8
  %2150 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2144, <16 x i16> %2148) #8
  %2151 = add <8 x i32> %2146, %10
  %2152 = add <8 x i32> %2147, %10
  %2153 = add <8 x i32> %2149, %10
  %2154 = add <8 x i32> %2150, %10
  %2155 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2151, i32 %4) #8
  %2156 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2152, i32 %4) #8
  %2157 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2153, i32 %4) #8
  %2158 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2154, i32 %4) #8
  %2159 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2155, <8 x i32> %2156) #8
  %2160 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2157, <8 x i32> %2158) #8
  %2161 = shufflevector <16 x i16> %2096, <16 x i16> %2122, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %2162 = shufflevector <16 x i16> %2096, <16 x i16> %2122, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %2163 = bitcast <8 x i32> %332 to <16 x i16>
  %2164 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2161, <16 x i16> %2163) #8
  %2165 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2162, <16 x i16> %2163) #8
  %2166 = bitcast <8 x i32> %338 to <16 x i16>
  %2167 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2161, <16 x i16> %2166) #8
  %2168 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2162, <16 x i16> %2166) #8
  %2169 = add <8 x i32> %2164, %10
  %2170 = add <8 x i32> %2165, %10
  %2171 = add <8 x i32> %2167, %10
  %2172 = add <8 x i32> %2168, %10
  %2173 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2169, i32 %4) #8
  %2174 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2170, i32 %4) #8
  %2175 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2171, i32 %4) #8
  %2176 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2172, i32 %4) #8
  %2177 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2173, <8 x i32> %2174) #8
  %2178 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2175, <8 x i32> %2176) #8
  %2179 = shufflevector <16 x i16> %2095, <16 x i16> %2121, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %2180 = shufflevector <16 x i16> %2095, <16 x i16> %2121, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %2181 = bitcast <8 x i32> %347 to <16 x i16>
  %2182 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2179, <16 x i16> %2181) #8
  %2183 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2180, <16 x i16> %2181) #8
  %2184 = bitcast <8 x i32> %353 to <16 x i16>
  %2185 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2179, <16 x i16> %2184) #8
  %2186 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2180, <16 x i16> %2184) #8
  %2187 = add <8 x i32> %2182, %10
  %2188 = add <8 x i32> %2183, %10
  %2189 = add <8 x i32> %2185, %10
  %2190 = add <8 x i32> %2186, %10
  %2191 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2187, i32 %4) #8
  %2192 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2188, i32 %4) #8
  %2193 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2189, i32 %4) #8
  %2194 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2190, i32 %4) #8
  %2195 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2191, <8 x i32> %2192) #8
  %2196 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2193, <8 x i32> %2194) #8
  %2197 = shufflevector <16 x i16> %2097, <16 x i16> %2119, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %2198 = shufflevector <16 x i16> %2097, <16 x i16> %2119, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %2199 = bitcast <8 x i32> %362 to <16 x i16>
  %2200 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2197, <16 x i16> %2199) #8
  %2201 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2198, <16 x i16> %2199) #8
  %2202 = bitcast <8 x i32> %368 to <16 x i16>
  %2203 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2197, <16 x i16> %2202) #8
  %2204 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2198, <16 x i16> %2202) #8
  %2205 = add <8 x i32> %2200, %10
  %2206 = add <8 x i32> %2201, %10
  %2207 = add <8 x i32> %2203, %10
  %2208 = add <8 x i32> %2204, %10
  %2209 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2205, i32 %4) #8
  %2210 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2206, i32 %4) #8
  %2211 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2207, i32 %4) #8
  %2212 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2208, i32 %4) #8
  %2213 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2209, <8 x i32> %2210) #8
  %2214 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2211, <8 x i32> %2212) #8
  %2215 = shufflevector <16 x i16> %2098, <16 x i16> %2120, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %2216 = shufflevector <16 x i16> %2098, <16 x i16> %2120, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %2217 = bitcast <8 x i32> %377 to <16 x i16>
  %2218 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2215, <16 x i16> %2217) #8
  %2219 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2216, <16 x i16> %2217) #8
  %2220 = bitcast <8 x i32> %383 to <16 x i16>
  %2221 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2215, <16 x i16> %2220) #8
  %2222 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2216, <16 x i16> %2220) #8
  %2223 = add <8 x i32> %2218, %10
  %2224 = add <8 x i32> %2219, %10
  %2225 = add <8 x i32> %2221, %10
  %2226 = add <8 x i32> %2222, %10
  %2227 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2223, i32 %4) #8
  %2228 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2224, i32 %4) #8
  %2229 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2225, i32 %4) #8
  %2230 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2226, i32 %4) #8
  %2231 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2227, <8 x i32> %2228) #8
  %2232 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2229, <8 x i32> %2230) #8
  %2233 = shufflevector <16 x i16> %2100, <16 x i16> %2118, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %2234 = shufflevector <16 x i16> %2100, <16 x i16> %2118, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %2235 = bitcast <8 x i32> %392 to <16 x i16>
  %2236 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2233, <16 x i16> %2235) #8
  %2237 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2234, <16 x i16> %2235) #8
  %2238 = bitcast <8 x i32> %398 to <16 x i16>
  %2239 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2233, <16 x i16> %2238) #8
  %2240 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2234, <16 x i16> %2238) #8
  %2241 = add <8 x i32> %2236, %10
  %2242 = add <8 x i32> %2237, %10
  %2243 = add <8 x i32> %2239, %10
  %2244 = add <8 x i32> %2240, %10
  %2245 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2241, i32 %4) #8
  %2246 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2242, i32 %4) #8
  %2247 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2243, i32 %4) #8
  %2248 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2244, i32 %4) #8
  %2249 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2245, <8 x i32> %2246) #8
  %2250 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2247, <8 x i32> %2248) #8
  %2251 = shufflevector <16 x i16> %2099, <16 x i16> %2117, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %2252 = shufflevector <16 x i16> %2099, <16 x i16> %2117, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %2253 = bitcast <8 x i32> %407 to <16 x i16>
  %2254 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2251, <16 x i16> %2253) #8
  %2255 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2252, <16 x i16> %2253) #8
  %2256 = bitcast <8 x i32> %413 to <16 x i16>
  %2257 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2251, <16 x i16> %2256) #8
  %2258 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2252, <16 x i16> %2256) #8
  %2259 = add <8 x i32> %2254, %10
  %2260 = add <8 x i32> %2255, %10
  %2261 = add <8 x i32> %2257, %10
  %2262 = add <8 x i32> %2258, %10
  %2263 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2259, i32 %4) #8
  %2264 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2260, i32 %4) #8
  %2265 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2261, i32 %4) #8
  %2266 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2262, i32 %4) #8
  %2267 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2263, <8 x i32> %2264) #8
  %2268 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2265, <8 x i32> %2266) #8
  %2269 = shufflevector <16 x i16> %2101, <16 x i16> %2115, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %2270 = shufflevector <16 x i16> %2101, <16 x i16> %2115, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %2271 = bitcast <8 x i32> %422 to <16 x i16>
  %2272 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2269, <16 x i16> %2271) #8
  %2273 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2270, <16 x i16> %2271) #8
  %2274 = bitcast <8 x i32> %428 to <16 x i16>
  %2275 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2269, <16 x i16> %2274) #8
  %2276 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2270, <16 x i16> %2274) #8
  %2277 = add <8 x i32> %2272, %10
  %2278 = add <8 x i32> %2273, %10
  %2279 = add <8 x i32> %2275, %10
  %2280 = add <8 x i32> %2276, %10
  %2281 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2277, i32 %4) #8
  %2282 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2278, i32 %4) #8
  %2283 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2279, i32 %4) #8
  %2284 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2280, i32 %4) #8
  %2285 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2281, <8 x i32> %2282) #8
  %2286 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2283, <8 x i32> %2284) #8
  %2287 = shufflevector <16 x i16> %2102, <16 x i16> %2116, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %2288 = shufflevector <16 x i16> %2102, <16 x i16> %2116, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %2289 = bitcast <8 x i32> %437 to <16 x i16>
  %2290 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2287, <16 x i16> %2289) #8
  %2291 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2288, <16 x i16> %2289) #8
  %2292 = bitcast <8 x i32> %443 to <16 x i16>
  %2293 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2287, <16 x i16> %2292) #8
  %2294 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2288, <16 x i16> %2292) #8
  %2295 = add <8 x i32> %2290, %10
  %2296 = add <8 x i32> %2291, %10
  %2297 = add <8 x i32> %2293, %10
  %2298 = add <8 x i32> %2294, %10
  %2299 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2295, i32 %4) #8
  %2300 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2296, i32 %4) #8
  %2301 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2297, i32 %4) #8
  %2302 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2298, i32 %4) #8
  %2303 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2299, <8 x i32> %2300) #8
  %2304 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2301, <8 x i32> %2302) #8
  %2305 = shufflevector <16 x i16> %2104, <16 x i16> %2114, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %2306 = shufflevector <16 x i16> %2104, <16 x i16> %2114, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %2307 = bitcast <8 x i32> %452 to <16 x i16>
  %2308 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2305, <16 x i16> %2307) #8
  %2309 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2306, <16 x i16> %2307) #8
  %2310 = bitcast <8 x i32> %458 to <16 x i16>
  %2311 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2305, <16 x i16> %2310) #8
  %2312 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2306, <16 x i16> %2310) #8
  %2313 = add <8 x i32> %2308, %10
  %2314 = add <8 x i32> %2309, %10
  %2315 = add <8 x i32> %2311, %10
  %2316 = add <8 x i32> %2312, %10
  %2317 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2313, i32 %4) #8
  %2318 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2314, i32 %4) #8
  %2319 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2315, i32 %4) #8
  %2320 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2316, i32 %4) #8
  %2321 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2317, <8 x i32> %2318) #8
  %2322 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2319, <8 x i32> %2320) #8
  %2323 = shufflevector <16 x i16> %2103, <16 x i16> %2113, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %2324 = shufflevector <16 x i16> %2103, <16 x i16> %2113, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %2325 = bitcast <8 x i32> %467 to <16 x i16>
  %2326 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2323, <16 x i16> %2325) #8
  %2327 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2324, <16 x i16> %2325) #8
  %2328 = bitcast <8 x i32> %473 to <16 x i16>
  %2329 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2323, <16 x i16> %2328) #8
  %2330 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2324, <16 x i16> %2328) #8
  %2331 = add <8 x i32> %2326, %10
  %2332 = add <8 x i32> %2327, %10
  %2333 = add <8 x i32> %2329, %10
  %2334 = add <8 x i32> %2330, %10
  %2335 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2331, i32 %4) #8
  %2336 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2332, i32 %4) #8
  %2337 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2333, i32 %4) #8
  %2338 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2334, i32 %4) #8
  %2339 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2335, <8 x i32> %2336) #8
  %2340 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2337, <8 x i32> %2338) #8
  %2341 = shufflevector <16 x i16> %2105, <16 x i16> %2111, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %2342 = shufflevector <16 x i16> %2105, <16 x i16> %2111, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %2343 = bitcast <8 x i32> %482 to <16 x i16>
  %2344 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2341, <16 x i16> %2343) #8
  %2345 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2342, <16 x i16> %2343) #8
  %2346 = bitcast <8 x i32> %488 to <16 x i16>
  %2347 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2341, <16 x i16> %2346) #8
  %2348 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2342, <16 x i16> %2346) #8
  %2349 = add <8 x i32> %2344, %10
  %2350 = add <8 x i32> %2345, %10
  %2351 = add <8 x i32> %2347, %10
  %2352 = add <8 x i32> %2348, %10
  %2353 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2349, i32 %4) #8
  %2354 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2350, i32 %4) #8
  %2355 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2351, i32 %4) #8
  %2356 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2352, i32 %4) #8
  %2357 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2353, <8 x i32> %2354) #8
  %2358 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2355, <8 x i32> %2356) #8
  %2359 = shufflevector <16 x i16> %2106, <16 x i16> %2112, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %2360 = shufflevector <16 x i16> %2106, <16 x i16> %2112, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %2361 = bitcast <8 x i32> %497 to <16 x i16>
  %2362 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2359, <16 x i16> %2361) #8
  %2363 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2360, <16 x i16> %2361) #8
  %2364 = bitcast <8 x i32> %503 to <16 x i16>
  %2365 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2359, <16 x i16> %2364) #8
  %2366 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2360, <16 x i16> %2364) #8
  %2367 = add <8 x i32> %2362, %10
  %2368 = add <8 x i32> %2363, %10
  %2369 = add <8 x i32> %2365, %10
  %2370 = add <8 x i32> %2366, %10
  %2371 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2367, i32 %4) #8
  %2372 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2368, i32 %4) #8
  %2373 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2369, i32 %4) #8
  %2374 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2370, i32 %4) #8
  %2375 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2371, <8 x i32> %2372) #8
  %2376 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2373, <8 x i32> %2374) #8
  %2377 = shufflevector <16 x i16> %2108, <16 x i16> %2110, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %2378 = shufflevector <16 x i16> %2108, <16 x i16> %2110, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %2379 = bitcast <8 x i32> %512 to <16 x i16>
  %2380 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2377, <16 x i16> %2379) #8
  %2381 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2378, <16 x i16> %2379) #8
  %2382 = bitcast <8 x i32> %518 to <16 x i16>
  %2383 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2377, <16 x i16> %2382) #8
  %2384 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2378, <16 x i16> %2382) #8
  %2385 = add <8 x i32> %2380, %10
  %2386 = add <8 x i32> %2381, %10
  %2387 = add <8 x i32> %2383, %10
  %2388 = add <8 x i32> %2384, %10
  %2389 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2385, i32 %4) #8
  %2390 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2386, i32 %4) #8
  %2391 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2387, i32 %4) #8
  %2392 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2388, i32 %4) #8
  %2393 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2389, <8 x i32> %2390) #8
  %2394 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2391, <8 x i32> %2392) #8
  %2395 = shufflevector <16 x i16> %2107, <16 x i16> %2109, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %2396 = shufflevector <16 x i16> %2107, <16 x i16> %2109, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %2397 = bitcast <8 x i32> %527 to <16 x i16>
  %2398 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2395, <16 x i16> %2397) #8
  %2399 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2396, <16 x i16> %2397) #8
  %2400 = bitcast <8 x i32> %533 to <16 x i16>
  %2401 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2395, <16 x i16> %2400) #8
  %2402 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2396, <16 x i16> %2400) #8
  %2403 = add <8 x i32> %2398, %10
  %2404 = add <8 x i32> %2399, %10
  %2405 = add <8 x i32> %2401, %10
  %2406 = add <8 x i32> %2402, %10
  %2407 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2403, i32 %4) #8
  %2408 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2404, i32 %4) #8
  %2409 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2405, i32 %4) #8
  %2410 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2406, i32 %4) #8
  %2411 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2407, <8 x i32> %2408) #8
  %2412 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2409, <8 x i32> %2410) #8
  %2413 = bitcast <4 x i64>* %1 to <16 x i16>*
  store <16 x i16> %1389, <16 x i16>* %2413, align 32
  %2414 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %2415 = bitcast <4 x i64>* %2414 to <16 x i16>*
  store <16 x i16> %2141, <16 x i16>* %2415, align 32
  %2416 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %2417 = bitcast <4 x i64>* %2416 to <16 x i16>*
  store <16 x i16> %1965, <16 x i16>* %2417, align 32
  %2418 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %2419 = bitcast <4 x i64>* %2418 to <16 x i16>*
  store <16 x i16> %2412, <16 x i16>* %2419, align 32
  %2420 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %2421 = bitcast <4 x i64>* %2420 to <16 x i16>*
  store <16 x i16> %1745, <16 x i16>* %2421, align 32
  %2422 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %2423 = bitcast <4 x i64>* %2422 to <16 x i16>*
  store <16 x i16> %2285, <16 x i16>* %2423, align 32
  %2424 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %2425 = bitcast <4 x i64>* %2424 to <16 x i16>*
  store <16 x i16> %2092, <16 x i16>* %2425, align 32
  %2426 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %2427 = bitcast <4 x i64>* %2426 to <16 x i16>*
  store <16 x i16> %2268, <16 x i16>* %2427, align 32
  %2428 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %2429 = bitcast <4 x i64>* %2428 to <16 x i16>*
  store <16 x i16> %1607, <16 x i16>* %2429, align 32
  %2430 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %2431 = bitcast <4 x i64>* %2430 to <16 x i16>*
  store <16 x i16> %2213, <16 x i16>* %2431, align 32
  %2432 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %2433 = bitcast <4 x i64>* %2432 to <16 x i16>*
  store <16 x i16> %2037, <16 x i16>* %2433, align 32
  %2434 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %2435 = bitcast <4 x i64>* %2434 to <16 x i16>*
  store <16 x i16> %2340, <16 x i16>* %2435, align 32
  %2436 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %2437 = bitcast <4 x i64>* %2436 to <16 x i16>*
  store <16 x i16> %1800, <16 x i16>* %2437, align 32
  %2438 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %2439 = bitcast <4 x i64>* %2438 to <16 x i16>*
  store <16 x i16> %2357, <16 x i16>* %2439, align 32
  %2440 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %2441 = bitcast <4 x i64>* %2440 to <16 x i16>*
  store <16 x i16> %2020, <16 x i16>* %2441, align 32
  %2442 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 15
  %2443 = bitcast <4 x i64>* %2442 to <16 x i16>*
  store <16 x i16> %2196, <16 x i16>* %2443, align 32
  %2444 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 16
  %2445 = bitcast <4 x i64>* %2444 to <16 x i16>*
  store <16 x i16> %1405, <16 x i16>* %2445, align 32
  %2446 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 17
  %2447 = bitcast <4 x i64>* %2446 to <16 x i16>*
  store <16 x i16> %2177, <16 x i16>* %2447, align 32
  %2448 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 18
  %2449 = bitcast <4 x i64>* %2448 to <16 x i16>*
  store <16 x i16> %2001, <16 x i16>* %2449, align 32
  %2450 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 19
  %2451 = bitcast <4 x i64>* %2450 to <16 x i16>*
  store <16 x i16> %2376, <16 x i16>* %2451, align 32
  %2452 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 20
  %2453 = bitcast <4 x i64>* %2452 to <16 x i16>*
  store <16 x i16> %1781, <16 x i16>* %2453, align 32
  %2454 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 21
  %2455 = bitcast <4 x i64>* %2454 to <16 x i16>*
  store <16 x i16> %2321, <16 x i16>* %2455, align 32
  %2456 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 22
  %2457 = bitcast <4 x i64>* %2456 to <16 x i16>*
  store <16 x i16> %2056, <16 x i16>* %2457, align 32
  %2458 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 23
  %2459 = bitcast <4 x i64>* %2458 to <16 x i16>*
  store <16 x i16> %2232, <16 x i16>* %2459, align 32
  %2460 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 24
  %2461 = bitcast <4 x i64>* %2460 to <16 x i16>*
  store <16 x i16> %1624, <16 x i16>* %2461, align 32
  %2462 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 25
  %2463 = bitcast <4 x i64>* %2462 to <16 x i16>*
  store <16 x i16> %2249, <16 x i16>* %2463, align 32
  %2464 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 26
  %2465 = bitcast <4 x i64>* %2464 to <16 x i16>*
  store <16 x i16> %2073, <16 x i16>* %2465, align 32
  %2466 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 27
  %2467 = bitcast <4 x i64>* %2466 to <16 x i16>*
  store <16 x i16> %2304, <16 x i16>* %2467, align 32
  %2468 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 28
  %2469 = bitcast <4 x i64>* %2468 to <16 x i16>*
  store <16 x i16> %1764, <16 x i16>* %2469, align 32
  %2470 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 29
  %2471 = bitcast <4 x i64>* %2470 to <16 x i16>*
  store <16 x i16> %2393, <16 x i16>* %2471, align 32
  %2472 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 30
  %2473 = bitcast <4 x i64>* %2472 to <16 x i16>*
  store <16 x i16> %1984, <16 x i16>* %2473, align 32
  %2474 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 31
  %2475 = bitcast <4 x i64>* %2474 to <16 x i16>*
  store <16 x i16> %2160, <16 x i16>* %2475, align 32
  %2476 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 32
  %2477 = bitcast <4 x i64>* %2476 to <16 x i16>*
  store <16 x i16> %1390, <16 x i16>* %2477, align 32
  %2478 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 33
  %2479 = bitcast <4 x i64>* %2478 to <16 x i16>*
  store <16 x i16> %2159, <16 x i16>* %2479, align 32
  %2480 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 34
  %2481 = bitcast <4 x i64>* %2480 to <16 x i16>*
  store <16 x i16> %1983, <16 x i16>* %2481, align 32
  %2482 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 35
  %2483 = bitcast <4 x i64>* %2482 to <16 x i16>*
  store <16 x i16> %2394, <16 x i16>* %2483, align 32
  %2484 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 36
  %2485 = bitcast <4 x i64>* %2484 to <16 x i16>*
  store <16 x i16> %1763, <16 x i16>* %2485, align 32
  %2486 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 37
  %2487 = bitcast <4 x i64>* %2486 to <16 x i16>*
  store <16 x i16> %2303, <16 x i16>* %2487, align 32
  %2488 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 38
  %2489 = bitcast <4 x i64>* %2488 to <16 x i16>*
  store <16 x i16> %2074, <16 x i16>* %2489, align 32
  %2490 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 39
  %2491 = bitcast <4 x i64>* %2490 to <16 x i16>*
  store <16 x i16> %2250, <16 x i16>* %2491, align 32
  %2492 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 40
  %2493 = bitcast <4 x i64>* %2492 to <16 x i16>*
  store <16 x i16> %1623, <16 x i16>* %2493, align 32
  %2494 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 41
  %2495 = bitcast <4 x i64>* %2494 to <16 x i16>*
  store <16 x i16> %2231, <16 x i16>* %2495, align 32
  %2496 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 42
  %2497 = bitcast <4 x i64>* %2496 to <16 x i16>*
  store <16 x i16> %2055, <16 x i16>* %2497, align 32
  %2498 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 43
  %2499 = bitcast <4 x i64>* %2498 to <16 x i16>*
  store <16 x i16> %2322, <16 x i16>* %2499, align 32
  %2500 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 44
  %2501 = bitcast <4 x i64>* %2500 to <16 x i16>*
  store <16 x i16> %1782, <16 x i16>* %2501, align 32
  %2502 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 45
  %2503 = bitcast <4 x i64>* %2502 to <16 x i16>*
  store <16 x i16> %2375, <16 x i16>* %2503, align 32
  %2504 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 46
  %2505 = bitcast <4 x i64>* %2504 to <16 x i16>*
  store <16 x i16> %2002, <16 x i16>* %2505, align 32
  %2506 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 47
  %2507 = bitcast <4 x i64>* %2506 to <16 x i16>*
  store <16 x i16> %2178, <16 x i16>* %2507, align 32
  %2508 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 48
  %2509 = bitcast <4 x i64>* %2508 to <16 x i16>*
  store <16 x i16> %1406, <16 x i16>* %2509, align 32
  %2510 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 49
  %2511 = bitcast <4 x i64>* %2510 to <16 x i16>*
  store <16 x i16> %2195, <16 x i16>* %2511, align 32
  %2512 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 50
  %2513 = bitcast <4 x i64>* %2512 to <16 x i16>*
  store <16 x i16> %2019, <16 x i16>* %2513, align 32
  %2514 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 51
  %2515 = bitcast <4 x i64>* %2514 to <16 x i16>*
  store <16 x i16> %2358, <16 x i16>* %2515, align 32
  %2516 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 52
  %2517 = bitcast <4 x i64>* %2516 to <16 x i16>*
  store <16 x i16> %1799, <16 x i16>* %2517, align 32
  %2518 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 53
  %2519 = bitcast <4 x i64>* %2518 to <16 x i16>*
  store <16 x i16> %2339, <16 x i16>* %2519, align 32
  %2520 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 54
  %2521 = bitcast <4 x i64>* %2520 to <16 x i16>*
  store <16 x i16> %2038, <16 x i16>* %2521, align 32
  %2522 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 55
  %2523 = bitcast <4 x i64>* %2522 to <16 x i16>*
  store <16 x i16> %2214, <16 x i16>* %2523, align 32
  %2524 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 56
  %2525 = bitcast <4 x i64>* %2524 to <16 x i16>*
  store <16 x i16> %1608, <16 x i16>* %2525, align 32
  %2526 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 57
  %2527 = bitcast <4 x i64>* %2526 to <16 x i16>*
  store <16 x i16> %2267, <16 x i16>* %2527, align 32
  %2528 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 58
  %2529 = bitcast <4 x i64>* %2528 to <16 x i16>*
  store <16 x i16> %2091, <16 x i16>* %2529, align 32
  %2530 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 59
  %2531 = bitcast <4 x i64>* %2530 to <16 x i16>*
  store <16 x i16> %2286, <16 x i16>* %2531, align 32
  %2532 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 60
  %2533 = bitcast <4 x i64>* %2532 to <16 x i16>*
  store <16 x i16> %1746, <16 x i16>* %2533, align 32
  %2534 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 61
  %2535 = bitcast <4 x i64>* %2534 to <16 x i16>*
  store <16 x i16> %2411, <16 x i16>* %2535, align 32
  %2536 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 62
  %2537 = bitcast <4 x i64>* %2536 to <16 x i16>*
  store <16 x i16> %1966, <16 x i16>* %2537, align 32
  %2538 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 63
  %2539 = bitcast <4 x i64>* %2538 to <16 x i16>*
  store <16 x i16> %2142, <16 x i16>* %2539, align 32
  ret void
}

; Function Attrs: inlinehint nounwind ssp uwtable
define internal fastcc void @fdct64_new_avx2(<4 x i64>* readonly, <4 x i64>*, i8 signext) unnamed_addr #4 {
  %4 = sext i8 %2 to i32
  %5 = add nsw i32 %4, -10
  %6 = sext i32 %5 to i64
  %7 = add nsw i32 %4, -1
  %8 = shl i32 1, %7
  %9 = insertelement <8 x i32> undef, i32 %8, i32 0
  %10 = shufflevector <8 x i32> %9, <8 x i32> undef, <8 x i32> zeroinitializer
  %11 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 32
  %12 = load i32, i32* %11, align 16
  %13 = sub nsw i32 0, %12
  %14 = insertelement <8 x i32> undef, i32 %13, i32 0
  %15 = shufflevector <8 x i32> %14, <8 x i32> undef, <8 x i32> zeroinitializer
  %16 = insertelement <8 x i32> undef, i32 %12, i32 0
  %17 = shufflevector <8 x i32> %16, <8 x i32> undef, <8 x i32> zeroinitializer
  %18 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 16
  %19 = load i32, i32* %18, align 16
  %20 = sub nsw i32 0, %19
  %21 = insertelement <8 x i32> undef, i32 %20, i32 0
  %22 = shufflevector <8 x i32> %21, <8 x i32> undef, <8 x i32> zeroinitializer
  %23 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 48
  %24 = load i32, i32* %23, align 16
  %25 = insertelement <8 x i32> undef, i32 %24, i32 0
  %26 = shufflevector <8 x i32> %25, <8 x i32> undef, <8 x i32> zeroinitializer
  %27 = sub nsw i32 0, %24
  %28 = insertelement <8 x i32> undef, i32 %27, i32 0
  %29 = shufflevector <8 x i32> %28, <8 x i32> undef, <8 x i32> zeroinitializer
  %30 = insertelement <8 x i32> undef, i32 %19, i32 0
  %31 = shufflevector <8 x i32> %30, <8 x i32> undef, <8 x i32> zeroinitializer
  %32 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 8
  %33 = load i32, i32* %32, align 16
  %34 = sub nsw i32 0, %33
  %35 = insertelement <8 x i32> undef, i32 %34, i32 0
  %36 = shufflevector <8 x i32> %35, <8 x i32> undef, <8 x i32> zeroinitializer
  %37 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 56
  %38 = load i32, i32* %37, align 16
  %39 = insertelement <8 x i32> undef, i32 %38, i32 0
  %40 = shufflevector <8 x i32> %39, <8 x i32> undef, <8 x i32> zeroinitializer
  %41 = sub nsw i32 0, %38
  %42 = insertelement <8 x i32> undef, i32 %41, i32 0
  %43 = shufflevector <8 x i32> %42, <8 x i32> undef, <8 x i32> zeroinitializer
  %44 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 40
  %45 = load i32, i32* %44, align 16
  %46 = sub nsw i32 0, %45
  %47 = insertelement <8 x i32> undef, i32 %46, i32 0
  %48 = shufflevector <8 x i32> %47, <8 x i32> undef, <8 x i32> zeroinitializer
  %49 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 24
  %50 = load i32, i32* %49, align 16
  %51 = insertelement <8 x i32> undef, i32 %50, i32 0
  %52 = shufflevector <8 x i32> %51, <8 x i32> undef, <8 x i32> zeroinitializer
  %53 = sub nsw i32 0, %50
  %54 = insertelement <8 x i32> undef, i32 %53, i32 0
  %55 = shufflevector <8 x i32> %54, <8 x i32> undef, <8 x i32> zeroinitializer
  %56 = insertelement <8 x i32> undef, i32 %33, i32 0
  %57 = shufflevector <8 x i32> %56, <8 x i32> undef, <8 x i32> zeroinitializer
  %58 = insertelement <8 x i32> undef, i32 %45, i32 0
  %59 = shufflevector <8 x i32> %58, <8 x i32> undef, <8 x i32> zeroinitializer
  %60 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 60
  %61 = load i32, i32* %60, align 16
  %62 = insertelement <8 x i32> undef, i32 %61, i32 0
  %63 = shufflevector <8 x i32> %62, <8 x i32> undef, <8 x i32> zeroinitializer
  %64 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 4
  %65 = load i32, i32* %64, align 16
  %66 = insertelement <8 x i32> undef, i32 %65, i32 0
  %67 = shufflevector <8 x i32> %66, <8 x i32> undef, <8 x i32> zeroinitializer
  %68 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 28
  %69 = load i32, i32* %68, align 16
  %70 = insertelement <8 x i32> undef, i32 %69, i32 0
  %71 = shufflevector <8 x i32> %70, <8 x i32> undef, <8 x i32> zeroinitializer
  %72 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 36
  %73 = load i32, i32* %72, align 16
  %74 = insertelement <8 x i32> undef, i32 %73, i32 0
  %75 = shufflevector <8 x i32> %74, <8 x i32> undef, <8 x i32> zeroinitializer
  %76 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 44
  %77 = load i32, i32* %76, align 16
  %78 = insertelement <8 x i32> undef, i32 %77, i32 0
  %79 = shufflevector <8 x i32> %78, <8 x i32> undef, <8 x i32> zeroinitializer
  %80 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 20
  %81 = load i32, i32* %80, align 16
  %82 = insertelement <8 x i32> undef, i32 %81, i32 0
  %83 = shufflevector <8 x i32> %82, <8 x i32> undef, <8 x i32> zeroinitializer
  %84 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 12
  %85 = load i32, i32* %84, align 16
  %86 = insertelement <8 x i32> undef, i32 %85, i32 0
  %87 = shufflevector <8 x i32> %86, <8 x i32> undef, <8 x i32> zeroinitializer
  %88 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 52
  %89 = load i32, i32* %88, align 16
  %90 = insertelement <8 x i32> undef, i32 %89, i32 0
  %91 = shufflevector <8 x i32> %90, <8 x i32> undef, <8 x i32> zeroinitializer
  %92 = sub nsw i32 0, %65
  %93 = insertelement <8 x i32> undef, i32 %92, i32 0
  %94 = shufflevector <8 x i32> %93, <8 x i32> undef, <8 x i32> zeroinitializer
  %95 = sub nsw i32 0, %61
  %96 = insertelement <8 x i32> undef, i32 %95, i32 0
  %97 = shufflevector <8 x i32> %96, <8 x i32> undef, <8 x i32> zeroinitializer
  %98 = sub nsw i32 0, %73
  %99 = insertelement <8 x i32> undef, i32 %98, i32 0
  %100 = shufflevector <8 x i32> %99, <8 x i32> undef, <8 x i32> zeroinitializer
  %101 = sub nsw i32 0, %69
  %102 = insertelement <8 x i32> undef, i32 %101, i32 0
  %103 = shufflevector <8 x i32> %102, <8 x i32> undef, <8 x i32> zeroinitializer
  %104 = sub nsw i32 0, %81
  %105 = insertelement <8 x i32> undef, i32 %104, i32 0
  %106 = shufflevector <8 x i32> %105, <8 x i32> undef, <8 x i32> zeroinitializer
  %107 = sub nsw i32 0, %77
  %108 = insertelement <8 x i32> undef, i32 %107, i32 0
  %109 = shufflevector <8 x i32> %108, <8 x i32> undef, <8 x i32> zeroinitializer
  %110 = sub nsw i32 0, %89
  %111 = insertelement <8 x i32> undef, i32 %110, i32 0
  %112 = shufflevector <8 x i32> %111, <8 x i32> undef, <8 x i32> zeroinitializer
  %113 = sub nsw i32 0, %85
  %114 = insertelement <8 x i32> undef, i32 %113, i32 0
  %115 = shufflevector <8 x i32> %114, <8 x i32> undef, <8 x i32> zeroinitializer
  %116 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 62
  %117 = load i32, i32* %116, align 8
  %118 = insertelement <8 x i32> undef, i32 %117, i32 0
  %119 = shufflevector <8 x i32> %118, <8 x i32> undef, <8 x i32> zeroinitializer
  %120 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 2
  %121 = load i32, i32* %120, align 8
  %122 = insertelement <8 x i32> undef, i32 %121, i32 0
  %123 = shufflevector <8 x i32> %122, <8 x i32> undef, <8 x i32> zeroinitializer
  %124 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 30
  %125 = load i32, i32* %124, align 8
  %126 = insertelement <8 x i32> undef, i32 %125, i32 0
  %127 = shufflevector <8 x i32> %126, <8 x i32> undef, <8 x i32> zeroinitializer
  %128 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 34
  %129 = load i32, i32* %128, align 8
  %130 = insertelement <8 x i32> undef, i32 %129, i32 0
  %131 = shufflevector <8 x i32> %130, <8 x i32> undef, <8 x i32> zeroinitializer
  %132 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 46
  %133 = load i32, i32* %132, align 8
  %134 = insertelement <8 x i32> undef, i32 %133, i32 0
  %135 = shufflevector <8 x i32> %134, <8 x i32> undef, <8 x i32> zeroinitializer
  %136 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 18
  %137 = load i32, i32* %136, align 8
  %138 = insertelement <8 x i32> undef, i32 %137, i32 0
  %139 = shufflevector <8 x i32> %138, <8 x i32> undef, <8 x i32> zeroinitializer
  %140 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 14
  %141 = load i32, i32* %140, align 8
  %142 = insertelement <8 x i32> undef, i32 %141, i32 0
  %143 = shufflevector <8 x i32> %142, <8 x i32> undef, <8 x i32> zeroinitializer
  %144 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 50
  %145 = load i32, i32* %144, align 8
  %146 = insertelement <8 x i32> undef, i32 %145, i32 0
  %147 = shufflevector <8 x i32> %146, <8 x i32> undef, <8 x i32> zeroinitializer
  %148 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 54
  %149 = load i32, i32* %148, align 8
  %150 = insertelement <8 x i32> undef, i32 %149, i32 0
  %151 = shufflevector <8 x i32> %150, <8 x i32> undef, <8 x i32> zeroinitializer
  %152 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 10
  %153 = load i32, i32* %152, align 8
  %154 = insertelement <8 x i32> undef, i32 %153, i32 0
  %155 = shufflevector <8 x i32> %154, <8 x i32> undef, <8 x i32> zeroinitializer
  %156 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 22
  %157 = load i32, i32* %156, align 8
  %158 = insertelement <8 x i32> undef, i32 %157, i32 0
  %159 = shufflevector <8 x i32> %158, <8 x i32> undef, <8 x i32> zeroinitializer
  %160 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 42
  %161 = load i32, i32* %160, align 8
  %162 = insertelement <8 x i32> undef, i32 %161, i32 0
  %163 = shufflevector <8 x i32> %162, <8 x i32> undef, <8 x i32> zeroinitializer
  %164 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 38
  %165 = load i32, i32* %164, align 8
  %166 = insertelement <8 x i32> undef, i32 %165, i32 0
  %167 = shufflevector <8 x i32> %166, <8 x i32> undef, <8 x i32> zeroinitializer
  %168 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 26
  %169 = load i32, i32* %168, align 8
  %170 = insertelement <8 x i32> undef, i32 %169, i32 0
  %171 = shufflevector <8 x i32> %170, <8 x i32> undef, <8 x i32> zeroinitializer
  %172 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 6
  %173 = load i32, i32* %172, align 8
  %174 = insertelement <8 x i32> undef, i32 %173, i32 0
  %175 = shufflevector <8 x i32> %174, <8 x i32> undef, <8 x i32> zeroinitializer
  %176 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 58
  %177 = load i32, i32* %176, align 8
  %178 = insertelement <8 x i32> undef, i32 %177, i32 0
  %179 = shufflevector <8 x i32> %178, <8 x i32> undef, <8 x i32> zeroinitializer
  %180 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 63
  %181 = load i32, i32* %180, align 4
  %182 = insertelement <8 x i32> undef, i32 %181, i32 0
  %183 = shufflevector <8 x i32> %182, <8 x i32> undef, <8 x i32> zeroinitializer
  %184 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 1
  %185 = load i32, i32* %184, align 4
  %186 = insertelement <8 x i32> undef, i32 %185, i32 0
  %187 = shufflevector <8 x i32> %186, <8 x i32> undef, <8 x i32> zeroinitializer
  %188 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 31
  %189 = load i32, i32* %188, align 4
  %190 = insertelement <8 x i32> undef, i32 %189, i32 0
  %191 = shufflevector <8 x i32> %190, <8 x i32> undef, <8 x i32> zeroinitializer
  %192 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 33
  %193 = load i32, i32* %192, align 4
  %194 = insertelement <8 x i32> undef, i32 %193, i32 0
  %195 = shufflevector <8 x i32> %194, <8 x i32> undef, <8 x i32> zeroinitializer
  %196 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 47
  %197 = load i32, i32* %196, align 4
  %198 = insertelement <8 x i32> undef, i32 %197, i32 0
  %199 = shufflevector <8 x i32> %198, <8 x i32> undef, <8 x i32> zeroinitializer
  %200 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 17
  %201 = load i32, i32* %200, align 4
  %202 = insertelement <8 x i32> undef, i32 %201, i32 0
  %203 = shufflevector <8 x i32> %202, <8 x i32> undef, <8 x i32> zeroinitializer
  %204 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 15
  %205 = load i32, i32* %204, align 4
  %206 = insertelement <8 x i32> undef, i32 %205, i32 0
  %207 = shufflevector <8 x i32> %206, <8 x i32> undef, <8 x i32> zeroinitializer
  %208 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 49
  %209 = load i32, i32* %208, align 4
  %210 = insertelement <8 x i32> undef, i32 %209, i32 0
  %211 = shufflevector <8 x i32> %210, <8 x i32> undef, <8 x i32> zeroinitializer
  %212 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 55
  %213 = load i32, i32* %212, align 4
  %214 = insertelement <8 x i32> undef, i32 %213, i32 0
  %215 = shufflevector <8 x i32> %214, <8 x i32> undef, <8 x i32> zeroinitializer
  %216 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 9
  %217 = load i32, i32* %216, align 4
  %218 = insertelement <8 x i32> undef, i32 %217, i32 0
  %219 = shufflevector <8 x i32> %218, <8 x i32> undef, <8 x i32> zeroinitializer
  %220 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 23
  %221 = load i32, i32* %220, align 4
  %222 = insertelement <8 x i32> undef, i32 %221, i32 0
  %223 = shufflevector <8 x i32> %222, <8 x i32> undef, <8 x i32> zeroinitializer
  %224 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 41
  %225 = load i32, i32* %224, align 4
  %226 = insertelement <8 x i32> undef, i32 %225, i32 0
  %227 = shufflevector <8 x i32> %226, <8 x i32> undef, <8 x i32> zeroinitializer
  %228 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 39
  %229 = load i32, i32* %228, align 4
  %230 = insertelement <8 x i32> undef, i32 %229, i32 0
  %231 = shufflevector <8 x i32> %230, <8 x i32> undef, <8 x i32> zeroinitializer
  %232 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 25
  %233 = load i32, i32* %232, align 4
  %234 = insertelement <8 x i32> undef, i32 %233, i32 0
  %235 = shufflevector <8 x i32> %234, <8 x i32> undef, <8 x i32> zeroinitializer
  %236 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 7
  %237 = load i32, i32* %236, align 4
  %238 = insertelement <8 x i32> undef, i32 %237, i32 0
  %239 = shufflevector <8 x i32> %238, <8 x i32> undef, <8 x i32> zeroinitializer
  %240 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 57
  %241 = load i32, i32* %240, align 4
  %242 = insertelement <8 x i32> undef, i32 %241, i32 0
  %243 = shufflevector <8 x i32> %242, <8 x i32> undef, <8 x i32> zeroinitializer
  %244 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 59
  %245 = load i32, i32* %244, align 4
  %246 = insertelement <8 x i32> undef, i32 %245, i32 0
  %247 = shufflevector <8 x i32> %246, <8 x i32> undef, <8 x i32> zeroinitializer
  %248 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 5
  %249 = load i32, i32* %248, align 4
  %250 = insertelement <8 x i32> undef, i32 %249, i32 0
  %251 = shufflevector <8 x i32> %250, <8 x i32> undef, <8 x i32> zeroinitializer
  %252 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 27
  %253 = load i32, i32* %252, align 4
  %254 = insertelement <8 x i32> undef, i32 %253, i32 0
  %255 = shufflevector <8 x i32> %254, <8 x i32> undef, <8 x i32> zeroinitializer
  %256 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 37
  %257 = load i32, i32* %256, align 4
  %258 = insertelement <8 x i32> undef, i32 %257, i32 0
  %259 = shufflevector <8 x i32> %258, <8 x i32> undef, <8 x i32> zeroinitializer
  %260 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 43
  %261 = load i32, i32* %260, align 4
  %262 = insertelement <8 x i32> undef, i32 %261, i32 0
  %263 = shufflevector <8 x i32> %262, <8 x i32> undef, <8 x i32> zeroinitializer
  %264 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 21
  %265 = load i32, i32* %264, align 4
  %266 = insertelement <8 x i32> undef, i32 %265, i32 0
  %267 = shufflevector <8 x i32> %266, <8 x i32> undef, <8 x i32> zeroinitializer
  %268 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 11
  %269 = load i32, i32* %268, align 4
  %270 = insertelement <8 x i32> undef, i32 %269, i32 0
  %271 = shufflevector <8 x i32> %270, <8 x i32> undef, <8 x i32> zeroinitializer
  %272 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 53
  %273 = load i32, i32* %272, align 4
  %274 = insertelement <8 x i32> undef, i32 %273, i32 0
  %275 = shufflevector <8 x i32> %274, <8 x i32> undef, <8 x i32> zeroinitializer
  %276 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 51
  %277 = load i32, i32* %276, align 4
  %278 = insertelement <8 x i32> undef, i32 %277, i32 0
  %279 = shufflevector <8 x i32> %278, <8 x i32> undef, <8 x i32> zeroinitializer
  %280 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 13
  %281 = load i32, i32* %280, align 4
  %282 = insertelement <8 x i32> undef, i32 %281, i32 0
  %283 = shufflevector <8 x i32> %282, <8 x i32> undef, <8 x i32> zeroinitializer
  %284 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 19
  %285 = load i32, i32* %284, align 4
  %286 = insertelement <8 x i32> undef, i32 %285, i32 0
  %287 = shufflevector <8 x i32> %286, <8 x i32> undef, <8 x i32> zeroinitializer
  %288 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 45
  %289 = load i32, i32* %288, align 4
  %290 = insertelement <8 x i32> undef, i32 %289, i32 0
  %291 = shufflevector <8 x i32> %290, <8 x i32> undef, <8 x i32> zeroinitializer
  %292 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 35
  %293 = load i32, i32* %292, align 4
  %294 = insertelement <8 x i32> undef, i32 %293, i32 0
  %295 = shufflevector <8 x i32> %294, <8 x i32> undef, <8 x i32> zeroinitializer
  %296 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 29
  %297 = load i32, i32* %296, align 4
  %298 = insertelement <8 x i32> undef, i32 %297, i32 0
  %299 = shufflevector <8 x i32> %298, <8 x i32> undef, <8 x i32> zeroinitializer
  %300 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 3
  %301 = load i32, i32* %300, align 4
  %302 = insertelement <8 x i32> undef, i32 %301, i32 0
  %303 = shufflevector <8 x i32> %302, <8 x i32> undef, <8 x i32> zeroinitializer
  %304 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 61
  %305 = load i32, i32* %304, align 4
  %306 = insertelement <8 x i32> undef, i32 %305, i32 0
  %307 = shufflevector <8 x i32> %306, <8 x i32> undef, <8 x i32> zeroinitializer
  %308 = bitcast <4 x i64>* %0 to <8 x i32>*
  %309 = load <8 x i32>, <8 x i32>* %308, align 32
  %310 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 63
  %311 = bitcast <4 x i64>* %310 to <8 x i32>*
  %312 = load <8 x i32>, <8 x i32>* %311, align 32
  %313 = add <8 x i32> %312, %309
  %314 = sub <8 x i32> %309, %312
  %315 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 1
  %316 = bitcast <4 x i64>* %315 to <8 x i32>*
  %317 = load <8 x i32>, <8 x i32>* %316, align 32
  %318 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 62
  %319 = bitcast <4 x i64>* %318 to <8 x i32>*
  %320 = load <8 x i32>, <8 x i32>* %319, align 32
  %321 = add <8 x i32> %320, %317
  %322 = sub <8 x i32> %317, %320
  %323 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 2
  %324 = bitcast <4 x i64>* %323 to <8 x i32>*
  %325 = load <8 x i32>, <8 x i32>* %324, align 32
  %326 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 61
  %327 = bitcast <4 x i64>* %326 to <8 x i32>*
  %328 = load <8 x i32>, <8 x i32>* %327, align 32
  %329 = add <8 x i32> %328, %325
  %330 = sub <8 x i32> %325, %328
  %331 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 3
  %332 = bitcast <4 x i64>* %331 to <8 x i32>*
  %333 = load <8 x i32>, <8 x i32>* %332, align 32
  %334 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 60
  %335 = bitcast <4 x i64>* %334 to <8 x i32>*
  %336 = load <8 x i32>, <8 x i32>* %335, align 32
  %337 = add <8 x i32> %336, %333
  %338 = sub <8 x i32> %333, %336
  %339 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 4
  %340 = bitcast <4 x i64>* %339 to <8 x i32>*
  %341 = load <8 x i32>, <8 x i32>* %340, align 32
  %342 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 59
  %343 = bitcast <4 x i64>* %342 to <8 x i32>*
  %344 = load <8 x i32>, <8 x i32>* %343, align 32
  %345 = add <8 x i32> %344, %341
  %346 = sub <8 x i32> %341, %344
  %347 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 5
  %348 = bitcast <4 x i64>* %347 to <8 x i32>*
  %349 = load <8 x i32>, <8 x i32>* %348, align 32
  %350 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 58
  %351 = bitcast <4 x i64>* %350 to <8 x i32>*
  %352 = load <8 x i32>, <8 x i32>* %351, align 32
  %353 = add <8 x i32> %352, %349
  %354 = sub <8 x i32> %349, %352
  %355 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 6
  %356 = bitcast <4 x i64>* %355 to <8 x i32>*
  %357 = load <8 x i32>, <8 x i32>* %356, align 32
  %358 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 57
  %359 = bitcast <4 x i64>* %358 to <8 x i32>*
  %360 = load <8 x i32>, <8 x i32>* %359, align 32
  %361 = add <8 x i32> %360, %357
  %362 = sub <8 x i32> %357, %360
  %363 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 7
  %364 = bitcast <4 x i64>* %363 to <8 x i32>*
  %365 = load <8 x i32>, <8 x i32>* %364, align 32
  %366 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 56
  %367 = bitcast <4 x i64>* %366 to <8 x i32>*
  %368 = load <8 x i32>, <8 x i32>* %367, align 32
  %369 = add <8 x i32> %368, %365
  %370 = sub <8 x i32> %365, %368
  %371 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 8
  %372 = bitcast <4 x i64>* %371 to <8 x i32>*
  %373 = load <8 x i32>, <8 x i32>* %372, align 32
  %374 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 55
  %375 = bitcast <4 x i64>* %374 to <8 x i32>*
  %376 = load <8 x i32>, <8 x i32>* %375, align 32
  %377 = add <8 x i32> %376, %373
  %378 = sub <8 x i32> %373, %376
  %379 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 9
  %380 = bitcast <4 x i64>* %379 to <8 x i32>*
  %381 = load <8 x i32>, <8 x i32>* %380, align 32
  %382 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 54
  %383 = bitcast <4 x i64>* %382 to <8 x i32>*
  %384 = load <8 x i32>, <8 x i32>* %383, align 32
  %385 = add <8 x i32> %384, %381
  %386 = sub <8 x i32> %381, %384
  %387 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 10
  %388 = bitcast <4 x i64>* %387 to <8 x i32>*
  %389 = load <8 x i32>, <8 x i32>* %388, align 32
  %390 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 53
  %391 = bitcast <4 x i64>* %390 to <8 x i32>*
  %392 = load <8 x i32>, <8 x i32>* %391, align 32
  %393 = add <8 x i32> %392, %389
  %394 = sub <8 x i32> %389, %392
  %395 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 11
  %396 = bitcast <4 x i64>* %395 to <8 x i32>*
  %397 = load <8 x i32>, <8 x i32>* %396, align 32
  %398 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 52
  %399 = bitcast <4 x i64>* %398 to <8 x i32>*
  %400 = load <8 x i32>, <8 x i32>* %399, align 32
  %401 = add <8 x i32> %400, %397
  %402 = sub <8 x i32> %397, %400
  %403 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 12
  %404 = bitcast <4 x i64>* %403 to <8 x i32>*
  %405 = load <8 x i32>, <8 x i32>* %404, align 32
  %406 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 51
  %407 = bitcast <4 x i64>* %406 to <8 x i32>*
  %408 = load <8 x i32>, <8 x i32>* %407, align 32
  %409 = add <8 x i32> %408, %405
  %410 = sub <8 x i32> %405, %408
  %411 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 13
  %412 = bitcast <4 x i64>* %411 to <8 x i32>*
  %413 = load <8 x i32>, <8 x i32>* %412, align 32
  %414 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 50
  %415 = bitcast <4 x i64>* %414 to <8 x i32>*
  %416 = load <8 x i32>, <8 x i32>* %415, align 32
  %417 = add <8 x i32> %416, %413
  %418 = sub <8 x i32> %413, %416
  %419 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 14
  %420 = bitcast <4 x i64>* %419 to <8 x i32>*
  %421 = load <8 x i32>, <8 x i32>* %420, align 32
  %422 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 49
  %423 = bitcast <4 x i64>* %422 to <8 x i32>*
  %424 = load <8 x i32>, <8 x i32>* %423, align 32
  %425 = add <8 x i32> %424, %421
  %426 = sub <8 x i32> %421, %424
  %427 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 15
  %428 = bitcast <4 x i64>* %427 to <8 x i32>*
  %429 = load <8 x i32>, <8 x i32>* %428, align 32
  %430 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 48
  %431 = bitcast <4 x i64>* %430 to <8 x i32>*
  %432 = load <8 x i32>, <8 x i32>* %431, align 32
  %433 = add <8 x i32> %432, %429
  %434 = sub <8 x i32> %429, %432
  %435 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 16
  %436 = bitcast <4 x i64>* %435 to <8 x i32>*
  %437 = load <8 x i32>, <8 x i32>* %436, align 32
  %438 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 47
  %439 = bitcast <4 x i64>* %438 to <8 x i32>*
  %440 = load <8 x i32>, <8 x i32>* %439, align 32
  %441 = add <8 x i32> %440, %437
  %442 = sub <8 x i32> %437, %440
  %443 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 17
  %444 = bitcast <4 x i64>* %443 to <8 x i32>*
  %445 = load <8 x i32>, <8 x i32>* %444, align 32
  %446 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 46
  %447 = bitcast <4 x i64>* %446 to <8 x i32>*
  %448 = load <8 x i32>, <8 x i32>* %447, align 32
  %449 = add <8 x i32> %448, %445
  %450 = sub <8 x i32> %445, %448
  %451 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 18
  %452 = bitcast <4 x i64>* %451 to <8 x i32>*
  %453 = load <8 x i32>, <8 x i32>* %452, align 32
  %454 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 45
  %455 = bitcast <4 x i64>* %454 to <8 x i32>*
  %456 = load <8 x i32>, <8 x i32>* %455, align 32
  %457 = add <8 x i32> %456, %453
  %458 = sub <8 x i32> %453, %456
  %459 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 19
  %460 = bitcast <4 x i64>* %459 to <8 x i32>*
  %461 = load <8 x i32>, <8 x i32>* %460, align 32
  %462 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 44
  %463 = bitcast <4 x i64>* %462 to <8 x i32>*
  %464 = load <8 x i32>, <8 x i32>* %463, align 32
  %465 = add <8 x i32> %464, %461
  %466 = sub <8 x i32> %461, %464
  %467 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 20
  %468 = bitcast <4 x i64>* %467 to <8 x i32>*
  %469 = load <8 x i32>, <8 x i32>* %468, align 32
  %470 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 43
  %471 = bitcast <4 x i64>* %470 to <8 x i32>*
  %472 = load <8 x i32>, <8 x i32>* %471, align 32
  %473 = add <8 x i32> %472, %469
  %474 = sub <8 x i32> %469, %472
  %475 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 21
  %476 = bitcast <4 x i64>* %475 to <8 x i32>*
  %477 = load <8 x i32>, <8 x i32>* %476, align 32
  %478 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 42
  %479 = bitcast <4 x i64>* %478 to <8 x i32>*
  %480 = load <8 x i32>, <8 x i32>* %479, align 32
  %481 = add <8 x i32> %480, %477
  %482 = sub <8 x i32> %477, %480
  %483 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 22
  %484 = bitcast <4 x i64>* %483 to <8 x i32>*
  %485 = load <8 x i32>, <8 x i32>* %484, align 32
  %486 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 41
  %487 = bitcast <4 x i64>* %486 to <8 x i32>*
  %488 = load <8 x i32>, <8 x i32>* %487, align 32
  %489 = add <8 x i32> %488, %485
  %490 = sub <8 x i32> %485, %488
  %491 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 23
  %492 = bitcast <4 x i64>* %491 to <8 x i32>*
  %493 = load <8 x i32>, <8 x i32>* %492, align 32
  %494 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 40
  %495 = bitcast <4 x i64>* %494 to <8 x i32>*
  %496 = load <8 x i32>, <8 x i32>* %495, align 32
  %497 = add <8 x i32> %496, %493
  %498 = sub <8 x i32> %493, %496
  %499 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 24
  %500 = bitcast <4 x i64>* %499 to <8 x i32>*
  %501 = load <8 x i32>, <8 x i32>* %500, align 32
  %502 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 39
  %503 = bitcast <4 x i64>* %502 to <8 x i32>*
  %504 = load <8 x i32>, <8 x i32>* %503, align 32
  %505 = add <8 x i32> %504, %501
  %506 = sub <8 x i32> %501, %504
  %507 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 25
  %508 = bitcast <4 x i64>* %507 to <8 x i32>*
  %509 = load <8 x i32>, <8 x i32>* %508, align 32
  %510 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 38
  %511 = bitcast <4 x i64>* %510 to <8 x i32>*
  %512 = load <8 x i32>, <8 x i32>* %511, align 32
  %513 = add <8 x i32> %512, %509
  %514 = sub <8 x i32> %509, %512
  %515 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 26
  %516 = bitcast <4 x i64>* %515 to <8 x i32>*
  %517 = load <8 x i32>, <8 x i32>* %516, align 32
  %518 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 37
  %519 = bitcast <4 x i64>* %518 to <8 x i32>*
  %520 = load <8 x i32>, <8 x i32>* %519, align 32
  %521 = add <8 x i32> %520, %517
  %522 = sub <8 x i32> %517, %520
  %523 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 27
  %524 = bitcast <4 x i64>* %523 to <8 x i32>*
  %525 = load <8 x i32>, <8 x i32>* %524, align 32
  %526 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 36
  %527 = bitcast <4 x i64>* %526 to <8 x i32>*
  %528 = load <8 x i32>, <8 x i32>* %527, align 32
  %529 = add <8 x i32> %528, %525
  %530 = sub <8 x i32> %525, %528
  %531 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 28
  %532 = bitcast <4 x i64>* %531 to <8 x i32>*
  %533 = load <8 x i32>, <8 x i32>* %532, align 32
  %534 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 35
  %535 = bitcast <4 x i64>* %534 to <8 x i32>*
  %536 = load <8 x i32>, <8 x i32>* %535, align 32
  %537 = add <8 x i32> %536, %533
  %538 = sub <8 x i32> %533, %536
  %539 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 29
  %540 = bitcast <4 x i64>* %539 to <8 x i32>*
  %541 = load <8 x i32>, <8 x i32>* %540, align 32
  %542 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 34
  %543 = bitcast <4 x i64>* %542 to <8 x i32>*
  %544 = load <8 x i32>, <8 x i32>* %543, align 32
  %545 = add <8 x i32> %544, %541
  %546 = sub <8 x i32> %541, %544
  %547 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 30
  %548 = bitcast <4 x i64>* %547 to <8 x i32>*
  %549 = load <8 x i32>, <8 x i32>* %548, align 32
  %550 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 33
  %551 = bitcast <4 x i64>* %550 to <8 x i32>*
  %552 = load <8 x i32>, <8 x i32>* %551, align 32
  %553 = add <8 x i32> %552, %549
  %554 = sub <8 x i32> %549, %552
  %555 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 31
  %556 = bitcast <4 x i64>* %555 to <8 x i32>*
  %557 = load <8 x i32>, <8 x i32>* %556, align 32
  %558 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 32
  %559 = bitcast <4 x i64>* %558 to <8 x i32>*
  %560 = load <8 x i32>, <8 x i32>* %559, align 32
  %561 = add <8 x i32> %560, %557
  %562 = sub <8 x i32> %557, %560
  %563 = add <8 x i32> %561, %313
  %564 = sub <8 x i32> %313, %561
  %565 = add <8 x i32> %553, %321
  %566 = sub <8 x i32> %321, %553
  %567 = add <8 x i32> %545, %329
  %568 = sub <8 x i32> %329, %545
  %569 = add <8 x i32> %537, %337
  %570 = sub <8 x i32> %337, %537
  %571 = add <8 x i32> %529, %345
  %572 = sub <8 x i32> %345, %529
  %573 = add <8 x i32> %521, %353
  %574 = sub <8 x i32> %353, %521
  %575 = add <8 x i32> %513, %361
  %576 = sub <8 x i32> %361, %513
  %577 = add <8 x i32> %505, %369
  %578 = sub <8 x i32> %369, %505
  %579 = add <8 x i32> %497, %377
  %580 = sub <8 x i32> %377, %497
  %581 = add <8 x i32> %489, %385
  %582 = sub <8 x i32> %385, %489
  %583 = add <8 x i32> %481, %393
  %584 = sub <8 x i32> %393, %481
  %585 = add <8 x i32> %473, %401
  %586 = sub <8 x i32> %401, %473
  %587 = add <8 x i32> %465, %409
  %588 = sub <8 x i32> %409, %465
  %589 = add <8 x i32> %457, %417
  %590 = sub <8 x i32> %417, %457
  %591 = add <8 x i32> %449, %425
  %592 = sub <8 x i32> %425, %449
  %593 = add <8 x i32> %441, %433
  %594 = sub <8 x i32> %433, %441
  %595 = mul <8 x i32> %498, %15
  %596 = mul <8 x i32> %378, %17
  %597 = add <8 x i32> %596, %10
  %598 = add <8 x i32> %597, %595
  %599 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %598, i32 %4) #8
  %600 = mul <8 x i32> %498, %17
  %601 = mul <8 x i32> %15, %378
  %602 = sub <8 x i32> %10, %601
  %603 = add <8 x i32> %602, %600
  %604 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %603, i32 %4) #8
  %605 = mul <8 x i32> %490, %15
  %606 = mul <8 x i32> %386, %17
  %607 = add <8 x i32> %606, %10
  %608 = add <8 x i32> %607, %605
  %609 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %608, i32 %4) #8
  %610 = mul <8 x i32> %490, %17
  %611 = mul <8 x i32> %15, %386
  %612 = sub <8 x i32> %10, %611
  %613 = add <8 x i32> %612, %610
  %614 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %613, i32 %4) #8
  %615 = mul <8 x i32> %482, %15
  %616 = mul <8 x i32> %394, %17
  %617 = add <8 x i32> %616, %10
  %618 = add <8 x i32> %617, %615
  %619 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %618, i32 %4) #8
  %620 = mul <8 x i32> %482, %17
  %621 = mul <8 x i32> %15, %394
  %622 = sub <8 x i32> %10, %621
  %623 = add <8 x i32> %622, %620
  %624 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %623, i32 %4) #8
  %625 = mul <8 x i32> %474, %15
  %626 = mul <8 x i32> %402, %17
  %627 = add <8 x i32> %626, %10
  %628 = add <8 x i32> %627, %625
  %629 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %628, i32 %4) #8
  %630 = mul <8 x i32> %474, %17
  %631 = mul <8 x i32> %15, %402
  %632 = sub <8 x i32> %10, %631
  %633 = add <8 x i32> %632, %630
  %634 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %633, i32 %4) #8
  %635 = mul <8 x i32> %466, %15
  %636 = mul <8 x i32> %410, %17
  %637 = add <8 x i32> %636, %10
  %638 = add <8 x i32> %637, %635
  %639 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %638, i32 %4) #8
  %640 = mul <8 x i32> %466, %17
  %641 = mul <8 x i32> %15, %410
  %642 = sub <8 x i32> %10, %641
  %643 = add <8 x i32> %642, %640
  %644 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %643, i32 %4) #8
  %645 = mul <8 x i32> %458, %15
  %646 = mul <8 x i32> %418, %17
  %647 = add <8 x i32> %646, %10
  %648 = add <8 x i32> %647, %645
  %649 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %648, i32 %4) #8
  %650 = mul <8 x i32> %458, %17
  %651 = mul <8 x i32> %15, %418
  %652 = sub <8 x i32> %10, %651
  %653 = add <8 x i32> %652, %650
  %654 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %653, i32 %4) #8
  %655 = mul <8 x i32> %450, %15
  %656 = mul <8 x i32> %426, %17
  %657 = add <8 x i32> %656, %10
  %658 = add <8 x i32> %657, %655
  %659 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %658, i32 %4) #8
  %660 = mul <8 x i32> %450, %17
  %661 = mul <8 x i32> %15, %426
  %662 = sub <8 x i32> %10, %661
  %663 = add <8 x i32> %662, %660
  %664 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %663, i32 %4) #8
  %665 = mul <8 x i32> %442, %15
  %666 = mul <8 x i32> %434, %17
  %667 = add <8 x i32> %666, %10
  %668 = add <8 x i32> %667, %665
  %669 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %668, i32 %4) #8
  %670 = mul <8 x i32> %442, %17
  %671 = mul <8 x i32> %15, %434
  %672 = sub <8 x i32> %10, %671
  %673 = add <8 x i32> %672, %670
  %674 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %673, i32 %4) #8
  %675 = add <8 x i32> %563, %593
  %676 = sub <8 x i32> %563, %593
  %677 = add <8 x i32> %565, %591
  %678 = sub <8 x i32> %565, %591
  %679 = add <8 x i32> %567, %589
  %680 = sub <8 x i32> %567, %589
  %681 = add <8 x i32> %569, %587
  %682 = sub <8 x i32> %569, %587
  %683 = add <8 x i32> %571, %585
  %684 = sub <8 x i32> %571, %585
  %685 = add <8 x i32> %573, %583
  %686 = sub <8 x i32> %573, %583
  %687 = add <8 x i32> %575, %581
  %688 = sub <8 x i32> %575, %581
  %689 = add <8 x i32> %577, %579
  %690 = sub <8 x i32> %577, %579
  %691 = mul <8 x i32> %586, %15
  %692 = mul <8 x i32> %572, %17
  %693 = add <8 x i32> %691, %10
  %694 = add <8 x i32> %693, %692
  %695 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %694, i32 %4) #8
  %696 = mul <8 x i32> %586, %17
  %697 = add <8 x i32> %696, %10
  %698 = mul <8 x i32> %15, %572
  %699 = sub <8 x i32> %697, %698
  %700 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %699, i32 %4) #8
  %701 = mul <8 x i32> %584, %15
  %702 = mul <8 x i32> %574, %17
  %703 = add <8 x i32> %701, %10
  %704 = add <8 x i32> %703, %702
  %705 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %704, i32 %4) #8
  %706 = mul <8 x i32> %584, %17
  %707 = add <8 x i32> %706, %10
  %708 = mul <8 x i32> %15, %574
  %709 = sub <8 x i32> %707, %708
  %710 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %709, i32 %4) #8
  %711 = mul <8 x i32> %582, %15
  %712 = mul <8 x i32> %576, %17
  %713 = add <8 x i32> %711, %10
  %714 = add <8 x i32> %713, %712
  %715 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %714, i32 %4) #8
  %716 = mul <8 x i32> %582, %17
  %717 = add <8 x i32> %716, %10
  %718 = mul <8 x i32> %15, %576
  %719 = sub <8 x i32> %717, %718
  %720 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %719, i32 %4) #8
  %721 = mul <8 x i32> %580, %15
  %722 = mul <8 x i32> %578, %17
  %723 = add <8 x i32> %721, %10
  %724 = add <8 x i32> %723, %722
  %725 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %724, i32 %4) #8
  %726 = mul <8 x i32> %580, %17
  %727 = add <8 x i32> %726, %10
  %728 = mul <8 x i32> %15, %578
  %729 = sub <8 x i32> %727, %728
  %730 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %729, i32 %4) #8
  %731 = add <8 x i32> %669, %562
  %732 = sub <8 x i32> %562, %669
  %733 = add <8 x i32> %659, %554
  %734 = sub <8 x i32> %554, %659
  %735 = add <8 x i32> %649, %546
  %736 = sub <8 x i32> %546, %649
  %737 = add <8 x i32> %639, %538
  %738 = sub <8 x i32> %538, %639
  %739 = add <8 x i32> %629, %530
  %740 = sub <8 x i32> %530, %629
  %741 = add <8 x i32> %619, %522
  %742 = sub <8 x i32> %522, %619
  %743 = add <8 x i32> %609, %514
  %744 = sub <8 x i32> %514, %609
  %745 = add <8 x i32> %599, %506
  %746 = sub <8 x i32> %506, %599
  %747 = add <8 x i32> %674, %314
  %748 = sub <8 x i32> %314, %674
  %749 = add <8 x i32> %664, %322
  %750 = sub <8 x i32> %322, %664
  %751 = add <8 x i32> %654, %330
  %752 = sub <8 x i32> %330, %654
  %753 = add <8 x i32> %644, %338
  %754 = sub <8 x i32> %338, %644
  %755 = add <8 x i32> %634, %346
  %756 = sub <8 x i32> %346, %634
  %757 = add <8 x i32> %624, %354
  %758 = sub <8 x i32> %354, %624
  %759 = add <8 x i32> %614, %362
  %760 = sub <8 x i32> %362, %614
  %761 = add <8 x i32> %604, %370
  %762 = sub <8 x i32> %370, %604
  %763 = add <8 x i32> %675, %689
  %764 = sub <8 x i32> %675, %689
  %765 = add <8 x i32> %677, %687
  %766 = sub <8 x i32> %677, %687
  %767 = add <8 x i32> %679, %685
  %768 = sub <8 x i32> %679, %685
  %769 = add <8 x i32> %681, %683
  %770 = sub <8 x i32> %681, %683
  %771 = mul <8 x i32> %686, %15
  %772 = mul <8 x i32> %680, %17
  %773 = add <8 x i32> %771, %10
  %774 = add <8 x i32> %773, %772
  %775 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %774, i32 %4) #8
  %776 = mul <8 x i32> %686, %17
  %777 = add <8 x i32> %776, %10
  %778 = mul <8 x i32> %15, %680
  %779 = sub <8 x i32> %777, %778
  %780 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %779, i32 %4) #8
  %781 = mul <8 x i32> %684, %15
  %782 = mul <8 x i32> %682, %17
  %783 = add <8 x i32> %781, %10
  %784 = add <8 x i32> %783, %782
  %785 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %784, i32 %4) #8
  %786 = mul <8 x i32> %684, %17
  %787 = add <8 x i32> %786, %10
  %788 = mul <8 x i32> %15, %682
  %789 = sub <8 x i32> %787, %788
  %790 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %789, i32 %4) #8
  %791 = add <8 x i32> %725, %594
  %792 = sub <8 x i32> %594, %725
  %793 = add <8 x i32> %715, %592
  %794 = sub <8 x i32> %592, %715
  %795 = add <8 x i32> %705, %590
  %796 = sub <8 x i32> %590, %705
  %797 = add <8 x i32> %695, %588
  %798 = sub <8 x i32> %588, %695
  %799 = add <8 x i32> %730, %564
  %800 = sub <8 x i32> %564, %730
  %801 = add <8 x i32> %720, %566
  %802 = sub <8 x i32> %566, %720
  %803 = add <8 x i32> %710, %568
  %804 = sub <8 x i32> %568, %710
  %805 = add <8 x i32> %700, %570
  %806 = sub <8 x i32> %570, %700
  %807 = mul <8 x i32> %739, %22
  %808 = mul <8 x i32> %755, %26
  %809 = add <8 x i32> %807, %10
  %810 = add <8 x i32> %809, %808
  %811 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %810, i32 %4) #8
  %812 = mul <8 x i32> %739, %26
  %813 = add <8 x i32> %812, %10
  %814 = mul <8 x i32> %22, %755
  %815 = sub <8 x i32> %813, %814
  %816 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %815, i32 %4) #8
  %817 = mul <8 x i32> %741, %22
  %818 = mul <8 x i32> %757, %26
  %819 = add <8 x i32> %817, %10
  %820 = add <8 x i32> %819, %818
  %821 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %820, i32 %4) #8
  %822 = mul <8 x i32> %741, %26
  %823 = add <8 x i32> %822, %10
  %824 = mul <8 x i32> %22, %757
  %825 = sub <8 x i32> %823, %824
  %826 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %825, i32 %4) #8
  %827 = mul <8 x i32> %743, %22
  %828 = mul <8 x i32> %759, %26
  %829 = add <8 x i32> %827, %10
  %830 = add <8 x i32> %829, %828
  %831 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %830, i32 %4) #8
  %832 = mul <8 x i32> %743, %26
  %833 = add <8 x i32> %832, %10
  %834 = mul <8 x i32> %22, %759
  %835 = sub <8 x i32> %833, %834
  %836 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %835, i32 %4) #8
  %837 = mul <8 x i32> %745, %22
  %838 = mul <8 x i32> %761, %26
  %839 = add <8 x i32> %837, %10
  %840 = add <8 x i32> %839, %838
  %841 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %840, i32 %4) #8
  %842 = mul <8 x i32> %745, %26
  %843 = add <8 x i32> %842, %10
  %844 = mul <8 x i32> %22, %761
  %845 = sub <8 x i32> %843, %844
  %846 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %845, i32 %4) #8
  %847 = mul <8 x i32> %746, %29
  %848 = mul <8 x i32> %762, %22
  %849 = add <8 x i32> %847, %10
  %850 = add <8 x i32> %849, %848
  %851 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %850, i32 %4) #8
  %852 = mul <8 x i32> %746, %22
  %853 = add <8 x i32> %852, %10
  %854 = mul <8 x i32> %29, %762
  %855 = sub <8 x i32> %853, %854
  %856 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %855, i32 %4) #8
  %857 = mul <8 x i32> %744, %29
  %858 = mul <8 x i32> %760, %22
  %859 = add <8 x i32> %857, %10
  %860 = add <8 x i32> %859, %858
  %861 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %860, i32 %4) #8
  %862 = mul <8 x i32> %744, %22
  %863 = add <8 x i32> %862, %10
  %864 = mul <8 x i32> %29, %760
  %865 = sub <8 x i32> %863, %864
  %866 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %865, i32 %4) #8
  %867 = mul <8 x i32> %742, %29
  %868 = mul <8 x i32> %758, %22
  %869 = add <8 x i32> %867, %10
  %870 = add <8 x i32> %869, %868
  %871 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %870, i32 %4) #8
  %872 = mul <8 x i32> %742, %22
  %873 = add <8 x i32> %872, %10
  %874 = mul <8 x i32> %29, %758
  %875 = sub <8 x i32> %873, %874
  %876 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %875, i32 %4) #8
  %877 = mul <8 x i32> %740, %29
  %878 = mul <8 x i32> %756, %22
  %879 = add <8 x i32> %877, %10
  %880 = add <8 x i32> %879, %878
  %881 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %880, i32 %4) #8
  %882 = mul <8 x i32> %740, %22
  %883 = add <8 x i32> %882, %10
  %884 = mul <8 x i32> %29, %756
  %885 = sub <8 x i32> %883, %884
  %886 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %885, i32 %4) #8
  %887 = add <8 x i32> %763, %769
  %888 = sub <8 x i32> %763, %769
  %889 = add <8 x i32> %765, %767
  %890 = sub <8 x i32> %765, %767
  %891 = mul <8 x i32> %768, %15
  %892 = mul <8 x i32> %766, %17
  %893 = add <8 x i32> %891, %10
  %894 = add <8 x i32> %893, %892
  %895 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %894, i32 %4) #8
  %896 = mul <8 x i32> %768, %17
  %897 = add <8 x i32> %896, %10
  %898 = mul <8 x i32> %15, %766
  %899 = sub <8 x i32> %897, %898
  %900 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %899, i32 %4) #8
  %901 = add <8 x i32> %785, %690
  %902 = sub <8 x i32> %690, %785
  %903 = add <8 x i32> %775, %688
  %904 = sub <8 x i32> %688, %775
  %905 = add <8 x i32> %790, %676
  %906 = sub <8 x i32> %676, %790
  %907 = add <8 x i32> %780, %678
  %908 = sub <8 x i32> %678, %780
  %909 = mul <8 x i32> %795, %22
  %910 = mul <8 x i32> %803, %26
  %911 = add <8 x i32> %909, %10
  %912 = add <8 x i32> %911, %910
  %913 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %912, i32 %4) #8
  %914 = mul <8 x i32> %795, %26
  %915 = add <8 x i32> %914, %10
  %916 = mul <8 x i32> %22, %803
  %917 = sub <8 x i32> %915, %916
  %918 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %917, i32 %4) #8
  %919 = mul <8 x i32> %797, %22
  %920 = mul <8 x i32> %805, %26
  %921 = add <8 x i32> %919, %10
  %922 = add <8 x i32> %921, %920
  %923 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %922, i32 %4) #8
  %924 = mul <8 x i32> %797, %26
  %925 = add <8 x i32> %924, %10
  %926 = mul <8 x i32> %22, %805
  %927 = sub <8 x i32> %925, %926
  %928 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %927, i32 %4) #8
  %929 = mul <8 x i32> %798, %29
  %930 = mul <8 x i32> %806, %22
  %931 = add <8 x i32> %929, %10
  %932 = add <8 x i32> %931, %930
  %933 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %932, i32 %4) #8
  %934 = mul <8 x i32> %798, %22
  %935 = add <8 x i32> %934, %10
  %936 = mul <8 x i32> %29, %806
  %937 = sub <8 x i32> %935, %936
  %938 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %937, i32 %4) #8
  %939 = mul <8 x i32> %796, %29
  %940 = mul <8 x i32> %804, %22
  %941 = add <8 x i32> %939, %10
  %942 = add <8 x i32> %941, %940
  %943 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %942, i32 %4) #8
  %944 = mul <8 x i32> %796, %22
  %945 = add <8 x i32> %944, %10
  %946 = mul <8 x i32> %29, %804
  %947 = sub <8 x i32> %945, %946
  %948 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %947, i32 %4) #8
  %949 = add <8 x i32> %841, %731
  %950 = sub <8 x i32> %731, %841
  %951 = add <8 x i32> %831, %733
  %952 = sub <8 x i32> %733, %831
  %953 = add <8 x i32> %821, %735
  %954 = sub <8 x i32> %735, %821
  %955 = add <8 x i32> %811, %737
  %956 = sub <8 x i32> %737, %811
  %957 = add <8 x i32> %851, %732
  %958 = sub <8 x i32> %732, %851
  %959 = add <8 x i32> %861, %734
  %960 = sub <8 x i32> %734, %861
  %961 = add <8 x i32> %871, %736
  %962 = sub <8 x i32> %736, %871
  %963 = add <8 x i32> %881, %738
  %964 = sub <8 x i32> %738, %881
  %965 = add <8 x i32> %856, %748
  %966 = sub <8 x i32> %748, %856
  %967 = add <8 x i32> %866, %750
  %968 = sub <8 x i32> %750, %866
  %969 = add <8 x i32> %876, %752
  %970 = sub <8 x i32> %752, %876
  %971 = add <8 x i32> %886, %754
  %972 = sub <8 x i32> %754, %886
  %973 = add <8 x i32> %846, %747
  %974 = sub <8 x i32> %747, %846
  %975 = add <8 x i32> %836, %749
  %976 = sub <8 x i32> %749, %836
  %977 = add <8 x i32> %826, %751
  %978 = sub <8 x i32> %751, %826
  %979 = add <8 x i32> %816, %753
  %980 = sub <8 x i32> %753, %816
  %981 = mul <8 x i32> %887, %17
  %982 = mul <8 x i32> %889, %17
  %983 = add <8 x i32> %981, %10
  %984 = add <8 x i32> %983, %982
  %985 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %984, i32 %4) #8
  %986 = sub <8 x i32> %983, %982
  %987 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %986, i32 %4) #8
  %988 = mul <8 x i32> %890, %26
  %989 = mul <8 x i32> %888, %31
  %990 = add <8 x i32> %988, %10
  %991 = add <8 x i32> %990, %989
  %992 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %991, i32 %4) #8
  %993 = mul <8 x i32> %888, %26
  %994 = mul <8 x i32> %31, %890
  %995 = sub <8 x i32> %10, %994
  %996 = add <8 x i32> %995, %993
  %997 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %996, i32 %4) #8
  %998 = add <8 x i32> %895, %770
  %999 = sub <8 x i32> %770, %895
  %1000 = add <8 x i32> %900, %764
  %1001 = sub <8 x i32> %764, %900
  %1002 = mul <8 x i32> %903, %22
  %1003 = mul <8 x i32> %907, %26
  %1004 = add <8 x i32> %1002, %10
  %1005 = add <8 x i32> %1004, %1003
  %1006 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1005, i32 %4) #8
  %1007 = mul <8 x i32> %903, %26
  %1008 = add <8 x i32> %1007, %10
  %1009 = mul <8 x i32> %22, %907
  %1010 = sub <8 x i32> %1008, %1009
  %1011 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1010, i32 %4) #8
  %1012 = mul <8 x i32> %904, %29
  %1013 = mul <8 x i32> %908, %22
  %1014 = add <8 x i32> %1012, %10
  %1015 = add <8 x i32> %1014, %1013
  %1016 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1015, i32 %4) #8
  %1017 = mul <8 x i32> %904, %22
  %1018 = add <8 x i32> %1017, %10
  %1019 = mul <8 x i32> %29, %908
  %1020 = sub <8 x i32> %1018, %1019
  %1021 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1020, i32 %4) #8
  %1022 = add <8 x i32> %923, %791
  %1023 = sub <8 x i32> %791, %923
  %1024 = add <8 x i32> %913, %793
  %1025 = sub <8 x i32> %793, %913
  %1026 = add <8 x i32> %933, %792
  %1027 = sub <8 x i32> %792, %933
  %1028 = add <8 x i32> %943, %794
  %1029 = sub <8 x i32> %794, %943
  %1030 = add <8 x i32> %938, %800
  %1031 = sub <8 x i32> %800, %938
  %1032 = add <8 x i32> %948, %802
  %1033 = sub <8 x i32> %802, %948
  %1034 = add <8 x i32> %928, %799
  %1035 = sub <8 x i32> %799, %928
  %1036 = add <8 x i32> %918, %801
  %1037 = sub <8 x i32> %801, %918
  %1038 = mul <8 x i32> %953, %36
  %1039 = mul <8 x i32> %977, %40
  %1040 = add <8 x i32> %1038, %10
  %1041 = add <8 x i32> %1040, %1039
  %1042 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1041, i32 %4) #8
  %1043 = mul <8 x i32> %953, %40
  %1044 = add <8 x i32> %1043, %10
  %1045 = mul <8 x i32> %36, %977
  %1046 = sub <8 x i32> %1044, %1045
  %1047 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1046, i32 %4) #8
  %1048 = mul <8 x i32> %955, %36
  %1049 = mul <8 x i32> %979, %40
  %1050 = add <8 x i32> %1048, %10
  %1051 = add <8 x i32> %1050, %1049
  %1052 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1051, i32 %4) #8
  %1053 = mul <8 x i32> %955, %40
  %1054 = add <8 x i32> %1053, %10
  %1055 = mul <8 x i32> %36, %979
  %1056 = sub <8 x i32> %1054, %1055
  %1057 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1056, i32 %4) #8
  %1058 = mul <8 x i32> %956, %43
  %1059 = mul <8 x i32> %980, %36
  %1060 = add <8 x i32> %1058, %10
  %1061 = add <8 x i32> %1060, %1059
  %1062 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1061, i32 %4) #8
  %1063 = mul <8 x i32> %956, %36
  %1064 = add <8 x i32> %1063, %10
  %1065 = mul <8 x i32> %43, %980
  %1066 = sub <8 x i32> %1064, %1065
  %1067 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1066, i32 %4) #8
  %1068 = mul <8 x i32> %954, %43
  %1069 = mul <8 x i32> %978, %36
  %1070 = add <8 x i32> %1068, %10
  %1071 = add <8 x i32> %1070, %1069
  %1072 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1071, i32 %4) #8
  %1073 = mul <8 x i32> %954, %36
  %1074 = add <8 x i32> %1073, %10
  %1075 = mul <8 x i32> %43, %978
  %1076 = sub <8 x i32> %1074, %1075
  %1077 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1076, i32 %4) #8
  %1078 = mul <8 x i32> %962, %48
  %1079 = mul <8 x i32> %970, %52
  %1080 = add <8 x i32> %1078, %10
  %1081 = add <8 x i32> %1080, %1079
  %1082 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1081, i32 %4) #8
  %1083 = mul <8 x i32> %962, %52
  %1084 = add <8 x i32> %1083, %10
  %1085 = mul <8 x i32> %48, %970
  %1086 = sub <8 x i32> %1084, %1085
  %1087 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1086, i32 %4) #8
  %1088 = mul <8 x i32> %964, %48
  %1089 = mul <8 x i32> %972, %52
  %1090 = add <8 x i32> %1088, %10
  %1091 = add <8 x i32> %1090, %1089
  %1092 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1091, i32 %4) #8
  %1093 = mul <8 x i32> %964, %52
  %1094 = add <8 x i32> %1093, %10
  %1095 = mul <8 x i32> %48, %972
  %1096 = sub <8 x i32> %1094, %1095
  %1097 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1096, i32 %4) #8
  %1098 = mul <8 x i32> %963, %55
  %1099 = mul <8 x i32> %971, %48
  %1100 = add <8 x i32> %1098, %10
  %1101 = add <8 x i32> %1100, %1099
  %1102 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1101, i32 %4) #8
  %1103 = mul <8 x i32> %963, %48
  %1104 = add <8 x i32> %1103, %10
  %1105 = mul <8 x i32> %55, %971
  %1106 = sub <8 x i32> %1104, %1105
  %1107 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1106, i32 %4) #8
  %1108 = mul <8 x i32> %961, %55
  %1109 = mul <8 x i32> %969, %48
  %1110 = add <8 x i32> %1108, %10
  %1111 = add <8 x i32> %1110, %1109
  %1112 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1111, i32 %4) #8
  %1113 = mul <8 x i32> %961, %48
  %1114 = add <8 x i32> %1113, %10
  %1115 = mul <8 x i32> %55, %969
  %1116 = sub <8 x i32> %1114, %1115
  %1117 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1116, i32 %4) #8
  %1118 = mul <8 x i32> %998, %40
  %1119 = mul <8 x i32> %1000, %57
  %1120 = add <8 x i32> %1118, %10
  %1121 = add <8 x i32> %1120, %1119
  %1122 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1121, i32 %4) #8
  %1123 = mul <8 x i32> %1000, %40
  %1124 = mul <8 x i32> %57, %998
  %1125 = sub <8 x i32> %10, %1124
  %1126 = add <8 x i32> %1125, %1123
  %1127 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1126, i32 %4) #8
  %1128 = mul <8 x i32> %999, %52
  %1129 = mul <8 x i32> %1001, %59
  %1130 = add <8 x i32> %1128, %10
  %1131 = add <8 x i32> %1130, %1129
  %1132 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1131, i32 %4) #8
  %1133 = mul <8 x i32> %1001, %52
  %1134 = mul <8 x i32> %59, %999
  %1135 = sub <8 x i32> %10, %1134
  %1136 = add <8 x i32> %1135, %1133
  %1137 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1136, i32 %4) #8
  %1138 = add <8 x i32> %1006, %901
  %1139 = sub <8 x i32> %901, %1006
  %1140 = add <8 x i32> %1016, %902
  %1141 = sub <8 x i32> %902, %1016
  %1142 = add <8 x i32> %1021, %906
  %1143 = sub <8 x i32> %906, %1021
  %1144 = add <8 x i32> %1011, %905
  %1145 = sub <8 x i32> %905, %1011
  %1146 = mul <8 x i32> %1024, %36
  %1147 = mul <8 x i32> %1036, %40
  %1148 = add <8 x i32> %1146, %10
  %1149 = add <8 x i32> %1148, %1147
  %1150 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1149, i32 %4) #8
  %1151 = mul <8 x i32> %1024, %40
  %1152 = add <8 x i32> %1151, %10
  %1153 = mul <8 x i32> %36, %1036
  %1154 = sub <8 x i32> %1152, %1153
  %1155 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1154, i32 %4) #8
  %1156 = mul <8 x i32> %1025, %43
  %1157 = mul <8 x i32> %1037, %36
  %1158 = add <8 x i32> %1156, %10
  %1159 = add <8 x i32> %1158, %1157
  %1160 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1159, i32 %4) #8
  %1161 = mul <8 x i32> %1025, %36
  %1162 = add <8 x i32> %1161, %10
  %1163 = mul <8 x i32> %43, %1037
  %1164 = sub <8 x i32> %1162, %1163
  %1165 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1164, i32 %4) #8
  %1166 = mul <8 x i32> %1029, %48
  %1167 = mul <8 x i32> %1033, %52
  %1168 = add <8 x i32> %1166, %10
  %1169 = add <8 x i32> %1168, %1167
  %1170 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1169, i32 %4) #8
  %1171 = mul <8 x i32> %1029, %52
  %1172 = add <8 x i32> %1171, %10
  %1173 = mul <8 x i32> %48, %1033
  %1174 = sub <8 x i32> %1172, %1173
  %1175 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1174, i32 %4) #8
  %1176 = mul <8 x i32> %1028, %55
  %1177 = mul <8 x i32> %1032, %48
  %1178 = add <8 x i32> %1176, %10
  %1179 = add <8 x i32> %1178, %1177
  %1180 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1179, i32 %4) #8
  %1181 = mul <8 x i32> %1028, %48
  %1182 = add <8 x i32> %1181, %10
  %1183 = mul <8 x i32> %55, %1032
  %1184 = sub <8 x i32> %1182, %1183
  %1185 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1184, i32 %4) #8
  %1186 = add <8 x i32> %1052, %949
  %1187 = sub <8 x i32> %949, %1052
  %1188 = add <8 x i32> %1042, %951
  %1189 = sub <8 x i32> %951, %1042
  %1190 = add <8 x i32> %1062, %950
  %1191 = sub <8 x i32> %950, %1062
  %1192 = add <8 x i32> %1072, %952
  %1193 = sub <8 x i32> %952, %1072
  %1194 = add <8 x i32> %1092, %958
  %1195 = sub <8 x i32> %958, %1092
  %1196 = add <8 x i32> %1082, %960
  %1197 = sub <8 x i32> %960, %1082
  %1198 = add <8 x i32> %1102, %957
  %1199 = sub <8 x i32> %957, %1102
  %1200 = add <8 x i32> %1112, %959
  %1201 = sub <8 x i32> %959, %1112
  %1202 = add <8 x i32> %1107, %965
  %1203 = sub <8 x i32> %965, %1107
  %1204 = add <8 x i32> %1117, %967
  %1205 = sub <8 x i32> %967, %1117
  %1206 = add <8 x i32> %1097, %966
  %1207 = sub <8 x i32> %966, %1097
  %1208 = add <8 x i32> %1087, %968
  %1209 = sub <8 x i32> %968, %1087
  %1210 = add <8 x i32> %1067, %974
  %1211 = sub <8 x i32> %974, %1067
  %1212 = add <8 x i32> %1077, %976
  %1213 = sub <8 x i32> %976, %1077
  %1214 = add <8 x i32> %1057, %973
  %1215 = sub <8 x i32> %973, %1057
  %1216 = add <8 x i32> %1047, %975
  %1217 = sub <8 x i32> %975, %1047
  %1218 = mul <8 x i32> %1138, %63
  %1219 = mul <8 x i32> %1144, %67
  %1220 = add <8 x i32> %1218, %10
  %1221 = add <8 x i32> %1220, %1219
  %1222 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1221, i32 %4) #8
  %1223 = mul <8 x i32> %1144, %63
  %1224 = mul <8 x i32> %67, %1138
  %1225 = sub <8 x i32> %10, %1224
  %1226 = add <8 x i32> %1225, %1223
  %1227 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1226, i32 %4) #8
  %1228 = mul <8 x i32> %1139, %71
  %1229 = mul <8 x i32> %1145, %75
  %1230 = add <8 x i32> %1228, %10
  %1231 = add <8 x i32> %1230, %1229
  %1232 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1231, i32 %4) #8
  %1233 = mul <8 x i32> %1145, %71
  %1234 = mul <8 x i32> %75, %1139
  %1235 = sub <8 x i32> %10, %1234
  %1236 = add <8 x i32> %1235, %1233
  %1237 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1236, i32 %4) #8
  %1238 = mul <8 x i32> %1141, %79
  %1239 = mul <8 x i32> %1143, %83
  %1240 = add <8 x i32> %1238, %10
  %1241 = add <8 x i32> %1240, %1239
  %1242 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1241, i32 %4) #8
  %1243 = mul <8 x i32> %1143, %79
  %1244 = mul <8 x i32> %83, %1141
  %1245 = sub <8 x i32> %10, %1244
  %1246 = add <8 x i32> %1245, %1243
  %1247 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1246, i32 %4) #8
  %1248 = mul <8 x i32> %1140, %87
  %1249 = mul <8 x i32> %1142, %91
  %1250 = add <8 x i32> %1248, %10
  %1251 = add <8 x i32> %1250, %1249
  %1252 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1251, i32 %4) #8
  %1253 = mul <8 x i32> %1142, %87
  %1254 = mul <8 x i32> %91, %1140
  %1255 = sub <8 x i32> %10, %1254
  %1256 = add <8 x i32> %1255, %1253
  %1257 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1256, i32 %4) #8
  %1258 = add <8 x i32> %1150, %1022
  %1259 = sub <8 x i32> %1022, %1150
  %1260 = add <8 x i32> %1160, %1023
  %1261 = sub <8 x i32> %1023, %1160
  %1262 = add <8 x i32> %1170, %1027
  %1263 = sub <8 x i32> %1027, %1170
  %1264 = add <8 x i32> %1180, %1026
  %1265 = sub <8 x i32> %1026, %1180
  %1266 = add <8 x i32> %1185, %1030
  %1267 = sub <8 x i32> %1030, %1185
  %1268 = add <8 x i32> %1175, %1031
  %1269 = sub <8 x i32> %1031, %1175
  %1270 = add <8 x i32> %1165, %1035
  %1271 = sub <8 x i32> %1035, %1165
  %1272 = add <8 x i32> %1155, %1034
  %1273 = sub <8 x i32> %1034, %1155
  %1274 = mul <8 x i32> %1188, %94
  %1275 = mul <8 x i32> %1216, %63
  %1276 = add <8 x i32> %1274, %10
  %1277 = add <8 x i32> %1276, %1275
  %1278 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1277, i32 %4) #8
  %1279 = mul <8 x i32> %1188, %63
  %1280 = add <8 x i32> %1279, %10
  %1281 = mul <8 x i32> %94, %1216
  %1282 = sub <8 x i32> %1280, %1281
  %1283 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1282, i32 %4) #8
  %1284 = mul <8 x i32> %1189, %97
  %1285 = mul <8 x i32> %1217, %94
  %1286 = add <8 x i32> %1284, %10
  %1287 = add <8 x i32> %1286, %1285
  %1288 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1287, i32 %4) #8
  %1289 = mul <8 x i32> %1189, %94
  %1290 = add <8 x i32> %1289, %10
  %1291 = mul <8 x i32> %97, %1217
  %1292 = sub <8 x i32> %1290, %1291
  %1293 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1292, i32 %4) #8
  %1294 = mul <8 x i32> %1193, %100
  %1295 = mul <8 x i32> %1213, %71
  %1296 = add <8 x i32> %1294, %10
  %1297 = add <8 x i32> %1296, %1295
  %1298 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1297, i32 %4) #8
  %1299 = mul <8 x i32> %1193, %71
  %1300 = add <8 x i32> %1299, %10
  %1301 = mul <8 x i32> %100, %1213
  %1302 = sub <8 x i32> %1300, %1301
  %1303 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1302, i32 %4) #8
  %1304 = mul <8 x i32> %1192, %103
  %1305 = mul <8 x i32> %1212, %100
  %1306 = add <8 x i32> %1304, %10
  %1307 = add <8 x i32> %1306, %1305
  %1308 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1307, i32 %4) #8
  %1309 = mul <8 x i32> %1192, %100
  %1310 = add <8 x i32> %1309, %10
  %1311 = mul <8 x i32> %103, %1212
  %1312 = sub <8 x i32> %1310, %1311
  %1313 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1312, i32 %4) #8
  %1314 = mul <8 x i32> %1196, %106
  %1315 = mul <8 x i32> %1208, %79
  %1316 = add <8 x i32> %1314, %10
  %1317 = add <8 x i32> %1316, %1315
  %1318 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1317, i32 %4) #8
  %1319 = mul <8 x i32> %1196, %79
  %1320 = add <8 x i32> %1319, %10
  %1321 = mul <8 x i32> %106, %1208
  %1322 = sub <8 x i32> %1320, %1321
  %1323 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1322, i32 %4) #8
  %1324 = mul <8 x i32> %1197, %109
  %1325 = mul <8 x i32> %1209, %106
  %1326 = add <8 x i32> %1324, %10
  %1327 = add <8 x i32> %1326, %1325
  %1328 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1327, i32 %4) #8
  %1329 = mul <8 x i32> %1197, %106
  %1330 = add <8 x i32> %1329, %10
  %1331 = mul <8 x i32> %109, %1209
  %1332 = sub <8 x i32> %1330, %1331
  %1333 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1332, i32 %4) #8
  %1334 = mul <8 x i32> %1201, %112
  %1335 = mul <8 x i32> %1205, %87
  %1336 = add <8 x i32> %1334, %10
  %1337 = add <8 x i32> %1336, %1335
  %1338 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1337, i32 %4) #8
  %1339 = mul <8 x i32> %1201, %87
  %1340 = add <8 x i32> %1339, %10
  %1341 = mul <8 x i32> %112, %1205
  %1342 = sub <8 x i32> %1340, %1341
  %1343 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1342, i32 %4) #8
  %1344 = mul <8 x i32> %1200, %115
  %1345 = mul <8 x i32> %1204, %112
  %1346 = add <8 x i32> %1344, %10
  %1347 = add <8 x i32> %1346, %1345
  %1348 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1347, i32 %4) #8
  %1349 = mul <8 x i32> %1200, %112
  %1350 = add <8 x i32> %1349, %10
  %1351 = mul <8 x i32> %115, %1204
  %1352 = sub <8 x i32> %1350, %1351
  %1353 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1352, i32 %4) #8
  %1354 = mul <8 x i32> %1258, %119
  %1355 = mul <8 x i32> %1272, %123
  %1356 = add <8 x i32> %1354, %10
  %1357 = add <8 x i32> %1356, %1355
  %1358 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1357, i32 %4) #8
  %1359 = mul <8 x i32> %1272, %119
  %1360 = mul <8 x i32> %123, %1258
  %1361 = sub <8 x i32> %10, %1360
  %1362 = add <8 x i32> %1361, %1359
  %1363 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1362, i32 %4) #8
  %1364 = mul <8 x i32> %1259, %127
  %1365 = mul <8 x i32> %1273, %131
  %1366 = add <8 x i32> %1364, %10
  %1367 = add <8 x i32> %1366, %1365
  %1368 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1367, i32 %4) #8
  %1369 = mul <8 x i32> %1273, %127
  %1370 = mul <8 x i32> %131, %1259
  %1371 = sub <8 x i32> %10, %1370
  %1372 = add <8 x i32> %1371, %1369
  %1373 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1372, i32 %4) #8
  %1374 = mul <8 x i32> %1261, %135
  %1375 = mul <8 x i32> %1271, %139
  %1376 = add <8 x i32> %1374, %10
  %1377 = add <8 x i32> %1376, %1375
  %1378 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1377, i32 %4) #8
  %1379 = mul <8 x i32> %1271, %135
  %1380 = mul <8 x i32> %139, %1261
  %1381 = sub <8 x i32> %10, %1380
  %1382 = add <8 x i32> %1381, %1379
  %1383 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1382, i32 %4) #8
  %1384 = mul <8 x i32> %1260, %143
  %1385 = mul <8 x i32> %1270, %147
  %1386 = add <8 x i32> %1384, %10
  %1387 = add <8 x i32> %1386, %1385
  %1388 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1387, i32 %4) #8
  %1389 = mul <8 x i32> %1270, %143
  %1390 = mul <8 x i32> %147, %1260
  %1391 = sub <8 x i32> %10, %1390
  %1392 = add <8 x i32> %1391, %1389
  %1393 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1392, i32 %4) #8
  %1394 = mul <8 x i32> %1262, %151
  %1395 = mul <8 x i32> %1268, %155
  %1396 = add <8 x i32> %1394, %10
  %1397 = add <8 x i32> %1396, %1395
  %1398 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1397, i32 %4) #8
  %1399 = mul <8 x i32> %1268, %151
  %1400 = mul <8 x i32> %155, %1262
  %1401 = sub <8 x i32> %10, %1400
  %1402 = add <8 x i32> %1401, %1399
  %1403 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1402, i32 %4) #8
  %1404 = mul <8 x i32> %1263, %159
  %1405 = mul <8 x i32> %1269, %163
  %1406 = add <8 x i32> %1404, %10
  %1407 = add <8 x i32> %1406, %1405
  %1408 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1407, i32 %4) #8
  %1409 = mul <8 x i32> %1269, %159
  %1410 = mul <8 x i32> %163, %1263
  %1411 = sub <8 x i32> %10, %1410
  %1412 = add <8 x i32> %1411, %1409
  %1413 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1412, i32 %4) #8
  %1414 = mul <8 x i32> %1265, %167
  %1415 = mul <8 x i32> %1267, %171
  %1416 = add <8 x i32> %1414, %10
  %1417 = add <8 x i32> %1416, %1415
  %1418 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1417, i32 %4) #8
  %1419 = mul <8 x i32> %1267, %167
  %1420 = mul <8 x i32> %171, %1265
  %1421 = sub <8 x i32> %10, %1420
  %1422 = add <8 x i32> %1421, %1419
  %1423 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1422, i32 %4) #8
  %1424 = mul <8 x i32> %1264, %175
  %1425 = mul <8 x i32> %1266, %179
  %1426 = add <8 x i32> %1424, %10
  %1427 = add <8 x i32> %1426, %1425
  %1428 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1427, i32 %4) #8
  %1429 = mul <8 x i32> %1266, %175
  %1430 = mul <8 x i32> %179, %1264
  %1431 = sub <8 x i32> %10, %1430
  %1432 = add <8 x i32> %1431, %1429
  %1433 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1432, i32 %4) #8
  %1434 = add <8 x i32> %1278, %1186
  %1435 = sub <8 x i32> %1186, %1278
  %1436 = add <8 x i32> %1288, %1187
  %1437 = sub <8 x i32> %1187, %1288
  %1438 = add <8 x i32> %1298, %1191
  %1439 = sub <8 x i32> %1191, %1298
  %1440 = add <8 x i32> %1308, %1190
  %1441 = sub <8 x i32> %1190, %1308
  %1442 = add <8 x i32> %1318, %1194
  %1443 = sub <8 x i32> %1194, %1318
  %1444 = add <8 x i32> %1328, %1195
  %1445 = sub <8 x i32> %1195, %1328
  %1446 = add <8 x i32> %1338, %1199
  %1447 = sub <8 x i32> %1199, %1338
  %1448 = add <8 x i32> %1348, %1198
  %1449 = sub <8 x i32> %1198, %1348
  %1450 = add <8 x i32> %1353, %1202
  %1451 = sub <8 x i32> %1202, %1353
  %1452 = add <8 x i32> %1343, %1203
  %1453 = sub <8 x i32> %1203, %1343
  %1454 = add <8 x i32> %1333, %1207
  %1455 = sub <8 x i32> %1207, %1333
  %1456 = add <8 x i32> %1323, %1206
  %1457 = sub <8 x i32> %1206, %1323
  %1458 = add <8 x i32> %1313, %1210
  %1459 = sub <8 x i32> %1210, %1313
  %1460 = add <8 x i32> %1303, %1211
  %1461 = sub <8 x i32> %1211, %1303
  %1462 = add <8 x i32> %1293, %1215
  %1463 = sub <8 x i32> %1215, %1293
  %1464 = add <8 x i32> %1283, %1214
  %1465 = sub <8 x i32> %1214, %1283
  %1466 = mul <8 x i32> %1434, %183
  %1467 = mul <8 x i32> %1464, %187
  %1468 = add <8 x i32> %1466, %10
  %1469 = add <8 x i32> %1468, %1467
  %1470 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1469, i32 %4) #8
  %1471 = mul <8 x i32> %1464, %183
  %1472 = mul <8 x i32> %187, %1434
  %1473 = sub <8 x i32> %10, %1472
  %1474 = add <8 x i32> %1473, %1471
  %1475 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1474, i32 %4) #8
  %1476 = mul <8 x i32> %1435, %191
  %1477 = mul <8 x i32> %1465, %195
  %1478 = add <8 x i32> %1476, %10
  %1479 = add <8 x i32> %1478, %1477
  %1480 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1479, i32 %4) #8
  %1481 = mul <8 x i32> %1465, %191
  %1482 = mul <8 x i32> %195, %1435
  %1483 = sub <8 x i32> %10, %1482
  %1484 = add <8 x i32> %1483, %1481
  %1485 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1484, i32 %4) #8
  %1486 = mul <8 x i32> %1437, %199
  %1487 = mul <8 x i32> %1463, %203
  %1488 = add <8 x i32> %1486, %10
  %1489 = add <8 x i32> %1488, %1487
  %1490 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1489, i32 %4) #8
  %1491 = mul <8 x i32> %1463, %199
  %1492 = mul <8 x i32> %203, %1437
  %1493 = sub <8 x i32> %10, %1492
  %1494 = add <8 x i32> %1493, %1491
  %1495 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1494, i32 %4) #8
  %1496 = mul <8 x i32> %1436, %207
  %1497 = mul <8 x i32> %1462, %211
  %1498 = add <8 x i32> %1496, %10
  %1499 = add <8 x i32> %1498, %1497
  %1500 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1499, i32 %4) #8
  %1501 = mul <8 x i32> %1462, %207
  %1502 = mul <8 x i32> %211, %1436
  %1503 = sub <8 x i32> %10, %1502
  %1504 = add <8 x i32> %1503, %1501
  %1505 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1504, i32 %4) #8
  %1506 = mul <8 x i32> %1438, %215
  %1507 = mul <8 x i32> %1460, %219
  %1508 = add <8 x i32> %1506, %10
  %1509 = add <8 x i32> %1508, %1507
  %1510 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1509, i32 %4) #8
  %1511 = mul <8 x i32> %1460, %215
  %1512 = mul <8 x i32> %219, %1438
  %1513 = sub <8 x i32> %10, %1512
  %1514 = add <8 x i32> %1513, %1511
  %1515 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1514, i32 %4) #8
  %1516 = mul <8 x i32> %1439, %223
  %1517 = mul <8 x i32> %1461, %227
  %1518 = add <8 x i32> %1516, %10
  %1519 = add <8 x i32> %1518, %1517
  %1520 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1519, i32 %4) #8
  %1521 = mul <8 x i32> %1461, %223
  %1522 = mul <8 x i32> %227, %1439
  %1523 = sub <8 x i32> %10, %1522
  %1524 = add <8 x i32> %1523, %1521
  %1525 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1524, i32 %4) #8
  %1526 = mul <8 x i32> %1441, %231
  %1527 = mul <8 x i32> %1459, %235
  %1528 = add <8 x i32> %1526, %10
  %1529 = add <8 x i32> %1528, %1527
  %1530 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1529, i32 %4) #8
  %1531 = mul <8 x i32> %1459, %231
  %1532 = mul <8 x i32> %235, %1441
  %1533 = sub <8 x i32> %10, %1532
  %1534 = add <8 x i32> %1533, %1531
  %1535 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1534, i32 %4) #8
  %1536 = mul <8 x i32> %1440, %239
  %1537 = mul <8 x i32> %1458, %243
  %1538 = add <8 x i32> %1536, %10
  %1539 = add <8 x i32> %1538, %1537
  %1540 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1539, i32 %4) #8
  %1541 = mul <8 x i32> %1458, %239
  %1542 = mul <8 x i32> %243, %1440
  %1543 = sub <8 x i32> %10, %1542
  %1544 = add <8 x i32> %1543, %1541
  %1545 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1544, i32 %4) #8
  %1546 = mul <8 x i32> %1442, %247
  %1547 = mul <8 x i32> %1456, %251
  %1548 = add <8 x i32> %1546, %10
  %1549 = add <8 x i32> %1548, %1547
  %1550 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1549, i32 %4) #8
  %1551 = mul <8 x i32> %1456, %247
  %1552 = mul <8 x i32> %251, %1442
  %1553 = sub <8 x i32> %10, %1552
  %1554 = add <8 x i32> %1553, %1551
  %1555 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1554, i32 %4) #8
  %1556 = mul <8 x i32> %1443, %255
  %1557 = mul <8 x i32> %1457, %259
  %1558 = add <8 x i32> %1556, %10
  %1559 = add <8 x i32> %1558, %1557
  %1560 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1559, i32 %4) #8
  %1561 = mul <8 x i32> %1457, %255
  %1562 = mul <8 x i32> %259, %1443
  %1563 = sub <8 x i32> %10, %1562
  %1564 = add <8 x i32> %1563, %1561
  %1565 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1564, i32 %4) #8
  %1566 = mul <8 x i32> %1445, %263
  %1567 = mul <8 x i32> %1455, %267
  %1568 = add <8 x i32> %1566, %10
  %1569 = add <8 x i32> %1568, %1567
  %1570 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1569, i32 %4) #8
  %1571 = mul <8 x i32> %1455, %263
  %1572 = mul <8 x i32> %267, %1445
  %1573 = sub <8 x i32> %10, %1572
  %1574 = add <8 x i32> %1573, %1571
  %1575 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1574, i32 %4) #8
  %1576 = mul <8 x i32> %1444, %271
  %1577 = mul <8 x i32> %1454, %275
  %1578 = add <8 x i32> %1576, %10
  %1579 = add <8 x i32> %1578, %1577
  %1580 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1579, i32 %4) #8
  %1581 = mul <8 x i32> %1454, %271
  %1582 = mul <8 x i32> %275, %1444
  %1583 = sub <8 x i32> %10, %1582
  %1584 = add <8 x i32> %1583, %1581
  %1585 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1584, i32 %4) #8
  %1586 = mul <8 x i32> %1446, %279
  %1587 = mul <8 x i32> %1452, %283
  %1588 = add <8 x i32> %1586, %10
  %1589 = add <8 x i32> %1588, %1587
  %1590 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1589, i32 %4) #8
  %1591 = mul <8 x i32> %1452, %279
  %1592 = mul <8 x i32> %283, %1446
  %1593 = sub <8 x i32> %10, %1592
  %1594 = add <8 x i32> %1593, %1591
  %1595 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1594, i32 %4) #8
  %1596 = mul <8 x i32> %1447, %287
  %1597 = mul <8 x i32> %1453, %291
  %1598 = add <8 x i32> %1596, %10
  %1599 = add <8 x i32> %1598, %1597
  %1600 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1599, i32 %4) #8
  %1601 = mul <8 x i32> %1453, %287
  %1602 = mul <8 x i32> %291, %1447
  %1603 = sub <8 x i32> %10, %1602
  %1604 = add <8 x i32> %1603, %1601
  %1605 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1604, i32 %4) #8
  %1606 = mul <8 x i32> %1449, %295
  %1607 = mul <8 x i32> %1451, %299
  %1608 = add <8 x i32> %1606, %10
  %1609 = add <8 x i32> %1608, %1607
  %1610 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1609, i32 %4) #8
  %1611 = mul <8 x i32> %1451, %295
  %1612 = mul <8 x i32> %299, %1449
  %1613 = sub <8 x i32> %10, %1612
  %1614 = add <8 x i32> %1613, %1611
  %1615 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1614, i32 %4) #8
  %1616 = mul <8 x i32> %1448, %303
  %1617 = mul <8 x i32> %1450, %307
  %1618 = add <8 x i32> %1616, %10
  %1619 = add <8 x i32> %1618, %1617
  %1620 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1619, i32 %4) #8
  %1621 = mul <8 x i32> %1450, %303
  %1622 = mul <8 x i32> %307, %1448
  %1623 = sub <8 x i32> %10, %1622
  %1624 = add <8 x i32> %1623, %1621
  %1625 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1624, i32 %4) #8
  %1626 = bitcast <4 x i64>* %1 to <8 x i32>*
  store <8 x i32> %985, <8 x i32>* %1626, align 32
  %1627 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %1628 = bitcast <4 x i64>* %1627 to <8 x i32>*
  store <8 x i32> %1470, <8 x i32>* %1628, align 32
  %1629 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %1630 = bitcast <4 x i64>* %1629 to <8 x i32>*
  store <8 x i32> %1358, <8 x i32>* %1630, align 32
  %1631 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %1632 = bitcast <4 x i64>* %1631 to <8 x i32>*
  store <8 x i32> %1625, <8 x i32>* %1632, align 32
  %1633 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %1634 = bitcast <4 x i64>* %1633 to <8 x i32>*
  store <8 x i32> %1222, <8 x i32>* %1634, align 32
  %1635 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %1636 = bitcast <4 x i64>* %1635 to <8 x i32>*
  store <8 x i32> %1550, <8 x i32>* %1636, align 32
  %1637 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %1638 = bitcast <4 x i64>* %1637 to <8 x i32>*
  store <8 x i32> %1433, <8 x i32>* %1638, align 32
  %1639 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %1640 = bitcast <4 x i64>* %1639 to <8 x i32>*
  store <8 x i32> %1545, <8 x i32>* %1640, align 32
  %1641 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %1642 = bitcast <4 x i64>* %1641 to <8 x i32>*
  store <8 x i32> %1122, <8 x i32>* %1642, align 32
  %1643 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %1644 = bitcast <4 x i64>* %1643 to <8 x i32>*
  store <8 x i32> %1510, <8 x i32>* %1644, align 32
  %1645 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %1646 = bitcast <4 x i64>* %1645 to <8 x i32>*
  store <8 x i32> %1398, <8 x i32>* %1646, align 32
  %1647 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %1648 = bitcast <4 x i64>* %1647 to <8 x i32>*
  store <8 x i32> %1585, <8 x i32>* %1648, align 32
  %1649 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %1650 = bitcast <4 x i64>* %1649 to <8 x i32>*
  store <8 x i32> %1257, <8 x i32>* %1650, align 32
  %1651 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %1652 = bitcast <4 x i64>* %1651 to <8 x i32>*
  store <8 x i32> %1590, <8 x i32>* %1652, align 32
  %1653 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %1654 = bitcast <4 x i64>* %1653 to <8 x i32>*
  store <8 x i32> %1393, <8 x i32>* %1654, align 32
  %1655 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 15
  %1656 = bitcast <4 x i64>* %1655 to <8 x i32>*
  store <8 x i32> %1505, <8 x i32>* %1656, align 32
  %1657 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 16
  %1658 = bitcast <4 x i64>* %1657 to <8 x i32>*
  store <8 x i32> %992, <8 x i32>* %1658, align 32
  %1659 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 17
  %1660 = bitcast <4 x i64>* %1659 to <8 x i32>*
  store <8 x i32> %1490, <8 x i32>* %1660, align 32
  %1661 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 18
  %1662 = bitcast <4 x i64>* %1661 to <8 x i32>*
  store <8 x i32> %1378, <8 x i32>* %1662, align 32
  %1663 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 19
  %1664 = bitcast <4 x i64>* %1663 to <8 x i32>*
  store <8 x i32> %1605, <8 x i32>* %1664, align 32
  %1665 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 20
  %1666 = bitcast <4 x i64>* %1665 to <8 x i32>*
  store <8 x i32> %1242, <8 x i32>* %1666, align 32
  %1667 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 21
  %1668 = bitcast <4 x i64>* %1667 to <8 x i32>*
  store <8 x i32> %1570, <8 x i32>* %1668, align 32
  %1669 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 22
  %1670 = bitcast <4 x i64>* %1669 to <8 x i32>*
  store <8 x i32> %1413, <8 x i32>* %1670, align 32
  %1671 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 23
  %1672 = bitcast <4 x i64>* %1671 to <8 x i32>*
  store <8 x i32> %1525, <8 x i32>* %1672, align 32
  %1673 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 24
  %1674 = bitcast <4 x i64>* %1673 to <8 x i32>*
  store <8 x i32> %1137, <8 x i32>* %1674, align 32
  %1675 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 25
  %1676 = bitcast <4 x i64>* %1675 to <8 x i32>*
  store <8 x i32> %1530, <8 x i32>* %1676, align 32
  %1677 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 26
  %1678 = bitcast <4 x i64>* %1677 to <8 x i32>*
  store <8 x i32> %1418, <8 x i32>* %1678, align 32
  %1679 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 27
  %1680 = bitcast <4 x i64>* %1679 to <8 x i32>*
  store <8 x i32> %1565, <8 x i32>* %1680, align 32
  %1681 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 28
  %1682 = bitcast <4 x i64>* %1681 to <8 x i32>*
  store <8 x i32> %1237, <8 x i32>* %1682, align 32
  %1683 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 29
  %1684 = bitcast <4 x i64>* %1683 to <8 x i32>*
  store <8 x i32> %1610, <8 x i32>* %1684, align 32
  %1685 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 30
  %1686 = bitcast <4 x i64>* %1685 to <8 x i32>*
  store <8 x i32> %1373, <8 x i32>* %1686, align 32
  %1687 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 31
  %1688 = bitcast <4 x i64>* %1687 to <8 x i32>*
  store <8 x i32> %1485, <8 x i32>* %1688, align 32
  %1689 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 32
  %1690 = bitcast <4 x i64>* %1689 to <8 x i32>*
  store <8 x i32> %987, <8 x i32>* %1690, align 32
  %1691 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 33
  %1692 = bitcast <4 x i64>* %1691 to <8 x i32>*
  store <8 x i32> %1480, <8 x i32>* %1692, align 32
  %1693 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 34
  %1694 = bitcast <4 x i64>* %1693 to <8 x i32>*
  store <8 x i32> %1368, <8 x i32>* %1694, align 32
  %1695 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 35
  %1696 = bitcast <4 x i64>* %1695 to <8 x i32>*
  store <8 x i32> %1615, <8 x i32>* %1696, align 32
  %1697 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 36
  %1698 = bitcast <4 x i64>* %1697 to <8 x i32>*
  store <8 x i32> %1232, <8 x i32>* %1698, align 32
  %1699 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 37
  %1700 = bitcast <4 x i64>* %1699 to <8 x i32>*
  store <8 x i32> %1560, <8 x i32>* %1700, align 32
  %1701 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 38
  %1702 = bitcast <4 x i64>* %1701 to <8 x i32>*
  store <8 x i32> %1423, <8 x i32>* %1702, align 32
  %1703 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 39
  %1704 = bitcast <4 x i64>* %1703 to <8 x i32>*
  store <8 x i32> %1535, <8 x i32>* %1704, align 32
  %1705 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 40
  %1706 = bitcast <4 x i64>* %1705 to <8 x i32>*
  store <8 x i32> %1132, <8 x i32>* %1706, align 32
  %1707 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 41
  %1708 = bitcast <4 x i64>* %1707 to <8 x i32>*
  store <8 x i32> %1520, <8 x i32>* %1708, align 32
  %1709 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 42
  %1710 = bitcast <4 x i64>* %1709 to <8 x i32>*
  store <8 x i32> %1408, <8 x i32>* %1710, align 32
  %1711 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 43
  %1712 = bitcast <4 x i64>* %1711 to <8 x i32>*
  store <8 x i32> %1575, <8 x i32>* %1712, align 32
  %1713 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 44
  %1714 = bitcast <4 x i64>* %1713 to <8 x i32>*
  store <8 x i32> %1247, <8 x i32>* %1714, align 32
  %1715 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 45
  %1716 = bitcast <4 x i64>* %1715 to <8 x i32>*
  store <8 x i32> %1600, <8 x i32>* %1716, align 32
  %1717 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 46
  %1718 = bitcast <4 x i64>* %1717 to <8 x i32>*
  store <8 x i32> %1383, <8 x i32>* %1718, align 32
  %1719 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 47
  %1720 = bitcast <4 x i64>* %1719 to <8 x i32>*
  store <8 x i32> %1495, <8 x i32>* %1720, align 32
  %1721 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 48
  %1722 = bitcast <4 x i64>* %1721 to <8 x i32>*
  store <8 x i32> %997, <8 x i32>* %1722, align 32
  %1723 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 49
  %1724 = bitcast <4 x i64>* %1723 to <8 x i32>*
  store <8 x i32> %1500, <8 x i32>* %1724, align 32
  %1725 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 50
  %1726 = bitcast <4 x i64>* %1725 to <8 x i32>*
  store <8 x i32> %1388, <8 x i32>* %1726, align 32
  %1727 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 51
  %1728 = bitcast <4 x i64>* %1727 to <8 x i32>*
  store <8 x i32> %1595, <8 x i32>* %1728, align 32
  %1729 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 52
  %1730 = bitcast <4 x i64>* %1729 to <8 x i32>*
  store <8 x i32> %1252, <8 x i32>* %1730, align 32
  %1731 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 53
  %1732 = bitcast <4 x i64>* %1731 to <8 x i32>*
  store <8 x i32> %1580, <8 x i32>* %1732, align 32
  %1733 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 54
  %1734 = bitcast <4 x i64>* %1733 to <8 x i32>*
  store <8 x i32> %1403, <8 x i32>* %1734, align 32
  %1735 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 55
  %1736 = bitcast <4 x i64>* %1735 to <8 x i32>*
  store <8 x i32> %1515, <8 x i32>* %1736, align 32
  %1737 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 56
  %1738 = bitcast <4 x i64>* %1737 to <8 x i32>*
  store <8 x i32> %1127, <8 x i32>* %1738, align 32
  %1739 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 57
  %1740 = bitcast <4 x i64>* %1739 to <8 x i32>*
  store <8 x i32> %1540, <8 x i32>* %1740, align 32
  %1741 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 58
  %1742 = bitcast <4 x i64>* %1741 to <8 x i32>*
  store <8 x i32> %1428, <8 x i32>* %1742, align 32
  %1743 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 59
  %1744 = bitcast <4 x i64>* %1743 to <8 x i32>*
  store <8 x i32> %1555, <8 x i32>* %1744, align 32
  %1745 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 60
  %1746 = bitcast <4 x i64>* %1745 to <8 x i32>*
  store <8 x i32> %1227, <8 x i32>* %1746, align 32
  %1747 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 61
  %1748 = bitcast <4 x i64>* %1747 to <8 x i32>*
  store <8 x i32> %1620, <8 x i32>* %1748, align 32
  %1749 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 62
  %1750 = bitcast <4 x i64>* %1749 to <8 x i32>*
  store <8 x i32> %1363, <8 x i32>* %1750, align 32
  %1751 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 63
  %1752 = bitcast <4 x i64>* %1751 to <8 x i32>*
  store <8 x i32> %1475, <8 x i32>* %1752, align 32
  ret void
}

; Function Attrs: nounwind readnone
declare <8 x i32> @llvm.x86.avx2.pslli.d(<8 x i32>, i32) #7

; Function Attrs: inlinehint nounwind ssp uwtable
define internal void @fdct8x16_new_avx2(<2 x i64>* nocapture readonly, <2 x i64>* nocapture, i8 signext) #4 {
  %4 = sext i8 %2 to i32
  %5 = add nsw i32 %4, -10
  %6 = sext i32 %5 to i64
  %7 = add nsw i32 %4, -1
  %8 = shl i32 1, %7
  %9 = insertelement <8 x i32> undef, i32 %8, i32 0
  %10 = shufflevector <8 x i32> %9, <8 x i32> undef, <8 x i32> zeroinitializer
  %11 = insertelement <4 x i32> undef, i32 %8, i32 0
  %12 = shufflevector <4 x i32> %11, <4 x i32> undef, <4 x i32> zeroinitializer
  %13 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 32
  %14 = load i32, i32* %13, align 16
  %15 = sub i32 0, %14
  %16 = and i32 %15, 65535
  %17 = shl i32 %14, 16
  %18 = or i32 %16, %17
  %19 = insertelement <4 x i32> undef, i32 %18, i32 0
  %20 = shufflevector <4 x i32> %19, <4 x i32> undef, <4 x i32> zeroinitializer
  %21 = bitcast <4 x i32> %20 to <2 x i64>
  %22 = and i32 %14, 65535
  %23 = or i32 %22, %17
  %24 = insertelement <4 x i32> undef, i32 %23, i32 0
  %25 = shufflevector <4 x i32> %24, <4 x i32> undef, <4 x i32> zeroinitializer
  %26 = bitcast <4 x i32> %25 to <2 x i64>
  %27 = sub i32 0, %17
  %28 = or i32 %22, %27
  %29 = insertelement <4 x i32> undef, i32 %28, i32 0
  %30 = shufflevector <4 x i32> %29, <4 x i32> undef, <4 x i32> zeroinitializer
  %31 = bitcast <4 x i32> %30 to <2 x i64>
  %32 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 48
  %33 = load i32, i32* %32, align 16
  %34 = and i32 %33, 65535
  %35 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 16
  %36 = load i32, i32* %35, align 16
  %37 = shl i32 %36, 16
  %38 = or i32 %37, %34
  %39 = insertelement <4 x i32> undef, i32 %38, i32 0
  %40 = shufflevector <4 x i32> %39, <4 x i32> undef, <4 x i32> zeroinitializer
  %41 = bitcast <4 x i32> %40 to <2 x i64>
  %42 = sub i32 0, %36
  %43 = and i32 %42, 65535
  %44 = shl i32 %33, 16
  %45 = or i32 %43, %44
  %46 = insertelement <4 x i32> undef, i32 %45, i32 0
  %47 = shufflevector <4 x i32> %46, <4 x i32> undef, <4 x i32> zeroinitializer
  %48 = bitcast <4 x i32> %47 to <2 x i64>
  %49 = sub i32 0, %33
  %50 = and i32 %49, 65535
  %51 = sub i32 0, %37
  %52 = or i32 %50, %51
  %53 = insertelement <4 x i32> undef, i32 %52, i32 0
  %54 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 56
  %55 = load i32, i32* %54, align 16
  %56 = and i32 %55, 65535
  %57 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 8
  %58 = load i32, i32* %57, align 16
  %59 = shl i32 %58, 16
  %60 = or i32 %59, %56
  %61 = insertelement <4 x i32> undef, i32 %60, i32 0
  %62 = shufflevector <4 x i32> %61, <4 x i32> undef, <4 x i32> zeroinitializer
  %63 = bitcast <4 x i32> %62 to <2 x i64>
  %64 = sub i32 0, %58
  %65 = and i32 %64, 65535
  %66 = shl i32 %55, 16
  %67 = or i32 %65, %66
  %68 = insertelement <4 x i32> undef, i32 %67, i32 0
  %69 = shufflevector <4 x i32> %68, <4 x i32> undef, <4 x i32> zeroinitializer
  %70 = bitcast <4 x i32> %69 to <2 x i64>
  %71 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 24
  %72 = load i32, i32* %71, align 16
  %73 = and i32 %72, 65535
  %74 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 40
  %75 = load i32, i32* %74, align 16
  %76 = shl i32 %75, 16
  %77 = or i32 %76, %73
  %78 = insertelement <4 x i32> undef, i32 %77, i32 0
  %79 = sub i32 0, %75
  %80 = and i32 %79, 65535
  %81 = shl i32 %72, 16
  %82 = or i32 %80, %81
  %83 = insertelement <4 x i32> undef, i32 %82, i32 0
  %84 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 60
  %85 = load i32, i32* %84, align 16
  %86 = and i32 %85, 65535
  %87 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 4
  %88 = load i32, i32* %87, align 16
  %89 = shl i32 %88, 16
  %90 = or i32 %89, %86
  %91 = insertelement <4 x i32> undef, i32 %90, i32 0
  %92 = shufflevector <4 x i32> %91, <4 x i32> undef, <4 x i32> zeroinitializer
  %93 = bitcast <4 x i32> %92 to <2 x i64>
  %94 = sub i32 0, %88
  %95 = and i32 %94, 65535
  %96 = shl i32 %85, 16
  %97 = or i32 %95, %96
  %98 = insertelement <4 x i32> undef, i32 %97, i32 0
  %99 = shufflevector <4 x i32> %98, <4 x i32> undef, <4 x i32> zeroinitializer
  %100 = bitcast <4 x i32> %99 to <2 x i64>
  %101 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 28
  %102 = load i32, i32* %101, align 16
  %103 = and i32 %102, 65535
  %104 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 36
  %105 = load i32, i32* %104, align 16
  %106 = shl i32 %105, 16
  %107 = or i32 %106, %103
  %108 = insertelement <4 x i32> undef, i32 %107, i32 0
  %109 = sub i32 0, %105
  %110 = and i32 %109, 65535
  %111 = shl i32 %102, 16
  %112 = or i32 %110, %111
  %113 = insertelement <4 x i32> undef, i32 %112, i32 0
  %114 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 44
  %115 = load i32, i32* %114, align 16
  %116 = and i32 %115, 65535
  %117 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 20
  %118 = load i32, i32* %117, align 16
  %119 = shl i32 %118, 16
  %120 = or i32 %119, %116
  %121 = insertelement <4 x i32> undef, i32 %120, i32 0
  %122 = shufflevector <4 x i32> %121, <4 x i32> undef, <4 x i32> zeroinitializer
  %123 = bitcast <4 x i32> %122 to <2 x i64>
  %124 = sub i32 0, %118
  %125 = and i32 %124, 65535
  %126 = shl i32 %115, 16
  %127 = or i32 %125, %126
  %128 = insertelement <4 x i32> undef, i32 %127, i32 0
  %129 = shufflevector <4 x i32> %128, <4 x i32> undef, <4 x i32> zeroinitializer
  %130 = bitcast <4 x i32> %129 to <2 x i64>
  %131 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 12
  %132 = load i32, i32* %131, align 16
  %133 = and i32 %132, 65535
  %134 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 52
  %135 = load i32, i32* %134, align 16
  %136 = shl i32 %135, 16
  %137 = or i32 %136, %133
  %138 = insertelement <4 x i32> undef, i32 %137, i32 0
  %139 = sub i32 0, %135
  %140 = and i32 %139, 65535
  %141 = shl i32 %132, 16
  %142 = or i32 %140, %141
  %143 = insertelement <4 x i32> undef, i32 %142, i32 0
  %144 = shufflevector <2 x i64> %21, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %145 = bitcast <4 x i64> %144 to <8 x i32>
  %146 = shufflevector <4 x i32> %19, <4 x i32> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %147 = shufflevector <8 x i32> %145, <8 x i32> %146, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %148 = shufflevector <2 x i64> %26, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %149 = bitcast <4 x i64> %148 to <8 x i32>
  %150 = shufflevector <4 x i32> %24, <4 x i32> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %151 = shufflevector <8 x i32> %149, <8 x i32> %150, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %152 = shufflevector <4 x i32> %39, <4 x i32> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %153 = shufflevector <8 x i32> %149, <8 x i32> %152, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %154 = shufflevector <2 x i64> %31, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %155 = bitcast <4 x i64> %154 to <8 x i32>
  %156 = shufflevector <4 x i32> %46, <4 x i32> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %157 = shufflevector <8 x i32> %155, <8 x i32> %156, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %158 = shufflevector <2 x i64> %48, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %159 = bitcast <4 x i64> %158 to <8 x i32>
  %160 = shufflevector <4 x i32> %53, <4 x i32> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %161 = shufflevector <8 x i32> %159, <8 x i32> %160, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %162 = shufflevector <2 x i64> %41, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %163 = bitcast <4 x i64> %162 to <8 x i32>
  %164 = shufflevector <8 x i32> %163, <8 x i32> %156, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %165 = shufflevector <2 x i64> %63, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %166 = bitcast <4 x i64> %165 to <8 x i32>
  %167 = shufflevector <4 x i32> %78, <4 x i32> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %168 = shufflevector <8 x i32> %166, <8 x i32> %167, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %169 = shufflevector <2 x i64> %70, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %170 = bitcast <4 x i64> %169 to <8 x i32>
  %171 = shufflevector <4 x i32> %83, <4 x i32> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %172 = shufflevector <8 x i32> %170, <8 x i32> %171, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %173 = shufflevector <2 x i64> %93, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %174 = bitcast <4 x i64> %173 to <8 x i32>
  %175 = shufflevector <4 x i32> %108, <4 x i32> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %176 = shufflevector <8 x i32> %174, <8 x i32> %175, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %177 = shufflevector <2 x i64> %100, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %178 = bitcast <4 x i64> %177 to <8 x i32>
  %179 = shufflevector <4 x i32> %113, <4 x i32> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %180 = shufflevector <8 x i32> %178, <8 x i32> %179, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %181 = shufflevector <2 x i64> %123, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %182 = bitcast <4 x i64> %181 to <8 x i32>
  %183 = shufflevector <4 x i32> %138, <4 x i32> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %184 = shufflevector <8 x i32> %182, <8 x i32> %183, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %185 = shufflevector <2 x i64> %130, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %186 = bitcast <4 x i64> %185 to <8 x i32>
  %187 = shufflevector <4 x i32> %143, <4 x i32> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %188 = shufflevector <8 x i32> %186, <8 x i32> %187, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %189 = load <2 x i64>, <2 x i64>* %0, align 16
  %190 = shufflevector <2 x i64> %189, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %191 = bitcast <4 x i64> %190 to <8 x i32>
  %192 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %193 = bitcast <2 x i64>* %192 to <4 x i32>*
  %194 = load <4 x i32>, <4 x i32>* %193, align 16
  %195 = shufflevector <4 x i32> %194, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %196 = shufflevector <8 x i32> %191, <8 x i32> %195, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %197 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 15
  %198 = load <2 x i64>, <2 x i64>* %197, align 16
  %199 = shufflevector <2 x i64> %198, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %200 = bitcast <4 x i64> %199 to <8 x i32>
  %201 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 14
  %202 = bitcast <2 x i64>* %201 to <4 x i32>*
  %203 = load <4 x i32>, <4 x i32>* %202, align 16
  %204 = shufflevector <4 x i32> %203, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %205 = shufflevector <8 x i32> %200, <8 x i32> %204, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %206 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %207 = load <2 x i64>, <2 x i64>* %206, align 16
  %208 = shufflevector <2 x i64> %207, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %209 = bitcast <4 x i64> %208 to <8 x i32>
  %210 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %211 = bitcast <2 x i64>* %210 to <4 x i32>*
  %212 = load <4 x i32>, <4 x i32>* %211, align 16
  %213 = shufflevector <4 x i32> %212, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %214 = shufflevector <8 x i32> %209, <8 x i32> %213, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %215 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 13
  %216 = load <2 x i64>, <2 x i64>* %215, align 16
  %217 = shufflevector <2 x i64> %216, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %218 = bitcast <4 x i64> %217 to <8 x i32>
  %219 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 12
  %220 = bitcast <2 x i64>* %219 to <4 x i32>*
  %221 = load <4 x i32>, <4 x i32>* %220, align 16
  %222 = shufflevector <4 x i32> %221, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %223 = shufflevector <8 x i32> %218, <8 x i32> %222, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %224 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 5
  %225 = load <2 x i64>, <2 x i64>* %224, align 16
  %226 = shufflevector <2 x i64> %225, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %227 = bitcast <4 x i64> %226 to <8 x i32>
  %228 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 4
  %229 = bitcast <2 x i64>* %228 to <4 x i32>*
  %230 = load <4 x i32>, <4 x i32>* %229, align 16
  %231 = shufflevector <4 x i32> %230, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %232 = shufflevector <8 x i32> %227, <8 x i32> %231, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %233 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 10
  %234 = load <2 x i64>, <2 x i64>* %233, align 16
  %235 = shufflevector <2 x i64> %234, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %236 = bitcast <4 x i64> %235 to <8 x i32>
  %237 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 11
  %238 = bitcast <2 x i64>* %237 to <4 x i32>*
  %239 = load <4 x i32>, <4 x i32>* %238, align 16
  %240 = shufflevector <4 x i32> %239, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %241 = shufflevector <8 x i32> %236, <8 x i32> %240, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %242 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 7
  %243 = load <2 x i64>, <2 x i64>* %242, align 16
  %244 = shufflevector <2 x i64> %243, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %245 = bitcast <4 x i64> %244 to <8 x i32>
  %246 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 6
  %247 = bitcast <2 x i64>* %246 to <4 x i32>*
  %248 = load <4 x i32>, <4 x i32>* %247, align 16
  %249 = shufflevector <4 x i32> %248, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %250 = shufflevector <8 x i32> %245, <8 x i32> %249, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %251 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 8
  %252 = load <2 x i64>, <2 x i64>* %251, align 16
  %253 = shufflevector <2 x i64> %252, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %254 = bitcast <4 x i64> %253 to <8 x i32>
  %255 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 9
  %256 = bitcast <2 x i64>* %255 to <4 x i32>*
  %257 = load <4 x i32>, <4 x i32>* %256, align 16
  %258 = shufflevector <4 x i32> %257, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %259 = shufflevector <8 x i32> %254, <8 x i32> %258, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %260 = bitcast <8 x i32> %196 to <16 x i16>
  %261 = bitcast <8 x i32> %205 to <16 x i16>
  %262 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %260, <16 x i16> %261) #8
  %263 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %260, <16 x i16> %261) #8
  %264 = bitcast <8 x i32> %214 to <16 x i16>
  %265 = bitcast <8 x i32> %223 to <16 x i16>
  %266 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %264, <16 x i16> %265) #8
  %267 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %264, <16 x i16> %265) #8
  %268 = bitcast <8 x i32> %232 to <16 x i16>
  %269 = bitcast <8 x i32> %241 to <16 x i16>
  %270 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %268, <16 x i16> %269) #8
  %271 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %268, <16 x i16> %269) #8
  %272 = bitcast <8 x i32> %250 to <16 x i16>
  %273 = bitcast <8 x i32> %259 to <16 x i16>
  %274 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %272, <16 x i16> %273) #8
  %275 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %272, <16 x i16> %273) #8
  %276 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %262, <16 x i16> %274) #8
  %277 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %262, <16 x i16> %274) #8
  %278 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %266, <16 x i16> %270) #8
  %279 = bitcast <16 x i16> %278 to <4 x i64>
  %280 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %266, <16 x i16> %270) #8
  %281 = bitcast <16 x i16> %280 to <4 x i64>
  %282 = shufflevector <16 x i16> %271, <16 x i16> %267, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %283 = shufflevector <16 x i16> %271, <16 x i16> %267, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %284 = bitcast <8 x i32> %147 to <16 x i16>
  %285 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %282, <16 x i16> %284) #8
  %286 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %283, <16 x i16> %284) #8
  %287 = bitcast <8 x i32> %151 to <16 x i16>
  %288 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %282, <16 x i16> %287) #8
  %289 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %283, <16 x i16> %287) #8
  %290 = add <8 x i32> %285, %10
  %291 = add <8 x i32> %286, %10
  %292 = add <8 x i32> %288, %10
  %293 = add <8 x i32> %289, %10
  %294 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %290, i32 %4) #8
  %295 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %291, i32 %4) #8
  %296 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %292, i32 %4) #8
  %297 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %293, i32 %4) #8
  %298 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %294, <8 x i32> %295) #8
  %299 = bitcast <16 x i16> %298 to <4 x i64>
  %300 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %296, <8 x i32> %297) #8
  %301 = bitcast <16 x i16> %300 to <4 x i64>
  %302 = shufflevector <4 x i64> %299, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %303 = shufflevector <4 x i64> %301, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %304 = shufflevector <4 x i64> %299, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %305 = shufflevector <4 x i64> %301, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %306 = shufflevector <2 x i64> %304, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %307 = bitcast <4 x i64> %306 to <8 x i32>
  %308 = bitcast <2 x i64> %302 to <4 x i32>
  %309 = shufflevector <4 x i32> %308, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %310 = shufflevector <8 x i32> %307, <8 x i32> %309, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %311 = shufflevector <2 x i64> %305, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %312 = bitcast <4 x i64> %311 to <8 x i32>
  %313 = bitcast <2 x i64> %303 to <4 x i32>
  %314 = shufflevector <4 x i32> %313, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %315 = shufflevector <8 x i32> %312, <8 x i32> %314, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %316 = shufflevector <4 x i64> %279, <4 x i64> undef, <4 x i32> <i32 2, i32 3, i32 0, i32 1>
  %317 = bitcast <4 x i64> %316 to <16 x i16>
  %318 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %276, <16 x i16> %317) #8
  %319 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %276, <16 x i16> %317) #8
  %320 = bitcast <16 x i16> %277 to <8 x i32>
  %321 = bitcast <16 x i16> %280 to <8 x i32>
  %322 = shufflevector <8 x i32> %320, <8 x i32> %321, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 12, i32 13, i32 14, i32 15>
  %323 = shufflevector <4 x i64> %281, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %324 = shufflevector <8 x i32> %320, <8 x i32> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %325 = bitcast <2 x i64> %323 to <8 x i16>
  %326 = bitcast <4 x i32> %324 to <8 x i16>
  %327 = shufflevector <8 x i16> %325, <8 x i16> %326, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %328 = shufflevector <8 x i16> %325, <8 x i16> %326, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %329 = bitcast <4 x i32> %20 to <8 x i16>
  %330 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %327, <8 x i16> %329) #8
  %331 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %328, <8 x i16> %329) #8
  %332 = bitcast <4 x i32> %25 to <8 x i16>
  %333 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %327, <8 x i16> %332) #8
  %334 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %328, <8 x i16> %332) #8
  %335 = add <4 x i32> %330, %12
  %336 = add <4 x i32> %331, %12
  %337 = add <4 x i32> %333, %12
  %338 = add <4 x i32> %334, %12
  %339 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %335, i32 %4) #8
  %340 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %336, i32 %4) #8
  %341 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %337, i32 %4) #8
  %342 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %338, i32 %4) #8
  %343 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %339, <4 x i32> %340) #8
  %344 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %341, <4 x i32> %342) #8
  %345 = bitcast <8 x i16> %344 to <2 x i64>
  %346 = shufflevector <2 x i64> %345, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %347 = bitcast <4 x i64> %346 to <8 x i32>
  %348 = bitcast <8 x i16> %343 to <4 x i32>
  %349 = shufflevector <4 x i32> %348, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %350 = shufflevector <8 x i32> %347, <8 x i32> %349, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %351 = bitcast <8 x i32> %310 to <16 x i16>
  %352 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %275, <16 x i16> %351) #8
  %353 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %275, <16 x i16> %351) #8
  %354 = bitcast <8 x i32> %315 to <16 x i16>
  %355 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %263, <16 x i16> %354) #8
  %356 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %263, <16 x i16> %354) #8
  %357 = bitcast <16 x i16> %318 to <8 x i32>
  %358 = bitcast <16 x i16> %319 to <8 x i32>
  %359 = shufflevector <8 x i32> %357, <8 x i32> %358, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 12, i32 13, i32 14, i32 15>
  %360 = shufflevector <8 x i32> %357, <8 x i32> %358, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11>
  %361 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 8
  %362 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 4
  %363 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 12
  %364 = bitcast <8 x i32> %359 to <16 x i16>
  %365 = bitcast <8 x i32> %360 to <16 x i16>
  %366 = shufflevector <16 x i16> %364, <16 x i16> %365, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %367 = shufflevector <16 x i16> %364, <16 x i16> %365, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %368 = bitcast <8 x i32> %153 to <16 x i16>
  %369 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %366, <16 x i16> %368) #8
  %370 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %367, <16 x i16> %368) #8
  %371 = bitcast <8 x i32> %157 to <16 x i16>
  %372 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %366, <16 x i16> %371) #8
  %373 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %367, <16 x i16> %371) #8
  %374 = add <8 x i32> %369, %10
  %375 = add <8 x i32> %370, %10
  %376 = add <8 x i32> %372, %10
  %377 = add <8 x i32> %373, %10
  %378 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %374, i32 %4) #8
  %379 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %375, i32 %4) #8
  %380 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %376, i32 %4) #8
  %381 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %377, i32 %4) #8
  %382 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %378, <8 x i32> %379) #8
  %383 = bitcast <16 x i16> %382 to <4 x i64>
  %384 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %380, <8 x i32> %381) #8
  %385 = bitcast <16 x i16> %384 to <4 x i64>
  %386 = shufflevector <4 x i64> %383, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %386, <2 x i64>* %1, align 16
  %387 = shufflevector <4 x i64> %385, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %387, <2 x i64>* %361, align 16
  %388 = shufflevector <4 x i64> %383, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  store <2 x i64> %388, <2 x i64>* %362, align 16
  %389 = shufflevector <4 x i64> %385, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  store <2 x i64> %389, <2 x i64>* %363, align 16
  %390 = bitcast <8 x i32> %322 to <16 x i16>
  %391 = bitcast <8 x i32> %350 to <16 x i16>
  %392 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %390, <16 x i16> %391) #8
  %393 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %390, <16 x i16> %391) #8
  %394 = bitcast <16 x i16> %352 to <8 x i32>
  %395 = bitcast <16 x i16> %353 to <8 x i32>
  %396 = shufflevector <8 x i32> %394, <8 x i32> %395, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %397 = bitcast <16 x i16> %356 to <8 x i32>
  %398 = bitcast <16 x i16> %355 to <8 x i32>
  %399 = shufflevector <8 x i32> %397, <8 x i32> %398, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %400 = shufflevector <8 x i32> %394, <8 x i32> %395, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
  %401 = shufflevector <8 x i32> %398, <8 x i32> %397, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
  %402 = bitcast <8 x i32> %400 to <16 x i16>
  %403 = bitcast <8 x i32> %401 to <16 x i16>
  %404 = shufflevector <16 x i16> %402, <16 x i16> %403, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %405 = shufflevector <16 x i16> %402, <16 x i16> %403, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %406 = bitcast <8 x i32> %161 to <16 x i16>
  %407 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %404, <16 x i16> %406) #8
  %408 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %405, <16 x i16> %406) #8
  %409 = bitcast <8 x i32> %164 to <16 x i16>
  %410 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %404, <16 x i16> %409) #8
  %411 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %405, <16 x i16> %409) #8
  %412 = add <8 x i32> %407, %10
  %413 = add <8 x i32> %408, %10
  %414 = add <8 x i32> %410, %10
  %415 = add <8 x i32> %411, %10
  %416 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %412, i32 %4) #8
  %417 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %413, i32 %4) #8
  %418 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %414, i32 %4) #8
  %419 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %415, i32 %4) #8
  %420 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %416, <8 x i32> %417) #8
  %421 = bitcast <16 x i16> %420 to <4 x i64>
  %422 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %418, <8 x i32> %419) #8
  %423 = bitcast <16 x i16> %422 to <4 x i64>
  %424 = shufflevector <4 x i64> %423, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %425 = shufflevector <4 x i64> %421, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %426 = shufflevector <4 x i64> %423, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %427 = bitcast <16 x i16> %420 to <8 x i32>
  %428 = bitcast <2 x i64> %425 to <4 x i32>
  %429 = shufflevector <4 x i32> %428, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %430 = shufflevector <8 x i32> %427, <8 x i32> %429, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %431 = shufflevector <2 x i64> %426, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %432 = bitcast <4 x i64> %431 to <8 x i32>
  %433 = bitcast <2 x i64> %424 to <4 x i32>
  %434 = shufflevector <4 x i32> %433, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %435 = shufflevector <8 x i32> %432, <8 x i32> %434, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %436 = bitcast <16 x i16> %392 to <8 x i32>
  %437 = bitcast <16 x i16> %393 to <8 x i32>
  %438 = shufflevector <8 x i32> %436, <8 x i32> %437, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
  %439 = shufflevector <8 x i32> %436, <8 x i32> %437, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %440 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 2
  %441 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 14
  %442 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 10
  %443 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 6
  %444 = bitcast <8 x i32> %438 to <16 x i16>
  %445 = bitcast <8 x i32> %439 to <16 x i16>
  %446 = shufflevector <16 x i16> %444, <16 x i16> %445, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %447 = shufflevector <16 x i16> %444, <16 x i16> %445, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %448 = bitcast <8 x i32> %168 to <16 x i16>
  %449 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %446, <16 x i16> %448) #8
  %450 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %447, <16 x i16> %448) #8
  %451 = bitcast <8 x i32> %172 to <16 x i16>
  %452 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %446, <16 x i16> %451) #8
  %453 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %447, <16 x i16> %451) #8
  %454 = add <8 x i32> %449, %10
  %455 = add <8 x i32> %450, %10
  %456 = add <8 x i32> %452, %10
  %457 = add <8 x i32> %453, %10
  %458 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %454, i32 %4) #8
  %459 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %455, i32 %4) #8
  %460 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %456, i32 %4) #8
  %461 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %457, i32 %4) #8
  %462 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %458, <8 x i32> %459) #8
  %463 = bitcast <16 x i16> %462 to <4 x i64>
  %464 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %460, <8 x i32> %461) #8
  %465 = bitcast <16 x i16> %464 to <4 x i64>
  %466 = shufflevector <4 x i64> %463, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %466, <2 x i64>* %440, align 16
  %467 = shufflevector <4 x i64> %465, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %467, <2 x i64>* %441, align 16
  %468 = shufflevector <4 x i64> %463, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  store <2 x i64> %468, <2 x i64>* %442, align 16
  %469 = shufflevector <4 x i64> %465, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  store <2 x i64> %469, <2 x i64>* %443, align 16
  %470 = bitcast <8 x i32> %396 to <16 x i16>
  %471 = bitcast <8 x i32> %430 to <16 x i16>
  %472 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %470, <16 x i16> %471) #8
  %473 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %470, <16 x i16> %471) #8
  %474 = bitcast <8 x i32> %399 to <16 x i16>
  %475 = bitcast <8 x i32> %435 to <16 x i16>
  %476 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %474, <16 x i16> %475) #8
  %477 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %474, <16 x i16> %475) #8
  %478 = bitcast <16 x i16> %472 to <8 x i32>
  %479 = bitcast <16 x i16> %473 to <8 x i32>
  %480 = shufflevector <8 x i32> %478, <8 x i32> %479, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %481 = bitcast <16 x i16> %476 to <8 x i32>
  %482 = bitcast <16 x i16> %477 to <8 x i32>
  %483 = shufflevector <8 x i32> %481, <8 x i32> %482, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
  %484 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 1
  %485 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 15
  %486 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 9
  %487 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 7
  %488 = bitcast <8 x i32> %480 to <16 x i16>
  %489 = bitcast <8 x i32> %483 to <16 x i16>
  %490 = shufflevector <16 x i16> %488, <16 x i16> %489, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %491 = shufflevector <16 x i16> %488, <16 x i16> %489, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %492 = bitcast <8 x i32> %176 to <16 x i16>
  %493 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %490, <16 x i16> %492) #8
  %494 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %491, <16 x i16> %492) #8
  %495 = bitcast <8 x i32> %180 to <16 x i16>
  %496 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %490, <16 x i16> %495) #8
  %497 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %491, <16 x i16> %495) #8
  %498 = add <8 x i32> %493, %10
  %499 = add <8 x i32> %494, %10
  %500 = add <8 x i32> %496, %10
  %501 = add <8 x i32> %497, %10
  %502 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %498, i32 %4) #8
  %503 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %499, i32 %4) #8
  %504 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %500, i32 %4) #8
  %505 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %501, i32 %4) #8
  %506 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %502, <8 x i32> %503) #8
  %507 = bitcast <16 x i16> %506 to <4 x i64>
  %508 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %504, <8 x i32> %505) #8
  %509 = bitcast <16 x i16> %508 to <4 x i64>
  %510 = shufflevector <4 x i64> %507, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %510, <2 x i64>* %484, align 16
  %511 = shufflevector <4 x i64> %509, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %511, <2 x i64>* %485, align 16
  %512 = shufflevector <4 x i64> %507, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  store <2 x i64> %512, <2 x i64>* %486, align 16
  %513 = shufflevector <4 x i64> %509, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  store <2 x i64> %513, <2 x i64>* %487, align 16
  %514 = shufflevector <8 x i32> %479, <8 x i32> %478, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
  %515 = shufflevector <8 x i32> %482, <8 x i32> %481, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %516 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 5
  %517 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 11
  %518 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 13
  %519 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 3
  %520 = bitcast <8 x i32> %514 to <16 x i16>
  %521 = bitcast <8 x i32> %515 to <16 x i16>
  %522 = shufflevector <16 x i16> %520, <16 x i16> %521, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %523 = shufflevector <16 x i16> %520, <16 x i16> %521, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %524 = bitcast <8 x i32> %184 to <16 x i16>
  %525 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %522, <16 x i16> %524) #8
  %526 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %523, <16 x i16> %524) #8
  %527 = bitcast <8 x i32> %188 to <16 x i16>
  %528 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %522, <16 x i16> %527) #8
  %529 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %523, <16 x i16> %527) #8
  %530 = add <8 x i32> %525, %10
  %531 = add <8 x i32> %526, %10
  %532 = add <8 x i32> %528, %10
  %533 = add <8 x i32> %529, %10
  %534 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %530, i32 %4) #8
  %535 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %531, i32 %4) #8
  %536 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %532, i32 %4) #8
  %537 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %533, i32 %4) #8
  %538 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %534, <8 x i32> %535) #8
  %539 = bitcast <16 x i16> %538 to <4 x i64>
  %540 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %536, <8 x i32> %537) #8
  %541 = bitcast <16 x i16> %540 to <4 x i64>
  %542 = shufflevector <4 x i64> %539, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %542, <2 x i64>* %516, align 16
  %543 = shufflevector <4 x i64> %541, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %543, <2 x i64>* %517, align 16
  %544 = shufflevector <4 x i64> %539, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  store <2 x i64> %544, <2 x i64>* %518, align 16
  %545 = shufflevector <4 x i64> %541, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  store <2 x i64> %545, <2 x i64>* %519, align 16
  ret void
}

; Function Attrs: inlinehint nounwind ssp uwtable
define internal void @fadst8x16_new_avx2(<2 x i64>* nocapture readonly, <2 x i64>* nocapture, i8 signext) #4 {
  %4 = sext i8 %2 to i32
  %5 = add nsw i32 %4, -10
  %6 = sext i32 %5 to i64
  %7 = add nsw i32 %4, -1
  %8 = shl i32 1, %7
  %9 = insertelement <8 x i32> undef, i32 %8, i32 0
  %10 = shufflevector <8 x i32> %9, <8 x i32> undef, <8 x i32> zeroinitializer
  %11 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 32
  %12 = load i32, i32* %11, align 16
  %13 = and i32 %12, 65535
  %14 = shl i32 %12, 16
  %15 = or i32 %13, %14
  %16 = insertelement <4 x i32> undef, i32 %15, i32 0
  %17 = shufflevector <4 x i32> %16, <4 x i32> undef, <4 x i32> zeroinitializer
  %18 = bitcast <4 x i32> %17 to <2 x i64>
  %19 = sub i32 0, %14
  %20 = or i32 %13, %19
  %21 = insertelement <4 x i32> undef, i32 %20, i32 0
  %22 = shufflevector <4 x i32> %21, <4 x i32> undef, <4 x i32> zeroinitializer
  %23 = bitcast <4 x i32> %22 to <2 x i64>
  %24 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 16
  %25 = load i32, i32* %24, align 16
  %26 = and i32 %25, 65535
  %27 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 48
  %28 = load i32, i32* %27, align 16
  %29 = shl i32 %28, 16
  %30 = or i32 %29, %26
  %31 = insertelement <4 x i32> undef, i32 %30, i32 0
  %32 = shufflevector <4 x i32> %31, <4 x i32> undef, <4 x i32> zeroinitializer
  %33 = bitcast <4 x i32> %32 to <2 x i64>
  %34 = and i32 %28, 65535
  %35 = shl i32 %25, 16
  %36 = sub i32 0, %35
  %37 = or i32 %34, %36
  %38 = insertelement <4 x i32> undef, i32 %37, i32 0
  %39 = shufflevector <4 x i32> %38, <4 x i32> undef, <4 x i32> zeroinitializer
  %40 = bitcast <4 x i32> %39 to <2 x i64>
  %41 = sub i32 0, %28
  %42 = and i32 %41, 65535
  %43 = or i32 %42, %35
  %44 = insertelement <4 x i32> undef, i32 %43, i32 0
  %45 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 8
  %46 = load i32, i32* %45, align 16
  %47 = and i32 %46, 65535
  %48 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 56
  %49 = load i32, i32* %48, align 16
  %50 = shl i32 %49, 16
  %51 = or i32 %50, %47
  %52 = insertelement <4 x i32> undef, i32 %51, i32 0
  %53 = shufflevector <4 x i32> %52, <4 x i32> undef, <4 x i32> zeroinitializer
  %54 = bitcast <4 x i32> %53 to <2 x i64>
  %55 = and i32 %49, 65535
  %56 = shl i32 %46, 16
  %57 = sub i32 0, %56
  %58 = or i32 %55, %57
  %59 = insertelement <4 x i32> undef, i32 %58, i32 0
  %60 = shufflevector <4 x i32> %59, <4 x i32> undef, <4 x i32> zeroinitializer
  %61 = bitcast <4 x i32> %60 to <2 x i64>
  %62 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 40
  %63 = load i32, i32* %62, align 16
  %64 = and i32 %63, 65535
  %65 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 24
  %66 = load i32, i32* %65, align 16
  %67 = shl i32 %66, 16
  %68 = or i32 %67, %64
  %69 = insertelement <4 x i32> undef, i32 %68, i32 0
  %70 = and i32 %66, 65535
  %71 = shl i32 %63, 16
  %72 = sub i32 0, %71
  %73 = or i32 %70, %72
  %74 = insertelement <4 x i32> undef, i32 %73, i32 0
  %75 = sub i32 0, %49
  %76 = and i32 %75, 65535
  %77 = or i32 %76, %56
  %78 = insertelement <4 x i32> undef, i32 %77, i32 0
  %79 = shufflevector <4 x i32> %78, <4 x i32> undef, <4 x i32> zeroinitializer
  %80 = bitcast <4 x i32> %79 to <2 x i64>
  %81 = sub i32 0, %66
  %82 = and i32 %81, 65535
  %83 = or i32 %82, %71
  %84 = insertelement <4 x i32> undef, i32 %83, i32 0
  %85 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 2
  %86 = load i32, i32* %85, align 8
  %87 = and i32 %86, 65535
  %88 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 62
  %89 = load i32, i32* %88, align 8
  %90 = shl i32 %89, 16
  %91 = or i32 %90, %87
  %92 = insertelement <4 x i32> undef, i32 %91, i32 0
  %93 = shufflevector <4 x i32> %92, <4 x i32> undef, <4 x i32> zeroinitializer
  %94 = bitcast <4 x i32> %93 to <2 x i64>
  %95 = and i32 %89, 65535
  %96 = shl i32 %86, 16
  %97 = sub i32 0, %96
  %98 = or i32 %95, %97
  %99 = insertelement <4 x i32> undef, i32 %98, i32 0
  %100 = shufflevector <4 x i32> %99, <4 x i32> undef, <4 x i32> zeroinitializer
  %101 = bitcast <4 x i32> %100 to <2 x i64>
  %102 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 10
  %103 = load i32, i32* %102, align 8
  %104 = and i32 %103, 65535
  %105 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 54
  %106 = load i32, i32* %105, align 8
  %107 = shl i32 %106, 16
  %108 = or i32 %107, %104
  %109 = insertelement <4 x i32> undef, i32 %108, i32 0
  %110 = and i32 %106, 65535
  %111 = shl i32 %103, 16
  %112 = sub i32 0, %111
  %113 = or i32 %110, %112
  %114 = insertelement <4 x i32> undef, i32 %113, i32 0
  %115 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 18
  %116 = load i32, i32* %115, align 8
  %117 = and i32 %116, 65535
  %118 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 46
  %119 = load i32, i32* %118, align 8
  %120 = shl i32 %119, 16
  %121 = or i32 %120, %117
  %122 = insertelement <4 x i32> undef, i32 %121, i32 0
  %123 = shufflevector <4 x i32> %122, <4 x i32> undef, <4 x i32> zeroinitializer
  %124 = bitcast <4 x i32> %123 to <2 x i64>
  %125 = and i32 %119, 65535
  %126 = shl i32 %116, 16
  %127 = sub i32 0, %126
  %128 = or i32 %125, %127
  %129 = insertelement <4 x i32> undef, i32 %128, i32 0
  %130 = shufflevector <4 x i32> %129, <4 x i32> undef, <4 x i32> zeroinitializer
  %131 = bitcast <4 x i32> %130 to <2 x i64>
  %132 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 26
  %133 = load i32, i32* %132, align 8
  %134 = and i32 %133, 65535
  %135 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 38
  %136 = load i32, i32* %135, align 8
  %137 = shl i32 %136, 16
  %138 = or i32 %137, %134
  %139 = insertelement <4 x i32> undef, i32 %138, i32 0
  %140 = and i32 %136, 65535
  %141 = shl i32 %133, 16
  %142 = sub i32 0, %141
  %143 = or i32 %140, %142
  %144 = insertelement <4 x i32> undef, i32 %143, i32 0
  %145 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 34
  %146 = load i32, i32* %145, align 8
  %147 = and i32 %146, 65535
  %148 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 30
  %149 = load i32, i32* %148, align 8
  %150 = shl i32 %149, 16
  %151 = or i32 %150, %147
  %152 = insertelement <4 x i32> undef, i32 %151, i32 0
  %153 = shufflevector <4 x i32> %152, <4 x i32> undef, <4 x i32> zeroinitializer
  %154 = bitcast <4 x i32> %153 to <2 x i64>
  %155 = and i32 %149, 65535
  %156 = shl i32 %146, 16
  %157 = sub i32 0, %156
  %158 = or i32 %155, %157
  %159 = insertelement <4 x i32> undef, i32 %158, i32 0
  %160 = shufflevector <4 x i32> %159, <4 x i32> undef, <4 x i32> zeroinitializer
  %161 = bitcast <4 x i32> %160 to <2 x i64>
  %162 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 42
  %163 = load i32, i32* %162, align 8
  %164 = and i32 %163, 65535
  %165 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 22
  %166 = load i32, i32* %165, align 8
  %167 = shl i32 %166, 16
  %168 = or i32 %167, %164
  %169 = insertelement <4 x i32> undef, i32 %168, i32 0
  %170 = and i32 %166, 65535
  %171 = shl i32 %163, 16
  %172 = sub i32 0, %171
  %173 = or i32 %170, %172
  %174 = insertelement <4 x i32> undef, i32 %173, i32 0
  %175 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 50
  %176 = load i32, i32* %175, align 8
  %177 = and i32 %176, 65535
  %178 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 14
  %179 = load i32, i32* %178, align 8
  %180 = shl i32 %179, 16
  %181 = or i32 %180, %177
  %182 = insertelement <4 x i32> undef, i32 %181, i32 0
  %183 = shufflevector <4 x i32> %182, <4 x i32> undef, <4 x i32> zeroinitializer
  %184 = bitcast <4 x i32> %183 to <2 x i64>
  %185 = and i32 %179, 65535
  %186 = shl i32 %176, 16
  %187 = sub i32 0, %186
  %188 = or i32 %185, %187
  %189 = insertelement <4 x i32> undef, i32 %188, i32 0
  %190 = shufflevector <4 x i32> %189, <4 x i32> undef, <4 x i32> zeroinitializer
  %191 = bitcast <4 x i32> %190 to <2 x i64>
  %192 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 58
  %193 = load i32, i32* %192, align 8
  %194 = and i32 %193, 65535
  %195 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 6
  %196 = load i32, i32* %195, align 8
  %197 = shl i32 %196, 16
  %198 = or i32 %197, %194
  %199 = insertelement <4 x i32> undef, i32 %198, i32 0
  %200 = and i32 %196, 65535
  %201 = shl i32 %193, 16
  %202 = sub i32 0, %201
  %203 = or i32 %200, %202
  %204 = insertelement <4 x i32> undef, i32 %203, i32 0
  %205 = shufflevector <2 x i64> %18, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %206 = bitcast <4 x i64> %205 to <8 x i32>
  %207 = shufflevector <4 x i32> %16, <4 x i32> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %208 = shufflevector <8 x i32> %206, <8 x i32> %207, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %209 = shufflevector <2 x i64> %23, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %210 = bitcast <4 x i64> %209 to <8 x i32>
  %211 = shufflevector <4 x i32> %21, <4 x i32> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %212 = shufflevector <8 x i32> %210, <8 x i32> %211, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %213 = shufflevector <2 x i64> %33, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %214 = bitcast <4 x i64> %213 to <8 x i32>
  %215 = shufflevector <4 x i32> %44, <4 x i32> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %216 = shufflevector <8 x i32> %214, <8 x i32> %215, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %217 = shufflevector <2 x i64> %40, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %218 = bitcast <4 x i64> %217 to <8 x i32>
  %219 = shufflevector <4 x i32> %31, <4 x i32> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %220 = shufflevector <8 x i32> %218, <8 x i32> %219, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %221 = shufflevector <2 x i64> %54, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %222 = bitcast <4 x i64> %221 to <8 x i32>
  %223 = shufflevector <4 x i32> %69, <4 x i32> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %224 = shufflevector <8 x i32> %222, <8 x i32> %223, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %225 = shufflevector <2 x i64> %61, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %226 = bitcast <4 x i64> %225 to <8 x i32>
  %227 = shufflevector <4 x i32> %74, <4 x i32> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %228 = shufflevector <8 x i32> %226, <8 x i32> %227, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %229 = shufflevector <2 x i64> %80, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %230 = bitcast <4 x i64> %229 to <8 x i32>
  %231 = shufflevector <4 x i32> %84, <4 x i32> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %232 = shufflevector <8 x i32> %230, <8 x i32> %231, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %233 = shufflevector <2 x i64> %94, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %234 = bitcast <4 x i64> %233 to <8 x i32>
  %235 = shufflevector <4 x i32> %109, <4 x i32> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %236 = shufflevector <8 x i32> %234, <8 x i32> %235, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %237 = shufflevector <2 x i64> %101, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %238 = bitcast <4 x i64> %237 to <8 x i32>
  %239 = shufflevector <4 x i32> %114, <4 x i32> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %240 = shufflevector <8 x i32> %238, <8 x i32> %239, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %241 = shufflevector <2 x i64> %124, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %242 = bitcast <4 x i64> %241 to <8 x i32>
  %243 = shufflevector <4 x i32> %139, <4 x i32> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %244 = shufflevector <8 x i32> %242, <8 x i32> %243, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %245 = shufflevector <2 x i64> %131, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %246 = bitcast <4 x i64> %245 to <8 x i32>
  %247 = shufflevector <4 x i32> %144, <4 x i32> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %248 = shufflevector <8 x i32> %246, <8 x i32> %247, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %249 = shufflevector <2 x i64> %154, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %250 = bitcast <4 x i64> %249 to <8 x i32>
  %251 = shufflevector <4 x i32> %169, <4 x i32> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %252 = shufflevector <8 x i32> %250, <8 x i32> %251, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %253 = shufflevector <2 x i64> %161, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %254 = bitcast <4 x i64> %253 to <8 x i32>
  %255 = shufflevector <4 x i32> %174, <4 x i32> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %256 = shufflevector <8 x i32> %254, <8 x i32> %255, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %257 = shufflevector <2 x i64> %184, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %258 = bitcast <4 x i64> %257 to <8 x i32>
  %259 = shufflevector <4 x i32> %199, <4 x i32> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %260 = shufflevector <8 x i32> %258, <8 x i32> %259, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %261 = shufflevector <2 x i64> %191, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %262 = bitcast <4 x i64> %261 to <8 x i32>
  %263 = shufflevector <4 x i32> %204, <4 x i32> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %264 = shufflevector <8 x i32> %262, <8 x i32> %263, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %265 = load <2 x i64>, <2 x i64>* %0, align 16
  %266 = shufflevector <2 x i64> %265, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %267 = bitcast <4 x i64> %266 to <8 x i32>
  %268 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 4
  %269 = bitcast <2 x i64>* %268 to <4 x i32>*
  %270 = load <4 x i32>, <4 x i32>* %269, align 16
  %271 = shufflevector <4 x i32> %270, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %272 = shufflevector <8 x i32> %267, <8 x i32> %271, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %273 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %274 = load <2 x i64>, <2 x i64>* %273, align 16
  %275 = shufflevector <2 x i64> %274, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %276 = bitcast <4 x i64> %275 to <8 x i32>
  %277 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 6
  %278 = bitcast <2 x i64>* %277 to <4 x i32>*
  %279 = load <4 x i32>, <4 x i32>* %278, align 16
  %280 = shufflevector <4 x i32> %279, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %281 = shufflevector <8 x i32> %276, <8 x i32> %280, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %282 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 8
  %283 = load <2 x i64>, <2 x i64>* %282, align 16
  %284 = shufflevector <2 x i64> %283, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %285 = bitcast <4 x i64> %284 to <8 x i32>
  %286 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 12
  %287 = bitcast <2 x i64>* %286 to <4 x i32>*
  %288 = load <4 x i32>, <4 x i32>* %287, align 16
  %289 = shufflevector <4 x i32> %288, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %290 = shufflevector <8 x i32> %285, <8 x i32> %289, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %291 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 10
  %292 = load <2 x i64>, <2 x i64>* %291, align 16
  %293 = shufflevector <2 x i64> %292, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %294 = bitcast <4 x i64> %293 to <8 x i32>
  %295 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 14
  %296 = bitcast <2 x i64>* %295 to <4 x i32>*
  %297 = load <4 x i32>, <4 x i32>* %296, align 16
  %298 = shufflevector <4 x i32> %297, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %299 = shufflevector <8 x i32> %294, <8 x i32> %298, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %300 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %301 = load <2 x i64>, <2 x i64>* %300, align 16
  %302 = shufflevector <2 x i64> %301, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %303 = bitcast <4 x i64> %302 to <8 x i32>
  %304 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 9
  %305 = bitcast <2 x i64>* %304 to <4 x i32>*
  %306 = load <4 x i32>, <4 x i32>* %305, align 16
  %307 = shufflevector <4 x i32> %306, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %308 = shufflevector <8 x i32> %303, <8 x i32> %307, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %309 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %310 = load <2 x i64>, <2 x i64>* %309, align 16
  %311 = shufflevector <2 x i64> %310, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %312 = bitcast <4 x i64> %311 to <8 x i32>
  %313 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 11
  %314 = bitcast <2 x i64>* %313 to <4 x i32>*
  %315 = load <4 x i32>, <4 x i32>* %314, align 16
  %316 = shufflevector <4 x i32> %315, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %317 = shufflevector <8 x i32> %312, <8 x i32> %316, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %318 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 5
  %319 = load <2 x i64>, <2 x i64>* %318, align 16
  %320 = shufflevector <2 x i64> %319, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %321 = bitcast <4 x i64> %320 to <8 x i32>
  %322 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 13
  %323 = bitcast <2 x i64>* %322 to <4 x i32>*
  %324 = load <4 x i32>, <4 x i32>* %323, align 16
  %325 = shufflevector <4 x i32> %324, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %326 = shufflevector <8 x i32> %321, <8 x i32> %325, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %327 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 7
  %328 = load <2 x i64>, <2 x i64>* %327, align 16
  %329 = shufflevector <2 x i64> %328, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %330 = bitcast <4 x i64> %329 to <8 x i32>
  %331 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 15
  %332 = bitcast <2 x i64>* %331 to <4 x i32>*
  %333 = load <4 x i32>, <4 x i32>* %332, align 16
  %334 = shufflevector <4 x i32> %333, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %335 = shufflevector <8 x i32> %330, <8 x i32> %334, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %336 = bitcast <8 x i32> %335 to <16 x i16>
  %337 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %336) #8
  %338 = bitcast <8 x i32> %317 to <16 x i16>
  %339 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %338) #8
  %340 = bitcast <8 x i32> %308 to <16 x i16>
  %341 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %340) #8
  %342 = bitcast <8 x i32> %326 to <16 x i16>
  %343 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %342) #8
  %344 = bitcast <16 x i16> %337 to <8 x i32>
  %345 = shufflevector <8 x i32> %272, <8 x i32> %344, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 12, i32 13, i32 14, i32 15>
  %346 = bitcast <16 x i16> %339 to <8 x i32>
  %347 = shufflevector <8 x i32> %346, <8 x i32> %290, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 12, i32 13, i32 14, i32 15>
  %348 = bitcast <16 x i16> %341 to <8 x i32>
  %349 = shufflevector <8 x i32> %348, <8 x i32> %299, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 12, i32 13, i32 14, i32 15>
  %350 = bitcast <16 x i16> %343 to <8 x i32>
  %351 = shufflevector <8 x i32> %281, <8 x i32> %350, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 12, i32 13, i32 14, i32 15>
  %352 = shufflevector <8 x i32> %344, <8 x i32> %272, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 12, i32 13, i32 14, i32 15>
  %353 = shufflevector <8 x i32> %290, <8 x i32> %346, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 12, i32 13, i32 14, i32 15>
  %354 = bitcast <8 x i32> %352 to <16 x i16>
  %355 = bitcast <8 x i32> %353 to <16 x i16>
  %356 = shufflevector <16 x i16> %354, <16 x i16> %355, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %357 = shufflevector <16 x i16> %354, <16 x i16> %355, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %358 = bitcast <8 x i32> %208 to <16 x i16>
  %359 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %356, <16 x i16> %358) #8
  %360 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %357, <16 x i16> %358) #8
  %361 = bitcast <8 x i32> %212 to <16 x i16>
  %362 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %356, <16 x i16> %361) #8
  %363 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %357, <16 x i16> %361) #8
  %364 = add <8 x i32> %359, %10
  %365 = add <8 x i32> %360, %10
  %366 = add <8 x i32> %362, %10
  %367 = add <8 x i32> %363, %10
  %368 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %364, i32 %4) #8
  %369 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %365, i32 %4) #8
  %370 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %366, i32 %4) #8
  %371 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %367, i32 %4) #8
  %372 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %368, <8 x i32> %369) #8
  %373 = bitcast <16 x i16> %372 to <4 x i64>
  %374 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %370, <8 x i32> %371) #8
  %375 = bitcast <16 x i16> %374 to <4 x i64>
  %376 = shufflevector <4 x i64> %375, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %377 = shufflevector <4 x i64> %373, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %378 = shufflevector <4 x i64> %375, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %379 = bitcast <16 x i16> %372 to <8 x i32>
  %380 = bitcast <2 x i64> %376 to <4 x i32>
  %381 = shufflevector <4 x i32> %380, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %382 = shufflevector <8 x i32> %379, <8 x i32> %381, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %383 = shufflevector <2 x i64> %377, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %384 = bitcast <4 x i64> %383 to <8 x i32>
  %385 = bitcast <2 x i64> %378 to <4 x i32>
  %386 = shufflevector <4 x i32> %385, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %387 = shufflevector <8 x i32> %384, <8 x i32> %386, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %388 = shufflevector <8 x i32> %281, <8 x i32> %350, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11>
  %389 = shufflevector <8 x i32> %348, <8 x i32> %299, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11>
  %390 = bitcast <8 x i32> %388 to <16 x i16>
  %391 = bitcast <8 x i32> %389 to <16 x i16>
  %392 = shufflevector <16 x i16> %390, <16 x i16> %391, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %393 = shufflevector <16 x i16> %390, <16 x i16> %391, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %394 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %392, <16 x i16> %358) #8
  %395 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %393, <16 x i16> %358) #8
  %396 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %392, <16 x i16> %361) #8
  %397 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %393, <16 x i16> %361) #8
  %398 = add <8 x i32> %394, %10
  %399 = add <8 x i32> %395, %10
  %400 = add <8 x i32> %396, %10
  %401 = add <8 x i32> %397, %10
  %402 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %398, i32 %4) #8
  %403 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %399, i32 %4) #8
  %404 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %400, i32 %4) #8
  %405 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %401, i32 %4) #8
  %406 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %402, <8 x i32> %403) #8
  %407 = bitcast <16 x i16> %406 to <4 x i64>
  %408 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %404, <8 x i32> %405) #8
  %409 = bitcast <16 x i16> %408 to <4 x i64>
  %410 = shufflevector <4 x i64> %409, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %411 = shufflevector <4 x i64> %407, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %412 = shufflevector <4 x i64> %409, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %413 = bitcast <16 x i16> %406 to <8 x i32>
  %414 = bitcast <2 x i64> %410 to <4 x i32>
  %415 = shufflevector <4 x i32> %414, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %416 = shufflevector <8 x i32> %413, <8 x i32> %415, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %417 = shufflevector <2 x i64> %411, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %418 = bitcast <4 x i64> %417 to <8 x i32>
  %419 = bitcast <2 x i64> %412 to <4 x i32>
  %420 = shufflevector <4 x i32> %419, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %421 = shufflevector <8 x i32> %418, <8 x i32> %420, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %422 = bitcast <8 x i32> %345 to <16 x i16>
  %423 = bitcast <8 x i32> %382 to <16 x i16>
  %424 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %422, <16 x i16> %423) #8
  %425 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %422, <16 x i16> %423) #8
  %426 = bitcast <8 x i32> %347 to <16 x i16>
  %427 = bitcast <8 x i32> %387 to <16 x i16>
  %428 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %426, <16 x i16> %427) #8
  %429 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %426, <16 x i16> %427) #8
  %430 = bitcast <8 x i32> %349 to <16 x i16>
  %431 = bitcast <8 x i32> %416 to <16 x i16>
  %432 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %430, <16 x i16> %431) #8
  %433 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %430, <16 x i16> %431) #8
  %434 = bitcast <8 x i32> %351 to <16 x i16>
  %435 = bitcast <8 x i32> %421 to <16 x i16>
  %436 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %434, <16 x i16> %435) #8
  %437 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %434, <16 x i16> %435) #8
  %438 = bitcast <16 x i16> %428 to <8 x i32>
  %439 = bitcast <16 x i16> %429 to <8 x i32>
  %440 = shufflevector <8 x i32> %438, <8 x i32> %439, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %441 = shufflevector <8 x i32> %438, <8 x i32> %439, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
  %442 = bitcast <8 x i32> %440 to <16 x i16>
  %443 = bitcast <8 x i32> %441 to <16 x i16>
  %444 = shufflevector <16 x i16> %442, <16 x i16> %443, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %445 = shufflevector <16 x i16> %442, <16 x i16> %443, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %446 = bitcast <8 x i32> %216 to <16 x i16>
  %447 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %444, <16 x i16> %446) #8
  %448 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %445, <16 x i16> %446) #8
  %449 = bitcast <8 x i32> %220 to <16 x i16>
  %450 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %444, <16 x i16> %449) #8
  %451 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %445, <16 x i16> %449) #8
  %452 = add <8 x i32> %447, %10
  %453 = add <8 x i32> %448, %10
  %454 = add <8 x i32> %450, %10
  %455 = add <8 x i32> %451, %10
  %456 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %452, i32 %4) #8
  %457 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %453, i32 %4) #8
  %458 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %454, i32 %4) #8
  %459 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %455, i32 %4) #8
  %460 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %456, <8 x i32> %457) #8
  %461 = bitcast <16 x i16> %460 to <4 x i64>
  %462 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %458, <8 x i32> %459) #8
  %463 = bitcast <16 x i16> %462 to <4 x i64>
  %464 = shufflevector <4 x i64> %463, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %465 = shufflevector <4 x i64> %461, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %466 = shufflevector <4 x i64> %463, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %467 = bitcast <16 x i16> %460 to <8 x i32>
  %468 = bitcast <2 x i64> %464 to <4 x i32>
  %469 = shufflevector <4 x i32> %468, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %470 = shufflevector <8 x i32> %467, <8 x i32> %469, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %471 = shufflevector <2 x i64> %465, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %472 = bitcast <4 x i64> %471 to <8 x i32>
  %473 = bitcast <2 x i64> %466 to <4 x i32>
  %474 = shufflevector <4 x i32> %473, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %475 = shufflevector <8 x i32> %472, <8 x i32> %474, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %476 = bitcast <16 x i16> %436 to <8 x i32>
  %477 = bitcast <16 x i16> %437 to <8 x i32>
  %478 = shufflevector <8 x i32> %476, <8 x i32> %477, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %479 = shufflevector <8 x i32> %476, <8 x i32> %477, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
  %480 = bitcast <8 x i32> %478 to <16 x i16>
  %481 = bitcast <8 x i32> %479 to <16 x i16>
  %482 = shufflevector <16 x i16> %480, <16 x i16> %481, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %483 = shufflevector <16 x i16> %480, <16 x i16> %481, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %484 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %482, <16 x i16> %446) #8
  %485 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %483, <16 x i16> %446) #8
  %486 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %482, <16 x i16> %449) #8
  %487 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %483, <16 x i16> %449) #8
  %488 = add <8 x i32> %484, %10
  %489 = add <8 x i32> %485, %10
  %490 = add <8 x i32> %486, %10
  %491 = add <8 x i32> %487, %10
  %492 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %488, i32 %4) #8
  %493 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %489, i32 %4) #8
  %494 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %490, i32 %4) #8
  %495 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %491, i32 %4) #8
  %496 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %492, <8 x i32> %493) #8
  %497 = bitcast <16 x i16> %496 to <4 x i64>
  %498 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %494, <8 x i32> %495) #8
  %499 = bitcast <16 x i16> %498 to <4 x i64>
  %500 = shufflevector <4 x i64> %499, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %501 = shufflevector <4 x i64> %497, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %502 = shufflevector <4 x i64> %499, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %503 = bitcast <16 x i16> %496 to <8 x i32>
  %504 = bitcast <2 x i64> %500 to <4 x i32>
  %505 = shufflevector <4 x i32> %504, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %506 = shufflevector <8 x i32> %503, <8 x i32> %505, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %507 = shufflevector <2 x i64> %501, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %508 = bitcast <4 x i64> %507 to <8 x i32>
  %509 = bitcast <2 x i64> %502 to <4 x i32>
  %510 = shufflevector <4 x i32> %509, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %511 = shufflevector <8 x i32> %508, <8 x i32> %510, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %512 = bitcast <8 x i32> %470 to <16 x i16>
  %513 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %424, <16 x i16> %512) #8
  %514 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %424, <16 x i16> %512) #8
  %515 = bitcast <8 x i32> %475 to <16 x i16>
  %516 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %425, <16 x i16> %515) #8
  %517 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %425, <16 x i16> %515) #8
  %518 = bitcast <8 x i32> %506 to <16 x i16>
  %519 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %432, <16 x i16> %518) #8
  %520 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %432, <16 x i16> %518) #8
  %521 = bitcast <8 x i32> %511 to <16 x i16>
  %522 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %433, <16 x i16> %521) #8
  %523 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %433, <16 x i16> %521) #8
  %524 = bitcast <16 x i16> %519 to <8 x i32>
  %525 = bitcast <16 x i16> %522 to <8 x i32>
  %526 = shufflevector <8 x i32> %524, <8 x i32> %525, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %527 = shufflevector <8 x i32> %524, <8 x i32> %525, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
  %528 = bitcast <8 x i32> %526 to <16 x i16>
  %529 = bitcast <8 x i32> %527 to <16 x i16>
  %530 = shufflevector <16 x i16> %528, <16 x i16> %529, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %531 = shufflevector <16 x i16> %528, <16 x i16> %529, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %532 = bitcast <8 x i32> %224 to <16 x i16>
  %533 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %530, <16 x i16> %532) #8
  %534 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %531, <16 x i16> %532) #8
  %535 = bitcast <8 x i32> %228 to <16 x i16>
  %536 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %530, <16 x i16> %535) #8
  %537 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %531, <16 x i16> %535) #8
  %538 = add <8 x i32> %533, %10
  %539 = add <8 x i32> %534, %10
  %540 = add <8 x i32> %536, %10
  %541 = add <8 x i32> %537, %10
  %542 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %538, i32 %4) #8
  %543 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %539, i32 %4) #8
  %544 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %540, i32 %4) #8
  %545 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %541, i32 %4) #8
  %546 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %542, <8 x i32> %543) #8
  %547 = bitcast <16 x i16> %546 to <4 x i64>
  %548 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %544, <8 x i32> %545) #8
  %549 = bitcast <16 x i16> %548 to <4 x i64>
  %550 = shufflevector <4 x i64> %549, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %551 = shufflevector <4 x i64> %547, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %552 = shufflevector <4 x i64> %549, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %553 = bitcast <16 x i16> %546 to <8 x i32>
  %554 = bitcast <2 x i64> %550 to <4 x i32>
  %555 = shufflevector <4 x i32> %554, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %556 = shufflevector <8 x i32> %553, <8 x i32> %555, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %557 = shufflevector <2 x i64> %551, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %558 = bitcast <4 x i64> %557 to <8 x i32>
  %559 = bitcast <2 x i64> %552 to <4 x i32>
  %560 = shufflevector <4 x i32> %559, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %561 = shufflevector <8 x i32> %558, <8 x i32> %560, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %562 = bitcast <16 x i16> %520 to <8 x i32>
  %563 = bitcast <16 x i16> %523 to <8 x i32>
  %564 = shufflevector <8 x i32> %562, <8 x i32> %563, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %565 = shufflevector <8 x i32> %562, <8 x i32> %563, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
  %566 = bitcast <8 x i32> %564 to <16 x i16>
  %567 = bitcast <8 x i32> %565 to <16 x i16>
  %568 = shufflevector <16 x i16> %566, <16 x i16> %567, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %569 = shufflevector <16 x i16> %566, <16 x i16> %567, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %570 = bitcast <8 x i32> %232 to <16 x i16>
  %571 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %568, <16 x i16> %570) #8
  %572 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %569, <16 x i16> %570) #8
  %573 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %568, <16 x i16> %532) #8
  %574 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %569, <16 x i16> %532) #8
  %575 = add <8 x i32> %571, %10
  %576 = add <8 x i32> %572, %10
  %577 = add <8 x i32> %573, %10
  %578 = add <8 x i32> %574, %10
  %579 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %575, i32 %4) #8
  %580 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %576, i32 %4) #8
  %581 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %577, i32 %4) #8
  %582 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %578, i32 %4) #8
  %583 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %579, <8 x i32> %580) #8
  %584 = bitcast <16 x i16> %583 to <4 x i64>
  %585 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %581, <8 x i32> %582) #8
  %586 = bitcast <16 x i16> %585 to <4 x i64>
  %587 = shufflevector <4 x i64> %586, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %588 = shufflevector <4 x i64> %584, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %589 = shufflevector <4 x i64> %586, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %590 = bitcast <16 x i16> %583 to <8 x i32>
  %591 = bitcast <2 x i64> %587 to <4 x i32>
  %592 = shufflevector <4 x i32> %591, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %593 = shufflevector <8 x i32> %590, <8 x i32> %592, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %594 = shufflevector <2 x i64> %588, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %595 = bitcast <4 x i64> %594 to <8 x i32>
  %596 = bitcast <2 x i64> %589 to <4 x i32>
  %597 = shufflevector <4 x i32> %596, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %598 = shufflevector <8 x i32> %595, <8 x i32> %597, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %599 = bitcast <8 x i32> %556 to <16 x i16>
  %600 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %513, <16 x i16> %599) #8
  %601 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %513, <16 x i16> %599) #8
  %602 = bitcast <8 x i32> %561 to <16 x i16>
  %603 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %516, <16 x i16> %602) #8
  %604 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %516, <16 x i16> %602) #8
  %605 = bitcast <8 x i32> %593 to <16 x i16>
  %606 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %514, <16 x i16> %605) #8
  %607 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %514, <16 x i16> %605) #8
  %608 = bitcast <8 x i32> %598 to <16 x i16>
  %609 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %517, <16 x i16> %608) #8
  %610 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %517, <16 x i16> %608) #8
  %611 = bitcast <16 x i16> %600 to <8 x i32>
  %612 = bitcast <16 x i16> %603 to <8 x i32>
  %613 = shufflevector <8 x i32> %611, <8 x i32> %612, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %614 = shufflevector <8 x i32> %611, <8 x i32> %612, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
  %615 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 15
  %616 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 13
  %617 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 2
  %618 = bitcast <8 x i32> %613 to <16 x i16>
  %619 = bitcast <8 x i32> %614 to <16 x i16>
  %620 = shufflevector <16 x i16> %618, <16 x i16> %619, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %621 = shufflevector <16 x i16> %618, <16 x i16> %619, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %622 = bitcast <8 x i32> %236 to <16 x i16>
  %623 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %620, <16 x i16> %622) #8
  %624 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %621, <16 x i16> %622) #8
  %625 = bitcast <8 x i32> %240 to <16 x i16>
  %626 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %620, <16 x i16> %625) #8
  %627 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %621, <16 x i16> %625) #8
  %628 = add <8 x i32> %623, %10
  %629 = add <8 x i32> %624, %10
  %630 = add <8 x i32> %626, %10
  %631 = add <8 x i32> %627, %10
  %632 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %628, i32 %4) #8
  %633 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %629, i32 %4) #8
  %634 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %630, i32 %4) #8
  %635 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %631, i32 %4) #8
  %636 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %632, <8 x i32> %633) #8
  %637 = bitcast <16 x i16> %636 to <4 x i64>
  %638 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %634, <8 x i32> %635) #8
  %639 = bitcast <16 x i16> %638 to <4 x i64>
  %640 = shufflevector <4 x i64> %637, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %640, <2 x i64>* %615, align 16
  %641 = shufflevector <4 x i64> %639, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %641, <2 x i64>* %1, align 16
  %642 = shufflevector <4 x i64> %637, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  store <2 x i64> %642, <2 x i64>* %616, align 16
  %643 = shufflevector <4 x i64> %639, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  store <2 x i64> %643, <2 x i64>* %617, align 16
  %644 = bitcast <16 x i16> %606 to <8 x i32>
  %645 = bitcast <16 x i16> %609 to <8 x i32>
  %646 = shufflevector <8 x i32> %644, <8 x i32> %645, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %647 = shufflevector <8 x i32> %644, <8 x i32> %645, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
  %648 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 11
  %649 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 4
  %650 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 9
  %651 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 6
  %652 = bitcast <8 x i32> %646 to <16 x i16>
  %653 = bitcast <8 x i32> %647 to <16 x i16>
  %654 = shufflevector <16 x i16> %652, <16 x i16> %653, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %655 = shufflevector <16 x i16> %652, <16 x i16> %653, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %656 = bitcast <8 x i32> %244 to <16 x i16>
  %657 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %654, <16 x i16> %656) #8
  %658 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %655, <16 x i16> %656) #8
  %659 = bitcast <8 x i32> %248 to <16 x i16>
  %660 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %654, <16 x i16> %659) #8
  %661 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %655, <16 x i16> %659) #8
  %662 = add <8 x i32> %657, %10
  %663 = add <8 x i32> %658, %10
  %664 = add <8 x i32> %660, %10
  %665 = add <8 x i32> %661, %10
  %666 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %662, i32 %4) #8
  %667 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %663, i32 %4) #8
  %668 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %664, i32 %4) #8
  %669 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %665, i32 %4) #8
  %670 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %666, <8 x i32> %667) #8
  %671 = bitcast <16 x i16> %670 to <4 x i64>
  %672 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %668, <8 x i32> %669) #8
  %673 = bitcast <16 x i16> %672 to <4 x i64>
  %674 = shufflevector <4 x i64> %671, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %674, <2 x i64>* %648, align 16
  %675 = shufflevector <4 x i64> %673, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %675, <2 x i64>* %649, align 16
  %676 = shufflevector <4 x i64> %671, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  store <2 x i64> %676, <2 x i64>* %650, align 16
  %677 = shufflevector <4 x i64> %673, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  store <2 x i64> %677, <2 x i64>* %651, align 16
  %678 = bitcast <16 x i16> %601 to <8 x i32>
  %679 = bitcast <16 x i16> %604 to <8 x i32>
  %680 = shufflevector <8 x i32> %678, <8 x i32> %679, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %681 = shufflevector <8 x i32> %678, <8 x i32> %679, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
  %682 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 7
  %683 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 8
  %684 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 5
  %685 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 10
  %686 = bitcast <8 x i32> %680 to <16 x i16>
  %687 = bitcast <8 x i32> %681 to <16 x i16>
  %688 = shufflevector <16 x i16> %686, <16 x i16> %687, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %689 = shufflevector <16 x i16> %686, <16 x i16> %687, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %690 = bitcast <8 x i32> %252 to <16 x i16>
  %691 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %688, <16 x i16> %690) #8
  %692 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %689, <16 x i16> %690) #8
  %693 = bitcast <8 x i32> %256 to <16 x i16>
  %694 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %688, <16 x i16> %693) #8
  %695 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %689, <16 x i16> %693) #8
  %696 = add <8 x i32> %691, %10
  %697 = add <8 x i32> %692, %10
  %698 = add <8 x i32> %694, %10
  %699 = add <8 x i32> %695, %10
  %700 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %696, i32 %4) #8
  %701 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %697, i32 %4) #8
  %702 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %698, i32 %4) #8
  %703 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %699, i32 %4) #8
  %704 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %700, <8 x i32> %701) #8
  %705 = bitcast <16 x i16> %704 to <4 x i64>
  %706 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %702, <8 x i32> %703) #8
  %707 = bitcast <16 x i16> %706 to <4 x i64>
  %708 = shufflevector <4 x i64> %705, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %708, <2 x i64>* %682, align 16
  %709 = shufflevector <4 x i64> %707, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %709, <2 x i64>* %683, align 16
  %710 = shufflevector <4 x i64> %705, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  store <2 x i64> %710, <2 x i64>* %684, align 16
  %711 = shufflevector <4 x i64> %707, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  store <2 x i64> %711, <2 x i64>* %685, align 16
  %712 = bitcast <16 x i16> %607 to <8 x i32>
  %713 = bitcast <16 x i16> %610 to <8 x i32>
  %714 = shufflevector <8 x i32> %712, <8 x i32> %713, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %715 = shufflevector <8 x i32> %712, <8 x i32> %713, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
  %716 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 3
  %717 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 12
  %718 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 1
  %719 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 14
  %720 = bitcast <8 x i32> %714 to <16 x i16>
  %721 = bitcast <8 x i32> %715 to <16 x i16>
  %722 = shufflevector <16 x i16> %720, <16 x i16> %721, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %723 = shufflevector <16 x i16> %720, <16 x i16> %721, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %724 = bitcast <8 x i32> %260 to <16 x i16>
  %725 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %722, <16 x i16> %724) #8
  %726 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %723, <16 x i16> %724) #8
  %727 = bitcast <8 x i32> %264 to <16 x i16>
  %728 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %722, <16 x i16> %727) #8
  %729 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %723, <16 x i16> %727) #8
  %730 = add <8 x i32> %725, %10
  %731 = add <8 x i32> %726, %10
  %732 = add <8 x i32> %728, %10
  %733 = add <8 x i32> %729, %10
  %734 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %730, i32 %4) #8
  %735 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %731, i32 %4) #8
  %736 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %732, i32 %4) #8
  %737 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %733, i32 %4) #8
  %738 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %734, <8 x i32> %735) #8
  %739 = bitcast <16 x i16> %738 to <4 x i64>
  %740 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %736, <8 x i32> %737) #8
  %741 = bitcast <16 x i16> %740 to <4 x i64>
  %742 = shufflevector <4 x i64> %739, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %742, <2 x i64>* %716, align 16
  %743 = shufflevector <4 x i64> %741, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %743, <2 x i64>* %717, align 16
  %744 = shufflevector <4 x i64> %739, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  store <2 x i64> %744, <2 x i64>* %718, align 16
  %745 = shufflevector <4 x i64> %741, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  store <2 x i64> %745, <2 x i64>* %719, align 16
  ret void
}

; Function Attrs: inlinehint nofree nounwind ssp uwtable
define internal void @fidentity8x16_new_avx2(<2 x i64>* nocapture readonly, <2 x i64>* nocapture, i8 signext) #5 {
  br label %5

4:                                                ; preds = %5
  ret void

5:                                                ; preds = %3, %5
  %6 = phi i64 [ 0, %3 ], [ %32, %5 ]
  %7 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %6
  %8 = load <2 x i64>, <2 x i64>* %7, align 16
  %9 = shufflevector <2 x i64> %8, <2 x i64> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %10 = bitcast <4 x i64> %9 to <8 x i32>
  %11 = or i64 %6, 1
  %12 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %11
  %13 = bitcast <2 x i64>* %12 to <4 x i32>*
  %14 = load <4 x i32>, <4 x i32>* %13, align 16
  %15 = shufflevector <4 x i32> %14, <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
  %16 = shufflevector <8 x i32> %10, <8 x i32> %15, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %17 = bitcast <8 x i32> %16 to <16 x i16>
  %18 = shufflevector <16 x i16> %17, <16 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %19 = shufflevector <16 x i16> %17, <16 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %20 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %18, <16 x i16> <i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048>) #8
  %21 = ashr <8 x i32> %20, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %22 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %19, <16 x i16> <i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048, i16 11586, i16 2048>) #8
  %23 = ashr <8 x i32> %22, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %24 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %21, <8 x i32> %23) #8
  %25 = bitcast <16 x i16> %24 to <4 x i64>
  %26 = shufflevector <4 x i64> %25, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %27 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %6
  store <2 x i64> %26, <2 x i64>* %27, align 16
  %28 = bitcast <16 x i16> %24 to <8 x i32>
  %29 = shufflevector <8 x i32> %28, <8 x i32> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %30 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %11
  %31 = bitcast <2 x i64>* %30 to <4 x i32>*
  store <4 x i32> %29, <4 x i32>* %31, align 16
  %32 = add nuw nsw i64 %6, 2
  %33 = icmp ult i64 %32, 16
  br i1 %33, label %5, label %4
}

; Function Attrs: nounwind readnone
declare <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16>, <8 x i16>) #7

; Function Attrs: nounwind readnone
declare <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32>, i32) #7

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32>, <4 x i32>) #7

; Function Attrs: inlinehint nounwind ssp uwtable
define internal void @fdct8x8_new_avx2(<4 x i64>* nocapture readonly, <4 x i64>* nocapture, i8 signext) #4 {
  %4 = sext i8 %2 to i32
  %5 = add nsw i32 %4, -10
  %6 = sext i32 %5 to i64
  %7 = add nsw i32 %4, -1
  %8 = shl i32 1, %7
  %9 = insertelement <8 x i32> undef, i32 %8, i32 0
  %10 = shufflevector <8 x i32> %9, <8 x i32> undef, <8 x i32> zeroinitializer
  %11 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 32
  %12 = load i32, i32* %11, align 16
  %13 = sub i32 0, %12
  %14 = and i32 %13, 65535
  %15 = and i32 %12, 65535
  %16 = shl nuw i32 %15, 16
  %17 = or i32 %16, %14
  %18 = insertelement <8 x i32> undef, i32 %17, i32 0
  %19 = shufflevector <8 x i32> %18, <8 x i32> undef, <8 x i32> zeroinitializer
  %20 = or i32 %16, %15
  %21 = insertelement <8 x i32> undef, i32 %20, i32 0
  %22 = shufflevector <8 x i32> %21, <8 x i32> undef, <8 x i32> zeroinitializer
  %23 = shl nuw i32 %14, 16
  %24 = or i32 %23, %15
  %25 = insertelement <8 x i32> undef, i32 %24, i32 0
  %26 = shufflevector <8 x i32> %25, <8 x i32> undef, <8 x i32> zeroinitializer
  %27 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 48
  %28 = load i32, i32* %27, align 16
  %29 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 16
  %30 = load i32, i32* %29, align 16
  %31 = and i32 %28, 65535
  %32 = shl i32 %30, 16
  %33 = or i32 %32, %31
  %34 = insertelement <8 x i32> undef, i32 %33, i32 0
  %35 = shufflevector <8 x i32> %34, <8 x i32> undef, <8 x i32> zeroinitializer
  %36 = sub i32 0, %30
  %37 = and i32 %36, 65535
  %38 = shl nuw i32 %31, 16
  %39 = or i32 %38, %37
  %40 = insertelement <8 x i32> undef, i32 %39, i32 0
  %41 = shufflevector <8 x i32> %40, <8 x i32> undef, <8 x i32> zeroinitializer
  %42 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 56
  %43 = load i32, i32* %42, align 16
  %44 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 8
  %45 = load i32, i32* %44, align 16
  %46 = and i32 %43, 65535
  %47 = shl i32 %45, 16
  %48 = or i32 %47, %46
  %49 = insertelement <8 x i32> undef, i32 %48, i32 0
  %50 = shufflevector <8 x i32> %49, <8 x i32> undef, <8 x i32> zeroinitializer
  %51 = sub i32 0, %45
  %52 = and i32 %51, 65535
  %53 = shl nuw i32 %46, 16
  %54 = or i32 %53, %52
  %55 = insertelement <8 x i32> undef, i32 %54, i32 0
  %56 = shufflevector <8 x i32> %55, <8 x i32> undef, <8 x i32> zeroinitializer
  %57 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 24
  %58 = load i32, i32* %57, align 16
  %59 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 40
  %60 = load i32, i32* %59, align 16
  %61 = and i32 %58, 65535
  %62 = shl i32 %60, 16
  %63 = or i32 %62, %61
  %64 = insertelement <8 x i32> undef, i32 %63, i32 0
  %65 = shufflevector <8 x i32> %64, <8 x i32> undef, <8 x i32> zeroinitializer
  %66 = sub i32 0, %60
  %67 = and i32 %66, 65535
  %68 = shl nuw i32 %61, 16
  %69 = or i32 %68, %67
  %70 = insertelement <8 x i32> undef, i32 %69, i32 0
  %71 = shufflevector <8 x i32> %70, <8 x i32> undef, <8 x i32> zeroinitializer
  %72 = bitcast <4 x i64>* %0 to <16 x i16>*
  %73 = load <16 x i16>, <16 x i16>* %72, align 32
  %74 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 7
  %75 = bitcast <4 x i64>* %74 to <16 x i16>*
  %76 = load <16 x i16>, <16 x i16>* %75, align 32
  %77 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %73, <16 x i16> %76) #8
  %78 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %73, <16 x i16> %76) #8
  %79 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 1
  %80 = bitcast <4 x i64>* %79 to <16 x i16>*
  %81 = load <16 x i16>, <16 x i16>* %80, align 32
  %82 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 6
  %83 = bitcast <4 x i64>* %82 to <16 x i16>*
  %84 = load <16 x i16>, <16 x i16>* %83, align 32
  %85 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %81, <16 x i16> %84) #8
  %86 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %81, <16 x i16> %84) #8
  %87 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 2
  %88 = bitcast <4 x i64>* %87 to <16 x i16>*
  %89 = load <16 x i16>, <16 x i16>* %88, align 32
  %90 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 5
  %91 = bitcast <4 x i64>* %90 to <16 x i16>*
  %92 = load <16 x i16>, <16 x i16>* %91, align 32
  %93 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %89, <16 x i16> %92) #8
  %94 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %89, <16 x i16> %92) #8
  %95 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 3
  %96 = bitcast <4 x i64>* %95 to <16 x i16>*
  %97 = load <16 x i16>, <16 x i16>* %96, align 32
  %98 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 4
  %99 = bitcast <4 x i64>* %98 to <16 x i16>*
  %100 = load <16 x i16>, <16 x i16>* %99, align 32
  %101 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %97, <16 x i16> %100) #8
  %102 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %97, <16 x i16> %100) #8
  %103 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %77, <16 x i16> %101) #8
  %104 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %77, <16 x i16> %101) #8
  %105 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %85, <16 x i16> %93) #8
  %106 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %85, <16 x i16> %93) #8
  %107 = shufflevector <16 x i16> %94, <16 x i16> %86, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %108 = shufflevector <16 x i16> %94, <16 x i16> %86, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %109 = bitcast <8 x i32> %19 to <16 x i16>
  %110 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %107, <16 x i16> %109) #8
  %111 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %108, <16 x i16> %109) #8
  %112 = bitcast <8 x i32> %22 to <16 x i16>
  %113 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %107, <16 x i16> %112) #8
  %114 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %108, <16 x i16> %112) #8
  %115 = add <8 x i32> %110, %10
  %116 = add <8 x i32> %111, %10
  %117 = add <8 x i32> %113, %10
  %118 = add <8 x i32> %114, %10
  %119 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %115, i32 %4) #8
  %120 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %116, i32 %4) #8
  %121 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %117, i32 %4) #8
  %122 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %118, i32 %4) #8
  %123 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %119, <8 x i32> %120) #8
  %124 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %121, <8 x i32> %122) #8
  %125 = shufflevector <16 x i16> %103, <16 x i16> %105, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %126 = shufflevector <16 x i16> %103, <16 x i16> %105, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %127 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %125, <16 x i16> %112) #8
  %128 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %126, <16 x i16> %112) #8
  %129 = bitcast <8 x i32> %26 to <16 x i16>
  %130 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %125, <16 x i16> %129) #8
  %131 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %126, <16 x i16> %129) #8
  %132 = add <8 x i32> %127, %10
  %133 = add <8 x i32> %128, %10
  %134 = add <8 x i32> %130, %10
  %135 = add <8 x i32> %131, %10
  %136 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %132, i32 %4) #8
  %137 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %133, i32 %4) #8
  %138 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %134, i32 %4) #8
  %139 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %135, i32 %4) #8
  %140 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %136, <8 x i32> %137) #8
  %141 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %138, <8 x i32> %139) #8
  %142 = shufflevector <16 x i16> %106, <16 x i16> %104, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %143 = shufflevector <16 x i16> %106, <16 x i16> %104, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %144 = bitcast <8 x i32> %35 to <16 x i16>
  %145 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %142, <16 x i16> %144) #8
  %146 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %143, <16 x i16> %144) #8
  %147 = bitcast <8 x i32> %41 to <16 x i16>
  %148 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %142, <16 x i16> %147) #8
  %149 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %143, <16 x i16> %147) #8
  %150 = add <8 x i32> %145, %10
  %151 = add <8 x i32> %146, %10
  %152 = add <8 x i32> %148, %10
  %153 = add <8 x i32> %149, %10
  %154 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %150, i32 %4) #8
  %155 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %151, i32 %4) #8
  %156 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %152, i32 %4) #8
  %157 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %153, i32 %4) #8
  %158 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %154, <8 x i32> %155) #8
  %159 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %156, <8 x i32> %157) #8
  %160 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %102, <16 x i16> %123) #8
  %161 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %102, <16 x i16> %123) #8
  %162 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %78, <16 x i16> %124) #8
  %163 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %78, <16 x i16> %124) #8
  %164 = shufflevector <16 x i16> %160, <16 x i16> %163, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %165 = shufflevector <16 x i16> %160, <16 x i16> %163, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %166 = bitcast <8 x i32> %50 to <16 x i16>
  %167 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %164, <16 x i16> %166) #8
  %168 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %165, <16 x i16> %166) #8
  %169 = bitcast <8 x i32> %56 to <16 x i16>
  %170 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %164, <16 x i16> %169) #8
  %171 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %165, <16 x i16> %169) #8
  %172 = add <8 x i32> %167, %10
  %173 = add <8 x i32> %168, %10
  %174 = add <8 x i32> %170, %10
  %175 = add <8 x i32> %171, %10
  %176 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %172, i32 %4) #8
  %177 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %173, i32 %4) #8
  %178 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %174, i32 %4) #8
  %179 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %175, i32 %4) #8
  %180 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %176, <8 x i32> %177) #8
  %181 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %178, <8 x i32> %179) #8
  %182 = shufflevector <16 x i16> %161, <16 x i16> %162, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %183 = shufflevector <16 x i16> %161, <16 x i16> %162, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %184 = bitcast <8 x i32> %65 to <16 x i16>
  %185 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %182, <16 x i16> %184) #8
  %186 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %183, <16 x i16> %184) #8
  %187 = bitcast <8 x i32> %71 to <16 x i16>
  %188 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %182, <16 x i16> %187) #8
  %189 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %183, <16 x i16> %187) #8
  %190 = add <8 x i32> %185, %10
  %191 = add <8 x i32> %186, %10
  %192 = add <8 x i32> %188, %10
  %193 = add <8 x i32> %189, %10
  %194 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %190, i32 %4) #8
  %195 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %191, i32 %4) #8
  %196 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %192, i32 %4) #8
  %197 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %193, i32 %4) #8
  %198 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %194, <8 x i32> %195) #8
  %199 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %196, <8 x i32> %197) #8
  %200 = bitcast <4 x i64>* %1 to <16 x i16>*
  store <16 x i16> %140, <16 x i16>* %200, align 32
  %201 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %202 = bitcast <4 x i64>* %201 to <16 x i16>*
  store <16 x i16> %180, <16 x i16>* %202, align 32
  %203 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %204 = bitcast <4 x i64>* %203 to <16 x i16>*
  store <16 x i16> %158, <16 x i16>* %204, align 32
  %205 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %206 = bitcast <4 x i64>* %205 to <16 x i16>*
  store <16 x i16> %199, <16 x i16>* %206, align 32
  %207 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %208 = bitcast <4 x i64>* %207 to <16 x i16>*
  store <16 x i16> %141, <16 x i16>* %208, align 32
  %209 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %210 = bitcast <4 x i64>* %209 to <16 x i16>*
  store <16 x i16> %198, <16 x i16>* %210, align 32
  %211 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %212 = bitcast <4 x i64>* %211 to <16 x i16>*
  store <16 x i16> %159, <16 x i16>* %212, align 32
  %213 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %214 = bitcast <4 x i64>* %213 to <16 x i16>*
  store <16 x i16> %181, <16 x i16>* %214, align 32
  ret void
}

; Function Attrs: inlinehint nounwind ssp uwtable
define internal void @fadst8x8_new_avx2(<4 x i64>* nocapture readonly, <4 x i64>* nocapture, i8 signext) #4 {
  %4 = sext i8 %2 to i32
  %5 = add nsw i32 %4, -10
  %6 = sext i32 %5 to i64
  %7 = add nsw i32 %4, -1
  %8 = shl i32 1, %7
  %9 = insertelement <8 x i32> undef, i32 %8, i32 0
  %10 = shufflevector <8 x i32> %9, <8 x i32> undef, <8 x i32> zeroinitializer
  %11 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 32
  %12 = load i32, i32* %11, align 16
  %13 = and i32 %12, 65535
  %14 = shl nuw i32 %13, 16
  %15 = or i32 %14, %13
  %16 = insertelement <8 x i32> undef, i32 %15, i32 0
  %17 = shufflevector <8 x i32> %16, <8 x i32> undef, <8 x i32> zeroinitializer
  %18 = shl i32 %12, 16
  %19 = sub i32 0, %18
  %20 = or i32 %13, %19
  %21 = insertelement <8 x i32> undef, i32 %20, i32 0
  %22 = shufflevector <8 x i32> %21, <8 x i32> undef, <8 x i32> zeroinitializer
  %23 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 16
  %24 = load i32, i32* %23, align 16
  %25 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 48
  %26 = load i32, i32* %25, align 16
  %27 = and i32 %24, 65535
  %28 = and i32 %26, 65535
  %29 = shl nuw i32 %28, 16
  %30 = or i32 %29, %27
  %31 = insertelement <8 x i32> undef, i32 %30, i32 0
  %32 = shufflevector <8 x i32> %31, <8 x i32> undef, <8 x i32> zeroinitializer
  %33 = shl i32 %24, 16
  %34 = sub i32 0, %33
  %35 = or i32 %28, %34
  %36 = insertelement <8 x i32> undef, i32 %35, i32 0
  %37 = shufflevector <8 x i32> %36, <8 x i32> undef, <8 x i32> zeroinitializer
  %38 = sub i32 0, %26
  %39 = and i32 %38, 65535
  %40 = shl nuw i32 %27, 16
  %41 = or i32 %40, %39
  %42 = insertelement <8 x i32> undef, i32 %41, i32 0
  %43 = shufflevector <8 x i32> %42, <8 x i32> undef, <8 x i32> zeroinitializer
  %44 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 4
  %45 = load i32, i32* %44, align 16
  %46 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 60
  %47 = load i32, i32* %46, align 16
  %48 = and i32 %45, 65535
  %49 = and i32 %47, 65535
  %50 = shl nuw i32 %49, 16
  %51 = or i32 %50, %48
  %52 = insertelement <8 x i32> undef, i32 %51, i32 0
  %53 = shufflevector <8 x i32> %52, <8 x i32> undef, <8 x i32> zeroinitializer
  %54 = shl i32 %45, 16
  %55 = sub i32 0, %54
  %56 = or i32 %49, %55
  %57 = insertelement <8 x i32> undef, i32 %56, i32 0
  %58 = shufflevector <8 x i32> %57, <8 x i32> undef, <8 x i32> zeroinitializer
  %59 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 20
  %60 = load i32, i32* %59, align 16
  %61 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 44
  %62 = load i32, i32* %61, align 16
  %63 = and i32 %60, 65535
  %64 = and i32 %62, 65535
  %65 = shl nuw i32 %64, 16
  %66 = or i32 %65, %63
  %67 = insertelement <8 x i32> undef, i32 %66, i32 0
  %68 = shufflevector <8 x i32> %67, <8 x i32> undef, <8 x i32> zeroinitializer
  %69 = shl i32 %60, 16
  %70 = sub i32 0, %69
  %71 = or i32 %64, %70
  %72 = insertelement <8 x i32> undef, i32 %71, i32 0
  %73 = shufflevector <8 x i32> %72, <8 x i32> undef, <8 x i32> zeroinitializer
  %74 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 36
  %75 = load i32, i32* %74, align 16
  %76 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 28
  %77 = load i32, i32* %76, align 16
  %78 = and i32 %75, 65535
  %79 = and i32 %77, 65535
  %80 = shl nuw i32 %79, 16
  %81 = or i32 %80, %78
  %82 = insertelement <8 x i32> undef, i32 %81, i32 0
  %83 = shufflevector <8 x i32> %82, <8 x i32> undef, <8 x i32> zeroinitializer
  %84 = shl i32 %75, 16
  %85 = sub i32 0, %84
  %86 = or i32 %79, %85
  %87 = insertelement <8 x i32> undef, i32 %86, i32 0
  %88 = shufflevector <8 x i32> %87, <8 x i32> undef, <8 x i32> zeroinitializer
  %89 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 52
  %90 = load i32, i32* %89, align 16
  %91 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 12
  %92 = load i32, i32* %91, align 16
  %93 = and i32 %90, 65535
  %94 = and i32 %92, 65535
  %95 = shl nuw i32 %94, 16
  %96 = or i32 %95, %93
  %97 = insertelement <8 x i32> undef, i32 %96, i32 0
  %98 = shufflevector <8 x i32> %97, <8 x i32> undef, <8 x i32> zeroinitializer
  %99 = shl i32 %90, 16
  %100 = sub i32 0, %99
  %101 = or i32 %94, %100
  %102 = insertelement <8 x i32> undef, i32 %101, i32 0
  %103 = shufflevector <8 x i32> %102, <8 x i32> undef, <8 x i32> zeroinitializer
  %104 = bitcast <4 x i64>* %0 to <16 x i16>*
  %105 = load <16 x i16>, <16 x i16>* %104, align 32
  %106 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 7
  %107 = bitcast <4 x i64>* %106 to <16 x i16>*
  %108 = load <16 x i16>, <16 x i16>* %107, align 32
  %109 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %108) #8
  %110 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 3
  %111 = bitcast <4 x i64>* %110 to <16 x i16>*
  %112 = load <16 x i16>, <16 x i16>* %111, align 32
  %113 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %112) #8
  %114 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 4
  %115 = bitcast <4 x i64>* %114 to <16 x i16>*
  %116 = load <16 x i16>, <16 x i16>* %115, align 32
  %117 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 1
  %118 = bitcast <4 x i64>* %117 to <16 x i16>*
  %119 = load <16 x i16>, <16 x i16>* %118, align 32
  %120 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %119) #8
  %121 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 6
  %122 = bitcast <4 x i64>* %121 to <16 x i16>*
  %123 = load <16 x i16>, <16 x i16>* %122, align 32
  %124 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 2
  %125 = bitcast <4 x i64>* %124 to <16 x i16>*
  %126 = load <16 x i16>, <16 x i16>* %125, align 32
  %127 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 5
  %128 = bitcast <4 x i64>* %127 to <16 x i16>*
  %129 = load <16 x i16>, <16 x i16>* %128, align 32
  %130 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %129) #8
  %131 = shufflevector <16 x i16> %113, <16 x i16> %116, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %132 = shufflevector <16 x i16> %113, <16 x i16> %116, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %133 = bitcast <8 x i32> %17 to <16 x i16>
  %134 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %131, <16 x i16> %133) #8
  %135 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %132, <16 x i16> %133) #8
  %136 = bitcast <8 x i32> %22 to <16 x i16>
  %137 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %131, <16 x i16> %136) #8
  %138 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %132, <16 x i16> %136) #8
  %139 = add <8 x i32> %134, %10
  %140 = add <8 x i32> %135, %10
  %141 = add <8 x i32> %137, %10
  %142 = add <8 x i32> %138, %10
  %143 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %139, i32 %4) #8
  %144 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %140, i32 %4) #8
  %145 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %141, i32 %4) #8
  %146 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %142, i32 %4) #8
  %147 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %143, <8 x i32> %144) #8
  %148 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %145, <8 x i32> %146) #8
  %149 = shufflevector <16 x i16> %126, <16 x i16> %130, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %150 = shufflevector <16 x i16> %126, <16 x i16> %130, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %151 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %149, <16 x i16> %133) #8
  %152 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %150, <16 x i16> %133) #8
  %153 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %149, <16 x i16> %136) #8
  %154 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %150, <16 x i16> %136) #8
  %155 = add <8 x i32> %151, %10
  %156 = add <8 x i32> %152, %10
  %157 = add <8 x i32> %153, %10
  %158 = add <8 x i32> %154, %10
  %159 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %155, i32 %4) #8
  %160 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %156, i32 %4) #8
  %161 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %157, i32 %4) #8
  %162 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %158, i32 %4) #8
  %163 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %159, <8 x i32> %160) #8
  %164 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %161, <8 x i32> %162) #8
  %165 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %105, <16 x i16> %147) #8
  %166 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %105, <16 x i16> %147) #8
  %167 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %109, <16 x i16> %148) #8
  %168 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %109, <16 x i16> %148) #8
  %169 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %120, <16 x i16> %163) #8
  %170 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %120, <16 x i16> %163) #8
  %171 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %123, <16 x i16> %164) #8
  %172 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %123, <16 x i16> %164) #8
  %173 = shufflevector <16 x i16> %169, <16 x i16> %171, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %174 = shufflevector <16 x i16> %169, <16 x i16> %171, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %175 = bitcast <8 x i32> %32 to <16 x i16>
  %176 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %173, <16 x i16> %175) #8
  %177 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %174, <16 x i16> %175) #8
  %178 = bitcast <8 x i32> %37 to <16 x i16>
  %179 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %173, <16 x i16> %178) #8
  %180 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %174, <16 x i16> %178) #8
  %181 = add <8 x i32> %176, %10
  %182 = add <8 x i32> %177, %10
  %183 = add <8 x i32> %179, %10
  %184 = add <8 x i32> %180, %10
  %185 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %181, i32 %4) #8
  %186 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %182, i32 %4) #8
  %187 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %183, i32 %4) #8
  %188 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %184, i32 %4) #8
  %189 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %185, <8 x i32> %186) #8
  %190 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %187, <8 x i32> %188) #8
  %191 = shufflevector <16 x i16> %170, <16 x i16> %172, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %192 = shufflevector <16 x i16> %170, <16 x i16> %172, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %193 = bitcast <8 x i32> %43 to <16 x i16>
  %194 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %191, <16 x i16> %193) #8
  %195 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %192, <16 x i16> %193) #8
  %196 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %191, <16 x i16> %175) #8
  %197 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %192, <16 x i16> %175) #8
  %198 = add <8 x i32> %194, %10
  %199 = add <8 x i32> %195, %10
  %200 = add <8 x i32> %196, %10
  %201 = add <8 x i32> %197, %10
  %202 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %198, i32 %4) #8
  %203 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %199, i32 %4) #8
  %204 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %200, i32 %4) #8
  %205 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %201, i32 %4) #8
  %206 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %202, <8 x i32> %203) #8
  %207 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %204, <8 x i32> %205) #8
  %208 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %165, <16 x i16> %189) #8
  %209 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %165, <16 x i16> %189) #8
  %210 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %167, <16 x i16> %190) #8
  %211 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %167, <16 x i16> %190) #8
  %212 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %166, <16 x i16> %206) #8
  %213 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %166, <16 x i16> %206) #8
  %214 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %168, <16 x i16> %207) #8
  %215 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %168, <16 x i16> %207) #8
  %216 = shufflevector <16 x i16> %208, <16 x i16> %210, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %217 = shufflevector <16 x i16> %208, <16 x i16> %210, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %218 = bitcast <8 x i32> %53 to <16 x i16>
  %219 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %216, <16 x i16> %218) #8
  %220 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %217, <16 x i16> %218) #8
  %221 = bitcast <8 x i32> %58 to <16 x i16>
  %222 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %216, <16 x i16> %221) #8
  %223 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %217, <16 x i16> %221) #8
  %224 = add <8 x i32> %219, %10
  %225 = add <8 x i32> %220, %10
  %226 = add <8 x i32> %222, %10
  %227 = add <8 x i32> %223, %10
  %228 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %224, i32 %4) #8
  %229 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %225, i32 %4) #8
  %230 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %226, i32 %4) #8
  %231 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %227, i32 %4) #8
  %232 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %228, <8 x i32> %229) #8
  %233 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %230, <8 x i32> %231) #8
  %234 = shufflevector <16 x i16> %212, <16 x i16> %214, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %235 = shufflevector <16 x i16> %212, <16 x i16> %214, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %236 = bitcast <8 x i32> %68 to <16 x i16>
  %237 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %234, <16 x i16> %236) #8
  %238 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %235, <16 x i16> %236) #8
  %239 = bitcast <8 x i32> %73 to <16 x i16>
  %240 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %234, <16 x i16> %239) #8
  %241 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %235, <16 x i16> %239) #8
  %242 = add <8 x i32> %237, %10
  %243 = add <8 x i32> %238, %10
  %244 = add <8 x i32> %240, %10
  %245 = add <8 x i32> %241, %10
  %246 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %242, i32 %4) #8
  %247 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %243, i32 %4) #8
  %248 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %244, i32 %4) #8
  %249 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %245, i32 %4) #8
  %250 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %246, <8 x i32> %247) #8
  %251 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %248, <8 x i32> %249) #8
  %252 = shufflevector <16 x i16> %209, <16 x i16> %211, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %253 = shufflevector <16 x i16> %209, <16 x i16> %211, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %254 = bitcast <8 x i32> %83 to <16 x i16>
  %255 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %252, <16 x i16> %254) #8
  %256 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %253, <16 x i16> %254) #8
  %257 = bitcast <8 x i32> %88 to <16 x i16>
  %258 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %252, <16 x i16> %257) #8
  %259 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %253, <16 x i16> %257) #8
  %260 = add <8 x i32> %255, %10
  %261 = add <8 x i32> %256, %10
  %262 = add <8 x i32> %258, %10
  %263 = add <8 x i32> %259, %10
  %264 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %260, i32 %4) #8
  %265 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %261, i32 %4) #8
  %266 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %262, i32 %4) #8
  %267 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %263, i32 %4) #8
  %268 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %264, <8 x i32> %265) #8
  %269 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %266, <8 x i32> %267) #8
  %270 = shufflevector <16 x i16> %213, <16 x i16> %215, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %271 = shufflevector <16 x i16> %213, <16 x i16> %215, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %272 = bitcast <8 x i32> %98 to <16 x i16>
  %273 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %270, <16 x i16> %272) #8
  %274 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %271, <16 x i16> %272) #8
  %275 = bitcast <8 x i32> %103 to <16 x i16>
  %276 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %270, <16 x i16> %275) #8
  %277 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %271, <16 x i16> %275) #8
  %278 = add <8 x i32> %273, %10
  %279 = add <8 x i32> %274, %10
  %280 = add <8 x i32> %276, %10
  %281 = add <8 x i32> %277, %10
  %282 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %278, i32 %4) #8
  %283 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %279, i32 %4) #8
  %284 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %280, i32 %4) #8
  %285 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %281, i32 %4) #8
  %286 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %282, <8 x i32> %283) #8
  %287 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %284, <8 x i32> %285) #8
  %288 = bitcast <4 x i64>* %1 to <16 x i16>*
  store <16 x i16> %233, <16 x i16>* %288, align 32
  %289 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %290 = bitcast <4 x i64>* %289 to <16 x i16>*
  store <16 x i16> %286, <16 x i16>* %290, align 32
  %291 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %292 = bitcast <4 x i64>* %291 to <16 x i16>*
  store <16 x i16> %251, <16 x i16>* %292, align 32
  %293 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %294 = bitcast <4 x i64>* %293 to <16 x i16>*
  store <16 x i16> %268, <16 x i16>* %294, align 32
  %295 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %296 = bitcast <4 x i64>* %295 to <16 x i16>*
  store <16 x i16> %269, <16 x i16>* %296, align 32
  %297 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %298 = bitcast <4 x i64>* %297 to <16 x i16>*
  store <16 x i16> %250, <16 x i16>* %298, align 32
  %299 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %300 = bitcast <4 x i64>* %299 to <16 x i16>*
  store <16 x i16> %287, <16 x i16>* %300, align 32
  %301 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %302 = bitcast <4 x i64>* %301 to <16 x i16>*
  store <16 x i16> %232, <16 x i16>* %302, align 32
  ret void
}

; Function Attrs: inlinehint nofree nounwind ssp uwtable
define internal void @fidentity8x8_new_avx2(<4 x i64>* nocapture readonly, <4 x i64>* nocapture, i8 signext) #5 {
  %4 = bitcast <4 x i64>* %0 to <16 x i16>*
  %5 = load <16 x i16>, <16 x i16>* %4, align 32
  %6 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %5, <16 x i16> %5) #8
  %7 = bitcast <4 x i64>* %1 to <16 x i16>*
  store <16 x i16> %6, <16 x i16>* %7, align 32
  %8 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 1
  %9 = bitcast <4 x i64>* %8 to <16 x i16>*
  %10 = load <16 x i16>, <16 x i16>* %9, align 32
  %11 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %10, <16 x i16> %10) #8
  %12 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %13 = bitcast <4 x i64>* %12 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %13, align 32
  %14 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 2
  %15 = bitcast <4 x i64>* %14 to <16 x i16>*
  %16 = load <16 x i16>, <16 x i16>* %15, align 32
  %17 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %16, <16 x i16> %16) #8
  %18 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %19 = bitcast <4 x i64>* %18 to <16 x i16>*
  store <16 x i16> %17, <16 x i16>* %19, align 32
  %20 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 3
  %21 = bitcast <4 x i64>* %20 to <16 x i16>*
  %22 = load <16 x i16>, <16 x i16>* %21, align 32
  %23 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %22, <16 x i16> %22) #8
  %24 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %25 = bitcast <4 x i64>* %24 to <16 x i16>*
  store <16 x i16> %23, <16 x i16>* %25, align 32
  %26 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 4
  %27 = bitcast <4 x i64>* %26 to <16 x i16>*
  %28 = load <16 x i16>, <16 x i16>* %27, align 32
  %29 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %28, <16 x i16> %28) #8
  %30 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %31 = bitcast <4 x i64>* %30 to <16 x i16>*
  store <16 x i16> %29, <16 x i16>* %31, align 32
  %32 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 5
  %33 = bitcast <4 x i64>* %32 to <16 x i16>*
  %34 = load <16 x i16>, <16 x i16>* %33, align 32
  %35 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %34, <16 x i16> %34) #8
  %36 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %37 = bitcast <4 x i64>* %36 to <16 x i16>*
  store <16 x i16> %35, <16 x i16>* %37, align 32
  %38 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 6
  %39 = bitcast <4 x i64>* %38 to <16 x i16>*
  %40 = load <16 x i16>, <16 x i16>* %39, align 32
  %41 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %40, <16 x i16> %40) #8
  %42 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %43 = bitcast <4 x i64>* %42 to <16 x i16>*
  store <16 x i16> %41, <16 x i16>* %43, align 32
  %44 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 7
  %45 = bitcast <4 x i64>* %44 to <16 x i16>*
  %46 = load <16 x i16>, <16 x i16>* %45, align 32
  %47 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %46, <16 x i16> %46) #8
  %48 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %49 = bitcast <4 x i64>* %48 to <16 x i16>*
  store <16 x i16> %47, <16 x i16>* %49, align 32
  ret void
}

; Function Attrs: nounwind readnone speculatable
declare <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16>, <8 x i16>) #6

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16>, i32) #7

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16>, i32) #7

; Function Attrs: inlinehint nounwind ssp uwtable
define internal fastcc void @fdct32_avx2(<4 x i64>* readonly, <4 x i64>*, i8 signext) unnamed_addr #4 {
  %4 = sext i8 %2 to i32
  %5 = add nsw i32 %4, -10
  %6 = sext i32 %5 to i64
  %7 = add nsw i32 %4, -1
  %8 = shl i32 1, %7
  %9 = insertelement <8 x i32> undef, i32 %8, i32 0
  %10 = shufflevector <8 x i32> %9, <8 x i32> undef, <8 x i32> zeroinitializer
  %11 = bitcast <4 x i64>* %0 to <8 x i32>*
  %12 = load <8 x i32>, <8 x i32>* %11, align 32
  %13 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 31
  %14 = bitcast <4 x i64>* %13 to <8 x i32>*
  %15 = load <8 x i32>, <8 x i32>* %14, align 32
  %16 = add <8 x i32> %15, %12
  %17 = sub <8 x i32> %12, %15
  %18 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 1
  %19 = bitcast <4 x i64>* %18 to <8 x i32>*
  %20 = load <8 x i32>, <8 x i32>* %19, align 32
  %21 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 30
  %22 = bitcast <4 x i64>* %21 to <8 x i32>*
  %23 = load <8 x i32>, <8 x i32>* %22, align 32
  %24 = add <8 x i32> %23, %20
  %25 = sub <8 x i32> %20, %23
  %26 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 2
  %27 = bitcast <4 x i64>* %26 to <8 x i32>*
  %28 = load <8 x i32>, <8 x i32>* %27, align 32
  %29 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 29
  %30 = bitcast <4 x i64>* %29 to <8 x i32>*
  %31 = load <8 x i32>, <8 x i32>* %30, align 32
  %32 = add <8 x i32> %31, %28
  %33 = sub <8 x i32> %28, %31
  %34 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 3
  %35 = bitcast <4 x i64>* %34 to <8 x i32>*
  %36 = load <8 x i32>, <8 x i32>* %35, align 32
  %37 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 28
  %38 = bitcast <4 x i64>* %37 to <8 x i32>*
  %39 = load <8 x i32>, <8 x i32>* %38, align 32
  %40 = add <8 x i32> %39, %36
  %41 = sub <8 x i32> %36, %39
  %42 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 4
  %43 = bitcast <4 x i64>* %42 to <8 x i32>*
  %44 = load <8 x i32>, <8 x i32>* %43, align 32
  %45 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 27
  %46 = bitcast <4 x i64>* %45 to <8 x i32>*
  %47 = load <8 x i32>, <8 x i32>* %46, align 32
  %48 = add <8 x i32> %47, %44
  %49 = sub <8 x i32> %44, %47
  %50 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 5
  %51 = bitcast <4 x i64>* %50 to <8 x i32>*
  %52 = load <8 x i32>, <8 x i32>* %51, align 32
  %53 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 26
  %54 = bitcast <4 x i64>* %53 to <8 x i32>*
  %55 = load <8 x i32>, <8 x i32>* %54, align 32
  %56 = add <8 x i32> %55, %52
  %57 = sub <8 x i32> %52, %55
  %58 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 6
  %59 = bitcast <4 x i64>* %58 to <8 x i32>*
  %60 = load <8 x i32>, <8 x i32>* %59, align 32
  %61 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 25
  %62 = bitcast <4 x i64>* %61 to <8 x i32>*
  %63 = load <8 x i32>, <8 x i32>* %62, align 32
  %64 = add <8 x i32> %63, %60
  %65 = sub <8 x i32> %60, %63
  %66 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 7
  %67 = bitcast <4 x i64>* %66 to <8 x i32>*
  %68 = load <8 x i32>, <8 x i32>* %67, align 32
  %69 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 24
  %70 = bitcast <4 x i64>* %69 to <8 x i32>*
  %71 = load <8 x i32>, <8 x i32>* %70, align 32
  %72 = add <8 x i32> %71, %68
  %73 = sub <8 x i32> %68, %71
  %74 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 8
  %75 = bitcast <4 x i64>* %74 to <8 x i32>*
  %76 = load <8 x i32>, <8 x i32>* %75, align 32
  %77 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 23
  %78 = bitcast <4 x i64>* %77 to <8 x i32>*
  %79 = load <8 x i32>, <8 x i32>* %78, align 32
  %80 = add <8 x i32> %79, %76
  %81 = sub <8 x i32> %76, %79
  %82 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 9
  %83 = bitcast <4 x i64>* %82 to <8 x i32>*
  %84 = load <8 x i32>, <8 x i32>* %83, align 32
  %85 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 22
  %86 = bitcast <4 x i64>* %85 to <8 x i32>*
  %87 = load <8 x i32>, <8 x i32>* %86, align 32
  %88 = add <8 x i32> %87, %84
  %89 = sub <8 x i32> %84, %87
  %90 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 10
  %91 = bitcast <4 x i64>* %90 to <8 x i32>*
  %92 = load <8 x i32>, <8 x i32>* %91, align 32
  %93 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 21
  %94 = bitcast <4 x i64>* %93 to <8 x i32>*
  %95 = load <8 x i32>, <8 x i32>* %94, align 32
  %96 = add <8 x i32> %95, %92
  %97 = sub <8 x i32> %92, %95
  %98 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 11
  %99 = bitcast <4 x i64>* %98 to <8 x i32>*
  %100 = load <8 x i32>, <8 x i32>* %99, align 32
  %101 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 20
  %102 = bitcast <4 x i64>* %101 to <8 x i32>*
  %103 = load <8 x i32>, <8 x i32>* %102, align 32
  %104 = add <8 x i32> %103, %100
  %105 = sub <8 x i32> %100, %103
  %106 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 12
  %107 = bitcast <4 x i64>* %106 to <8 x i32>*
  %108 = load <8 x i32>, <8 x i32>* %107, align 32
  %109 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 19
  %110 = bitcast <4 x i64>* %109 to <8 x i32>*
  %111 = load <8 x i32>, <8 x i32>* %110, align 32
  %112 = add <8 x i32> %111, %108
  %113 = sub <8 x i32> %108, %111
  %114 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 13
  %115 = bitcast <4 x i64>* %114 to <8 x i32>*
  %116 = load <8 x i32>, <8 x i32>* %115, align 32
  %117 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 18
  %118 = bitcast <4 x i64>* %117 to <8 x i32>*
  %119 = load <8 x i32>, <8 x i32>* %118, align 32
  %120 = add <8 x i32> %119, %116
  %121 = sub <8 x i32> %116, %119
  %122 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 14
  %123 = bitcast <4 x i64>* %122 to <8 x i32>*
  %124 = load <8 x i32>, <8 x i32>* %123, align 32
  %125 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 17
  %126 = bitcast <4 x i64>* %125 to <8 x i32>*
  %127 = load <8 x i32>, <8 x i32>* %126, align 32
  %128 = add <8 x i32> %127, %124
  %129 = sub <8 x i32> %124, %127
  %130 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 15
  %131 = bitcast <4 x i64>* %130 to <8 x i32>*
  %132 = load <8 x i32>, <8 x i32>* %131, align 32
  %133 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 16
  %134 = bitcast <4 x i64>* %133 to <8 x i32>*
  %135 = load <8 x i32>, <8 x i32>* %134, align 32
  %136 = add <8 x i32> %135, %132
  %137 = sub <8 x i32> %132, %135
  %138 = add <8 x i32> %136, %16
  %139 = sub <8 x i32> %16, %136
  %140 = add <8 x i32> %128, %24
  %141 = sub <8 x i32> %24, %128
  %142 = add <8 x i32> %120, %32
  %143 = sub <8 x i32> %32, %120
  %144 = add <8 x i32> %112, %40
  %145 = sub <8 x i32> %40, %112
  %146 = add <8 x i32> %104, %48
  %147 = sub <8 x i32> %48, %104
  %148 = add <8 x i32> %96, %56
  %149 = sub <8 x i32> %56, %96
  %150 = add <8 x i32> %88, %64
  %151 = sub <8 x i32> %64, %88
  %152 = add <8 x i32> %80, %72
  %153 = sub <8 x i32> %72, %80
  %154 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 32
  %155 = load i32, i32* %154, align 16
  %156 = sub nsw i32 0, %155
  %157 = insertelement <8 x i32> undef, i32 %156, i32 0
  %158 = shufflevector <8 x i32> %157, <8 x i32> undef, <8 x i32> zeroinitializer
  %159 = insertelement <8 x i32> undef, i32 %155, i32 0
  %160 = shufflevector <8 x i32> %159, <8 x i32> undef, <8 x i32> zeroinitializer
  %161 = mul <8 x i32> %158, %105
  %162 = mul <8 x i32> %160, %49
  %163 = add <8 x i32> %161, %10
  %164 = add <8 x i32> %163, %162
  %165 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %164, i32 %4) #8
  %166 = mul <8 x i32> %160, %105
  %167 = add <8 x i32> %166, %10
  %168 = mul <8 x i32> %49, %158
  %169 = sub <8 x i32> %167, %168
  %170 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %169, i32 %4) #8
  %171 = mul <8 x i32> %158, %97
  %172 = mul <8 x i32> %160, %57
  %173 = add <8 x i32> %171, %10
  %174 = add <8 x i32> %173, %172
  %175 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %174, i32 %4) #8
  %176 = mul <8 x i32> %160, %97
  %177 = add <8 x i32> %176, %10
  %178 = mul <8 x i32> %57, %158
  %179 = sub <8 x i32> %177, %178
  %180 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %179, i32 %4) #8
  %181 = mul <8 x i32> %158, %89
  %182 = mul <8 x i32> %160, %65
  %183 = add <8 x i32> %181, %10
  %184 = add <8 x i32> %183, %182
  %185 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %184, i32 %4) #8
  %186 = mul <8 x i32> %160, %89
  %187 = add <8 x i32> %186, %10
  %188 = mul <8 x i32> %65, %158
  %189 = sub <8 x i32> %187, %188
  %190 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %189, i32 %4) #8
  %191 = mul <8 x i32> %158, %81
  %192 = mul <8 x i32> %160, %73
  %193 = add <8 x i32> %191, %10
  %194 = add <8 x i32> %193, %192
  %195 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %194, i32 %4) #8
  %196 = mul <8 x i32> %160, %81
  %197 = add <8 x i32> %196, %10
  %198 = mul <8 x i32> %73, %158
  %199 = sub <8 x i32> %197, %198
  %200 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %199, i32 %4) #8
  %201 = add <8 x i32> %138, %152
  %202 = sub <8 x i32> %138, %152
  %203 = add <8 x i32> %140, %150
  %204 = sub <8 x i32> %140, %150
  %205 = add <8 x i32> %142, %148
  %206 = sub <8 x i32> %142, %148
  %207 = add <8 x i32> %144, %146
  %208 = sub <8 x i32> %144, %146
  %209 = mul <8 x i32> %158, %149
  %210 = mul <8 x i32> %160, %143
  %211 = add <8 x i32> %209, %10
  %212 = add <8 x i32> %211, %210
  %213 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %212, i32 %4) #8
  %214 = mul <8 x i32> %160, %149
  %215 = add <8 x i32> %214, %10
  %216 = mul <8 x i32> %143, %158
  %217 = sub <8 x i32> %215, %216
  %218 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %217, i32 %4) #8
  %219 = mul <8 x i32> %158, %147
  %220 = mul <8 x i32> %160, %145
  %221 = add <8 x i32> %219, %10
  %222 = add <8 x i32> %221, %220
  %223 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %222, i32 %4) #8
  %224 = mul <8 x i32> %160, %147
  %225 = add <8 x i32> %224, %10
  %226 = mul <8 x i32> %145, %158
  %227 = sub <8 x i32> %225, %226
  %228 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %227, i32 %4) #8
  %229 = add <8 x i32> %195, %137
  %230 = sub <8 x i32> %137, %195
  %231 = add <8 x i32> %185, %129
  %232 = sub <8 x i32> %129, %185
  %233 = add <8 x i32> %175, %121
  %234 = sub <8 x i32> %121, %175
  %235 = add <8 x i32> %165, %113
  %236 = sub <8 x i32> %113, %165
  %237 = add <8 x i32> %200, %17
  %238 = sub <8 x i32> %17, %200
  %239 = add <8 x i32> %190, %25
  %240 = sub <8 x i32> %25, %190
  %241 = add <8 x i32> %180, %33
  %242 = sub <8 x i32> %33, %180
  %243 = add <8 x i32> %170, %41
  %244 = sub <8 x i32> %41, %170
  %245 = add <8 x i32> %201, %207
  %246 = sub <8 x i32> %201, %207
  %247 = add <8 x i32> %203, %205
  %248 = sub <8 x i32> %203, %205
  %249 = mul <8 x i32> %158, %206
  %250 = mul <8 x i32> %160, %204
  %251 = add <8 x i32> %249, %10
  %252 = add <8 x i32> %251, %250
  %253 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %252, i32 %4) #8
  %254 = mul <8 x i32> %160, %206
  %255 = add <8 x i32> %254, %10
  %256 = mul <8 x i32> %204, %158
  %257 = sub <8 x i32> %255, %256
  %258 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %257, i32 %4) #8
  %259 = add <8 x i32> %223, %153
  %260 = sub <8 x i32> %153, %223
  %261 = add <8 x i32> %213, %151
  %262 = sub <8 x i32> %151, %213
  %263 = add <8 x i32> %228, %139
  %264 = sub <8 x i32> %139, %228
  %265 = add <8 x i32> %218, %141
  %266 = sub <8 x i32> %141, %218
  %267 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 16
  %268 = load i32, i32* %267, align 16
  %269 = sub nsw i32 0, %268
  %270 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 48
  %271 = load i32, i32* %270, align 16
  %272 = insertelement <8 x i32> undef, i32 %269, i32 0
  %273 = shufflevector <8 x i32> %272, <8 x i32> undef, <8 x i32> zeroinitializer
  %274 = insertelement <8 x i32> undef, i32 %271, i32 0
  %275 = shufflevector <8 x i32> %274, <8 x i32> undef, <8 x i32> zeroinitializer
  %276 = mul <8 x i32> %273, %233
  %277 = mul <8 x i32> %275, %241
  %278 = add <8 x i32> %276, %10
  %279 = add <8 x i32> %278, %277
  %280 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %279, i32 %4) #8
  %281 = mul <8 x i32> %275, %233
  %282 = mul <8 x i32> %241, %273
  %283 = sub <8 x i32> %10, %282
  %284 = add <8 x i32> %283, %281
  %285 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %284, i32 %4) #8
  %286 = mul <8 x i32> %273, %235
  %287 = mul <8 x i32> %275, %243
  %288 = add <8 x i32> %286, %10
  %289 = add <8 x i32> %288, %287
  %290 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %289, i32 %4) #8
  %291 = mul <8 x i32> %275, %235
  %292 = mul <8 x i32> %243, %273
  %293 = sub <8 x i32> %10, %292
  %294 = add <8 x i32> %293, %291
  %295 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %294, i32 %4) #8
  %296 = sub nsw i32 0, %271
  %297 = insertelement <8 x i32> undef, i32 %296, i32 0
  %298 = shufflevector <8 x i32> %297, <8 x i32> undef, <8 x i32> zeroinitializer
  %299 = mul <8 x i32> %298, %236
  %300 = mul <8 x i32> %273, %244
  %301 = add <8 x i32> %300, %10
  %302 = add <8 x i32> %301, %299
  %303 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %302, i32 %4) #8
  %304 = mul <8 x i32> %273, %236
  %305 = add <8 x i32> %304, %10
  %306 = mul <8 x i32> %244, %298
  %307 = sub <8 x i32> %305, %306
  %308 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %307, i32 %4) #8
  %309 = mul <8 x i32> %298, %234
  %310 = mul <8 x i32> %273, %242
  %311 = add <8 x i32> %310, %10
  %312 = add <8 x i32> %311, %309
  %313 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %312, i32 %4) #8
  %314 = mul <8 x i32> %273, %234
  %315 = add <8 x i32> %314, %10
  %316 = mul <8 x i32> %242, %298
  %317 = sub <8 x i32> %315, %316
  %318 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %317, i32 %4) #8
  %319 = mul <8 x i32> %245, %160
  %320 = mul <8 x i32> %160, %247
  %321 = add <8 x i32> %319, %10
  %322 = add <8 x i32> %321, %320
  %323 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %322, i32 %4) #8
  %324 = sub <8 x i32> %321, %320
  %325 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %324, i32 %4) #8
  %326 = insertelement <8 x i32> undef, i32 %268, i32 0
  %327 = shufflevector <8 x i32> %326, <8 x i32> undef, <8 x i32> zeroinitializer
  %328 = mul <8 x i32> %275, %248
  %329 = mul <8 x i32> %327, %246
  %330 = add <8 x i32> %329, %10
  %331 = add <8 x i32> %330, %328
  %332 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %331, i32 %4) #8
  %333 = mul <8 x i32> %275, %246
  %334 = mul <8 x i32> %248, %327
  %335 = sub <8 x i32> %10, %334
  %336 = add <8 x i32> %335, %333
  %337 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %336, i32 %4) #8
  %338 = add <8 x i32> %253, %208
  %339 = sub <8 x i32> %208, %253
  %340 = add <8 x i32> %258, %202
  %341 = sub <8 x i32> %202, %258
  %342 = mul <8 x i32> %273, %261
  %343 = mul <8 x i32> %275, %265
  %344 = add <8 x i32> %342, %10
  %345 = add <8 x i32> %344, %343
  %346 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %345, i32 %4) #8
  %347 = mul <8 x i32> %275, %261
  %348 = mul <8 x i32> %265, %273
  %349 = sub <8 x i32> %10, %348
  %350 = add <8 x i32> %349, %347
  %351 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %350, i32 %4) #8
  %352 = mul <8 x i32> %298, %262
  %353 = mul <8 x i32> %273, %266
  %354 = add <8 x i32> %353, %10
  %355 = add <8 x i32> %354, %352
  %356 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %355, i32 %4) #8
  %357 = mul <8 x i32> %273, %262
  %358 = add <8 x i32> %357, %10
  %359 = mul <8 x i32> %266, %298
  %360 = sub <8 x i32> %358, %359
  %361 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %360, i32 %4) #8
  %362 = add <8 x i32> %290, %229
  %363 = sub <8 x i32> %229, %290
  %364 = add <8 x i32> %280, %231
  %365 = sub <8 x i32> %231, %280
  %366 = add <8 x i32> %303, %230
  %367 = sub <8 x i32> %230, %303
  %368 = add <8 x i32> %313, %232
  %369 = sub <8 x i32> %232, %313
  %370 = add <8 x i32> %308, %238
  %371 = sub <8 x i32> %238, %308
  %372 = add <8 x i32> %318, %240
  %373 = sub <8 x i32> %240, %318
  %374 = add <8 x i32> %295, %237
  %375 = sub <8 x i32> %237, %295
  %376 = add <8 x i32> %285, %239
  %377 = sub <8 x i32> %239, %285
  %378 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 56
  %379 = load i32, i32* %378, align 16
  %380 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 8
  %381 = load i32, i32* %380, align 16
  %382 = insertelement <8 x i32> undef, i32 %379, i32 0
  %383 = shufflevector <8 x i32> %382, <8 x i32> undef, <8 x i32> zeroinitializer
  %384 = insertelement <8 x i32> undef, i32 %381, i32 0
  %385 = shufflevector <8 x i32> %384, <8 x i32> undef, <8 x i32> zeroinitializer
  %386 = mul <8 x i32> %383, %338
  %387 = mul <8 x i32> %385, %340
  %388 = add <8 x i32> %386, %10
  %389 = add <8 x i32> %388, %387
  %390 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %389, i32 %4) #8
  %391 = mul <8 x i32> %383, %340
  %392 = mul <8 x i32> %338, %385
  %393 = add <8 x i32> %391, %10
  %394 = sub <8 x i32> %393, %392
  %395 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %394, i32 %4) #8
  %396 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 24
  %397 = load i32, i32* %396, align 16
  %398 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 40
  %399 = load i32, i32* %398, align 16
  %400 = insertelement <8 x i32> undef, i32 %397, i32 0
  %401 = shufflevector <8 x i32> %400, <8 x i32> undef, <8 x i32> zeroinitializer
  %402 = insertelement <8 x i32> undef, i32 %399, i32 0
  %403 = shufflevector <8 x i32> %402, <8 x i32> undef, <8 x i32> zeroinitializer
  %404 = mul <8 x i32> %401, %339
  %405 = mul <8 x i32> %403, %341
  %406 = add <8 x i32> %404, %10
  %407 = add <8 x i32> %406, %405
  %408 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %407, i32 %4) #8
  %409 = mul <8 x i32> %401, %341
  %410 = mul <8 x i32> %339, %403
  %411 = add <8 x i32> %409, %10
  %412 = sub <8 x i32> %411, %410
  %413 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %412, i32 %4) #8
  %414 = add <8 x i32> %346, %259
  %415 = sub <8 x i32> %259, %346
  %416 = add <8 x i32> %356, %260
  %417 = sub <8 x i32> %260, %356
  %418 = add <8 x i32> %361, %264
  %419 = sub <8 x i32> %264, %361
  %420 = add <8 x i32> %351, %263
  %421 = sub <8 x i32> %263, %351
  %422 = sub nsw i32 0, %381
  %423 = insertelement <8 x i32> undef, i32 %422, i32 0
  %424 = shufflevector <8 x i32> %423, <8 x i32> undef, <8 x i32> zeroinitializer
  %425 = mul <8 x i32> %424, %364
  %426 = mul <8 x i32> %383, %376
  %427 = add <8 x i32> %426, %10
  %428 = add <8 x i32> %427, %425
  %429 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %428, i32 %4) #8
  %430 = mul <8 x i32> %383, %364
  %431 = add <8 x i32> %430, %10
  %432 = mul <8 x i32> %376, %424
  %433 = sub <8 x i32> %431, %432
  %434 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %433, i32 %4) #8
  %435 = sub nsw i32 0, %379
  %436 = insertelement <8 x i32> undef, i32 %435, i32 0
  %437 = shufflevector <8 x i32> %436, <8 x i32> undef, <8 x i32> zeroinitializer
  %438 = mul <8 x i32> %437, %365
  %439 = mul <8 x i32> %424, %377
  %440 = add <8 x i32> %438, %10
  %441 = add <8 x i32> %440, %439
  %442 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %441, i32 %4) #8
  %443 = mul <8 x i32> %424, %365
  %444 = mul <8 x i32> %377, %437
  %445 = sub <8 x i32> %10, %444
  %446 = add <8 x i32> %445, %443
  %447 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %446, i32 %4) #8
  %448 = sub nsw i32 0, %399
  %449 = insertelement <8 x i32> undef, i32 %448, i32 0
  %450 = shufflevector <8 x i32> %449, <8 x i32> undef, <8 x i32> zeroinitializer
  %451 = mul <8 x i32> %450, %369
  %452 = mul <8 x i32> %401, %373
  %453 = add <8 x i32> %452, %10
  %454 = add <8 x i32> %453, %451
  %455 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %454, i32 %4) #8
  %456 = mul <8 x i32> %401, %369
  %457 = add <8 x i32> %456, %10
  %458 = mul <8 x i32> %373, %450
  %459 = sub <8 x i32> %457, %458
  %460 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %459, i32 %4) #8
  %461 = sub nsw i32 0, %397
  %462 = insertelement <8 x i32> undef, i32 %461, i32 0
  %463 = shufflevector <8 x i32> %462, <8 x i32> undef, <8 x i32> zeroinitializer
  %464 = mul <8 x i32> %463, %368
  %465 = mul <8 x i32> %450, %372
  %466 = add <8 x i32> %464, %10
  %467 = add <8 x i32> %466, %465
  %468 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %467, i32 %4) #8
  %469 = mul <8 x i32> %450, %368
  %470 = mul <8 x i32> %372, %463
  %471 = sub <8 x i32> %10, %470
  %472 = add <8 x i32> %471, %469
  %473 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %472, i32 %4) #8
  %474 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 60
  %475 = load i32, i32* %474, align 16
  %476 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 4
  %477 = load i32, i32* %476, align 16
  %478 = insertelement <8 x i32> undef, i32 %475, i32 0
  %479 = shufflevector <8 x i32> %478, <8 x i32> undef, <8 x i32> zeroinitializer
  %480 = insertelement <8 x i32> undef, i32 %477, i32 0
  %481 = shufflevector <8 x i32> %480, <8 x i32> undef, <8 x i32> zeroinitializer
  %482 = mul <8 x i32> %479, %414
  %483 = mul <8 x i32> %481, %420
  %484 = add <8 x i32> %482, %10
  %485 = add <8 x i32> %484, %483
  %486 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %485, i32 %4) #8
  %487 = mul <8 x i32> %479, %420
  %488 = mul <8 x i32> %414, %481
  %489 = add <8 x i32> %487, %10
  %490 = sub <8 x i32> %489, %488
  %491 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %490, i32 %4) #8
  %492 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 28
  %493 = load i32, i32* %492, align 16
  %494 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 36
  %495 = load i32, i32* %494, align 16
  %496 = insertelement <8 x i32> undef, i32 %493, i32 0
  %497 = shufflevector <8 x i32> %496, <8 x i32> undef, <8 x i32> zeroinitializer
  %498 = insertelement <8 x i32> undef, i32 %495, i32 0
  %499 = shufflevector <8 x i32> %498, <8 x i32> undef, <8 x i32> zeroinitializer
  %500 = mul <8 x i32> %497, %415
  %501 = mul <8 x i32> %499, %421
  %502 = add <8 x i32> %500, %10
  %503 = add <8 x i32> %502, %501
  %504 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %503, i32 %4) #8
  %505 = mul <8 x i32> %497, %421
  %506 = mul <8 x i32> %415, %499
  %507 = add <8 x i32> %505, %10
  %508 = sub <8 x i32> %507, %506
  %509 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %508, i32 %4) #8
  %510 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 44
  %511 = load i32, i32* %510, align 16
  %512 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 20
  %513 = load i32, i32* %512, align 16
  %514 = insertelement <8 x i32> undef, i32 %511, i32 0
  %515 = shufflevector <8 x i32> %514, <8 x i32> undef, <8 x i32> zeroinitializer
  %516 = insertelement <8 x i32> undef, i32 %513, i32 0
  %517 = shufflevector <8 x i32> %516, <8 x i32> undef, <8 x i32> zeroinitializer
  %518 = mul <8 x i32> %515, %417
  %519 = mul <8 x i32> %517, %419
  %520 = add <8 x i32> %518, %10
  %521 = add <8 x i32> %520, %519
  %522 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %521, i32 %4) #8
  %523 = mul <8 x i32> %515, %419
  %524 = mul <8 x i32> %417, %517
  %525 = add <8 x i32> %523, %10
  %526 = sub <8 x i32> %525, %524
  %527 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %526, i32 %4) #8
  %528 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 12
  %529 = load i32, i32* %528, align 16
  %530 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 52
  %531 = load i32, i32* %530, align 16
  %532 = insertelement <8 x i32> undef, i32 %529, i32 0
  %533 = shufflevector <8 x i32> %532, <8 x i32> undef, <8 x i32> zeroinitializer
  %534 = insertelement <8 x i32> undef, i32 %531, i32 0
  %535 = shufflevector <8 x i32> %534, <8 x i32> undef, <8 x i32> zeroinitializer
  %536 = mul <8 x i32> %533, %416
  %537 = mul <8 x i32> %535, %418
  %538 = add <8 x i32> %536, %10
  %539 = add <8 x i32> %538, %537
  %540 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %539, i32 %4) #8
  %541 = mul <8 x i32> %533, %418
  %542 = mul <8 x i32> %416, %535
  %543 = add <8 x i32> %541, %10
  %544 = sub <8 x i32> %543, %542
  %545 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %544, i32 %4) #8
  %546 = add <8 x i32> %429, %362
  %547 = sub <8 x i32> %362, %429
  %548 = add <8 x i32> %442, %363
  %549 = sub <8 x i32> %363, %442
  %550 = add <8 x i32> %455, %367
  %551 = sub <8 x i32> %367, %455
  %552 = add <8 x i32> %468, %366
  %553 = sub <8 x i32> %366, %468
  %554 = add <8 x i32> %473, %370
  %555 = sub <8 x i32> %370, %473
  %556 = add <8 x i32> %460, %371
  %557 = sub <8 x i32> %371, %460
  %558 = add <8 x i32> %447, %375
  %559 = sub <8 x i32> %375, %447
  %560 = add <8 x i32> %434, %374
  %561 = sub <8 x i32> %374, %434
  %562 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 62
  %563 = load i32, i32* %562, align 8
  %564 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 2
  %565 = load i32, i32* %564, align 8
  %566 = insertelement <8 x i32> undef, i32 %563, i32 0
  %567 = shufflevector <8 x i32> %566, <8 x i32> undef, <8 x i32> zeroinitializer
  %568 = insertelement <8 x i32> undef, i32 %565, i32 0
  %569 = shufflevector <8 x i32> %568, <8 x i32> undef, <8 x i32> zeroinitializer
  %570 = mul <8 x i32> %567, %546
  %571 = mul <8 x i32> %569, %560
  %572 = add <8 x i32> %570, %10
  %573 = add <8 x i32> %572, %571
  %574 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %573, i32 %4) #8
  %575 = mul <8 x i32> %567, %560
  %576 = mul <8 x i32> %546, %569
  %577 = add <8 x i32> %575, %10
  %578 = sub <8 x i32> %577, %576
  %579 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %578, i32 %4) #8
  %580 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 30
  %581 = load i32, i32* %580, align 8
  %582 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 34
  %583 = load i32, i32* %582, align 8
  %584 = insertelement <8 x i32> undef, i32 %581, i32 0
  %585 = shufflevector <8 x i32> %584, <8 x i32> undef, <8 x i32> zeroinitializer
  %586 = insertelement <8 x i32> undef, i32 %583, i32 0
  %587 = shufflevector <8 x i32> %586, <8 x i32> undef, <8 x i32> zeroinitializer
  %588 = mul <8 x i32> %585, %547
  %589 = mul <8 x i32> %587, %561
  %590 = add <8 x i32> %588, %10
  %591 = add <8 x i32> %590, %589
  %592 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %591, i32 %4) #8
  %593 = mul <8 x i32> %585, %561
  %594 = mul <8 x i32> %547, %587
  %595 = add <8 x i32> %593, %10
  %596 = sub <8 x i32> %595, %594
  %597 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %596, i32 %4) #8
  %598 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 46
  %599 = load i32, i32* %598, align 8
  %600 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 18
  %601 = load i32, i32* %600, align 8
  %602 = insertelement <8 x i32> undef, i32 %599, i32 0
  %603 = shufflevector <8 x i32> %602, <8 x i32> undef, <8 x i32> zeroinitializer
  %604 = insertelement <8 x i32> undef, i32 %601, i32 0
  %605 = shufflevector <8 x i32> %604, <8 x i32> undef, <8 x i32> zeroinitializer
  %606 = mul <8 x i32> %603, %549
  %607 = mul <8 x i32> %605, %559
  %608 = add <8 x i32> %606, %10
  %609 = add <8 x i32> %608, %607
  %610 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %609, i32 %4) #8
  %611 = mul <8 x i32> %603, %559
  %612 = mul <8 x i32> %549, %605
  %613 = add <8 x i32> %611, %10
  %614 = sub <8 x i32> %613, %612
  %615 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %614, i32 %4) #8
  %616 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 14
  %617 = load i32, i32* %616, align 8
  %618 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 50
  %619 = load i32, i32* %618, align 8
  %620 = insertelement <8 x i32> undef, i32 %617, i32 0
  %621 = shufflevector <8 x i32> %620, <8 x i32> undef, <8 x i32> zeroinitializer
  %622 = insertelement <8 x i32> undef, i32 %619, i32 0
  %623 = shufflevector <8 x i32> %622, <8 x i32> undef, <8 x i32> zeroinitializer
  %624 = mul <8 x i32> %621, %548
  %625 = mul <8 x i32> %623, %558
  %626 = add <8 x i32> %624, %10
  %627 = add <8 x i32> %626, %625
  %628 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %627, i32 %4) #8
  %629 = mul <8 x i32> %621, %558
  %630 = mul <8 x i32> %548, %623
  %631 = add <8 x i32> %629, %10
  %632 = sub <8 x i32> %631, %630
  %633 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %632, i32 %4) #8
  %634 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 54
  %635 = load i32, i32* %634, align 8
  %636 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 10
  %637 = load i32, i32* %636, align 8
  %638 = insertelement <8 x i32> undef, i32 %635, i32 0
  %639 = shufflevector <8 x i32> %638, <8 x i32> undef, <8 x i32> zeroinitializer
  %640 = insertelement <8 x i32> undef, i32 %637, i32 0
  %641 = shufflevector <8 x i32> %640, <8 x i32> undef, <8 x i32> zeroinitializer
  %642 = mul <8 x i32> %639, %550
  %643 = mul <8 x i32> %641, %556
  %644 = add <8 x i32> %642, %10
  %645 = add <8 x i32> %644, %643
  %646 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %645, i32 %4) #8
  %647 = mul <8 x i32> %639, %556
  %648 = mul <8 x i32> %550, %641
  %649 = add <8 x i32> %647, %10
  %650 = sub <8 x i32> %649, %648
  %651 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %650, i32 %4) #8
  %652 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 22
  %653 = load i32, i32* %652, align 8
  %654 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 42
  %655 = load i32, i32* %654, align 8
  %656 = insertelement <8 x i32> undef, i32 %653, i32 0
  %657 = shufflevector <8 x i32> %656, <8 x i32> undef, <8 x i32> zeroinitializer
  %658 = insertelement <8 x i32> undef, i32 %655, i32 0
  %659 = shufflevector <8 x i32> %658, <8 x i32> undef, <8 x i32> zeroinitializer
  %660 = mul <8 x i32> %657, %551
  %661 = mul <8 x i32> %659, %557
  %662 = add <8 x i32> %660, %10
  %663 = add <8 x i32> %662, %661
  %664 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %663, i32 %4) #8
  %665 = mul <8 x i32> %657, %557
  %666 = mul <8 x i32> %551, %659
  %667 = add <8 x i32> %665, %10
  %668 = sub <8 x i32> %667, %666
  %669 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %668, i32 %4) #8
  %670 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 38
  %671 = load i32, i32* %670, align 8
  %672 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 26
  %673 = load i32, i32* %672, align 8
  %674 = insertelement <8 x i32> undef, i32 %671, i32 0
  %675 = shufflevector <8 x i32> %674, <8 x i32> undef, <8 x i32> zeroinitializer
  %676 = insertelement <8 x i32> undef, i32 %673, i32 0
  %677 = shufflevector <8 x i32> %676, <8 x i32> undef, <8 x i32> zeroinitializer
  %678 = mul <8 x i32> %675, %553
  %679 = mul <8 x i32> %677, %555
  %680 = add <8 x i32> %678, %10
  %681 = add <8 x i32> %680, %679
  %682 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %681, i32 %4) #8
  %683 = mul <8 x i32> %675, %555
  %684 = mul <8 x i32> %553, %677
  %685 = add <8 x i32> %683, %10
  %686 = sub <8 x i32> %685, %684
  %687 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %686, i32 %4) #8
  %688 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 6
  %689 = load i32, i32* %688, align 8
  %690 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %6, i64 58
  %691 = load i32, i32* %690, align 8
  %692 = insertelement <8 x i32> undef, i32 %689, i32 0
  %693 = shufflevector <8 x i32> %692, <8 x i32> undef, <8 x i32> zeroinitializer
  %694 = insertelement <8 x i32> undef, i32 %691, i32 0
  %695 = shufflevector <8 x i32> %694, <8 x i32> undef, <8 x i32> zeroinitializer
  %696 = mul <8 x i32> %693, %552
  %697 = mul <8 x i32> %695, %554
  %698 = add <8 x i32> %696, %10
  %699 = add <8 x i32> %698, %697
  %700 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %699, i32 %4) #8
  %701 = mul <8 x i32> %693, %554
  %702 = mul <8 x i32> %552, %695
  %703 = add <8 x i32> %701, %10
  %704 = sub <8 x i32> %703, %702
  %705 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %704, i32 %4) #8
  %706 = bitcast <4 x i64>* %1 to <8 x i32>*
  store <8 x i32> %323, <8 x i32>* %706, align 32
  %707 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %708 = bitcast <4 x i64>* %707 to <8 x i32>*
  store <8 x i32> %574, <8 x i32>* %708, align 32
  %709 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %710 = bitcast <4 x i64>* %709 to <8 x i32>*
  store <8 x i32> %486, <8 x i32>* %710, align 32
  %711 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %712 = bitcast <4 x i64>* %711 to <8 x i32>*
  store <8 x i32> %705, <8 x i32>* %712, align 32
  %713 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %714 = bitcast <4 x i64>* %713 to <8 x i32>*
  store <8 x i32> %390, <8 x i32>* %714, align 32
  %715 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %716 = bitcast <4 x i64>* %715 to <8 x i32>*
  store <8 x i32> %646, <8 x i32>* %716, align 32
  %717 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %718 = bitcast <4 x i64>* %717 to <8 x i32>*
  store <8 x i32> %545, <8 x i32>* %718, align 32
  %719 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %720 = bitcast <4 x i64>* %719 to <8 x i32>*
  store <8 x i32> %633, <8 x i32>* %720, align 32
  %721 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %722 = bitcast <4 x i64>* %721 to <8 x i32>*
  store <8 x i32> %332, <8 x i32>* %722, align 32
  %723 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %724 = bitcast <4 x i64>* %723 to <8 x i32>*
  store <8 x i32> %610, <8 x i32>* %724, align 32
  %725 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %726 = bitcast <4 x i64>* %725 to <8 x i32>*
  store <8 x i32> %522, <8 x i32>* %726, align 32
  %727 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %728 = bitcast <4 x i64>* %727 to <8 x i32>*
  store <8 x i32> %669, <8 x i32>* %728, align 32
  %729 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %730 = bitcast <4 x i64>* %729 to <8 x i32>*
  store <8 x i32> %413, <8 x i32>* %730, align 32
  %731 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %732 = bitcast <4 x i64>* %731 to <8 x i32>*
  store <8 x i32> %682, <8 x i32>* %732, align 32
  %733 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %734 = bitcast <4 x i64>* %733 to <8 x i32>*
  store <8 x i32> %509, <8 x i32>* %734, align 32
  %735 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 15
  %736 = bitcast <4 x i64>* %735 to <8 x i32>*
  store <8 x i32> %597, <8 x i32>* %736, align 32
  %737 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 16
  %738 = bitcast <4 x i64>* %737 to <8 x i32>*
  store <8 x i32> %325, <8 x i32>* %738, align 32
  %739 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 17
  %740 = bitcast <4 x i64>* %739 to <8 x i32>*
  store <8 x i32> %592, <8 x i32>* %740, align 32
  %741 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 18
  %742 = bitcast <4 x i64>* %741 to <8 x i32>*
  store <8 x i32> %504, <8 x i32>* %742, align 32
  %743 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 19
  %744 = bitcast <4 x i64>* %743 to <8 x i32>*
  store <8 x i32> %687, <8 x i32>* %744, align 32
  %745 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 20
  %746 = bitcast <4 x i64>* %745 to <8 x i32>*
  store <8 x i32> %408, <8 x i32>* %746, align 32
  %747 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 21
  %748 = bitcast <4 x i64>* %747 to <8 x i32>*
  store <8 x i32> %664, <8 x i32>* %748, align 32
  %749 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 22
  %750 = bitcast <4 x i64>* %749 to <8 x i32>*
  store <8 x i32> %527, <8 x i32>* %750, align 32
  %751 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 23
  %752 = bitcast <4 x i64>* %751 to <8 x i32>*
  store <8 x i32> %615, <8 x i32>* %752, align 32
  %753 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 24
  %754 = bitcast <4 x i64>* %753 to <8 x i32>*
  store <8 x i32> %337, <8 x i32>* %754, align 32
  %755 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 25
  %756 = bitcast <4 x i64>* %755 to <8 x i32>*
  store <8 x i32> %628, <8 x i32>* %756, align 32
  %757 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 26
  %758 = bitcast <4 x i64>* %757 to <8 x i32>*
  store <8 x i32> %540, <8 x i32>* %758, align 32
  %759 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 27
  %760 = bitcast <4 x i64>* %759 to <8 x i32>*
  store <8 x i32> %651, <8 x i32>* %760, align 32
  %761 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 28
  %762 = bitcast <4 x i64>* %761 to <8 x i32>*
  store <8 x i32> %395, <8 x i32>* %762, align 32
  %763 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 29
  %764 = bitcast <4 x i64>* %763 to <8 x i32>*
  store <8 x i32> %700, <8 x i32>* %764, align 32
  %765 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 30
  %766 = bitcast <4 x i64>* %765 to <8 x i32>*
  store <8 x i32> %491, <8 x i32>* %766, align 32
  %767 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 31
  %768 = bitcast <4 x i64>* %767 to <8 x i32>*
  store <8 x i32> %579, <8 x i32>* %768, align 32
  ret void
}

attributes #0 = { nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+avx,+avx2,+cx8,+fxsr,+mmx,+popcnt,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { argmemonly nounwind }
attributes #2 = { "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+avx,+avx2,+cx8,+fxsr,+mmx,+popcnt,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #3 = { nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="256" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+avx,+avx2,+cx8,+fxsr,+mmx,+popcnt,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #4 = { inlinehint nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="256" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+avx,+avx2,+cx8,+fxsr,+mmx,+popcnt,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #5 = { inlinehint nofree nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="256" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+avx,+avx2,+cx8,+fxsr,+mmx,+popcnt,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #6 = { nounwind readnone speculatable }
attributes #7 = { nounwind readnone }
attributes #8 = { nounwind }

!llvm.module.flags = !{!0, !1}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 2}
