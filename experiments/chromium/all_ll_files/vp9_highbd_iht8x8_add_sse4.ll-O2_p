; ModuleID = '../../third_party/libvpx/source/libvpx/vp9/common/x86/vp9_highbd_iht8x8_add_sse4.c'
source_filename = "../../third_party/libvpx/source/libvpx/vp9/common/x86/vp9_highbd_iht8x8_add_sse4.c"
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

; Function Attrs: nounwind ssp uwtable
define hidden void @vp9_highbd_iht8x8_64_add_sse4_1(i32* nocapture readonly, i16* nocapture, i32, i32, i32) local_unnamed_addr #0 {
  %6 = alloca [16 x <2 x i64>], align 16
  %7 = alloca [8 x <2 x i64>], align 16
  %8 = bitcast [16 x <2 x i64>]* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %8) #5
  %9 = bitcast i32* %0 to <2 x i64>*
  %10 = load <2 x i64>, <2 x i64>* %9, align 16
  %11 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 0
  store <2 x i64> %10, <2 x i64>* %11, align 16
  %12 = getelementptr inbounds i32, i32* %0, i64 4
  %13 = bitcast i32* %12 to <2 x i64>*
  %14 = load <2 x i64>, <2 x i64>* %13, align 16
  %15 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 4
  store <2 x i64> %14, <2 x i64>* %15, align 16
  %16 = getelementptr inbounds i32, i32* %0, i64 8
  %17 = bitcast i32* %16 to <2 x i64>*
  %18 = load <2 x i64>, <2 x i64>* %17, align 16
  %19 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 1
  store <2 x i64> %18, <2 x i64>* %19, align 16
  %20 = getelementptr inbounds i32, i32* %0, i64 12
  %21 = bitcast i32* %20 to <2 x i64>*
  %22 = load <2 x i64>, <2 x i64>* %21, align 16
  %23 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 5
  store <2 x i64> %22, <2 x i64>* %23, align 16
  %24 = getelementptr inbounds i32, i32* %0, i64 16
  %25 = bitcast i32* %24 to <2 x i64>*
  %26 = load <2 x i64>, <2 x i64>* %25, align 16
  %27 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 2
  store <2 x i64> %26, <2 x i64>* %27, align 16
  %28 = getelementptr inbounds i32, i32* %0, i64 20
  %29 = bitcast i32* %28 to <2 x i64>*
  %30 = load <2 x i64>, <2 x i64>* %29, align 16
  %31 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 6
  store <2 x i64> %30, <2 x i64>* %31, align 16
  %32 = getelementptr inbounds i32, i32* %0, i64 24
  %33 = bitcast i32* %32 to <2 x i64>*
  %34 = load <2 x i64>, <2 x i64>* %33, align 16
  %35 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 3
  store <2 x i64> %34, <2 x i64>* %35, align 16
  %36 = getelementptr inbounds i32, i32* %0, i64 28
  %37 = bitcast i32* %36 to <2 x i64>*
  %38 = load <2 x i64>, <2 x i64>* %37, align 16
  %39 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 7
  store <2 x i64> %38, <2 x i64>* %39, align 16
  %40 = getelementptr inbounds i32, i32* %0, i64 32
  %41 = bitcast i32* %40 to <2 x i64>*
  %42 = load <2 x i64>, <2 x i64>* %41, align 16
  %43 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 8
  store <2 x i64> %42, <2 x i64>* %43, align 16
  %44 = getelementptr inbounds i32, i32* %0, i64 36
  %45 = bitcast i32* %44 to <2 x i64>*
  %46 = load <2 x i64>, <2 x i64>* %45, align 16
  %47 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 12
  store <2 x i64> %46, <2 x i64>* %47, align 16
  %48 = getelementptr inbounds i32, i32* %0, i64 40
  %49 = bitcast i32* %48 to <2 x i64>*
  %50 = load <2 x i64>, <2 x i64>* %49, align 16
  %51 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 9
  store <2 x i64> %50, <2 x i64>* %51, align 16
  %52 = getelementptr inbounds i32, i32* %0, i64 44
  %53 = bitcast i32* %52 to <2 x i64>*
  %54 = load <2 x i64>, <2 x i64>* %53, align 16
  %55 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 13
  store <2 x i64> %54, <2 x i64>* %55, align 16
  %56 = getelementptr inbounds i32, i32* %0, i64 48
  %57 = bitcast i32* %56 to <2 x i64>*
  %58 = load <2 x i64>, <2 x i64>* %57, align 16
  %59 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 10
  store <2 x i64> %58, <2 x i64>* %59, align 16
  %60 = getelementptr inbounds i32, i32* %0, i64 52
  %61 = bitcast i32* %60 to <2 x i64>*
  %62 = load <2 x i64>, <2 x i64>* %61, align 16
  %63 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 14
  store <2 x i64> %62, <2 x i64>* %63, align 16
  %64 = getelementptr inbounds i32, i32* %0, i64 56
  %65 = bitcast i32* %64 to <2 x i64>*
  %66 = load <2 x i64>, <2 x i64>* %65, align 16
  %67 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 11
  store <2 x i64> %66, <2 x i64>* %67, align 16
  %68 = getelementptr inbounds i32, i32* %0, i64 60
  %69 = bitcast i32* %68 to <2 x i64>*
  %70 = load <2 x i64>, <2 x i64>* %69, align 16
  %71 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 15
  store <2 x i64> %70, <2 x i64>* %71, align 16
  %72 = icmp eq i32 %4, 8
  br i1 %72, label %73, label %156

73:                                               ; preds = %5
  %74 = bitcast [8 x <2 x i64>]* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 128, i8* nonnull %74) #5
  %75 = bitcast <2 x i64> %10 to <4 x i32>
  %76 = bitcast <2 x i64> %14 to <4 x i32>
  %77 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %75, <4 x i32> %76) #5
  %78 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %7, i64 0, i64 0
  %79 = bitcast [8 x <2 x i64>]* %7 to <8 x i16>*
  store <8 x i16> %77, <8 x i16>* %79, align 16
  %80 = bitcast <2 x i64> %18 to <4 x i32>
  %81 = bitcast <2 x i64> %22 to <4 x i32>
  %82 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %80, <4 x i32> %81) #5
  %83 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %7, i64 0, i64 1
  %84 = bitcast <2 x i64>* %83 to <8 x i16>*
  store <8 x i16> %82, <8 x i16>* %84, align 16
  %85 = bitcast <2 x i64> %26 to <4 x i32>
  %86 = bitcast <2 x i64> %30 to <4 x i32>
  %87 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %85, <4 x i32> %86) #5
  %88 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %7, i64 0, i64 2
  %89 = bitcast <2 x i64>* %88 to <8 x i16>*
  store <8 x i16> %87, <8 x i16>* %89, align 16
  %90 = bitcast <2 x i64> %34 to <4 x i32>
  %91 = bitcast <2 x i64> %38 to <4 x i32>
  %92 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %90, <4 x i32> %91) #5
  %93 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %7, i64 0, i64 3
  %94 = bitcast <2 x i64>* %93 to <8 x i16>*
  store <8 x i16> %92, <8 x i16>* %94, align 16
  %95 = bitcast <2 x i64> %42 to <4 x i32>
  %96 = bitcast <2 x i64> %46 to <4 x i32>
  %97 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %95, <4 x i32> %96) #5
  %98 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %7, i64 0, i64 4
  %99 = bitcast <2 x i64>* %98 to <8 x i16>*
  store <8 x i16> %97, <8 x i16>* %99, align 16
  %100 = bitcast <2 x i64> %50 to <4 x i32>
  %101 = bitcast <2 x i64> %54 to <4 x i32>
  %102 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %100, <4 x i32> %101) #5
  %103 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %7, i64 0, i64 5
  %104 = bitcast <2 x i64>* %103 to <8 x i16>*
  store <8 x i16> %102, <8 x i16>* %104, align 16
  %105 = bitcast <2 x i64> %58 to <4 x i32>
  %106 = bitcast <2 x i64> %62 to <4 x i32>
  %107 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %105, <4 x i32> %106) #5
  %108 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %7, i64 0, i64 6
  %109 = bitcast <2 x i64>* %108 to <8 x i16>*
  store <8 x i16> %107, <8 x i16>* %109, align 16
  %110 = bitcast <2 x i64> %66 to <4 x i32>
  %111 = bitcast <2 x i64> %70 to <4 x i32>
  %112 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %110, <4 x i32> %111) #5
  %113 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %7, i64 0, i64 7
  %114 = bitcast <2 x i64>* %113 to <8 x i16>*
  store <8 x i16> %112, <8 x i16>* %114, align 16
  %115 = icmp ult i32 %3, 2
  br i1 %115, label %116, label %117

116:                                              ; preds = %73
  call void @vpx_idct8_sse2(<2 x i64>* nonnull %78) #5
  br label %118

117:                                              ; preds = %73
  call void @iadst8_sse2(<2 x i64>* nonnull %78) #5
  br label %118

118:                                              ; preds = %117, %116
  %119 = and i32 %3, -3
  %120 = icmp eq i32 %119, 0
  br i1 %120, label %121, label %122

121:                                              ; preds = %118
  call void @vpx_idct8_sse2(<2 x i64>* nonnull %78) #5
  br label %123

122:                                              ; preds = %118
  call void @iadst8_sse2(<2 x i64>* nonnull %78) #5
  br label %123

123:                                              ; preds = %122, %121
  %124 = load <8 x i16>, <8 x i16>* %79, align 16
  %125 = add <8 x i16> %124, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %126 = bitcast [16 x <2 x i64>]* %6 to <8 x i16>*
  %127 = load <8 x i16>, <8 x i16>* %84, align 16
  %128 = add <8 x i16> %127, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %129 = bitcast <2 x i64>* %19 to <8 x i16>*
  %130 = load <8 x i16>, <8 x i16>* %89, align 16
  %131 = add <8 x i16> %130, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %132 = bitcast <2 x i64>* %27 to <8 x i16>*
  %133 = load <8 x i16>, <8 x i16>* %94, align 16
  %134 = add <8 x i16> %133, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %135 = bitcast <2 x i64>* %35 to <8 x i16>*
  %136 = load <8 x i16>, <8 x i16>* %99, align 16
  %137 = add <8 x i16> %136, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %138 = bitcast <2 x i64>* %15 to <8 x i16>*
  %139 = load <8 x i16>, <8 x i16>* %104, align 16
  %140 = add <8 x i16> %139, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %141 = bitcast <2 x i64>* %23 to <8 x i16>*
  %142 = load <8 x i16>, <8 x i16>* %109, align 16
  %143 = add <8 x i16> %142, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %144 = bitcast <2 x i64>* %31 to <8 x i16>*
  %145 = load <8 x i16>, <8 x i16>* %114, align 16
  %146 = add <8 x i16> %145, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %147 = bitcast <2 x i64>* %39 to <8 x i16>*
  %148 = ashr <8 x i16> %125, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %148, <8 x i16>* %126, align 16
  %149 = ashr <8 x i16> %128, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %149, <8 x i16>* %129, align 16
  %150 = ashr <8 x i16> %131, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %150, <8 x i16>* %132, align 16
  %151 = ashr <8 x i16> %134, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %151, <8 x i16>* %135, align 16
  %152 = ashr <8 x i16> %137, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %152, <8 x i16>* %138, align 16
  %153 = ashr <8 x i16> %140, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %153, <8 x i16>* %141, align 16
  %154 = ashr <8 x i16> %143, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %154, <8 x i16>* %144, align 16
  %155 = ashr <8 x i16> %146, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %155, <8 x i16>* %147, align 16
  call void @llvm.lifetime.end.p0i8(i64 128, i8* nonnull %74) #5
  br label %254

156:                                              ; preds = %5
  %157 = icmp ult i32 %3, 2
  br i1 %157, label %158, label %159

158:                                              ; preds = %156
  call void @vpx_highbd_idct8x8_half1d_sse4_1(<2 x i64>* nonnull %11) #5
  call void @vpx_highbd_idct8x8_half1d_sse4_1(<2 x i64>* %43) #5
  br label %160

159:                                              ; preds = %156
  call fastcc void @highbd_iadst8_sse4_1(<2 x i64>* nonnull %11)
  call fastcc void @highbd_iadst8_sse4_1(<2 x i64>* %43)
  br label %160

160:                                              ; preds = %159, %158
  %161 = load <2 x i64>, <2 x i64>* %15, align 16
  %162 = load <2 x i64>, <2 x i64>* %23, align 16
  %163 = load <2 x i64>, <2 x i64>* %31, align 16
  %164 = load <2 x i64>, <2 x i64>* %39, align 16
  %165 = load <2 x i64>, <2 x i64>* %43, align 16
  store <2 x i64> %165, <2 x i64>* %15, align 16
  %166 = load <2 x i64>, <2 x i64>* %51, align 16
  store <2 x i64> %166, <2 x i64>* %23, align 16
  %167 = load <2 x i64>, <2 x i64>* %59, align 16
  store <2 x i64> %167, <2 x i64>* %31, align 16
  %168 = load <2 x i64>, <2 x i64>* %67, align 16
  store <2 x i64> %168, <2 x i64>* %39, align 16
  %169 = and i32 %3, -3
  %170 = icmp eq i32 %169, 0
  br i1 %170, label %171, label %172

171:                                              ; preds = %160
  call void @vpx_highbd_idct8x8_half1d_sse4_1(<2 x i64>* nonnull %11) #5
  store <2 x i64> %161, <2 x i64>* %43, align 16
  store <2 x i64> %162, <2 x i64>* %51, align 16
  store <2 x i64> %163, <2 x i64>* %59, align 16
  store <2 x i64> %164, <2 x i64>* %67, align 16
  call void @vpx_highbd_idct8x8_half1d_sse4_1(<2 x i64>* %43) #5
  br label %173

172:                                              ; preds = %160
  call fastcc void @highbd_iadst8_sse4_1(<2 x i64>* nonnull %11)
  store <2 x i64> %161, <2 x i64>* %43, align 16
  store <2 x i64> %162, <2 x i64>* %51, align 16
  store <2 x i64> %163, <2 x i64>* %59, align 16
  store <2 x i64> %164, <2 x i64>* %67, align 16
  call fastcc void @highbd_iadst8_sse4_1(<2 x i64>* %43)
  br label %173

173:                                              ; preds = %172, %171
  %174 = bitcast [16 x <2 x i64>]* %6 to <4 x i32>*
  %175 = load <4 x i32>, <4 x i32>* %174, align 16
  %176 = bitcast <2 x i64>* %43 to <4 x i32>*
  %177 = load <4 x i32>, <4 x i32>* %176, align 16
  %178 = add <4 x i32> %175, <i32 16, i32 16, i32 16, i32 16>
  %179 = add <4 x i32> %177, <i32 16, i32 16, i32 16, i32 16>
  %180 = ashr <4 x i32> %178, <i32 5, i32 5, i32 5, i32 5>
  %181 = ashr <4 x i32> %179, <i32 5, i32 5, i32 5, i32 5>
  %182 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %180, <4 x i32> %181) #5
  %183 = bitcast [16 x <2 x i64>]* %6 to <8 x i16>*
  store <8 x i16> %182, <8 x i16>* %183, align 16
  %184 = bitcast <2 x i64>* %19 to <4 x i32>*
  %185 = load <4 x i32>, <4 x i32>* %184, align 16
  %186 = bitcast <2 x i64>* %51 to <4 x i32>*
  %187 = load <4 x i32>, <4 x i32>* %186, align 16
  %188 = add <4 x i32> %185, <i32 16, i32 16, i32 16, i32 16>
  %189 = add <4 x i32> %187, <i32 16, i32 16, i32 16, i32 16>
  %190 = ashr <4 x i32> %188, <i32 5, i32 5, i32 5, i32 5>
  %191 = ashr <4 x i32> %189, <i32 5, i32 5, i32 5, i32 5>
  %192 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %190, <4 x i32> %191) #5
  %193 = bitcast <2 x i64>* %19 to <8 x i16>*
  store <8 x i16> %192, <8 x i16>* %193, align 16
  %194 = bitcast <2 x i64>* %27 to <4 x i32>*
  %195 = load <4 x i32>, <4 x i32>* %194, align 16
  %196 = bitcast <2 x i64>* %59 to <4 x i32>*
  %197 = load <4 x i32>, <4 x i32>* %196, align 16
  %198 = add <4 x i32> %195, <i32 16, i32 16, i32 16, i32 16>
  %199 = add <4 x i32> %197, <i32 16, i32 16, i32 16, i32 16>
  %200 = ashr <4 x i32> %198, <i32 5, i32 5, i32 5, i32 5>
  %201 = ashr <4 x i32> %199, <i32 5, i32 5, i32 5, i32 5>
  %202 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %200, <4 x i32> %201) #5
  %203 = bitcast <2 x i64>* %27 to <8 x i16>*
  store <8 x i16> %202, <8 x i16>* %203, align 16
  %204 = bitcast <2 x i64>* %35 to <4 x i32>*
  %205 = load <4 x i32>, <4 x i32>* %204, align 16
  %206 = bitcast <2 x i64>* %67 to <4 x i32>*
  %207 = load <4 x i32>, <4 x i32>* %206, align 16
  %208 = add <4 x i32> %205, <i32 16, i32 16, i32 16, i32 16>
  %209 = add <4 x i32> %207, <i32 16, i32 16, i32 16, i32 16>
  %210 = ashr <4 x i32> %208, <i32 5, i32 5, i32 5, i32 5>
  %211 = ashr <4 x i32> %209, <i32 5, i32 5, i32 5, i32 5>
  %212 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %210, <4 x i32> %211) #5
  %213 = bitcast <2 x i64>* %35 to <8 x i16>*
  store <8 x i16> %212, <8 x i16>* %213, align 16
  %214 = bitcast <2 x i64>* %15 to <4 x i32>*
  %215 = load <4 x i32>, <4 x i32>* %214, align 16
  %216 = bitcast <2 x i64>* %47 to <4 x i32>*
  %217 = load <4 x i32>, <4 x i32>* %216, align 16
  %218 = add <4 x i32> %215, <i32 16, i32 16, i32 16, i32 16>
  %219 = add <4 x i32> %217, <i32 16, i32 16, i32 16, i32 16>
  %220 = ashr <4 x i32> %218, <i32 5, i32 5, i32 5, i32 5>
  %221 = ashr <4 x i32> %219, <i32 5, i32 5, i32 5, i32 5>
  %222 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %220, <4 x i32> %221) #5
  %223 = bitcast <2 x i64>* %15 to <8 x i16>*
  store <8 x i16> %222, <8 x i16>* %223, align 16
  %224 = bitcast <2 x i64>* %23 to <4 x i32>*
  %225 = load <4 x i32>, <4 x i32>* %224, align 16
  %226 = bitcast <2 x i64>* %55 to <4 x i32>*
  %227 = load <4 x i32>, <4 x i32>* %226, align 16
  %228 = add <4 x i32> %225, <i32 16, i32 16, i32 16, i32 16>
  %229 = add <4 x i32> %227, <i32 16, i32 16, i32 16, i32 16>
  %230 = ashr <4 x i32> %228, <i32 5, i32 5, i32 5, i32 5>
  %231 = ashr <4 x i32> %229, <i32 5, i32 5, i32 5, i32 5>
  %232 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %230, <4 x i32> %231) #5
  %233 = bitcast <2 x i64>* %23 to <8 x i16>*
  store <8 x i16> %232, <8 x i16>* %233, align 16
  %234 = bitcast <2 x i64>* %31 to <4 x i32>*
  %235 = load <4 x i32>, <4 x i32>* %234, align 16
  %236 = bitcast <2 x i64>* %63 to <4 x i32>*
  %237 = load <4 x i32>, <4 x i32>* %236, align 16
  %238 = add <4 x i32> %235, <i32 16, i32 16, i32 16, i32 16>
  %239 = add <4 x i32> %237, <i32 16, i32 16, i32 16, i32 16>
  %240 = ashr <4 x i32> %238, <i32 5, i32 5, i32 5, i32 5>
  %241 = ashr <4 x i32> %239, <i32 5, i32 5, i32 5, i32 5>
  %242 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %240, <4 x i32> %241) #5
  %243 = bitcast <2 x i64>* %31 to <8 x i16>*
  store <8 x i16> %242, <8 x i16>* %243, align 16
  %244 = bitcast <2 x i64>* %39 to <4 x i32>*
  %245 = load <4 x i32>, <4 x i32>* %244, align 16
  %246 = bitcast <2 x i64>* %71 to <4 x i32>*
  %247 = load <4 x i32>, <4 x i32>* %246, align 16
  %248 = add <4 x i32> %245, <i32 16, i32 16, i32 16, i32 16>
  %249 = add <4 x i32> %247, <i32 16, i32 16, i32 16, i32 16>
  %250 = ashr <4 x i32> %248, <i32 5, i32 5, i32 5, i32 5>
  %251 = ashr <4 x i32> %249, <i32 5, i32 5, i32 5, i32 5>
  %252 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %250, <4 x i32> %251) #5
  %253 = bitcast <2 x i64>* %39 to <8 x i16>*
  store <8 x i16> %252, <8 x i16>* %253, align 16
  br label %254

254:                                              ; preds = %173, %123
  %255 = phi <8 x i16> [ %252, %173 ], [ %155, %123 ]
  %256 = phi <8 x i16> [ %242, %173 ], [ %154, %123 ]
  %257 = phi <8 x i16> [ %232, %173 ], [ %153, %123 ]
  %258 = phi <8 x i16> [ %222, %173 ], [ %152, %123 ]
  %259 = phi <8 x i16> [ %212, %173 ], [ %151, %123 ]
  %260 = phi <8 x i16> [ %202, %173 ], [ %150, %123 ]
  %261 = phi <8 x i16> [ %192, %173 ], [ %149, %123 ]
  %262 = phi <8 x i16> [ %182, %173 ], [ %148, %123 ]
  %263 = bitcast i16* %1 to <8 x i16>*
  %264 = load <8 x i16>, <8 x i16>* %263, align 16
  %265 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>, i32 %4) #5
  %266 = add <8 x i16> %265, <i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1>
  %267 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %264, <8 x i16> %262) #5
  %268 = icmp sgt <8 x i16> %267, zeroinitializer
  %269 = select <8 x i1> %268, <8 x i16> %267, <8 x i16> zeroinitializer
  %270 = icmp slt <8 x i16> %269, %266
  %271 = select <8 x i1> %270, <8 x i16> %269, <8 x i16> %266
  store <8 x i16> %271, <8 x i16>* %263, align 16
  %272 = sext i32 %2 to i64
  %273 = getelementptr inbounds i16, i16* %1, i64 %272
  %274 = bitcast i16* %273 to <8 x i16>*
  %275 = load <8 x i16>, <8 x i16>* %274, align 16
  %276 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %275, <8 x i16> %261) #5
  %277 = icmp sgt <8 x i16> %276, zeroinitializer
  %278 = select <8 x i1> %277, <8 x i16> %276, <8 x i16> zeroinitializer
  %279 = icmp slt <8 x i16> %278, %266
  %280 = select <8 x i1> %279, <8 x i16> %278, <8 x i16> %266
  store <8 x i16> %280, <8 x i16>* %274, align 16
  %281 = getelementptr inbounds i16, i16* %273, i64 %272
  %282 = bitcast i16* %281 to <8 x i16>*
  %283 = load <8 x i16>, <8 x i16>* %282, align 16
  %284 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %283, <8 x i16> %260) #5
  %285 = icmp sgt <8 x i16> %284, zeroinitializer
  %286 = select <8 x i1> %285, <8 x i16> %284, <8 x i16> zeroinitializer
  %287 = icmp slt <8 x i16> %286, %266
  %288 = select <8 x i1> %287, <8 x i16> %286, <8 x i16> %266
  store <8 x i16> %288, <8 x i16>* %282, align 16
  %289 = getelementptr inbounds i16, i16* %281, i64 %272
  %290 = bitcast i16* %289 to <8 x i16>*
  %291 = load <8 x i16>, <8 x i16>* %290, align 16
  %292 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %291, <8 x i16> %259) #5
  %293 = icmp sgt <8 x i16> %292, zeroinitializer
  %294 = select <8 x i1> %293, <8 x i16> %292, <8 x i16> zeroinitializer
  %295 = icmp slt <8 x i16> %294, %266
  %296 = select <8 x i1> %295, <8 x i16> %294, <8 x i16> %266
  store <8 x i16> %296, <8 x i16>* %290, align 16
  %297 = getelementptr inbounds i16, i16* %289, i64 %272
  %298 = bitcast i16* %297 to <8 x i16>*
  %299 = load <8 x i16>, <8 x i16>* %298, align 16
  %300 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %299, <8 x i16> %258) #5
  %301 = icmp sgt <8 x i16> %300, zeroinitializer
  %302 = select <8 x i1> %301, <8 x i16> %300, <8 x i16> zeroinitializer
  %303 = icmp slt <8 x i16> %302, %266
  %304 = select <8 x i1> %303, <8 x i16> %302, <8 x i16> %266
  store <8 x i16> %304, <8 x i16>* %298, align 16
  %305 = getelementptr inbounds i16, i16* %297, i64 %272
  %306 = bitcast i16* %305 to <8 x i16>*
  %307 = load <8 x i16>, <8 x i16>* %306, align 16
  %308 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %307, <8 x i16> %257) #5
  %309 = icmp sgt <8 x i16> %308, zeroinitializer
  %310 = select <8 x i1> %309, <8 x i16> %308, <8 x i16> zeroinitializer
  %311 = icmp slt <8 x i16> %310, %266
  %312 = select <8 x i1> %311, <8 x i16> %310, <8 x i16> %266
  store <8 x i16> %312, <8 x i16>* %306, align 16
  %313 = getelementptr inbounds i16, i16* %305, i64 %272
  %314 = bitcast i16* %313 to <8 x i16>*
  %315 = load <8 x i16>, <8 x i16>* %314, align 16
  %316 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %315, <8 x i16> %256) #5
  %317 = icmp sgt <8 x i16> %316, zeroinitializer
  %318 = select <8 x i1> %317, <8 x i16> %316, <8 x i16> zeroinitializer
  %319 = icmp slt <8 x i16> %318, %266
  %320 = select <8 x i1> %319, <8 x i16> %318, <8 x i16> %266
  store <8 x i16> %320, <8 x i16>* %314, align 16
  %321 = getelementptr inbounds i16, i16* %313, i64 %272
  %322 = bitcast i16* %321 to <8 x i16>*
  %323 = load <8 x i16>, <8 x i16>* %322, align 16
  %324 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %323, <8 x i16> %255) #5
  %325 = icmp sgt <8 x i16> %324, zeroinitializer
  %326 = select <8 x i1> %325, <8 x i16> %324, <8 x i16> zeroinitializer
  %327 = icmp slt <8 x i16> %326, %266
  %328 = select <8 x i1> %327, <8 x i16> %326, <8 x i16> %266
  store <8 x i16> %328, <8 x i16>* %322, align 16
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %8) #5
  ret void
}

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #1

declare void @vpx_idct8_sse2(<2 x i64>*) local_unnamed_addr #2

declare void @iadst8_sse2(<2 x i64>*) local_unnamed_addr #2

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #1

declare void @vpx_highbd_idct8x8_half1d_sse4_1(<2 x i64>*) local_unnamed_addr #2

; Function Attrs: nounwind ssp uwtable
define internal fastcc void @highbd_iadst8_sse4_1(<2 x i64>*) unnamed_addr #0 {
  %2 = bitcast <2 x i64>* %0 to <4 x i32>*
  %3 = load <4 x i32>, <4 x i32>* %2, align 16
  %4 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %5 = bitcast <2 x i64>* %4 to <4 x i32>*
  %6 = load <4 x i32>, <4 x i32>* %5, align 16
  %7 = shufflevector <4 x i32> %3, <4 x i32> %6, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %8 = bitcast <4 x i32> %7 to <2 x i64>
  %9 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %10 = bitcast <2 x i64>* %9 to <4 x i32>*
  %11 = load <4 x i32>, <4 x i32>* %10, align 16
  %12 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %13 = bitcast <2 x i64>* %12 to <4 x i32>*
  %14 = load <4 x i32>, <4 x i32>* %13, align 16
  %15 = shufflevector <4 x i32> %11, <4 x i32> %14, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %16 = bitcast <4 x i32> %15 to <2 x i64>
  %17 = shufflevector <4 x i32> %3, <4 x i32> %6, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %18 = bitcast <4 x i32> %17 to <2 x i64>
  %19 = shufflevector <4 x i32> %11, <4 x i32> %14, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %20 = bitcast <4 x i32> %19 to <2 x i64>
  %21 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 4
  %22 = bitcast <2 x i64>* %21 to <4 x i32>*
  %23 = load <4 x i32>, <4 x i32>* %22, align 16
  %24 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 5
  %25 = bitcast <2 x i64>* %24 to <4 x i32>*
  %26 = load <4 x i32>, <4 x i32>* %25, align 16
  %27 = shufflevector <4 x i32> %23, <4 x i32> %26, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %28 = bitcast <4 x i32> %27 to <2 x i64>
  %29 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 6
  %30 = bitcast <2 x i64>* %29 to <4 x i32>*
  %31 = load <4 x i32>, <4 x i32>* %30, align 16
  %32 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 7
  %33 = bitcast <2 x i64>* %32 to <4 x i32>*
  %34 = load <4 x i32>, <4 x i32>* %33, align 16
  %35 = shufflevector <4 x i32> %31, <4 x i32> %34, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %36 = bitcast <4 x i32> %35 to <2 x i64>
  %37 = shufflevector <4 x i32> %23, <4 x i32> %26, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %38 = bitcast <4 x i32> %37 to <2 x i64>
  %39 = shufflevector <4 x i32> %31, <4 x i32> %34, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %40 = bitcast <4 x i32> %39 to <2 x i64>
  %41 = shufflevector <2 x i64> %8, <2 x i64> %16, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %41, <2 x i64>* %0, align 16
  %42 = shufflevector <2 x i64> %8, <2 x i64> %16, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %42, <2 x i64>* %4, align 16
  %43 = shufflevector <2 x i64> %18, <2 x i64> %20, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %43, <2 x i64>* %9, align 16
  %44 = shufflevector <2 x i64> %18, <2 x i64> %20, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %44, <2 x i64>* %12, align 16
  %45 = shufflevector <2 x i64> %28, <2 x i64> %36, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %45, <2 x i64>* %21, align 16
  %46 = shufflevector <2 x i64> %28, <2 x i64> %36, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %46, <2 x i64>* %24, align 16
  %47 = shufflevector <2 x i64> %38, <2 x i64> %40, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %47, <2 x i64>* %29, align 16
  %48 = shufflevector <2 x i64> %38, <2 x i64> %40, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %48, <2 x i64>* %32, align 16
  %49 = bitcast <2 x i64> %48 to <4 x i32>
  %50 = shufflevector <4 x i32> %49, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %51 = bitcast <4 x i32> %50 to <2 x i64>
  %52 = shufflevector <4 x i32> %49, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %53 = bitcast <4 x i32> %52 to <2 x i64>
  %54 = bitcast <2 x i64> %41 to <4 x i32>
  %55 = shufflevector <4 x i32> %54, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %56 = bitcast <4 x i32> %55 to <2 x i64>
  %57 = shufflevector <4 x i32> %54, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %58 = bitcast <4 x i32> %57 to <2 x i64>
  %59 = shl <2 x i64> %51, <i64 32, i64 32>
  %60 = ashr exact <2 x i64> %59, <i64 32, i64 32>
  %61 = mul nsw <2 x i64> %60, <i64 65220, i64 65220>
  %62 = shl <2 x i64> %53, <i64 32, i64 32>
  %63 = ashr exact <2 x i64> %62, <i64 32, i64 32>
  %64 = mul nsw <2 x i64> %63, <i64 65220, i64 65220>
  %65 = shl <2 x i64> %56, <i64 32, i64 32>
  %66 = ashr exact <2 x i64> %65, <i64 32, i64 32>
  %67 = shl <2 x i64> %58, <i64 32, i64 32>
  %68 = ashr exact <2 x i64> %67, <i64 32, i64 32>
  %69 = mul nsw <2 x i64> %60, <i64 6424, i64 6424>
  %70 = mul nsw <2 x i64> %63, <i64 6424, i64 6424>
  %71 = mul nsw <2 x i64> %66, <i64 6424, i64 6424>
  %72 = mul nsw <2 x i64> %68, <i64 6424, i64 6424>
  %73 = add nsw <2 x i64> %61, %71
  %74 = add nsw <2 x i64> %64, %72
  %75 = mul nsw <2 x i64> %66, <i64 -65220, i64 -65220>
  %76 = add nsw <2 x i64> %69, %75
  %77 = mul nsw <2 x i64> %68, <i64 -65220, i64 -65220>
  %78 = add nsw <2 x i64> %70, %77
  %79 = bitcast <2 x i64> %44 to <4 x i32>
  %80 = shufflevector <4 x i32> %79, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %81 = bitcast <4 x i32> %80 to <2 x i64>
  %82 = shufflevector <4 x i32> %79, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %83 = bitcast <4 x i32> %82 to <2 x i64>
  %84 = bitcast <2 x i64> %45 to <4 x i32>
  %85 = shufflevector <4 x i32> %84, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %86 = bitcast <4 x i32> %85 to <2 x i64>
  %87 = shufflevector <4 x i32> %84, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %88 = bitcast <4 x i32> %87 to <2 x i64>
  %89 = shl <2 x i64> %81, <i64 32, i64 32>
  %90 = ashr exact <2 x i64> %89, <i64 32, i64 32>
  %91 = mul nsw <2 x i64> %90, <i64 41576, i64 41576>
  %92 = shl <2 x i64> %83, <i64 32, i64 32>
  %93 = ashr exact <2 x i64> %92, <i64 32, i64 32>
  %94 = mul nsw <2 x i64> %93, <i64 41576, i64 41576>
  %95 = shl <2 x i64> %86, <i64 32, i64 32>
  %96 = ashr exact <2 x i64> %95, <i64 32, i64 32>
  %97 = shl <2 x i64> %88, <i64 32, i64 32>
  %98 = ashr exact <2 x i64> %97, <i64 32, i64 32>
  %99 = mul nsw <2 x i64> %90, <i64 50660, i64 50660>
  %100 = mul nsw <2 x i64> %93, <i64 50660, i64 50660>
  %101 = mul nsw <2 x i64> %96, <i64 50660, i64 50660>
  %102 = mul nsw <2 x i64> %98, <i64 50660, i64 50660>
  %103 = add nsw <2 x i64> %101, %91
  %104 = add nsw <2 x i64> %102, %94
  %105 = mul nsw <2 x i64> %96, <i64 -41576, i64 -41576>
  %106 = add nsw <2 x i64> %105, %99
  %107 = mul nsw <2 x i64> %98, <i64 -41576, i64 -41576>
  %108 = add nsw <2 x i64> %107, %100
  %109 = bitcast <2 x i64> %46 to <4 x i32>
  %110 = shufflevector <4 x i32> %109, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %111 = bitcast <4 x i32> %110 to <2 x i64>
  %112 = shufflevector <4 x i32> %109, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %113 = bitcast <4 x i32> %112 to <2 x i64>
  %114 = bitcast <2 x i64> %43 to <4 x i32>
  %115 = shufflevector <4 x i32> %114, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %116 = bitcast <4 x i32> %115 to <2 x i64>
  %117 = shufflevector <4 x i32> %114, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %118 = bitcast <4 x i32> %117 to <2 x i64>
  %119 = shl <2 x i64> %111, <i64 32, i64 32>
  %120 = ashr exact <2 x i64> %119, <i64 32, i64 32>
  %121 = mul nsw <2 x i64> %120, <i64 57796, i64 57796>
  %122 = shl <2 x i64> %113, <i64 32, i64 32>
  %123 = ashr exact <2 x i64> %122, <i64 32, i64 32>
  %124 = mul nsw <2 x i64> %123, <i64 57796, i64 57796>
  %125 = shl <2 x i64> %116, <i64 32, i64 32>
  %126 = ashr exact <2 x i64> %125, <i64 32, i64 32>
  %127 = shl <2 x i64> %118, <i64 32, i64 32>
  %128 = ashr exact <2 x i64> %127, <i64 32, i64 32>
  %129 = mul nsw <2 x i64> %120, <i64 30892, i64 30892>
  %130 = mul nsw <2 x i64> %123, <i64 30892, i64 30892>
  %131 = mul nsw <2 x i64> %126, <i64 30892, i64 30892>
  %132 = mul nsw <2 x i64> %128, <i64 30892, i64 30892>
  %133 = add nsw <2 x i64> %121, %131
  %134 = add nsw <2 x i64> %124, %132
  %135 = mul nsw <2 x i64> %126, <i64 -57796, i64 -57796>
  %136 = add nsw <2 x i64> %129, %135
  %137 = mul nsw <2 x i64> %128, <i64 -57796, i64 -57796>
  %138 = add nsw <2 x i64> %130, %137
  %139 = bitcast <2 x i64> %42 to <4 x i32>
  %140 = shufflevector <4 x i32> %139, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %141 = bitcast <4 x i32> %140 to <2 x i64>
  %142 = shufflevector <4 x i32> %139, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %143 = bitcast <4 x i32> %142 to <2 x i64>
  %144 = bitcast <2 x i64> %47 to <4 x i32>
  %145 = shufflevector <4 x i32> %144, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %146 = bitcast <4 x i32> %145 to <2 x i64>
  %147 = shufflevector <4 x i32> %144, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %148 = bitcast <4 x i32> %147 to <2 x i64>
  %149 = shl <2 x i64> %141, <i64 32, i64 32>
  %150 = ashr exact <2 x i64> %149, <i64 32, i64 32>
  %151 = mul nsw <2 x i64> %150, <i64 19024, i64 19024>
  %152 = shl <2 x i64> %143, <i64 32, i64 32>
  %153 = ashr exact <2 x i64> %152, <i64 32, i64 32>
  %154 = mul nsw <2 x i64> %153, <i64 19024, i64 19024>
  %155 = shl <2 x i64> %146, <i64 32, i64 32>
  %156 = ashr exact <2 x i64> %155, <i64 32, i64 32>
  %157 = shl <2 x i64> %148, <i64 32, i64 32>
  %158 = ashr exact <2 x i64> %157, <i64 32, i64 32>
  %159 = mul nsw <2 x i64> %150, <i64 62716, i64 62716>
  %160 = mul nsw <2 x i64> %153, <i64 62716, i64 62716>
  %161 = mul nsw <2 x i64> %156, <i64 62716, i64 62716>
  %162 = mul nsw <2 x i64> %158, <i64 62716, i64 62716>
  %163 = add nsw <2 x i64> %161, %151
  %164 = add nsw <2 x i64> %162, %154
  %165 = mul nsw <2 x i64> %156, <i64 -19024, i64 -19024>
  %166 = add nsw <2 x i64> %165, %159
  %167 = mul nsw <2 x i64> %158, <i64 -19024, i64 -19024>
  %168 = add nsw <2 x i64> %167, %160
  %169 = add nsw <2 x i64> %73, <i64 32768, i64 32768>
  %170 = add nsw <2 x i64> %169, %103
  %171 = bitcast <2 x i64> %170 to <16 x i8>
  %172 = shufflevector <16 x i8> %171, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %173 = add nsw <2 x i64> %74, <i64 32768, i64 32768>
  %174 = add nsw <2 x i64> %173, %104
  %175 = bitcast <2 x i64> %174 to <16 x i8>
  %176 = shufflevector <16 x i8> %175, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %177 = add nsw <2 x i64> %76, <i64 32768, i64 32768>
  %178 = add nsw <2 x i64> %177, %106
  %179 = bitcast <2 x i64> %178 to <16 x i8>
  %180 = shufflevector <16 x i8> %179, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %181 = add nsw <2 x i64> %78, <i64 32768, i64 32768>
  %182 = add nsw <2 x i64> %181, %108
  %183 = bitcast <2 x i64> %182 to <16 x i8>
  %184 = shufflevector <16 x i8> %183, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %185 = add nsw <2 x i64> %133, <i64 32768, i64 32768>
  %186 = add nsw <2 x i64> %185, %163
  %187 = bitcast <2 x i64> %186 to <16 x i8>
  %188 = shufflevector <16 x i8> %187, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %189 = add nsw <2 x i64> %134, <i64 32768, i64 32768>
  %190 = add nsw <2 x i64> %189, %164
  %191 = bitcast <2 x i64> %190 to <16 x i8>
  %192 = shufflevector <16 x i8> %191, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %193 = add nsw <2 x i64> %136, <i64 32768, i64 32768>
  %194 = add nsw <2 x i64> %193, %166
  %195 = bitcast <2 x i64> %194 to <16 x i8>
  %196 = shufflevector <16 x i8> %195, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %197 = add nsw <2 x i64> %138, <i64 32768, i64 32768>
  %198 = add nsw <2 x i64> %197, %168
  %199 = bitcast <2 x i64> %198 to <16 x i8>
  %200 = shufflevector <16 x i8> %199, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %201 = sub nsw <2 x i64> <i64 32768, i64 32768>, %103
  %202 = add nsw <2 x i64> %201, %73
  %203 = bitcast <2 x i64> %202 to <16 x i8>
  %204 = shufflevector <16 x i8> %203, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %205 = sub nsw <2 x i64> <i64 32768, i64 32768>, %104
  %206 = add nsw <2 x i64> %205, %74
  %207 = bitcast <2 x i64> %206 to <16 x i8>
  %208 = shufflevector <16 x i8> %207, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %209 = sub nsw <2 x i64> <i64 32768, i64 32768>, %106
  %210 = add nsw <2 x i64> %209, %76
  %211 = bitcast <2 x i64> %210 to <16 x i8>
  %212 = shufflevector <16 x i8> %211, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %213 = sub nsw <2 x i64> <i64 32768, i64 32768>, %108
  %214 = add nsw <2 x i64> %213, %78
  %215 = bitcast <2 x i64> %214 to <16 x i8>
  %216 = shufflevector <16 x i8> %215, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %217 = sub nsw <2 x i64> <i64 32768, i64 32768>, %163
  %218 = add nsw <2 x i64> %217, %133
  %219 = bitcast <2 x i64> %218 to <16 x i8>
  %220 = shufflevector <16 x i8> %219, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %221 = sub nsw <2 x i64> <i64 32768, i64 32768>, %164
  %222 = add nsw <2 x i64> %221, %134
  %223 = bitcast <2 x i64> %222 to <16 x i8>
  %224 = shufflevector <16 x i8> %223, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %225 = sub nsw <2 x i64> <i64 32768, i64 32768>, %166
  %226 = add nsw <2 x i64> %225, %136
  %227 = bitcast <2 x i64> %226 to <16 x i8>
  %228 = shufflevector <16 x i8> %227, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %229 = sub nsw <2 x i64> <i64 32768, i64 32768>, %168
  %230 = add nsw <2 x i64> %229, %138
  %231 = bitcast <2 x i64> %230 to <16 x i8>
  %232 = shufflevector <16 x i8> %231, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %233 = bitcast <16 x i8> %172 to <4 x i32>
  %234 = bitcast <16 x i8> %176 to <4 x i32>
  %235 = shufflevector <4 x i32> %233, <4 x i32> %234, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %236 = shufflevector <4 x i32> %233, <4 x i32> %234, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %237 = shufflevector <4 x i32> %235, <4 x i32> %236, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %238 = bitcast <16 x i8> %180 to <4 x i32>
  %239 = bitcast <16 x i8> %184 to <4 x i32>
  %240 = shufflevector <4 x i32> %238, <4 x i32> %239, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %241 = shufflevector <4 x i32> %238, <4 x i32> %239, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %242 = shufflevector <4 x i32> %240, <4 x i32> %241, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %243 = bitcast <16 x i8> %188 to <4 x i32>
  %244 = bitcast <16 x i8> %192 to <4 x i32>
  %245 = shufflevector <4 x i32> %243, <4 x i32> %244, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %246 = shufflevector <4 x i32> %243, <4 x i32> %244, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %247 = shufflevector <4 x i32> %245, <4 x i32> %246, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %248 = bitcast <16 x i8> %196 to <4 x i32>
  %249 = bitcast <16 x i8> %200 to <4 x i32>
  %250 = shufflevector <4 x i32> %248, <4 x i32> %249, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %251 = shufflevector <4 x i32> %248, <4 x i32> %249, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %252 = shufflevector <4 x i32> %250, <4 x i32> %251, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %253 = bitcast <16 x i8> %204 to <4 x i32>
  %254 = bitcast <16 x i8> %208 to <4 x i32>
  %255 = shufflevector <4 x i32> %253, <4 x i32> %254, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %256 = shufflevector <4 x i32> %253, <4 x i32> %254, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %257 = shufflevector <4 x i32> %255, <4 x i32> %256, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %258 = bitcast <16 x i8> %212 to <4 x i32>
  %259 = bitcast <16 x i8> %216 to <4 x i32>
  %260 = shufflevector <4 x i32> %258, <4 x i32> %259, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %261 = shufflevector <4 x i32> %258, <4 x i32> %259, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %262 = shufflevector <4 x i32> %260, <4 x i32> %261, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %263 = bitcast <16 x i8> %220 to <4 x i32>
  %264 = bitcast <16 x i8> %224 to <4 x i32>
  %265 = shufflevector <4 x i32> %263, <4 x i32> %264, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %266 = shufflevector <4 x i32> %263, <4 x i32> %264, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %267 = shufflevector <4 x i32> %265, <4 x i32> %266, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %268 = bitcast <16 x i8> %228 to <4 x i32>
  %269 = bitcast <16 x i8> %232 to <4 x i32>
  %270 = shufflevector <4 x i32> %268, <4 x i32> %269, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %271 = shufflevector <4 x i32> %268, <4 x i32> %269, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %272 = shufflevector <4 x i32> %270, <4 x i32> %271, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %273 = add <4 x i32> %247, %237
  %274 = add <4 x i32> %252, %242
  %275 = sub <4 x i32> %237, %247
  %276 = sub <4 x i32> %242, %252
  %277 = shufflevector <4 x i32> %257, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %278 = bitcast <4 x i32> %277 to <2 x i64>
  %279 = shufflevector <4 x i32> %257, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %280 = bitcast <4 x i32> %279 to <2 x i64>
  %281 = shufflevector <4 x i32> %262, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %282 = bitcast <4 x i32> %281 to <2 x i64>
  %283 = shufflevector <4 x i32> %262, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %284 = bitcast <4 x i32> %283 to <2 x i64>
  %285 = shl <2 x i64> %278, <i64 32, i64 32>
  %286 = ashr exact <2 x i64> %285, <i64 32, i64 32>
  %287 = mul nsw <2 x i64> %286, <i64 60548, i64 60548>
  %288 = shl <2 x i64> %280, <i64 32, i64 32>
  %289 = ashr exact <2 x i64> %288, <i64 32, i64 32>
  %290 = mul nsw <2 x i64> %289, <i64 60548, i64 60548>
  %291 = shl <2 x i64> %282, <i64 32, i64 32>
  %292 = ashr exact <2 x i64> %291, <i64 32, i64 32>
  %293 = shl <2 x i64> %284, <i64 32, i64 32>
  %294 = ashr exact <2 x i64> %293, <i64 32, i64 32>
  %295 = mul nsw <2 x i64> %286, <i64 25080, i64 25080>
  %296 = mul nsw <2 x i64> %289, <i64 25080, i64 25080>
  %297 = mul nsw <2 x i64> %292, <i64 25080, i64 25080>
  %298 = mul nsw <2 x i64> %294, <i64 25080, i64 25080>
  %299 = add nsw <2 x i64> %297, %287
  %300 = add nsw <2 x i64> %298, %290
  %301 = mul nsw <2 x i64> %292, <i64 -60548, i64 -60548>
  %302 = add nsw <2 x i64> %295, %301
  %303 = mul nsw <2 x i64> %294, <i64 -60548, i64 -60548>
  %304 = add nsw <2 x i64> %296, %303
  %305 = shufflevector <4 x i32> %272, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %306 = bitcast <4 x i32> %305 to <2 x i64>
  %307 = shufflevector <4 x i32> %272, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %308 = bitcast <4 x i32> %307 to <2 x i64>
  %309 = shufflevector <4 x i32> %267, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %310 = bitcast <4 x i32> %309 to <2 x i64>
  %311 = shufflevector <4 x i32> %267, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %312 = bitcast <4 x i32> %311 to <2 x i64>
  %313 = shl <2 x i64> %306, <i64 32, i64 32>
  %314 = ashr exact <2 x i64> %313, <i64 32, i64 32>
  %315 = mul nsw <2 x i64> %314, <i64 25080, i64 25080>
  %316 = shl <2 x i64> %308, <i64 32, i64 32>
  %317 = ashr exact <2 x i64> %316, <i64 32, i64 32>
  %318 = mul nsw <2 x i64> %317, <i64 25080, i64 25080>
  %319 = shl <2 x i64> %310, <i64 32, i64 32>
  %320 = ashr exact <2 x i64> %319, <i64 32, i64 32>
  %321 = shl <2 x i64> %312, <i64 32, i64 32>
  %322 = ashr exact <2 x i64> %321, <i64 32, i64 32>
  %323 = mul nsw <2 x i64> %314, <i64 60548, i64 60548>
  %324 = mul nsw <2 x i64> %317, <i64 60548, i64 60548>
  %325 = mul nsw <2 x i64> %320, <i64 60548, i64 60548>
  %326 = mul nsw <2 x i64> %322, <i64 60548, i64 60548>
  %327 = add nsw <2 x i64> %325, %315
  %328 = add nsw <2 x i64> %326, %318
  %329 = mul nsw <2 x i64> %320, <i64 -25080, i64 -25080>
  %330 = add nsw <2 x i64> %323, %329
  %331 = mul nsw <2 x i64> %322, <i64 -25080, i64 -25080>
  %332 = add nsw <2 x i64> %324, %331
  %333 = add nsw <2 x i64> %299, <i64 32768, i64 32768>
  %334 = add nsw <2 x i64> %333, %330
  %335 = bitcast <2 x i64> %334 to <16 x i8>
  %336 = shufflevector <16 x i8> %335, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %337 = add nsw <2 x i64> %300, <i64 32768, i64 32768>
  %338 = add nsw <2 x i64> %337, %332
  %339 = bitcast <2 x i64> %338 to <16 x i8>
  %340 = shufflevector <16 x i8> %339, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %341 = add nsw <2 x i64> %302, <i64 32768, i64 32768>
  %342 = add nsw <2 x i64> %341, %327
  %343 = bitcast <2 x i64> %342 to <16 x i8>
  %344 = shufflevector <16 x i8> %343, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %345 = add nsw <2 x i64> %304, <i64 32768, i64 32768>
  %346 = add nsw <2 x i64> %345, %328
  %347 = bitcast <2 x i64> %346 to <16 x i8>
  %348 = shufflevector <16 x i8> %347, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %349 = sub nsw <2 x i64> <i64 32768, i64 32768>, %330
  %350 = add nsw <2 x i64> %349, %299
  %351 = bitcast <2 x i64> %350 to <16 x i8>
  %352 = shufflevector <16 x i8> %351, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %353 = sub nsw <2 x i64> <i64 32768, i64 32768>, %332
  %354 = add nsw <2 x i64> %353, %300
  %355 = bitcast <2 x i64> %354 to <16 x i8>
  %356 = shufflevector <16 x i8> %355, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %357 = sub nsw <2 x i64> <i64 32768, i64 32768>, %327
  %358 = add nsw <2 x i64> %357, %302
  %359 = bitcast <2 x i64> %358 to <16 x i8>
  %360 = shufflevector <16 x i8> %359, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %361 = sub nsw <2 x i64> <i64 32768, i64 32768>, %328
  %362 = add nsw <2 x i64> %361, %304
  %363 = bitcast <2 x i64> %362 to <16 x i8>
  %364 = shufflevector <16 x i8> %363, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %365 = bitcast <16 x i8> %336 to <4 x i32>
  %366 = bitcast <16 x i8> %340 to <4 x i32>
  %367 = shufflevector <4 x i32> %365, <4 x i32> %366, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %368 = shufflevector <4 x i32> %365, <4 x i32> %366, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %369 = shufflevector <4 x i32> %367, <4 x i32> %368, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %370 = bitcast <16 x i8> %344 to <4 x i32>
  %371 = bitcast <16 x i8> %348 to <4 x i32>
  %372 = shufflevector <4 x i32> %370, <4 x i32> %371, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %373 = shufflevector <4 x i32> %370, <4 x i32> %371, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %374 = shufflevector <4 x i32> %372, <4 x i32> %373, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %375 = bitcast <16 x i8> %352 to <4 x i32>
  %376 = bitcast <16 x i8> %356 to <4 x i32>
  %377 = shufflevector <4 x i32> %375, <4 x i32> %376, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %378 = shufflevector <4 x i32> %375, <4 x i32> %376, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %379 = shufflevector <4 x i32> %377, <4 x i32> %378, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %380 = bitcast <16 x i8> %360 to <4 x i32>
  %381 = bitcast <16 x i8> %364 to <4 x i32>
  %382 = shufflevector <4 x i32> %380, <4 x i32> %381, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %383 = shufflevector <4 x i32> %380, <4 x i32> %381, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %384 = shufflevector <4 x i32> %382, <4 x i32> %383, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %385 = add <4 x i32> %276, %275
  %386 = sub <4 x i32> %275, %276
  %387 = add <4 x i32> %384, %379
  %388 = sub <4 x i32> %379, %384
  %389 = shufflevector <4 x i32> %385, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %390 = bitcast <4 x i32> %389 to <2 x i64>
  %391 = shufflevector <4 x i32> %385, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %392 = bitcast <4 x i32> %391 to <2 x i64>
  %393 = shl <2 x i64> %390, <i64 32, i64 32>
  %394 = ashr exact <2 x i64> %393, <i64 32, i64 32>
  %395 = mul nsw <2 x i64> %394, <i64 46340, i64 46340>
  %396 = shl <2 x i64> %392, <i64 32, i64 32>
  %397 = ashr exact <2 x i64> %396, <i64 32, i64 32>
  %398 = mul nsw <2 x i64> %397, <i64 46340, i64 46340>
  %399 = shufflevector <4 x i32> %386, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %400 = bitcast <4 x i32> %399 to <2 x i64>
  %401 = shufflevector <4 x i32> %386, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %402 = bitcast <4 x i32> %401 to <2 x i64>
  %403 = shl <2 x i64> %400, <i64 32, i64 32>
  %404 = ashr exact <2 x i64> %403, <i64 32, i64 32>
  %405 = mul nsw <2 x i64> %404, <i64 46340, i64 46340>
  %406 = shl <2 x i64> %402, <i64 32, i64 32>
  %407 = ashr exact <2 x i64> %406, <i64 32, i64 32>
  %408 = mul nsw <2 x i64> %407, <i64 46340, i64 46340>
  %409 = shufflevector <4 x i32> %387, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %410 = bitcast <4 x i32> %409 to <2 x i64>
  %411 = shufflevector <4 x i32> %387, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %412 = bitcast <4 x i32> %411 to <2 x i64>
  %413 = shl <2 x i64> %410, <i64 32, i64 32>
  %414 = ashr exact <2 x i64> %413, <i64 32, i64 32>
  %415 = mul nsw <2 x i64> %414, <i64 46340, i64 46340>
  %416 = shl <2 x i64> %412, <i64 32, i64 32>
  %417 = ashr exact <2 x i64> %416, <i64 32, i64 32>
  %418 = mul nsw <2 x i64> %417, <i64 46340, i64 46340>
  %419 = shufflevector <4 x i32> %388, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %420 = bitcast <4 x i32> %419 to <2 x i64>
  %421 = shufflevector <4 x i32> %388, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %422 = bitcast <4 x i32> %421 to <2 x i64>
  %423 = shl <2 x i64> %420, <i64 32, i64 32>
  %424 = ashr exact <2 x i64> %423, <i64 32, i64 32>
  %425 = mul nsw <2 x i64> %424, <i64 46340, i64 46340>
  %426 = shl <2 x i64> %422, <i64 32, i64 32>
  %427 = ashr exact <2 x i64> %426, <i64 32, i64 32>
  %428 = mul nsw <2 x i64> %427, <i64 46340, i64 46340>
  %429 = add nsw <2 x i64> %395, <i64 32768, i64 32768>
  %430 = bitcast <2 x i64> %429 to <16 x i8>
  %431 = shufflevector <16 x i8> %430, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %432 = add nsw <2 x i64> %398, <i64 32768, i64 32768>
  %433 = bitcast <2 x i64> %432 to <16 x i8>
  %434 = shufflevector <16 x i8> %433, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %435 = add nsw <2 x i64> %405, <i64 32768, i64 32768>
  %436 = bitcast <2 x i64> %435 to <16 x i8>
  %437 = shufflevector <16 x i8> %436, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %438 = add nsw <2 x i64> %408, <i64 32768, i64 32768>
  %439 = bitcast <2 x i64> %438 to <16 x i8>
  %440 = shufflevector <16 x i8> %439, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %441 = add nsw <2 x i64> %415, <i64 32768, i64 32768>
  %442 = bitcast <2 x i64> %441 to <16 x i8>
  %443 = shufflevector <16 x i8> %442, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %444 = add nsw <2 x i64> %418, <i64 32768, i64 32768>
  %445 = bitcast <2 x i64> %444 to <16 x i8>
  %446 = shufflevector <16 x i8> %445, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %447 = add nsw <2 x i64> %425, <i64 32768, i64 32768>
  %448 = bitcast <2 x i64> %447 to <16 x i8>
  %449 = shufflevector <16 x i8> %448, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %450 = add nsw <2 x i64> %428, <i64 32768, i64 32768>
  %451 = bitcast <2 x i64> %450 to <16 x i8>
  %452 = shufflevector <16 x i8> %451, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %453 = bitcast <16 x i8> %431 to <4 x i32>
  %454 = bitcast <16 x i8> %434 to <4 x i32>
  %455 = shufflevector <4 x i32> %453, <4 x i32> %454, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %456 = shufflevector <4 x i32> %453, <4 x i32> %454, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %457 = shufflevector <4 x i32> %455, <4 x i32> %456, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %458 = bitcast <16 x i8> %437 to <4 x i32>
  %459 = bitcast <16 x i8> %440 to <4 x i32>
  %460 = shufflevector <4 x i32> %458, <4 x i32> %459, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %461 = shufflevector <4 x i32> %458, <4 x i32> %459, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %462 = shufflevector <4 x i32> %460, <4 x i32> %461, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %463 = bitcast <16 x i8> %443 to <4 x i32>
  %464 = bitcast <16 x i8> %446 to <4 x i32>
  %465 = shufflevector <4 x i32> %463, <4 x i32> %464, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %466 = shufflevector <4 x i32> %463, <4 x i32> %464, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %467 = shufflevector <4 x i32> %465, <4 x i32> %466, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %468 = bitcast <16 x i8> %449 to <4 x i32>
  %469 = bitcast <16 x i8> %452 to <4 x i32>
  %470 = shufflevector <4 x i32> %468, <4 x i32> %469, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %471 = shufflevector <4 x i32> %468, <4 x i32> %469, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %472 = shufflevector <4 x i32> %470, <4 x i32> %471, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  store <4 x i32> %273, <4 x i32>* %2, align 16
  %473 = sub <4 x i32> zeroinitializer, %369
  store <4 x i32> %473, <4 x i32>* %5, align 16
  store <4 x i32> %467, <4 x i32>* %10, align 16
  %474 = sub <4 x i32> zeroinitializer, %457
  store <4 x i32> %474, <4 x i32>* %13, align 16
  store <4 x i32> %462, <4 x i32>* %22, align 16
  %475 = sub <4 x i32> zeroinitializer, %472
  store <4 x i32> %475, <4 x i32>* %25, align 16
  store <4 x i32> %374, <4 x i32>* %30, align 16
  %476 = sub <4 x i32> zeroinitializer, %274
  store <4 x i32> %476, <4 x i32>* %33, align 16
  ret void
}

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32>, <4 x i32>) #3

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16>, i32) #3

; Function Attrs: nounwind readnone speculatable
declare <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16>, <8 x i16>) #4

attributes #0 = { nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="128" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+sse4.1,+ssse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { argmemonly nounwind }
attributes #2 = { "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+sse4.1,+ssse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #3 = { nounwind readnone }
attributes #4 = { nounwind readnone speculatable }
attributes #5 = { nounwind }

!llvm.module.flags = !{!0, !1}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 2}
