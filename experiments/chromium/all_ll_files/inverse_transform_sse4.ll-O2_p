; ModuleID = '../../third_party/libgav1/src/src/dsp/x86/inverse_transform_sse4.cc'
source_filename = "../../third_party/libgav1/src/src/dsp/x86/inverse_transform_sse4.cc"
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

%"struct.libgav1::dsp::Dsp" = type { void (i8*, i8*, i32, i32, i8*, i64)*, void (i8*, i64, i8*, i32*)*, [2 x [3 x void (i16*, i64, i32, i32, i32, i32, i32, i8*, i64)*]], [19 x void (i8*, i64, [32 x i16]*, i32)*], [19 x [3 x void ([32 x i16]*, i32, i32, i8*, i64)*]], [2 x [2 x [2 x [2 x void (i8*, i64, i32, i32, i32, i32, i32, i32, i8*, i64)*]]]], [2 x void (i8*, i64, i32, i32, i32, i32, i32, i32, i32, i32, i8*, i64)*], void (i8*, i64, i8*, i32, i32, i32, i1)*, void (i8*, i64, i8*, i8*, i32, i32, i32, i32, i1, i1)*, void (i8*, i64, i8*, i32, i32, i32, i1)*, void (i8*, i8*, i8, i8, i32, i32, i8*, i64)*, %"struct.libgav1::dsp::FilmGrainFuncs", void (i8*, i64, i8*, i8*, i8, i32, i32)*, [3 x void (i8*, i8*, i64, i8*, i64, i32, i32)*], void (i8*, i32, i32)*, void (i8*, i32)*, [19 x [10 x void (i8*, i64, i8*, i8*)*]], [4 x [5 x [2 x void (i8, i8, i32, i8*, i32, i32, i8*)*]]], [4 x [2 x void (i8*, i64, i32, i32, i32)*]], [2 x void (%"struct.libgav1::RestorationUnitInfo"*, i8*, i64, i8*, i64, i8*, i64, i32, i32, %"union.libgav1::RestorationBuffer"*, i8*)*], [3 x [2 x void (i8*, i8*, i64, i8*, i64, i32, i32, i8*, i64)*]], void (%"struct.libgav1::ReferenceInfo"*, i32, i32, i32, i32, i32, i32, %"struct.libgav1::TemporalMotionField"*)*, [3 x void (%"struct.libgav1::MotionVector"*, i8*, i32*, i32, %"union.libgav1::CompoundMotionVector"*)*], [3 x void (%"struct.libgav1::MotionVector"*, i8*, i32, i32, %"struct.libgav1::MotionVector"*)*], [2 x void (i8*, i64, i32, i32, i8*, i64)*], void (i32, i32, i32, i8*)*, void (i8*, i8*, i64, i32, i32, i32, i32, i32, i8*, i64)*, void (i8*, i64, i32, i32, i32*, i32, i32, i32, i32, i32, i32, i16, i16, i16, i16, i8*, i64)*, void (i8*, i64, i32, i32, i32*, i32, i32, i32, i32, i32, i32, i16, i16, i16, i16, i8*, i64)*, [6 x [6 x [2 x void (i8*, i8*, i8*, i64)*]]] }
%"struct.libgav1::dsp::FilmGrainFuncs" = type { [3 x void (%"struct.libgav1::FilmGrainParams"*, i8*)*], [2 x [4 x void (%"struct.libgav1::FilmGrainParams"*, i8*, i32, i32, i8*, i8*)*]], [2 x void (i8*, i32, i32, i32, i32, i32, i8*)*], void (i8*, i32, i32, i32, i32, i8*)*, void (i32, i8*, i8*, i8*)*, void (i8*, i32, i32, i32, i32, i32, i32, i8*, i8*, i64, i8*, i64)*, [2 x void (i8, %"struct.libgav1::FilmGrainParams"*, i8*, i32, i32, i32, i32, i32, i32, i32, i8*, i8*, i64, i8*, i64, i8*, i64)*] }
%"struct.libgav1::FilmGrainParams" = type { i8, i8, i8, i8, i8, i8, i8, i8, [14 x i8], [14 x i8], [10 x i8], [10 x i8], [10 x i8], [10 x i8], i8, i8, [24 x i8], [25 x i8], [25 x i8], i8, i16, i32, i32, i8, i8, i16, i8, i8, i16 }
%"struct.libgav1::RestorationUnitInfo" = type { i8, %"struct.libgav1::SgrProjInfo", [16 x i8], %"struct.libgav1::WienerInfo" }
%"struct.libgav1::SgrProjInfo" = type { i32, [2 x i32] }
%"struct.libgav1::WienerInfo" = type { [2 x i16], [28 x i8], [2 x [4 x i16]], [16 x i8] }
%"union.libgav1::RestorationBuffer" = type { %"struct.libgav1::SgrBuffer", [5024 x i8] }
%"struct.libgav1::SgrBuffer" = type { [1152 x i16], [1440 x i16], [1152 x i32], [1440 x i32], [1024 x i16], [768 x i16], [512 x i16], [1024 x i32], [768 x i32], [512 x i32], [288 x i8], [288 x i32] }
%"struct.libgav1::ReferenceInfo" = type { %"struct.std::__1::array", %"struct.std::__1::array.0", %"struct.std::__1::array.0", %"struct.std::__1::array.1", %"struct.std::__1::array.2", %"class.libgav1::Array2D", %"class.libgav1::Array2D.4" }
%"struct.std::__1::array" = type { [8 x i8] }
%"struct.std::__1::array.0" = type { [8 x i8] }
%"struct.std::__1::array.1" = type { [8 x i8] }
%"struct.std::__1::array.2" = type { [8 x i16] }
%"class.libgav1::Array2D" = type { %"class.std::__1::unique_ptr", i64, i64, %"class.libgav1::Array2DView" }
%"class.std::__1::unique_ptr" = type { %"class.std::__1::__compressed_pair" }
%"class.std::__1::__compressed_pair" = type { %"struct.std::__1::__compressed_pair_elem" }
%"struct.std::__1::__compressed_pair_elem" = type { i8* }
%"class.libgav1::Array2DView" = type { i32, i32, i8* }
%"class.libgav1::Array2D.4" = type { %"class.std::__1::unique_ptr.5", i64, i64, %"class.libgav1::Array2DView.11" }
%"class.std::__1::unique_ptr.5" = type { %"class.std::__1::__compressed_pair.6" }
%"class.std::__1::__compressed_pair.6" = type { %"struct.std::__1::__compressed_pair_elem.7" }
%"struct.std::__1::__compressed_pair_elem.7" = type { %"struct.libgav1::MotionVector"* }
%"struct.libgav1::MotionVector" = type { %union.anon }
%union.anon = type { i32 }
%"class.libgav1::Array2DView.11" = type { i32, i32, %"struct.libgav1::MotionVector"* }
%"struct.libgav1::TemporalMotionField" = type { %"class.libgav1::Array2D.4", %"class.libgav1::Array2D.12" }
%"class.libgav1::Array2D.12" = type { %"class.std::__1::unique_ptr.13", i64, i64, %"class.libgav1::Array2DView.19" }
%"class.std::__1::unique_ptr.13" = type { %"class.std::__1::__compressed_pair.14" }
%"class.std::__1::__compressed_pair.14" = type { %"struct.std::__1::__compressed_pair_elem.15" }
%"struct.std::__1::__compressed_pair_elem.15" = type { i8* }
%"class.libgav1::Array2DView.19" = type { i32, i32, i8* }
%"union.libgav1::CompoundMotionVector" = type { i64 }

@_ZN7libgav116kTransformHeightE = external local_unnamed_addr constant [19 x i8], align 16
@_ZN7libgav115kTransformWidthE = external local_unnamed_addr constant [19 x i8], align 16
@_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_118kTransformRowShiftE = internal unnamed_addr constant [19 x i8] c"\00\00\01\00\01\01\02\01\01\02\01\02\02\01\02\01\02\01\02", align 16

; Function Attrs: nounwind ssp uwtable
define hidden void @_ZN7libgav13dsp27InverseTransformInit_SSE4_1Ev() local_unnamed_addr #0 {
  %1 = tail call %"struct.libgav1::dsp::Dsp"* @_ZN7libgav112dsp_internal19GetWritableDspTableEi(i32 8) #8
  %2 = getelementptr inbounds %"struct.libgav1::dsp::Dsp", %"struct.libgav1::dsp::Dsp"* %1, i64 0, i32 17, i64 0, i64 0, i64 0
  %3 = bitcast void (i8, i8, i32, i8*, i32, i32, i8*)** %2 to <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*>*
  store <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*> <void (i8, i8, i32, i8*, i32, i32, i8*)* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_127Dct4TransformLoopRow_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_, void (i8, i8, i32, i8*, i32, i32, i8*)* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_130Dct4TransformLoopColumn_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_>, <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*>* %3, align 8
  %4 = getelementptr inbounds %"struct.libgav1::dsp::Dsp", %"struct.libgav1::dsp::Dsp"* %1, i64 0, i32 17, i64 0, i64 1, i64 0
  %5 = bitcast void (i8, i8, i32, i8*, i32, i32, i8*)** %4 to <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*>*
  store <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*> <void (i8, i8, i32, i8*, i32, i32, i8*)* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_127Dct8TransformLoopRow_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_, void (i8, i8, i32, i8*, i32, i32, i8*)* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_130Dct8TransformLoopColumn_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_>, <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*>* %5, align 8
  %6 = getelementptr inbounds %"struct.libgav1::dsp::Dsp", %"struct.libgav1::dsp::Dsp"* %1, i64 0, i32 17, i64 0, i64 2, i64 0
  %7 = bitcast void (i8, i8, i32, i8*, i32, i32, i8*)** %6 to <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*>*
  store <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*> <void (i8, i8, i32, i8*, i32, i32, i8*)* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_128Dct16TransformLoopRow_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_, void (i8, i8, i32, i8*, i32, i32, i8*)* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_131Dct16TransformLoopColumn_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_>, <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*>* %7, align 8
  %8 = getelementptr inbounds %"struct.libgav1::dsp::Dsp", %"struct.libgav1::dsp::Dsp"* %1, i64 0, i32 17, i64 0, i64 3, i64 0
  %9 = bitcast void (i8, i8, i32, i8*, i32, i32, i8*)** %8 to <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*>*
  store <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*> <void (i8, i8, i32, i8*, i32, i32, i8*)* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_128Dct32TransformLoopRow_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_, void (i8, i8, i32, i8*, i32, i32, i8*)* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_131Dct32TransformLoopColumn_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_>, <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*>* %9, align 8
  %10 = getelementptr inbounds %"struct.libgav1::dsp::Dsp", %"struct.libgav1::dsp::Dsp"* %1, i64 0, i32 17, i64 0, i64 4, i64 0
  %11 = bitcast void (i8, i8, i32, i8*, i32, i32, i8*)** %10 to <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*>*
  store <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*> <void (i8, i8, i32, i8*, i32, i32, i8*)* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_128Dct64TransformLoopRow_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_, void (i8, i8, i32, i8*, i32, i32, i8*)* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_131Dct64TransformLoopColumn_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_>, <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*>* %11, align 8
  %12 = getelementptr inbounds %"struct.libgav1::dsp::Dsp", %"struct.libgav1::dsp::Dsp"* %1, i64 0, i32 17, i64 1, i64 0, i64 0
  %13 = bitcast void (i8, i8, i32, i8*, i32, i32, i8*)** %12 to <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*>*
  store <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*> <void (i8, i8, i32, i8*, i32, i32, i8*)* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_128Adst4TransformLoopRow_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_, void (i8, i8, i32, i8*, i32, i32, i8*)* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_131Adst4TransformLoopColumn_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_>, <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*>* %13, align 8
  %14 = getelementptr inbounds %"struct.libgav1::dsp::Dsp", %"struct.libgav1::dsp::Dsp"* %1, i64 0, i32 17, i64 1, i64 1, i64 0
  %15 = bitcast void (i8, i8, i32, i8*, i32, i32, i8*)** %14 to <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*>*
  store <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*> <void (i8, i8, i32, i8*, i32, i32, i8*)* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_128Adst8TransformLoopRow_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_, void (i8, i8, i32, i8*, i32, i32, i8*)* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_131Adst8TransformLoopColumn_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_>, <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*>* %15, align 8
  %16 = getelementptr inbounds %"struct.libgav1::dsp::Dsp", %"struct.libgav1::dsp::Dsp"* %1, i64 0, i32 17, i64 1, i64 2, i64 0
  %17 = bitcast void (i8, i8, i32, i8*, i32, i32, i8*)** %16 to <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*>*
  store <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*> <void (i8, i8, i32, i8*, i32, i32, i8*)* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_129Adst16TransformLoopRow_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_, void (i8, i8, i32, i8*, i32, i32, i8*)* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_132Adst16TransformLoopColumn_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_>, <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*>* %17, align 8
  %18 = getelementptr inbounds %"struct.libgav1::dsp::Dsp", %"struct.libgav1::dsp::Dsp"* %1, i64 0, i32 17, i64 2, i64 0, i64 0
  %19 = bitcast void (i8, i8, i32, i8*, i32, i32, i8*)** %18 to <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*>*
  store <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*> <void (i8, i8, i32, i8*, i32, i32, i8*)* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_132Identity4TransformLoopRow_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_, void (i8, i8, i32, i8*, i32, i32, i8*)* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_135Identity4TransformLoopColumn_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_>, <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*>* %19, align 8
  %20 = getelementptr inbounds %"struct.libgav1::dsp::Dsp", %"struct.libgav1::dsp::Dsp"* %1, i64 0, i32 17, i64 2, i64 1, i64 0
  %21 = bitcast void (i8, i8, i32, i8*, i32, i32, i8*)** %20 to <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*>*
  store <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*> <void (i8, i8, i32, i8*, i32, i32, i8*)* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_132Identity8TransformLoopRow_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_, void (i8, i8, i32, i8*, i32, i32, i8*)* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_135Identity8TransformLoopColumn_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_>, <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*>* %21, align 8
  %22 = getelementptr inbounds %"struct.libgav1::dsp::Dsp", %"struct.libgav1::dsp::Dsp"* %1, i64 0, i32 17, i64 2, i64 2, i64 0
  %23 = bitcast void (i8, i8, i32, i8*, i32, i32, i8*)** %22 to <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*>*
  store <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*> <void (i8, i8, i32, i8*, i32, i32, i8*)* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_133Identity16TransformLoopRow_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_, void (i8, i8, i32, i8*, i32, i32, i8*)* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_136Identity16TransformLoopColumn_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_>, <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*>* %23, align 8
  %24 = getelementptr inbounds %"struct.libgav1::dsp::Dsp", %"struct.libgav1::dsp::Dsp"* %1, i64 0, i32 17, i64 2, i64 3, i64 0
  %25 = bitcast void (i8, i8, i32, i8*, i32, i32, i8*)** %24 to <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*>*
  store <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*> <void (i8, i8, i32, i8*, i32, i32, i8*)* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_133Identity32TransformLoopRow_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_, void (i8, i8, i32, i8*, i32, i32, i8*)* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_136Identity32TransformLoopColumn_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_>, <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*>* %25, align 8
  %26 = getelementptr inbounds %"struct.libgav1::dsp::Dsp", %"struct.libgav1::dsp::Dsp"* %1, i64 0, i32 17, i64 3, i64 0, i64 0
  %27 = bitcast void (i8, i8, i32, i8*, i32, i32, i8*)** %26 to <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*>*
  store <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*> <void (i8, i8, i32, i8*, i32, i32, i8*)* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_127Wht4TransformLoopRow_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_, void (i8, i8, i32, i8*, i32, i32, i8*)* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_130Wht4TransformLoopColumn_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_>, <2 x void (i8, i8, i32, i8*, i32, i32, i8*)*>* %27, align 8
  ret void
}

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #1

declare %"struct.libgav1::dsp::Dsp"* @_ZN7libgav112dsp_internal19GetWritableDspTableEi(i32) local_unnamed_addr #2

; Function Attrs: nofree nounwind ssp uwtable
define internal void @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_127Dct4TransformLoopRow_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_(i8 zeroext, i8 zeroext, i32, i8* nocapture, i32, i32, i8* nocapture readnone) #3 {
  %8 = bitcast i8* %3 to i16*
  %9 = zext i8 %1 to i64
  %10 = getelementptr inbounds [19 x i8], [19 x i8]* @_ZN7libgav116kTransformHeightE, i64 0, i64 %9
  %11 = load i8, i8* %10, align 1
  %12 = icmp eq i8 %11, 8
  %13 = icmp eq i8 %11, 16
  %14 = icmp sgt i32 %2, 1
  br i1 %14, label %45, label %15

15:                                               ; preds = %7
  %16 = zext i1 %13 to i32
  %17 = load i16, i16* %8, align 2
  %18 = sext i16 %17 to i32
  %19 = insertelement <4 x i32> <i32 undef, i32 undef, i32 0, i32 0>, i32 %18, i32 0
  %20 = bitcast <4 x i32> %19 to <8 x i16>
  %21 = shufflevector <8 x i16> %20, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 4, i32 5, i32 6, i32 7>
  %22 = sext i1 %12 to i16
  %23 = insertelement <8 x i16> undef, i16 %22, i32 0
  %24 = shufflevector <8 x i16> %23, <8 x i16> undef, <8 x i32> zeroinitializer
  %25 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %21, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  %26 = bitcast <8 x i16> %21 to <16 x i8>
  %27 = bitcast <8 x i16> %25 to <16 x i8>
  %28 = bitcast <8 x i16> %24 to <16 x i8>
  %29 = tail call <16 x i8> @llvm.x86.sse41.pblendvb(<16 x i8> %26, <16 x i8> %27, <16 x i8> %28) #8
  %30 = bitcast <16 x i8> %29 to <8 x i16>
  %31 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %30, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  %32 = insertelement <4 x i32> undef, i32 %16, i32 0
  %33 = shufflevector <4 x i32> %32, <4 x i32> undef, <4 x i32> zeroinitializer
  %34 = shufflevector <4 x i32> %32, <4 x i32> undef, <2 x i32> zeroinitializer
  %35 = zext <2 x i32> %34 to <2 x i64>
  %36 = shufflevector <8 x i16> %31, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %37 = sext <4 x i16> %36 to <4 x i32>
  %38 = add <4 x i32> %33, %37
  %39 = bitcast <2 x i64> %35 to <4 x i32>
  %40 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %38, <4 x i32> %39) #8
  %41 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %40, <4 x i32> undef) #8
  %42 = bitcast <8 x i16> %41 to <2 x i64>
  %43 = extractelement <2 x i64> %42, i32 0
  %44 = bitcast i8* %3 to i64*
  store i64 %43, i64* %44, align 1
  br label %283

45:                                               ; preds = %7
  br i1 %12, label %46, label %57

46:                                               ; preds = %45
  %47 = shl nsw i32 %2, 2
  %48 = sext i32 %47 to i64
  br label %49

49:                                               ; preds = %49, %46
  %50 = phi i64 [ %55, %49 ], [ 0, %46 ]
  %51 = getelementptr inbounds i16, i16* %8, i64 %50
  %52 = bitcast i16* %51 to <8 x i16>*
  %53 = load <8 x i16>, <8 x i16>* %52, align 1
  %54 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %53, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %54, <8 x i16>* %52, align 1
  %55 = add nuw nsw i64 %50, 8
  %56 = icmp slt i64 %55, %48
  br i1 %56, label %49, label %57

57:                                               ; preds = %49, %45
  %58 = icmp slt i32 %2, 5
  br i1 %58, label %63, label %59

59:                                               ; preds = %57
  %60 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %61 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %62 = sext i32 %2 to i64
  br label %141

63:                                               ; preds = %57
  %64 = bitcast i8* %3 to i64*
  %65 = load i64, i64* %64, align 1
  %66 = insertelement <2 x i64> undef, i64 %65, i32 0
  %67 = getelementptr inbounds i8, i8* %3, i64 8
  %68 = bitcast i8* %67 to i64*
  %69 = load i64, i64* %68, align 1
  %70 = insertelement <2 x i64> undef, i64 %69, i32 0
  %71 = getelementptr inbounds i8, i8* %3, i64 16
  %72 = bitcast i8* %71 to i64*
  %73 = load i64, i64* %72, align 1
  %74 = insertelement <2 x i64> undef, i64 %73, i32 0
  %75 = getelementptr inbounds i8, i8* %3, i64 24
  %76 = bitcast i8* %75 to i64*
  %77 = load i64, i64* %76, align 1
  %78 = insertelement <2 x i64> undef, i64 %77, i32 0
  %79 = bitcast <2 x i64> %66 to <8 x i16>
  %80 = bitcast <2 x i64> %70 to <8 x i16>
  %81 = shufflevector <8 x i16> %79, <8 x i16> %80, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %82 = bitcast <2 x i64> %74 to <8 x i16>
  %83 = bitcast <2 x i64> %78 to <8 x i16>
  %84 = shufflevector <8 x i16> %82, <8 x i16> %83, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %85 = bitcast <8 x i16> %81 to <4 x i32>
  %86 = bitcast <8 x i16> %84 to <4 x i32>
  %87 = shufflevector <4 x i32> %85, <4 x i32> %86, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %88 = shufflevector <4 x i32> %85, <4 x i32> %86, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %89 = bitcast <4 x i32> %87 to <16 x i8>
  %90 = shufflevector <16 x i8> %89, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %91 = bitcast <4 x i32> %88 to <16 x i8>
  %92 = shufflevector <16 x i8> %91, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %93 = bitcast <4 x i32> %87 to <8 x i16>
  %94 = bitcast <4 x i32> %88 to <8 x i16>
  %95 = shufflevector <8 x i16> %93, <8 x i16> %94, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %96 = shufflevector <8 x i16> %94, <8 x i16> %93, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %97 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %98 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %95, <8 x i16> %97) #8
  %99 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %96, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %100 = add <4 x i32> %98, <i32 2048, i32 2048, i32 2048, i32 2048>
  %101 = ashr <4 x i32> %100, <i32 12, i32 12, i32 12, i32 12>
  %102 = add <4 x i32> %99, <i32 2048, i32 2048, i32 2048, i32 2048>
  %103 = ashr <4 x i32> %102, <i32 12, i32 12, i32 12, i32 12>
  %104 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %101, <4 x i32> %101) #8
  %105 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %103, <4 x i32> %103) #8
  %106 = bitcast <16 x i8> %90 to <8 x i16>
  %107 = bitcast <16 x i8> %92 to <8 x i16>
  %108 = shufflevector <8 x i16> %106, <8 x i16> %107, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %109 = shufflevector <8 x i16> %107, <8 x i16> %106, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %110 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %111 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %108, <8 x i16> %110) #8
  %112 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %109, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %113 = add <4 x i32> %111, <i32 2048, i32 2048, i32 2048, i32 2048>
  %114 = ashr <4 x i32> %113, <i32 12, i32 12, i32 12, i32 12>
  %115 = add <4 x i32> %112, <i32 2048, i32 2048, i32 2048, i32 2048>
  %116 = ashr <4 x i32> %115, <i32 12, i32 12, i32 12, i32 12>
  %117 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %114, <4 x i32> %114) #8
  %118 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %116, <4 x i32> %116) #8
  %119 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %105, <8 x i16> %118) #8
  %120 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %105, <8 x i16> %118) #8
  %121 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %104, <8 x i16> %117) #8
  %122 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %104, <8 x i16> %117) #8
  %123 = shufflevector <8 x i16> %119, <8 x i16> %121, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %124 = shufflevector <8 x i16> %122, <8 x i16> %120, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %125 = bitcast <8 x i16> %123 to <4 x i32>
  %126 = bitcast <8 x i16> %124 to <4 x i32>
  %127 = shufflevector <4 x i32> %125, <4 x i32> %126, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %128 = shufflevector <4 x i32> %125, <4 x i32> %126, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %129 = bitcast <4 x i32> %127 to <2 x i64>
  %130 = bitcast <4 x i32> %127 to <16 x i8>
  %131 = shufflevector <16 x i8> %130, <16 x i8> undef, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %132 = bitcast <16 x i8> %131 to <2 x i64>
  %133 = bitcast <4 x i32> %128 to <2 x i64>
  %134 = bitcast <4 x i32> %128 to <16 x i8>
  %135 = shufflevector <16 x i8> %134, <16 x i8> undef, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %136 = bitcast <16 x i8> %135 to <2 x i64>
  %137 = extractelement <2 x i64> %129, i32 0
  store i64 %137, i64* %64, align 1
  %138 = extractelement <2 x i64> %132, i32 0
  store i64 %138, i64* %68, align 1
  %139 = extractelement <2 x i64> %133, i32 0
  store i64 %139, i64* %72, align 1
  %140 = extractelement <2 x i64> %136, i32 0
  store i64 %140, i64* %76, align 1
  br label %267

141:                                              ; preds = %59, %141
  %142 = phi i64 [ 0, %59 ], [ %265, %141 ]
  %143 = shl i64 %142, 2
  %144 = and i64 %143, 4294967264
  %145 = getelementptr inbounds i16, i16* %8, i64 %144
  %146 = bitcast i16* %145 to i64*
  %147 = load i64, i64* %146, align 1
  %148 = insertelement <2 x i64> undef, i64 %147, i32 0
  %149 = getelementptr inbounds i16, i16* %145, i64 4
  %150 = bitcast i16* %149 to i64*
  %151 = load i64, i64* %150, align 1
  %152 = insertelement <2 x i64> undef, i64 %151, i32 0
  %153 = getelementptr inbounds i16, i16* %145, i64 8
  %154 = bitcast i16* %153 to i64*
  %155 = load i64, i64* %154, align 1
  %156 = insertelement <2 x i64> undef, i64 %155, i32 0
  %157 = getelementptr inbounds i16, i16* %145, i64 12
  %158 = bitcast i16* %157 to i64*
  %159 = load i64, i64* %158, align 1
  %160 = insertelement <2 x i64> undef, i64 %159, i32 0
  %161 = getelementptr inbounds i16, i16* %145, i64 16
  %162 = bitcast i16* %161 to i64*
  %163 = load i64, i64* %162, align 1
  %164 = insertelement <2 x i64> undef, i64 %163, i32 0
  %165 = getelementptr inbounds i16, i16* %145, i64 20
  %166 = bitcast i16* %165 to i64*
  %167 = load i64, i64* %166, align 1
  %168 = insertelement <2 x i64> undef, i64 %167, i32 0
  %169 = getelementptr inbounds i16, i16* %145, i64 24
  %170 = bitcast i16* %169 to i64*
  %171 = load i64, i64* %170, align 1
  %172 = insertelement <2 x i64> undef, i64 %171, i32 0
  %173 = getelementptr inbounds i16, i16* %145, i64 28
  %174 = bitcast i16* %173 to i64*
  %175 = load i64, i64* %174, align 1
  %176 = insertelement <2 x i64> undef, i64 %175, i32 0
  %177 = bitcast <2 x i64> %148 to <8 x i16>
  %178 = bitcast <2 x i64> %152 to <8 x i16>
  %179 = shufflevector <8 x i16> %177, <8 x i16> %178, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %180 = bitcast <2 x i64> %156 to <8 x i16>
  %181 = bitcast <2 x i64> %160 to <8 x i16>
  %182 = shufflevector <8 x i16> %180, <8 x i16> %181, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %183 = bitcast <2 x i64> %164 to <8 x i16>
  %184 = bitcast <2 x i64> %168 to <8 x i16>
  %185 = shufflevector <8 x i16> %183, <8 x i16> %184, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %186 = bitcast <2 x i64> %172 to <8 x i16>
  %187 = bitcast <2 x i64> %176 to <8 x i16>
  %188 = shufflevector <8 x i16> %186, <8 x i16> %187, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %189 = bitcast <8 x i16> %179 to <4 x i32>
  %190 = bitcast <8 x i16> %182 to <4 x i32>
  %191 = shufflevector <4 x i32> %189, <4 x i32> %190, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %192 = bitcast <4 x i32> %191 to <2 x i64>
  %193 = bitcast <8 x i16> %185 to <4 x i32>
  %194 = bitcast <8 x i16> %188 to <4 x i32>
  %195 = shufflevector <4 x i32> %193, <4 x i32> %194, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %196 = bitcast <4 x i32> %195 to <2 x i64>
  %197 = shufflevector <4 x i32> %189, <4 x i32> %190, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %198 = bitcast <4 x i32> %197 to <2 x i64>
  %199 = shufflevector <4 x i32> %193, <4 x i32> %194, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %200 = bitcast <4 x i32> %199 to <2 x i64>
  %201 = shufflevector <2 x i64> %192, <2 x i64> %196, <2 x i32> <i32 0, i32 2>
  %202 = shufflevector <2 x i64> %192, <2 x i64> %196, <2 x i32> <i32 1, i32 3>
  %203 = shufflevector <2 x i64> %198, <2 x i64> %200, <2 x i32> <i32 0, i32 2>
  %204 = shufflevector <2 x i64> %198, <2 x i64> %200, <2 x i32> <i32 1, i32 3>
  %205 = bitcast <2 x i64> %201 to <8 x i16>
  %206 = bitcast <2 x i64> %203 to <8 x i16>
  %207 = shufflevector <8 x i16> %205, <8 x i16> %206, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %208 = shufflevector <8 x i16> %206, <8 x i16> %205, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %209 = shufflevector <8 x i16> %205, <8 x i16> %206, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %210 = shufflevector <8 x i16> %206, <8 x i16> %205, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %211 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %207, <8 x i16> %60) #8
  %212 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %208, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %213 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %209, <8 x i16> %60) #8
  %214 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %210, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %215 = add <4 x i32> %211, <i32 2048, i32 2048, i32 2048, i32 2048>
  %216 = ashr <4 x i32> %215, <i32 12, i32 12, i32 12, i32 12>
  %217 = add <4 x i32> %212, <i32 2048, i32 2048, i32 2048, i32 2048>
  %218 = ashr <4 x i32> %217, <i32 12, i32 12, i32 12, i32 12>
  %219 = add <4 x i32> %213, <i32 2048, i32 2048, i32 2048, i32 2048>
  %220 = ashr <4 x i32> %219, <i32 12, i32 12, i32 12, i32 12>
  %221 = add <4 x i32> %214, <i32 2048, i32 2048, i32 2048, i32 2048>
  %222 = ashr <4 x i32> %221, <i32 12, i32 12, i32 12, i32 12>
  %223 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %216, <4 x i32> %220) #8
  %224 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %218, <4 x i32> %222) #8
  %225 = bitcast <2 x i64> %202 to <8 x i16>
  %226 = bitcast <2 x i64> %204 to <8 x i16>
  %227 = shufflevector <8 x i16> %225, <8 x i16> %226, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %228 = shufflevector <8 x i16> %226, <8 x i16> %225, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %229 = shufflevector <8 x i16> %225, <8 x i16> %226, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %230 = shufflevector <8 x i16> %226, <8 x i16> %225, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %231 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %227, <8 x i16> %61) #8
  %232 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %228, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %233 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %229, <8 x i16> %61) #8
  %234 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %230, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %235 = add <4 x i32> %231, <i32 2048, i32 2048, i32 2048, i32 2048>
  %236 = ashr <4 x i32> %235, <i32 12, i32 12, i32 12, i32 12>
  %237 = add <4 x i32> %232, <i32 2048, i32 2048, i32 2048, i32 2048>
  %238 = ashr <4 x i32> %237, <i32 12, i32 12, i32 12, i32 12>
  %239 = add <4 x i32> %233, <i32 2048, i32 2048, i32 2048, i32 2048>
  %240 = ashr <4 x i32> %239, <i32 12, i32 12, i32 12, i32 12>
  %241 = add <4 x i32> %234, <i32 2048, i32 2048, i32 2048, i32 2048>
  %242 = ashr <4 x i32> %241, <i32 12, i32 12, i32 12, i32 12>
  %243 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %236, <4 x i32> %240) #8
  %244 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %238, <4 x i32> %242) #8
  %245 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %224, <8 x i16> %244) #8
  %246 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %224, <8 x i16> %244) #8
  %247 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %223, <8 x i16> %243) #8
  %248 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %223, <8 x i16> %243) #8
  %249 = shufflevector <8 x i16> %245, <8 x i16> %247, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %250 = shufflevector <8 x i16> %248, <8 x i16> %246, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %251 = shufflevector <8 x i16> %245, <8 x i16> %247, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %252 = shufflevector <8 x i16> %248, <8 x i16> %246, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %253 = bitcast <8 x i16> %249 to <4 x i32>
  %254 = bitcast <8 x i16> %250 to <4 x i32>
  %255 = shufflevector <4 x i32> %253, <4 x i32> %254, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %256 = bitcast <8 x i16> %251 to <4 x i32>
  %257 = bitcast <8 x i16> %252 to <4 x i32>
  %258 = shufflevector <4 x i32> %256, <4 x i32> %257, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %259 = shufflevector <4 x i32> %253, <4 x i32> %254, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %260 = shufflevector <4 x i32> %256, <4 x i32> %257, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %261 = bitcast i16* %145 to <4 x i32>*
  store <4 x i32> %255, <4 x i32>* %261, align 1
  %262 = bitcast i16* %153 to <4 x i32>*
  store <4 x i32> %259, <4 x i32>* %262, align 1
  %263 = bitcast i16* %161 to <4 x i32>*
  store <4 x i32> %258, <4 x i32>* %263, align 1
  %264 = bitcast i16* %169 to <4 x i32>*
  store <4 x i32> %260, <4 x i32>* %264, align 1
  %265 = add nuw nsw i64 %142, 8
  %266 = icmp slt i64 %265, %62
  br i1 %266, label %141, label %267

267:                                              ; preds = %141, %63
  br i1 %13, label %268, label %283

268:                                              ; preds = %267
  %269 = shl nsw i32 %2, 2
  %270 = sext i32 %269 to i64
  br label %271

271:                                              ; preds = %271, %268
  %272 = phi i64 [ %281, %271 ], [ 0, %268 ]
  %273 = getelementptr inbounds i16, i16* %8, i64 %272
  %274 = bitcast i16* %273 to <8 x i16>*
  %275 = load <8 x i16>, <8 x i16>* %274, align 1
  %276 = icmp sgt <8 x i16> %275, <i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765>
  %277 = add <8 x i16> %275, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %278 = ashr <8 x i16> %277, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %279 = lshr <8 x i16> %277, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %280 = select <8 x i1> %276, <8 x i16> %279, <8 x i16> %278
  store <8 x i16> %280, <8 x i16>* %274, align 1
  %281 = add nuw nsw i64 %272, 8
  %282 = icmp slt i64 %281, %270
  br i1 %282, label %271, label %283

283:                                              ; preds = %271, %15, %267
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_130Dct4TransformLoopColumn_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_(i8 zeroext, i8 zeroext, i32, i8* nocapture, i32, i32, i8* nocapture readonly) #4 {
  %8 = bitcast i8* %3 to i16*
  %9 = zext i8 %1 to i64
  %10 = getelementptr inbounds [19 x i8], [19 x i8]* @_ZN7libgav115kTransformWidthE, i64 0, i64 %9
  %11 = load i8, i8* %10, align 1
  %12 = zext i8 %11 to i32
  %13 = zext i8 %0 to i32
  %14 = shl i32 1, %13
  %15 = and i32 %14, 33104
  %16 = icmp eq i32 %15, 0
  br i1 %16, label %59, label %17

17:                                               ; preds = %7
  %18 = icmp ugt i8 %11, 15
  br i1 %18, label %19, label %35

19:                                               ; preds = %17
  %20 = shl nuw nsw i32 %12, 2
  %21 = zext i32 %20 to i64
  br label %22

22:                                               ; preds = %22, %19
  %23 = phi i64 [ 0, %19 ], [ %33, %22 ]
  %24 = getelementptr inbounds i16, i16* %8, i64 %23
  %25 = bitcast i16* %24 to <16 x i8>*
  %26 = load <16 x i8>, <16 x i8>* %25, align 1
  %27 = or i64 %23, 8
  %28 = getelementptr inbounds i16, i16* %8, i64 %27
  %29 = bitcast i16* %28 to <16 x i8>*
  %30 = load <16 x i8>, <16 x i8>* %29, align 1
  %31 = shufflevector <16 x i8> %26, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  %32 = shufflevector <16 x i8> %30, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %32, <16 x i8>* %25, align 1
  store <16 x i8> %31, <16 x i8>* %29, align 1
  %33 = add nuw nsw i64 %23, 16
  %34 = icmp ult i64 %33, %21
  br i1 %34, label %22, label %59

35:                                               ; preds = %17
  %36 = icmp eq i8 %11, 8
  %37 = bitcast i8* %3 to <16 x i8>*
  %38 = load <16 x i8>, <16 x i8>* %37, align 1
  br i1 %36, label %45, label %39

39:                                               ; preds = %35
  %40 = shufflevector <16 x i8> %38, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %40, <16 x i8>* %37, align 1
  %41 = getelementptr inbounds i8, i8* %3, i64 16
  %42 = bitcast i8* %41 to <16 x i8>*
  %43 = load <16 x i8>, <16 x i8>* %42, align 1
  %44 = shufflevector <16 x i8> %43, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %44, <16 x i8>* %42, align 1
  br label %59

45:                                               ; preds = %35
  %46 = shufflevector <16 x i8> %38, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %46, <16 x i8>* %37, align 1
  %47 = getelementptr inbounds i8, i8* %3, i64 16
  %48 = bitcast i8* %47 to <16 x i8>*
  %49 = load <16 x i8>, <16 x i8>* %48, align 1
  %50 = shufflevector <16 x i8> %49, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %50, <16 x i8>* %48, align 1
  %51 = getelementptr inbounds i8, i8* %3, i64 32
  %52 = bitcast i8* %51 to <16 x i8>*
  %53 = load <16 x i8>, <16 x i8>* %52, align 1
  %54 = shufflevector <16 x i8> %53, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %54, <16 x i8>* %52, align 1
  %55 = getelementptr inbounds i8, i8* %3, i64 48
  %56 = bitcast i8* %55 to <16 x i8>*
  %57 = load <16 x i8>, <16 x i8>* %56, align 1
  %58 = shufflevector <16 x i8> %57, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %58, <16 x i8>* %56, align 1
  br label %59

59:                                               ; preds = %22, %7, %45, %39
  %60 = icmp sgt i32 %2, 1
  %61 = icmp eq i8 %11, 4
  br i1 %60, label %91, label %62

62:                                               ; preds = %59
  br i1 %61, label %65, label %63

63:                                               ; preds = %62
  %64 = zext i8 %11 to i64
  br label %73

65:                                               ; preds = %62
  %66 = bitcast i8* %3 to i64*
  %67 = load i64, i64* %66, align 1
  %68 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %67, i32 0
  %69 = bitcast <2 x i64> %68 to <8 x i16>
  %70 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %69, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  %71 = bitcast <8 x i16> %70 to <2 x i64>
  %72 = extractelement <2 x i64> %71, i32 0
  store i64 %72, i64* %66, align 1
  br label %81

73:                                               ; preds = %73, %63
  %74 = phi i64 [ 0, %63 ], [ %79, %73 ]
  %75 = getelementptr inbounds i16, i16* %8, i64 %74
  %76 = bitcast i16* %75 to <8 x i16>*
  %77 = load <8 x i16>, <8 x i16>* %76, align 1
  %78 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %77, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %78, <8 x i16>* %76, align 1
  %79 = add nuw nsw i64 %74, 8
  %80 = icmp ult i64 %79, %64
  br i1 %80, label %73, label %81

81:                                               ; preds = %73, %65
  %82 = phi i64 [ 4, %65 ], [ %64, %73 ]
  %83 = shl nuw nsw i64 %82, 1
  %84 = getelementptr inbounds i16, i16* %8, i64 %82
  %85 = bitcast i16* %84 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 2 %85, i8* align 2 %3, i64 %83, i1 false) #8
  %86 = getelementptr inbounds i16, i16* %8, i64 %83
  %87 = bitcast i16* %86 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 2 %87, i8* align 2 %3, i64 %83, i1 false) #8
  %88 = mul nuw nsw i64 %82, 3
  %89 = getelementptr inbounds i16, i16* %8, i64 %88
  %90 = bitcast i16* %89 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 2 %90, i8* align 2 %3, i64 %83, i1 false) #8
  br label %222

91:                                               ; preds = %59
  br i1 %61, label %101, label %92

92:                                               ; preds = %91
  %93 = zext i8 %11 to i64
  %94 = shl nuw nsw i32 %12, 1
  %95 = zext i32 %94 to i64
  %96 = mul nuw nsw i32 %12, 3
  %97 = zext i32 %96 to i64
  %98 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %99 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %100 = zext i8 %11 to i64
  br label %166

101:                                              ; preds = %91
  %102 = bitcast i8* %3 to i64*
  %103 = load i64, i64* %102, align 1
  %104 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %103, i32 0
  %105 = getelementptr inbounds i8, i8* %3, i64 8
  %106 = bitcast i8* %105 to i64*
  %107 = load i64, i64* %106, align 1
  %108 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %107, i32 0
  %109 = getelementptr inbounds i8, i8* %3, i64 16
  %110 = bitcast i8* %109 to i64*
  %111 = load i64, i64* %110, align 1
  %112 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %111, i32 0
  %113 = getelementptr inbounds i8, i8* %3, i64 24
  %114 = bitcast i8* %113 to i64*
  %115 = load i64, i64* %114, align 1
  %116 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %115, i32 0
  %117 = bitcast <2 x i64> %104 to <8 x i16>
  %118 = bitcast <2 x i64> %112 to <8 x i16>
  %119 = shufflevector <8 x i16> %117, <8 x i16> %118, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %120 = shufflevector <8 x i16> %118, <8 x i16> %117, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %121 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %122 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %119, <8 x i16> %121) #8
  %123 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %120, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %124 = add <4 x i32> %122, <i32 2048, i32 2048, i32 2048, i32 2048>
  %125 = ashr <4 x i32> %124, <i32 12, i32 12, i32 12, i32 12>
  %126 = add <4 x i32> %123, <i32 2048, i32 2048, i32 2048, i32 2048>
  %127 = ashr <4 x i32> %126, <i32 12, i32 12, i32 12, i32 12>
  %128 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %125, <4 x i32> %125) #8
  %129 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %127, <4 x i32> %127) #8
  %130 = bitcast <2 x i64> %108 to <8 x i16>
  %131 = bitcast <2 x i64> %116 to <8 x i16>
  %132 = shufflevector <8 x i16> %130, <8 x i16> %131, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %133 = shufflevector <8 x i16> %131, <8 x i16> %130, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %134 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %135 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %132, <8 x i16> %134) #8
  %136 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %133, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %137 = add <4 x i32> %135, <i32 2048, i32 2048, i32 2048, i32 2048>
  %138 = ashr <4 x i32> %137, <i32 12, i32 12, i32 12, i32 12>
  %139 = add <4 x i32> %136, <i32 2048, i32 2048, i32 2048, i32 2048>
  %140 = ashr <4 x i32> %139, <i32 12, i32 12, i32 12, i32 12>
  %141 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %138, <4 x i32> %138) #8
  %142 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %140, <4 x i32> %140) #8
  %143 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %129, <8 x i16> %142) #8
  %144 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %129, <8 x i16> %142) #8
  %145 = bitcast <8 x i16> %143 to <2 x i64>
  %146 = bitcast <8 x i16> %144 to <2 x i64>
  %147 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %128, <8 x i16> %141) #8
  %148 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %128, <8 x i16> %141) #8
  %149 = bitcast <8 x i16> %147 to <2 x i64>
  %150 = bitcast <8 x i16> %148 to <2 x i64>
  %151 = extractelement <2 x i64> %145, i32 0
  store i64 %151, i64* %102, align 1
  %152 = extractelement <2 x i64> %149, i32 0
  store i64 %152, i64* %106, align 1
  %153 = extractelement <2 x i64> %150, i32 0
  store i64 %153, i64* %110, align 1
  %154 = extractelement <2 x i64> %146, i32 0
  store i64 %154, i64* %114, align 1
  %155 = bitcast i8* %6 to i64*
  %156 = load i64, i64* %155, align 8
  %157 = getelementptr inbounds i8, i8* %6, i64 8
  %158 = bitcast i8* %157 to i8**
  %159 = load i8*, i8** %158, align 8
  %160 = sext i32 %5 to i64
  %161 = ashr i64 %156, 32
  %162 = mul nsw i64 %161, %160
  %163 = getelementptr inbounds i8, i8* %159, i64 %162
  %164 = sext i32 %4 to i64
  %165 = getelementptr inbounds i8, i8* %163, i64 %164
  br label %300

166:                                              ; preds = %92, %166
  %167 = phi i64 [ 0, %92 ], [ %220, %166 ]
  %168 = getelementptr inbounds i16, i16* %8, i64 %167
  %169 = bitcast i16* %168 to <8 x i16>*
  %170 = load <8 x i16>, <8 x i16>* %169, align 1
  %171 = getelementptr inbounds i16, i16* %168, i64 %93
  %172 = bitcast i16* %171 to <8 x i16>*
  %173 = load <8 x i16>, <8 x i16>* %172, align 1
  %174 = getelementptr inbounds i16, i16* %168, i64 %95
  %175 = bitcast i16* %174 to <8 x i16>*
  %176 = load <8 x i16>, <8 x i16>* %175, align 1
  %177 = getelementptr inbounds i16, i16* %168, i64 %97
  %178 = bitcast i16* %177 to <8 x i16>*
  %179 = load <8 x i16>, <8 x i16>* %178, align 1
  %180 = shufflevector <8 x i16> %170, <8 x i16> %176, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %181 = shufflevector <8 x i16> %176, <8 x i16> %170, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %182 = shufflevector <8 x i16> %170, <8 x i16> %176, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %183 = shufflevector <8 x i16> %176, <8 x i16> %170, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %184 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %180, <8 x i16> %98) #8
  %185 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %181, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %186 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %182, <8 x i16> %98) #8
  %187 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %183, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %188 = add <4 x i32> %184, <i32 2048, i32 2048, i32 2048, i32 2048>
  %189 = ashr <4 x i32> %188, <i32 12, i32 12, i32 12, i32 12>
  %190 = add <4 x i32> %185, <i32 2048, i32 2048, i32 2048, i32 2048>
  %191 = ashr <4 x i32> %190, <i32 12, i32 12, i32 12, i32 12>
  %192 = add <4 x i32> %186, <i32 2048, i32 2048, i32 2048, i32 2048>
  %193 = ashr <4 x i32> %192, <i32 12, i32 12, i32 12, i32 12>
  %194 = add <4 x i32> %187, <i32 2048, i32 2048, i32 2048, i32 2048>
  %195 = ashr <4 x i32> %194, <i32 12, i32 12, i32 12, i32 12>
  %196 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %189, <4 x i32> %193) #8
  %197 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %191, <4 x i32> %195) #8
  %198 = shufflevector <8 x i16> %173, <8 x i16> %179, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %199 = shufflevector <8 x i16> %179, <8 x i16> %173, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %200 = shufflevector <8 x i16> %173, <8 x i16> %179, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %201 = shufflevector <8 x i16> %179, <8 x i16> %173, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %202 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %198, <8 x i16> %99) #8
  %203 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %199, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %204 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %200, <8 x i16> %99) #8
  %205 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %201, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %206 = add <4 x i32> %202, <i32 2048, i32 2048, i32 2048, i32 2048>
  %207 = ashr <4 x i32> %206, <i32 12, i32 12, i32 12, i32 12>
  %208 = add <4 x i32> %203, <i32 2048, i32 2048, i32 2048, i32 2048>
  %209 = ashr <4 x i32> %208, <i32 12, i32 12, i32 12, i32 12>
  %210 = add <4 x i32> %204, <i32 2048, i32 2048, i32 2048, i32 2048>
  %211 = ashr <4 x i32> %210, <i32 12, i32 12, i32 12, i32 12>
  %212 = add <4 x i32> %205, <i32 2048, i32 2048, i32 2048, i32 2048>
  %213 = ashr <4 x i32> %212, <i32 12, i32 12, i32 12, i32 12>
  %214 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %207, <4 x i32> %211) #8
  %215 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %209, <4 x i32> %213) #8
  %216 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %197, <8 x i16> %215) #8
  %217 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %197, <8 x i16> %215) #8
  %218 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %196, <8 x i16> %214) #8
  %219 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %196, <8 x i16> %214) #8
  store <8 x i16> %216, <8 x i16>* %169, align 1
  store <8 x i16> %218, <8 x i16>* %172, align 1
  store <8 x i16> %219, <8 x i16>* %175, align 1
  store <8 x i16> %217, <8 x i16>* %178, align 1
  %220 = add nuw nsw i64 %167, 8
  %221 = icmp ult i64 %220, %100
  br i1 %221, label %166, label %222

222:                                              ; preds = %166, %81
  %223 = bitcast i8* %6 to i64*
  %224 = load i64, i64* %223, align 8
  %225 = getelementptr inbounds i8, i8* %6, i64 8
  %226 = bitcast i8* %225 to i8**
  %227 = load i8*, i8** %226, align 8
  %228 = sext i32 %5 to i64
  %229 = ashr i64 %224, 32
  %230 = mul nsw i64 %229, %228
  %231 = getelementptr inbounds i8, i8* %227, i64 %230
  %232 = sext i32 %4 to i64
  %233 = getelementptr inbounds i8, i8* %231, i64 %232
  switch i8 %11, label %372 [
    i8 4, label %234
    i8 8, label %237
  ]

234:                                              ; preds = %222
  %235 = bitcast i8* %3 to i64*
  %236 = load i64, i64* %235, align 1
  br label %300

237:                                              ; preds = %222
  %238 = bitcast i8* %3 to <8 x i16>*
  %239 = load <8 x i16>, <8 x i16>* %238, align 1
  %240 = bitcast i8* %233 to i64*
  %241 = load i64, i64* %240, align 1
  %242 = insertelement <2 x i64> undef, i64 %241, i32 0
  %243 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %239, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %244 = ashr <8 x i16> %243, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %245 = bitcast <2 x i64> %242 to <16 x i8>
  %246 = shufflevector <16 x i8> %245, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %247 = zext <8 x i8> %246 to <8 x i16>
  %248 = add nsw <8 x i16> %244, %247
  %249 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %248, <8 x i16> undef) #8
  %250 = bitcast <16 x i8> %249 to <2 x i64>
  %251 = extractelement <2 x i64> %250, i32 0
  store i64 %251, i64* %240, align 1
  %252 = getelementptr inbounds i8, i8* %233, i64 %229
  %253 = getelementptr inbounds i8, i8* %3, i64 16
  %254 = bitcast i8* %253 to <8 x i16>*
  %255 = load <8 x i16>, <8 x i16>* %254, align 1
  %256 = bitcast i8* %252 to i64*
  %257 = load i64, i64* %256, align 1
  %258 = insertelement <2 x i64> undef, i64 %257, i32 0
  %259 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %255, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %260 = ashr <8 x i16> %259, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %261 = bitcast <2 x i64> %258 to <16 x i8>
  %262 = shufflevector <16 x i8> %261, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %263 = zext <8 x i8> %262 to <8 x i16>
  %264 = add nsw <8 x i16> %260, %263
  %265 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %264, <8 x i16> undef) #8
  %266 = bitcast <16 x i8> %265 to <2 x i64>
  %267 = extractelement <2 x i64> %266, i32 0
  store i64 %267, i64* %256, align 1
  %268 = getelementptr inbounds i8, i8* %252, i64 %229
  %269 = getelementptr inbounds i8, i8* %3, i64 32
  %270 = bitcast i8* %269 to <8 x i16>*
  %271 = load <8 x i16>, <8 x i16>* %270, align 1
  %272 = bitcast i8* %268 to i64*
  %273 = load i64, i64* %272, align 1
  %274 = insertelement <2 x i64> undef, i64 %273, i32 0
  %275 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %271, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %276 = ashr <8 x i16> %275, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %277 = bitcast <2 x i64> %274 to <16 x i8>
  %278 = shufflevector <16 x i8> %277, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %279 = zext <8 x i8> %278 to <8 x i16>
  %280 = add nsw <8 x i16> %276, %279
  %281 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %280, <8 x i16> undef) #8
  %282 = bitcast <16 x i8> %281 to <2 x i64>
  %283 = extractelement <2 x i64> %282, i32 0
  store i64 %283, i64* %272, align 1
  %284 = getelementptr inbounds i8, i8* %268, i64 %229
  %285 = getelementptr inbounds i8, i8* %3, i64 48
  %286 = bitcast i8* %285 to <8 x i16>*
  %287 = load <8 x i16>, <8 x i16>* %286, align 1
  %288 = bitcast i8* %284 to i64*
  %289 = load i64, i64* %288, align 1
  %290 = insertelement <2 x i64> undef, i64 %289, i32 0
  %291 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %287, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %292 = ashr <8 x i16> %291, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %293 = bitcast <2 x i64> %290 to <16 x i8>
  %294 = shufflevector <16 x i8> %293, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %295 = zext <8 x i8> %294 to <8 x i16>
  %296 = add nsw <8 x i16> %292, %295
  %297 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %296, <8 x i16> undef) #8
  %298 = bitcast <16 x i8> %297 to <2 x i64>
  %299 = extractelement <2 x i64> %298, i32 0
  store i64 %299, i64* %288, align 1
  br label %404

300:                                              ; preds = %234, %101
  %301 = phi i64 [ %151, %101 ], [ %236, %234 ]
  %302 = phi i8* [ %165, %101 ], [ %233, %234 ]
  %303 = phi i64 [ %161, %101 ], [ %229, %234 ]
  %304 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %301, i32 0
  %305 = bitcast i8* %302 to i32*
  %306 = load i32, i32* %305, align 1
  %307 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %306, i32 0
  %308 = bitcast <2 x i64> %304 to <8 x i16>
  %309 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %308, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %310 = ashr <8 x i16> %309, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %311 = bitcast <4 x i32> %307 to <16 x i8>
  %312 = shufflevector <16 x i8> %311, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %313 = zext <8 x i8> %312 to <8 x i16>
  %314 = add nsw <8 x i16> %310, %313
  %315 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %314, <8 x i16> undef) #8
  %316 = bitcast <16 x i8> %315 to <4 x i32>
  %317 = extractelement <4 x i32> %316, i32 0
  store i32 %317, i32* %305, align 1
  %318 = getelementptr inbounds i8, i8* %302, i64 %303
  %319 = getelementptr inbounds i8, i8* %3, i64 8
  %320 = bitcast i8* %319 to i64*
  %321 = load i64, i64* %320, align 1
  %322 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %321, i32 0
  %323 = bitcast i8* %318 to i32*
  %324 = load i32, i32* %323, align 1
  %325 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %324, i32 0
  %326 = bitcast <2 x i64> %322 to <8 x i16>
  %327 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %326, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %328 = ashr <8 x i16> %327, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %329 = bitcast <4 x i32> %325 to <16 x i8>
  %330 = shufflevector <16 x i8> %329, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %331 = zext <8 x i8> %330 to <8 x i16>
  %332 = add nsw <8 x i16> %328, %331
  %333 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %332, <8 x i16> undef) #8
  %334 = bitcast <16 x i8> %333 to <4 x i32>
  %335 = extractelement <4 x i32> %334, i32 0
  store i32 %335, i32* %323, align 1
  %336 = getelementptr inbounds i8, i8* %318, i64 %303
  %337 = getelementptr inbounds i8, i8* %3, i64 16
  %338 = bitcast i8* %337 to i64*
  %339 = load i64, i64* %338, align 1
  %340 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %339, i32 0
  %341 = bitcast i8* %336 to i32*
  %342 = load i32, i32* %341, align 1
  %343 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %342, i32 0
  %344 = bitcast <2 x i64> %340 to <8 x i16>
  %345 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %344, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %346 = ashr <8 x i16> %345, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %347 = bitcast <4 x i32> %343 to <16 x i8>
  %348 = shufflevector <16 x i8> %347, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %349 = zext <8 x i8> %348 to <8 x i16>
  %350 = add nsw <8 x i16> %346, %349
  %351 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %350, <8 x i16> undef) #8
  %352 = bitcast <16 x i8> %351 to <4 x i32>
  %353 = extractelement <4 x i32> %352, i32 0
  store i32 %353, i32* %341, align 1
  %354 = getelementptr inbounds i8, i8* %336, i64 %303
  %355 = getelementptr inbounds i8, i8* %3, i64 24
  %356 = bitcast i8* %355 to i64*
  %357 = load i64, i64* %356, align 1
  %358 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %357, i32 0
  %359 = bitcast i8* %354 to i32*
  %360 = load i32, i32* %359, align 1
  %361 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %360, i32 0
  %362 = bitcast <2 x i64> %358 to <8 x i16>
  %363 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %362, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %364 = ashr <8 x i16> %363, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %365 = bitcast <4 x i32> %361 to <16 x i8>
  %366 = shufflevector <16 x i8> %365, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %367 = zext <8 x i8> %366 to <8 x i16>
  %368 = add nsw <8 x i16> %364, %367
  %369 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %368, <8 x i16> undef) #8
  %370 = bitcast <16 x i8> %369 to <4 x i32>
  %371 = extractelement <4 x i32> %370, i32 0
  store i32 %371, i32* %359, align 1
  br label %404

372:                                              ; preds = %222
  %373 = zext i8 %11 to i64
  br label %374

374:                                              ; preds = %374, %372
  %375 = phi i64 [ %398, %374 ], [ 0, %372 ]
  %376 = add nsw i64 %375, %232
  %377 = getelementptr inbounds i16, i16* %8, i64 %375
  %378 = bitcast i16* %377 to <8 x i16>*
  %379 = load <8 x i16>, <8 x i16>* %378, align 1
  %380 = or i64 %375, 8
  %381 = getelementptr inbounds i16, i16* %8, i64 %380
  %382 = bitcast i16* %381 to <8 x i16>*
  %383 = load <8 x i16>, <8 x i16>* %382, align 1
  %384 = getelementptr inbounds i8, i8* %231, i64 %376
  %385 = bitcast i8* %384 to <16 x i8>*
  %386 = load <16 x i8>, <16 x i8>* %385, align 1
  %387 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %379, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %388 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %383, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %389 = ashr <8 x i16> %387, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %390 = ashr <8 x i16> %388, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %391 = shufflevector <16 x i8> %386, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %392 = zext <8 x i8> %391 to <8 x i16>
  %393 = shufflevector <16 x i8> %386, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %394 = zext <8 x i8> %393 to <8 x i16>
  %395 = add nsw <8 x i16> %389, %392
  %396 = add nsw <8 x i16> %390, %394
  %397 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %395, <8 x i16> %396) #8
  store <16 x i8> %397, <16 x i8>* %385, align 1
  %398 = add nuw nsw i64 %375, 16
  %399 = icmp ult i64 %398, %373
  br i1 %399, label %374, label %400

400:                                              ; preds = %374
  %401 = add nsw i64 %228, 1
  %402 = mul nsw i64 %401, %229
  %403 = getelementptr inbounds i8, i8* %227, i64 %402
  br label %405

404:                                              ; preds = %469, %237, %300
  ret void

405:                                              ; preds = %405, %400
  %406 = phi i64 [ %430, %405 ], [ 0, %400 ]
  %407 = add nsw i64 %406, %232
  %408 = add nuw nsw i64 %406, %373
  %409 = getelementptr inbounds i16, i16* %8, i64 %408
  %410 = bitcast i16* %409 to <8 x i16>*
  %411 = load <8 x i16>, <8 x i16>* %410, align 1
  %412 = add nuw nsw i64 %408, 8
  %413 = getelementptr inbounds i16, i16* %8, i64 %412
  %414 = bitcast i16* %413 to <8 x i16>*
  %415 = load <8 x i16>, <8 x i16>* %414, align 1
  %416 = getelementptr inbounds i8, i8* %403, i64 %407
  %417 = bitcast i8* %416 to <16 x i8>*
  %418 = load <16 x i8>, <16 x i8>* %417, align 1
  %419 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %411, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %420 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %415, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %421 = ashr <8 x i16> %419, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %422 = ashr <8 x i16> %420, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %423 = shufflevector <16 x i8> %418, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %424 = zext <8 x i8> %423 to <8 x i16>
  %425 = shufflevector <16 x i8> %418, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %426 = zext <8 x i8> %425 to <8 x i16>
  %427 = add nsw <8 x i16> %421, %424
  %428 = add nsw <8 x i16> %422, %426
  %429 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %427, <8 x i16> %428) #8
  store <16 x i8> %429, <16 x i8>* %417, align 1
  %430 = add nuw nsw i64 %406, 16
  %431 = icmp ult i64 %430, %373
  br i1 %431, label %405, label %432

432:                                              ; preds = %405
  %433 = add nsw i64 %228, 2
  %434 = shl nuw nsw i64 %373, 1
  %435 = mul nsw i64 %433, %229
  %436 = getelementptr inbounds i8, i8* %227, i64 %435
  br label %437

437:                                              ; preds = %437, %432
  %438 = phi i64 [ %462, %437 ], [ 0, %432 ]
  %439 = add nsw i64 %438, %232
  %440 = add nuw nsw i64 %438, %434
  %441 = getelementptr inbounds i16, i16* %8, i64 %440
  %442 = bitcast i16* %441 to <8 x i16>*
  %443 = load <8 x i16>, <8 x i16>* %442, align 1
  %444 = add nuw nsw i64 %440, 8
  %445 = getelementptr inbounds i16, i16* %8, i64 %444
  %446 = bitcast i16* %445 to <8 x i16>*
  %447 = load <8 x i16>, <8 x i16>* %446, align 1
  %448 = getelementptr inbounds i8, i8* %436, i64 %439
  %449 = bitcast i8* %448 to <16 x i8>*
  %450 = load <16 x i8>, <16 x i8>* %449, align 1
  %451 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %443, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %452 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %447, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %453 = ashr <8 x i16> %451, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %454 = ashr <8 x i16> %452, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %455 = shufflevector <16 x i8> %450, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %456 = zext <8 x i8> %455 to <8 x i16>
  %457 = shufflevector <16 x i8> %450, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %458 = zext <8 x i8> %457 to <8 x i16>
  %459 = add nsw <8 x i16> %453, %456
  %460 = add nsw <8 x i16> %454, %458
  %461 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %459, <8 x i16> %460) #8
  store <16 x i8> %461, <16 x i8>* %449, align 1
  %462 = add nuw nsw i64 %438, 16
  %463 = icmp ult i64 %462, %373
  br i1 %463, label %437, label %464

464:                                              ; preds = %437
  %465 = add nsw i64 %228, 3
  %466 = mul nuw nsw i64 %373, 3
  %467 = mul nsw i64 %465, %229
  %468 = getelementptr inbounds i8, i8* %227, i64 %467
  br label %469

469:                                              ; preds = %469, %464
  %470 = phi i64 [ %494, %469 ], [ 0, %464 ]
  %471 = add nsw i64 %470, %232
  %472 = add nuw nsw i64 %470, %466
  %473 = getelementptr inbounds i16, i16* %8, i64 %472
  %474 = bitcast i16* %473 to <8 x i16>*
  %475 = load <8 x i16>, <8 x i16>* %474, align 1
  %476 = add nuw nsw i64 %472, 8
  %477 = getelementptr inbounds i16, i16* %8, i64 %476
  %478 = bitcast i16* %477 to <8 x i16>*
  %479 = load <8 x i16>, <8 x i16>* %478, align 1
  %480 = getelementptr inbounds i8, i8* %468, i64 %471
  %481 = bitcast i8* %480 to <16 x i8>*
  %482 = load <16 x i8>, <16 x i8>* %481, align 1
  %483 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %475, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %484 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %479, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %485 = ashr <8 x i16> %483, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %486 = ashr <8 x i16> %484, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %487 = shufflevector <16 x i8> %482, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %488 = zext <8 x i8> %487 to <8 x i16>
  %489 = shufflevector <16 x i8> %482, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %490 = zext <8 x i8> %489 to <8 x i16>
  %491 = add nsw <8 x i16> %485, %488
  %492 = add nsw <8 x i16> %486, %490
  %493 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %491, <8 x i16> %492) #8
  store <16 x i8> %493, <16 x i8>* %481, align 1
  %494 = add nuw nsw i64 %470, 16
  %495 = icmp ult i64 %494, %373
  br i1 %495, label %469, label %404
}

; Function Attrs: nofree nounwind ssp uwtable
define internal void @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_127Dct8TransformLoopRow_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_(i8 zeroext, i8 zeroext, i32, i8* nocapture, i32, i32, i8* nocapture readnone) #3 {
  %8 = bitcast i8* %3 to i16*
  %9 = zext i8 %1 to i64
  %10 = lshr i64 173354, %9
  %11 = and i64 %10, 1
  %12 = icmp ne i64 %11, 0
  %13 = getelementptr inbounds [19 x i8], [19 x i8]* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_118kTransformRowShiftE, i64 0, i64 %9
  %14 = load i8, i8* %13, align 1
  %15 = icmp sgt i32 %2, 1
  br i1 %15, label %54, label %16

16:                                               ; preds = %7
  %17 = zext i8 %14 to i32
  %18 = load i16, i16* %8, align 2
  %19 = sext i16 %18 to i32
  %20 = insertelement <4 x i32> undef, i32 %19, i32 0
  %21 = bitcast <4 x i32> %20 to <8 x i16>
  %22 = shufflevector <8 x i16> %21, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %23 = bitcast <8 x i16> %22 to <4 x i32>
  %24 = shufflevector <4 x i32> %23, <4 x i32> undef, <4 x i32> zeroinitializer
  %25 = sext i1 %12 to i16
  %26 = insertelement <8 x i16> undef, i16 %25, i32 0
  %27 = shufflevector <8 x i16> %26, <8 x i16> undef, <8 x i32> zeroinitializer
  %28 = bitcast <4 x i32> %24 to <8 x i16>
  %29 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %28, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  %30 = bitcast <4 x i32> %24 to <16 x i8>
  %31 = bitcast <8 x i16> %29 to <16 x i8>
  %32 = bitcast <8 x i16> %27 to <16 x i8>
  %33 = tail call <16 x i8> @llvm.x86.sse41.pblendvb(<16 x i8> %30, <16 x i8> %31, <16 x i8> %32) #8
  %34 = bitcast <16 x i8> %33 to <8 x i16>
  %35 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %34, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  %36 = insertelement <4 x i32> undef, i32 %17, i32 0
  %37 = shufflevector <4 x i32> %36, <4 x i32> undef, <4 x i32> zeroinitializer
  %38 = shufflevector <4 x i32> %36, <4 x i32> undef, <2 x i32> zeroinitializer
  %39 = zext <2 x i32> %38 to <2 x i64>
  %40 = shufflevector <8 x i16> %35, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %41 = sext <4 x i16> %40 to <4 x i32>
  %42 = bitcast <8 x i16> %35 to <16 x i8>
  %43 = shufflevector <16 x i8> %42, <16 x i8> undef, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %44 = bitcast <16 x i8> %43 to <8 x i16>
  %45 = shufflevector <8 x i16> %44, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %46 = sext <4 x i16> %45 to <4 x i32>
  %47 = add <4 x i32> %37, %41
  %48 = add <4 x i32> %37, %46
  %49 = bitcast <2 x i64> %39 to <4 x i32>
  %50 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %47, <4 x i32> %49) #8
  %51 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %48, <4 x i32> %49) #8
  %52 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %50, <4 x i32> %51) #8
  %53 = bitcast i8* %3 to <8 x i16>*
  store <8 x i16> %52, <8 x i16>* %53, align 1
  br label %514

54:                                               ; preds = %7
  br i1 %12, label %55, label %110

55:                                               ; preds = %54
  %56 = sext i32 %2 to i64
  %57 = add nsw i64 %56, -1
  %58 = and i64 %56, 3
  %59 = icmp ult i64 %57, 3
  br i1 %59, label %95, label %60

60:                                               ; preds = %55
  %61 = sub nsw i64 %56, %58
  br label %62

62:                                               ; preds = %62, %60
  %63 = phi i64 [ 0, %60 ], [ %92, %62 ]
  %64 = phi i64 [ %61, %60 ], [ %93, %62 ]
  %65 = shl i64 %63, 3
  %66 = and i64 %65, 4294967264
  %67 = getelementptr inbounds i16, i16* %8, i64 %66
  %68 = bitcast i16* %67 to <8 x i16>*
  %69 = load <8 x i16>, <8 x i16>* %68, align 1
  %70 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %69, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %70, <8 x i16>* %68, align 1
  %71 = shl i64 %63, 3
  %72 = and i64 %71, 4294967264
  %73 = or i64 %72, 8
  %74 = getelementptr inbounds i16, i16* %8, i64 %73
  %75 = bitcast i16* %74 to <8 x i16>*
  %76 = load <8 x i16>, <8 x i16>* %75, align 1
  %77 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %76, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %77, <8 x i16>* %75, align 1
  %78 = shl i64 %63, 3
  %79 = and i64 %78, 4294967264
  %80 = or i64 %79, 16
  %81 = getelementptr inbounds i16, i16* %8, i64 %80
  %82 = bitcast i16* %81 to <8 x i16>*
  %83 = load <8 x i16>, <8 x i16>* %82, align 1
  %84 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %83, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %84, <8 x i16>* %82, align 1
  %85 = shl i64 %63, 3
  %86 = and i64 %85, 4294967264
  %87 = or i64 %86, 24
  %88 = getelementptr inbounds i16, i16* %8, i64 %87
  %89 = bitcast i16* %88 to <8 x i16>*
  %90 = load <8 x i16>, <8 x i16>* %89, align 1
  %91 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %90, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %91, <8 x i16>* %89, align 1
  %92 = add nuw nsw i64 %63, 4
  %93 = add i64 %64, -4
  %94 = icmp eq i64 %93, 0
  br i1 %94, label %95, label %62

95:                                               ; preds = %62, %55
  %96 = phi i64 [ 0, %55 ], [ %92, %62 ]
  %97 = icmp eq i64 %58, 0
  br i1 %97, label %110, label %98

98:                                               ; preds = %95, %98
  %99 = phi i64 [ %107, %98 ], [ %96, %95 ]
  %100 = phi i64 [ %108, %98 ], [ %58, %95 ]
  %101 = shl i64 %99, 3
  %102 = and i64 %101, 4294967288
  %103 = getelementptr inbounds i16, i16* %8, i64 %102
  %104 = bitcast i16* %103 to <8 x i16>*
  %105 = load <8 x i16>, <8 x i16>* %104, align 1
  %106 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %105, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %106, <8 x i16>* %104, align 1
  %107 = add nuw nsw i64 %99, 1
  %108 = add i64 %100, -1
  %109 = icmp eq i64 %108, 0
  br i1 %109, label %110, label %98, !llvm.loop !2

110:                                              ; preds = %95, %98, %54
  %111 = icmp slt i32 %2, 5
  br i1 %111, label %118, label %112

112:                                              ; preds = %110
  %113 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %114 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %115 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %116 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %117 = sext i32 %2 to i64
  br label %256

118:                                              ; preds = %110
  %119 = bitcast i8* %3 to <8 x i16>*
  %120 = load <8 x i16>, <8 x i16>* %119, align 1
  %121 = getelementptr inbounds i8, i8* %3, i64 16
  %122 = bitcast i8* %121 to <8 x i16>*
  %123 = load <8 x i16>, <8 x i16>* %122, align 1
  %124 = getelementptr inbounds i8, i8* %3, i64 32
  %125 = bitcast i8* %124 to <8 x i16>*
  %126 = load <8 x i16>, <8 x i16>* %125, align 1
  %127 = getelementptr inbounds i8, i8* %3, i64 48
  %128 = bitcast i8* %127 to <8 x i16>*
  %129 = load <8 x i16>, <8 x i16>* %128, align 1
  %130 = shufflevector <8 x i16> %120, <8 x i16> %123, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %131 = shufflevector <8 x i16> %126, <8 x i16> %129, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %132 = shufflevector <8 x i16> %120, <8 x i16> %123, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %133 = shufflevector <8 x i16> %126, <8 x i16> %129, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %134 = bitcast <8 x i16> %130 to <4 x i32>
  %135 = bitcast <8 x i16> %131 to <4 x i32>
  %136 = shufflevector <4 x i32> %134, <4 x i32> %135, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %137 = bitcast <4 x i32> %136 to <2 x i64>
  %138 = bitcast <8 x i16> %132 to <4 x i32>
  %139 = bitcast <8 x i16> %133 to <4 x i32>
  %140 = shufflevector <4 x i32> %138, <4 x i32> %139, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %141 = bitcast <4 x i32> %140 to <2 x i64>
  %142 = shufflevector <4 x i32> %134, <4 x i32> %135, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %143 = bitcast <4 x i32> %142 to <2 x i64>
  %144 = shufflevector <4 x i32> %138, <4 x i32> %139, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %145 = bitcast <4 x i32> %144 to <2 x i64>
  %146 = insertelement <2 x i64> %137, i64 0, i32 1
  %147 = shufflevector <2 x i64> %137, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  %148 = insertelement <2 x i64> %143, i64 0, i32 1
  %149 = shufflevector <2 x i64> %143, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  %150 = insertelement <2 x i64> %141, i64 0, i32 1
  %151 = shufflevector <2 x i64> %141, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  %152 = insertelement <2 x i64> %145, i64 0, i32 1
  %153 = shufflevector <2 x i64> %145, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  %154 = bitcast <2 x i64> %146 to <8 x i16>
  %155 = bitcast <2 x i64> %150 to <8 x i16>
  %156 = shufflevector <8 x i16> %154, <8 x i16> %155, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %157 = shufflevector <8 x i16> %155, <8 x i16> %154, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %158 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %159 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %156, <8 x i16> %158) #8
  %160 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %157, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %161 = add <4 x i32> %159, <i32 2048, i32 2048, i32 2048, i32 2048>
  %162 = ashr <4 x i32> %161, <i32 12, i32 12, i32 12, i32 12>
  %163 = add <4 x i32> %160, <i32 2048, i32 2048, i32 2048, i32 2048>
  %164 = ashr <4 x i32> %163, <i32 12, i32 12, i32 12, i32 12>
  %165 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %162, <4 x i32> %162) #8
  %166 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %164, <4 x i32> %164) #8
  %167 = bitcast <2 x i64> %148 to <8 x i16>
  %168 = bitcast <2 x i64> %152 to <8 x i16>
  %169 = shufflevector <8 x i16> %167, <8 x i16> %168, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %170 = shufflevector <8 x i16> %168, <8 x i16> %167, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %171 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %172 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %169, <8 x i16> %171) #8
  %173 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %170, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %174 = add <4 x i32> %172, <i32 2048, i32 2048, i32 2048, i32 2048>
  %175 = ashr <4 x i32> %174, <i32 12, i32 12, i32 12, i32 12>
  %176 = add <4 x i32> %173, <i32 2048, i32 2048, i32 2048, i32 2048>
  %177 = ashr <4 x i32> %176, <i32 12, i32 12, i32 12, i32 12>
  %178 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %175, <4 x i32> %175) #8
  %179 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %177, <4 x i32> %177) #8
  %180 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %166, <8 x i16> %179) #8
  %181 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %166, <8 x i16> %179) #8
  %182 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %165, <8 x i16> %178) #8
  %183 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %165, <8 x i16> %178) #8
  %184 = bitcast <2 x i64> %147 to <8 x i16>
  %185 = bitcast <2 x i64> %153 to <8 x i16>
  %186 = shufflevector <8 x i16> %184, <8 x i16> %185, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %187 = shufflevector <8 x i16> %185, <8 x i16> %184, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %188 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %189 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %186, <8 x i16> %188) #8
  %190 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %187, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %191 = add <4 x i32> %189, <i32 2048, i32 2048, i32 2048, i32 2048>
  %192 = ashr <4 x i32> %191, <i32 12, i32 12, i32 12, i32 12>
  %193 = add <4 x i32> %190, <i32 2048, i32 2048, i32 2048, i32 2048>
  %194 = ashr <4 x i32> %193, <i32 12, i32 12, i32 12, i32 12>
  %195 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %192, <4 x i32> %192) #8
  %196 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %194, <4 x i32> %194) #8
  %197 = bitcast <2 x i64> %151 to <8 x i16>
  %198 = bitcast <2 x i64> %149 to <8 x i16>
  %199 = shufflevector <8 x i16> %197, <8 x i16> %198, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %200 = shufflevector <8 x i16> %198, <8 x i16> %197, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %201 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %202 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %199, <8 x i16> %201) #8
  %203 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %200, <8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>) #8
  %204 = add <4 x i32> %202, <i32 2048, i32 2048, i32 2048, i32 2048>
  %205 = ashr <4 x i32> %204, <i32 12, i32 12, i32 12, i32 12>
  %206 = add <4 x i32> %203, <i32 2048, i32 2048, i32 2048, i32 2048>
  %207 = ashr <4 x i32> %206, <i32 12, i32 12, i32 12, i32 12>
  %208 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %205, <4 x i32> %205) #8
  %209 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %207, <4 x i32> %207) #8
  %210 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %195, <8 x i16> %208) #8
  %211 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %195, <8 x i16> %208) #8
  %212 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %196, <8 x i16> %209) #8
  %213 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %196, <8 x i16> %209) #8
  %214 = shufflevector <8 x i16> %213, <8 x i16> %211, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %215 = shufflevector <8 x i16> %211, <8 x i16> %213, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %216 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %214, <8 x i16> %158) #8
  %217 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %215, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %218 = add <4 x i32> %216, <i32 2048, i32 2048, i32 2048, i32 2048>
  %219 = ashr <4 x i32> %218, <i32 12, i32 12, i32 12, i32 12>
  %220 = add <4 x i32> %217, <i32 2048, i32 2048, i32 2048, i32 2048>
  %221 = ashr <4 x i32> %220, <i32 12, i32 12, i32 12, i32 12>
  %222 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %219, <4 x i32> %219) #8
  %223 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %221, <4 x i32> %221) #8
  %224 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %180, <8 x i16> %212) #8
  %225 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %180, <8 x i16> %212) #8
  %226 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %182, <8 x i16> %223) #8
  %227 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %182, <8 x i16> %223) #8
  %228 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %183, <8 x i16> %222) #8
  %229 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %183, <8 x i16> %222) #8
  %230 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %181, <8 x i16> %210) #8
  %231 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %181, <8 x i16> %210) #8
  %232 = shufflevector <8 x i16> %224, <8 x i16> %226, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %233 = shufflevector <8 x i16> %228, <8 x i16> %230, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %234 = shufflevector <8 x i16> %231, <8 x i16> %229, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %235 = shufflevector <8 x i16> %227, <8 x i16> %225, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %236 = bitcast <8 x i16> %232 to <4 x i32>
  %237 = bitcast <8 x i16> %233 to <4 x i32>
  %238 = shufflevector <4 x i32> %236, <4 x i32> %237, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %239 = bitcast <4 x i32> %238 to <2 x i64>
  %240 = bitcast <8 x i16> %234 to <4 x i32>
  %241 = bitcast <8 x i16> %235 to <4 x i32>
  %242 = shufflevector <4 x i32> %240, <4 x i32> %241, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %243 = bitcast <4 x i32> %242 to <2 x i64>
  %244 = shufflevector <4 x i32> %236, <4 x i32> %237, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %245 = bitcast <4 x i32> %244 to <2 x i64>
  %246 = shufflevector <4 x i32> %240, <4 x i32> %241, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %247 = bitcast <4 x i32> %246 to <2 x i64>
  %248 = shufflevector <2 x i64> %239, <2 x i64> %243, <2 x i32> <i32 0, i32 2>
  %249 = shufflevector <2 x i64> %239, <2 x i64> %243, <2 x i32> <i32 1, i32 3>
  %250 = shufflevector <2 x i64> %245, <2 x i64> %247, <2 x i32> <i32 0, i32 2>
  %251 = shufflevector <2 x i64> %245, <2 x i64> %247, <2 x i32> <i32 1, i32 3>
  %252 = bitcast i8* %3 to <2 x i64>*
  store <2 x i64> %248, <2 x i64>* %252, align 1
  %253 = bitcast i8* %121 to <2 x i64>*
  store <2 x i64> %249, <2 x i64>* %253, align 1
  %254 = bitcast i8* %124 to <2 x i64>*
  store <2 x i64> %250, <2 x i64>* %254, align 1
  %255 = bitcast i8* %127 to <2 x i64>*
  store <2 x i64> %251, <2 x i64>* %255, align 1
  br label %488

256:                                              ; preds = %112, %256
  %257 = phi i64 [ 0, %112 ], [ %486, %256 ]
  %258 = shl i64 %257, 3
  %259 = and i64 %258, 4294967232
  %260 = getelementptr inbounds i16, i16* %8, i64 %259
  %261 = bitcast i16* %260 to <2 x i64>*
  %262 = bitcast i16* %260 to <8 x i16>*
  %263 = load <8 x i16>, <8 x i16>* %262, align 1
  %264 = getelementptr inbounds i16, i16* %260, i64 8
  %265 = bitcast i16* %264 to <2 x i64>*
  %266 = bitcast i16* %264 to <8 x i16>*
  %267 = load <8 x i16>, <8 x i16>* %266, align 1
  %268 = getelementptr inbounds i16, i16* %260, i64 16
  %269 = bitcast i16* %268 to <2 x i64>*
  %270 = bitcast i16* %268 to <8 x i16>*
  %271 = load <8 x i16>, <8 x i16>* %270, align 1
  %272 = getelementptr inbounds i16, i16* %260, i64 24
  %273 = bitcast i16* %272 to <2 x i64>*
  %274 = bitcast i16* %272 to <8 x i16>*
  %275 = load <8 x i16>, <8 x i16>* %274, align 1
  %276 = getelementptr inbounds i16, i16* %260, i64 32
  %277 = bitcast i16* %276 to <2 x i64>*
  %278 = bitcast i16* %276 to <8 x i16>*
  %279 = load <8 x i16>, <8 x i16>* %278, align 1
  %280 = getelementptr inbounds i16, i16* %260, i64 40
  %281 = bitcast i16* %280 to <2 x i64>*
  %282 = bitcast i16* %280 to <8 x i16>*
  %283 = load <8 x i16>, <8 x i16>* %282, align 1
  %284 = getelementptr inbounds i16, i16* %260, i64 48
  %285 = bitcast i16* %284 to <2 x i64>*
  %286 = bitcast i16* %284 to <8 x i16>*
  %287 = load <8 x i16>, <8 x i16>* %286, align 1
  %288 = getelementptr inbounds i16, i16* %260, i64 56
  %289 = bitcast i16* %288 to <2 x i64>*
  %290 = bitcast i16* %288 to <8 x i16>*
  %291 = load <8 x i16>, <8 x i16>* %290, align 1
  %292 = shufflevector <8 x i16> %263, <8 x i16> %267, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %293 = shufflevector <8 x i16> %271, <8 x i16> %275, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %294 = shufflevector <8 x i16> %279, <8 x i16> %283, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %295 = shufflevector <8 x i16> %287, <8 x i16> %291, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %296 = shufflevector <8 x i16> %263, <8 x i16> %267, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %297 = shufflevector <8 x i16> %271, <8 x i16> %275, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %298 = shufflevector <8 x i16> %279, <8 x i16> %283, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %299 = shufflevector <8 x i16> %287, <8 x i16> %291, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %300 = bitcast <8 x i16> %292 to <4 x i32>
  %301 = bitcast <8 x i16> %293 to <4 x i32>
  %302 = shufflevector <4 x i32> %300, <4 x i32> %301, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %303 = bitcast <4 x i32> %302 to <2 x i64>
  %304 = bitcast <8 x i16> %294 to <4 x i32>
  %305 = bitcast <8 x i16> %295 to <4 x i32>
  %306 = shufflevector <4 x i32> %304, <4 x i32> %305, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %307 = bitcast <4 x i32> %306 to <2 x i64>
  %308 = bitcast <8 x i16> %296 to <4 x i32>
  %309 = bitcast <8 x i16> %297 to <4 x i32>
  %310 = shufflevector <4 x i32> %308, <4 x i32> %309, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %311 = bitcast <4 x i32> %310 to <2 x i64>
  %312 = bitcast <8 x i16> %298 to <4 x i32>
  %313 = bitcast <8 x i16> %299 to <4 x i32>
  %314 = shufflevector <4 x i32> %312, <4 x i32> %313, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %315 = bitcast <4 x i32> %314 to <2 x i64>
  %316 = shufflevector <4 x i32> %300, <4 x i32> %301, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %317 = bitcast <4 x i32> %316 to <2 x i64>
  %318 = shufflevector <4 x i32> %304, <4 x i32> %305, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %319 = bitcast <4 x i32> %318 to <2 x i64>
  %320 = shufflevector <4 x i32> %308, <4 x i32> %309, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %321 = bitcast <4 x i32> %320 to <2 x i64>
  %322 = shufflevector <4 x i32> %312, <4 x i32> %313, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %323 = bitcast <4 x i32> %322 to <2 x i64>
  %324 = shufflevector <2 x i64> %303, <2 x i64> %307, <2 x i32> <i32 0, i32 2>
  %325 = shufflevector <2 x i64> %303, <2 x i64> %307, <2 x i32> <i32 1, i32 3>
  %326 = shufflevector <2 x i64> %317, <2 x i64> %319, <2 x i32> <i32 0, i32 2>
  %327 = shufflevector <2 x i64> %317, <2 x i64> %319, <2 x i32> <i32 1, i32 3>
  %328 = shufflevector <2 x i64> %311, <2 x i64> %315, <2 x i32> <i32 0, i32 2>
  %329 = shufflevector <2 x i64> %311, <2 x i64> %315, <2 x i32> <i32 1, i32 3>
  %330 = shufflevector <2 x i64> %321, <2 x i64> %323, <2 x i32> <i32 0, i32 2>
  %331 = shufflevector <2 x i64> %321, <2 x i64> %323, <2 x i32> <i32 1, i32 3>
  %332 = bitcast <2 x i64> %324 to <8 x i16>
  %333 = bitcast <2 x i64> %328 to <8 x i16>
  %334 = shufflevector <8 x i16> %332, <8 x i16> %333, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %335 = shufflevector <8 x i16> %333, <8 x i16> %332, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %336 = shufflevector <8 x i16> %332, <8 x i16> %333, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %337 = shufflevector <8 x i16> %333, <8 x i16> %332, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %338 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %334, <8 x i16> %113) #8
  %339 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %335, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %340 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %336, <8 x i16> %113) #8
  %341 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %337, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %342 = add <4 x i32> %338, <i32 2048, i32 2048, i32 2048, i32 2048>
  %343 = ashr <4 x i32> %342, <i32 12, i32 12, i32 12, i32 12>
  %344 = add <4 x i32> %339, <i32 2048, i32 2048, i32 2048, i32 2048>
  %345 = ashr <4 x i32> %344, <i32 12, i32 12, i32 12, i32 12>
  %346 = add <4 x i32> %340, <i32 2048, i32 2048, i32 2048, i32 2048>
  %347 = ashr <4 x i32> %346, <i32 12, i32 12, i32 12, i32 12>
  %348 = add <4 x i32> %341, <i32 2048, i32 2048, i32 2048, i32 2048>
  %349 = ashr <4 x i32> %348, <i32 12, i32 12, i32 12, i32 12>
  %350 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %343, <4 x i32> %347) #8
  %351 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %345, <4 x i32> %349) #8
  %352 = bitcast <2 x i64> %326 to <8 x i16>
  %353 = bitcast <2 x i64> %330 to <8 x i16>
  %354 = shufflevector <8 x i16> %352, <8 x i16> %353, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %355 = shufflevector <8 x i16> %353, <8 x i16> %352, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %356 = shufflevector <8 x i16> %352, <8 x i16> %353, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %357 = shufflevector <8 x i16> %353, <8 x i16> %352, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %358 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %354, <8 x i16> %114) #8
  %359 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %355, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %360 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %356, <8 x i16> %114) #8
  %361 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %357, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %362 = add <4 x i32> %358, <i32 2048, i32 2048, i32 2048, i32 2048>
  %363 = ashr <4 x i32> %362, <i32 12, i32 12, i32 12, i32 12>
  %364 = add <4 x i32> %359, <i32 2048, i32 2048, i32 2048, i32 2048>
  %365 = ashr <4 x i32> %364, <i32 12, i32 12, i32 12, i32 12>
  %366 = add <4 x i32> %360, <i32 2048, i32 2048, i32 2048, i32 2048>
  %367 = ashr <4 x i32> %366, <i32 12, i32 12, i32 12, i32 12>
  %368 = add <4 x i32> %361, <i32 2048, i32 2048, i32 2048, i32 2048>
  %369 = ashr <4 x i32> %368, <i32 12, i32 12, i32 12, i32 12>
  %370 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %363, <4 x i32> %367) #8
  %371 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %365, <4 x i32> %369) #8
  %372 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %351, <8 x i16> %371) #8
  %373 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %351, <8 x i16> %371) #8
  %374 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %350, <8 x i16> %370) #8
  %375 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %350, <8 x i16> %370) #8
  %376 = bitcast <2 x i64> %325 to <8 x i16>
  %377 = bitcast <2 x i64> %331 to <8 x i16>
  %378 = shufflevector <8 x i16> %376, <8 x i16> %377, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %379 = shufflevector <8 x i16> %377, <8 x i16> %376, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %380 = shufflevector <8 x i16> %376, <8 x i16> %377, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %381 = shufflevector <8 x i16> %377, <8 x i16> %376, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %382 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %378, <8 x i16> %115) #8
  %383 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %379, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %384 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %380, <8 x i16> %115) #8
  %385 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %381, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %386 = add <4 x i32> %382, <i32 2048, i32 2048, i32 2048, i32 2048>
  %387 = ashr <4 x i32> %386, <i32 12, i32 12, i32 12, i32 12>
  %388 = add <4 x i32> %383, <i32 2048, i32 2048, i32 2048, i32 2048>
  %389 = ashr <4 x i32> %388, <i32 12, i32 12, i32 12, i32 12>
  %390 = add <4 x i32> %384, <i32 2048, i32 2048, i32 2048, i32 2048>
  %391 = ashr <4 x i32> %390, <i32 12, i32 12, i32 12, i32 12>
  %392 = add <4 x i32> %385, <i32 2048, i32 2048, i32 2048, i32 2048>
  %393 = ashr <4 x i32> %392, <i32 12, i32 12, i32 12, i32 12>
  %394 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %387, <4 x i32> %391) #8
  %395 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %389, <4 x i32> %393) #8
  %396 = bitcast <2 x i64> %329 to <8 x i16>
  %397 = bitcast <2 x i64> %327 to <8 x i16>
  %398 = shufflevector <8 x i16> %396, <8 x i16> %397, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %399 = shufflevector <8 x i16> %397, <8 x i16> %396, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %400 = shufflevector <8 x i16> %396, <8 x i16> %397, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %401 = shufflevector <8 x i16> %397, <8 x i16> %396, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %402 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %398, <8 x i16> %116) #8
  %403 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %399, <8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>) #8
  %404 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %400, <8 x i16> %116) #8
  %405 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %401, <8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>) #8
  %406 = add <4 x i32> %402, <i32 2048, i32 2048, i32 2048, i32 2048>
  %407 = ashr <4 x i32> %406, <i32 12, i32 12, i32 12, i32 12>
  %408 = add <4 x i32> %403, <i32 2048, i32 2048, i32 2048, i32 2048>
  %409 = ashr <4 x i32> %408, <i32 12, i32 12, i32 12, i32 12>
  %410 = add <4 x i32> %404, <i32 2048, i32 2048, i32 2048, i32 2048>
  %411 = ashr <4 x i32> %410, <i32 12, i32 12, i32 12, i32 12>
  %412 = add <4 x i32> %405, <i32 2048, i32 2048, i32 2048, i32 2048>
  %413 = ashr <4 x i32> %412, <i32 12, i32 12, i32 12, i32 12>
  %414 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %407, <4 x i32> %411) #8
  %415 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %409, <4 x i32> %413) #8
  %416 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %394, <8 x i16> %414) #8
  %417 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %394, <8 x i16> %414) #8
  %418 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %395, <8 x i16> %415) #8
  %419 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %395, <8 x i16> %415) #8
  %420 = shufflevector <8 x i16> %419, <8 x i16> %417, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %421 = shufflevector <8 x i16> %417, <8 x i16> %419, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %422 = shufflevector <8 x i16> %419, <8 x i16> %417, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %423 = shufflevector <8 x i16> %417, <8 x i16> %419, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %424 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %420, <8 x i16> %113) #8
  %425 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %421, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %426 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %422, <8 x i16> %113) #8
  %427 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %423, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %428 = add <4 x i32> %424, <i32 2048, i32 2048, i32 2048, i32 2048>
  %429 = ashr <4 x i32> %428, <i32 12, i32 12, i32 12, i32 12>
  %430 = add <4 x i32> %425, <i32 2048, i32 2048, i32 2048, i32 2048>
  %431 = ashr <4 x i32> %430, <i32 12, i32 12, i32 12, i32 12>
  %432 = add <4 x i32> %426, <i32 2048, i32 2048, i32 2048, i32 2048>
  %433 = ashr <4 x i32> %432, <i32 12, i32 12, i32 12, i32 12>
  %434 = add <4 x i32> %427, <i32 2048, i32 2048, i32 2048, i32 2048>
  %435 = ashr <4 x i32> %434, <i32 12, i32 12, i32 12, i32 12>
  %436 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %429, <4 x i32> %433) #8
  %437 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %431, <4 x i32> %435) #8
  %438 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %372, <8 x i16> %418) #8
  %439 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %372, <8 x i16> %418) #8
  %440 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %374, <8 x i16> %437) #8
  %441 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %374, <8 x i16> %437) #8
  %442 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %375, <8 x i16> %436) #8
  %443 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %375, <8 x i16> %436) #8
  %444 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %373, <8 x i16> %416) #8
  %445 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %373, <8 x i16> %416) #8
  %446 = shufflevector <8 x i16> %438, <8 x i16> %440, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %447 = shufflevector <8 x i16> %442, <8 x i16> %444, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %448 = shufflevector <8 x i16> %445, <8 x i16> %443, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %449 = shufflevector <8 x i16> %441, <8 x i16> %439, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %450 = shufflevector <8 x i16> %438, <8 x i16> %440, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %451 = shufflevector <8 x i16> %442, <8 x i16> %444, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %452 = shufflevector <8 x i16> %445, <8 x i16> %443, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %453 = shufflevector <8 x i16> %441, <8 x i16> %439, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %454 = bitcast <8 x i16> %446 to <4 x i32>
  %455 = bitcast <8 x i16> %447 to <4 x i32>
  %456 = shufflevector <4 x i32> %454, <4 x i32> %455, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %457 = bitcast <4 x i32> %456 to <2 x i64>
  %458 = bitcast <8 x i16> %448 to <4 x i32>
  %459 = bitcast <8 x i16> %449 to <4 x i32>
  %460 = shufflevector <4 x i32> %458, <4 x i32> %459, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %461 = bitcast <4 x i32> %460 to <2 x i64>
  %462 = bitcast <8 x i16> %450 to <4 x i32>
  %463 = bitcast <8 x i16> %451 to <4 x i32>
  %464 = shufflevector <4 x i32> %462, <4 x i32> %463, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %465 = bitcast <4 x i32> %464 to <2 x i64>
  %466 = bitcast <8 x i16> %452 to <4 x i32>
  %467 = bitcast <8 x i16> %453 to <4 x i32>
  %468 = shufflevector <4 x i32> %466, <4 x i32> %467, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %469 = bitcast <4 x i32> %468 to <2 x i64>
  %470 = shufflevector <4 x i32> %454, <4 x i32> %455, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %471 = bitcast <4 x i32> %470 to <2 x i64>
  %472 = shufflevector <4 x i32> %458, <4 x i32> %459, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %473 = bitcast <4 x i32> %472 to <2 x i64>
  %474 = shufflevector <4 x i32> %462, <4 x i32> %463, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %475 = bitcast <4 x i32> %474 to <2 x i64>
  %476 = shufflevector <4 x i32> %466, <4 x i32> %467, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %477 = bitcast <4 x i32> %476 to <2 x i64>
  %478 = shufflevector <2 x i64> %457, <2 x i64> %461, <2 x i32> <i32 0, i32 2>
  %479 = shufflevector <2 x i64> %457, <2 x i64> %461, <2 x i32> <i32 1, i32 3>
  %480 = shufflevector <2 x i64> %471, <2 x i64> %473, <2 x i32> <i32 0, i32 2>
  %481 = shufflevector <2 x i64> %471, <2 x i64> %473, <2 x i32> <i32 1, i32 3>
  %482 = shufflevector <2 x i64> %465, <2 x i64> %469, <2 x i32> <i32 0, i32 2>
  %483 = shufflevector <2 x i64> %465, <2 x i64> %469, <2 x i32> <i32 1, i32 3>
  %484 = shufflevector <2 x i64> %475, <2 x i64> %477, <2 x i32> <i32 0, i32 2>
  %485 = shufflevector <2 x i64> %475, <2 x i64> %477, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %478, <2 x i64>* %261, align 1
  store <2 x i64> %479, <2 x i64>* %265, align 1
  store <2 x i64> %480, <2 x i64>* %269, align 1
  store <2 x i64> %481, <2 x i64>* %273, align 1
  store <2 x i64> %482, <2 x i64>* %277, align 1
  store <2 x i64> %483, <2 x i64>* %281, align 1
  store <2 x i64> %484, <2 x i64>* %285, align 1
  store <2 x i64> %485, <2 x i64>* %289, align 1
  %486 = add nuw nsw i64 %257, 8
  %487 = icmp slt i64 %486, %117
  br i1 %487, label %256, label %488

488:                                              ; preds = %256, %118
  %489 = lshr i64 524276, %9
  %490 = and i64 %489, 1
  %491 = icmp eq i64 %490, 0
  br i1 %491, label %514, label %492

492:                                              ; preds = %488
  %493 = zext i8 %14 to i16
  %494 = insertelement <8 x i16> undef, i16 %493, i32 0
  %495 = shufflevector <8 x i16> %494, <8 x i16> undef, <8 x i32> zeroinitializer
  %496 = shufflevector <8 x i16> %494, <8 x i16> undef, <2 x i32> zeroinitializer
  %497 = zext <2 x i16> %496 to <2 x i64>
  %498 = bitcast <2 x i64> %497 to <8 x i16>
  %499 = sext i32 %2 to i64
  br label %500

500:                                              ; preds = %500, %492
  %501 = phi i64 [ %512, %500 ], [ 0, %492 ]
  %502 = shl i64 %501, 3
  %503 = and i64 %502, 4294967288
  %504 = getelementptr inbounds i16, i16* %8, i64 %503
  %505 = bitcast i16* %504 to <8 x i16>*
  %506 = load <8 x i16>, <8 x i16>* %505, align 1
  %507 = icmp sgt <8 x i16> %506, <i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765>
  %508 = add <8 x i16> %506, %495
  %509 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %508, <8 x i16> %498) #8
  %510 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %508, <8 x i16> %498) #8
  %511 = select <8 x i1> %507, <8 x i16> %510, <8 x i16> %509
  store <8 x i16> %511, <8 x i16>* %505, align 1
  %512 = add nuw nsw i64 %501, 1
  %513 = icmp slt i64 %512, %499
  br i1 %513, label %500, label %514

514:                                              ; preds = %500, %16, %488
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_130Dct8TransformLoopColumn_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_(i8 zeroext, i8 zeroext, i32, i8* nocapture, i32, i32, i8* nocapture readonly) #4 {
  %8 = bitcast i8* %3 to i16*
  %9 = zext i8 %1 to i64
  %10 = getelementptr inbounds [19 x i8], [19 x i8]* @_ZN7libgav115kTransformWidthE, i64 0, i64 %9
  %11 = load i8, i8* %10, align 1
  %12 = zext i8 %11 to i64
  %13 = zext i8 %0 to i32
  %14 = shl i32 1, %13
  %15 = and i32 %14, 33104
  %16 = icmp eq i32 %15, 0
  br i1 %16, label %82, label %17

17:                                               ; preds = %7
  %18 = icmp ugt i8 %11, 15
  br i1 %18, label %19, label %34

19:                                               ; preds = %17
  %20 = shl nuw nsw i64 %12, 3
  br label %21

21:                                               ; preds = %21, %19
  %22 = phi i64 [ 0, %19 ], [ %32, %21 ]
  %23 = getelementptr inbounds i16, i16* %8, i64 %22
  %24 = bitcast i16* %23 to <16 x i8>*
  %25 = load <16 x i8>, <16 x i8>* %24, align 1
  %26 = or i64 %22, 8
  %27 = getelementptr inbounds i16, i16* %8, i64 %26
  %28 = bitcast i16* %27 to <16 x i8>*
  %29 = load <16 x i8>, <16 x i8>* %28, align 1
  %30 = shufflevector <16 x i8> %25, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  %31 = shufflevector <16 x i8> %29, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %31, <16 x i8>* %24, align 1
  store <16 x i8> %30, <16 x i8>* %28, align 1
  %32 = add nuw nsw i64 %22, 16
  %33 = icmp ult i64 %32, %20
  br i1 %33, label %21, label %82

34:                                               ; preds = %17
  %35 = icmp eq i8 %11, 8
  %36 = bitcast i8* %3 to <16 x i8>*
  %37 = load <16 x i8>, <16 x i8>* %36, align 1
  br i1 %35, label %52, label %38

38:                                               ; preds = %34
  %39 = shufflevector <16 x i8> %37, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %39, <16 x i8>* %36, align 1
  %40 = getelementptr inbounds i8, i8* %3, i64 16
  %41 = bitcast i8* %40 to <16 x i8>*
  %42 = load <16 x i8>, <16 x i8>* %41, align 1
  %43 = shufflevector <16 x i8> %42, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %43, <16 x i8>* %41, align 1
  %44 = getelementptr inbounds i8, i8* %3, i64 32
  %45 = bitcast i8* %44 to <16 x i8>*
  %46 = load <16 x i8>, <16 x i8>* %45, align 1
  %47 = shufflevector <16 x i8> %46, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %47, <16 x i8>* %45, align 1
  %48 = getelementptr inbounds i8, i8* %3, i64 48
  %49 = bitcast i8* %48 to <16 x i8>*
  %50 = load <16 x i8>, <16 x i8>* %49, align 1
  %51 = shufflevector <16 x i8> %50, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %51, <16 x i8>* %49, align 1
  br label %82

52:                                               ; preds = %34
  %53 = shufflevector <16 x i8> %37, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %53, <16 x i8>* %36, align 1
  %54 = getelementptr inbounds i8, i8* %3, i64 16
  %55 = bitcast i8* %54 to <16 x i8>*
  %56 = load <16 x i8>, <16 x i8>* %55, align 1
  %57 = shufflevector <16 x i8> %56, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %57, <16 x i8>* %55, align 1
  %58 = getelementptr inbounds i8, i8* %3, i64 32
  %59 = bitcast i8* %58 to <16 x i8>*
  %60 = load <16 x i8>, <16 x i8>* %59, align 1
  %61 = shufflevector <16 x i8> %60, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %61, <16 x i8>* %59, align 1
  %62 = getelementptr inbounds i8, i8* %3, i64 48
  %63 = bitcast i8* %62 to <16 x i8>*
  %64 = load <16 x i8>, <16 x i8>* %63, align 1
  %65 = shufflevector <16 x i8> %64, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %65, <16 x i8>* %63, align 1
  %66 = getelementptr inbounds i8, i8* %3, i64 64
  %67 = bitcast i8* %66 to <16 x i8>*
  %68 = load <16 x i8>, <16 x i8>* %67, align 1
  %69 = shufflevector <16 x i8> %68, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %69, <16 x i8>* %67, align 1
  %70 = getelementptr inbounds i8, i8* %3, i64 80
  %71 = bitcast i8* %70 to <16 x i8>*
  %72 = load <16 x i8>, <16 x i8>* %71, align 1
  %73 = shufflevector <16 x i8> %72, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %73, <16 x i8>* %71, align 1
  %74 = getelementptr inbounds i8, i8* %3, i64 96
  %75 = bitcast i8* %74 to <16 x i8>*
  %76 = load <16 x i8>, <16 x i8>* %75, align 1
  %77 = shufflevector <16 x i8> %76, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %77, <16 x i8>* %75, align 1
  %78 = getelementptr inbounds i8, i8* %3, i64 112
  %79 = bitcast i8* %78 to <16 x i8>*
  %80 = load <16 x i8>, <16 x i8>* %79, align 1
  %81 = shufflevector <16 x i8> %80, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %81, <16 x i8>* %79, align 1
  br label %82

82:                                               ; preds = %21, %7, %52, %38
  %83 = icmp sgt i32 %2, 1
  %84 = icmp eq i8 %11, 4
  br i1 %83, label %126, label %85

85:                                               ; preds = %82
  br i1 %84, label %88, label %86

86:                                               ; preds = %85
  %87 = zext i8 %11 to i64
  br label %96

88:                                               ; preds = %85
  %89 = bitcast i8* %3 to i64*
  %90 = load i64, i64* %89, align 1
  %91 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %90, i32 0
  %92 = bitcast <2 x i64> %91 to <8 x i16>
  %93 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %92, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  %94 = bitcast <8 x i16> %93 to <2 x i64>
  %95 = extractelement <2 x i64> %94, i32 0
  store i64 %95, i64* %89, align 1
  br label %104

96:                                               ; preds = %96, %86
  %97 = phi i64 [ 0, %86 ], [ %102, %96 ]
  %98 = getelementptr inbounds i16, i16* %8, i64 %97
  %99 = bitcast i16* %98 to <8 x i16>*
  %100 = load <8 x i16>, <8 x i16>* %99, align 1
  %101 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %100, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %101, <8 x i16>* %99, align 1
  %102 = add nuw nsw i64 %97, 8
  %103 = icmp ult i64 %102, %87
  br i1 %103, label %96, label %104

104:                                              ; preds = %96, %88
  %105 = phi i64 [ 4, %88 ], [ %87, %96 ]
  %106 = shl nuw nsw i64 %105, 1
  %107 = getelementptr inbounds i16, i16* %8, i64 %105
  %108 = bitcast i16* %107 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 2 %108, i8* align 2 %3, i64 %106, i1 false) #8
  %109 = getelementptr inbounds i16, i16* %8, i64 %106
  %110 = bitcast i16* %109 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 2 %110, i8* align 2 %3, i64 %106, i1 false) #8
  %111 = mul nuw nsw i64 %105, 3
  %112 = getelementptr inbounds i16, i16* %8, i64 %111
  %113 = bitcast i16* %112 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 2 %113, i8* align 2 %3, i64 %106, i1 false) #8
  %114 = shl nuw nsw i64 %105, 2
  %115 = getelementptr inbounds i16, i16* %8, i64 %114
  %116 = bitcast i16* %115 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 2 %116, i8* align 2 %3, i64 %106, i1 false) #8
  %117 = mul nuw nsw i64 %105, 5
  %118 = getelementptr inbounds i16, i16* %8, i64 %117
  %119 = bitcast i16* %118 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 2 %119, i8* align 2 %3, i64 %106, i1 false) #8
  %120 = mul nuw nsw i64 %105, 6
  %121 = getelementptr inbounds i16, i16* %8, i64 %120
  %122 = bitcast i16* %121 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 2 %122, i8* align 2 %3, i64 %106, i1 false) #8
  %123 = mul nuw nsw i64 %105, 7
  %124 = getelementptr inbounds i16, i16* %8, i64 %123
  %125 = bitcast i16* %124 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 2 %125, i8* align 2 %3, i64 %106, i1 false) #8
  br label %411

126:                                              ; preds = %82
  br i1 %84, label %140, label %127

127:                                              ; preds = %126
  %128 = zext i8 %11 to i64
  %129 = shl nuw nsw i64 %128, 1
  %130 = mul nuw nsw i64 %128, 3
  %131 = shl nuw nsw i64 %128, 2
  %132 = mul nuw nsw i64 %128, 5
  %133 = mul nuw nsw i64 %128, 6
  %134 = mul nuw nsw i64 %128, 7
  %135 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %136 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %137 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %138 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %139 = zext i8 %11 to i64
  br label %277

140:                                              ; preds = %126
  %141 = bitcast i8* %3 to i64*
  %142 = load i64, i64* %141, align 1
  %143 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %142, i32 0
  %144 = getelementptr inbounds i8, i8* %3, i64 8
  %145 = bitcast i8* %144 to i64*
  %146 = load i64, i64* %145, align 1
  %147 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %146, i32 0
  %148 = getelementptr inbounds i8, i8* %3, i64 16
  %149 = bitcast i8* %148 to i64*
  %150 = load i64, i64* %149, align 1
  %151 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %150, i32 0
  %152 = getelementptr inbounds i8, i8* %3, i64 24
  %153 = bitcast i8* %152 to i64*
  %154 = load i64, i64* %153, align 1
  %155 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %154, i32 0
  %156 = getelementptr inbounds i8, i8* %3, i64 32
  %157 = bitcast i8* %156 to i64*
  %158 = load i64, i64* %157, align 1
  %159 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %158, i32 0
  %160 = getelementptr inbounds i8, i8* %3, i64 40
  %161 = bitcast i8* %160 to i64*
  %162 = load i64, i64* %161, align 1
  %163 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %162, i32 0
  %164 = getelementptr inbounds i8, i8* %3, i64 48
  %165 = bitcast i8* %164 to i64*
  %166 = load i64, i64* %165, align 1
  %167 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %166, i32 0
  %168 = getelementptr inbounds i8, i8* %3, i64 56
  %169 = bitcast i8* %168 to i64*
  %170 = load i64, i64* %169, align 1
  %171 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %170, i32 0
  %172 = bitcast <2 x i64> %143 to <8 x i16>
  %173 = bitcast <2 x i64> %159 to <8 x i16>
  %174 = shufflevector <8 x i16> %172, <8 x i16> %173, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %175 = shufflevector <8 x i16> %173, <8 x i16> %172, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %176 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %177 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %174, <8 x i16> %176) #8
  %178 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %175, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %179 = add <4 x i32> %177, <i32 2048, i32 2048, i32 2048, i32 2048>
  %180 = ashr <4 x i32> %179, <i32 12, i32 12, i32 12, i32 12>
  %181 = add <4 x i32> %178, <i32 2048, i32 2048, i32 2048, i32 2048>
  %182 = ashr <4 x i32> %181, <i32 12, i32 12, i32 12, i32 12>
  %183 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %180, <4 x i32> %180) #8
  %184 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %182, <4 x i32> %182) #8
  %185 = bitcast <2 x i64> %151 to <8 x i16>
  %186 = bitcast <2 x i64> %167 to <8 x i16>
  %187 = shufflevector <8 x i16> %185, <8 x i16> %186, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %188 = shufflevector <8 x i16> %186, <8 x i16> %185, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %189 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %190 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %187, <8 x i16> %189) #8
  %191 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %188, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %192 = add <4 x i32> %190, <i32 2048, i32 2048, i32 2048, i32 2048>
  %193 = ashr <4 x i32> %192, <i32 12, i32 12, i32 12, i32 12>
  %194 = add <4 x i32> %191, <i32 2048, i32 2048, i32 2048, i32 2048>
  %195 = ashr <4 x i32> %194, <i32 12, i32 12, i32 12, i32 12>
  %196 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %193, <4 x i32> %193) #8
  %197 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %195, <4 x i32> %195) #8
  %198 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %184, <8 x i16> %197) #8
  %199 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %184, <8 x i16> %197) #8
  %200 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %183, <8 x i16> %196) #8
  %201 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %183, <8 x i16> %196) #8
  %202 = bitcast <2 x i64> %147 to <8 x i16>
  %203 = bitcast <2 x i64> %171 to <8 x i16>
  %204 = shufflevector <8 x i16> %202, <8 x i16> %203, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %205 = shufflevector <8 x i16> %203, <8 x i16> %202, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %206 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %207 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %204, <8 x i16> %206) #8
  %208 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %205, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %209 = add <4 x i32> %207, <i32 2048, i32 2048, i32 2048, i32 2048>
  %210 = ashr <4 x i32> %209, <i32 12, i32 12, i32 12, i32 12>
  %211 = add <4 x i32> %208, <i32 2048, i32 2048, i32 2048, i32 2048>
  %212 = ashr <4 x i32> %211, <i32 12, i32 12, i32 12, i32 12>
  %213 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %210, <4 x i32> %210) #8
  %214 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %212, <4 x i32> %212) #8
  %215 = bitcast <2 x i64> %163 to <8 x i16>
  %216 = bitcast <2 x i64> %155 to <8 x i16>
  %217 = shufflevector <8 x i16> %215, <8 x i16> %216, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %218 = shufflevector <8 x i16> %216, <8 x i16> %215, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %219 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %220 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %217, <8 x i16> %219) #8
  %221 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %218, <8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>) #8
  %222 = add <4 x i32> %220, <i32 2048, i32 2048, i32 2048, i32 2048>
  %223 = ashr <4 x i32> %222, <i32 12, i32 12, i32 12, i32 12>
  %224 = add <4 x i32> %221, <i32 2048, i32 2048, i32 2048, i32 2048>
  %225 = ashr <4 x i32> %224, <i32 12, i32 12, i32 12, i32 12>
  %226 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %223, <4 x i32> %223) #8
  %227 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %225, <4 x i32> %225) #8
  %228 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %213, <8 x i16> %226) #8
  %229 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %213, <8 x i16> %226) #8
  %230 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %214, <8 x i16> %227) #8
  %231 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %214, <8 x i16> %227) #8
  %232 = shufflevector <8 x i16> %231, <8 x i16> %229, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %233 = shufflevector <8 x i16> %229, <8 x i16> %231, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %234 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %232, <8 x i16> %176) #8
  %235 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %233, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %236 = add <4 x i32> %234, <i32 2048, i32 2048, i32 2048, i32 2048>
  %237 = ashr <4 x i32> %236, <i32 12, i32 12, i32 12, i32 12>
  %238 = add <4 x i32> %235, <i32 2048, i32 2048, i32 2048, i32 2048>
  %239 = ashr <4 x i32> %238, <i32 12, i32 12, i32 12, i32 12>
  %240 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %237, <4 x i32> %237) #8
  %241 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %239, <4 x i32> %239) #8
  %242 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %198, <8 x i16> %230) #8
  %243 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %198, <8 x i16> %230) #8
  %244 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %200, <8 x i16> %241) #8
  %245 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %200, <8 x i16> %241) #8
  %246 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %201, <8 x i16> %240) #8
  %247 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %201, <8 x i16> %240) #8
  %248 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %199, <8 x i16> %228) #8
  %249 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %199, <8 x i16> %228) #8
  %250 = bitcast <8 x i16> %249 to <2 x i64>
  %251 = bitcast <8 x i16> %248 to <2 x i64>
  %252 = bitcast <8 x i16> %247 to <2 x i64>
  %253 = bitcast <8 x i16> %246 to <2 x i64>
  %254 = bitcast <8 x i16> %245 to <2 x i64>
  %255 = bitcast <8 x i16> %244 to <2 x i64>
  %256 = bitcast <8 x i16> %243 to <2 x i64>
  %257 = bitcast <8 x i16> %242 to <2 x i64>
  %258 = extractelement <2 x i64> %257, i32 0
  store i64 %258, i64* %141, align 1
  %259 = extractelement <2 x i64> %255, i32 0
  store i64 %259, i64* %145, align 1
  %260 = extractelement <2 x i64> %253, i32 0
  store i64 %260, i64* %149, align 1
  %261 = extractelement <2 x i64> %251, i32 0
  store i64 %261, i64* %153, align 1
  %262 = extractelement <2 x i64> %250, i32 0
  store i64 %262, i64* %157, align 1
  %263 = extractelement <2 x i64> %252, i32 0
  store i64 %263, i64* %161, align 1
  %264 = extractelement <2 x i64> %254, i32 0
  store i64 %264, i64* %165, align 1
  %265 = extractelement <2 x i64> %256, i32 0
  store i64 %265, i64* %169, align 1
  %266 = bitcast i8* %6 to i64*
  %267 = load i64, i64* %266, align 8
  %268 = getelementptr inbounds i8, i8* %6, i64 8
  %269 = bitcast i8* %268 to i8**
  %270 = load i8*, i8** %269, align 8
  %271 = sext i32 %5 to i64
  %272 = ashr i64 %267, 32
  %273 = mul nsw i64 %272, %271
  %274 = getelementptr inbounds i8, i8* %270, i64 %273
  %275 = sext i32 %4 to i64
  %276 = getelementptr inbounds i8, i8* %274, i64 %275
  br label %553

277:                                              ; preds = %127, %277
  %278 = phi i64 [ 0, %127 ], [ %409, %277 ]
  %279 = getelementptr inbounds i16, i16* %8, i64 %278
  %280 = bitcast i16* %279 to <8 x i16>*
  %281 = load <8 x i16>, <8 x i16>* %280, align 1
  %282 = getelementptr inbounds i16, i16* %279, i64 %128
  %283 = bitcast i16* %282 to <8 x i16>*
  %284 = load <8 x i16>, <8 x i16>* %283, align 1
  %285 = getelementptr inbounds i16, i16* %279, i64 %129
  %286 = bitcast i16* %285 to <8 x i16>*
  %287 = load <8 x i16>, <8 x i16>* %286, align 1
  %288 = getelementptr inbounds i16, i16* %279, i64 %130
  %289 = bitcast i16* %288 to <8 x i16>*
  %290 = load <8 x i16>, <8 x i16>* %289, align 1
  %291 = getelementptr inbounds i16, i16* %279, i64 %131
  %292 = bitcast i16* %291 to <8 x i16>*
  %293 = load <8 x i16>, <8 x i16>* %292, align 1
  %294 = getelementptr inbounds i16, i16* %279, i64 %132
  %295 = bitcast i16* %294 to <8 x i16>*
  %296 = load <8 x i16>, <8 x i16>* %295, align 1
  %297 = getelementptr inbounds i16, i16* %279, i64 %133
  %298 = bitcast i16* %297 to <8 x i16>*
  %299 = load <8 x i16>, <8 x i16>* %298, align 1
  %300 = getelementptr inbounds i16, i16* %279, i64 %134
  %301 = bitcast i16* %300 to <8 x i16>*
  %302 = load <8 x i16>, <8 x i16>* %301, align 1
  %303 = shufflevector <8 x i16> %281, <8 x i16> %293, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %304 = shufflevector <8 x i16> %293, <8 x i16> %281, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %305 = shufflevector <8 x i16> %281, <8 x i16> %293, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %306 = shufflevector <8 x i16> %293, <8 x i16> %281, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %307 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %303, <8 x i16> %135) #8
  %308 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %304, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %309 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %305, <8 x i16> %135) #8
  %310 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %306, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %311 = add <4 x i32> %307, <i32 2048, i32 2048, i32 2048, i32 2048>
  %312 = ashr <4 x i32> %311, <i32 12, i32 12, i32 12, i32 12>
  %313 = add <4 x i32> %308, <i32 2048, i32 2048, i32 2048, i32 2048>
  %314 = ashr <4 x i32> %313, <i32 12, i32 12, i32 12, i32 12>
  %315 = add <4 x i32> %309, <i32 2048, i32 2048, i32 2048, i32 2048>
  %316 = ashr <4 x i32> %315, <i32 12, i32 12, i32 12, i32 12>
  %317 = add <4 x i32> %310, <i32 2048, i32 2048, i32 2048, i32 2048>
  %318 = ashr <4 x i32> %317, <i32 12, i32 12, i32 12, i32 12>
  %319 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %312, <4 x i32> %316) #8
  %320 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %314, <4 x i32> %318) #8
  %321 = shufflevector <8 x i16> %287, <8 x i16> %299, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %322 = shufflevector <8 x i16> %299, <8 x i16> %287, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %323 = shufflevector <8 x i16> %287, <8 x i16> %299, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %324 = shufflevector <8 x i16> %299, <8 x i16> %287, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %325 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %321, <8 x i16> %136) #8
  %326 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %322, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %327 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %323, <8 x i16> %136) #8
  %328 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %324, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %329 = add <4 x i32> %325, <i32 2048, i32 2048, i32 2048, i32 2048>
  %330 = ashr <4 x i32> %329, <i32 12, i32 12, i32 12, i32 12>
  %331 = add <4 x i32> %326, <i32 2048, i32 2048, i32 2048, i32 2048>
  %332 = ashr <4 x i32> %331, <i32 12, i32 12, i32 12, i32 12>
  %333 = add <4 x i32> %327, <i32 2048, i32 2048, i32 2048, i32 2048>
  %334 = ashr <4 x i32> %333, <i32 12, i32 12, i32 12, i32 12>
  %335 = add <4 x i32> %328, <i32 2048, i32 2048, i32 2048, i32 2048>
  %336 = ashr <4 x i32> %335, <i32 12, i32 12, i32 12, i32 12>
  %337 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %330, <4 x i32> %334) #8
  %338 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %332, <4 x i32> %336) #8
  %339 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %320, <8 x i16> %338) #8
  %340 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %320, <8 x i16> %338) #8
  %341 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %319, <8 x i16> %337) #8
  %342 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %319, <8 x i16> %337) #8
  %343 = shufflevector <8 x i16> %284, <8 x i16> %302, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %344 = shufflevector <8 x i16> %302, <8 x i16> %284, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %345 = shufflevector <8 x i16> %284, <8 x i16> %302, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %346 = shufflevector <8 x i16> %302, <8 x i16> %284, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %347 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %343, <8 x i16> %137) #8
  %348 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %344, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %349 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %345, <8 x i16> %137) #8
  %350 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %346, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %351 = add <4 x i32> %347, <i32 2048, i32 2048, i32 2048, i32 2048>
  %352 = ashr <4 x i32> %351, <i32 12, i32 12, i32 12, i32 12>
  %353 = add <4 x i32> %348, <i32 2048, i32 2048, i32 2048, i32 2048>
  %354 = ashr <4 x i32> %353, <i32 12, i32 12, i32 12, i32 12>
  %355 = add <4 x i32> %349, <i32 2048, i32 2048, i32 2048, i32 2048>
  %356 = ashr <4 x i32> %355, <i32 12, i32 12, i32 12, i32 12>
  %357 = add <4 x i32> %350, <i32 2048, i32 2048, i32 2048, i32 2048>
  %358 = ashr <4 x i32> %357, <i32 12, i32 12, i32 12, i32 12>
  %359 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %352, <4 x i32> %356) #8
  %360 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %354, <4 x i32> %358) #8
  %361 = shufflevector <8 x i16> %296, <8 x i16> %290, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %362 = shufflevector <8 x i16> %290, <8 x i16> %296, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %363 = shufflevector <8 x i16> %296, <8 x i16> %290, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %364 = shufflevector <8 x i16> %290, <8 x i16> %296, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %365 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %361, <8 x i16> %138) #8
  %366 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %362, <8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>) #8
  %367 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %363, <8 x i16> %138) #8
  %368 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %364, <8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>) #8
  %369 = add <4 x i32> %365, <i32 2048, i32 2048, i32 2048, i32 2048>
  %370 = ashr <4 x i32> %369, <i32 12, i32 12, i32 12, i32 12>
  %371 = add <4 x i32> %366, <i32 2048, i32 2048, i32 2048, i32 2048>
  %372 = ashr <4 x i32> %371, <i32 12, i32 12, i32 12, i32 12>
  %373 = add <4 x i32> %367, <i32 2048, i32 2048, i32 2048, i32 2048>
  %374 = ashr <4 x i32> %373, <i32 12, i32 12, i32 12, i32 12>
  %375 = add <4 x i32> %368, <i32 2048, i32 2048, i32 2048, i32 2048>
  %376 = ashr <4 x i32> %375, <i32 12, i32 12, i32 12, i32 12>
  %377 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %370, <4 x i32> %374) #8
  %378 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %372, <4 x i32> %376) #8
  %379 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %359, <8 x i16> %377) #8
  %380 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %359, <8 x i16> %377) #8
  %381 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %360, <8 x i16> %378) #8
  %382 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %360, <8 x i16> %378) #8
  %383 = shufflevector <8 x i16> %382, <8 x i16> %380, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %384 = shufflevector <8 x i16> %380, <8 x i16> %382, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %385 = shufflevector <8 x i16> %382, <8 x i16> %380, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %386 = shufflevector <8 x i16> %380, <8 x i16> %382, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %387 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %383, <8 x i16> %135) #8
  %388 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %384, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %389 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %385, <8 x i16> %135) #8
  %390 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %386, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %391 = add <4 x i32> %387, <i32 2048, i32 2048, i32 2048, i32 2048>
  %392 = ashr <4 x i32> %391, <i32 12, i32 12, i32 12, i32 12>
  %393 = add <4 x i32> %388, <i32 2048, i32 2048, i32 2048, i32 2048>
  %394 = ashr <4 x i32> %393, <i32 12, i32 12, i32 12, i32 12>
  %395 = add <4 x i32> %389, <i32 2048, i32 2048, i32 2048, i32 2048>
  %396 = ashr <4 x i32> %395, <i32 12, i32 12, i32 12, i32 12>
  %397 = add <4 x i32> %390, <i32 2048, i32 2048, i32 2048, i32 2048>
  %398 = ashr <4 x i32> %397, <i32 12, i32 12, i32 12, i32 12>
  %399 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %392, <4 x i32> %396) #8
  %400 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %394, <4 x i32> %398) #8
  %401 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %339, <8 x i16> %381) #8
  %402 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %339, <8 x i16> %381) #8
  %403 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %341, <8 x i16> %400) #8
  %404 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %341, <8 x i16> %400) #8
  %405 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %342, <8 x i16> %399) #8
  %406 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %342, <8 x i16> %399) #8
  %407 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %340, <8 x i16> %379) #8
  %408 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %340, <8 x i16> %379) #8
  store <8 x i16> %401, <8 x i16>* %280, align 1
  store <8 x i16> %403, <8 x i16>* %283, align 1
  store <8 x i16> %405, <8 x i16>* %286, align 1
  store <8 x i16> %407, <8 x i16>* %289, align 1
  store <8 x i16> %408, <8 x i16>* %292, align 1
  store <8 x i16> %406, <8 x i16>* %295, align 1
  store <8 x i16> %404, <8 x i16>* %298, align 1
  store <8 x i16> %402, <8 x i16>* %301, align 1
  %409 = add nuw nsw i64 %278, 8
  %410 = icmp ult i64 %409, %139
  br i1 %410, label %277, label %411

411:                                              ; preds = %277, %104
  %412 = bitcast i8* %6 to i64*
  %413 = load i64, i64* %412, align 8
  %414 = getelementptr inbounds i8, i8* %6, i64 8
  %415 = bitcast i8* %414 to i8**
  %416 = load i8*, i8** %415, align 8
  %417 = sext i32 %5 to i64
  %418 = ashr i64 %413, 32
  %419 = mul nsw i64 %418, %417
  %420 = getelementptr inbounds i8, i8* %416, i64 %419
  %421 = sext i32 %4 to i64
  %422 = getelementptr inbounds i8, i8* %420, i64 %421
  switch i8 %11, label %697 [
    i8 4, label %423
    i8 8, label %426
  ]

423:                                              ; preds = %411
  %424 = bitcast i8* %3 to i64*
  %425 = load i64, i64* %424, align 1
  br label %553

426:                                              ; preds = %411
  %427 = bitcast i8* %3 to <8 x i16>*
  %428 = load <8 x i16>, <8 x i16>* %427, align 1
  %429 = bitcast i8* %422 to i64*
  %430 = load i64, i64* %429, align 1
  %431 = insertelement <2 x i64> undef, i64 %430, i32 0
  %432 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %428, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %433 = ashr <8 x i16> %432, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %434 = bitcast <2 x i64> %431 to <16 x i8>
  %435 = shufflevector <16 x i8> %434, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %436 = zext <8 x i8> %435 to <8 x i16>
  %437 = add nsw <8 x i16> %433, %436
  %438 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %437, <8 x i16> undef) #8
  %439 = bitcast <16 x i8> %438 to <2 x i64>
  %440 = extractelement <2 x i64> %439, i32 0
  store i64 %440, i64* %429, align 1
  %441 = getelementptr inbounds i8, i8* %422, i64 %418
  %442 = getelementptr inbounds i8, i8* %3, i64 16
  %443 = bitcast i8* %442 to <8 x i16>*
  %444 = load <8 x i16>, <8 x i16>* %443, align 1
  %445 = bitcast i8* %441 to i64*
  %446 = load i64, i64* %445, align 1
  %447 = insertelement <2 x i64> undef, i64 %446, i32 0
  %448 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %444, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %449 = ashr <8 x i16> %448, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %450 = bitcast <2 x i64> %447 to <16 x i8>
  %451 = shufflevector <16 x i8> %450, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %452 = zext <8 x i8> %451 to <8 x i16>
  %453 = add nsw <8 x i16> %449, %452
  %454 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %453, <8 x i16> undef) #8
  %455 = bitcast <16 x i8> %454 to <2 x i64>
  %456 = extractelement <2 x i64> %455, i32 0
  store i64 %456, i64* %445, align 1
  %457 = getelementptr inbounds i8, i8* %441, i64 %418
  %458 = getelementptr inbounds i8, i8* %3, i64 32
  %459 = bitcast i8* %458 to <8 x i16>*
  %460 = load <8 x i16>, <8 x i16>* %459, align 1
  %461 = bitcast i8* %457 to i64*
  %462 = load i64, i64* %461, align 1
  %463 = insertelement <2 x i64> undef, i64 %462, i32 0
  %464 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %460, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %465 = ashr <8 x i16> %464, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %466 = bitcast <2 x i64> %463 to <16 x i8>
  %467 = shufflevector <16 x i8> %466, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %468 = zext <8 x i8> %467 to <8 x i16>
  %469 = add nsw <8 x i16> %465, %468
  %470 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %469, <8 x i16> undef) #8
  %471 = bitcast <16 x i8> %470 to <2 x i64>
  %472 = extractelement <2 x i64> %471, i32 0
  store i64 %472, i64* %461, align 1
  %473 = getelementptr inbounds i8, i8* %457, i64 %418
  %474 = getelementptr inbounds i8, i8* %3, i64 48
  %475 = bitcast i8* %474 to <8 x i16>*
  %476 = load <8 x i16>, <8 x i16>* %475, align 1
  %477 = bitcast i8* %473 to i64*
  %478 = load i64, i64* %477, align 1
  %479 = insertelement <2 x i64> undef, i64 %478, i32 0
  %480 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %476, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %481 = ashr <8 x i16> %480, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %482 = bitcast <2 x i64> %479 to <16 x i8>
  %483 = shufflevector <16 x i8> %482, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %484 = zext <8 x i8> %483 to <8 x i16>
  %485 = add nsw <8 x i16> %481, %484
  %486 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %485, <8 x i16> undef) #8
  %487 = bitcast <16 x i8> %486 to <2 x i64>
  %488 = extractelement <2 x i64> %487, i32 0
  store i64 %488, i64* %477, align 1
  %489 = getelementptr inbounds i8, i8* %473, i64 %418
  %490 = getelementptr inbounds i8, i8* %3, i64 64
  %491 = bitcast i8* %490 to <8 x i16>*
  %492 = load <8 x i16>, <8 x i16>* %491, align 1
  %493 = bitcast i8* %489 to i64*
  %494 = load i64, i64* %493, align 1
  %495 = insertelement <2 x i64> undef, i64 %494, i32 0
  %496 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %492, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %497 = ashr <8 x i16> %496, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %498 = bitcast <2 x i64> %495 to <16 x i8>
  %499 = shufflevector <16 x i8> %498, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %500 = zext <8 x i8> %499 to <8 x i16>
  %501 = add nsw <8 x i16> %497, %500
  %502 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %501, <8 x i16> undef) #8
  %503 = bitcast <16 x i8> %502 to <2 x i64>
  %504 = extractelement <2 x i64> %503, i32 0
  store i64 %504, i64* %493, align 1
  %505 = getelementptr inbounds i8, i8* %489, i64 %418
  %506 = getelementptr inbounds i8, i8* %3, i64 80
  %507 = bitcast i8* %506 to <8 x i16>*
  %508 = load <8 x i16>, <8 x i16>* %507, align 1
  %509 = bitcast i8* %505 to i64*
  %510 = load i64, i64* %509, align 1
  %511 = insertelement <2 x i64> undef, i64 %510, i32 0
  %512 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %508, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %513 = ashr <8 x i16> %512, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %514 = bitcast <2 x i64> %511 to <16 x i8>
  %515 = shufflevector <16 x i8> %514, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %516 = zext <8 x i8> %515 to <8 x i16>
  %517 = add nsw <8 x i16> %513, %516
  %518 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %517, <8 x i16> undef) #8
  %519 = bitcast <16 x i8> %518 to <2 x i64>
  %520 = extractelement <2 x i64> %519, i32 0
  store i64 %520, i64* %509, align 1
  %521 = getelementptr inbounds i8, i8* %505, i64 %418
  %522 = getelementptr inbounds i8, i8* %3, i64 96
  %523 = bitcast i8* %522 to <8 x i16>*
  %524 = load <8 x i16>, <8 x i16>* %523, align 1
  %525 = bitcast i8* %521 to i64*
  %526 = load i64, i64* %525, align 1
  %527 = insertelement <2 x i64> undef, i64 %526, i32 0
  %528 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %524, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %529 = ashr <8 x i16> %528, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %530 = bitcast <2 x i64> %527 to <16 x i8>
  %531 = shufflevector <16 x i8> %530, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %532 = zext <8 x i8> %531 to <8 x i16>
  %533 = add nsw <8 x i16> %529, %532
  %534 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %533, <8 x i16> undef) #8
  %535 = bitcast <16 x i8> %534 to <2 x i64>
  %536 = extractelement <2 x i64> %535, i32 0
  store i64 %536, i64* %525, align 1
  %537 = getelementptr inbounds i8, i8* %521, i64 %418
  %538 = getelementptr inbounds i8, i8* %3, i64 112
  %539 = bitcast i8* %538 to <8 x i16>*
  %540 = load <8 x i16>, <8 x i16>* %539, align 1
  %541 = bitcast i8* %537 to i64*
  %542 = load i64, i64* %541, align 1
  %543 = insertelement <2 x i64> undef, i64 %542, i32 0
  %544 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %540, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %545 = ashr <8 x i16> %544, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %546 = bitcast <2 x i64> %543 to <16 x i8>
  %547 = shufflevector <16 x i8> %546, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %548 = zext <8 x i8> %547 to <8 x i16>
  %549 = add nsw <8 x i16> %545, %548
  %550 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %549, <8 x i16> undef) #8
  %551 = bitcast <16 x i8> %550 to <2 x i64>
  %552 = extractelement <2 x i64> %551, i32 0
  store i64 %552, i64* %541, align 1
  br label %735

553:                                              ; preds = %423, %140
  %554 = phi i64 [ %258, %140 ], [ %425, %423 ]
  %555 = phi i8* [ %276, %140 ], [ %422, %423 ]
  %556 = phi i64 [ %272, %140 ], [ %418, %423 ]
  %557 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %554, i32 0
  %558 = bitcast i8* %555 to i32*
  %559 = load i32, i32* %558, align 1
  %560 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %559, i32 0
  %561 = bitcast <2 x i64> %557 to <8 x i16>
  %562 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %561, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %563 = ashr <8 x i16> %562, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %564 = bitcast <4 x i32> %560 to <16 x i8>
  %565 = shufflevector <16 x i8> %564, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %566 = zext <8 x i8> %565 to <8 x i16>
  %567 = add nsw <8 x i16> %563, %566
  %568 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %567, <8 x i16> undef) #8
  %569 = bitcast <16 x i8> %568 to <4 x i32>
  %570 = extractelement <4 x i32> %569, i32 0
  store i32 %570, i32* %558, align 1
  %571 = getelementptr inbounds i8, i8* %555, i64 %556
  %572 = getelementptr inbounds i8, i8* %3, i64 8
  %573 = bitcast i8* %572 to i64*
  %574 = load i64, i64* %573, align 1
  %575 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %574, i32 0
  %576 = bitcast i8* %571 to i32*
  %577 = load i32, i32* %576, align 1
  %578 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %577, i32 0
  %579 = bitcast <2 x i64> %575 to <8 x i16>
  %580 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %579, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %581 = ashr <8 x i16> %580, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %582 = bitcast <4 x i32> %578 to <16 x i8>
  %583 = shufflevector <16 x i8> %582, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %584 = zext <8 x i8> %583 to <8 x i16>
  %585 = add nsw <8 x i16> %581, %584
  %586 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %585, <8 x i16> undef) #8
  %587 = bitcast <16 x i8> %586 to <4 x i32>
  %588 = extractelement <4 x i32> %587, i32 0
  store i32 %588, i32* %576, align 1
  %589 = getelementptr inbounds i8, i8* %571, i64 %556
  %590 = getelementptr inbounds i8, i8* %3, i64 16
  %591 = bitcast i8* %590 to i64*
  %592 = load i64, i64* %591, align 1
  %593 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %592, i32 0
  %594 = bitcast i8* %589 to i32*
  %595 = load i32, i32* %594, align 1
  %596 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %595, i32 0
  %597 = bitcast <2 x i64> %593 to <8 x i16>
  %598 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %597, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %599 = ashr <8 x i16> %598, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %600 = bitcast <4 x i32> %596 to <16 x i8>
  %601 = shufflevector <16 x i8> %600, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %602 = zext <8 x i8> %601 to <8 x i16>
  %603 = add nsw <8 x i16> %599, %602
  %604 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %603, <8 x i16> undef) #8
  %605 = bitcast <16 x i8> %604 to <4 x i32>
  %606 = extractelement <4 x i32> %605, i32 0
  store i32 %606, i32* %594, align 1
  %607 = getelementptr inbounds i8, i8* %589, i64 %556
  %608 = getelementptr inbounds i8, i8* %3, i64 24
  %609 = bitcast i8* %608 to i64*
  %610 = load i64, i64* %609, align 1
  %611 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %610, i32 0
  %612 = bitcast i8* %607 to i32*
  %613 = load i32, i32* %612, align 1
  %614 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %613, i32 0
  %615 = bitcast <2 x i64> %611 to <8 x i16>
  %616 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %615, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %617 = ashr <8 x i16> %616, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %618 = bitcast <4 x i32> %614 to <16 x i8>
  %619 = shufflevector <16 x i8> %618, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %620 = zext <8 x i8> %619 to <8 x i16>
  %621 = add nsw <8 x i16> %617, %620
  %622 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %621, <8 x i16> undef) #8
  %623 = bitcast <16 x i8> %622 to <4 x i32>
  %624 = extractelement <4 x i32> %623, i32 0
  store i32 %624, i32* %612, align 1
  %625 = getelementptr inbounds i8, i8* %607, i64 %556
  %626 = getelementptr inbounds i8, i8* %3, i64 32
  %627 = bitcast i8* %626 to i64*
  %628 = load i64, i64* %627, align 1
  %629 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %628, i32 0
  %630 = bitcast i8* %625 to i32*
  %631 = load i32, i32* %630, align 1
  %632 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %631, i32 0
  %633 = bitcast <2 x i64> %629 to <8 x i16>
  %634 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %633, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %635 = ashr <8 x i16> %634, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %636 = bitcast <4 x i32> %632 to <16 x i8>
  %637 = shufflevector <16 x i8> %636, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %638 = zext <8 x i8> %637 to <8 x i16>
  %639 = add nsw <8 x i16> %635, %638
  %640 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %639, <8 x i16> undef) #8
  %641 = bitcast <16 x i8> %640 to <4 x i32>
  %642 = extractelement <4 x i32> %641, i32 0
  store i32 %642, i32* %630, align 1
  %643 = getelementptr inbounds i8, i8* %625, i64 %556
  %644 = getelementptr inbounds i8, i8* %3, i64 40
  %645 = bitcast i8* %644 to i64*
  %646 = load i64, i64* %645, align 1
  %647 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %646, i32 0
  %648 = bitcast i8* %643 to i32*
  %649 = load i32, i32* %648, align 1
  %650 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %649, i32 0
  %651 = bitcast <2 x i64> %647 to <8 x i16>
  %652 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %651, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %653 = ashr <8 x i16> %652, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %654 = bitcast <4 x i32> %650 to <16 x i8>
  %655 = shufflevector <16 x i8> %654, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %656 = zext <8 x i8> %655 to <8 x i16>
  %657 = add nsw <8 x i16> %653, %656
  %658 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %657, <8 x i16> undef) #8
  %659 = bitcast <16 x i8> %658 to <4 x i32>
  %660 = extractelement <4 x i32> %659, i32 0
  store i32 %660, i32* %648, align 1
  %661 = getelementptr inbounds i8, i8* %643, i64 %556
  %662 = getelementptr inbounds i8, i8* %3, i64 48
  %663 = bitcast i8* %662 to i64*
  %664 = load i64, i64* %663, align 1
  %665 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %664, i32 0
  %666 = bitcast i8* %661 to i32*
  %667 = load i32, i32* %666, align 1
  %668 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %667, i32 0
  %669 = bitcast <2 x i64> %665 to <8 x i16>
  %670 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %669, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %671 = ashr <8 x i16> %670, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %672 = bitcast <4 x i32> %668 to <16 x i8>
  %673 = shufflevector <16 x i8> %672, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %674 = zext <8 x i8> %673 to <8 x i16>
  %675 = add nsw <8 x i16> %671, %674
  %676 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %675, <8 x i16> undef) #8
  %677 = bitcast <16 x i8> %676 to <4 x i32>
  %678 = extractelement <4 x i32> %677, i32 0
  store i32 %678, i32* %666, align 1
  %679 = getelementptr inbounds i8, i8* %661, i64 %556
  %680 = getelementptr inbounds i8, i8* %3, i64 56
  %681 = bitcast i8* %680 to i64*
  %682 = load i64, i64* %681, align 1
  %683 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %682, i32 0
  %684 = bitcast i8* %679 to i32*
  %685 = load i32, i32* %684, align 1
  %686 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %685, i32 0
  %687 = bitcast <2 x i64> %683 to <8 x i16>
  %688 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %687, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %689 = ashr <8 x i16> %688, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %690 = bitcast <4 x i32> %686 to <16 x i8>
  %691 = shufflevector <16 x i8> %690, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %692 = zext <8 x i8> %691 to <8 x i16>
  %693 = add nsw <8 x i16> %689, %692
  %694 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %693, <8 x i16> undef) #8
  %695 = bitcast <16 x i8> %694 to <4 x i32>
  %696 = extractelement <4 x i32> %695, i32 0
  store i32 %696, i32* %684, align 1
  br label %735

697:                                              ; preds = %411
  %698 = zext i8 %11 to i64
  br label %699

699:                                              ; preds = %732, %697
  %700 = phi i64 [ 0, %697 ], [ %733, %732 ]
  %701 = add nsw i64 %700, %417
  %702 = mul nuw nsw i64 %700, %698
  %703 = mul nsw i64 %701, %418
  %704 = getelementptr inbounds i8, i8* %416, i64 %703
  br label %705

705:                                              ; preds = %705, %699
  %706 = phi i64 [ %730, %705 ], [ 0, %699 ]
  %707 = add nsw i64 %706, %421
  %708 = add nuw nsw i64 %706, %702
  %709 = getelementptr inbounds i16, i16* %8, i64 %708
  %710 = bitcast i16* %709 to <8 x i16>*
  %711 = load <8 x i16>, <8 x i16>* %710, align 1
  %712 = add nuw nsw i64 %708, 8
  %713 = getelementptr inbounds i16, i16* %8, i64 %712
  %714 = bitcast i16* %713 to <8 x i16>*
  %715 = load <8 x i16>, <8 x i16>* %714, align 1
  %716 = getelementptr inbounds i8, i8* %704, i64 %707
  %717 = bitcast i8* %716 to <16 x i8>*
  %718 = load <16 x i8>, <16 x i8>* %717, align 1
  %719 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %711, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %720 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %715, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %721 = ashr <8 x i16> %719, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %722 = ashr <8 x i16> %720, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %723 = shufflevector <16 x i8> %718, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %724 = zext <8 x i8> %723 to <8 x i16>
  %725 = shufflevector <16 x i8> %718, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %726 = zext <8 x i8> %725 to <8 x i16>
  %727 = add nsw <8 x i16> %721, %724
  %728 = add nsw <8 x i16> %722, %726
  %729 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %727, <8 x i16> %728) #8
  store <16 x i8> %729, <16 x i8>* %717, align 1
  %730 = add nuw nsw i64 %706, 16
  %731 = icmp ult i64 %730, %698
  br i1 %731, label %705, label %732

732:                                              ; preds = %705
  %733 = add nuw nsw i64 %700, 1
  %734 = icmp eq i64 %733, 8
  br i1 %734, label %735, label %699

735:                                              ; preds = %732, %426, %553
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_128Dct16TransformLoopRow_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_(i8 zeroext, i8 zeroext, i32, i8*, i32, i32, i8* nocapture readnone) #4 {
  %8 = bitcast i8* %3 to i16*
  %9 = zext i8 %1 to i64
  %10 = lshr i64 173354, %9
  %11 = and i64 %10, 1
  %12 = icmp ne i64 %11, 0
  %13 = getelementptr inbounds [19 x i8], [19 x i8]* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_118kTransformRowShiftE, i64 0, i64 %9
  %14 = load i8, i8* %13, align 1
  %15 = icmp sgt i32 %2, 1
  br i1 %15, label %56, label %16

16:                                               ; preds = %7
  %17 = zext i8 %14 to i32
  %18 = load i16, i16* %8, align 2
  %19 = sext i16 %18 to i32
  %20 = insertelement <4 x i32> undef, i32 %19, i32 0
  %21 = bitcast <4 x i32> %20 to <8 x i16>
  %22 = shufflevector <8 x i16> %21, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %23 = bitcast <8 x i16> %22 to <4 x i32>
  %24 = shufflevector <4 x i32> %23, <4 x i32> undef, <4 x i32> zeroinitializer
  %25 = sext i1 %12 to i16
  %26 = insertelement <8 x i16> undef, i16 %25, i32 0
  %27 = shufflevector <8 x i16> %26, <8 x i16> undef, <8 x i32> zeroinitializer
  %28 = bitcast <4 x i32> %24 to <8 x i16>
  %29 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %28, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  %30 = bitcast <4 x i32> %24 to <16 x i8>
  %31 = bitcast <8 x i16> %29 to <16 x i8>
  %32 = bitcast <8 x i16> %27 to <16 x i8>
  %33 = tail call <16 x i8> @llvm.x86.sse41.pblendvb(<16 x i8> %30, <16 x i8> %31, <16 x i8> %32) #8
  %34 = bitcast <16 x i8> %33 to <8 x i16>
  %35 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %34, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  %36 = insertelement <4 x i32> undef, i32 %17, i32 0
  %37 = shufflevector <4 x i32> %36, <4 x i32> undef, <4 x i32> zeroinitializer
  %38 = shufflevector <4 x i32> %36, <4 x i32> undef, <2 x i32> zeroinitializer
  %39 = zext <2 x i32> %38 to <2 x i64>
  %40 = shufflevector <8 x i16> %35, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %41 = sext <4 x i16> %40 to <4 x i32>
  %42 = bitcast <8 x i16> %35 to <16 x i8>
  %43 = shufflevector <16 x i8> %42, <16 x i8> undef, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %44 = bitcast <16 x i8> %43 to <8 x i16>
  %45 = shufflevector <8 x i16> %44, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %46 = sext <4 x i16> %45 to <4 x i32>
  %47 = add <4 x i32> %37, %41
  %48 = add <4 x i32> %37, %46
  %49 = bitcast <2 x i64> %39 to <4 x i32>
  %50 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %47, <4 x i32> %49) #8
  %51 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %48, <4 x i32> %49) #8
  %52 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %50, <4 x i32> %51) #8
  %53 = bitcast i8* %3 to <8 x i16>*
  store <8 x i16> %52, <8 x i16>* %53, align 1
  %54 = getelementptr inbounds i8, i8* %3, i64 16
  %55 = bitcast i8* %54 to <8 x i16>*
  store <8 x i16> %52, <8 x i16>* %55, align 1
  br label %1004

56:                                               ; preds = %7
  br i1 %12, label %57, label %108

57:                                               ; preds = %56
  %58 = sext i32 %2 to i64
  %59 = and i64 %58, 1
  %60 = icmp eq i32 %2, 1
  br i1 %60, label %95, label %61

61:                                               ; preds = %57
  %62 = sub nsw i64 %58, %59
  br label %63

63:                                               ; preds = %63, %61
  %64 = phi i64 [ 0, %61 ], [ %89, %63 ]
  %65 = phi i64 [ %62, %61 ], [ %90, %63 ]
  %66 = shl i64 %64, 4
  %67 = and i64 %66, 4294967264
  %68 = getelementptr inbounds i16, i16* %8, i64 %67
  %69 = bitcast i16* %68 to <8 x i16>*
  %70 = load <8 x i16>, <8 x i16>* %69, align 1
  %71 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %70, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %71, <8 x i16>* %69, align 1
  %72 = or i64 %67, 8
  %73 = getelementptr inbounds i16, i16* %8, i64 %72
  %74 = bitcast i16* %73 to <8 x i16>*
  %75 = load <8 x i16>, <8 x i16>* %74, align 1
  %76 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %75, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %76, <8 x i16>* %74, align 1
  %77 = shl i64 %64, 4
  %78 = and i64 %77, 4294967264
  %79 = or i64 %78, 16
  %80 = getelementptr inbounds i16, i16* %8, i64 %79
  %81 = bitcast i16* %80 to <8 x i16>*
  %82 = load <8 x i16>, <8 x i16>* %81, align 1
  %83 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %82, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %83, <8 x i16>* %81, align 1
  %84 = or i64 %78, 24
  %85 = getelementptr inbounds i16, i16* %8, i64 %84
  %86 = bitcast i16* %85 to <8 x i16>*
  %87 = load <8 x i16>, <8 x i16>* %86, align 1
  %88 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %87, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %88, <8 x i16>* %86, align 1
  %89 = add nuw nsw i64 %64, 2
  %90 = add i64 %65, -2
  %91 = icmp eq i64 %90, 0
  br i1 %91, label %92, label %63

92:                                               ; preds = %63
  %93 = shl i64 %89, 4
  %94 = and i64 %93, 4294967264
  br label %95

95:                                               ; preds = %92, %57
  %96 = phi i64 [ 0, %57 ], [ %94, %92 ]
  %97 = icmp eq i64 %59, 0
  br i1 %97, label %108, label %98

98:                                               ; preds = %95
  %99 = getelementptr inbounds i16, i16* %8, i64 %96
  %100 = bitcast i16* %99 to <8 x i16>*
  %101 = load <8 x i16>, <8 x i16>* %100, align 1
  %102 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %101, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %102, <8 x i16>* %100, align 1
  %103 = or i64 %96, 8
  %104 = getelementptr inbounds i16, i16* %8, i64 %103
  %105 = bitcast i16* %104 to <8 x i16>*
  %106 = load <8 x i16>, <8 x i16>* %105, align 1
  %107 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %106, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %107, <8 x i16>* %105, align 1
  br label %108

108:                                              ; preds = %98, %95, %56
  %109 = icmp slt i32 %2, 5
  br i1 %109, label %121, label %110

110:                                              ; preds = %108
  %111 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %112 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %113 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %114 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %115 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %116 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %117 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %118 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %119 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %120 = sext i32 %2 to i64
  br label %445

121:                                              ; preds = %108
  %122 = bitcast i8* %3 to <8 x i16>*
  %123 = load <8 x i16>, <8 x i16>* %122, align 1
  %124 = getelementptr inbounds i8, i8* %3, i64 32
  %125 = bitcast i8* %124 to <8 x i16>*
  %126 = load <8 x i16>, <8 x i16>* %125, align 1
  %127 = getelementptr inbounds i8, i8* %3, i64 64
  %128 = bitcast i8* %127 to <8 x i16>*
  %129 = load <8 x i16>, <8 x i16>* %128, align 1
  %130 = getelementptr inbounds i8, i8* %3, i64 96
  %131 = bitcast i8* %130 to <8 x i16>*
  %132 = load <8 x i16>, <8 x i16>* %131, align 1
  %133 = shufflevector <8 x i16> %123, <8 x i16> %126, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %134 = shufflevector <8 x i16> %129, <8 x i16> %132, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %135 = shufflevector <8 x i16> %123, <8 x i16> %126, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %136 = shufflevector <8 x i16> %129, <8 x i16> %132, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %137 = bitcast <8 x i16> %133 to <4 x i32>
  %138 = bitcast <8 x i16> %134 to <4 x i32>
  %139 = shufflevector <4 x i32> %137, <4 x i32> %138, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %140 = bitcast <4 x i32> %139 to <2 x i64>
  %141 = bitcast <8 x i16> %135 to <4 x i32>
  %142 = bitcast <8 x i16> %136 to <4 x i32>
  %143 = shufflevector <4 x i32> %141, <4 x i32> %142, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %144 = bitcast <4 x i32> %143 to <2 x i64>
  %145 = shufflevector <4 x i32> %137, <4 x i32> %138, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %146 = bitcast <4 x i32> %145 to <2 x i64>
  %147 = shufflevector <4 x i32> %141, <4 x i32> %142, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %148 = bitcast <4 x i32> %147 to <2 x i64>
  %149 = insertelement <2 x i64> %140, i64 0, i32 1
  %150 = shufflevector <2 x i64> %140, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  %151 = insertelement <2 x i64> %146, i64 0, i32 1
  %152 = shufflevector <2 x i64> %146, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  %153 = insertelement <2 x i64> %144, i64 0, i32 1
  %154 = shufflevector <2 x i64> %144, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  %155 = insertelement <2 x i64> %148, i64 0, i32 1
  %156 = shufflevector <2 x i64> %148, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  %157 = getelementptr inbounds i8, i8* %3, i64 16
  %158 = bitcast i8* %157 to <8 x i16>*
  %159 = load <8 x i16>, <8 x i16>* %158, align 1
  %160 = getelementptr inbounds i8, i8* %3, i64 48
  %161 = bitcast i8* %160 to <8 x i16>*
  %162 = load <8 x i16>, <8 x i16>* %161, align 1
  %163 = getelementptr inbounds i8, i8* %3, i64 80
  %164 = bitcast i8* %163 to <8 x i16>*
  %165 = load <8 x i16>, <8 x i16>* %164, align 1
  %166 = getelementptr inbounds i8, i8* %3, i64 112
  %167 = bitcast i8* %166 to <8 x i16>*
  %168 = load <8 x i16>, <8 x i16>* %167, align 1
  %169 = shufflevector <8 x i16> %159, <8 x i16> %162, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %170 = shufflevector <8 x i16> %165, <8 x i16> %168, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %171 = shufflevector <8 x i16> %159, <8 x i16> %162, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %172 = shufflevector <8 x i16> %165, <8 x i16> %168, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %173 = bitcast <8 x i16> %169 to <4 x i32>
  %174 = bitcast <8 x i16> %170 to <4 x i32>
  %175 = shufflevector <4 x i32> %173, <4 x i32> %174, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %176 = bitcast <4 x i32> %175 to <2 x i64>
  %177 = bitcast <8 x i16> %171 to <4 x i32>
  %178 = bitcast <8 x i16> %172 to <4 x i32>
  %179 = shufflevector <4 x i32> %177, <4 x i32> %178, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %180 = bitcast <4 x i32> %179 to <2 x i64>
  %181 = shufflevector <4 x i32> %173, <4 x i32> %174, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %182 = bitcast <4 x i32> %181 to <2 x i64>
  %183 = shufflevector <4 x i32> %177, <4 x i32> %178, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %184 = bitcast <4 x i32> %183 to <2 x i64>
  %185 = insertelement <2 x i64> %176, i64 0, i32 1
  %186 = shufflevector <2 x i64> %176, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  %187 = insertelement <2 x i64> %182, i64 0, i32 1
  %188 = shufflevector <2 x i64> %182, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  %189 = insertelement <2 x i64> %180, i64 0, i32 1
  %190 = shufflevector <2 x i64> %180, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  %191 = insertelement <2 x i64> %184, i64 0, i32 1
  %192 = shufflevector <2 x i64> %184, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  %193 = bitcast <2 x i64> %149 to <8 x i16>
  %194 = bitcast <2 x i64> %185 to <8 x i16>
  %195 = shufflevector <8 x i16> %193, <8 x i16> %194, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %196 = shufflevector <8 x i16> %194, <8 x i16> %193, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %197 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %198 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %195, <8 x i16> %197) #8
  %199 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %196, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %200 = add <4 x i32> %198, <i32 2048, i32 2048, i32 2048, i32 2048>
  %201 = ashr <4 x i32> %200, <i32 12, i32 12, i32 12, i32 12>
  %202 = add <4 x i32> %199, <i32 2048, i32 2048, i32 2048, i32 2048>
  %203 = ashr <4 x i32> %202, <i32 12, i32 12, i32 12, i32 12>
  %204 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %201, <4 x i32> %201) #8
  %205 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %203, <4 x i32> %203) #8
  %206 = bitcast <2 x i64> %153 to <8 x i16>
  %207 = bitcast <2 x i64> %189 to <8 x i16>
  %208 = shufflevector <8 x i16> %206, <8 x i16> %207, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %209 = shufflevector <8 x i16> %207, <8 x i16> %206, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %210 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %211 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %208, <8 x i16> %210) #8
  %212 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %209, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %213 = add <4 x i32> %211, <i32 2048, i32 2048, i32 2048, i32 2048>
  %214 = ashr <4 x i32> %213, <i32 12, i32 12, i32 12, i32 12>
  %215 = add <4 x i32> %212, <i32 2048, i32 2048, i32 2048, i32 2048>
  %216 = ashr <4 x i32> %215, <i32 12, i32 12, i32 12, i32 12>
  %217 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %214, <4 x i32> %214) #8
  %218 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %216, <4 x i32> %216) #8
  %219 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %205, <8 x i16> %218) #8
  %220 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %205, <8 x i16> %218) #8
  %221 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %204, <8 x i16> %217) #8
  %222 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %204, <8 x i16> %217) #8
  %223 = bitcast <2 x i64> %151 to <8 x i16>
  %224 = bitcast <2 x i64> %191 to <8 x i16>
  %225 = shufflevector <8 x i16> %223, <8 x i16> %224, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %226 = shufflevector <8 x i16> %224, <8 x i16> %223, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %227 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %228 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %225, <8 x i16> %227) #8
  %229 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %226, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %230 = add <4 x i32> %228, <i32 2048, i32 2048, i32 2048, i32 2048>
  %231 = ashr <4 x i32> %230, <i32 12, i32 12, i32 12, i32 12>
  %232 = add <4 x i32> %229, <i32 2048, i32 2048, i32 2048, i32 2048>
  %233 = ashr <4 x i32> %232, <i32 12, i32 12, i32 12, i32 12>
  %234 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %231, <4 x i32> %231) #8
  %235 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %233, <4 x i32> %233) #8
  %236 = bitcast <2 x i64> %187 to <8 x i16>
  %237 = bitcast <2 x i64> %155 to <8 x i16>
  %238 = shufflevector <8 x i16> %236, <8 x i16> %237, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %239 = shufflevector <8 x i16> %237, <8 x i16> %236, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %240 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %241 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %238, <8 x i16> %240) #8
  %242 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %239, <8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>) #8
  %243 = add <4 x i32> %241, <i32 2048, i32 2048, i32 2048, i32 2048>
  %244 = ashr <4 x i32> %243, <i32 12, i32 12, i32 12, i32 12>
  %245 = add <4 x i32> %242, <i32 2048, i32 2048, i32 2048, i32 2048>
  %246 = ashr <4 x i32> %245, <i32 12, i32 12, i32 12, i32 12>
  %247 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %244, <4 x i32> %244) #8
  %248 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %246, <4 x i32> %246) #8
  %249 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %234, <8 x i16> %247) #8
  %250 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %234, <8 x i16> %247) #8
  %251 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %235, <8 x i16> %248) #8
  %252 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %235, <8 x i16> %248) #8
  %253 = shufflevector <8 x i16> %252, <8 x i16> %250, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %254 = shufflevector <8 x i16> %250, <8 x i16> %252, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %255 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %253, <8 x i16> %197) #8
  %256 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %254, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %257 = add <4 x i32> %255, <i32 2048, i32 2048, i32 2048, i32 2048>
  %258 = ashr <4 x i32> %257, <i32 12, i32 12, i32 12, i32 12>
  %259 = add <4 x i32> %256, <i32 2048, i32 2048, i32 2048, i32 2048>
  %260 = ashr <4 x i32> %259, <i32 12, i32 12, i32 12, i32 12>
  %261 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %258, <4 x i32> %258) #8
  %262 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %260, <4 x i32> %260) #8
  %263 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %219, <8 x i16> %251) #8
  %264 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %219, <8 x i16> %251) #8
  %265 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %221, <8 x i16> %262) #8
  %266 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %221, <8 x i16> %262) #8
  %267 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %222, <8 x i16> %261) #8
  %268 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %222, <8 x i16> %261) #8
  %269 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %220, <8 x i16> %249) #8
  %270 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %220, <8 x i16> %249) #8
  %271 = bitcast <2 x i64> %150 to <8 x i16>
  %272 = bitcast <2 x i64> %192 to <8 x i16>
  %273 = shufflevector <8 x i16> %271, <8 x i16> %272, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %274 = shufflevector <8 x i16> %272, <8 x i16> %271, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %275 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %276 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %273, <8 x i16> %275) #8
  %277 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %274, <8 x i16> <i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076>) #8
  %278 = add <4 x i32> %276, <i32 2048, i32 2048, i32 2048, i32 2048>
  %279 = ashr <4 x i32> %278, <i32 12, i32 12, i32 12, i32 12>
  %280 = add <4 x i32> %277, <i32 2048, i32 2048, i32 2048, i32 2048>
  %281 = ashr <4 x i32> %280, <i32 12, i32 12, i32 12, i32 12>
  %282 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %279, <4 x i32> %279) #8
  %283 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %281, <4 x i32> %281) #8
  %284 = bitcast <2 x i64> %186 to <8 x i16>
  %285 = bitcast <2 x i64> %156 to <8 x i16>
  %286 = shufflevector <8 x i16> %284, <8 x i16> %285, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %287 = shufflevector <8 x i16> %285, <8 x i16> %284, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %288 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %289 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %286, <8 x i16> %288) #8
  %290 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %287, <8 x i16> <i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598>) #8
  %291 = add <4 x i32> %289, <i32 2048, i32 2048, i32 2048, i32 2048>
  %292 = ashr <4 x i32> %291, <i32 12, i32 12, i32 12, i32 12>
  %293 = add <4 x i32> %290, <i32 2048, i32 2048, i32 2048, i32 2048>
  %294 = ashr <4 x i32> %293, <i32 12, i32 12, i32 12, i32 12>
  %295 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %292, <4 x i32> %292) #8
  %296 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %294, <4 x i32> %294) #8
  %297 = bitcast <2 x i64> %154 to <8 x i16>
  %298 = bitcast <2 x i64> %188 to <8 x i16>
  %299 = shufflevector <8 x i16> %297, <8 x i16> %298, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %300 = shufflevector <8 x i16> %298, <8 x i16> %297, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %301 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %302 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %299, <8 x i16> %301) #8
  %303 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %300, <8 x i16> <i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612>) #8
  %304 = add <4 x i32> %302, <i32 2048, i32 2048, i32 2048, i32 2048>
  %305 = ashr <4 x i32> %304, <i32 12, i32 12, i32 12, i32 12>
  %306 = add <4 x i32> %303, <i32 2048, i32 2048, i32 2048, i32 2048>
  %307 = ashr <4 x i32> %306, <i32 12, i32 12, i32 12, i32 12>
  %308 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %305, <4 x i32> %305) #8
  %309 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %307, <4 x i32> %307) #8
  %310 = bitcast <2 x i64> %190 to <8 x i16>
  %311 = bitcast <2 x i64> %152 to <8 x i16>
  %312 = shufflevector <8 x i16> %310, <8 x i16> %311, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %313 = shufflevector <8 x i16> %311, <8 x i16> %310, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %314 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %315 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %312, <8 x i16> %314) #8
  %316 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %313, <8 x i16> <i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189>) #8
  %317 = add <4 x i32> %315, <i32 2048, i32 2048, i32 2048, i32 2048>
  %318 = ashr <4 x i32> %317, <i32 12, i32 12, i32 12, i32 12>
  %319 = add <4 x i32> %316, <i32 2048, i32 2048, i32 2048, i32 2048>
  %320 = ashr <4 x i32> %319, <i32 12, i32 12, i32 12, i32 12>
  %321 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %318, <4 x i32> %318) #8
  %322 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %320, <4 x i32> %320) #8
  %323 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %282, <8 x i16> %295) #8
  %324 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %282, <8 x i16> %295) #8
  %325 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %321, <8 x i16> %308) #8
  %326 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %321, <8 x i16> %308) #8
  %327 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %322, <8 x i16> %309) #8
  %328 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %322, <8 x i16> %309) #8
  %329 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %283, <8 x i16> %296) #8
  %330 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %283, <8 x i16> %296) #8
  %331 = shufflevector <8 x i16> %330, <8 x i16> %324, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %332 = shufflevector <8 x i16> %324, <8 x i16> %330, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %333 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %331, <8 x i16> %210) #8
  %334 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %332, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %335 = add <4 x i32> %333, <i32 2048, i32 2048, i32 2048, i32 2048>
  %336 = ashr <4 x i32> %335, <i32 12, i32 12, i32 12, i32 12>
  %337 = add <4 x i32> %334, <i32 2048, i32 2048, i32 2048, i32 2048>
  %338 = ashr <4 x i32> %337, <i32 12, i32 12, i32 12, i32 12>
  %339 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %336, <4 x i32> %336) #8
  %340 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %338, <4 x i32> %338) #8
  %341 = shufflevector <8 x i16> %328, <8 x i16> %326, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %342 = shufflevector <8 x i16> %326, <8 x i16> %328, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %343 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %344 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %341, <8 x i16> %343) #8
  %345 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %342, <8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>) #8
  %346 = add <4 x i32> %344, <i32 2048, i32 2048, i32 2048, i32 2048>
  %347 = ashr <4 x i32> %346, <i32 12, i32 12, i32 12, i32 12>
  %348 = add <4 x i32> %345, <i32 2048, i32 2048, i32 2048, i32 2048>
  %349 = ashr <4 x i32> %348, <i32 12, i32 12, i32 12, i32 12>
  %350 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %347, <4 x i32> %347) #8
  %351 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %349, <4 x i32> %349) #8
  %352 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %323, <8 x i16> %325) #8
  %353 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %323, <8 x i16> %325) #8
  %354 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %339, <8 x i16> %350) #8
  %355 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %339, <8 x i16> %350) #8
  %356 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %329, <8 x i16> %327) #8
  %357 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %329, <8 x i16> %327) #8
  %358 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %340, <8 x i16> %351) #8
  %359 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %340, <8 x i16> %351) #8
  %360 = shufflevector <8 x i16> %359, <8 x i16> %355, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %361 = shufflevector <8 x i16> %355, <8 x i16> %359, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %362 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %360, <8 x i16> %197) #8
  %363 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %361, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %364 = add <4 x i32> %362, <i32 2048, i32 2048, i32 2048, i32 2048>
  %365 = ashr <4 x i32> %364, <i32 12, i32 12, i32 12, i32 12>
  %366 = add <4 x i32> %363, <i32 2048, i32 2048, i32 2048, i32 2048>
  %367 = ashr <4 x i32> %366, <i32 12, i32 12, i32 12, i32 12>
  %368 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %365, <4 x i32> %365) #8
  %369 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %367, <4 x i32> %367) #8
  %370 = shufflevector <8 x i16> %357, <8 x i16> %353, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %371 = shufflevector <8 x i16> %353, <8 x i16> %357, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %372 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %370, <8 x i16> %197) #8
  %373 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %371, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %374 = add <4 x i32> %372, <i32 2048, i32 2048, i32 2048, i32 2048>
  %375 = ashr <4 x i32> %374, <i32 12, i32 12, i32 12, i32 12>
  %376 = add <4 x i32> %373, <i32 2048, i32 2048, i32 2048, i32 2048>
  %377 = ashr <4 x i32> %376, <i32 12, i32 12, i32 12, i32 12>
  %378 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %375, <4 x i32> %375) #8
  %379 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %377, <4 x i32> %377) #8
  %380 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %263, <8 x i16> %356) #8
  %381 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %263, <8 x i16> %356) #8
  %382 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %265, <8 x i16> %358) #8
  %383 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %265, <8 x i16> %358) #8
  %384 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %267, <8 x i16> %369) #8
  %385 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %267, <8 x i16> %369) #8
  %386 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %269, <8 x i16> %379) #8
  %387 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %269, <8 x i16> %379) #8
  %388 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %270, <8 x i16> %378) #8
  %389 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %270, <8 x i16> %378) #8
  %390 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %268, <8 x i16> %368) #8
  %391 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %268, <8 x i16> %368) #8
  %392 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %266, <8 x i16> %354) #8
  %393 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %266, <8 x i16> %354) #8
  %394 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %264, <8 x i16> %352) #8
  %395 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %264, <8 x i16> %352) #8
  %396 = shufflevector <8 x i16> %380, <8 x i16> %382, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %397 = shufflevector <8 x i16> %384, <8 x i16> %386, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %398 = shufflevector <8 x i16> %388, <8 x i16> %390, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %399 = shufflevector <8 x i16> %392, <8 x i16> %394, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %400 = bitcast <8 x i16> %396 to <4 x i32>
  %401 = bitcast <8 x i16> %397 to <4 x i32>
  %402 = shufflevector <4 x i32> %400, <4 x i32> %401, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %403 = bitcast <4 x i32> %402 to <2 x i64>
  %404 = bitcast <8 x i16> %398 to <4 x i32>
  %405 = bitcast <8 x i16> %399 to <4 x i32>
  %406 = shufflevector <4 x i32> %404, <4 x i32> %405, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %407 = bitcast <4 x i32> %406 to <2 x i64>
  %408 = shufflevector <4 x i32> %400, <4 x i32> %401, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %409 = bitcast <4 x i32> %408 to <2 x i64>
  %410 = shufflevector <4 x i32> %404, <4 x i32> %405, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %411 = bitcast <4 x i32> %410 to <2 x i64>
  %412 = shufflevector <2 x i64> %403, <2 x i64> %407, <2 x i32> <i32 0, i32 2>
  %413 = shufflevector <2 x i64> %403, <2 x i64> %407, <2 x i32> <i32 1, i32 3>
  %414 = shufflevector <2 x i64> %409, <2 x i64> %411, <2 x i32> <i32 0, i32 2>
  %415 = shufflevector <2 x i64> %409, <2 x i64> %411, <2 x i32> <i32 1, i32 3>
  %416 = bitcast i8* %3 to <2 x i64>*
  store <2 x i64> %412, <2 x i64>* %416, align 1
  %417 = bitcast i8* %124 to <2 x i64>*
  store <2 x i64> %413, <2 x i64>* %417, align 1
  %418 = bitcast i8* %127 to <2 x i64>*
  store <2 x i64> %414, <2 x i64>* %418, align 1
  %419 = bitcast i8* %130 to <2 x i64>*
  store <2 x i64> %415, <2 x i64>* %419, align 1
  %420 = shufflevector <8 x i16> %395, <8 x i16> %393, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %421 = shufflevector <8 x i16> %391, <8 x i16> %389, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %422 = shufflevector <8 x i16> %387, <8 x i16> %385, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %423 = shufflevector <8 x i16> %383, <8 x i16> %381, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %424 = bitcast <8 x i16> %420 to <4 x i32>
  %425 = bitcast <8 x i16> %421 to <4 x i32>
  %426 = shufflevector <4 x i32> %424, <4 x i32> %425, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %427 = bitcast <4 x i32> %426 to <2 x i64>
  %428 = bitcast <8 x i16> %422 to <4 x i32>
  %429 = bitcast <8 x i16> %423 to <4 x i32>
  %430 = shufflevector <4 x i32> %428, <4 x i32> %429, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %431 = bitcast <4 x i32> %430 to <2 x i64>
  %432 = shufflevector <4 x i32> %424, <4 x i32> %425, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %433 = bitcast <4 x i32> %432 to <2 x i64>
  %434 = shufflevector <4 x i32> %428, <4 x i32> %429, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %435 = bitcast <4 x i32> %434 to <2 x i64>
  %436 = shufflevector <2 x i64> %427, <2 x i64> %431, <2 x i32> <i32 0, i32 2>
  %437 = shufflevector <2 x i64> %427, <2 x i64> %431, <2 x i32> <i32 1, i32 3>
  %438 = shufflevector <2 x i64> %433, <2 x i64> %435, <2 x i32> <i32 0, i32 2>
  %439 = shufflevector <2 x i64> %433, <2 x i64> %435, <2 x i32> <i32 1, i32 3>
  %440 = bitcast i8* %157 to <2 x i64>*
  store <2 x i64> %436, <2 x i64>* %440, align 1
  %441 = bitcast i8* %160 to <2 x i64>*
  store <2 x i64> %437, <2 x i64>* %441, align 1
  %442 = bitcast i8* %163 to <2 x i64>*
  store <2 x i64> %438, <2 x i64>* %442, align 1
  %443 = bitcast i8* %166 to <2 x i64>*
  store <2 x i64> %439, <2 x i64>* %443, align 1
  %444 = sext i32 %2 to i64
  br label %973

445:                                              ; preds = %110, %445
  %446 = phi i64 [ 0, %110 ], [ %971, %445 ]
  %447 = shl i64 %446, 4
  %448 = and i64 %447, 4294967168
  %449 = getelementptr inbounds i16, i16* %8, i64 %448
  %450 = bitcast i16* %449 to <8 x i16>*
  %451 = load <8 x i16>, <8 x i16>* %450, align 1
  %452 = getelementptr inbounds i16, i16* %449, i64 16
  %453 = bitcast i16* %452 to <8 x i16>*
  %454 = load <8 x i16>, <8 x i16>* %453, align 1
  %455 = getelementptr inbounds i16, i16* %449, i64 32
  %456 = bitcast i16* %455 to <8 x i16>*
  %457 = load <8 x i16>, <8 x i16>* %456, align 1
  %458 = getelementptr inbounds i16, i16* %449, i64 48
  %459 = bitcast i16* %458 to <8 x i16>*
  %460 = load <8 x i16>, <8 x i16>* %459, align 1
  %461 = getelementptr inbounds i16, i16* %449, i64 64
  %462 = bitcast i16* %461 to <8 x i16>*
  %463 = load <8 x i16>, <8 x i16>* %462, align 1
  %464 = getelementptr inbounds i16, i16* %449, i64 80
  %465 = bitcast i16* %464 to <8 x i16>*
  %466 = load <8 x i16>, <8 x i16>* %465, align 1
  %467 = getelementptr inbounds i16, i16* %449, i64 96
  %468 = bitcast i16* %467 to <8 x i16>*
  %469 = load <8 x i16>, <8 x i16>* %468, align 1
  %470 = getelementptr inbounds i16, i16* %449, i64 112
  %471 = bitcast i16* %470 to <8 x i16>*
  %472 = load <8 x i16>, <8 x i16>* %471, align 1
  %473 = shufflevector <8 x i16> %451, <8 x i16> %454, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %474 = shufflevector <8 x i16> %457, <8 x i16> %460, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %475 = shufflevector <8 x i16> %463, <8 x i16> %466, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %476 = shufflevector <8 x i16> %469, <8 x i16> %472, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %477 = shufflevector <8 x i16> %451, <8 x i16> %454, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %478 = shufflevector <8 x i16> %457, <8 x i16> %460, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %479 = shufflevector <8 x i16> %463, <8 x i16> %466, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %480 = shufflevector <8 x i16> %469, <8 x i16> %472, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %481 = bitcast <8 x i16> %473 to <4 x i32>
  %482 = bitcast <8 x i16> %474 to <4 x i32>
  %483 = shufflevector <4 x i32> %481, <4 x i32> %482, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %484 = bitcast <4 x i32> %483 to <2 x i64>
  %485 = bitcast <8 x i16> %475 to <4 x i32>
  %486 = bitcast <8 x i16> %476 to <4 x i32>
  %487 = shufflevector <4 x i32> %485, <4 x i32> %486, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %488 = bitcast <4 x i32> %487 to <2 x i64>
  %489 = bitcast <8 x i16> %477 to <4 x i32>
  %490 = bitcast <8 x i16> %478 to <4 x i32>
  %491 = shufflevector <4 x i32> %489, <4 x i32> %490, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %492 = bitcast <4 x i32> %491 to <2 x i64>
  %493 = bitcast <8 x i16> %479 to <4 x i32>
  %494 = bitcast <8 x i16> %480 to <4 x i32>
  %495 = shufflevector <4 x i32> %493, <4 x i32> %494, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %496 = bitcast <4 x i32> %495 to <2 x i64>
  %497 = shufflevector <4 x i32> %481, <4 x i32> %482, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %498 = bitcast <4 x i32> %497 to <2 x i64>
  %499 = shufflevector <4 x i32> %485, <4 x i32> %486, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %500 = bitcast <4 x i32> %499 to <2 x i64>
  %501 = shufflevector <4 x i32> %489, <4 x i32> %490, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %502 = bitcast <4 x i32> %501 to <2 x i64>
  %503 = shufflevector <4 x i32> %493, <4 x i32> %494, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %504 = bitcast <4 x i32> %503 to <2 x i64>
  %505 = shufflevector <2 x i64> %484, <2 x i64> %488, <2 x i32> <i32 0, i32 2>
  %506 = shufflevector <2 x i64> %484, <2 x i64> %488, <2 x i32> <i32 1, i32 3>
  %507 = shufflevector <2 x i64> %498, <2 x i64> %500, <2 x i32> <i32 0, i32 2>
  %508 = shufflevector <2 x i64> %498, <2 x i64> %500, <2 x i32> <i32 1, i32 3>
  %509 = shufflevector <2 x i64> %492, <2 x i64> %496, <2 x i32> <i32 0, i32 2>
  %510 = shufflevector <2 x i64> %492, <2 x i64> %496, <2 x i32> <i32 1, i32 3>
  %511 = shufflevector <2 x i64> %502, <2 x i64> %504, <2 x i32> <i32 0, i32 2>
  %512 = shufflevector <2 x i64> %502, <2 x i64> %504, <2 x i32> <i32 1, i32 3>
  %513 = getelementptr inbounds i16, i16* %449, i64 8
  %514 = bitcast i16* %513 to <8 x i16>*
  %515 = load <8 x i16>, <8 x i16>* %514, align 1
  %516 = getelementptr inbounds i16, i16* %449, i64 24
  %517 = bitcast i16* %516 to <8 x i16>*
  %518 = load <8 x i16>, <8 x i16>* %517, align 1
  %519 = getelementptr inbounds i16, i16* %449, i64 40
  %520 = bitcast i16* %519 to <8 x i16>*
  %521 = load <8 x i16>, <8 x i16>* %520, align 1
  %522 = getelementptr inbounds i16, i16* %449, i64 56
  %523 = bitcast i16* %522 to <8 x i16>*
  %524 = load <8 x i16>, <8 x i16>* %523, align 1
  %525 = getelementptr inbounds i16, i16* %449, i64 72
  %526 = bitcast i16* %525 to <8 x i16>*
  %527 = load <8 x i16>, <8 x i16>* %526, align 1
  %528 = getelementptr inbounds i16, i16* %449, i64 88
  %529 = bitcast i16* %528 to <8 x i16>*
  %530 = load <8 x i16>, <8 x i16>* %529, align 1
  %531 = getelementptr inbounds i16, i16* %449, i64 104
  %532 = bitcast i16* %531 to <8 x i16>*
  %533 = load <8 x i16>, <8 x i16>* %532, align 1
  %534 = getelementptr inbounds i16, i16* %449, i64 120
  %535 = bitcast i16* %534 to <8 x i16>*
  %536 = load <8 x i16>, <8 x i16>* %535, align 1
  %537 = shufflevector <8 x i16> %515, <8 x i16> %518, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %538 = shufflevector <8 x i16> %521, <8 x i16> %524, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %539 = shufflevector <8 x i16> %527, <8 x i16> %530, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %540 = shufflevector <8 x i16> %533, <8 x i16> %536, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %541 = shufflevector <8 x i16> %515, <8 x i16> %518, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %542 = shufflevector <8 x i16> %521, <8 x i16> %524, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %543 = shufflevector <8 x i16> %527, <8 x i16> %530, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %544 = shufflevector <8 x i16> %533, <8 x i16> %536, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %545 = bitcast <8 x i16> %537 to <4 x i32>
  %546 = bitcast <8 x i16> %538 to <4 x i32>
  %547 = shufflevector <4 x i32> %545, <4 x i32> %546, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %548 = bitcast <4 x i32> %547 to <2 x i64>
  %549 = bitcast <8 x i16> %539 to <4 x i32>
  %550 = bitcast <8 x i16> %540 to <4 x i32>
  %551 = shufflevector <4 x i32> %549, <4 x i32> %550, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %552 = bitcast <4 x i32> %551 to <2 x i64>
  %553 = bitcast <8 x i16> %541 to <4 x i32>
  %554 = bitcast <8 x i16> %542 to <4 x i32>
  %555 = shufflevector <4 x i32> %553, <4 x i32> %554, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %556 = bitcast <4 x i32> %555 to <2 x i64>
  %557 = bitcast <8 x i16> %543 to <4 x i32>
  %558 = bitcast <8 x i16> %544 to <4 x i32>
  %559 = shufflevector <4 x i32> %557, <4 x i32> %558, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %560 = bitcast <4 x i32> %559 to <2 x i64>
  %561 = shufflevector <4 x i32> %545, <4 x i32> %546, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %562 = bitcast <4 x i32> %561 to <2 x i64>
  %563 = shufflevector <4 x i32> %549, <4 x i32> %550, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %564 = bitcast <4 x i32> %563 to <2 x i64>
  %565 = shufflevector <4 x i32> %553, <4 x i32> %554, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %566 = bitcast <4 x i32> %565 to <2 x i64>
  %567 = shufflevector <4 x i32> %557, <4 x i32> %558, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %568 = bitcast <4 x i32> %567 to <2 x i64>
  %569 = shufflevector <2 x i64> %548, <2 x i64> %552, <2 x i32> <i32 0, i32 2>
  %570 = shufflevector <2 x i64> %548, <2 x i64> %552, <2 x i32> <i32 1, i32 3>
  %571 = shufflevector <2 x i64> %562, <2 x i64> %564, <2 x i32> <i32 0, i32 2>
  %572 = shufflevector <2 x i64> %562, <2 x i64> %564, <2 x i32> <i32 1, i32 3>
  %573 = shufflevector <2 x i64> %556, <2 x i64> %560, <2 x i32> <i32 0, i32 2>
  %574 = shufflevector <2 x i64> %556, <2 x i64> %560, <2 x i32> <i32 1, i32 3>
  %575 = shufflevector <2 x i64> %566, <2 x i64> %568, <2 x i32> <i32 0, i32 2>
  %576 = shufflevector <2 x i64> %566, <2 x i64> %568, <2 x i32> <i32 1, i32 3>
  %577 = bitcast <2 x i64> %505 to <8 x i16>
  %578 = bitcast <2 x i64> %569 to <8 x i16>
  %579 = shufflevector <8 x i16> %577, <8 x i16> %578, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %580 = shufflevector <8 x i16> %578, <8 x i16> %577, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %581 = shufflevector <8 x i16> %577, <8 x i16> %578, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %582 = shufflevector <8 x i16> %578, <8 x i16> %577, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %583 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %579, <8 x i16> %111) #8
  %584 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %580, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %585 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %581, <8 x i16> %111) #8
  %586 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %582, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %587 = add <4 x i32> %583, <i32 2048, i32 2048, i32 2048, i32 2048>
  %588 = ashr <4 x i32> %587, <i32 12, i32 12, i32 12, i32 12>
  %589 = add <4 x i32> %584, <i32 2048, i32 2048, i32 2048, i32 2048>
  %590 = ashr <4 x i32> %589, <i32 12, i32 12, i32 12, i32 12>
  %591 = add <4 x i32> %585, <i32 2048, i32 2048, i32 2048, i32 2048>
  %592 = ashr <4 x i32> %591, <i32 12, i32 12, i32 12, i32 12>
  %593 = add <4 x i32> %586, <i32 2048, i32 2048, i32 2048, i32 2048>
  %594 = ashr <4 x i32> %593, <i32 12, i32 12, i32 12, i32 12>
  %595 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %588, <4 x i32> %592) #8
  %596 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %590, <4 x i32> %594) #8
  %597 = bitcast <2 x i64> %509 to <8 x i16>
  %598 = bitcast <2 x i64> %573 to <8 x i16>
  %599 = shufflevector <8 x i16> %597, <8 x i16> %598, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %600 = shufflevector <8 x i16> %598, <8 x i16> %597, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %601 = shufflevector <8 x i16> %597, <8 x i16> %598, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %602 = shufflevector <8 x i16> %598, <8 x i16> %597, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %603 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %599, <8 x i16> %112) #8
  %604 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %600, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %605 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %601, <8 x i16> %112) #8
  %606 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %602, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %607 = add <4 x i32> %603, <i32 2048, i32 2048, i32 2048, i32 2048>
  %608 = ashr <4 x i32> %607, <i32 12, i32 12, i32 12, i32 12>
  %609 = add <4 x i32> %604, <i32 2048, i32 2048, i32 2048, i32 2048>
  %610 = ashr <4 x i32> %609, <i32 12, i32 12, i32 12, i32 12>
  %611 = add <4 x i32> %605, <i32 2048, i32 2048, i32 2048, i32 2048>
  %612 = ashr <4 x i32> %611, <i32 12, i32 12, i32 12, i32 12>
  %613 = add <4 x i32> %606, <i32 2048, i32 2048, i32 2048, i32 2048>
  %614 = ashr <4 x i32> %613, <i32 12, i32 12, i32 12, i32 12>
  %615 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %608, <4 x i32> %612) #8
  %616 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %610, <4 x i32> %614) #8
  %617 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %596, <8 x i16> %616) #8
  %618 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %596, <8 x i16> %616) #8
  %619 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %595, <8 x i16> %615) #8
  %620 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %595, <8 x i16> %615) #8
  %621 = bitcast <2 x i64> %507 to <8 x i16>
  %622 = bitcast <2 x i64> %575 to <8 x i16>
  %623 = shufflevector <8 x i16> %621, <8 x i16> %622, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %624 = shufflevector <8 x i16> %622, <8 x i16> %621, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %625 = shufflevector <8 x i16> %621, <8 x i16> %622, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %626 = shufflevector <8 x i16> %622, <8 x i16> %621, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %627 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %623, <8 x i16> %113) #8
  %628 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %624, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %629 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %625, <8 x i16> %113) #8
  %630 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %626, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %631 = add <4 x i32> %627, <i32 2048, i32 2048, i32 2048, i32 2048>
  %632 = ashr <4 x i32> %631, <i32 12, i32 12, i32 12, i32 12>
  %633 = add <4 x i32> %628, <i32 2048, i32 2048, i32 2048, i32 2048>
  %634 = ashr <4 x i32> %633, <i32 12, i32 12, i32 12, i32 12>
  %635 = add <4 x i32> %629, <i32 2048, i32 2048, i32 2048, i32 2048>
  %636 = ashr <4 x i32> %635, <i32 12, i32 12, i32 12, i32 12>
  %637 = add <4 x i32> %630, <i32 2048, i32 2048, i32 2048, i32 2048>
  %638 = ashr <4 x i32> %637, <i32 12, i32 12, i32 12, i32 12>
  %639 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %632, <4 x i32> %636) #8
  %640 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %634, <4 x i32> %638) #8
  %641 = bitcast <2 x i64> %571 to <8 x i16>
  %642 = bitcast <2 x i64> %511 to <8 x i16>
  %643 = shufflevector <8 x i16> %641, <8 x i16> %642, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %644 = shufflevector <8 x i16> %642, <8 x i16> %641, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %645 = shufflevector <8 x i16> %641, <8 x i16> %642, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %646 = shufflevector <8 x i16> %642, <8 x i16> %641, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %647 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %643, <8 x i16> %114) #8
  %648 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %644, <8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>) #8
  %649 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %645, <8 x i16> %114) #8
  %650 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %646, <8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>) #8
  %651 = add <4 x i32> %647, <i32 2048, i32 2048, i32 2048, i32 2048>
  %652 = ashr <4 x i32> %651, <i32 12, i32 12, i32 12, i32 12>
  %653 = add <4 x i32> %648, <i32 2048, i32 2048, i32 2048, i32 2048>
  %654 = ashr <4 x i32> %653, <i32 12, i32 12, i32 12, i32 12>
  %655 = add <4 x i32> %649, <i32 2048, i32 2048, i32 2048, i32 2048>
  %656 = ashr <4 x i32> %655, <i32 12, i32 12, i32 12, i32 12>
  %657 = add <4 x i32> %650, <i32 2048, i32 2048, i32 2048, i32 2048>
  %658 = ashr <4 x i32> %657, <i32 12, i32 12, i32 12, i32 12>
  %659 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %652, <4 x i32> %656) #8
  %660 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %654, <4 x i32> %658) #8
  %661 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %639, <8 x i16> %659) #8
  %662 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %639, <8 x i16> %659) #8
  %663 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %640, <8 x i16> %660) #8
  %664 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %640, <8 x i16> %660) #8
  %665 = shufflevector <8 x i16> %664, <8 x i16> %662, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %666 = shufflevector <8 x i16> %662, <8 x i16> %664, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %667 = shufflevector <8 x i16> %664, <8 x i16> %662, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %668 = shufflevector <8 x i16> %662, <8 x i16> %664, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %669 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %665, <8 x i16> %111) #8
  %670 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %666, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %671 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %667, <8 x i16> %111) #8
  %672 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %668, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %673 = add <4 x i32> %669, <i32 2048, i32 2048, i32 2048, i32 2048>
  %674 = ashr <4 x i32> %673, <i32 12, i32 12, i32 12, i32 12>
  %675 = add <4 x i32> %670, <i32 2048, i32 2048, i32 2048, i32 2048>
  %676 = ashr <4 x i32> %675, <i32 12, i32 12, i32 12, i32 12>
  %677 = add <4 x i32> %671, <i32 2048, i32 2048, i32 2048, i32 2048>
  %678 = ashr <4 x i32> %677, <i32 12, i32 12, i32 12, i32 12>
  %679 = add <4 x i32> %672, <i32 2048, i32 2048, i32 2048, i32 2048>
  %680 = ashr <4 x i32> %679, <i32 12, i32 12, i32 12, i32 12>
  %681 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %674, <4 x i32> %678) #8
  %682 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %676, <4 x i32> %680) #8
  %683 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %617, <8 x i16> %663) #8
  %684 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %617, <8 x i16> %663) #8
  %685 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %619, <8 x i16> %682) #8
  %686 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %619, <8 x i16> %682) #8
  %687 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %620, <8 x i16> %681) #8
  %688 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %620, <8 x i16> %681) #8
  %689 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %618, <8 x i16> %661) #8
  %690 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %618, <8 x i16> %661) #8
  %691 = bitcast <2 x i64> %506 to <8 x i16>
  %692 = bitcast <2 x i64> %576 to <8 x i16>
  %693 = shufflevector <8 x i16> %691, <8 x i16> %692, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %694 = shufflevector <8 x i16> %692, <8 x i16> %691, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %695 = shufflevector <8 x i16> %691, <8 x i16> %692, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %696 = shufflevector <8 x i16> %692, <8 x i16> %691, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %697 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %693, <8 x i16> %115) #8
  %698 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %694, <8 x i16> <i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076>) #8
  %699 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %695, <8 x i16> %115) #8
  %700 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %696, <8 x i16> <i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076>) #8
  %701 = add <4 x i32> %697, <i32 2048, i32 2048, i32 2048, i32 2048>
  %702 = ashr <4 x i32> %701, <i32 12, i32 12, i32 12, i32 12>
  %703 = add <4 x i32> %698, <i32 2048, i32 2048, i32 2048, i32 2048>
  %704 = ashr <4 x i32> %703, <i32 12, i32 12, i32 12, i32 12>
  %705 = add <4 x i32> %699, <i32 2048, i32 2048, i32 2048, i32 2048>
  %706 = ashr <4 x i32> %705, <i32 12, i32 12, i32 12, i32 12>
  %707 = add <4 x i32> %700, <i32 2048, i32 2048, i32 2048, i32 2048>
  %708 = ashr <4 x i32> %707, <i32 12, i32 12, i32 12, i32 12>
  %709 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %702, <4 x i32> %706) #8
  %710 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %704, <4 x i32> %708) #8
  %711 = bitcast <2 x i64> %570 to <8 x i16>
  %712 = bitcast <2 x i64> %512 to <8 x i16>
  %713 = shufflevector <8 x i16> %711, <8 x i16> %712, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %714 = shufflevector <8 x i16> %712, <8 x i16> %711, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %715 = shufflevector <8 x i16> %711, <8 x i16> %712, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %716 = shufflevector <8 x i16> %712, <8 x i16> %711, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %717 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %713, <8 x i16> %116) #8
  %718 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %714, <8 x i16> <i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598>) #8
  %719 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %715, <8 x i16> %116) #8
  %720 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %716, <8 x i16> <i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598>) #8
  %721 = add <4 x i32> %717, <i32 2048, i32 2048, i32 2048, i32 2048>
  %722 = ashr <4 x i32> %721, <i32 12, i32 12, i32 12, i32 12>
  %723 = add <4 x i32> %718, <i32 2048, i32 2048, i32 2048, i32 2048>
  %724 = ashr <4 x i32> %723, <i32 12, i32 12, i32 12, i32 12>
  %725 = add <4 x i32> %719, <i32 2048, i32 2048, i32 2048, i32 2048>
  %726 = ashr <4 x i32> %725, <i32 12, i32 12, i32 12, i32 12>
  %727 = add <4 x i32> %720, <i32 2048, i32 2048, i32 2048, i32 2048>
  %728 = ashr <4 x i32> %727, <i32 12, i32 12, i32 12, i32 12>
  %729 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %722, <4 x i32> %726) #8
  %730 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %724, <4 x i32> %728) #8
  %731 = bitcast <2 x i64> %510 to <8 x i16>
  %732 = bitcast <2 x i64> %572 to <8 x i16>
  %733 = shufflevector <8 x i16> %731, <8 x i16> %732, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %734 = shufflevector <8 x i16> %732, <8 x i16> %731, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %735 = shufflevector <8 x i16> %731, <8 x i16> %732, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %736 = shufflevector <8 x i16> %732, <8 x i16> %731, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %737 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %733, <8 x i16> %117) #8
  %738 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %734, <8 x i16> <i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612>) #8
  %739 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %735, <8 x i16> %117) #8
  %740 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %736, <8 x i16> <i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612>) #8
  %741 = add <4 x i32> %737, <i32 2048, i32 2048, i32 2048, i32 2048>
  %742 = ashr <4 x i32> %741, <i32 12, i32 12, i32 12, i32 12>
  %743 = add <4 x i32> %738, <i32 2048, i32 2048, i32 2048, i32 2048>
  %744 = ashr <4 x i32> %743, <i32 12, i32 12, i32 12, i32 12>
  %745 = add <4 x i32> %739, <i32 2048, i32 2048, i32 2048, i32 2048>
  %746 = ashr <4 x i32> %745, <i32 12, i32 12, i32 12, i32 12>
  %747 = add <4 x i32> %740, <i32 2048, i32 2048, i32 2048, i32 2048>
  %748 = ashr <4 x i32> %747, <i32 12, i32 12, i32 12, i32 12>
  %749 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %742, <4 x i32> %746) #8
  %750 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %744, <4 x i32> %748) #8
  %751 = bitcast <2 x i64> %574 to <8 x i16>
  %752 = bitcast <2 x i64> %508 to <8 x i16>
  %753 = shufflevector <8 x i16> %751, <8 x i16> %752, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %754 = shufflevector <8 x i16> %752, <8 x i16> %751, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %755 = shufflevector <8 x i16> %751, <8 x i16> %752, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %756 = shufflevector <8 x i16> %752, <8 x i16> %751, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %757 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %753, <8 x i16> %118) #8
  %758 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %754, <8 x i16> <i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189>) #8
  %759 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %755, <8 x i16> %118) #8
  %760 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %756, <8 x i16> <i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189>) #8
  %761 = add <4 x i32> %757, <i32 2048, i32 2048, i32 2048, i32 2048>
  %762 = ashr <4 x i32> %761, <i32 12, i32 12, i32 12, i32 12>
  %763 = add <4 x i32> %758, <i32 2048, i32 2048, i32 2048, i32 2048>
  %764 = ashr <4 x i32> %763, <i32 12, i32 12, i32 12, i32 12>
  %765 = add <4 x i32> %759, <i32 2048, i32 2048, i32 2048, i32 2048>
  %766 = ashr <4 x i32> %765, <i32 12, i32 12, i32 12, i32 12>
  %767 = add <4 x i32> %760, <i32 2048, i32 2048, i32 2048, i32 2048>
  %768 = ashr <4 x i32> %767, <i32 12, i32 12, i32 12, i32 12>
  %769 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %762, <4 x i32> %766) #8
  %770 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %764, <4 x i32> %768) #8
  %771 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %709, <8 x i16> %729) #8
  %772 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %709, <8 x i16> %729) #8
  %773 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %769, <8 x i16> %749) #8
  %774 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %769, <8 x i16> %749) #8
  %775 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %770, <8 x i16> %750) #8
  %776 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %770, <8 x i16> %750) #8
  %777 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %710, <8 x i16> %730) #8
  %778 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %710, <8 x i16> %730) #8
  %779 = shufflevector <8 x i16> %778, <8 x i16> %772, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %780 = shufflevector <8 x i16> %772, <8 x i16> %778, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %781 = shufflevector <8 x i16> %778, <8 x i16> %772, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %782 = shufflevector <8 x i16> %772, <8 x i16> %778, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %783 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %779, <8 x i16> %112) #8
  %784 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %780, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %785 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %781, <8 x i16> %112) #8
  %786 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %782, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %787 = add <4 x i32> %783, <i32 2048, i32 2048, i32 2048, i32 2048>
  %788 = ashr <4 x i32> %787, <i32 12, i32 12, i32 12, i32 12>
  %789 = add <4 x i32> %784, <i32 2048, i32 2048, i32 2048, i32 2048>
  %790 = ashr <4 x i32> %789, <i32 12, i32 12, i32 12, i32 12>
  %791 = add <4 x i32> %785, <i32 2048, i32 2048, i32 2048, i32 2048>
  %792 = ashr <4 x i32> %791, <i32 12, i32 12, i32 12, i32 12>
  %793 = add <4 x i32> %786, <i32 2048, i32 2048, i32 2048, i32 2048>
  %794 = ashr <4 x i32> %793, <i32 12, i32 12, i32 12, i32 12>
  %795 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %788, <4 x i32> %792) #8
  %796 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %790, <4 x i32> %794) #8
  %797 = shufflevector <8 x i16> %776, <8 x i16> %774, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %798 = shufflevector <8 x i16> %774, <8 x i16> %776, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %799 = shufflevector <8 x i16> %776, <8 x i16> %774, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %800 = shufflevector <8 x i16> %774, <8 x i16> %776, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %801 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %797, <8 x i16> %119) #8
  %802 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %798, <8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>) #8
  %803 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %799, <8 x i16> %119) #8
  %804 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %800, <8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>) #8
  %805 = add <4 x i32> %801, <i32 2048, i32 2048, i32 2048, i32 2048>
  %806 = ashr <4 x i32> %805, <i32 12, i32 12, i32 12, i32 12>
  %807 = add <4 x i32> %802, <i32 2048, i32 2048, i32 2048, i32 2048>
  %808 = ashr <4 x i32> %807, <i32 12, i32 12, i32 12, i32 12>
  %809 = add <4 x i32> %803, <i32 2048, i32 2048, i32 2048, i32 2048>
  %810 = ashr <4 x i32> %809, <i32 12, i32 12, i32 12, i32 12>
  %811 = add <4 x i32> %804, <i32 2048, i32 2048, i32 2048, i32 2048>
  %812 = ashr <4 x i32> %811, <i32 12, i32 12, i32 12, i32 12>
  %813 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %806, <4 x i32> %810) #8
  %814 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %808, <4 x i32> %812) #8
  %815 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %771, <8 x i16> %773) #8
  %816 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %771, <8 x i16> %773) #8
  %817 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %795, <8 x i16> %813) #8
  %818 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %795, <8 x i16> %813) #8
  %819 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %777, <8 x i16> %775) #8
  %820 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %777, <8 x i16> %775) #8
  %821 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %796, <8 x i16> %814) #8
  %822 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %796, <8 x i16> %814) #8
  %823 = shufflevector <8 x i16> %822, <8 x i16> %818, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %824 = shufflevector <8 x i16> %818, <8 x i16> %822, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %825 = shufflevector <8 x i16> %822, <8 x i16> %818, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %826 = shufflevector <8 x i16> %818, <8 x i16> %822, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %827 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %823, <8 x i16> %111) #8
  %828 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %824, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %829 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %825, <8 x i16> %111) #8
  %830 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %826, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %831 = add <4 x i32> %827, <i32 2048, i32 2048, i32 2048, i32 2048>
  %832 = ashr <4 x i32> %831, <i32 12, i32 12, i32 12, i32 12>
  %833 = add <4 x i32> %828, <i32 2048, i32 2048, i32 2048, i32 2048>
  %834 = ashr <4 x i32> %833, <i32 12, i32 12, i32 12, i32 12>
  %835 = add <4 x i32> %829, <i32 2048, i32 2048, i32 2048, i32 2048>
  %836 = ashr <4 x i32> %835, <i32 12, i32 12, i32 12, i32 12>
  %837 = add <4 x i32> %830, <i32 2048, i32 2048, i32 2048, i32 2048>
  %838 = ashr <4 x i32> %837, <i32 12, i32 12, i32 12, i32 12>
  %839 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %832, <4 x i32> %836) #8
  %840 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %834, <4 x i32> %838) #8
  %841 = shufflevector <8 x i16> %820, <8 x i16> %816, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %842 = shufflevector <8 x i16> %816, <8 x i16> %820, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %843 = shufflevector <8 x i16> %820, <8 x i16> %816, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %844 = shufflevector <8 x i16> %816, <8 x i16> %820, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %845 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %841, <8 x i16> %111) #8
  %846 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %842, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %847 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %843, <8 x i16> %111) #8
  %848 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %844, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %849 = add <4 x i32> %845, <i32 2048, i32 2048, i32 2048, i32 2048>
  %850 = ashr <4 x i32> %849, <i32 12, i32 12, i32 12, i32 12>
  %851 = add <4 x i32> %846, <i32 2048, i32 2048, i32 2048, i32 2048>
  %852 = ashr <4 x i32> %851, <i32 12, i32 12, i32 12, i32 12>
  %853 = add <4 x i32> %847, <i32 2048, i32 2048, i32 2048, i32 2048>
  %854 = ashr <4 x i32> %853, <i32 12, i32 12, i32 12, i32 12>
  %855 = add <4 x i32> %848, <i32 2048, i32 2048, i32 2048, i32 2048>
  %856 = ashr <4 x i32> %855, <i32 12, i32 12, i32 12, i32 12>
  %857 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %850, <4 x i32> %854) #8
  %858 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %852, <4 x i32> %856) #8
  %859 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %683, <8 x i16> %819) #8
  %860 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %683, <8 x i16> %819) #8
  %861 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %685, <8 x i16> %821) #8
  %862 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %685, <8 x i16> %821) #8
  %863 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %687, <8 x i16> %840) #8
  %864 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %687, <8 x i16> %840) #8
  %865 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %689, <8 x i16> %858) #8
  %866 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %689, <8 x i16> %858) #8
  %867 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %690, <8 x i16> %857) #8
  %868 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %690, <8 x i16> %857) #8
  %869 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %688, <8 x i16> %839) #8
  %870 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %688, <8 x i16> %839) #8
  %871 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %686, <8 x i16> %817) #8
  %872 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %686, <8 x i16> %817) #8
  %873 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %684, <8 x i16> %815) #8
  %874 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %684, <8 x i16> %815) #8
  %875 = shufflevector <8 x i16> %859, <8 x i16> %861, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %876 = shufflevector <8 x i16> %863, <8 x i16> %865, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %877 = shufflevector <8 x i16> %867, <8 x i16> %869, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %878 = shufflevector <8 x i16> %871, <8 x i16> %873, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %879 = shufflevector <8 x i16> %859, <8 x i16> %861, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %880 = shufflevector <8 x i16> %863, <8 x i16> %865, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %881 = shufflevector <8 x i16> %867, <8 x i16> %869, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %882 = shufflevector <8 x i16> %871, <8 x i16> %873, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %883 = bitcast <8 x i16> %875 to <4 x i32>
  %884 = bitcast <8 x i16> %876 to <4 x i32>
  %885 = shufflevector <4 x i32> %883, <4 x i32> %884, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %886 = bitcast <4 x i32> %885 to <2 x i64>
  %887 = bitcast <8 x i16> %877 to <4 x i32>
  %888 = bitcast <8 x i16> %878 to <4 x i32>
  %889 = shufflevector <4 x i32> %887, <4 x i32> %888, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %890 = bitcast <4 x i32> %889 to <2 x i64>
  %891 = bitcast <8 x i16> %879 to <4 x i32>
  %892 = bitcast <8 x i16> %880 to <4 x i32>
  %893 = shufflevector <4 x i32> %891, <4 x i32> %892, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %894 = bitcast <4 x i32> %893 to <2 x i64>
  %895 = bitcast <8 x i16> %881 to <4 x i32>
  %896 = bitcast <8 x i16> %882 to <4 x i32>
  %897 = shufflevector <4 x i32> %895, <4 x i32> %896, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %898 = bitcast <4 x i32> %897 to <2 x i64>
  %899 = shufflevector <4 x i32> %883, <4 x i32> %884, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %900 = bitcast <4 x i32> %899 to <2 x i64>
  %901 = shufflevector <4 x i32> %887, <4 x i32> %888, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %902 = bitcast <4 x i32> %901 to <2 x i64>
  %903 = shufflevector <4 x i32> %891, <4 x i32> %892, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %904 = bitcast <4 x i32> %903 to <2 x i64>
  %905 = shufflevector <4 x i32> %895, <4 x i32> %896, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %906 = bitcast <4 x i32> %905 to <2 x i64>
  %907 = shufflevector <2 x i64> %886, <2 x i64> %890, <2 x i32> <i32 0, i32 2>
  %908 = shufflevector <2 x i64> %886, <2 x i64> %890, <2 x i32> <i32 1, i32 3>
  %909 = shufflevector <2 x i64> %900, <2 x i64> %902, <2 x i32> <i32 0, i32 2>
  %910 = shufflevector <2 x i64> %900, <2 x i64> %902, <2 x i32> <i32 1, i32 3>
  %911 = shufflevector <2 x i64> %894, <2 x i64> %898, <2 x i32> <i32 0, i32 2>
  %912 = shufflevector <2 x i64> %894, <2 x i64> %898, <2 x i32> <i32 1, i32 3>
  %913 = shufflevector <2 x i64> %904, <2 x i64> %906, <2 x i32> <i32 0, i32 2>
  %914 = shufflevector <2 x i64> %904, <2 x i64> %906, <2 x i32> <i32 1, i32 3>
  %915 = bitcast i16* %449 to <2 x i64>*
  store <2 x i64> %907, <2 x i64>* %915, align 1
  %916 = bitcast i16* %452 to <2 x i64>*
  store <2 x i64> %908, <2 x i64>* %916, align 1
  %917 = bitcast i16* %455 to <2 x i64>*
  store <2 x i64> %909, <2 x i64>* %917, align 1
  %918 = bitcast i16* %458 to <2 x i64>*
  store <2 x i64> %910, <2 x i64>* %918, align 1
  %919 = bitcast i16* %461 to <2 x i64>*
  store <2 x i64> %911, <2 x i64>* %919, align 1
  %920 = bitcast i16* %464 to <2 x i64>*
  store <2 x i64> %912, <2 x i64>* %920, align 1
  %921 = bitcast i16* %467 to <2 x i64>*
  store <2 x i64> %913, <2 x i64>* %921, align 1
  %922 = bitcast i16* %470 to <2 x i64>*
  store <2 x i64> %914, <2 x i64>* %922, align 1
  %923 = shufflevector <8 x i16> %874, <8 x i16> %872, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %924 = shufflevector <8 x i16> %870, <8 x i16> %868, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %925 = shufflevector <8 x i16> %866, <8 x i16> %864, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %926 = shufflevector <8 x i16> %862, <8 x i16> %860, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %927 = shufflevector <8 x i16> %874, <8 x i16> %872, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %928 = shufflevector <8 x i16> %870, <8 x i16> %868, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %929 = shufflevector <8 x i16> %866, <8 x i16> %864, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %930 = shufflevector <8 x i16> %862, <8 x i16> %860, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %931 = bitcast <8 x i16> %923 to <4 x i32>
  %932 = bitcast <8 x i16> %924 to <4 x i32>
  %933 = shufflevector <4 x i32> %931, <4 x i32> %932, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %934 = bitcast <4 x i32> %933 to <2 x i64>
  %935 = bitcast <8 x i16> %925 to <4 x i32>
  %936 = bitcast <8 x i16> %926 to <4 x i32>
  %937 = shufflevector <4 x i32> %935, <4 x i32> %936, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %938 = bitcast <4 x i32> %937 to <2 x i64>
  %939 = bitcast <8 x i16> %927 to <4 x i32>
  %940 = bitcast <8 x i16> %928 to <4 x i32>
  %941 = shufflevector <4 x i32> %939, <4 x i32> %940, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %942 = bitcast <4 x i32> %941 to <2 x i64>
  %943 = bitcast <8 x i16> %929 to <4 x i32>
  %944 = bitcast <8 x i16> %930 to <4 x i32>
  %945 = shufflevector <4 x i32> %943, <4 x i32> %944, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %946 = bitcast <4 x i32> %945 to <2 x i64>
  %947 = shufflevector <4 x i32> %931, <4 x i32> %932, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %948 = bitcast <4 x i32> %947 to <2 x i64>
  %949 = shufflevector <4 x i32> %935, <4 x i32> %936, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %950 = bitcast <4 x i32> %949 to <2 x i64>
  %951 = shufflevector <4 x i32> %939, <4 x i32> %940, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %952 = bitcast <4 x i32> %951 to <2 x i64>
  %953 = shufflevector <4 x i32> %943, <4 x i32> %944, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %954 = bitcast <4 x i32> %953 to <2 x i64>
  %955 = shufflevector <2 x i64> %934, <2 x i64> %938, <2 x i32> <i32 0, i32 2>
  %956 = shufflevector <2 x i64> %934, <2 x i64> %938, <2 x i32> <i32 1, i32 3>
  %957 = shufflevector <2 x i64> %948, <2 x i64> %950, <2 x i32> <i32 0, i32 2>
  %958 = shufflevector <2 x i64> %948, <2 x i64> %950, <2 x i32> <i32 1, i32 3>
  %959 = shufflevector <2 x i64> %942, <2 x i64> %946, <2 x i32> <i32 0, i32 2>
  %960 = shufflevector <2 x i64> %942, <2 x i64> %946, <2 x i32> <i32 1, i32 3>
  %961 = shufflevector <2 x i64> %952, <2 x i64> %954, <2 x i32> <i32 0, i32 2>
  %962 = shufflevector <2 x i64> %952, <2 x i64> %954, <2 x i32> <i32 1, i32 3>
  %963 = bitcast i16* %513 to <2 x i64>*
  store <2 x i64> %955, <2 x i64>* %963, align 1
  %964 = bitcast i16* %516 to <2 x i64>*
  store <2 x i64> %956, <2 x i64>* %964, align 1
  %965 = bitcast i16* %519 to <2 x i64>*
  store <2 x i64> %957, <2 x i64>* %965, align 1
  %966 = bitcast i16* %522 to <2 x i64>*
  store <2 x i64> %958, <2 x i64>* %966, align 1
  %967 = bitcast i16* %525 to <2 x i64>*
  store <2 x i64> %959, <2 x i64>* %967, align 1
  %968 = bitcast i16* %528 to <2 x i64>*
  store <2 x i64> %960, <2 x i64>* %968, align 1
  %969 = bitcast i16* %531 to <2 x i64>*
  store <2 x i64> %961, <2 x i64>* %969, align 1
  %970 = bitcast i16* %534 to <2 x i64>*
  store <2 x i64> %962, <2 x i64>* %970, align 1
  %971 = add nuw nsw i64 %446, 8
  %972 = icmp slt i64 %971, %120
  br i1 %972, label %445, label %973

973:                                              ; preds = %445, %121
  %974 = phi i64 [ %444, %121 ], [ %120, %445 ]
  %975 = zext i8 %14 to i16
  %976 = insertelement <8 x i16> undef, i16 %975, i32 0
  %977 = shufflevector <8 x i16> %976, <8 x i16> undef, <8 x i32> zeroinitializer
  %978 = shufflevector <8 x i16> %976, <8 x i16> undef, <2 x i32> zeroinitializer
  %979 = zext <2 x i16> %978 to <2 x i64>
  %980 = bitcast <2 x i64> %979 to <8 x i16>
  br label %981

981:                                              ; preds = %981, %973
  %982 = phi i64 [ %1002, %981 ], [ 0, %973 ]
  %983 = shl i64 %982, 4
  %984 = and i64 %983, 4294967280
  %985 = getelementptr inbounds i16, i16* %8, i64 %984
  %986 = bitcast i16* %985 to <8 x i16>*
  %987 = load <8 x i16>, <8 x i16>* %986, align 1
  %988 = icmp sgt <8 x i16> %987, <i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765>
  %989 = add <8 x i16> %987, %977
  %990 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %989, <8 x i16> %980) #8
  %991 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %989, <8 x i16> %980) #8
  %992 = select <8 x i1> %988, <8 x i16> %991, <8 x i16> %990
  store <8 x i16> %992, <8 x i16>* %986, align 1
  %993 = or i64 %984, 8
  %994 = getelementptr inbounds i16, i16* %8, i64 %993
  %995 = bitcast i16* %994 to <8 x i16>*
  %996 = load <8 x i16>, <8 x i16>* %995, align 1
  %997 = icmp sgt <8 x i16> %996, <i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765>
  %998 = add <8 x i16> %996, %977
  %999 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %998, <8 x i16> %980) #8
  %1000 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %998, <8 x i16> %980) #8
  %1001 = select <8 x i1> %997, <8 x i16> %1000, <8 x i16> %999
  store <8 x i16> %1001, <8 x i16>* %995, align 1
  %1002 = add nuw nsw i64 %982, 1
  %1003 = icmp slt i64 %1002, %974
  br i1 %1003, label %981, label %1004

1004:                                             ; preds = %981, %16
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_131Dct16TransformLoopColumn_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_(i8 zeroext, i8 zeroext, i32, i8*, i32, i32, i8* nocapture readonly) #4 {
  %8 = bitcast i8* %3 to i16*
  %9 = zext i8 %1 to i64
  %10 = getelementptr inbounds [19 x i8], [19 x i8]* @_ZN7libgav115kTransformWidthE, i64 0, i64 %9
  %11 = load i8, i8* %10, align 1
  %12 = zext i8 %11 to i64
  %13 = zext i8 %0 to i32
  %14 = shl i32 1, %13
  %15 = and i32 %14, 33104
  %16 = icmp eq i32 %15, 0
  br i1 %16, label %130, label %17

17:                                               ; preds = %7
  %18 = icmp ugt i8 %11, 15
  br i1 %18, label %19, label %34

19:                                               ; preds = %17
  %20 = shl nuw nsw i64 %12, 4
  br label %21

21:                                               ; preds = %21, %19
  %22 = phi i64 [ 0, %19 ], [ %32, %21 ]
  %23 = getelementptr inbounds i16, i16* %8, i64 %22
  %24 = bitcast i16* %23 to <16 x i8>*
  %25 = load <16 x i8>, <16 x i8>* %24, align 1
  %26 = or i64 %22, 8
  %27 = getelementptr inbounds i16, i16* %8, i64 %26
  %28 = bitcast i16* %27 to <16 x i8>*
  %29 = load <16 x i8>, <16 x i8>* %28, align 1
  %30 = shufflevector <16 x i8> %25, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  %31 = shufflevector <16 x i8> %29, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %31, <16 x i8>* %24, align 1
  store <16 x i8> %30, <16 x i8>* %28, align 1
  %32 = add nuw nsw i64 %22, 16
  %33 = icmp ult i64 %32, %20
  br i1 %33, label %21, label %130

34:                                               ; preds = %17
  %35 = icmp eq i8 %11, 8
  %36 = bitcast i8* %3 to <16 x i8>*
  %37 = load <16 x i8>, <16 x i8>* %36, align 1
  br i1 %35, label %68, label %38

38:                                               ; preds = %34
  %39 = shufflevector <16 x i8> %37, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %39, <16 x i8>* %36, align 1
  %40 = getelementptr inbounds i8, i8* %3, i64 16
  %41 = bitcast i8* %40 to <16 x i8>*
  %42 = load <16 x i8>, <16 x i8>* %41, align 1
  %43 = shufflevector <16 x i8> %42, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %43, <16 x i8>* %41, align 1
  %44 = getelementptr inbounds i8, i8* %3, i64 32
  %45 = bitcast i8* %44 to <16 x i8>*
  %46 = load <16 x i8>, <16 x i8>* %45, align 1
  %47 = shufflevector <16 x i8> %46, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %47, <16 x i8>* %45, align 1
  %48 = getelementptr inbounds i8, i8* %3, i64 48
  %49 = bitcast i8* %48 to <16 x i8>*
  %50 = load <16 x i8>, <16 x i8>* %49, align 1
  %51 = shufflevector <16 x i8> %50, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %51, <16 x i8>* %49, align 1
  %52 = getelementptr inbounds i8, i8* %3, i64 64
  %53 = bitcast i8* %52 to <16 x i8>*
  %54 = load <16 x i8>, <16 x i8>* %53, align 1
  %55 = shufflevector <16 x i8> %54, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %55, <16 x i8>* %53, align 1
  %56 = getelementptr inbounds i8, i8* %3, i64 80
  %57 = bitcast i8* %56 to <16 x i8>*
  %58 = load <16 x i8>, <16 x i8>* %57, align 1
  %59 = shufflevector <16 x i8> %58, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %59, <16 x i8>* %57, align 1
  %60 = getelementptr inbounds i8, i8* %3, i64 96
  %61 = bitcast i8* %60 to <16 x i8>*
  %62 = load <16 x i8>, <16 x i8>* %61, align 1
  %63 = shufflevector <16 x i8> %62, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %63, <16 x i8>* %61, align 1
  %64 = getelementptr inbounds i8, i8* %3, i64 112
  %65 = bitcast i8* %64 to <16 x i8>*
  %66 = load <16 x i8>, <16 x i8>* %65, align 1
  %67 = shufflevector <16 x i8> %66, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %67, <16 x i8>* %65, align 1
  br label %130

68:                                               ; preds = %34
  %69 = shufflevector <16 x i8> %37, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %69, <16 x i8>* %36, align 1
  %70 = getelementptr inbounds i8, i8* %3, i64 16
  %71 = bitcast i8* %70 to <16 x i8>*
  %72 = load <16 x i8>, <16 x i8>* %71, align 1
  %73 = shufflevector <16 x i8> %72, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %73, <16 x i8>* %71, align 1
  %74 = getelementptr inbounds i8, i8* %3, i64 32
  %75 = bitcast i8* %74 to <16 x i8>*
  %76 = load <16 x i8>, <16 x i8>* %75, align 1
  %77 = shufflevector <16 x i8> %76, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %77, <16 x i8>* %75, align 1
  %78 = getelementptr inbounds i8, i8* %3, i64 48
  %79 = bitcast i8* %78 to <16 x i8>*
  %80 = load <16 x i8>, <16 x i8>* %79, align 1
  %81 = shufflevector <16 x i8> %80, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %81, <16 x i8>* %79, align 1
  %82 = getelementptr inbounds i8, i8* %3, i64 64
  %83 = bitcast i8* %82 to <16 x i8>*
  %84 = load <16 x i8>, <16 x i8>* %83, align 1
  %85 = shufflevector <16 x i8> %84, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %85, <16 x i8>* %83, align 1
  %86 = getelementptr inbounds i8, i8* %3, i64 80
  %87 = bitcast i8* %86 to <16 x i8>*
  %88 = load <16 x i8>, <16 x i8>* %87, align 1
  %89 = shufflevector <16 x i8> %88, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %89, <16 x i8>* %87, align 1
  %90 = getelementptr inbounds i8, i8* %3, i64 96
  %91 = bitcast i8* %90 to <16 x i8>*
  %92 = load <16 x i8>, <16 x i8>* %91, align 1
  %93 = shufflevector <16 x i8> %92, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %93, <16 x i8>* %91, align 1
  %94 = getelementptr inbounds i8, i8* %3, i64 112
  %95 = bitcast i8* %94 to <16 x i8>*
  %96 = load <16 x i8>, <16 x i8>* %95, align 1
  %97 = shufflevector <16 x i8> %96, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %97, <16 x i8>* %95, align 1
  %98 = getelementptr inbounds i8, i8* %3, i64 128
  %99 = bitcast i8* %98 to <16 x i8>*
  %100 = load <16 x i8>, <16 x i8>* %99, align 1
  %101 = shufflevector <16 x i8> %100, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %101, <16 x i8>* %99, align 1
  %102 = getelementptr inbounds i8, i8* %3, i64 144
  %103 = bitcast i8* %102 to <16 x i8>*
  %104 = load <16 x i8>, <16 x i8>* %103, align 1
  %105 = shufflevector <16 x i8> %104, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %105, <16 x i8>* %103, align 1
  %106 = getelementptr inbounds i8, i8* %3, i64 160
  %107 = bitcast i8* %106 to <16 x i8>*
  %108 = load <16 x i8>, <16 x i8>* %107, align 1
  %109 = shufflevector <16 x i8> %108, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %109, <16 x i8>* %107, align 1
  %110 = getelementptr inbounds i8, i8* %3, i64 176
  %111 = bitcast i8* %110 to <16 x i8>*
  %112 = load <16 x i8>, <16 x i8>* %111, align 1
  %113 = shufflevector <16 x i8> %112, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %113, <16 x i8>* %111, align 1
  %114 = getelementptr inbounds i8, i8* %3, i64 192
  %115 = bitcast i8* %114 to <16 x i8>*
  %116 = load <16 x i8>, <16 x i8>* %115, align 1
  %117 = shufflevector <16 x i8> %116, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %117, <16 x i8>* %115, align 1
  %118 = getelementptr inbounds i8, i8* %3, i64 208
  %119 = bitcast i8* %118 to <16 x i8>*
  %120 = load <16 x i8>, <16 x i8>* %119, align 1
  %121 = shufflevector <16 x i8> %120, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %121, <16 x i8>* %119, align 1
  %122 = getelementptr inbounds i8, i8* %3, i64 224
  %123 = bitcast i8* %122 to <16 x i8>*
  %124 = load <16 x i8>, <16 x i8>* %123, align 1
  %125 = shufflevector <16 x i8> %124, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %125, <16 x i8>* %123, align 1
  %126 = getelementptr inbounds i8, i8* %3, i64 240
  %127 = bitcast i8* %126 to <16 x i8>*
  %128 = load <16 x i8>, <16 x i8>* %127, align 1
  %129 = shufflevector <16 x i8> %128, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %129, <16 x i8>* %127, align 1
  br label %130

130:                                              ; preds = %21, %7, %68, %38
  %131 = icmp sgt i32 %2, 1
  %132 = icmp eq i8 %11, 4
  br i1 %131, label %198, label %133

133:                                              ; preds = %130
  br i1 %132, label %136, label %134

134:                                              ; preds = %133
  %135 = zext i8 %11 to i64
  br label %144

136:                                              ; preds = %133
  %137 = bitcast i8* %3 to i64*
  %138 = load i64, i64* %137, align 1
  %139 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %138, i32 0
  %140 = bitcast <2 x i64> %139 to <8 x i16>
  %141 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %140, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  %142 = bitcast <8 x i16> %141 to <2 x i64>
  %143 = extractelement <2 x i64> %142, i32 0
  store i64 %143, i64* %137, align 1
  br label %152

144:                                              ; preds = %144, %134
  %145 = phi i64 [ 0, %134 ], [ %150, %144 ]
  %146 = getelementptr inbounds i16, i16* %8, i64 %145
  %147 = bitcast i16* %146 to <8 x i16>*
  %148 = load <8 x i16>, <8 x i16>* %147, align 1
  %149 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %148, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %149, <8 x i16>* %147, align 1
  %150 = add nuw nsw i64 %145, 8
  %151 = icmp ult i64 %150, %135
  br i1 %151, label %144, label %152

152:                                              ; preds = %144, %136
  %153 = phi i64 [ 4, %136 ], [ %135, %144 ]
  %154 = shl nuw nsw i64 %153, 1
  %155 = getelementptr inbounds i16, i16* %8, i64 %153
  %156 = bitcast i16* %155 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 2 %156, i8* align 2 %3, i64 %154, i1 false) #8
  %157 = getelementptr inbounds i16, i16* %8, i64 %154
  %158 = bitcast i16* %157 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 2 %158, i8* align 2 %3, i64 %154, i1 false) #8
  %159 = mul nuw nsw i64 %153, 3
  %160 = getelementptr inbounds i16, i16* %8, i64 %159
  %161 = bitcast i16* %160 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 2 %161, i8* align 2 %3, i64 %154, i1 false) #8
  %162 = shl nuw nsw i64 %153, 2
  %163 = getelementptr inbounds i16, i16* %8, i64 %162
  %164 = bitcast i16* %163 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 2 %164, i8* align 2 %3, i64 %154, i1 false) #8
  %165 = mul nuw nsw i64 %153, 5
  %166 = getelementptr inbounds i16, i16* %8, i64 %165
  %167 = bitcast i16* %166 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 2 %167, i8* align 2 %3, i64 %154, i1 false) #8
  %168 = mul nuw nsw i64 %153, 6
  %169 = getelementptr inbounds i16, i16* %8, i64 %168
  %170 = bitcast i16* %169 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 2 %170, i8* align 2 %3, i64 %154, i1 false) #8
  %171 = mul nuw nsw i64 %153, 7
  %172 = getelementptr inbounds i16, i16* %8, i64 %171
  %173 = bitcast i16* %172 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 2 %173, i8* align 2 %3, i64 %154, i1 false) #8
  %174 = shl nuw nsw i64 %153, 3
  %175 = getelementptr inbounds i16, i16* %8, i64 %174
  %176 = bitcast i16* %175 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 2 %176, i8* align 2 %3, i64 %154, i1 false) #8
  %177 = mul nuw nsw i64 %153, 9
  %178 = getelementptr inbounds i16, i16* %8, i64 %177
  %179 = bitcast i16* %178 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 2 %179, i8* align 2 %3, i64 %154, i1 false) #8
  %180 = mul nuw nsw i64 %153, 10
  %181 = getelementptr inbounds i16, i16* %8, i64 %180
  %182 = bitcast i16* %181 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 2 %182, i8* align 2 %3, i64 %154, i1 false) #8
  %183 = mul nuw nsw i64 %153, 11
  %184 = getelementptr inbounds i16, i16* %8, i64 %183
  %185 = bitcast i16* %184 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 2 %185, i8* align 2 %3, i64 %154, i1 false) #8
  %186 = mul nuw nsw i64 %153, 12
  %187 = getelementptr inbounds i16, i16* %8, i64 %186
  %188 = bitcast i16* %187 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 2 %188, i8* align 2 %3, i64 %154, i1 false) #8
  %189 = mul nuw nsw i64 %153, 13
  %190 = getelementptr inbounds i16, i16* %8, i64 %189
  %191 = bitcast i16* %190 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 2 %191, i8* align 2 %3, i64 %154, i1 false) #8
  %192 = mul nuw nsw i64 %153, 14
  %193 = getelementptr inbounds i16, i16* %8, i64 %192
  %194 = bitcast i16* %193 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 2 %194, i8* align 2 %3, i64 %154, i1 false) #8
  %195 = mul nuw nsw i64 %153, 15
  %196 = getelementptr inbounds i16, i16* %8, i64 %195
  %197 = bitcast i16* %196 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 2 %197, i8* align 2 %3, i64 %154, i1 false) #8
  br label %869

198:                                              ; preds = %130
  br i1 %132, label %225, label %199

199:                                              ; preds = %198
  %200 = zext i8 %11 to i64
  %201 = shl nuw nsw i64 %200, 1
  %202 = mul nuw nsw i64 %200, 3
  %203 = shl nuw nsw i64 %200, 2
  %204 = mul nuw nsw i64 %200, 5
  %205 = mul nuw nsw i64 %200, 6
  %206 = mul nuw nsw i64 %200, 7
  %207 = shl nuw nsw i64 %200, 3
  %208 = mul nuw nsw i64 %200, 9
  %209 = mul nuw nsw i64 %200, 10
  %210 = mul nuw nsw i64 %200, 11
  %211 = mul nuw nsw i64 %200, 12
  %212 = mul nuw nsw i64 %200, 13
  %213 = mul nuw nsw i64 %200, 14
  %214 = mul nuw nsw i64 %200, 15
  %215 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %216 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %217 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %218 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %219 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %220 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %221 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %222 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %223 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %224 = zext i8 %11 to i64
  br label %535

225:                                              ; preds = %198
  %226 = bitcast i8* %3 to i64*
  %227 = load i64, i64* %226, align 1
  %228 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %227, i32 0
  %229 = getelementptr inbounds i8, i8* %3, i64 8
  %230 = bitcast i8* %229 to i64*
  %231 = load i64, i64* %230, align 1
  %232 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %231, i32 0
  %233 = getelementptr inbounds i8, i8* %3, i64 16
  %234 = bitcast i8* %233 to i64*
  %235 = load i64, i64* %234, align 1
  %236 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %235, i32 0
  %237 = getelementptr inbounds i8, i8* %3, i64 24
  %238 = bitcast i8* %237 to i64*
  %239 = load i64, i64* %238, align 1
  %240 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %239, i32 0
  %241 = getelementptr inbounds i8, i8* %3, i64 32
  %242 = bitcast i8* %241 to i64*
  %243 = load i64, i64* %242, align 1
  %244 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %243, i32 0
  %245 = getelementptr inbounds i8, i8* %3, i64 40
  %246 = bitcast i8* %245 to i64*
  %247 = load i64, i64* %246, align 1
  %248 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %247, i32 0
  %249 = getelementptr inbounds i8, i8* %3, i64 48
  %250 = bitcast i8* %249 to i64*
  %251 = load i64, i64* %250, align 1
  %252 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %251, i32 0
  %253 = getelementptr inbounds i8, i8* %3, i64 56
  %254 = bitcast i8* %253 to i64*
  %255 = load i64, i64* %254, align 1
  %256 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %255, i32 0
  %257 = getelementptr inbounds i8, i8* %3, i64 64
  %258 = bitcast i8* %257 to i64*
  %259 = load i64, i64* %258, align 1
  %260 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %259, i32 0
  %261 = getelementptr inbounds i8, i8* %3, i64 72
  %262 = bitcast i8* %261 to i64*
  %263 = load i64, i64* %262, align 1
  %264 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %263, i32 0
  %265 = getelementptr inbounds i8, i8* %3, i64 80
  %266 = bitcast i8* %265 to i64*
  %267 = load i64, i64* %266, align 1
  %268 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %267, i32 0
  %269 = getelementptr inbounds i8, i8* %3, i64 88
  %270 = bitcast i8* %269 to i64*
  %271 = load i64, i64* %270, align 1
  %272 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %271, i32 0
  %273 = getelementptr inbounds i8, i8* %3, i64 96
  %274 = bitcast i8* %273 to i64*
  %275 = load i64, i64* %274, align 1
  %276 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %275, i32 0
  %277 = getelementptr inbounds i8, i8* %3, i64 104
  %278 = bitcast i8* %277 to i64*
  %279 = load i64, i64* %278, align 1
  %280 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %279, i32 0
  %281 = getelementptr inbounds i8, i8* %3, i64 112
  %282 = bitcast i8* %281 to i64*
  %283 = load i64, i64* %282, align 1
  %284 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %283, i32 0
  %285 = getelementptr inbounds i8, i8* %3, i64 120
  %286 = bitcast i8* %285 to i64*
  %287 = load i64, i64* %286, align 1
  %288 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %287, i32 0
  %289 = bitcast <2 x i64> %228 to <8 x i16>
  %290 = bitcast <2 x i64> %260 to <8 x i16>
  %291 = shufflevector <8 x i16> %289, <8 x i16> %290, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %292 = shufflevector <8 x i16> %290, <8 x i16> %289, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %293 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %294 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %291, <8 x i16> %293) #8
  %295 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %292, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %296 = add <4 x i32> %294, <i32 2048, i32 2048, i32 2048, i32 2048>
  %297 = ashr <4 x i32> %296, <i32 12, i32 12, i32 12, i32 12>
  %298 = add <4 x i32> %295, <i32 2048, i32 2048, i32 2048, i32 2048>
  %299 = ashr <4 x i32> %298, <i32 12, i32 12, i32 12, i32 12>
  %300 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %297, <4 x i32> %297) #8
  %301 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %299, <4 x i32> %299) #8
  %302 = bitcast <2 x i64> %244 to <8 x i16>
  %303 = bitcast <2 x i64> %276 to <8 x i16>
  %304 = shufflevector <8 x i16> %302, <8 x i16> %303, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %305 = shufflevector <8 x i16> %303, <8 x i16> %302, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %306 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %307 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %304, <8 x i16> %306) #8
  %308 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %305, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %309 = add <4 x i32> %307, <i32 2048, i32 2048, i32 2048, i32 2048>
  %310 = ashr <4 x i32> %309, <i32 12, i32 12, i32 12, i32 12>
  %311 = add <4 x i32> %308, <i32 2048, i32 2048, i32 2048, i32 2048>
  %312 = ashr <4 x i32> %311, <i32 12, i32 12, i32 12, i32 12>
  %313 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %310, <4 x i32> %310) #8
  %314 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %312, <4 x i32> %312) #8
  %315 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %301, <8 x i16> %314) #8
  %316 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %301, <8 x i16> %314) #8
  %317 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %300, <8 x i16> %313) #8
  %318 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %300, <8 x i16> %313) #8
  %319 = bitcast <2 x i64> %236 to <8 x i16>
  %320 = bitcast <2 x i64> %284 to <8 x i16>
  %321 = shufflevector <8 x i16> %319, <8 x i16> %320, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %322 = shufflevector <8 x i16> %320, <8 x i16> %319, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %323 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %324 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %321, <8 x i16> %323) #8
  %325 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %322, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %326 = add <4 x i32> %324, <i32 2048, i32 2048, i32 2048, i32 2048>
  %327 = ashr <4 x i32> %326, <i32 12, i32 12, i32 12, i32 12>
  %328 = add <4 x i32> %325, <i32 2048, i32 2048, i32 2048, i32 2048>
  %329 = ashr <4 x i32> %328, <i32 12, i32 12, i32 12, i32 12>
  %330 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %327, <4 x i32> %327) #8
  %331 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %329, <4 x i32> %329) #8
  %332 = bitcast <2 x i64> %268 to <8 x i16>
  %333 = bitcast <2 x i64> %252 to <8 x i16>
  %334 = shufflevector <8 x i16> %332, <8 x i16> %333, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %335 = shufflevector <8 x i16> %333, <8 x i16> %332, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %336 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %337 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %334, <8 x i16> %336) #8
  %338 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %335, <8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>) #8
  %339 = add <4 x i32> %337, <i32 2048, i32 2048, i32 2048, i32 2048>
  %340 = ashr <4 x i32> %339, <i32 12, i32 12, i32 12, i32 12>
  %341 = add <4 x i32> %338, <i32 2048, i32 2048, i32 2048, i32 2048>
  %342 = ashr <4 x i32> %341, <i32 12, i32 12, i32 12, i32 12>
  %343 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %340, <4 x i32> %340) #8
  %344 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %342, <4 x i32> %342) #8
  %345 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %330, <8 x i16> %343) #8
  %346 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %330, <8 x i16> %343) #8
  %347 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %331, <8 x i16> %344) #8
  %348 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %331, <8 x i16> %344) #8
  %349 = shufflevector <8 x i16> %348, <8 x i16> %346, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %350 = shufflevector <8 x i16> %346, <8 x i16> %348, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %351 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %349, <8 x i16> %293) #8
  %352 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %350, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %353 = add <4 x i32> %351, <i32 2048, i32 2048, i32 2048, i32 2048>
  %354 = ashr <4 x i32> %353, <i32 12, i32 12, i32 12, i32 12>
  %355 = add <4 x i32> %352, <i32 2048, i32 2048, i32 2048, i32 2048>
  %356 = ashr <4 x i32> %355, <i32 12, i32 12, i32 12, i32 12>
  %357 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %354, <4 x i32> %354) #8
  %358 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %356, <4 x i32> %356) #8
  %359 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %315, <8 x i16> %347) #8
  %360 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %315, <8 x i16> %347) #8
  %361 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %317, <8 x i16> %358) #8
  %362 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %317, <8 x i16> %358) #8
  %363 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %318, <8 x i16> %357) #8
  %364 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %318, <8 x i16> %357) #8
  %365 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %316, <8 x i16> %345) #8
  %366 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %316, <8 x i16> %345) #8
  %367 = bitcast <2 x i64> %232 to <8 x i16>
  %368 = bitcast <2 x i64> %288 to <8 x i16>
  %369 = shufflevector <8 x i16> %367, <8 x i16> %368, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %370 = shufflevector <8 x i16> %368, <8 x i16> %367, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %371 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %372 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %369, <8 x i16> %371) #8
  %373 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %370, <8 x i16> <i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076>) #8
  %374 = add <4 x i32> %372, <i32 2048, i32 2048, i32 2048, i32 2048>
  %375 = ashr <4 x i32> %374, <i32 12, i32 12, i32 12, i32 12>
  %376 = add <4 x i32> %373, <i32 2048, i32 2048, i32 2048, i32 2048>
  %377 = ashr <4 x i32> %376, <i32 12, i32 12, i32 12, i32 12>
  %378 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %375, <4 x i32> %375) #8
  %379 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %377, <4 x i32> %377) #8
  %380 = bitcast <2 x i64> %264 to <8 x i16>
  %381 = bitcast <2 x i64> %256 to <8 x i16>
  %382 = shufflevector <8 x i16> %380, <8 x i16> %381, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %383 = shufflevector <8 x i16> %381, <8 x i16> %380, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %384 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %385 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %382, <8 x i16> %384) #8
  %386 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %383, <8 x i16> <i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598>) #8
  %387 = add <4 x i32> %385, <i32 2048, i32 2048, i32 2048, i32 2048>
  %388 = ashr <4 x i32> %387, <i32 12, i32 12, i32 12, i32 12>
  %389 = add <4 x i32> %386, <i32 2048, i32 2048, i32 2048, i32 2048>
  %390 = ashr <4 x i32> %389, <i32 12, i32 12, i32 12, i32 12>
  %391 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %388, <4 x i32> %388) #8
  %392 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %390, <4 x i32> %390) #8
  %393 = bitcast <2 x i64> %248 to <8 x i16>
  %394 = bitcast <2 x i64> %272 to <8 x i16>
  %395 = shufflevector <8 x i16> %393, <8 x i16> %394, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %396 = shufflevector <8 x i16> %394, <8 x i16> %393, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %397 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %398 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %395, <8 x i16> %397) #8
  %399 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %396, <8 x i16> <i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612>) #8
  %400 = add <4 x i32> %398, <i32 2048, i32 2048, i32 2048, i32 2048>
  %401 = ashr <4 x i32> %400, <i32 12, i32 12, i32 12, i32 12>
  %402 = add <4 x i32> %399, <i32 2048, i32 2048, i32 2048, i32 2048>
  %403 = ashr <4 x i32> %402, <i32 12, i32 12, i32 12, i32 12>
  %404 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %401, <4 x i32> %401) #8
  %405 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %403, <4 x i32> %403) #8
  %406 = bitcast <2 x i64> %280 to <8 x i16>
  %407 = bitcast <2 x i64> %240 to <8 x i16>
  %408 = shufflevector <8 x i16> %406, <8 x i16> %407, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %409 = shufflevector <8 x i16> %407, <8 x i16> %406, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %410 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %411 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %408, <8 x i16> %410) #8
  %412 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %409, <8 x i16> <i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189>) #8
  %413 = add <4 x i32> %411, <i32 2048, i32 2048, i32 2048, i32 2048>
  %414 = ashr <4 x i32> %413, <i32 12, i32 12, i32 12, i32 12>
  %415 = add <4 x i32> %412, <i32 2048, i32 2048, i32 2048, i32 2048>
  %416 = ashr <4 x i32> %415, <i32 12, i32 12, i32 12, i32 12>
  %417 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %414, <4 x i32> %414) #8
  %418 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %416, <4 x i32> %416) #8
  %419 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %378, <8 x i16> %391) #8
  %420 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %378, <8 x i16> %391) #8
  %421 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %417, <8 x i16> %404) #8
  %422 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %417, <8 x i16> %404) #8
  %423 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %418, <8 x i16> %405) #8
  %424 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %418, <8 x i16> %405) #8
  %425 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %379, <8 x i16> %392) #8
  %426 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %379, <8 x i16> %392) #8
  %427 = shufflevector <8 x i16> %426, <8 x i16> %420, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %428 = shufflevector <8 x i16> %420, <8 x i16> %426, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %429 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %427, <8 x i16> %306) #8
  %430 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %428, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %431 = add <4 x i32> %429, <i32 2048, i32 2048, i32 2048, i32 2048>
  %432 = ashr <4 x i32> %431, <i32 12, i32 12, i32 12, i32 12>
  %433 = add <4 x i32> %430, <i32 2048, i32 2048, i32 2048, i32 2048>
  %434 = ashr <4 x i32> %433, <i32 12, i32 12, i32 12, i32 12>
  %435 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %432, <4 x i32> %432) #8
  %436 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %434, <4 x i32> %434) #8
  %437 = shufflevector <8 x i16> %424, <8 x i16> %422, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %438 = shufflevector <8 x i16> %422, <8 x i16> %424, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %439 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %440 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %437, <8 x i16> %439) #8
  %441 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %438, <8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>) #8
  %442 = add <4 x i32> %440, <i32 2048, i32 2048, i32 2048, i32 2048>
  %443 = ashr <4 x i32> %442, <i32 12, i32 12, i32 12, i32 12>
  %444 = add <4 x i32> %441, <i32 2048, i32 2048, i32 2048, i32 2048>
  %445 = ashr <4 x i32> %444, <i32 12, i32 12, i32 12, i32 12>
  %446 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %443, <4 x i32> %443) #8
  %447 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %445, <4 x i32> %445) #8
  %448 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %419, <8 x i16> %421) #8
  %449 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %419, <8 x i16> %421) #8
  %450 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %435, <8 x i16> %446) #8
  %451 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %435, <8 x i16> %446) #8
  %452 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %425, <8 x i16> %423) #8
  %453 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %425, <8 x i16> %423) #8
  %454 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %436, <8 x i16> %447) #8
  %455 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %436, <8 x i16> %447) #8
  %456 = shufflevector <8 x i16> %455, <8 x i16> %451, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %457 = shufflevector <8 x i16> %451, <8 x i16> %455, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %458 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %456, <8 x i16> %293) #8
  %459 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %457, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %460 = add <4 x i32> %458, <i32 2048, i32 2048, i32 2048, i32 2048>
  %461 = ashr <4 x i32> %460, <i32 12, i32 12, i32 12, i32 12>
  %462 = add <4 x i32> %459, <i32 2048, i32 2048, i32 2048, i32 2048>
  %463 = ashr <4 x i32> %462, <i32 12, i32 12, i32 12, i32 12>
  %464 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %461, <4 x i32> %461) #8
  %465 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %463, <4 x i32> %463) #8
  %466 = shufflevector <8 x i16> %453, <8 x i16> %449, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %467 = shufflevector <8 x i16> %449, <8 x i16> %453, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %468 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %466, <8 x i16> %293) #8
  %469 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %467, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %470 = add <4 x i32> %468, <i32 2048, i32 2048, i32 2048, i32 2048>
  %471 = ashr <4 x i32> %470, <i32 12, i32 12, i32 12, i32 12>
  %472 = add <4 x i32> %469, <i32 2048, i32 2048, i32 2048, i32 2048>
  %473 = ashr <4 x i32> %472, <i32 12, i32 12, i32 12, i32 12>
  %474 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %471, <4 x i32> %471) #8
  %475 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %473, <4 x i32> %473) #8
  %476 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %359, <8 x i16> %452) #8
  %477 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %359, <8 x i16> %452) #8
  %478 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %361, <8 x i16> %454) #8
  %479 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %361, <8 x i16> %454) #8
  %480 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %363, <8 x i16> %465) #8
  %481 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %363, <8 x i16> %465) #8
  %482 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %365, <8 x i16> %475) #8
  %483 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %365, <8 x i16> %475) #8
  %484 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %366, <8 x i16> %474) #8
  %485 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %366, <8 x i16> %474) #8
  %486 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %364, <8 x i16> %464) #8
  %487 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %364, <8 x i16> %464) #8
  %488 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %362, <8 x i16> %450) #8
  %489 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %362, <8 x i16> %450) #8
  %490 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %360, <8 x i16> %448) #8
  %491 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %360, <8 x i16> %448) #8
  %492 = bitcast <8 x i16> %491 to <2 x i64>
  %493 = bitcast <8 x i16> %490 to <2 x i64>
  %494 = bitcast <8 x i16> %489 to <2 x i64>
  %495 = bitcast <8 x i16> %488 to <2 x i64>
  %496 = bitcast <8 x i16> %487 to <2 x i64>
  %497 = bitcast <8 x i16> %486 to <2 x i64>
  %498 = bitcast <8 x i16> %485 to <2 x i64>
  %499 = bitcast <8 x i16> %484 to <2 x i64>
  %500 = bitcast <8 x i16> %483 to <2 x i64>
  %501 = bitcast <8 x i16> %482 to <2 x i64>
  %502 = bitcast <8 x i16> %481 to <2 x i64>
  %503 = bitcast <8 x i16> %480 to <2 x i64>
  %504 = bitcast <8 x i16> %479 to <2 x i64>
  %505 = bitcast <8 x i16> %478 to <2 x i64>
  %506 = bitcast <8 x i16> %477 to <2 x i64>
  %507 = bitcast <8 x i16> %476 to <2 x i64>
  %508 = extractelement <2 x i64> %507, i32 0
  store i64 %508, i64* %226, align 1
  %509 = extractelement <2 x i64> %505, i32 0
  store i64 %509, i64* %230, align 1
  %510 = extractelement <2 x i64> %503, i32 0
  store i64 %510, i64* %234, align 1
  %511 = extractelement <2 x i64> %501, i32 0
  store i64 %511, i64* %238, align 1
  %512 = extractelement <2 x i64> %499, i32 0
  store i64 %512, i64* %242, align 1
  %513 = extractelement <2 x i64> %497, i32 0
  store i64 %513, i64* %246, align 1
  %514 = extractelement <2 x i64> %495, i32 0
  store i64 %514, i64* %250, align 1
  %515 = extractelement <2 x i64> %493, i32 0
  store i64 %515, i64* %254, align 1
  %516 = extractelement <2 x i64> %492, i32 0
  store i64 %516, i64* %258, align 1
  %517 = extractelement <2 x i64> %494, i32 0
  store i64 %517, i64* %262, align 1
  %518 = extractelement <2 x i64> %496, i32 0
  store i64 %518, i64* %266, align 1
  %519 = extractelement <2 x i64> %498, i32 0
  store i64 %519, i64* %270, align 1
  %520 = extractelement <2 x i64> %500, i32 0
  store i64 %520, i64* %274, align 1
  %521 = extractelement <2 x i64> %502, i32 0
  store i64 %521, i64* %278, align 1
  %522 = extractelement <2 x i64> %504, i32 0
  store i64 %522, i64* %282, align 1
  %523 = extractelement <2 x i64> %506, i32 0
  store i64 %523, i64* %286, align 1
  %524 = bitcast i8* %6 to i64*
  %525 = load i64, i64* %524, align 8
  %526 = getelementptr inbounds i8, i8* %6, i64 8
  %527 = bitcast i8* %526 to i8**
  %528 = load i8*, i8** %527, align 8
  %529 = sext i32 %5 to i64
  %530 = ashr i64 %525, 32
  %531 = mul nsw i64 %530, %529
  %532 = getelementptr inbounds i8, i8* %528, i64 %531
  %533 = sext i32 %4 to i64
  %534 = getelementptr inbounds i8, i8* %532, i64 %533
  br label %881

535:                                              ; preds = %199, %535
  %536 = phi i64 [ 0, %199 ], [ %867, %535 ]
  %537 = getelementptr inbounds i16, i16* %8, i64 %536
  %538 = bitcast i16* %537 to <8 x i16>*
  %539 = load <8 x i16>, <8 x i16>* %538, align 1
  %540 = getelementptr inbounds i16, i16* %537, i64 %200
  %541 = bitcast i16* %540 to <8 x i16>*
  %542 = load <8 x i16>, <8 x i16>* %541, align 1
  %543 = getelementptr inbounds i16, i16* %537, i64 %201
  %544 = bitcast i16* %543 to <8 x i16>*
  %545 = load <8 x i16>, <8 x i16>* %544, align 1
  %546 = getelementptr inbounds i16, i16* %537, i64 %202
  %547 = bitcast i16* %546 to <8 x i16>*
  %548 = load <8 x i16>, <8 x i16>* %547, align 1
  %549 = getelementptr inbounds i16, i16* %537, i64 %203
  %550 = bitcast i16* %549 to <8 x i16>*
  %551 = load <8 x i16>, <8 x i16>* %550, align 1
  %552 = getelementptr inbounds i16, i16* %537, i64 %204
  %553 = bitcast i16* %552 to <8 x i16>*
  %554 = load <8 x i16>, <8 x i16>* %553, align 1
  %555 = getelementptr inbounds i16, i16* %537, i64 %205
  %556 = bitcast i16* %555 to <8 x i16>*
  %557 = load <8 x i16>, <8 x i16>* %556, align 1
  %558 = getelementptr inbounds i16, i16* %537, i64 %206
  %559 = bitcast i16* %558 to <8 x i16>*
  %560 = load <8 x i16>, <8 x i16>* %559, align 1
  %561 = getelementptr inbounds i16, i16* %537, i64 %207
  %562 = bitcast i16* %561 to <8 x i16>*
  %563 = load <8 x i16>, <8 x i16>* %562, align 1
  %564 = getelementptr inbounds i16, i16* %537, i64 %208
  %565 = bitcast i16* %564 to <8 x i16>*
  %566 = load <8 x i16>, <8 x i16>* %565, align 1
  %567 = getelementptr inbounds i16, i16* %537, i64 %209
  %568 = bitcast i16* %567 to <8 x i16>*
  %569 = load <8 x i16>, <8 x i16>* %568, align 1
  %570 = getelementptr inbounds i16, i16* %537, i64 %210
  %571 = bitcast i16* %570 to <8 x i16>*
  %572 = load <8 x i16>, <8 x i16>* %571, align 1
  %573 = getelementptr inbounds i16, i16* %537, i64 %211
  %574 = bitcast i16* %573 to <8 x i16>*
  %575 = load <8 x i16>, <8 x i16>* %574, align 1
  %576 = getelementptr inbounds i16, i16* %537, i64 %212
  %577 = bitcast i16* %576 to <8 x i16>*
  %578 = load <8 x i16>, <8 x i16>* %577, align 1
  %579 = getelementptr inbounds i16, i16* %537, i64 %213
  %580 = bitcast i16* %579 to <8 x i16>*
  %581 = load <8 x i16>, <8 x i16>* %580, align 1
  %582 = getelementptr inbounds i16, i16* %537, i64 %214
  %583 = bitcast i16* %582 to <8 x i16>*
  %584 = load <8 x i16>, <8 x i16>* %583, align 1
  %585 = shufflevector <8 x i16> %539, <8 x i16> %563, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %586 = shufflevector <8 x i16> %563, <8 x i16> %539, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %587 = shufflevector <8 x i16> %539, <8 x i16> %563, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %588 = shufflevector <8 x i16> %563, <8 x i16> %539, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %589 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %585, <8 x i16> %215) #8
  %590 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %586, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %591 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %587, <8 x i16> %215) #8
  %592 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %588, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %593 = add <4 x i32> %589, <i32 2048, i32 2048, i32 2048, i32 2048>
  %594 = ashr <4 x i32> %593, <i32 12, i32 12, i32 12, i32 12>
  %595 = add <4 x i32> %590, <i32 2048, i32 2048, i32 2048, i32 2048>
  %596 = ashr <4 x i32> %595, <i32 12, i32 12, i32 12, i32 12>
  %597 = add <4 x i32> %591, <i32 2048, i32 2048, i32 2048, i32 2048>
  %598 = ashr <4 x i32> %597, <i32 12, i32 12, i32 12, i32 12>
  %599 = add <4 x i32> %592, <i32 2048, i32 2048, i32 2048, i32 2048>
  %600 = ashr <4 x i32> %599, <i32 12, i32 12, i32 12, i32 12>
  %601 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %594, <4 x i32> %598) #8
  %602 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %596, <4 x i32> %600) #8
  %603 = shufflevector <8 x i16> %551, <8 x i16> %575, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %604 = shufflevector <8 x i16> %575, <8 x i16> %551, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %605 = shufflevector <8 x i16> %551, <8 x i16> %575, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %606 = shufflevector <8 x i16> %575, <8 x i16> %551, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %607 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %603, <8 x i16> %216) #8
  %608 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %604, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %609 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %605, <8 x i16> %216) #8
  %610 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %606, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %611 = add <4 x i32> %607, <i32 2048, i32 2048, i32 2048, i32 2048>
  %612 = ashr <4 x i32> %611, <i32 12, i32 12, i32 12, i32 12>
  %613 = add <4 x i32> %608, <i32 2048, i32 2048, i32 2048, i32 2048>
  %614 = ashr <4 x i32> %613, <i32 12, i32 12, i32 12, i32 12>
  %615 = add <4 x i32> %609, <i32 2048, i32 2048, i32 2048, i32 2048>
  %616 = ashr <4 x i32> %615, <i32 12, i32 12, i32 12, i32 12>
  %617 = add <4 x i32> %610, <i32 2048, i32 2048, i32 2048, i32 2048>
  %618 = ashr <4 x i32> %617, <i32 12, i32 12, i32 12, i32 12>
  %619 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %612, <4 x i32> %616) #8
  %620 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %614, <4 x i32> %618) #8
  %621 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %602, <8 x i16> %620) #8
  %622 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %602, <8 x i16> %620) #8
  %623 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %601, <8 x i16> %619) #8
  %624 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %601, <8 x i16> %619) #8
  %625 = shufflevector <8 x i16> %545, <8 x i16> %581, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %626 = shufflevector <8 x i16> %581, <8 x i16> %545, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %627 = shufflevector <8 x i16> %545, <8 x i16> %581, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %628 = shufflevector <8 x i16> %581, <8 x i16> %545, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %629 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %625, <8 x i16> %217) #8
  %630 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %626, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %631 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %627, <8 x i16> %217) #8
  %632 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %628, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %633 = add <4 x i32> %629, <i32 2048, i32 2048, i32 2048, i32 2048>
  %634 = ashr <4 x i32> %633, <i32 12, i32 12, i32 12, i32 12>
  %635 = add <4 x i32> %630, <i32 2048, i32 2048, i32 2048, i32 2048>
  %636 = ashr <4 x i32> %635, <i32 12, i32 12, i32 12, i32 12>
  %637 = add <4 x i32> %631, <i32 2048, i32 2048, i32 2048, i32 2048>
  %638 = ashr <4 x i32> %637, <i32 12, i32 12, i32 12, i32 12>
  %639 = add <4 x i32> %632, <i32 2048, i32 2048, i32 2048, i32 2048>
  %640 = ashr <4 x i32> %639, <i32 12, i32 12, i32 12, i32 12>
  %641 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %634, <4 x i32> %638) #8
  %642 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %636, <4 x i32> %640) #8
  %643 = shufflevector <8 x i16> %569, <8 x i16> %557, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %644 = shufflevector <8 x i16> %557, <8 x i16> %569, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %645 = shufflevector <8 x i16> %569, <8 x i16> %557, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %646 = shufflevector <8 x i16> %557, <8 x i16> %569, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %647 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %643, <8 x i16> %218) #8
  %648 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %644, <8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>) #8
  %649 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %645, <8 x i16> %218) #8
  %650 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %646, <8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>) #8
  %651 = add <4 x i32> %647, <i32 2048, i32 2048, i32 2048, i32 2048>
  %652 = ashr <4 x i32> %651, <i32 12, i32 12, i32 12, i32 12>
  %653 = add <4 x i32> %648, <i32 2048, i32 2048, i32 2048, i32 2048>
  %654 = ashr <4 x i32> %653, <i32 12, i32 12, i32 12, i32 12>
  %655 = add <4 x i32> %649, <i32 2048, i32 2048, i32 2048, i32 2048>
  %656 = ashr <4 x i32> %655, <i32 12, i32 12, i32 12, i32 12>
  %657 = add <4 x i32> %650, <i32 2048, i32 2048, i32 2048, i32 2048>
  %658 = ashr <4 x i32> %657, <i32 12, i32 12, i32 12, i32 12>
  %659 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %652, <4 x i32> %656) #8
  %660 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %654, <4 x i32> %658) #8
  %661 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %641, <8 x i16> %659) #8
  %662 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %641, <8 x i16> %659) #8
  %663 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %642, <8 x i16> %660) #8
  %664 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %642, <8 x i16> %660) #8
  %665 = shufflevector <8 x i16> %664, <8 x i16> %662, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %666 = shufflevector <8 x i16> %662, <8 x i16> %664, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %667 = shufflevector <8 x i16> %664, <8 x i16> %662, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %668 = shufflevector <8 x i16> %662, <8 x i16> %664, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %669 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %665, <8 x i16> %215) #8
  %670 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %666, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %671 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %667, <8 x i16> %215) #8
  %672 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %668, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %673 = add <4 x i32> %669, <i32 2048, i32 2048, i32 2048, i32 2048>
  %674 = ashr <4 x i32> %673, <i32 12, i32 12, i32 12, i32 12>
  %675 = add <4 x i32> %670, <i32 2048, i32 2048, i32 2048, i32 2048>
  %676 = ashr <4 x i32> %675, <i32 12, i32 12, i32 12, i32 12>
  %677 = add <4 x i32> %671, <i32 2048, i32 2048, i32 2048, i32 2048>
  %678 = ashr <4 x i32> %677, <i32 12, i32 12, i32 12, i32 12>
  %679 = add <4 x i32> %672, <i32 2048, i32 2048, i32 2048, i32 2048>
  %680 = ashr <4 x i32> %679, <i32 12, i32 12, i32 12, i32 12>
  %681 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %674, <4 x i32> %678) #8
  %682 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %676, <4 x i32> %680) #8
  %683 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %621, <8 x i16> %663) #8
  %684 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %621, <8 x i16> %663) #8
  %685 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %623, <8 x i16> %682) #8
  %686 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %623, <8 x i16> %682) #8
  %687 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %624, <8 x i16> %681) #8
  %688 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %624, <8 x i16> %681) #8
  %689 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %622, <8 x i16> %661) #8
  %690 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %622, <8 x i16> %661) #8
  %691 = shufflevector <8 x i16> %542, <8 x i16> %584, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %692 = shufflevector <8 x i16> %584, <8 x i16> %542, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %693 = shufflevector <8 x i16> %542, <8 x i16> %584, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %694 = shufflevector <8 x i16> %584, <8 x i16> %542, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %695 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %691, <8 x i16> %219) #8
  %696 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %692, <8 x i16> <i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076>) #8
  %697 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %693, <8 x i16> %219) #8
  %698 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %694, <8 x i16> <i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076>) #8
  %699 = add <4 x i32> %695, <i32 2048, i32 2048, i32 2048, i32 2048>
  %700 = ashr <4 x i32> %699, <i32 12, i32 12, i32 12, i32 12>
  %701 = add <4 x i32> %696, <i32 2048, i32 2048, i32 2048, i32 2048>
  %702 = ashr <4 x i32> %701, <i32 12, i32 12, i32 12, i32 12>
  %703 = add <4 x i32> %697, <i32 2048, i32 2048, i32 2048, i32 2048>
  %704 = ashr <4 x i32> %703, <i32 12, i32 12, i32 12, i32 12>
  %705 = add <4 x i32> %698, <i32 2048, i32 2048, i32 2048, i32 2048>
  %706 = ashr <4 x i32> %705, <i32 12, i32 12, i32 12, i32 12>
  %707 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %700, <4 x i32> %704) #8
  %708 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %702, <4 x i32> %706) #8
  %709 = shufflevector <8 x i16> %566, <8 x i16> %560, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %710 = shufflevector <8 x i16> %560, <8 x i16> %566, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %711 = shufflevector <8 x i16> %566, <8 x i16> %560, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %712 = shufflevector <8 x i16> %560, <8 x i16> %566, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %713 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %709, <8 x i16> %220) #8
  %714 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %710, <8 x i16> <i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598>) #8
  %715 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %711, <8 x i16> %220) #8
  %716 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %712, <8 x i16> <i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598>) #8
  %717 = add <4 x i32> %713, <i32 2048, i32 2048, i32 2048, i32 2048>
  %718 = ashr <4 x i32> %717, <i32 12, i32 12, i32 12, i32 12>
  %719 = add <4 x i32> %714, <i32 2048, i32 2048, i32 2048, i32 2048>
  %720 = ashr <4 x i32> %719, <i32 12, i32 12, i32 12, i32 12>
  %721 = add <4 x i32> %715, <i32 2048, i32 2048, i32 2048, i32 2048>
  %722 = ashr <4 x i32> %721, <i32 12, i32 12, i32 12, i32 12>
  %723 = add <4 x i32> %716, <i32 2048, i32 2048, i32 2048, i32 2048>
  %724 = ashr <4 x i32> %723, <i32 12, i32 12, i32 12, i32 12>
  %725 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %718, <4 x i32> %722) #8
  %726 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %720, <4 x i32> %724) #8
  %727 = shufflevector <8 x i16> %554, <8 x i16> %572, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %728 = shufflevector <8 x i16> %572, <8 x i16> %554, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %729 = shufflevector <8 x i16> %554, <8 x i16> %572, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %730 = shufflevector <8 x i16> %572, <8 x i16> %554, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %731 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %727, <8 x i16> %221) #8
  %732 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %728, <8 x i16> <i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612>) #8
  %733 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %729, <8 x i16> %221) #8
  %734 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %730, <8 x i16> <i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612>) #8
  %735 = add <4 x i32> %731, <i32 2048, i32 2048, i32 2048, i32 2048>
  %736 = ashr <4 x i32> %735, <i32 12, i32 12, i32 12, i32 12>
  %737 = add <4 x i32> %732, <i32 2048, i32 2048, i32 2048, i32 2048>
  %738 = ashr <4 x i32> %737, <i32 12, i32 12, i32 12, i32 12>
  %739 = add <4 x i32> %733, <i32 2048, i32 2048, i32 2048, i32 2048>
  %740 = ashr <4 x i32> %739, <i32 12, i32 12, i32 12, i32 12>
  %741 = add <4 x i32> %734, <i32 2048, i32 2048, i32 2048, i32 2048>
  %742 = ashr <4 x i32> %741, <i32 12, i32 12, i32 12, i32 12>
  %743 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %736, <4 x i32> %740) #8
  %744 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %738, <4 x i32> %742) #8
  %745 = shufflevector <8 x i16> %578, <8 x i16> %548, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %746 = shufflevector <8 x i16> %548, <8 x i16> %578, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %747 = shufflevector <8 x i16> %578, <8 x i16> %548, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %748 = shufflevector <8 x i16> %548, <8 x i16> %578, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %749 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %745, <8 x i16> %222) #8
  %750 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %746, <8 x i16> <i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189>) #8
  %751 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %747, <8 x i16> %222) #8
  %752 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %748, <8 x i16> <i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189>) #8
  %753 = add <4 x i32> %749, <i32 2048, i32 2048, i32 2048, i32 2048>
  %754 = ashr <4 x i32> %753, <i32 12, i32 12, i32 12, i32 12>
  %755 = add <4 x i32> %750, <i32 2048, i32 2048, i32 2048, i32 2048>
  %756 = ashr <4 x i32> %755, <i32 12, i32 12, i32 12, i32 12>
  %757 = add <4 x i32> %751, <i32 2048, i32 2048, i32 2048, i32 2048>
  %758 = ashr <4 x i32> %757, <i32 12, i32 12, i32 12, i32 12>
  %759 = add <4 x i32> %752, <i32 2048, i32 2048, i32 2048, i32 2048>
  %760 = ashr <4 x i32> %759, <i32 12, i32 12, i32 12, i32 12>
  %761 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %754, <4 x i32> %758) #8
  %762 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %756, <4 x i32> %760) #8
  %763 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %707, <8 x i16> %725) #8
  %764 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %707, <8 x i16> %725) #8
  %765 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %761, <8 x i16> %743) #8
  %766 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %761, <8 x i16> %743) #8
  %767 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %762, <8 x i16> %744) #8
  %768 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %762, <8 x i16> %744) #8
  %769 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %708, <8 x i16> %726) #8
  %770 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %708, <8 x i16> %726) #8
  %771 = shufflevector <8 x i16> %770, <8 x i16> %764, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %772 = shufflevector <8 x i16> %764, <8 x i16> %770, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %773 = shufflevector <8 x i16> %770, <8 x i16> %764, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %774 = shufflevector <8 x i16> %764, <8 x i16> %770, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %775 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %771, <8 x i16> %216) #8
  %776 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %772, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %777 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %773, <8 x i16> %216) #8
  %778 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %774, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %779 = add <4 x i32> %775, <i32 2048, i32 2048, i32 2048, i32 2048>
  %780 = ashr <4 x i32> %779, <i32 12, i32 12, i32 12, i32 12>
  %781 = add <4 x i32> %776, <i32 2048, i32 2048, i32 2048, i32 2048>
  %782 = ashr <4 x i32> %781, <i32 12, i32 12, i32 12, i32 12>
  %783 = add <4 x i32> %777, <i32 2048, i32 2048, i32 2048, i32 2048>
  %784 = ashr <4 x i32> %783, <i32 12, i32 12, i32 12, i32 12>
  %785 = add <4 x i32> %778, <i32 2048, i32 2048, i32 2048, i32 2048>
  %786 = ashr <4 x i32> %785, <i32 12, i32 12, i32 12, i32 12>
  %787 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %780, <4 x i32> %784) #8
  %788 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %782, <4 x i32> %786) #8
  %789 = shufflevector <8 x i16> %768, <8 x i16> %766, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %790 = shufflevector <8 x i16> %766, <8 x i16> %768, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %791 = shufflevector <8 x i16> %768, <8 x i16> %766, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %792 = shufflevector <8 x i16> %766, <8 x i16> %768, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %793 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %789, <8 x i16> %223) #8
  %794 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %790, <8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>) #8
  %795 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %791, <8 x i16> %223) #8
  %796 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %792, <8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>) #8
  %797 = add <4 x i32> %793, <i32 2048, i32 2048, i32 2048, i32 2048>
  %798 = ashr <4 x i32> %797, <i32 12, i32 12, i32 12, i32 12>
  %799 = add <4 x i32> %794, <i32 2048, i32 2048, i32 2048, i32 2048>
  %800 = ashr <4 x i32> %799, <i32 12, i32 12, i32 12, i32 12>
  %801 = add <4 x i32> %795, <i32 2048, i32 2048, i32 2048, i32 2048>
  %802 = ashr <4 x i32> %801, <i32 12, i32 12, i32 12, i32 12>
  %803 = add <4 x i32> %796, <i32 2048, i32 2048, i32 2048, i32 2048>
  %804 = ashr <4 x i32> %803, <i32 12, i32 12, i32 12, i32 12>
  %805 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %798, <4 x i32> %802) #8
  %806 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %800, <4 x i32> %804) #8
  %807 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %763, <8 x i16> %765) #8
  %808 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %763, <8 x i16> %765) #8
  %809 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %787, <8 x i16> %805) #8
  %810 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %787, <8 x i16> %805) #8
  %811 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %769, <8 x i16> %767) #8
  %812 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %769, <8 x i16> %767) #8
  %813 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %788, <8 x i16> %806) #8
  %814 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %788, <8 x i16> %806) #8
  %815 = shufflevector <8 x i16> %814, <8 x i16> %810, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %816 = shufflevector <8 x i16> %810, <8 x i16> %814, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %817 = shufflevector <8 x i16> %814, <8 x i16> %810, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %818 = shufflevector <8 x i16> %810, <8 x i16> %814, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %819 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %815, <8 x i16> %215) #8
  %820 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %816, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %821 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %817, <8 x i16> %215) #8
  %822 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %818, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %823 = add <4 x i32> %819, <i32 2048, i32 2048, i32 2048, i32 2048>
  %824 = ashr <4 x i32> %823, <i32 12, i32 12, i32 12, i32 12>
  %825 = add <4 x i32> %820, <i32 2048, i32 2048, i32 2048, i32 2048>
  %826 = ashr <4 x i32> %825, <i32 12, i32 12, i32 12, i32 12>
  %827 = add <4 x i32> %821, <i32 2048, i32 2048, i32 2048, i32 2048>
  %828 = ashr <4 x i32> %827, <i32 12, i32 12, i32 12, i32 12>
  %829 = add <4 x i32> %822, <i32 2048, i32 2048, i32 2048, i32 2048>
  %830 = ashr <4 x i32> %829, <i32 12, i32 12, i32 12, i32 12>
  %831 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %824, <4 x i32> %828) #8
  %832 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %826, <4 x i32> %830) #8
  %833 = shufflevector <8 x i16> %812, <8 x i16> %808, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %834 = shufflevector <8 x i16> %808, <8 x i16> %812, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %835 = shufflevector <8 x i16> %812, <8 x i16> %808, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %836 = shufflevector <8 x i16> %808, <8 x i16> %812, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %837 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %833, <8 x i16> %215) #8
  %838 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %834, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %839 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %835, <8 x i16> %215) #8
  %840 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %836, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %841 = add <4 x i32> %837, <i32 2048, i32 2048, i32 2048, i32 2048>
  %842 = ashr <4 x i32> %841, <i32 12, i32 12, i32 12, i32 12>
  %843 = add <4 x i32> %838, <i32 2048, i32 2048, i32 2048, i32 2048>
  %844 = ashr <4 x i32> %843, <i32 12, i32 12, i32 12, i32 12>
  %845 = add <4 x i32> %839, <i32 2048, i32 2048, i32 2048, i32 2048>
  %846 = ashr <4 x i32> %845, <i32 12, i32 12, i32 12, i32 12>
  %847 = add <4 x i32> %840, <i32 2048, i32 2048, i32 2048, i32 2048>
  %848 = ashr <4 x i32> %847, <i32 12, i32 12, i32 12, i32 12>
  %849 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %842, <4 x i32> %846) #8
  %850 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %844, <4 x i32> %848) #8
  %851 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %683, <8 x i16> %811) #8
  %852 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %683, <8 x i16> %811) #8
  %853 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %685, <8 x i16> %813) #8
  %854 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %685, <8 x i16> %813) #8
  %855 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %687, <8 x i16> %832) #8
  %856 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %687, <8 x i16> %832) #8
  %857 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %689, <8 x i16> %850) #8
  %858 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %689, <8 x i16> %850) #8
  %859 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %690, <8 x i16> %849) #8
  %860 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %690, <8 x i16> %849) #8
  %861 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %688, <8 x i16> %831) #8
  %862 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %688, <8 x i16> %831) #8
  %863 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %686, <8 x i16> %809) #8
  %864 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %686, <8 x i16> %809) #8
  %865 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %684, <8 x i16> %807) #8
  %866 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %684, <8 x i16> %807) #8
  store <8 x i16> %851, <8 x i16>* %538, align 1
  store <8 x i16> %853, <8 x i16>* %541, align 1
  store <8 x i16> %855, <8 x i16>* %544, align 1
  store <8 x i16> %857, <8 x i16>* %547, align 1
  store <8 x i16> %859, <8 x i16>* %550, align 1
  store <8 x i16> %861, <8 x i16>* %553, align 1
  store <8 x i16> %863, <8 x i16>* %556, align 1
  store <8 x i16> %865, <8 x i16>* %559, align 1
  store <8 x i16> %866, <8 x i16>* %562, align 1
  store <8 x i16> %864, <8 x i16>* %565, align 1
  store <8 x i16> %862, <8 x i16>* %568, align 1
  store <8 x i16> %860, <8 x i16>* %571, align 1
  store <8 x i16> %858, <8 x i16>* %574, align 1
  store <8 x i16> %856, <8 x i16>* %577, align 1
  store <8 x i16> %854, <8 x i16>* %580, align 1
  store <8 x i16> %852, <8 x i16>* %583, align 1
  %867 = add nuw nsw i64 %536, 8
  %868 = icmp ult i64 %867, %224
  br i1 %868, label %535, label %869

869:                                              ; preds = %535, %152
  %870 = bitcast i8* %6 to i64*
  %871 = load i64, i64* %870, align 8
  %872 = getelementptr inbounds i8, i8* %6, i64 8
  %873 = bitcast i8* %872 to i8**
  %874 = load i8*, i8** %873, align 8
  %875 = sext i32 %5 to i64
  %876 = ashr i64 %871, 32
  %877 = mul nsw i64 %876, %875
  %878 = getelementptr inbounds i8, i8* %874, i64 %877
  %879 = sext i32 %4 to i64
  %880 = getelementptr inbounds i8, i8* %878, i64 %879
  switch i8 %11, label %884 [
    i8 4, label %881
    i8 8, label %910
  ]

881:                                              ; preds = %225, %869
  %882 = phi i8* [ %534, %225 ], [ %880, %869 ]
  %883 = phi i64 [ %530, %225 ], [ %876, %869 ]
  br label %886

884:                                              ; preds = %869
  %885 = zext i8 %11 to i64
  br label %932

886:                                              ; preds = %886, %881
  %887 = phi i64 [ 0, %881 ], [ %908, %886 ]
  %888 = phi i8* [ %882, %881 ], [ %907, %886 ]
  %889 = shl i64 %887, 2
  %890 = getelementptr inbounds i16, i16* %8, i64 %889
  %891 = bitcast i16* %890 to i64*
  %892 = load i64, i64* %891, align 1
  %893 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %892, i32 0
  %894 = bitcast i8* %888 to i32*
  %895 = load i32, i32* %894, align 1
  %896 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %895, i32 0
  %897 = bitcast <2 x i64> %893 to <8 x i16>
  %898 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %897, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %899 = ashr <8 x i16> %898, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %900 = bitcast <4 x i32> %896 to <16 x i8>
  %901 = shufflevector <16 x i8> %900, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %902 = zext <8 x i8> %901 to <8 x i16>
  %903 = add nsw <8 x i16> %899, %902
  %904 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %903, <8 x i16> undef) #8
  %905 = bitcast <16 x i8> %904 to <4 x i32>
  %906 = extractelement <4 x i32> %905, i32 0
  store i32 %906, i32* %894, align 1
  %907 = getelementptr inbounds i8, i8* %888, i64 %883
  %908 = add nuw nsw i64 %887, 1
  %909 = icmp eq i64 %908, 16
  br i1 %909, label %968, label %886

910:                                              ; preds = %869, %910
  %911 = phi i64 [ %930, %910 ], [ 0, %869 ]
  %912 = phi i8* [ %929, %910 ], [ %880, %869 ]
  %913 = shl i64 %911, 3
  %914 = getelementptr inbounds i16, i16* %8, i64 %913
  %915 = bitcast i16* %914 to <8 x i16>*
  %916 = load <8 x i16>, <8 x i16>* %915, align 1
  %917 = bitcast i8* %912 to i64*
  %918 = load i64, i64* %917, align 1
  %919 = insertelement <2 x i64> undef, i64 %918, i32 0
  %920 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %916, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %921 = ashr <8 x i16> %920, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %922 = bitcast <2 x i64> %919 to <16 x i8>
  %923 = shufflevector <16 x i8> %922, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %924 = zext <8 x i8> %923 to <8 x i16>
  %925 = add nsw <8 x i16> %921, %924
  %926 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %925, <8 x i16> undef) #8
  %927 = bitcast <16 x i8> %926 to <2 x i64>
  %928 = extractelement <2 x i64> %927, i32 0
  store i64 %928, i64* %917, align 1
  %929 = getelementptr inbounds i8, i8* %912, i64 %876
  %930 = add nuw nsw i64 %911, 1
  %931 = icmp eq i64 %930, 16
  br i1 %931, label %968, label %910

932:                                              ; preds = %965, %884
  %933 = phi i64 [ 0, %884 ], [ %966, %965 ]
  %934 = add nsw i64 %933, %875
  %935 = mul nuw nsw i64 %933, %885
  %936 = mul nsw i64 %934, %876
  %937 = getelementptr inbounds i8, i8* %874, i64 %936
  br label %938

938:                                              ; preds = %938, %932
  %939 = phi i64 [ %963, %938 ], [ 0, %932 ]
  %940 = add nsw i64 %939, %879
  %941 = add nuw nsw i64 %939, %935
  %942 = getelementptr inbounds i16, i16* %8, i64 %941
  %943 = bitcast i16* %942 to <8 x i16>*
  %944 = load <8 x i16>, <8 x i16>* %943, align 1
  %945 = add nuw nsw i64 %941, 8
  %946 = getelementptr inbounds i16, i16* %8, i64 %945
  %947 = bitcast i16* %946 to <8 x i16>*
  %948 = load <8 x i16>, <8 x i16>* %947, align 1
  %949 = getelementptr inbounds i8, i8* %937, i64 %940
  %950 = bitcast i8* %949 to <16 x i8>*
  %951 = load <16 x i8>, <16 x i8>* %950, align 1
  %952 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %944, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %953 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %948, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %954 = ashr <8 x i16> %952, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %955 = ashr <8 x i16> %953, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %956 = shufflevector <16 x i8> %951, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %957 = zext <8 x i8> %956 to <8 x i16>
  %958 = shufflevector <16 x i8> %951, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %959 = zext <8 x i8> %958 to <8 x i16>
  %960 = add nsw <8 x i16> %954, %957
  %961 = add nsw <8 x i16> %955, %959
  %962 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %960, <8 x i16> %961) #8
  store <16 x i8> %962, <16 x i8>* %950, align 1
  %963 = add nuw nsw i64 %939, 16
  %964 = icmp ult i64 %963, %885
  br i1 %964, label %938, label %965

965:                                              ; preds = %938
  %966 = add nuw nsw i64 %933, 1
  %967 = icmp eq i64 %966, 16
  br i1 %967, label %968, label %932

968:                                              ; preds = %910, %965, %886
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_128Dct32TransformLoopRow_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_(i8 zeroext, i8 zeroext, i32, i8* nocapture, i32, i32, i8* nocapture readnone) #4 {
  %8 = alloca [32 x <2 x i64>], align 16
  %9 = alloca [32 x <2 x i64>], align 16
  %10 = bitcast i8* %3 to i16*
  %11 = zext i8 %1 to i64
  %12 = lshr i64 173354, %11
  %13 = and i64 %12, 1
  %14 = icmp ne i64 %13, 0
  %15 = getelementptr inbounds [19 x i8], [19 x i8]* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_118kTransformRowShiftE, i64 0, i64 %11
  %16 = load i8, i8* %15, align 1
  %17 = icmp sgt i32 %2, 1
  br i1 %17, label %62, label %18

18:                                               ; preds = %7
  %19 = zext i8 %16 to i32
  %20 = load i16, i16* %10, align 2
  %21 = sext i16 %20 to i32
  %22 = insertelement <4 x i32> undef, i32 %21, i32 0
  %23 = bitcast <4 x i32> %22 to <8 x i16>
  %24 = shufflevector <8 x i16> %23, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %25 = bitcast <8 x i16> %24 to <4 x i32>
  %26 = shufflevector <4 x i32> %25, <4 x i32> undef, <4 x i32> zeroinitializer
  %27 = sext i1 %14 to i16
  %28 = insertelement <8 x i16> undef, i16 %27, i32 0
  %29 = shufflevector <8 x i16> %28, <8 x i16> undef, <8 x i32> zeroinitializer
  %30 = bitcast <4 x i32> %26 to <8 x i16>
  %31 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %30, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  %32 = bitcast <4 x i32> %26 to <16 x i8>
  %33 = bitcast <8 x i16> %31 to <16 x i8>
  %34 = bitcast <8 x i16> %29 to <16 x i8>
  %35 = tail call <16 x i8> @llvm.x86.sse41.pblendvb(<16 x i8> %32, <16 x i8> %33, <16 x i8> %34) #8
  %36 = bitcast <16 x i8> %35 to <8 x i16>
  %37 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %36, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  %38 = insertelement <4 x i32> undef, i32 %19, i32 0
  %39 = shufflevector <4 x i32> %38, <4 x i32> undef, <4 x i32> zeroinitializer
  %40 = shufflevector <4 x i32> %38, <4 x i32> undef, <2 x i32> zeroinitializer
  %41 = zext <2 x i32> %40 to <2 x i64>
  %42 = shufflevector <8 x i16> %37, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %43 = sext <4 x i16> %42 to <4 x i32>
  %44 = bitcast <8 x i16> %37 to <16 x i8>
  %45 = shufflevector <16 x i8> %44, <16 x i8> undef, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %46 = bitcast <16 x i8> %45 to <8 x i16>
  %47 = shufflevector <8 x i16> %46, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %48 = sext <4 x i16> %47 to <4 x i32>
  %49 = add <4 x i32> %39, %43
  %50 = add <4 x i32> %39, %48
  %51 = bitcast <2 x i64> %41 to <4 x i32>
  %52 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %49, <4 x i32> %51) #8
  %53 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %50, <4 x i32> %51) #8
  %54 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %52, <4 x i32> %53) #8
  %55 = bitcast i8* %3 to <8 x i16>*
  store <8 x i16> %54, <8 x i16>* %55, align 1
  %56 = getelementptr inbounds i8, i8* %3, i64 16
  %57 = bitcast i8* %56 to <8 x i16>*
  store <8 x i16> %54, <8 x i16>* %57, align 1
  %58 = getelementptr inbounds i8, i8* %3, i64 32
  %59 = bitcast i8* %58 to <8 x i16>*
  store <8 x i16> %54, <8 x i16>* %59, align 1
  %60 = getelementptr inbounds i8, i8* %3, i64 48
  %61 = bitcast i8* %60 to <8 x i16>*
  store <8 x i16> %54, <8 x i16>* %61, align 1
  br label %1224

62:                                               ; preds = %7
  %63 = sext i32 %2 to i64
  br i1 %14, label %64, label %89

64:                                               ; preds = %62, %64
  %65 = phi i64 [ %87, %64 ], [ 0, %62 ]
  %66 = shl i64 %65, 5
  %67 = and i64 %66, 4294967264
  %68 = getelementptr inbounds i16, i16* %10, i64 %67
  %69 = bitcast i16* %68 to <8 x i16>*
  %70 = load <8 x i16>, <8 x i16>* %69, align 1
  %71 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %70, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %71, <8 x i16>* %69, align 1
  %72 = or i64 %67, 8
  %73 = getelementptr inbounds i16, i16* %10, i64 %72
  %74 = bitcast i16* %73 to <8 x i16>*
  %75 = load <8 x i16>, <8 x i16>* %74, align 1
  %76 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %75, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %76, <8 x i16>* %74, align 1
  %77 = or i64 %67, 16
  %78 = getelementptr inbounds i16, i16* %10, i64 %77
  %79 = bitcast i16* %78 to <8 x i16>*
  %80 = load <8 x i16>, <8 x i16>* %79, align 1
  %81 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %80, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %81, <8 x i16>* %79, align 1
  %82 = or i64 %67, 24
  %83 = getelementptr inbounds i16, i16* %10, i64 %82
  %84 = bitcast i16* %83 to <8 x i16>*
  %85 = load <8 x i16>, <8 x i16>* %84, align 1
  %86 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %85, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %86, <8 x i16>* %84, align 1
  %87 = add nuw nsw i64 %65, 1
  %88 = icmp eq i64 %87, %63
  br i1 %88, label %89, label %64

89:                                               ; preds = %64, %62
  %90 = bitcast [32 x <2 x i64>]* %8 to i8*
  %91 = bitcast [32 x <2 x i64>]* %9 to i8*
  %92 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 16
  %93 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 24
  %94 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 20
  %95 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 28
  %96 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 18
  %97 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 26
  %98 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 22
  %99 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 30
  %100 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 17
  %101 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 25
  %102 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 21
  %103 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 29
  %104 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 0
  %105 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %8, i64 0, i64 0
  %106 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %8, i64 0, i64 1
  %107 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 8
  %108 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %8, i64 0, i64 2
  %109 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %8, i64 0, i64 3
  %110 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 4
  %111 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %8, i64 0, i64 4
  %112 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %8, i64 0, i64 5
  %113 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 12
  %114 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %8, i64 0, i64 6
  %115 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %8, i64 0, i64 7
  %116 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 2
  %117 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %8, i64 0, i64 8
  %118 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %8, i64 0, i64 9
  %119 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 10
  %120 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %8, i64 0, i64 10
  %121 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %8, i64 0, i64 11
  %122 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 6
  %123 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %8, i64 0, i64 12
  %124 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %8, i64 0, i64 13
  %125 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 14
  %126 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %8, i64 0, i64 14
  %127 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %8, i64 0, i64 15
  %128 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 1
  %129 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %8, i64 0, i64 16
  %130 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %8, i64 0, i64 17
  %131 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 9
  %132 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %8, i64 0, i64 18
  %133 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %8, i64 0, i64 19
  %134 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 5
  %135 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %8, i64 0, i64 20
  %136 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %8, i64 0, i64 21
  %137 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 13
  %138 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %8, i64 0, i64 22
  %139 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %8, i64 0, i64 23
  %140 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 3
  %141 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %8, i64 0, i64 24
  %142 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 19
  %143 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %8, i64 0, i64 25
  %144 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 11
  %145 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %8, i64 0, i64 26
  %146 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 27
  %147 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %8, i64 0, i64 27
  %148 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 7
  %149 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %8, i64 0, i64 28
  %150 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 23
  %151 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %8, i64 0, i64 29
  %152 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 15
  %153 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %8, i64 0, i64 30
  %154 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 31
  %155 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %8, i64 0, i64 31
  %156 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %157 = bitcast [32 x <2 x i64>]* %8 to <8 x i16>*
  %158 = bitcast <2 x i64>* %106 to <8 x i16>*
  %159 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %160 = bitcast <2 x i64>* %108 to <8 x i16>*
  %161 = bitcast <2 x i64>* %109 to <8 x i16>*
  %162 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %163 = bitcast <2 x i64>* %111 to <8 x i16>*
  %164 = bitcast <2 x i64>* %115 to <8 x i16>*
  %165 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %166 = bitcast <2 x i64>* %112 to <8 x i16>*
  %167 = bitcast <2 x i64>* %114 to <8 x i16>*
  %168 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %169 = bitcast <2 x i64>* %117 to <8 x i16>*
  %170 = bitcast <2 x i64>* %127 to <8 x i16>*
  %171 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %172 = bitcast <2 x i64>* %118 to <8 x i16>*
  %173 = bitcast <2 x i64>* %126 to <8 x i16>*
  %174 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %175 = bitcast <2 x i64>* %120 to <8 x i16>*
  %176 = bitcast <2 x i64>* %124 to <8 x i16>*
  %177 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %178 = bitcast <2 x i64>* %121 to <8 x i16>*
  %179 = bitcast <2 x i64>* %123 to <8 x i16>*
  %180 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %181 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 201, i16 4091, i16 201, i16 4091, i16 201, i16 4091, i16 201, i16 4091>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %182 = bitcast <2 x i64>* %129 to <8 x i16>*
  %183 = bitcast <2 x i64>* %155 to <8 x i16>*
  %184 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3035, i16 2751, i16 3035, i16 2751, i16 3035, i16 2751, i16 3035, i16 2751>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %185 = bitcast <2 x i64>* %130 to <8 x i16>*
  %186 = bitcast <2 x i64>* %153 to <8 x i16>*
  %187 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1751, i16 3703, i16 1751, i16 3703, i16 1751, i16 3703, i16 1751, i16 3703>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %188 = bitcast <2 x i64>* %132 to <8 x i16>*
  %189 = bitcast <2 x i64>* %151 to <8 x i16>*
  %190 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3857, i16 1380, i16 3857, i16 1380, i16 3857, i16 1380, i16 3857, i16 1380>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %191 = bitcast <2 x i64>* %133 to <8 x i16>*
  %192 = bitcast <2 x i64>* %149 to <8 x i16>*
  %193 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 995, i16 3973, i16 995, i16 3973, i16 995, i16 3973, i16 995, i16 3973>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %194 = bitcast <2 x i64>* %135 to <8 x i16>*
  %195 = bitcast <2 x i64>* %147 to <8 x i16>*
  %196 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3513, i16 2106, i16 3513, i16 2106, i16 3513, i16 2106, i16 3513, i16 2106>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %197 = bitcast <2 x i64>* %136 to <8 x i16>*
  %198 = bitcast <2 x i64>* %145 to <8 x i16>*
  %199 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2440, i16 3290, i16 2440, i16 3290, i16 2440, i16 3290, i16 2440, i16 3290>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %200 = bitcast <2 x i64>* %138 to <8 x i16>*
  %201 = bitcast <2 x i64>* %143 to <8 x i16>*
  %202 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 4052, i16 601, i16 4052, i16 601, i16 4052, i16 601, i16 4052, i16 601>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %203 = bitcast <2 x i64>* %139 to <8 x i16>*
  %204 = bitcast <2 x i64>* %141 to <8 x i16>*
  %205 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 -4017, i16 799, i16 -4017, i16 799, i16 -4017, i16 799, i16 -4017, i16 799>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %206 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 -2276, i16 3406, i16 -2276, i16 3406, i16 -2276, i16 3406, i16 -2276, i16 3406>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  br label %207

207:                                              ; preds = %1173, %89
  %208 = phi i64 [ %1174, %1173 ], [ 0, %89 ]
  %209 = shl i64 %208, 5
  %210 = and i64 %209, 4294967040
  %211 = getelementptr inbounds i16, i16* %10, i64 %210
  call void @llvm.lifetime.start.p0i8(i64 512, i8* nonnull %90) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %90, i8 -86, i64 512, i1 false) #8
  call void @llvm.lifetime.start.p0i8(i64 512, i8* nonnull %91) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %91, i8 -86, i64 512, i1 false) #8
  br label %212

212:                                              ; preds = %212, %207
  %213 = phi i64 [ 0, %207 ], [ %293, %212 ]
  %214 = getelementptr inbounds i16, i16* %211, i64 %213
  %215 = bitcast i16* %214 to <8 x i16>*
  %216 = load <8 x i16>, <8 x i16>* %215, align 1
  %217 = add nuw nsw i64 %213, 32
  %218 = getelementptr inbounds i16, i16* %211, i64 %217
  %219 = bitcast i16* %218 to <8 x i16>*
  %220 = load <8 x i16>, <8 x i16>* %219, align 1
  %221 = add nuw nsw i64 %213, 64
  %222 = getelementptr inbounds i16, i16* %211, i64 %221
  %223 = bitcast i16* %222 to <8 x i16>*
  %224 = load <8 x i16>, <8 x i16>* %223, align 1
  %225 = add nuw nsw i64 %213, 96
  %226 = getelementptr inbounds i16, i16* %211, i64 %225
  %227 = bitcast i16* %226 to <8 x i16>*
  %228 = load <8 x i16>, <8 x i16>* %227, align 1
  %229 = add nuw nsw i64 %213, 128
  %230 = getelementptr inbounds i16, i16* %211, i64 %229
  %231 = bitcast i16* %230 to <8 x i16>*
  %232 = load <8 x i16>, <8 x i16>* %231, align 1
  %233 = add nuw nsw i64 %213, 160
  %234 = getelementptr inbounds i16, i16* %211, i64 %233
  %235 = bitcast i16* %234 to <8 x i16>*
  %236 = load <8 x i16>, <8 x i16>* %235, align 1
  %237 = add nuw nsw i64 %213, 192
  %238 = getelementptr inbounds i16, i16* %211, i64 %237
  %239 = bitcast i16* %238 to <8 x i16>*
  %240 = load <8 x i16>, <8 x i16>* %239, align 1
  %241 = add nuw nsw i64 %213, 224
  %242 = getelementptr inbounds i16, i16* %211, i64 %241
  %243 = bitcast i16* %242 to <8 x i16>*
  %244 = load <8 x i16>, <8 x i16>* %243, align 1
  %245 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %9, i64 0, i64 %213
  %246 = shufflevector <8 x i16> %216, <8 x i16> %220, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %247 = shufflevector <8 x i16> %224, <8 x i16> %228, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %248 = shufflevector <8 x i16> %232, <8 x i16> %236, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %249 = shufflevector <8 x i16> %240, <8 x i16> %244, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %250 = shufflevector <8 x i16> %216, <8 x i16> %220, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %251 = shufflevector <8 x i16> %224, <8 x i16> %228, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %252 = shufflevector <8 x i16> %232, <8 x i16> %236, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %253 = shufflevector <8 x i16> %240, <8 x i16> %244, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %254 = bitcast <8 x i16> %246 to <4 x i32>
  %255 = bitcast <8 x i16> %247 to <4 x i32>
  %256 = shufflevector <4 x i32> %254, <4 x i32> %255, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %257 = bitcast <4 x i32> %256 to <2 x i64>
  %258 = bitcast <8 x i16> %248 to <4 x i32>
  %259 = bitcast <8 x i16> %249 to <4 x i32>
  %260 = shufflevector <4 x i32> %258, <4 x i32> %259, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %261 = bitcast <4 x i32> %260 to <2 x i64>
  %262 = bitcast <8 x i16> %250 to <4 x i32>
  %263 = bitcast <8 x i16> %251 to <4 x i32>
  %264 = shufflevector <4 x i32> %262, <4 x i32> %263, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %265 = bitcast <4 x i32> %264 to <2 x i64>
  %266 = bitcast <8 x i16> %252 to <4 x i32>
  %267 = bitcast <8 x i16> %253 to <4 x i32>
  %268 = shufflevector <4 x i32> %266, <4 x i32> %267, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %269 = bitcast <4 x i32> %268 to <2 x i64>
  %270 = shufflevector <4 x i32> %254, <4 x i32> %255, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %271 = bitcast <4 x i32> %270 to <2 x i64>
  %272 = shufflevector <4 x i32> %258, <4 x i32> %259, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %273 = bitcast <4 x i32> %272 to <2 x i64>
  %274 = shufflevector <4 x i32> %262, <4 x i32> %263, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %275 = bitcast <4 x i32> %274 to <2 x i64>
  %276 = shufflevector <4 x i32> %266, <4 x i32> %267, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %277 = bitcast <4 x i32> %276 to <2 x i64>
  %278 = shufflevector <2 x i64> %257, <2 x i64> %261, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %278, <2 x i64>* %245, align 16
  %279 = shufflevector <2 x i64> %257, <2 x i64> %261, <2 x i32> <i32 1, i32 3>
  %280 = getelementptr inbounds <2 x i64>, <2 x i64>* %245, i64 1
  store <2 x i64> %279, <2 x i64>* %280, align 16
  %281 = shufflevector <2 x i64> %271, <2 x i64> %273, <2 x i32> <i32 0, i32 2>
  %282 = getelementptr inbounds <2 x i64>, <2 x i64>* %245, i64 2
  store <2 x i64> %281, <2 x i64>* %282, align 16
  %283 = shufflevector <2 x i64> %271, <2 x i64> %273, <2 x i32> <i32 1, i32 3>
  %284 = getelementptr inbounds <2 x i64>, <2 x i64>* %245, i64 3
  store <2 x i64> %283, <2 x i64>* %284, align 16
  %285 = shufflevector <2 x i64> %265, <2 x i64> %269, <2 x i32> <i32 0, i32 2>
  %286 = getelementptr inbounds <2 x i64>, <2 x i64>* %245, i64 4
  store <2 x i64> %285, <2 x i64>* %286, align 16
  %287 = shufflevector <2 x i64> %265, <2 x i64> %269, <2 x i32> <i32 1, i32 3>
  %288 = getelementptr inbounds <2 x i64>, <2 x i64>* %245, i64 5
  store <2 x i64> %287, <2 x i64>* %288, align 16
  %289 = shufflevector <2 x i64> %275, <2 x i64> %277, <2 x i32> <i32 0, i32 2>
  %290 = getelementptr inbounds <2 x i64>, <2 x i64>* %245, i64 6
  store <2 x i64> %289, <2 x i64>* %290, align 16
  %291 = shufflevector <2 x i64> %275, <2 x i64> %277, <2 x i32> <i32 1, i32 3>
  %292 = getelementptr inbounds <2 x i64>, <2 x i64>* %245, i64 7
  store <2 x i64> %291, <2 x i64>* %292, align 16
  %293 = add nuw nsw i64 %213, 8
  %294 = icmp ult i64 %293, 32
  br i1 %294, label %212, label %295

295:                                              ; preds = %212
  %296 = load <2 x i64>, <2 x i64>* %92, align 16
  %297 = load <2 x i64>, <2 x i64>* %93, align 16
  %298 = load <2 x i64>, <2 x i64>* %94, align 16
  %299 = load <2 x i64>, <2 x i64>* %95, align 16
  %300 = load <2 x i64>, <2 x i64>* %96, align 16
  %301 = load <2 x i64>, <2 x i64>* %97, align 16
  %302 = load <2 x i64>, <2 x i64>* %98, align 16
  %303 = load <2 x i64>, <2 x i64>* %99, align 16
  %304 = load <2 x i64>, <2 x i64>* %100, align 16
  %305 = load <2 x i64>, <2 x i64>* %101, align 16
  %306 = load <2 x i64>, <2 x i64>* %102, align 16
  %307 = load <2 x i64>, <2 x i64>* %103, align 16
  %308 = load <2 x i64>, <2 x i64>* %104, align 16
  store <2 x i64> %308, <2 x i64>* %105, align 16
  store <2 x i64> %296, <2 x i64>* %106, align 16
  %309 = load <2 x i64>, <2 x i64>* %107, align 16
  store <2 x i64> %309, <2 x i64>* %108, align 16
  store <2 x i64> %297, <2 x i64>* %109, align 16
  %310 = load <2 x i64>, <2 x i64>* %110, align 16
  store <2 x i64> %310, <2 x i64>* %111, align 16
  store <2 x i64> %298, <2 x i64>* %112, align 16
  %311 = load <2 x i64>, <2 x i64>* %113, align 16
  store <2 x i64> %311, <2 x i64>* %114, align 16
  store <2 x i64> %299, <2 x i64>* %115, align 16
  %312 = load <2 x i64>, <2 x i64>* %116, align 16
  store <2 x i64> %312, <2 x i64>* %117, align 16
  store <2 x i64> %300, <2 x i64>* %118, align 16
  %313 = load <2 x i64>, <2 x i64>* %119, align 16
  store <2 x i64> %313, <2 x i64>* %120, align 16
  store <2 x i64> %301, <2 x i64>* %121, align 16
  %314 = load <2 x i64>, <2 x i64>* %122, align 16
  store <2 x i64> %314, <2 x i64>* %123, align 16
  store <2 x i64> %302, <2 x i64>* %124, align 16
  %315 = load <2 x i64>, <2 x i64>* %125, align 16
  store <2 x i64> %315, <2 x i64>* %126, align 16
  store <2 x i64> %303, <2 x i64>* %127, align 16
  %316 = load <2 x i64>, <2 x i64>* %128, align 16
  store <2 x i64> %316, <2 x i64>* %129, align 16
  store <2 x i64> %304, <2 x i64>* %130, align 16
  %317 = load <2 x i64>, <2 x i64>* %131, align 16
  store <2 x i64> %317, <2 x i64>* %132, align 16
  store <2 x i64> %305, <2 x i64>* %133, align 16
  %318 = load <2 x i64>, <2 x i64>* %134, align 16
  store <2 x i64> %318, <2 x i64>* %135, align 16
  store <2 x i64> %306, <2 x i64>* %136, align 16
  %319 = load <2 x i64>, <2 x i64>* %137, align 16
  store <2 x i64> %319, <2 x i64>* %138, align 16
  store <2 x i64> %307, <2 x i64>* %139, align 16
  %320 = load <2 x i64>, <2 x i64>* %140, align 16
  store <2 x i64> %320, <2 x i64>* %141, align 16
  %321 = load <2 x i64>, <2 x i64>* %142, align 16
  store <2 x i64> %321, <2 x i64>* %143, align 16
  %322 = load <2 x i64>, <2 x i64>* %144, align 16
  store <2 x i64> %322, <2 x i64>* %145, align 16
  %323 = load <2 x i64>, <2 x i64>* %146, align 16
  store <2 x i64> %323, <2 x i64>* %147, align 16
  %324 = load <2 x i64>, <2 x i64>* %148, align 16
  store <2 x i64> %324, <2 x i64>* %149, align 16
  %325 = load <2 x i64>, <2 x i64>* %150, align 16
  store <2 x i64> %325, <2 x i64>* %151, align 16
  %326 = load <2 x i64>, <2 x i64>* %152, align 16
  store <2 x i64> %326, <2 x i64>* %153, align 16
  %327 = load <2 x i64>, <2 x i64>* %154, align 16
  store <2 x i64> %327, <2 x i64>* %155, align 16
  %328 = bitcast <2 x i64> %308 to <8 x i16>
  %329 = bitcast <2 x i64> %296 to <8 x i16>
  %330 = shufflevector <8 x i16> %328, <8 x i16> %329, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %331 = shufflevector <8 x i16> %329, <8 x i16> %328, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %332 = shufflevector <8 x i16> %328, <8 x i16> %329, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %333 = shufflevector <8 x i16> %329, <8 x i16> %328, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %334 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %330, <8 x i16> %156) #8
  %335 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %331, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %336 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %332, <8 x i16> %156) #8
  %337 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %333, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %338 = add <4 x i32> %334, <i32 2048, i32 2048, i32 2048, i32 2048>
  %339 = ashr <4 x i32> %338, <i32 12, i32 12, i32 12, i32 12>
  %340 = add <4 x i32> %335, <i32 2048, i32 2048, i32 2048, i32 2048>
  %341 = ashr <4 x i32> %340, <i32 12, i32 12, i32 12, i32 12>
  %342 = add <4 x i32> %336, <i32 2048, i32 2048, i32 2048, i32 2048>
  %343 = ashr <4 x i32> %342, <i32 12, i32 12, i32 12, i32 12>
  %344 = add <4 x i32> %337, <i32 2048, i32 2048, i32 2048, i32 2048>
  %345 = ashr <4 x i32> %344, <i32 12, i32 12, i32 12, i32 12>
  %346 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %339, <4 x i32> %343) #8
  %347 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %341, <4 x i32> %345) #8
  %348 = bitcast <2 x i64> %309 to <8 x i16>
  %349 = bitcast <2 x i64> %297 to <8 x i16>
  %350 = shufflevector <8 x i16> %348, <8 x i16> %349, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %351 = shufflevector <8 x i16> %349, <8 x i16> %348, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %352 = shufflevector <8 x i16> %348, <8 x i16> %349, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %353 = shufflevector <8 x i16> %349, <8 x i16> %348, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %354 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %350, <8 x i16> %159) #8
  %355 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %351, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %356 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %352, <8 x i16> %159) #8
  %357 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %353, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %358 = add <4 x i32> %354, <i32 2048, i32 2048, i32 2048, i32 2048>
  %359 = ashr <4 x i32> %358, <i32 12, i32 12, i32 12, i32 12>
  %360 = add <4 x i32> %355, <i32 2048, i32 2048, i32 2048, i32 2048>
  %361 = ashr <4 x i32> %360, <i32 12, i32 12, i32 12, i32 12>
  %362 = add <4 x i32> %356, <i32 2048, i32 2048, i32 2048, i32 2048>
  %363 = ashr <4 x i32> %362, <i32 12, i32 12, i32 12, i32 12>
  %364 = add <4 x i32> %357, <i32 2048, i32 2048, i32 2048, i32 2048>
  %365 = ashr <4 x i32> %364, <i32 12, i32 12, i32 12, i32 12>
  %366 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %359, <4 x i32> %363) #8
  %367 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %361, <4 x i32> %365) #8
  %368 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %347, <8 x i16> %367) #8
  %369 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %347, <8 x i16> %367) #8
  %370 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %346, <8 x i16> %366) #8
  %371 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %346, <8 x i16> %366) #8
  %372 = bitcast <2 x i64> %310 to <8 x i16>
  %373 = bitcast <2 x i64> %299 to <8 x i16>
  %374 = shufflevector <8 x i16> %372, <8 x i16> %373, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %375 = shufflevector <8 x i16> %373, <8 x i16> %372, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %376 = shufflevector <8 x i16> %372, <8 x i16> %373, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %377 = shufflevector <8 x i16> %373, <8 x i16> %372, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %378 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %374, <8 x i16> %162) #8
  %379 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %375, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %380 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %376, <8 x i16> %162) #8
  %381 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %377, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %382 = add <4 x i32> %378, <i32 2048, i32 2048, i32 2048, i32 2048>
  %383 = ashr <4 x i32> %382, <i32 12, i32 12, i32 12, i32 12>
  %384 = add <4 x i32> %379, <i32 2048, i32 2048, i32 2048, i32 2048>
  %385 = ashr <4 x i32> %384, <i32 12, i32 12, i32 12, i32 12>
  %386 = add <4 x i32> %380, <i32 2048, i32 2048, i32 2048, i32 2048>
  %387 = ashr <4 x i32> %386, <i32 12, i32 12, i32 12, i32 12>
  %388 = add <4 x i32> %381, <i32 2048, i32 2048, i32 2048, i32 2048>
  %389 = ashr <4 x i32> %388, <i32 12, i32 12, i32 12, i32 12>
  %390 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %383, <4 x i32> %387) #8
  %391 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %385, <4 x i32> %389) #8
  %392 = load <8 x i16>, <8 x i16>* %166, align 16
  %393 = load <8 x i16>, <8 x i16>* %167, align 16
  %394 = shufflevector <8 x i16> %392, <8 x i16> %393, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %395 = shufflevector <8 x i16> %393, <8 x i16> %392, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %396 = shufflevector <8 x i16> %392, <8 x i16> %393, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %397 = shufflevector <8 x i16> %393, <8 x i16> %392, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %398 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %394, <8 x i16> %165) #8
  %399 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %395, <8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>) #8
  %400 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %396, <8 x i16> %165) #8
  %401 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %397, <8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>) #8
  %402 = add <4 x i32> %398, <i32 2048, i32 2048, i32 2048, i32 2048>
  %403 = ashr <4 x i32> %402, <i32 12, i32 12, i32 12, i32 12>
  %404 = add <4 x i32> %399, <i32 2048, i32 2048, i32 2048, i32 2048>
  %405 = ashr <4 x i32> %404, <i32 12, i32 12, i32 12, i32 12>
  %406 = add <4 x i32> %400, <i32 2048, i32 2048, i32 2048, i32 2048>
  %407 = ashr <4 x i32> %406, <i32 12, i32 12, i32 12, i32 12>
  %408 = add <4 x i32> %401, <i32 2048, i32 2048, i32 2048, i32 2048>
  %409 = ashr <4 x i32> %408, <i32 12, i32 12, i32 12, i32 12>
  %410 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %403, <4 x i32> %407) #8
  %411 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %405, <4 x i32> %409) #8
  %412 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %390, <8 x i16> %410) #8
  %413 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %390, <8 x i16> %410) #8
  %414 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %391, <8 x i16> %411) #8
  %415 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %391, <8 x i16> %411) #8
  %416 = shufflevector <8 x i16> %415, <8 x i16> %413, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %417 = shufflevector <8 x i16> %413, <8 x i16> %415, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %418 = shufflevector <8 x i16> %415, <8 x i16> %413, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %419 = shufflevector <8 x i16> %413, <8 x i16> %415, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %420 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %416, <8 x i16> %156) #8
  %421 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %417, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %422 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %418, <8 x i16> %156) #8
  %423 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %419, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %424 = add <4 x i32> %420, <i32 2048, i32 2048, i32 2048, i32 2048>
  %425 = ashr <4 x i32> %424, <i32 12, i32 12, i32 12, i32 12>
  %426 = add <4 x i32> %421, <i32 2048, i32 2048, i32 2048, i32 2048>
  %427 = ashr <4 x i32> %426, <i32 12, i32 12, i32 12, i32 12>
  %428 = add <4 x i32> %422, <i32 2048, i32 2048, i32 2048, i32 2048>
  %429 = ashr <4 x i32> %428, <i32 12, i32 12, i32 12, i32 12>
  %430 = add <4 x i32> %423, <i32 2048, i32 2048, i32 2048, i32 2048>
  %431 = ashr <4 x i32> %430, <i32 12, i32 12, i32 12, i32 12>
  %432 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %425, <4 x i32> %429) #8
  %433 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %427, <4 x i32> %431) #8
  %434 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %368, <8 x i16> %414) #8
  %435 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %368, <8 x i16> %414) #8
  store <8 x i16> %434, <8 x i16>* %157, align 16
  store <8 x i16> %435, <8 x i16>* %164, align 16
  %436 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %370, <8 x i16> %433) #8
  %437 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %370, <8 x i16> %433) #8
  store <8 x i16> %436, <8 x i16>* %158, align 16
  store <8 x i16> %437, <8 x i16>* %167, align 16
  %438 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %371, <8 x i16> %432) #8
  %439 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %371, <8 x i16> %432) #8
  store <8 x i16> %438, <8 x i16>* %160, align 16
  store <8 x i16> %439, <8 x i16>* %166, align 16
  %440 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %369, <8 x i16> %412) #8
  %441 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %369, <8 x i16> %412) #8
  store <8 x i16> %440, <8 x i16>* %161, align 16
  store <8 x i16> %441, <8 x i16>* %163, align 16
  %442 = load <8 x i16>, <8 x i16>* %169, align 16
  %443 = load <8 x i16>, <8 x i16>* %170, align 16
  %444 = shufflevector <8 x i16> %442, <8 x i16> %443, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %445 = shufflevector <8 x i16> %443, <8 x i16> %442, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %446 = shufflevector <8 x i16> %442, <8 x i16> %443, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %447 = shufflevector <8 x i16> %443, <8 x i16> %442, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %448 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %444, <8 x i16> %168) #8
  %449 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %445, <8 x i16> <i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076>) #8
  %450 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %446, <8 x i16> %168) #8
  %451 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %447, <8 x i16> <i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076>) #8
  %452 = add <4 x i32> %448, <i32 2048, i32 2048, i32 2048, i32 2048>
  %453 = ashr <4 x i32> %452, <i32 12, i32 12, i32 12, i32 12>
  %454 = add <4 x i32> %449, <i32 2048, i32 2048, i32 2048, i32 2048>
  %455 = ashr <4 x i32> %454, <i32 12, i32 12, i32 12, i32 12>
  %456 = add <4 x i32> %450, <i32 2048, i32 2048, i32 2048, i32 2048>
  %457 = ashr <4 x i32> %456, <i32 12, i32 12, i32 12, i32 12>
  %458 = add <4 x i32> %451, <i32 2048, i32 2048, i32 2048, i32 2048>
  %459 = ashr <4 x i32> %458, <i32 12, i32 12, i32 12, i32 12>
  %460 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %453, <4 x i32> %457) #8
  %461 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %455, <4 x i32> %459) #8
  %462 = load <8 x i16>, <8 x i16>* %172, align 16
  %463 = load <8 x i16>, <8 x i16>* %173, align 16
  %464 = shufflevector <8 x i16> %462, <8 x i16> %463, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %465 = shufflevector <8 x i16> %463, <8 x i16> %462, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %466 = shufflevector <8 x i16> %462, <8 x i16> %463, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %467 = shufflevector <8 x i16> %463, <8 x i16> %462, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %468 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %464, <8 x i16> %171) #8
  %469 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %465, <8 x i16> <i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598>) #8
  %470 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %466, <8 x i16> %171) #8
  %471 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %467, <8 x i16> <i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598>) #8
  %472 = add <4 x i32> %468, <i32 2048, i32 2048, i32 2048, i32 2048>
  %473 = ashr <4 x i32> %472, <i32 12, i32 12, i32 12, i32 12>
  %474 = add <4 x i32> %469, <i32 2048, i32 2048, i32 2048, i32 2048>
  %475 = ashr <4 x i32> %474, <i32 12, i32 12, i32 12, i32 12>
  %476 = add <4 x i32> %470, <i32 2048, i32 2048, i32 2048, i32 2048>
  %477 = ashr <4 x i32> %476, <i32 12, i32 12, i32 12, i32 12>
  %478 = add <4 x i32> %471, <i32 2048, i32 2048, i32 2048, i32 2048>
  %479 = ashr <4 x i32> %478, <i32 12, i32 12, i32 12, i32 12>
  %480 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %473, <4 x i32> %477) #8
  %481 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %475, <4 x i32> %479) #8
  %482 = load <8 x i16>, <8 x i16>* %175, align 16
  %483 = load <8 x i16>, <8 x i16>* %176, align 16
  %484 = shufflevector <8 x i16> %482, <8 x i16> %483, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %485 = shufflevector <8 x i16> %483, <8 x i16> %482, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %486 = shufflevector <8 x i16> %482, <8 x i16> %483, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %487 = shufflevector <8 x i16> %483, <8 x i16> %482, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %488 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %484, <8 x i16> %174) #8
  %489 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %485, <8 x i16> <i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612>) #8
  %490 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %486, <8 x i16> %174) #8
  %491 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %487, <8 x i16> <i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612>) #8
  %492 = add <4 x i32> %488, <i32 2048, i32 2048, i32 2048, i32 2048>
  %493 = ashr <4 x i32> %492, <i32 12, i32 12, i32 12, i32 12>
  %494 = add <4 x i32> %489, <i32 2048, i32 2048, i32 2048, i32 2048>
  %495 = ashr <4 x i32> %494, <i32 12, i32 12, i32 12, i32 12>
  %496 = add <4 x i32> %490, <i32 2048, i32 2048, i32 2048, i32 2048>
  %497 = ashr <4 x i32> %496, <i32 12, i32 12, i32 12, i32 12>
  %498 = add <4 x i32> %491, <i32 2048, i32 2048, i32 2048, i32 2048>
  %499 = ashr <4 x i32> %498, <i32 12, i32 12, i32 12, i32 12>
  %500 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %493, <4 x i32> %497) #8
  %501 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %495, <4 x i32> %499) #8
  %502 = load <8 x i16>, <8 x i16>* %178, align 16
  %503 = load <8 x i16>, <8 x i16>* %179, align 16
  %504 = shufflevector <8 x i16> %502, <8 x i16> %503, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %505 = shufflevector <8 x i16> %503, <8 x i16> %502, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %506 = shufflevector <8 x i16> %502, <8 x i16> %503, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %507 = shufflevector <8 x i16> %503, <8 x i16> %502, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %508 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %504, <8 x i16> %177) #8
  %509 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %505, <8 x i16> <i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189>) #8
  %510 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %506, <8 x i16> %177) #8
  %511 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %507, <8 x i16> <i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189>) #8
  %512 = add <4 x i32> %508, <i32 2048, i32 2048, i32 2048, i32 2048>
  %513 = ashr <4 x i32> %512, <i32 12, i32 12, i32 12, i32 12>
  %514 = add <4 x i32> %509, <i32 2048, i32 2048, i32 2048, i32 2048>
  %515 = ashr <4 x i32> %514, <i32 12, i32 12, i32 12, i32 12>
  %516 = add <4 x i32> %510, <i32 2048, i32 2048, i32 2048, i32 2048>
  %517 = ashr <4 x i32> %516, <i32 12, i32 12, i32 12, i32 12>
  %518 = add <4 x i32> %511, <i32 2048, i32 2048, i32 2048, i32 2048>
  %519 = ashr <4 x i32> %518, <i32 12, i32 12, i32 12, i32 12>
  %520 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %513, <4 x i32> %517) #8
  %521 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %515, <4 x i32> %519) #8
  %522 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %460, <8 x i16> %480) #8
  %523 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %460, <8 x i16> %480) #8
  %524 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %520, <8 x i16> %500) #8
  %525 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %520, <8 x i16> %500) #8
  %526 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %521, <8 x i16> %501) #8
  %527 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %521, <8 x i16> %501) #8
  %528 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %461, <8 x i16> %481) #8
  %529 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %461, <8 x i16> %481) #8
  %530 = shufflevector <8 x i16> %529, <8 x i16> %523, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %531 = shufflevector <8 x i16> %523, <8 x i16> %529, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %532 = shufflevector <8 x i16> %529, <8 x i16> %523, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %533 = shufflevector <8 x i16> %523, <8 x i16> %529, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %534 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %530, <8 x i16> %159) #8
  %535 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %531, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %536 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %532, <8 x i16> %159) #8
  %537 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %533, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %538 = add <4 x i32> %534, <i32 2048, i32 2048, i32 2048, i32 2048>
  %539 = ashr <4 x i32> %538, <i32 12, i32 12, i32 12, i32 12>
  %540 = add <4 x i32> %535, <i32 2048, i32 2048, i32 2048, i32 2048>
  %541 = ashr <4 x i32> %540, <i32 12, i32 12, i32 12, i32 12>
  %542 = add <4 x i32> %536, <i32 2048, i32 2048, i32 2048, i32 2048>
  %543 = ashr <4 x i32> %542, <i32 12, i32 12, i32 12, i32 12>
  %544 = add <4 x i32> %537, <i32 2048, i32 2048, i32 2048, i32 2048>
  %545 = ashr <4 x i32> %544, <i32 12, i32 12, i32 12, i32 12>
  %546 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %539, <4 x i32> %543) #8
  %547 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %541, <4 x i32> %545) #8
  %548 = shufflevector <8 x i16> %527, <8 x i16> %525, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %549 = shufflevector <8 x i16> %525, <8 x i16> %527, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %550 = shufflevector <8 x i16> %527, <8 x i16> %525, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %551 = shufflevector <8 x i16> %525, <8 x i16> %527, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %552 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %548, <8 x i16> %180) #8
  %553 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %549, <8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>) #8
  %554 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %550, <8 x i16> %180) #8
  %555 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %551, <8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>) #8
  %556 = add <4 x i32> %552, <i32 2048, i32 2048, i32 2048, i32 2048>
  %557 = ashr <4 x i32> %556, <i32 12, i32 12, i32 12, i32 12>
  %558 = add <4 x i32> %553, <i32 2048, i32 2048, i32 2048, i32 2048>
  %559 = ashr <4 x i32> %558, <i32 12, i32 12, i32 12, i32 12>
  %560 = add <4 x i32> %554, <i32 2048, i32 2048, i32 2048, i32 2048>
  %561 = ashr <4 x i32> %560, <i32 12, i32 12, i32 12, i32 12>
  %562 = add <4 x i32> %555, <i32 2048, i32 2048, i32 2048, i32 2048>
  %563 = ashr <4 x i32> %562, <i32 12, i32 12, i32 12, i32 12>
  %564 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %557, <4 x i32> %561) #8
  %565 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %559, <4 x i32> %563) #8
  %566 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %522, <8 x i16> %524) #8
  %567 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %522, <8 x i16> %524) #8
  %568 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %546, <8 x i16> %564) #8
  %569 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %546, <8 x i16> %564) #8
  %570 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %528, <8 x i16> %526) #8
  %571 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %528, <8 x i16> %526) #8
  %572 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %547, <8 x i16> %565) #8
  %573 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %547, <8 x i16> %565) #8
  %574 = shufflevector <8 x i16> %573, <8 x i16> %569, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %575 = shufflevector <8 x i16> %569, <8 x i16> %573, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %576 = shufflevector <8 x i16> %573, <8 x i16> %569, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %577 = shufflevector <8 x i16> %569, <8 x i16> %573, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %578 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %574, <8 x i16> %156) #8
  %579 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %575, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %580 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %576, <8 x i16> %156) #8
  %581 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %577, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %582 = add <4 x i32> %578, <i32 2048, i32 2048, i32 2048, i32 2048>
  %583 = ashr <4 x i32> %582, <i32 12, i32 12, i32 12, i32 12>
  %584 = add <4 x i32> %579, <i32 2048, i32 2048, i32 2048, i32 2048>
  %585 = ashr <4 x i32> %584, <i32 12, i32 12, i32 12, i32 12>
  %586 = add <4 x i32> %580, <i32 2048, i32 2048, i32 2048, i32 2048>
  %587 = ashr <4 x i32> %586, <i32 12, i32 12, i32 12, i32 12>
  %588 = add <4 x i32> %581, <i32 2048, i32 2048, i32 2048, i32 2048>
  %589 = ashr <4 x i32> %588, <i32 12, i32 12, i32 12, i32 12>
  %590 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %583, <4 x i32> %587) #8
  %591 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %585, <4 x i32> %589) #8
  %592 = shufflevector <8 x i16> %571, <8 x i16> %567, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %593 = shufflevector <8 x i16> %567, <8 x i16> %571, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %594 = shufflevector <8 x i16> %571, <8 x i16> %567, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %595 = shufflevector <8 x i16> %567, <8 x i16> %571, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %596 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %592, <8 x i16> %156) #8
  %597 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %593, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %598 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %594, <8 x i16> %156) #8
  %599 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %595, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %600 = add <4 x i32> %596, <i32 2048, i32 2048, i32 2048, i32 2048>
  %601 = ashr <4 x i32> %600, <i32 12, i32 12, i32 12, i32 12>
  %602 = add <4 x i32> %597, <i32 2048, i32 2048, i32 2048, i32 2048>
  %603 = ashr <4 x i32> %602, <i32 12, i32 12, i32 12, i32 12>
  %604 = add <4 x i32> %598, <i32 2048, i32 2048, i32 2048, i32 2048>
  %605 = ashr <4 x i32> %604, <i32 12, i32 12, i32 12, i32 12>
  %606 = add <4 x i32> %599, <i32 2048, i32 2048, i32 2048, i32 2048>
  %607 = ashr <4 x i32> %606, <i32 12, i32 12, i32 12, i32 12>
  %608 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %601, <4 x i32> %605) #8
  %609 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %603, <4 x i32> %607) #8
  %610 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %434, <8 x i16> %570) #8
  %611 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %434, <8 x i16> %570) #8
  store <8 x i16> %610, <8 x i16>* %157, align 16
  store <8 x i16> %611, <8 x i16>* %170, align 16
  %612 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %436, <8 x i16> %572) #8
  %613 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %436, <8 x i16> %572) #8
  store <8 x i16> %612, <8 x i16>* %158, align 16
  store <8 x i16> %613, <8 x i16>* %173, align 16
  %614 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %438, <8 x i16> %591) #8
  %615 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %438, <8 x i16> %591) #8
  store <8 x i16> %614, <8 x i16>* %160, align 16
  store <8 x i16> %615, <8 x i16>* %176, align 16
  %616 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %440, <8 x i16> %609) #8
  %617 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %440, <8 x i16> %609) #8
  store <8 x i16> %616, <8 x i16>* %161, align 16
  store <8 x i16> %617, <8 x i16>* %179, align 16
  %618 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %441, <8 x i16> %608) #8
  %619 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %441, <8 x i16> %608) #8
  store <8 x i16> %618, <8 x i16>* %163, align 16
  store <8 x i16> %619, <8 x i16>* %178, align 16
  %620 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %439, <8 x i16> %590) #8
  %621 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %439, <8 x i16> %590) #8
  store <8 x i16> %620, <8 x i16>* %166, align 16
  store <8 x i16> %621, <8 x i16>* %175, align 16
  %622 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %437, <8 x i16> %568) #8
  %623 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %437, <8 x i16> %568) #8
  store <8 x i16> %622, <8 x i16>* %167, align 16
  store <8 x i16> %623, <8 x i16>* %172, align 16
  %624 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %435, <8 x i16> %566) #8
  %625 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %435, <8 x i16> %566) #8
  store <8 x i16> %624, <8 x i16>* %164, align 16
  store <8 x i16> %625, <8 x i16>* %169, align 16
  %626 = load <8 x i16>, <8 x i16>* %182, align 16
  %627 = load <8 x i16>, <8 x i16>* %183, align 16
  %628 = shufflevector <8 x i16> %626, <8 x i16> %627, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %629 = shufflevector <8 x i16> %627, <8 x i16> %626, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %630 = shufflevector <8 x i16> %626, <8 x i16> %627, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %631 = shufflevector <8 x i16> %627, <8 x i16> %626, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %632 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %628, <8 x i16> %181) #8
  %633 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %629, <8 x i16> <i16 201, i16 4091, i16 201, i16 4091, i16 201, i16 4091, i16 201, i16 4091>) #8
  %634 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %630, <8 x i16> %181) #8
  %635 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %631, <8 x i16> <i16 201, i16 4091, i16 201, i16 4091, i16 201, i16 4091, i16 201, i16 4091>) #8
  %636 = add <4 x i32> %632, <i32 2048, i32 2048, i32 2048, i32 2048>
  %637 = ashr <4 x i32> %636, <i32 12, i32 12, i32 12, i32 12>
  %638 = add <4 x i32> %633, <i32 2048, i32 2048, i32 2048, i32 2048>
  %639 = ashr <4 x i32> %638, <i32 12, i32 12, i32 12, i32 12>
  %640 = add <4 x i32> %634, <i32 2048, i32 2048, i32 2048, i32 2048>
  %641 = ashr <4 x i32> %640, <i32 12, i32 12, i32 12, i32 12>
  %642 = add <4 x i32> %635, <i32 2048, i32 2048, i32 2048, i32 2048>
  %643 = ashr <4 x i32> %642, <i32 12, i32 12, i32 12, i32 12>
  %644 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %637, <4 x i32> %641) #8
  %645 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %639, <4 x i32> %643) #8
  store <8 x i16> %644, <8 x i16>* %182, align 16
  store <8 x i16> %645, <8 x i16>* %183, align 16
  %646 = load <8 x i16>, <8 x i16>* %185, align 16
  %647 = load <8 x i16>, <8 x i16>* %186, align 16
  %648 = shufflevector <8 x i16> %646, <8 x i16> %647, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %649 = shufflevector <8 x i16> %647, <8 x i16> %646, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %650 = shufflevector <8 x i16> %646, <8 x i16> %647, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %651 = shufflevector <8 x i16> %647, <8 x i16> %646, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %652 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %648, <8 x i16> %184) #8
  %653 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %649, <8 x i16> <i16 3035, i16 2751, i16 3035, i16 2751, i16 3035, i16 2751, i16 3035, i16 2751>) #8
  %654 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %650, <8 x i16> %184) #8
  %655 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %651, <8 x i16> <i16 3035, i16 2751, i16 3035, i16 2751, i16 3035, i16 2751, i16 3035, i16 2751>) #8
  %656 = add <4 x i32> %652, <i32 2048, i32 2048, i32 2048, i32 2048>
  %657 = ashr <4 x i32> %656, <i32 12, i32 12, i32 12, i32 12>
  %658 = add <4 x i32> %653, <i32 2048, i32 2048, i32 2048, i32 2048>
  %659 = ashr <4 x i32> %658, <i32 12, i32 12, i32 12, i32 12>
  %660 = add <4 x i32> %654, <i32 2048, i32 2048, i32 2048, i32 2048>
  %661 = ashr <4 x i32> %660, <i32 12, i32 12, i32 12, i32 12>
  %662 = add <4 x i32> %655, <i32 2048, i32 2048, i32 2048, i32 2048>
  %663 = ashr <4 x i32> %662, <i32 12, i32 12, i32 12, i32 12>
  %664 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %657, <4 x i32> %661) #8
  %665 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %659, <4 x i32> %663) #8
  store <8 x i16> %664, <8 x i16>* %185, align 16
  store <8 x i16> %665, <8 x i16>* %186, align 16
  %666 = load <8 x i16>, <8 x i16>* %188, align 16
  %667 = load <8 x i16>, <8 x i16>* %189, align 16
  %668 = shufflevector <8 x i16> %666, <8 x i16> %667, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %669 = shufflevector <8 x i16> %667, <8 x i16> %666, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %670 = shufflevector <8 x i16> %666, <8 x i16> %667, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %671 = shufflevector <8 x i16> %667, <8 x i16> %666, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %672 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %668, <8 x i16> %187) #8
  %673 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %669, <8 x i16> <i16 1751, i16 3703, i16 1751, i16 3703, i16 1751, i16 3703, i16 1751, i16 3703>) #8
  %674 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %670, <8 x i16> %187) #8
  %675 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %671, <8 x i16> <i16 1751, i16 3703, i16 1751, i16 3703, i16 1751, i16 3703, i16 1751, i16 3703>) #8
  %676 = add <4 x i32> %672, <i32 2048, i32 2048, i32 2048, i32 2048>
  %677 = ashr <4 x i32> %676, <i32 12, i32 12, i32 12, i32 12>
  %678 = add <4 x i32> %673, <i32 2048, i32 2048, i32 2048, i32 2048>
  %679 = ashr <4 x i32> %678, <i32 12, i32 12, i32 12, i32 12>
  %680 = add <4 x i32> %674, <i32 2048, i32 2048, i32 2048, i32 2048>
  %681 = ashr <4 x i32> %680, <i32 12, i32 12, i32 12, i32 12>
  %682 = add <4 x i32> %675, <i32 2048, i32 2048, i32 2048, i32 2048>
  %683 = ashr <4 x i32> %682, <i32 12, i32 12, i32 12, i32 12>
  %684 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %677, <4 x i32> %681) #8
  %685 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %679, <4 x i32> %683) #8
  store <8 x i16> %684, <8 x i16>* %188, align 16
  store <8 x i16> %685, <8 x i16>* %189, align 16
  %686 = load <8 x i16>, <8 x i16>* %191, align 16
  %687 = load <8 x i16>, <8 x i16>* %192, align 16
  %688 = shufflevector <8 x i16> %686, <8 x i16> %687, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %689 = shufflevector <8 x i16> %687, <8 x i16> %686, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %690 = shufflevector <8 x i16> %686, <8 x i16> %687, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %691 = shufflevector <8 x i16> %687, <8 x i16> %686, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %692 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %688, <8 x i16> %190) #8
  %693 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %689, <8 x i16> <i16 3857, i16 1380, i16 3857, i16 1380, i16 3857, i16 1380, i16 3857, i16 1380>) #8
  %694 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %690, <8 x i16> %190) #8
  %695 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %691, <8 x i16> <i16 3857, i16 1380, i16 3857, i16 1380, i16 3857, i16 1380, i16 3857, i16 1380>) #8
  %696 = add <4 x i32> %692, <i32 2048, i32 2048, i32 2048, i32 2048>
  %697 = ashr <4 x i32> %696, <i32 12, i32 12, i32 12, i32 12>
  %698 = add <4 x i32> %693, <i32 2048, i32 2048, i32 2048, i32 2048>
  %699 = ashr <4 x i32> %698, <i32 12, i32 12, i32 12, i32 12>
  %700 = add <4 x i32> %694, <i32 2048, i32 2048, i32 2048, i32 2048>
  %701 = ashr <4 x i32> %700, <i32 12, i32 12, i32 12, i32 12>
  %702 = add <4 x i32> %695, <i32 2048, i32 2048, i32 2048, i32 2048>
  %703 = ashr <4 x i32> %702, <i32 12, i32 12, i32 12, i32 12>
  %704 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %697, <4 x i32> %701) #8
  %705 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %699, <4 x i32> %703) #8
  store <8 x i16> %705, <8 x i16>* %192, align 16
  %706 = load <8 x i16>, <8 x i16>* %194, align 16
  %707 = load <8 x i16>, <8 x i16>* %195, align 16
  %708 = shufflevector <8 x i16> %706, <8 x i16> %707, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %709 = shufflevector <8 x i16> %707, <8 x i16> %706, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %710 = shufflevector <8 x i16> %706, <8 x i16> %707, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %711 = shufflevector <8 x i16> %707, <8 x i16> %706, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %712 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %708, <8 x i16> %193) #8
  %713 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %709, <8 x i16> <i16 995, i16 3973, i16 995, i16 3973, i16 995, i16 3973, i16 995, i16 3973>) #8
  %714 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %710, <8 x i16> %193) #8
  %715 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %711, <8 x i16> <i16 995, i16 3973, i16 995, i16 3973, i16 995, i16 3973, i16 995, i16 3973>) #8
  %716 = add <4 x i32> %712, <i32 2048, i32 2048, i32 2048, i32 2048>
  %717 = ashr <4 x i32> %716, <i32 12, i32 12, i32 12, i32 12>
  %718 = add <4 x i32> %713, <i32 2048, i32 2048, i32 2048, i32 2048>
  %719 = ashr <4 x i32> %718, <i32 12, i32 12, i32 12, i32 12>
  %720 = add <4 x i32> %714, <i32 2048, i32 2048, i32 2048, i32 2048>
  %721 = ashr <4 x i32> %720, <i32 12, i32 12, i32 12, i32 12>
  %722 = add <4 x i32> %715, <i32 2048, i32 2048, i32 2048, i32 2048>
  %723 = ashr <4 x i32> %722, <i32 12, i32 12, i32 12, i32 12>
  %724 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %717, <4 x i32> %721) #8
  %725 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %719, <4 x i32> %723) #8
  %726 = load <8 x i16>, <8 x i16>* %197, align 16
  %727 = load <8 x i16>, <8 x i16>* %198, align 16
  %728 = shufflevector <8 x i16> %726, <8 x i16> %727, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %729 = shufflevector <8 x i16> %727, <8 x i16> %726, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %730 = shufflevector <8 x i16> %726, <8 x i16> %727, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %731 = shufflevector <8 x i16> %727, <8 x i16> %726, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %732 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %728, <8 x i16> %196) #8
  %733 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %729, <8 x i16> <i16 3513, i16 2106, i16 3513, i16 2106, i16 3513, i16 2106, i16 3513, i16 2106>) #8
  %734 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %730, <8 x i16> %196) #8
  %735 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %731, <8 x i16> <i16 3513, i16 2106, i16 3513, i16 2106, i16 3513, i16 2106, i16 3513, i16 2106>) #8
  %736 = add <4 x i32> %732, <i32 2048, i32 2048, i32 2048, i32 2048>
  %737 = ashr <4 x i32> %736, <i32 12, i32 12, i32 12, i32 12>
  %738 = add <4 x i32> %733, <i32 2048, i32 2048, i32 2048, i32 2048>
  %739 = ashr <4 x i32> %738, <i32 12, i32 12, i32 12, i32 12>
  %740 = add <4 x i32> %734, <i32 2048, i32 2048, i32 2048, i32 2048>
  %741 = ashr <4 x i32> %740, <i32 12, i32 12, i32 12, i32 12>
  %742 = add <4 x i32> %735, <i32 2048, i32 2048, i32 2048, i32 2048>
  %743 = ashr <4 x i32> %742, <i32 12, i32 12, i32 12, i32 12>
  %744 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %737, <4 x i32> %741) #8
  %745 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %739, <4 x i32> %743) #8
  %746 = load <8 x i16>, <8 x i16>* %200, align 16
  %747 = load <8 x i16>, <8 x i16>* %201, align 16
  %748 = shufflevector <8 x i16> %746, <8 x i16> %747, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %749 = shufflevector <8 x i16> %747, <8 x i16> %746, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %750 = shufflevector <8 x i16> %746, <8 x i16> %747, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %751 = shufflevector <8 x i16> %747, <8 x i16> %746, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %752 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %748, <8 x i16> %199) #8
  %753 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %749, <8 x i16> <i16 2440, i16 3290, i16 2440, i16 3290, i16 2440, i16 3290, i16 2440, i16 3290>) #8
  %754 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %750, <8 x i16> %199) #8
  %755 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %751, <8 x i16> <i16 2440, i16 3290, i16 2440, i16 3290, i16 2440, i16 3290, i16 2440, i16 3290>) #8
  %756 = add <4 x i32> %752, <i32 2048, i32 2048, i32 2048, i32 2048>
  %757 = ashr <4 x i32> %756, <i32 12, i32 12, i32 12, i32 12>
  %758 = add <4 x i32> %753, <i32 2048, i32 2048, i32 2048, i32 2048>
  %759 = ashr <4 x i32> %758, <i32 12, i32 12, i32 12, i32 12>
  %760 = add <4 x i32> %754, <i32 2048, i32 2048, i32 2048, i32 2048>
  %761 = ashr <4 x i32> %760, <i32 12, i32 12, i32 12, i32 12>
  %762 = add <4 x i32> %755, <i32 2048, i32 2048, i32 2048, i32 2048>
  %763 = ashr <4 x i32> %762, <i32 12, i32 12, i32 12, i32 12>
  %764 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %757, <4 x i32> %761) #8
  %765 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %759, <4 x i32> %763) #8
  %766 = load <8 x i16>, <8 x i16>* %203, align 16
  %767 = load <8 x i16>, <8 x i16>* %204, align 16
  %768 = shufflevector <8 x i16> %766, <8 x i16> %767, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %769 = shufflevector <8 x i16> %767, <8 x i16> %766, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %770 = shufflevector <8 x i16> %766, <8 x i16> %767, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %771 = shufflevector <8 x i16> %767, <8 x i16> %766, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %772 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %768, <8 x i16> %202) #8
  %773 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %769, <8 x i16> <i16 4052, i16 601, i16 4052, i16 601, i16 4052, i16 601, i16 4052, i16 601>) #8
  %774 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %770, <8 x i16> %202) #8
  %775 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %771, <8 x i16> <i16 4052, i16 601, i16 4052, i16 601, i16 4052, i16 601, i16 4052, i16 601>) #8
  %776 = add <4 x i32> %772, <i32 2048, i32 2048, i32 2048, i32 2048>
  %777 = ashr <4 x i32> %776, <i32 12, i32 12, i32 12, i32 12>
  %778 = add <4 x i32> %773, <i32 2048, i32 2048, i32 2048, i32 2048>
  %779 = ashr <4 x i32> %778, <i32 12, i32 12, i32 12, i32 12>
  %780 = add <4 x i32> %774, <i32 2048, i32 2048, i32 2048, i32 2048>
  %781 = ashr <4 x i32> %780, <i32 12, i32 12, i32 12, i32 12>
  %782 = add <4 x i32> %775, <i32 2048, i32 2048, i32 2048, i32 2048>
  %783 = ashr <4 x i32> %782, <i32 12, i32 12, i32 12, i32 12>
  %784 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %777, <4 x i32> %781) #8
  %785 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %779, <4 x i32> %783) #8
  %786 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %644, <8 x i16> %664) #8
  %787 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %644, <8 x i16> %664) #8
  %788 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %704, <8 x i16> %684) #8
  %789 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %704, <8 x i16> %684) #8
  store <8 x i16> %788, <8 x i16>* %191, align 16
  %790 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %724, <8 x i16> %744) #8
  %791 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %724, <8 x i16> %744) #8
  store <8 x i16> %790, <8 x i16>* %194, align 16
  %792 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %784, <8 x i16> %764) #8
  %793 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %784, <8 x i16> %764) #8
  store <8 x i16> %792, <8 x i16>* %203, align 16
  %794 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %785, <8 x i16> %765) #8
  %795 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %785, <8 x i16> %765) #8
  store <8 x i16> %794, <8 x i16>* %204, align 16
  %796 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %725, <8 x i16> %745) #8
  %797 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %725, <8 x i16> %745) #8
  store <8 x i16> %796, <8 x i16>* %195, align 16
  %798 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %705, <8 x i16> %685) #8
  %799 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %705, <8 x i16> %685) #8
  store <8 x i16> %798, <8 x i16>* %192, align 16
  %800 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %645, <8 x i16> %665) #8
  %801 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %645, <8 x i16> %665) #8
  %802 = shufflevector <8 x i16> %801, <8 x i16> %787, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %803 = shufflevector <8 x i16> %787, <8 x i16> %801, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %804 = shufflevector <8 x i16> %801, <8 x i16> %787, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %805 = shufflevector <8 x i16> %787, <8 x i16> %801, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %806 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %802, <8 x i16> %162) #8
  %807 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %803, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %808 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %804, <8 x i16> %162) #8
  %809 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %805, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %810 = add <4 x i32> %806, <i32 2048, i32 2048, i32 2048, i32 2048>
  %811 = ashr <4 x i32> %810, <i32 12, i32 12, i32 12, i32 12>
  %812 = add <4 x i32> %807, <i32 2048, i32 2048, i32 2048, i32 2048>
  %813 = ashr <4 x i32> %812, <i32 12, i32 12, i32 12, i32 12>
  %814 = add <4 x i32> %808, <i32 2048, i32 2048, i32 2048, i32 2048>
  %815 = ashr <4 x i32> %814, <i32 12, i32 12, i32 12, i32 12>
  %816 = add <4 x i32> %809, <i32 2048, i32 2048, i32 2048, i32 2048>
  %817 = ashr <4 x i32> %816, <i32 12, i32 12, i32 12, i32 12>
  %818 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %811, <4 x i32> %815) #8
  %819 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %813, <4 x i32> %817) #8
  %820 = shufflevector <8 x i16> %799, <8 x i16> %789, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %821 = shufflevector <8 x i16> %789, <8 x i16> %799, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %822 = shufflevector <8 x i16> %799, <8 x i16> %789, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %823 = shufflevector <8 x i16> %789, <8 x i16> %799, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %824 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %820, <8 x i16> %205) #8
  %825 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %821, <8 x i16> <i16 -4017, i16 799, i16 -4017, i16 799, i16 -4017, i16 799, i16 -4017, i16 799>) #8
  %826 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %822, <8 x i16> %205) #8
  %827 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %823, <8 x i16> <i16 -4017, i16 799, i16 -4017, i16 799, i16 -4017, i16 799, i16 -4017, i16 799>) #8
  %828 = add <4 x i32> %824, <i32 2048, i32 2048, i32 2048, i32 2048>
  %829 = ashr <4 x i32> %828, <i32 12, i32 12, i32 12, i32 12>
  %830 = add <4 x i32> %825, <i32 2048, i32 2048, i32 2048, i32 2048>
  %831 = ashr <4 x i32> %830, <i32 12, i32 12, i32 12, i32 12>
  %832 = add <4 x i32> %826, <i32 2048, i32 2048, i32 2048, i32 2048>
  %833 = ashr <4 x i32> %832, <i32 12, i32 12, i32 12, i32 12>
  %834 = add <4 x i32> %827, <i32 2048, i32 2048, i32 2048, i32 2048>
  %835 = ashr <4 x i32> %834, <i32 12, i32 12, i32 12, i32 12>
  %836 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %829, <4 x i32> %833) #8
  %837 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %831, <4 x i32> %835) #8
  %838 = shufflevector <8 x i16> %797, <8 x i16> %791, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %839 = shufflevector <8 x i16> %791, <8 x i16> %797, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %840 = shufflevector <8 x i16> %797, <8 x i16> %791, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %841 = shufflevector <8 x i16> %791, <8 x i16> %797, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %842 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %838, <8 x i16> %165) #8
  %843 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %839, <8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>) #8
  %844 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %840, <8 x i16> %165) #8
  %845 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %841, <8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>) #8
  %846 = add <4 x i32> %842, <i32 2048, i32 2048, i32 2048, i32 2048>
  %847 = ashr <4 x i32> %846, <i32 12, i32 12, i32 12, i32 12>
  %848 = add <4 x i32> %843, <i32 2048, i32 2048, i32 2048, i32 2048>
  %849 = ashr <4 x i32> %848, <i32 12, i32 12, i32 12, i32 12>
  %850 = add <4 x i32> %844, <i32 2048, i32 2048, i32 2048, i32 2048>
  %851 = ashr <4 x i32> %850, <i32 12, i32 12, i32 12, i32 12>
  %852 = add <4 x i32> %845, <i32 2048, i32 2048, i32 2048, i32 2048>
  %853 = ashr <4 x i32> %852, <i32 12, i32 12, i32 12, i32 12>
  %854 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %847, <4 x i32> %851) #8
  %855 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %849, <4 x i32> %853) #8
  %856 = shufflevector <8 x i16> %795, <8 x i16> %793, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %857 = shufflevector <8 x i16> %793, <8 x i16> %795, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %858 = shufflevector <8 x i16> %795, <8 x i16> %793, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %859 = shufflevector <8 x i16> %793, <8 x i16> %795, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %860 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %856, <8 x i16> %206) #8
  %861 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %857, <8 x i16> <i16 -2276, i16 3406, i16 -2276, i16 3406, i16 -2276, i16 3406, i16 -2276, i16 3406>) #8
  %862 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %858, <8 x i16> %206) #8
  %863 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %859, <8 x i16> <i16 -2276, i16 3406, i16 -2276, i16 3406, i16 -2276, i16 3406, i16 -2276, i16 3406>) #8
  %864 = add <4 x i32> %860, <i32 2048, i32 2048, i32 2048, i32 2048>
  %865 = ashr <4 x i32> %864, <i32 12, i32 12, i32 12, i32 12>
  %866 = add <4 x i32> %861, <i32 2048, i32 2048, i32 2048, i32 2048>
  %867 = ashr <4 x i32> %866, <i32 12, i32 12, i32 12, i32 12>
  %868 = add <4 x i32> %862, <i32 2048, i32 2048, i32 2048, i32 2048>
  %869 = ashr <4 x i32> %868, <i32 12, i32 12, i32 12, i32 12>
  %870 = add <4 x i32> %863, <i32 2048, i32 2048, i32 2048, i32 2048>
  %871 = ashr <4 x i32> %870, <i32 12, i32 12, i32 12, i32 12>
  %872 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %865, <4 x i32> %869) #8
  %873 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %867, <4 x i32> %871) #8
  %874 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %786, <8 x i16> %788) #8
  %875 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %786, <8 x i16> %788) #8
  %876 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %818, <8 x i16> %836) #8
  %877 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %818, <8 x i16> %836) #8
  %878 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %792, <8 x i16> %790) #8
  %879 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %792, <8 x i16> %790) #8
  %880 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %872, <8 x i16> %854) #8
  %881 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %872, <8 x i16> %854) #8
  %882 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %794, <8 x i16> %796) #8
  %883 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %794, <8 x i16> %796) #8
  %884 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %873, <8 x i16> %855) #8
  %885 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %873, <8 x i16> %855) #8
  store <8 x i16> %884, <8 x i16>* %201, align 16
  %886 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %800, <8 x i16> %798) #8
  %887 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %800, <8 x i16> %798) #8
  %888 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %819, <8 x i16> %837) #8
  %889 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %819, <8 x i16> %837) #8
  %890 = shufflevector <8 x i16> %889, <8 x i16> %877, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %891 = shufflevector <8 x i16> %877, <8 x i16> %889, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %892 = shufflevector <8 x i16> %889, <8 x i16> %877, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %893 = shufflevector <8 x i16> %877, <8 x i16> %889, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %894 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %890, <8 x i16> %159) #8
  %895 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %891, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %896 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %892, <8 x i16> %159) #8
  %897 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %893, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %898 = add <4 x i32> %894, <i32 2048, i32 2048, i32 2048, i32 2048>
  %899 = ashr <4 x i32> %898, <i32 12, i32 12, i32 12, i32 12>
  %900 = add <4 x i32> %895, <i32 2048, i32 2048, i32 2048, i32 2048>
  %901 = ashr <4 x i32> %900, <i32 12, i32 12, i32 12, i32 12>
  %902 = add <4 x i32> %896, <i32 2048, i32 2048, i32 2048, i32 2048>
  %903 = ashr <4 x i32> %902, <i32 12, i32 12, i32 12, i32 12>
  %904 = add <4 x i32> %897, <i32 2048, i32 2048, i32 2048, i32 2048>
  %905 = ashr <4 x i32> %904, <i32 12, i32 12, i32 12, i32 12>
  %906 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %899, <4 x i32> %903) #8
  %907 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %901, <4 x i32> %905) #8
  %908 = shufflevector <8 x i16> %887, <8 x i16> %875, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %909 = shufflevector <8 x i16> %875, <8 x i16> %887, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %910 = shufflevector <8 x i16> %887, <8 x i16> %875, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %911 = shufflevector <8 x i16> %875, <8 x i16> %887, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %912 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %908, <8 x i16> %159) #8
  %913 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %909, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %914 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %910, <8 x i16> %159) #8
  %915 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %911, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %916 = add <4 x i32> %912, <i32 2048, i32 2048, i32 2048, i32 2048>
  %917 = ashr <4 x i32> %916, <i32 12, i32 12, i32 12, i32 12>
  %918 = add <4 x i32> %913, <i32 2048, i32 2048, i32 2048, i32 2048>
  %919 = ashr <4 x i32> %918, <i32 12, i32 12, i32 12, i32 12>
  %920 = add <4 x i32> %914, <i32 2048, i32 2048, i32 2048, i32 2048>
  %921 = ashr <4 x i32> %920, <i32 12, i32 12, i32 12, i32 12>
  %922 = add <4 x i32> %915, <i32 2048, i32 2048, i32 2048, i32 2048>
  %923 = ashr <4 x i32> %922, <i32 12, i32 12, i32 12, i32 12>
  %924 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %917, <4 x i32> %921) #8
  %925 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %919, <4 x i32> %923) #8
  %926 = shufflevector <8 x i16> %883, <8 x i16> %879, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %927 = shufflevector <8 x i16> %879, <8 x i16> %883, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %928 = shufflevector <8 x i16> %883, <8 x i16> %879, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %929 = shufflevector <8 x i16> %879, <8 x i16> %883, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %930 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %926, <8 x i16> %180) #8
  %931 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %927, <8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>) #8
  %932 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %928, <8 x i16> %180) #8
  %933 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %929, <8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>) #8
  %934 = add <4 x i32> %930, <i32 2048, i32 2048, i32 2048, i32 2048>
  %935 = ashr <4 x i32> %934, <i32 12, i32 12, i32 12, i32 12>
  %936 = add <4 x i32> %931, <i32 2048, i32 2048, i32 2048, i32 2048>
  %937 = ashr <4 x i32> %936, <i32 12, i32 12, i32 12, i32 12>
  %938 = add <4 x i32> %932, <i32 2048, i32 2048, i32 2048, i32 2048>
  %939 = ashr <4 x i32> %938, <i32 12, i32 12, i32 12, i32 12>
  %940 = add <4 x i32> %933, <i32 2048, i32 2048, i32 2048, i32 2048>
  %941 = ashr <4 x i32> %940, <i32 12, i32 12, i32 12, i32 12>
  %942 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %935, <4 x i32> %939) #8
  %943 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %937, <4 x i32> %941) #8
  %944 = shufflevector <8 x i16> %885, <8 x i16> %881, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %945 = shufflevector <8 x i16> %881, <8 x i16> %885, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %946 = shufflevector <8 x i16> %885, <8 x i16> %881, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %947 = shufflevector <8 x i16> %881, <8 x i16> %885, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %948 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %944, <8 x i16> %180) #8
  %949 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %945, <8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>) #8
  %950 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %946, <8 x i16> %180) #8
  %951 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %947, <8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>) #8
  %952 = add <4 x i32> %948, <i32 2048, i32 2048, i32 2048, i32 2048>
  %953 = ashr <4 x i32> %952, <i32 12, i32 12, i32 12, i32 12>
  %954 = add <4 x i32> %949, <i32 2048, i32 2048, i32 2048, i32 2048>
  %955 = ashr <4 x i32> %954, <i32 12, i32 12, i32 12, i32 12>
  %956 = add <4 x i32> %950, <i32 2048, i32 2048, i32 2048, i32 2048>
  %957 = ashr <4 x i32> %956, <i32 12, i32 12, i32 12, i32 12>
  %958 = add <4 x i32> %951, <i32 2048, i32 2048, i32 2048, i32 2048>
  %959 = ashr <4 x i32> %958, <i32 12, i32 12, i32 12, i32 12>
  %960 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %953, <4 x i32> %957) #8
  %961 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %955, <4 x i32> %959) #8
  %962 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %874, <8 x i16> %878) #8
  %963 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %874, <8 x i16> %878) #8
  store <8 x i16> %962, <8 x i16>* %182, align 16
  %964 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %876, <8 x i16> %880) #8
  %965 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %876, <8 x i16> %880) #8
  store <8 x i16> %964, <8 x i16>* %185, align 16
  %966 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %906, <8 x i16> %960) #8
  %967 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %906, <8 x i16> %960) #8
  store <8 x i16> %966, <8 x i16>* %188, align 16
  %968 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %924, <8 x i16> %942) #8
  %969 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %924, <8 x i16> %942) #8
  store <8 x i16> %968, <8 x i16>* %191, align 16
  %970 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %886, <8 x i16> %882) #8
  %971 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %886, <8 x i16> %882) #8
  %972 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %888, <8 x i16> %884) #8
  %973 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %888, <8 x i16> %884) #8
  %974 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %907, <8 x i16> %961) #8
  %975 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %907, <8 x i16> %961) #8
  %976 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %925, <8 x i16> %943) #8
  %977 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %925, <8 x i16> %943) #8
  %978 = shufflevector <8 x i16> %977, <8 x i16> %969, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %979 = shufflevector <8 x i16> %969, <8 x i16> %977, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %980 = shufflevector <8 x i16> %977, <8 x i16> %969, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %981 = shufflevector <8 x i16> %969, <8 x i16> %977, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %982 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %978, <8 x i16> %156) #8
  %983 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %979, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %984 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %980, <8 x i16> %156) #8
  %985 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %981, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %986 = add <4 x i32> %982, <i32 2048, i32 2048, i32 2048, i32 2048>
  %987 = ashr <4 x i32> %986, <i32 12, i32 12, i32 12, i32 12>
  %988 = add <4 x i32> %983, <i32 2048, i32 2048, i32 2048, i32 2048>
  %989 = ashr <4 x i32> %988, <i32 12, i32 12, i32 12, i32 12>
  %990 = add <4 x i32> %984, <i32 2048, i32 2048, i32 2048, i32 2048>
  %991 = ashr <4 x i32> %990, <i32 12, i32 12, i32 12, i32 12>
  %992 = add <4 x i32> %985, <i32 2048, i32 2048, i32 2048, i32 2048>
  %993 = ashr <4 x i32> %992, <i32 12, i32 12, i32 12, i32 12>
  %994 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %987, <4 x i32> %991) #8
  %995 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %989, <4 x i32> %993) #8
  store <8 x i16> %994, <8 x i16>* %194, align 16
  %996 = shufflevector <8 x i16> %975, <8 x i16> %967, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %997 = shufflevector <8 x i16> %967, <8 x i16> %975, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %998 = shufflevector <8 x i16> %975, <8 x i16> %967, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %999 = shufflevector <8 x i16> %967, <8 x i16> %975, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1000 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %996, <8 x i16> %156) #8
  %1001 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %997, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1002 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %998, <8 x i16> %156) #8
  %1003 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %999, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1004 = add <4 x i32> %1000, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1005 = ashr <4 x i32> %1004, <i32 12, i32 12, i32 12, i32 12>
  %1006 = add <4 x i32> %1001, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1007 = ashr <4 x i32> %1006, <i32 12, i32 12, i32 12, i32 12>
  %1008 = add <4 x i32> %1002, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1009 = ashr <4 x i32> %1008, <i32 12, i32 12, i32 12, i32 12>
  %1010 = add <4 x i32> %1003, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1011 = ashr <4 x i32> %1010, <i32 12, i32 12, i32 12, i32 12>
  %1012 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1005, <4 x i32> %1009) #8
  %1013 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1007, <4 x i32> %1011) #8
  %1014 = shufflevector <8 x i16> %973, <8 x i16> %965, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1015 = shufflevector <8 x i16> %965, <8 x i16> %973, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1016 = shufflevector <8 x i16> %973, <8 x i16> %965, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1017 = shufflevector <8 x i16> %965, <8 x i16> %973, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1018 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1014, <8 x i16> %156) #8
  %1019 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1015, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1020 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1016, <8 x i16> %156) #8
  %1021 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1017, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1022 = add <4 x i32> %1018, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1023 = ashr <4 x i32> %1022, <i32 12, i32 12, i32 12, i32 12>
  %1024 = add <4 x i32> %1019, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1025 = ashr <4 x i32> %1024, <i32 12, i32 12, i32 12, i32 12>
  %1026 = add <4 x i32> %1020, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1027 = ashr <4 x i32> %1026, <i32 12, i32 12, i32 12, i32 12>
  %1028 = add <4 x i32> %1021, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1029 = ashr <4 x i32> %1028, <i32 12, i32 12, i32 12, i32 12>
  %1030 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1023, <4 x i32> %1027) #8
  %1031 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1025, <4 x i32> %1029) #8
  %1032 = shufflevector <8 x i16> %971, <8 x i16> %963, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1033 = shufflevector <8 x i16> %963, <8 x i16> %971, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1034 = shufflevector <8 x i16> %971, <8 x i16> %963, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1035 = shufflevector <8 x i16> %963, <8 x i16> %971, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1036 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1032, <8 x i16> %156) #8
  %1037 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1033, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1038 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1034, <8 x i16> %156) #8
  %1039 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1035, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1040 = add <4 x i32> %1036, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1041 = ashr <4 x i32> %1040, <i32 12, i32 12, i32 12, i32 12>
  %1042 = add <4 x i32> %1037, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1043 = ashr <4 x i32> %1042, <i32 12, i32 12, i32 12, i32 12>
  %1044 = add <4 x i32> %1038, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1045 = ashr <4 x i32> %1044, <i32 12, i32 12, i32 12, i32 12>
  %1046 = add <4 x i32> %1039, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1047 = ashr <4 x i32> %1046, <i32 12, i32 12, i32 12, i32 12>
  %1048 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1041, <4 x i32> %1045) #8
  %1049 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1043, <4 x i32> %1047) #8
  %1050 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %610, <8 x i16> %970) #8
  %1051 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %610, <8 x i16> %970) #8
  store <8 x i16> %1050, <8 x i16>* %157, align 16
  store <8 x i16> %1051, <8 x i16>* %183, align 16
  %1052 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %612, <8 x i16> %972) #8
  %1053 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %612, <8 x i16> %972) #8
  store <8 x i16> %1052, <8 x i16>* %158, align 16
  store <8 x i16> %1053, <8 x i16>* %186, align 16
  %1054 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %614, <8 x i16> %974) #8
  %1055 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %614, <8 x i16> %974) #8
  store <8 x i16> %1054, <8 x i16>* %160, align 16
  store <8 x i16> %1055, <8 x i16>* %189, align 16
  %1056 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %616, <8 x i16> %976) #8
  %1057 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %616, <8 x i16> %976) #8
  store <8 x i16> %1056, <8 x i16>* %161, align 16
  store <8 x i16> %1057, <8 x i16>* %192, align 16
  %1058 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %618, <8 x i16> %995) #8
  %1059 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %618, <8 x i16> %995) #8
  store <8 x i16> %1058, <8 x i16>* %163, align 16
  store <8 x i16> %1059, <8 x i16>* %195, align 16
  %1060 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %620, <8 x i16> %1013) #8
  %1061 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %620, <8 x i16> %1013) #8
  store <8 x i16> %1060, <8 x i16>* %166, align 16
  store <8 x i16> %1061, <8 x i16>* %198, align 16
  %1062 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %622, <8 x i16> %1031) #8
  %1063 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %622, <8 x i16> %1031) #8
  store <8 x i16> %1062, <8 x i16>* %167, align 16
  store <8 x i16> %1063, <8 x i16>* %201, align 16
  %1064 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %624, <8 x i16> %1049) #8
  %1065 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %624, <8 x i16> %1049) #8
  store <8 x i16> %1064, <8 x i16>* %164, align 16
  store <8 x i16> %1065, <8 x i16>* %204, align 16
  %1066 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %625, <8 x i16> %1048) #8
  %1067 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %625, <8 x i16> %1048) #8
  store <8 x i16> %1066, <8 x i16>* %169, align 16
  store <8 x i16> %1067, <8 x i16>* %203, align 16
  %1068 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %623, <8 x i16> %1030) #8
  %1069 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %623, <8 x i16> %1030) #8
  store <8 x i16> %1068, <8 x i16>* %172, align 16
  store <8 x i16> %1069, <8 x i16>* %200, align 16
  %1070 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %621, <8 x i16> %1012) #8
  %1071 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %621, <8 x i16> %1012) #8
  store <8 x i16> %1070, <8 x i16>* %175, align 16
  store <8 x i16> %1071, <8 x i16>* %197, align 16
  %1072 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %619, <8 x i16> %994) #8
  %1073 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %619, <8 x i16> %994) #8
  store <8 x i16> %1072, <8 x i16>* %178, align 16
  store <8 x i16> %1073, <8 x i16>* %194, align 16
  %1074 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %617, <8 x i16> %968) #8
  %1075 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %617, <8 x i16> %968) #8
  store <8 x i16> %1074, <8 x i16>* %179, align 16
  store <8 x i16> %1075, <8 x i16>* %191, align 16
  %1076 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %615, <8 x i16> %966) #8
  %1077 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %615, <8 x i16> %966) #8
  store <8 x i16> %1076, <8 x i16>* %176, align 16
  store <8 x i16> %1077, <8 x i16>* %188, align 16
  %1078 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %613, <8 x i16> %964) #8
  %1079 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %613, <8 x i16> %964) #8
  store <8 x i16> %1078, <8 x i16>* %173, align 16
  store <8 x i16> %1079, <8 x i16>* %185, align 16
  %1080 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %611, <8 x i16> %962) #8
  %1081 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %611, <8 x i16> %962) #8
  store <8 x i16> %1080, <8 x i16>* %170, align 16
  store <8 x i16> %1081, <8 x i16>* %182, align 16
  br label %1082

1082:                                             ; preds = %1082, %295
  %1083 = phi i64 [ 0, %295 ], [ %1171, %1082 ]
  %1084 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %8, i64 0, i64 %1083
  %1085 = bitcast <2 x i64>* %1084 to <8 x i16>*
  %1086 = load <8 x i16>, <8 x i16>* %1085, align 16
  %1087 = getelementptr inbounds <2 x i64>, <2 x i64>* %1084, i64 1
  %1088 = bitcast <2 x i64>* %1087 to <8 x i16>*
  %1089 = load <8 x i16>, <8 x i16>* %1088, align 16
  %1090 = shufflevector <8 x i16> %1086, <8 x i16> %1089, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1091 = getelementptr inbounds <2 x i64>, <2 x i64>* %1084, i64 2
  %1092 = bitcast <2 x i64>* %1091 to <8 x i16>*
  %1093 = load <8 x i16>, <8 x i16>* %1092, align 16
  %1094 = getelementptr inbounds <2 x i64>, <2 x i64>* %1084, i64 3
  %1095 = bitcast <2 x i64>* %1094 to <8 x i16>*
  %1096 = load <8 x i16>, <8 x i16>* %1095, align 16
  %1097 = shufflevector <8 x i16> %1093, <8 x i16> %1096, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1098 = getelementptr inbounds <2 x i64>, <2 x i64>* %1084, i64 4
  %1099 = bitcast <2 x i64>* %1098 to <8 x i16>*
  %1100 = load <8 x i16>, <8 x i16>* %1099, align 16
  %1101 = getelementptr inbounds <2 x i64>, <2 x i64>* %1084, i64 5
  %1102 = bitcast <2 x i64>* %1101 to <8 x i16>*
  %1103 = load <8 x i16>, <8 x i16>* %1102, align 16
  %1104 = shufflevector <8 x i16> %1100, <8 x i16> %1103, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1105 = getelementptr inbounds <2 x i64>, <2 x i64>* %1084, i64 6
  %1106 = bitcast <2 x i64>* %1105 to <8 x i16>*
  %1107 = load <8 x i16>, <8 x i16>* %1106, align 16
  %1108 = getelementptr inbounds <2 x i64>, <2 x i64>* %1084, i64 7
  %1109 = bitcast <2 x i64>* %1108 to <8 x i16>*
  %1110 = load <8 x i16>, <8 x i16>* %1109, align 16
  %1111 = shufflevector <8 x i16> %1107, <8 x i16> %1110, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1112 = shufflevector <8 x i16> %1086, <8 x i16> %1089, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1113 = shufflevector <8 x i16> %1093, <8 x i16> %1096, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1114 = shufflevector <8 x i16> %1100, <8 x i16> %1103, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1115 = shufflevector <8 x i16> %1107, <8 x i16> %1110, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1116 = bitcast <8 x i16> %1090 to <4 x i32>
  %1117 = bitcast <8 x i16> %1097 to <4 x i32>
  %1118 = shufflevector <4 x i32> %1116, <4 x i32> %1117, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1119 = bitcast <4 x i32> %1118 to <2 x i64>
  %1120 = bitcast <8 x i16> %1104 to <4 x i32>
  %1121 = bitcast <8 x i16> %1111 to <4 x i32>
  %1122 = shufflevector <4 x i32> %1120, <4 x i32> %1121, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1123 = bitcast <4 x i32> %1122 to <2 x i64>
  %1124 = bitcast <8 x i16> %1112 to <4 x i32>
  %1125 = bitcast <8 x i16> %1113 to <4 x i32>
  %1126 = shufflevector <4 x i32> %1124, <4 x i32> %1125, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1127 = bitcast <4 x i32> %1126 to <2 x i64>
  %1128 = bitcast <8 x i16> %1114 to <4 x i32>
  %1129 = bitcast <8 x i16> %1115 to <4 x i32>
  %1130 = shufflevector <4 x i32> %1128, <4 x i32> %1129, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1131 = bitcast <4 x i32> %1130 to <2 x i64>
  %1132 = shufflevector <4 x i32> %1116, <4 x i32> %1117, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1133 = bitcast <4 x i32> %1132 to <2 x i64>
  %1134 = shufflevector <4 x i32> %1120, <4 x i32> %1121, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1135 = bitcast <4 x i32> %1134 to <2 x i64>
  %1136 = shufflevector <4 x i32> %1124, <4 x i32> %1125, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1137 = bitcast <4 x i32> %1136 to <2 x i64>
  %1138 = shufflevector <4 x i32> %1128, <4 x i32> %1129, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1139 = bitcast <4 x i32> %1138 to <2 x i64>
  %1140 = shufflevector <2 x i64> %1119, <2 x i64> %1123, <2 x i32> <i32 0, i32 2>
  %1141 = shufflevector <2 x i64> %1119, <2 x i64> %1123, <2 x i32> <i32 1, i32 3>
  %1142 = shufflevector <2 x i64> %1133, <2 x i64> %1135, <2 x i32> <i32 0, i32 2>
  %1143 = shufflevector <2 x i64> %1133, <2 x i64> %1135, <2 x i32> <i32 1, i32 3>
  %1144 = shufflevector <2 x i64> %1127, <2 x i64> %1131, <2 x i32> <i32 0, i32 2>
  %1145 = shufflevector <2 x i64> %1127, <2 x i64> %1131, <2 x i32> <i32 1, i32 3>
  %1146 = shufflevector <2 x i64> %1137, <2 x i64> %1139, <2 x i32> <i32 0, i32 2>
  %1147 = shufflevector <2 x i64> %1137, <2 x i64> %1139, <2 x i32> <i32 1, i32 3>
  %1148 = getelementptr inbounds i16, i16* %211, i64 %1083
  %1149 = bitcast i16* %1148 to <2 x i64>*
  store <2 x i64> %1140, <2 x i64>* %1149, align 1
  %1150 = add nuw nsw i64 %1083, 32
  %1151 = getelementptr inbounds i16, i16* %211, i64 %1150
  %1152 = bitcast i16* %1151 to <2 x i64>*
  store <2 x i64> %1141, <2 x i64>* %1152, align 1
  %1153 = add nuw nsw i64 %1083, 64
  %1154 = getelementptr inbounds i16, i16* %211, i64 %1153
  %1155 = bitcast i16* %1154 to <2 x i64>*
  store <2 x i64> %1142, <2 x i64>* %1155, align 1
  %1156 = add nuw nsw i64 %1083, 96
  %1157 = getelementptr inbounds i16, i16* %211, i64 %1156
  %1158 = bitcast i16* %1157 to <2 x i64>*
  store <2 x i64> %1143, <2 x i64>* %1158, align 1
  %1159 = add nuw nsw i64 %1083, 128
  %1160 = getelementptr inbounds i16, i16* %211, i64 %1159
  %1161 = bitcast i16* %1160 to <2 x i64>*
  store <2 x i64> %1144, <2 x i64>* %1161, align 1
  %1162 = add nuw nsw i64 %1083, 160
  %1163 = getelementptr inbounds i16, i16* %211, i64 %1162
  %1164 = bitcast i16* %1163 to <2 x i64>*
  store <2 x i64> %1145, <2 x i64>* %1164, align 1
  %1165 = add nuw nsw i64 %1083, 192
  %1166 = getelementptr inbounds i16, i16* %211, i64 %1165
  %1167 = bitcast i16* %1166 to <2 x i64>*
  store <2 x i64> %1146, <2 x i64>* %1167, align 1
  %1168 = add nuw nsw i64 %1083, 224
  %1169 = getelementptr inbounds i16, i16* %211, i64 %1168
  %1170 = bitcast i16* %1169 to <2 x i64>*
  store <2 x i64> %1147, <2 x i64>* %1170, align 1
  %1171 = add nuw nsw i64 %1083, 8
  %1172 = icmp ult i64 %1171, 32
  br i1 %1172, label %1082, label %1173

1173:                                             ; preds = %1082
  call void @llvm.lifetime.end.p0i8(i64 512, i8* nonnull %91) #8
  call void @llvm.lifetime.end.p0i8(i64 512, i8* nonnull %90) #8
  %1174 = add nuw nsw i64 %208, 8
  %1175 = icmp slt i64 %1174, %63
  br i1 %1175, label %207, label %1176

1176:                                             ; preds = %1173
  %1177 = zext i8 %16 to i16
  %1178 = insertelement <8 x i16> undef, i16 %1177, i32 0
  %1179 = shufflevector <8 x i16> %1178, <8 x i16> undef, <8 x i32> zeroinitializer
  %1180 = shufflevector <8 x i16> %1178, <8 x i16> undef, <2 x i32> zeroinitializer
  %1181 = zext <2 x i16> %1180 to <2 x i64>
  %1182 = bitcast <2 x i64> %1181 to <8 x i16>
  br label %1183

1183:                                             ; preds = %1183, %1176
  %1184 = phi i64 [ %1222, %1183 ], [ 0, %1176 ]
  %1185 = shl i64 %1184, 5
  %1186 = and i64 %1185, 4294967264
  %1187 = getelementptr inbounds i16, i16* %10, i64 %1186
  %1188 = bitcast i16* %1187 to <8 x i16>*
  %1189 = load <8 x i16>, <8 x i16>* %1188, align 1
  %1190 = icmp sgt <8 x i16> %1189, <i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765>
  %1191 = add <8 x i16> %1189, %1179
  %1192 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %1191, <8 x i16> %1182) #8
  %1193 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %1191, <8 x i16> %1182) #8
  %1194 = select <8 x i1> %1190, <8 x i16> %1193, <8 x i16> %1192
  store <8 x i16> %1194, <8 x i16>* %1188, align 1
  %1195 = or i64 %1186, 8
  %1196 = getelementptr inbounds i16, i16* %10, i64 %1195
  %1197 = bitcast i16* %1196 to <8 x i16>*
  %1198 = load <8 x i16>, <8 x i16>* %1197, align 1
  %1199 = icmp sgt <8 x i16> %1198, <i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765>
  %1200 = add <8 x i16> %1198, %1179
  %1201 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %1200, <8 x i16> %1182) #8
  %1202 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %1200, <8 x i16> %1182) #8
  %1203 = select <8 x i1> %1199, <8 x i16> %1202, <8 x i16> %1201
  store <8 x i16> %1203, <8 x i16>* %1197, align 1
  %1204 = or i64 %1186, 16
  %1205 = getelementptr inbounds i16, i16* %10, i64 %1204
  %1206 = bitcast i16* %1205 to <8 x i16>*
  %1207 = load <8 x i16>, <8 x i16>* %1206, align 1
  %1208 = icmp sgt <8 x i16> %1207, <i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765>
  %1209 = add <8 x i16> %1207, %1179
  %1210 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %1209, <8 x i16> %1182) #8
  %1211 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %1209, <8 x i16> %1182) #8
  %1212 = select <8 x i1> %1208, <8 x i16> %1211, <8 x i16> %1210
  store <8 x i16> %1212, <8 x i16>* %1206, align 1
  %1213 = or i64 %1186, 24
  %1214 = getelementptr inbounds i16, i16* %10, i64 %1213
  %1215 = bitcast i16* %1214 to <8 x i16>*
  %1216 = load <8 x i16>, <8 x i16>* %1215, align 1
  %1217 = icmp sgt <8 x i16> %1216, <i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765>
  %1218 = add <8 x i16> %1216, %1179
  %1219 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %1218, <8 x i16> %1182) #8
  %1220 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %1218, <8 x i16> %1182) #8
  %1221 = select <8 x i1> %1217, <8 x i16> %1220, <8 x i16> %1219
  store <8 x i16> %1221, <8 x i16>* %1215, align 1
  %1222 = add nuw nsw i64 %1184, 1
  %1223 = icmp slt i64 %1222, %63
  br i1 %1223, label %1183, label %1224

1224:                                             ; preds = %1183, %18
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_131Dct32TransformLoopColumn_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_(i8 zeroext, i8 zeroext, i32, i8*, i32, i32, i8* nocapture readonly) #4 {
  %8 = bitcast i8* %3 to i16*
  %9 = zext i8 %1 to i64
  %10 = getelementptr inbounds [19 x i8], [19 x i8]* @_ZN7libgav115kTransformWidthE, i64 0, i64 %9
  %11 = load i8, i8* %10, align 1
  %12 = icmp sgt i32 %2, 1
  br i1 %12, label %13, label %65

13:                                               ; preds = %7
  %14 = zext i8 %11 to i64
  %15 = shl nuw nsw i64 %14, 1
  %16 = mul nuw nsw i64 %14, 3
  %17 = shl nuw nsw i64 %14, 2
  %18 = mul nuw nsw i64 %14, 5
  %19 = mul nuw nsw i64 %14, 6
  %20 = mul nuw nsw i64 %14, 7
  %21 = shl nuw nsw i64 %14, 3
  %22 = mul nuw nsw i64 %14, 9
  %23 = mul nuw nsw i64 %14, 10
  %24 = mul nuw nsw i64 %14, 11
  %25 = mul nuw nsw i64 %14, 12
  %26 = mul nuw nsw i64 %14, 13
  %27 = mul nuw nsw i64 %14, 14
  %28 = mul nuw nsw i64 %14, 15
  %29 = shl nuw nsw i64 %14, 4
  %30 = mul nuw nsw i64 %14, 17
  %31 = mul nuw nsw i64 %14, 18
  %32 = mul nuw nsw i64 %14, 19
  %33 = mul nuw nsw i64 %14, 20
  %34 = mul nuw nsw i64 %14, 21
  %35 = mul nuw nsw i64 %14, 22
  %36 = mul nuw nsw i64 %14, 23
  %37 = mul nuw nsw i64 %14, 24
  %38 = mul nuw nsw i64 %14, 25
  %39 = mul nuw nsw i64 %14, 26
  %40 = mul nuw nsw i64 %14, 27
  %41 = mul nuw nsw i64 %14, 28
  %42 = mul nuw nsw i64 %14, 29
  %43 = mul nuw nsw i64 %14, 30
  %44 = mul nuw nsw i64 %14, 31
  %45 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %46 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %47 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %48 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %49 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %50 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %51 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %52 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %53 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %54 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 201, i16 4091, i16 201, i16 4091, i16 201, i16 4091, i16 201, i16 4091>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %55 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3035, i16 2751, i16 3035, i16 2751, i16 3035, i16 2751, i16 3035, i16 2751>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %56 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1751, i16 3703, i16 1751, i16 3703, i16 1751, i16 3703, i16 1751, i16 3703>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %57 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3857, i16 1380, i16 3857, i16 1380, i16 3857, i16 1380, i16 3857, i16 1380>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %58 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 995, i16 3973, i16 995, i16 3973, i16 995, i16 3973, i16 995, i16 3973>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %59 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3513, i16 2106, i16 3513, i16 2106, i16 3513, i16 2106, i16 3513, i16 2106>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %60 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2440, i16 3290, i16 2440, i16 3290, i16 2440, i16 3290, i16 2440, i16 3290>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %61 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 4052, i16 601, i16 4052, i16 601, i16 4052, i16 601, i16 4052, i16 601>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %62 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 -4017, i16 799, i16 -4017, i16 799, i16 -4017, i16 799, i16 -4017, i16 799>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %63 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 -2276, i16 3406, i16 -2276, i16 3406, i16 -2276, i16 3406, i16 -2276, i16 3406>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %64 = zext i8 %11 to i64
  br label %103

65:                                               ; preds = %7
  %66 = icmp eq i8 %11, 4
  br i1 %66, label %69, label %67

67:                                               ; preds = %65
  %68 = zext i8 %11 to i64
  br label %77

69:                                               ; preds = %65
  %70 = bitcast i8* %3 to i64*
  %71 = load i64, i64* %70, align 1
  %72 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %71, i32 0
  %73 = bitcast <2 x i64> %72 to <8 x i16>
  %74 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %73, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  %75 = bitcast <8 x i16> %74 to <2 x i64>
  %76 = extractelement <2 x i64> %75, i32 0
  store i64 %76, i64* %70, align 1
  br label %85

77:                                               ; preds = %77, %67
  %78 = phi i64 [ 0, %67 ], [ %83, %77 ]
  %79 = getelementptr inbounds i16, i16* %8, i64 %78
  %80 = bitcast i16* %79 to <8 x i16>*
  %81 = load <8 x i16>, <8 x i16>* %80, align 1
  %82 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %81, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %82, <8 x i16>* %80, align 1
  %83 = add nuw nsw i64 %78, 8
  %84 = icmp ult i64 %83, %68
  br i1 %84, label %77, label %85

85:                                               ; preds = %77, %69
  %86 = phi i64 [ 4, %69 ], [ %68, %77 ]
  %87 = shl nuw nsw i64 %86, 1
  br label %88

88:                                               ; preds = %1022, %85
  %89 = phi i64 [ 1, %85 ], [ %1026, %1022 ]
  %90 = mul nuw nsw i64 %89, %86
  %91 = getelementptr inbounds i16, i16* %8, i64 %90
  %92 = bitcast i16* %91 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 2 %92, i8* align 2 %3, i64 %87, i1 false) #8
  %93 = add nuw nsw i64 %89, 1
  %94 = mul nuw nsw i64 %93, %86
  %95 = getelementptr inbounds i16, i16* %8, i64 %94
  %96 = bitcast i16* %95 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 2 %96, i8* align 2 %3, i64 %87, i1 false) #8
  %97 = add nuw nsw i64 %89, 2
  %98 = mul nuw nsw i64 %97, %86
  %99 = getelementptr inbounds i16, i16* %8, i64 %98
  %100 = bitcast i16* %99 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 2 %100, i8* align 2 %3, i64 %87, i1 false) #8
  %101 = add nuw nsw i64 %89, 3
  %102 = icmp eq i64 %101, 32
  br i1 %102, label %925, label %1022

103:                                              ; preds = %13, %103
  %104 = phi i64 [ 0, %13 ], [ %923, %103 ]
  %105 = getelementptr inbounds i16, i16* %8, i64 %104
  %106 = bitcast i16* %105 to <8 x i16>*
  %107 = load <8 x i16>, <8 x i16>* %106, align 1
  %108 = getelementptr inbounds i16, i16* %105, i64 %14
  %109 = bitcast i16* %108 to <8 x i16>*
  %110 = load <8 x i16>, <8 x i16>* %109, align 1
  %111 = getelementptr inbounds i16, i16* %105, i64 %15
  %112 = bitcast i16* %111 to <8 x i16>*
  %113 = load <8 x i16>, <8 x i16>* %112, align 1
  %114 = getelementptr inbounds i16, i16* %105, i64 %16
  %115 = bitcast i16* %114 to <8 x i16>*
  %116 = load <8 x i16>, <8 x i16>* %115, align 1
  %117 = getelementptr inbounds i16, i16* %105, i64 %17
  %118 = bitcast i16* %117 to <8 x i16>*
  %119 = load <8 x i16>, <8 x i16>* %118, align 1
  %120 = getelementptr inbounds i16, i16* %105, i64 %18
  %121 = bitcast i16* %120 to <8 x i16>*
  %122 = load <8 x i16>, <8 x i16>* %121, align 1
  %123 = getelementptr inbounds i16, i16* %105, i64 %19
  %124 = bitcast i16* %123 to <8 x i16>*
  %125 = load <8 x i16>, <8 x i16>* %124, align 1
  %126 = getelementptr inbounds i16, i16* %105, i64 %20
  %127 = bitcast i16* %126 to <8 x i16>*
  %128 = load <8 x i16>, <8 x i16>* %127, align 1
  %129 = getelementptr inbounds i16, i16* %105, i64 %21
  %130 = bitcast i16* %129 to <8 x i16>*
  %131 = load <8 x i16>, <8 x i16>* %130, align 1
  %132 = getelementptr inbounds i16, i16* %105, i64 %22
  %133 = bitcast i16* %132 to <8 x i16>*
  %134 = load <8 x i16>, <8 x i16>* %133, align 1
  %135 = getelementptr inbounds i16, i16* %105, i64 %23
  %136 = bitcast i16* %135 to <8 x i16>*
  %137 = load <8 x i16>, <8 x i16>* %136, align 1
  %138 = getelementptr inbounds i16, i16* %105, i64 %24
  %139 = bitcast i16* %138 to <8 x i16>*
  %140 = load <8 x i16>, <8 x i16>* %139, align 1
  %141 = getelementptr inbounds i16, i16* %105, i64 %25
  %142 = bitcast i16* %141 to <8 x i16>*
  %143 = load <8 x i16>, <8 x i16>* %142, align 1
  %144 = getelementptr inbounds i16, i16* %105, i64 %26
  %145 = bitcast i16* %144 to <8 x i16>*
  %146 = load <8 x i16>, <8 x i16>* %145, align 1
  %147 = getelementptr inbounds i16, i16* %105, i64 %27
  %148 = bitcast i16* %147 to <8 x i16>*
  %149 = load <8 x i16>, <8 x i16>* %148, align 1
  %150 = getelementptr inbounds i16, i16* %105, i64 %28
  %151 = bitcast i16* %150 to <8 x i16>*
  %152 = load <8 x i16>, <8 x i16>* %151, align 1
  %153 = getelementptr inbounds i16, i16* %105, i64 %29
  %154 = bitcast i16* %153 to <8 x i16>*
  %155 = load <8 x i16>, <8 x i16>* %154, align 1
  %156 = getelementptr inbounds i16, i16* %105, i64 %30
  %157 = bitcast i16* %156 to <8 x i16>*
  %158 = load <8 x i16>, <8 x i16>* %157, align 1
  %159 = getelementptr inbounds i16, i16* %105, i64 %31
  %160 = bitcast i16* %159 to <8 x i16>*
  %161 = load <8 x i16>, <8 x i16>* %160, align 1
  %162 = getelementptr inbounds i16, i16* %105, i64 %32
  %163 = bitcast i16* %162 to <8 x i16>*
  %164 = load <8 x i16>, <8 x i16>* %163, align 1
  %165 = getelementptr inbounds i16, i16* %105, i64 %33
  %166 = bitcast i16* %165 to <8 x i16>*
  %167 = load <8 x i16>, <8 x i16>* %166, align 1
  %168 = getelementptr inbounds i16, i16* %105, i64 %34
  %169 = bitcast i16* %168 to <8 x i16>*
  %170 = load <8 x i16>, <8 x i16>* %169, align 1
  %171 = getelementptr inbounds i16, i16* %105, i64 %35
  %172 = bitcast i16* %171 to <8 x i16>*
  %173 = load <8 x i16>, <8 x i16>* %172, align 1
  %174 = getelementptr inbounds i16, i16* %105, i64 %36
  %175 = bitcast i16* %174 to <8 x i16>*
  %176 = load <8 x i16>, <8 x i16>* %175, align 1
  %177 = getelementptr inbounds i16, i16* %105, i64 %37
  %178 = bitcast i16* %177 to <8 x i16>*
  %179 = load <8 x i16>, <8 x i16>* %178, align 1
  %180 = getelementptr inbounds i16, i16* %105, i64 %38
  %181 = bitcast i16* %180 to <8 x i16>*
  %182 = load <8 x i16>, <8 x i16>* %181, align 1
  %183 = getelementptr inbounds i16, i16* %105, i64 %39
  %184 = bitcast i16* %183 to <8 x i16>*
  %185 = load <8 x i16>, <8 x i16>* %184, align 1
  %186 = getelementptr inbounds i16, i16* %105, i64 %40
  %187 = bitcast i16* %186 to <8 x i16>*
  %188 = load <8 x i16>, <8 x i16>* %187, align 1
  %189 = getelementptr inbounds i16, i16* %105, i64 %41
  %190 = bitcast i16* %189 to <8 x i16>*
  %191 = load <8 x i16>, <8 x i16>* %190, align 1
  %192 = getelementptr inbounds i16, i16* %105, i64 %42
  %193 = bitcast i16* %192 to <8 x i16>*
  %194 = load <8 x i16>, <8 x i16>* %193, align 1
  %195 = getelementptr inbounds i16, i16* %105, i64 %43
  %196 = bitcast i16* %195 to <8 x i16>*
  %197 = load <8 x i16>, <8 x i16>* %196, align 1
  %198 = getelementptr inbounds i16, i16* %105, i64 %44
  %199 = bitcast i16* %198 to <8 x i16>*
  %200 = load <8 x i16>, <8 x i16>* %199, align 1
  %201 = shufflevector <8 x i16> %107, <8 x i16> %155, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %202 = shufflevector <8 x i16> %155, <8 x i16> %107, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %203 = shufflevector <8 x i16> %107, <8 x i16> %155, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %204 = shufflevector <8 x i16> %155, <8 x i16> %107, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %205 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %201, <8 x i16> %45) #8
  %206 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %202, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %207 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %203, <8 x i16> %45) #8
  %208 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %204, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %209 = add <4 x i32> %205, <i32 2048, i32 2048, i32 2048, i32 2048>
  %210 = ashr <4 x i32> %209, <i32 12, i32 12, i32 12, i32 12>
  %211 = add <4 x i32> %206, <i32 2048, i32 2048, i32 2048, i32 2048>
  %212 = ashr <4 x i32> %211, <i32 12, i32 12, i32 12, i32 12>
  %213 = add <4 x i32> %207, <i32 2048, i32 2048, i32 2048, i32 2048>
  %214 = ashr <4 x i32> %213, <i32 12, i32 12, i32 12, i32 12>
  %215 = add <4 x i32> %208, <i32 2048, i32 2048, i32 2048, i32 2048>
  %216 = ashr <4 x i32> %215, <i32 12, i32 12, i32 12, i32 12>
  %217 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %210, <4 x i32> %214) #8
  %218 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %212, <4 x i32> %216) #8
  %219 = shufflevector <8 x i16> %131, <8 x i16> %179, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %220 = shufflevector <8 x i16> %179, <8 x i16> %131, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %221 = shufflevector <8 x i16> %131, <8 x i16> %179, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %222 = shufflevector <8 x i16> %179, <8 x i16> %131, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %223 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %219, <8 x i16> %46) #8
  %224 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %220, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %225 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %221, <8 x i16> %46) #8
  %226 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %222, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %227 = add <4 x i32> %223, <i32 2048, i32 2048, i32 2048, i32 2048>
  %228 = ashr <4 x i32> %227, <i32 12, i32 12, i32 12, i32 12>
  %229 = add <4 x i32> %224, <i32 2048, i32 2048, i32 2048, i32 2048>
  %230 = ashr <4 x i32> %229, <i32 12, i32 12, i32 12, i32 12>
  %231 = add <4 x i32> %225, <i32 2048, i32 2048, i32 2048, i32 2048>
  %232 = ashr <4 x i32> %231, <i32 12, i32 12, i32 12, i32 12>
  %233 = add <4 x i32> %226, <i32 2048, i32 2048, i32 2048, i32 2048>
  %234 = ashr <4 x i32> %233, <i32 12, i32 12, i32 12, i32 12>
  %235 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %228, <4 x i32> %232) #8
  %236 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %230, <4 x i32> %234) #8
  %237 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %218, <8 x i16> %236) #8
  %238 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %218, <8 x i16> %236) #8
  %239 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %217, <8 x i16> %235) #8
  %240 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %217, <8 x i16> %235) #8
  %241 = shufflevector <8 x i16> %119, <8 x i16> %191, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %242 = shufflevector <8 x i16> %191, <8 x i16> %119, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %243 = shufflevector <8 x i16> %119, <8 x i16> %191, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %244 = shufflevector <8 x i16> %191, <8 x i16> %119, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %245 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %241, <8 x i16> %47) #8
  %246 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %242, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %247 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %243, <8 x i16> %47) #8
  %248 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %244, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %249 = add <4 x i32> %245, <i32 2048, i32 2048, i32 2048, i32 2048>
  %250 = ashr <4 x i32> %249, <i32 12, i32 12, i32 12, i32 12>
  %251 = add <4 x i32> %246, <i32 2048, i32 2048, i32 2048, i32 2048>
  %252 = ashr <4 x i32> %251, <i32 12, i32 12, i32 12, i32 12>
  %253 = add <4 x i32> %247, <i32 2048, i32 2048, i32 2048, i32 2048>
  %254 = ashr <4 x i32> %253, <i32 12, i32 12, i32 12, i32 12>
  %255 = add <4 x i32> %248, <i32 2048, i32 2048, i32 2048, i32 2048>
  %256 = ashr <4 x i32> %255, <i32 12, i32 12, i32 12, i32 12>
  %257 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %250, <4 x i32> %254) #8
  %258 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %252, <4 x i32> %256) #8
  %259 = shufflevector <8 x i16> %167, <8 x i16> %143, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %260 = shufflevector <8 x i16> %143, <8 x i16> %167, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %261 = shufflevector <8 x i16> %167, <8 x i16> %143, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %262 = shufflevector <8 x i16> %143, <8 x i16> %167, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %263 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %259, <8 x i16> %48) #8
  %264 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %260, <8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>) #8
  %265 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %261, <8 x i16> %48) #8
  %266 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %262, <8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>) #8
  %267 = add <4 x i32> %263, <i32 2048, i32 2048, i32 2048, i32 2048>
  %268 = ashr <4 x i32> %267, <i32 12, i32 12, i32 12, i32 12>
  %269 = add <4 x i32> %264, <i32 2048, i32 2048, i32 2048, i32 2048>
  %270 = ashr <4 x i32> %269, <i32 12, i32 12, i32 12, i32 12>
  %271 = add <4 x i32> %265, <i32 2048, i32 2048, i32 2048, i32 2048>
  %272 = ashr <4 x i32> %271, <i32 12, i32 12, i32 12, i32 12>
  %273 = add <4 x i32> %266, <i32 2048, i32 2048, i32 2048, i32 2048>
  %274 = ashr <4 x i32> %273, <i32 12, i32 12, i32 12, i32 12>
  %275 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %268, <4 x i32> %272) #8
  %276 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %270, <4 x i32> %274) #8
  %277 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %257, <8 x i16> %275) #8
  %278 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %257, <8 x i16> %275) #8
  %279 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %258, <8 x i16> %276) #8
  %280 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %258, <8 x i16> %276) #8
  %281 = shufflevector <8 x i16> %280, <8 x i16> %278, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %282 = shufflevector <8 x i16> %278, <8 x i16> %280, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %283 = shufflevector <8 x i16> %280, <8 x i16> %278, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %284 = shufflevector <8 x i16> %278, <8 x i16> %280, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %285 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %281, <8 x i16> %45) #8
  %286 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %282, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %287 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %283, <8 x i16> %45) #8
  %288 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %284, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %289 = add <4 x i32> %285, <i32 2048, i32 2048, i32 2048, i32 2048>
  %290 = ashr <4 x i32> %289, <i32 12, i32 12, i32 12, i32 12>
  %291 = add <4 x i32> %286, <i32 2048, i32 2048, i32 2048, i32 2048>
  %292 = ashr <4 x i32> %291, <i32 12, i32 12, i32 12, i32 12>
  %293 = add <4 x i32> %287, <i32 2048, i32 2048, i32 2048, i32 2048>
  %294 = ashr <4 x i32> %293, <i32 12, i32 12, i32 12, i32 12>
  %295 = add <4 x i32> %288, <i32 2048, i32 2048, i32 2048, i32 2048>
  %296 = ashr <4 x i32> %295, <i32 12, i32 12, i32 12, i32 12>
  %297 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %290, <4 x i32> %294) #8
  %298 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %292, <4 x i32> %296) #8
  %299 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %237, <8 x i16> %279) #8
  %300 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %237, <8 x i16> %279) #8
  %301 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %239, <8 x i16> %298) #8
  %302 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %239, <8 x i16> %298) #8
  %303 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %240, <8 x i16> %297) #8
  %304 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %240, <8 x i16> %297) #8
  %305 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %238, <8 x i16> %277) #8
  %306 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %238, <8 x i16> %277) #8
  %307 = shufflevector <8 x i16> %113, <8 x i16> %197, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %308 = shufflevector <8 x i16> %197, <8 x i16> %113, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %309 = shufflevector <8 x i16> %113, <8 x i16> %197, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %310 = shufflevector <8 x i16> %197, <8 x i16> %113, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %311 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %307, <8 x i16> %49) #8
  %312 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %308, <8 x i16> <i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076>) #8
  %313 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %309, <8 x i16> %49) #8
  %314 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %310, <8 x i16> <i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076>) #8
  %315 = add <4 x i32> %311, <i32 2048, i32 2048, i32 2048, i32 2048>
  %316 = ashr <4 x i32> %315, <i32 12, i32 12, i32 12, i32 12>
  %317 = add <4 x i32> %312, <i32 2048, i32 2048, i32 2048, i32 2048>
  %318 = ashr <4 x i32> %317, <i32 12, i32 12, i32 12, i32 12>
  %319 = add <4 x i32> %313, <i32 2048, i32 2048, i32 2048, i32 2048>
  %320 = ashr <4 x i32> %319, <i32 12, i32 12, i32 12, i32 12>
  %321 = add <4 x i32> %314, <i32 2048, i32 2048, i32 2048, i32 2048>
  %322 = ashr <4 x i32> %321, <i32 12, i32 12, i32 12, i32 12>
  %323 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %316, <4 x i32> %320) #8
  %324 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %318, <4 x i32> %322) #8
  %325 = shufflevector <8 x i16> %161, <8 x i16> %149, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %326 = shufflevector <8 x i16> %149, <8 x i16> %161, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %327 = shufflevector <8 x i16> %161, <8 x i16> %149, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %328 = shufflevector <8 x i16> %149, <8 x i16> %161, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %329 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %325, <8 x i16> %50) #8
  %330 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %326, <8 x i16> <i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598>) #8
  %331 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %327, <8 x i16> %50) #8
  %332 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %328, <8 x i16> <i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598>) #8
  %333 = add <4 x i32> %329, <i32 2048, i32 2048, i32 2048, i32 2048>
  %334 = ashr <4 x i32> %333, <i32 12, i32 12, i32 12, i32 12>
  %335 = add <4 x i32> %330, <i32 2048, i32 2048, i32 2048, i32 2048>
  %336 = ashr <4 x i32> %335, <i32 12, i32 12, i32 12, i32 12>
  %337 = add <4 x i32> %331, <i32 2048, i32 2048, i32 2048, i32 2048>
  %338 = ashr <4 x i32> %337, <i32 12, i32 12, i32 12, i32 12>
  %339 = add <4 x i32> %332, <i32 2048, i32 2048, i32 2048, i32 2048>
  %340 = ashr <4 x i32> %339, <i32 12, i32 12, i32 12, i32 12>
  %341 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %334, <4 x i32> %338) #8
  %342 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %336, <4 x i32> %340) #8
  %343 = shufflevector <8 x i16> %137, <8 x i16> %173, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %344 = shufflevector <8 x i16> %173, <8 x i16> %137, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %345 = shufflevector <8 x i16> %137, <8 x i16> %173, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %346 = shufflevector <8 x i16> %173, <8 x i16> %137, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %347 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %343, <8 x i16> %51) #8
  %348 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %344, <8 x i16> <i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612>) #8
  %349 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %345, <8 x i16> %51) #8
  %350 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %346, <8 x i16> <i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612>) #8
  %351 = add <4 x i32> %347, <i32 2048, i32 2048, i32 2048, i32 2048>
  %352 = ashr <4 x i32> %351, <i32 12, i32 12, i32 12, i32 12>
  %353 = add <4 x i32> %348, <i32 2048, i32 2048, i32 2048, i32 2048>
  %354 = ashr <4 x i32> %353, <i32 12, i32 12, i32 12, i32 12>
  %355 = add <4 x i32> %349, <i32 2048, i32 2048, i32 2048, i32 2048>
  %356 = ashr <4 x i32> %355, <i32 12, i32 12, i32 12, i32 12>
  %357 = add <4 x i32> %350, <i32 2048, i32 2048, i32 2048, i32 2048>
  %358 = ashr <4 x i32> %357, <i32 12, i32 12, i32 12, i32 12>
  %359 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %352, <4 x i32> %356) #8
  %360 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %354, <4 x i32> %358) #8
  %361 = shufflevector <8 x i16> %185, <8 x i16> %125, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %362 = shufflevector <8 x i16> %125, <8 x i16> %185, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %363 = shufflevector <8 x i16> %185, <8 x i16> %125, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %364 = shufflevector <8 x i16> %125, <8 x i16> %185, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %365 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %361, <8 x i16> %52) #8
  %366 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %362, <8 x i16> <i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189>) #8
  %367 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %363, <8 x i16> %52) #8
  %368 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %364, <8 x i16> <i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189>) #8
  %369 = add <4 x i32> %365, <i32 2048, i32 2048, i32 2048, i32 2048>
  %370 = ashr <4 x i32> %369, <i32 12, i32 12, i32 12, i32 12>
  %371 = add <4 x i32> %366, <i32 2048, i32 2048, i32 2048, i32 2048>
  %372 = ashr <4 x i32> %371, <i32 12, i32 12, i32 12, i32 12>
  %373 = add <4 x i32> %367, <i32 2048, i32 2048, i32 2048, i32 2048>
  %374 = ashr <4 x i32> %373, <i32 12, i32 12, i32 12, i32 12>
  %375 = add <4 x i32> %368, <i32 2048, i32 2048, i32 2048, i32 2048>
  %376 = ashr <4 x i32> %375, <i32 12, i32 12, i32 12, i32 12>
  %377 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %370, <4 x i32> %374) #8
  %378 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %372, <4 x i32> %376) #8
  %379 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %323, <8 x i16> %341) #8
  %380 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %323, <8 x i16> %341) #8
  %381 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %377, <8 x i16> %359) #8
  %382 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %377, <8 x i16> %359) #8
  %383 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %378, <8 x i16> %360) #8
  %384 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %378, <8 x i16> %360) #8
  %385 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %324, <8 x i16> %342) #8
  %386 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %324, <8 x i16> %342) #8
  %387 = shufflevector <8 x i16> %386, <8 x i16> %380, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %388 = shufflevector <8 x i16> %380, <8 x i16> %386, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %389 = shufflevector <8 x i16> %386, <8 x i16> %380, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %390 = shufflevector <8 x i16> %380, <8 x i16> %386, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %391 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %387, <8 x i16> %46) #8
  %392 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %388, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %393 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %389, <8 x i16> %46) #8
  %394 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %390, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %395 = add <4 x i32> %391, <i32 2048, i32 2048, i32 2048, i32 2048>
  %396 = ashr <4 x i32> %395, <i32 12, i32 12, i32 12, i32 12>
  %397 = add <4 x i32> %392, <i32 2048, i32 2048, i32 2048, i32 2048>
  %398 = ashr <4 x i32> %397, <i32 12, i32 12, i32 12, i32 12>
  %399 = add <4 x i32> %393, <i32 2048, i32 2048, i32 2048, i32 2048>
  %400 = ashr <4 x i32> %399, <i32 12, i32 12, i32 12, i32 12>
  %401 = add <4 x i32> %394, <i32 2048, i32 2048, i32 2048, i32 2048>
  %402 = ashr <4 x i32> %401, <i32 12, i32 12, i32 12, i32 12>
  %403 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %396, <4 x i32> %400) #8
  %404 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %398, <4 x i32> %402) #8
  %405 = shufflevector <8 x i16> %384, <8 x i16> %382, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %406 = shufflevector <8 x i16> %382, <8 x i16> %384, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %407 = shufflevector <8 x i16> %384, <8 x i16> %382, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %408 = shufflevector <8 x i16> %382, <8 x i16> %384, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %409 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %405, <8 x i16> %53) #8
  %410 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %406, <8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>) #8
  %411 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %407, <8 x i16> %53) #8
  %412 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %408, <8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>) #8
  %413 = add <4 x i32> %409, <i32 2048, i32 2048, i32 2048, i32 2048>
  %414 = ashr <4 x i32> %413, <i32 12, i32 12, i32 12, i32 12>
  %415 = add <4 x i32> %410, <i32 2048, i32 2048, i32 2048, i32 2048>
  %416 = ashr <4 x i32> %415, <i32 12, i32 12, i32 12, i32 12>
  %417 = add <4 x i32> %411, <i32 2048, i32 2048, i32 2048, i32 2048>
  %418 = ashr <4 x i32> %417, <i32 12, i32 12, i32 12, i32 12>
  %419 = add <4 x i32> %412, <i32 2048, i32 2048, i32 2048, i32 2048>
  %420 = ashr <4 x i32> %419, <i32 12, i32 12, i32 12, i32 12>
  %421 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %414, <4 x i32> %418) #8
  %422 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %416, <4 x i32> %420) #8
  %423 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %379, <8 x i16> %381) #8
  %424 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %379, <8 x i16> %381) #8
  %425 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %403, <8 x i16> %421) #8
  %426 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %403, <8 x i16> %421) #8
  %427 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %385, <8 x i16> %383) #8
  %428 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %385, <8 x i16> %383) #8
  %429 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %404, <8 x i16> %422) #8
  %430 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %404, <8 x i16> %422) #8
  %431 = shufflevector <8 x i16> %430, <8 x i16> %426, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %432 = shufflevector <8 x i16> %426, <8 x i16> %430, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %433 = shufflevector <8 x i16> %430, <8 x i16> %426, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %434 = shufflevector <8 x i16> %426, <8 x i16> %430, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %435 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %431, <8 x i16> %45) #8
  %436 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %432, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %437 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %433, <8 x i16> %45) #8
  %438 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %434, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %439 = add <4 x i32> %435, <i32 2048, i32 2048, i32 2048, i32 2048>
  %440 = ashr <4 x i32> %439, <i32 12, i32 12, i32 12, i32 12>
  %441 = add <4 x i32> %436, <i32 2048, i32 2048, i32 2048, i32 2048>
  %442 = ashr <4 x i32> %441, <i32 12, i32 12, i32 12, i32 12>
  %443 = add <4 x i32> %437, <i32 2048, i32 2048, i32 2048, i32 2048>
  %444 = ashr <4 x i32> %443, <i32 12, i32 12, i32 12, i32 12>
  %445 = add <4 x i32> %438, <i32 2048, i32 2048, i32 2048, i32 2048>
  %446 = ashr <4 x i32> %445, <i32 12, i32 12, i32 12, i32 12>
  %447 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %440, <4 x i32> %444) #8
  %448 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %442, <4 x i32> %446) #8
  %449 = shufflevector <8 x i16> %428, <8 x i16> %424, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %450 = shufflevector <8 x i16> %424, <8 x i16> %428, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %451 = shufflevector <8 x i16> %428, <8 x i16> %424, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %452 = shufflevector <8 x i16> %424, <8 x i16> %428, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %453 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %449, <8 x i16> %45) #8
  %454 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %450, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %455 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %451, <8 x i16> %45) #8
  %456 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %452, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %457 = add <4 x i32> %453, <i32 2048, i32 2048, i32 2048, i32 2048>
  %458 = ashr <4 x i32> %457, <i32 12, i32 12, i32 12, i32 12>
  %459 = add <4 x i32> %454, <i32 2048, i32 2048, i32 2048, i32 2048>
  %460 = ashr <4 x i32> %459, <i32 12, i32 12, i32 12, i32 12>
  %461 = add <4 x i32> %455, <i32 2048, i32 2048, i32 2048, i32 2048>
  %462 = ashr <4 x i32> %461, <i32 12, i32 12, i32 12, i32 12>
  %463 = add <4 x i32> %456, <i32 2048, i32 2048, i32 2048, i32 2048>
  %464 = ashr <4 x i32> %463, <i32 12, i32 12, i32 12, i32 12>
  %465 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %458, <4 x i32> %462) #8
  %466 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %460, <4 x i32> %464) #8
  %467 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %299, <8 x i16> %427) #8
  %468 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %299, <8 x i16> %427) #8
  %469 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %301, <8 x i16> %429) #8
  %470 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %301, <8 x i16> %429) #8
  %471 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %303, <8 x i16> %448) #8
  %472 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %303, <8 x i16> %448) #8
  %473 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %305, <8 x i16> %466) #8
  %474 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %305, <8 x i16> %466) #8
  %475 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %306, <8 x i16> %465) #8
  %476 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %306, <8 x i16> %465) #8
  %477 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %304, <8 x i16> %447) #8
  %478 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %304, <8 x i16> %447) #8
  %479 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %302, <8 x i16> %425) #8
  %480 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %302, <8 x i16> %425) #8
  %481 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %300, <8 x i16> %423) #8
  %482 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %300, <8 x i16> %423) #8
  %483 = shufflevector <8 x i16> %110, <8 x i16> %200, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %484 = shufflevector <8 x i16> %200, <8 x i16> %110, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %485 = shufflevector <8 x i16> %110, <8 x i16> %200, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %486 = shufflevector <8 x i16> %200, <8 x i16> %110, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %487 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %483, <8 x i16> %54) #8
  %488 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %484, <8 x i16> <i16 201, i16 4091, i16 201, i16 4091, i16 201, i16 4091, i16 201, i16 4091>) #8
  %489 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %485, <8 x i16> %54) #8
  %490 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %486, <8 x i16> <i16 201, i16 4091, i16 201, i16 4091, i16 201, i16 4091, i16 201, i16 4091>) #8
  %491 = add <4 x i32> %487, <i32 2048, i32 2048, i32 2048, i32 2048>
  %492 = ashr <4 x i32> %491, <i32 12, i32 12, i32 12, i32 12>
  %493 = add <4 x i32> %488, <i32 2048, i32 2048, i32 2048, i32 2048>
  %494 = ashr <4 x i32> %493, <i32 12, i32 12, i32 12, i32 12>
  %495 = add <4 x i32> %489, <i32 2048, i32 2048, i32 2048, i32 2048>
  %496 = ashr <4 x i32> %495, <i32 12, i32 12, i32 12, i32 12>
  %497 = add <4 x i32> %490, <i32 2048, i32 2048, i32 2048, i32 2048>
  %498 = ashr <4 x i32> %497, <i32 12, i32 12, i32 12, i32 12>
  %499 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %492, <4 x i32> %496) #8
  %500 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %494, <4 x i32> %498) #8
  %501 = shufflevector <8 x i16> %158, <8 x i16> %152, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %502 = shufflevector <8 x i16> %152, <8 x i16> %158, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %503 = shufflevector <8 x i16> %158, <8 x i16> %152, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %504 = shufflevector <8 x i16> %152, <8 x i16> %158, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %505 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %501, <8 x i16> %55) #8
  %506 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %502, <8 x i16> <i16 3035, i16 2751, i16 3035, i16 2751, i16 3035, i16 2751, i16 3035, i16 2751>) #8
  %507 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %503, <8 x i16> %55) #8
  %508 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %504, <8 x i16> <i16 3035, i16 2751, i16 3035, i16 2751, i16 3035, i16 2751, i16 3035, i16 2751>) #8
  %509 = add <4 x i32> %505, <i32 2048, i32 2048, i32 2048, i32 2048>
  %510 = ashr <4 x i32> %509, <i32 12, i32 12, i32 12, i32 12>
  %511 = add <4 x i32> %506, <i32 2048, i32 2048, i32 2048, i32 2048>
  %512 = ashr <4 x i32> %511, <i32 12, i32 12, i32 12, i32 12>
  %513 = add <4 x i32> %507, <i32 2048, i32 2048, i32 2048, i32 2048>
  %514 = ashr <4 x i32> %513, <i32 12, i32 12, i32 12, i32 12>
  %515 = add <4 x i32> %508, <i32 2048, i32 2048, i32 2048, i32 2048>
  %516 = ashr <4 x i32> %515, <i32 12, i32 12, i32 12, i32 12>
  %517 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %510, <4 x i32> %514) #8
  %518 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %512, <4 x i32> %516) #8
  %519 = shufflevector <8 x i16> %134, <8 x i16> %176, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %520 = shufflevector <8 x i16> %176, <8 x i16> %134, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %521 = shufflevector <8 x i16> %134, <8 x i16> %176, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %522 = shufflevector <8 x i16> %176, <8 x i16> %134, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %523 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %519, <8 x i16> %56) #8
  %524 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %520, <8 x i16> <i16 1751, i16 3703, i16 1751, i16 3703, i16 1751, i16 3703, i16 1751, i16 3703>) #8
  %525 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %521, <8 x i16> %56) #8
  %526 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %522, <8 x i16> <i16 1751, i16 3703, i16 1751, i16 3703, i16 1751, i16 3703, i16 1751, i16 3703>) #8
  %527 = add <4 x i32> %523, <i32 2048, i32 2048, i32 2048, i32 2048>
  %528 = ashr <4 x i32> %527, <i32 12, i32 12, i32 12, i32 12>
  %529 = add <4 x i32> %524, <i32 2048, i32 2048, i32 2048, i32 2048>
  %530 = ashr <4 x i32> %529, <i32 12, i32 12, i32 12, i32 12>
  %531 = add <4 x i32> %525, <i32 2048, i32 2048, i32 2048, i32 2048>
  %532 = ashr <4 x i32> %531, <i32 12, i32 12, i32 12, i32 12>
  %533 = add <4 x i32> %526, <i32 2048, i32 2048, i32 2048, i32 2048>
  %534 = ashr <4 x i32> %533, <i32 12, i32 12, i32 12, i32 12>
  %535 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %528, <4 x i32> %532) #8
  %536 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %530, <4 x i32> %534) #8
  %537 = shufflevector <8 x i16> %182, <8 x i16> %128, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %538 = shufflevector <8 x i16> %128, <8 x i16> %182, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %539 = shufflevector <8 x i16> %182, <8 x i16> %128, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %540 = shufflevector <8 x i16> %128, <8 x i16> %182, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %541 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %537, <8 x i16> %57) #8
  %542 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %538, <8 x i16> <i16 3857, i16 1380, i16 3857, i16 1380, i16 3857, i16 1380, i16 3857, i16 1380>) #8
  %543 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %539, <8 x i16> %57) #8
  %544 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %540, <8 x i16> <i16 3857, i16 1380, i16 3857, i16 1380, i16 3857, i16 1380, i16 3857, i16 1380>) #8
  %545 = add <4 x i32> %541, <i32 2048, i32 2048, i32 2048, i32 2048>
  %546 = ashr <4 x i32> %545, <i32 12, i32 12, i32 12, i32 12>
  %547 = add <4 x i32> %542, <i32 2048, i32 2048, i32 2048, i32 2048>
  %548 = ashr <4 x i32> %547, <i32 12, i32 12, i32 12, i32 12>
  %549 = add <4 x i32> %543, <i32 2048, i32 2048, i32 2048, i32 2048>
  %550 = ashr <4 x i32> %549, <i32 12, i32 12, i32 12, i32 12>
  %551 = add <4 x i32> %544, <i32 2048, i32 2048, i32 2048, i32 2048>
  %552 = ashr <4 x i32> %551, <i32 12, i32 12, i32 12, i32 12>
  %553 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %546, <4 x i32> %550) #8
  %554 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %548, <4 x i32> %552) #8
  %555 = shufflevector <8 x i16> %122, <8 x i16> %188, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %556 = shufflevector <8 x i16> %188, <8 x i16> %122, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %557 = shufflevector <8 x i16> %122, <8 x i16> %188, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %558 = shufflevector <8 x i16> %188, <8 x i16> %122, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %559 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %555, <8 x i16> %58) #8
  %560 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %556, <8 x i16> <i16 995, i16 3973, i16 995, i16 3973, i16 995, i16 3973, i16 995, i16 3973>) #8
  %561 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %557, <8 x i16> %58) #8
  %562 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %558, <8 x i16> <i16 995, i16 3973, i16 995, i16 3973, i16 995, i16 3973, i16 995, i16 3973>) #8
  %563 = add <4 x i32> %559, <i32 2048, i32 2048, i32 2048, i32 2048>
  %564 = ashr <4 x i32> %563, <i32 12, i32 12, i32 12, i32 12>
  %565 = add <4 x i32> %560, <i32 2048, i32 2048, i32 2048, i32 2048>
  %566 = ashr <4 x i32> %565, <i32 12, i32 12, i32 12, i32 12>
  %567 = add <4 x i32> %561, <i32 2048, i32 2048, i32 2048, i32 2048>
  %568 = ashr <4 x i32> %567, <i32 12, i32 12, i32 12, i32 12>
  %569 = add <4 x i32> %562, <i32 2048, i32 2048, i32 2048, i32 2048>
  %570 = ashr <4 x i32> %569, <i32 12, i32 12, i32 12, i32 12>
  %571 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %564, <4 x i32> %568) #8
  %572 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %566, <4 x i32> %570) #8
  %573 = shufflevector <8 x i16> %170, <8 x i16> %140, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %574 = shufflevector <8 x i16> %140, <8 x i16> %170, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %575 = shufflevector <8 x i16> %170, <8 x i16> %140, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %576 = shufflevector <8 x i16> %140, <8 x i16> %170, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %577 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %573, <8 x i16> %59) #8
  %578 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %574, <8 x i16> <i16 3513, i16 2106, i16 3513, i16 2106, i16 3513, i16 2106, i16 3513, i16 2106>) #8
  %579 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %575, <8 x i16> %59) #8
  %580 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %576, <8 x i16> <i16 3513, i16 2106, i16 3513, i16 2106, i16 3513, i16 2106, i16 3513, i16 2106>) #8
  %581 = add <4 x i32> %577, <i32 2048, i32 2048, i32 2048, i32 2048>
  %582 = ashr <4 x i32> %581, <i32 12, i32 12, i32 12, i32 12>
  %583 = add <4 x i32> %578, <i32 2048, i32 2048, i32 2048, i32 2048>
  %584 = ashr <4 x i32> %583, <i32 12, i32 12, i32 12, i32 12>
  %585 = add <4 x i32> %579, <i32 2048, i32 2048, i32 2048, i32 2048>
  %586 = ashr <4 x i32> %585, <i32 12, i32 12, i32 12, i32 12>
  %587 = add <4 x i32> %580, <i32 2048, i32 2048, i32 2048, i32 2048>
  %588 = ashr <4 x i32> %587, <i32 12, i32 12, i32 12, i32 12>
  %589 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %582, <4 x i32> %586) #8
  %590 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %584, <4 x i32> %588) #8
  %591 = shufflevector <8 x i16> %146, <8 x i16> %164, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %592 = shufflevector <8 x i16> %164, <8 x i16> %146, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %593 = shufflevector <8 x i16> %146, <8 x i16> %164, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %594 = shufflevector <8 x i16> %164, <8 x i16> %146, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %595 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %591, <8 x i16> %60) #8
  %596 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %592, <8 x i16> <i16 2440, i16 3290, i16 2440, i16 3290, i16 2440, i16 3290, i16 2440, i16 3290>) #8
  %597 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %593, <8 x i16> %60) #8
  %598 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %594, <8 x i16> <i16 2440, i16 3290, i16 2440, i16 3290, i16 2440, i16 3290, i16 2440, i16 3290>) #8
  %599 = add <4 x i32> %595, <i32 2048, i32 2048, i32 2048, i32 2048>
  %600 = ashr <4 x i32> %599, <i32 12, i32 12, i32 12, i32 12>
  %601 = add <4 x i32> %596, <i32 2048, i32 2048, i32 2048, i32 2048>
  %602 = ashr <4 x i32> %601, <i32 12, i32 12, i32 12, i32 12>
  %603 = add <4 x i32> %597, <i32 2048, i32 2048, i32 2048, i32 2048>
  %604 = ashr <4 x i32> %603, <i32 12, i32 12, i32 12, i32 12>
  %605 = add <4 x i32> %598, <i32 2048, i32 2048, i32 2048, i32 2048>
  %606 = ashr <4 x i32> %605, <i32 12, i32 12, i32 12, i32 12>
  %607 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %600, <4 x i32> %604) #8
  %608 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %602, <4 x i32> %606) #8
  %609 = shufflevector <8 x i16> %194, <8 x i16> %116, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %610 = shufflevector <8 x i16> %116, <8 x i16> %194, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %611 = shufflevector <8 x i16> %194, <8 x i16> %116, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %612 = shufflevector <8 x i16> %116, <8 x i16> %194, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %613 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %609, <8 x i16> %61) #8
  %614 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %610, <8 x i16> <i16 4052, i16 601, i16 4052, i16 601, i16 4052, i16 601, i16 4052, i16 601>) #8
  %615 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %611, <8 x i16> %61) #8
  %616 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %612, <8 x i16> <i16 4052, i16 601, i16 4052, i16 601, i16 4052, i16 601, i16 4052, i16 601>) #8
  %617 = add <4 x i32> %613, <i32 2048, i32 2048, i32 2048, i32 2048>
  %618 = ashr <4 x i32> %617, <i32 12, i32 12, i32 12, i32 12>
  %619 = add <4 x i32> %614, <i32 2048, i32 2048, i32 2048, i32 2048>
  %620 = ashr <4 x i32> %619, <i32 12, i32 12, i32 12, i32 12>
  %621 = add <4 x i32> %615, <i32 2048, i32 2048, i32 2048, i32 2048>
  %622 = ashr <4 x i32> %621, <i32 12, i32 12, i32 12, i32 12>
  %623 = add <4 x i32> %616, <i32 2048, i32 2048, i32 2048, i32 2048>
  %624 = ashr <4 x i32> %623, <i32 12, i32 12, i32 12, i32 12>
  %625 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %618, <4 x i32> %622) #8
  %626 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %620, <4 x i32> %624) #8
  %627 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %499, <8 x i16> %517) #8
  %628 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %499, <8 x i16> %517) #8
  %629 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %553, <8 x i16> %535) #8
  %630 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %553, <8 x i16> %535) #8
  %631 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %571, <8 x i16> %589) #8
  %632 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %571, <8 x i16> %589) #8
  %633 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %625, <8 x i16> %607) #8
  %634 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %625, <8 x i16> %607) #8
  %635 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %626, <8 x i16> %608) #8
  %636 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %626, <8 x i16> %608) #8
  %637 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %572, <8 x i16> %590) #8
  %638 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %572, <8 x i16> %590) #8
  %639 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %554, <8 x i16> %536) #8
  %640 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %554, <8 x i16> %536) #8
  %641 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %500, <8 x i16> %518) #8
  %642 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %500, <8 x i16> %518) #8
  %643 = shufflevector <8 x i16> %642, <8 x i16> %628, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %644 = shufflevector <8 x i16> %628, <8 x i16> %642, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %645 = shufflevector <8 x i16> %642, <8 x i16> %628, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %646 = shufflevector <8 x i16> %628, <8 x i16> %642, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %647 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %643, <8 x i16> %47) #8
  %648 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %644, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %649 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %645, <8 x i16> %47) #8
  %650 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %646, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %651 = add <4 x i32> %647, <i32 2048, i32 2048, i32 2048, i32 2048>
  %652 = ashr <4 x i32> %651, <i32 12, i32 12, i32 12, i32 12>
  %653 = add <4 x i32> %648, <i32 2048, i32 2048, i32 2048, i32 2048>
  %654 = ashr <4 x i32> %653, <i32 12, i32 12, i32 12, i32 12>
  %655 = add <4 x i32> %649, <i32 2048, i32 2048, i32 2048, i32 2048>
  %656 = ashr <4 x i32> %655, <i32 12, i32 12, i32 12, i32 12>
  %657 = add <4 x i32> %650, <i32 2048, i32 2048, i32 2048, i32 2048>
  %658 = ashr <4 x i32> %657, <i32 12, i32 12, i32 12, i32 12>
  %659 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %652, <4 x i32> %656) #8
  %660 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %654, <4 x i32> %658) #8
  %661 = shufflevector <8 x i16> %640, <8 x i16> %630, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %662 = shufflevector <8 x i16> %630, <8 x i16> %640, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %663 = shufflevector <8 x i16> %640, <8 x i16> %630, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %664 = shufflevector <8 x i16> %630, <8 x i16> %640, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %665 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %661, <8 x i16> %62) #8
  %666 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %662, <8 x i16> <i16 -4017, i16 799, i16 -4017, i16 799, i16 -4017, i16 799, i16 -4017, i16 799>) #8
  %667 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %663, <8 x i16> %62) #8
  %668 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %664, <8 x i16> <i16 -4017, i16 799, i16 -4017, i16 799, i16 -4017, i16 799, i16 -4017, i16 799>) #8
  %669 = add <4 x i32> %665, <i32 2048, i32 2048, i32 2048, i32 2048>
  %670 = ashr <4 x i32> %669, <i32 12, i32 12, i32 12, i32 12>
  %671 = add <4 x i32> %666, <i32 2048, i32 2048, i32 2048, i32 2048>
  %672 = ashr <4 x i32> %671, <i32 12, i32 12, i32 12, i32 12>
  %673 = add <4 x i32> %667, <i32 2048, i32 2048, i32 2048, i32 2048>
  %674 = ashr <4 x i32> %673, <i32 12, i32 12, i32 12, i32 12>
  %675 = add <4 x i32> %668, <i32 2048, i32 2048, i32 2048, i32 2048>
  %676 = ashr <4 x i32> %675, <i32 12, i32 12, i32 12, i32 12>
  %677 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %670, <4 x i32> %674) #8
  %678 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %672, <4 x i32> %676) #8
  %679 = shufflevector <8 x i16> %638, <8 x i16> %632, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %680 = shufflevector <8 x i16> %632, <8 x i16> %638, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %681 = shufflevector <8 x i16> %638, <8 x i16> %632, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %682 = shufflevector <8 x i16> %632, <8 x i16> %638, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %683 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %679, <8 x i16> %48) #8
  %684 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %680, <8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>) #8
  %685 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %681, <8 x i16> %48) #8
  %686 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %682, <8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>) #8
  %687 = add <4 x i32> %683, <i32 2048, i32 2048, i32 2048, i32 2048>
  %688 = ashr <4 x i32> %687, <i32 12, i32 12, i32 12, i32 12>
  %689 = add <4 x i32> %684, <i32 2048, i32 2048, i32 2048, i32 2048>
  %690 = ashr <4 x i32> %689, <i32 12, i32 12, i32 12, i32 12>
  %691 = add <4 x i32> %685, <i32 2048, i32 2048, i32 2048, i32 2048>
  %692 = ashr <4 x i32> %691, <i32 12, i32 12, i32 12, i32 12>
  %693 = add <4 x i32> %686, <i32 2048, i32 2048, i32 2048, i32 2048>
  %694 = ashr <4 x i32> %693, <i32 12, i32 12, i32 12, i32 12>
  %695 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %688, <4 x i32> %692) #8
  %696 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %690, <4 x i32> %694) #8
  %697 = shufflevector <8 x i16> %636, <8 x i16> %634, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %698 = shufflevector <8 x i16> %634, <8 x i16> %636, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %699 = shufflevector <8 x i16> %636, <8 x i16> %634, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %700 = shufflevector <8 x i16> %634, <8 x i16> %636, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %701 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %697, <8 x i16> %63) #8
  %702 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %698, <8 x i16> <i16 -2276, i16 3406, i16 -2276, i16 3406, i16 -2276, i16 3406, i16 -2276, i16 3406>) #8
  %703 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %699, <8 x i16> %63) #8
  %704 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %700, <8 x i16> <i16 -2276, i16 3406, i16 -2276, i16 3406, i16 -2276, i16 3406, i16 -2276, i16 3406>) #8
  %705 = add <4 x i32> %701, <i32 2048, i32 2048, i32 2048, i32 2048>
  %706 = ashr <4 x i32> %705, <i32 12, i32 12, i32 12, i32 12>
  %707 = add <4 x i32> %702, <i32 2048, i32 2048, i32 2048, i32 2048>
  %708 = ashr <4 x i32> %707, <i32 12, i32 12, i32 12, i32 12>
  %709 = add <4 x i32> %703, <i32 2048, i32 2048, i32 2048, i32 2048>
  %710 = ashr <4 x i32> %709, <i32 12, i32 12, i32 12, i32 12>
  %711 = add <4 x i32> %704, <i32 2048, i32 2048, i32 2048, i32 2048>
  %712 = ashr <4 x i32> %711, <i32 12, i32 12, i32 12, i32 12>
  %713 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %706, <4 x i32> %710) #8
  %714 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %708, <4 x i32> %712) #8
  %715 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %627, <8 x i16> %629) #8
  %716 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %627, <8 x i16> %629) #8
  %717 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %659, <8 x i16> %677) #8
  %718 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %659, <8 x i16> %677) #8
  %719 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %633, <8 x i16> %631) #8
  %720 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %633, <8 x i16> %631) #8
  %721 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %713, <8 x i16> %695) #8
  %722 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %713, <8 x i16> %695) #8
  %723 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %635, <8 x i16> %637) #8
  %724 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %635, <8 x i16> %637) #8
  %725 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %714, <8 x i16> %696) #8
  %726 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %714, <8 x i16> %696) #8
  %727 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %641, <8 x i16> %639) #8
  %728 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %641, <8 x i16> %639) #8
  %729 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %660, <8 x i16> %678) #8
  %730 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %660, <8 x i16> %678) #8
  %731 = shufflevector <8 x i16> %730, <8 x i16> %718, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %732 = shufflevector <8 x i16> %718, <8 x i16> %730, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %733 = shufflevector <8 x i16> %730, <8 x i16> %718, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %734 = shufflevector <8 x i16> %718, <8 x i16> %730, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %735 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %731, <8 x i16> %46) #8
  %736 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %732, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %737 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %733, <8 x i16> %46) #8
  %738 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %734, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %739 = add <4 x i32> %735, <i32 2048, i32 2048, i32 2048, i32 2048>
  %740 = ashr <4 x i32> %739, <i32 12, i32 12, i32 12, i32 12>
  %741 = add <4 x i32> %736, <i32 2048, i32 2048, i32 2048, i32 2048>
  %742 = ashr <4 x i32> %741, <i32 12, i32 12, i32 12, i32 12>
  %743 = add <4 x i32> %737, <i32 2048, i32 2048, i32 2048, i32 2048>
  %744 = ashr <4 x i32> %743, <i32 12, i32 12, i32 12, i32 12>
  %745 = add <4 x i32> %738, <i32 2048, i32 2048, i32 2048, i32 2048>
  %746 = ashr <4 x i32> %745, <i32 12, i32 12, i32 12, i32 12>
  %747 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %740, <4 x i32> %744) #8
  %748 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %742, <4 x i32> %746) #8
  %749 = shufflevector <8 x i16> %728, <8 x i16> %716, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %750 = shufflevector <8 x i16> %716, <8 x i16> %728, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %751 = shufflevector <8 x i16> %728, <8 x i16> %716, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %752 = shufflevector <8 x i16> %716, <8 x i16> %728, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %753 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %749, <8 x i16> %46) #8
  %754 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %750, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %755 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %751, <8 x i16> %46) #8
  %756 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %752, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %757 = add <4 x i32> %753, <i32 2048, i32 2048, i32 2048, i32 2048>
  %758 = ashr <4 x i32> %757, <i32 12, i32 12, i32 12, i32 12>
  %759 = add <4 x i32> %754, <i32 2048, i32 2048, i32 2048, i32 2048>
  %760 = ashr <4 x i32> %759, <i32 12, i32 12, i32 12, i32 12>
  %761 = add <4 x i32> %755, <i32 2048, i32 2048, i32 2048, i32 2048>
  %762 = ashr <4 x i32> %761, <i32 12, i32 12, i32 12, i32 12>
  %763 = add <4 x i32> %756, <i32 2048, i32 2048, i32 2048, i32 2048>
  %764 = ashr <4 x i32> %763, <i32 12, i32 12, i32 12, i32 12>
  %765 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %758, <4 x i32> %762) #8
  %766 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %760, <4 x i32> %764) #8
  %767 = shufflevector <8 x i16> %724, <8 x i16> %720, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %768 = shufflevector <8 x i16> %720, <8 x i16> %724, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %769 = shufflevector <8 x i16> %724, <8 x i16> %720, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %770 = shufflevector <8 x i16> %720, <8 x i16> %724, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %771 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %767, <8 x i16> %53) #8
  %772 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %768, <8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>) #8
  %773 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %769, <8 x i16> %53) #8
  %774 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %770, <8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>) #8
  %775 = add <4 x i32> %771, <i32 2048, i32 2048, i32 2048, i32 2048>
  %776 = ashr <4 x i32> %775, <i32 12, i32 12, i32 12, i32 12>
  %777 = add <4 x i32> %772, <i32 2048, i32 2048, i32 2048, i32 2048>
  %778 = ashr <4 x i32> %777, <i32 12, i32 12, i32 12, i32 12>
  %779 = add <4 x i32> %773, <i32 2048, i32 2048, i32 2048, i32 2048>
  %780 = ashr <4 x i32> %779, <i32 12, i32 12, i32 12, i32 12>
  %781 = add <4 x i32> %774, <i32 2048, i32 2048, i32 2048, i32 2048>
  %782 = ashr <4 x i32> %781, <i32 12, i32 12, i32 12, i32 12>
  %783 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %776, <4 x i32> %780) #8
  %784 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %778, <4 x i32> %782) #8
  %785 = shufflevector <8 x i16> %726, <8 x i16> %722, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %786 = shufflevector <8 x i16> %722, <8 x i16> %726, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %787 = shufflevector <8 x i16> %726, <8 x i16> %722, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %788 = shufflevector <8 x i16> %722, <8 x i16> %726, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %789 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %785, <8 x i16> %53) #8
  %790 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %786, <8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>) #8
  %791 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %787, <8 x i16> %53) #8
  %792 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %788, <8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>) #8
  %793 = add <4 x i32> %789, <i32 2048, i32 2048, i32 2048, i32 2048>
  %794 = ashr <4 x i32> %793, <i32 12, i32 12, i32 12, i32 12>
  %795 = add <4 x i32> %790, <i32 2048, i32 2048, i32 2048, i32 2048>
  %796 = ashr <4 x i32> %795, <i32 12, i32 12, i32 12, i32 12>
  %797 = add <4 x i32> %791, <i32 2048, i32 2048, i32 2048, i32 2048>
  %798 = ashr <4 x i32> %797, <i32 12, i32 12, i32 12, i32 12>
  %799 = add <4 x i32> %792, <i32 2048, i32 2048, i32 2048, i32 2048>
  %800 = ashr <4 x i32> %799, <i32 12, i32 12, i32 12, i32 12>
  %801 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %794, <4 x i32> %798) #8
  %802 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %796, <4 x i32> %800) #8
  %803 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %715, <8 x i16> %719) #8
  %804 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %715, <8 x i16> %719) #8
  %805 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %717, <8 x i16> %721) #8
  %806 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %717, <8 x i16> %721) #8
  %807 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %747, <8 x i16> %801) #8
  %808 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %747, <8 x i16> %801) #8
  %809 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %765, <8 x i16> %783) #8
  %810 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %765, <8 x i16> %783) #8
  %811 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %727, <8 x i16> %723) #8
  %812 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %727, <8 x i16> %723) #8
  %813 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %729, <8 x i16> %725) #8
  %814 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %729, <8 x i16> %725) #8
  %815 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %748, <8 x i16> %802) #8
  %816 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %748, <8 x i16> %802) #8
  %817 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %766, <8 x i16> %784) #8
  %818 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %766, <8 x i16> %784) #8
  %819 = shufflevector <8 x i16> %818, <8 x i16> %810, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %820 = shufflevector <8 x i16> %810, <8 x i16> %818, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %821 = shufflevector <8 x i16> %818, <8 x i16> %810, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %822 = shufflevector <8 x i16> %810, <8 x i16> %818, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %823 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %819, <8 x i16> %45) #8
  %824 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %820, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %825 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %821, <8 x i16> %45) #8
  %826 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %822, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %827 = add <4 x i32> %823, <i32 2048, i32 2048, i32 2048, i32 2048>
  %828 = ashr <4 x i32> %827, <i32 12, i32 12, i32 12, i32 12>
  %829 = add <4 x i32> %824, <i32 2048, i32 2048, i32 2048, i32 2048>
  %830 = ashr <4 x i32> %829, <i32 12, i32 12, i32 12, i32 12>
  %831 = add <4 x i32> %825, <i32 2048, i32 2048, i32 2048, i32 2048>
  %832 = ashr <4 x i32> %831, <i32 12, i32 12, i32 12, i32 12>
  %833 = add <4 x i32> %826, <i32 2048, i32 2048, i32 2048, i32 2048>
  %834 = ashr <4 x i32> %833, <i32 12, i32 12, i32 12, i32 12>
  %835 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %828, <4 x i32> %832) #8
  %836 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %830, <4 x i32> %834) #8
  %837 = shufflevector <8 x i16> %816, <8 x i16> %808, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %838 = shufflevector <8 x i16> %808, <8 x i16> %816, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %839 = shufflevector <8 x i16> %816, <8 x i16> %808, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %840 = shufflevector <8 x i16> %808, <8 x i16> %816, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %841 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %837, <8 x i16> %45) #8
  %842 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %838, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %843 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %839, <8 x i16> %45) #8
  %844 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %840, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %845 = add <4 x i32> %841, <i32 2048, i32 2048, i32 2048, i32 2048>
  %846 = ashr <4 x i32> %845, <i32 12, i32 12, i32 12, i32 12>
  %847 = add <4 x i32> %842, <i32 2048, i32 2048, i32 2048, i32 2048>
  %848 = ashr <4 x i32> %847, <i32 12, i32 12, i32 12, i32 12>
  %849 = add <4 x i32> %843, <i32 2048, i32 2048, i32 2048, i32 2048>
  %850 = ashr <4 x i32> %849, <i32 12, i32 12, i32 12, i32 12>
  %851 = add <4 x i32> %844, <i32 2048, i32 2048, i32 2048, i32 2048>
  %852 = ashr <4 x i32> %851, <i32 12, i32 12, i32 12, i32 12>
  %853 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %846, <4 x i32> %850) #8
  %854 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %848, <4 x i32> %852) #8
  %855 = shufflevector <8 x i16> %814, <8 x i16> %806, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %856 = shufflevector <8 x i16> %806, <8 x i16> %814, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %857 = shufflevector <8 x i16> %814, <8 x i16> %806, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %858 = shufflevector <8 x i16> %806, <8 x i16> %814, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %859 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %855, <8 x i16> %45) #8
  %860 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %856, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %861 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %857, <8 x i16> %45) #8
  %862 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %858, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %863 = add <4 x i32> %859, <i32 2048, i32 2048, i32 2048, i32 2048>
  %864 = ashr <4 x i32> %863, <i32 12, i32 12, i32 12, i32 12>
  %865 = add <4 x i32> %860, <i32 2048, i32 2048, i32 2048, i32 2048>
  %866 = ashr <4 x i32> %865, <i32 12, i32 12, i32 12, i32 12>
  %867 = add <4 x i32> %861, <i32 2048, i32 2048, i32 2048, i32 2048>
  %868 = ashr <4 x i32> %867, <i32 12, i32 12, i32 12, i32 12>
  %869 = add <4 x i32> %862, <i32 2048, i32 2048, i32 2048, i32 2048>
  %870 = ashr <4 x i32> %869, <i32 12, i32 12, i32 12, i32 12>
  %871 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %864, <4 x i32> %868) #8
  %872 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %866, <4 x i32> %870) #8
  %873 = shufflevector <8 x i16> %812, <8 x i16> %804, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %874 = shufflevector <8 x i16> %804, <8 x i16> %812, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %875 = shufflevector <8 x i16> %812, <8 x i16> %804, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %876 = shufflevector <8 x i16> %804, <8 x i16> %812, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %877 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %873, <8 x i16> %45) #8
  %878 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %874, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %879 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %875, <8 x i16> %45) #8
  %880 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %876, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %881 = add <4 x i32> %877, <i32 2048, i32 2048, i32 2048, i32 2048>
  %882 = ashr <4 x i32> %881, <i32 12, i32 12, i32 12, i32 12>
  %883 = add <4 x i32> %878, <i32 2048, i32 2048, i32 2048, i32 2048>
  %884 = ashr <4 x i32> %883, <i32 12, i32 12, i32 12, i32 12>
  %885 = add <4 x i32> %879, <i32 2048, i32 2048, i32 2048, i32 2048>
  %886 = ashr <4 x i32> %885, <i32 12, i32 12, i32 12, i32 12>
  %887 = add <4 x i32> %880, <i32 2048, i32 2048, i32 2048, i32 2048>
  %888 = ashr <4 x i32> %887, <i32 12, i32 12, i32 12, i32 12>
  %889 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %882, <4 x i32> %886) #8
  %890 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %884, <4 x i32> %888) #8
  %891 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %467, <8 x i16> %811) #8
  %892 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %467, <8 x i16> %811) #8
  %893 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %469, <8 x i16> %813) #8
  %894 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %469, <8 x i16> %813) #8
  %895 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %471, <8 x i16> %815) #8
  %896 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %471, <8 x i16> %815) #8
  %897 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %473, <8 x i16> %817) #8
  %898 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %473, <8 x i16> %817) #8
  %899 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %475, <8 x i16> %836) #8
  %900 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %475, <8 x i16> %836) #8
  %901 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %477, <8 x i16> %854) #8
  %902 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %477, <8 x i16> %854) #8
  %903 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %479, <8 x i16> %872) #8
  %904 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %479, <8 x i16> %872) #8
  %905 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %481, <8 x i16> %890) #8
  %906 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %481, <8 x i16> %890) #8
  %907 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %482, <8 x i16> %889) #8
  %908 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %482, <8 x i16> %889) #8
  %909 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %480, <8 x i16> %871) #8
  %910 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %480, <8 x i16> %871) #8
  %911 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %478, <8 x i16> %853) #8
  %912 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %478, <8 x i16> %853) #8
  %913 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %476, <8 x i16> %835) #8
  %914 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %476, <8 x i16> %835) #8
  %915 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %474, <8 x i16> %809) #8
  %916 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %474, <8 x i16> %809) #8
  %917 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %472, <8 x i16> %807) #8
  %918 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %472, <8 x i16> %807) #8
  %919 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %470, <8 x i16> %805) #8
  %920 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %470, <8 x i16> %805) #8
  %921 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %468, <8 x i16> %803) #8
  %922 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %468, <8 x i16> %803) #8
  store <8 x i16> %891, <8 x i16>* %106, align 1
  store <8 x i16> %893, <8 x i16>* %109, align 1
  store <8 x i16> %895, <8 x i16>* %112, align 1
  store <8 x i16> %897, <8 x i16>* %115, align 1
  store <8 x i16> %899, <8 x i16>* %118, align 1
  store <8 x i16> %901, <8 x i16>* %121, align 1
  store <8 x i16> %903, <8 x i16>* %124, align 1
  store <8 x i16> %905, <8 x i16>* %127, align 1
  store <8 x i16> %907, <8 x i16>* %130, align 1
  store <8 x i16> %909, <8 x i16>* %133, align 1
  store <8 x i16> %911, <8 x i16>* %136, align 1
  store <8 x i16> %913, <8 x i16>* %139, align 1
  store <8 x i16> %915, <8 x i16>* %142, align 1
  store <8 x i16> %917, <8 x i16>* %145, align 1
  store <8 x i16> %919, <8 x i16>* %148, align 1
  store <8 x i16> %921, <8 x i16>* %151, align 1
  store <8 x i16> %922, <8 x i16>* %154, align 1
  store <8 x i16> %920, <8 x i16>* %157, align 1
  store <8 x i16> %918, <8 x i16>* %160, align 1
  store <8 x i16> %916, <8 x i16>* %163, align 1
  store <8 x i16> %914, <8 x i16>* %166, align 1
  store <8 x i16> %912, <8 x i16>* %169, align 1
  store <8 x i16> %910, <8 x i16>* %172, align 1
  store <8 x i16> %908, <8 x i16>* %175, align 1
  store <8 x i16> %906, <8 x i16>* %178, align 1
  store <8 x i16> %904, <8 x i16>* %181, align 1
  store <8 x i16> %902, <8 x i16>* %184, align 1
  store <8 x i16> %900, <8 x i16>* %187, align 1
  store <8 x i16> %898, <8 x i16>* %190, align 1
  store <8 x i16> %896, <8 x i16>* %193, align 1
  store <8 x i16> %894, <8 x i16>* %196, align 1
  store <8 x i16> %892, <8 x i16>* %199, align 1
  %923 = add nuw nsw i64 %104, 8
  %924 = icmp ult i64 %923, %64
  br i1 %924, label %103, label %925

925:                                              ; preds = %88, %103
  %926 = bitcast i8* %6 to i64*
  %927 = load i64, i64* %926, align 8
  %928 = getelementptr inbounds i8, i8* %6, i64 8
  %929 = bitcast i8* %928 to i8**
  %930 = load i8*, i8** %929, align 8
  %931 = sext i32 %5 to i64
  %932 = ashr i64 %927, 32
  %933 = mul nsw i64 %932, %931
  %934 = getelementptr inbounds i8, i8* %930, i64 %933
  %935 = sext i32 %4 to i64
  %936 = getelementptr inbounds i8, i8* %934, i64 %935
  switch i8 %11, label %937 [
    i8 4, label %939
    i8 8, label %963
  ]

937:                                              ; preds = %925
  %938 = zext i8 %11 to i64
  br label %985

939:                                              ; preds = %925, %939
  %940 = phi i64 [ %961, %939 ], [ 0, %925 ]
  %941 = phi i8* [ %960, %939 ], [ %936, %925 ]
  %942 = shl i64 %940, 2
  %943 = getelementptr inbounds i16, i16* %8, i64 %942
  %944 = bitcast i16* %943 to i64*
  %945 = load i64, i64* %944, align 1
  %946 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %945, i32 0
  %947 = bitcast i8* %941 to i32*
  %948 = load i32, i32* %947, align 1
  %949 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %948, i32 0
  %950 = bitcast <2 x i64> %946 to <8 x i16>
  %951 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %950, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %952 = ashr <8 x i16> %951, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %953 = bitcast <4 x i32> %949 to <16 x i8>
  %954 = shufflevector <16 x i8> %953, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %955 = zext <8 x i8> %954 to <8 x i16>
  %956 = add nsw <8 x i16> %952, %955
  %957 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %956, <8 x i16> undef) #8
  %958 = bitcast <16 x i8> %957 to <4 x i32>
  %959 = extractelement <4 x i32> %958, i32 0
  store i32 %959, i32* %947, align 1
  %960 = getelementptr inbounds i8, i8* %941, i64 %932
  %961 = add nuw nsw i64 %940, 1
  %962 = icmp eq i64 %961, 32
  br i1 %962, label %1021, label %939

963:                                              ; preds = %925, %963
  %964 = phi i64 [ %983, %963 ], [ 0, %925 ]
  %965 = phi i8* [ %982, %963 ], [ %936, %925 ]
  %966 = shl i64 %964, 3
  %967 = getelementptr inbounds i16, i16* %8, i64 %966
  %968 = bitcast i16* %967 to <8 x i16>*
  %969 = load <8 x i16>, <8 x i16>* %968, align 1
  %970 = bitcast i8* %965 to i64*
  %971 = load i64, i64* %970, align 1
  %972 = insertelement <2 x i64> undef, i64 %971, i32 0
  %973 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %969, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %974 = ashr <8 x i16> %973, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %975 = bitcast <2 x i64> %972 to <16 x i8>
  %976 = shufflevector <16 x i8> %975, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %977 = zext <8 x i8> %976 to <8 x i16>
  %978 = add nsw <8 x i16> %974, %977
  %979 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %978, <8 x i16> undef) #8
  %980 = bitcast <16 x i8> %979 to <2 x i64>
  %981 = extractelement <2 x i64> %980, i32 0
  store i64 %981, i64* %970, align 1
  %982 = getelementptr inbounds i8, i8* %965, i64 %932
  %983 = add nuw nsw i64 %964, 1
  %984 = icmp eq i64 %983, 32
  br i1 %984, label %1021, label %963

985:                                              ; preds = %1018, %937
  %986 = phi i64 [ 0, %937 ], [ %1019, %1018 ]
  %987 = add nsw i64 %986, %931
  %988 = mul nuw nsw i64 %986, %938
  %989 = mul nsw i64 %987, %932
  %990 = getelementptr inbounds i8, i8* %930, i64 %989
  br label %991

991:                                              ; preds = %991, %985
  %992 = phi i64 [ %1016, %991 ], [ 0, %985 ]
  %993 = add nsw i64 %992, %935
  %994 = add nuw nsw i64 %992, %988
  %995 = getelementptr inbounds i16, i16* %8, i64 %994
  %996 = bitcast i16* %995 to <8 x i16>*
  %997 = load <8 x i16>, <8 x i16>* %996, align 1
  %998 = add nuw nsw i64 %994, 8
  %999 = getelementptr inbounds i16, i16* %8, i64 %998
  %1000 = bitcast i16* %999 to <8 x i16>*
  %1001 = load <8 x i16>, <8 x i16>* %1000, align 1
  %1002 = getelementptr inbounds i8, i8* %990, i64 %993
  %1003 = bitcast i8* %1002 to <16 x i8>*
  %1004 = load <16 x i8>, <16 x i8>* %1003, align 1
  %1005 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %997, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %1006 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1001, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %1007 = ashr <8 x i16> %1005, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %1008 = ashr <8 x i16> %1006, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %1009 = shufflevector <16 x i8> %1004, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %1010 = zext <8 x i8> %1009 to <8 x i16>
  %1011 = shufflevector <16 x i8> %1004, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %1012 = zext <8 x i8> %1011 to <8 x i16>
  %1013 = add nsw <8 x i16> %1007, %1010
  %1014 = add nsw <8 x i16> %1008, %1012
  %1015 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %1013, <8 x i16> %1014) #8
  store <16 x i8> %1015, <16 x i8>* %1003, align 1
  %1016 = add nuw nsw i64 %992, 16
  %1017 = icmp ult i64 %1016, %938
  br i1 %1017, label %991, label %1018

1018:                                             ; preds = %991
  %1019 = add nuw nsw i64 %986, 1
  %1020 = icmp eq i64 %1019, 32
  br i1 %1020, label %1021, label %985

1021:                                             ; preds = %963, %939, %1018
  ret void

1022:                                             ; preds = %88
  %1023 = mul nuw nsw i64 %101, %86
  %1024 = getelementptr inbounds i16, i16* %8, i64 %1023
  %1025 = bitcast i16* %1024 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 2 %1025, i8* align 2 %3, i64 %87, i1 false) #8
  %1026 = add nuw nsw i64 %89, 4
  br label %88
}

; Function Attrs: nounwind ssp uwtable
define internal void @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_128Dct64TransformLoopRow_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_(i8 zeroext, i8 zeroext, i32, i8*, i32, i32, i8* nocapture readnone) #4 {
  %8 = bitcast i8* %3 to i16*
  %9 = zext i8 %1 to i64
  %10 = lshr i64 173354, %9
  %11 = and i64 %10, 1
  %12 = icmp ne i64 %11, 0
  %13 = getelementptr inbounds [19 x i8], [19 x i8]* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_118kTransformRowShiftE, i64 0, i64 %9
  %14 = load i8, i8* %13, align 1
  %15 = icmp sgt i32 %2, 1
  br i1 %15, label %68, label %16

16:                                               ; preds = %7
  %17 = zext i8 %14 to i32
  %18 = load i16, i16* %8, align 2
  %19 = sext i16 %18 to i32
  %20 = insertelement <4 x i32> undef, i32 %19, i32 0
  %21 = bitcast <4 x i32> %20 to <8 x i16>
  %22 = shufflevector <8 x i16> %21, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %23 = bitcast <8 x i16> %22 to <4 x i32>
  %24 = shufflevector <4 x i32> %23, <4 x i32> undef, <4 x i32> zeroinitializer
  %25 = sext i1 %12 to i16
  %26 = insertelement <8 x i16> undef, i16 %25, i32 0
  %27 = shufflevector <8 x i16> %26, <8 x i16> undef, <8 x i32> zeroinitializer
  %28 = bitcast <4 x i32> %24 to <8 x i16>
  %29 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %28, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  %30 = bitcast <4 x i32> %24 to <16 x i8>
  %31 = bitcast <8 x i16> %29 to <16 x i8>
  %32 = bitcast <8 x i16> %27 to <16 x i8>
  %33 = tail call <16 x i8> @llvm.x86.sse41.pblendvb(<16 x i8> %30, <16 x i8> %31, <16 x i8> %32) #8
  %34 = bitcast <16 x i8> %33 to <8 x i16>
  %35 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %34, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  %36 = insertelement <4 x i32> undef, i32 %17, i32 0
  %37 = shufflevector <4 x i32> %36, <4 x i32> undef, <4 x i32> zeroinitializer
  %38 = shufflevector <4 x i32> %36, <4 x i32> undef, <2 x i32> zeroinitializer
  %39 = zext <2 x i32> %38 to <2 x i64>
  %40 = shufflevector <8 x i16> %35, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %41 = sext <4 x i16> %40 to <4 x i32>
  %42 = bitcast <8 x i16> %35 to <16 x i8>
  %43 = shufflevector <16 x i8> %42, <16 x i8> undef, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %44 = bitcast <16 x i8> %43 to <8 x i16>
  %45 = shufflevector <8 x i16> %44, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %46 = sext <4 x i16> %45 to <4 x i32>
  %47 = add <4 x i32> %37, %41
  %48 = add <4 x i32> %37, %46
  %49 = bitcast <2 x i64> %39 to <4 x i32>
  %50 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %47, <4 x i32> %49) #8
  %51 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %48, <4 x i32> %49) #8
  %52 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %50, <4 x i32> %51) #8
  %53 = bitcast i8* %3 to <8 x i16>*
  store <8 x i16> %52, <8 x i16>* %53, align 1
  %54 = getelementptr inbounds i8, i8* %3, i64 16
  %55 = bitcast i8* %54 to <8 x i16>*
  store <8 x i16> %52, <8 x i16>* %55, align 1
  %56 = getelementptr inbounds i8, i8* %3, i64 32
  %57 = bitcast i8* %56 to <8 x i16>*
  store <8 x i16> %52, <8 x i16>* %57, align 1
  %58 = getelementptr inbounds i8, i8* %3, i64 48
  %59 = bitcast i8* %58 to <8 x i16>*
  store <8 x i16> %52, <8 x i16>* %59, align 1
  %60 = getelementptr inbounds i8, i8* %3, i64 64
  %61 = bitcast i8* %60 to <8 x i16>*
  store <8 x i16> %52, <8 x i16>* %61, align 1
  %62 = getelementptr inbounds i8, i8* %3, i64 80
  %63 = bitcast i8* %62 to <8 x i16>*
  store <8 x i16> %52, <8 x i16>* %63, align 1
  %64 = getelementptr inbounds i8, i8* %3, i64 96
  %65 = bitcast i8* %64 to <8 x i16>*
  store <8 x i16> %52, <8 x i16>* %65, align 1
  %66 = getelementptr inbounds i8, i8* %3, i64 112
  %67 = bitcast i8* %66 to <8 x i16>*
  store <8 x i16> %52, <8 x i16>* %67, align 1
  br label %191

68:                                               ; preds = %7
  br i1 %12, label %69, label %96

69:                                               ; preds = %68
  %70 = sext i32 %2 to i64
  br label %71

71:                                               ; preds = %71, %69
  %72 = phi i64 [ %94, %71 ], [ 0, %69 ]
  %73 = shl i64 %72, 6
  %74 = and i64 %73, 4294967232
  %75 = getelementptr inbounds i16, i16* %8, i64 %74
  %76 = bitcast i16* %75 to <8 x i16>*
  %77 = load <8 x i16>, <8 x i16>* %76, align 1
  %78 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %77, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %78, <8 x i16>* %76, align 1
  %79 = or i64 %74, 8
  %80 = getelementptr inbounds i16, i16* %8, i64 %79
  %81 = bitcast i16* %80 to <8 x i16>*
  %82 = load <8 x i16>, <8 x i16>* %81, align 1
  %83 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %82, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %83, <8 x i16>* %81, align 1
  %84 = or i64 %74, 16
  %85 = getelementptr inbounds i16, i16* %8, i64 %84
  %86 = bitcast i16* %85 to <8 x i16>*
  %87 = load <8 x i16>, <8 x i16>* %86, align 1
  %88 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %87, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %88, <8 x i16>* %86, align 1
  %89 = or i64 %74, 24
  %90 = getelementptr inbounds i16, i16* %8, i64 %89
  %91 = bitcast i16* %90 to <8 x i16>*
  %92 = load <8 x i16>, <8 x i16>* %91, align 1
  %93 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %92, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %93, <8 x i16>* %91, align 1
  %94 = add nuw nsw i64 %72, 1
  %95 = icmp eq i64 %94, %70
  br i1 %95, label %96, label %71

96:                                               ; preds = %71, %68
  br label %97

97:                                               ; preds = %96, %97
  %98 = phi i64 [ %103, %97 ], [ 0, %96 ]
  %99 = shl i64 %98, 6
  %100 = and i64 %99, 4294966784
  %101 = getelementptr inbounds i16, i16* %8, i64 %100
  %102 = bitcast i16* %101 to i8*
  tail call fastcc void @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_112Dct64_SSE4_1EPvib(i8* %102, i32 64, i1 zeroext true)
  %103 = add nuw i64 %98, 8
  %104 = trunc i64 %103 to i32
  %105 = icmp slt i32 %104, %2
  br i1 %105, label %97, label %106

106:                                              ; preds = %97
  %107 = zext i8 %14 to i16
  %108 = insertelement <8 x i16> undef, i16 %107, i32 0
  %109 = shufflevector <8 x i16> %108, <8 x i16> undef, <8 x i32> zeroinitializer
  %110 = shufflevector <8 x i16> %108, <8 x i16> undef, <2 x i32> zeroinitializer
  %111 = zext <2 x i16> %110 to <2 x i64>
  %112 = bitcast <2 x i64> %111 to <8 x i16>
  %113 = sext i32 %2 to i64
  br label %114

114:                                              ; preds = %114, %106
  %115 = phi i64 [ %189, %114 ], [ 0, %106 ]
  %116 = shl i64 %115, 6
  %117 = and i64 %116, 4294967232
  %118 = getelementptr inbounds i16, i16* %8, i64 %117
  %119 = bitcast i16* %118 to <8 x i16>*
  %120 = load <8 x i16>, <8 x i16>* %119, align 1
  %121 = icmp sgt <8 x i16> %120, <i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765>
  %122 = add <8 x i16> %120, %109
  %123 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %122, <8 x i16> %112) #8
  %124 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %122, <8 x i16> %112) #8
  %125 = select <8 x i1> %121, <8 x i16> %124, <8 x i16> %123
  store <8 x i16> %125, <8 x i16>* %119, align 1
  %126 = or i64 %117, 8
  %127 = getelementptr inbounds i16, i16* %8, i64 %126
  %128 = bitcast i16* %127 to <8 x i16>*
  %129 = load <8 x i16>, <8 x i16>* %128, align 1
  %130 = icmp sgt <8 x i16> %129, <i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765>
  %131 = add <8 x i16> %129, %109
  %132 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %131, <8 x i16> %112) #8
  %133 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %131, <8 x i16> %112) #8
  %134 = select <8 x i1> %130, <8 x i16> %133, <8 x i16> %132
  store <8 x i16> %134, <8 x i16>* %128, align 1
  %135 = or i64 %117, 16
  %136 = getelementptr inbounds i16, i16* %8, i64 %135
  %137 = bitcast i16* %136 to <8 x i16>*
  %138 = load <8 x i16>, <8 x i16>* %137, align 1
  %139 = icmp sgt <8 x i16> %138, <i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765>
  %140 = add <8 x i16> %138, %109
  %141 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %140, <8 x i16> %112) #8
  %142 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %140, <8 x i16> %112) #8
  %143 = select <8 x i1> %139, <8 x i16> %142, <8 x i16> %141
  store <8 x i16> %143, <8 x i16>* %137, align 1
  %144 = or i64 %117, 24
  %145 = getelementptr inbounds i16, i16* %8, i64 %144
  %146 = bitcast i16* %145 to <8 x i16>*
  %147 = load <8 x i16>, <8 x i16>* %146, align 1
  %148 = icmp sgt <8 x i16> %147, <i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765>
  %149 = add <8 x i16> %147, %109
  %150 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %149, <8 x i16> %112) #8
  %151 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %149, <8 x i16> %112) #8
  %152 = select <8 x i1> %148, <8 x i16> %151, <8 x i16> %150
  store <8 x i16> %152, <8 x i16>* %146, align 1
  %153 = or i64 %117, 32
  %154 = getelementptr inbounds i16, i16* %8, i64 %153
  %155 = bitcast i16* %154 to <8 x i16>*
  %156 = load <8 x i16>, <8 x i16>* %155, align 1
  %157 = icmp sgt <8 x i16> %156, <i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765>
  %158 = add <8 x i16> %156, %109
  %159 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %158, <8 x i16> %112) #8
  %160 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %158, <8 x i16> %112) #8
  %161 = select <8 x i1> %157, <8 x i16> %160, <8 x i16> %159
  store <8 x i16> %161, <8 x i16>* %155, align 1
  %162 = or i64 %117, 40
  %163 = getelementptr inbounds i16, i16* %8, i64 %162
  %164 = bitcast i16* %163 to <8 x i16>*
  %165 = load <8 x i16>, <8 x i16>* %164, align 1
  %166 = icmp sgt <8 x i16> %165, <i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765>
  %167 = add <8 x i16> %165, %109
  %168 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %167, <8 x i16> %112) #8
  %169 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %167, <8 x i16> %112) #8
  %170 = select <8 x i1> %166, <8 x i16> %169, <8 x i16> %168
  store <8 x i16> %170, <8 x i16>* %164, align 1
  %171 = or i64 %117, 48
  %172 = getelementptr inbounds i16, i16* %8, i64 %171
  %173 = bitcast i16* %172 to <8 x i16>*
  %174 = load <8 x i16>, <8 x i16>* %173, align 1
  %175 = icmp sgt <8 x i16> %174, <i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765>
  %176 = add <8 x i16> %174, %109
  %177 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %176, <8 x i16> %112) #8
  %178 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %176, <8 x i16> %112) #8
  %179 = select <8 x i1> %175, <8 x i16> %178, <8 x i16> %177
  store <8 x i16> %179, <8 x i16>* %173, align 1
  %180 = or i64 %117, 56
  %181 = getelementptr inbounds i16, i16* %8, i64 %180
  %182 = bitcast i16* %181 to <8 x i16>*
  %183 = load <8 x i16>, <8 x i16>* %182, align 1
  %184 = icmp sgt <8 x i16> %183, <i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765>
  %185 = add <8 x i16> %183, %109
  %186 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %185, <8 x i16> %112) #8
  %187 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %185, <8 x i16> %112) #8
  %188 = select <8 x i1> %184, <8 x i16> %187, <8 x i16> %186
  store <8 x i16> %188, <8 x i16>* %182, align 1
  %189 = add nuw nsw i64 %115, 1
  %190 = icmp slt i64 %189, %113
  br i1 %190, label %114, label %191

191:                                              ; preds = %114, %16
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_131Dct64TransformLoopColumn_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_(i8 zeroext, i8 zeroext, i32, i8*, i32, i32, i8* nocapture readonly) #4 {
  %8 = bitcast i8* %3 to i16*
  %9 = zext i8 %1 to i64
  %10 = getelementptr inbounds [19 x i8], [19 x i8]* @_ZN7libgav115kTransformWidthE, i64 0, i64 %9
  %11 = load i8, i8* %10, align 1
  %12 = zext i8 %11 to i32
  %13 = icmp sgt i32 %2, 1
  br i1 %13, label %14, label %16

14:                                               ; preds = %7
  %15 = zext i8 %11 to i64
  br label %54

16:                                               ; preds = %7
  %17 = icmp eq i8 %11, 4
  br i1 %17, label %20, label %18

18:                                               ; preds = %16
  %19 = zext i8 %11 to i64
  br label %28

20:                                               ; preds = %16
  %21 = bitcast i8* %3 to i64*
  %22 = load i64, i64* %21, align 1
  %23 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %22, i32 0
  %24 = bitcast <2 x i64> %23 to <8 x i16>
  %25 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %24, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  %26 = bitcast <8 x i16> %25 to <2 x i64>
  %27 = extractelement <2 x i64> %26, i32 0
  store i64 %27, i64* %21, align 1
  br label %36

28:                                               ; preds = %28, %18
  %29 = phi i64 [ 0, %18 ], [ %34, %28 ]
  %30 = getelementptr inbounds i16, i16* %8, i64 %29
  %31 = bitcast i16* %30 to <8 x i16>*
  %32 = load <8 x i16>, <8 x i16>* %31, align 1
  %33 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %32, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %33, <8 x i16>* %31, align 1
  %34 = add nuw nsw i64 %29, 8
  %35 = icmp ult i64 %34, %19
  br i1 %35, label %28, label %36

36:                                               ; preds = %28, %20
  %37 = phi i64 [ 4, %20 ], [ %19, %28 ]
  %38 = shl nuw nsw i64 %37, 1
  br label %39

39:                                               ; preds = %39, %36
  %40 = phi i64 [ 1, %36 ], [ %52, %39 ]
  %41 = mul nuw nsw i64 %40, %37
  %42 = getelementptr inbounds i16, i16* %8, i64 %41
  %43 = bitcast i16* %42 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 2 %43, i8* align 2 %3, i64 %38, i1 false) #8
  %44 = add nuw nsw i64 %40, 1
  %45 = mul nuw nsw i64 %44, %37
  %46 = getelementptr inbounds i16, i16* %8, i64 %45
  %47 = bitcast i16* %46 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 2 %47, i8* align 2 %3, i64 %38, i1 false) #8
  %48 = add nuw nsw i64 %40, 2
  %49 = mul nuw nsw i64 %48, %37
  %50 = getelementptr inbounds i16, i16* %8, i64 %49
  %51 = bitcast i16* %50 to i8*
  tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 2 %51, i8* align 2 %3, i64 %38, i1 false) #8
  %52 = add nuw nsw i64 %40, 3
  %53 = icmp eq i64 %52, 64
  br i1 %53, label %60, label %39

54:                                               ; preds = %14, %54
  %55 = phi i64 [ 0, %14 ], [ %58, %54 ]
  %56 = getelementptr inbounds i16, i16* %8, i64 %55
  %57 = bitcast i16* %56 to i8*
  tail call fastcc void @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_112Dct64_SSE4_1EPvib(i8* %57, i32 %12, i1 zeroext false)
  %58 = add nuw nsw i64 %55, 8
  %59 = icmp ult i64 %58, %15
  br i1 %59, label %54, label %60

60:                                               ; preds = %39, %54
  %61 = bitcast i8* %6 to i64*
  %62 = load i64, i64* %61, align 8
  %63 = getelementptr inbounds i8, i8* %6, i64 8
  %64 = bitcast i8* %63 to i8**
  %65 = load i8*, i8** %64, align 8
  %66 = sext i32 %5 to i64
  %67 = ashr i64 %62, 32
  %68 = mul nsw i64 %67, %66
  %69 = getelementptr inbounds i8, i8* %65, i64 %68
  %70 = sext i32 %4 to i64
  %71 = getelementptr inbounds i8, i8* %69, i64 %70
  switch i8 %11, label %72 [
    i8 4, label %74
    i8 8, label %98
  ]

72:                                               ; preds = %60
  %73 = zext i8 %11 to i64
  br label %120

74:                                               ; preds = %60, %74
  %75 = phi i64 [ %96, %74 ], [ 0, %60 ]
  %76 = phi i8* [ %95, %74 ], [ %71, %60 ]
  %77 = shl i64 %75, 2
  %78 = getelementptr inbounds i16, i16* %8, i64 %77
  %79 = bitcast i16* %78 to i64*
  %80 = load i64, i64* %79, align 1
  %81 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %80, i32 0
  %82 = bitcast i8* %76 to i32*
  %83 = load i32, i32* %82, align 1
  %84 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %83, i32 0
  %85 = bitcast <2 x i64> %81 to <8 x i16>
  %86 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %85, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %87 = ashr <8 x i16> %86, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %88 = bitcast <4 x i32> %84 to <16 x i8>
  %89 = shufflevector <16 x i8> %88, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %90 = zext <8 x i8> %89 to <8 x i16>
  %91 = add nsw <8 x i16> %87, %90
  %92 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %91, <8 x i16> undef) #8
  %93 = bitcast <16 x i8> %92 to <4 x i32>
  %94 = extractelement <4 x i32> %93, i32 0
  store i32 %94, i32* %82, align 1
  %95 = getelementptr inbounds i8, i8* %76, i64 %67
  %96 = add nuw nsw i64 %75, 1
  %97 = icmp eq i64 %96, 64
  br i1 %97, label %156, label %74

98:                                               ; preds = %60, %98
  %99 = phi i64 [ %118, %98 ], [ 0, %60 ]
  %100 = phi i8* [ %117, %98 ], [ %71, %60 ]
  %101 = shl i64 %99, 3
  %102 = getelementptr inbounds i16, i16* %8, i64 %101
  %103 = bitcast i16* %102 to <8 x i16>*
  %104 = load <8 x i16>, <8 x i16>* %103, align 1
  %105 = bitcast i8* %100 to i64*
  %106 = load i64, i64* %105, align 1
  %107 = insertelement <2 x i64> undef, i64 %106, i32 0
  %108 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %104, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %109 = ashr <8 x i16> %108, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %110 = bitcast <2 x i64> %107 to <16 x i8>
  %111 = shufflevector <16 x i8> %110, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %112 = zext <8 x i8> %111 to <8 x i16>
  %113 = add nsw <8 x i16> %109, %112
  %114 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %113, <8 x i16> undef) #8
  %115 = bitcast <16 x i8> %114 to <2 x i64>
  %116 = extractelement <2 x i64> %115, i32 0
  store i64 %116, i64* %105, align 1
  %117 = getelementptr inbounds i8, i8* %100, i64 %67
  %118 = add nuw nsw i64 %99, 1
  %119 = icmp eq i64 %118, 64
  br i1 %119, label %156, label %98

120:                                              ; preds = %153, %72
  %121 = phi i64 [ 0, %72 ], [ %154, %153 ]
  %122 = add nsw i64 %121, %66
  %123 = mul nuw nsw i64 %121, %73
  %124 = mul nsw i64 %122, %67
  %125 = getelementptr inbounds i8, i8* %65, i64 %124
  br label %126

126:                                              ; preds = %126, %120
  %127 = phi i64 [ %151, %126 ], [ 0, %120 ]
  %128 = add nsw i64 %127, %70
  %129 = add nuw nsw i64 %127, %123
  %130 = getelementptr inbounds i16, i16* %8, i64 %129
  %131 = bitcast i16* %130 to <8 x i16>*
  %132 = load <8 x i16>, <8 x i16>* %131, align 1
  %133 = add nuw nsw i64 %129, 8
  %134 = getelementptr inbounds i16, i16* %8, i64 %133
  %135 = bitcast i16* %134 to <8 x i16>*
  %136 = load <8 x i16>, <8 x i16>* %135, align 1
  %137 = getelementptr inbounds i8, i8* %125, i64 %128
  %138 = bitcast i8* %137 to <16 x i8>*
  %139 = load <16 x i8>, <16 x i8>* %138, align 1
  %140 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %132, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %141 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %136, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %142 = ashr <8 x i16> %140, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %143 = ashr <8 x i16> %141, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %144 = shufflevector <16 x i8> %139, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %145 = zext <8 x i8> %144 to <8 x i16>
  %146 = shufflevector <16 x i8> %139, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %147 = zext <8 x i8> %146 to <8 x i16>
  %148 = add nsw <8 x i16> %142, %145
  %149 = add nsw <8 x i16> %143, %147
  %150 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %148, <8 x i16> %149) #8
  store <16 x i8> %150, <16 x i8>* %138, align 1
  %151 = add nuw nsw i64 %127, 16
  %152 = icmp ult i64 %151, %73
  br i1 %152, label %126, label %153

153:                                              ; preds = %126
  %154 = add nuw nsw i64 %121, 1
  %155 = icmp eq i64 %154, 64
  br i1 %155, label %156, label %120

156:                                              ; preds = %98, %74, %153
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define internal void @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_128Adst4TransformLoopRow_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_(i8 zeroext, i8 zeroext, i32, i8* nocapture, i32, i32, i8* nocapture readnone) #3 {
  %8 = bitcast i8* %3 to i16*
  %9 = zext i8 %1 to i64
  %10 = getelementptr inbounds [19 x i8], [19 x i8]* @_ZN7libgav116kTransformHeightE, i64 0, i64 %9
  %11 = load i8, i8* %10, align 1
  %12 = icmp eq i8 %11, 16
  %13 = icmp eq i8 %11, 8
  %14 = icmp sgt i32 %2, 1
  br i1 %14, label %48, label %15

15:                                               ; preds = %7
  %16 = zext i1 %12 to i32
  %17 = load i16, i16* %8, align 2
  %18 = sext i16 %17 to i32
  %19 = insertelement <4 x i32> undef, i32 %18, i32 0
  %20 = bitcast <4 x i32> %19 to <8 x i16>
  %21 = shufflevector <8 x i16> %20, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %22 = bitcast <8 x i16> %21 to <4 x i32>
  %23 = shufflevector <4 x i32> %22, <4 x i32> undef, <4 x i32> zeroinitializer
  %24 = sext i1 %13 to i16
  %25 = insertelement <8 x i16> undef, i16 %24, i32 0
  %26 = shufflevector <8 x i16> %25, <8 x i16> undef, <8 x i32> zeroinitializer
  %27 = bitcast <4 x i32> %23 to <8 x i16>
  %28 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %27, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  %29 = bitcast <4 x i32> %23 to <16 x i8>
  %30 = bitcast <8 x i16> %28 to <16 x i8>
  %31 = bitcast <8 x i16> %26 to <16 x i8>
  %32 = tail call <16 x i8> @llvm.x86.sse41.pblendvb(<16 x i8> %29, <16 x i8> %30, <16 x i8> %31) #8
  %33 = bitcast <16 x i8> %32 to <8 x i16>
  %34 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %33, <8 x i16> <i16 1321, i16 0, i16 2482, i16 0, i16 3344, i16 0, i16 2482, i16 1321>) #8
  %35 = add <4 x i32> %34, <i32 2048, i32 2048, i32 2048, i32 2048>
  %36 = ashr <4 x i32> %35, <i32 12, i32 12, i32 12, i32 12>
  %37 = insertelement <4 x i32> undef, i32 %16, i32 0
  %38 = shufflevector <4 x i32> %37, <4 x i32> undef, <4 x i32> zeroinitializer
  %39 = shufflevector <4 x i32> %37, <4 x i32> undef, <2 x i32> zeroinitializer
  %40 = zext <2 x i32> %39 to <2 x i64>
  %41 = add <4 x i32> %36, %38
  %42 = bitcast <2 x i64> %40 to <4 x i32>
  %43 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %41, <4 x i32> %42) #8
  %44 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %43, <4 x i32> undef) #8
  %45 = bitcast <8 x i16> %44 to <2 x i64>
  %46 = extractelement <2 x i64> %45, i32 0
  %47 = bitcast i8* %3 to i64*
  store i64 %46, i64* %47, align 1
  br label %176

48:                                               ; preds = %7
  br i1 %13, label %49, label %60

49:                                               ; preds = %48
  %50 = shl nsw i32 %2, 2
  %51 = sext i32 %50 to i64
  br label %52

52:                                               ; preds = %52, %49
  %53 = phi i64 [ %58, %52 ], [ 0, %49 ]
  %54 = getelementptr inbounds i16, i16* %8, i64 %53
  %55 = bitcast i16* %54 to <8 x i16>*
  %56 = load <8 x i16>, <8 x i16>* %55, align 1
  %57 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %56, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %57, <8 x i16>* %55, align 1
  %58 = add nuw nsw i64 %53, 8
  %59 = icmp slt i64 %58, %51
  br i1 %59, label %52, label %60

60:                                               ; preds = %52, %48
  %61 = sext i32 %2 to i64
  br label %62

62:                                               ; preds = %62, %60
  %63 = phi i64 [ %158, %62 ], [ 0, %60 ]
  %64 = shl i64 %63, 2
  %65 = and i64 %64, 4294967280
  %66 = getelementptr inbounds i16, i16* %8, i64 %65
  %67 = bitcast i16* %66 to i64*
  %68 = load i64, i64* %67, align 1
  %69 = insertelement <2 x i64> undef, i64 %68, i32 0
  %70 = getelementptr inbounds i16, i16* %66, i64 4
  %71 = bitcast i16* %70 to i64*
  %72 = load i64, i64* %71, align 1
  %73 = insertelement <2 x i64> undef, i64 %72, i32 0
  %74 = getelementptr inbounds i16, i16* %66, i64 8
  %75 = bitcast i16* %74 to i64*
  %76 = load i64, i64* %75, align 1
  %77 = insertelement <2 x i64> undef, i64 %76, i32 0
  %78 = getelementptr inbounds i16, i16* %66, i64 12
  %79 = bitcast i16* %78 to i64*
  %80 = load i64, i64* %79, align 1
  %81 = insertelement <2 x i64> undef, i64 %80, i32 0
  %82 = bitcast <2 x i64> %69 to <8 x i16>
  %83 = bitcast <2 x i64> %73 to <8 x i16>
  %84 = shufflevector <8 x i16> %82, <8 x i16> %83, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %85 = bitcast <2 x i64> %77 to <8 x i16>
  %86 = bitcast <2 x i64> %81 to <8 x i16>
  %87 = shufflevector <8 x i16> %85, <8 x i16> %86, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %88 = bitcast <8 x i16> %84 to <4 x i32>
  %89 = bitcast <8 x i16> %87 to <4 x i32>
  %90 = shufflevector <4 x i32> %88, <4 x i32> %89, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %91 = shufflevector <4 x i32> %88, <4 x i32> %89, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %92 = bitcast <4 x i32> %90 to <16 x i8>
  %93 = shufflevector <16 x i8> %92, <16 x i8> undef, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %94 = bitcast <4 x i32> %91 to <16 x i8>
  %95 = shufflevector <16 x i8> %94, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %96 = bitcast <4 x i32> %90 to <8 x i16>
  %97 = bitcast <16 x i8> %95 to <8 x i16>
  %98 = shufflevector <8 x i16> %96, <8 x i16> %97, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %99 = bitcast <4 x i32> %91 to <8 x i16>
  %100 = shufflevector <8 x i16> %96, <8 x i16> %99, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %101 = bitcast <16 x i8> %93 to <8 x i16>
  %102 = shufflevector <8 x i16> %101, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %103 = zext <4 x i16> %102 to <4 x i32>
  %104 = shufflevector <8 x i16> %99, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %105 = zext <4 x i16> %104 to <4 x i32>
  %106 = shufflevector <8 x i16> %97, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %107 = zext <4 x i16> %106 to <4 x i32>
  %108 = bitcast <4 x i32> %107 to <8 x i16>
  %109 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %108, <8 x i16> <i16 2482, i16 2482, i16 2482, i16 2482, i16 2482, i16 2482, i16 2482, i16 2482>) #8
  %110 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %108, <8 x i16> <i16 3803, i16 3803, i16 3803, i16 3803, i16 3803, i16 3803, i16 3803, i16 3803>) #8
  %111 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %98, <8 x i16> <i16 3344, i16 3344, i16 3344, i16 3344, i16 3344, i16 3344, i16 3344, i16 3344>) #8
  %112 = bitcast <4 x i32> %105 to <8 x i16>
  %113 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %112, <8 x i16> <i16 3344, i16 3344, i16 3344, i16 3344, i16 3344, i16 3344, i16 3344, i16 3344>) #8
  %114 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %100, <8 x i16> <i16 1321, i16 3803, i16 1321, i16 3803, i16 1321, i16 3803, i16 1321, i16 3803>) #8
  %115 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %100, <8 x i16> <i16 2482, i16 -1321, i16 2482, i16 -1321, i16 2482, i16 -1321, i16 2482, i16 -1321>) #8
  %116 = bitcast <4 x i32> %103 to <8 x i16>
  %117 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %116, <8 x i16> <i16 3344, i16 3344, i16 3344, i16 3344, i16 3344, i16 3344, i16 3344, i16 3344>) #8
  %118 = add <4 x i32> %114, %109
  %119 = sub <4 x i32> %115, %110
  %120 = add <4 x i32> %117, <i32 2048, i32 2048, i32 2048, i32 2048>
  %121 = add <4 x i32> %120, %118
  %122 = ashr <4 x i32> %121, <i32 12, i32 12, i32 12, i32 12>
  %123 = add <4 x i32> %120, %119
  %124 = ashr <4 x i32> %123, <i32 12, i32 12, i32 12, i32 12>
  %125 = add <4 x i32> %111, <i32 2048, i32 2048, i32 2048, i32 2048>
  %126 = sub <4 x i32> %125, %113
  %127 = ashr <4 x i32> %126, <i32 12, i32 12, i32 12, i32 12>
  %128 = add <4 x i32> %118, <i32 2048, i32 2048, i32 2048, i32 2048>
  %129 = add <4 x i32> %128, %119
  %130 = sub <4 x i32> %129, %117
  %131 = ashr <4 x i32> %130, <i32 12, i32 12, i32 12, i32 12>
  %132 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %122, <4 x i32> %124) #8
  %133 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %127, <4 x i32> %131) #8
  %134 = bitcast <8 x i16> %132 to <16 x i8>
  %135 = shufflevector <16 x i8> %134, <16 x i8> undef, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %136 = bitcast <8 x i16> %133 to <16 x i8>
  %137 = shufflevector <16 x i8> %136, <16 x i8> undef, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %138 = bitcast <16 x i8> %135 to <8 x i16>
  %139 = shufflevector <8 x i16> %132, <8 x i16> %138, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %140 = bitcast <16 x i8> %137 to <8 x i16>
  %141 = shufflevector <8 x i16> %133, <8 x i16> %140, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %142 = bitcast <8 x i16> %139 to <4 x i32>
  %143 = bitcast <8 x i16> %141 to <4 x i32>
  %144 = shufflevector <4 x i32> %142, <4 x i32> %143, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %145 = shufflevector <4 x i32> %142, <4 x i32> %143, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %146 = bitcast <4 x i32> %144 to <2 x i64>
  %147 = bitcast <4 x i32> %144 to <16 x i8>
  %148 = shufflevector <16 x i8> %147, <16 x i8> undef, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %149 = bitcast <4 x i32> %145 to <2 x i64>
  %150 = bitcast <4 x i32> %145 to <16 x i8>
  %151 = shufflevector <16 x i8> %150, <16 x i8> undef, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %152 = bitcast <16 x i8> %148 to <2 x i64>
  %153 = bitcast <16 x i8> %151 to <2 x i64>
  %154 = extractelement <2 x i64> %146, i32 0
  store i64 %154, i64* %67, align 1
  %155 = extractelement <2 x i64> %152, i32 0
  store i64 %155, i64* %71, align 1
  %156 = extractelement <2 x i64> %149, i32 0
  store i64 %156, i64* %75, align 1
  %157 = extractelement <2 x i64> %153, i32 0
  store i64 %157, i64* %79, align 1
  %158 = add nuw nsw i64 %63, 4
  %159 = icmp slt i64 %158, %61
  br i1 %159, label %62, label %160

160:                                              ; preds = %62
  br i1 %12, label %161, label %176

161:                                              ; preds = %160
  %162 = shl nsw i32 %2, 2
  %163 = sext i32 %162 to i64
  br label %164

164:                                              ; preds = %164, %161
  %165 = phi i64 [ %174, %164 ], [ 0, %161 ]
  %166 = getelementptr inbounds i16, i16* %8, i64 %165
  %167 = bitcast i16* %166 to <8 x i16>*
  %168 = load <8 x i16>, <8 x i16>* %167, align 1
  %169 = icmp sgt <8 x i16> %168, <i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765>
  %170 = add <8 x i16> %168, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %171 = ashr <8 x i16> %170, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %172 = lshr <8 x i16> %170, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %173 = select <8 x i1> %169, <8 x i16> %172, <8 x i16> %171
  store <8 x i16> %173, <8 x i16>* %167, align 1
  %174 = add nuw nsw i64 %165, 8
  %175 = icmp slt i64 %174, %163
  br i1 %175, label %164, label %176

176:                                              ; preds = %164, %15, %160
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define internal void @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_131Adst4TransformLoopColumn_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_(i8 zeroext, i8 zeroext, i32, i8* nocapture, i32, i32, i8* nocapture readonly) #3 {
  %8 = bitcast i8* %3 to i16*
  %9 = zext i8 %1 to i64
  %10 = getelementptr inbounds [19 x i8], [19 x i8]* @_ZN7libgav115kTransformWidthE, i64 0, i64 %9
  %11 = load i8, i8* %10, align 1
  %12 = zext i8 %11 to i32
  %13 = zext i8 %0 to i32
  %14 = shl i32 1, %13
  %15 = and i32 %14, 33104
  %16 = icmp eq i32 %15, 0
  br i1 %16, label %59, label %17

17:                                               ; preds = %7
  %18 = icmp ugt i8 %11, 15
  br i1 %18, label %19, label %35

19:                                               ; preds = %17
  %20 = shl nuw nsw i32 %12, 2
  %21 = zext i32 %20 to i64
  br label %22

22:                                               ; preds = %22, %19
  %23 = phi i64 [ 0, %19 ], [ %33, %22 ]
  %24 = getelementptr inbounds i16, i16* %8, i64 %23
  %25 = bitcast i16* %24 to <16 x i8>*
  %26 = load <16 x i8>, <16 x i8>* %25, align 1
  %27 = or i64 %23, 8
  %28 = getelementptr inbounds i16, i16* %8, i64 %27
  %29 = bitcast i16* %28 to <16 x i8>*
  %30 = load <16 x i8>, <16 x i8>* %29, align 1
  %31 = shufflevector <16 x i8> %26, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  %32 = shufflevector <16 x i8> %30, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %32, <16 x i8>* %25, align 1
  store <16 x i8> %31, <16 x i8>* %29, align 1
  %33 = add nuw nsw i64 %23, 16
  %34 = icmp ult i64 %33, %21
  br i1 %34, label %22, label %59

35:                                               ; preds = %17
  %36 = icmp eq i8 %11, 8
  %37 = bitcast i8* %3 to <16 x i8>*
  %38 = load <16 x i8>, <16 x i8>* %37, align 1
  br i1 %36, label %45, label %39

39:                                               ; preds = %35
  %40 = shufflevector <16 x i8> %38, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %40, <16 x i8>* %37, align 1
  %41 = getelementptr inbounds i8, i8* %3, i64 16
  %42 = bitcast i8* %41 to <16 x i8>*
  %43 = load <16 x i8>, <16 x i8>* %42, align 1
  %44 = shufflevector <16 x i8> %43, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %44, <16 x i8>* %42, align 1
  br label %59

45:                                               ; preds = %35
  %46 = shufflevector <16 x i8> %38, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %46, <16 x i8>* %37, align 1
  %47 = getelementptr inbounds i8, i8* %3, i64 16
  %48 = bitcast i8* %47 to <16 x i8>*
  %49 = load <16 x i8>, <16 x i8>* %48, align 1
  %50 = shufflevector <16 x i8> %49, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %50, <16 x i8>* %48, align 1
  %51 = getelementptr inbounds i8, i8* %3, i64 32
  %52 = bitcast i8* %51 to <16 x i8>*
  %53 = load <16 x i8>, <16 x i8>* %52, align 1
  %54 = shufflevector <16 x i8> %53, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %54, <16 x i8>* %52, align 1
  %55 = getelementptr inbounds i8, i8* %3, i64 48
  %56 = bitcast i8* %55 to <16 x i8>*
  %57 = load <16 x i8>, <16 x i8>* %56, align 1
  %58 = shufflevector <16 x i8> %57, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %58, <16 x i8>* %56, align 1
  br label %59

59:                                               ; preds = %22, %7, %45, %39
  %60 = icmp sgt i32 %2, 1
  br i1 %60, label %61, label %68

61:                                               ; preds = %59
  %62 = zext i8 %11 to i64
  %63 = shl nuw nsw i32 %12, 1
  %64 = zext i32 %63 to i64
  %65 = mul nuw nsw i32 %12, 3
  %66 = zext i32 %65 to i64
  %67 = zext i8 %11 to i64
  br label %116

68:                                               ; preds = %59
  %69 = shl nuw nsw i32 %12, 1
  %70 = mul nuw nsw i32 %12, 3
  %71 = zext i8 %11 to i64
  %72 = zext i32 %69 to i64
  %73 = zext i32 %70 to i64
  br label %74

74:                                               ; preds = %74, %68
  %75 = phi i64 [ %114, %74 ], [ 0, %68 ]
  %76 = getelementptr inbounds i16, i16* %8, i64 %75
  %77 = bitcast i16* %76 to i64*
  %78 = load i64, i64* %77, align 1
  %79 = insertelement <2 x i64> undef, i64 %78, i32 0
  %80 = bitcast <2 x i64> %79 to <8 x i16>
  %81 = shufflevector <8 x i16> %80, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %82 = sext <4 x i16> %81 to <4 x i32>
  %83 = mul nsw <4 x i32> %82, <i32 1321, i32 1321, i32 1321, i32 1321>
  %84 = mul nsw <4 x i32> %82, <i32 2482, i32 2482, i32 2482, i32 2482>
  %85 = mul nsw <4 x i32> %82, <i32 3344, i32 3344, i32 3344, i32 3344>
  %86 = mul nsw <4 x i32> %82, <i32 3803, i32 3803, i32 3803, i32 3803>
  %87 = add nsw <4 x i32> %83, <i32 2048, i32 2048, i32 2048, i32 2048>
  %88 = ashr <4 x i32> %87, <i32 12, i32 12, i32 12, i32 12>
  %89 = add nsw <4 x i32> %84, <i32 2048, i32 2048, i32 2048, i32 2048>
  %90 = ashr <4 x i32> %89, <i32 12, i32 12, i32 12, i32 12>
  %91 = add nsw <4 x i32> %85, <i32 2048, i32 2048, i32 2048, i32 2048>
  %92 = ashr <4 x i32> %91, <i32 12, i32 12, i32 12, i32 12>
  %93 = add nsw <4 x i32> %86, <i32 2048, i32 2048, i32 2048, i32 2048>
  %94 = ashr <4 x i32> %93, <i32 12, i32 12, i32 12, i32 12>
  %95 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %88, <4 x i32> %90) #8
  %96 = bitcast <8 x i16> %95 to <2 x i64>
  %97 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %92, <4 x i32> %94) #8
  %98 = bitcast <8 x i16> %97 to <2 x i64>
  %99 = extractelement <2 x i64> %96, i32 0
  store i64 %99, i64* %77, align 1
  %100 = add nuw nsw i64 %75, %71
  %101 = getelementptr inbounds i16, i16* %8, i64 %100
  %102 = bitcast <8 x i16> %95 to <4 x float>
  %103 = shufflevector <4 x float> %102, <4 x float> undef, <2 x i32> <i32 2, i32 3>
  %104 = bitcast i16* %101 to <2 x float>*
  store <2 x float> %103, <2 x float>* %104, align 1
  %105 = add nuw nsw i64 %75, %72
  %106 = getelementptr inbounds i16, i16* %8, i64 %105
  %107 = extractelement <2 x i64> %98, i32 0
  %108 = bitcast i16* %106 to i64*
  store i64 %107, i64* %108, align 1
  %109 = add nuw nsw i64 %75, %73
  %110 = getelementptr inbounds i16, i16* %8, i64 %109
  %111 = bitcast <8 x i16> %97 to <4 x float>
  %112 = shufflevector <4 x float> %111, <4 x float> undef, <2 x i32> <i32 2, i32 3>
  %113 = bitcast i16* %110 to <2 x float>*
  store <2 x float> %112, <2 x float>* %113, align 1
  %114 = add nuw nsw i64 %75, 4
  %115 = icmp ult i64 %114, %71
  br i1 %115, label %74, label %186

116:                                              ; preds = %61, %116
  %117 = phi i64 [ 0, %61 ], [ %184, %116 ]
  %118 = getelementptr inbounds i16, i16* %8, i64 %117
  %119 = bitcast i16* %118 to i64*
  %120 = load i64, i64* %119, align 1
  %121 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %120, i32 0
  %122 = getelementptr inbounds i16, i16* %118, i64 %62
  %123 = bitcast i16* %122 to i64*
  %124 = load i64, i64* %123, align 1
  %125 = insertelement <2 x i64> undef, i64 %124, i32 0
  %126 = getelementptr inbounds i16, i16* %118, i64 %64
  %127 = bitcast i16* %126 to i64*
  %128 = load i64, i64* %127, align 1
  %129 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %128, i32 0
  %130 = getelementptr inbounds i16, i16* %118, i64 %66
  %131 = bitcast i16* %130 to i64*
  %132 = load i64, i64* %131, align 1
  %133 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %132, i32 0
  %134 = bitcast <2 x i64> %121 to <8 x i16>
  %135 = bitcast <2 x i64> %133 to <8 x i16>
  %136 = shufflevector <8 x i16> %134, <8 x i16> %135, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %137 = bitcast <2 x i64> %129 to <8 x i16>
  %138 = shufflevector <8 x i16> %134, <8 x i16> %137, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %139 = bitcast <2 x i64> %125 to <8 x i16>
  %140 = shufflevector <8 x i16> %139, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %141 = zext <4 x i16> %140 to <4 x i32>
  %142 = shufflevector <8 x i16> %137, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %143 = zext <4 x i16> %142 to <4 x i32>
  %144 = shufflevector <8 x i16> %135, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %145 = zext <4 x i16> %144 to <4 x i32>
  %146 = bitcast <4 x i32> %145 to <8 x i16>
  %147 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %146, <8 x i16> <i16 2482, i16 2482, i16 2482, i16 2482, i16 2482, i16 2482, i16 2482, i16 2482>) #8
  %148 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %146, <8 x i16> <i16 3803, i16 3803, i16 3803, i16 3803, i16 3803, i16 3803, i16 3803, i16 3803>) #8
  %149 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %136, <8 x i16> <i16 3344, i16 3344, i16 3344, i16 3344, i16 3344, i16 3344, i16 3344, i16 3344>) #8
  %150 = bitcast <4 x i32> %143 to <8 x i16>
  %151 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %150, <8 x i16> <i16 3344, i16 3344, i16 3344, i16 3344, i16 3344, i16 3344, i16 3344, i16 3344>) #8
  %152 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %138, <8 x i16> <i16 1321, i16 3803, i16 1321, i16 3803, i16 1321, i16 3803, i16 1321, i16 3803>) #8
  %153 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %138, <8 x i16> <i16 2482, i16 -1321, i16 2482, i16 -1321, i16 2482, i16 -1321, i16 2482, i16 -1321>) #8
  %154 = bitcast <4 x i32> %141 to <8 x i16>
  %155 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %154, <8 x i16> <i16 3344, i16 3344, i16 3344, i16 3344, i16 3344, i16 3344, i16 3344, i16 3344>) #8
  %156 = add <4 x i32> %152, %147
  %157 = sub <4 x i32> %153, %148
  %158 = add <4 x i32> %155, <i32 2048, i32 2048, i32 2048, i32 2048>
  %159 = add <4 x i32> %158, %156
  %160 = ashr <4 x i32> %159, <i32 12, i32 12, i32 12, i32 12>
  %161 = add <4 x i32> %158, %157
  %162 = ashr <4 x i32> %161, <i32 12, i32 12, i32 12, i32 12>
  %163 = add <4 x i32> %149, <i32 2048, i32 2048, i32 2048, i32 2048>
  %164 = sub <4 x i32> %163, %151
  %165 = ashr <4 x i32> %164, <i32 12, i32 12, i32 12, i32 12>
  %166 = add <4 x i32> %156, <i32 2048, i32 2048, i32 2048, i32 2048>
  %167 = add <4 x i32> %166, %157
  %168 = sub <4 x i32> %167, %155
  %169 = ashr <4 x i32> %168, <i32 12, i32 12, i32 12, i32 12>
  %170 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %160, <4 x i32> %162) #8
  %171 = bitcast <8 x i16> %170 to <2 x i64>
  %172 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %165, <4 x i32> %169) #8
  %173 = bitcast <8 x i16> %172 to <2 x i64>
  %174 = bitcast <8 x i16> %170 to <16 x i8>
  %175 = shufflevector <16 x i8> %174, <16 x i8> undef, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %176 = bitcast <8 x i16> %172 to <16 x i8>
  %177 = shufflevector <16 x i8> %176, <16 x i8> undef, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %178 = bitcast <16 x i8> %175 to <2 x i64>
  %179 = bitcast <16 x i8> %177 to <2 x i64>
  %180 = extractelement <2 x i64> %171, i32 0
  store i64 %180, i64* %119, align 1
  %181 = extractelement <2 x i64> %178, i32 0
  store i64 %181, i64* %123, align 1
  %182 = extractelement <2 x i64> %173, i32 0
  store i64 %182, i64* %127, align 1
  %183 = extractelement <2 x i64> %179, i32 0
  store i64 %183, i64* %131, align 1
  %184 = add nuw nsw i64 %117, 4
  %185 = icmp ult i64 %184, %67
  br i1 %185, label %116, label %186

186:                                              ; preds = %74, %116
  %187 = bitcast i8* %6 to i64*
  %188 = load i64, i64* %187, align 8
  %189 = getelementptr inbounds i8, i8* %6, i64 8
  %190 = bitcast i8* %189 to i8**
  %191 = load i8*, i8** %190, align 8
  %192 = and i32 %14, 16608
  %193 = icmp ne i32 %192, 0
  %194 = sext i32 %5 to i64
  %195 = ashr i64 %188, 32
  %196 = mul nsw i64 %195, %194
  %197 = getelementptr inbounds i8, i8* %191, i64 %196
  %198 = sext i32 %4 to i64
  %199 = getelementptr inbounds i8, i8* %197, i64 %198
  switch i8 %11, label %344 [
    i8 4, label %268
    i8 8, label %200
  ]

200:                                              ; preds = %186
  %201 = select i1 %193, i64 24, i64 0
  %202 = getelementptr inbounds i16, i16* %8, i64 %201
  %203 = bitcast i16* %202 to <8 x i16>*
  %204 = load <8 x i16>, <8 x i16>* %203, align 1
  %205 = bitcast i8* %199 to i64*
  %206 = load i64, i64* %205, align 1
  %207 = insertelement <2 x i64> undef, i64 %206, i32 0
  %208 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %204, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %209 = ashr <8 x i16> %208, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %210 = bitcast <2 x i64> %207 to <16 x i8>
  %211 = shufflevector <16 x i8> %210, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %212 = zext <8 x i8> %211 to <8 x i16>
  %213 = add nsw <8 x i16> %209, %212
  %214 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %213, <8 x i16> undef) #8
  %215 = bitcast <16 x i8> %214 to <2 x i64>
  %216 = extractelement <2 x i64> %215, i32 0
  store i64 %216, i64* %205, align 1
  %217 = getelementptr inbounds i8, i8* %199, i64 %195
  %218 = select i1 %193, i64 16, i64 8
  %219 = getelementptr inbounds i16, i16* %8, i64 %218
  %220 = bitcast i16* %219 to <8 x i16>*
  %221 = load <8 x i16>, <8 x i16>* %220, align 1
  %222 = bitcast i8* %217 to i64*
  %223 = load i64, i64* %222, align 1
  %224 = insertelement <2 x i64> undef, i64 %223, i32 0
  %225 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %221, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %226 = ashr <8 x i16> %225, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %227 = bitcast <2 x i64> %224 to <16 x i8>
  %228 = shufflevector <16 x i8> %227, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %229 = zext <8 x i8> %228 to <8 x i16>
  %230 = add nsw <8 x i16> %226, %229
  %231 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %230, <8 x i16> undef) #8
  %232 = bitcast <16 x i8> %231 to <2 x i64>
  %233 = extractelement <2 x i64> %232, i32 0
  store i64 %233, i64* %222, align 1
  %234 = getelementptr inbounds i8, i8* %217, i64 %195
  %235 = select i1 %193, i64 8, i64 16
  %236 = getelementptr inbounds i16, i16* %8, i64 %235
  %237 = bitcast i16* %236 to <8 x i16>*
  %238 = load <8 x i16>, <8 x i16>* %237, align 1
  %239 = bitcast i8* %234 to i64*
  %240 = load i64, i64* %239, align 1
  %241 = insertelement <2 x i64> undef, i64 %240, i32 0
  %242 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %238, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %243 = ashr <8 x i16> %242, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %244 = bitcast <2 x i64> %241 to <16 x i8>
  %245 = shufflevector <16 x i8> %244, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %246 = zext <8 x i8> %245 to <8 x i16>
  %247 = add nsw <8 x i16> %243, %246
  %248 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %247, <8 x i16> undef) #8
  %249 = bitcast <16 x i8> %248 to <2 x i64>
  %250 = extractelement <2 x i64> %249, i32 0
  store i64 %250, i64* %239, align 1
  %251 = getelementptr inbounds i8, i8* %234, i64 %195
  %252 = select i1 %193, i64 0, i64 24
  %253 = getelementptr inbounds i16, i16* %8, i64 %252
  %254 = bitcast i16* %253 to <8 x i16>*
  %255 = load <8 x i16>, <8 x i16>* %254, align 1
  %256 = bitcast i8* %251 to i64*
  %257 = load i64, i64* %256, align 1
  %258 = insertelement <2 x i64> undef, i64 %257, i32 0
  %259 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %255, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %260 = ashr <8 x i16> %259, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %261 = bitcast <2 x i64> %258 to <16 x i8>
  %262 = shufflevector <16 x i8> %261, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %263 = zext <8 x i8> %262 to <8 x i16>
  %264 = add nsw <8 x i16> %260, %263
  %265 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %264, <8 x i16> undef) #8
  %266 = bitcast <16 x i8> %265 to <2 x i64>
  %267 = extractelement <2 x i64> %266, i32 0
  store i64 %267, i64* %256, align 1
  br label %383

268:                                              ; preds = %186
  %269 = select i1 %193, i64 12, i64 0
  %270 = getelementptr inbounds i16, i16* %8, i64 %269
  %271 = bitcast i16* %270 to i64*
  %272 = load i64, i64* %271, align 1
  %273 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %272, i32 0
  %274 = bitcast i8* %199 to i32*
  %275 = load i32, i32* %274, align 1
  %276 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %275, i32 0
  %277 = bitcast <2 x i64> %273 to <8 x i16>
  %278 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %277, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %279 = ashr <8 x i16> %278, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %280 = bitcast <4 x i32> %276 to <16 x i8>
  %281 = shufflevector <16 x i8> %280, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %282 = zext <8 x i8> %281 to <8 x i16>
  %283 = add nsw <8 x i16> %279, %282
  %284 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %283, <8 x i16> undef) #8
  %285 = bitcast <16 x i8> %284 to <4 x i32>
  %286 = extractelement <4 x i32> %285, i32 0
  store i32 %286, i32* %274, align 1
  %287 = getelementptr inbounds i8, i8* %199, i64 %195
  %288 = select i1 %193, i64 8, i64 4
  %289 = getelementptr inbounds i16, i16* %8, i64 %288
  %290 = bitcast i16* %289 to i64*
  %291 = load i64, i64* %290, align 1
  %292 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %291, i32 0
  %293 = bitcast i8* %287 to i32*
  %294 = load i32, i32* %293, align 1
  %295 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %294, i32 0
  %296 = bitcast <2 x i64> %292 to <8 x i16>
  %297 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %296, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %298 = ashr <8 x i16> %297, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %299 = bitcast <4 x i32> %295 to <16 x i8>
  %300 = shufflevector <16 x i8> %299, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %301 = zext <8 x i8> %300 to <8 x i16>
  %302 = add nsw <8 x i16> %298, %301
  %303 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %302, <8 x i16> undef) #8
  %304 = bitcast <16 x i8> %303 to <4 x i32>
  %305 = extractelement <4 x i32> %304, i32 0
  store i32 %305, i32* %293, align 1
  %306 = getelementptr inbounds i8, i8* %287, i64 %195
  %307 = select i1 %193, i64 4, i64 8
  %308 = getelementptr inbounds i16, i16* %8, i64 %307
  %309 = bitcast i16* %308 to i64*
  %310 = load i64, i64* %309, align 1
  %311 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %310, i32 0
  %312 = bitcast i8* %306 to i32*
  %313 = load i32, i32* %312, align 1
  %314 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %313, i32 0
  %315 = bitcast <2 x i64> %311 to <8 x i16>
  %316 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %315, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %317 = ashr <8 x i16> %316, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %318 = bitcast <4 x i32> %314 to <16 x i8>
  %319 = shufflevector <16 x i8> %318, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %320 = zext <8 x i8> %319 to <8 x i16>
  %321 = add nsw <8 x i16> %317, %320
  %322 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %321, <8 x i16> undef) #8
  %323 = bitcast <16 x i8> %322 to <4 x i32>
  %324 = extractelement <4 x i32> %323, i32 0
  store i32 %324, i32* %312, align 1
  %325 = getelementptr inbounds i8, i8* %306, i64 %195
  %326 = select i1 %193, i64 0, i64 12
  %327 = getelementptr inbounds i16, i16* %8, i64 %326
  %328 = bitcast i16* %327 to i64*
  %329 = load i64, i64* %328, align 1
  %330 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %329, i32 0
  %331 = bitcast i8* %325 to i32*
  %332 = load i32, i32* %331, align 1
  %333 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %332, i32 0
  %334 = bitcast <2 x i64> %330 to <8 x i16>
  %335 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %334, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %336 = ashr <8 x i16> %335, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %337 = bitcast <4 x i32> %333 to <16 x i8>
  %338 = shufflevector <16 x i8> %337, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %339 = zext <8 x i8> %338 to <8 x i16>
  %340 = add nsw <8 x i16> %336, %339
  %341 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %340, <8 x i16> undef) #8
  %342 = bitcast <16 x i8> %341 to <4 x i32>
  %343 = extractelement <4 x i32> %342, i32 0
  store i32 %343, i32* %331, align 1
  br label %383

344:                                              ; preds = %186
  %345 = zext i8 %11 to i64
  %346 = select i1 %193, i32 3, i32 0
  %347 = mul nuw nsw i32 %346, %12
  %348 = zext i32 %347 to i64
  br label %349

349:                                              ; preds = %349, %344
  %350 = phi i64 [ %374, %349 ], [ 0, %344 ]
  %351 = add nsw i64 %350, %198
  %352 = add nuw nsw i64 %350, %348
  %353 = getelementptr inbounds i16, i16* %8, i64 %352
  %354 = bitcast i16* %353 to <8 x i16>*
  %355 = load <8 x i16>, <8 x i16>* %354, align 1
  %356 = add nuw nsw i64 %352, 8
  %357 = getelementptr inbounds i16, i16* %8, i64 %356
  %358 = bitcast i16* %357 to <8 x i16>*
  %359 = load <8 x i16>, <8 x i16>* %358, align 1
  %360 = getelementptr inbounds i8, i8* %197, i64 %351
  %361 = bitcast i8* %360 to <16 x i8>*
  %362 = load <16 x i8>, <16 x i8>* %361, align 1
  %363 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %355, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %364 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %359, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %365 = ashr <8 x i16> %363, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %366 = ashr <8 x i16> %364, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %367 = shufflevector <16 x i8> %362, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %368 = zext <8 x i8> %367 to <8 x i16>
  %369 = shufflevector <16 x i8> %362, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %370 = zext <8 x i8> %369 to <8 x i16>
  %371 = add nsw <8 x i16> %365, %368
  %372 = add nsw <8 x i16> %366, %370
  %373 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %371, <8 x i16> %372) #8
  store <16 x i8> %373, <16 x i8>* %361, align 1
  %374 = add nuw nsw i64 %350, 16
  %375 = icmp ult i64 %374, %345
  br i1 %375, label %349, label %376

376:                                              ; preds = %349
  %377 = add nsw i64 %194, 1
  %378 = select i1 %193, i32 2, i32 1
  %379 = mul nuw nsw i32 %378, %12
  %380 = mul nsw i64 %377, %195
  %381 = getelementptr inbounds i8, i8* %191, i64 %380
  %382 = zext i32 %379 to i64
  br label %384

383:                                              ; preds = %452, %200, %268
  ret void

384:                                              ; preds = %384, %376
  %385 = phi i64 [ %409, %384 ], [ 0, %376 ]
  %386 = add nsw i64 %385, %198
  %387 = add nuw nsw i64 %385, %382
  %388 = getelementptr inbounds i16, i16* %8, i64 %387
  %389 = bitcast i16* %388 to <8 x i16>*
  %390 = load <8 x i16>, <8 x i16>* %389, align 1
  %391 = add nuw nsw i64 %387, 8
  %392 = getelementptr inbounds i16, i16* %8, i64 %391
  %393 = bitcast i16* %392 to <8 x i16>*
  %394 = load <8 x i16>, <8 x i16>* %393, align 1
  %395 = getelementptr inbounds i8, i8* %381, i64 %386
  %396 = bitcast i8* %395 to <16 x i8>*
  %397 = load <16 x i8>, <16 x i8>* %396, align 1
  %398 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %390, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %399 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %394, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %400 = ashr <8 x i16> %398, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %401 = ashr <8 x i16> %399, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %402 = shufflevector <16 x i8> %397, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %403 = zext <8 x i8> %402 to <8 x i16>
  %404 = shufflevector <16 x i8> %397, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %405 = zext <8 x i8> %404 to <8 x i16>
  %406 = add nsw <8 x i16> %400, %403
  %407 = add nsw <8 x i16> %401, %405
  %408 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %406, <8 x i16> %407) #8
  store <16 x i8> %408, <16 x i8>* %396, align 1
  %409 = add nuw nsw i64 %385, 16
  %410 = icmp ult i64 %409, %345
  br i1 %410, label %384, label %411

411:                                              ; preds = %384
  %412 = add nsw i64 %194, 2
  %413 = select i1 %193, i32 1, i32 2
  %414 = mul nuw nsw i32 %413, %12
  %415 = mul nsw i64 %412, %195
  %416 = getelementptr inbounds i8, i8* %191, i64 %415
  %417 = zext i32 %414 to i64
  br label %418

418:                                              ; preds = %418, %411
  %419 = phi i64 [ %443, %418 ], [ 0, %411 ]
  %420 = add nsw i64 %419, %198
  %421 = add nuw nsw i64 %419, %417
  %422 = getelementptr inbounds i16, i16* %8, i64 %421
  %423 = bitcast i16* %422 to <8 x i16>*
  %424 = load <8 x i16>, <8 x i16>* %423, align 1
  %425 = add nuw nsw i64 %421, 8
  %426 = getelementptr inbounds i16, i16* %8, i64 %425
  %427 = bitcast i16* %426 to <8 x i16>*
  %428 = load <8 x i16>, <8 x i16>* %427, align 1
  %429 = getelementptr inbounds i8, i8* %416, i64 %420
  %430 = bitcast i8* %429 to <16 x i8>*
  %431 = load <16 x i8>, <16 x i8>* %430, align 1
  %432 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %424, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %433 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %428, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %434 = ashr <8 x i16> %432, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %435 = ashr <8 x i16> %433, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %436 = shufflevector <16 x i8> %431, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %437 = zext <8 x i8> %436 to <8 x i16>
  %438 = shufflevector <16 x i8> %431, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %439 = zext <8 x i8> %438 to <8 x i16>
  %440 = add nsw <8 x i16> %434, %437
  %441 = add nsw <8 x i16> %435, %439
  %442 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %440, <8 x i16> %441) #8
  store <16 x i8> %442, <16 x i8>* %430, align 1
  %443 = add nuw nsw i64 %419, 16
  %444 = icmp ult i64 %443, %345
  br i1 %444, label %418, label %445

445:                                              ; preds = %418
  %446 = add nsw i64 %194, 3
  %447 = select i1 %193, i32 0, i32 3
  %448 = mul nuw nsw i32 %447, %12
  %449 = mul nsw i64 %446, %195
  %450 = getelementptr inbounds i8, i8* %191, i64 %449
  %451 = zext i32 %448 to i64
  br label %452

452:                                              ; preds = %452, %445
  %453 = phi i64 [ %477, %452 ], [ 0, %445 ]
  %454 = add nsw i64 %453, %198
  %455 = add nuw nsw i64 %453, %451
  %456 = getelementptr inbounds i16, i16* %8, i64 %455
  %457 = bitcast i16* %456 to <8 x i16>*
  %458 = load <8 x i16>, <8 x i16>* %457, align 1
  %459 = add nuw nsw i64 %455, 8
  %460 = getelementptr inbounds i16, i16* %8, i64 %459
  %461 = bitcast i16* %460 to <8 x i16>*
  %462 = load <8 x i16>, <8 x i16>* %461, align 1
  %463 = getelementptr inbounds i8, i8* %450, i64 %454
  %464 = bitcast i8* %463 to <16 x i8>*
  %465 = load <16 x i8>, <16 x i8>* %464, align 1
  %466 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %458, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %467 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %462, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %468 = ashr <8 x i16> %466, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %469 = ashr <8 x i16> %467, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %470 = shufflevector <16 x i8> %465, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %471 = zext <8 x i8> %470 to <8 x i16>
  %472 = shufflevector <16 x i8> %465, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %473 = zext <8 x i8> %472 to <8 x i16>
  %474 = add nsw <8 x i16> %468, %471
  %475 = add nsw <8 x i16> %469, %473
  %476 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %474, <8 x i16> %475) #8
  store <16 x i8> %476, <16 x i8>* %464, align 1
  %477 = add nuw nsw i64 %453, 16
  %478 = icmp ult i64 %477, %345
  br i1 %478, label %452, label %383
}

; Function Attrs: nofree nounwind ssp uwtable
define internal void @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_128Adst8TransformLoopRow_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_(i8 zeroext, i8 zeroext, i32, i8* nocapture, i32, i32, i8* nocapture readnone) #3 {
  %8 = bitcast i8* %3 to i16*
  %9 = zext i8 %1 to i64
  %10 = lshr i64 173354, %9
  %11 = and i64 %10, 1
  %12 = icmp ne i64 %11, 0
  %13 = getelementptr inbounds [19 x i8], [19 x i8]* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_118kTransformRowShiftE, i64 0, i64 %9
  %14 = load i8, i8* %13, align 1
  %15 = icmp sgt i32 %2, 1
  br i1 %15, label %95, label %16

16:                                               ; preds = %7
  %17 = zext i8 %14 to i32
  %18 = load i16, i16* %8, align 2
  %19 = sext i16 %18 to i32
  %20 = insertelement <4 x i32> <i32 undef, i32 undef, i32 0, i32 0>, i32 %19, i32 0
  %21 = bitcast <4 x i32> %20 to <8 x i16>
  %22 = shufflevector <8 x i16> %21, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 4, i32 5, i32 6, i32 7>
  %23 = sext i1 %12 to i16
  %24 = insertelement <8 x i16> undef, i16 %23, i32 0
  %25 = shufflevector <8 x i16> %24, <8 x i16> undef, <8 x i32> zeroinitializer
  %26 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %22, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  %27 = bitcast <8 x i16> %22 to <16 x i8>
  %28 = bitcast <8 x i16> %26 to <16 x i8>
  %29 = bitcast <8 x i16> %25 to <16 x i8>
  %30 = tail call <16 x i8> @llvm.x86.sse41.pblendvb(<16 x i8> %27, <16 x i8> %28, <16 x i8> %29) #8
  %31 = bitcast <16 x i8> %30 to <8 x i16>
  %32 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %31, <8 x i16> <i16 -32608, i16 -32608, i16 -32608, i16 -32608, i16 -32608, i16 -32608, i16 -32608, i16 -32608>) #8
  %33 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %31, <8 x i16> <i16 3208, i16 3208, i16 3208, i16 3208, i16 3208, i16 3208, i16 3208, i16 3208>) #8
  %34 = shufflevector <8 x i16> %33, <8 x i16> %32, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %35 = shufflevector <8 x i16> %32, <8 x i16> %33, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %36 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %37 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %34, <8 x i16> %36) #8
  %38 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %35, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %39 = add <4 x i32> %37, <i32 2048, i32 2048, i32 2048, i32 2048>
  %40 = ashr <4 x i32> %39, <i32 12, i32 12, i32 12, i32 12>
  %41 = add <4 x i32> %38, <i32 2048, i32 2048, i32 2048, i32 2048>
  %42 = ashr <4 x i32> %41, <i32 12, i32 12, i32 12, i32 12>
  %43 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %40, <4 x i32> %40) #8
  %44 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %42, <4 x i32> %42) #8
  %45 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %46 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %34, <8 x i16> %45) #8
  %47 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %35, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %48 = add <4 x i32> %46, <i32 2048, i32 2048, i32 2048, i32 2048>
  %49 = ashr <4 x i32> %48, <i32 12, i32 12, i32 12, i32 12>
  %50 = add <4 x i32> %47, <i32 2048, i32 2048, i32 2048, i32 2048>
  %51 = ashr <4 x i32> %50, <i32 12, i32 12, i32 12, i32 12>
  %52 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %49, <4 x i32> undef) #8
  %53 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %51, <4 x i32> %51) #8
  %54 = shufflevector <8 x i16> %44, <8 x i16> %43, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %55 = shufflevector <8 x i16> %43, <8 x i16> %44, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %56 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %54, <8 x i16> %45) #8
  %57 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %55, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %58 = add <4 x i32> %56, <i32 2048, i32 2048, i32 2048, i32 2048>
  %59 = ashr <4 x i32> %58, <i32 12, i32 12, i32 12, i32 12>
  %60 = add <4 x i32> %57, <i32 2048, i32 2048, i32 2048, i32 2048>
  %61 = ashr <4 x i32> %60, <i32 12, i32 12, i32 12, i32 12>
  %62 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %59, <4 x i32> %59) #8
  %63 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %61, <4 x i32> undef) #8
  %64 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %44) #8
  %65 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %53) #8
  %66 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %62) #8
  %67 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %32) #8
  %68 = shufflevector <8 x i16> %33, <8 x i16> %64, <8 x i32> <i32 0, i32 8, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %69 = shufflevector <8 x i16> %63, <8 x i16> %65, <8 x i32> <i32 0, i32 8, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %70 = shufflevector <8 x i16> %52, <8 x i16> %66, <8 x i32> <i32 0, i32 8, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %71 = shufflevector <8 x i16> %43, <8 x i16> %67, <8 x i32> <i32 0, i32 8, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %72 = bitcast <8 x i16> %68 to <4 x i32>
  %73 = bitcast <8 x i16> %69 to <4 x i32>
  %74 = shufflevector <4 x i32> %72, <4 x i32> %73, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %75 = bitcast <8 x i16> %70 to <4 x i32>
  %76 = bitcast <8 x i16> %71 to <4 x i32>
  %77 = shufflevector <4 x i32> %75, <4 x i32> %76, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %78 = insertelement <4 x i32> undef, i32 %17, i32 0
  %79 = shufflevector <4 x i32> %78, <4 x i32> undef, <4 x i32> zeroinitializer
  %80 = shufflevector <4 x i32> %78, <4 x i32> undef, <2 x i32> zeroinitializer
  %81 = zext <2 x i32> %80 to <2 x i64>
  %82 = bitcast <4 x i32> %74 to <8 x i16>
  %83 = shufflevector <8 x i16> %82, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %84 = sext <4 x i16> %83 to <4 x i32>
  %85 = add <4 x i32> %79, %84
  %86 = bitcast <4 x i32> %77 to <8 x i16>
  %87 = shufflevector <8 x i16> %86, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %88 = sext <4 x i16> %87 to <4 x i32>
  %89 = add <4 x i32> %79, %88
  %90 = bitcast <2 x i64> %81 to <4 x i32>
  %91 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %85, <4 x i32> %90) #8
  %92 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %89, <4 x i32> %90) #8
  %93 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %91, <4 x i32> %92) #8
  %94 = bitcast i8* %3 to <8 x i16>*
  store <8 x i16> %93, <8 x i16>* %94, align 1
  br label %653

95:                                               ; preds = %7
  br i1 %12, label %96, label %151

96:                                               ; preds = %95
  %97 = sext i32 %2 to i64
  %98 = add nsw i64 %97, -1
  %99 = and i64 %97, 3
  %100 = icmp ult i64 %98, 3
  br i1 %100, label %136, label %101

101:                                              ; preds = %96
  %102 = sub nsw i64 %97, %99
  br label %103

103:                                              ; preds = %103, %101
  %104 = phi i64 [ 0, %101 ], [ %133, %103 ]
  %105 = phi i64 [ %102, %101 ], [ %134, %103 ]
  %106 = shl i64 %104, 3
  %107 = and i64 %106, 4294967264
  %108 = getelementptr inbounds i16, i16* %8, i64 %107
  %109 = bitcast i16* %108 to <8 x i16>*
  %110 = load <8 x i16>, <8 x i16>* %109, align 1
  %111 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %110, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %111, <8 x i16>* %109, align 1
  %112 = shl i64 %104, 3
  %113 = and i64 %112, 4294967264
  %114 = or i64 %113, 8
  %115 = getelementptr inbounds i16, i16* %8, i64 %114
  %116 = bitcast i16* %115 to <8 x i16>*
  %117 = load <8 x i16>, <8 x i16>* %116, align 1
  %118 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %117, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %118, <8 x i16>* %116, align 1
  %119 = shl i64 %104, 3
  %120 = and i64 %119, 4294967264
  %121 = or i64 %120, 16
  %122 = getelementptr inbounds i16, i16* %8, i64 %121
  %123 = bitcast i16* %122 to <8 x i16>*
  %124 = load <8 x i16>, <8 x i16>* %123, align 1
  %125 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %124, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %125, <8 x i16>* %123, align 1
  %126 = shl i64 %104, 3
  %127 = and i64 %126, 4294967264
  %128 = or i64 %127, 24
  %129 = getelementptr inbounds i16, i16* %8, i64 %128
  %130 = bitcast i16* %129 to <8 x i16>*
  %131 = load <8 x i16>, <8 x i16>* %130, align 1
  %132 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %131, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %132, <8 x i16>* %130, align 1
  %133 = add nuw nsw i64 %104, 4
  %134 = add i64 %105, -4
  %135 = icmp eq i64 %134, 0
  br i1 %135, label %136, label %103

136:                                              ; preds = %103, %96
  %137 = phi i64 [ 0, %96 ], [ %133, %103 ]
  %138 = icmp eq i64 %99, 0
  br i1 %138, label %151, label %139

139:                                              ; preds = %136, %139
  %140 = phi i64 [ %148, %139 ], [ %137, %136 ]
  %141 = phi i64 [ %149, %139 ], [ %99, %136 ]
  %142 = shl i64 %140, 3
  %143 = and i64 %142, 4294967288
  %144 = getelementptr inbounds i16, i16* %8, i64 %143
  %145 = bitcast i16* %144 to <8 x i16>*
  %146 = load <8 x i16>, <8 x i16>* %145, align 1
  %147 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %146, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %147, <8 x i16>* %145, align 1
  %148 = add nuw nsw i64 %140, 1
  %149 = add i64 %141, -1
  %150 = icmp eq i64 %149, 0
  br i1 %150, label %151, label %139, !llvm.loop !4

151:                                              ; preds = %136, %139, %95
  %152 = icmp slt i32 %2, 5
  br i1 %152, label %162, label %153

153:                                              ; preds = %151
  %154 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %155 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %156 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %157 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %158 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %159 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %160 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %161 = sext i32 %2 to i64
  br label %337

162:                                              ; preds = %151
  %163 = bitcast i8* %3 to <8 x i16>*
  %164 = load <8 x i16>, <8 x i16>* %163, align 1
  %165 = getelementptr inbounds i8, i8* %3, i64 16
  %166 = bitcast i8* %165 to <8 x i16>*
  %167 = load <8 x i16>, <8 x i16>* %166, align 1
  %168 = getelementptr inbounds i8, i8* %3, i64 32
  %169 = bitcast i8* %168 to <8 x i16>*
  %170 = load <8 x i16>, <8 x i16>* %169, align 1
  %171 = getelementptr inbounds i8, i8* %3, i64 48
  %172 = bitcast i8* %171 to <8 x i16>*
  %173 = load <8 x i16>, <8 x i16>* %172, align 1
  %174 = shufflevector <8 x i16> %164, <8 x i16> %167, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %175 = shufflevector <8 x i16> %170, <8 x i16> %173, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %176 = shufflevector <8 x i16> %164, <8 x i16> %167, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %177 = shufflevector <8 x i16> %170, <8 x i16> %173, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %178 = bitcast <8 x i16> %174 to <4 x i32>
  %179 = bitcast <8 x i16> %175 to <4 x i32>
  %180 = shufflevector <4 x i32> %178, <4 x i32> %179, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %181 = bitcast <4 x i32> %180 to <2 x i64>
  %182 = bitcast <8 x i16> %176 to <4 x i32>
  %183 = bitcast <8 x i16> %177 to <4 x i32>
  %184 = shufflevector <4 x i32> %182, <4 x i32> %183, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %185 = bitcast <4 x i32> %184 to <2 x i64>
  %186 = shufflevector <4 x i32> %178, <4 x i32> %179, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %187 = bitcast <4 x i32> %186 to <2 x i64>
  %188 = shufflevector <4 x i32> %182, <4 x i32> %183, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %189 = bitcast <4 x i32> %188 to <2 x i64>
  %190 = insertelement <2 x i64> %181, i64 0, i32 1
  %191 = shufflevector <2 x i64> %181, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  %192 = insertelement <2 x i64> %187, i64 0, i32 1
  %193 = shufflevector <2 x i64> %187, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  %194 = insertelement <2 x i64> %185, i64 0, i32 1
  %195 = shufflevector <2 x i64> %185, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  %196 = insertelement <2 x i64> %189, i64 0, i32 1
  %197 = shufflevector <2 x i64> %189, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  %198 = bitcast <2 x i64> %197 to <8 x i16>
  %199 = bitcast <2 x i64> %190 to <8 x i16>
  %200 = shufflevector <8 x i16> %198, <8 x i16> %199, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %201 = shufflevector <8 x i16> %199, <8 x i16> %198, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %202 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %203 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %200, <8 x i16> %202) #8
  %204 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %201, <8 x i16> <i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076>) #8
  %205 = add <4 x i32> %203, <i32 2048, i32 2048, i32 2048, i32 2048>
  %206 = ashr <4 x i32> %205, <i32 12, i32 12, i32 12, i32 12>
  %207 = add <4 x i32> %204, <i32 2048, i32 2048, i32 2048, i32 2048>
  %208 = ashr <4 x i32> %207, <i32 12, i32 12, i32 12, i32 12>
  %209 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %206, <4 x i32> %206) #8
  %210 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %208, <4 x i32> %208) #8
  %211 = bitcast <2 x i64> %195 to <8 x i16>
  %212 = bitcast <2 x i64> %192 to <8 x i16>
  %213 = shufflevector <8 x i16> %211, <8 x i16> %212, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %214 = shufflevector <8 x i16> %212, <8 x i16> %211, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %215 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %216 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %213, <8 x i16> %215) #8
  %217 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %214, <8 x i16> <i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612>) #8
  %218 = add <4 x i32> %216, <i32 2048, i32 2048, i32 2048, i32 2048>
  %219 = ashr <4 x i32> %218, <i32 12, i32 12, i32 12, i32 12>
  %220 = add <4 x i32> %217, <i32 2048, i32 2048, i32 2048, i32 2048>
  %221 = ashr <4 x i32> %220, <i32 12, i32 12, i32 12, i32 12>
  %222 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %219, <4 x i32> %219) #8
  %223 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %221, <4 x i32> %221) #8
  %224 = bitcast <2 x i64> %193 to <8 x i16>
  %225 = bitcast <2 x i64> %194 to <8 x i16>
  %226 = shufflevector <8 x i16> %224, <8 x i16> %225, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %227 = shufflevector <8 x i16> %225, <8 x i16> %224, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %228 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %229 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %226, <8 x i16> %228) #8
  %230 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %227, <8 x i16> <i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598>) #8
  %231 = add <4 x i32> %229, <i32 2048, i32 2048, i32 2048, i32 2048>
  %232 = ashr <4 x i32> %231, <i32 12, i32 12, i32 12, i32 12>
  %233 = add <4 x i32> %230, <i32 2048, i32 2048, i32 2048, i32 2048>
  %234 = ashr <4 x i32> %233, <i32 12, i32 12, i32 12, i32 12>
  %235 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %232, <4 x i32> %232) #8
  %236 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %234, <4 x i32> %234) #8
  %237 = bitcast <2 x i64> %191 to <8 x i16>
  %238 = bitcast <2 x i64> %196 to <8 x i16>
  %239 = shufflevector <8 x i16> %237, <8 x i16> %238, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %240 = shufflevector <8 x i16> %238, <8 x i16> %237, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %241 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %242 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %239, <8 x i16> %241) #8
  %243 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %240, <8 x i16> <i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189>) #8
  %244 = add <4 x i32> %242, <i32 2048, i32 2048, i32 2048, i32 2048>
  %245 = ashr <4 x i32> %244, <i32 12, i32 12, i32 12, i32 12>
  %246 = add <4 x i32> %243, <i32 2048, i32 2048, i32 2048, i32 2048>
  %247 = ashr <4 x i32> %246, <i32 12, i32 12, i32 12, i32 12>
  %248 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %245, <4 x i32> %245) #8
  %249 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %247, <4 x i32> %247) #8
  %250 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %210, <8 x i16> %236) #8
  %251 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %210, <8 x i16> %236) #8
  %252 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %209, <8 x i16> %235) #8
  %253 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %209, <8 x i16> %235) #8
  %254 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %223, <8 x i16> %249) #8
  %255 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %223, <8 x i16> %249) #8
  %256 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %222, <8 x i16> %248) #8
  %257 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %222, <8 x i16> %248) #8
  %258 = shufflevector <8 x i16> %251, <8 x i16> %253, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %259 = shufflevector <8 x i16> %253, <8 x i16> %251, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %260 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %261 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %258, <8 x i16> %260) #8
  %262 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %259, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %263 = add <4 x i32> %261, <i32 2048, i32 2048, i32 2048, i32 2048>
  %264 = ashr <4 x i32> %263, <i32 12, i32 12, i32 12, i32 12>
  %265 = add <4 x i32> %262, <i32 2048, i32 2048, i32 2048, i32 2048>
  %266 = ashr <4 x i32> %265, <i32 12, i32 12, i32 12, i32 12>
  %267 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %264, <4 x i32> %264) #8
  %268 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %266, <4 x i32> %266) #8
  %269 = shufflevector <8 x i16> %257, <8 x i16> %255, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %270 = shufflevector <8 x i16> %255, <8 x i16> %257, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %271 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %272 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %269, <8 x i16> %271) #8
  %273 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %270, <8 x i16> <i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567>) #8
  %274 = add <4 x i32> %272, <i32 2048, i32 2048, i32 2048, i32 2048>
  %275 = ashr <4 x i32> %274, <i32 12, i32 12, i32 12, i32 12>
  %276 = add <4 x i32> %273, <i32 2048, i32 2048, i32 2048, i32 2048>
  %277 = ashr <4 x i32> %276, <i32 12, i32 12, i32 12, i32 12>
  %278 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %275, <4 x i32> %275) #8
  %279 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %277, <4 x i32> %277) #8
  %280 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %250, <8 x i16> %254) #8
  %281 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %250, <8 x i16> %254) #8
  %282 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %268, <8 x i16> %278) #8
  %283 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %268, <8 x i16> %278) #8
  %284 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %252, <8 x i16> %256) #8
  %285 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %252, <8 x i16> %256) #8
  %286 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %267, <8 x i16> %279) #8
  %287 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %267, <8 x i16> %279) #8
  %288 = shufflevector <8 x i16> %281, <8 x i16> %285, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %289 = shufflevector <8 x i16> %285, <8 x i16> %281, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %290 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %291 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %288, <8 x i16> %290) #8
  %292 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %289, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %293 = add <4 x i32> %291, <i32 2048, i32 2048, i32 2048, i32 2048>
  %294 = ashr <4 x i32> %293, <i32 12, i32 12, i32 12, i32 12>
  %295 = add <4 x i32> %292, <i32 2048, i32 2048, i32 2048, i32 2048>
  %296 = ashr <4 x i32> %295, <i32 12, i32 12, i32 12, i32 12>
  %297 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %294, <4 x i32> undef) #8
  %298 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %296, <4 x i32> %296) #8
  %299 = shufflevector <8 x i16> %283, <8 x i16> %287, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %300 = shufflevector <8 x i16> %287, <8 x i16> %283, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %301 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %299, <8 x i16> %290) #8
  %302 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %300, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %303 = add <4 x i32> %301, <i32 2048, i32 2048, i32 2048, i32 2048>
  %304 = ashr <4 x i32> %303, <i32 12, i32 12, i32 12, i32 12>
  %305 = add <4 x i32> %302, <i32 2048, i32 2048, i32 2048, i32 2048>
  %306 = ashr <4 x i32> %305, <i32 12, i32 12, i32 12, i32 12>
  %307 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %304, <4 x i32> %304) #8
  %308 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %306, <4 x i32> undef) #8
  %309 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %282) #8
  %310 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %298) #8
  %311 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %307) #8
  %312 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %284) #8
  %313 = shufflevector <8 x i16> %280, <8 x i16> %309, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %314 = shufflevector <8 x i16> %308, <8 x i16> %310, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %315 = shufflevector <8 x i16> %297, <8 x i16> %311, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %316 = shufflevector <8 x i16> %286, <8 x i16> %312, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %317 = bitcast <8 x i16> %313 to <4 x i32>
  %318 = bitcast <8 x i16> %314 to <4 x i32>
  %319 = shufflevector <4 x i32> %317, <4 x i32> %318, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %320 = bitcast <4 x i32> %319 to <2 x i64>
  %321 = bitcast <8 x i16> %315 to <4 x i32>
  %322 = bitcast <8 x i16> %316 to <4 x i32>
  %323 = shufflevector <4 x i32> %321, <4 x i32> %322, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %324 = bitcast <4 x i32> %323 to <2 x i64>
  %325 = shufflevector <4 x i32> %317, <4 x i32> %318, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %326 = bitcast <4 x i32> %325 to <2 x i64>
  %327 = shufflevector <4 x i32> %321, <4 x i32> %322, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %328 = bitcast <4 x i32> %327 to <2 x i64>
  %329 = shufflevector <2 x i64> %320, <2 x i64> %324, <2 x i32> <i32 0, i32 2>
  %330 = shufflevector <2 x i64> %320, <2 x i64> %324, <2 x i32> <i32 1, i32 3>
  %331 = shufflevector <2 x i64> %326, <2 x i64> %328, <2 x i32> <i32 0, i32 2>
  %332 = shufflevector <2 x i64> %326, <2 x i64> %328, <2 x i32> <i32 1, i32 3>
  %333 = bitcast i8* %3 to <2 x i64>*
  store <2 x i64> %329, <2 x i64>* %333, align 1
  %334 = bitcast i8* %165 to <2 x i64>*
  store <2 x i64> %330, <2 x i64>* %334, align 1
  %335 = bitcast i8* %168 to <2 x i64>*
  store <2 x i64> %331, <2 x i64>* %335, align 1
  %336 = bitcast i8* %171 to <2 x i64>*
  store <2 x i64> %332, <2 x i64>* %336, align 1
  br label %627

337:                                              ; preds = %153, %337
  %338 = phi i64 [ 0, %153 ], [ %625, %337 ]
  %339 = shl i64 %338, 3
  %340 = and i64 %339, 4294967232
  %341 = getelementptr inbounds i16, i16* %8, i64 %340
  %342 = bitcast i16* %341 to <2 x i64>*
  %343 = bitcast i16* %341 to <8 x i16>*
  %344 = load <8 x i16>, <8 x i16>* %343, align 1
  %345 = getelementptr inbounds i16, i16* %341, i64 8
  %346 = bitcast i16* %345 to <2 x i64>*
  %347 = bitcast i16* %345 to <8 x i16>*
  %348 = load <8 x i16>, <8 x i16>* %347, align 1
  %349 = getelementptr inbounds i16, i16* %341, i64 16
  %350 = bitcast i16* %349 to <2 x i64>*
  %351 = bitcast i16* %349 to <8 x i16>*
  %352 = load <8 x i16>, <8 x i16>* %351, align 1
  %353 = getelementptr inbounds i16, i16* %341, i64 24
  %354 = bitcast i16* %353 to <2 x i64>*
  %355 = bitcast i16* %353 to <8 x i16>*
  %356 = load <8 x i16>, <8 x i16>* %355, align 1
  %357 = getelementptr inbounds i16, i16* %341, i64 32
  %358 = bitcast i16* %357 to <2 x i64>*
  %359 = bitcast i16* %357 to <8 x i16>*
  %360 = load <8 x i16>, <8 x i16>* %359, align 1
  %361 = getelementptr inbounds i16, i16* %341, i64 40
  %362 = bitcast i16* %361 to <2 x i64>*
  %363 = bitcast i16* %361 to <8 x i16>*
  %364 = load <8 x i16>, <8 x i16>* %363, align 1
  %365 = getelementptr inbounds i16, i16* %341, i64 48
  %366 = bitcast i16* %365 to <2 x i64>*
  %367 = bitcast i16* %365 to <8 x i16>*
  %368 = load <8 x i16>, <8 x i16>* %367, align 1
  %369 = getelementptr inbounds i16, i16* %341, i64 56
  %370 = bitcast i16* %369 to <2 x i64>*
  %371 = bitcast i16* %369 to <8 x i16>*
  %372 = load <8 x i16>, <8 x i16>* %371, align 1
  %373 = shufflevector <8 x i16> %344, <8 x i16> %348, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %374 = shufflevector <8 x i16> %352, <8 x i16> %356, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %375 = shufflevector <8 x i16> %360, <8 x i16> %364, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %376 = shufflevector <8 x i16> %368, <8 x i16> %372, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %377 = shufflevector <8 x i16> %344, <8 x i16> %348, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %378 = shufflevector <8 x i16> %352, <8 x i16> %356, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %379 = shufflevector <8 x i16> %360, <8 x i16> %364, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %380 = shufflevector <8 x i16> %368, <8 x i16> %372, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %381 = bitcast <8 x i16> %373 to <4 x i32>
  %382 = bitcast <8 x i16> %374 to <4 x i32>
  %383 = shufflevector <4 x i32> %381, <4 x i32> %382, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %384 = bitcast <4 x i32> %383 to <2 x i64>
  %385 = bitcast <8 x i16> %375 to <4 x i32>
  %386 = bitcast <8 x i16> %376 to <4 x i32>
  %387 = shufflevector <4 x i32> %385, <4 x i32> %386, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %388 = bitcast <4 x i32> %387 to <2 x i64>
  %389 = bitcast <8 x i16> %377 to <4 x i32>
  %390 = bitcast <8 x i16> %378 to <4 x i32>
  %391 = shufflevector <4 x i32> %389, <4 x i32> %390, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %392 = bitcast <4 x i32> %391 to <2 x i64>
  %393 = bitcast <8 x i16> %379 to <4 x i32>
  %394 = bitcast <8 x i16> %380 to <4 x i32>
  %395 = shufflevector <4 x i32> %393, <4 x i32> %394, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %396 = bitcast <4 x i32> %395 to <2 x i64>
  %397 = shufflevector <4 x i32> %381, <4 x i32> %382, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %398 = bitcast <4 x i32> %397 to <2 x i64>
  %399 = shufflevector <4 x i32> %385, <4 x i32> %386, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %400 = bitcast <4 x i32> %399 to <2 x i64>
  %401 = shufflevector <4 x i32> %389, <4 x i32> %390, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %402 = bitcast <4 x i32> %401 to <2 x i64>
  %403 = shufflevector <4 x i32> %393, <4 x i32> %394, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %404 = bitcast <4 x i32> %403 to <2 x i64>
  %405 = shufflevector <2 x i64> %384, <2 x i64> %388, <2 x i32> <i32 0, i32 2>
  %406 = shufflevector <2 x i64> %384, <2 x i64> %388, <2 x i32> <i32 1, i32 3>
  %407 = shufflevector <2 x i64> %398, <2 x i64> %400, <2 x i32> <i32 0, i32 2>
  %408 = shufflevector <2 x i64> %398, <2 x i64> %400, <2 x i32> <i32 1, i32 3>
  %409 = shufflevector <2 x i64> %392, <2 x i64> %396, <2 x i32> <i32 0, i32 2>
  %410 = shufflevector <2 x i64> %392, <2 x i64> %396, <2 x i32> <i32 1, i32 3>
  %411 = shufflevector <2 x i64> %402, <2 x i64> %404, <2 x i32> <i32 0, i32 2>
  %412 = shufflevector <2 x i64> %402, <2 x i64> %404, <2 x i32> <i32 1, i32 3>
  %413 = bitcast <2 x i64> %412 to <8 x i16>
  %414 = bitcast <2 x i64> %405 to <8 x i16>
  %415 = shufflevector <8 x i16> %413, <8 x i16> %414, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %416 = shufflevector <8 x i16> %414, <8 x i16> %413, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %417 = shufflevector <8 x i16> %413, <8 x i16> %414, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %418 = shufflevector <8 x i16> %414, <8 x i16> %413, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %419 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %415, <8 x i16> %154) #8
  %420 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %416, <8 x i16> <i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076>) #8
  %421 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %417, <8 x i16> %154) #8
  %422 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %418, <8 x i16> <i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076>) #8
  %423 = add <4 x i32> %419, <i32 2048, i32 2048, i32 2048, i32 2048>
  %424 = ashr <4 x i32> %423, <i32 12, i32 12, i32 12, i32 12>
  %425 = add <4 x i32> %420, <i32 2048, i32 2048, i32 2048, i32 2048>
  %426 = ashr <4 x i32> %425, <i32 12, i32 12, i32 12, i32 12>
  %427 = add <4 x i32> %421, <i32 2048, i32 2048, i32 2048, i32 2048>
  %428 = ashr <4 x i32> %427, <i32 12, i32 12, i32 12, i32 12>
  %429 = add <4 x i32> %422, <i32 2048, i32 2048, i32 2048, i32 2048>
  %430 = ashr <4 x i32> %429, <i32 12, i32 12, i32 12, i32 12>
  %431 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %424, <4 x i32> %428) #8
  %432 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %426, <4 x i32> %430) #8
  %433 = bitcast <2 x i64> %410 to <8 x i16>
  %434 = bitcast <2 x i64> %407 to <8 x i16>
  %435 = shufflevector <8 x i16> %433, <8 x i16> %434, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %436 = shufflevector <8 x i16> %434, <8 x i16> %433, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %437 = shufflevector <8 x i16> %433, <8 x i16> %434, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %438 = shufflevector <8 x i16> %434, <8 x i16> %433, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %439 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %435, <8 x i16> %155) #8
  %440 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %436, <8 x i16> <i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612>) #8
  %441 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %437, <8 x i16> %155) #8
  %442 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %438, <8 x i16> <i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612>) #8
  %443 = add <4 x i32> %439, <i32 2048, i32 2048, i32 2048, i32 2048>
  %444 = ashr <4 x i32> %443, <i32 12, i32 12, i32 12, i32 12>
  %445 = add <4 x i32> %440, <i32 2048, i32 2048, i32 2048, i32 2048>
  %446 = ashr <4 x i32> %445, <i32 12, i32 12, i32 12, i32 12>
  %447 = add <4 x i32> %441, <i32 2048, i32 2048, i32 2048, i32 2048>
  %448 = ashr <4 x i32> %447, <i32 12, i32 12, i32 12, i32 12>
  %449 = add <4 x i32> %442, <i32 2048, i32 2048, i32 2048, i32 2048>
  %450 = ashr <4 x i32> %449, <i32 12, i32 12, i32 12, i32 12>
  %451 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %444, <4 x i32> %448) #8
  %452 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %446, <4 x i32> %450) #8
  %453 = bitcast <2 x i64> %408 to <8 x i16>
  %454 = bitcast <2 x i64> %409 to <8 x i16>
  %455 = shufflevector <8 x i16> %453, <8 x i16> %454, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %456 = shufflevector <8 x i16> %454, <8 x i16> %453, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %457 = shufflevector <8 x i16> %453, <8 x i16> %454, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %458 = shufflevector <8 x i16> %454, <8 x i16> %453, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %459 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %455, <8 x i16> %156) #8
  %460 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %456, <8 x i16> <i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598>) #8
  %461 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %457, <8 x i16> %156) #8
  %462 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %458, <8 x i16> <i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598>) #8
  %463 = add <4 x i32> %459, <i32 2048, i32 2048, i32 2048, i32 2048>
  %464 = ashr <4 x i32> %463, <i32 12, i32 12, i32 12, i32 12>
  %465 = add <4 x i32> %460, <i32 2048, i32 2048, i32 2048, i32 2048>
  %466 = ashr <4 x i32> %465, <i32 12, i32 12, i32 12, i32 12>
  %467 = add <4 x i32> %461, <i32 2048, i32 2048, i32 2048, i32 2048>
  %468 = ashr <4 x i32> %467, <i32 12, i32 12, i32 12, i32 12>
  %469 = add <4 x i32> %462, <i32 2048, i32 2048, i32 2048, i32 2048>
  %470 = ashr <4 x i32> %469, <i32 12, i32 12, i32 12, i32 12>
  %471 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %464, <4 x i32> %468) #8
  %472 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %466, <4 x i32> %470) #8
  %473 = bitcast <2 x i64> %406 to <8 x i16>
  %474 = bitcast <2 x i64> %411 to <8 x i16>
  %475 = shufflevector <8 x i16> %473, <8 x i16> %474, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %476 = shufflevector <8 x i16> %474, <8 x i16> %473, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %477 = shufflevector <8 x i16> %473, <8 x i16> %474, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %478 = shufflevector <8 x i16> %474, <8 x i16> %473, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %479 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %475, <8 x i16> %157) #8
  %480 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %476, <8 x i16> <i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189>) #8
  %481 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %477, <8 x i16> %157) #8
  %482 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %478, <8 x i16> <i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189>) #8
  %483 = add <4 x i32> %479, <i32 2048, i32 2048, i32 2048, i32 2048>
  %484 = ashr <4 x i32> %483, <i32 12, i32 12, i32 12, i32 12>
  %485 = add <4 x i32> %480, <i32 2048, i32 2048, i32 2048, i32 2048>
  %486 = ashr <4 x i32> %485, <i32 12, i32 12, i32 12, i32 12>
  %487 = add <4 x i32> %481, <i32 2048, i32 2048, i32 2048, i32 2048>
  %488 = ashr <4 x i32> %487, <i32 12, i32 12, i32 12, i32 12>
  %489 = add <4 x i32> %482, <i32 2048, i32 2048, i32 2048, i32 2048>
  %490 = ashr <4 x i32> %489, <i32 12, i32 12, i32 12, i32 12>
  %491 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %484, <4 x i32> %488) #8
  %492 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %486, <4 x i32> %490) #8
  %493 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %432, <8 x i16> %472) #8
  %494 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %432, <8 x i16> %472) #8
  %495 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %431, <8 x i16> %471) #8
  %496 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %431, <8 x i16> %471) #8
  %497 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %452, <8 x i16> %492) #8
  %498 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %452, <8 x i16> %492) #8
  %499 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %451, <8 x i16> %491) #8
  %500 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %451, <8 x i16> %491) #8
  %501 = shufflevector <8 x i16> %494, <8 x i16> %496, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %502 = shufflevector <8 x i16> %496, <8 x i16> %494, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %503 = shufflevector <8 x i16> %494, <8 x i16> %496, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %504 = shufflevector <8 x i16> %496, <8 x i16> %494, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %505 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %501, <8 x i16> %158) #8
  %506 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %502, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %507 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %503, <8 x i16> %158) #8
  %508 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %504, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %509 = add <4 x i32> %505, <i32 2048, i32 2048, i32 2048, i32 2048>
  %510 = ashr <4 x i32> %509, <i32 12, i32 12, i32 12, i32 12>
  %511 = add <4 x i32> %506, <i32 2048, i32 2048, i32 2048, i32 2048>
  %512 = ashr <4 x i32> %511, <i32 12, i32 12, i32 12, i32 12>
  %513 = add <4 x i32> %507, <i32 2048, i32 2048, i32 2048, i32 2048>
  %514 = ashr <4 x i32> %513, <i32 12, i32 12, i32 12, i32 12>
  %515 = add <4 x i32> %508, <i32 2048, i32 2048, i32 2048, i32 2048>
  %516 = ashr <4 x i32> %515, <i32 12, i32 12, i32 12, i32 12>
  %517 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %510, <4 x i32> %514) #8
  %518 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %512, <4 x i32> %516) #8
  %519 = shufflevector <8 x i16> %500, <8 x i16> %498, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %520 = shufflevector <8 x i16> %498, <8 x i16> %500, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %521 = shufflevector <8 x i16> %500, <8 x i16> %498, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %522 = shufflevector <8 x i16> %498, <8 x i16> %500, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %523 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %519, <8 x i16> %159) #8
  %524 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %520, <8 x i16> <i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567>) #8
  %525 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %521, <8 x i16> %159) #8
  %526 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %522, <8 x i16> <i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567>) #8
  %527 = add <4 x i32> %523, <i32 2048, i32 2048, i32 2048, i32 2048>
  %528 = ashr <4 x i32> %527, <i32 12, i32 12, i32 12, i32 12>
  %529 = add <4 x i32> %524, <i32 2048, i32 2048, i32 2048, i32 2048>
  %530 = ashr <4 x i32> %529, <i32 12, i32 12, i32 12, i32 12>
  %531 = add <4 x i32> %525, <i32 2048, i32 2048, i32 2048, i32 2048>
  %532 = ashr <4 x i32> %531, <i32 12, i32 12, i32 12, i32 12>
  %533 = add <4 x i32> %526, <i32 2048, i32 2048, i32 2048, i32 2048>
  %534 = ashr <4 x i32> %533, <i32 12, i32 12, i32 12, i32 12>
  %535 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %528, <4 x i32> %532) #8
  %536 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %530, <4 x i32> %534) #8
  %537 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %493, <8 x i16> %497) #8
  %538 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %493, <8 x i16> %497) #8
  %539 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %518, <8 x i16> %535) #8
  %540 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %518, <8 x i16> %535) #8
  %541 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %495, <8 x i16> %499) #8
  %542 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %495, <8 x i16> %499) #8
  %543 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %517, <8 x i16> %536) #8
  %544 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %517, <8 x i16> %536) #8
  %545 = shufflevector <8 x i16> %538, <8 x i16> %542, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %546 = shufflevector <8 x i16> %542, <8 x i16> %538, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %547 = shufflevector <8 x i16> %538, <8 x i16> %542, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %548 = shufflevector <8 x i16> %542, <8 x i16> %538, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %549 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %545, <8 x i16> %160) #8
  %550 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %546, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %551 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %547, <8 x i16> %160) #8
  %552 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %548, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %553 = add <4 x i32> %549, <i32 2048, i32 2048, i32 2048, i32 2048>
  %554 = ashr <4 x i32> %553, <i32 12, i32 12, i32 12, i32 12>
  %555 = add <4 x i32> %550, <i32 2048, i32 2048, i32 2048, i32 2048>
  %556 = ashr <4 x i32> %555, <i32 12, i32 12, i32 12, i32 12>
  %557 = add <4 x i32> %551, <i32 2048, i32 2048, i32 2048, i32 2048>
  %558 = ashr <4 x i32> %557, <i32 12, i32 12, i32 12, i32 12>
  %559 = add <4 x i32> %552, <i32 2048, i32 2048, i32 2048, i32 2048>
  %560 = ashr <4 x i32> %559, <i32 12, i32 12, i32 12, i32 12>
  %561 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %554, <4 x i32> %558) #8
  %562 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %556, <4 x i32> %560) #8
  %563 = shufflevector <8 x i16> %540, <8 x i16> %544, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %564 = shufflevector <8 x i16> %544, <8 x i16> %540, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %565 = shufflevector <8 x i16> %540, <8 x i16> %544, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %566 = shufflevector <8 x i16> %544, <8 x i16> %540, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %567 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %563, <8 x i16> %160) #8
  %568 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %564, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %569 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %565, <8 x i16> %160) #8
  %570 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %566, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %571 = add <4 x i32> %567, <i32 2048, i32 2048, i32 2048, i32 2048>
  %572 = ashr <4 x i32> %571, <i32 12, i32 12, i32 12, i32 12>
  %573 = add <4 x i32> %568, <i32 2048, i32 2048, i32 2048, i32 2048>
  %574 = ashr <4 x i32> %573, <i32 12, i32 12, i32 12, i32 12>
  %575 = add <4 x i32> %569, <i32 2048, i32 2048, i32 2048, i32 2048>
  %576 = ashr <4 x i32> %575, <i32 12, i32 12, i32 12, i32 12>
  %577 = add <4 x i32> %570, <i32 2048, i32 2048, i32 2048, i32 2048>
  %578 = ashr <4 x i32> %577, <i32 12, i32 12, i32 12, i32 12>
  %579 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %572, <4 x i32> %576) #8
  %580 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %574, <4 x i32> %578) #8
  %581 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %539) #8
  %582 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %562) #8
  %583 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %579) #8
  %584 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %541) #8
  %585 = shufflevector <8 x i16> %537, <8 x i16> %581, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %586 = shufflevector <8 x i16> %580, <8 x i16> %582, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %587 = shufflevector <8 x i16> %561, <8 x i16> %583, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %588 = shufflevector <8 x i16> %543, <8 x i16> %584, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %589 = shufflevector <8 x i16> %537, <8 x i16> %581, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %590 = shufflevector <8 x i16> %580, <8 x i16> %582, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %591 = shufflevector <8 x i16> %561, <8 x i16> %583, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %592 = shufflevector <8 x i16> %543, <8 x i16> %584, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %593 = bitcast <8 x i16> %585 to <4 x i32>
  %594 = bitcast <8 x i16> %586 to <4 x i32>
  %595 = shufflevector <4 x i32> %593, <4 x i32> %594, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %596 = bitcast <4 x i32> %595 to <2 x i64>
  %597 = bitcast <8 x i16> %587 to <4 x i32>
  %598 = bitcast <8 x i16> %588 to <4 x i32>
  %599 = shufflevector <4 x i32> %597, <4 x i32> %598, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %600 = bitcast <4 x i32> %599 to <2 x i64>
  %601 = bitcast <8 x i16> %589 to <4 x i32>
  %602 = bitcast <8 x i16> %590 to <4 x i32>
  %603 = shufflevector <4 x i32> %601, <4 x i32> %602, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %604 = bitcast <4 x i32> %603 to <2 x i64>
  %605 = bitcast <8 x i16> %591 to <4 x i32>
  %606 = bitcast <8 x i16> %592 to <4 x i32>
  %607 = shufflevector <4 x i32> %605, <4 x i32> %606, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %608 = bitcast <4 x i32> %607 to <2 x i64>
  %609 = shufflevector <4 x i32> %593, <4 x i32> %594, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %610 = bitcast <4 x i32> %609 to <2 x i64>
  %611 = shufflevector <4 x i32> %597, <4 x i32> %598, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %612 = bitcast <4 x i32> %611 to <2 x i64>
  %613 = shufflevector <4 x i32> %601, <4 x i32> %602, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %614 = bitcast <4 x i32> %613 to <2 x i64>
  %615 = shufflevector <4 x i32> %605, <4 x i32> %606, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %616 = bitcast <4 x i32> %615 to <2 x i64>
  %617 = shufflevector <2 x i64> %596, <2 x i64> %600, <2 x i32> <i32 0, i32 2>
  %618 = shufflevector <2 x i64> %596, <2 x i64> %600, <2 x i32> <i32 1, i32 3>
  %619 = shufflevector <2 x i64> %610, <2 x i64> %612, <2 x i32> <i32 0, i32 2>
  %620 = shufflevector <2 x i64> %610, <2 x i64> %612, <2 x i32> <i32 1, i32 3>
  %621 = shufflevector <2 x i64> %604, <2 x i64> %608, <2 x i32> <i32 0, i32 2>
  %622 = shufflevector <2 x i64> %604, <2 x i64> %608, <2 x i32> <i32 1, i32 3>
  %623 = shufflevector <2 x i64> %614, <2 x i64> %616, <2 x i32> <i32 0, i32 2>
  %624 = shufflevector <2 x i64> %614, <2 x i64> %616, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %617, <2 x i64>* %342, align 1
  store <2 x i64> %618, <2 x i64>* %346, align 1
  store <2 x i64> %619, <2 x i64>* %350, align 1
  store <2 x i64> %620, <2 x i64>* %354, align 1
  store <2 x i64> %621, <2 x i64>* %358, align 1
  store <2 x i64> %622, <2 x i64>* %362, align 1
  store <2 x i64> %623, <2 x i64>* %366, align 1
  store <2 x i64> %624, <2 x i64>* %370, align 1
  %625 = add nuw nsw i64 %338, 8
  %626 = icmp slt i64 %625, %161
  br i1 %626, label %337, label %627

627:                                              ; preds = %337, %162
  %628 = lshr i64 524276, %9
  %629 = and i64 %628, 1
  %630 = icmp eq i64 %629, 0
  br i1 %630, label %653, label %631

631:                                              ; preds = %627
  %632 = zext i8 %14 to i16
  %633 = insertelement <8 x i16> undef, i16 %632, i32 0
  %634 = shufflevector <8 x i16> %633, <8 x i16> undef, <8 x i32> zeroinitializer
  %635 = shufflevector <8 x i16> %633, <8 x i16> undef, <2 x i32> zeroinitializer
  %636 = zext <2 x i16> %635 to <2 x i64>
  %637 = bitcast <2 x i64> %636 to <8 x i16>
  %638 = sext i32 %2 to i64
  br label %639

639:                                              ; preds = %639, %631
  %640 = phi i64 [ %651, %639 ], [ 0, %631 ]
  %641 = shl i64 %640, 3
  %642 = and i64 %641, 4294967288
  %643 = getelementptr inbounds i16, i16* %8, i64 %642
  %644 = bitcast i16* %643 to <8 x i16>*
  %645 = load <8 x i16>, <8 x i16>* %644, align 1
  %646 = icmp sgt <8 x i16> %645, <i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765>
  %647 = add <8 x i16> %645, %634
  %648 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %647, <8 x i16> %637) #8
  %649 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %647, <8 x i16> %637) #8
  %650 = select <8 x i1> %646, <8 x i16> %649, <8 x i16> %648
  store <8 x i16> %650, <8 x i16>* %644, align 1
  %651 = add nuw nsw i64 %640, 1
  %652 = icmp slt i64 %651, %638
  br i1 %652, label %639, label %653

653:                                              ; preds = %639, %16, %627
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_131Adst8TransformLoopColumn_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_(i8 zeroext, i8 zeroext, i32, i8* nocapture, i32, i32, i8* nocapture readonly) #4 {
  %8 = bitcast i8* %3 to i16*
  %9 = zext i8 %1 to i64
  %10 = getelementptr inbounds [19 x i8], [19 x i8]* @_ZN7libgav115kTransformWidthE, i64 0, i64 %9
  %11 = load i8, i8* %10, align 1
  %12 = zext i8 %11 to i32
  %13 = zext i8 %0 to i32
  %14 = shl i32 1, %13
  %15 = and i32 %14, 33104
  %16 = icmp eq i32 %15, 0
  br i1 %16, label %83, label %17

17:                                               ; preds = %7
  %18 = icmp ugt i8 %11, 15
  br i1 %18, label %19, label %35

19:                                               ; preds = %17
  %20 = shl nuw nsw i32 %12, 3
  %21 = zext i32 %20 to i64
  br label %22

22:                                               ; preds = %22, %19
  %23 = phi i64 [ 0, %19 ], [ %33, %22 ]
  %24 = getelementptr inbounds i16, i16* %8, i64 %23
  %25 = bitcast i16* %24 to <16 x i8>*
  %26 = load <16 x i8>, <16 x i8>* %25, align 1
  %27 = or i64 %23, 8
  %28 = getelementptr inbounds i16, i16* %8, i64 %27
  %29 = bitcast i16* %28 to <16 x i8>*
  %30 = load <16 x i8>, <16 x i8>* %29, align 1
  %31 = shufflevector <16 x i8> %26, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  %32 = shufflevector <16 x i8> %30, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %32, <16 x i8>* %25, align 1
  store <16 x i8> %31, <16 x i8>* %29, align 1
  %33 = add nuw nsw i64 %23, 16
  %34 = icmp ult i64 %33, %21
  br i1 %34, label %22, label %83

35:                                               ; preds = %17
  %36 = icmp eq i8 %11, 8
  %37 = bitcast i8* %3 to <16 x i8>*
  %38 = load <16 x i8>, <16 x i8>* %37, align 1
  br i1 %36, label %53, label %39

39:                                               ; preds = %35
  %40 = shufflevector <16 x i8> %38, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %40, <16 x i8>* %37, align 1
  %41 = getelementptr inbounds i8, i8* %3, i64 16
  %42 = bitcast i8* %41 to <16 x i8>*
  %43 = load <16 x i8>, <16 x i8>* %42, align 1
  %44 = shufflevector <16 x i8> %43, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %44, <16 x i8>* %42, align 1
  %45 = getelementptr inbounds i8, i8* %3, i64 32
  %46 = bitcast i8* %45 to <16 x i8>*
  %47 = load <16 x i8>, <16 x i8>* %46, align 1
  %48 = shufflevector <16 x i8> %47, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %48, <16 x i8>* %46, align 1
  %49 = getelementptr inbounds i8, i8* %3, i64 48
  %50 = bitcast i8* %49 to <16 x i8>*
  %51 = load <16 x i8>, <16 x i8>* %50, align 1
  %52 = shufflevector <16 x i8> %51, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %52, <16 x i8>* %50, align 1
  br label %83

53:                                               ; preds = %35
  %54 = shufflevector <16 x i8> %38, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %54, <16 x i8>* %37, align 1
  %55 = getelementptr inbounds i8, i8* %3, i64 16
  %56 = bitcast i8* %55 to <16 x i8>*
  %57 = load <16 x i8>, <16 x i8>* %56, align 1
  %58 = shufflevector <16 x i8> %57, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %58, <16 x i8>* %56, align 1
  %59 = getelementptr inbounds i8, i8* %3, i64 32
  %60 = bitcast i8* %59 to <16 x i8>*
  %61 = load <16 x i8>, <16 x i8>* %60, align 1
  %62 = shufflevector <16 x i8> %61, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %62, <16 x i8>* %60, align 1
  %63 = getelementptr inbounds i8, i8* %3, i64 48
  %64 = bitcast i8* %63 to <16 x i8>*
  %65 = load <16 x i8>, <16 x i8>* %64, align 1
  %66 = shufflevector <16 x i8> %65, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %66, <16 x i8>* %64, align 1
  %67 = getelementptr inbounds i8, i8* %3, i64 64
  %68 = bitcast i8* %67 to <16 x i8>*
  %69 = load <16 x i8>, <16 x i8>* %68, align 1
  %70 = shufflevector <16 x i8> %69, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %70, <16 x i8>* %68, align 1
  %71 = getelementptr inbounds i8, i8* %3, i64 80
  %72 = bitcast i8* %71 to <16 x i8>*
  %73 = load <16 x i8>, <16 x i8>* %72, align 1
  %74 = shufflevector <16 x i8> %73, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %74, <16 x i8>* %72, align 1
  %75 = getelementptr inbounds i8, i8* %3, i64 96
  %76 = bitcast i8* %75 to <16 x i8>*
  %77 = load <16 x i8>, <16 x i8>* %76, align 1
  %78 = shufflevector <16 x i8> %77, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %78, <16 x i8>* %76, align 1
  %79 = getelementptr inbounds i8, i8* %3, i64 112
  %80 = bitcast i8* %79 to <16 x i8>*
  %81 = load <16 x i8>, <16 x i8>* %80, align 1
  %82 = shufflevector <16 x i8> %81, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %82, <16 x i8>* %80, align 1
  br label %83

83:                                               ; preds = %22, %7, %53, %39
  %84 = icmp sgt i32 %2, 1
  br i1 %84, label %169, label %85

85:                                               ; preds = %83
  %86 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %87 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %88 = zext i8 %11 to i64
  %89 = shl nuw nsw i64 %88, 1
  %90 = mul nuw nsw i64 %88, 3
  %91 = shl nuw nsw i64 %88, 2
  %92 = mul nuw nsw i64 %88, 5
  %93 = mul nuw nsw i64 %88, 6
  %94 = mul nuw nsw i64 %88, 7
  br label %95

95:                                               ; preds = %95, %85
  %96 = phi i32 [ 0, %85 ], [ %166, %95 ]
  %97 = phi i16* [ %8, %85 ], [ %167, %95 ]
  %98 = bitcast i16* %97 to i64*
  %99 = load i64, i64* %98, align 1
  %100 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %99, i32 0
  %101 = bitcast <2 x i64> %100 to <8 x i16>
  %102 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %101, <8 x i16> <i16 -32608, i16 -32608, i16 -32608, i16 -32608, i16 -32608, i16 -32608, i16 -32608, i16 -32608>) #8
  %103 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %101, <8 x i16> <i16 3208, i16 3208, i16 3208, i16 3208, i16 3208, i16 3208, i16 3208, i16 3208>) #8
  %104 = shufflevector <8 x i16> %103, <8 x i16> %102, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %105 = shufflevector <8 x i16> %102, <8 x i16> %103, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %106 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %104, <8 x i16> %86) #8
  %107 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %105, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %108 = add <4 x i32> %106, <i32 2048, i32 2048, i32 2048, i32 2048>
  %109 = ashr <4 x i32> %108, <i32 12, i32 12, i32 12, i32 12>
  %110 = add <4 x i32> %107, <i32 2048, i32 2048, i32 2048, i32 2048>
  %111 = ashr <4 x i32> %110, <i32 12, i32 12, i32 12, i32 12>
  %112 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %109, <4 x i32> %109) #8
  %113 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %111, <4 x i32> %111) #8
  %114 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %104, <8 x i16> %87) #8
  %115 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %105, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %116 = add <4 x i32> %114, <i32 2048, i32 2048, i32 2048, i32 2048>
  %117 = ashr <4 x i32> %116, <i32 12, i32 12, i32 12, i32 12>
  %118 = add <4 x i32> %115, <i32 2048, i32 2048, i32 2048, i32 2048>
  %119 = ashr <4 x i32> %118, <i32 12, i32 12, i32 12, i32 12>
  %120 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %117, <4 x i32> undef) #8
  %121 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %119, <4 x i32> %119) #8
  %122 = shufflevector <8 x i16> %113, <8 x i16> %112, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %123 = shufflevector <8 x i16> %112, <8 x i16> %113, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %124 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %122, <8 x i16> %87) #8
  %125 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %123, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %126 = add <4 x i32> %124, <i32 2048, i32 2048, i32 2048, i32 2048>
  %127 = ashr <4 x i32> %126, <i32 12, i32 12, i32 12, i32 12>
  %128 = add <4 x i32> %125, <i32 2048, i32 2048, i32 2048, i32 2048>
  %129 = ashr <4 x i32> %128, <i32 12, i32 12, i32 12, i32 12>
  %130 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %127, <4 x i32> %127) #8
  %131 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %129, <4 x i32> undef) #8
  %132 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %113) #8
  %133 = bitcast <8 x i16> %132 to <2 x i64>
  %134 = bitcast <8 x i16> %131 to <2 x i64>
  %135 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %121) #8
  %136 = bitcast <8 x i16> %135 to <2 x i64>
  %137 = bitcast <8 x i16> %120 to <2 x i64>
  %138 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %130) #8
  %139 = bitcast <8 x i16> %138 to <2 x i64>
  %140 = bitcast <8 x i16> %112 to <2 x i64>
  %141 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %102) #8
  %142 = bitcast <8 x i16> %141 to <2 x i64>
  %143 = bitcast <8 x i16> %103 to <2 x i64>
  %144 = extractelement <2 x i64> %143, i32 0
  store i64 %144, i64* %98, align 1
  %145 = getelementptr inbounds i16, i16* %97, i64 %88
  %146 = extractelement <2 x i64> %133, i32 0
  %147 = bitcast i16* %145 to i64*
  store i64 %146, i64* %147, align 1
  %148 = getelementptr inbounds i16, i16* %97, i64 %89
  %149 = extractelement <2 x i64> %134, i32 0
  %150 = bitcast i16* %148 to i64*
  store i64 %149, i64* %150, align 1
  %151 = getelementptr inbounds i16, i16* %97, i64 %90
  %152 = extractelement <2 x i64> %136, i32 0
  %153 = bitcast i16* %151 to i64*
  store i64 %152, i64* %153, align 1
  %154 = getelementptr inbounds i16, i16* %97, i64 %91
  %155 = extractelement <2 x i64> %137, i32 0
  %156 = bitcast i16* %154 to i64*
  store i64 %155, i64* %156, align 1
  %157 = getelementptr inbounds i16, i16* %97, i64 %92
  %158 = extractelement <2 x i64> %139, i32 0
  %159 = bitcast i16* %157 to i64*
  store i64 %158, i64* %159, align 1
  %160 = getelementptr inbounds i16, i16* %97, i64 %93
  %161 = extractelement <2 x i64> %140, i32 0
  %162 = bitcast i16* %160 to i64*
  store i64 %161, i64* %162, align 1
  %163 = getelementptr inbounds i16, i16* %97, i64 %94
  %164 = extractelement <2 x i64> %142, i32 0
  %165 = bitcast i16* %163 to i64*
  store i64 %164, i64* %165, align 1
  %166 = add nuw nsw i32 %96, 4
  %167 = getelementptr inbounds i16, i16* %97, i64 4
  %168 = icmp ult i32 %166, %12
  br i1 %168, label %95, label %555

169:                                              ; preds = %83
  %170 = icmp eq i8 %11, 4
  br i1 %170, label %187, label %171

171:                                              ; preds = %169
  %172 = zext i8 %11 to i64
  %173 = shl nuw nsw i64 %172, 1
  %174 = mul nuw nsw i64 %172, 3
  %175 = shl nuw nsw i64 %172, 2
  %176 = mul nuw nsw i64 %172, 5
  %177 = mul nuw nsw i64 %172, 6
  %178 = mul nuw nsw i64 %172, 7
  %179 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %180 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %181 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %182 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %183 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %184 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %185 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %186 = zext i8 %11 to i64
  br label %363

187:                                              ; preds = %169
  %188 = bitcast i8* %3 to i64*
  %189 = load i64, i64* %188, align 1
  %190 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %189, i32 0
  %191 = getelementptr inbounds i8, i8* %3, i64 8
  %192 = bitcast i8* %191 to i64*
  %193 = load i64, i64* %192, align 1
  %194 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %193, i32 0
  %195 = getelementptr inbounds i8, i8* %3, i64 16
  %196 = bitcast i8* %195 to i64*
  %197 = load i64, i64* %196, align 1
  %198 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %197, i32 0
  %199 = getelementptr inbounds i8, i8* %3, i64 24
  %200 = bitcast i8* %199 to i64*
  %201 = load i64, i64* %200, align 1
  %202 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %201, i32 0
  %203 = getelementptr inbounds i8, i8* %3, i64 32
  %204 = bitcast i8* %203 to i64*
  %205 = load i64, i64* %204, align 1
  %206 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %205, i32 0
  %207 = getelementptr inbounds i8, i8* %3, i64 40
  %208 = bitcast i8* %207 to i64*
  %209 = load i64, i64* %208, align 1
  %210 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %209, i32 0
  %211 = getelementptr inbounds i8, i8* %3, i64 48
  %212 = bitcast i8* %211 to i64*
  %213 = load i64, i64* %212, align 1
  %214 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %213, i32 0
  %215 = getelementptr inbounds i8, i8* %3, i64 56
  %216 = bitcast i8* %215 to i64*
  %217 = load i64, i64* %216, align 1
  %218 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %217, i32 0
  %219 = bitcast <2 x i64> %218 to <8 x i16>
  %220 = bitcast <2 x i64> %190 to <8 x i16>
  %221 = shufflevector <8 x i16> %219, <8 x i16> %220, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %222 = shufflevector <8 x i16> %220, <8 x i16> %219, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %223 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %224 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %221, <8 x i16> %223) #8
  %225 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %222, <8 x i16> <i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076>) #8
  %226 = add <4 x i32> %224, <i32 2048, i32 2048, i32 2048, i32 2048>
  %227 = ashr <4 x i32> %226, <i32 12, i32 12, i32 12, i32 12>
  %228 = add <4 x i32> %225, <i32 2048, i32 2048, i32 2048, i32 2048>
  %229 = ashr <4 x i32> %228, <i32 12, i32 12, i32 12, i32 12>
  %230 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %227, <4 x i32> %227) #8
  %231 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %229, <4 x i32> %229) #8
  %232 = bitcast <2 x i64> %210 to <8 x i16>
  %233 = bitcast <2 x i64> %198 to <8 x i16>
  %234 = shufflevector <8 x i16> %232, <8 x i16> %233, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %235 = shufflevector <8 x i16> %233, <8 x i16> %232, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %236 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %237 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %234, <8 x i16> %236) #8
  %238 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %235, <8 x i16> <i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612>) #8
  %239 = add <4 x i32> %237, <i32 2048, i32 2048, i32 2048, i32 2048>
  %240 = ashr <4 x i32> %239, <i32 12, i32 12, i32 12, i32 12>
  %241 = add <4 x i32> %238, <i32 2048, i32 2048, i32 2048, i32 2048>
  %242 = ashr <4 x i32> %241, <i32 12, i32 12, i32 12, i32 12>
  %243 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %240, <4 x i32> %240) #8
  %244 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %242, <4 x i32> %242) #8
  %245 = bitcast <2 x i64> %202 to <8 x i16>
  %246 = bitcast <2 x i64> %206 to <8 x i16>
  %247 = shufflevector <8 x i16> %245, <8 x i16> %246, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %248 = shufflevector <8 x i16> %246, <8 x i16> %245, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %249 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %250 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %247, <8 x i16> %249) #8
  %251 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %248, <8 x i16> <i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598>) #8
  %252 = add <4 x i32> %250, <i32 2048, i32 2048, i32 2048, i32 2048>
  %253 = ashr <4 x i32> %252, <i32 12, i32 12, i32 12, i32 12>
  %254 = add <4 x i32> %251, <i32 2048, i32 2048, i32 2048, i32 2048>
  %255 = ashr <4 x i32> %254, <i32 12, i32 12, i32 12, i32 12>
  %256 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %253, <4 x i32> %253) #8
  %257 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %255, <4 x i32> %255) #8
  %258 = bitcast <2 x i64> %194 to <8 x i16>
  %259 = bitcast <2 x i64> %214 to <8 x i16>
  %260 = shufflevector <8 x i16> %258, <8 x i16> %259, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %261 = shufflevector <8 x i16> %259, <8 x i16> %258, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %262 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %263 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %260, <8 x i16> %262) #8
  %264 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %261, <8 x i16> <i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189>) #8
  %265 = add <4 x i32> %263, <i32 2048, i32 2048, i32 2048, i32 2048>
  %266 = ashr <4 x i32> %265, <i32 12, i32 12, i32 12, i32 12>
  %267 = add <4 x i32> %264, <i32 2048, i32 2048, i32 2048, i32 2048>
  %268 = ashr <4 x i32> %267, <i32 12, i32 12, i32 12, i32 12>
  %269 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %266, <4 x i32> %266) #8
  %270 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %268, <4 x i32> %268) #8
  %271 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %231, <8 x i16> %257) #8
  %272 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %231, <8 x i16> %257) #8
  %273 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %230, <8 x i16> %256) #8
  %274 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %230, <8 x i16> %256) #8
  %275 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %244, <8 x i16> %270) #8
  %276 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %244, <8 x i16> %270) #8
  %277 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %243, <8 x i16> %269) #8
  %278 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %243, <8 x i16> %269) #8
  %279 = shufflevector <8 x i16> %272, <8 x i16> %274, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %280 = shufflevector <8 x i16> %274, <8 x i16> %272, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %281 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %282 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %279, <8 x i16> %281) #8
  %283 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %280, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %284 = add <4 x i32> %282, <i32 2048, i32 2048, i32 2048, i32 2048>
  %285 = ashr <4 x i32> %284, <i32 12, i32 12, i32 12, i32 12>
  %286 = add <4 x i32> %283, <i32 2048, i32 2048, i32 2048, i32 2048>
  %287 = ashr <4 x i32> %286, <i32 12, i32 12, i32 12, i32 12>
  %288 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %285, <4 x i32> %285) #8
  %289 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %287, <4 x i32> %287) #8
  %290 = shufflevector <8 x i16> %278, <8 x i16> %276, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %291 = shufflevector <8 x i16> %276, <8 x i16> %278, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %292 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %293 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %290, <8 x i16> %292) #8
  %294 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %291, <8 x i16> <i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567>) #8
  %295 = add <4 x i32> %293, <i32 2048, i32 2048, i32 2048, i32 2048>
  %296 = ashr <4 x i32> %295, <i32 12, i32 12, i32 12, i32 12>
  %297 = add <4 x i32> %294, <i32 2048, i32 2048, i32 2048, i32 2048>
  %298 = ashr <4 x i32> %297, <i32 12, i32 12, i32 12, i32 12>
  %299 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %296, <4 x i32> %296) #8
  %300 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %298, <4 x i32> %298) #8
  %301 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %271, <8 x i16> %275) #8
  %302 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %271, <8 x i16> %275) #8
  %303 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %289, <8 x i16> %299) #8
  %304 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %289, <8 x i16> %299) #8
  %305 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %273, <8 x i16> %277) #8
  %306 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %273, <8 x i16> %277) #8
  %307 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %288, <8 x i16> %300) #8
  %308 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %288, <8 x i16> %300) #8
  %309 = shufflevector <8 x i16> %302, <8 x i16> %306, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %310 = shufflevector <8 x i16> %306, <8 x i16> %302, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %311 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %312 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %309, <8 x i16> %311) #8
  %313 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %310, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %314 = add <4 x i32> %312, <i32 2048, i32 2048, i32 2048, i32 2048>
  %315 = ashr <4 x i32> %314, <i32 12, i32 12, i32 12, i32 12>
  %316 = add <4 x i32> %313, <i32 2048, i32 2048, i32 2048, i32 2048>
  %317 = ashr <4 x i32> %316, <i32 12, i32 12, i32 12, i32 12>
  %318 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %315, <4 x i32> undef) #8
  %319 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %317, <4 x i32> %317) #8
  %320 = shufflevector <8 x i16> %304, <8 x i16> %308, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %321 = shufflevector <8 x i16> %308, <8 x i16> %304, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %322 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %320, <8 x i16> %311) #8
  %323 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %321, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %324 = add <4 x i32> %322, <i32 2048, i32 2048, i32 2048, i32 2048>
  %325 = ashr <4 x i32> %324, <i32 12, i32 12, i32 12, i32 12>
  %326 = add <4 x i32> %323, <i32 2048, i32 2048, i32 2048, i32 2048>
  %327 = ashr <4 x i32> %326, <i32 12, i32 12, i32 12, i32 12>
  %328 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %325, <4 x i32> %325) #8
  %329 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %327, <4 x i32> undef) #8
  %330 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %303) #8
  %331 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %319) #8
  %332 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %328) #8
  %333 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %305) #8
  %334 = bitcast <8 x i16> %333 to <2 x i64>
  %335 = bitcast <8 x i16> %332 to <2 x i64>
  %336 = bitcast <8 x i16> %331 to <2 x i64>
  %337 = bitcast <8 x i16> %330 to <2 x i64>
  %338 = bitcast <8 x i16> %329 to <2 x i64>
  %339 = bitcast <8 x i16> %318 to <2 x i64>
  %340 = bitcast <8 x i16> %307 to <2 x i64>
  %341 = bitcast <8 x i16> %301 to <2 x i64>
  %342 = extractelement <2 x i64> %341, i32 0
  store i64 %342, i64* %188, align 1
  %343 = extractelement <2 x i64> %337, i32 0
  store i64 %343, i64* %192, align 1
  %344 = extractelement <2 x i64> %338, i32 0
  store i64 %344, i64* %196, align 1
  %345 = extractelement <2 x i64> %336, i32 0
  store i64 %345, i64* %200, align 1
  %346 = extractelement <2 x i64> %339, i32 0
  store i64 %346, i64* %204, align 1
  %347 = extractelement <2 x i64> %335, i32 0
  store i64 %347, i64* %208, align 1
  %348 = extractelement <2 x i64> %340, i32 0
  store i64 %348, i64* %212, align 1
  %349 = extractelement <2 x i64> %334, i32 0
  store i64 %349, i64* %216, align 1
  %350 = bitcast i8* %6 to i64*
  %351 = load i64, i64* %350, align 8
  %352 = getelementptr inbounds i8, i8* %6, i64 8
  %353 = bitcast i8* %352 to i8**
  %354 = load i8*, i8** %353, align 8
  %355 = and i32 %14, 16608
  %356 = icmp ne i32 %355, 0
  %357 = sext i32 %5 to i64
  %358 = ashr i64 %351, 32
  %359 = mul nsw i64 %358, %357
  %360 = getelementptr inbounds i8, i8* %354, i64 %359
  %361 = sext i32 %4 to i64
  %362 = getelementptr inbounds i8, i8* %360, i64 %361
  br label %705

363:                                              ; preds = %171, %363
  %364 = phi i64 [ 0, %171 ], [ %553, %363 ]
  %365 = getelementptr inbounds i16, i16* %8, i64 %364
  %366 = bitcast i16* %365 to <8 x i16>*
  %367 = load <8 x i16>, <8 x i16>* %366, align 1
  %368 = getelementptr inbounds i16, i16* %365, i64 %172
  %369 = bitcast i16* %368 to <8 x i16>*
  %370 = load <8 x i16>, <8 x i16>* %369, align 1
  %371 = getelementptr inbounds i16, i16* %365, i64 %173
  %372 = bitcast i16* %371 to <8 x i16>*
  %373 = load <8 x i16>, <8 x i16>* %372, align 1
  %374 = getelementptr inbounds i16, i16* %365, i64 %174
  %375 = bitcast i16* %374 to <8 x i16>*
  %376 = load <8 x i16>, <8 x i16>* %375, align 1
  %377 = getelementptr inbounds i16, i16* %365, i64 %175
  %378 = bitcast i16* %377 to <8 x i16>*
  %379 = load <8 x i16>, <8 x i16>* %378, align 1
  %380 = getelementptr inbounds i16, i16* %365, i64 %176
  %381 = bitcast i16* %380 to <8 x i16>*
  %382 = load <8 x i16>, <8 x i16>* %381, align 1
  %383 = getelementptr inbounds i16, i16* %365, i64 %177
  %384 = bitcast i16* %383 to <8 x i16>*
  %385 = load <8 x i16>, <8 x i16>* %384, align 1
  %386 = getelementptr inbounds i16, i16* %365, i64 %178
  %387 = bitcast i16* %386 to <8 x i16>*
  %388 = load <8 x i16>, <8 x i16>* %387, align 1
  %389 = shufflevector <8 x i16> %388, <8 x i16> %367, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %390 = shufflevector <8 x i16> %367, <8 x i16> %388, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %391 = shufflevector <8 x i16> %388, <8 x i16> %367, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %392 = shufflevector <8 x i16> %367, <8 x i16> %388, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %393 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %389, <8 x i16> %179) #8
  %394 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %390, <8 x i16> <i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076>) #8
  %395 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %391, <8 x i16> %179) #8
  %396 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %392, <8 x i16> <i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076>) #8
  %397 = add <4 x i32> %393, <i32 2048, i32 2048, i32 2048, i32 2048>
  %398 = ashr <4 x i32> %397, <i32 12, i32 12, i32 12, i32 12>
  %399 = add <4 x i32> %394, <i32 2048, i32 2048, i32 2048, i32 2048>
  %400 = ashr <4 x i32> %399, <i32 12, i32 12, i32 12, i32 12>
  %401 = add <4 x i32> %395, <i32 2048, i32 2048, i32 2048, i32 2048>
  %402 = ashr <4 x i32> %401, <i32 12, i32 12, i32 12, i32 12>
  %403 = add <4 x i32> %396, <i32 2048, i32 2048, i32 2048, i32 2048>
  %404 = ashr <4 x i32> %403, <i32 12, i32 12, i32 12, i32 12>
  %405 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %398, <4 x i32> %402) #8
  %406 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %400, <4 x i32> %404) #8
  %407 = shufflevector <8 x i16> %382, <8 x i16> %373, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %408 = shufflevector <8 x i16> %373, <8 x i16> %382, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %409 = shufflevector <8 x i16> %382, <8 x i16> %373, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %410 = shufflevector <8 x i16> %373, <8 x i16> %382, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %411 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %407, <8 x i16> %180) #8
  %412 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %408, <8 x i16> <i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612>) #8
  %413 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %409, <8 x i16> %180) #8
  %414 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %410, <8 x i16> <i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612>) #8
  %415 = add <4 x i32> %411, <i32 2048, i32 2048, i32 2048, i32 2048>
  %416 = ashr <4 x i32> %415, <i32 12, i32 12, i32 12, i32 12>
  %417 = add <4 x i32> %412, <i32 2048, i32 2048, i32 2048, i32 2048>
  %418 = ashr <4 x i32> %417, <i32 12, i32 12, i32 12, i32 12>
  %419 = add <4 x i32> %413, <i32 2048, i32 2048, i32 2048, i32 2048>
  %420 = ashr <4 x i32> %419, <i32 12, i32 12, i32 12, i32 12>
  %421 = add <4 x i32> %414, <i32 2048, i32 2048, i32 2048, i32 2048>
  %422 = ashr <4 x i32> %421, <i32 12, i32 12, i32 12, i32 12>
  %423 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %416, <4 x i32> %420) #8
  %424 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %418, <4 x i32> %422) #8
  %425 = shufflevector <8 x i16> %376, <8 x i16> %379, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %426 = shufflevector <8 x i16> %379, <8 x i16> %376, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %427 = shufflevector <8 x i16> %376, <8 x i16> %379, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %428 = shufflevector <8 x i16> %379, <8 x i16> %376, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %429 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %425, <8 x i16> %181) #8
  %430 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %426, <8 x i16> <i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598>) #8
  %431 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %427, <8 x i16> %181) #8
  %432 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %428, <8 x i16> <i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598>) #8
  %433 = add <4 x i32> %429, <i32 2048, i32 2048, i32 2048, i32 2048>
  %434 = ashr <4 x i32> %433, <i32 12, i32 12, i32 12, i32 12>
  %435 = add <4 x i32> %430, <i32 2048, i32 2048, i32 2048, i32 2048>
  %436 = ashr <4 x i32> %435, <i32 12, i32 12, i32 12, i32 12>
  %437 = add <4 x i32> %431, <i32 2048, i32 2048, i32 2048, i32 2048>
  %438 = ashr <4 x i32> %437, <i32 12, i32 12, i32 12, i32 12>
  %439 = add <4 x i32> %432, <i32 2048, i32 2048, i32 2048, i32 2048>
  %440 = ashr <4 x i32> %439, <i32 12, i32 12, i32 12, i32 12>
  %441 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %434, <4 x i32> %438) #8
  %442 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %436, <4 x i32> %440) #8
  %443 = shufflevector <8 x i16> %370, <8 x i16> %385, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %444 = shufflevector <8 x i16> %385, <8 x i16> %370, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %445 = shufflevector <8 x i16> %370, <8 x i16> %385, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %446 = shufflevector <8 x i16> %385, <8 x i16> %370, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %447 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %443, <8 x i16> %182) #8
  %448 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %444, <8 x i16> <i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189>) #8
  %449 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %445, <8 x i16> %182) #8
  %450 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %446, <8 x i16> <i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189>) #8
  %451 = add <4 x i32> %447, <i32 2048, i32 2048, i32 2048, i32 2048>
  %452 = ashr <4 x i32> %451, <i32 12, i32 12, i32 12, i32 12>
  %453 = add <4 x i32> %448, <i32 2048, i32 2048, i32 2048, i32 2048>
  %454 = ashr <4 x i32> %453, <i32 12, i32 12, i32 12, i32 12>
  %455 = add <4 x i32> %449, <i32 2048, i32 2048, i32 2048, i32 2048>
  %456 = ashr <4 x i32> %455, <i32 12, i32 12, i32 12, i32 12>
  %457 = add <4 x i32> %450, <i32 2048, i32 2048, i32 2048, i32 2048>
  %458 = ashr <4 x i32> %457, <i32 12, i32 12, i32 12, i32 12>
  %459 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %452, <4 x i32> %456) #8
  %460 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %454, <4 x i32> %458) #8
  %461 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %406, <8 x i16> %442) #8
  %462 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %406, <8 x i16> %442) #8
  %463 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %405, <8 x i16> %441) #8
  %464 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %405, <8 x i16> %441) #8
  %465 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %424, <8 x i16> %460) #8
  %466 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %424, <8 x i16> %460) #8
  %467 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %423, <8 x i16> %459) #8
  %468 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %423, <8 x i16> %459) #8
  %469 = shufflevector <8 x i16> %462, <8 x i16> %464, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %470 = shufflevector <8 x i16> %464, <8 x i16> %462, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %471 = shufflevector <8 x i16> %462, <8 x i16> %464, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %472 = shufflevector <8 x i16> %464, <8 x i16> %462, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %473 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %469, <8 x i16> %183) #8
  %474 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %470, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %475 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %471, <8 x i16> %183) #8
  %476 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %472, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %477 = add <4 x i32> %473, <i32 2048, i32 2048, i32 2048, i32 2048>
  %478 = ashr <4 x i32> %477, <i32 12, i32 12, i32 12, i32 12>
  %479 = add <4 x i32> %474, <i32 2048, i32 2048, i32 2048, i32 2048>
  %480 = ashr <4 x i32> %479, <i32 12, i32 12, i32 12, i32 12>
  %481 = add <4 x i32> %475, <i32 2048, i32 2048, i32 2048, i32 2048>
  %482 = ashr <4 x i32> %481, <i32 12, i32 12, i32 12, i32 12>
  %483 = add <4 x i32> %476, <i32 2048, i32 2048, i32 2048, i32 2048>
  %484 = ashr <4 x i32> %483, <i32 12, i32 12, i32 12, i32 12>
  %485 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %478, <4 x i32> %482) #8
  %486 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %480, <4 x i32> %484) #8
  %487 = shufflevector <8 x i16> %468, <8 x i16> %466, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %488 = shufflevector <8 x i16> %466, <8 x i16> %468, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %489 = shufflevector <8 x i16> %468, <8 x i16> %466, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %490 = shufflevector <8 x i16> %466, <8 x i16> %468, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %491 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %487, <8 x i16> %184) #8
  %492 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %488, <8 x i16> <i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567>) #8
  %493 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %489, <8 x i16> %184) #8
  %494 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %490, <8 x i16> <i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567>) #8
  %495 = add <4 x i32> %491, <i32 2048, i32 2048, i32 2048, i32 2048>
  %496 = ashr <4 x i32> %495, <i32 12, i32 12, i32 12, i32 12>
  %497 = add <4 x i32> %492, <i32 2048, i32 2048, i32 2048, i32 2048>
  %498 = ashr <4 x i32> %497, <i32 12, i32 12, i32 12, i32 12>
  %499 = add <4 x i32> %493, <i32 2048, i32 2048, i32 2048, i32 2048>
  %500 = ashr <4 x i32> %499, <i32 12, i32 12, i32 12, i32 12>
  %501 = add <4 x i32> %494, <i32 2048, i32 2048, i32 2048, i32 2048>
  %502 = ashr <4 x i32> %501, <i32 12, i32 12, i32 12, i32 12>
  %503 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %496, <4 x i32> %500) #8
  %504 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %498, <4 x i32> %502) #8
  %505 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %461, <8 x i16> %465) #8
  %506 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %461, <8 x i16> %465) #8
  %507 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %486, <8 x i16> %503) #8
  %508 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %486, <8 x i16> %503) #8
  %509 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %463, <8 x i16> %467) #8
  %510 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %463, <8 x i16> %467) #8
  %511 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %485, <8 x i16> %504) #8
  %512 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %485, <8 x i16> %504) #8
  %513 = shufflevector <8 x i16> %506, <8 x i16> %510, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %514 = shufflevector <8 x i16> %510, <8 x i16> %506, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %515 = shufflevector <8 x i16> %506, <8 x i16> %510, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %516 = shufflevector <8 x i16> %510, <8 x i16> %506, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %517 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %513, <8 x i16> %185) #8
  %518 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %514, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %519 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %515, <8 x i16> %185) #8
  %520 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %516, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %521 = add <4 x i32> %517, <i32 2048, i32 2048, i32 2048, i32 2048>
  %522 = ashr <4 x i32> %521, <i32 12, i32 12, i32 12, i32 12>
  %523 = add <4 x i32> %518, <i32 2048, i32 2048, i32 2048, i32 2048>
  %524 = ashr <4 x i32> %523, <i32 12, i32 12, i32 12, i32 12>
  %525 = add <4 x i32> %519, <i32 2048, i32 2048, i32 2048, i32 2048>
  %526 = ashr <4 x i32> %525, <i32 12, i32 12, i32 12, i32 12>
  %527 = add <4 x i32> %520, <i32 2048, i32 2048, i32 2048, i32 2048>
  %528 = ashr <4 x i32> %527, <i32 12, i32 12, i32 12, i32 12>
  %529 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %522, <4 x i32> %526) #8
  %530 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %524, <4 x i32> %528) #8
  %531 = shufflevector <8 x i16> %508, <8 x i16> %512, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %532 = shufflevector <8 x i16> %512, <8 x i16> %508, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %533 = shufflevector <8 x i16> %508, <8 x i16> %512, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %534 = shufflevector <8 x i16> %512, <8 x i16> %508, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %535 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %531, <8 x i16> %185) #8
  %536 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %532, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %537 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %533, <8 x i16> %185) #8
  %538 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %534, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %539 = add <4 x i32> %535, <i32 2048, i32 2048, i32 2048, i32 2048>
  %540 = ashr <4 x i32> %539, <i32 12, i32 12, i32 12, i32 12>
  %541 = add <4 x i32> %536, <i32 2048, i32 2048, i32 2048, i32 2048>
  %542 = ashr <4 x i32> %541, <i32 12, i32 12, i32 12, i32 12>
  %543 = add <4 x i32> %537, <i32 2048, i32 2048, i32 2048, i32 2048>
  %544 = ashr <4 x i32> %543, <i32 12, i32 12, i32 12, i32 12>
  %545 = add <4 x i32> %538, <i32 2048, i32 2048, i32 2048, i32 2048>
  %546 = ashr <4 x i32> %545, <i32 12, i32 12, i32 12, i32 12>
  %547 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %540, <4 x i32> %544) #8
  %548 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %542, <4 x i32> %546) #8
  %549 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %507) #8
  %550 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %530) #8
  %551 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %547) #8
  %552 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %509) #8
  store <8 x i16> %505, <8 x i16>* %366, align 1
  store <8 x i16> %549, <8 x i16>* %369, align 1
  store <8 x i16> %548, <8 x i16>* %372, align 1
  store <8 x i16> %550, <8 x i16>* %375, align 1
  store <8 x i16> %529, <8 x i16>* %378, align 1
  store <8 x i16> %551, <8 x i16>* %381, align 1
  store <8 x i16> %511, <8 x i16>* %384, align 1
  store <8 x i16> %552, <8 x i16>* %387, align 1
  %553 = add nuw nsw i64 %364, 8
  %554 = icmp ult i64 %553, %186
  br i1 %554, label %363, label %555

555:                                              ; preds = %95, %363
  %556 = bitcast i8* %6 to i64*
  %557 = load i64, i64* %556, align 8
  %558 = getelementptr inbounds i8, i8* %6, i64 8
  %559 = bitcast i8* %558 to i8**
  %560 = load i8*, i8** %559, align 8
  %561 = and i32 %14, 16608
  %562 = icmp ne i32 %561, 0
  %563 = sext i32 %5 to i64
  %564 = ashr i64 %557, 32
  %565 = mul nsw i64 %564, %563
  %566 = getelementptr inbounds i8, i8* %560, i64 %565
  %567 = sext i32 %4 to i64
  %568 = getelementptr inbounds i8, i8* %566, i64 %567
  switch i8 %11, label %860 [
    i8 4, label %705
    i8 8, label %569
  ]

569:                                              ; preds = %555
  %570 = select i1 %562, i64 56, i64 0
  %571 = getelementptr inbounds i16, i16* %8, i64 %570
  %572 = bitcast i16* %571 to <8 x i16>*
  %573 = load <8 x i16>, <8 x i16>* %572, align 1
  %574 = bitcast i8* %568 to i64*
  %575 = load i64, i64* %574, align 1
  %576 = insertelement <2 x i64> undef, i64 %575, i32 0
  %577 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %573, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %578 = ashr <8 x i16> %577, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %579 = bitcast <2 x i64> %576 to <16 x i8>
  %580 = shufflevector <16 x i8> %579, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %581 = zext <8 x i8> %580 to <8 x i16>
  %582 = add nsw <8 x i16> %578, %581
  %583 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %582, <8 x i16> undef) #8
  %584 = bitcast <16 x i8> %583 to <2 x i64>
  %585 = extractelement <2 x i64> %584, i32 0
  store i64 %585, i64* %574, align 1
  %586 = getelementptr inbounds i8, i8* %568, i64 %564
  %587 = select i1 %562, i64 48, i64 8
  %588 = getelementptr inbounds i16, i16* %8, i64 %587
  %589 = bitcast i16* %588 to <8 x i16>*
  %590 = load <8 x i16>, <8 x i16>* %589, align 1
  %591 = bitcast i8* %586 to i64*
  %592 = load i64, i64* %591, align 1
  %593 = insertelement <2 x i64> undef, i64 %592, i32 0
  %594 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %590, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %595 = ashr <8 x i16> %594, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %596 = bitcast <2 x i64> %593 to <16 x i8>
  %597 = shufflevector <16 x i8> %596, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %598 = zext <8 x i8> %597 to <8 x i16>
  %599 = add nsw <8 x i16> %595, %598
  %600 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %599, <8 x i16> undef) #8
  %601 = bitcast <16 x i8> %600 to <2 x i64>
  %602 = extractelement <2 x i64> %601, i32 0
  store i64 %602, i64* %591, align 1
  %603 = getelementptr inbounds i8, i8* %586, i64 %564
  %604 = select i1 %562, i64 40, i64 16
  %605 = getelementptr inbounds i16, i16* %8, i64 %604
  %606 = bitcast i16* %605 to <8 x i16>*
  %607 = load <8 x i16>, <8 x i16>* %606, align 1
  %608 = bitcast i8* %603 to i64*
  %609 = load i64, i64* %608, align 1
  %610 = insertelement <2 x i64> undef, i64 %609, i32 0
  %611 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %607, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %612 = ashr <8 x i16> %611, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %613 = bitcast <2 x i64> %610 to <16 x i8>
  %614 = shufflevector <16 x i8> %613, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %615 = zext <8 x i8> %614 to <8 x i16>
  %616 = add nsw <8 x i16> %612, %615
  %617 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %616, <8 x i16> undef) #8
  %618 = bitcast <16 x i8> %617 to <2 x i64>
  %619 = extractelement <2 x i64> %618, i32 0
  store i64 %619, i64* %608, align 1
  %620 = getelementptr inbounds i8, i8* %603, i64 %564
  %621 = select i1 %562, i64 32, i64 24
  %622 = getelementptr inbounds i16, i16* %8, i64 %621
  %623 = bitcast i16* %622 to <8 x i16>*
  %624 = load <8 x i16>, <8 x i16>* %623, align 1
  %625 = bitcast i8* %620 to i64*
  %626 = load i64, i64* %625, align 1
  %627 = insertelement <2 x i64> undef, i64 %626, i32 0
  %628 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %624, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %629 = ashr <8 x i16> %628, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %630 = bitcast <2 x i64> %627 to <16 x i8>
  %631 = shufflevector <16 x i8> %630, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %632 = zext <8 x i8> %631 to <8 x i16>
  %633 = add nsw <8 x i16> %629, %632
  %634 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %633, <8 x i16> undef) #8
  %635 = bitcast <16 x i8> %634 to <2 x i64>
  %636 = extractelement <2 x i64> %635, i32 0
  store i64 %636, i64* %625, align 1
  %637 = getelementptr inbounds i8, i8* %620, i64 %564
  %638 = select i1 %562, i64 24, i64 32
  %639 = getelementptr inbounds i16, i16* %8, i64 %638
  %640 = bitcast i16* %639 to <8 x i16>*
  %641 = load <8 x i16>, <8 x i16>* %640, align 1
  %642 = bitcast i8* %637 to i64*
  %643 = load i64, i64* %642, align 1
  %644 = insertelement <2 x i64> undef, i64 %643, i32 0
  %645 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %641, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %646 = ashr <8 x i16> %645, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %647 = bitcast <2 x i64> %644 to <16 x i8>
  %648 = shufflevector <16 x i8> %647, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %649 = zext <8 x i8> %648 to <8 x i16>
  %650 = add nsw <8 x i16> %646, %649
  %651 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %650, <8 x i16> undef) #8
  %652 = bitcast <16 x i8> %651 to <2 x i64>
  %653 = extractelement <2 x i64> %652, i32 0
  store i64 %653, i64* %642, align 1
  %654 = getelementptr inbounds i8, i8* %637, i64 %564
  %655 = select i1 %562, i64 16, i64 40
  %656 = getelementptr inbounds i16, i16* %8, i64 %655
  %657 = bitcast i16* %656 to <8 x i16>*
  %658 = load <8 x i16>, <8 x i16>* %657, align 1
  %659 = bitcast i8* %654 to i64*
  %660 = load i64, i64* %659, align 1
  %661 = insertelement <2 x i64> undef, i64 %660, i32 0
  %662 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %658, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %663 = ashr <8 x i16> %662, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %664 = bitcast <2 x i64> %661 to <16 x i8>
  %665 = shufflevector <16 x i8> %664, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %666 = zext <8 x i8> %665 to <8 x i16>
  %667 = add nsw <8 x i16> %663, %666
  %668 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %667, <8 x i16> undef) #8
  %669 = bitcast <16 x i8> %668 to <2 x i64>
  %670 = extractelement <2 x i64> %669, i32 0
  store i64 %670, i64* %659, align 1
  %671 = getelementptr inbounds i8, i8* %654, i64 %564
  %672 = select i1 %562, i64 8, i64 48
  %673 = getelementptr inbounds i16, i16* %8, i64 %672
  %674 = bitcast i16* %673 to <8 x i16>*
  %675 = load <8 x i16>, <8 x i16>* %674, align 1
  %676 = bitcast i8* %671 to i64*
  %677 = load i64, i64* %676, align 1
  %678 = insertelement <2 x i64> undef, i64 %677, i32 0
  %679 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %675, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %680 = ashr <8 x i16> %679, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %681 = bitcast <2 x i64> %678 to <16 x i8>
  %682 = shufflevector <16 x i8> %681, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %683 = zext <8 x i8> %682 to <8 x i16>
  %684 = add nsw <8 x i16> %680, %683
  %685 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %684, <8 x i16> undef) #8
  %686 = bitcast <16 x i8> %685 to <2 x i64>
  %687 = extractelement <2 x i64> %686, i32 0
  store i64 %687, i64* %676, align 1
  %688 = getelementptr inbounds i8, i8* %671, i64 %564
  %689 = select i1 %562, i64 0, i64 56
  %690 = getelementptr inbounds i16, i16* %8, i64 %689
  %691 = bitcast i16* %690 to <8 x i16>*
  %692 = load <8 x i16>, <8 x i16>* %691, align 1
  %693 = bitcast i8* %688 to i64*
  %694 = load i64, i64* %693, align 1
  %695 = insertelement <2 x i64> undef, i64 %694, i32 0
  %696 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %692, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %697 = ashr <8 x i16> %696, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %698 = bitcast <2 x i64> %695 to <16 x i8>
  %699 = shufflevector <16 x i8> %698, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %700 = zext <8 x i8> %699 to <8 x i16>
  %701 = add nsw <8 x i16> %697, %700
  %702 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %701, <8 x i16> undef) #8
  %703 = bitcast <16 x i8> %702 to <2 x i64>
  %704 = extractelement <2 x i64> %703, i32 0
  store i64 %704, i64* %693, align 1
  br label %902

705:                                              ; preds = %187, %555
  %706 = phi i8* [ %362, %187 ], [ %568, %555 ]
  %707 = phi i64 [ %358, %187 ], [ %564, %555 ]
  %708 = phi i1 [ %356, %187 ], [ %562, %555 ]
  %709 = select i1 %708, i64 28, i64 0
  %710 = getelementptr inbounds i16, i16* %8, i64 %709
  %711 = bitcast i16* %710 to i64*
  %712 = load i64, i64* %711, align 1
  %713 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %712, i32 0
  %714 = bitcast i8* %706 to i32*
  %715 = load i32, i32* %714, align 1
  %716 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %715, i32 0
  %717 = bitcast <2 x i64> %713 to <8 x i16>
  %718 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %717, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %719 = ashr <8 x i16> %718, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %720 = bitcast <4 x i32> %716 to <16 x i8>
  %721 = shufflevector <16 x i8> %720, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %722 = zext <8 x i8> %721 to <8 x i16>
  %723 = add nsw <8 x i16> %719, %722
  %724 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %723, <8 x i16> undef) #8
  %725 = bitcast <16 x i8> %724 to <4 x i32>
  %726 = extractelement <4 x i32> %725, i32 0
  store i32 %726, i32* %714, align 1
  %727 = getelementptr inbounds i8, i8* %706, i64 %707
  %728 = select i1 %708, i64 24, i64 4
  %729 = getelementptr inbounds i16, i16* %8, i64 %728
  %730 = bitcast i16* %729 to i64*
  %731 = load i64, i64* %730, align 1
  %732 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %731, i32 0
  %733 = bitcast i8* %727 to i32*
  %734 = load i32, i32* %733, align 1
  %735 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %734, i32 0
  %736 = bitcast <2 x i64> %732 to <8 x i16>
  %737 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %736, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %738 = ashr <8 x i16> %737, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %739 = bitcast <4 x i32> %735 to <16 x i8>
  %740 = shufflevector <16 x i8> %739, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %741 = zext <8 x i8> %740 to <8 x i16>
  %742 = add nsw <8 x i16> %738, %741
  %743 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %742, <8 x i16> undef) #8
  %744 = bitcast <16 x i8> %743 to <4 x i32>
  %745 = extractelement <4 x i32> %744, i32 0
  store i32 %745, i32* %733, align 1
  %746 = getelementptr inbounds i8, i8* %727, i64 %707
  %747 = select i1 %708, i64 20, i64 8
  %748 = getelementptr inbounds i16, i16* %8, i64 %747
  %749 = bitcast i16* %748 to i64*
  %750 = load i64, i64* %749, align 1
  %751 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %750, i32 0
  %752 = bitcast i8* %746 to i32*
  %753 = load i32, i32* %752, align 1
  %754 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %753, i32 0
  %755 = bitcast <2 x i64> %751 to <8 x i16>
  %756 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %755, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %757 = ashr <8 x i16> %756, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %758 = bitcast <4 x i32> %754 to <16 x i8>
  %759 = shufflevector <16 x i8> %758, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %760 = zext <8 x i8> %759 to <8 x i16>
  %761 = add nsw <8 x i16> %757, %760
  %762 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %761, <8 x i16> undef) #8
  %763 = bitcast <16 x i8> %762 to <4 x i32>
  %764 = extractelement <4 x i32> %763, i32 0
  store i32 %764, i32* %752, align 1
  %765 = getelementptr inbounds i8, i8* %746, i64 %707
  %766 = select i1 %708, i64 16, i64 12
  %767 = getelementptr inbounds i16, i16* %8, i64 %766
  %768 = bitcast i16* %767 to i64*
  %769 = load i64, i64* %768, align 1
  %770 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %769, i32 0
  %771 = bitcast i8* %765 to i32*
  %772 = load i32, i32* %771, align 1
  %773 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %772, i32 0
  %774 = bitcast <2 x i64> %770 to <8 x i16>
  %775 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %774, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %776 = ashr <8 x i16> %775, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %777 = bitcast <4 x i32> %773 to <16 x i8>
  %778 = shufflevector <16 x i8> %777, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %779 = zext <8 x i8> %778 to <8 x i16>
  %780 = add nsw <8 x i16> %776, %779
  %781 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %780, <8 x i16> undef) #8
  %782 = bitcast <16 x i8> %781 to <4 x i32>
  %783 = extractelement <4 x i32> %782, i32 0
  store i32 %783, i32* %771, align 1
  %784 = getelementptr inbounds i8, i8* %765, i64 %707
  %785 = select i1 %708, i64 12, i64 16
  %786 = getelementptr inbounds i16, i16* %8, i64 %785
  %787 = bitcast i16* %786 to i64*
  %788 = load i64, i64* %787, align 1
  %789 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %788, i32 0
  %790 = bitcast i8* %784 to i32*
  %791 = load i32, i32* %790, align 1
  %792 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %791, i32 0
  %793 = bitcast <2 x i64> %789 to <8 x i16>
  %794 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %793, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %795 = ashr <8 x i16> %794, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %796 = bitcast <4 x i32> %792 to <16 x i8>
  %797 = shufflevector <16 x i8> %796, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %798 = zext <8 x i8> %797 to <8 x i16>
  %799 = add nsw <8 x i16> %795, %798
  %800 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %799, <8 x i16> undef) #8
  %801 = bitcast <16 x i8> %800 to <4 x i32>
  %802 = extractelement <4 x i32> %801, i32 0
  store i32 %802, i32* %790, align 1
  %803 = getelementptr inbounds i8, i8* %784, i64 %707
  %804 = select i1 %708, i64 8, i64 20
  %805 = getelementptr inbounds i16, i16* %8, i64 %804
  %806 = bitcast i16* %805 to i64*
  %807 = load i64, i64* %806, align 1
  %808 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %807, i32 0
  %809 = bitcast i8* %803 to i32*
  %810 = load i32, i32* %809, align 1
  %811 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %810, i32 0
  %812 = bitcast <2 x i64> %808 to <8 x i16>
  %813 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %812, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %814 = ashr <8 x i16> %813, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %815 = bitcast <4 x i32> %811 to <16 x i8>
  %816 = shufflevector <16 x i8> %815, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %817 = zext <8 x i8> %816 to <8 x i16>
  %818 = add nsw <8 x i16> %814, %817
  %819 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %818, <8 x i16> undef) #8
  %820 = bitcast <16 x i8> %819 to <4 x i32>
  %821 = extractelement <4 x i32> %820, i32 0
  store i32 %821, i32* %809, align 1
  %822 = getelementptr inbounds i8, i8* %803, i64 %707
  %823 = select i1 %708, i64 4, i64 24
  %824 = getelementptr inbounds i16, i16* %8, i64 %823
  %825 = bitcast i16* %824 to i64*
  %826 = load i64, i64* %825, align 1
  %827 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %826, i32 0
  %828 = bitcast i8* %822 to i32*
  %829 = load i32, i32* %828, align 1
  %830 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %829, i32 0
  %831 = bitcast <2 x i64> %827 to <8 x i16>
  %832 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %831, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %833 = ashr <8 x i16> %832, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %834 = bitcast <4 x i32> %830 to <16 x i8>
  %835 = shufflevector <16 x i8> %834, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %836 = zext <8 x i8> %835 to <8 x i16>
  %837 = add nsw <8 x i16> %833, %836
  %838 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %837, <8 x i16> undef) #8
  %839 = bitcast <16 x i8> %838 to <4 x i32>
  %840 = extractelement <4 x i32> %839, i32 0
  store i32 %840, i32* %828, align 1
  %841 = getelementptr inbounds i8, i8* %822, i64 %707
  %842 = select i1 %708, i64 0, i64 28
  %843 = getelementptr inbounds i16, i16* %8, i64 %842
  %844 = bitcast i16* %843 to i64*
  %845 = load i64, i64* %844, align 1
  %846 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %845, i32 0
  %847 = bitcast i8* %841 to i32*
  %848 = load i32, i32* %847, align 1
  %849 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %848, i32 0
  %850 = bitcast <2 x i64> %846 to <8 x i16>
  %851 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %850, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %852 = ashr <8 x i16> %851, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %853 = bitcast <4 x i32> %849 to <16 x i8>
  %854 = shufflevector <16 x i8> %853, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %855 = zext <8 x i8> %854 to <8 x i16>
  %856 = add nsw <8 x i16> %852, %855
  %857 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %856, <8 x i16> undef) #8
  %858 = bitcast <16 x i8> %857 to <4 x i32>
  %859 = extractelement <4 x i32> %858, i32 0
  store i32 %859, i32* %847, align 1
  br label %902

860:                                              ; preds = %555
  %861 = zext i8 %11 to i64
  br label %862

862:                                              ; preds = %899, %860
  %863 = phi i64 [ 0, %860 ], [ %900, %899 ]
  %864 = add nsw i64 %863, %563
  %865 = trunc i64 %863 to i32
  %866 = sub i32 7, %865
  %867 = select i1 %562, i32 %866, i32 %865
  %868 = mul nsw i32 %867, %12
  %869 = mul nsw i64 %864, %564
  %870 = getelementptr inbounds i8, i8* %560, i64 %869
  %871 = sext i32 %868 to i64
  br label %872

872:                                              ; preds = %872, %862
  %873 = phi i64 [ %897, %872 ], [ 0, %862 ]
  %874 = add nsw i64 %873, %567
  %875 = add nsw i64 %873, %871
  %876 = getelementptr inbounds i16, i16* %8, i64 %875
  %877 = bitcast i16* %876 to <8 x i16>*
  %878 = load <8 x i16>, <8 x i16>* %877, align 1
  %879 = add nsw i64 %875, 8
  %880 = getelementptr inbounds i16, i16* %8, i64 %879
  %881 = bitcast i16* %880 to <8 x i16>*
  %882 = load <8 x i16>, <8 x i16>* %881, align 1
  %883 = getelementptr inbounds i8, i8* %870, i64 %874
  %884 = bitcast i8* %883 to <16 x i8>*
  %885 = load <16 x i8>, <16 x i8>* %884, align 1
  %886 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %878, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %887 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %882, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %888 = ashr <8 x i16> %886, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %889 = ashr <8 x i16> %887, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %890 = shufflevector <16 x i8> %885, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %891 = zext <8 x i8> %890 to <8 x i16>
  %892 = shufflevector <16 x i8> %885, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %893 = zext <8 x i8> %892 to <8 x i16>
  %894 = add nsw <8 x i16> %888, %891
  %895 = add nsw <8 x i16> %889, %893
  %896 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %894, <8 x i16> %895) #8
  store <16 x i8> %896, <16 x i8>* %884, align 1
  %897 = add nuw nsw i64 %873, 16
  %898 = icmp ult i64 %897, %861
  br i1 %898, label %872, label %899

899:                                              ; preds = %872
  %900 = add nuw nsw i64 %863, 1
  %901 = icmp eq i64 %900, 8
  br i1 %901, label %902, label %862

902:                                              ; preds = %899, %569, %705
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_129Adst16TransformLoopRow_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_(i8 zeroext, i8 zeroext, i32, i8*, i32, i32, i8* nocapture readnone) #4 {
  %8 = bitcast i8* %3 to i16*
  %9 = zext i8 %1 to i64
  %10 = lshr i64 173354, %9
  %11 = and i64 %10, 1
  %12 = icmp ne i64 %11, 0
  %13 = getelementptr inbounds [19 x i8], [19 x i8]* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_118kTransformRowShiftE, i64 0, i64 %9
  %14 = load i8, i8* %13, align 1
  %15 = icmp sgt i32 %2, 1
  br i1 %15, label %159, label %16

16:                                               ; preds = %7
  %17 = zext i8 %14 to i32
  %18 = load i16, i16* %8, align 2
  %19 = sext i16 %18 to i32
  %20 = insertelement <4 x i32> <i32 undef, i32 undef, i32 0, i32 0>, i32 %19, i32 0
  %21 = bitcast <4 x i32> %20 to <8 x i16>
  %22 = shufflevector <8 x i16> %21, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 4, i32 5, i32 6, i32 7>
  %23 = sext i1 %12 to i16
  %24 = insertelement <8 x i16> undef, i16 %23, i32 0
  %25 = shufflevector <8 x i16> %24, <8 x i16> undef, <8 x i32> zeroinitializer
  %26 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %22, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  %27 = bitcast <8 x i16> %22 to <16 x i8>
  %28 = bitcast <8 x i16> %26 to <16 x i8>
  %29 = bitcast <8 x i16> %25 to <16 x i8>
  %30 = tail call <16 x i8> @llvm.x86.sse41.pblendvb(<16 x i8> %27, <16 x i8> %28, <16 x i8> %29) #8
  %31 = bitcast <16 x i8> %30 to <8 x i16>
  %32 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %31, <8 x i16> <i16 -32728, i16 -32728, i16 -32728, i16 -32728, i16 -32728, i16 -32728, i16 -32728, i16 -32728>) #8
  %33 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %31, <8 x i16> <i16 1608, i16 1608, i16 1608, i16 1608, i16 1608, i16 1608, i16 1608, i16 1608>) #8
  %34 = shufflevector <8 x i16> %33, <8 x i16> %32, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %35 = shufflevector <8 x i16> %32, <8 x i16> %33, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %36 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %37 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %34, <8 x i16> %36) #8
  %38 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %35, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %39 = add <4 x i32> %37, <i32 2048, i32 2048, i32 2048, i32 2048>
  %40 = ashr <4 x i32> %39, <i32 12, i32 12, i32 12, i32 12>
  %41 = add <4 x i32> %38, <i32 2048, i32 2048, i32 2048, i32 2048>
  %42 = ashr <4 x i32> %41, <i32 12, i32 12, i32 12, i32 12>
  %43 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %40, <4 x i32> %40) #8
  %44 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %42, <4 x i32> %42) #8
  %45 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %46 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %34, <8 x i16> %45) #8
  %47 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %35, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %48 = add <4 x i32> %46, <i32 2048, i32 2048, i32 2048, i32 2048>
  %49 = ashr <4 x i32> %48, <i32 12, i32 12, i32 12, i32 12>
  %50 = add <4 x i32> %47, <i32 2048, i32 2048, i32 2048, i32 2048>
  %51 = ashr <4 x i32> %50, <i32 12, i32 12, i32 12, i32 12>
  %52 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %49, <4 x i32> %49) #8
  %53 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %51, <4 x i32> %51) #8
  %54 = shufflevector <8 x i16> %44, <8 x i16> %43, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %55 = shufflevector <8 x i16> %43, <8 x i16> %44, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %56 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %54, <8 x i16> %45) #8
  %57 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %55, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %58 = add <4 x i32> %56, <i32 2048, i32 2048, i32 2048, i32 2048>
  %59 = ashr <4 x i32> %58, <i32 12, i32 12, i32 12, i32 12>
  %60 = add <4 x i32> %57, <i32 2048, i32 2048, i32 2048, i32 2048>
  %61 = ashr <4 x i32> %60, <i32 12, i32 12, i32 12, i32 12>
  %62 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %59, <4 x i32> %59) #8
  %63 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %61, <4 x i32> %61) #8
  %64 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %65 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %34, <8 x i16> %64) #8
  %66 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %35, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %67 = add <4 x i32> %65, <i32 2048, i32 2048, i32 2048, i32 2048>
  %68 = ashr <4 x i32> %67, <i32 12, i32 12, i32 12, i32 12>
  %69 = add <4 x i32> %66, <i32 2048, i32 2048, i32 2048, i32 2048>
  %70 = ashr <4 x i32> %69, <i32 12, i32 12, i32 12, i32 12>
  %71 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %68, <4 x i32> undef) #8
  %72 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %70, <4 x i32> %70) #8
  %73 = shufflevector <8 x i16> %53, <8 x i16> %52, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %74 = shufflevector <8 x i16> %52, <8 x i16> %53, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %75 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %73, <8 x i16> %64) #8
  %76 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %74, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %77 = add <4 x i32> %75, <i32 2048, i32 2048, i32 2048, i32 2048>
  %78 = ashr <4 x i32> %77, <i32 12, i32 12, i32 12, i32 12>
  %79 = add <4 x i32> %76, <i32 2048, i32 2048, i32 2048, i32 2048>
  %80 = ashr <4 x i32> %79, <i32 12, i32 12, i32 12, i32 12>
  %81 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %78, <4 x i32> %78) #8
  %82 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %80, <4 x i32> undef) #8
  %83 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %54, <8 x i16> %64) #8
  %84 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %55, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %85 = add <4 x i32> %83, <i32 2048, i32 2048, i32 2048, i32 2048>
  %86 = ashr <4 x i32> %85, <i32 12, i32 12, i32 12, i32 12>
  %87 = add <4 x i32> %84, <i32 2048, i32 2048, i32 2048, i32 2048>
  %88 = ashr <4 x i32> %87, <i32 12, i32 12, i32 12, i32 12>
  %89 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %86, <4 x i32> %86) #8
  %90 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %88, <4 x i32> undef) #8
  %91 = shufflevector <8 x i16> %63, <8 x i16> %62, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %92 = shufflevector <8 x i16> %62, <8 x i16> %63, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %93 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %91, <8 x i16> %64) #8
  %94 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %92, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %95 = add <4 x i32> %93, <i32 2048, i32 2048, i32 2048, i32 2048>
  %96 = ashr <4 x i32> %95, <i32 12, i32 12, i32 12, i32 12>
  %97 = add <4 x i32> %94, <i32 2048, i32 2048, i32 2048, i32 2048>
  %98 = ashr <4 x i32> %97, <i32 12, i32 12, i32 12, i32 12>
  %99 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %96, <4 x i32> undef) #8
  %100 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %98, <4 x i32> %98) #8
  %101 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %44) #8
  %102 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %53) #8
  %103 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %100) #8
  %104 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %72) #8
  %105 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %89) #8
  %106 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %81) #8
  %107 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %62) #8
  %108 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %32) #8
  %109 = insertelement <4 x i32> undef, i32 %17, i32 0
  %110 = shufflevector <4 x i32> %109, <4 x i32> undef, <4 x i32> zeroinitializer
  %111 = shufflevector <4 x i32> %109, <4 x i32> undef, <2 x i32> zeroinitializer
  %112 = zext <2 x i32> %111 to <2 x i64>
  %113 = bitcast <2 x i64> %112 to <4 x i32>
  %114 = shufflevector <8 x i16> %33, <8 x i16> %101, <8 x i32> <i32 0, i32 8, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %115 = shufflevector <8 x i16> %63, <8 x i16> %102, <8 x i32> <i32 0, i32 8, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %116 = shufflevector <8 x i16> %82, <8 x i16> %103, <8 x i32> <i32 0, i32 8, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %117 = shufflevector <8 x i16> %90, <8 x i16> %104, <8 x i32> <i32 0, i32 8, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %118 = bitcast <8 x i16> %114 to <4 x i32>
  %119 = bitcast <8 x i16> %115 to <4 x i32>
  %120 = shufflevector <4 x i32> %118, <4 x i32> %119, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %121 = bitcast <8 x i16> %116 to <4 x i32>
  %122 = bitcast <8 x i16> %117 to <4 x i32>
  %123 = shufflevector <4 x i32> %121, <4 x i32> %122, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %124 = bitcast <4 x i32> %120 to <8 x i16>
  %125 = shufflevector <8 x i16> %124, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %126 = sext <4 x i16> %125 to <4 x i32>
  %127 = add <4 x i32> %110, %126
  %128 = bitcast <4 x i32> %123 to <8 x i16>
  %129 = shufflevector <8 x i16> %128, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %130 = sext <4 x i16> %129 to <4 x i32>
  %131 = add <4 x i32> %110, %130
  %132 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %127, <4 x i32> %113) #8
  %133 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %131, <4 x i32> %113) #8
  %134 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %132, <4 x i32> %133) #8
  %135 = bitcast i8* %3 to <8 x i16>*
  store <8 x i16> %134, <8 x i16>* %135, align 1
  %136 = shufflevector <8 x i16> %71, <8 x i16> %105, <8 x i32> <i32 0, i32 8, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %137 = shufflevector <8 x i16> %99, <8 x i16> %106, <8 x i32> <i32 0, i32 8, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %138 = shufflevector <8 x i16> %52, <8 x i16> %107, <8 x i32> <i32 0, i32 8, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %139 = shufflevector <8 x i16> %43, <8 x i16> %108, <8 x i32> <i32 0, i32 8, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %140 = bitcast <8 x i16> %136 to <4 x i32>
  %141 = bitcast <8 x i16> %137 to <4 x i32>
  %142 = shufflevector <4 x i32> %140, <4 x i32> %141, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %143 = bitcast <8 x i16> %138 to <4 x i32>
  %144 = bitcast <8 x i16> %139 to <4 x i32>
  %145 = shufflevector <4 x i32> %143, <4 x i32> %144, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %146 = bitcast <4 x i32> %142 to <8 x i16>
  %147 = shufflevector <8 x i16> %146, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %148 = sext <4 x i16> %147 to <4 x i32>
  %149 = add <4 x i32> %110, %148
  %150 = bitcast <4 x i32> %145 to <8 x i16>
  %151 = shufflevector <8 x i16> %150, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %152 = sext <4 x i16> %151 to <4 x i32>
  %153 = add <4 x i32> %110, %152
  %154 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %149, <4 x i32> %113) #8
  %155 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %153, <4 x i32> %113) #8
  %156 = getelementptr inbounds i8, i8* %3, i64 16
  %157 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %154, <4 x i32> %155) #8
  %158 = bitcast i8* %156 to <8 x i16>*
  store <8 x i16> %157, <8 x i16>* %158, align 1
  br label %1331

159:                                              ; preds = %7
  br i1 %12, label %160, label %211

160:                                              ; preds = %159
  %161 = sext i32 %2 to i64
  %162 = and i64 %161, 1
  %163 = icmp eq i32 %2, 1
  br i1 %163, label %198, label %164

164:                                              ; preds = %160
  %165 = sub nsw i64 %161, %162
  br label %166

166:                                              ; preds = %166, %164
  %167 = phi i64 [ 0, %164 ], [ %192, %166 ]
  %168 = phi i64 [ %165, %164 ], [ %193, %166 ]
  %169 = shl i64 %167, 4
  %170 = and i64 %169, 4294967264
  %171 = getelementptr inbounds i16, i16* %8, i64 %170
  %172 = bitcast i16* %171 to <8 x i16>*
  %173 = load <8 x i16>, <8 x i16>* %172, align 1
  %174 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %173, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %174, <8 x i16>* %172, align 1
  %175 = or i64 %170, 8
  %176 = getelementptr inbounds i16, i16* %8, i64 %175
  %177 = bitcast i16* %176 to <8 x i16>*
  %178 = load <8 x i16>, <8 x i16>* %177, align 1
  %179 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %178, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %179, <8 x i16>* %177, align 1
  %180 = shl i64 %167, 4
  %181 = and i64 %180, 4294967264
  %182 = or i64 %181, 16
  %183 = getelementptr inbounds i16, i16* %8, i64 %182
  %184 = bitcast i16* %183 to <8 x i16>*
  %185 = load <8 x i16>, <8 x i16>* %184, align 1
  %186 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %185, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %186, <8 x i16>* %184, align 1
  %187 = or i64 %181, 24
  %188 = getelementptr inbounds i16, i16* %8, i64 %187
  %189 = bitcast i16* %188 to <8 x i16>*
  %190 = load <8 x i16>, <8 x i16>* %189, align 1
  %191 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %190, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %191, <8 x i16>* %189, align 1
  %192 = add nuw nsw i64 %167, 2
  %193 = add i64 %168, -2
  %194 = icmp eq i64 %193, 0
  br i1 %194, label %195, label %166

195:                                              ; preds = %166
  %196 = shl i64 %192, 4
  %197 = and i64 %196, 4294967264
  br label %198

198:                                              ; preds = %195, %160
  %199 = phi i64 [ 0, %160 ], [ %197, %195 ]
  %200 = icmp eq i64 %162, 0
  br i1 %200, label %211, label %201

201:                                              ; preds = %198
  %202 = getelementptr inbounds i16, i16* %8, i64 %199
  %203 = bitcast i16* %202 to <8 x i16>*
  %204 = load <8 x i16>, <8 x i16>* %203, align 1
  %205 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %204, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %205, <8 x i16>* %203, align 1
  %206 = or i64 %199, 8
  %207 = getelementptr inbounds i16, i16* %8, i64 %206
  %208 = bitcast i16* %207 to <8 x i16>*
  %209 = load <8 x i16>, <8 x i16>* %208, align 1
  %210 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %209, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %210, <8 x i16>* %208, align 1
  br label %211

211:                                              ; preds = %201, %198, %159
  %212 = icmp slt i32 %2, 5
  br i1 %212, label %230, label %213

213:                                              ; preds = %211
  %214 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 201, i16 4091, i16 201, i16 4091, i16 201, i16 4091, i16 201, i16 4091>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %215 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 995, i16 3973, i16 995, i16 3973, i16 995, i16 3973, i16 995, i16 3973>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %216 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1751, i16 3703, i16 1751, i16 3703, i16 1751, i16 3703, i16 1751, i16 3703>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %217 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2440, i16 3290, i16 2440, i16 3290, i16 2440, i16 3290, i16 2440, i16 3290>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %218 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3035, i16 2751, i16 3035, i16 2751, i16 3035, i16 2751, i16 3035, i16 2751>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %219 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3513, i16 2106, i16 3513, i16 2106, i16 3513, i16 2106, i16 3513, i16 2106>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %220 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3857, i16 1380, i16 3857, i16 1380, i16 3857, i16 1380, i16 3857, i16 1380>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %221 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 4052, i16 601, i16 4052, i16 601, i16 4052, i16 601, i16 4052, i16 601>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %222 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %223 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %224 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %225 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %226 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %227 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %228 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %229 = sext i32 %2 to i64
  br label %638

230:                                              ; preds = %211
  %231 = bitcast i8* %3 to <8 x i16>*
  %232 = load <8 x i16>, <8 x i16>* %231, align 1
  %233 = getelementptr inbounds i8, i8* %3, i64 32
  %234 = bitcast i8* %233 to <8 x i16>*
  %235 = load <8 x i16>, <8 x i16>* %234, align 1
  %236 = getelementptr inbounds i8, i8* %3, i64 64
  %237 = bitcast i8* %236 to <8 x i16>*
  %238 = load <8 x i16>, <8 x i16>* %237, align 1
  %239 = getelementptr inbounds i8, i8* %3, i64 96
  %240 = bitcast i8* %239 to <8 x i16>*
  %241 = load <8 x i16>, <8 x i16>* %240, align 1
  %242 = shufflevector <8 x i16> %232, <8 x i16> %235, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %243 = shufflevector <8 x i16> %238, <8 x i16> %241, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %244 = shufflevector <8 x i16> %232, <8 x i16> %235, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %245 = shufflevector <8 x i16> %238, <8 x i16> %241, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %246 = bitcast <8 x i16> %242 to <4 x i32>
  %247 = bitcast <8 x i16> %243 to <4 x i32>
  %248 = shufflevector <4 x i32> %246, <4 x i32> %247, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %249 = bitcast <4 x i32> %248 to <2 x i64>
  %250 = bitcast <8 x i16> %244 to <4 x i32>
  %251 = bitcast <8 x i16> %245 to <4 x i32>
  %252 = shufflevector <4 x i32> %250, <4 x i32> %251, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %253 = bitcast <4 x i32> %252 to <2 x i64>
  %254 = shufflevector <4 x i32> %246, <4 x i32> %247, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %255 = bitcast <4 x i32> %254 to <2 x i64>
  %256 = shufflevector <4 x i32> %250, <4 x i32> %251, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %257 = bitcast <4 x i32> %256 to <2 x i64>
  %258 = insertelement <2 x i64> %249, i64 0, i32 1
  %259 = shufflevector <2 x i64> %249, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  %260 = insertelement <2 x i64> %255, i64 0, i32 1
  %261 = shufflevector <2 x i64> %255, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  %262 = insertelement <2 x i64> %253, i64 0, i32 1
  %263 = shufflevector <2 x i64> %253, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  %264 = insertelement <2 x i64> %257, i64 0, i32 1
  %265 = shufflevector <2 x i64> %257, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  %266 = getelementptr inbounds i8, i8* %3, i64 16
  %267 = bitcast i8* %266 to <8 x i16>*
  %268 = load <8 x i16>, <8 x i16>* %267, align 1
  %269 = getelementptr inbounds i8, i8* %3, i64 48
  %270 = bitcast i8* %269 to <8 x i16>*
  %271 = load <8 x i16>, <8 x i16>* %270, align 1
  %272 = getelementptr inbounds i8, i8* %3, i64 80
  %273 = bitcast i8* %272 to <8 x i16>*
  %274 = load <8 x i16>, <8 x i16>* %273, align 1
  %275 = getelementptr inbounds i8, i8* %3, i64 112
  %276 = bitcast i8* %275 to <8 x i16>*
  %277 = load <8 x i16>, <8 x i16>* %276, align 1
  %278 = shufflevector <8 x i16> %268, <8 x i16> %271, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %279 = shufflevector <8 x i16> %274, <8 x i16> %277, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %280 = shufflevector <8 x i16> %268, <8 x i16> %271, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %281 = shufflevector <8 x i16> %274, <8 x i16> %277, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %282 = bitcast <8 x i16> %278 to <4 x i32>
  %283 = bitcast <8 x i16> %279 to <4 x i32>
  %284 = shufflevector <4 x i32> %282, <4 x i32> %283, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %285 = bitcast <4 x i32> %284 to <2 x i64>
  %286 = bitcast <8 x i16> %280 to <4 x i32>
  %287 = bitcast <8 x i16> %281 to <4 x i32>
  %288 = shufflevector <4 x i32> %286, <4 x i32> %287, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %289 = bitcast <4 x i32> %288 to <2 x i64>
  %290 = shufflevector <4 x i32> %282, <4 x i32> %283, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %291 = bitcast <4 x i32> %290 to <2 x i64>
  %292 = shufflevector <4 x i32> %286, <4 x i32> %287, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %293 = bitcast <4 x i32> %292 to <2 x i64>
  %294 = insertelement <2 x i64> %285, i64 0, i32 1
  %295 = shufflevector <2 x i64> %285, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  %296 = insertelement <2 x i64> %291, i64 0, i32 1
  %297 = shufflevector <2 x i64> %291, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  %298 = insertelement <2 x i64> %289, i64 0, i32 1
  %299 = shufflevector <2 x i64> %289, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  %300 = insertelement <2 x i64> %293, i64 0, i32 1
  %301 = shufflevector <2 x i64> %293, <2 x i64> <i64 undef, i64 0>, <2 x i32> <i32 1, i32 3>
  %302 = bitcast <2 x i64> %301 to <8 x i16>
  %303 = bitcast <2 x i64> %258 to <8 x i16>
  %304 = shufflevector <8 x i16> %302, <8 x i16> %303, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %305 = shufflevector <8 x i16> %303, <8 x i16> %302, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %306 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 201, i16 4091, i16 201, i16 4091, i16 201, i16 4091, i16 201, i16 4091>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %307 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %304, <8 x i16> %306) #8
  %308 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %305, <8 x i16> <i16 201, i16 4091, i16 201, i16 4091, i16 201, i16 4091, i16 201, i16 4091>) #8
  %309 = add <4 x i32> %307, <i32 2048, i32 2048, i32 2048, i32 2048>
  %310 = ashr <4 x i32> %309, <i32 12, i32 12, i32 12, i32 12>
  %311 = add <4 x i32> %308, <i32 2048, i32 2048, i32 2048, i32 2048>
  %312 = ashr <4 x i32> %311, <i32 12, i32 12, i32 12, i32 12>
  %313 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %310, <4 x i32> %310) #8
  %314 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %312, <4 x i32> %312) #8
  %315 = bitcast <2 x i64> %299 to <8 x i16>
  %316 = bitcast <2 x i64> %260 to <8 x i16>
  %317 = shufflevector <8 x i16> %315, <8 x i16> %316, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %318 = shufflevector <8 x i16> %316, <8 x i16> %315, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %319 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 995, i16 3973, i16 995, i16 3973, i16 995, i16 3973, i16 995, i16 3973>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %320 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %317, <8 x i16> %319) #8
  %321 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %318, <8 x i16> <i16 995, i16 3973, i16 995, i16 3973, i16 995, i16 3973, i16 995, i16 3973>) #8
  %322 = add <4 x i32> %320, <i32 2048, i32 2048, i32 2048, i32 2048>
  %323 = ashr <4 x i32> %322, <i32 12, i32 12, i32 12, i32 12>
  %324 = add <4 x i32> %321, <i32 2048, i32 2048, i32 2048, i32 2048>
  %325 = ashr <4 x i32> %324, <i32 12, i32 12, i32 12, i32 12>
  %326 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %323, <4 x i32> %323) #8
  %327 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %325, <4 x i32> %325) #8
  %328 = bitcast <2 x i64> %297 to <8 x i16>
  %329 = bitcast <2 x i64> %262 to <8 x i16>
  %330 = shufflevector <8 x i16> %328, <8 x i16> %329, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %331 = shufflevector <8 x i16> %329, <8 x i16> %328, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %332 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1751, i16 3703, i16 1751, i16 3703, i16 1751, i16 3703, i16 1751, i16 3703>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %333 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %330, <8 x i16> %332) #8
  %334 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %331, <8 x i16> <i16 1751, i16 3703, i16 1751, i16 3703, i16 1751, i16 3703, i16 1751, i16 3703>) #8
  %335 = add <4 x i32> %333, <i32 2048, i32 2048, i32 2048, i32 2048>
  %336 = ashr <4 x i32> %335, <i32 12, i32 12, i32 12, i32 12>
  %337 = add <4 x i32> %334, <i32 2048, i32 2048, i32 2048, i32 2048>
  %338 = ashr <4 x i32> %337, <i32 12, i32 12, i32 12, i32 12>
  %339 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %336, <4 x i32> %336) #8
  %340 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %338, <4 x i32> %338) #8
  %341 = bitcast <2 x i64> %295 to <8 x i16>
  %342 = bitcast <2 x i64> %264 to <8 x i16>
  %343 = shufflevector <8 x i16> %341, <8 x i16> %342, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %344 = shufflevector <8 x i16> %342, <8 x i16> %341, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %345 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2440, i16 3290, i16 2440, i16 3290, i16 2440, i16 3290, i16 2440, i16 3290>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %346 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %343, <8 x i16> %345) #8
  %347 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %344, <8 x i16> <i16 2440, i16 3290, i16 2440, i16 3290, i16 2440, i16 3290, i16 2440, i16 3290>) #8
  %348 = add <4 x i32> %346, <i32 2048, i32 2048, i32 2048, i32 2048>
  %349 = ashr <4 x i32> %348, <i32 12, i32 12, i32 12, i32 12>
  %350 = add <4 x i32> %347, <i32 2048, i32 2048, i32 2048, i32 2048>
  %351 = ashr <4 x i32> %350, <i32 12, i32 12, i32 12, i32 12>
  %352 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %349, <4 x i32> %349) #8
  %353 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %351, <4 x i32> %351) #8
  %354 = bitcast <2 x i64> %265 to <8 x i16>
  %355 = bitcast <2 x i64> %294 to <8 x i16>
  %356 = shufflevector <8 x i16> %354, <8 x i16> %355, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %357 = shufflevector <8 x i16> %355, <8 x i16> %354, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %358 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3035, i16 2751, i16 3035, i16 2751, i16 3035, i16 2751, i16 3035, i16 2751>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %359 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %356, <8 x i16> %358) #8
  %360 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %357, <8 x i16> <i16 3035, i16 2751, i16 3035, i16 2751, i16 3035, i16 2751, i16 3035, i16 2751>) #8
  %361 = add <4 x i32> %359, <i32 2048, i32 2048, i32 2048, i32 2048>
  %362 = ashr <4 x i32> %361, <i32 12, i32 12, i32 12, i32 12>
  %363 = add <4 x i32> %360, <i32 2048, i32 2048, i32 2048, i32 2048>
  %364 = ashr <4 x i32> %363, <i32 12, i32 12, i32 12, i32 12>
  %365 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %362, <4 x i32> %362) #8
  %366 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %364, <4 x i32> %364) #8
  %367 = bitcast <2 x i64> %263 to <8 x i16>
  %368 = bitcast <2 x i64> %296 to <8 x i16>
  %369 = shufflevector <8 x i16> %367, <8 x i16> %368, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %370 = shufflevector <8 x i16> %368, <8 x i16> %367, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %371 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3513, i16 2106, i16 3513, i16 2106, i16 3513, i16 2106, i16 3513, i16 2106>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %372 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %369, <8 x i16> %371) #8
  %373 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %370, <8 x i16> <i16 3513, i16 2106, i16 3513, i16 2106, i16 3513, i16 2106, i16 3513, i16 2106>) #8
  %374 = add <4 x i32> %372, <i32 2048, i32 2048, i32 2048, i32 2048>
  %375 = ashr <4 x i32> %374, <i32 12, i32 12, i32 12, i32 12>
  %376 = add <4 x i32> %373, <i32 2048, i32 2048, i32 2048, i32 2048>
  %377 = ashr <4 x i32> %376, <i32 12, i32 12, i32 12, i32 12>
  %378 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %375, <4 x i32> %375) #8
  %379 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %377, <4 x i32> %377) #8
  %380 = bitcast <2 x i64> %261 to <8 x i16>
  %381 = bitcast <2 x i64> %298 to <8 x i16>
  %382 = shufflevector <8 x i16> %380, <8 x i16> %381, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %383 = shufflevector <8 x i16> %381, <8 x i16> %380, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %384 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3857, i16 1380, i16 3857, i16 1380, i16 3857, i16 1380, i16 3857, i16 1380>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %385 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %382, <8 x i16> %384) #8
  %386 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %383, <8 x i16> <i16 3857, i16 1380, i16 3857, i16 1380, i16 3857, i16 1380, i16 3857, i16 1380>) #8
  %387 = add <4 x i32> %385, <i32 2048, i32 2048, i32 2048, i32 2048>
  %388 = ashr <4 x i32> %387, <i32 12, i32 12, i32 12, i32 12>
  %389 = add <4 x i32> %386, <i32 2048, i32 2048, i32 2048, i32 2048>
  %390 = ashr <4 x i32> %389, <i32 12, i32 12, i32 12, i32 12>
  %391 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %388, <4 x i32> %388) #8
  %392 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %390, <4 x i32> %390) #8
  %393 = bitcast <2 x i64> %259 to <8 x i16>
  %394 = bitcast <2 x i64> %300 to <8 x i16>
  %395 = shufflevector <8 x i16> %393, <8 x i16> %394, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %396 = shufflevector <8 x i16> %394, <8 x i16> %393, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %397 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 4052, i16 601, i16 4052, i16 601, i16 4052, i16 601, i16 4052, i16 601>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %398 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %395, <8 x i16> %397) #8
  %399 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %396, <8 x i16> <i16 4052, i16 601, i16 4052, i16 601, i16 4052, i16 601, i16 4052, i16 601>) #8
  %400 = add <4 x i32> %398, <i32 2048, i32 2048, i32 2048, i32 2048>
  %401 = ashr <4 x i32> %400, <i32 12, i32 12, i32 12, i32 12>
  %402 = add <4 x i32> %399, <i32 2048, i32 2048, i32 2048, i32 2048>
  %403 = ashr <4 x i32> %402, <i32 12, i32 12, i32 12, i32 12>
  %404 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %401, <4 x i32> %401) #8
  %405 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %403, <4 x i32> %403) #8
  %406 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %314, <8 x i16> %366) #8
  %407 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %314, <8 x i16> %366) #8
  %408 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %313, <8 x i16> %365) #8
  %409 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %313, <8 x i16> %365) #8
  %410 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %327, <8 x i16> %379) #8
  %411 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %327, <8 x i16> %379) #8
  %412 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %326, <8 x i16> %378) #8
  %413 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %326, <8 x i16> %378) #8
  %414 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %340, <8 x i16> %392) #8
  %415 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %340, <8 x i16> %392) #8
  %416 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %339, <8 x i16> %391) #8
  %417 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %339, <8 x i16> %391) #8
  %418 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %353, <8 x i16> %405) #8
  %419 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %353, <8 x i16> %405) #8
  %420 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %352, <8 x i16> %404) #8
  %421 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %352, <8 x i16> %404) #8
  %422 = shufflevector <8 x i16> %407, <8 x i16> %409, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %423 = shufflevector <8 x i16> %409, <8 x i16> %407, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %424 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %425 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %422, <8 x i16> %424) #8
  %426 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %423, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %427 = add <4 x i32> %425, <i32 2048, i32 2048, i32 2048, i32 2048>
  %428 = ashr <4 x i32> %427, <i32 12, i32 12, i32 12, i32 12>
  %429 = add <4 x i32> %426, <i32 2048, i32 2048, i32 2048, i32 2048>
  %430 = ashr <4 x i32> %429, <i32 12, i32 12, i32 12, i32 12>
  %431 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %428, <4 x i32> %428) #8
  %432 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %430, <4 x i32> %430) #8
  %433 = shufflevector <8 x i16> %417, <8 x i16> %415, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %434 = shufflevector <8 x i16> %415, <8 x i16> %417, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %435 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %436 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %433, <8 x i16> %435) #8
  %437 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %434, <8 x i16> <i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799>) #8
  %438 = add <4 x i32> %436, <i32 2048, i32 2048, i32 2048, i32 2048>
  %439 = ashr <4 x i32> %438, <i32 12, i32 12, i32 12, i32 12>
  %440 = add <4 x i32> %437, <i32 2048, i32 2048, i32 2048, i32 2048>
  %441 = ashr <4 x i32> %440, <i32 12, i32 12, i32 12, i32 12>
  %442 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %439, <4 x i32> %439) #8
  %443 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %441, <4 x i32> %441) #8
  %444 = shufflevector <8 x i16> %411, <8 x i16> %413, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %445 = shufflevector <8 x i16> %413, <8 x i16> %411, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %446 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %447 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %444, <8 x i16> %446) #8
  %448 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %445, <8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>) #8
  %449 = add <4 x i32> %447, <i32 2048, i32 2048, i32 2048, i32 2048>
  %450 = ashr <4 x i32> %449, <i32 12, i32 12, i32 12, i32 12>
  %451 = add <4 x i32> %448, <i32 2048, i32 2048, i32 2048, i32 2048>
  %452 = ashr <4 x i32> %451, <i32 12, i32 12, i32 12, i32 12>
  %453 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %450, <4 x i32> %450) #8
  %454 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %452, <4 x i32> %452) #8
  %455 = shufflevector <8 x i16> %421, <8 x i16> %419, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %456 = shufflevector <8 x i16> %419, <8 x i16> %421, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %457 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %458 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %455, <8 x i16> %457) #8
  %459 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %456, <8 x i16> <i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406>) #8
  %460 = add <4 x i32> %458, <i32 2048, i32 2048, i32 2048, i32 2048>
  %461 = ashr <4 x i32> %460, <i32 12, i32 12, i32 12, i32 12>
  %462 = add <4 x i32> %459, <i32 2048, i32 2048, i32 2048, i32 2048>
  %463 = ashr <4 x i32> %462, <i32 12, i32 12, i32 12, i32 12>
  %464 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %461, <4 x i32> %461) #8
  %465 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %463, <4 x i32> %463) #8
  %466 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %406, <8 x i16> %414) #8
  %467 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %406, <8 x i16> %414) #8
  %468 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %432, <8 x i16> %442) #8
  %469 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %432, <8 x i16> %442) #8
  %470 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %408, <8 x i16> %416) #8
  %471 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %408, <8 x i16> %416) #8
  %472 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %431, <8 x i16> %443) #8
  %473 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %431, <8 x i16> %443) #8
  %474 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %410, <8 x i16> %418) #8
  %475 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %410, <8 x i16> %418) #8
  %476 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %454, <8 x i16> %464) #8
  %477 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %454, <8 x i16> %464) #8
  %478 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %412, <8 x i16> %420) #8
  %479 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %412, <8 x i16> %420) #8
  %480 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %453, <8 x i16> %465) #8
  %481 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %453, <8 x i16> %465) #8
  %482 = shufflevector <8 x i16> %467, <8 x i16> %471, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %483 = shufflevector <8 x i16> %471, <8 x i16> %467, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %484 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %485 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %482, <8 x i16> %484) #8
  %486 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %483, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %487 = add <4 x i32> %485, <i32 2048, i32 2048, i32 2048, i32 2048>
  %488 = ashr <4 x i32> %487, <i32 12, i32 12, i32 12, i32 12>
  %489 = add <4 x i32> %486, <i32 2048, i32 2048, i32 2048, i32 2048>
  %490 = ashr <4 x i32> %489, <i32 12, i32 12, i32 12, i32 12>
  %491 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %488, <4 x i32> %488) #8
  %492 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %490, <4 x i32> %490) #8
  %493 = shufflevector <8 x i16> %469, <8 x i16> %473, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %494 = shufflevector <8 x i16> %473, <8 x i16> %469, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %495 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %493, <8 x i16> %484) #8
  %496 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %494, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %497 = add <4 x i32> %495, <i32 2048, i32 2048, i32 2048, i32 2048>
  %498 = ashr <4 x i32> %497, <i32 12, i32 12, i32 12, i32 12>
  %499 = add <4 x i32> %496, <i32 2048, i32 2048, i32 2048, i32 2048>
  %500 = ashr <4 x i32> %499, <i32 12, i32 12, i32 12, i32 12>
  %501 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %498, <4 x i32> %498) #8
  %502 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %500, <4 x i32> %500) #8
  %503 = shufflevector <8 x i16> %479, <8 x i16> %475, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %504 = shufflevector <8 x i16> %475, <8 x i16> %479, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %505 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %506 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %503, <8 x i16> %505) #8
  %507 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %504, <8 x i16> <i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567>) #8
  %508 = add <4 x i32> %506, <i32 2048, i32 2048, i32 2048, i32 2048>
  %509 = ashr <4 x i32> %508, <i32 12, i32 12, i32 12, i32 12>
  %510 = add <4 x i32> %507, <i32 2048, i32 2048, i32 2048, i32 2048>
  %511 = ashr <4 x i32> %510, <i32 12, i32 12, i32 12, i32 12>
  %512 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %509, <4 x i32> %509) #8
  %513 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %511, <4 x i32> %511) #8
  %514 = shufflevector <8 x i16> %481, <8 x i16> %477, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %515 = shufflevector <8 x i16> %477, <8 x i16> %481, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %516 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %514, <8 x i16> %505) #8
  %517 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %515, <8 x i16> <i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567>) #8
  %518 = add <4 x i32> %516, <i32 2048, i32 2048, i32 2048, i32 2048>
  %519 = ashr <4 x i32> %518, <i32 12, i32 12, i32 12, i32 12>
  %520 = add <4 x i32> %517, <i32 2048, i32 2048, i32 2048, i32 2048>
  %521 = ashr <4 x i32> %520, <i32 12, i32 12, i32 12, i32 12>
  %522 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %519, <4 x i32> %519) #8
  %523 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %521, <4 x i32> %521) #8
  %524 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %466, <8 x i16> %474) #8
  %525 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %466, <8 x i16> %474) #8
  %526 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %492, <8 x i16> %512) #8
  %527 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %492, <8 x i16> %512) #8
  %528 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %468, <8 x i16> %476) #8
  %529 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %468, <8 x i16> %476) #8
  %530 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %502, <8 x i16> %522) #8
  %531 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %502, <8 x i16> %522) #8
  %532 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %470, <8 x i16> %478) #8
  %533 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %470, <8 x i16> %478) #8
  %534 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %491, <8 x i16> %513) #8
  %535 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %491, <8 x i16> %513) #8
  %536 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %472, <8 x i16> %480) #8
  %537 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %472, <8 x i16> %480) #8
  %538 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %501, <8 x i16> %523) #8
  %539 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %501, <8 x i16> %523) #8
  %540 = shufflevector <8 x i16> %525, <8 x i16> %533, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %541 = shufflevector <8 x i16> %533, <8 x i16> %525, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %542 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %543 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %540, <8 x i16> %542) #8
  %544 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %541, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %545 = add <4 x i32> %543, <i32 2048, i32 2048, i32 2048, i32 2048>
  %546 = ashr <4 x i32> %545, <i32 12, i32 12, i32 12, i32 12>
  %547 = add <4 x i32> %544, <i32 2048, i32 2048, i32 2048, i32 2048>
  %548 = ashr <4 x i32> %547, <i32 12, i32 12, i32 12, i32 12>
  %549 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %546, <4 x i32> undef) #8
  %550 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %548, <4 x i32> %548) #8
  %551 = shufflevector <8 x i16> %527, <8 x i16> %535, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %552 = shufflevector <8 x i16> %535, <8 x i16> %527, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %553 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %551, <8 x i16> %542) #8
  %554 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %552, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %555 = add <4 x i32> %553, <i32 2048, i32 2048, i32 2048, i32 2048>
  %556 = ashr <4 x i32> %555, <i32 12, i32 12, i32 12, i32 12>
  %557 = add <4 x i32> %554, <i32 2048, i32 2048, i32 2048, i32 2048>
  %558 = ashr <4 x i32> %557, <i32 12, i32 12, i32 12, i32 12>
  %559 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %556, <4 x i32> %556) #8
  %560 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %558, <4 x i32> undef) #8
  %561 = shufflevector <8 x i16> %529, <8 x i16> %537, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %562 = shufflevector <8 x i16> %537, <8 x i16> %529, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %563 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %561, <8 x i16> %542) #8
  %564 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %562, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %565 = add <4 x i32> %563, <i32 2048, i32 2048, i32 2048, i32 2048>
  %566 = ashr <4 x i32> %565, <i32 12, i32 12, i32 12, i32 12>
  %567 = add <4 x i32> %564, <i32 2048, i32 2048, i32 2048, i32 2048>
  %568 = ashr <4 x i32> %567, <i32 12, i32 12, i32 12, i32 12>
  %569 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %566, <4 x i32> %566) #8
  %570 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %568, <4 x i32> undef) #8
  %571 = shufflevector <8 x i16> %531, <8 x i16> %539, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %572 = shufflevector <8 x i16> %539, <8 x i16> %531, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %573 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %571, <8 x i16> %542) #8
  %574 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %572, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %575 = add <4 x i32> %573, <i32 2048, i32 2048, i32 2048, i32 2048>
  %576 = ashr <4 x i32> %575, <i32 12, i32 12, i32 12, i32 12>
  %577 = add <4 x i32> %574, <i32 2048, i32 2048, i32 2048, i32 2048>
  %578 = ashr <4 x i32> %577, <i32 12, i32 12, i32 12, i32 12>
  %579 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %576, <4 x i32> undef) #8
  %580 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %578, <4 x i32> %578) #8
  %581 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %528) #8
  %582 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %526) #8
  %583 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %580) #8
  %584 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %550) #8
  %585 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %569) #8
  %586 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %559) #8
  %587 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %538) #8
  %588 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %532) #8
  %589 = shufflevector <8 x i16> %524, <8 x i16> %581, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %590 = shufflevector <8 x i16> %530, <8 x i16> %582, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %591 = shufflevector <8 x i16> %560, <8 x i16> %583, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %592 = shufflevector <8 x i16> %570, <8 x i16> %584, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %593 = bitcast <8 x i16> %589 to <4 x i32>
  %594 = bitcast <8 x i16> %590 to <4 x i32>
  %595 = shufflevector <4 x i32> %593, <4 x i32> %594, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %596 = bitcast <4 x i32> %595 to <2 x i64>
  %597 = bitcast <8 x i16> %591 to <4 x i32>
  %598 = bitcast <8 x i16> %592 to <4 x i32>
  %599 = shufflevector <4 x i32> %597, <4 x i32> %598, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %600 = bitcast <4 x i32> %599 to <2 x i64>
  %601 = shufflevector <4 x i32> %593, <4 x i32> %594, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %602 = bitcast <4 x i32> %601 to <2 x i64>
  %603 = shufflevector <4 x i32> %597, <4 x i32> %598, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %604 = bitcast <4 x i32> %603 to <2 x i64>
  %605 = shufflevector <2 x i64> %596, <2 x i64> %600, <2 x i32> <i32 0, i32 2>
  %606 = shufflevector <2 x i64> %596, <2 x i64> %600, <2 x i32> <i32 1, i32 3>
  %607 = shufflevector <2 x i64> %602, <2 x i64> %604, <2 x i32> <i32 0, i32 2>
  %608 = shufflevector <2 x i64> %602, <2 x i64> %604, <2 x i32> <i32 1, i32 3>
  %609 = bitcast i8* %3 to <2 x i64>*
  store <2 x i64> %605, <2 x i64>* %609, align 1
  %610 = bitcast i8* %233 to <2 x i64>*
  store <2 x i64> %606, <2 x i64>* %610, align 1
  %611 = bitcast i8* %236 to <2 x i64>*
  store <2 x i64> %607, <2 x i64>* %611, align 1
  %612 = bitcast i8* %239 to <2 x i64>*
  store <2 x i64> %608, <2 x i64>* %612, align 1
  %613 = shufflevector <8 x i16> %549, <8 x i16> %585, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %614 = shufflevector <8 x i16> %579, <8 x i16> %586, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %615 = shufflevector <8 x i16> %534, <8 x i16> %587, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %616 = shufflevector <8 x i16> %536, <8 x i16> %588, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %617 = bitcast <8 x i16> %613 to <4 x i32>
  %618 = bitcast <8 x i16> %614 to <4 x i32>
  %619 = shufflevector <4 x i32> %617, <4 x i32> %618, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %620 = bitcast <4 x i32> %619 to <2 x i64>
  %621 = bitcast <8 x i16> %615 to <4 x i32>
  %622 = bitcast <8 x i16> %616 to <4 x i32>
  %623 = shufflevector <4 x i32> %621, <4 x i32> %622, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %624 = bitcast <4 x i32> %623 to <2 x i64>
  %625 = shufflevector <4 x i32> %617, <4 x i32> %618, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %626 = bitcast <4 x i32> %625 to <2 x i64>
  %627 = shufflevector <4 x i32> %621, <4 x i32> %622, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %628 = bitcast <4 x i32> %627 to <2 x i64>
  %629 = shufflevector <2 x i64> %620, <2 x i64> %624, <2 x i32> <i32 0, i32 2>
  %630 = shufflevector <2 x i64> %620, <2 x i64> %624, <2 x i32> <i32 1, i32 3>
  %631 = shufflevector <2 x i64> %626, <2 x i64> %628, <2 x i32> <i32 0, i32 2>
  %632 = shufflevector <2 x i64> %626, <2 x i64> %628, <2 x i32> <i32 1, i32 3>
  %633 = bitcast i8* %266 to <2 x i64>*
  store <2 x i64> %629, <2 x i64>* %633, align 1
  %634 = bitcast i8* %269 to <2 x i64>*
  store <2 x i64> %630, <2 x i64>* %634, align 1
  %635 = bitcast i8* %272 to <2 x i64>*
  store <2 x i64> %631, <2 x i64>* %635, align 1
  %636 = bitcast i8* %275 to <2 x i64>*
  store <2 x i64> %632, <2 x i64>* %636, align 1
  %637 = sext i32 %2 to i64
  br label %1300

638:                                              ; preds = %213, %638
  %639 = phi i64 [ 0, %213 ], [ %1298, %638 ]
  %640 = shl i64 %639, 4
  %641 = and i64 %640, 4294967168
  %642 = getelementptr inbounds i16, i16* %8, i64 %641
  %643 = bitcast i16* %642 to <8 x i16>*
  %644 = load <8 x i16>, <8 x i16>* %643, align 1
  %645 = getelementptr inbounds i16, i16* %642, i64 16
  %646 = bitcast i16* %645 to <8 x i16>*
  %647 = load <8 x i16>, <8 x i16>* %646, align 1
  %648 = getelementptr inbounds i16, i16* %642, i64 32
  %649 = bitcast i16* %648 to <8 x i16>*
  %650 = load <8 x i16>, <8 x i16>* %649, align 1
  %651 = getelementptr inbounds i16, i16* %642, i64 48
  %652 = bitcast i16* %651 to <8 x i16>*
  %653 = load <8 x i16>, <8 x i16>* %652, align 1
  %654 = getelementptr inbounds i16, i16* %642, i64 64
  %655 = bitcast i16* %654 to <8 x i16>*
  %656 = load <8 x i16>, <8 x i16>* %655, align 1
  %657 = getelementptr inbounds i16, i16* %642, i64 80
  %658 = bitcast i16* %657 to <8 x i16>*
  %659 = load <8 x i16>, <8 x i16>* %658, align 1
  %660 = getelementptr inbounds i16, i16* %642, i64 96
  %661 = bitcast i16* %660 to <8 x i16>*
  %662 = load <8 x i16>, <8 x i16>* %661, align 1
  %663 = getelementptr inbounds i16, i16* %642, i64 112
  %664 = bitcast i16* %663 to <8 x i16>*
  %665 = load <8 x i16>, <8 x i16>* %664, align 1
  %666 = shufflevector <8 x i16> %644, <8 x i16> %647, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %667 = shufflevector <8 x i16> %650, <8 x i16> %653, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %668 = shufflevector <8 x i16> %656, <8 x i16> %659, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %669 = shufflevector <8 x i16> %662, <8 x i16> %665, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %670 = shufflevector <8 x i16> %644, <8 x i16> %647, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %671 = shufflevector <8 x i16> %650, <8 x i16> %653, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %672 = shufflevector <8 x i16> %656, <8 x i16> %659, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %673 = shufflevector <8 x i16> %662, <8 x i16> %665, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %674 = bitcast <8 x i16> %666 to <4 x i32>
  %675 = bitcast <8 x i16> %667 to <4 x i32>
  %676 = shufflevector <4 x i32> %674, <4 x i32> %675, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %677 = bitcast <4 x i32> %676 to <2 x i64>
  %678 = bitcast <8 x i16> %668 to <4 x i32>
  %679 = bitcast <8 x i16> %669 to <4 x i32>
  %680 = shufflevector <4 x i32> %678, <4 x i32> %679, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %681 = bitcast <4 x i32> %680 to <2 x i64>
  %682 = bitcast <8 x i16> %670 to <4 x i32>
  %683 = bitcast <8 x i16> %671 to <4 x i32>
  %684 = shufflevector <4 x i32> %682, <4 x i32> %683, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %685 = bitcast <4 x i32> %684 to <2 x i64>
  %686 = bitcast <8 x i16> %672 to <4 x i32>
  %687 = bitcast <8 x i16> %673 to <4 x i32>
  %688 = shufflevector <4 x i32> %686, <4 x i32> %687, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %689 = bitcast <4 x i32> %688 to <2 x i64>
  %690 = shufflevector <4 x i32> %674, <4 x i32> %675, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %691 = bitcast <4 x i32> %690 to <2 x i64>
  %692 = shufflevector <4 x i32> %678, <4 x i32> %679, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %693 = bitcast <4 x i32> %692 to <2 x i64>
  %694 = shufflevector <4 x i32> %682, <4 x i32> %683, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %695 = bitcast <4 x i32> %694 to <2 x i64>
  %696 = shufflevector <4 x i32> %686, <4 x i32> %687, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %697 = bitcast <4 x i32> %696 to <2 x i64>
  %698 = shufflevector <2 x i64> %677, <2 x i64> %681, <2 x i32> <i32 0, i32 2>
  %699 = shufflevector <2 x i64> %677, <2 x i64> %681, <2 x i32> <i32 1, i32 3>
  %700 = shufflevector <2 x i64> %691, <2 x i64> %693, <2 x i32> <i32 0, i32 2>
  %701 = shufflevector <2 x i64> %691, <2 x i64> %693, <2 x i32> <i32 1, i32 3>
  %702 = shufflevector <2 x i64> %685, <2 x i64> %689, <2 x i32> <i32 0, i32 2>
  %703 = shufflevector <2 x i64> %685, <2 x i64> %689, <2 x i32> <i32 1, i32 3>
  %704 = shufflevector <2 x i64> %695, <2 x i64> %697, <2 x i32> <i32 0, i32 2>
  %705 = shufflevector <2 x i64> %695, <2 x i64> %697, <2 x i32> <i32 1, i32 3>
  %706 = getelementptr inbounds i16, i16* %642, i64 8
  %707 = bitcast i16* %706 to <8 x i16>*
  %708 = load <8 x i16>, <8 x i16>* %707, align 1
  %709 = getelementptr inbounds i16, i16* %642, i64 24
  %710 = bitcast i16* %709 to <8 x i16>*
  %711 = load <8 x i16>, <8 x i16>* %710, align 1
  %712 = getelementptr inbounds i16, i16* %642, i64 40
  %713 = bitcast i16* %712 to <8 x i16>*
  %714 = load <8 x i16>, <8 x i16>* %713, align 1
  %715 = getelementptr inbounds i16, i16* %642, i64 56
  %716 = bitcast i16* %715 to <8 x i16>*
  %717 = load <8 x i16>, <8 x i16>* %716, align 1
  %718 = getelementptr inbounds i16, i16* %642, i64 72
  %719 = bitcast i16* %718 to <8 x i16>*
  %720 = load <8 x i16>, <8 x i16>* %719, align 1
  %721 = getelementptr inbounds i16, i16* %642, i64 88
  %722 = bitcast i16* %721 to <8 x i16>*
  %723 = load <8 x i16>, <8 x i16>* %722, align 1
  %724 = getelementptr inbounds i16, i16* %642, i64 104
  %725 = bitcast i16* %724 to <8 x i16>*
  %726 = load <8 x i16>, <8 x i16>* %725, align 1
  %727 = getelementptr inbounds i16, i16* %642, i64 120
  %728 = bitcast i16* %727 to <8 x i16>*
  %729 = load <8 x i16>, <8 x i16>* %728, align 1
  %730 = shufflevector <8 x i16> %708, <8 x i16> %711, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %731 = shufflevector <8 x i16> %714, <8 x i16> %717, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %732 = shufflevector <8 x i16> %720, <8 x i16> %723, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %733 = shufflevector <8 x i16> %726, <8 x i16> %729, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %734 = shufflevector <8 x i16> %708, <8 x i16> %711, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %735 = shufflevector <8 x i16> %714, <8 x i16> %717, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %736 = shufflevector <8 x i16> %720, <8 x i16> %723, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %737 = shufflevector <8 x i16> %726, <8 x i16> %729, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %738 = bitcast <8 x i16> %730 to <4 x i32>
  %739 = bitcast <8 x i16> %731 to <4 x i32>
  %740 = shufflevector <4 x i32> %738, <4 x i32> %739, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %741 = bitcast <4 x i32> %740 to <2 x i64>
  %742 = bitcast <8 x i16> %732 to <4 x i32>
  %743 = bitcast <8 x i16> %733 to <4 x i32>
  %744 = shufflevector <4 x i32> %742, <4 x i32> %743, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %745 = bitcast <4 x i32> %744 to <2 x i64>
  %746 = bitcast <8 x i16> %734 to <4 x i32>
  %747 = bitcast <8 x i16> %735 to <4 x i32>
  %748 = shufflevector <4 x i32> %746, <4 x i32> %747, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %749 = bitcast <4 x i32> %748 to <2 x i64>
  %750 = bitcast <8 x i16> %736 to <4 x i32>
  %751 = bitcast <8 x i16> %737 to <4 x i32>
  %752 = shufflevector <4 x i32> %750, <4 x i32> %751, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %753 = bitcast <4 x i32> %752 to <2 x i64>
  %754 = shufflevector <4 x i32> %738, <4 x i32> %739, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %755 = bitcast <4 x i32> %754 to <2 x i64>
  %756 = shufflevector <4 x i32> %742, <4 x i32> %743, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %757 = bitcast <4 x i32> %756 to <2 x i64>
  %758 = shufflevector <4 x i32> %746, <4 x i32> %747, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %759 = bitcast <4 x i32> %758 to <2 x i64>
  %760 = shufflevector <4 x i32> %750, <4 x i32> %751, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %761 = bitcast <4 x i32> %760 to <2 x i64>
  %762 = shufflevector <2 x i64> %741, <2 x i64> %745, <2 x i32> <i32 0, i32 2>
  %763 = shufflevector <2 x i64> %741, <2 x i64> %745, <2 x i32> <i32 1, i32 3>
  %764 = shufflevector <2 x i64> %755, <2 x i64> %757, <2 x i32> <i32 0, i32 2>
  %765 = shufflevector <2 x i64> %755, <2 x i64> %757, <2 x i32> <i32 1, i32 3>
  %766 = shufflevector <2 x i64> %749, <2 x i64> %753, <2 x i32> <i32 0, i32 2>
  %767 = shufflevector <2 x i64> %749, <2 x i64> %753, <2 x i32> <i32 1, i32 3>
  %768 = shufflevector <2 x i64> %759, <2 x i64> %761, <2 x i32> <i32 0, i32 2>
  %769 = shufflevector <2 x i64> %759, <2 x i64> %761, <2 x i32> <i32 1, i32 3>
  %770 = bitcast <2 x i64> %698 to <8 x i16>
  %771 = bitcast <2 x i64> %700 to <8 x i16>
  %772 = bitcast <2 x i64> %702 to <8 x i16>
  %773 = bitcast <2 x i64> %704 to <8 x i16>
  %774 = bitcast <2 x i64> %705 to <8 x i16>
  %775 = bitcast <2 x i64> %703 to <8 x i16>
  %776 = bitcast <2 x i64> %701 to <8 x i16>
  %777 = bitcast <2 x i64> %768 to <8 x i16>
  %778 = bitcast <2 x i64> %766 to <8 x i16>
  %779 = bitcast <2 x i64> %764 to <8 x i16>
  %780 = bitcast <2 x i64> %762 to <8 x i16>
  %781 = bitcast <2 x i64> %763 to <8 x i16>
  %782 = bitcast <2 x i64> %765 to <8 x i16>
  %783 = bitcast <2 x i64> %767 to <8 x i16>
  %784 = bitcast <2 x i64> %769 to <8 x i16>
  %785 = bitcast <2 x i64> %699 to <8 x i16>
  %786 = shufflevector <8 x i16> %784, <8 x i16> %770, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %787 = shufflevector <8 x i16> %770, <8 x i16> %784, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %788 = shufflevector <8 x i16> %784, <8 x i16> %770, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %789 = shufflevector <8 x i16> %770, <8 x i16> %784, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %790 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %786, <8 x i16> %214) #8
  %791 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %787, <8 x i16> <i16 201, i16 4091, i16 201, i16 4091, i16 201, i16 4091, i16 201, i16 4091>) #8
  %792 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %788, <8 x i16> %214) #8
  %793 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %789, <8 x i16> <i16 201, i16 4091, i16 201, i16 4091, i16 201, i16 4091, i16 201, i16 4091>) #8
  %794 = add <4 x i32> %790, <i32 2048, i32 2048, i32 2048, i32 2048>
  %795 = ashr <4 x i32> %794, <i32 12, i32 12, i32 12, i32 12>
  %796 = add <4 x i32> %791, <i32 2048, i32 2048, i32 2048, i32 2048>
  %797 = ashr <4 x i32> %796, <i32 12, i32 12, i32 12, i32 12>
  %798 = add <4 x i32> %792, <i32 2048, i32 2048, i32 2048, i32 2048>
  %799 = ashr <4 x i32> %798, <i32 12, i32 12, i32 12, i32 12>
  %800 = add <4 x i32> %793, <i32 2048, i32 2048, i32 2048, i32 2048>
  %801 = ashr <4 x i32> %800, <i32 12, i32 12, i32 12, i32 12>
  %802 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %795, <4 x i32> %799) #8
  %803 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %797, <4 x i32> %801) #8
  %804 = shufflevector <8 x i16> %783, <8 x i16> %771, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %805 = shufflevector <8 x i16> %771, <8 x i16> %783, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %806 = shufflevector <8 x i16> %783, <8 x i16> %771, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %807 = shufflevector <8 x i16> %771, <8 x i16> %783, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %808 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %804, <8 x i16> %215) #8
  %809 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %805, <8 x i16> <i16 995, i16 3973, i16 995, i16 3973, i16 995, i16 3973, i16 995, i16 3973>) #8
  %810 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %806, <8 x i16> %215) #8
  %811 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %807, <8 x i16> <i16 995, i16 3973, i16 995, i16 3973, i16 995, i16 3973, i16 995, i16 3973>) #8
  %812 = add <4 x i32> %808, <i32 2048, i32 2048, i32 2048, i32 2048>
  %813 = ashr <4 x i32> %812, <i32 12, i32 12, i32 12, i32 12>
  %814 = add <4 x i32> %809, <i32 2048, i32 2048, i32 2048, i32 2048>
  %815 = ashr <4 x i32> %814, <i32 12, i32 12, i32 12, i32 12>
  %816 = add <4 x i32> %810, <i32 2048, i32 2048, i32 2048, i32 2048>
  %817 = ashr <4 x i32> %816, <i32 12, i32 12, i32 12, i32 12>
  %818 = add <4 x i32> %811, <i32 2048, i32 2048, i32 2048, i32 2048>
  %819 = ashr <4 x i32> %818, <i32 12, i32 12, i32 12, i32 12>
  %820 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %813, <4 x i32> %817) #8
  %821 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %815, <4 x i32> %819) #8
  %822 = shufflevector <8 x i16> %782, <8 x i16> %772, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %823 = shufflevector <8 x i16> %772, <8 x i16> %782, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %824 = shufflevector <8 x i16> %782, <8 x i16> %772, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %825 = shufflevector <8 x i16> %772, <8 x i16> %782, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %826 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %822, <8 x i16> %216) #8
  %827 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %823, <8 x i16> <i16 1751, i16 3703, i16 1751, i16 3703, i16 1751, i16 3703, i16 1751, i16 3703>) #8
  %828 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %824, <8 x i16> %216) #8
  %829 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %825, <8 x i16> <i16 1751, i16 3703, i16 1751, i16 3703, i16 1751, i16 3703, i16 1751, i16 3703>) #8
  %830 = add <4 x i32> %826, <i32 2048, i32 2048, i32 2048, i32 2048>
  %831 = ashr <4 x i32> %830, <i32 12, i32 12, i32 12, i32 12>
  %832 = add <4 x i32> %827, <i32 2048, i32 2048, i32 2048, i32 2048>
  %833 = ashr <4 x i32> %832, <i32 12, i32 12, i32 12, i32 12>
  %834 = add <4 x i32> %828, <i32 2048, i32 2048, i32 2048, i32 2048>
  %835 = ashr <4 x i32> %834, <i32 12, i32 12, i32 12, i32 12>
  %836 = add <4 x i32> %829, <i32 2048, i32 2048, i32 2048, i32 2048>
  %837 = ashr <4 x i32> %836, <i32 12, i32 12, i32 12, i32 12>
  %838 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %831, <4 x i32> %835) #8
  %839 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %833, <4 x i32> %837) #8
  %840 = shufflevector <8 x i16> %781, <8 x i16> %773, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %841 = shufflevector <8 x i16> %773, <8 x i16> %781, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %842 = shufflevector <8 x i16> %781, <8 x i16> %773, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %843 = shufflevector <8 x i16> %773, <8 x i16> %781, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %844 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %840, <8 x i16> %217) #8
  %845 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %841, <8 x i16> <i16 2440, i16 3290, i16 2440, i16 3290, i16 2440, i16 3290, i16 2440, i16 3290>) #8
  %846 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %842, <8 x i16> %217) #8
  %847 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %843, <8 x i16> <i16 2440, i16 3290, i16 2440, i16 3290, i16 2440, i16 3290, i16 2440, i16 3290>) #8
  %848 = add <4 x i32> %844, <i32 2048, i32 2048, i32 2048, i32 2048>
  %849 = ashr <4 x i32> %848, <i32 12, i32 12, i32 12, i32 12>
  %850 = add <4 x i32> %845, <i32 2048, i32 2048, i32 2048, i32 2048>
  %851 = ashr <4 x i32> %850, <i32 12, i32 12, i32 12, i32 12>
  %852 = add <4 x i32> %846, <i32 2048, i32 2048, i32 2048, i32 2048>
  %853 = ashr <4 x i32> %852, <i32 12, i32 12, i32 12, i32 12>
  %854 = add <4 x i32> %847, <i32 2048, i32 2048, i32 2048, i32 2048>
  %855 = ashr <4 x i32> %854, <i32 12, i32 12, i32 12, i32 12>
  %856 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %849, <4 x i32> %853) #8
  %857 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %851, <4 x i32> %855) #8
  %858 = shufflevector <8 x i16> %774, <8 x i16> %780, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %859 = shufflevector <8 x i16> %780, <8 x i16> %774, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %860 = shufflevector <8 x i16> %774, <8 x i16> %780, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %861 = shufflevector <8 x i16> %780, <8 x i16> %774, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %862 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %858, <8 x i16> %218) #8
  %863 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %859, <8 x i16> <i16 3035, i16 2751, i16 3035, i16 2751, i16 3035, i16 2751, i16 3035, i16 2751>) #8
  %864 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %860, <8 x i16> %218) #8
  %865 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %861, <8 x i16> <i16 3035, i16 2751, i16 3035, i16 2751, i16 3035, i16 2751, i16 3035, i16 2751>) #8
  %866 = add <4 x i32> %862, <i32 2048, i32 2048, i32 2048, i32 2048>
  %867 = ashr <4 x i32> %866, <i32 12, i32 12, i32 12, i32 12>
  %868 = add <4 x i32> %863, <i32 2048, i32 2048, i32 2048, i32 2048>
  %869 = ashr <4 x i32> %868, <i32 12, i32 12, i32 12, i32 12>
  %870 = add <4 x i32> %864, <i32 2048, i32 2048, i32 2048, i32 2048>
  %871 = ashr <4 x i32> %870, <i32 12, i32 12, i32 12, i32 12>
  %872 = add <4 x i32> %865, <i32 2048, i32 2048, i32 2048, i32 2048>
  %873 = ashr <4 x i32> %872, <i32 12, i32 12, i32 12, i32 12>
  %874 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %867, <4 x i32> %871) #8
  %875 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %869, <4 x i32> %873) #8
  %876 = shufflevector <8 x i16> %775, <8 x i16> %779, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %877 = shufflevector <8 x i16> %779, <8 x i16> %775, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %878 = shufflevector <8 x i16> %775, <8 x i16> %779, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %879 = shufflevector <8 x i16> %779, <8 x i16> %775, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %880 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %876, <8 x i16> %219) #8
  %881 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %877, <8 x i16> <i16 3513, i16 2106, i16 3513, i16 2106, i16 3513, i16 2106, i16 3513, i16 2106>) #8
  %882 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %878, <8 x i16> %219) #8
  %883 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %879, <8 x i16> <i16 3513, i16 2106, i16 3513, i16 2106, i16 3513, i16 2106, i16 3513, i16 2106>) #8
  %884 = add <4 x i32> %880, <i32 2048, i32 2048, i32 2048, i32 2048>
  %885 = ashr <4 x i32> %884, <i32 12, i32 12, i32 12, i32 12>
  %886 = add <4 x i32> %881, <i32 2048, i32 2048, i32 2048, i32 2048>
  %887 = ashr <4 x i32> %886, <i32 12, i32 12, i32 12, i32 12>
  %888 = add <4 x i32> %882, <i32 2048, i32 2048, i32 2048, i32 2048>
  %889 = ashr <4 x i32> %888, <i32 12, i32 12, i32 12, i32 12>
  %890 = add <4 x i32> %883, <i32 2048, i32 2048, i32 2048, i32 2048>
  %891 = ashr <4 x i32> %890, <i32 12, i32 12, i32 12, i32 12>
  %892 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %885, <4 x i32> %889) #8
  %893 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %887, <4 x i32> %891) #8
  %894 = shufflevector <8 x i16> %776, <8 x i16> %778, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %895 = shufflevector <8 x i16> %778, <8 x i16> %776, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %896 = shufflevector <8 x i16> %776, <8 x i16> %778, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %897 = shufflevector <8 x i16> %778, <8 x i16> %776, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %898 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %894, <8 x i16> %220) #8
  %899 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %895, <8 x i16> <i16 3857, i16 1380, i16 3857, i16 1380, i16 3857, i16 1380, i16 3857, i16 1380>) #8
  %900 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %896, <8 x i16> %220) #8
  %901 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %897, <8 x i16> <i16 3857, i16 1380, i16 3857, i16 1380, i16 3857, i16 1380, i16 3857, i16 1380>) #8
  %902 = add <4 x i32> %898, <i32 2048, i32 2048, i32 2048, i32 2048>
  %903 = ashr <4 x i32> %902, <i32 12, i32 12, i32 12, i32 12>
  %904 = add <4 x i32> %899, <i32 2048, i32 2048, i32 2048, i32 2048>
  %905 = ashr <4 x i32> %904, <i32 12, i32 12, i32 12, i32 12>
  %906 = add <4 x i32> %900, <i32 2048, i32 2048, i32 2048, i32 2048>
  %907 = ashr <4 x i32> %906, <i32 12, i32 12, i32 12, i32 12>
  %908 = add <4 x i32> %901, <i32 2048, i32 2048, i32 2048, i32 2048>
  %909 = ashr <4 x i32> %908, <i32 12, i32 12, i32 12, i32 12>
  %910 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %903, <4 x i32> %907) #8
  %911 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %905, <4 x i32> %909) #8
  %912 = shufflevector <8 x i16> %785, <8 x i16> %777, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %913 = shufflevector <8 x i16> %777, <8 x i16> %785, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %914 = shufflevector <8 x i16> %785, <8 x i16> %777, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %915 = shufflevector <8 x i16> %777, <8 x i16> %785, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %916 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %912, <8 x i16> %221) #8
  %917 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %913, <8 x i16> <i16 4052, i16 601, i16 4052, i16 601, i16 4052, i16 601, i16 4052, i16 601>) #8
  %918 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %914, <8 x i16> %221) #8
  %919 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %915, <8 x i16> <i16 4052, i16 601, i16 4052, i16 601, i16 4052, i16 601, i16 4052, i16 601>) #8
  %920 = add <4 x i32> %916, <i32 2048, i32 2048, i32 2048, i32 2048>
  %921 = ashr <4 x i32> %920, <i32 12, i32 12, i32 12, i32 12>
  %922 = add <4 x i32> %917, <i32 2048, i32 2048, i32 2048, i32 2048>
  %923 = ashr <4 x i32> %922, <i32 12, i32 12, i32 12, i32 12>
  %924 = add <4 x i32> %918, <i32 2048, i32 2048, i32 2048, i32 2048>
  %925 = ashr <4 x i32> %924, <i32 12, i32 12, i32 12, i32 12>
  %926 = add <4 x i32> %919, <i32 2048, i32 2048, i32 2048, i32 2048>
  %927 = ashr <4 x i32> %926, <i32 12, i32 12, i32 12, i32 12>
  %928 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %921, <4 x i32> %925) #8
  %929 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %923, <4 x i32> %927) #8
  %930 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %803, <8 x i16> %875) #8
  %931 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %803, <8 x i16> %875) #8
  %932 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %802, <8 x i16> %874) #8
  %933 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %802, <8 x i16> %874) #8
  %934 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %821, <8 x i16> %893) #8
  %935 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %821, <8 x i16> %893) #8
  %936 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %820, <8 x i16> %892) #8
  %937 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %820, <8 x i16> %892) #8
  %938 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %839, <8 x i16> %911) #8
  %939 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %839, <8 x i16> %911) #8
  %940 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %838, <8 x i16> %910) #8
  %941 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %838, <8 x i16> %910) #8
  %942 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %857, <8 x i16> %929) #8
  %943 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %857, <8 x i16> %929) #8
  %944 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %856, <8 x i16> %928) #8
  %945 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %856, <8 x i16> %928) #8
  %946 = shufflevector <8 x i16> %931, <8 x i16> %933, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %947 = shufflevector <8 x i16> %933, <8 x i16> %931, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %948 = shufflevector <8 x i16> %931, <8 x i16> %933, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %949 = shufflevector <8 x i16> %933, <8 x i16> %931, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %950 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %946, <8 x i16> %222) #8
  %951 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %947, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %952 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %948, <8 x i16> %222) #8
  %953 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %949, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %954 = add <4 x i32> %950, <i32 2048, i32 2048, i32 2048, i32 2048>
  %955 = ashr <4 x i32> %954, <i32 12, i32 12, i32 12, i32 12>
  %956 = add <4 x i32> %951, <i32 2048, i32 2048, i32 2048, i32 2048>
  %957 = ashr <4 x i32> %956, <i32 12, i32 12, i32 12, i32 12>
  %958 = add <4 x i32> %952, <i32 2048, i32 2048, i32 2048, i32 2048>
  %959 = ashr <4 x i32> %958, <i32 12, i32 12, i32 12, i32 12>
  %960 = add <4 x i32> %953, <i32 2048, i32 2048, i32 2048, i32 2048>
  %961 = ashr <4 x i32> %960, <i32 12, i32 12, i32 12, i32 12>
  %962 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %955, <4 x i32> %959) #8
  %963 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %957, <4 x i32> %961) #8
  %964 = shufflevector <8 x i16> %941, <8 x i16> %939, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %965 = shufflevector <8 x i16> %939, <8 x i16> %941, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %966 = shufflevector <8 x i16> %941, <8 x i16> %939, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %967 = shufflevector <8 x i16> %939, <8 x i16> %941, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %968 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %964, <8 x i16> %223) #8
  %969 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %965, <8 x i16> <i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799>) #8
  %970 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %966, <8 x i16> %223) #8
  %971 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %967, <8 x i16> <i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799>) #8
  %972 = add <4 x i32> %968, <i32 2048, i32 2048, i32 2048, i32 2048>
  %973 = ashr <4 x i32> %972, <i32 12, i32 12, i32 12, i32 12>
  %974 = add <4 x i32> %969, <i32 2048, i32 2048, i32 2048, i32 2048>
  %975 = ashr <4 x i32> %974, <i32 12, i32 12, i32 12, i32 12>
  %976 = add <4 x i32> %970, <i32 2048, i32 2048, i32 2048, i32 2048>
  %977 = ashr <4 x i32> %976, <i32 12, i32 12, i32 12, i32 12>
  %978 = add <4 x i32> %971, <i32 2048, i32 2048, i32 2048, i32 2048>
  %979 = ashr <4 x i32> %978, <i32 12, i32 12, i32 12, i32 12>
  %980 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %973, <4 x i32> %977) #8
  %981 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %975, <4 x i32> %979) #8
  %982 = shufflevector <8 x i16> %935, <8 x i16> %937, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %983 = shufflevector <8 x i16> %937, <8 x i16> %935, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %984 = shufflevector <8 x i16> %935, <8 x i16> %937, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %985 = shufflevector <8 x i16> %937, <8 x i16> %935, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %986 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %982, <8 x i16> %224) #8
  %987 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %983, <8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>) #8
  %988 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %984, <8 x i16> %224) #8
  %989 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %985, <8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>) #8
  %990 = add <4 x i32> %986, <i32 2048, i32 2048, i32 2048, i32 2048>
  %991 = ashr <4 x i32> %990, <i32 12, i32 12, i32 12, i32 12>
  %992 = add <4 x i32> %987, <i32 2048, i32 2048, i32 2048, i32 2048>
  %993 = ashr <4 x i32> %992, <i32 12, i32 12, i32 12, i32 12>
  %994 = add <4 x i32> %988, <i32 2048, i32 2048, i32 2048, i32 2048>
  %995 = ashr <4 x i32> %994, <i32 12, i32 12, i32 12, i32 12>
  %996 = add <4 x i32> %989, <i32 2048, i32 2048, i32 2048, i32 2048>
  %997 = ashr <4 x i32> %996, <i32 12, i32 12, i32 12, i32 12>
  %998 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %991, <4 x i32> %995) #8
  %999 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %993, <4 x i32> %997) #8
  %1000 = shufflevector <8 x i16> %945, <8 x i16> %943, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1001 = shufflevector <8 x i16> %943, <8 x i16> %945, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1002 = shufflevector <8 x i16> %945, <8 x i16> %943, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1003 = shufflevector <8 x i16> %943, <8 x i16> %945, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1004 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1000, <8 x i16> %225) #8
  %1005 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1001, <8 x i16> <i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406>) #8
  %1006 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1002, <8 x i16> %225) #8
  %1007 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1003, <8 x i16> <i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406>) #8
  %1008 = add <4 x i32> %1004, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1009 = ashr <4 x i32> %1008, <i32 12, i32 12, i32 12, i32 12>
  %1010 = add <4 x i32> %1005, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1011 = ashr <4 x i32> %1010, <i32 12, i32 12, i32 12, i32 12>
  %1012 = add <4 x i32> %1006, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1013 = ashr <4 x i32> %1012, <i32 12, i32 12, i32 12, i32 12>
  %1014 = add <4 x i32> %1007, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1015 = ashr <4 x i32> %1014, <i32 12, i32 12, i32 12, i32 12>
  %1016 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1009, <4 x i32> %1013) #8
  %1017 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1011, <4 x i32> %1015) #8
  %1018 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %930, <8 x i16> %938) #8
  %1019 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %930, <8 x i16> %938) #8
  %1020 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %963, <8 x i16> %980) #8
  %1021 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %963, <8 x i16> %980) #8
  %1022 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %932, <8 x i16> %940) #8
  %1023 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %932, <8 x i16> %940) #8
  %1024 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %962, <8 x i16> %981) #8
  %1025 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %962, <8 x i16> %981) #8
  %1026 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %934, <8 x i16> %942) #8
  %1027 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %934, <8 x i16> %942) #8
  %1028 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %999, <8 x i16> %1016) #8
  %1029 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %999, <8 x i16> %1016) #8
  %1030 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %936, <8 x i16> %944) #8
  %1031 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %936, <8 x i16> %944) #8
  %1032 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %998, <8 x i16> %1017) #8
  %1033 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %998, <8 x i16> %1017) #8
  %1034 = shufflevector <8 x i16> %1019, <8 x i16> %1023, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1035 = shufflevector <8 x i16> %1023, <8 x i16> %1019, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1036 = shufflevector <8 x i16> %1019, <8 x i16> %1023, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1037 = shufflevector <8 x i16> %1023, <8 x i16> %1019, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1038 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1034, <8 x i16> %226) #8
  %1039 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1035, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %1040 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1036, <8 x i16> %226) #8
  %1041 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1037, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %1042 = add <4 x i32> %1038, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1043 = ashr <4 x i32> %1042, <i32 12, i32 12, i32 12, i32 12>
  %1044 = add <4 x i32> %1039, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1045 = ashr <4 x i32> %1044, <i32 12, i32 12, i32 12, i32 12>
  %1046 = add <4 x i32> %1040, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1047 = ashr <4 x i32> %1046, <i32 12, i32 12, i32 12, i32 12>
  %1048 = add <4 x i32> %1041, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1049 = ashr <4 x i32> %1048, <i32 12, i32 12, i32 12, i32 12>
  %1050 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1043, <4 x i32> %1047) #8
  %1051 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1045, <4 x i32> %1049) #8
  %1052 = shufflevector <8 x i16> %1021, <8 x i16> %1025, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1053 = shufflevector <8 x i16> %1025, <8 x i16> %1021, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1054 = shufflevector <8 x i16> %1021, <8 x i16> %1025, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1055 = shufflevector <8 x i16> %1025, <8 x i16> %1021, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1056 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1052, <8 x i16> %226) #8
  %1057 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1053, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %1058 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1054, <8 x i16> %226) #8
  %1059 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1055, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %1060 = add <4 x i32> %1056, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1061 = ashr <4 x i32> %1060, <i32 12, i32 12, i32 12, i32 12>
  %1062 = add <4 x i32> %1057, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1063 = ashr <4 x i32> %1062, <i32 12, i32 12, i32 12, i32 12>
  %1064 = add <4 x i32> %1058, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1065 = ashr <4 x i32> %1064, <i32 12, i32 12, i32 12, i32 12>
  %1066 = add <4 x i32> %1059, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1067 = ashr <4 x i32> %1066, <i32 12, i32 12, i32 12, i32 12>
  %1068 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1061, <4 x i32> %1065) #8
  %1069 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1063, <4 x i32> %1067) #8
  %1070 = shufflevector <8 x i16> %1031, <8 x i16> %1027, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1071 = shufflevector <8 x i16> %1027, <8 x i16> %1031, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1072 = shufflevector <8 x i16> %1031, <8 x i16> %1027, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1073 = shufflevector <8 x i16> %1027, <8 x i16> %1031, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1074 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1070, <8 x i16> %227) #8
  %1075 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1071, <8 x i16> <i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567>) #8
  %1076 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1072, <8 x i16> %227) #8
  %1077 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1073, <8 x i16> <i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567>) #8
  %1078 = add <4 x i32> %1074, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1079 = ashr <4 x i32> %1078, <i32 12, i32 12, i32 12, i32 12>
  %1080 = add <4 x i32> %1075, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1081 = ashr <4 x i32> %1080, <i32 12, i32 12, i32 12, i32 12>
  %1082 = add <4 x i32> %1076, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1083 = ashr <4 x i32> %1082, <i32 12, i32 12, i32 12, i32 12>
  %1084 = add <4 x i32> %1077, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1085 = ashr <4 x i32> %1084, <i32 12, i32 12, i32 12, i32 12>
  %1086 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1079, <4 x i32> %1083) #8
  %1087 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1081, <4 x i32> %1085) #8
  %1088 = shufflevector <8 x i16> %1033, <8 x i16> %1029, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1089 = shufflevector <8 x i16> %1029, <8 x i16> %1033, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1090 = shufflevector <8 x i16> %1033, <8 x i16> %1029, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1091 = shufflevector <8 x i16> %1029, <8 x i16> %1033, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1092 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1088, <8 x i16> %227) #8
  %1093 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1089, <8 x i16> <i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567>) #8
  %1094 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1090, <8 x i16> %227) #8
  %1095 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1091, <8 x i16> <i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567>) #8
  %1096 = add <4 x i32> %1092, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1097 = ashr <4 x i32> %1096, <i32 12, i32 12, i32 12, i32 12>
  %1098 = add <4 x i32> %1093, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1099 = ashr <4 x i32> %1098, <i32 12, i32 12, i32 12, i32 12>
  %1100 = add <4 x i32> %1094, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1101 = ashr <4 x i32> %1100, <i32 12, i32 12, i32 12, i32 12>
  %1102 = add <4 x i32> %1095, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1103 = ashr <4 x i32> %1102, <i32 12, i32 12, i32 12, i32 12>
  %1104 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1097, <4 x i32> %1101) #8
  %1105 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1099, <4 x i32> %1103) #8
  %1106 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1018, <8 x i16> %1026) #8
  %1107 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1018, <8 x i16> %1026) #8
  %1108 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1051, <8 x i16> %1086) #8
  %1109 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1051, <8 x i16> %1086) #8
  %1110 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1020, <8 x i16> %1028) #8
  %1111 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1020, <8 x i16> %1028) #8
  %1112 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1069, <8 x i16> %1104) #8
  %1113 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1069, <8 x i16> %1104) #8
  %1114 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1022, <8 x i16> %1030) #8
  %1115 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1022, <8 x i16> %1030) #8
  %1116 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1050, <8 x i16> %1087) #8
  %1117 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1050, <8 x i16> %1087) #8
  %1118 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1024, <8 x i16> %1032) #8
  %1119 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1024, <8 x i16> %1032) #8
  %1120 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1068, <8 x i16> %1105) #8
  %1121 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1068, <8 x i16> %1105) #8
  %1122 = shufflevector <8 x i16> %1107, <8 x i16> %1115, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1123 = shufflevector <8 x i16> %1115, <8 x i16> %1107, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1124 = shufflevector <8 x i16> %1107, <8 x i16> %1115, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1125 = shufflevector <8 x i16> %1115, <8 x i16> %1107, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1126 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1122, <8 x i16> %228) #8
  %1127 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1123, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1128 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1124, <8 x i16> %228) #8
  %1129 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1125, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1130 = add <4 x i32> %1126, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1131 = ashr <4 x i32> %1130, <i32 12, i32 12, i32 12, i32 12>
  %1132 = add <4 x i32> %1127, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1133 = ashr <4 x i32> %1132, <i32 12, i32 12, i32 12, i32 12>
  %1134 = add <4 x i32> %1128, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1135 = ashr <4 x i32> %1134, <i32 12, i32 12, i32 12, i32 12>
  %1136 = add <4 x i32> %1129, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1137 = ashr <4 x i32> %1136, <i32 12, i32 12, i32 12, i32 12>
  %1138 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1131, <4 x i32> %1135) #8
  %1139 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1133, <4 x i32> %1137) #8
  %1140 = shufflevector <8 x i16> %1109, <8 x i16> %1117, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1141 = shufflevector <8 x i16> %1117, <8 x i16> %1109, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1142 = shufflevector <8 x i16> %1109, <8 x i16> %1117, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1143 = shufflevector <8 x i16> %1117, <8 x i16> %1109, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1144 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1140, <8 x i16> %228) #8
  %1145 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1141, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1146 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1142, <8 x i16> %228) #8
  %1147 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1143, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1148 = add <4 x i32> %1144, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1149 = ashr <4 x i32> %1148, <i32 12, i32 12, i32 12, i32 12>
  %1150 = add <4 x i32> %1145, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1151 = ashr <4 x i32> %1150, <i32 12, i32 12, i32 12, i32 12>
  %1152 = add <4 x i32> %1146, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1153 = ashr <4 x i32> %1152, <i32 12, i32 12, i32 12, i32 12>
  %1154 = add <4 x i32> %1147, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1155 = ashr <4 x i32> %1154, <i32 12, i32 12, i32 12, i32 12>
  %1156 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1149, <4 x i32> %1153) #8
  %1157 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1151, <4 x i32> %1155) #8
  %1158 = shufflevector <8 x i16> %1111, <8 x i16> %1119, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1159 = shufflevector <8 x i16> %1119, <8 x i16> %1111, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1160 = shufflevector <8 x i16> %1111, <8 x i16> %1119, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1161 = shufflevector <8 x i16> %1119, <8 x i16> %1111, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1162 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1158, <8 x i16> %228) #8
  %1163 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1159, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1164 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1160, <8 x i16> %228) #8
  %1165 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1161, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1166 = add <4 x i32> %1162, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1167 = ashr <4 x i32> %1166, <i32 12, i32 12, i32 12, i32 12>
  %1168 = add <4 x i32> %1163, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1169 = ashr <4 x i32> %1168, <i32 12, i32 12, i32 12, i32 12>
  %1170 = add <4 x i32> %1164, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1171 = ashr <4 x i32> %1170, <i32 12, i32 12, i32 12, i32 12>
  %1172 = add <4 x i32> %1165, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1173 = ashr <4 x i32> %1172, <i32 12, i32 12, i32 12, i32 12>
  %1174 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1167, <4 x i32> %1171) #8
  %1175 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1169, <4 x i32> %1173) #8
  %1176 = shufflevector <8 x i16> %1113, <8 x i16> %1121, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1177 = shufflevector <8 x i16> %1121, <8 x i16> %1113, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1178 = shufflevector <8 x i16> %1113, <8 x i16> %1121, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1179 = shufflevector <8 x i16> %1121, <8 x i16> %1113, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1180 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1176, <8 x i16> %228) #8
  %1181 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1177, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1182 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1178, <8 x i16> %228) #8
  %1183 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1179, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1184 = add <4 x i32> %1180, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1185 = ashr <4 x i32> %1184, <i32 12, i32 12, i32 12, i32 12>
  %1186 = add <4 x i32> %1181, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1187 = ashr <4 x i32> %1186, <i32 12, i32 12, i32 12, i32 12>
  %1188 = add <4 x i32> %1182, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1189 = ashr <4 x i32> %1188, <i32 12, i32 12, i32 12, i32 12>
  %1190 = add <4 x i32> %1183, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1191 = ashr <4 x i32> %1190, <i32 12, i32 12, i32 12, i32 12>
  %1192 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1185, <4 x i32> %1189) #8
  %1193 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1187, <4 x i32> %1191) #8
  %1194 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %1110) #8
  %1195 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %1108) #8
  %1196 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %1193) #8
  %1197 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %1139) #8
  %1198 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %1174) #8
  %1199 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %1156) #8
  %1200 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %1120) #8
  %1201 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %1114) #8
  %1202 = shufflevector <8 x i16> %1106, <8 x i16> %1194, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1203 = shufflevector <8 x i16> %1112, <8 x i16> %1195, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1204 = shufflevector <8 x i16> %1157, <8 x i16> %1196, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1205 = shufflevector <8 x i16> %1175, <8 x i16> %1197, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1206 = shufflevector <8 x i16> %1106, <8 x i16> %1194, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1207 = shufflevector <8 x i16> %1112, <8 x i16> %1195, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1208 = shufflevector <8 x i16> %1157, <8 x i16> %1196, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1209 = shufflevector <8 x i16> %1175, <8 x i16> %1197, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1210 = bitcast <8 x i16> %1202 to <4 x i32>
  %1211 = bitcast <8 x i16> %1203 to <4 x i32>
  %1212 = shufflevector <4 x i32> %1210, <4 x i32> %1211, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1213 = bitcast <4 x i32> %1212 to <2 x i64>
  %1214 = bitcast <8 x i16> %1204 to <4 x i32>
  %1215 = bitcast <8 x i16> %1205 to <4 x i32>
  %1216 = shufflevector <4 x i32> %1214, <4 x i32> %1215, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1217 = bitcast <4 x i32> %1216 to <2 x i64>
  %1218 = bitcast <8 x i16> %1206 to <4 x i32>
  %1219 = bitcast <8 x i16> %1207 to <4 x i32>
  %1220 = shufflevector <4 x i32> %1218, <4 x i32> %1219, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1221 = bitcast <4 x i32> %1220 to <2 x i64>
  %1222 = bitcast <8 x i16> %1208 to <4 x i32>
  %1223 = bitcast <8 x i16> %1209 to <4 x i32>
  %1224 = shufflevector <4 x i32> %1222, <4 x i32> %1223, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1225 = bitcast <4 x i32> %1224 to <2 x i64>
  %1226 = shufflevector <4 x i32> %1210, <4 x i32> %1211, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1227 = bitcast <4 x i32> %1226 to <2 x i64>
  %1228 = shufflevector <4 x i32> %1214, <4 x i32> %1215, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1229 = bitcast <4 x i32> %1228 to <2 x i64>
  %1230 = shufflevector <4 x i32> %1218, <4 x i32> %1219, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1231 = bitcast <4 x i32> %1230 to <2 x i64>
  %1232 = shufflevector <4 x i32> %1222, <4 x i32> %1223, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1233 = bitcast <4 x i32> %1232 to <2 x i64>
  %1234 = shufflevector <2 x i64> %1213, <2 x i64> %1217, <2 x i32> <i32 0, i32 2>
  %1235 = shufflevector <2 x i64> %1213, <2 x i64> %1217, <2 x i32> <i32 1, i32 3>
  %1236 = shufflevector <2 x i64> %1227, <2 x i64> %1229, <2 x i32> <i32 0, i32 2>
  %1237 = shufflevector <2 x i64> %1227, <2 x i64> %1229, <2 x i32> <i32 1, i32 3>
  %1238 = shufflevector <2 x i64> %1221, <2 x i64> %1225, <2 x i32> <i32 0, i32 2>
  %1239 = shufflevector <2 x i64> %1221, <2 x i64> %1225, <2 x i32> <i32 1, i32 3>
  %1240 = shufflevector <2 x i64> %1231, <2 x i64> %1233, <2 x i32> <i32 0, i32 2>
  %1241 = shufflevector <2 x i64> %1231, <2 x i64> %1233, <2 x i32> <i32 1, i32 3>
  %1242 = bitcast i16* %642 to <2 x i64>*
  store <2 x i64> %1234, <2 x i64>* %1242, align 1
  %1243 = bitcast i16* %645 to <2 x i64>*
  store <2 x i64> %1235, <2 x i64>* %1243, align 1
  %1244 = bitcast i16* %648 to <2 x i64>*
  store <2 x i64> %1236, <2 x i64>* %1244, align 1
  %1245 = bitcast i16* %651 to <2 x i64>*
  store <2 x i64> %1237, <2 x i64>* %1245, align 1
  %1246 = bitcast i16* %654 to <2 x i64>*
  store <2 x i64> %1238, <2 x i64>* %1246, align 1
  %1247 = bitcast i16* %657 to <2 x i64>*
  store <2 x i64> %1239, <2 x i64>* %1247, align 1
  %1248 = bitcast i16* %660 to <2 x i64>*
  store <2 x i64> %1240, <2 x i64>* %1248, align 1
  %1249 = bitcast i16* %663 to <2 x i64>*
  store <2 x i64> %1241, <2 x i64>* %1249, align 1
  %1250 = shufflevector <8 x i16> %1138, <8 x i16> %1198, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1251 = shufflevector <8 x i16> %1192, <8 x i16> %1199, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1252 = shufflevector <8 x i16> %1116, <8 x i16> %1200, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1253 = shufflevector <8 x i16> %1118, <8 x i16> %1201, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1254 = shufflevector <8 x i16> %1138, <8 x i16> %1198, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1255 = shufflevector <8 x i16> %1192, <8 x i16> %1199, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1256 = shufflevector <8 x i16> %1116, <8 x i16> %1200, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1257 = shufflevector <8 x i16> %1118, <8 x i16> %1201, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1258 = bitcast <8 x i16> %1250 to <4 x i32>
  %1259 = bitcast <8 x i16> %1251 to <4 x i32>
  %1260 = shufflevector <4 x i32> %1258, <4 x i32> %1259, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1261 = bitcast <4 x i32> %1260 to <2 x i64>
  %1262 = bitcast <8 x i16> %1252 to <4 x i32>
  %1263 = bitcast <8 x i16> %1253 to <4 x i32>
  %1264 = shufflevector <4 x i32> %1262, <4 x i32> %1263, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1265 = bitcast <4 x i32> %1264 to <2 x i64>
  %1266 = bitcast <8 x i16> %1254 to <4 x i32>
  %1267 = bitcast <8 x i16> %1255 to <4 x i32>
  %1268 = shufflevector <4 x i32> %1266, <4 x i32> %1267, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1269 = bitcast <4 x i32> %1268 to <2 x i64>
  %1270 = bitcast <8 x i16> %1256 to <4 x i32>
  %1271 = bitcast <8 x i16> %1257 to <4 x i32>
  %1272 = shufflevector <4 x i32> %1270, <4 x i32> %1271, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1273 = bitcast <4 x i32> %1272 to <2 x i64>
  %1274 = shufflevector <4 x i32> %1258, <4 x i32> %1259, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1275 = bitcast <4 x i32> %1274 to <2 x i64>
  %1276 = shufflevector <4 x i32> %1262, <4 x i32> %1263, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1277 = bitcast <4 x i32> %1276 to <2 x i64>
  %1278 = shufflevector <4 x i32> %1266, <4 x i32> %1267, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1279 = bitcast <4 x i32> %1278 to <2 x i64>
  %1280 = shufflevector <4 x i32> %1270, <4 x i32> %1271, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1281 = bitcast <4 x i32> %1280 to <2 x i64>
  %1282 = shufflevector <2 x i64> %1261, <2 x i64> %1265, <2 x i32> <i32 0, i32 2>
  %1283 = shufflevector <2 x i64> %1261, <2 x i64> %1265, <2 x i32> <i32 1, i32 3>
  %1284 = shufflevector <2 x i64> %1275, <2 x i64> %1277, <2 x i32> <i32 0, i32 2>
  %1285 = shufflevector <2 x i64> %1275, <2 x i64> %1277, <2 x i32> <i32 1, i32 3>
  %1286 = shufflevector <2 x i64> %1269, <2 x i64> %1273, <2 x i32> <i32 0, i32 2>
  %1287 = shufflevector <2 x i64> %1269, <2 x i64> %1273, <2 x i32> <i32 1, i32 3>
  %1288 = shufflevector <2 x i64> %1279, <2 x i64> %1281, <2 x i32> <i32 0, i32 2>
  %1289 = shufflevector <2 x i64> %1279, <2 x i64> %1281, <2 x i32> <i32 1, i32 3>
  %1290 = bitcast i16* %706 to <2 x i64>*
  store <2 x i64> %1282, <2 x i64>* %1290, align 1
  %1291 = bitcast i16* %709 to <2 x i64>*
  store <2 x i64> %1283, <2 x i64>* %1291, align 1
  %1292 = bitcast i16* %712 to <2 x i64>*
  store <2 x i64> %1284, <2 x i64>* %1292, align 1
  %1293 = bitcast i16* %715 to <2 x i64>*
  store <2 x i64> %1285, <2 x i64>* %1293, align 1
  %1294 = bitcast i16* %718 to <2 x i64>*
  store <2 x i64> %1286, <2 x i64>* %1294, align 1
  %1295 = bitcast i16* %721 to <2 x i64>*
  store <2 x i64> %1287, <2 x i64>* %1295, align 1
  %1296 = bitcast i16* %724 to <2 x i64>*
  store <2 x i64> %1288, <2 x i64>* %1296, align 1
  %1297 = bitcast i16* %727 to <2 x i64>*
  store <2 x i64> %1289, <2 x i64>* %1297, align 1
  %1298 = add nuw nsw i64 %639, 8
  %1299 = icmp slt i64 %1298, %229
  br i1 %1299, label %638, label %1300

1300:                                             ; preds = %638, %230
  %1301 = phi i64 [ %637, %230 ], [ %229, %638 ]
  %1302 = zext i8 %14 to i16
  %1303 = insertelement <8 x i16> undef, i16 %1302, i32 0
  %1304 = shufflevector <8 x i16> %1303, <8 x i16> undef, <8 x i32> zeroinitializer
  %1305 = shufflevector <8 x i16> %1303, <8 x i16> undef, <2 x i32> zeroinitializer
  %1306 = zext <2 x i16> %1305 to <2 x i64>
  %1307 = bitcast <2 x i64> %1306 to <8 x i16>
  br label %1308

1308:                                             ; preds = %1308, %1300
  %1309 = phi i64 [ %1329, %1308 ], [ 0, %1300 ]
  %1310 = shl i64 %1309, 4
  %1311 = and i64 %1310, 4294967280
  %1312 = getelementptr inbounds i16, i16* %8, i64 %1311
  %1313 = bitcast i16* %1312 to <8 x i16>*
  %1314 = load <8 x i16>, <8 x i16>* %1313, align 1
  %1315 = icmp sgt <8 x i16> %1314, <i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765>
  %1316 = add <8 x i16> %1314, %1304
  %1317 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %1316, <8 x i16> %1307) #8
  %1318 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %1316, <8 x i16> %1307) #8
  %1319 = select <8 x i1> %1315, <8 x i16> %1318, <8 x i16> %1317
  store <8 x i16> %1319, <8 x i16>* %1313, align 1
  %1320 = or i64 %1311, 8
  %1321 = getelementptr inbounds i16, i16* %8, i64 %1320
  %1322 = bitcast i16* %1321 to <8 x i16>*
  %1323 = load <8 x i16>, <8 x i16>* %1322, align 1
  %1324 = icmp sgt <8 x i16> %1323, <i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765, i16 32765>
  %1325 = add <8 x i16> %1323, %1304
  %1326 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %1325, <8 x i16> %1307) #8
  %1327 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %1325, <8 x i16> %1307) #8
  %1328 = select <8 x i1> %1324, <8 x i16> %1327, <8 x i16> %1326
  store <8 x i16> %1328, <8 x i16>* %1322, align 1
  %1329 = add nuw nsw i64 %1309, 1
  %1330 = icmp slt i64 %1329, %1301
  br i1 %1330, label %1308, label %1331

1331:                                             ; preds = %1308, %16
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_132Adst16TransformLoopColumn_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_(i8 zeroext, i8 zeroext, i32, i8*, i32, i32, i8* nocapture readonly) #4 {
  %8 = bitcast i8* %3 to i16*
  %9 = zext i8 %1 to i64
  %10 = getelementptr inbounds [19 x i8], [19 x i8]* @_ZN7libgav115kTransformWidthE, i64 0, i64 %9
  %11 = load i8, i8* %10, align 1
  %12 = zext i8 %11 to i32
  %13 = zext i8 %0 to i32
  %14 = shl i32 1, %13
  %15 = and i32 %14, 33104
  %16 = icmp eq i32 %15, 0
  br i1 %16, label %131, label %17

17:                                               ; preds = %7
  %18 = icmp ugt i8 %11, 15
  br i1 %18, label %19, label %35

19:                                               ; preds = %17
  %20 = shl nuw nsw i32 %12, 4
  %21 = zext i32 %20 to i64
  br label %22

22:                                               ; preds = %22, %19
  %23 = phi i64 [ 0, %19 ], [ %33, %22 ]
  %24 = getelementptr inbounds i16, i16* %8, i64 %23
  %25 = bitcast i16* %24 to <16 x i8>*
  %26 = load <16 x i8>, <16 x i8>* %25, align 1
  %27 = or i64 %23, 8
  %28 = getelementptr inbounds i16, i16* %8, i64 %27
  %29 = bitcast i16* %28 to <16 x i8>*
  %30 = load <16 x i8>, <16 x i8>* %29, align 1
  %31 = shufflevector <16 x i8> %26, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  %32 = shufflevector <16 x i8> %30, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %32, <16 x i8>* %25, align 1
  store <16 x i8> %31, <16 x i8>* %29, align 1
  %33 = add nuw nsw i64 %23, 16
  %34 = icmp ult i64 %33, %21
  br i1 %34, label %22, label %131

35:                                               ; preds = %17
  %36 = icmp eq i8 %11, 8
  %37 = bitcast i8* %3 to <16 x i8>*
  %38 = load <16 x i8>, <16 x i8>* %37, align 1
  br i1 %36, label %69, label %39

39:                                               ; preds = %35
  %40 = shufflevector <16 x i8> %38, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %40, <16 x i8>* %37, align 1
  %41 = getelementptr inbounds i8, i8* %3, i64 16
  %42 = bitcast i8* %41 to <16 x i8>*
  %43 = load <16 x i8>, <16 x i8>* %42, align 1
  %44 = shufflevector <16 x i8> %43, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %44, <16 x i8>* %42, align 1
  %45 = getelementptr inbounds i8, i8* %3, i64 32
  %46 = bitcast i8* %45 to <16 x i8>*
  %47 = load <16 x i8>, <16 x i8>* %46, align 1
  %48 = shufflevector <16 x i8> %47, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %48, <16 x i8>* %46, align 1
  %49 = getelementptr inbounds i8, i8* %3, i64 48
  %50 = bitcast i8* %49 to <16 x i8>*
  %51 = load <16 x i8>, <16 x i8>* %50, align 1
  %52 = shufflevector <16 x i8> %51, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %52, <16 x i8>* %50, align 1
  %53 = getelementptr inbounds i8, i8* %3, i64 64
  %54 = bitcast i8* %53 to <16 x i8>*
  %55 = load <16 x i8>, <16 x i8>* %54, align 1
  %56 = shufflevector <16 x i8> %55, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %56, <16 x i8>* %54, align 1
  %57 = getelementptr inbounds i8, i8* %3, i64 80
  %58 = bitcast i8* %57 to <16 x i8>*
  %59 = load <16 x i8>, <16 x i8>* %58, align 1
  %60 = shufflevector <16 x i8> %59, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %60, <16 x i8>* %58, align 1
  %61 = getelementptr inbounds i8, i8* %3, i64 96
  %62 = bitcast i8* %61 to <16 x i8>*
  %63 = load <16 x i8>, <16 x i8>* %62, align 1
  %64 = shufflevector <16 x i8> %63, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %64, <16 x i8>* %62, align 1
  %65 = getelementptr inbounds i8, i8* %3, i64 112
  %66 = bitcast i8* %65 to <16 x i8>*
  %67 = load <16 x i8>, <16 x i8>* %66, align 1
  %68 = shufflevector <16 x i8> %67, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %68, <16 x i8>* %66, align 1
  br label %131

69:                                               ; preds = %35
  %70 = shufflevector <16 x i8> %38, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %70, <16 x i8>* %37, align 1
  %71 = getelementptr inbounds i8, i8* %3, i64 16
  %72 = bitcast i8* %71 to <16 x i8>*
  %73 = load <16 x i8>, <16 x i8>* %72, align 1
  %74 = shufflevector <16 x i8> %73, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %74, <16 x i8>* %72, align 1
  %75 = getelementptr inbounds i8, i8* %3, i64 32
  %76 = bitcast i8* %75 to <16 x i8>*
  %77 = load <16 x i8>, <16 x i8>* %76, align 1
  %78 = shufflevector <16 x i8> %77, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %78, <16 x i8>* %76, align 1
  %79 = getelementptr inbounds i8, i8* %3, i64 48
  %80 = bitcast i8* %79 to <16 x i8>*
  %81 = load <16 x i8>, <16 x i8>* %80, align 1
  %82 = shufflevector <16 x i8> %81, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %82, <16 x i8>* %80, align 1
  %83 = getelementptr inbounds i8, i8* %3, i64 64
  %84 = bitcast i8* %83 to <16 x i8>*
  %85 = load <16 x i8>, <16 x i8>* %84, align 1
  %86 = shufflevector <16 x i8> %85, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %86, <16 x i8>* %84, align 1
  %87 = getelementptr inbounds i8, i8* %3, i64 80
  %88 = bitcast i8* %87 to <16 x i8>*
  %89 = load <16 x i8>, <16 x i8>* %88, align 1
  %90 = shufflevector <16 x i8> %89, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %90, <16 x i8>* %88, align 1
  %91 = getelementptr inbounds i8, i8* %3, i64 96
  %92 = bitcast i8* %91 to <16 x i8>*
  %93 = load <16 x i8>, <16 x i8>* %92, align 1
  %94 = shufflevector <16 x i8> %93, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %94, <16 x i8>* %92, align 1
  %95 = getelementptr inbounds i8, i8* %3, i64 112
  %96 = bitcast i8* %95 to <16 x i8>*
  %97 = load <16 x i8>, <16 x i8>* %96, align 1
  %98 = shufflevector <16 x i8> %97, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %98, <16 x i8>* %96, align 1
  %99 = getelementptr inbounds i8, i8* %3, i64 128
  %100 = bitcast i8* %99 to <16 x i8>*
  %101 = load <16 x i8>, <16 x i8>* %100, align 1
  %102 = shufflevector <16 x i8> %101, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %102, <16 x i8>* %100, align 1
  %103 = getelementptr inbounds i8, i8* %3, i64 144
  %104 = bitcast i8* %103 to <16 x i8>*
  %105 = load <16 x i8>, <16 x i8>* %104, align 1
  %106 = shufflevector <16 x i8> %105, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %106, <16 x i8>* %104, align 1
  %107 = getelementptr inbounds i8, i8* %3, i64 160
  %108 = bitcast i8* %107 to <16 x i8>*
  %109 = load <16 x i8>, <16 x i8>* %108, align 1
  %110 = shufflevector <16 x i8> %109, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %110, <16 x i8>* %108, align 1
  %111 = getelementptr inbounds i8, i8* %3, i64 176
  %112 = bitcast i8* %111 to <16 x i8>*
  %113 = load <16 x i8>, <16 x i8>* %112, align 1
  %114 = shufflevector <16 x i8> %113, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %114, <16 x i8>* %112, align 1
  %115 = getelementptr inbounds i8, i8* %3, i64 192
  %116 = bitcast i8* %115 to <16 x i8>*
  %117 = load <16 x i8>, <16 x i8>* %116, align 1
  %118 = shufflevector <16 x i8> %117, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %118, <16 x i8>* %116, align 1
  %119 = getelementptr inbounds i8, i8* %3, i64 208
  %120 = bitcast i8* %119 to <16 x i8>*
  %121 = load <16 x i8>, <16 x i8>* %120, align 1
  %122 = shufflevector <16 x i8> %121, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %122, <16 x i8>* %120, align 1
  %123 = getelementptr inbounds i8, i8* %3, i64 224
  %124 = bitcast i8* %123 to <16 x i8>*
  %125 = load <16 x i8>, <16 x i8>* %124, align 1
  %126 = shufflevector <16 x i8> %125, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %126, <16 x i8>* %124, align 1
  %127 = getelementptr inbounds i8, i8* %3, i64 240
  %128 = bitcast i8* %127 to <16 x i8>*
  %129 = load <16 x i8>, <16 x i8>* %128, align 1
  %130 = shufflevector <16 x i8> %129, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %130, <16 x i8>* %128, align 1
  br label %131

131:                                              ; preds = %22, %7, %69, %39
  %132 = icmp sgt i32 %2, 1
  br i1 %132, label %297, label %133

133:                                              ; preds = %131
  %134 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %135 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %136 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %137 = zext i8 %11 to i64
  %138 = shl nuw nsw i64 %137, 1
  %139 = mul nuw nsw i64 %137, 3
  %140 = shl nuw nsw i64 %137, 2
  %141 = mul nuw nsw i64 %137, 5
  %142 = mul nuw nsw i64 %137, 6
  %143 = mul nuw nsw i64 %137, 7
  %144 = shl nuw nsw i64 %137, 3
  %145 = mul nuw nsw i64 %137, 9
  %146 = mul nuw nsw i64 %137, 10
  %147 = mul nuw nsw i64 %137, 11
  %148 = mul nuw nsw i64 %137, 12
  %149 = mul nuw nsw i64 %137, 13
  %150 = mul nuw nsw i64 %137, 14
  %151 = mul nuw nsw i64 %137, 15
  br label %152

152:                                              ; preds = %152, %133
  %153 = phi i16* [ %8, %133 ], [ %295, %152 ]
  %154 = phi i32 [ 0, %133 ], [ %294, %152 ]
  %155 = bitcast i16* %153 to <8 x i16>*
  %156 = load <8 x i16>, <8 x i16>* %155, align 1
  %157 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %156, <8 x i16> <i16 -32728, i16 -32728, i16 -32728, i16 -32728, i16 -32728, i16 -32728, i16 -32728, i16 -32728>) #8
  %158 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %156, <8 x i16> <i16 1608, i16 1608, i16 1608, i16 1608, i16 1608, i16 1608, i16 1608, i16 1608>) #8
  %159 = shufflevector <8 x i16> %158, <8 x i16> %157, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %160 = shufflevector <8 x i16> %157, <8 x i16> %158, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %161 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %159, <8 x i16> %134) #8
  %162 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %160, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %163 = add <4 x i32> %161, <i32 2048, i32 2048, i32 2048, i32 2048>
  %164 = ashr <4 x i32> %163, <i32 12, i32 12, i32 12, i32 12>
  %165 = add <4 x i32> %162, <i32 2048, i32 2048, i32 2048, i32 2048>
  %166 = ashr <4 x i32> %165, <i32 12, i32 12, i32 12, i32 12>
  %167 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %164, <4 x i32> %164) #8
  %168 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %166, <4 x i32> %166) #8
  %169 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %159, <8 x i16> %135) #8
  %170 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %160, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %171 = add <4 x i32> %169, <i32 2048, i32 2048, i32 2048, i32 2048>
  %172 = ashr <4 x i32> %171, <i32 12, i32 12, i32 12, i32 12>
  %173 = add <4 x i32> %170, <i32 2048, i32 2048, i32 2048, i32 2048>
  %174 = ashr <4 x i32> %173, <i32 12, i32 12, i32 12, i32 12>
  %175 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %172, <4 x i32> %172) #8
  %176 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %174, <4 x i32> %174) #8
  %177 = shufflevector <8 x i16> %168, <8 x i16> %167, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %178 = shufflevector <8 x i16> %167, <8 x i16> %168, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %179 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %177, <8 x i16> %135) #8
  %180 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %178, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %181 = add <4 x i32> %179, <i32 2048, i32 2048, i32 2048, i32 2048>
  %182 = ashr <4 x i32> %181, <i32 12, i32 12, i32 12, i32 12>
  %183 = add <4 x i32> %180, <i32 2048, i32 2048, i32 2048, i32 2048>
  %184 = ashr <4 x i32> %183, <i32 12, i32 12, i32 12, i32 12>
  %185 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %182, <4 x i32> %182) #8
  %186 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %184, <4 x i32> %184) #8
  %187 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %159, <8 x i16> %136) #8
  %188 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %160, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %189 = add <4 x i32> %187, <i32 2048, i32 2048, i32 2048, i32 2048>
  %190 = ashr <4 x i32> %189, <i32 12, i32 12, i32 12, i32 12>
  %191 = add <4 x i32> %188, <i32 2048, i32 2048, i32 2048, i32 2048>
  %192 = ashr <4 x i32> %191, <i32 12, i32 12, i32 12, i32 12>
  %193 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %190, <4 x i32> undef) #8
  %194 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %192, <4 x i32> %192) #8
  %195 = shufflevector <8 x i16> %176, <8 x i16> %175, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %196 = shufflevector <8 x i16> %175, <8 x i16> %176, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %197 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %195, <8 x i16> %136) #8
  %198 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %196, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %199 = add <4 x i32> %197, <i32 2048, i32 2048, i32 2048, i32 2048>
  %200 = ashr <4 x i32> %199, <i32 12, i32 12, i32 12, i32 12>
  %201 = add <4 x i32> %198, <i32 2048, i32 2048, i32 2048, i32 2048>
  %202 = ashr <4 x i32> %201, <i32 12, i32 12, i32 12, i32 12>
  %203 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %200, <4 x i32> %200) #8
  %204 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %202, <4 x i32> undef) #8
  %205 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %177, <8 x i16> %136) #8
  %206 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %178, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %207 = add <4 x i32> %205, <i32 2048, i32 2048, i32 2048, i32 2048>
  %208 = ashr <4 x i32> %207, <i32 12, i32 12, i32 12, i32 12>
  %209 = add <4 x i32> %206, <i32 2048, i32 2048, i32 2048, i32 2048>
  %210 = ashr <4 x i32> %209, <i32 12, i32 12, i32 12, i32 12>
  %211 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %208, <4 x i32> %208) #8
  %212 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %210, <4 x i32> undef) #8
  %213 = shufflevector <8 x i16> %186, <8 x i16> %185, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %214 = shufflevector <8 x i16> %185, <8 x i16> %186, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %215 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %213, <8 x i16> %136) #8
  %216 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %214, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %217 = add <4 x i32> %215, <i32 2048, i32 2048, i32 2048, i32 2048>
  %218 = ashr <4 x i32> %217, <i32 12, i32 12, i32 12, i32 12>
  %219 = add <4 x i32> %216, <i32 2048, i32 2048, i32 2048, i32 2048>
  %220 = ashr <4 x i32> %219, <i32 12, i32 12, i32 12, i32 12>
  %221 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %218, <4 x i32> undef) #8
  %222 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %220, <4 x i32> %220) #8
  %223 = bitcast <8 x i16> %158 to <2 x i64>
  %224 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %168) #8
  %225 = bitcast <8 x i16> %224 to <2 x i64>
  %226 = bitcast <8 x i16> %186 to <2 x i64>
  %227 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %176) #8
  %228 = bitcast <8 x i16> %227 to <2 x i64>
  %229 = bitcast <8 x i16> %204 to <2 x i64>
  %230 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %222) #8
  %231 = bitcast <8 x i16> %230 to <2 x i64>
  %232 = bitcast <8 x i16> %212 to <2 x i64>
  %233 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %194) #8
  %234 = bitcast <8 x i16> %233 to <2 x i64>
  %235 = bitcast <8 x i16> %193 to <2 x i64>
  %236 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %211) #8
  %237 = bitcast <8 x i16> %236 to <2 x i64>
  %238 = bitcast <8 x i16> %221 to <2 x i64>
  %239 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %203) #8
  %240 = bitcast <8 x i16> %239 to <2 x i64>
  %241 = bitcast <8 x i16> %175 to <2 x i64>
  %242 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %185) #8
  %243 = bitcast <8 x i16> %242 to <2 x i64>
  %244 = bitcast <8 x i16> %167 to <2 x i64>
  %245 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %157) #8
  %246 = bitcast <8 x i16> %245 to <2 x i64>
  %247 = extractelement <2 x i64> %223, i32 0
  %248 = bitcast i16* %153 to i64*
  store i64 %247, i64* %248, align 1
  %249 = getelementptr inbounds i16, i16* %153, i64 %137
  %250 = extractelement <2 x i64> %225, i32 0
  %251 = bitcast i16* %249 to i64*
  store i64 %250, i64* %251, align 1
  %252 = getelementptr inbounds i16, i16* %153, i64 %138
  %253 = extractelement <2 x i64> %226, i32 0
  %254 = bitcast i16* %252 to i64*
  store i64 %253, i64* %254, align 1
  %255 = getelementptr inbounds i16, i16* %153, i64 %139
  %256 = extractelement <2 x i64> %228, i32 0
  %257 = bitcast i16* %255 to i64*
  store i64 %256, i64* %257, align 1
  %258 = getelementptr inbounds i16, i16* %153, i64 %140
  %259 = extractelement <2 x i64> %229, i32 0
  %260 = bitcast i16* %258 to i64*
  store i64 %259, i64* %260, align 1
  %261 = getelementptr inbounds i16, i16* %153, i64 %141
  %262 = extractelement <2 x i64> %231, i32 0
  %263 = bitcast i16* %261 to i64*
  store i64 %262, i64* %263, align 1
  %264 = getelementptr inbounds i16, i16* %153, i64 %142
  %265 = extractelement <2 x i64> %232, i32 0
  %266 = bitcast i16* %264 to i64*
  store i64 %265, i64* %266, align 1
  %267 = getelementptr inbounds i16, i16* %153, i64 %143
  %268 = extractelement <2 x i64> %234, i32 0
  %269 = bitcast i16* %267 to i64*
  store i64 %268, i64* %269, align 1
  %270 = getelementptr inbounds i16, i16* %153, i64 %144
  %271 = extractelement <2 x i64> %235, i32 0
  %272 = bitcast i16* %270 to i64*
  store i64 %271, i64* %272, align 1
  %273 = getelementptr inbounds i16, i16* %153, i64 %145
  %274 = extractelement <2 x i64> %237, i32 0
  %275 = bitcast i16* %273 to i64*
  store i64 %274, i64* %275, align 1
  %276 = getelementptr inbounds i16, i16* %153, i64 %146
  %277 = extractelement <2 x i64> %238, i32 0
  %278 = bitcast i16* %276 to i64*
  store i64 %277, i64* %278, align 1
  %279 = getelementptr inbounds i16, i16* %153, i64 %147
  %280 = extractelement <2 x i64> %240, i32 0
  %281 = bitcast i16* %279 to i64*
  store i64 %280, i64* %281, align 1
  %282 = getelementptr inbounds i16, i16* %153, i64 %148
  %283 = extractelement <2 x i64> %241, i32 0
  %284 = bitcast i16* %282 to i64*
  store i64 %283, i64* %284, align 1
  %285 = getelementptr inbounds i16, i16* %153, i64 %149
  %286 = extractelement <2 x i64> %243, i32 0
  %287 = bitcast i16* %285 to i64*
  store i64 %286, i64* %287, align 1
  %288 = getelementptr inbounds i16, i16* %153, i64 %150
  %289 = extractelement <2 x i64> %244, i32 0
  %290 = bitcast i16* %288 to i64*
  store i64 %289, i64* %290, align 1
  %291 = getelementptr inbounds i16, i16* %153, i64 %151
  %292 = extractelement <2 x i64> %246, i32 0
  %293 = bitcast i16* %291 to i64*
  store i64 %292, i64* %293, align 1
  %294 = add nuw nsw i32 %154, 4
  %295 = getelementptr inbounds i16, i16* %153, i64 4
  %296 = icmp ult i32 %294, %12
  br i1 %296, label %152, label %1195

297:                                              ; preds = %131
  %298 = icmp eq i8 %11, 4
  br i1 %298, label %331, label %299

299:                                              ; preds = %297
  %300 = zext i8 %11 to i64
  %301 = shl nuw nsw i64 %300, 1
  %302 = mul nuw nsw i64 %300, 3
  %303 = shl nuw nsw i64 %300, 2
  %304 = mul nuw nsw i64 %300, 5
  %305 = mul nuw nsw i64 %300, 6
  %306 = mul nuw nsw i64 %300, 7
  %307 = shl nuw nsw i64 %300, 3
  %308 = mul nuw nsw i64 %300, 9
  %309 = mul nuw nsw i64 %300, 10
  %310 = mul nuw nsw i64 %300, 11
  %311 = mul nuw nsw i64 %300, 12
  %312 = mul nuw nsw i64 %300, 13
  %313 = mul nuw nsw i64 %300, 14
  %314 = mul nuw nsw i64 %300, 15
  %315 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 201, i16 4091, i16 201, i16 4091, i16 201, i16 4091, i16 201, i16 4091>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %316 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 995, i16 3973, i16 995, i16 3973, i16 995, i16 3973, i16 995, i16 3973>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %317 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1751, i16 3703, i16 1751, i16 3703, i16 1751, i16 3703, i16 1751, i16 3703>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %318 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2440, i16 3290, i16 2440, i16 3290, i16 2440, i16 3290, i16 2440, i16 3290>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %319 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3035, i16 2751, i16 3035, i16 2751, i16 3035, i16 2751, i16 3035, i16 2751>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %320 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3513, i16 2106, i16 3513, i16 2106, i16 3513, i16 2106, i16 3513, i16 2106>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %321 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3857, i16 1380, i16 3857, i16 1380, i16 3857, i16 1380, i16 3857, i16 1380>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %322 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 4052, i16 601, i16 4052, i16 601, i16 4052, i16 601, i16 4052, i16 601>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %323 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %324 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %325 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %326 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %327 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %328 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %329 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %330 = zext i8 %11 to i64
  br label %727

331:                                              ; preds = %297
  %332 = bitcast i8* %3 to i64*
  %333 = load i64, i64* %332, align 1
  %334 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %333, i32 0
  %335 = getelementptr inbounds i8, i8* %3, i64 8
  %336 = bitcast i8* %335 to i64*
  %337 = load i64, i64* %336, align 1
  %338 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %337, i32 0
  %339 = getelementptr inbounds i8, i8* %3, i64 16
  %340 = bitcast i8* %339 to i64*
  %341 = load i64, i64* %340, align 1
  %342 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %341, i32 0
  %343 = getelementptr inbounds i8, i8* %3, i64 24
  %344 = bitcast i8* %343 to i64*
  %345 = load i64, i64* %344, align 1
  %346 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %345, i32 0
  %347 = getelementptr inbounds i8, i8* %3, i64 32
  %348 = bitcast i8* %347 to i64*
  %349 = load i64, i64* %348, align 1
  %350 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %349, i32 0
  %351 = getelementptr inbounds i8, i8* %3, i64 40
  %352 = bitcast i8* %351 to i64*
  %353 = load i64, i64* %352, align 1
  %354 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %353, i32 0
  %355 = getelementptr inbounds i8, i8* %3, i64 48
  %356 = bitcast i8* %355 to i64*
  %357 = load i64, i64* %356, align 1
  %358 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %357, i32 0
  %359 = getelementptr inbounds i8, i8* %3, i64 56
  %360 = bitcast i8* %359 to i64*
  %361 = load i64, i64* %360, align 1
  %362 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %361, i32 0
  %363 = getelementptr inbounds i8, i8* %3, i64 64
  %364 = bitcast i8* %363 to i64*
  %365 = load i64, i64* %364, align 1
  %366 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %365, i32 0
  %367 = getelementptr inbounds i8, i8* %3, i64 72
  %368 = bitcast i8* %367 to i64*
  %369 = load i64, i64* %368, align 1
  %370 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %369, i32 0
  %371 = getelementptr inbounds i8, i8* %3, i64 80
  %372 = bitcast i8* %371 to i64*
  %373 = load i64, i64* %372, align 1
  %374 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %373, i32 0
  %375 = getelementptr inbounds i8, i8* %3, i64 88
  %376 = bitcast i8* %375 to i64*
  %377 = load i64, i64* %376, align 1
  %378 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %377, i32 0
  %379 = getelementptr inbounds i8, i8* %3, i64 96
  %380 = bitcast i8* %379 to i64*
  %381 = load i64, i64* %380, align 1
  %382 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %381, i32 0
  %383 = getelementptr inbounds i8, i8* %3, i64 104
  %384 = bitcast i8* %383 to i64*
  %385 = load i64, i64* %384, align 1
  %386 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %385, i32 0
  %387 = getelementptr inbounds i8, i8* %3, i64 112
  %388 = bitcast i8* %387 to i64*
  %389 = load i64, i64* %388, align 1
  %390 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %389, i32 0
  %391 = getelementptr inbounds i8, i8* %3, i64 120
  %392 = bitcast i8* %391 to i64*
  %393 = load i64, i64* %392, align 1
  %394 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %393, i32 0
  %395 = bitcast <2 x i64> %394 to <8 x i16>
  %396 = bitcast <2 x i64> %334 to <8 x i16>
  %397 = shufflevector <8 x i16> %395, <8 x i16> %396, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %398 = shufflevector <8 x i16> %396, <8 x i16> %395, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %399 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 201, i16 4091, i16 201, i16 4091, i16 201, i16 4091, i16 201, i16 4091>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %400 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %397, <8 x i16> %399) #8
  %401 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %398, <8 x i16> <i16 201, i16 4091, i16 201, i16 4091, i16 201, i16 4091, i16 201, i16 4091>) #8
  %402 = add <4 x i32> %400, <i32 2048, i32 2048, i32 2048, i32 2048>
  %403 = ashr <4 x i32> %402, <i32 12, i32 12, i32 12, i32 12>
  %404 = add <4 x i32> %401, <i32 2048, i32 2048, i32 2048, i32 2048>
  %405 = ashr <4 x i32> %404, <i32 12, i32 12, i32 12, i32 12>
  %406 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %403, <4 x i32> %403) #8
  %407 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %405, <4 x i32> %405) #8
  %408 = bitcast <2 x i64> %386 to <8 x i16>
  %409 = bitcast <2 x i64> %342 to <8 x i16>
  %410 = shufflevector <8 x i16> %408, <8 x i16> %409, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %411 = shufflevector <8 x i16> %409, <8 x i16> %408, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %412 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 995, i16 3973, i16 995, i16 3973, i16 995, i16 3973, i16 995, i16 3973>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %413 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %410, <8 x i16> %412) #8
  %414 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %411, <8 x i16> <i16 995, i16 3973, i16 995, i16 3973, i16 995, i16 3973, i16 995, i16 3973>) #8
  %415 = add <4 x i32> %413, <i32 2048, i32 2048, i32 2048, i32 2048>
  %416 = ashr <4 x i32> %415, <i32 12, i32 12, i32 12, i32 12>
  %417 = add <4 x i32> %414, <i32 2048, i32 2048, i32 2048, i32 2048>
  %418 = ashr <4 x i32> %417, <i32 12, i32 12, i32 12, i32 12>
  %419 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %416, <4 x i32> %416) #8
  %420 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %418, <4 x i32> %418) #8
  %421 = bitcast <2 x i64> %378 to <8 x i16>
  %422 = bitcast <2 x i64> %350 to <8 x i16>
  %423 = shufflevector <8 x i16> %421, <8 x i16> %422, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %424 = shufflevector <8 x i16> %422, <8 x i16> %421, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %425 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1751, i16 3703, i16 1751, i16 3703, i16 1751, i16 3703, i16 1751, i16 3703>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %426 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %423, <8 x i16> %425) #8
  %427 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %424, <8 x i16> <i16 1751, i16 3703, i16 1751, i16 3703, i16 1751, i16 3703, i16 1751, i16 3703>) #8
  %428 = add <4 x i32> %426, <i32 2048, i32 2048, i32 2048, i32 2048>
  %429 = ashr <4 x i32> %428, <i32 12, i32 12, i32 12, i32 12>
  %430 = add <4 x i32> %427, <i32 2048, i32 2048, i32 2048, i32 2048>
  %431 = ashr <4 x i32> %430, <i32 12, i32 12, i32 12, i32 12>
  %432 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %429, <4 x i32> %429) #8
  %433 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %431, <4 x i32> %431) #8
  %434 = bitcast <2 x i64> %370 to <8 x i16>
  %435 = bitcast <2 x i64> %358 to <8 x i16>
  %436 = shufflevector <8 x i16> %434, <8 x i16> %435, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %437 = shufflevector <8 x i16> %435, <8 x i16> %434, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %438 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2440, i16 3290, i16 2440, i16 3290, i16 2440, i16 3290, i16 2440, i16 3290>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %439 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %436, <8 x i16> %438) #8
  %440 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %437, <8 x i16> <i16 2440, i16 3290, i16 2440, i16 3290, i16 2440, i16 3290, i16 2440, i16 3290>) #8
  %441 = add <4 x i32> %439, <i32 2048, i32 2048, i32 2048, i32 2048>
  %442 = ashr <4 x i32> %441, <i32 12, i32 12, i32 12, i32 12>
  %443 = add <4 x i32> %440, <i32 2048, i32 2048, i32 2048, i32 2048>
  %444 = ashr <4 x i32> %443, <i32 12, i32 12, i32 12, i32 12>
  %445 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %442, <4 x i32> %442) #8
  %446 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %444, <4 x i32> %444) #8
  %447 = bitcast <2 x i64> %362 to <8 x i16>
  %448 = bitcast <2 x i64> %366 to <8 x i16>
  %449 = shufflevector <8 x i16> %447, <8 x i16> %448, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %450 = shufflevector <8 x i16> %448, <8 x i16> %447, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %451 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3035, i16 2751, i16 3035, i16 2751, i16 3035, i16 2751, i16 3035, i16 2751>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %452 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %449, <8 x i16> %451) #8
  %453 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %450, <8 x i16> <i16 3035, i16 2751, i16 3035, i16 2751, i16 3035, i16 2751, i16 3035, i16 2751>) #8
  %454 = add <4 x i32> %452, <i32 2048, i32 2048, i32 2048, i32 2048>
  %455 = ashr <4 x i32> %454, <i32 12, i32 12, i32 12, i32 12>
  %456 = add <4 x i32> %453, <i32 2048, i32 2048, i32 2048, i32 2048>
  %457 = ashr <4 x i32> %456, <i32 12, i32 12, i32 12, i32 12>
  %458 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %455, <4 x i32> %455) #8
  %459 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %457, <4 x i32> %457) #8
  %460 = bitcast <2 x i64> %354 to <8 x i16>
  %461 = bitcast <2 x i64> %374 to <8 x i16>
  %462 = shufflevector <8 x i16> %460, <8 x i16> %461, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %463 = shufflevector <8 x i16> %461, <8 x i16> %460, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %464 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3513, i16 2106, i16 3513, i16 2106, i16 3513, i16 2106, i16 3513, i16 2106>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %465 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %462, <8 x i16> %464) #8
  %466 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %463, <8 x i16> <i16 3513, i16 2106, i16 3513, i16 2106, i16 3513, i16 2106, i16 3513, i16 2106>) #8
  %467 = add <4 x i32> %465, <i32 2048, i32 2048, i32 2048, i32 2048>
  %468 = ashr <4 x i32> %467, <i32 12, i32 12, i32 12, i32 12>
  %469 = add <4 x i32> %466, <i32 2048, i32 2048, i32 2048, i32 2048>
  %470 = ashr <4 x i32> %469, <i32 12, i32 12, i32 12, i32 12>
  %471 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %468, <4 x i32> %468) #8
  %472 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %470, <4 x i32> %470) #8
  %473 = bitcast <2 x i64> %346 to <8 x i16>
  %474 = bitcast <2 x i64> %382 to <8 x i16>
  %475 = shufflevector <8 x i16> %473, <8 x i16> %474, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %476 = shufflevector <8 x i16> %474, <8 x i16> %473, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %477 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3857, i16 1380, i16 3857, i16 1380, i16 3857, i16 1380, i16 3857, i16 1380>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %478 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %475, <8 x i16> %477) #8
  %479 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %476, <8 x i16> <i16 3857, i16 1380, i16 3857, i16 1380, i16 3857, i16 1380, i16 3857, i16 1380>) #8
  %480 = add <4 x i32> %478, <i32 2048, i32 2048, i32 2048, i32 2048>
  %481 = ashr <4 x i32> %480, <i32 12, i32 12, i32 12, i32 12>
  %482 = add <4 x i32> %479, <i32 2048, i32 2048, i32 2048, i32 2048>
  %483 = ashr <4 x i32> %482, <i32 12, i32 12, i32 12, i32 12>
  %484 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %481, <4 x i32> %481) #8
  %485 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %483, <4 x i32> %483) #8
  %486 = bitcast <2 x i64> %338 to <8 x i16>
  %487 = bitcast <2 x i64> %390 to <8 x i16>
  %488 = shufflevector <8 x i16> %486, <8 x i16> %487, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %489 = shufflevector <8 x i16> %487, <8 x i16> %486, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %490 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 4052, i16 601, i16 4052, i16 601, i16 4052, i16 601, i16 4052, i16 601>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %491 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %488, <8 x i16> %490) #8
  %492 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %489, <8 x i16> <i16 4052, i16 601, i16 4052, i16 601, i16 4052, i16 601, i16 4052, i16 601>) #8
  %493 = add <4 x i32> %491, <i32 2048, i32 2048, i32 2048, i32 2048>
  %494 = ashr <4 x i32> %493, <i32 12, i32 12, i32 12, i32 12>
  %495 = add <4 x i32> %492, <i32 2048, i32 2048, i32 2048, i32 2048>
  %496 = ashr <4 x i32> %495, <i32 12, i32 12, i32 12, i32 12>
  %497 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %494, <4 x i32> %494) #8
  %498 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %496, <4 x i32> %496) #8
  %499 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %407, <8 x i16> %459) #8
  %500 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %407, <8 x i16> %459) #8
  %501 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %406, <8 x i16> %458) #8
  %502 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %406, <8 x i16> %458) #8
  %503 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %420, <8 x i16> %472) #8
  %504 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %420, <8 x i16> %472) #8
  %505 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %419, <8 x i16> %471) #8
  %506 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %419, <8 x i16> %471) #8
  %507 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %433, <8 x i16> %485) #8
  %508 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %433, <8 x i16> %485) #8
  %509 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %432, <8 x i16> %484) #8
  %510 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %432, <8 x i16> %484) #8
  %511 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %446, <8 x i16> %498) #8
  %512 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %446, <8 x i16> %498) #8
  %513 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %445, <8 x i16> %497) #8
  %514 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %445, <8 x i16> %497) #8
  %515 = shufflevector <8 x i16> %500, <8 x i16> %502, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %516 = shufflevector <8 x i16> %502, <8 x i16> %500, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %517 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %518 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %515, <8 x i16> %517) #8
  %519 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %516, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %520 = add <4 x i32> %518, <i32 2048, i32 2048, i32 2048, i32 2048>
  %521 = ashr <4 x i32> %520, <i32 12, i32 12, i32 12, i32 12>
  %522 = add <4 x i32> %519, <i32 2048, i32 2048, i32 2048, i32 2048>
  %523 = ashr <4 x i32> %522, <i32 12, i32 12, i32 12, i32 12>
  %524 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %521, <4 x i32> %521) #8
  %525 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %523, <4 x i32> %523) #8
  %526 = shufflevector <8 x i16> %510, <8 x i16> %508, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %527 = shufflevector <8 x i16> %508, <8 x i16> %510, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %528 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %529 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %526, <8 x i16> %528) #8
  %530 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %527, <8 x i16> <i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799>) #8
  %531 = add <4 x i32> %529, <i32 2048, i32 2048, i32 2048, i32 2048>
  %532 = ashr <4 x i32> %531, <i32 12, i32 12, i32 12, i32 12>
  %533 = add <4 x i32> %530, <i32 2048, i32 2048, i32 2048, i32 2048>
  %534 = ashr <4 x i32> %533, <i32 12, i32 12, i32 12, i32 12>
  %535 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %532, <4 x i32> %532) #8
  %536 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %534, <4 x i32> %534) #8
  %537 = shufflevector <8 x i16> %504, <8 x i16> %506, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %538 = shufflevector <8 x i16> %506, <8 x i16> %504, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %539 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %540 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %537, <8 x i16> %539) #8
  %541 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %538, <8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>) #8
  %542 = add <4 x i32> %540, <i32 2048, i32 2048, i32 2048, i32 2048>
  %543 = ashr <4 x i32> %542, <i32 12, i32 12, i32 12, i32 12>
  %544 = add <4 x i32> %541, <i32 2048, i32 2048, i32 2048, i32 2048>
  %545 = ashr <4 x i32> %544, <i32 12, i32 12, i32 12, i32 12>
  %546 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %543, <4 x i32> %543) #8
  %547 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %545, <4 x i32> %545) #8
  %548 = shufflevector <8 x i16> %514, <8 x i16> %512, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %549 = shufflevector <8 x i16> %512, <8 x i16> %514, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %550 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %551 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %548, <8 x i16> %550) #8
  %552 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %549, <8 x i16> <i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406>) #8
  %553 = add <4 x i32> %551, <i32 2048, i32 2048, i32 2048, i32 2048>
  %554 = ashr <4 x i32> %553, <i32 12, i32 12, i32 12, i32 12>
  %555 = add <4 x i32> %552, <i32 2048, i32 2048, i32 2048, i32 2048>
  %556 = ashr <4 x i32> %555, <i32 12, i32 12, i32 12, i32 12>
  %557 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %554, <4 x i32> %554) #8
  %558 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %556, <4 x i32> %556) #8
  %559 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %499, <8 x i16> %507) #8
  %560 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %499, <8 x i16> %507) #8
  %561 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %525, <8 x i16> %535) #8
  %562 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %525, <8 x i16> %535) #8
  %563 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %501, <8 x i16> %509) #8
  %564 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %501, <8 x i16> %509) #8
  %565 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %524, <8 x i16> %536) #8
  %566 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %524, <8 x i16> %536) #8
  %567 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %503, <8 x i16> %511) #8
  %568 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %503, <8 x i16> %511) #8
  %569 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %547, <8 x i16> %557) #8
  %570 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %547, <8 x i16> %557) #8
  %571 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %505, <8 x i16> %513) #8
  %572 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %505, <8 x i16> %513) #8
  %573 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %546, <8 x i16> %558) #8
  %574 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %546, <8 x i16> %558) #8
  %575 = shufflevector <8 x i16> %560, <8 x i16> %564, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %576 = shufflevector <8 x i16> %564, <8 x i16> %560, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %577 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %578 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %575, <8 x i16> %577) #8
  %579 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %576, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %580 = add <4 x i32> %578, <i32 2048, i32 2048, i32 2048, i32 2048>
  %581 = ashr <4 x i32> %580, <i32 12, i32 12, i32 12, i32 12>
  %582 = add <4 x i32> %579, <i32 2048, i32 2048, i32 2048, i32 2048>
  %583 = ashr <4 x i32> %582, <i32 12, i32 12, i32 12, i32 12>
  %584 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %581, <4 x i32> %581) #8
  %585 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %583, <4 x i32> %583) #8
  %586 = shufflevector <8 x i16> %562, <8 x i16> %566, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %587 = shufflevector <8 x i16> %566, <8 x i16> %562, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %588 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %586, <8 x i16> %577) #8
  %589 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %587, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %590 = add <4 x i32> %588, <i32 2048, i32 2048, i32 2048, i32 2048>
  %591 = ashr <4 x i32> %590, <i32 12, i32 12, i32 12, i32 12>
  %592 = add <4 x i32> %589, <i32 2048, i32 2048, i32 2048, i32 2048>
  %593 = ashr <4 x i32> %592, <i32 12, i32 12, i32 12, i32 12>
  %594 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %591, <4 x i32> %591) #8
  %595 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %593, <4 x i32> %593) #8
  %596 = shufflevector <8 x i16> %572, <8 x i16> %568, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %597 = shufflevector <8 x i16> %568, <8 x i16> %572, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %598 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %599 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %596, <8 x i16> %598) #8
  %600 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %597, <8 x i16> <i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567>) #8
  %601 = add <4 x i32> %599, <i32 2048, i32 2048, i32 2048, i32 2048>
  %602 = ashr <4 x i32> %601, <i32 12, i32 12, i32 12, i32 12>
  %603 = add <4 x i32> %600, <i32 2048, i32 2048, i32 2048, i32 2048>
  %604 = ashr <4 x i32> %603, <i32 12, i32 12, i32 12, i32 12>
  %605 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %602, <4 x i32> %602) #8
  %606 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %604, <4 x i32> %604) #8
  %607 = shufflevector <8 x i16> %574, <8 x i16> %570, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %608 = shufflevector <8 x i16> %570, <8 x i16> %574, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %609 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %607, <8 x i16> %598) #8
  %610 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %608, <8 x i16> <i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567>) #8
  %611 = add <4 x i32> %609, <i32 2048, i32 2048, i32 2048, i32 2048>
  %612 = ashr <4 x i32> %611, <i32 12, i32 12, i32 12, i32 12>
  %613 = add <4 x i32> %610, <i32 2048, i32 2048, i32 2048, i32 2048>
  %614 = ashr <4 x i32> %613, <i32 12, i32 12, i32 12, i32 12>
  %615 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %612, <4 x i32> %612) #8
  %616 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %614, <4 x i32> %614) #8
  %617 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %559, <8 x i16> %567) #8
  %618 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %559, <8 x i16> %567) #8
  %619 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %585, <8 x i16> %605) #8
  %620 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %585, <8 x i16> %605) #8
  %621 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %561, <8 x i16> %569) #8
  %622 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %561, <8 x i16> %569) #8
  %623 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %595, <8 x i16> %615) #8
  %624 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %595, <8 x i16> %615) #8
  %625 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %563, <8 x i16> %571) #8
  %626 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %563, <8 x i16> %571) #8
  %627 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %584, <8 x i16> %606) #8
  %628 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %584, <8 x i16> %606) #8
  %629 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %565, <8 x i16> %573) #8
  %630 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %565, <8 x i16> %573) #8
  %631 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %594, <8 x i16> %616) #8
  %632 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %594, <8 x i16> %616) #8
  %633 = shufflevector <8 x i16> %618, <8 x i16> %626, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %634 = shufflevector <8 x i16> %626, <8 x i16> %618, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %635 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %636 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %633, <8 x i16> %635) #8
  %637 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %634, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %638 = add <4 x i32> %636, <i32 2048, i32 2048, i32 2048, i32 2048>
  %639 = ashr <4 x i32> %638, <i32 12, i32 12, i32 12, i32 12>
  %640 = add <4 x i32> %637, <i32 2048, i32 2048, i32 2048, i32 2048>
  %641 = ashr <4 x i32> %640, <i32 12, i32 12, i32 12, i32 12>
  %642 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %639, <4 x i32> undef) #8
  %643 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %641, <4 x i32> %641) #8
  %644 = shufflevector <8 x i16> %620, <8 x i16> %628, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %645 = shufflevector <8 x i16> %628, <8 x i16> %620, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %646 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %644, <8 x i16> %635) #8
  %647 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %645, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %648 = add <4 x i32> %646, <i32 2048, i32 2048, i32 2048, i32 2048>
  %649 = ashr <4 x i32> %648, <i32 12, i32 12, i32 12, i32 12>
  %650 = add <4 x i32> %647, <i32 2048, i32 2048, i32 2048, i32 2048>
  %651 = ashr <4 x i32> %650, <i32 12, i32 12, i32 12, i32 12>
  %652 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %649, <4 x i32> %649) #8
  %653 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %651, <4 x i32> undef) #8
  %654 = shufflevector <8 x i16> %622, <8 x i16> %630, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %655 = shufflevector <8 x i16> %630, <8 x i16> %622, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %656 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %654, <8 x i16> %635) #8
  %657 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %655, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %658 = add <4 x i32> %656, <i32 2048, i32 2048, i32 2048, i32 2048>
  %659 = ashr <4 x i32> %658, <i32 12, i32 12, i32 12, i32 12>
  %660 = add <4 x i32> %657, <i32 2048, i32 2048, i32 2048, i32 2048>
  %661 = ashr <4 x i32> %660, <i32 12, i32 12, i32 12, i32 12>
  %662 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %659, <4 x i32> %659) #8
  %663 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %661, <4 x i32> undef) #8
  %664 = shufflevector <8 x i16> %624, <8 x i16> %632, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %665 = shufflevector <8 x i16> %632, <8 x i16> %624, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %666 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %664, <8 x i16> %635) #8
  %667 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %665, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %668 = add <4 x i32> %666, <i32 2048, i32 2048, i32 2048, i32 2048>
  %669 = ashr <4 x i32> %668, <i32 12, i32 12, i32 12, i32 12>
  %670 = add <4 x i32> %667, <i32 2048, i32 2048, i32 2048, i32 2048>
  %671 = ashr <4 x i32> %670, <i32 12, i32 12, i32 12, i32 12>
  %672 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %669, <4 x i32> undef) #8
  %673 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %671, <4 x i32> %671) #8
  %674 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %621) #8
  %675 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %619) #8
  %676 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %673) #8
  %677 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %643) #8
  %678 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %662) #8
  %679 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %652) #8
  %680 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %631) #8
  %681 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %625) #8
  %682 = bitcast <8 x i16> %681 to <2 x i64>
  %683 = bitcast <8 x i16> %680 to <2 x i64>
  %684 = bitcast <8 x i16> %679 to <2 x i64>
  %685 = bitcast <8 x i16> %678 to <2 x i64>
  %686 = bitcast <8 x i16> %677 to <2 x i64>
  %687 = bitcast <8 x i16> %676 to <2 x i64>
  %688 = bitcast <8 x i16> %675 to <2 x i64>
  %689 = bitcast <8 x i16> %674 to <2 x i64>
  %690 = bitcast <8 x i16> %672 to <2 x i64>
  %691 = bitcast <8 x i16> %663 to <2 x i64>
  %692 = bitcast <8 x i16> %653 to <2 x i64>
  %693 = bitcast <8 x i16> %642 to <2 x i64>
  %694 = bitcast <8 x i16> %629 to <2 x i64>
  %695 = bitcast <8 x i16> %627 to <2 x i64>
  %696 = bitcast <8 x i16> %623 to <2 x i64>
  %697 = bitcast <8 x i16> %617 to <2 x i64>
  %698 = extractelement <2 x i64> %697, i32 0
  store i64 %698, i64* %332, align 1
  %699 = extractelement <2 x i64> %689, i32 0
  store i64 %699, i64* %336, align 1
  %700 = extractelement <2 x i64> %696, i32 0
  store i64 %700, i64* %340, align 1
  %701 = extractelement <2 x i64> %688, i32 0
  store i64 %701, i64* %344, align 1
  %702 = extractelement <2 x i64> %692, i32 0
  store i64 %702, i64* %348, align 1
  %703 = extractelement <2 x i64> %687, i32 0
  store i64 %703, i64* %352, align 1
  %704 = extractelement <2 x i64> %691, i32 0
  store i64 %704, i64* %356, align 1
  %705 = extractelement <2 x i64> %686, i32 0
  store i64 %705, i64* %360, align 1
  %706 = extractelement <2 x i64> %693, i32 0
  store i64 %706, i64* %364, align 1
  %707 = extractelement <2 x i64> %685, i32 0
  store i64 %707, i64* %368, align 1
  %708 = extractelement <2 x i64> %690, i32 0
  store i64 %708, i64* %372, align 1
  %709 = extractelement <2 x i64> %684, i32 0
  store i64 %709, i64* %376, align 1
  %710 = extractelement <2 x i64> %695, i32 0
  store i64 %710, i64* %380, align 1
  %711 = extractelement <2 x i64> %683, i32 0
  store i64 %711, i64* %384, align 1
  %712 = extractelement <2 x i64> %694, i32 0
  store i64 %712, i64* %388, align 1
  %713 = extractelement <2 x i64> %682, i32 0
  store i64 %713, i64* %392, align 1
  %714 = bitcast i8* %6 to i64*
  %715 = load i64, i64* %714, align 8
  %716 = getelementptr inbounds i8, i8* %6, i64 8
  %717 = bitcast i8* %716 to i8**
  %718 = load i8*, i8** %717, align 8
  %719 = and i32 %14, 16608
  %720 = icmp ne i32 %719, 0
  %721 = sext i32 %5 to i64
  %722 = ashr i64 %715, 32
  %723 = mul nsw i64 %722, %721
  %724 = getelementptr inbounds i8, i8* %718, i64 %723
  %725 = sext i32 %4 to i64
  %726 = getelementptr inbounds i8, i8* %724, i64 %725
  br label %1209

727:                                              ; preds = %299, %727
  %728 = phi i64 [ 0, %299 ], [ %1193, %727 ]
  %729 = getelementptr inbounds i16, i16* %8, i64 %728
  %730 = bitcast i16* %729 to <8 x i16>*
  %731 = load <8 x i16>, <8 x i16>* %730, align 1
  %732 = getelementptr inbounds i16, i16* %729, i64 %300
  %733 = bitcast i16* %732 to <8 x i16>*
  %734 = load <8 x i16>, <8 x i16>* %733, align 1
  %735 = getelementptr inbounds i16, i16* %729, i64 %301
  %736 = bitcast i16* %735 to <8 x i16>*
  %737 = load <8 x i16>, <8 x i16>* %736, align 1
  %738 = getelementptr inbounds i16, i16* %729, i64 %302
  %739 = bitcast i16* %738 to <8 x i16>*
  %740 = load <8 x i16>, <8 x i16>* %739, align 1
  %741 = getelementptr inbounds i16, i16* %729, i64 %303
  %742 = bitcast i16* %741 to <8 x i16>*
  %743 = load <8 x i16>, <8 x i16>* %742, align 1
  %744 = getelementptr inbounds i16, i16* %729, i64 %304
  %745 = bitcast i16* %744 to <8 x i16>*
  %746 = load <8 x i16>, <8 x i16>* %745, align 1
  %747 = getelementptr inbounds i16, i16* %729, i64 %305
  %748 = bitcast i16* %747 to <8 x i16>*
  %749 = load <8 x i16>, <8 x i16>* %748, align 1
  %750 = getelementptr inbounds i16, i16* %729, i64 %306
  %751 = bitcast i16* %750 to <8 x i16>*
  %752 = load <8 x i16>, <8 x i16>* %751, align 1
  %753 = getelementptr inbounds i16, i16* %729, i64 %307
  %754 = bitcast i16* %753 to <8 x i16>*
  %755 = load <8 x i16>, <8 x i16>* %754, align 1
  %756 = getelementptr inbounds i16, i16* %729, i64 %308
  %757 = bitcast i16* %756 to <8 x i16>*
  %758 = load <8 x i16>, <8 x i16>* %757, align 1
  %759 = getelementptr inbounds i16, i16* %729, i64 %309
  %760 = bitcast i16* %759 to <8 x i16>*
  %761 = load <8 x i16>, <8 x i16>* %760, align 1
  %762 = getelementptr inbounds i16, i16* %729, i64 %310
  %763 = bitcast i16* %762 to <8 x i16>*
  %764 = load <8 x i16>, <8 x i16>* %763, align 1
  %765 = getelementptr inbounds i16, i16* %729, i64 %311
  %766 = bitcast i16* %765 to <8 x i16>*
  %767 = load <8 x i16>, <8 x i16>* %766, align 1
  %768 = getelementptr inbounds i16, i16* %729, i64 %312
  %769 = bitcast i16* %768 to <8 x i16>*
  %770 = load <8 x i16>, <8 x i16>* %769, align 1
  %771 = getelementptr inbounds i16, i16* %729, i64 %313
  %772 = bitcast i16* %771 to <8 x i16>*
  %773 = load <8 x i16>, <8 x i16>* %772, align 1
  %774 = getelementptr inbounds i16, i16* %729, i64 %314
  %775 = bitcast i16* %774 to <8 x i16>*
  %776 = load <8 x i16>, <8 x i16>* %775, align 1
  %777 = shufflevector <8 x i16> %776, <8 x i16> %731, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %778 = shufflevector <8 x i16> %731, <8 x i16> %776, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %779 = shufflevector <8 x i16> %776, <8 x i16> %731, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %780 = shufflevector <8 x i16> %731, <8 x i16> %776, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %781 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %777, <8 x i16> %315) #8
  %782 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %778, <8 x i16> <i16 201, i16 4091, i16 201, i16 4091, i16 201, i16 4091, i16 201, i16 4091>) #8
  %783 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %779, <8 x i16> %315) #8
  %784 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %780, <8 x i16> <i16 201, i16 4091, i16 201, i16 4091, i16 201, i16 4091, i16 201, i16 4091>) #8
  %785 = add <4 x i32> %781, <i32 2048, i32 2048, i32 2048, i32 2048>
  %786 = ashr <4 x i32> %785, <i32 12, i32 12, i32 12, i32 12>
  %787 = add <4 x i32> %782, <i32 2048, i32 2048, i32 2048, i32 2048>
  %788 = ashr <4 x i32> %787, <i32 12, i32 12, i32 12, i32 12>
  %789 = add <4 x i32> %783, <i32 2048, i32 2048, i32 2048, i32 2048>
  %790 = ashr <4 x i32> %789, <i32 12, i32 12, i32 12, i32 12>
  %791 = add <4 x i32> %784, <i32 2048, i32 2048, i32 2048, i32 2048>
  %792 = ashr <4 x i32> %791, <i32 12, i32 12, i32 12, i32 12>
  %793 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %786, <4 x i32> %790) #8
  %794 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %788, <4 x i32> %792) #8
  %795 = shufflevector <8 x i16> %770, <8 x i16> %737, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %796 = shufflevector <8 x i16> %737, <8 x i16> %770, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %797 = shufflevector <8 x i16> %770, <8 x i16> %737, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %798 = shufflevector <8 x i16> %737, <8 x i16> %770, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %799 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %795, <8 x i16> %316) #8
  %800 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %796, <8 x i16> <i16 995, i16 3973, i16 995, i16 3973, i16 995, i16 3973, i16 995, i16 3973>) #8
  %801 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %797, <8 x i16> %316) #8
  %802 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %798, <8 x i16> <i16 995, i16 3973, i16 995, i16 3973, i16 995, i16 3973, i16 995, i16 3973>) #8
  %803 = add <4 x i32> %799, <i32 2048, i32 2048, i32 2048, i32 2048>
  %804 = ashr <4 x i32> %803, <i32 12, i32 12, i32 12, i32 12>
  %805 = add <4 x i32> %800, <i32 2048, i32 2048, i32 2048, i32 2048>
  %806 = ashr <4 x i32> %805, <i32 12, i32 12, i32 12, i32 12>
  %807 = add <4 x i32> %801, <i32 2048, i32 2048, i32 2048, i32 2048>
  %808 = ashr <4 x i32> %807, <i32 12, i32 12, i32 12, i32 12>
  %809 = add <4 x i32> %802, <i32 2048, i32 2048, i32 2048, i32 2048>
  %810 = ashr <4 x i32> %809, <i32 12, i32 12, i32 12, i32 12>
  %811 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %804, <4 x i32> %808) #8
  %812 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %806, <4 x i32> %810) #8
  %813 = shufflevector <8 x i16> %764, <8 x i16> %743, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %814 = shufflevector <8 x i16> %743, <8 x i16> %764, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %815 = shufflevector <8 x i16> %764, <8 x i16> %743, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %816 = shufflevector <8 x i16> %743, <8 x i16> %764, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %817 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %813, <8 x i16> %317) #8
  %818 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %814, <8 x i16> <i16 1751, i16 3703, i16 1751, i16 3703, i16 1751, i16 3703, i16 1751, i16 3703>) #8
  %819 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %815, <8 x i16> %317) #8
  %820 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %816, <8 x i16> <i16 1751, i16 3703, i16 1751, i16 3703, i16 1751, i16 3703, i16 1751, i16 3703>) #8
  %821 = add <4 x i32> %817, <i32 2048, i32 2048, i32 2048, i32 2048>
  %822 = ashr <4 x i32> %821, <i32 12, i32 12, i32 12, i32 12>
  %823 = add <4 x i32> %818, <i32 2048, i32 2048, i32 2048, i32 2048>
  %824 = ashr <4 x i32> %823, <i32 12, i32 12, i32 12, i32 12>
  %825 = add <4 x i32> %819, <i32 2048, i32 2048, i32 2048, i32 2048>
  %826 = ashr <4 x i32> %825, <i32 12, i32 12, i32 12, i32 12>
  %827 = add <4 x i32> %820, <i32 2048, i32 2048, i32 2048, i32 2048>
  %828 = ashr <4 x i32> %827, <i32 12, i32 12, i32 12, i32 12>
  %829 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %822, <4 x i32> %826) #8
  %830 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %824, <4 x i32> %828) #8
  %831 = shufflevector <8 x i16> %758, <8 x i16> %749, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %832 = shufflevector <8 x i16> %749, <8 x i16> %758, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %833 = shufflevector <8 x i16> %758, <8 x i16> %749, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %834 = shufflevector <8 x i16> %749, <8 x i16> %758, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %835 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %831, <8 x i16> %318) #8
  %836 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %832, <8 x i16> <i16 2440, i16 3290, i16 2440, i16 3290, i16 2440, i16 3290, i16 2440, i16 3290>) #8
  %837 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %833, <8 x i16> %318) #8
  %838 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %834, <8 x i16> <i16 2440, i16 3290, i16 2440, i16 3290, i16 2440, i16 3290, i16 2440, i16 3290>) #8
  %839 = add <4 x i32> %835, <i32 2048, i32 2048, i32 2048, i32 2048>
  %840 = ashr <4 x i32> %839, <i32 12, i32 12, i32 12, i32 12>
  %841 = add <4 x i32> %836, <i32 2048, i32 2048, i32 2048, i32 2048>
  %842 = ashr <4 x i32> %841, <i32 12, i32 12, i32 12, i32 12>
  %843 = add <4 x i32> %837, <i32 2048, i32 2048, i32 2048, i32 2048>
  %844 = ashr <4 x i32> %843, <i32 12, i32 12, i32 12, i32 12>
  %845 = add <4 x i32> %838, <i32 2048, i32 2048, i32 2048, i32 2048>
  %846 = ashr <4 x i32> %845, <i32 12, i32 12, i32 12, i32 12>
  %847 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %840, <4 x i32> %844) #8
  %848 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %842, <4 x i32> %846) #8
  %849 = shufflevector <8 x i16> %752, <8 x i16> %755, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %850 = shufflevector <8 x i16> %755, <8 x i16> %752, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %851 = shufflevector <8 x i16> %752, <8 x i16> %755, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %852 = shufflevector <8 x i16> %755, <8 x i16> %752, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %853 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %849, <8 x i16> %319) #8
  %854 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %850, <8 x i16> <i16 3035, i16 2751, i16 3035, i16 2751, i16 3035, i16 2751, i16 3035, i16 2751>) #8
  %855 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %851, <8 x i16> %319) #8
  %856 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %852, <8 x i16> <i16 3035, i16 2751, i16 3035, i16 2751, i16 3035, i16 2751, i16 3035, i16 2751>) #8
  %857 = add <4 x i32> %853, <i32 2048, i32 2048, i32 2048, i32 2048>
  %858 = ashr <4 x i32> %857, <i32 12, i32 12, i32 12, i32 12>
  %859 = add <4 x i32> %854, <i32 2048, i32 2048, i32 2048, i32 2048>
  %860 = ashr <4 x i32> %859, <i32 12, i32 12, i32 12, i32 12>
  %861 = add <4 x i32> %855, <i32 2048, i32 2048, i32 2048, i32 2048>
  %862 = ashr <4 x i32> %861, <i32 12, i32 12, i32 12, i32 12>
  %863 = add <4 x i32> %856, <i32 2048, i32 2048, i32 2048, i32 2048>
  %864 = ashr <4 x i32> %863, <i32 12, i32 12, i32 12, i32 12>
  %865 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %858, <4 x i32> %862) #8
  %866 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %860, <4 x i32> %864) #8
  %867 = shufflevector <8 x i16> %746, <8 x i16> %761, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %868 = shufflevector <8 x i16> %761, <8 x i16> %746, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %869 = shufflevector <8 x i16> %746, <8 x i16> %761, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %870 = shufflevector <8 x i16> %761, <8 x i16> %746, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %871 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %867, <8 x i16> %320) #8
  %872 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %868, <8 x i16> <i16 3513, i16 2106, i16 3513, i16 2106, i16 3513, i16 2106, i16 3513, i16 2106>) #8
  %873 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %869, <8 x i16> %320) #8
  %874 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %870, <8 x i16> <i16 3513, i16 2106, i16 3513, i16 2106, i16 3513, i16 2106, i16 3513, i16 2106>) #8
  %875 = add <4 x i32> %871, <i32 2048, i32 2048, i32 2048, i32 2048>
  %876 = ashr <4 x i32> %875, <i32 12, i32 12, i32 12, i32 12>
  %877 = add <4 x i32> %872, <i32 2048, i32 2048, i32 2048, i32 2048>
  %878 = ashr <4 x i32> %877, <i32 12, i32 12, i32 12, i32 12>
  %879 = add <4 x i32> %873, <i32 2048, i32 2048, i32 2048, i32 2048>
  %880 = ashr <4 x i32> %879, <i32 12, i32 12, i32 12, i32 12>
  %881 = add <4 x i32> %874, <i32 2048, i32 2048, i32 2048, i32 2048>
  %882 = ashr <4 x i32> %881, <i32 12, i32 12, i32 12, i32 12>
  %883 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %876, <4 x i32> %880) #8
  %884 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %878, <4 x i32> %882) #8
  %885 = shufflevector <8 x i16> %740, <8 x i16> %767, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %886 = shufflevector <8 x i16> %767, <8 x i16> %740, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %887 = shufflevector <8 x i16> %740, <8 x i16> %767, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %888 = shufflevector <8 x i16> %767, <8 x i16> %740, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %889 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %885, <8 x i16> %321) #8
  %890 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %886, <8 x i16> <i16 3857, i16 1380, i16 3857, i16 1380, i16 3857, i16 1380, i16 3857, i16 1380>) #8
  %891 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %887, <8 x i16> %321) #8
  %892 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %888, <8 x i16> <i16 3857, i16 1380, i16 3857, i16 1380, i16 3857, i16 1380, i16 3857, i16 1380>) #8
  %893 = add <4 x i32> %889, <i32 2048, i32 2048, i32 2048, i32 2048>
  %894 = ashr <4 x i32> %893, <i32 12, i32 12, i32 12, i32 12>
  %895 = add <4 x i32> %890, <i32 2048, i32 2048, i32 2048, i32 2048>
  %896 = ashr <4 x i32> %895, <i32 12, i32 12, i32 12, i32 12>
  %897 = add <4 x i32> %891, <i32 2048, i32 2048, i32 2048, i32 2048>
  %898 = ashr <4 x i32> %897, <i32 12, i32 12, i32 12, i32 12>
  %899 = add <4 x i32> %892, <i32 2048, i32 2048, i32 2048, i32 2048>
  %900 = ashr <4 x i32> %899, <i32 12, i32 12, i32 12, i32 12>
  %901 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %894, <4 x i32> %898) #8
  %902 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %896, <4 x i32> %900) #8
  %903 = shufflevector <8 x i16> %734, <8 x i16> %773, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %904 = shufflevector <8 x i16> %773, <8 x i16> %734, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %905 = shufflevector <8 x i16> %734, <8 x i16> %773, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %906 = shufflevector <8 x i16> %773, <8 x i16> %734, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %907 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %903, <8 x i16> %322) #8
  %908 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %904, <8 x i16> <i16 4052, i16 601, i16 4052, i16 601, i16 4052, i16 601, i16 4052, i16 601>) #8
  %909 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %905, <8 x i16> %322) #8
  %910 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %906, <8 x i16> <i16 4052, i16 601, i16 4052, i16 601, i16 4052, i16 601, i16 4052, i16 601>) #8
  %911 = add <4 x i32> %907, <i32 2048, i32 2048, i32 2048, i32 2048>
  %912 = ashr <4 x i32> %911, <i32 12, i32 12, i32 12, i32 12>
  %913 = add <4 x i32> %908, <i32 2048, i32 2048, i32 2048, i32 2048>
  %914 = ashr <4 x i32> %913, <i32 12, i32 12, i32 12, i32 12>
  %915 = add <4 x i32> %909, <i32 2048, i32 2048, i32 2048, i32 2048>
  %916 = ashr <4 x i32> %915, <i32 12, i32 12, i32 12, i32 12>
  %917 = add <4 x i32> %910, <i32 2048, i32 2048, i32 2048, i32 2048>
  %918 = ashr <4 x i32> %917, <i32 12, i32 12, i32 12, i32 12>
  %919 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %912, <4 x i32> %916) #8
  %920 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %914, <4 x i32> %918) #8
  %921 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %794, <8 x i16> %866) #8
  %922 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %794, <8 x i16> %866) #8
  %923 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %793, <8 x i16> %865) #8
  %924 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %793, <8 x i16> %865) #8
  %925 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %812, <8 x i16> %884) #8
  %926 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %812, <8 x i16> %884) #8
  %927 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %811, <8 x i16> %883) #8
  %928 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %811, <8 x i16> %883) #8
  %929 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %830, <8 x i16> %902) #8
  %930 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %830, <8 x i16> %902) #8
  %931 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %829, <8 x i16> %901) #8
  %932 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %829, <8 x i16> %901) #8
  %933 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %848, <8 x i16> %920) #8
  %934 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %848, <8 x i16> %920) #8
  %935 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %847, <8 x i16> %919) #8
  %936 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %847, <8 x i16> %919) #8
  %937 = shufflevector <8 x i16> %922, <8 x i16> %924, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %938 = shufflevector <8 x i16> %924, <8 x i16> %922, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %939 = shufflevector <8 x i16> %922, <8 x i16> %924, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %940 = shufflevector <8 x i16> %924, <8 x i16> %922, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %941 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %937, <8 x i16> %323) #8
  %942 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %938, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %943 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %939, <8 x i16> %323) #8
  %944 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %940, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %945 = add <4 x i32> %941, <i32 2048, i32 2048, i32 2048, i32 2048>
  %946 = ashr <4 x i32> %945, <i32 12, i32 12, i32 12, i32 12>
  %947 = add <4 x i32> %942, <i32 2048, i32 2048, i32 2048, i32 2048>
  %948 = ashr <4 x i32> %947, <i32 12, i32 12, i32 12, i32 12>
  %949 = add <4 x i32> %943, <i32 2048, i32 2048, i32 2048, i32 2048>
  %950 = ashr <4 x i32> %949, <i32 12, i32 12, i32 12, i32 12>
  %951 = add <4 x i32> %944, <i32 2048, i32 2048, i32 2048, i32 2048>
  %952 = ashr <4 x i32> %951, <i32 12, i32 12, i32 12, i32 12>
  %953 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %946, <4 x i32> %950) #8
  %954 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %948, <4 x i32> %952) #8
  %955 = shufflevector <8 x i16> %932, <8 x i16> %930, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %956 = shufflevector <8 x i16> %930, <8 x i16> %932, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %957 = shufflevector <8 x i16> %932, <8 x i16> %930, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %958 = shufflevector <8 x i16> %930, <8 x i16> %932, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %959 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %955, <8 x i16> %324) #8
  %960 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %956, <8 x i16> <i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799>) #8
  %961 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %957, <8 x i16> %324) #8
  %962 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %958, <8 x i16> <i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799>) #8
  %963 = add <4 x i32> %959, <i32 2048, i32 2048, i32 2048, i32 2048>
  %964 = ashr <4 x i32> %963, <i32 12, i32 12, i32 12, i32 12>
  %965 = add <4 x i32> %960, <i32 2048, i32 2048, i32 2048, i32 2048>
  %966 = ashr <4 x i32> %965, <i32 12, i32 12, i32 12, i32 12>
  %967 = add <4 x i32> %961, <i32 2048, i32 2048, i32 2048, i32 2048>
  %968 = ashr <4 x i32> %967, <i32 12, i32 12, i32 12, i32 12>
  %969 = add <4 x i32> %962, <i32 2048, i32 2048, i32 2048, i32 2048>
  %970 = ashr <4 x i32> %969, <i32 12, i32 12, i32 12, i32 12>
  %971 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %964, <4 x i32> %968) #8
  %972 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %966, <4 x i32> %970) #8
  %973 = shufflevector <8 x i16> %926, <8 x i16> %928, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %974 = shufflevector <8 x i16> %928, <8 x i16> %926, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %975 = shufflevector <8 x i16> %926, <8 x i16> %928, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %976 = shufflevector <8 x i16> %928, <8 x i16> %926, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %977 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %973, <8 x i16> %325) #8
  %978 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %974, <8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>) #8
  %979 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %975, <8 x i16> %325) #8
  %980 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %976, <8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>) #8
  %981 = add <4 x i32> %977, <i32 2048, i32 2048, i32 2048, i32 2048>
  %982 = ashr <4 x i32> %981, <i32 12, i32 12, i32 12, i32 12>
  %983 = add <4 x i32> %978, <i32 2048, i32 2048, i32 2048, i32 2048>
  %984 = ashr <4 x i32> %983, <i32 12, i32 12, i32 12, i32 12>
  %985 = add <4 x i32> %979, <i32 2048, i32 2048, i32 2048, i32 2048>
  %986 = ashr <4 x i32> %985, <i32 12, i32 12, i32 12, i32 12>
  %987 = add <4 x i32> %980, <i32 2048, i32 2048, i32 2048, i32 2048>
  %988 = ashr <4 x i32> %987, <i32 12, i32 12, i32 12, i32 12>
  %989 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %982, <4 x i32> %986) #8
  %990 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %984, <4 x i32> %988) #8
  %991 = shufflevector <8 x i16> %936, <8 x i16> %934, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %992 = shufflevector <8 x i16> %934, <8 x i16> %936, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %993 = shufflevector <8 x i16> %936, <8 x i16> %934, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %994 = shufflevector <8 x i16> %934, <8 x i16> %936, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %995 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %991, <8 x i16> %326) #8
  %996 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %992, <8 x i16> <i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406>) #8
  %997 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %993, <8 x i16> %326) #8
  %998 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %994, <8 x i16> <i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406>) #8
  %999 = add <4 x i32> %995, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1000 = ashr <4 x i32> %999, <i32 12, i32 12, i32 12, i32 12>
  %1001 = add <4 x i32> %996, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1002 = ashr <4 x i32> %1001, <i32 12, i32 12, i32 12, i32 12>
  %1003 = add <4 x i32> %997, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1004 = ashr <4 x i32> %1003, <i32 12, i32 12, i32 12, i32 12>
  %1005 = add <4 x i32> %998, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1006 = ashr <4 x i32> %1005, <i32 12, i32 12, i32 12, i32 12>
  %1007 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1000, <4 x i32> %1004) #8
  %1008 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1002, <4 x i32> %1006) #8
  %1009 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %921, <8 x i16> %929) #8
  %1010 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %921, <8 x i16> %929) #8
  %1011 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %954, <8 x i16> %971) #8
  %1012 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %954, <8 x i16> %971) #8
  %1013 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %923, <8 x i16> %931) #8
  %1014 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %923, <8 x i16> %931) #8
  %1015 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %953, <8 x i16> %972) #8
  %1016 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %953, <8 x i16> %972) #8
  %1017 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %925, <8 x i16> %933) #8
  %1018 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %925, <8 x i16> %933) #8
  %1019 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %990, <8 x i16> %1007) #8
  %1020 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %990, <8 x i16> %1007) #8
  %1021 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %927, <8 x i16> %935) #8
  %1022 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %927, <8 x i16> %935) #8
  %1023 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %989, <8 x i16> %1008) #8
  %1024 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %989, <8 x i16> %1008) #8
  %1025 = shufflevector <8 x i16> %1010, <8 x i16> %1014, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1026 = shufflevector <8 x i16> %1014, <8 x i16> %1010, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1027 = shufflevector <8 x i16> %1010, <8 x i16> %1014, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1028 = shufflevector <8 x i16> %1014, <8 x i16> %1010, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1029 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1025, <8 x i16> %327) #8
  %1030 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1026, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %1031 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1027, <8 x i16> %327) #8
  %1032 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1028, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %1033 = add <4 x i32> %1029, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1034 = ashr <4 x i32> %1033, <i32 12, i32 12, i32 12, i32 12>
  %1035 = add <4 x i32> %1030, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1036 = ashr <4 x i32> %1035, <i32 12, i32 12, i32 12, i32 12>
  %1037 = add <4 x i32> %1031, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1038 = ashr <4 x i32> %1037, <i32 12, i32 12, i32 12, i32 12>
  %1039 = add <4 x i32> %1032, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1040 = ashr <4 x i32> %1039, <i32 12, i32 12, i32 12, i32 12>
  %1041 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1034, <4 x i32> %1038) #8
  %1042 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1036, <4 x i32> %1040) #8
  %1043 = shufflevector <8 x i16> %1012, <8 x i16> %1016, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1044 = shufflevector <8 x i16> %1016, <8 x i16> %1012, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1045 = shufflevector <8 x i16> %1012, <8 x i16> %1016, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1046 = shufflevector <8 x i16> %1016, <8 x i16> %1012, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1047 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1043, <8 x i16> %327) #8
  %1048 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1044, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %1049 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1045, <8 x i16> %327) #8
  %1050 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1046, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %1051 = add <4 x i32> %1047, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1052 = ashr <4 x i32> %1051, <i32 12, i32 12, i32 12, i32 12>
  %1053 = add <4 x i32> %1048, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1054 = ashr <4 x i32> %1053, <i32 12, i32 12, i32 12, i32 12>
  %1055 = add <4 x i32> %1049, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1056 = ashr <4 x i32> %1055, <i32 12, i32 12, i32 12, i32 12>
  %1057 = add <4 x i32> %1050, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1058 = ashr <4 x i32> %1057, <i32 12, i32 12, i32 12, i32 12>
  %1059 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1052, <4 x i32> %1056) #8
  %1060 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1054, <4 x i32> %1058) #8
  %1061 = shufflevector <8 x i16> %1022, <8 x i16> %1018, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1062 = shufflevector <8 x i16> %1018, <8 x i16> %1022, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1063 = shufflevector <8 x i16> %1022, <8 x i16> %1018, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1064 = shufflevector <8 x i16> %1018, <8 x i16> %1022, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1065 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1061, <8 x i16> %328) #8
  %1066 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1062, <8 x i16> <i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567>) #8
  %1067 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1063, <8 x i16> %328) #8
  %1068 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1064, <8 x i16> <i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567>) #8
  %1069 = add <4 x i32> %1065, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1070 = ashr <4 x i32> %1069, <i32 12, i32 12, i32 12, i32 12>
  %1071 = add <4 x i32> %1066, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1072 = ashr <4 x i32> %1071, <i32 12, i32 12, i32 12, i32 12>
  %1073 = add <4 x i32> %1067, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1074 = ashr <4 x i32> %1073, <i32 12, i32 12, i32 12, i32 12>
  %1075 = add <4 x i32> %1068, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1076 = ashr <4 x i32> %1075, <i32 12, i32 12, i32 12, i32 12>
  %1077 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1070, <4 x i32> %1074) #8
  %1078 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1072, <4 x i32> %1076) #8
  %1079 = shufflevector <8 x i16> %1024, <8 x i16> %1020, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1080 = shufflevector <8 x i16> %1020, <8 x i16> %1024, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1081 = shufflevector <8 x i16> %1024, <8 x i16> %1020, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1082 = shufflevector <8 x i16> %1020, <8 x i16> %1024, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1083 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1079, <8 x i16> %328) #8
  %1084 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1080, <8 x i16> <i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567>) #8
  %1085 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1081, <8 x i16> %328) #8
  %1086 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1082, <8 x i16> <i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567>) #8
  %1087 = add <4 x i32> %1083, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1088 = ashr <4 x i32> %1087, <i32 12, i32 12, i32 12, i32 12>
  %1089 = add <4 x i32> %1084, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1090 = ashr <4 x i32> %1089, <i32 12, i32 12, i32 12, i32 12>
  %1091 = add <4 x i32> %1085, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1092 = ashr <4 x i32> %1091, <i32 12, i32 12, i32 12, i32 12>
  %1093 = add <4 x i32> %1086, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1094 = ashr <4 x i32> %1093, <i32 12, i32 12, i32 12, i32 12>
  %1095 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1088, <4 x i32> %1092) #8
  %1096 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1090, <4 x i32> %1094) #8
  %1097 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1009, <8 x i16> %1017) #8
  %1098 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1009, <8 x i16> %1017) #8
  %1099 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1042, <8 x i16> %1077) #8
  %1100 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1042, <8 x i16> %1077) #8
  %1101 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1011, <8 x i16> %1019) #8
  %1102 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1011, <8 x i16> %1019) #8
  %1103 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1060, <8 x i16> %1095) #8
  %1104 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1060, <8 x i16> %1095) #8
  %1105 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1013, <8 x i16> %1021) #8
  %1106 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1013, <8 x i16> %1021) #8
  %1107 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1041, <8 x i16> %1078) #8
  %1108 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1041, <8 x i16> %1078) #8
  %1109 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1015, <8 x i16> %1023) #8
  %1110 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1015, <8 x i16> %1023) #8
  %1111 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1059, <8 x i16> %1096) #8
  %1112 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1059, <8 x i16> %1096) #8
  %1113 = shufflevector <8 x i16> %1098, <8 x i16> %1106, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1114 = shufflevector <8 x i16> %1106, <8 x i16> %1098, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1115 = shufflevector <8 x i16> %1098, <8 x i16> %1106, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1116 = shufflevector <8 x i16> %1106, <8 x i16> %1098, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1117 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1113, <8 x i16> %329) #8
  %1118 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1114, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1119 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1115, <8 x i16> %329) #8
  %1120 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1116, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1121 = add <4 x i32> %1117, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1122 = ashr <4 x i32> %1121, <i32 12, i32 12, i32 12, i32 12>
  %1123 = add <4 x i32> %1118, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1124 = ashr <4 x i32> %1123, <i32 12, i32 12, i32 12, i32 12>
  %1125 = add <4 x i32> %1119, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1126 = ashr <4 x i32> %1125, <i32 12, i32 12, i32 12, i32 12>
  %1127 = add <4 x i32> %1120, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1128 = ashr <4 x i32> %1127, <i32 12, i32 12, i32 12, i32 12>
  %1129 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1122, <4 x i32> %1126) #8
  %1130 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1124, <4 x i32> %1128) #8
  %1131 = shufflevector <8 x i16> %1100, <8 x i16> %1108, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1132 = shufflevector <8 x i16> %1108, <8 x i16> %1100, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1133 = shufflevector <8 x i16> %1100, <8 x i16> %1108, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1134 = shufflevector <8 x i16> %1108, <8 x i16> %1100, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1135 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1131, <8 x i16> %329) #8
  %1136 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1132, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1137 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1133, <8 x i16> %329) #8
  %1138 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1134, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1139 = add <4 x i32> %1135, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1140 = ashr <4 x i32> %1139, <i32 12, i32 12, i32 12, i32 12>
  %1141 = add <4 x i32> %1136, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1142 = ashr <4 x i32> %1141, <i32 12, i32 12, i32 12, i32 12>
  %1143 = add <4 x i32> %1137, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1144 = ashr <4 x i32> %1143, <i32 12, i32 12, i32 12, i32 12>
  %1145 = add <4 x i32> %1138, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1146 = ashr <4 x i32> %1145, <i32 12, i32 12, i32 12, i32 12>
  %1147 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1140, <4 x i32> %1144) #8
  %1148 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1142, <4 x i32> %1146) #8
  %1149 = shufflevector <8 x i16> %1102, <8 x i16> %1110, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1150 = shufflevector <8 x i16> %1110, <8 x i16> %1102, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1151 = shufflevector <8 x i16> %1102, <8 x i16> %1110, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1152 = shufflevector <8 x i16> %1110, <8 x i16> %1102, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1153 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1149, <8 x i16> %329) #8
  %1154 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1150, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1155 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1151, <8 x i16> %329) #8
  %1156 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1152, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1157 = add <4 x i32> %1153, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1158 = ashr <4 x i32> %1157, <i32 12, i32 12, i32 12, i32 12>
  %1159 = add <4 x i32> %1154, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1160 = ashr <4 x i32> %1159, <i32 12, i32 12, i32 12, i32 12>
  %1161 = add <4 x i32> %1155, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1162 = ashr <4 x i32> %1161, <i32 12, i32 12, i32 12, i32 12>
  %1163 = add <4 x i32> %1156, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1164 = ashr <4 x i32> %1163, <i32 12, i32 12, i32 12, i32 12>
  %1165 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1158, <4 x i32> %1162) #8
  %1166 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1160, <4 x i32> %1164) #8
  %1167 = shufflevector <8 x i16> %1104, <8 x i16> %1112, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1168 = shufflevector <8 x i16> %1112, <8 x i16> %1104, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1169 = shufflevector <8 x i16> %1104, <8 x i16> %1112, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1170 = shufflevector <8 x i16> %1112, <8 x i16> %1104, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1171 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1167, <8 x i16> %329) #8
  %1172 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1168, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1173 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1169, <8 x i16> %329) #8
  %1174 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1170, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1175 = add <4 x i32> %1171, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1176 = ashr <4 x i32> %1175, <i32 12, i32 12, i32 12, i32 12>
  %1177 = add <4 x i32> %1172, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1178 = ashr <4 x i32> %1177, <i32 12, i32 12, i32 12, i32 12>
  %1179 = add <4 x i32> %1173, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1180 = ashr <4 x i32> %1179, <i32 12, i32 12, i32 12, i32 12>
  %1181 = add <4 x i32> %1174, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1182 = ashr <4 x i32> %1181, <i32 12, i32 12, i32 12, i32 12>
  %1183 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1176, <4 x i32> %1180) #8
  %1184 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1178, <4 x i32> %1182) #8
  %1185 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %1101) #8
  %1186 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %1099) #8
  %1187 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %1184) #8
  %1188 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %1130) #8
  %1189 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %1165) #8
  %1190 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %1147) #8
  %1191 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %1111) #8
  %1192 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> zeroinitializer, <8 x i16> %1105) #8
  store <8 x i16> %1097, <8 x i16>* %730, align 1
  store <8 x i16> %1185, <8 x i16>* %733, align 1
  store <8 x i16> %1103, <8 x i16>* %736, align 1
  store <8 x i16> %1186, <8 x i16>* %739, align 1
  store <8 x i16> %1148, <8 x i16>* %742, align 1
  store <8 x i16> %1187, <8 x i16>* %745, align 1
  store <8 x i16> %1166, <8 x i16>* %748, align 1
  store <8 x i16> %1188, <8 x i16>* %751, align 1
  store <8 x i16> %1129, <8 x i16>* %754, align 1
  store <8 x i16> %1189, <8 x i16>* %757, align 1
  store <8 x i16> %1183, <8 x i16>* %760, align 1
  store <8 x i16> %1190, <8 x i16>* %763, align 1
  store <8 x i16> %1107, <8 x i16>* %766, align 1
  store <8 x i16> %1191, <8 x i16>* %769, align 1
  store <8 x i16> %1109, <8 x i16>* %772, align 1
  store <8 x i16> %1192, <8 x i16>* %775, align 1
  %1193 = add nuw nsw i64 %728, 8
  %1194 = icmp ult i64 %1193, %330
  br i1 %1194, label %727, label %1195

1195:                                             ; preds = %152, %727
  %1196 = bitcast i8* %6 to i64*
  %1197 = load i64, i64* %1196, align 8
  %1198 = getelementptr inbounds i8, i8* %6, i64 8
  %1199 = bitcast i8* %1198 to i8**
  %1200 = load i8*, i8** %1199, align 8
  %1201 = and i32 %14, 16608
  %1202 = icmp ne i32 %1201, 0
  %1203 = sext i32 %5 to i64
  %1204 = ashr i64 %1197, 32
  %1205 = mul nsw i64 %1204, %1203
  %1206 = getelementptr inbounds i8, i8* %1200, i64 %1205
  %1207 = sext i32 %4 to i64
  %1208 = getelementptr inbounds i8, i8* %1206, i64 %1207
  switch i8 %11, label %1213 [
    i8 4, label %1209
    i8 8, label %1242
  ]

1209:                                             ; preds = %331, %1195
  %1210 = phi i8* [ %726, %331 ], [ %1208, %1195 ]
  %1211 = phi i64 [ %722, %331 ], [ %1204, %1195 ]
  %1212 = phi i1 [ %720, %331 ], [ %1202, %1195 ]
  br label %1215

1213:                                             ; preds = %1195
  %1214 = zext i8 %11 to i64
  br label %1267

1215:                                             ; preds = %1215, %1209
  %1216 = phi i8* [ %1239, %1215 ], [ %1210, %1209 ]
  %1217 = phi i32 [ %1240, %1215 ], [ 0, %1209 ]
  %1218 = sub nuw nsw i32 15, %1217
  %1219 = select i1 %1212, i32 %1218, i32 %1217
  %1220 = shl nsw i32 %1219, 2
  %1221 = sext i32 %1220 to i64
  %1222 = getelementptr inbounds i16, i16* %8, i64 %1221
  %1223 = bitcast i16* %1222 to i64*
  %1224 = load i64, i64* %1223, align 1
  %1225 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %1224, i32 0
  %1226 = bitcast i8* %1216 to i32*
  %1227 = load i32, i32* %1226, align 1
  %1228 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %1227, i32 0
  %1229 = bitcast <2 x i64> %1225 to <8 x i16>
  %1230 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1229, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %1231 = ashr <8 x i16> %1230, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %1232 = bitcast <4 x i32> %1228 to <16 x i8>
  %1233 = shufflevector <16 x i8> %1232, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %1234 = zext <8 x i8> %1233 to <8 x i16>
  %1235 = add nsw <8 x i16> %1231, %1234
  %1236 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %1235, <8 x i16> undef) #8
  %1237 = bitcast <16 x i8> %1236 to <4 x i32>
  %1238 = extractelement <4 x i32> %1237, i32 0
  store i32 %1238, i32* %1226, align 1
  %1239 = getelementptr inbounds i8, i8* %1216, i64 %1211
  %1240 = add nuw nsw i32 %1217, 1
  %1241 = icmp eq i32 %1240, 16
  br i1 %1241, label %1307, label %1215

1242:                                             ; preds = %1195, %1242
  %1243 = phi i8* [ %1264, %1242 ], [ %1208, %1195 ]
  %1244 = phi i32 [ %1265, %1242 ], [ 0, %1195 ]
  %1245 = sub nuw nsw i32 15, %1244
  %1246 = select i1 %1202, i32 %1245, i32 %1244
  %1247 = shl nsw i32 %1246, 3
  %1248 = sext i32 %1247 to i64
  %1249 = getelementptr inbounds i16, i16* %8, i64 %1248
  %1250 = bitcast i16* %1249 to <8 x i16>*
  %1251 = load <8 x i16>, <8 x i16>* %1250, align 1
  %1252 = bitcast i8* %1243 to i64*
  %1253 = load i64, i64* %1252, align 1
  %1254 = insertelement <2 x i64> undef, i64 %1253, i32 0
  %1255 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1251, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %1256 = ashr <8 x i16> %1255, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %1257 = bitcast <2 x i64> %1254 to <16 x i8>
  %1258 = shufflevector <16 x i8> %1257, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %1259 = zext <8 x i8> %1258 to <8 x i16>
  %1260 = add nsw <8 x i16> %1256, %1259
  %1261 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %1260, <8 x i16> undef) #8
  %1262 = bitcast <16 x i8> %1261 to <2 x i64>
  %1263 = extractelement <2 x i64> %1262, i32 0
  store i64 %1263, i64* %1252, align 1
  %1264 = getelementptr inbounds i8, i8* %1243, i64 %1204
  %1265 = add nuw nsw i32 %1244, 1
  %1266 = icmp eq i32 %1265, 16
  br i1 %1266, label %1307, label %1242

1267:                                             ; preds = %1304, %1213
  %1268 = phi i64 [ 0, %1213 ], [ %1305, %1304 ]
  %1269 = add nsw i64 %1268, %1203
  %1270 = trunc i64 %1268 to i32
  %1271 = sub i32 15, %1270
  %1272 = select i1 %1202, i32 %1271, i32 %1270
  %1273 = mul nsw i32 %1272, %12
  %1274 = mul nsw i64 %1269, %1204
  %1275 = getelementptr inbounds i8, i8* %1200, i64 %1274
  %1276 = sext i32 %1273 to i64
  br label %1277

1277:                                             ; preds = %1277, %1267
  %1278 = phi i64 [ %1302, %1277 ], [ 0, %1267 ]
  %1279 = add nsw i64 %1278, %1207
  %1280 = add nsw i64 %1278, %1276
  %1281 = getelementptr inbounds i16, i16* %8, i64 %1280
  %1282 = bitcast i16* %1281 to <8 x i16>*
  %1283 = load <8 x i16>, <8 x i16>* %1282, align 1
  %1284 = add nsw i64 %1280, 8
  %1285 = getelementptr inbounds i16, i16* %8, i64 %1284
  %1286 = bitcast i16* %1285 to <8 x i16>*
  %1287 = load <8 x i16>, <8 x i16>* %1286, align 1
  %1288 = getelementptr inbounds i8, i8* %1275, i64 %1279
  %1289 = bitcast i8* %1288 to <16 x i8>*
  %1290 = load <16 x i8>, <16 x i8>* %1289, align 1
  %1291 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1283, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %1292 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1287, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %1293 = ashr <8 x i16> %1291, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %1294 = ashr <8 x i16> %1292, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %1295 = shufflevector <16 x i8> %1290, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %1296 = zext <8 x i8> %1295 to <8 x i16>
  %1297 = shufflevector <16 x i8> %1290, <16 x i8> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %1298 = zext <8 x i8> %1297 to <8 x i16>
  %1299 = add nsw <8 x i16> %1293, %1296
  %1300 = add nsw <8 x i16> %1294, %1298
  %1301 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %1299, <8 x i16> %1300) #8
  store <16 x i8> %1301, <16 x i8>* %1289, align 1
  %1302 = add nuw nsw i64 %1278, 16
  %1303 = icmp ult i64 %1302, %1214
  br i1 %1303, label %1277, label %1304

1304:                                             ; preds = %1277
  %1305 = add nuw nsw i64 %1268, 1
  %1306 = icmp eq i64 %1305, 16
  br i1 %1306, label %1307, label %1267

1307:                                             ; preds = %1242, %1304, %1215
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define internal void @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_132Identity4TransformLoopRow_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_(i8 zeroext, i8 zeroext, i32, i8* nocapture, i32, i32, i8* nocapture readnone) #3 {
  %8 = icmp eq i8 %0, 9
  %9 = icmp eq i8 %1, 0
  %10 = and i1 %8, %9
  br i1 %10, label %101, label %11

11:                                               ; preds = %7
  %12 = bitcast i8* %3 to i16*
  %13 = zext i8 %1 to i64
  %14 = getelementptr inbounds [19 x i8], [19 x i8]* @_ZN7libgav116kTransformHeightE, i64 0, i64 %13
  %15 = load i8, i8* %14, align 1
  %16 = icmp eq i8 %15, 8
  %17 = icmp sgt i32 %2, 1
  br i1 %17, label %44, label %18

18:                                               ; preds = %11
  %19 = load i16, i16* %12, align 2
  %20 = sext i16 %19 to i32
  %21 = insertelement <4 x i32> <i32 undef, i32 0, i32 0, i32 0>, i32 %20, i32 0
  %22 = sext i1 %16 to i16
  %23 = insertelement <8 x i16> undef, i16 %22, i32 0
  %24 = shufflevector <8 x i16> %23, <8 x i16> undef, <8 x i32> zeroinitializer
  %25 = bitcast <4 x i32> %21 to <8 x i16>
  %26 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %25, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  %27 = bitcast <4 x i32> %21 to <16 x i8>
  %28 = bitcast <8 x i16> %26 to <16 x i8>
  %29 = bitcast <8 x i16> %24 to <16 x i8>
  %30 = tail call <16 x i8> @llvm.x86.sse41.pblendvb(<16 x i8> %27, <16 x i8> %28, <16 x i8> %29) #8
  %31 = icmp ugt i8 %15, 15
  %32 = zext i1 %31 to i16
  %33 = shl nuw nsw i16 %32, 12
  %34 = or i16 %33, 2048
  %35 = insertelement <8 x i16> undef, i16 %34, i32 0
  %36 = shufflevector <8 x i16> %35, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %37 = bitcast <16 x i8> %30 to <8 x i16>
  %38 = shufflevector <8 x i16> %36, <8 x i16> %37, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %39 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %38, <8 x i16> <i16 1, i16 5793, i16 1, i16 5793, i16 1, i16 5793, i16 1, i16 5793>) #8
  %40 = select i1 %31, i32 13, i32 12
  %41 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %39, i32 %40) #8
  %42 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %41, <4 x i32> undef) #8
  %43 = extractelement <8 x i16> %42, i64 0
  store i16 %43, i16* %12, align 2
  br label %101

44:                                               ; preds = %11
  br i1 %16, label %45, label %56

45:                                               ; preds = %44
  %46 = shl nsw i32 %2, 2
  %47 = sext i32 %46 to i64
  br label %48

48:                                               ; preds = %48, %45
  %49 = phi i64 [ %54, %48 ], [ 0, %45 ]
  %50 = getelementptr inbounds i16, i16* %12, i64 %49
  %51 = bitcast i16* %50 to <8 x i16>*
  %52 = load <8 x i16>, <8 x i16>* %51, align 1
  %53 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %52, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %53, <8 x i16>* %51, align 1
  %54 = add nuw nsw i64 %49, 8
  %55 = icmp slt i64 %54, %47
  br i1 %55, label %48, label %56

56:                                               ; preds = %48, %44
  %57 = icmp ult i8 %15, 16
  %58 = sext i32 %2 to i64
  br i1 %57, label %59, label %75

59:                                               ; preds = %56, %59
  %60 = phi i64 [ %73, %59 ], [ 0, %56 ]
  %61 = shl i64 %60, 2
  %62 = and i64 %61, 4294967280
  %63 = getelementptr inbounds i16, i16* %12, i64 %62
  %64 = bitcast i16* %63 to <8 x i16>*
  %65 = load <8 x i16>, <8 x i16>* %64, align 1
  %66 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %65, <8 x i16> <i16 13576, i16 13576, i16 13576, i16 13576, i16 13576, i16 13576, i16 13576, i16 13576>) #8
  %67 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %66, <8 x i16> %65) #8
  store <8 x i16> %67, <8 x i16>* %64, align 1
  %68 = getelementptr inbounds i16, i16* %63, i64 8
  %69 = bitcast i16* %68 to <8 x i16>*
  %70 = load <8 x i16>, <8 x i16>* %69, align 1
  %71 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %70, <8 x i16> <i16 13576, i16 13576, i16 13576, i16 13576, i16 13576, i16 13576, i16 13576, i16 13576>) #8
  %72 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %71, <8 x i16> %70) #8
  store <8 x i16> %72, <8 x i16>* %69, align 1
  %73 = add nuw nsw i64 %60, 4
  %74 = icmp slt i64 %73, %58
  br i1 %74, label %59, label %101

75:                                               ; preds = %56, %75
  %76 = phi i64 [ %99, %75 ], [ 0, %56 ]
  %77 = shl i64 %76, 2
  %78 = and i64 %77, 4294967280
  %79 = getelementptr inbounds i16, i16* %12, i64 %78
  %80 = bitcast i16* %79 to <8 x i16>*
  %81 = load <8 x i16>, <8 x i16>* %80, align 1
  %82 = shufflevector <8 x i16> <i16 6144, i16 6144, i16 6144, i16 6144, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i16> %81, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %83 = shufflevector <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 6144, i16 6144, i16 6144, i16 6144>, <8 x i16> %81, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %84 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %82, <8 x i16> <i16 1, i16 5793, i16 1, i16 5793, i16 1, i16 5793, i16 1, i16 5793>) #8
  %85 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %83, <8 x i16> <i16 1, i16 5793, i16 1, i16 5793, i16 1, i16 5793, i16 1, i16 5793>) #8
  %86 = ashr <4 x i32> %84, <i32 13, i32 13, i32 13, i32 13>
  %87 = ashr <4 x i32> %85, <i32 13, i32 13, i32 13, i32 13>
  %88 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %86, <4 x i32> %87) #8
  store <8 x i16> %88, <8 x i16>* %80, align 1
  %89 = getelementptr inbounds i16, i16* %79, i64 8
  %90 = bitcast i16* %89 to <8 x i16>*
  %91 = load <8 x i16>, <8 x i16>* %90, align 1
  %92 = shufflevector <8 x i16> <i16 6144, i16 6144, i16 6144, i16 6144, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i16> %91, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %93 = shufflevector <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 6144, i16 6144, i16 6144, i16 6144>, <8 x i16> %91, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %94 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %92, <8 x i16> <i16 1, i16 5793, i16 1, i16 5793, i16 1, i16 5793, i16 1, i16 5793>) #8
  %95 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %93, <8 x i16> <i16 1, i16 5793, i16 1, i16 5793, i16 1, i16 5793, i16 1, i16 5793>) #8
  %96 = ashr <4 x i32> %94, <i32 13, i32 13, i32 13, i32 13>
  %97 = ashr <4 x i32> %95, <i32 13, i32 13, i32 13, i32 13>
  %98 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %96, <4 x i32> %97) #8
  store <8 x i16> %98, <8 x i16>* %90, align 1
  %99 = add nuw nsw i64 %76, 4
  %100 = icmp slt i64 %99, %58
  br i1 %100, label %75, label %101

101:                                              ; preds = %75, %59, %18, %7
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define internal void @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_135Identity4TransformLoopColumn_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_(i8 zeroext, i8 zeroext, i32, i8* nocapture, i32, i32, i8* nocapture readonly) #3 {
  %8 = bitcast i8* %3 to i16*
  %9 = zext i8 %1 to i64
  %10 = getelementptr inbounds [19 x i8], [19 x i8]* @_ZN7libgav115kTransformWidthE, i64 0, i64 %9
  %11 = load i8, i8* %10, align 1
  %12 = zext i8 %11 to i64
  %13 = icmp eq i8 %0, 9
  br i1 %13, label %14, label %91

14:                                               ; preds = %7
  switch i8 %1, label %91 [
    i8 0, label %15
    i8 3, label %15
  ]

15:                                               ; preds = %14, %14
  %16 = bitcast i8* %6 to i64*
  %17 = load i64, i64* %16, align 8
  %18 = getelementptr inbounds i8, i8* %6, i64 8
  %19 = bitcast i8* %18 to i8**
  %20 = load i8*, i8** %19, align 8
  %21 = sext i32 %5 to i64
  %22 = ashr i64 %17, 32
  %23 = mul nsw i64 %22, %21
  %24 = getelementptr inbounds i8, i8* %20, i64 %23
  %25 = sext i32 %4 to i64
  %26 = getelementptr inbounds i8, i8* %24, i64 %25
  %27 = icmp eq i8 %11, 4
  %28 = sext i32 %2 to i64
  br i1 %27, label %29, label %58

29:                                               ; preds = %15, %29
  %30 = phi i64 [ %56, %29 ], [ 0, %15 ]
  %31 = phi i8* [ %55, %29 ], [ %26, %15 ]
  %32 = shl i64 %30, 2
  %33 = and i64 %32, 4294967292
  %34 = getelementptr inbounds i16, i16* %8, i64 %33
  %35 = bitcast i16* %34 to i64*
  %36 = load i64, i64* %35, align 1
  %37 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %36, i32 0
  %38 = bitcast <2 x i64> %37 to <8 x i16>
  %39 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %38, <8 x i16> <i16 13576, i16 13576, i16 13576, i16 13576, i16 13576, i16 13576, i16 13576, i16 13576>) #8
  %40 = bitcast i8* %31 to i32*
  %41 = load i32, i32* %40, align 1
  %42 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %41, i32 0
  %43 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %39, <8 x i16> %38) #8
  %44 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %43, <8 x i16> <i16 13576, i16 13576, i16 13576, i16 13576, i16 13576, i16 13576, i16 13576, i16 13576>) #8
  %45 = bitcast <4 x i32> %42 to <16 x i8>
  %46 = shufflevector <16 x i8> %45, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %47 = zext <8 x i8> %46 to <8 x i16>
  %48 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %44, <8 x i16> %43) #8
  %49 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %48, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %50 = ashr <8 x i16> %49, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %51 = add nsw <8 x i16> %50, %47
  %52 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %51, <8 x i16> undef) #8
  %53 = bitcast <16 x i8> %52 to <4 x i32>
  %54 = extractelement <4 x i32> %53, i32 0
  store i32 %54, i32* %40, align 1
  %55 = getelementptr inbounds i8, i8* %31, i64 %22
  %56 = add nuw nsw i64 %30, 1
  %57 = icmp slt i64 %56, %28
  br i1 %57, label %29, label %209

58:                                               ; preds = %15, %87
  %59 = phi i64 [ %89, %87 ], [ 0, %15 ]
  %60 = phi i8* [ %88, %87 ], [ %26, %15 ]
  %61 = mul nuw nsw i64 %59, %12
  br label %62

62:                                               ; preds = %62, %58
  %63 = phi i64 [ %85, %62 ], [ 0, %58 ]
  %64 = add nuw nsw i64 %63, %61
  %65 = getelementptr inbounds i16, i16* %8, i64 %64
  %66 = bitcast i16* %65 to <8 x i16>*
  %67 = load <8 x i16>, <8 x i16>* %66, align 1
  %68 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %67, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  %69 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %68, <8 x i16> %68) #8
  %70 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %69, <8 x i16> <i16 13576, i16 13576, i16 13576, i16 13576, i16 13576, i16 13576, i16 13576, i16 13576>) #8
  %71 = getelementptr inbounds i8, i8* %60, i64 %63
  %72 = bitcast i8* %71 to i64*
  %73 = load i64, i64* %72, align 1
  %74 = insertelement <2 x i64> undef, i64 %73, i32 0
  %75 = bitcast <2 x i64> %74 to <16 x i8>
  %76 = shufflevector <16 x i8> %75, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %77 = zext <8 x i8> %76 to <8 x i16>
  %78 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %70, <8 x i16> %69) #8
  %79 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %78, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %80 = ashr <8 x i16> %79, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %81 = add nsw <8 x i16> %80, %77
  %82 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %81, <8 x i16> undef) #8
  %83 = bitcast <16 x i8> %82 to <2 x i64>
  %84 = extractelement <2 x i64> %83, i32 0
  store i64 %84, i64* %72, align 1
  %85 = add nuw nsw i64 %63, 8
  %86 = icmp ult i64 %85, %12
  br i1 %86, label %62, label %87

87:                                               ; preds = %62
  %88 = getelementptr inbounds i8, i8* %60, i64 %22
  %89 = add nuw nsw i64 %59, 1
  %90 = icmp slt i64 %89, %28
  br i1 %90, label %58, label %209

91:                                               ; preds = %14, %7
  %92 = zext i8 %0 to i32
  %93 = shl i32 1, %92
  %94 = and i32 %93, 33104
  %95 = icmp eq i32 %94, 0
  br i1 %95, label %137, label %96

96:                                               ; preds = %91
  %97 = icmp ugt i8 %11, 15
  br i1 %97, label %98, label %113

98:                                               ; preds = %96
  %99 = shl nuw nsw i64 %12, 2
  br label %100

100:                                              ; preds = %100, %98
  %101 = phi i64 [ 0, %98 ], [ %111, %100 ]
  %102 = getelementptr inbounds i16, i16* %8, i64 %101
  %103 = bitcast i16* %102 to <16 x i8>*
  %104 = load <16 x i8>, <16 x i8>* %103, align 1
  %105 = or i64 %101, 8
  %106 = getelementptr inbounds i16, i16* %8, i64 %105
  %107 = bitcast i16* %106 to <16 x i8>*
  %108 = load <16 x i8>, <16 x i8>* %107, align 1
  %109 = shufflevector <16 x i8> %104, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  %110 = shufflevector <16 x i8> %108, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %110, <16 x i8>* %103, align 1
  store <16 x i8> %109, <16 x i8>* %107, align 1
  %111 = add nuw nsw i64 %101, 16
  %112 = icmp ult i64 %111, %99
  br i1 %112, label %100, label %137

113:                                              ; preds = %96
  %114 = icmp eq i8 %11, 8
  %115 = bitcast i8* %3 to <16 x i8>*
  %116 = load <16 x i8>, <16 x i8>* %115, align 1
  br i1 %114, label %123, label %117

117:                                              ; preds = %113
  %118 = shufflevector <16 x i8> %116, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %118, <16 x i8>* %115, align 1
  %119 = getelementptr inbounds i8, i8* %3, i64 16
  %120 = bitcast i8* %119 to <16 x i8>*
  %121 = load <16 x i8>, <16 x i8>* %120, align 1
  %122 = shufflevector <16 x i8> %121, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %122, <16 x i8>* %120, align 1
  br label %137

123:                                              ; preds = %113
  %124 = shufflevector <16 x i8> %116, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %124, <16 x i8>* %115, align 1
  %125 = getelementptr inbounds i8, i8* %3, i64 16
  %126 = bitcast i8* %125 to <16 x i8>*
  %127 = load <16 x i8>, <16 x i8>* %126, align 1
  %128 = shufflevector <16 x i8> %127, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %128, <16 x i8>* %126, align 1
  %129 = getelementptr inbounds i8, i8* %3, i64 32
  %130 = bitcast i8* %129 to <16 x i8>*
  %131 = load <16 x i8>, <16 x i8>* %130, align 1
  %132 = shufflevector <16 x i8> %131, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %132, <16 x i8>* %130, align 1
  %133 = getelementptr inbounds i8, i8* %3, i64 48
  %134 = bitcast i8* %133 to <16 x i8>*
  %135 = load <16 x i8>, <16 x i8>* %134, align 1
  %136 = shufflevector <16 x i8> %135, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %136, <16 x i8>* %134, align 1
  br label %137

137:                                              ; preds = %100, %91, %123, %117
  %138 = bitcast i8* %6 to i64*
  %139 = load i64, i64* %138, align 8
  %140 = getelementptr inbounds i8, i8* %6, i64 8
  %141 = bitcast i8* %140 to i8**
  %142 = load i8*, i8** %141, align 8
  %143 = sext i32 %5 to i64
  %144 = ashr i64 %139, 32
  %145 = mul nsw i64 %144, %143
  %146 = getelementptr inbounds i8, i8* %142, i64 %145
  %147 = sext i32 %4 to i64
  %148 = getelementptr inbounds i8, i8* %146, i64 %147
  %149 = icmp eq i8 %11, 4
  %150 = sext i32 %2 to i64
  br i1 %149, label %151, label %178

151:                                              ; preds = %137, %151
  %152 = phi i64 [ %176, %151 ], [ 0, %137 ]
  %153 = phi i8* [ %175, %151 ], [ %148, %137 ]
  %154 = shl i64 %152, 2
  %155 = and i64 %154, 4294967292
  %156 = getelementptr inbounds i16, i16* %8, i64 %155
  %157 = bitcast i16* %156 to i64*
  %158 = load i64, i64* %157, align 1
  %159 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %158, i32 0
  %160 = bitcast <2 x i64> %159 to <8 x i16>
  %161 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %160, <8 x i16> <i16 13576, i16 13576, i16 13576, i16 13576, i16 13576, i16 13576, i16 13576, i16 13576>) #8
  %162 = bitcast i8* %153 to i32*
  %163 = load i32, i32* %162, align 1
  %164 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %163, i32 0
  %165 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %161, <8 x i16> %160) #8
  %166 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %165, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %167 = ashr <8 x i16> %166, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %168 = bitcast <4 x i32> %164 to <16 x i8>
  %169 = shufflevector <16 x i8> %168, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %170 = zext <8 x i8> %169 to <8 x i16>
  %171 = add nsw <8 x i16> %167, %170
  %172 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %171, <8 x i16> undef) #8
  %173 = bitcast <16 x i8> %172 to <4 x i32>
  %174 = extractelement <4 x i32> %173, i32 0
  store i32 %174, i32* %162, align 1
  %175 = getelementptr inbounds i8, i8* %153, i64 %144
  %176 = add nuw nsw i64 %152, 1
  %177 = icmp slt i64 %176, %150
  br i1 %177, label %151, label %209

178:                                              ; preds = %137, %205
  %179 = phi i64 [ %207, %205 ], [ 0, %137 ]
  %180 = phi i8* [ %206, %205 ], [ %148, %137 ]
  %181 = mul nuw nsw i64 %179, %12
  br label %182

182:                                              ; preds = %182, %178
  %183 = phi i64 [ %203, %182 ], [ 0, %178 ]
  %184 = add nuw nsw i64 %183, %181
  %185 = getelementptr inbounds i16, i16* %8, i64 %184
  %186 = bitcast i16* %185 to <8 x i16>*
  %187 = load <8 x i16>, <8 x i16>* %186, align 1
  %188 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %187, <8 x i16> <i16 13576, i16 13576, i16 13576, i16 13576, i16 13576, i16 13576, i16 13576, i16 13576>) #8
  %189 = getelementptr inbounds i8, i8* %180, i64 %183
  %190 = bitcast i8* %189 to i64*
  %191 = load i64, i64* %190, align 1
  %192 = insertelement <2 x i64> undef, i64 %191, i32 0
  %193 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %188, <8 x i16> %187) #8
  %194 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %193, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %195 = ashr <8 x i16> %194, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %196 = bitcast <2 x i64> %192 to <16 x i8>
  %197 = shufflevector <16 x i8> %196, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %198 = zext <8 x i8> %197 to <8 x i16>
  %199 = add nsw <8 x i16> %195, %198
  %200 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %199, <8 x i16> undef) #8
  %201 = bitcast <16 x i8> %200 to <2 x i64>
  %202 = extractelement <2 x i64> %201, i32 0
  store i64 %202, i64* %190, align 1
  %203 = add nuw nsw i64 %183, 8
  %204 = icmp ult i64 %203, %12
  br i1 %204, label %182, label %205

205:                                              ; preds = %182
  %206 = getelementptr inbounds i8, i8* %180, i64 %144
  %207 = add nuw nsw i64 %179, 1
  %208 = icmp slt i64 %207, %150
  br i1 %208, label %178, label %209

209:                                              ; preds = %87, %29, %205, %151
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define internal void @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_132Identity8TransformLoopRow_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_(i8 zeroext, i8 zeroext, i32, i8* nocapture, i32, i32, i8* nocapture readnone) #3 {
  %8 = icmp eq i8 %0, 9
  %9 = icmp eq i8 %1, 3
  %10 = and i1 %8, %9
  br i1 %10, label %155, label %11

11:                                               ; preds = %7
  %12 = bitcast i8* %3 to i16*
  %13 = zext i8 %1 to i64
  %14 = getelementptr inbounds [19 x i8], [19 x i8]* @_ZN7libgav116kTransformHeightE, i64 0, i64 %13
  %15 = load i8, i8* %14, align 1
  %16 = lshr i64 173354, %13
  %17 = and i64 %16, 1
  %18 = icmp ne i64 %17, 0
  %19 = icmp sgt i32 %2, 1
  br i1 %19, label %49, label %20

20:                                               ; preds = %11
  %21 = getelementptr inbounds [19 x i8], [19 x i8]* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_118kTransformRowShiftE, i64 0, i64 %13
  %22 = load i8, i8* %21, align 1
  %23 = zext i8 %22 to i32
  %24 = load i16, i16* %12, align 2
  %25 = sext i16 %24 to i32
  %26 = insertelement <4 x i32> <i32 undef, i32 0, i32 0, i32 0>, i32 %25, i32 0
  %27 = sext i1 %18 to i16
  %28 = insertelement <8 x i16> undef, i16 %27, i32 0
  %29 = shufflevector <8 x i16> %28, <8 x i16> undef, <8 x i32> zeroinitializer
  %30 = bitcast <4 x i32> %26 to <8 x i16>
  %31 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %30, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  %32 = bitcast <4 x i32> %26 to <16 x i8>
  %33 = bitcast <8 x i16> %31 to <16 x i8>
  %34 = bitcast <8 x i16> %29 to <16 x i8>
  %35 = tail call <16 x i8> @llvm.x86.sse41.pblendvb(<16 x i8> %32, <16 x i8> %33, <16 x i8> %34) #8
  %36 = bitcast <16 x i8> %35 to <8 x i16>
  %37 = shufflevector <8 x i16> %36, <8 x i16> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %38 = sext <4 x i16> %37 to <4 x i32>
  %39 = shl nsw <4 x i32> %38, <i32 1, i32 1, i32 1, i32 1>
  %40 = insertelement <4 x i32> undef, i32 %23, i32 0
  %41 = shufflevector <4 x i32> %40, <4 x i32> undef, <4 x i32> zeroinitializer
  %42 = shufflevector <4 x i32> %40, <4 x i32> undef, <2 x i32> zeroinitializer
  %43 = zext <2 x i32> %42 to <2 x i64>
  %44 = add <4 x i32> %39, %41
  %45 = bitcast <2 x i64> %43 to <4 x i32>
  %46 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %44, <4 x i32> %45) #8
  %47 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %46, <4 x i32> undef) #8
  %48 = extractelement <8 x i16> %47, i64 0
  store i16 %48, i16* %12, align 2
  br label %155

49:                                               ; preds = %11
  br i1 %18, label %50, label %105

50:                                               ; preds = %49
  %51 = sext i32 %2 to i64
  %52 = add nsw i64 %51, -1
  %53 = and i64 %51, 3
  %54 = icmp ult i64 %52, 3
  br i1 %54, label %90, label %55

55:                                               ; preds = %50
  %56 = sub nsw i64 %51, %53
  br label %57

57:                                               ; preds = %57, %55
  %58 = phi i64 [ 0, %55 ], [ %87, %57 ]
  %59 = phi i64 [ %56, %55 ], [ %88, %57 ]
  %60 = shl i64 %58, 3
  %61 = and i64 %60, 4294967264
  %62 = getelementptr inbounds i16, i16* %12, i64 %61
  %63 = bitcast i16* %62 to <8 x i16>*
  %64 = load <8 x i16>, <8 x i16>* %63, align 1
  %65 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %64, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %65, <8 x i16>* %63, align 1
  %66 = shl i64 %58, 3
  %67 = and i64 %66, 4294967264
  %68 = or i64 %67, 8
  %69 = getelementptr inbounds i16, i16* %12, i64 %68
  %70 = bitcast i16* %69 to <8 x i16>*
  %71 = load <8 x i16>, <8 x i16>* %70, align 1
  %72 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %71, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %72, <8 x i16>* %70, align 1
  %73 = shl i64 %58, 3
  %74 = and i64 %73, 4294967264
  %75 = or i64 %74, 16
  %76 = getelementptr inbounds i16, i16* %12, i64 %75
  %77 = bitcast i16* %76 to <8 x i16>*
  %78 = load <8 x i16>, <8 x i16>* %77, align 1
  %79 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %78, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %79, <8 x i16>* %77, align 1
  %80 = shl i64 %58, 3
  %81 = and i64 %80, 4294967264
  %82 = or i64 %81, 24
  %83 = getelementptr inbounds i16, i16* %12, i64 %82
  %84 = bitcast i16* %83 to <8 x i16>*
  %85 = load <8 x i16>, <8 x i16>* %84, align 1
  %86 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %85, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %86, <8 x i16>* %84, align 1
  %87 = add nuw nsw i64 %58, 4
  %88 = add i64 %59, -4
  %89 = icmp eq i64 %88, 0
  br i1 %89, label %90, label %57

90:                                               ; preds = %57, %50
  %91 = phi i64 [ 0, %50 ], [ %87, %57 ]
  %92 = icmp eq i64 %53, 0
  br i1 %92, label %105, label %93

93:                                               ; preds = %90, %93
  %94 = phi i64 [ %102, %93 ], [ %91, %90 ]
  %95 = phi i64 [ %103, %93 ], [ %53, %90 ]
  %96 = shl i64 %94, 3
  %97 = and i64 %96, 4294967288
  %98 = getelementptr inbounds i16, i16* %12, i64 %97
  %99 = bitcast i16* %98 to <8 x i16>*
  %100 = load <8 x i16>, <8 x i16>* %99, align 1
  %101 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %100, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %101, <8 x i16>* %99, align 1
  %102 = add nuw nsw i64 %94, 1
  %103 = add i64 %95, -1
  %104 = icmp eq i64 %103, 0
  br i1 %104, label %105, label %93, !llvm.loop !5

105:                                              ; preds = %90, %93, %49
  %106 = and i8 %15, 24
  %107 = icmp eq i8 %106, 0
  br i1 %107, label %108, label %155

108:                                              ; preds = %105
  %109 = icmp eq i8 %15, 32
  %110 = sext i32 %2 to i64
  br i1 %109, label %111, label %133

111:                                              ; preds = %108, %111
  %112 = phi i64 [ %131, %111 ], [ 0, %108 ]
  %113 = shl i64 %112, 3
  %114 = and i64 %113, 4294967264
  %115 = getelementptr inbounds i16, i16* %12, i64 %114
  %116 = bitcast i16* %115 to <8 x i16>*
  %117 = load <8 x i16>, <8 x i16>* %116, align 1
  %118 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %117, <8 x i16> <i16 16384, i16 16384, i16 16384, i16 16384, i16 16384, i16 16384, i16 16384, i16 16384>) #8
  store <8 x i16> %118, <8 x i16>* %116, align 1
  %119 = getelementptr inbounds i16, i16* %115, i64 8
  %120 = bitcast i16* %119 to <8 x i16>*
  %121 = load <8 x i16>, <8 x i16>* %120, align 1
  %122 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %121, <8 x i16> <i16 16384, i16 16384, i16 16384, i16 16384, i16 16384, i16 16384, i16 16384, i16 16384>) #8
  store <8 x i16> %122, <8 x i16>* %120, align 1
  %123 = getelementptr inbounds i16, i16* %115, i64 16
  %124 = bitcast i16* %123 to <8 x i16>*
  %125 = load <8 x i16>, <8 x i16>* %124, align 1
  %126 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %125, <8 x i16> <i16 16384, i16 16384, i16 16384, i16 16384, i16 16384, i16 16384, i16 16384, i16 16384>) #8
  store <8 x i16> %126, <8 x i16>* %124, align 1
  %127 = getelementptr inbounds i16, i16* %115, i64 24
  %128 = bitcast i16* %127 to <8 x i16>*
  %129 = load <8 x i16>, <8 x i16>* %128, align 1
  %130 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %129, <8 x i16> <i16 16384, i16 16384, i16 16384, i16 16384, i16 16384, i16 16384, i16 16384, i16 16384>) #8
  store <8 x i16> %130, <8 x i16>* %128, align 1
  %131 = add nuw nsw i64 %112, 4
  %132 = icmp slt i64 %131, %110
  br i1 %132, label %111, label %155

133:                                              ; preds = %108, %133
  %134 = phi i64 [ %153, %133 ], [ 0, %108 ]
  %135 = shl i64 %134, 3
  %136 = and i64 %135, 4294967264
  %137 = getelementptr inbounds i16, i16* %12, i64 %136
  %138 = bitcast i16* %137 to <8 x i16>*
  %139 = load <8 x i16>, <8 x i16>* %138, align 1
  %140 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %139, <8 x i16> %139) #8
  store <8 x i16> %140, <8 x i16>* %138, align 1
  %141 = getelementptr inbounds i16, i16* %137, i64 8
  %142 = bitcast i16* %141 to <8 x i16>*
  %143 = load <8 x i16>, <8 x i16>* %142, align 1
  %144 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %143, <8 x i16> %143) #8
  store <8 x i16> %144, <8 x i16>* %142, align 1
  %145 = getelementptr inbounds i16, i16* %137, i64 16
  %146 = bitcast i16* %145 to <8 x i16>*
  %147 = load <8 x i16>, <8 x i16>* %146, align 1
  %148 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %147, <8 x i16> %147) #8
  store <8 x i16> %148, <8 x i16>* %146, align 1
  %149 = getelementptr inbounds i16, i16* %137, i64 24
  %150 = bitcast i16* %149 to <8 x i16>*
  %151 = load <8 x i16>, <8 x i16>* %150, align 1
  %152 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %151, <8 x i16> %151) #8
  store <8 x i16> %152, <8 x i16>* %150, align 1
  %153 = add nuw nsw i64 %134, 4
  %154 = icmp slt i64 %153, %110
  br i1 %154, label %133, label %155

155:                                              ; preds = %133, %111, %20, %105, %7
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define internal void @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_135Identity8TransformLoopColumn_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_(i8 zeroext, i8 zeroext, i32, i8* nocapture, i32, i32, i8* nocapture readonly) #3 {
  %8 = bitcast i8* %3 to i16*
  %9 = zext i8 %1 to i64
  %10 = getelementptr inbounds [19 x i8], [19 x i8]* @_ZN7libgav115kTransformWidthE, i64 0, i64 %9
  %11 = load i8, i8* %10, align 1
  %12 = zext i8 %11 to i64
  %13 = zext i8 %0 to i32
  %14 = shl i32 1, %13
  %15 = and i32 %14, 33104
  %16 = icmp eq i32 %15, 0
  br i1 %16, label %82, label %17

17:                                               ; preds = %7
  %18 = icmp ugt i8 %11, 15
  br i1 %18, label %19, label %34

19:                                               ; preds = %17
  %20 = shl nuw nsw i64 %12, 3
  br label %21

21:                                               ; preds = %21, %19
  %22 = phi i64 [ 0, %19 ], [ %32, %21 ]
  %23 = getelementptr inbounds i16, i16* %8, i64 %22
  %24 = bitcast i16* %23 to <16 x i8>*
  %25 = load <16 x i8>, <16 x i8>* %24, align 1
  %26 = or i64 %22, 8
  %27 = getelementptr inbounds i16, i16* %8, i64 %26
  %28 = bitcast i16* %27 to <16 x i8>*
  %29 = load <16 x i8>, <16 x i8>* %28, align 1
  %30 = shufflevector <16 x i8> %25, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  %31 = shufflevector <16 x i8> %29, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %31, <16 x i8>* %24, align 1
  store <16 x i8> %30, <16 x i8>* %28, align 1
  %32 = add nuw nsw i64 %22, 16
  %33 = icmp ult i64 %32, %20
  br i1 %33, label %21, label %82

34:                                               ; preds = %17
  %35 = icmp eq i8 %11, 8
  %36 = bitcast i8* %3 to <16 x i8>*
  %37 = load <16 x i8>, <16 x i8>* %36, align 1
  br i1 %35, label %52, label %38

38:                                               ; preds = %34
  %39 = shufflevector <16 x i8> %37, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %39, <16 x i8>* %36, align 1
  %40 = getelementptr inbounds i8, i8* %3, i64 16
  %41 = bitcast i8* %40 to <16 x i8>*
  %42 = load <16 x i8>, <16 x i8>* %41, align 1
  %43 = shufflevector <16 x i8> %42, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %43, <16 x i8>* %41, align 1
  %44 = getelementptr inbounds i8, i8* %3, i64 32
  %45 = bitcast i8* %44 to <16 x i8>*
  %46 = load <16 x i8>, <16 x i8>* %45, align 1
  %47 = shufflevector <16 x i8> %46, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %47, <16 x i8>* %45, align 1
  %48 = getelementptr inbounds i8, i8* %3, i64 48
  %49 = bitcast i8* %48 to <16 x i8>*
  %50 = load <16 x i8>, <16 x i8>* %49, align 1
  %51 = shufflevector <16 x i8> %50, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %51, <16 x i8>* %49, align 1
  br label %82

52:                                               ; preds = %34
  %53 = shufflevector <16 x i8> %37, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %53, <16 x i8>* %36, align 1
  %54 = getelementptr inbounds i8, i8* %3, i64 16
  %55 = bitcast i8* %54 to <16 x i8>*
  %56 = load <16 x i8>, <16 x i8>* %55, align 1
  %57 = shufflevector <16 x i8> %56, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %57, <16 x i8>* %55, align 1
  %58 = getelementptr inbounds i8, i8* %3, i64 32
  %59 = bitcast i8* %58 to <16 x i8>*
  %60 = load <16 x i8>, <16 x i8>* %59, align 1
  %61 = shufflevector <16 x i8> %60, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %61, <16 x i8>* %59, align 1
  %62 = getelementptr inbounds i8, i8* %3, i64 48
  %63 = bitcast i8* %62 to <16 x i8>*
  %64 = load <16 x i8>, <16 x i8>* %63, align 1
  %65 = shufflevector <16 x i8> %64, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %65, <16 x i8>* %63, align 1
  %66 = getelementptr inbounds i8, i8* %3, i64 64
  %67 = bitcast i8* %66 to <16 x i8>*
  %68 = load <16 x i8>, <16 x i8>* %67, align 1
  %69 = shufflevector <16 x i8> %68, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %69, <16 x i8>* %67, align 1
  %70 = getelementptr inbounds i8, i8* %3, i64 80
  %71 = bitcast i8* %70 to <16 x i8>*
  %72 = load <16 x i8>, <16 x i8>* %71, align 1
  %73 = shufflevector <16 x i8> %72, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %73, <16 x i8>* %71, align 1
  %74 = getelementptr inbounds i8, i8* %3, i64 96
  %75 = bitcast i8* %74 to <16 x i8>*
  %76 = load <16 x i8>, <16 x i8>* %75, align 1
  %77 = shufflevector <16 x i8> %76, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %77, <16 x i8>* %75, align 1
  %78 = getelementptr inbounds i8, i8* %3, i64 112
  %79 = bitcast i8* %78 to <16 x i8>*
  %80 = load <16 x i8>, <16 x i8>* %79, align 1
  %81 = shufflevector <16 x i8> %80, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %81, <16 x i8>* %79, align 1
  br label %82

82:                                               ; preds = %21, %7, %52, %38
  %83 = bitcast i8* %6 to i64*
  %84 = load i64, i64* %83, align 8
  %85 = getelementptr inbounds i8, i8* %6, i64 8
  %86 = bitcast i8* %85 to i8**
  %87 = load i8*, i8** %86, align 8
  %88 = sext i32 %5 to i64
  %89 = ashr i64 %84, 32
  %90 = mul nsw i64 %89, %88
  %91 = getelementptr inbounds i8, i8* %87, i64 %90
  %92 = sext i32 %4 to i64
  %93 = getelementptr inbounds i8, i8* %91, i64 %92
  %94 = icmp eq i8 %11, 4
  %95 = sext i32 %2 to i64
  br i1 %94, label %96, label %122

96:                                               ; preds = %82, %96
  %97 = phi i64 [ %120, %96 ], [ 0, %82 ]
  %98 = phi i8* [ %119, %96 ], [ %93, %82 ]
  %99 = shl i64 %97, 2
  %100 = and i64 %99, 4294967292
  %101 = getelementptr inbounds i16, i16* %8, i64 %100
  %102 = bitcast i16* %101 to i64*
  %103 = load i64, i64* %102, align 1
  %104 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %103, i32 0
  %105 = bitcast <2 x i64> %104 to <8 x i16>
  %106 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %105, <8 x i16> %105) #8
  %107 = bitcast i8* %98 to i32*
  %108 = load i32, i32* %107, align 1
  %109 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %108, i32 0
  %110 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %106, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %111 = ashr <8 x i16> %110, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %112 = bitcast <4 x i32> %109 to <16 x i8>
  %113 = shufflevector <16 x i8> %112, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %114 = zext <8 x i8> %113 to <8 x i16>
  %115 = add nsw <8 x i16> %111, %114
  %116 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %115, <8 x i16> undef) #8
  %117 = bitcast <16 x i8> %116 to <4 x i32>
  %118 = extractelement <4 x i32> %117, i32 0
  store i32 %118, i32* %107, align 1
  %119 = getelementptr inbounds i8, i8* %98, i64 %89
  %120 = add nuw nsw i64 %97, 1
  %121 = icmp slt i64 %120, %95
  br i1 %121, label %96, label %152

122:                                              ; preds = %82, %148
  %123 = phi i64 [ %150, %148 ], [ 0, %82 ]
  %124 = phi i8* [ %149, %148 ], [ %93, %82 ]
  %125 = mul nuw nsw i64 %123, %12
  br label %126

126:                                              ; preds = %126, %122
  %127 = phi i64 [ %146, %126 ], [ 0, %122 ]
  %128 = add nuw nsw i64 %127, %125
  %129 = getelementptr inbounds i16, i16* %8, i64 %128
  %130 = bitcast i16* %129 to <8 x i16>*
  %131 = load <8 x i16>, <8 x i16>* %130, align 1
  %132 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %131, <8 x i16> %131) #8
  %133 = getelementptr inbounds i8, i8* %124, i64 %127
  %134 = bitcast i8* %133 to i64*
  %135 = load i64, i64* %134, align 1
  %136 = insertelement <2 x i64> undef, i64 %135, i32 0
  %137 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %132, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %138 = ashr <8 x i16> %137, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %139 = bitcast <2 x i64> %136 to <16 x i8>
  %140 = shufflevector <16 x i8> %139, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %141 = zext <8 x i8> %140 to <8 x i16>
  %142 = add nsw <8 x i16> %138, %141
  %143 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %142, <8 x i16> undef) #8
  %144 = bitcast <16 x i8> %143 to <2 x i64>
  %145 = extractelement <2 x i64> %144, i32 0
  store i64 %145, i64* %134, align 1
  %146 = add nuw nsw i64 %127, 8
  %147 = icmp ult i64 %146, %12
  br i1 %147, label %126, label %148

148:                                              ; preds = %126
  %149 = getelementptr inbounds i8, i8* %124, i64 %89
  %150 = add nuw nsw i64 %123, 1
  %151 = icmp slt i64 %150, %95
  br i1 %151, label %122, label %152

152:                                              ; preds = %148, %96
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define internal void @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_133Identity16TransformLoopRow_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_(i8 zeroext, i8 zeroext, i32, i8* nocapture, i32, i32, i8* nocapture readnone) #3 {
  %8 = bitcast i8* %3 to i16*
  %9 = zext i8 %1 to i64
  %10 = lshr i64 173354, %9
  %11 = and i64 %10, 1
  %12 = icmp ne i64 %11, 0
  %13 = getelementptr inbounds [19 x i8], [19 x i8]* @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_118kTransformRowShiftE, i64 0, i64 %9
  %14 = load i8, i8* %13, align 1
  %15 = zext i8 %14 to i32
  %16 = icmp sgt i32 %2, 1
  br i1 %16, label %45, label %17

17:                                               ; preds = %7
  %18 = load i16, i16* %8, align 2
  %19 = sext i16 %18 to i32
  %20 = insertelement <4 x i32> <i32 undef, i32 0, i32 0, i32 0>, i32 %19, i32 0
  %21 = sext i1 %12 to i16
  %22 = insertelement <8 x i16> undef, i16 %21, i32 0
  %23 = shufflevector <8 x i16> %22, <8 x i16> undef, <8 x i32> zeroinitializer
  %24 = bitcast <4 x i32> %20 to <8 x i16>
  %25 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %24, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  %26 = bitcast <4 x i32> %20 to <16 x i8>
  %27 = bitcast <8 x i16> %25 to <16 x i8>
  %28 = bitcast <8 x i16> %23 to <16 x i8>
  %29 = tail call <16 x i8> @llvm.x86.sse41.pblendvb(<16 x i8> %26, <16 x i8> %27, <16 x i8> %28) #8
  %30 = zext i8 %14 to i16
  %31 = shl i16 %30, 12
  %32 = or i16 %31, 2048
  %33 = insertelement <8 x i16> undef, i16 %32, i32 0
  %34 = shufflevector <8 x i16> %33, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %35 = add nuw nsw i32 %15, 12
  %36 = zext i32 %35 to i64
  %37 = insertelement <2 x i64> undef, i64 %36, i32 0
  %38 = bitcast <16 x i8> %29 to <8 x i16>
  %39 = shufflevector <8 x i16> %34, <8 x i16> %38, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %40 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %39, <8 x i16> <i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586>) #8
  %41 = bitcast <2 x i64> %37 to <4 x i32>
  %42 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %40, <4 x i32> %41) #8
  %43 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %42, <4 x i32> undef) #8
  %44 = extractelement <8 x i16> %43, i64 0
  store i16 %44, i16* %8, align 2
  br label %193

45:                                               ; preds = %7
  %46 = sext i32 %2 to i64
  br i1 %12, label %47, label %97

47:                                               ; preds = %45
  %48 = and i64 %46, 1
  %49 = icmp eq i32 %2, 1
  br i1 %49, label %84, label %50

50:                                               ; preds = %47
  %51 = sub nsw i64 %46, %48
  br label %52

52:                                               ; preds = %52, %50
  %53 = phi i64 [ 0, %50 ], [ %78, %52 ]
  %54 = phi i64 [ %51, %50 ], [ %79, %52 ]
  %55 = shl i64 %53, 4
  %56 = and i64 %55, 4294967264
  %57 = getelementptr inbounds i16, i16* %8, i64 %56
  %58 = bitcast i16* %57 to <8 x i16>*
  %59 = load <8 x i16>, <8 x i16>* %58, align 1
  %60 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %59, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %60, <8 x i16>* %58, align 1
  %61 = or i64 %56, 8
  %62 = getelementptr inbounds i16, i16* %8, i64 %61
  %63 = bitcast i16* %62 to <8 x i16>*
  %64 = load <8 x i16>, <8 x i16>* %63, align 1
  %65 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %64, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %65, <8 x i16>* %63, align 1
  %66 = shl i64 %53, 4
  %67 = and i64 %66, 4294967264
  %68 = or i64 %67, 16
  %69 = getelementptr inbounds i16, i16* %8, i64 %68
  %70 = bitcast i16* %69 to <8 x i16>*
  %71 = load <8 x i16>, <8 x i16>* %70, align 1
  %72 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %71, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %72, <8 x i16>* %70, align 1
  %73 = or i64 %67, 24
  %74 = getelementptr inbounds i16, i16* %8, i64 %73
  %75 = bitcast i16* %74 to <8 x i16>*
  %76 = load <8 x i16>, <8 x i16>* %75, align 1
  %77 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %76, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %77, <8 x i16>* %75, align 1
  %78 = add nuw nsw i64 %53, 2
  %79 = add i64 %54, -2
  %80 = icmp eq i64 %79, 0
  br i1 %80, label %81, label %52

81:                                               ; preds = %52
  %82 = shl i64 %78, 4
  %83 = and i64 %82, 4294967264
  br label %84

84:                                               ; preds = %81, %47
  %85 = phi i64 [ 0, %47 ], [ %83, %81 ]
  %86 = icmp eq i64 %48, 0
  br i1 %86, label %97, label %87

87:                                               ; preds = %84
  %88 = getelementptr inbounds i16, i16* %8, i64 %85
  %89 = bitcast i16* %88 to <8 x i16>*
  %90 = load <8 x i16>, <8 x i16>* %89, align 1
  %91 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %90, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %91, <8 x i16>* %89, align 1
  %92 = or i64 %85, 8
  %93 = getelementptr inbounds i16, i16* %8, i64 %92
  %94 = bitcast i16* %93 to <8 x i16>*
  %95 = load <8 x i16>, <8 x i16>* %94, align 1
  %96 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %95, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %96, <8 x i16>* %94, align 1
  br label %97

97:                                               ; preds = %87, %84, %45
  %98 = zext i8 %14 to i16
  %99 = shl i16 %98, 12
  %100 = or i16 %99, 2048
  %101 = insertelement <8 x i16> undef, i16 %100, i32 0
  %102 = shufflevector <8 x i16> %101, <8 x i16> undef, <8 x i32> zeroinitializer
  %103 = add nuw nsw i32 %15, 12
  %104 = zext i32 %103 to i64
  %105 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %104, i32 0
  %106 = bitcast <2 x i64> %105 to <4 x i32>
  br label %107

107:                                              ; preds = %107, %97
  %108 = phi i64 [ %191, %107 ], [ 0, %97 ]
  %109 = shl i64 %108, 4
  %110 = and i64 %109, 4294967232
  %111 = getelementptr inbounds i16, i16* %8, i64 %110
  %112 = bitcast i16* %111 to <8 x i16>*
  %113 = load <8 x i16>, <8 x i16>* %112, align 1
  %114 = getelementptr inbounds i16, i16* %111, i64 8
  %115 = bitcast i16* %114 to <8 x i16>*
  %116 = load <8 x i16>, <8 x i16>* %115, align 1
  %117 = shufflevector <8 x i16> %102, <8 x i16> %113, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %118 = shufflevector <8 x i16> %102, <8 x i16> %113, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %119 = shufflevector <8 x i16> %102, <8 x i16> %116, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %120 = shufflevector <8 x i16> %102, <8 x i16> %116, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %121 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %117, <8 x i16> <i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586>) #8
  %122 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %118, <8 x i16> <i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586>) #8
  %123 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %119, <8 x i16> <i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586>) #8
  %124 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %120, <8 x i16> <i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586>) #8
  %125 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %121, <4 x i32> %106) #8
  %126 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %122, <4 x i32> %106) #8
  %127 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %123, <4 x i32> %106) #8
  %128 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %124, <4 x i32> %106) #8
  %129 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %125, <4 x i32> %126) #8
  store <8 x i16> %129, <8 x i16>* %112, align 1
  %130 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %127, <4 x i32> %128) #8
  store <8 x i16> %130, <8 x i16>* %115, align 1
  %131 = getelementptr inbounds i16, i16* %111, i64 16
  %132 = bitcast i16* %131 to <8 x i16>*
  %133 = load <8 x i16>, <8 x i16>* %132, align 1
  %134 = getelementptr inbounds i16, i16* %111, i64 24
  %135 = bitcast i16* %134 to <8 x i16>*
  %136 = load <8 x i16>, <8 x i16>* %135, align 1
  %137 = shufflevector <8 x i16> %102, <8 x i16> %133, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %138 = shufflevector <8 x i16> %102, <8 x i16> %133, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %139 = shufflevector <8 x i16> %102, <8 x i16> %136, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %140 = shufflevector <8 x i16> %102, <8 x i16> %136, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %141 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %137, <8 x i16> <i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586>) #8
  %142 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %138, <8 x i16> <i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586>) #8
  %143 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %139, <8 x i16> <i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586>) #8
  %144 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %140, <8 x i16> <i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586>) #8
  %145 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %141, <4 x i32> %106) #8
  %146 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %142, <4 x i32> %106) #8
  %147 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %143, <4 x i32> %106) #8
  %148 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %144, <4 x i32> %106) #8
  %149 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %145, <4 x i32> %146) #8
  store <8 x i16> %149, <8 x i16>* %132, align 1
  %150 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %147, <4 x i32> %148) #8
  store <8 x i16> %150, <8 x i16>* %135, align 1
  %151 = getelementptr inbounds i16, i16* %111, i64 32
  %152 = bitcast i16* %151 to <8 x i16>*
  %153 = load <8 x i16>, <8 x i16>* %152, align 1
  %154 = getelementptr inbounds i16, i16* %111, i64 40
  %155 = bitcast i16* %154 to <8 x i16>*
  %156 = load <8 x i16>, <8 x i16>* %155, align 1
  %157 = shufflevector <8 x i16> %102, <8 x i16> %153, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %158 = shufflevector <8 x i16> %102, <8 x i16> %153, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %159 = shufflevector <8 x i16> %102, <8 x i16> %156, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %160 = shufflevector <8 x i16> %102, <8 x i16> %156, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %161 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %157, <8 x i16> <i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586>) #8
  %162 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %158, <8 x i16> <i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586>) #8
  %163 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %159, <8 x i16> <i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586>) #8
  %164 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %160, <8 x i16> <i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586>) #8
  %165 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %161, <4 x i32> %106) #8
  %166 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %162, <4 x i32> %106) #8
  %167 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %163, <4 x i32> %106) #8
  %168 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %164, <4 x i32> %106) #8
  %169 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %165, <4 x i32> %166) #8
  store <8 x i16> %169, <8 x i16>* %152, align 1
  %170 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %167, <4 x i32> %168) #8
  store <8 x i16> %170, <8 x i16>* %155, align 1
  %171 = getelementptr inbounds i16, i16* %111, i64 48
  %172 = bitcast i16* %171 to <8 x i16>*
  %173 = load <8 x i16>, <8 x i16>* %172, align 1
  %174 = getelementptr inbounds i16, i16* %111, i64 56
  %175 = bitcast i16* %174 to <8 x i16>*
  %176 = load <8 x i16>, <8 x i16>* %175, align 1
  %177 = shufflevector <8 x i16> %102, <8 x i16> %173, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %178 = shufflevector <8 x i16> %102, <8 x i16> %173, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %179 = shufflevector <8 x i16> %102, <8 x i16> %176, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %180 = shufflevector <8 x i16> %102, <8 x i16> %176, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %181 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %177, <8 x i16> <i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586>) #8
  %182 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %178, <8 x i16> <i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586>) #8
  %183 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %179, <8 x i16> <i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586>) #8
  %184 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %180, <8 x i16> <i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586, i16 1, i16 11586>) #8
  %185 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %181, <4 x i32> %106) #8
  %186 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %182, <4 x i32> %106) #8
  %187 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %183, <4 x i32> %106) #8
  %188 = tail call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %184, <4 x i32> %106) #8
  %189 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %185, <4 x i32> %186) #8
  store <8 x i16> %189, <8 x i16>* %172, align 1
  %190 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %187, <4 x i32> %188) #8
  store <8 x i16> %190, <8 x i16>* %175, align 1
  %191 = add nuw nsw i64 %108, 4
  %192 = icmp slt i64 %191, %46
  br i1 %192, label %107, label %193

193:                                              ; preds = %107, %17
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define internal void @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_136Identity16TransformLoopColumn_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_(i8 zeroext, i8 zeroext, i32, i8*, i32, i32, i8* nocapture readonly) #3 {
  %8 = bitcast i8* %3 to i16*
  %9 = zext i8 %1 to i64
  %10 = getelementptr inbounds [19 x i8], [19 x i8]* @_ZN7libgav115kTransformWidthE, i64 0, i64 %9
  %11 = load i8, i8* %10, align 1
  %12 = zext i8 %11 to i64
  %13 = zext i8 %0 to i32
  %14 = shl i32 1, %13
  %15 = and i32 %14, 33104
  %16 = icmp eq i32 %15, 0
  br i1 %16, label %130, label %17

17:                                               ; preds = %7
  %18 = icmp ugt i8 %11, 15
  br i1 %18, label %19, label %34

19:                                               ; preds = %17
  %20 = shl nuw nsw i64 %12, 4
  br label %21

21:                                               ; preds = %21, %19
  %22 = phi i64 [ 0, %19 ], [ %32, %21 ]
  %23 = getelementptr inbounds i16, i16* %8, i64 %22
  %24 = bitcast i16* %23 to <16 x i8>*
  %25 = load <16 x i8>, <16 x i8>* %24, align 1
  %26 = or i64 %22, 8
  %27 = getelementptr inbounds i16, i16* %8, i64 %26
  %28 = bitcast i16* %27 to <16 x i8>*
  %29 = load <16 x i8>, <16 x i8>* %28, align 1
  %30 = shufflevector <16 x i8> %25, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  %31 = shufflevector <16 x i8> %29, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %31, <16 x i8>* %24, align 1
  store <16 x i8> %30, <16 x i8>* %28, align 1
  %32 = add nuw nsw i64 %22, 16
  %33 = icmp ult i64 %32, %20
  br i1 %33, label %21, label %130

34:                                               ; preds = %17
  %35 = icmp eq i8 %11, 8
  %36 = bitcast i8* %3 to <16 x i8>*
  %37 = load <16 x i8>, <16 x i8>* %36, align 1
  br i1 %35, label %68, label %38

38:                                               ; preds = %34
  %39 = shufflevector <16 x i8> %37, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %39, <16 x i8>* %36, align 1
  %40 = getelementptr inbounds i8, i8* %3, i64 16
  %41 = bitcast i8* %40 to <16 x i8>*
  %42 = load <16 x i8>, <16 x i8>* %41, align 1
  %43 = shufflevector <16 x i8> %42, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %43, <16 x i8>* %41, align 1
  %44 = getelementptr inbounds i8, i8* %3, i64 32
  %45 = bitcast i8* %44 to <16 x i8>*
  %46 = load <16 x i8>, <16 x i8>* %45, align 1
  %47 = shufflevector <16 x i8> %46, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %47, <16 x i8>* %45, align 1
  %48 = getelementptr inbounds i8, i8* %3, i64 48
  %49 = bitcast i8* %48 to <16 x i8>*
  %50 = load <16 x i8>, <16 x i8>* %49, align 1
  %51 = shufflevector <16 x i8> %50, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %51, <16 x i8>* %49, align 1
  %52 = getelementptr inbounds i8, i8* %3, i64 64
  %53 = bitcast i8* %52 to <16 x i8>*
  %54 = load <16 x i8>, <16 x i8>* %53, align 1
  %55 = shufflevector <16 x i8> %54, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %55, <16 x i8>* %53, align 1
  %56 = getelementptr inbounds i8, i8* %3, i64 80
  %57 = bitcast i8* %56 to <16 x i8>*
  %58 = load <16 x i8>, <16 x i8>* %57, align 1
  %59 = shufflevector <16 x i8> %58, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %59, <16 x i8>* %57, align 1
  %60 = getelementptr inbounds i8, i8* %3, i64 96
  %61 = bitcast i8* %60 to <16 x i8>*
  %62 = load <16 x i8>, <16 x i8>* %61, align 1
  %63 = shufflevector <16 x i8> %62, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %63, <16 x i8>* %61, align 1
  %64 = getelementptr inbounds i8, i8* %3, i64 112
  %65 = bitcast i8* %64 to <16 x i8>*
  %66 = load <16 x i8>, <16 x i8>* %65, align 1
  %67 = shufflevector <16 x i8> %66, <16 x i8> undef, <16 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9>
  store <16 x i8> %67, <16 x i8>* %65, align 1
  br label %130

68:                                               ; preds = %34
  %69 = shufflevector <16 x i8> %37, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %69, <16 x i8>* %36, align 1
  %70 = getelementptr inbounds i8, i8* %3, i64 16
  %71 = bitcast i8* %70 to <16 x i8>*
  %72 = load <16 x i8>, <16 x i8>* %71, align 1
  %73 = shufflevector <16 x i8> %72, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %73, <16 x i8>* %71, align 1
  %74 = getelementptr inbounds i8, i8* %3, i64 32
  %75 = bitcast i8* %74 to <16 x i8>*
  %76 = load <16 x i8>, <16 x i8>* %75, align 1
  %77 = shufflevector <16 x i8> %76, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %77, <16 x i8>* %75, align 1
  %78 = getelementptr inbounds i8, i8* %3, i64 48
  %79 = bitcast i8* %78 to <16 x i8>*
  %80 = load <16 x i8>, <16 x i8>* %79, align 1
  %81 = shufflevector <16 x i8> %80, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %81, <16 x i8>* %79, align 1
  %82 = getelementptr inbounds i8, i8* %3, i64 64
  %83 = bitcast i8* %82 to <16 x i8>*
  %84 = load <16 x i8>, <16 x i8>* %83, align 1
  %85 = shufflevector <16 x i8> %84, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %85, <16 x i8>* %83, align 1
  %86 = getelementptr inbounds i8, i8* %3, i64 80
  %87 = bitcast i8* %86 to <16 x i8>*
  %88 = load <16 x i8>, <16 x i8>* %87, align 1
  %89 = shufflevector <16 x i8> %88, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %89, <16 x i8>* %87, align 1
  %90 = getelementptr inbounds i8, i8* %3, i64 96
  %91 = bitcast i8* %90 to <16 x i8>*
  %92 = load <16 x i8>, <16 x i8>* %91, align 1
  %93 = shufflevector <16 x i8> %92, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %93, <16 x i8>* %91, align 1
  %94 = getelementptr inbounds i8, i8* %3, i64 112
  %95 = bitcast i8* %94 to <16 x i8>*
  %96 = load <16 x i8>, <16 x i8>* %95, align 1
  %97 = shufflevector <16 x i8> %96, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %97, <16 x i8>* %95, align 1
  %98 = getelementptr inbounds i8, i8* %3, i64 128
  %99 = bitcast i8* %98 to <16 x i8>*
  %100 = load <16 x i8>, <16 x i8>* %99, align 1
  %101 = shufflevector <16 x i8> %100, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %101, <16 x i8>* %99, align 1
  %102 = getelementptr inbounds i8, i8* %3, i64 144
  %103 = bitcast i8* %102 to <16 x i8>*
  %104 = load <16 x i8>, <16 x i8>* %103, align 1
  %105 = shufflevector <16 x i8> %104, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %105, <16 x i8>* %103, align 1
  %106 = getelementptr inbounds i8, i8* %3, i64 160
  %107 = bitcast i8* %106 to <16 x i8>*
  %108 = load <16 x i8>, <16 x i8>* %107, align 1
  %109 = shufflevector <16 x i8> %108, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %109, <16 x i8>* %107, align 1
  %110 = getelementptr inbounds i8, i8* %3, i64 176
  %111 = bitcast i8* %110 to <16 x i8>*
  %112 = load <16 x i8>, <16 x i8>* %111, align 1
  %113 = shufflevector <16 x i8> %112, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %113, <16 x i8>* %111, align 1
  %114 = getelementptr inbounds i8, i8* %3, i64 192
  %115 = bitcast i8* %114 to <16 x i8>*
  %116 = load <16 x i8>, <16 x i8>* %115, align 1
  %117 = shufflevector <16 x i8> %116, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %117, <16 x i8>* %115, align 1
  %118 = getelementptr inbounds i8, i8* %3, i64 208
  %119 = bitcast i8* %118 to <16 x i8>*
  %120 = load <16 x i8>, <16 x i8>* %119, align 1
  %121 = shufflevector <16 x i8> %120, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %121, <16 x i8>* %119, align 1
  %122 = getelementptr inbounds i8, i8* %3, i64 224
  %123 = bitcast i8* %122 to <16 x i8>*
  %124 = load <16 x i8>, <16 x i8>* %123, align 1
  %125 = shufflevector <16 x i8> %124, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %125, <16 x i8>* %123, align 1
  %126 = getelementptr inbounds i8, i8* %3, i64 240
  %127 = bitcast i8* %126 to <16 x i8>*
  %128 = load <16 x i8>, <16 x i8>* %127, align 1
  %129 = shufflevector <16 x i8> %128, <16 x i8> undef, <16 x i32> <i32 14, i32 15, i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
  store <16 x i8> %129, <16 x i8>* %127, align 1
  br label %130

130:                                              ; preds = %21, %7, %68, %38
  %131 = bitcast i8* %6 to i64*
  %132 = load i64, i64* %131, align 8
  %133 = getelementptr inbounds i8, i8* %6, i64 8
  %134 = bitcast i8* %133 to i8**
  %135 = load i8*, i8** %134, align 8
  %136 = sext i32 %5 to i64
  %137 = ashr i64 %132, 32
  %138 = mul nsw i64 %137, %136
  %139 = getelementptr inbounds i8, i8* %135, i64 %138
  %140 = sext i32 %4 to i64
  %141 = getelementptr inbounds i8, i8* %139, i64 %140
  %142 = icmp eq i8 %11, 4
  %143 = sext i32 %2 to i64
  br i1 %142, label %144, label %172

144:                                              ; preds = %130, %144
  %145 = phi i64 [ %170, %144 ], [ 0, %130 ]
  %146 = phi i8* [ %169, %144 ], [ %141, %130 ]
  %147 = shl i64 %145, 2
  %148 = and i64 %147, 4294967292
  %149 = getelementptr inbounds i16, i16* %8, i64 %148
  %150 = bitcast i16* %149 to i64*
  %151 = load i64, i64* %150, align 1
  %152 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %151, i32 0
  %153 = bitcast <2 x i64> %152 to <8 x i16>
  %154 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %153, <8 x i16> <i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152>) #8
  %155 = bitcast i8* %146 to i32*
  %156 = load i32, i32* %155, align 1
  %157 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %156, i32 0
  %158 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %153, <8 x i16> %153) #8
  %159 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %154, <8 x i16> %158) #8
  %160 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %159, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %161 = ashr <8 x i16> %160, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %162 = bitcast <4 x i32> %157 to <16 x i8>
  %163 = shufflevector <16 x i8> %162, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %164 = zext <8 x i8> %163 to <8 x i16>
  %165 = add nsw <8 x i16> %161, %164
  %166 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %165, <8 x i16> undef) #8
  %167 = bitcast <16 x i8> %166 to <4 x i32>
  %168 = extractelement <4 x i32> %167, i32 0
  store i32 %168, i32* %155, align 1
  %169 = getelementptr inbounds i8, i8* %146, i64 %137
  %170 = add nuw nsw i64 %145, 1
  %171 = icmp slt i64 %170, %143
  br i1 %171, label %144, label %204

172:                                              ; preds = %130, %200
  %173 = phi i64 [ %202, %200 ], [ 0, %130 ]
  %174 = phi i8* [ %201, %200 ], [ %141, %130 ]
  %175 = mul nuw nsw i64 %173, %12
  br label %176

176:                                              ; preds = %176, %172
  %177 = phi i64 [ %198, %176 ], [ 0, %172 ]
  %178 = add nuw nsw i64 %177, %175
  %179 = getelementptr inbounds i16, i16* %8, i64 %178
  %180 = bitcast i16* %179 to <8 x i16>*
  %181 = load <8 x i16>, <8 x i16>* %180, align 1
  %182 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %181, <8 x i16> <i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152, i16 27152>) #8
  %183 = getelementptr inbounds i8, i8* %174, i64 %177
  %184 = bitcast i8* %183 to i64*
  %185 = load i64, i64* %184, align 1
  %186 = insertelement <2 x i64> undef, i64 %185, i32 0
  %187 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %181, <8 x i16> %181) #8
  %188 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %182, <8 x i16> %187) #8
  %189 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %188, <8 x i16> <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>) #8
  %190 = ashr <8 x i16> %189, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %191 = bitcast <2 x i64> %186 to <16 x i8>
  %192 = shufflevector <16 x i8> %191, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %193 = zext <8 x i8> %192 to <8 x i16>
  %194 = add nsw <8 x i16> %190, %193
  %195 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %194, <8 x i16> undef) #8
  %196 = bitcast <16 x i8> %195 to <2 x i64>
  %197 = extractelement <2 x i64> %196, i32 0
  store i64 %197, i64* %184, align 1
  %198 = add nuw nsw i64 %177, 8
  %199 = icmp ult i64 %198, %12
  br i1 %199, label %176, label %200

200:                                              ; preds = %176
  %201 = getelementptr inbounds i8, i8* %174, i64 %137
  %202 = add nuw nsw i64 %173, 1
  %203 = icmp slt i64 %202, %143
  br i1 %203, label %172, label %204

204:                                              ; preds = %200, %144
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define internal void @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_133Identity32TransformLoopRow_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_(i8 zeroext, i8 zeroext, i32, i8* nocapture, i32, i32, i8* nocapture readnone) #3 {
  %8 = zext i8 %1 to i64
  %9 = getelementptr inbounds [19 x i8], [19 x i8]* @_ZN7libgav116kTransformHeightE, i64 0, i64 %8
  %10 = load i8, i8* %9, align 1
  %11 = and i8 %10, 40
  %12 = icmp eq i8 %11, 0
  br i1 %12, label %13, label %121

13:                                               ; preds = %7
  %14 = bitcast i8* %3 to i16*
  %15 = icmp sgt i32 %2, 1
  br i1 %15, label %24, label %16

16:                                               ; preds = %13
  %17 = load i16, i16* %14, align 2
  %18 = sext i16 %17 to i32
  %19 = insertelement <4 x i32> <i32 undef, i32 0, i32 0, i32 0>, i32 %18, i32 0
  %20 = bitcast <4 x i32> %19 to <8 x i16>
  %21 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %20, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  %22 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %21, <8 x i16> %21) #8
  %23 = extractelement <8 x i16> %22, i64 0
  store i16 %23, i16* %14, align 2
  br label %121

24:                                               ; preds = %13
  %25 = sext i32 %2 to i64
  br label %26

26:                                               ; preds = %26, %24
  %27 = phi i64 [ %49, %26 ], [ 0, %24 ]
  %28 = shl i64 %27, 5
  %29 = and i64 %28, 4294967264
  %30 = getelementptr inbounds i16, i16* %14, i64 %29
  %31 = bitcast i16* %30 to <8 x i16>*
  %32 = load <8 x i16>, <8 x i16>* %31, align 1
  %33 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %32, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %33, <8 x i16>* %31, align 1
  %34 = or i64 %29, 8
  %35 = getelementptr inbounds i16, i16* %14, i64 %34
  %36 = bitcast i16* %35 to <8 x i16>*
  %37 = load <8 x i16>, <8 x i16>* %36, align 1
  %38 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %37, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %38, <8 x i16>* %36, align 1
  %39 = or i64 %29, 16
  %40 = getelementptr inbounds i16, i16* %14, i64 %39
  %41 = bitcast i16* %40 to <8 x i16>*
  %42 = load <8 x i16>, <8 x i16>* %41, align 1
  %43 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %42, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %43, <8 x i16>* %41, align 1
  %44 = or i64 %29, 24
  %45 = getelementptr inbounds i16, i16* %14, i64 %44
  %46 = bitcast i16* %45 to <8 x i16>*
  %47 = load <8 x i16>, <8 x i16>* %46, align 1
  %48 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %47, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  store <8 x i16> %48, <8 x i16>* %46, align 1
  %49 = add nuw nsw i64 %27, 1
  %50 = icmp eq i64 %49, %25
  br i1 %50, label %51, label %26

51:                                               ; preds = %26, %51
  %52 = phi i64 [ %119, %51 ], [ 0, %26 ]
  %53 = shl i64 %52, 5
  %54 = and i64 %53, 4294967168
  %55 = getelementptr inbounds i16, i16* %14, i64 %54
  %56 = bitcast i16* %55 to <8 x i16>*
  %57 = load <8 x i16>, <8 x i16>* %56, align 1
  %58 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %57, <8 x i16> %57) #8
  store <8 x i16> %58, <8 x i16>* %56, align 1
  %59 = getelementptr inbounds i16, i16* %55, i64 8
  %60 = bitcast i16* %59 to <8 x i16>*
  %61 = load <8 x i16>, <8 x i16>* %60, align 1
  %62 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %61, <8 x i16> %61) #8
  store <8 x i16> %62, <8 x i16>* %60, align 1
  %63 = getelementptr inbounds i16, i16* %55, i64 16
  %64 = bitcast i16* %63 to <8 x i16>*
  %65 = load <8 x i16>, <8 x i16>* %64, align 1
  %66 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %65, <8 x i16> %65) #8
  store <8 x i16> %66, <8 x i16>* %64, align 1
  %67 = getelementptr inbounds i16, i16* %55, i64 24
  %68 = bitcast i16* %67 to <8 x i16>*
  %69 = load <8 x i16>, <8 x i16>* %68, align 1
  %70 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %69, <8 x i16> %69) #8
  store <8 x i16> %70, <8 x i16>* %68, align 1
  %71 = getelementptr inbounds i16, i16* %55, i64 32
  %72 = bitcast i16* %71 to <8 x i16>*
  %73 = load <8 x i16>, <8 x i16>* %72, align 1
  %74 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %73, <8 x i16> %73) #8
  store <8 x i16> %74, <8 x i16>* %72, align 1
  %75 = getelementptr inbounds i16, i16* %55, i64 40
  %76 = bitcast i16* %75 to <8 x i16>*
  %77 = load <8 x i16>, <8 x i16>* %76, align 1
  %78 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %77, <8 x i16> %77) #8
  store <8 x i16> %78, <8 x i16>* %76, align 1
  %79 = getelementptr inbounds i16, i16* %55, i64 48
  %80 = bitcast i16* %79 to <8 x i16>*
  %81 = load <8 x i16>, <8 x i16>* %80, align 1
  %82 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %81, <8 x i16> %81) #8
  store <8 x i16> %82, <8 x i16>* %80, align 1
  %83 = getelementptr inbounds i16, i16* %55, i64 56
  %84 = bitcast i16* %83 to <8 x i16>*
  %85 = load <8 x i16>, <8 x i16>* %84, align 1
  %86 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %85, <8 x i16> %85) #8
  store <8 x i16> %86, <8 x i16>* %84, align 1
  %87 = getelementptr inbounds i16, i16* %55, i64 64
  %88 = bitcast i16* %87 to <8 x i16>*
  %89 = load <8 x i16>, <8 x i16>* %88, align 1
  %90 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %89, <8 x i16> %89) #8
  store <8 x i16> %90, <8 x i16>* %88, align 1
  %91 = getelementptr inbounds i16, i16* %55, i64 72
  %92 = bitcast i16* %91 to <8 x i16>*
  %93 = load <8 x i16>, <8 x i16>* %92, align 1
  %94 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %93, <8 x i16> %93) #8
  store <8 x i16> %94, <8 x i16>* %92, align 1
  %95 = getelementptr inbounds i16, i16* %55, i64 80
  %96 = bitcast i16* %95 to <8 x i16>*
  %97 = load <8 x i16>, <8 x i16>* %96, align 1
  %98 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %97, <8 x i16> %97) #8
  store <8 x i16> %98, <8 x i16>* %96, align 1
  %99 = getelementptr inbounds i16, i16* %55, i64 88
  %100 = bitcast i16* %99 to <8 x i16>*
  %101 = load <8 x i16>, <8 x i16>* %100, align 1
  %102 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %101, <8 x i16> %101) #8
  store <8 x i16> %102, <8 x i16>* %100, align 1
  %103 = getelementptr inbounds i16, i16* %55, i64 96
  %104 = bitcast i16* %103 to <8 x i16>*
  %105 = load <8 x i16>, <8 x i16>* %104, align 1
  %106 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %105, <8 x i16> %105) #8
  store <8 x i16> %106, <8 x i16>* %104, align 1
  %107 = getelementptr inbounds i16, i16* %55, i64 104
  %108 = bitcast i16* %107 to <8 x i16>*
  %109 = load <8 x i16>, <8 x i16>* %108, align 1
  %110 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %109, <8 x i16> %109) #8
  store <8 x i16> %110, <8 x i16>* %108, align 1
  %111 = getelementptr inbounds i16, i16* %55, i64 112
  %112 = bitcast i16* %111 to <8 x i16>*
  %113 = load <8 x i16>, <8 x i16>* %112, align 1
  %114 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %113, <8 x i16> %113) #8
  store <8 x i16> %114, <8 x i16>* %112, align 1
  %115 = getelementptr inbounds i16, i16* %55, i64 120
  %116 = bitcast i16* %115 to <8 x i16>*
  %117 = load <8 x i16>, <8 x i16>* %116, align 1
  %118 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %117, <8 x i16> %117) #8
  store <8 x i16> %118, <8 x i16>* %116, align 1
  %119 = add nuw nsw i64 %52, 4
  %120 = icmp slt i64 %119, %25
  br i1 %120, label %51, label %121

121:                                              ; preds = %51, %16, %7
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define internal void @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_136Identity32TransformLoopColumn_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_(i8 zeroext, i8 zeroext, i32, i8* nocapture readonly, i32, i32, i8* nocapture readonly) #3 {
  %8 = bitcast i8* %3 to i16*
  %9 = zext i8 %1 to i64
  %10 = getelementptr inbounds [19 x i8], [19 x i8]* @_ZN7libgav115kTransformWidthE, i64 0, i64 %9
  %11 = load i8, i8* %10, align 1
  %12 = bitcast i8* %6 to i64*
  %13 = load i64, i64* %12, align 8
  %14 = getelementptr inbounds i8, i8* %6, i64 8
  %15 = bitcast i8* %14 to i8**
  %16 = load i8*, i8** %15, align 8
  %17 = sext i32 %5 to i64
  %18 = ashr i64 %13, 32
  %19 = mul nsw i64 %18, %17
  %20 = getelementptr inbounds i8, i8* %16, i64 %19
  %21 = sext i32 %4 to i64
  %22 = getelementptr inbounds i8, i8* %20, i64 %21
  %23 = zext i8 %11 to i64
  %24 = sext i32 %2 to i64
  br label %25

25:                                               ; preds = %50, %7
  %26 = phi i64 [ %52, %50 ], [ 0, %7 ]
  %27 = phi i8* [ %51, %50 ], [ %22, %7 ]
  %28 = mul nuw nsw i64 %26, %23
  br label %29

29:                                               ; preds = %29, %25
  %30 = phi i64 [ %48, %29 ], [ 0, %25 ]
  %31 = add nuw nsw i64 %30, %28
  %32 = getelementptr inbounds i16, i16* %8, i64 %31
  %33 = bitcast i16* %32 to <8 x i16>*
  %34 = load <8 x i16>, <8 x i16>* %33, align 1
  %35 = getelementptr inbounds i8, i8* %27, i64 %30
  %36 = bitcast i8* %35 to i64*
  %37 = load i64, i64* %36, align 1
  %38 = insertelement <2 x i64> undef, i64 %37, i32 0
  %39 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %34, <8 x i16> <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>) #8
  %40 = ashr <8 x i16> %39, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %41 = bitcast <2 x i64> %38 to <16 x i8>
  %42 = shufflevector <16 x i8> %41, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %43 = zext <8 x i8> %42 to <8 x i16>
  %44 = add nsw <8 x i16> %40, %43
  %45 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %44, <8 x i16> undef) #8
  %46 = bitcast <16 x i8> %45 to <2 x i64>
  %47 = extractelement <2 x i64> %46, i32 0
  store i64 %47, i64* %36, align 1
  %48 = add nuw nsw i64 %30, 8
  %49 = icmp ult i64 %48, %23
  br i1 %49, label %29, label %50

50:                                               ; preds = %29
  %51 = getelementptr inbounds i8, i8* %27, i64 %18
  %52 = add nuw nsw i64 %26, 1
  %53 = icmp slt i64 %52, %24
  br i1 %53, label %25, label %54

54:                                               ; preds = %50
  ret void
}

; Function Attrs: norecurse nounwind readnone ssp uwtable
define internal void @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_127Wht4TransformLoopRow_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_(i8 zeroext, i8 zeroext, i32, i8* nocapture, i32, i32, i8* nocapture) #5 {
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define internal void @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_130Wht4TransformLoopColumn_SSE4_1ENS_13TransformTypeENS_13TransformSizeEiPviiS5_(i8 zeroext, i8 zeroext, i32, i8* nocapture readonly, i32, i32, i8* nocapture readonly) #3 {
  %8 = bitcast i8* %6 to i64*
  %9 = load i64, i64* %8, align 8
  %10 = getelementptr inbounds i8, i8* %6, i64 8
  %11 = bitcast i8* %10 to i8**
  %12 = load i8*, i8** %11, align 8
  %13 = icmp eq i32 %2, 1
  br i1 %13, label %14, label %35

14:                                               ; preds = %7
  %15 = bitcast i8* %3 to i16*
  %16 = load i16, i16* %15, align 2
  %17 = sext i16 %16 to i32
  %18 = ashr i32 %17, 2
  %19 = ashr i32 %17, 3
  %20 = sub nsw i32 %18, %19
  %21 = ashr i32 %20, 1
  %22 = trunc i32 %21 to i16
  %23 = sub nsw i32 %20, %21
  %24 = trunc i32 %23 to i16
  %25 = ashr i32 %17, 4
  %26 = sub nsw i32 %19, %25
  %27 = trunc i32 %26 to i16
  %28 = trunc i32 %25 to i16
  %29 = insertelement <8 x i16> undef, i16 %27, i32 0
  %30 = shufflevector <8 x i16> %29, <8 x i16> undef, <8 x i32> <i32 undef, i32 0, i32 0, i32 0, i32 0, i32 0, i32 0, i32 0>
  %31 = insertelement <8 x i16> %30, i16 %24, i64 0
  %32 = insertelement <8 x i16> undef, i16 %28, i32 0
  %33 = shufflevector <8 x i16> %32, <8 x i16> undef, <8 x i32> <i32 undef, i32 0, i32 0, i32 0, i32 0, i32 0, i32 0, i32 0>
  %34 = insertelement <8 x i16> %33, i16 %22, i64 0
  br label %103

35:                                               ; preds = %7
  %36 = bitcast i8* %3 to i64*
  %37 = load i64, i64* %36, align 1
  %38 = insertelement <2 x i64> undef, i64 %37, i32 0
  %39 = getelementptr inbounds i8, i8* %3, i64 8
  %40 = bitcast i8* %39 to i64*
  %41 = load i64, i64* %40, align 1
  %42 = insertelement <2 x i64> undef, i64 %41, i32 0
  %43 = getelementptr inbounds i8, i8* %3, i64 16
  %44 = bitcast i8* %43 to i64*
  %45 = load i64, i64* %44, align 1
  %46 = insertelement <2 x i64> undef, i64 %45, i32 0
  %47 = getelementptr inbounds i8, i8* %3, i64 24
  %48 = bitcast i8* %47 to i64*
  %49 = load i64, i64* %48, align 1
  %50 = insertelement <2 x i64> undef, i64 %49, i32 0
  %51 = bitcast <2 x i64> %38 to <8 x i16>
  %52 = bitcast <2 x i64> %50 to <8 x i16>
  %53 = shufflevector <8 x i16> %51, <8 x i16> %52, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %54 = bitcast <2 x i64> %42 to <8 x i16>
  %55 = bitcast <2 x i64> %46 to <8 x i16>
  %56 = shufflevector <8 x i16> %54, <8 x i16> %55, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %57 = bitcast <8 x i16> %53 to <4 x i32>
  %58 = bitcast <8 x i16> %56 to <4 x i32>
  %59 = shufflevector <4 x i32> %57, <4 x i32> %58, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %60 = shufflevector <4 x i32> %57, <4 x i32> %58, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %61 = bitcast <4 x i32> %59 to <16 x i8>
  %62 = shufflevector <16 x i8> %61, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %63 = bitcast <4 x i32> %60 to <16 x i8>
  %64 = shufflevector <16 x i8> %63, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %65 = bitcast <4 x i32> %59 to <8 x i16>
  %66 = ashr <8 x i16> %65, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %67 = bitcast <16 x i8> %62 to <8 x i16>
  %68 = ashr <8 x i16> %67, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %69 = bitcast <4 x i32> %60 to <8 x i16>
  %70 = ashr <8 x i16> %69, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %71 = bitcast <16 x i8> %64 to <8 x i16>
  %72 = ashr <8 x i16> %71, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %73 = add nsw <8 x i16> %68, %66
  %74 = sub nsw <8 x i16> %70, %72
  %75 = sub nsw <8 x i16> %73, %74
  %76 = ashr <8 x i16> %75, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %77 = sub nsw <8 x i16> %76, %72
  %78 = sub nsw <8 x i16> %76, %68
  %79 = sub <8 x i16> %73, %77
  %80 = add <8 x i16> %78, %74
  %81 = shufflevector <8 x i16> %79, <8 x i16> %77, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %82 = shufflevector <8 x i16> %78, <8 x i16> %80, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %83 = bitcast <8 x i16> %81 to <4 x i32>
  %84 = bitcast <8 x i16> %82 to <4 x i32>
  %85 = shufflevector <4 x i32> %83, <4 x i32> %84, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %86 = shufflevector <4 x i32> %83, <4 x i32> %84, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %87 = bitcast <4 x i32> %85 to <16 x i8>
  %88 = shufflevector <16 x i8> %87, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %89 = bitcast <4 x i32> %86 to <16 x i8>
  %90 = shufflevector <16 x i8> %89, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %91 = bitcast <4 x i32> %85 to <8 x i16>
  %92 = bitcast <4 x i32> %86 to <8 x i16>
  %93 = add <8 x i16> %92, %91
  %94 = bitcast <16 x i8> %90 to <8 x i16>
  %95 = bitcast <16 x i8> %88 to <8 x i16>
  %96 = sub <8 x i16> %94, %95
  %97 = sub <8 x i16> %93, %96
  %98 = ashr <8 x i16> %97, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %99 = sub <8 x i16> %98, %95
  %100 = sub <8 x i16> %98, %92
  %101 = sub <8 x i16> %93, %99
  %102 = add <8 x i16> %100, %96
  br label %103

103:                                              ; preds = %14, %35
  %104 = phi <8 x i16> [ %102, %35 ], [ %34, %14 ]
  %105 = phi <8 x i16> [ %100, %35 ], [ %34, %14 ]
  %106 = phi <8 x i16> [ %99, %35 ], [ %34, %14 ]
  %107 = phi <8 x i16> [ %101, %35 ], [ %31, %14 ]
  %108 = sext i32 %5 to i64
  %109 = ashr i64 %9, 32
  %110 = mul nsw i64 %109, %108
  %111 = getelementptr inbounds i8, i8* %12, i64 %110
  %112 = sext i32 %4 to i64
  %113 = getelementptr inbounds i8, i8* %111, i64 %112
  %114 = bitcast i8* %113 to i32*
  %115 = load i32, i32* %114, align 1
  %116 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %115, i32 0
  %117 = bitcast <4 x i32> %116 to <16 x i8>
  %118 = shufflevector <16 x i8> %117, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %119 = zext <8 x i8> %118 to <8 x i16>
  %120 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %119, <8 x i16> %107) #8
  %121 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %120, <8 x i16> undef) #8
  %122 = bitcast <16 x i8> %121 to <4 x i32>
  %123 = extractelement <4 x i32> %122, i32 0
  store i32 %123, i32* %114, align 1
  %124 = getelementptr inbounds i8, i8* %113, i64 %109
  %125 = bitcast i8* %124 to i32*
  %126 = load i32, i32* %125, align 1
  %127 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %126, i32 0
  %128 = bitcast <4 x i32> %127 to <16 x i8>
  %129 = shufflevector <16 x i8> %128, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %130 = zext <8 x i8> %129 to <8 x i16>
  %131 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %130, <8 x i16> %106) #8
  %132 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %131, <8 x i16> undef) #8
  %133 = bitcast <16 x i8> %132 to <4 x i32>
  %134 = extractelement <4 x i32> %133, i32 0
  store i32 %134, i32* %125, align 1
  %135 = getelementptr inbounds i8, i8* %124, i64 %109
  %136 = bitcast i8* %135 to i32*
  %137 = load i32, i32* %136, align 1
  %138 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %137, i32 0
  %139 = bitcast <4 x i32> %138 to <16 x i8>
  %140 = shufflevector <16 x i8> %139, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %141 = zext <8 x i8> %140 to <8 x i16>
  %142 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %141, <8 x i16> %105) #8
  %143 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %142, <8 x i16> undef) #8
  %144 = bitcast <16 x i8> %143 to <4 x i32>
  %145 = extractelement <4 x i32> %144, i32 0
  store i32 %145, i32* %136, align 1
  %146 = getelementptr inbounds i8, i8* %135, i64 %109
  %147 = bitcast i8* %146 to i32*
  %148 = load i32, i32* %147, align 1
  %149 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %148, i32 0
  %150 = bitcast <4 x i32> %149 to <16 x i8>
  %151 = shufflevector <16 x i8> %150, <16 x i8> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %152 = zext <8 x i8> %151 to <8 x i16>
  %153 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %152, <8 x i16> %104) #8
  %154 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %153, <8 x i16> undef) #8
  %155 = bitcast <16 x i8> %154 to <4 x i32>
  %156 = extractelement <4 x i32> %155, i32 0
  store i32 %156, i32* %147, align 1
  ret void
}

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #1

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16>, <8 x i16>) #6

; Function Attrs: nounwind readnone
declare <16 x i8> @llvm.x86.sse41.pblendvb(<16 x i8>, <16 x i8>, <16 x i8>) #6

; Function Attrs: nounwind readnone
declare <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32>, <4 x i32>) #6

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32>, <4 x i32>) #6

; Function Attrs: argmemonly nounwind
declare void @llvm.memset.p0i8.i64(i8* nocapture writeonly, i8, i64, i1 immarg) #1

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16>, <8 x i16>) #6

; Function Attrs: nounwind readnone
declare <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16>, <8 x i16>) #6

; Function Attrs: nounwind readnone
declare <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32>, i32) #6

; Function Attrs: nounwind readnone speculatable
declare <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16>, <8 x i16>) #7

; Function Attrs: nounwind readnone speculatable
declare <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16>, <8 x i16>) #7

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16>, <8 x i16>) #6

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16>, <8 x i16>) #6

; Function Attrs: argmemonly nounwind
declare void @llvm.memcpy.p0i8.p0i8.i64(i8* nocapture writeonly, i8* nocapture readonly, i64, i1 immarg) #1

; Function Attrs: nounwind readnone
declare <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16>, <8 x i16>) #6

; Function Attrs: nounwind ssp uwtable
define internal fastcc void @_ZN7libgav13dsp12low_bitdepth12_GLOBAL__N_112Dct64_SSE4_1EPvib(i8*, i32, i1 zeroext) unnamed_addr #4 {
  %4 = alloca [64 x <2 x i64>], align 16
  %5 = alloca [32 x <2 x i64>], align 16
  %6 = bitcast i8* %0 to i16*
  %7 = bitcast [64 x <2 x i64>]* %4 to i8*
  call void @llvm.lifetime.start.p0i8(i64 1024, i8* nonnull %7) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %7, i8 -86, i64 1024, i1 false)
  %8 = bitcast [32 x <2 x i64>]* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 512, i8* nonnull %8) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %8, i8 -86, i64 512, i1 false)
  br i1 %2, label %9, label %100

9:                                                ; preds = %3
  %10 = sext i32 %1 to i64
  %11 = shl nsw i64 %10, 1
  %12 = mul nsw i64 %10, 3
  %13 = shl nsw i64 %10, 2
  %14 = mul nsw i64 %10, 5
  %15 = mul nsw i64 %10, 6
  %16 = mul nsw i64 %10, 7
  br label %17

17:                                               ; preds = %9, %17
  %18 = phi i64 [ 0, %9 ], [ %98, %17 ]
  %19 = getelementptr inbounds i16, i16* %6, i64 %18
  %20 = bitcast i16* %19 to <8 x i16>*
  %21 = load <8 x i16>, <8 x i16>* %20, align 1
  %22 = add nsw i64 %18, %10
  %23 = getelementptr inbounds i16, i16* %6, i64 %22
  %24 = bitcast i16* %23 to <8 x i16>*
  %25 = load <8 x i16>, <8 x i16>* %24, align 1
  %26 = add nsw i64 %11, %18
  %27 = getelementptr inbounds i16, i16* %6, i64 %26
  %28 = bitcast i16* %27 to <8 x i16>*
  %29 = load <8 x i16>, <8 x i16>* %28, align 1
  %30 = add nsw i64 %12, %18
  %31 = getelementptr inbounds i16, i16* %6, i64 %30
  %32 = bitcast i16* %31 to <8 x i16>*
  %33 = load <8 x i16>, <8 x i16>* %32, align 1
  %34 = add nsw i64 %13, %18
  %35 = getelementptr inbounds i16, i16* %6, i64 %34
  %36 = bitcast i16* %35 to <8 x i16>*
  %37 = load <8 x i16>, <8 x i16>* %36, align 1
  %38 = add nsw i64 %14, %18
  %39 = getelementptr inbounds i16, i16* %6, i64 %38
  %40 = bitcast i16* %39 to <8 x i16>*
  %41 = load <8 x i16>, <8 x i16>* %40, align 1
  %42 = add nsw i64 %15, %18
  %43 = getelementptr inbounds i16, i16* %6, i64 %42
  %44 = bitcast i16* %43 to <8 x i16>*
  %45 = load <8 x i16>, <8 x i16>* %44, align 1
  %46 = add nsw i64 %16, %18
  %47 = getelementptr inbounds i16, i16* %6, i64 %46
  %48 = bitcast i16* %47 to <8 x i16>*
  %49 = load <8 x i16>, <8 x i16>* %48, align 1
  %50 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 %18
  %51 = shufflevector <8 x i16> %21, <8 x i16> %25, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %52 = shufflevector <8 x i16> %29, <8 x i16> %33, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %53 = shufflevector <8 x i16> %37, <8 x i16> %41, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %54 = shufflevector <8 x i16> %45, <8 x i16> %49, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %55 = shufflevector <8 x i16> %21, <8 x i16> %25, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %56 = shufflevector <8 x i16> %29, <8 x i16> %33, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %57 = shufflevector <8 x i16> %37, <8 x i16> %41, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %58 = shufflevector <8 x i16> %45, <8 x i16> %49, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %59 = bitcast <8 x i16> %51 to <4 x i32>
  %60 = bitcast <8 x i16> %52 to <4 x i32>
  %61 = shufflevector <4 x i32> %59, <4 x i32> %60, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %62 = bitcast <4 x i32> %61 to <2 x i64>
  %63 = bitcast <8 x i16> %53 to <4 x i32>
  %64 = bitcast <8 x i16> %54 to <4 x i32>
  %65 = shufflevector <4 x i32> %63, <4 x i32> %64, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %66 = bitcast <4 x i32> %65 to <2 x i64>
  %67 = bitcast <8 x i16> %55 to <4 x i32>
  %68 = bitcast <8 x i16> %56 to <4 x i32>
  %69 = shufflevector <4 x i32> %67, <4 x i32> %68, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %70 = bitcast <4 x i32> %69 to <2 x i64>
  %71 = bitcast <8 x i16> %57 to <4 x i32>
  %72 = bitcast <8 x i16> %58 to <4 x i32>
  %73 = shufflevector <4 x i32> %71, <4 x i32> %72, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %74 = bitcast <4 x i32> %73 to <2 x i64>
  %75 = shufflevector <4 x i32> %59, <4 x i32> %60, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %76 = bitcast <4 x i32> %75 to <2 x i64>
  %77 = shufflevector <4 x i32> %63, <4 x i32> %64, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %78 = bitcast <4 x i32> %77 to <2 x i64>
  %79 = shufflevector <4 x i32> %67, <4 x i32> %68, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %80 = bitcast <4 x i32> %79 to <2 x i64>
  %81 = shufflevector <4 x i32> %71, <4 x i32> %72, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %82 = bitcast <4 x i32> %81 to <2 x i64>
  %83 = shufflevector <2 x i64> %62, <2 x i64> %66, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %83, <2 x i64>* %50, align 16
  %84 = shufflevector <2 x i64> %62, <2 x i64> %66, <2 x i32> <i32 1, i32 3>
  %85 = getelementptr inbounds <2 x i64>, <2 x i64>* %50, i64 1
  store <2 x i64> %84, <2 x i64>* %85, align 16
  %86 = shufflevector <2 x i64> %76, <2 x i64> %78, <2 x i32> <i32 0, i32 2>
  %87 = getelementptr inbounds <2 x i64>, <2 x i64>* %50, i64 2
  store <2 x i64> %86, <2 x i64>* %87, align 16
  %88 = shufflevector <2 x i64> %76, <2 x i64> %78, <2 x i32> <i32 1, i32 3>
  %89 = getelementptr inbounds <2 x i64>, <2 x i64>* %50, i64 3
  store <2 x i64> %88, <2 x i64>* %89, align 16
  %90 = shufflevector <2 x i64> %70, <2 x i64> %74, <2 x i32> <i32 0, i32 2>
  %91 = getelementptr inbounds <2 x i64>, <2 x i64>* %50, i64 4
  store <2 x i64> %90, <2 x i64>* %91, align 16
  %92 = shufflevector <2 x i64> %70, <2 x i64> %74, <2 x i32> <i32 1, i32 3>
  %93 = getelementptr inbounds <2 x i64>, <2 x i64>* %50, i64 5
  store <2 x i64> %92, <2 x i64>* %93, align 16
  %94 = shufflevector <2 x i64> %80, <2 x i64> %82, <2 x i32> <i32 0, i32 2>
  %95 = getelementptr inbounds <2 x i64>, <2 x i64>* %50, i64 6
  store <2 x i64> %94, <2 x i64>* %95, align 16
  %96 = shufflevector <2 x i64> %80, <2 x i64> %82, <2 x i32> <i32 1, i32 3>
  %97 = getelementptr inbounds <2 x i64>, <2 x i64>* %50, i64 7
  store <2 x i64> %96, <2 x i64>* %97, align 16
  %98 = add nuw nsw i64 %18, 8
  %99 = icmp ult i64 %98, 32
  br i1 %99, label %17, label %259

100:                                              ; preds = %3
  %101 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 0
  %102 = sext i32 %1 to i64
  %103 = bitcast i8* %0 to <2 x i64>*
  %104 = load <2 x i64>, <2 x i64>* %103, align 1
  store <2 x i64> %104, <2 x i64>* %101, align 16
  %105 = getelementptr inbounds i16, i16* %6, i64 %102
  %106 = bitcast i16* %105 to <2 x i64>*
  %107 = load <2 x i64>, <2 x i64>* %106, align 1
  %108 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 1
  store <2 x i64> %107, <2 x i64>* %108, align 16
  %109 = shl nsw i64 %102, 1
  %110 = getelementptr inbounds i16, i16* %6, i64 %109
  %111 = bitcast i16* %110 to <2 x i64>*
  %112 = load <2 x i64>, <2 x i64>* %111, align 1
  %113 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 2
  store <2 x i64> %112, <2 x i64>* %113, align 16
  %114 = mul nsw i64 %102, 3
  %115 = getelementptr inbounds i16, i16* %6, i64 %114
  %116 = bitcast i16* %115 to <2 x i64>*
  %117 = load <2 x i64>, <2 x i64>* %116, align 1
  %118 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 3
  store <2 x i64> %117, <2 x i64>* %118, align 16
  %119 = shl nsw i64 %102, 2
  %120 = getelementptr inbounds i16, i16* %6, i64 %119
  %121 = bitcast i16* %120 to <2 x i64>*
  %122 = load <2 x i64>, <2 x i64>* %121, align 1
  %123 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 4
  store <2 x i64> %122, <2 x i64>* %123, align 16
  %124 = mul nsw i64 %102, 5
  %125 = getelementptr inbounds i16, i16* %6, i64 %124
  %126 = bitcast i16* %125 to <2 x i64>*
  %127 = load <2 x i64>, <2 x i64>* %126, align 1
  %128 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 5
  store <2 x i64> %127, <2 x i64>* %128, align 16
  %129 = mul nsw i64 %102, 6
  %130 = getelementptr inbounds i16, i16* %6, i64 %129
  %131 = bitcast i16* %130 to <2 x i64>*
  %132 = load <2 x i64>, <2 x i64>* %131, align 1
  %133 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 6
  store <2 x i64> %132, <2 x i64>* %133, align 16
  %134 = mul nsw i64 %102, 7
  %135 = getelementptr inbounds i16, i16* %6, i64 %134
  %136 = bitcast i16* %135 to <2 x i64>*
  %137 = load <2 x i64>, <2 x i64>* %136, align 1
  %138 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 7
  store <2 x i64> %137, <2 x i64>* %138, align 16
  %139 = shl nsw i64 %102, 3
  %140 = getelementptr inbounds i16, i16* %6, i64 %139
  %141 = bitcast i16* %140 to <2 x i64>*
  %142 = load <2 x i64>, <2 x i64>* %141, align 1
  %143 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 8
  store <2 x i64> %142, <2 x i64>* %143, align 16
  %144 = mul nsw i64 %102, 9
  %145 = getelementptr inbounds i16, i16* %6, i64 %144
  %146 = bitcast i16* %145 to <2 x i64>*
  %147 = load <2 x i64>, <2 x i64>* %146, align 1
  %148 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 9
  store <2 x i64> %147, <2 x i64>* %148, align 16
  %149 = mul nsw i64 %102, 10
  %150 = getelementptr inbounds i16, i16* %6, i64 %149
  %151 = bitcast i16* %150 to <2 x i64>*
  %152 = load <2 x i64>, <2 x i64>* %151, align 1
  %153 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 10
  store <2 x i64> %152, <2 x i64>* %153, align 16
  %154 = mul nsw i64 %102, 11
  %155 = getelementptr inbounds i16, i16* %6, i64 %154
  %156 = bitcast i16* %155 to <2 x i64>*
  %157 = load <2 x i64>, <2 x i64>* %156, align 1
  %158 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 11
  store <2 x i64> %157, <2 x i64>* %158, align 16
  %159 = mul nsw i64 %102, 12
  %160 = getelementptr inbounds i16, i16* %6, i64 %159
  %161 = bitcast i16* %160 to <2 x i64>*
  %162 = load <2 x i64>, <2 x i64>* %161, align 1
  %163 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 12
  store <2 x i64> %162, <2 x i64>* %163, align 16
  %164 = mul nsw i64 %102, 13
  %165 = getelementptr inbounds i16, i16* %6, i64 %164
  %166 = bitcast i16* %165 to <2 x i64>*
  %167 = load <2 x i64>, <2 x i64>* %166, align 1
  %168 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 13
  store <2 x i64> %167, <2 x i64>* %168, align 16
  %169 = mul nsw i64 %102, 14
  %170 = getelementptr inbounds i16, i16* %6, i64 %169
  %171 = bitcast i16* %170 to <2 x i64>*
  %172 = load <2 x i64>, <2 x i64>* %171, align 1
  %173 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 14
  store <2 x i64> %172, <2 x i64>* %173, align 16
  %174 = mul nsw i64 %102, 15
  %175 = getelementptr inbounds i16, i16* %6, i64 %174
  %176 = bitcast i16* %175 to <2 x i64>*
  %177 = load <2 x i64>, <2 x i64>* %176, align 1
  %178 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 15
  store <2 x i64> %177, <2 x i64>* %178, align 16
  %179 = shl nsw i64 %102, 4
  %180 = getelementptr inbounds i16, i16* %6, i64 %179
  %181 = bitcast i16* %180 to <2 x i64>*
  %182 = load <2 x i64>, <2 x i64>* %181, align 1
  %183 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 16
  store <2 x i64> %182, <2 x i64>* %183, align 16
  %184 = mul nsw i64 %102, 17
  %185 = getelementptr inbounds i16, i16* %6, i64 %184
  %186 = bitcast i16* %185 to <2 x i64>*
  %187 = load <2 x i64>, <2 x i64>* %186, align 1
  %188 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 17
  store <2 x i64> %187, <2 x i64>* %188, align 16
  %189 = mul nsw i64 %102, 18
  %190 = getelementptr inbounds i16, i16* %6, i64 %189
  %191 = bitcast i16* %190 to <2 x i64>*
  %192 = load <2 x i64>, <2 x i64>* %191, align 1
  %193 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 18
  store <2 x i64> %192, <2 x i64>* %193, align 16
  %194 = mul nsw i64 %102, 19
  %195 = getelementptr inbounds i16, i16* %6, i64 %194
  %196 = bitcast i16* %195 to <2 x i64>*
  %197 = load <2 x i64>, <2 x i64>* %196, align 1
  %198 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 19
  store <2 x i64> %197, <2 x i64>* %198, align 16
  %199 = mul nsw i64 %102, 20
  %200 = getelementptr inbounds i16, i16* %6, i64 %199
  %201 = bitcast i16* %200 to <2 x i64>*
  %202 = load <2 x i64>, <2 x i64>* %201, align 1
  %203 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 20
  store <2 x i64> %202, <2 x i64>* %203, align 16
  %204 = mul nsw i64 %102, 21
  %205 = getelementptr inbounds i16, i16* %6, i64 %204
  %206 = bitcast i16* %205 to <2 x i64>*
  %207 = load <2 x i64>, <2 x i64>* %206, align 1
  %208 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 21
  store <2 x i64> %207, <2 x i64>* %208, align 16
  %209 = mul nsw i64 %102, 22
  %210 = getelementptr inbounds i16, i16* %6, i64 %209
  %211 = bitcast i16* %210 to <2 x i64>*
  %212 = load <2 x i64>, <2 x i64>* %211, align 1
  %213 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 22
  store <2 x i64> %212, <2 x i64>* %213, align 16
  %214 = mul nsw i64 %102, 23
  %215 = getelementptr inbounds i16, i16* %6, i64 %214
  %216 = bitcast i16* %215 to <2 x i64>*
  %217 = load <2 x i64>, <2 x i64>* %216, align 1
  %218 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 23
  store <2 x i64> %217, <2 x i64>* %218, align 16
  %219 = mul nsw i64 %102, 24
  %220 = getelementptr inbounds i16, i16* %6, i64 %219
  %221 = bitcast i16* %220 to <2 x i64>*
  %222 = load <2 x i64>, <2 x i64>* %221, align 1
  %223 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 24
  store <2 x i64> %222, <2 x i64>* %223, align 16
  %224 = mul nsw i64 %102, 25
  %225 = getelementptr inbounds i16, i16* %6, i64 %224
  %226 = bitcast i16* %225 to <2 x i64>*
  %227 = load <2 x i64>, <2 x i64>* %226, align 1
  %228 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 25
  store <2 x i64> %227, <2 x i64>* %228, align 16
  %229 = mul nsw i64 %102, 26
  %230 = getelementptr inbounds i16, i16* %6, i64 %229
  %231 = bitcast i16* %230 to <2 x i64>*
  %232 = load <2 x i64>, <2 x i64>* %231, align 1
  %233 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 26
  store <2 x i64> %232, <2 x i64>* %233, align 16
  %234 = mul nsw i64 %102, 27
  %235 = getelementptr inbounds i16, i16* %6, i64 %234
  %236 = bitcast i16* %235 to <2 x i64>*
  %237 = load <2 x i64>, <2 x i64>* %236, align 1
  %238 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 27
  store <2 x i64> %237, <2 x i64>* %238, align 16
  %239 = mul nsw i64 %102, 28
  %240 = getelementptr inbounds i16, i16* %6, i64 %239
  %241 = bitcast i16* %240 to <2 x i64>*
  %242 = load <2 x i64>, <2 x i64>* %241, align 1
  %243 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 28
  store <2 x i64> %242, <2 x i64>* %243, align 16
  %244 = mul nsw i64 %102, 29
  %245 = getelementptr inbounds i16, i16* %6, i64 %244
  %246 = bitcast i16* %245 to <2 x i64>*
  %247 = load <2 x i64>, <2 x i64>* %246, align 1
  %248 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 29
  store <2 x i64> %247, <2 x i64>* %248, align 16
  %249 = mul nsw i64 %102, 30
  %250 = getelementptr inbounds i16, i16* %6, i64 %249
  %251 = bitcast i16* %250 to <2 x i64>*
  %252 = load <2 x i64>, <2 x i64>* %251, align 1
  %253 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 30
  store <2 x i64> %252, <2 x i64>* %253, align 16
  %254 = mul nsw i64 %102, 31
  %255 = getelementptr inbounds i16, i16* %6, i64 %254
  %256 = bitcast i16* %255 to <2 x i64>*
  %257 = load <2 x i64>, <2 x i64>* %256, align 1
  %258 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 31
  store <2 x i64> %257, <2 x i64>* %258, align 16
  br label %284

259:                                              ; preds = %17
  %260 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 16
  %261 = load <2 x i64>, <2 x i64>* %260, align 16
  %262 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 24
  %263 = load <2 x i64>, <2 x i64>* %262, align 16
  %264 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 20
  %265 = load <2 x i64>, <2 x i64>* %264, align 16
  %266 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 28
  %267 = load <2 x i64>, <2 x i64>* %266, align 16
  %268 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 18
  %269 = load <2 x i64>, <2 x i64>* %268, align 16
  %270 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 26
  %271 = load <2 x i64>, <2 x i64>* %270, align 16
  %272 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 22
  %273 = load <2 x i64>, <2 x i64>* %272, align 16
  %274 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 30
  %275 = load <2 x i64>, <2 x i64>* %274, align 16
  %276 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 17
  %277 = load <2 x i64>, <2 x i64>* %276, align 16
  %278 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 25
  %279 = load <2 x i64>, <2 x i64>* %278, align 16
  %280 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 21
  %281 = load <2 x i64>, <2 x i64>* %280, align 16
  %282 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 29
  %283 = load <2 x i64>, <2 x i64>* %282, align 16
  br label %284

284:                                              ; preds = %259, %100
  %285 = phi <2 x i64> [ %283, %259 ], [ %247, %100 ]
  %286 = phi <2 x i64> [ %281, %259 ], [ %207, %100 ]
  %287 = phi <2 x i64> [ %279, %259 ], [ %227, %100 ]
  %288 = phi <2 x i64> [ %277, %259 ], [ %187, %100 ]
  %289 = phi <2 x i64> [ %275, %259 ], [ %252, %100 ]
  %290 = phi <2 x i64> [ %273, %259 ], [ %212, %100 ]
  %291 = phi <2 x i64> [ %271, %259 ], [ %232, %100 ]
  %292 = phi <2 x i64> [ %269, %259 ], [ %192, %100 ]
  %293 = phi <2 x i64> [ %267, %259 ], [ %242, %100 ]
  %294 = phi <2 x i64> [ %265, %259 ], [ %202, %100 ]
  %295 = phi <2 x i64> [ %263, %259 ], [ %222, %100 ]
  %296 = phi <2 x i64> [ %261, %259 ], [ %182, %100 ]
  %297 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 0
  %298 = load <2 x i64>, <2 x i64>* %297, align 16
  %299 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 0
  store <2 x i64> %298, <2 x i64>* %299, align 16
  %300 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 2
  store <2 x i64> %296, <2 x i64>* %300, align 16
  %301 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 8
  %302 = load <2 x i64>, <2 x i64>* %301, align 16
  %303 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 4
  store <2 x i64> %302, <2 x i64>* %303, align 16
  %304 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 6
  store <2 x i64> %295, <2 x i64>* %304, align 16
  %305 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 4
  %306 = load <2 x i64>, <2 x i64>* %305, align 16
  %307 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 8
  store <2 x i64> %306, <2 x i64>* %307, align 16
  %308 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 10
  store <2 x i64> %294, <2 x i64>* %308, align 16
  %309 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 12
  %310 = load <2 x i64>, <2 x i64>* %309, align 16
  %311 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 12
  store <2 x i64> %310, <2 x i64>* %311, align 16
  %312 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 14
  store <2 x i64> %293, <2 x i64>* %312, align 16
  %313 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 2
  %314 = load <2 x i64>, <2 x i64>* %313, align 16
  %315 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 16
  store <2 x i64> %314, <2 x i64>* %315, align 16
  %316 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 18
  store <2 x i64> %292, <2 x i64>* %316, align 16
  %317 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 10
  %318 = load <2 x i64>, <2 x i64>* %317, align 16
  %319 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 20
  store <2 x i64> %318, <2 x i64>* %319, align 16
  %320 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 22
  store <2 x i64> %291, <2 x i64>* %320, align 16
  %321 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 6
  %322 = load <2 x i64>, <2 x i64>* %321, align 16
  %323 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 24
  store <2 x i64> %322, <2 x i64>* %323, align 16
  %324 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 26
  store <2 x i64> %290, <2 x i64>* %324, align 16
  %325 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 14
  %326 = load <2 x i64>, <2 x i64>* %325, align 16
  %327 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 28
  store <2 x i64> %326, <2 x i64>* %327, align 16
  %328 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 30
  store <2 x i64> %289, <2 x i64>* %328, align 16
  %329 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 1
  %330 = load <2 x i64>, <2 x i64>* %329, align 16
  %331 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 32
  store <2 x i64> %330, <2 x i64>* %331, align 16
  %332 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 34
  store <2 x i64> %288, <2 x i64>* %332, align 16
  %333 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 9
  %334 = load <2 x i64>, <2 x i64>* %333, align 16
  %335 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 36
  store <2 x i64> %334, <2 x i64>* %335, align 16
  %336 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 38
  store <2 x i64> %287, <2 x i64>* %336, align 16
  %337 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 5
  %338 = load <2 x i64>, <2 x i64>* %337, align 16
  %339 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 40
  store <2 x i64> %338, <2 x i64>* %339, align 16
  %340 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 42
  store <2 x i64> %286, <2 x i64>* %340, align 16
  %341 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 13
  %342 = load <2 x i64>, <2 x i64>* %341, align 16
  %343 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 44
  store <2 x i64> %342, <2 x i64>* %343, align 16
  %344 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 46
  store <2 x i64> %285, <2 x i64>* %344, align 16
  %345 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 3
  %346 = load <2 x i64>, <2 x i64>* %345, align 16
  %347 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 48
  store <2 x i64> %346, <2 x i64>* %347, align 16
  %348 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 19
  %349 = load <2 x i64>, <2 x i64>* %348, align 16
  %350 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 50
  store <2 x i64> %349, <2 x i64>* %350, align 16
  %351 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 11
  %352 = load <2 x i64>, <2 x i64>* %351, align 16
  %353 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 52
  store <2 x i64> %352, <2 x i64>* %353, align 16
  %354 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 27
  %355 = load <2 x i64>, <2 x i64>* %354, align 16
  %356 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 54
  store <2 x i64> %355, <2 x i64>* %356, align 16
  %357 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 7
  %358 = load <2 x i64>, <2 x i64>* %357, align 16
  %359 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 56
  store <2 x i64> %358, <2 x i64>* %359, align 16
  %360 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 23
  %361 = load <2 x i64>, <2 x i64>* %360, align 16
  %362 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 58
  store <2 x i64> %361, <2 x i64>* %362, align 16
  %363 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 15
  %364 = load <2 x i64>, <2 x i64>* %363, align 16
  %365 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 60
  store <2 x i64> %364, <2 x i64>* %365, align 16
  %366 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 31
  %367 = load <2 x i64>, <2 x i64>* %366, align 16
  %368 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 62
  store <2 x i64> %367, <2 x i64>* %368, align 16
  %369 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 1
  %370 = bitcast [64 x <2 x i64>]* %4 to <8 x i16>*
  %371 = load <8 x i16>, <8 x i16>* %370, align 16
  %372 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %371, <8 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #8
  %373 = bitcast <2 x i64>* %369 to <8 x i16>*
  %374 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 3
  %375 = bitcast <2 x i64>* %300 to <8 x i16>*
  %376 = load <8 x i16>, <8 x i16>* %375, align 16
  %377 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %376, <8 x i16> <i16 12536, i16 12536, i16 12536, i16 12536, i16 12536, i16 12536, i16 12536, i16 12536>) #8
  %378 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %376, <8 x i16> <i16 30272, i16 30272, i16 30272, i16 30272, i16 30272, i16 30272, i16 30272, i16 30272>) #8
  %379 = bitcast <2 x i64>* %374 to <8 x i16>*
  %380 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %372, <8 x i16> %378) #8
  %381 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %372, <8 x i16> %378) #8
  %382 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %372, <8 x i16> %377) #8
  %383 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %372, <8 x i16> %377) #8
  %384 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 7
  %385 = bitcast <2 x i64>* %303 to <8 x i16>*
  %386 = load <8 x i16>, <8 x i16>* %385, align 16
  %387 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %386, <8 x i16> <i16 6392, i16 6392, i16 6392, i16 6392, i16 6392, i16 6392, i16 6392, i16 6392>) #8
  %388 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %386, <8 x i16> <i16 32136, i16 32136, i16 32136, i16 32136, i16 32136, i16 32136, i16 32136, i16 32136>) #8
  %389 = bitcast <2 x i64>* %384 to <8 x i16>*
  %390 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 5
  %391 = bitcast <2 x i64>* %304 to <8 x i16>*
  %392 = load <8 x i16>, <8 x i16>* %391, align 16
  %393 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %392, <8 x i16> <i16 -18208, i16 -18208, i16 -18208, i16 -18208, i16 -18208, i16 -18208, i16 -18208, i16 -18208>) #8
  %394 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %392, <8 x i16> <i16 27248, i16 27248, i16 27248, i16 27248, i16 27248, i16 27248, i16 27248, i16 27248>) #8
  %395 = bitcast <2 x i64>* %390 to <8 x i16>*
  %396 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %387, <8 x i16> %393) #8
  %397 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %387, <8 x i16> %393) #8
  %398 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %388, <8 x i16> %394) #8
  %399 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %388, <8 x i16> %394) #8
  %400 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %401 = shufflevector <8 x i16> %399, <8 x i16> %397, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %402 = shufflevector <8 x i16> %397, <8 x i16> %399, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %403 = shufflevector <8 x i16> %399, <8 x i16> %397, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %404 = shufflevector <8 x i16> %397, <8 x i16> %399, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %405 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %401, <8 x i16> %400) #8
  %406 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %402, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %407 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %403, <8 x i16> %400) #8
  %408 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %404, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %409 = add <4 x i32> %405, <i32 2048, i32 2048, i32 2048, i32 2048>
  %410 = ashr <4 x i32> %409, <i32 12, i32 12, i32 12, i32 12>
  %411 = add <4 x i32> %406, <i32 2048, i32 2048, i32 2048, i32 2048>
  %412 = ashr <4 x i32> %411, <i32 12, i32 12, i32 12, i32 12>
  %413 = add <4 x i32> %407, <i32 2048, i32 2048, i32 2048, i32 2048>
  %414 = ashr <4 x i32> %413, <i32 12, i32 12, i32 12, i32 12>
  %415 = add <4 x i32> %408, <i32 2048, i32 2048, i32 2048, i32 2048>
  %416 = ashr <4 x i32> %415, <i32 12, i32 12, i32 12, i32 12>
  %417 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %410, <4 x i32> %414) #8
  %418 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %412, <4 x i32> %416) #8
  %419 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %380, <8 x i16> %398) #8
  %420 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %380, <8 x i16> %398) #8
  store <8 x i16> %419, <8 x i16>* %370, align 16
  store <8 x i16> %420, <8 x i16>* %389, align 16
  %421 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %382, <8 x i16> %418) #8
  %422 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %382, <8 x i16> %418) #8
  store <8 x i16> %421, <8 x i16>* %373, align 16
  store <8 x i16> %422, <8 x i16>* %391, align 16
  %423 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %383, <8 x i16> %417) #8
  %424 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %383, <8 x i16> %417) #8
  store <8 x i16> %423, <8 x i16>* %375, align 16
  store <8 x i16> %424, <8 x i16>* %395, align 16
  %425 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %381, <8 x i16> %396) #8
  %426 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %381, <8 x i16> %396) #8
  store <8 x i16> %425, <8 x i16>* %379, align 16
  store <8 x i16> %426, <8 x i16>* %385, align 16
  %427 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 15
  %428 = bitcast <2 x i64>* %307 to <8 x i16>*
  %429 = load <8 x i16>, <8 x i16>* %428, align 16
  %430 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %429, <8 x i16> <i16 3208, i16 3208, i16 3208, i16 3208, i16 3208, i16 3208, i16 3208, i16 3208>) #8
  %431 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %429, <8 x i16> <i16 32608, i16 32608, i16 32608, i16 32608, i16 32608, i16 32608, i16 32608, i16 32608>) #8
  %432 = bitcast <2 x i64>* %427 to <8 x i16>*
  %433 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 9
  %434 = bitcast <2 x i64>* %312 to <8 x i16>*
  %435 = load <8 x i16>, <8 x i16>* %434, align 16
  %436 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %435, <8 x i16> <i16 -20784, i16 -20784, i16 -20784, i16 -20784, i16 -20784, i16 -20784, i16 -20784, i16 -20784>) #8
  %437 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %435, <8 x i16> <i16 25328, i16 25328, i16 25328, i16 25328, i16 25328, i16 25328, i16 25328, i16 25328>) #8
  %438 = bitcast <2 x i64>* %433 to <8 x i16>*
  %439 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 13
  %440 = bitcast <2 x i64>* %308 to <8 x i16>*
  %441 = load <8 x i16>, <8 x i16>* %440, align 16
  %442 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %441, <8 x i16> <i16 15448, i16 15448, i16 15448, i16 15448, i16 15448, i16 15448, i16 15448, i16 15448>) #8
  %443 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %441, <8 x i16> <i16 28896, i16 28896, i16 28896, i16 28896, i16 28896, i16 28896, i16 28896, i16 28896>) #8
  %444 = bitcast <2 x i64>* %439 to <8 x i16>*
  %445 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 11
  %446 = bitcast <2 x i64>* %311 to <8 x i16>*
  %447 = load <8 x i16>, <8 x i16>* %446, align 16
  %448 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %447, <8 x i16> <i16 -9512, i16 -9512, i16 -9512, i16 -9512, i16 -9512, i16 -9512, i16 -9512, i16 -9512>) #8
  %449 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %447, <8 x i16> <i16 31360, i16 31360, i16 31360, i16 31360, i16 31360, i16 31360, i16 31360, i16 31360>) #8
  %450 = bitcast <2 x i64>* %445 to <8 x i16>*
  %451 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %430, <8 x i16> %436) #8
  %452 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %430, <8 x i16> %436) #8
  %453 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %448, <8 x i16> %442) #8
  %454 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %448, <8 x i16> %442) #8
  %455 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %449, <8 x i16> %443) #8
  %456 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %449, <8 x i16> %443) #8
  %457 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %431, <8 x i16> %437) #8
  %458 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %431, <8 x i16> %437) #8
  %459 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %460 = shufflevector <8 x i16> %458, <8 x i16> %452, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %461 = shufflevector <8 x i16> %452, <8 x i16> %458, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %462 = shufflevector <8 x i16> %458, <8 x i16> %452, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %463 = shufflevector <8 x i16> %452, <8 x i16> %458, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %464 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %460, <8 x i16> %459) #8
  %465 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %461, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %466 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %462, <8 x i16> %459) #8
  %467 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %463, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %468 = add <4 x i32> %464, <i32 2048, i32 2048, i32 2048, i32 2048>
  %469 = ashr <4 x i32> %468, <i32 12, i32 12, i32 12, i32 12>
  %470 = add <4 x i32> %465, <i32 2048, i32 2048, i32 2048, i32 2048>
  %471 = ashr <4 x i32> %470, <i32 12, i32 12, i32 12, i32 12>
  %472 = add <4 x i32> %466, <i32 2048, i32 2048, i32 2048, i32 2048>
  %473 = ashr <4 x i32> %472, <i32 12, i32 12, i32 12, i32 12>
  %474 = add <4 x i32> %467, <i32 2048, i32 2048, i32 2048, i32 2048>
  %475 = ashr <4 x i32> %474, <i32 12, i32 12, i32 12, i32 12>
  %476 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %469, <4 x i32> %473) #8
  %477 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %471, <4 x i32> %475) #8
  %478 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %479 = shufflevector <8 x i16> %456, <8 x i16> %454, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %480 = shufflevector <8 x i16> %454, <8 x i16> %456, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %481 = shufflevector <8 x i16> %456, <8 x i16> %454, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %482 = shufflevector <8 x i16> %454, <8 x i16> %456, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %483 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %479, <8 x i16> %478) #8
  %484 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %480, <8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>) #8
  %485 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %481, <8 x i16> %478) #8
  %486 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %482, <8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>) #8
  %487 = add <4 x i32> %483, <i32 2048, i32 2048, i32 2048, i32 2048>
  %488 = ashr <4 x i32> %487, <i32 12, i32 12, i32 12, i32 12>
  %489 = add <4 x i32> %484, <i32 2048, i32 2048, i32 2048, i32 2048>
  %490 = ashr <4 x i32> %489, <i32 12, i32 12, i32 12, i32 12>
  %491 = add <4 x i32> %485, <i32 2048, i32 2048, i32 2048, i32 2048>
  %492 = ashr <4 x i32> %491, <i32 12, i32 12, i32 12, i32 12>
  %493 = add <4 x i32> %486, <i32 2048, i32 2048, i32 2048, i32 2048>
  %494 = ashr <4 x i32> %493, <i32 12, i32 12, i32 12, i32 12>
  %495 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %488, <4 x i32> %492) #8
  %496 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %490, <4 x i32> %494) #8
  %497 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %451, <8 x i16> %453) #8
  %498 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %451, <8 x i16> %453) #8
  %499 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %476, <8 x i16> %495) #8
  %500 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %476, <8 x i16> %495) #8
  %501 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %457, <8 x i16> %455) #8
  %502 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %457, <8 x i16> %455) #8
  %503 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %477, <8 x i16> %496) #8
  %504 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %477, <8 x i16> %496) #8
  %505 = shufflevector <8 x i16> %504, <8 x i16> %500, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %506 = shufflevector <8 x i16> %500, <8 x i16> %504, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %507 = shufflevector <8 x i16> %504, <8 x i16> %500, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %508 = shufflevector <8 x i16> %500, <8 x i16> %504, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %509 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %505, <8 x i16> %400) #8
  %510 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %506, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %511 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %507, <8 x i16> %400) #8
  %512 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %508, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %513 = add <4 x i32> %509, <i32 2048, i32 2048, i32 2048, i32 2048>
  %514 = ashr <4 x i32> %513, <i32 12, i32 12, i32 12, i32 12>
  %515 = add <4 x i32> %510, <i32 2048, i32 2048, i32 2048, i32 2048>
  %516 = ashr <4 x i32> %515, <i32 12, i32 12, i32 12, i32 12>
  %517 = add <4 x i32> %511, <i32 2048, i32 2048, i32 2048, i32 2048>
  %518 = ashr <4 x i32> %517, <i32 12, i32 12, i32 12, i32 12>
  %519 = add <4 x i32> %512, <i32 2048, i32 2048, i32 2048, i32 2048>
  %520 = ashr <4 x i32> %519, <i32 12, i32 12, i32 12, i32 12>
  %521 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %514, <4 x i32> %518) #8
  %522 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %516, <4 x i32> %520) #8
  %523 = shufflevector <8 x i16> %502, <8 x i16> %498, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %524 = shufflevector <8 x i16> %498, <8 x i16> %502, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %525 = shufflevector <8 x i16> %502, <8 x i16> %498, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %526 = shufflevector <8 x i16> %498, <8 x i16> %502, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %527 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %523, <8 x i16> %400) #8
  %528 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %524, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %529 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %525, <8 x i16> %400) #8
  %530 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %526, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %531 = add <4 x i32> %527, <i32 2048, i32 2048, i32 2048, i32 2048>
  %532 = ashr <4 x i32> %531, <i32 12, i32 12, i32 12, i32 12>
  %533 = add <4 x i32> %528, <i32 2048, i32 2048, i32 2048, i32 2048>
  %534 = ashr <4 x i32> %533, <i32 12, i32 12, i32 12, i32 12>
  %535 = add <4 x i32> %529, <i32 2048, i32 2048, i32 2048, i32 2048>
  %536 = ashr <4 x i32> %535, <i32 12, i32 12, i32 12, i32 12>
  %537 = add <4 x i32> %530, <i32 2048, i32 2048, i32 2048, i32 2048>
  %538 = ashr <4 x i32> %537, <i32 12, i32 12, i32 12, i32 12>
  %539 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %532, <4 x i32> %536) #8
  %540 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %534, <4 x i32> %538) #8
  %541 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %419, <8 x i16> %501) #8
  %542 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %419, <8 x i16> %501) #8
  store <8 x i16> %541, <8 x i16>* %370, align 16
  store <8 x i16> %542, <8 x i16>* %432, align 16
  %543 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %421, <8 x i16> %503) #8
  %544 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %421, <8 x i16> %503) #8
  store <8 x i16> %543, <8 x i16>* %373, align 16
  store <8 x i16> %544, <8 x i16>* %434, align 16
  %545 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %423, <8 x i16> %522) #8
  %546 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %423, <8 x i16> %522) #8
  store <8 x i16> %545, <8 x i16>* %375, align 16
  store <8 x i16> %546, <8 x i16>* %444, align 16
  %547 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %425, <8 x i16> %540) #8
  %548 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %425, <8 x i16> %540) #8
  store <8 x i16> %547, <8 x i16>* %379, align 16
  store <8 x i16> %548, <8 x i16>* %446, align 16
  %549 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %426, <8 x i16> %539) #8
  %550 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %426, <8 x i16> %539) #8
  store <8 x i16> %549, <8 x i16>* %385, align 16
  store <8 x i16> %550, <8 x i16>* %450, align 16
  %551 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %424, <8 x i16> %521) #8
  %552 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %424, <8 x i16> %521) #8
  store <8 x i16> %551, <8 x i16>* %395, align 16
  store <8 x i16> %552, <8 x i16>* %440, align 16
  %553 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %422, <8 x i16> %499) #8
  %554 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %422, <8 x i16> %499) #8
  store <8 x i16> %553, <8 x i16>* %391, align 16
  store <8 x i16> %554, <8 x i16>* %438, align 16
  %555 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %420, <8 x i16> %497) #8
  %556 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %420, <8 x i16> %497) #8
  store <8 x i16> %555, <8 x i16>* %389, align 16
  store <8 x i16> %556, <8 x i16>* %428, align 16
  %557 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 31
  %558 = bitcast <2 x i64>* %315 to <8 x i16>*
  %559 = load <8 x i16>, <8 x i16>* %558, align 16
  %560 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %559, <8 x i16> <i16 1608, i16 1608, i16 1608, i16 1608, i16 1608, i16 1608, i16 1608, i16 1608>) #8
  %561 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %559, <8 x i16> <i16 32728, i16 32728, i16 32728, i16 32728, i16 32728, i16 32728, i16 32728, i16 32728>) #8
  %562 = bitcast <2 x i64>* %557 to <8 x i16>*
  %563 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 17
  %564 = bitcast <2 x i64>* %328 to <8 x i16>*
  %565 = load <8 x i16>, <8 x i16>* %564, align 16
  %566 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %565, <8 x i16> <i16 -22008, i16 -22008, i16 -22008, i16 -22008, i16 -22008, i16 -22008, i16 -22008, i16 -22008>) #8
  %567 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %565, <8 x i16> <i16 24280, i16 24280, i16 24280, i16 24280, i16 24280, i16 24280, i16 24280, i16 24280>) #8
  %568 = bitcast <2 x i64>* %563 to <8 x i16>*
  %569 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 29
  %570 = bitcast <2 x i64>* %316 to <8 x i16>*
  %571 = load <8 x i16>, <8 x i16>* %570, align 16
  %572 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %571, <8 x i16> <i16 14008, i16 14008, i16 14008, i16 14008, i16 14008, i16 14008, i16 14008, i16 14008>) #8
  %573 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %571, <8 x i16> <i16 29624, i16 29624, i16 29624, i16 29624, i16 29624, i16 29624, i16 29624, i16 29624>) #8
  %574 = bitcast <2 x i64>* %569 to <8 x i16>*
  %575 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 19
  %576 = bitcast <2 x i64>* %327 to <8 x i16>*
  %577 = load <8 x i16>, <8 x i16>* %576, align 16
  %578 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %577, <8 x i16> <i16 -11040, i16 -11040, i16 -11040, i16 -11040, i16 -11040, i16 -11040, i16 -11040, i16 -11040>) #8
  %579 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %577, <8 x i16> <i16 30856, i16 30856, i16 30856, i16 30856, i16 30856, i16 30856, i16 30856, i16 30856>) #8
  %580 = bitcast <2 x i64>* %575 to <8 x i16>*
  %581 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 27
  %582 = bitcast <2 x i64>* %319 to <8 x i16>*
  %583 = load <8 x i16>, <8 x i16>* %582, align 16
  %584 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %583, <8 x i16> <i16 7960, i16 7960, i16 7960, i16 7960, i16 7960, i16 7960, i16 7960, i16 7960>) #8
  %585 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %583, <8 x i16> <i16 31784, i16 31784, i16 31784, i16 31784, i16 31784, i16 31784, i16 31784, i16 31784>) #8
  %586 = bitcast <2 x i64>* %581 to <8 x i16>*
  %587 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 21
  %588 = bitcast <2 x i64>* %324 to <8 x i16>*
  %589 = load <8 x i16>, <8 x i16>* %588, align 16
  %590 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %589, <8 x i16> <i16 -16848, i16 -16848, i16 -16848, i16 -16848, i16 -16848, i16 -16848, i16 -16848, i16 -16848>) #8
  %591 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %589, <8 x i16> <i16 28104, i16 28104, i16 28104, i16 28104, i16 28104, i16 28104, i16 28104, i16 28104>) #8
  %592 = bitcast <2 x i64>* %587 to <8 x i16>*
  %593 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 25
  %594 = bitcast <2 x i64>* %320 to <8 x i16>*
  %595 = load <8 x i16>, <8 x i16>* %594, align 16
  %596 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %595, <8 x i16> <i16 19520, i16 19520, i16 19520, i16 19520, i16 19520, i16 19520, i16 19520, i16 19520>) #8
  %597 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %595, <8 x i16> <i16 26320, i16 26320, i16 26320, i16 26320, i16 26320, i16 26320, i16 26320, i16 26320>) #8
  %598 = bitcast <2 x i64>* %593 to <8 x i16>*
  %599 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 23
  %600 = bitcast <2 x i64>* %323 to <8 x i16>*
  %601 = load <8 x i16>, <8 x i16>* %600, align 16
  %602 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %601, <8 x i16> <i16 -4808, i16 -4808, i16 -4808, i16 -4808, i16 -4808, i16 -4808, i16 -4808, i16 -4808>) #8
  %603 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %601, <8 x i16> <i16 32416, i16 32416, i16 32416, i16 32416, i16 32416, i16 32416, i16 32416, i16 32416>) #8
  %604 = bitcast <2 x i64>* %599 to <8 x i16>*
  %605 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %560, <8 x i16> %566) #8
  %606 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %560, <8 x i16> %566) #8
  %607 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %578, <8 x i16> %572) #8
  %608 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %578, <8 x i16> %572) #8
  %609 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %584, <8 x i16> %590) #8
  %610 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %584, <8 x i16> %590) #8
  store <8 x i16> %609, <8 x i16>* %582, align 16
  %611 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %602, <8 x i16> %596) #8
  %612 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %602, <8 x i16> %596) #8
  %613 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %603, <8 x i16> %597) #8
  %614 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %603, <8 x i16> %597) #8
  %615 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %585, <8 x i16> %591) #8
  %616 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %585, <8 x i16> %591) #8
  store <8 x i16> %615, <8 x i16>* %586, align 16
  %617 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %579, <8 x i16> %573) #8
  %618 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %579, <8 x i16> %573) #8
  store <8 x i16> %617, <8 x i16>* %576, align 16
  %619 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %561, <8 x i16> %567) #8
  %620 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %561, <8 x i16> %567) #8
  %621 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %622 = shufflevector <8 x i16> %620, <8 x i16> %606, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %623 = shufflevector <8 x i16> %606, <8 x i16> %620, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %624 = shufflevector <8 x i16> %620, <8 x i16> %606, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %625 = shufflevector <8 x i16> %606, <8 x i16> %620, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %626 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %622, <8 x i16> %621) #8
  %627 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %623, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %628 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %624, <8 x i16> %621) #8
  %629 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %625, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %630 = add <4 x i32> %626, <i32 2048, i32 2048, i32 2048, i32 2048>
  %631 = ashr <4 x i32> %630, <i32 12, i32 12, i32 12, i32 12>
  %632 = add <4 x i32> %627, <i32 2048, i32 2048, i32 2048, i32 2048>
  %633 = ashr <4 x i32> %632, <i32 12, i32 12, i32 12, i32 12>
  %634 = add <4 x i32> %628, <i32 2048, i32 2048, i32 2048, i32 2048>
  %635 = ashr <4 x i32> %634, <i32 12, i32 12, i32 12, i32 12>
  %636 = add <4 x i32> %629, <i32 2048, i32 2048, i32 2048, i32 2048>
  %637 = ashr <4 x i32> %636, <i32 12, i32 12, i32 12, i32 12>
  %638 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %631, <4 x i32> %635) #8
  %639 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %633, <4 x i32> %637) #8
  %640 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 -4017, i16 799, i16 -4017, i16 799, i16 -4017, i16 799, i16 -4017, i16 799>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %641 = shufflevector <8 x i16> %618, <8 x i16> %608, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %642 = shufflevector <8 x i16> %608, <8 x i16> %618, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %643 = shufflevector <8 x i16> %618, <8 x i16> %608, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %644 = shufflevector <8 x i16> %608, <8 x i16> %618, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %645 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %641, <8 x i16> %640) #8
  %646 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %642, <8 x i16> <i16 -4017, i16 799, i16 -4017, i16 799, i16 -4017, i16 799, i16 -4017, i16 799>) #8
  %647 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %643, <8 x i16> %640) #8
  %648 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %644, <8 x i16> <i16 -4017, i16 799, i16 -4017, i16 799, i16 -4017, i16 799, i16 -4017, i16 799>) #8
  %649 = add <4 x i32> %645, <i32 2048, i32 2048, i32 2048, i32 2048>
  %650 = ashr <4 x i32> %649, <i32 12, i32 12, i32 12, i32 12>
  %651 = add <4 x i32> %646, <i32 2048, i32 2048, i32 2048, i32 2048>
  %652 = ashr <4 x i32> %651, <i32 12, i32 12, i32 12, i32 12>
  %653 = add <4 x i32> %647, <i32 2048, i32 2048, i32 2048, i32 2048>
  %654 = ashr <4 x i32> %653, <i32 12, i32 12, i32 12, i32 12>
  %655 = add <4 x i32> %648, <i32 2048, i32 2048, i32 2048, i32 2048>
  %656 = ashr <4 x i32> %655, <i32 12, i32 12, i32 12, i32 12>
  %657 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %650, <4 x i32> %654) #8
  %658 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %652, <4 x i32> %656) #8
  %659 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %660 = shufflevector <8 x i16> %616, <8 x i16> %610, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %661 = shufflevector <8 x i16> %610, <8 x i16> %616, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %662 = shufflevector <8 x i16> %616, <8 x i16> %610, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %663 = shufflevector <8 x i16> %610, <8 x i16> %616, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %664 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %660, <8 x i16> %659) #8
  %665 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %661, <8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>) #8
  %666 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %662, <8 x i16> %659) #8
  %667 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %663, <8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>) #8
  %668 = add <4 x i32> %664, <i32 2048, i32 2048, i32 2048, i32 2048>
  %669 = ashr <4 x i32> %668, <i32 12, i32 12, i32 12, i32 12>
  %670 = add <4 x i32> %665, <i32 2048, i32 2048, i32 2048, i32 2048>
  %671 = ashr <4 x i32> %670, <i32 12, i32 12, i32 12, i32 12>
  %672 = add <4 x i32> %666, <i32 2048, i32 2048, i32 2048, i32 2048>
  %673 = ashr <4 x i32> %672, <i32 12, i32 12, i32 12, i32 12>
  %674 = add <4 x i32> %667, <i32 2048, i32 2048, i32 2048, i32 2048>
  %675 = ashr <4 x i32> %674, <i32 12, i32 12, i32 12, i32 12>
  %676 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %669, <4 x i32> %673) #8
  %677 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %671, <4 x i32> %675) #8
  %678 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 -2276, i16 3406, i16 -2276, i16 3406, i16 -2276, i16 3406, i16 -2276, i16 3406>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %679 = shufflevector <8 x i16> %614, <8 x i16> %612, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %680 = shufflevector <8 x i16> %612, <8 x i16> %614, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %681 = shufflevector <8 x i16> %614, <8 x i16> %612, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %682 = shufflevector <8 x i16> %612, <8 x i16> %614, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %683 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %679, <8 x i16> %678) #8
  %684 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %680, <8 x i16> <i16 -2276, i16 3406, i16 -2276, i16 3406, i16 -2276, i16 3406, i16 -2276, i16 3406>) #8
  %685 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %681, <8 x i16> %678) #8
  %686 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %682, <8 x i16> <i16 -2276, i16 3406, i16 -2276, i16 3406, i16 -2276, i16 3406, i16 -2276, i16 3406>) #8
  %687 = add <4 x i32> %683, <i32 2048, i32 2048, i32 2048, i32 2048>
  %688 = ashr <4 x i32> %687, <i32 12, i32 12, i32 12, i32 12>
  %689 = add <4 x i32> %684, <i32 2048, i32 2048, i32 2048, i32 2048>
  %690 = ashr <4 x i32> %689, <i32 12, i32 12, i32 12, i32 12>
  %691 = add <4 x i32> %685, <i32 2048, i32 2048, i32 2048, i32 2048>
  %692 = ashr <4 x i32> %691, <i32 12, i32 12, i32 12, i32 12>
  %693 = add <4 x i32> %686, <i32 2048, i32 2048, i32 2048, i32 2048>
  %694 = ashr <4 x i32> %693, <i32 12, i32 12, i32 12, i32 12>
  %695 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %688, <4 x i32> %692) #8
  %696 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %690, <4 x i32> %694) #8
  %697 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %605, <8 x i16> %607) #8
  %698 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %605, <8 x i16> %607) #8
  %699 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %638, <8 x i16> %657) #8
  %700 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %638, <8 x i16> %657) #8
  %701 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %611, <8 x i16> %609) #8
  %702 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %611, <8 x i16> %609) #8
  %703 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %695, <8 x i16> %676) #8
  %704 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %695, <8 x i16> %676) #8
  %705 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %613, <8 x i16> %615) #8
  %706 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %613, <8 x i16> %615) #8
  %707 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %696, <8 x i16> %677) #8
  %708 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %696, <8 x i16> %677) #8
  store <8 x i16> %707, <8 x i16>* %598, align 16
  %709 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %619, <8 x i16> %617) #8
  %710 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %619, <8 x i16> %617) #8
  %711 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %639, <8 x i16> %658) #8
  %712 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %639, <8 x i16> %658) #8
  %713 = shufflevector <8 x i16> %712, <8 x i16> %700, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %714 = shufflevector <8 x i16> %700, <8 x i16> %712, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %715 = shufflevector <8 x i16> %712, <8 x i16> %700, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %716 = shufflevector <8 x i16> %700, <8 x i16> %712, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %717 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %713, <8 x i16> %459) #8
  %718 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %714, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %719 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %715, <8 x i16> %459) #8
  %720 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %716, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %721 = add <4 x i32> %717, <i32 2048, i32 2048, i32 2048, i32 2048>
  %722 = ashr <4 x i32> %721, <i32 12, i32 12, i32 12, i32 12>
  %723 = add <4 x i32> %718, <i32 2048, i32 2048, i32 2048, i32 2048>
  %724 = ashr <4 x i32> %723, <i32 12, i32 12, i32 12, i32 12>
  %725 = add <4 x i32> %719, <i32 2048, i32 2048, i32 2048, i32 2048>
  %726 = ashr <4 x i32> %725, <i32 12, i32 12, i32 12, i32 12>
  %727 = add <4 x i32> %720, <i32 2048, i32 2048, i32 2048, i32 2048>
  %728 = ashr <4 x i32> %727, <i32 12, i32 12, i32 12, i32 12>
  %729 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %722, <4 x i32> %726) #8
  %730 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %724, <4 x i32> %728) #8
  %731 = shufflevector <8 x i16> %710, <8 x i16> %698, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %732 = shufflevector <8 x i16> %698, <8 x i16> %710, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %733 = shufflevector <8 x i16> %710, <8 x i16> %698, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %734 = shufflevector <8 x i16> %698, <8 x i16> %710, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %735 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %731, <8 x i16> %459) #8
  %736 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %732, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %737 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %733, <8 x i16> %459) #8
  %738 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %734, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %739 = add <4 x i32> %735, <i32 2048, i32 2048, i32 2048, i32 2048>
  %740 = ashr <4 x i32> %739, <i32 12, i32 12, i32 12, i32 12>
  %741 = add <4 x i32> %736, <i32 2048, i32 2048, i32 2048, i32 2048>
  %742 = ashr <4 x i32> %741, <i32 12, i32 12, i32 12, i32 12>
  %743 = add <4 x i32> %737, <i32 2048, i32 2048, i32 2048, i32 2048>
  %744 = ashr <4 x i32> %743, <i32 12, i32 12, i32 12, i32 12>
  %745 = add <4 x i32> %738, <i32 2048, i32 2048, i32 2048, i32 2048>
  %746 = ashr <4 x i32> %745, <i32 12, i32 12, i32 12, i32 12>
  %747 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %740, <4 x i32> %744) #8
  %748 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %742, <4 x i32> %746) #8
  %749 = shufflevector <8 x i16> %706, <8 x i16> %702, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %750 = shufflevector <8 x i16> %702, <8 x i16> %706, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %751 = shufflevector <8 x i16> %706, <8 x i16> %702, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %752 = shufflevector <8 x i16> %702, <8 x i16> %706, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %753 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %749, <8 x i16> %478) #8
  %754 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %750, <8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>) #8
  %755 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %751, <8 x i16> %478) #8
  %756 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %752, <8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>) #8
  %757 = add <4 x i32> %753, <i32 2048, i32 2048, i32 2048, i32 2048>
  %758 = ashr <4 x i32> %757, <i32 12, i32 12, i32 12, i32 12>
  %759 = add <4 x i32> %754, <i32 2048, i32 2048, i32 2048, i32 2048>
  %760 = ashr <4 x i32> %759, <i32 12, i32 12, i32 12, i32 12>
  %761 = add <4 x i32> %755, <i32 2048, i32 2048, i32 2048, i32 2048>
  %762 = ashr <4 x i32> %761, <i32 12, i32 12, i32 12, i32 12>
  %763 = add <4 x i32> %756, <i32 2048, i32 2048, i32 2048, i32 2048>
  %764 = ashr <4 x i32> %763, <i32 12, i32 12, i32 12, i32 12>
  %765 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %758, <4 x i32> %762) #8
  %766 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %760, <4 x i32> %764) #8
  %767 = shufflevector <8 x i16> %708, <8 x i16> %704, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %768 = shufflevector <8 x i16> %704, <8 x i16> %708, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %769 = shufflevector <8 x i16> %708, <8 x i16> %704, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %770 = shufflevector <8 x i16> %704, <8 x i16> %708, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %771 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %767, <8 x i16> %478) #8
  %772 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %768, <8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>) #8
  %773 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %769, <8 x i16> %478) #8
  %774 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %770, <8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>) #8
  %775 = add <4 x i32> %771, <i32 2048, i32 2048, i32 2048, i32 2048>
  %776 = ashr <4 x i32> %775, <i32 12, i32 12, i32 12, i32 12>
  %777 = add <4 x i32> %772, <i32 2048, i32 2048, i32 2048, i32 2048>
  %778 = ashr <4 x i32> %777, <i32 12, i32 12, i32 12, i32 12>
  %779 = add <4 x i32> %773, <i32 2048, i32 2048, i32 2048, i32 2048>
  %780 = ashr <4 x i32> %779, <i32 12, i32 12, i32 12, i32 12>
  %781 = add <4 x i32> %774, <i32 2048, i32 2048, i32 2048, i32 2048>
  %782 = ashr <4 x i32> %781, <i32 12, i32 12, i32 12, i32 12>
  %783 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %776, <4 x i32> %780) #8
  %784 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %778, <4 x i32> %782) #8
  %785 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %697, <8 x i16> %701) #8
  %786 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %697, <8 x i16> %701) #8
  store <8 x i16> %785, <8 x i16>* %558, align 16
  %787 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %699, <8 x i16> %703) #8
  %788 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %699, <8 x i16> %703) #8
  store <8 x i16> %787, <8 x i16>* %568, align 16
  %789 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %729, <8 x i16> %783) #8
  %790 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %729, <8 x i16> %783) #8
  store <8 x i16> %789, <8 x i16>* %570, align 16
  %791 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %747, <8 x i16> %765) #8
  %792 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %747, <8 x i16> %765) #8
  store <8 x i16> %791, <8 x i16>* %580, align 16
  %793 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %709, <8 x i16> %705) #8
  %794 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %709, <8 x i16> %705) #8
  %795 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %711, <8 x i16> %707) #8
  %796 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %711, <8 x i16> %707) #8
  %797 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %730, <8 x i16> %784) #8
  %798 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %730, <8 x i16> %784) #8
  %799 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %748, <8 x i16> %766) #8
  %800 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %748, <8 x i16> %766) #8
  %801 = shufflevector <8 x i16> %800, <8 x i16> %792, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %802 = shufflevector <8 x i16> %792, <8 x i16> %800, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %803 = shufflevector <8 x i16> %800, <8 x i16> %792, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %804 = shufflevector <8 x i16> %792, <8 x i16> %800, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %805 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %801, <8 x i16> %400) #8
  %806 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %802, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %807 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %803, <8 x i16> %400) #8
  %808 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %804, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %809 = add <4 x i32> %805, <i32 2048, i32 2048, i32 2048, i32 2048>
  %810 = ashr <4 x i32> %809, <i32 12, i32 12, i32 12, i32 12>
  %811 = add <4 x i32> %806, <i32 2048, i32 2048, i32 2048, i32 2048>
  %812 = ashr <4 x i32> %811, <i32 12, i32 12, i32 12, i32 12>
  %813 = add <4 x i32> %807, <i32 2048, i32 2048, i32 2048, i32 2048>
  %814 = ashr <4 x i32> %813, <i32 12, i32 12, i32 12, i32 12>
  %815 = add <4 x i32> %808, <i32 2048, i32 2048, i32 2048, i32 2048>
  %816 = ashr <4 x i32> %815, <i32 12, i32 12, i32 12, i32 12>
  %817 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %810, <4 x i32> %814) #8
  %818 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %812, <4 x i32> %816) #8
  store <8 x i16> %817, <8 x i16>* %582, align 16
  %819 = shufflevector <8 x i16> %798, <8 x i16> %790, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %820 = shufflevector <8 x i16> %790, <8 x i16> %798, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %821 = shufflevector <8 x i16> %798, <8 x i16> %790, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %822 = shufflevector <8 x i16> %790, <8 x i16> %798, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %823 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %819, <8 x i16> %400) #8
  %824 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %820, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %825 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %821, <8 x i16> %400) #8
  %826 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %822, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %827 = add <4 x i32> %823, <i32 2048, i32 2048, i32 2048, i32 2048>
  %828 = ashr <4 x i32> %827, <i32 12, i32 12, i32 12, i32 12>
  %829 = add <4 x i32> %824, <i32 2048, i32 2048, i32 2048, i32 2048>
  %830 = ashr <4 x i32> %829, <i32 12, i32 12, i32 12, i32 12>
  %831 = add <4 x i32> %825, <i32 2048, i32 2048, i32 2048, i32 2048>
  %832 = ashr <4 x i32> %831, <i32 12, i32 12, i32 12, i32 12>
  %833 = add <4 x i32> %826, <i32 2048, i32 2048, i32 2048, i32 2048>
  %834 = ashr <4 x i32> %833, <i32 12, i32 12, i32 12, i32 12>
  %835 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %828, <4 x i32> %832) #8
  %836 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %830, <4 x i32> %834) #8
  %837 = shufflevector <8 x i16> %796, <8 x i16> %788, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %838 = shufflevector <8 x i16> %788, <8 x i16> %796, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %839 = shufflevector <8 x i16> %796, <8 x i16> %788, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %840 = shufflevector <8 x i16> %788, <8 x i16> %796, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %841 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %837, <8 x i16> %400) #8
  %842 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %838, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %843 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %839, <8 x i16> %400) #8
  %844 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %840, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %845 = add <4 x i32> %841, <i32 2048, i32 2048, i32 2048, i32 2048>
  %846 = ashr <4 x i32> %845, <i32 12, i32 12, i32 12, i32 12>
  %847 = add <4 x i32> %842, <i32 2048, i32 2048, i32 2048, i32 2048>
  %848 = ashr <4 x i32> %847, <i32 12, i32 12, i32 12, i32 12>
  %849 = add <4 x i32> %843, <i32 2048, i32 2048, i32 2048, i32 2048>
  %850 = ashr <4 x i32> %849, <i32 12, i32 12, i32 12, i32 12>
  %851 = add <4 x i32> %844, <i32 2048, i32 2048, i32 2048, i32 2048>
  %852 = ashr <4 x i32> %851, <i32 12, i32 12, i32 12, i32 12>
  %853 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %846, <4 x i32> %850) #8
  %854 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %848, <4 x i32> %852) #8
  %855 = shufflevector <8 x i16> %794, <8 x i16> %786, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %856 = shufflevector <8 x i16> %786, <8 x i16> %794, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %857 = shufflevector <8 x i16> %794, <8 x i16> %786, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %858 = shufflevector <8 x i16> %786, <8 x i16> %794, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %859 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %855, <8 x i16> %400) #8
  %860 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %856, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %861 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %857, <8 x i16> %400) #8
  %862 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %858, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %863 = add <4 x i32> %859, <i32 2048, i32 2048, i32 2048, i32 2048>
  %864 = ashr <4 x i32> %863, <i32 12, i32 12, i32 12, i32 12>
  %865 = add <4 x i32> %860, <i32 2048, i32 2048, i32 2048, i32 2048>
  %866 = ashr <4 x i32> %865, <i32 12, i32 12, i32 12, i32 12>
  %867 = add <4 x i32> %861, <i32 2048, i32 2048, i32 2048, i32 2048>
  %868 = ashr <4 x i32> %867, <i32 12, i32 12, i32 12, i32 12>
  %869 = add <4 x i32> %862, <i32 2048, i32 2048, i32 2048, i32 2048>
  %870 = ashr <4 x i32> %869, <i32 12, i32 12, i32 12, i32 12>
  %871 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %864, <4 x i32> %868) #8
  %872 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %866, <4 x i32> %870) #8
  %873 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %541, <8 x i16> %793) #8
  %874 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %541, <8 x i16> %793) #8
  store <8 x i16> %873, <8 x i16>* %370, align 16
  store <8 x i16> %874, <8 x i16>* %562, align 16
  %875 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %543, <8 x i16> %795) #8
  %876 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %543, <8 x i16> %795) #8
  store <8 x i16> %875, <8 x i16>* %373, align 16
  store <8 x i16> %876, <8 x i16>* %564, align 16
  %877 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %545, <8 x i16> %797) #8
  %878 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %545, <8 x i16> %797) #8
  store <8 x i16> %877, <8 x i16>* %375, align 16
  store <8 x i16> %878, <8 x i16>* %574, align 16
  %879 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %547, <8 x i16> %799) #8
  %880 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %547, <8 x i16> %799) #8
  store <8 x i16> %879, <8 x i16>* %379, align 16
  store <8 x i16> %880, <8 x i16>* %576, align 16
  %881 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %549, <8 x i16> %818) #8
  %882 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %549, <8 x i16> %818) #8
  store <8 x i16> %881, <8 x i16>* %385, align 16
  store <8 x i16> %882, <8 x i16>* %586, align 16
  %883 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %551, <8 x i16> %836) #8
  %884 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %551, <8 x i16> %836) #8
  store <8 x i16> %883, <8 x i16>* %395, align 16
  store <8 x i16> %884, <8 x i16>* %588, align 16
  %885 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %553, <8 x i16> %854) #8
  %886 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %553, <8 x i16> %854) #8
  store <8 x i16> %885, <8 x i16>* %391, align 16
  store <8 x i16> %886, <8 x i16>* %598, align 16
  %887 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %555, <8 x i16> %872) #8
  %888 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %555, <8 x i16> %872) #8
  store <8 x i16> %887, <8 x i16>* %389, align 16
  store <8 x i16> %888, <8 x i16>* %600, align 16
  %889 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %556, <8 x i16> %871) #8
  %890 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %556, <8 x i16> %871) #8
  store <8 x i16> %889, <8 x i16>* %428, align 16
  store <8 x i16> %890, <8 x i16>* %604, align 16
  %891 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %554, <8 x i16> %853) #8
  %892 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %554, <8 x i16> %853) #8
  store <8 x i16> %891, <8 x i16>* %438, align 16
  store <8 x i16> %892, <8 x i16>* %594, align 16
  %893 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %552, <8 x i16> %835) #8
  %894 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %552, <8 x i16> %835) #8
  store <8 x i16> %893, <8 x i16>* %440, align 16
  store <8 x i16> %894, <8 x i16>* %592, align 16
  %895 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %550, <8 x i16> %817) #8
  %896 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %550, <8 x i16> %817) #8
  store <8 x i16> %895, <8 x i16>* %450, align 16
  store <8 x i16> %896, <8 x i16>* %582, align 16
  %897 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %548, <8 x i16> %791) #8
  %898 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %548, <8 x i16> %791) #8
  store <8 x i16> %897, <8 x i16>* %446, align 16
  store <8 x i16> %898, <8 x i16>* %580, align 16
  %899 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %546, <8 x i16> %789) #8
  %900 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %546, <8 x i16> %789) #8
  store <8 x i16> %899, <8 x i16>* %444, align 16
  store <8 x i16> %900, <8 x i16>* %570, align 16
  %901 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %544, <8 x i16> %787) #8
  %902 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %544, <8 x i16> %787) #8
  store <8 x i16> %901, <8 x i16>* %434, align 16
  store <8 x i16> %902, <8 x i16>* %568, align 16
  %903 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %542, <8 x i16> %785) #8
  %904 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %542, <8 x i16> %785) #8
  store <8 x i16> %903, <8 x i16>* %432, align 16
  store <8 x i16> %904, <8 x i16>* %558, align 16
  %905 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 63
  %906 = bitcast <2 x i64>* %331 to <8 x i16>*
  %907 = load <8 x i16>, <8 x i16>* %906, align 16
  %908 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %907, <8 x i16> <i16 808, i16 808, i16 808, i16 808, i16 808, i16 808, i16 808, i16 808>) #8
  %909 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %907, <8 x i16> <i16 32760, i16 32760, i16 32760, i16 32760, i16 32760, i16 32760, i16 32760, i16 32760>) #8
  store <8 x i16> %908, <8 x i16>* %906, align 16
  %910 = bitcast <2 x i64>* %905 to <8 x i16>*
  store <8 x i16> %909, <8 x i16>* %910, align 16
  %911 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 33
  %912 = bitcast <2 x i64>* %368 to <8 x i16>*
  %913 = load <8 x i16>, <8 x i16>* %912, align 16
  %914 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %913, <8 x i16> <i16 -22592, i16 -22592, i16 -22592, i16 -22592, i16 -22592, i16 -22592, i16 -22592, i16 -22592>) #8
  %915 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %913, <8 x i16> <i16 23736, i16 23736, i16 23736, i16 23736, i16 23736, i16 23736, i16 23736, i16 23736>) #8
  %916 = bitcast <2 x i64>* %911 to <8 x i16>*
  store <8 x i16> %914, <8 x i16>* %916, align 16
  store <8 x i16> %915, <8 x i16>* %912, align 16
  %917 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 61
  %918 = bitcast <2 x i64>* %332 to <8 x i16>*
  %919 = load <8 x i16>, <8 x i16>* %918, align 16
  %920 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %919, <8 x i16> <i16 13280, i16 13280, i16 13280, i16 13280, i16 13280, i16 13280, i16 13280, i16 13280>) #8
  %921 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %919, <8 x i16> <i16 29960, i16 29960, i16 29960, i16 29960, i16 29960, i16 29960, i16 29960, i16 29960>) #8
  store <8 x i16> %920, <8 x i16>* %918, align 16
  %922 = bitcast <2 x i64>* %917 to <8 x i16>*
  store <8 x i16> %921, <8 x i16>* %922, align 16
  %923 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 35
  %924 = bitcast <2 x i64>* %365 to <8 x i16>*
  %925 = load <8 x i16>, <8 x i16>* %924, align 16
  %926 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %925, <8 x i16> <i16 -11792, i16 -11792, i16 -11792, i16 -11792, i16 -11792, i16 -11792, i16 -11792, i16 -11792>) #8
  %927 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %925, <8 x i16> <i16 30576, i16 30576, i16 30576, i16 30576, i16 30576, i16 30576, i16 30576, i16 30576>) #8
  %928 = bitcast <2 x i64>* %923 to <8 x i16>*
  store <8 x i16> %926, <8 x i16>* %928, align 16
  store <8 x i16> %927, <8 x i16>* %924, align 16
  %929 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 59
  %930 = bitcast <2 x i64>* %335 to <8 x i16>*
  %931 = load <8 x i16>, <8 x i16>* %930, align 16
  %932 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %931, <8 x i16> <i16 7176, i16 7176, i16 7176, i16 7176, i16 7176, i16 7176, i16 7176, i16 7176>) #8
  %933 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %931, <8 x i16> <i16 31968, i16 31968, i16 31968, i16 31968, i16 31968, i16 31968, i16 31968, i16 31968>) #8
  store <8 x i16> %932, <8 x i16>* %930, align 16
  %934 = bitcast <2 x i64>* %929 to <8 x i16>*
  store <8 x i16> %933, <8 x i16>* %934, align 16
  %935 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 37
  %936 = bitcast <2 x i64>* %362 to <8 x i16>*
  %937 = load <8 x i16>, <8 x i16>* %936, align 16
  %938 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %937, <8 x i16> <i16 -17528, i16 -17528, i16 -17528, i16 -17528, i16 -17528, i16 -17528, i16 -17528, i16 -17528>) #8
  %939 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %937, <8 x i16> <i16 27688, i16 27688, i16 27688, i16 27688, i16 27688, i16 27688, i16 27688, i16 27688>) #8
  %940 = bitcast <2 x i64>* %935 to <8 x i16>*
  store <8 x i16> %939, <8 x i16>* %936, align 16
  %941 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 57
  %942 = bitcast <2 x i64>* %336 to <8 x i16>*
  %943 = load <8 x i16>, <8 x i16>* %942, align 16
  %944 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %943, <8 x i16> <i16 18872, i16 18872, i16 18872, i16 18872, i16 18872, i16 18872, i16 18872, i16 18872>) #8
  %945 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %943, <8 x i16> <i16 26792, i16 26792, i16 26792, i16 26792, i16 26792, i16 26792, i16 26792, i16 26792>) #8
  %946 = bitcast <2 x i64>* %941 to <8 x i16>*
  store <8 x i16> %945, <8 x i16>* %946, align 16
  %947 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 39
  %948 = bitcast <2 x i64>* %359 to <8 x i16>*
  %949 = load <8 x i16>, <8 x i16>* %948, align 16
  %950 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %949, <8 x i16> <i16 -5600, i16 -5600, i16 -5600, i16 -5600, i16 -5600, i16 -5600, i16 -5600, i16 -5600>) #8
  %951 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %949, <8 x i16> <i16 32288, i16 32288, i16 32288, i16 32288, i16 32288, i16 32288, i16 32288, i16 32288>) #8
  %952 = bitcast <2 x i64>* %947 to <8 x i16>*
  store <8 x i16> %951, <8 x i16>* %948, align 16
  %953 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 55
  %954 = bitcast <2 x i64>* %339 to <8 x i16>*
  %955 = load <8 x i16>, <8 x i16>* %954, align 16
  %956 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %955, <8 x i16> <i16 4008, i16 4008, i16 4008, i16 4008, i16 4008, i16 4008, i16 4008, i16 4008>) #8
  %957 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %955, <8 x i16> <i16 32520, i16 32520, i16 32520, i16 32520, i16 32520, i16 32520, i16 32520, i16 32520>) #8
  %958 = bitcast <2 x i64>* %953 to <8 x i16>*
  store <8 x i16> %957, <8 x i16>* %958, align 16
  %959 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 41
  %960 = bitcast <2 x i64>* %356 to <8 x i16>*
  %961 = load <8 x i16>, <8 x i16>* %960, align 16
  %962 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %961, <8 x i16> <i16 -20160, i16 -20160, i16 -20160, i16 -20160, i16 -20160, i16 -20160, i16 -20160, i16 -20160>) #8
  %963 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %961, <8 x i16> <i16 25832, i16 25832, i16 25832, i16 25832, i16 25832, i16 25832, i16 25832, i16 25832>) #8
  %964 = bitcast <2 x i64>* %959 to <8 x i16>*
  %965 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 53
  %966 = bitcast <2 x i64>* %340 to <8 x i16>*
  %967 = load <8 x i16>, <8 x i16>* %966, align 16
  %968 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %967, <8 x i16> <i16 16152, i16 16152, i16 16152, i16 16152, i16 16152, i16 16152, i16 16152, i16 16152>) #8
  %969 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %967, <8 x i16> <i16 28512, i16 28512, i16 28512, i16 28512, i16 28512, i16 28512, i16 28512, i16 28512>) #8
  %970 = bitcast <2 x i64>* %965 to <8 x i16>*
  %971 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 43
  %972 = bitcast <2 x i64>* %353 to <8 x i16>*
  %973 = load <8 x i16>, <8 x i16>* %972, align 16
  %974 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %973, <8 x i16> <i16 -8736, i16 -8736, i16 -8736, i16 -8736, i16 -8736, i16 -8736, i16 -8736, i16 -8736>) #8
  %975 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %973, <8 x i16> <i16 31584, i16 31584, i16 31584, i16 31584, i16 31584, i16 31584, i16 31584, i16 31584>) #8
  %976 = bitcast <2 x i64>* %971 to <8 x i16>*
  %977 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 51
  %978 = bitcast <2 x i64>* %343 to <8 x i16>*
  %979 = load <8 x i16>, <8 x i16>* %978, align 16
  %980 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %979, <8 x i16> <i16 10280, i16 10280, i16 10280, i16 10280, i16 10280, i16 10280, i16 10280, i16 10280>) #8
  %981 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %979, <8 x i16> <i16 31112, i16 31112, i16 31112, i16 31112, i16 31112, i16 31112, i16 31112, i16 31112>) #8
  %982 = bitcast <2 x i64>* %977 to <8 x i16>*
  %983 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 45
  %984 = bitcast <2 x i64>* %350 to <8 x i16>*
  %985 = load <8 x i16>, <8 x i16>* %984, align 16
  %986 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %985, <8 x i16> <i16 -14736, i16 -14736, i16 -14736, i16 -14736, i16 -14736, i16 -14736, i16 -14736, i16 -14736>) #8
  %987 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %985, <8 x i16> <i16 29272, i16 29272, i16 29272, i16 29272, i16 29272, i16 29272, i16 29272, i16 29272>) #8
  %988 = bitcast <2 x i64>* %983 to <8 x i16>*
  %989 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 49
  %990 = bitcast <2 x i64>* %344 to <8 x i16>*
  %991 = load <8 x i16>, <8 x i16>* %990, align 16
  %992 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %991, <8 x i16> <i16 21400, i16 21400, i16 21400, i16 21400, i16 21400, i16 21400, i16 21400, i16 21400>) #8
  %993 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %991, <8 x i16> <i16 24816, i16 24816, i16 24816, i16 24816, i16 24816, i16 24816, i16 24816, i16 24816>) #8
  %994 = bitcast <2 x i64>* %989 to <8 x i16>*
  %995 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 47
  %996 = bitcast <2 x i64>* %347 to <8 x i16>*
  %997 = load <8 x i16>, <8 x i16>* %996, align 16
  %998 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %997, <8 x i16> <i16 -2408, i16 -2408, i16 -2408, i16 -2408, i16 -2408, i16 -2408, i16 -2408, i16 -2408>) #8
  %999 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %997, <8 x i16> <i16 32680, i16 32680, i16 32680, i16 32680, i16 32680, i16 32680, i16 32680, i16 32680>) #8
  %1000 = bitcast <2 x i64>* %995 to <8 x i16>*
  %1001 = load <8 x i16>, <8 x i16>* %906, align 16
  %1002 = load <8 x i16>, <8 x i16>* %916, align 16
  %1003 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1001, <8 x i16> %1002) #8
  %1004 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1001, <8 x i16> %1002) #8
  store <8 x i16> %1003, <8 x i16>* %906, align 16
  %1005 = load <8 x i16>, <8 x i16>* %928, align 16
  %1006 = load <8 x i16>, <8 x i16>* %918, align 16
  %1007 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1005, <8 x i16> %1006) #8
  %1008 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1005, <8 x i16> %1006) #8
  store <8 x i16> %1008, <8 x i16>* %918, align 16
  store <8 x i16> %1007, <8 x i16>* %928, align 16
  %1009 = load <8 x i16>, <8 x i16>* %930, align 16
  %1010 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1009, <8 x i16> %938) #8
  %1011 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1009, <8 x i16> %938) #8
  store <8 x i16> %1010, <8 x i16>* %930, align 16
  store <8 x i16> %1011, <8 x i16>* %940, align 16
  %1012 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %950, <8 x i16> %944) #8
  %1013 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %950, <8 x i16> %944) #8
  store <8 x i16> %1013, <8 x i16>* %942, align 16
  store <8 x i16> %1012, <8 x i16>* %952, align 16
  %1014 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %956, <8 x i16> %962) #8
  %1015 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %956, <8 x i16> %962) #8
  store <8 x i16> %1014, <8 x i16>* %954, align 16
  store <8 x i16> %1015, <8 x i16>* %964, align 16
  %1016 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %974, <8 x i16> %968) #8
  %1017 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %974, <8 x i16> %968) #8
  store <8 x i16> %1017, <8 x i16>* %966, align 16
  store <8 x i16> %1016, <8 x i16>* %976, align 16
  %1018 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %980, <8 x i16> %986) #8
  %1019 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %980, <8 x i16> %986) #8
  store <8 x i16> %1018, <8 x i16>* %978, align 16
  store <8 x i16> %1019, <8 x i16>* %988, align 16
  %1020 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %998, <8 x i16> %992) #8
  %1021 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %998, <8 x i16> %992) #8
  store <8 x i16> %1021, <8 x i16>* %990, align 16
  store <8 x i16> %1020, <8 x i16>* %1000, align 16
  %1022 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %999, <8 x i16> %993) #8
  %1023 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %999, <8 x i16> %993) #8
  store <8 x i16> %1022, <8 x i16>* %996, align 16
  store <8 x i16> %1023, <8 x i16>* %994, align 16
  %1024 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %981, <8 x i16> %987) #8
  %1025 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %981, <8 x i16> %987) #8
  store <8 x i16> %1025, <8 x i16>* %984, align 16
  store <8 x i16> %1024, <8 x i16>* %982, align 16
  %1026 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %975, <8 x i16> %969) #8
  %1027 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %975, <8 x i16> %969) #8
  store <8 x i16> %1026, <8 x i16>* %972, align 16
  store <8 x i16> %1027, <8 x i16>* %970, align 16
  %1028 = load <8 x i16>, <8 x i16>* %958, align 16
  %1029 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1028, <8 x i16> %963) #8
  %1030 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1028, <8 x i16> %963) #8
  store <8 x i16> %1030, <8 x i16>* %960, align 16
  store <8 x i16> %1029, <8 x i16>* %958, align 16
  %1031 = load <8 x i16>, <8 x i16>* %948, align 16
  %1032 = load <8 x i16>, <8 x i16>* %946, align 16
  %1033 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1031, <8 x i16> %1032) #8
  %1034 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1031, <8 x i16> %1032) #8
  store <8 x i16> %1033, <8 x i16>* %948, align 16
  %1035 = load <8 x i16>, <8 x i16>* %934, align 16
  %1036 = load <8 x i16>, <8 x i16>* %936, align 16
  %1037 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1035, <8 x i16> %1036) #8
  %1038 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1035, <8 x i16> %1036) #8
  store <8 x i16> %1037, <8 x i16>* %934, align 16
  %1039 = load <8 x i16>, <8 x i16>* %924, align 16
  %1040 = load <8 x i16>, <8 x i16>* %922, align 16
  %1041 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1039, <8 x i16> %1040) #8
  %1042 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1039, <8 x i16> %1040) #8
  store <8 x i16> %1041, <8 x i16>* %924, align 16
  %1043 = load <8 x i16>, <8 x i16>* %910, align 16
  %1044 = load <8 x i16>, <8 x i16>* %912, align 16
  %1045 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1043, <8 x i16> %1044) #8
  %1046 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1043, <8 x i16> %1044) #8
  store <8 x i16> %1045, <8 x i16>* %910, align 16
  %1047 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %1048 = shufflevector <8 x i16> %1046, <8 x i16> %1004, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1049 = shufflevector <8 x i16> %1004, <8 x i16> %1046, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1050 = shufflevector <8 x i16> %1046, <8 x i16> %1004, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1051 = shufflevector <8 x i16> %1004, <8 x i16> %1046, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1052 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1048, <8 x i16> %1047) #8
  %1053 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1049, <8 x i16> <i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076>) #8
  %1054 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1050, <8 x i16> %1047) #8
  %1055 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1051, <8 x i16> <i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076, i16 401, i16 4076>) #8
  %1056 = add <4 x i32> %1052, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1057 = ashr <4 x i32> %1056, <i32 12, i32 12, i32 12, i32 12>
  %1058 = add <4 x i32> %1053, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1059 = ashr <4 x i32> %1058, <i32 12, i32 12, i32 12, i32 12>
  %1060 = add <4 x i32> %1054, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1061 = ashr <4 x i32> %1060, <i32 12, i32 12, i32 12, i32 12>
  %1062 = add <4 x i32> %1055, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1063 = ashr <4 x i32> %1062, <i32 12, i32 12, i32 12, i32 12>
  %1064 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1057, <4 x i32> %1061) #8
  %1065 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1059, <4 x i32> %1063) #8
  store <8 x i16> %1065, <8 x i16>* %912, align 16
  store <8 x i16> %1064, <8 x i16>* %916, align 16
  %1066 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 -4076, i16 401, i16 -4076, i16 401, i16 -4076, i16 401, i16 -4076, i16 401>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %1067 = shufflevector <8 x i16> %1042, <8 x i16> %1008, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1068 = shufflevector <8 x i16> %1008, <8 x i16> %1042, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1069 = shufflevector <8 x i16> %1042, <8 x i16> %1008, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1070 = shufflevector <8 x i16> %1008, <8 x i16> %1042, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1071 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1067, <8 x i16> %1066) #8
  %1072 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1068, <8 x i16> <i16 -4076, i16 401, i16 -4076, i16 401, i16 -4076, i16 401, i16 -4076, i16 401>) #8
  %1073 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1069, <8 x i16> %1066) #8
  %1074 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1070, <8 x i16> <i16 -4076, i16 401, i16 -4076, i16 401, i16 -4076, i16 401, i16 -4076, i16 401>) #8
  %1075 = add <4 x i32> %1071, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1076 = ashr <4 x i32> %1075, <i32 12, i32 12, i32 12, i32 12>
  %1077 = add <4 x i32> %1072, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1078 = ashr <4 x i32> %1077, <i32 12, i32 12, i32 12, i32 12>
  %1079 = add <4 x i32> %1073, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1080 = ashr <4 x i32> %1079, <i32 12, i32 12, i32 12, i32 12>
  %1081 = add <4 x i32> %1074, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1082 = ashr <4 x i32> %1081, <i32 12, i32 12, i32 12, i32 12>
  %1083 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1076, <4 x i32> %1080) #8
  %1084 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1078, <4 x i32> %1082) #8
  store <8 x i16> %1084, <8 x i16>* %922, align 16
  store <8 x i16> %1083, <8 x i16>* %918, align 16
  %1085 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %1086 = shufflevector <8 x i16> %1038, <8 x i16> %1011, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1087 = shufflevector <8 x i16> %1011, <8 x i16> %1038, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1088 = shufflevector <8 x i16> %1038, <8 x i16> %1011, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1089 = shufflevector <8 x i16> %1011, <8 x i16> %1038, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1090 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1086, <8 x i16> %1085) #8
  %1091 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1087, <8 x i16> <i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598>) #8
  %1092 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1088, <8 x i16> %1085) #8
  %1093 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1089, <8 x i16> <i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598, i16 3166, i16 2598>) #8
  %1094 = add <4 x i32> %1090, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1095 = ashr <4 x i32> %1094, <i32 12, i32 12, i32 12, i32 12>
  %1096 = add <4 x i32> %1091, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1097 = ashr <4 x i32> %1096, <i32 12, i32 12, i32 12, i32 12>
  %1098 = add <4 x i32> %1092, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1099 = ashr <4 x i32> %1098, <i32 12, i32 12, i32 12, i32 12>
  %1100 = add <4 x i32> %1093, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1101 = ashr <4 x i32> %1100, <i32 12, i32 12, i32 12, i32 12>
  %1102 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1095, <4 x i32> %1099) #8
  %1103 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1097, <4 x i32> %1101) #8
  store <8 x i16> %1103, <8 x i16>* %936, align 16
  store <8 x i16> %1102, <8 x i16>* %940, align 16
  %1104 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 -2598, i16 3166, i16 -2598, i16 3166, i16 -2598, i16 3166, i16 -2598, i16 3166>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %1105 = shufflevector <8 x i16> %1034, <8 x i16> %1013, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1106 = shufflevector <8 x i16> %1013, <8 x i16> %1034, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1107 = shufflevector <8 x i16> %1034, <8 x i16> %1013, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1108 = shufflevector <8 x i16> %1013, <8 x i16> %1034, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1109 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1105, <8 x i16> %1104) #8
  %1110 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1106, <8 x i16> <i16 -2598, i16 3166, i16 -2598, i16 3166, i16 -2598, i16 3166, i16 -2598, i16 3166>) #8
  %1111 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1107, <8 x i16> %1104) #8
  %1112 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1108, <8 x i16> <i16 -2598, i16 3166, i16 -2598, i16 3166, i16 -2598, i16 3166, i16 -2598, i16 3166>) #8
  %1113 = add <4 x i32> %1109, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1114 = ashr <4 x i32> %1113, <i32 12, i32 12, i32 12, i32 12>
  %1115 = add <4 x i32> %1110, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1116 = ashr <4 x i32> %1115, <i32 12, i32 12, i32 12, i32 12>
  %1117 = add <4 x i32> %1111, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1118 = ashr <4 x i32> %1117, <i32 12, i32 12, i32 12, i32 12>
  %1119 = add <4 x i32> %1112, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1120 = ashr <4 x i32> %1119, <i32 12, i32 12, i32 12, i32 12>
  %1121 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1114, <4 x i32> %1118) #8
  %1122 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1116, <4 x i32> %1120) #8
  store <8 x i16> %1122, <8 x i16>* %946, align 16
  store <8 x i16> %1121, <8 x i16>* %942, align 16
  %1123 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %1124 = shufflevector <8 x i16> %1030, <8 x i16> %1015, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1125 = shufflevector <8 x i16> %1015, <8 x i16> %1030, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1126 = shufflevector <8 x i16> %1030, <8 x i16> %1015, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1127 = shufflevector <8 x i16> %1015, <8 x i16> %1030, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1128 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1124, <8 x i16> %1123) #8
  %1129 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1125, <8 x i16> <i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612>) #8
  %1130 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1126, <8 x i16> %1123) #8
  %1131 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1127, <8 x i16> <i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612, i16 1931, i16 3612>) #8
  %1132 = add <4 x i32> %1128, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1133 = ashr <4 x i32> %1132, <i32 12, i32 12, i32 12, i32 12>
  %1134 = add <4 x i32> %1129, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1135 = ashr <4 x i32> %1134, <i32 12, i32 12, i32 12, i32 12>
  %1136 = add <4 x i32> %1130, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1137 = ashr <4 x i32> %1136, <i32 12, i32 12, i32 12, i32 12>
  %1138 = add <4 x i32> %1131, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1139 = ashr <4 x i32> %1138, <i32 12, i32 12, i32 12, i32 12>
  %1140 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1133, <4 x i32> %1137) #8
  %1141 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1135, <4 x i32> %1139) #8
  store <8 x i16> %1141, <8 x i16>* %960, align 16
  %1142 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 -3612, i16 1931, i16 -3612, i16 1931, i16 -3612, i16 1931, i16 -3612, i16 1931>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %1143 = shufflevector <8 x i16> %1027, <8 x i16> %1017, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1144 = shufflevector <8 x i16> %1017, <8 x i16> %1027, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1145 = shufflevector <8 x i16> %1027, <8 x i16> %1017, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1146 = shufflevector <8 x i16> %1017, <8 x i16> %1027, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1147 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1143, <8 x i16> %1142) #8
  %1148 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1144, <8 x i16> <i16 -3612, i16 1931, i16 -3612, i16 1931, i16 -3612, i16 1931, i16 -3612, i16 1931>) #8
  %1149 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1145, <8 x i16> %1142) #8
  %1150 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1146, <8 x i16> <i16 -3612, i16 1931, i16 -3612, i16 1931, i16 -3612, i16 1931, i16 -3612, i16 1931>) #8
  %1151 = add <4 x i32> %1147, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1152 = ashr <4 x i32> %1151, <i32 12, i32 12, i32 12, i32 12>
  %1153 = add <4 x i32> %1148, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1154 = ashr <4 x i32> %1153, <i32 12, i32 12, i32 12, i32 12>
  %1155 = add <4 x i32> %1149, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1156 = ashr <4 x i32> %1155, <i32 12, i32 12, i32 12, i32 12>
  %1157 = add <4 x i32> %1150, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1158 = ashr <4 x i32> %1157, <i32 12, i32 12, i32 12, i32 12>
  %1159 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1152, <4 x i32> %1156) #8
  %1160 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1154, <4 x i32> %1158) #8
  %1161 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %1162 = shufflevector <8 x i16> %1025, <8 x i16> %1019, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1163 = shufflevector <8 x i16> %1019, <8 x i16> %1025, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1164 = shufflevector <8 x i16> %1025, <8 x i16> %1019, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1165 = shufflevector <8 x i16> %1019, <8 x i16> %1025, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1166 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1162, <8 x i16> %1161) #8
  %1167 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1163, <8 x i16> <i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189>) #8
  %1168 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1164, <8 x i16> %1161) #8
  %1169 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1165, <8 x i16> <i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189, i16 3920, i16 1189>) #8
  %1170 = add <4 x i32> %1166, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1171 = ashr <4 x i32> %1170, <i32 12, i32 12, i32 12, i32 12>
  %1172 = add <4 x i32> %1167, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1173 = ashr <4 x i32> %1172, <i32 12, i32 12, i32 12, i32 12>
  %1174 = add <4 x i32> %1168, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1175 = ashr <4 x i32> %1174, <i32 12, i32 12, i32 12, i32 12>
  %1176 = add <4 x i32> %1169, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1177 = ashr <4 x i32> %1176, <i32 12, i32 12, i32 12, i32 12>
  %1178 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1171, <4 x i32> %1175) #8
  %1179 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1173, <4 x i32> %1177) #8
  %1180 = tail call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> <i16 -1189, i16 3920, i16 -1189, i16 3920, i16 -1189, i16 3920, i16 -1189, i16 3920>, <8 x i16> <i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768, i16 1, i16 -32768>) #8
  %1181 = shufflevector <8 x i16> %1023, <8 x i16> %1021, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1182 = shufflevector <8 x i16> %1021, <8 x i16> %1023, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1183 = shufflevector <8 x i16> %1023, <8 x i16> %1021, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1184 = shufflevector <8 x i16> %1021, <8 x i16> %1023, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1185 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1181, <8 x i16> %1180) #8
  %1186 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1182, <8 x i16> <i16 -1189, i16 3920, i16 -1189, i16 3920, i16 -1189, i16 3920, i16 -1189, i16 3920>) #8
  %1187 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1183, <8 x i16> %1180) #8
  %1188 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1184, <8 x i16> <i16 -1189, i16 3920, i16 -1189, i16 3920, i16 -1189, i16 3920, i16 -1189, i16 3920>) #8
  %1189 = add <4 x i32> %1185, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1190 = ashr <4 x i32> %1189, <i32 12, i32 12, i32 12, i32 12>
  %1191 = add <4 x i32> %1186, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1192 = ashr <4 x i32> %1191, <i32 12, i32 12, i32 12, i32 12>
  %1193 = add <4 x i32> %1187, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1194 = ashr <4 x i32> %1193, <i32 12, i32 12, i32 12, i32 12>
  %1195 = add <4 x i32> %1188, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1196 = ashr <4 x i32> %1195, <i32 12, i32 12, i32 12, i32 12>
  %1197 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1190, <4 x i32> %1194) #8
  %1198 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1192, <4 x i32> %1196) #8
  %1199 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1003, <8 x i16> %1007) #8
  %1200 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1003, <8 x i16> %1007) #8
  store <8 x i16> %1199, <8 x i16>* %906, align 16
  store <8 x i16> %1200, <8 x i16>* %928, align 16
  %1201 = load <8 x i16>, <8 x i16>* %916, align 16
  %1202 = load <8 x i16>, <8 x i16>* %918, align 16
  %1203 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1201, <8 x i16> %1202) #8
  %1204 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1201, <8 x i16> %1202) #8
  store <8 x i16> %1203, <8 x i16>* %916, align 16
  %1205 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1012, <8 x i16> %1010) #8
  %1206 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1012, <8 x i16> %1010) #8
  store <8 x i16> %1206, <8 x i16>* %930, align 16
  store <8 x i16> %1205, <8 x i16>* %952, align 16
  %1207 = load <8 x i16>, <8 x i16>* %940, align 16
  %1208 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1121, <8 x i16> %1207) #8
  %1209 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1121, <8 x i16> %1207) #8
  store <8 x i16> %1209, <8 x i16>* %940, align 16
  store <8 x i16> %1208, <8 x i16>* %942, align 16
  %1210 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1014, <8 x i16> %1016) #8
  %1211 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1014, <8 x i16> %1016) #8
  store <8 x i16> %1210, <8 x i16>* %954, align 16
  store <8 x i16> %1211, <8 x i16>* %976, align 16
  %1212 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1140, <8 x i16> %1159) #8
  %1213 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1140, <8 x i16> %1159) #8
  store <8 x i16> %1212, <8 x i16>* %964, align 16
  store <8 x i16> %1213, <8 x i16>* %966, align 16
  %1214 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1020, <8 x i16> %1018) #8
  %1215 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1020, <8 x i16> %1018) #8
  store <8 x i16> %1215, <8 x i16>* %978, align 16
  store <8 x i16> %1214, <8 x i16>* %1000, align 16
  %1216 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1197, <8 x i16> %1178) #8
  %1217 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1197, <8 x i16> %1178) #8
  store <8 x i16> %1217, <8 x i16>* %988, align 16
  store <8 x i16> %1216, <8 x i16>* %990, align 16
  %1218 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1022, <8 x i16> %1024) #8
  %1219 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1022, <8 x i16> %1024) #8
  store <8 x i16> %1218, <8 x i16>* %996, align 16
  store <8 x i16> %1219, <8 x i16>* %982, align 16
  %1220 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1198, <8 x i16> %1179) #8
  %1221 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1198, <8 x i16> %1179) #8
  store <8 x i16> %1220, <8 x i16>* %994, align 16
  store <8 x i16> %1221, <8 x i16>* %984, align 16
  %1222 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1029, <8 x i16> %1026) #8
  %1223 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1029, <8 x i16> %1026) #8
  store <8 x i16> %1223, <8 x i16>* %972, align 16
  store <8 x i16> %1222, <8 x i16>* %958, align 16
  %1224 = load <8 x i16>, <8 x i16>* %960, align 16
  %1225 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1224, <8 x i16> %1160) #8
  %1226 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1224, <8 x i16> %1160) #8
  store <8 x i16> %1226, <8 x i16>* %970, align 16
  store <8 x i16> %1225, <8 x i16>* %960, align 16
  %1227 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1033, <8 x i16> %1037) #8
  %1228 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1033, <8 x i16> %1037) #8
  store <8 x i16> %1227, <8 x i16>* %948, align 16
  %1229 = load <8 x i16>, <8 x i16>* %946, align 16
  %1230 = load <8 x i16>, <8 x i16>* %936, align 16
  %1231 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1229, <8 x i16> %1230) #8
  %1232 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1229, <8 x i16> %1230) #8
  store <8 x i16> %1231, <8 x i16>* %946, align 16
  %1233 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1045, <8 x i16> %1041) #8
  %1234 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1045, <8 x i16> %1041) #8
  store <8 x i16> %1233, <8 x i16>* %910, align 16
  %1235 = load <8 x i16>, <8 x i16>* %912, align 16
  %1236 = load <8 x i16>, <8 x i16>* %922, align 16
  %1237 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1235, <8 x i16> %1236) #8
  %1238 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1235, <8 x i16> %1236) #8
  store <8 x i16> %1237, <8 x i16>* %912, align 16
  %1239 = shufflevector <8 x i16> %1238, <8 x i16> %1204, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1240 = shufflevector <8 x i16> %1204, <8 x i16> %1238, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1241 = shufflevector <8 x i16> %1238, <8 x i16> %1204, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1242 = shufflevector <8 x i16> %1204, <8 x i16> %1238, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1243 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1239, <8 x i16> %621) #8
  %1244 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1240, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %1245 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1241, <8 x i16> %621) #8
  %1246 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1242, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %1247 = add <4 x i32> %1243, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1248 = ashr <4 x i32> %1247, <i32 12, i32 12, i32 12, i32 12>
  %1249 = add <4 x i32> %1244, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1250 = ashr <4 x i32> %1249, <i32 12, i32 12, i32 12, i32 12>
  %1251 = add <4 x i32> %1245, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1252 = ashr <4 x i32> %1251, <i32 12, i32 12, i32 12, i32 12>
  %1253 = add <4 x i32> %1246, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1254 = ashr <4 x i32> %1253, <i32 12, i32 12, i32 12, i32 12>
  %1255 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1248, <4 x i32> %1252) #8
  %1256 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1250, <4 x i32> %1254) #8
  store <8 x i16> %1256, <8 x i16>* %922, align 16
  store <8 x i16> %1255, <8 x i16>* %918, align 16
  %1257 = shufflevector <8 x i16> %1234, <8 x i16> %1200, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1258 = shufflevector <8 x i16> %1200, <8 x i16> %1234, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1259 = shufflevector <8 x i16> %1234, <8 x i16> %1200, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1260 = shufflevector <8 x i16> %1200, <8 x i16> %1234, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1261 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1257, <8 x i16> %621) #8
  %1262 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1258, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %1263 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1259, <8 x i16> %621) #8
  %1264 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1260, <8 x i16> <i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017, i16 799, i16 4017>) #8
  %1265 = add <4 x i32> %1261, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1266 = ashr <4 x i32> %1265, <i32 12, i32 12, i32 12, i32 12>
  %1267 = add <4 x i32> %1262, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1268 = ashr <4 x i32> %1267, <i32 12, i32 12, i32 12, i32 12>
  %1269 = add <4 x i32> %1263, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1270 = ashr <4 x i32> %1269, <i32 12, i32 12, i32 12, i32 12>
  %1271 = add <4 x i32> %1264, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1272 = ashr <4 x i32> %1271, <i32 12, i32 12, i32 12, i32 12>
  %1273 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1266, <4 x i32> %1270) #8
  %1274 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1268, <4 x i32> %1272) #8
  store <8 x i16> %1274, <8 x i16>* %924, align 16
  store <8 x i16> %1273, <8 x i16>* %928, align 16
  %1275 = shufflevector <8 x i16> %1228, <8 x i16> %1206, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1276 = shufflevector <8 x i16> %1206, <8 x i16> %1228, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1277 = shufflevector <8 x i16> %1228, <8 x i16> %1206, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1278 = shufflevector <8 x i16> %1206, <8 x i16> %1228, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1279 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1275, <8 x i16> %640) #8
  %1280 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1276, <8 x i16> <i16 -4017, i16 799, i16 -4017, i16 799, i16 -4017, i16 799, i16 -4017, i16 799>) #8
  %1281 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1277, <8 x i16> %640) #8
  %1282 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1278, <8 x i16> <i16 -4017, i16 799, i16 -4017, i16 799, i16 -4017, i16 799, i16 -4017, i16 799>) #8
  %1283 = add <4 x i32> %1279, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1284 = ashr <4 x i32> %1283, <i32 12, i32 12, i32 12, i32 12>
  %1285 = add <4 x i32> %1280, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1286 = ashr <4 x i32> %1285, <i32 12, i32 12, i32 12, i32 12>
  %1287 = add <4 x i32> %1281, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1288 = ashr <4 x i32> %1287, <i32 12, i32 12, i32 12, i32 12>
  %1289 = add <4 x i32> %1282, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1290 = ashr <4 x i32> %1289, <i32 12, i32 12, i32 12, i32 12>
  %1291 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1284, <4 x i32> %1288) #8
  %1292 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1286, <4 x i32> %1290) #8
  store <8 x i16> %1292, <8 x i16>* %934, align 16
  store <8 x i16> %1291, <8 x i16>* %930, align 16
  %1293 = shufflevector <8 x i16> %1232, <8 x i16> %1209, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1294 = shufflevector <8 x i16> %1209, <8 x i16> %1232, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1295 = shufflevector <8 x i16> %1232, <8 x i16> %1209, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1296 = shufflevector <8 x i16> %1209, <8 x i16> %1232, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1297 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1293, <8 x i16> %640) #8
  %1298 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1294, <8 x i16> <i16 -4017, i16 799, i16 -4017, i16 799, i16 -4017, i16 799, i16 -4017, i16 799>) #8
  %1299 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1295, <8 x i16> %640) #8
  %1300 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1296, <8 x i16> <i16 -4017, i16 799, i16 -4017, i16 799, i16 -4017, i16 799, i16 -4017, i16 799>) #8
  %1301 = add <4 x i32> %1297, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1302 = ashr <4 x i32> %1301, <i32 12, i32 12, i32 12, i32 12>
  %1303 = add <4 x i32> %1298, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1304 = ashr <4 x i32> %1303, <i32 12, i32 12, i32 12, i32 12>
  %1305 = add <4 x i32> %1299, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1306 = ashr <4 x i32> %1305, <i32 12, i32 12, i32 12, i32 12>
  %1307 = add <4 x i32> %1300, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1308 = ashr <4 x i32> %1307, <i32 12, i32 12, i32 12, i32 12>
  %1309 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1302, <4 x i32> %1306) #8
  %1310 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1304, <4 x i32> %1308) #8
  store <8 x i16> %1310, <8 x i16>* %936, align 16
  %1311 = shufflevector <8 x i16> %1226, <8 x i16> %1213, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1312 = shufflevector <8 x i16> %1213, <8 x i16> %1226, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1313 = shufflevector <8 x i16> %1226, <8 x i16> %1213, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1314 = shufflevector <8 x i16> %1213, <8 x i16> %1226, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1315 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1311, <8 x i16> %659) #8
  %1316 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1312, <8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>) #8
  %1317 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1313, <8 x i16> %659) #8
  %1318 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1314, <8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>) #8
  %1319 = add <4 x i32> %1315, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1320 = ashr <4 x i32> %1319, <i32 12, i32 12, i32 12, i32 12>
  %1321 = add <4 x i32> %1316, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1322 = ashr <4 x i32> %1321, <i32 12, i32 12, i32 12, i32 12>
  %1323 = add <4 x i32> %1317, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1324 = ashr <4 x i32> %1323, <i32 12, i32 12, i32 12, i32 12>
  %1325 = add <4 x i32> %1318, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1326 = ashr <4 x i32> %1325, <i32 12, i32 12, i32 12, i32 12>
  %1327 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1320, <4 x i32> %1324) #8
  %1328 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1322, <4 x i32> %1326) #8
  store <8 x i16> %1328, <8 x i16>* %970, align 16
  %1329 = shufflevector <8 x i16> %1223, <8 x i16> %1211, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1330 = shufflevector <8 x i16> %1211, <8 x i16> %1223, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1331 = shufflevector <8 x i16> %1223, <8 x i16> %1211, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1332 = shufflevector <8 x i16> %1211, <8 x i16> %1223, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1333 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1329, <8 x i16> %659) #8
  %1334 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1330, <8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>) #8
  %1335 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1331, <8 x i16> %659) #8
  %1336 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1332, <8 x i16> <i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276, i16 3406, i16 2276>) #8
  %1337 = add <4 x i32> %1333, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1338 = ashr <4 x i32> %1337, <i32 12, i32 12, i32 12, i32 12>
  %1339 = add <4 x i32> %1334, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1340 = ashr <4 x i32> %1339, <i32 12, i32 12, i32 12, i32 12>
  %1341 = add <4 x i32> %1335, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1342 = ashr <4 x i32> %1341, <i32 12, i32 12, i32 12, i32 12>
  %1343 = add <4 x i32> %1336, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1344 = ashr <4 x i32> %1343, <i32 12, i32 12, i32 12, i32 12>
  %1345 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1338, <4 x i32> %1342) #8
  %1346 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1340, <4 x i32> %1344) #8
  %1347 = shufflevector <8 x i16> %1219, <8 x i16> %1215, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1348 = shufflevector <8 x i16> %1215, <8 x i16> %1219, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1349 = shufflevector <8 x i16> %1219, <8 x i16> %1215, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1350 = shufflevector <8 x i16> %1215, <8 x i16> %1219, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1351 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1347, <8 x i16> %678) #8
  %1352 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1348, <8 x i16> <i16 -2276, i16 3406, i16 -2276, i16 3406, i16 -2276, i16 3406, i16 -2276, i16 3406>) #8
  %1353 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1349, <8 x i16> %678) #8
  %1354 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1350, <8 x i16> <i16 -2276, i16 3406, i16 -2276, i16 3406, i16 -2276, i16 3406, i16 -2276, i16 3406>) #8
  %1355 = add <4 x i32> %1351, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1356 = ashr <4 x i32> %1355, <i32 12, i32 12, i32 12, i32 12>
  %1357 = add <4 x i32> %1352, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1358 = ashr <4 x i32> %1357, <i32 12, i32 12, i32 12, i32 12>
  %1359 = add <4 x i32> %1353, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1360 = ashr <4 x i32> %1359, <i32 12, i32 12, i32 12, i32 12>
  %1361 = add <4 x i32> %1354, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1362 = ashr <4 x i32> %1361, <i32 12, i32 12, i32 12, i32 12>
  %1363 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1356, <4 x i32> %1360) #8
  %1364 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1358, <4 x i32> %1362) #8
  %1365 = shufflevector <8 x i16> %1221, <8 x i16> %1217, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1366 = shufflevector <8 x i16> %1217, <8 x i16> %1221, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1367 = shufflevector <8 x i16> %1221, <8 x i16> %1217, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1368 = shufflevector <8 x i16> %1217, <8 x i16> %1221, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1369 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1365, <8 x i16> %678) #8
  %1370 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1366, <8 x i16> <i16 -2276, i16 3406, i16 -2276, i16 3406, i16 -2276, i16 3406, i16 -2276, i16 3406>) #8
  %1371 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1367, <8 x i16> %678) #8
  %1372 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1368, <8 x i16> <i16 -2276, i16 3406, i16 -2276, i16 3406, i16 -2276, i16 3406, i16 -2276, i16 3406>) #8
  %1373 = add <4 x i32> %1369, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1374 = ashr <4 x i32> %1373, <i32 12, i32 12, i32 12, i32 12>
  %1375 = add <4 x i32> %1370, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1376 = ashr <4 x i32> %1375, <i32 12, i32 12, i32 12, i32 12>
  %1377 = add <4 x i32> %1371, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1378 = ashr <4 x i32> %1377, <i32 12, i32 12, i32 12, i32 12>
  %1379 = add <4 x i32> %1372, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1380 = ashr <4 x i32> %1379, <i32 12, i32 12, i32 12, i32 12>
  %1381 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1374, <4 x i32> %1378) #8
  %1382 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1376, <4 x i32> %1380) #8
  %1383 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1199, <8 x i16> %1205) #8
  %1384 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1199, <8 x i16> %1205) #8
  store <8 x i16> %1383, <8 x i16>* %906, align 16
  store <8 x i16> %1384, <8 x i16>* %952, align 16
  %1385 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1203, <8 x i16> %1208) #8
  %1386 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1203, <8 x i16> %1208) #8
  store <8 x i16> %1385, <8 x i16>* %916, align 16
  store <8 x i16> %1386, <8 x i16>* %942, align 16
  %1387 = load <8 x i16>, <8 x i16>* %918, align 16
  %1388 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1387, <8 x i16> %1309) #8
  %1389 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1387, <8 x i16> %1309) #8
  store <8 x i16> %1388, <8 x i16>* %918, align 16
  %1390 = load <8 x i16>, <8 x i16>* %928, align 16
  %1391 = load <8 x i16>, <8 x i16>* %930, align 16
  %1392 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1390, <8 x i16> %1391) #8
  %1393 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1390, <8 x i16> %1391) #8
  store <8 x i16> %1392, <8 x i16>* %928, align 16
  %1394 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1214, <8 x i16> %1210) #8
  %1395 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1214, <8 x i16> %1210) #8
  store <8 x i16> %1395, <8 x i16>* %954, align 16
  store <8 x i16> %1394, <8 x i16>* %1000, align 16
  %1396 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1216, <8 x i16> %1212) #8
  %1397 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1216, <8 x i16> %1212) #8
  store <8 x i16> %1397, <8 x i16>* %964, align 16
  store <8 x i16> %1396, <8 x i16>* %990, align 16
  %1398 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1381, <8 x i16> %1327) #8
  %1399 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1381, <8 x i16> %1327) #8
  store <8 x i16> %1399, <8 x i16>* %966, align 16
  store <8 x i16> %1398, <8 x i16>* %988, align 16
  %1400 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1363, <8 x i16> %1345) #8
  %1401 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1363, <8 x i16> %1345) #8
  store <8 x i16> %1401, <8 x i16>* %976, align 16
  store <8 x i16> %1400, <8 x i16>* %978, align 16
  %1402 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1218, <8 x i16> %1222) #8
  %1403 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1218, <8 x i16> %1222) #8
  store <8 x i16> %1402, <8 x i16>* %996, align 16
  store <8 x i16> %1403, <8 x i16>* %958, align 16
  %1404 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1220, <8 x i16> %1225) #8
  %1405 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1220, <8 x i16> %1225) #8
  store <8 x i16> %1404, <8 x i16>* %994, align 16
  store <8 x i16> %1405, <8 x i16>* %960, align 16
  %1406 = load <8 x i16>, <8 x i16>* %970, align 16
  %1407 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1382, <8 x i16> %1406) #8
  %1408 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1382, <8 x i16> %1406) #8
  store <8 x i16> %1407, <8 x i16>* %984, align 16
  store <8 x i16> %1408, <8 x i16>* %970, align 16
  %1409 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1364, <8 x i16> %1346) #8
  %1410 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1364, <8 x i16> %1346) #8
  store <8 x i16> %1409, <8 x i16>* %982, align 16
  store <8 x i16> %1410, <8 x i16>* %972, align 16
  %1411 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1233, <8 x i16> %1227) #8
  %1412 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1233, <8 x i16> %1227) #8
  store <8 x i16> %1411, <8 x i16>* %910, align 16
  %1413 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1237, <8 x i16> %1231) #8
  %1414 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1237, <8 x i16> %1231) #8
  store <8 x i16> %1413, <8 x i16>* %912, align 16
  %1415 = load <8 x i16>, <8 x i16>* %922, align 16
  %1416 = load <8 x i16>, <8 x i16>* %936, align 16
  %1417 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1415, <8 x i16> %1416) #8
  %1418 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1415, <8 x i16> %1416) #8
  store <8 x i16> %1417, <8 x i16>* %922, align 16
  %1419 = load <8 x i16>, <8 x i16>* %924, align 16
  %1420 = load <8 x i16>, <8 x i16>* %934, align 16
  %1421 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1419, <8 x i16> %1420) #8
  %1422 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1419, <8 x i16> %1420) #8
  store <8 x i16> %1421, <8 x i16>* %924, align 16
  %1423 = shufflevector <8 x i16> %1422, <8 x i16> %1393, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1424 = shufflevector <8 x i16> %1393, <8 x i16> %1422, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1425 = shufflevector <8 x i16> %1422, <8 x i16> %1393, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1426 = shufflevector <8 x i16> %1393, <8 x i16> %1422, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1427 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1423, <8 x i16> %459) #8
  %1428 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1424, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %1429 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1425, <8 x i16> %459) #8
  %1430 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1426, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %1431 = add <4 x i32> %1427, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1432 = ashr <4 x i32> %1431, <i32 12, i32 12, i32 12, i32 12>
  %1433 = add <4 x i32> %1428, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1434 = ashr <4 x i32> %1433, <i32 12, i32 12, i32 12, i32 12>
  %1435 = add <4 x i32> %1429, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1436 = ashr <4 x i32> %1435, <i32 12, i32 12, i32 12, i32 12>
  %1437 = add <4 x i32> %1430, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1438 = ashr <4 x i32> %1437, <i32 12, i32 12, i32 12, i32 12>
  %1439 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1432, <4 x i32> %1436) #8
  %1440 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1434, <4 x i32> %1438) #8
  store <8 x i16> %1440, <8 x i16>* %934, align 16
  store <8 x i16> %1439, <8 x i16>* %930, align 16
  %1441 = shufflevector <8 x i16> %1418, <8 x i16> %1389, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1442 = shufflevector <8 x i16> %1389, <8 x i16> %1418, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1443 = shufflevector <8 x i16> %1418, <8 x i16> %1389, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1444 = shufflevector <8 x i16> %1389, <8 x i16> %1418, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1445 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1441, <8 x i16> %459) #8
  %1446 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1442, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %1447 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1443, <8 x i16> %459) #8
  %1448 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1444, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %1449 = add <4 x i32> %1445, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1450 = ashr <4 x i32> %1449, <i32 12, i32 12, i32 12, i32 12>
  %1451 = add <4 x i32> %1446, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1452 = ashr <4 x i32> %1451, <i32 12, i32 12, i32 12, i32 12>
  %1453 = add <4 x i32> %1447, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1454 = ashr <4 x i32> %1453, <i32 12, i32 12, i32 12, i32 12>
  %1455 = add <4 x i32> %1448, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1456 = ashr <4 x i32> %1455, <i32 12, i32 12, i32 12, i32 12>
  %1457 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1450, <4 x i32> %1454) #8
  %1458 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1452, <4 x i32> %1456) #8
  store <8 x i16> %1458, <8 x i16>* %936, align 16
  store <8 x i16> %1457, <8 x i16>* %940, align 16
  %1459 = shufflevector <8 x i16> %1414, <8 x i16> %1386, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1460 = shufflevector <8 x i16> %1386, <8 x i16> %1414, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1461 = shufflevector <8 x i16> %1414, <8 x i16> %1386, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1462 = shufflevector <8 x i16> %1386, <8 x i16> %1414, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1463 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1459, <8 x i16> %459) #8
  %1464 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1460, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %1465 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1461, <8 x i16> %459) #8
  %1466 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1462, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %1467 = add <4 x i32> %1463, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1468 = ashr <4 x i32> %1467, <i32 12, i32 12, i32 12, i32 12>
  %1469 = add <4 x i32> %1464, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1470 = ashr <4 x i32> %1469, <i32 12, i32 12, i32 12, i32 12>
  %1471 = add <4 x i32> %1465, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1472 = ashr <4 x i32> %1471, <i32 12, i32 12, i32 12, i32 12>
  %1473 = add <4 x i32> %1466, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1474 = ashr <4 x i32> %1473, <i32 12, i32 12, i32 12, i32 12>
  %1475 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1468, <4 x i32> %1472) #8
  %1476 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1470, <4 x i32> %1474) #8
  store <8 x i16> %1476, <8 x i16>* %946, align 16
  store <8 x i16> %1475, <8 x i16>* %942, align 16
  %1477 = shufflevector <8 x i16> %1412, <8 x i16> %1384, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1478 = shufflevector <8 x i16> %1384, <8 x i16> %1412, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1479 = shufflevector <8 x i16> %1412, <8 x i16> %1384, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1480 = shufflevector <8 x i16> %1384, <8 x i16> %1412, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1481 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1477, <8 x i16> %459) #8
  %1482 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1478, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %1483 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1479, <8 x i16> %459) #8
  %1484 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1480, <8 x i16> <i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784, i16 1567, i16 3784>) #8
  %1485 = add <4 x i32> %1481, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1486 = ashr <4 x i32> %1485, <i32 12, i32 12, i32 12, i32 12>
  %1487 = add <4 x i32> %1482, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1488 = ashr <4 x i32> %1487, <i32 12, i32 12, i32 12, i32 12>
  %1489 = add <4 x i32> %1483, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1490 = ashr <4 x i32> %1489, <i32 12, i32 12, i32 12, i32 12>
  %1491 = add <4 x i32> %1484, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1492 = ashr <4 x i32> %1491, <i32 12, i32 12, i32 12, i32 12>
  %1493 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1486, <4 x i32> %1490) #8
  %1494 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1488, <4 x i32> %1492) #8
  store <8 x i16> %1494, <8 x i16>* %948, align 16
  store <8 x i16> %1493, <8 x i16>* %952, align 16
  %1495 = shufflevector <8 x i16> %1403, <8 x i16> %1395, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1496 = shufflevector <8 x i16> %1395, <8 x i16> %1403, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1497 = shufflevector <8 x i16> %1403, <8 x i16> %1395, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1498 = shufflevector <8 x i16> %1395, <8 x i16> %1403, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1499 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1495, <8 x i16> %478) #8
  %1500 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1496, <8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>) #8
  %1501 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1497, <8 x i16> %478) #8
  %1502 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1498, <8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>) #8
  %1503 = add <4 x i32> %1499, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1504 = ashr <4 x i32> %1503, <i32 12, i32 12, i32 12, i32 12>
  %1505 = add <4 x i32> %1500, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1506 = ashr <4 x i32> %1505, <i32 12, i32 12, i32 12, i32 12>
  %1507 = add <4 x i32> %1501, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1508 = ashr <4 x i32> %1507, <i32 12, i32 12, i32 12, i32 12>
  %1509 = add <4 x i32> %1502, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1510 = ashr <4 x i32> %1509, <i32 12, i32 12, i32 12, i32 12>
  %1511 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1504, <4 x i32> %1508) #8
  %1512 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1506, <4 x i32> %1510) #8
  store <8 x i16> %1512, <8 x i16>* %958, align 16
  %1513 = shufflevector <8 x i16> %1405, <8 x i16> %1397, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1514 = shufflevector <8 x i16> %1397, <8 x i16> %1405, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1515 = shufflevector <8 x i16> %1405, <8 x i16> %1397, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1516 = shufflevector <8 x i16> %1397, <8 x i16> %1405, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1517 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1513, <8 x i16> %478) #8
  %1518 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1514, <8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>) #8
  %1519 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1515, <8 x i16> %478) #8
  %1520 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1516, <8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>) #8
  %1521 = add <4 x i32> %1517, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1522 = ashr <4 x i32> %1521, <i32 12, i32 12, i32 12, i32 12>
  %1523 = add <4 x i32> %1518, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1524 = ashr <4 x i32> %1523, <i32 12, i32 12, i32 12, i32 12>
  %1525 = add <4 x i32> %1519, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1526 = ashr <4 x i32> %1525, <i32 12, i32 12, i32 12, i32 12>
  %1527 = add <4 x i32> %1520, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1528 = ashr <4 x i32> %1527, <i32 12, i32 12, i32 12, i32 12>
  %1529 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1522, <4 x i32> %1526) #8
  %1530 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1524, <4 x i32> %1528) #8
  store <8 x i16> %1530, <8 x i16>* %960, align 16
  %1531 = shufflevector <8 x i16> %1408, <8 x i16> %1399, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1532 = shufflevector <8 x i16> %1399, <8 x i16> %1408, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1533 = shufflevector <8 x i16> %1408, <8 x i16> %1399, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1534 = shufflevector <8 x i16> %1399, <8 x i16> %1408, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1535 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1531, <8 x i16> %478) #8
  %1536 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1532, <8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>) #8
  %1537 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1533, <8 x i16> %478) #8
  %1538 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1534, <8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>) #8
  %1539 = add <4 x i32> %1535, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1540 = ashr <4 x i32> %1539, <i32 12, i32 12, i32 12, i32 12>
  %1541 = add <4 x i32> %1536, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1542 = ashr <4 x i32> %1541, <i32 12, i32 12, i32 12, i32 12>
  %1543 = add <4 x i32> %1537, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1544 = ashr <4 x i32> %1543, <i32 12, i32 12, i32 12, i32 12>
  %1545 = add <4 x i32> %1538, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1546 = ashr <4 x i32> %1545, <i32 12, i32 12, i32 12, i32 12>
  %1547 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1540, <4 x i32> %1544) #8
  %1548 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1542, <4 x i32> %1546) #8
  %1549 = shufflevector <8 x i16> %1410, <8 x i16> %1401, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1550 = shufflevector <8 x i16> %1401, <8 x i16> %1410, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1551 = shufflevector <8 x i16> %1410, <8 x i16> %1401, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1552 = shufflevector <8 x i16> %1401, <8 x i16> %1410, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1553 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1549, <8 x i16> %478) #8
  %1554 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1550, <8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>) #8
  %1555 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1551, <8 x i16> %478) #8
  %1556 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1552, <8 x i16> <i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567, i16 -3784, i16 1567>) #8
  %1557 = add <4 x i32> %1553, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1558 = ashr <4 x i32> %1557, <i32 12, i32 12, i32 12, i32 12>
  %1559 = add <4 x i32> %1554, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1560 = ashr <4 x i32> %1559, <i32 12, i32 12, i32 12, i32 12>
  %1561 = add <4 x i32> %1555, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1562 = ashr <4 x i32> %1561, <i32 12, i32 12, i32 12, i32 12>
  %1563 = add <4 x i32> %1556, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1564 = ashr <4 x i32> %1563, <i32 12, i32 12, i32 12, i32 12>
  %1565 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1558, <4 x i32> %1562) #8
  %1566 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1560, <4 x i32> %1564) #8
  %1567 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1383, <8 x i16> %1394) #8
  %1568 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1383, <8 x i16> %1394) #8
  store <8 x i16> %1567, <8 x i16>* %906, align 16
  store <8 x i16> %1568, <8 x i16>* %1000, align 16
  %1569 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1385, <8 x i16> %1396) #8
  %1570 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1385, <8 x i16> %1396) #8
  store <8 x i16> %1569, <8 x i16>* %916, align 16
  store <8 x i16> %1570, <8 x i16>* %990, align 16
  %1571 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1388, <8 x i16> %1398) #8
  %1572 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1388, <8 x i16> %1398) #8
  store <8 x i16> %1571, <8 x i16>* %918, align 16
  store <8 x i16> %1572, <8 x i16>* %988, align 16
  %1573 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1392, <8 x i16> %1400) #8
  %1574 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1392, <8 x i16> %1400) #8
  store <8 x i16> %1573, <8 x i16>* %928, align 16
  store <8 x i16> %1574, <8 x i16>* %978, align 16
  %1575 = load <8 x i16>, <8 x i16>* %930, align 16
  %1576 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1575, <8 x i16> %1565) #8
  %1577 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1575, <8 x i16> %1565) #8
  store <8 x i16> %1576, <8 x i16>* %930, align 16
  store <8 x i16> %1577, <8 x i16>* %976, align 16
  %1578 = load <8 x i16>, <8 x i16>* %940, align 16
  %1579 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1578, <8 x i16> %1547) #8
  %1580 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1578, <8 x i16> %1547) #8
  store <8 x i16> %1579, <8 x i16>* %940, align 16
  store <8 x i16> %1580, <8 x i16>* %966, align 16
  %1581 = load <8 x i16>, <8 x i16>* %942, align 16
  %1582 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1581, <8 x i16> %1529) #8
  %1583 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1581, <8 x i16> %1529) #8
  store <8 x i16> %1582, <8 x i16>* %942, align 16
  %1584 = load <8 x i16>, <8 x i16>* %952, align 16
  %1585 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1584, <8 x i16> %1511) #8
  %1586 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1584, <8 x i16> %1511) #8
  store <8 x i16> %1585, <8 x i16>* %952, align 16
  %1587 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1411, <8 x i16> %1402) #8
  %1588 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1411, <8 x i16> %1402) #8
  store <8 x i16> %1588, <8 x i16>* %996, align 16
  store <8 x i16> %1587, <8 x i16>* %910, align 16
  %1589 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1413, <8 x i16> %1404) #8
  %1590 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1413, <8 x i16> %1404) #8
  store <8 x i16> %1590, <8 x i16>* %994, align 16
  store <8 x i16> %1589, <8 x i16>* %912, align 16
  %1591 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1417, <8 x i16> %1407) #8
  %1592 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1417, <8 x i16> %1407) #8
  store <8 x i16> %1592, <8 x i16>* %984, align 16
  store <8 x i16> %1591, <8 x i16>* %922, align 16
  %1593 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1421, <8 x i16> %1409) #8
  %1594 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1421, <8 x i16> %1409) #8
  store <8 x i16> %1594, <8 x i16>* %982, align 16
  store <8 x i16> %1593, <8 x i16>* %924, align 16
  %1595 = load <8 x i16>, <8 x i16>* %934, align 16
  %1596 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1595, <8 x i16> %1566) #8
  %1597 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1595, <8 x i16> %1566) #8
  store <8 x i16> %1596, <8 x i16>* %934, align 16
  %1598 = load <8 x i16>, <8 x i16>* %936, align 16
  %1599 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1598, <8 x i16> %1548) #8
  %1600 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1598, <8 x i16> %1548) #8
  store <8 x i16> %1599, <8 x i16>* %936, align 16
  %1601 = load <8 x i16>, <8 x i16>* %946, align 16
  %1602 = load <8 x i16>, <8 x i16>* %960, align 16
  %1603 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1601, <8 x i16> %1602) #8
  %1604 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1601, <8 x i16> %1602) #8
  store <8 x i16> %1603, <8 x i16>* %946, align 16
  %1605 = load <8 x i16>, <8 x i16>* %948, align 16
  %1606 = load <8 x i16>, <8 x i16>* %958, align 16
  %1607 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1605, <8 x i16> %1606) #8
  %1608 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1605, <8 x i16> %1606) #8
  store <8 x i16> %1607, <8 x i16>* %948, align 16
  %1609 = shufflevector <8 x i16> %1608, <8 x i16> %1586, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1610 = shufflevector <8 x i16> %1586, <8 x i16> %1608, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1611 = shufflevector <8 x i16> %1608, <8 x i16> %1586, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1612 = shufflevector <8 x i16> %1586, <8 x i16> %1608, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1613 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1609, <8 x i16> %400) #8
  %1614 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1610, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1615 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1611, <8 x i16> %400) #8
  %1616 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1612, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1617 = add <4 x i32> %1613, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1618 = ashr <4 x i32> %1617, <i32 12, i32 12, i32 12, i32 12>
  %1619 = add <4 x i32> %1614, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1620 = ashr <4 x i32> %1619, <i32 12, i32 12, i32 12, i32 12>
  %1621 = add <4 x i32> %1615, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1622 = ashr <4 x i32> %1621, <i32 12, i32 12, i32 12, i32 12>
  %1623 = add <4 x i32> %1616, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1624 = ashr <4 x i32> %1623, <i32 12, i32 12, i32 12, i32 12>
  %1625 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1618, <4 x i32> %1622) #8
  %1626 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1620, <4 x i32> %1624) #8
  store <8 x i16> %1626, <8 x i16>* %958, align 16
  store <8 x i16> %1625, <8 x i16>* %954, align 16
  %1627 = shufflevector <8 x i16> %1604, <8 x i16> %1583, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1628 = shufflevector <8 x i16> %1583, <8 x i16> %1604, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1629 = shufflevector <8 x i16> %1604, <8 x i16> %1583, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1630 = shufflevector <8 x i16> %1583, <8 x i16> %1604, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1631 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1627, <8 x i16> %400) #8
  %1632 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1628, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1633 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1629, <8 x i16> %400) #8
  %1634 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1630, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1635 = add <4 x i32> %1631, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1636 = ashr <4 x i32> %1635, <i32 12, i32 12, i32 12, i32 12>
  %1637 = add <4 x i32> %1632, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1638 = ashr <4 x i32> %1637, <i32 12, i32 12, i32 12, i32 12>
  %1639 = add <4 x i32> %1633, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1640 = ashr <4 x i32> %1639, <i32 12, i32 12, i32 12, i32 12>
  %1641 = add <4 x i32> %1634, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1642 = ashr <4 x i32> %1641, <i32 12, i32 12, i32 12, i32 12>
  %1643 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1636, <4 x i32> %1640) #8
  %1644 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1638, <4 x i32> %1642) #8
  store <8 x i16> %1644, <8 x i16>* %960, align 16
  store <8 x i16> %1643, <8 x i16>* %964, align 16
  %1645 = shufflevector <8 x i16> %1600, <8 x i16> %1580, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1646 = shufflevector <8 x i16> %1580, <8 x i16> %1600, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1647 = shufflevector <8 x i16> %1600, <8 x i16> %1580, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1648 = shufflevector <8 x i16> %1580, <8 x i16> %1600, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1649 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1645, <8 x i16> %400) #8
  %1650 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1646, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1651 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1647, <8 x i16> %400) #8
  %1652 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1648, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1653 = add <4 x i32> %1649, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1654 = ashr <4 x i32> %1653, <i32 12, i32 12, i32 12, i32 12>
  %1655 = add <4 x i32> %1650, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1656 = ashr <4 x i32> %1655, <i32 12, i32 12, i32 12, i32 12>
  %1657 = add <4 x i32> %1651, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1658 = ashr <4 x i32> %1657, <i32 12, i32 12, i32 12, i32 12>
  %1659 = add <4 x i32> %1652, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1660 = ashr <4 x i32> %1659, <i32 12, i32 12, i32 12, i32 12>
  %1661 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1654, <4 x i32> %1658) #8
  %1662 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1656, <4 x i32> %1660) #8
  store <8 x i16> %1662, <8 x i16>* %970, align 16
  store <8 x i16> %1661, <8 x i16>* %966, align 16
  %1663 = shufflevector <8 x i16> %1597, <8 x i16> %1577, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1664 = shufflevector <8 x i16> %1577, <8 x i16> %1597, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1665 = shufflevector <8 x i16> %1597, <8 x i16> %1577, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1666 = shufflevector <8 x i16> %1577, <8 x i16> %1597, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1667 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1663, <8 x i16> %400) #8
  %1668 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1664, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1669 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1665, <8 x i16> %400) #8
  %1670 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1666, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1671 = add <4 x i32> %1667, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1672 = ashr <4 x i32> %1671, <i32 12, i32 12, i32 12, i32 12>
  %1673 = add <4 x i32> %1668, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1674 = ashr <4 x i32> %1673, <i32 12, i32 12, i32 12, i32 12>
  %1675 = add <4 x i32> %1669, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1676 = ashr <4 x i32> %1675, <i32 12, i32 12, i32 12, i32 12>
  %1677 = add <4 x i32> %1670, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1678 = ashr <4 x i32> %1677, <i32 12, i32 12, i32 12, i32 12>
  %1679 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1672, <4 x i32> %1676) #8
  %1680 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1674, <4 x i32> %1678) #8
  store <8 x i16> %1680, <8 x i16>* %972, align 16
  store <8 x i16> %1679, <8 x i16>* %976, align 16
  %1681 = shufflevector <8 x i16> %1594, <8 x i16> %1574, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1682 = shufflevector <8 x i16> %1574, <8 x i16> %1594, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1683 = shufflevector <8 x i16> %1594, <8 x i16> %1574, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1684 = shufflevector <8 x i16> %1574, <8 x i16> %1594, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1685 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1681, <8 x i16> %400) #8
  %1686 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1682, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1687 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1683, <8 x i16> %400) #8
  %1688 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1684, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1689 = add <4 x i32> %1685, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1690 = ashr <4 x i32> %1689, <i32 12, i32 12, i32 12, i32 12>
  %1691 = add <4 x i32> %1686, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1692 = ashr <4 x i32> %1691, <i32 12, i32 12, i32 12, i32 12>
  %1693 = add <4 x i32> %1687, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1694 = ashr <4 x i32> %1693, <i32 12, i32 12, i32 12, i32 12>
  %1695 = add <4 x i32> %1688, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1696 = ashr <4 x i32> %1695, <i32 12, i32 12, i32 12, i32 12>
  %1697 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1690, <4 x i32> %1694) #8
  %1698 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1692, <4 x i32> %1696) #8
  store <8 x i16> %1698, <8 x i16>* %982, align 16
  store <8 x i16> %1697, <8 x i16>* %978, align 16
  %1699 = shufflevector <8 x i16> %1592, <8 x i16> %1572, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1700 = shufflevector <8 x i16> %1572, <8 x i16> %1592, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1701 = shufflevector <8 x i16> %1592, <8 x i16> %1572, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1702 = shufflevector <8 x i16> %1572, <8 x i16> %1592, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1703 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1699, <8 x i16> %400) #8
  %1704 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1700, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1705 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1701, <8 x i16> %400) #8
  %1706 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1702, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1707 = add <4 x i32> %1703, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1708 = ashr <4 x i32> %1707, <i32 12, i32 12, i32 12, i32 12>
  %1709 = add <4 x i32> %1704, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1710 = ashr <4 x i32> %1709, <i32 12, i32 12, i32 12, i32 12>
  %1711 = add <4 x i32> %1705, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1712 = ashr <4 x i32> %1711, <i32 12, i32 12, i32 12, i32 12>
  %1713 = add <4 x i32> %1706, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1714 = ashr <4 x i32> %1713, <i32 12, i32 12, i32 12, i32 12>
  %1715 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1708, <4 x i32> %1712) #8
  %1716 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1710, <4 x i32> %1714) #8
  store <8 x i16> %1716, <8 x i16>* %984, align 16
  store <8 x i16> %1715, <8 x i16>* %988, align 16
  %1717 = shufflevector <8 x i16> %1590, <8 x i16> %1570, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1718 = shufflevector <8 x i16> %1570, <8 x i16> %1590, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1719 = shufflevector <8 x i16> %1590, <8 x i16> %1570, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1720 = shufflevector <8 x i16> %1570, <8 x i16> %1590, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1721 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1717, <8 x i16> %400) #8
  %1722 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1718, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1723 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1719, <8 x i16> %400) #8
  %1724 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1720, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1725 = add <4 x i32> %1721, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1726 = ashr <4 x i32> %1725, <i32 12, i32 12, i32 12, i32 12>
  %1727 = add <4 x i32> %1722, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1728 = ashr <4 x i32> %1727, <i32 12, i32 12, i32 12, i32 12>
  %1729 = add <4 x i32> %1723, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1730 = ashr <4 x i32> %1729, <i32 12, i32 12, i32 12, i32 12>
  %1731 = add <4 x i32> %1724, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1732 = ashr <4 x i32> %1731, <i32 12, i32 12, i32 12, i32 12>
  %1733 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1726, <4 x i32> %1730) #8
  %1734 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1728, <4 x i32> %1732) #8
  store <8 x i16> %1734, <8 x i16>* %994, align 16
  store <8 x i16> %1733, <8 x i16>* %990, align 16
  %1735 = shufflevector <8 x i16> %1588, <8 x i16> %1568, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1736 = shufflevector <8 x i16> %1568, <8 x i16> %1588, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1737 = shufflevector <8 x i16> %1588, <8 x i16> %1568, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1738 = shufflevector <8 x i16> %1568, <8 x i16> %1588, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1739 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1735, <8 x i16> %400) #8
  %1740 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1736, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1741 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1737, <8 x i16> %400) #8
  %1742 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1738, <8 x i16> <i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896, i16 2896>) #8
  %1743 = add <4 x i32> %1739, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1744 = ashr <4 x i32> %1743, <i32 12, i32 12, i32 12, i32 12>
  %1745 = add <4 x i32> %1740, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1746 = ashr <4 x i32> %1745, <i32 12, i32 12, i32 12, i32 12>
  %1747 = add <4 x i32> %1741, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1748 = ashr <4 x i32> %1747, <i32 12, i32 12, i32 12, i32 12>
  %1749 = add <4 x i32> %1742, <i32 2048, i32 2048, i32 2048, i32 2048>
  %1750 = ashr <4 x i32> %1749, <i32 12, i32 12, i32 12, i32 12>
  %1751 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1744, <4 x i32> %1748) #8
  %1752 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1746, <4 x i32> %1750) #8
  store <8 x i16> %1752, <8 x i16>* %996, align 16
  store <8 x i16> %1751, <8 x i16>* %1000, align 16
  br label %1762

1753:                                             ; preds = %1762
  %1754 = sext i32 %1 to i64
  br i1 %2, label %1755, label %1896

1755:                                             ; preds = %1753
  %1756 = shl nsw i64 %1754, 1
  %1757 = mul nsw i64 %1754, 3
  %1758 = shl nsw i64 %1754, 2
  %1759 = mul nsw i64 %1754, 5
  %1760 = mul nsw i64 %1754, 6
  %1761 = mul nsw i64 %1754, 7
  br label %1805

1762:                                             ; preds = %284, %1762
  %1763 = phi i64 [ 0, %284 ], [ %1803, %1762 ]
  %1764 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 %1763
  %1765 = sub nuw nsw i64 63, %1763
  %1766 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 %1765
  %1767 = bitcast <2 x i64>* %1764 to <8 x i16>*
  %1768 = load <8 x i16>, <8 x i16>* %1767, align 16
  %1769 = bitcast <2 x i64>* %1766 to <8 x i16>*
  %1770 = load <8 x i16>, <8 x i16>* %1769, align 16
  %1771 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1768, <8 x i16> %1770) #8
  %1772 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1768, <8 x i16> %1770) #8
  store <8 x i16> %1771, <8 x i16>* %1767, align 16
  store <8 x i16> %1772, <8 x i16>* %1769, align 16
  %1773 = or i64 %1763, 1
  %1774 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 %1773
  %1775 = sub nuw nsw i64 62, %1763
  %1776 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 %1775
  %1777 = bitcast <2 x i64>* %1774 to <8 x i16>*
  %1778 = load <8 x i16>, <8 x i16>* %1777, align 16
  %1779 = bitcast <2 x i64>* %1776 to <8 x i16>*
  %1780 = load <8 x i16>, <8 x i16>* %1779, align 16
  %1781 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1778, <8 x i16> %1780) #8
  %1782 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1778, <8 x i16> %1780) #8
  store <8 x i16> %1781, <8 x i16>* %1777, align 16
  store <8 x i16> %1782, <8 x i16>* %1779, align 16
  %1783 = or i64 %1763, 2
  %1784 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 %1783
  %1785 = sub nuw nsw i64 61, %1763
  %1786 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 %1785
  %1787 = bitcast <2 x i64>* %1784 to <8 x i16>*
  %1788 = load <8 x i16>, <8 x i16>* %1787, align 16
  %1789 = bitcast <2 x i64>* %1786 to <8 x i16>*
  %1790 = load <8 x i16>, <8 x i16>* %1789, align 16
  %1791 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1788, <8 x i16> %1790) #8
  %1792 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1788, <8 x i16> %1790) #8
  store <8 x i16> %1791, <8 x i16>* %1787, align 16
  store <8 x i16> %1792, <8 x i16>* %1789, align 16
  %1793 = or i64 %1763, 3
  %1794 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 %1793
  %1795 = sub nuw nsw i64 60, %1763
  %1796 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 %1795
  %1797 = bitcast <2 x i64>* %1794 to <8 x i16>*
  %1798 = load <8 x i16>, <8 x i16>* %1797, align 16
  %1799 = bitcast <2 x i64>* %1796 to <8 x i16>*
  %1800 = load <8 x i16>, <8 x i16>* %1799, align 16
  %1801 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1798, <8 x i16> %1800) #8
  %1802 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1798, <8 x i16> %1800) #8
  store <8 x i16> %1801, <8 x i16>* %1797, align 16
  store <8 x i16> %1802, <8 x i16>* %1799, align 16
  %1803 = add nuw nsw i64 %1763, 4
  %1804 = icmp ult i64 %1803, 32
  br i1 %1804, label %1762, label %1753

1805:                                             ; preds = %1755, %1805
  %1806 = phi i64 [ 0, %1755 ], [ %1894, %1805 ]
  %1807 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 %1806
  %1808 = bitcast <2 x i64>* %1807 to <8 x i16>*
  %1809 = load <8 x i16>, <8 x i16>* %1808, align 16
  %1810 = getelementptr inbounds <2 x i64>, <2 x i64>* %1807, i64 1
  %1811 = bitcast <2 x i64>* %1810 to <8 x i16>*
  %1812 = load <8 x i16>, <8 x i16>* %1811, align 16
  %1813 = shufflevector <8 x i16> %1809, <8 x i16> %1812, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1814 = getelementptr inbounds <2 x i64>, <2 x i64>* %1807, i64 2
  %1815 = bitcast <2 x i64>* %1814 to <8 x i16>*
  %1816 = load <8 x i16>, <8 x i16>* %1815, align 16
  %1817 = getelementptr inbounds <2 x i64>, <2 x i64>* %1807, i64 3
  %1818 = bitcast <2 x i64>* %1817 to <8 x i16>*
  %1819 = load <8 x i16>, <8 x i16>* %1818, align 16
  %1820 = shufflevector <8 x i16> %1816, <8 x i16> %1819, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1821 = getelementptr inbounds <2 x i64>, <2 x i64>* %1807, i64 4
  %1822 = bitcast <2 x i64>* %1821 to <8 x i16>*
  %1823 = load <8 x i16>, <8 x i16>* %1822, align 16
  %1824 = getelementptr inbounds <2 x i64>, <2 x i64>* %1807, i64 5
  %1825 = bitcast <2 x i64>* %1824 to <8 x i16>*
  %1826 = load <8 x i16>, <8 x i16>* %1825, align 16
  %1827 = shufflevector <8 x i16> %1823, <8 x i16> %1826, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1828 = getelementptr inbounds <2 x i64>, <2 x i64>* %1807, i64 6
  %1829 = bitcast <2 x i64>* %1828 to <8 x i16>*
  %1830 = load <8 x i16>, <8 x i16>* %1829, align 16
  %1831 = getelementptr inbounds <2 x i64>, <2 x i64>* %1807, i64 7
  %1832 = bitcast <2 x i64>* %1831 to <8 x i16>*
  %1833 = load <8 x i16>, <8 x i16>* %1832, align 16
  %1834 = shufflevector <8 x i16> %1830, <8 x i16> %1833, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1835 = shufflevector <8 x i16> %1809, <8 x i16> %1812, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1836 = shufflevector <8 x i16> %1816, <8 x i16> %1819, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1837 = shufflevector <8 x i16> %1823, <8 x i16> %1826, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1838 = shufflevector <8 x i16> %1830, <8 x i16> %1833, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1839 = bitcast <8 x i16> %1813 to <4 x i32>
  %1840 = bitcast <8 x i16> %1820 to <4 x i32>
  %1841 = shufflevector <4 x i32> %1839, <4 x i32> %1840, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1842 = bitcast <4 x i32> %1841 to <2 x i64>
  %1843 = bitcast <8 x i16> %1827 to <4 x i32>
  %1844 = bitcast <8 x i16> %1834 to <4 x i32>
  %1845 = shufflevector <4 x i32> %1843, <4 x i32> %1844, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1846 = bitcast <4 x i32> %1845 to <2 x i64>
  %1847 = bitcast <8 x i16> %1835 to <4 x i32>
  %1848 = bitcast <8 x i16> %1836 to <4 x i32>
  %1849 = shufflevector <4 x i32> %1847, <4 x i32> %1848, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1850 = bitcast <4 x i32> %1849 to <2 x i64>
  %1851 = bitcast <8 x i16> %1837 to <4 x i32>
  %1852 = bitcast <8 x i16> %1838 to <4 x i32>
  %1853 = shufflevector <4 x i32> %1851, <4 x i32> %1852, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1854 = bitcast <4 x i32> %1853 to <2 x i64>
  %1855 = shufflevector <4 x i32> %1839, <4 x i32> %1840, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1856 = bitcast <4 x i32> %1855 to <2 x i64>
  %1857 = shufflevector <4 x i32> %1843, <4 x i32> %1844, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1858 = bitcast <4 x i32> %1857 to <2 x i64>
  %1859 = shufflevector <4 x i32> %1847, <4 x i32> %1848, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1860 = bitcast <4 x i32> %1859 to <2 x i64>
  %1861 = shufflevector <4 x i32> %1851, <4 x i32> %1852, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1862 = bitcast <4 x i32> %1861 to <2 x i64>
  %1863 = shufflevector <2 x i64> %1842, <2 x i64> %1846, <2 x i32> <i32 0, i32 2>
  %1864 = shufflevector <2 x i64> %1842, <2 x i64> %1846, <2 x i32> <i32 1, i32 3>
  %1865 = shufflevector <2 x i64> %1856, <2 x i64> %1858, <2 x i32> <i32 0, i32 2>
  %1866 = shufflevector <2 x i64> %1856, <2 x i64> %1858, <2 x i32> <i32 1, i32 3>
  %1867 = shufflevector <2 x i64> %1850, <2 x i64> %1854, <2 x i32> <i32 0, i32 2>
  %1868 = shufflevector <2 x i64> %1850, <2 x i64> %1854, <2 x i32> <i32 1, i32 3>
  %1869 = shufflevector <2 x i64> %1860, <2 x i64> %1862, <2 x i32> <i32 0, i32 2>
  %1870 = shufflevector <2 x i64> %1860, <2 x i64> %1862, <2 x i32> <i32 1, i32 3>
  %1871 = getelementptr inbounds i16, i16* %6, i64 %1806
  %1872 = bitcast i16* %1871 to <2 x i64>*
  store <2 x i64> %1863, <2 x i64>* %1872, align 1
  %1873 = add nsw i64 %1806, %1754
  %1874 = getelementptr inbounds i16, i16* %6, i64 %1873
  %1875 = bitcast i16* %1874 to <2 x i64>*
  store <2 x i64> %1864, <2 x i64>* %1875, align 1
  %1876 = add nsw i64 %1756, %1806
  %1877 = getelementptr inbounds i16, i16* %6, i64 %1876
  %1878 = bitcast i16* %1877 to <2 x i64>*
  store <2 x i64> %1865, <2 x i64>* %1878, align 1
  %1879 = add nsw i64 %1757, %1806
  %1880 = getelementptr inbounds i16, i16* %6, i64 %1879
  %1881 = bitcast i16* %1880 to <2 x i64>*
  store <2 x i64> %1866, <2 x i64>* %1881, align 1
  %1882 = add nsw i64 %1758, %1806
  %1883 = getelementptr inbounds i16, i16* %6, i64 %1882
  %1884 = bitcast i16* %1883 to <2 x i64>*
  store <2 x i64> %1867, <2 x i64>* %1884, align 1
  %1885 = add nsw i64 %1759, %1806
  %1886 = getelementptr inbounds i16, i16* %6, i64 %1885
  %1887 = bitcast i16* %1886 to <2 x i64>*
  store <2 x i64> %1868, <2 x i64>* %1887, align 1
  %1888 = add nsw i64 %1760, %1806
  %1889 = getelementptr inbounds i16, i16* %6, i64 %1888
  %1890 = bitcast i16* %1889 to <2 x i64>*
  store <2 x i64> %1869, <2 x i64>* %1890, align 1
  %1891 = add nsw i64 %1761, %1806
  %1892 = getelementptr inbounds i16, i16* %6, i64 %1891
  %1893 = bitcast i16* %1892 to <2 x i64>*
  store <2 x i64> %1870, <2 x i64>* %1893, align 1
  %1894 = add nuw nsw i64 %1806, 8
  %1895 = icmp ult i64 %1894, 64
  br i1 %1895, label %1805, label %1923

1896:                                             ; preds = %1753, %1896
  %1897 = phi i64 [ %1921, %1896 ], [ 0, %1753 ]
  %1898 = mul nsw i64 %1897, %1754
  %1899 = getelementptr inbounds i16, i16* %6, i64 %1898
  %1900 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 %1897
  %1901 = load <2 x i64>, <2 x i64>* %1900, align 16
  %1902 = bitcast i16* %1899 to <2 x i64>*
  store <2 x i64> %1901, <2 x i64>* %1902, align 1
  %1903 = or i64 %1897, 1
  %1904 = mul nsw i64 %1903, %1754
  %1905 = getelementptr inbounds i16, i16* %6, i64 %1904
  %1906 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 %1903
  %1907 = load <2 x i64>, <2 x i64>* %1906, align 16
  %1908 = bitcast i16* %1905 to <2 x i64>*
  store <2 x i64> %1907, <2 x i64>* %1908, align 1
  %1909 = or i64 %1897, 2
  %1910 = mul nsw i64 %1909, %1754
  %1911 = getelementptr inbounds i16, i16* %6, i64 %1910
  %1912 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 %1909
  %1913 = load <2 x i64>, <2 x i64>* %1912, align 16
  %1914 = bitcast i16* %1911 to <2 x i64>*
  store <2 x i64> %1913, <2 x i64>* %1914, align 1
  %1915 = or i64 %1897, 3
  %1916 = mul nsw i64 %1915, %1754
  %1917 = getelementptr inbounds i16, i16* %6, i64 %1916
  %1918 = getelementptr inbounds [64 x <2 x i64>], [64 x <2 x i64>]* %4, i64 0, i64 %1915
  %1919 = load <2 x i64>, <2 x i64>* %1918, align 16
  %1920 = bitcast i16* %1917 to <2 x i64>*
  store <2 x i64> %1919, <2 x i64>* %1920, align 1
  %1921 = add nuw nsw i64 %1897, 4
  %1922 = icmp ult i64 %1921, 64
  br i1 %1922, label %1896, label %1923

1923:                                             ; preds = %1896, %1805
  call void @llvm.lifetime.end.p0i8(i64 512, i8* nonnull %8) #8
  call void @llvm.lifetime.end.p0i8(i64 1024, i8* nonnull %7) #8
  ret void
}

attributes #0 = { nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+sse4.1,+ssse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { argmemonly nounwind }
attributes #2 = { "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+sse4.1,+ssse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #3 = { nofree nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="128" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+sse4.1,+ssse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #4 = { nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="128" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+sse4.1,+ssse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #5 = { norecurse nounwind readnone ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+sse4.1,+ssse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #6 = { nounwind readnone }
attributes #7 = { nounwind readnone speculatable }
attributes #8 = { nounwind }

!llvm.module.flags = !{!0, !1}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 2}
!2 = distinct !{!2, !3}
!3 = !{!"llvm.loop.unroll.disable"}
!4 = distinct !{!4, !3}
!5 = distinct !{!5, !3}
