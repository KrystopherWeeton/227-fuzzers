; ModuleID = '../../third_party/libvpx/source/libvpx/vpx_dsp/x86/loopfilter_avx2.c'
source_filename = "../../third_party/libvpx/source/libvpx/vpx_dsp/x86/loopfilter_avx2.c"
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

; Function Attrs: nofree nounwind ssp uwtable
define hidden void @vpx_lpf_horizontal_16_avx2(i8* nocapture, i32, i8* nocapture readonly, i8* nocapture readonly, i8* nocapture readonly) local_unnamed_addr #0 {
  %6 = load i8, i8* %4, align 1
  %7 = zext i8 %6 to i32
  %8 = insertelement <4 x i32> undef, i32 %7, i32 0
  %9 = bitcast <4 x i32> %8 to <16 x i8>
  %10 = shufflevector <16 x i8> %9, <16 x i8> undef, <16 x i32> zeroinitializer
  %11 = load i8, i8* %3, align 1
  %12 = zext i8 %11 to i32
  %13 = insertelement <4 x i32> undef, i32 %12, i32 0
  %14 = bitcast <4 x i32> %13 to <16 x i8>
  %15 = shufflevector <16 x i8> %14, <16 x i8> undef, <16 x i32> zeroinitializer
  %16 = load i8, i8* %2, align 1
  %17 = zext i8 %16 to i32
  %18 = insertelement <4 x i32> undef, i32 %17, i32 0
  %19 = bitcast <4 x i32> %18 to <16 x i8>
  %20 = shufflevector <16 x i8> %19, <16 x i8> undef, <16 x i32> zeroinitializer
  %21 = mul nsw i32 %1, 5
  %22 = sext i32 %21 to i64
  %23 = sub nsw i64 0, %22
  %24 = getelementptr inbounds i8, i8* %0, i64 %23
  %25 = bitcast i8* %24 to i64*
  %26 = load i64, i64* %25, align 1
  %27 = insertelement <2 x i64> undef, i64 %26, i32 0
  %28 = bitcast <2 x i64> %27 to <4 x float>
  %29 = shl nsw i32 %1, 2
  %30 = sext i32 %29 to i64
  %31 = getelementptr inbounds i8, i8* %0, i64 %30
  %32 = bitcast i8* %31 to <2 x float>*
  %33 = load <2 x float>, <2 x float>* %32, align 1
  %34 = shufflevector <2 x float> %33, <2 x float> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %35 = shufflevector <4 x float> %28, <4 x float> %34, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
  %36 = bitcast <4 x float> %35 to <2 x i64>
  %37 = sub nsw i64 0, %30
  %38 = getelementptr inbounds i8, i8* %0, i64 %37
  %39 = bitcast i8* %38 to i64*
  %40 = load i64, i64* %39, align 1
  %41 = insertelement <2 x i64> undef, i64 %40, i32 0
  %42 = bitcast <2 x i64> %41 to <4 x float>
  %43 = mul nsw i32 %1, 3
  %44 = sext i32 %43 to i64
  %45 = getelementptr inbounds i8, i8* %0, i64 %44
  %46 = bitcast i8* %45 to <2 x float>*
  %47 = load <2 x float>, <2 x float>* %46, align 1
  %48 = shufflevector <2 x float> %47, <2 x float> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %49 = shufflevector <4 x float> %42, <4 x float> %48, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
  %50 = bitcast <4 x float> %49 to <2 x i64>
  %51 = sub nsw i64 0, %44
  %52 = getelementptr inbounds i8, i8* %0, i64 %51
  %53 = bitcast i8* %52 to i64*
  %54 = load i64, i64* %53, align 1
  %55 = insertelement <2 x i64> undef, i64 %54, i32 0
  %56 = bitcast <2 x i64> %55 to <4 x float>
  %57 = shl nsw i32 %1, 1
  %58 = sext i32 %57 to i64
  %59 = getelementptr inbounds i8, i8* %0, i64 %58
  %60 = bitcast i8* %59 to <2 x float>*
  %61 = load <2 x float>, <2 x float>* %60, align 1
  %62 = shufflevector <2 x float> %61, <2 x float> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %63 = shufflevector <4 x float> %56, <4 x float> %62, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
  %64 = bitcast <4 x float> %63 to <2 x i64>
  %65 = sub nsw i64 0, %58
  %66 = getelementptr inbounds i8, i8* %0, i64 %65
  %67 = bitcast i8* %66 to i64*
  %68 = load i64, i64* %67, align 1
  %69 = insertelement <2 x i64> undef, i64 %68, i32 0
  %70 = bitcast <2 x i64> %69 to <4 x float>
  %71 = sext i32 %1 to i64
  %72 = getelementptr inbounds i8, i8* %0, i64 %71
  %73 = bitcast i8* %72 to <2 x float>*
  %74 = load <2 x float>, <2 x float>* %73, align 1
  %75 = shufflevector <2 x float> %74, <2 x float> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %76 = shufflevector <4 x float> %70, <4 x float> %75, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
  %77 = bitcast <4 x float> %76 to <4 x i32>
  %78 = shufflevector <4 x i32> %77, <4 x i32> undef, <4 x i32> <i32 2, i32 3, i32 0, i32 1>
  %79 = sub nsw i64 0, %71
  %80 = getelementptr inbounds i8, i8* %0, i64 %79
  %81 = bitcast i8* %80 to i64*
  %82 = load i64, i64* %81, align 1
  %83 = insertelement <2 x i64> undef, i64 %82, i32 0
  %84 = bitcast <2 x i64> %83 to <4 x float>
  %85 = bitcast i8* %0 to <2 x float>*
  %86 = load <2 x float>, <2 x float>* %85, align 1
  %87 = shufflevector <2 x float> %86, <2 x float> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %88 = shufflevector <4 x float> %84, <4 x float> %87, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
  %89 = bitcast <4 x float> %88 to <4 x i32>
  %90 = shufflevector <4 x i32> %89, <4 x i32> undef, <4 x i32> <i32 2, i32 3, i32 0, i32 1>
  %91 = bitcast <4 x float> %76 to <16 x i8>
  %92 = bitcast <4 x float> %88 to <16 x i8>
  %93 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %91, <16 x i8> %92) #4
  %94 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %92, <16 x i8> %91) #4
  %95 = or <16 x i8> %94, %93
  %96 = shufflevector <16 x i8> %95, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %97 = bitcast <4 x i32> %90 to <16 x i8>
  %98 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %92, <16 x i8> %97) #4
  %99 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %97, <16 x i8> %92) #4
  %100 = or <16 x i8> %99, %98
  %101 = bitcast <4 x i32> %78 to <16 x i8>
  %102 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %91, <16 x i8> %101) #4
  %103 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %101, <16 x i8> %91) #4
  %104 = or <16 x i8> %103, %102
  %105 = icmp ugt <16 x i8> %95, %96
  %106 = select <16 x i1> %105, <16 x i8> %95, <16 x i8> %96
  %107 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %106, <16 x i8> %10) #4
  %108 = icmp ne <16 x i8> %107, zeroinitializer
  %109 = tail call <16 x i8> @llvm.uadd.sat.v16i8(<16 x i8> %100, <16 x i8> %100) #4
  %110 = bitcast <16 x i8> %104 to <8 x i16>
  %111 = lshr <8 x i16> %110, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %112 = bitcast <8 x i16> %111 to <16 x i8>
  %113 = and <16 x i8> %112, <i8 127, i8 127, i8 127, i8 127, i8 127, i8 127, i8 127, i8 127, i8 127, i8 127, i8 127, i8 127, i8 127, i8 127, i8 127, i8 127>
  %114 = tail call <16 x i8> @llvm.uadd.sat.v16i8(<16 x i8> %109, <16 x i8> %113) #4
  %115 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %114, <16 x i8> %20) #4
  %116 = icmp ne <16 x i8> %115, zeroinitializer
  %117 = sext <16 x i1> %116 to <16 x i8>
  %118 = icmp ugt <16 x i8> %95, %117
  %119 = select <16 x i1> %118, <16 x i8> %95, <16 x i8> %117
  %120 = bitcast <4 x float> %63 to <16 x i8>
  %121 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %120, <16 x i8> %91) #4
  %122 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %91, <16 x i8> %120) #4
  %123 = or <16 x i8> %122, %121
  %124 = bitcast <4 x float> %49 to <16 x i8>
  %125 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %124, <16 x i8> %120) #4
  %126 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %120, <16 x i8> %124) #4
  %127 = or <16 x i8> %126, %125
  %128 = icmp ugt <16 x i8> %123, %127
  %129 = select <16 x i1> %128, <16 x i8> %123, <16 x i8> %127
  %130 = icmp ugt <16 x i8> %129, %119
  %131 = select <16 x i1> %130, <16 x i8> %129, <16 x i8> %119
  %132 = shufflevector <16 x i8> %131, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %133 = icmp ugt <16 x i8> %131, %132
  %134 = select <16 x i1> %133, <16 x i8> %131, <16 x i8> %132
  %135 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %134, <16 x i8> %15) #4
  %136 = icmp eq <16 x i8> %135, zeroinitializer
  %137 = xor <16 x i8> %91, <i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128>
  %138 = xor <16 x i8> %101, <i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128>
  %139 = tail call <16 x i8> @llvm.ssub.sat.v16i8(<16 x i8> %137, <16 x i8> %138) #4
  %140 = xor <16 x i8> %97, <i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128>
  %141 = xor <16 x i8> %92, <i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128>
  %142 = tail call <16 x i8> @llvm.ssub.sat.v16i8(<16 x i8> %140, <16 x i8> %141) #4
  %143 = sext <16 x i1> %108 to <16 x i8>
  %144 = and <16 x i8> %139, %143
  %145 = tail call <16 x i8> @llvm.sadd.sat.v16i8(<16 x i8> %144, <16 x i8> %142) #4
  %146 = tail call <16 x i8> @llvm.sadd.sat.v16i8(<16 x i8> %145, <16 x i8> %142) #4
  %147 = tail call <16 x i8> @llvm.sadd.sat.v16i8(<16 x i8> %146, <16 x i8> %142) #4
  %148 = select <16 x i1> %136, <16 x i8> %147, <16 x i8> zeroinitializer
  %149 = tail call <16 x i8> @llvm.sadd.sat.v16i8(<16 x i8> %148, <16 x i8> <i8 4, i8 4, i8 4, i8 4, i8 4, i8 4, i8 4, i8 4, i8 4, i8 4, i8 4, i8 4, i8 4, i8 4, i8 4, i8 4>) #4
  %150 = tail call <16 x i8> @llvm.sadd.sat.v16i8(<16 x i8> %148, <16 x i8> <i8 3, i8 3, i8 3, i8 3, i8 3, i8 3, i8 3, i8 3, i8 3, i8 3, i8 3, i8 3, i8 3, i8 3, i8 3, i8 3>) #4
  %151 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %149, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %152 = bitcast <16 x i8> %151 to <8 x i16>
  %153 = ashr <8 x i16> %152, <i16 11, i16 11, i16 11, i16 11, i16 11, i16 11, i16 11, i16 11>
  %154 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %150, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %155 = bitcast <16 x i8> %154 to <8 x i16>
  %156 = ashr <8 x i16> %155, <i16 11, i16 11, i16 11, i16 11, i16 11, i16 11, i16 11, i16 11>
  %157 = sub nsw <8 x i16> zeroinitializer, %153
  %158 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %156, <8 x i16> %157) #4
  %159 = tail call <16 x i8> @llvm.sadd.sat.v16i8(<16 x i8> %141, <16 x i8> %158) #4
  %160 = bitcast <16 x i8> %159 to <2 x i64>
  %161 = xor <2 x i64> %160, <i64 -9187201950435737472, i64 -9187201950435737472>
  %162 = add nsw <8 x i16> %153, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %163 = ashr <8 x i16> %162, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %164 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %143, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %165 = bitcast <16 x i8> %164 to <8 x i16>
  %166 = ashr <8 x i16> %165, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %167 = xor <8 x i16> %166, <i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1>
  %168 = and <8 x i16> %163, %167
  %169 = sub nsw <8 x i16> zeroinitializer, %168
  %170 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %168, <8 x i16> %169) #4
  %171 = tail call <16 x i8> @llvm.sadd.sat.v16i8(<16 x i8> %137, <16 x i8> %170) #4
  %172 = bitcast <16 x i8> %171 to <2 x i64>
  %173 = xor <2 x i64> %172, <i64 -9187201950435737472, i64 -9187201950435737472>
  %174 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %120, <16 x i8> %92) #4
  %175 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %92, <16 x i8> %120) #4
  %176 = or <16 x i8> %175, %174
  %177 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %124, <16 x i8> %92) #4
  %178 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %92, <16 x i8> %124) #4
  %179 = or <16 x i8> %178, %177
  %180 = icmp ugt <16 x i8> %176, %179
  %181 = select <16 x i1> %180, <16 x i8> %176, <16 x i8> %179
  %182 = icmp ugt <16 x i8> %95, %181
  %183 = select <16 x i1> %182, <16 x i8> %95, <16 x i8> %181
  %184 = shufflevector <16 x i8> %183, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %185 = icmp ugt <16 x i8> %183, %184
  %186 = select <16 x i1> %185, <16 x i8> %183, <16 x i8> %184
  %187 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %186, <16 x i8> <i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1>) #4
  %188 = icmp eq <16 x i8> %187, zeroinitializer
  %189 = and <16 x i1> %136, %188
  %190 = sext <16 x i1> %189 to <16 x i8>
  %191 = mul nsw i32 %1, 6
  %192 = sext i32 %191 to i64
  %193 = sub nsw i64 0, %192
  %194 = getelementptr inbounds i8, i8* %0, i64 %193
  %195 = bitcast i8* %194 to i64*
  %196 = load i64, i64* %195, align 1
  %197 = insertelement <2 x i64> undef, i64 %196, i32 0
  %198 = bitcast <2 x i64> %197 to <4 x float>
  %199 = getelementptr inbounds i8, i8* %0, i64 %22
  %200 = bitcast i8* %199 to <2 x float>*
  %201 = load <2 x float>, <2 x float>* %200, align 1
  %202 = shufflevector <2 x float> %201, <2 x float> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %203 = shufflevector <4 x float> %198, <4 x float> %202, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
  %204 = bitcast <4 x float> %203 to <2 x i64>
  %205 = mul nsw i32 %1, 7
  %206 = sext i32 %205 to i64
  %207 = sub nsw i64 0, %206
  %208 = getelementptr inbounds i8, i8* %0, i64 %207
  %209 = bitcast i8* %208 to i64*
  %210 = load i64, i64* %209, align 1
  %211 = insertelement <2 x i64> undef, i64 %210, i32 0
  %212 = bitcast <2 x i64> %211 to <4 x float>
  %213 = getelementptr inbounds i8, i8* %0, i64 %192
  %214 = bitcast i8* %213 to <2 x float>*
  %215 = load <2 x float>, <2 x float>* %214, align 1
  %216 = shufflevector <2 x float> %215, <2 x float> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %217 = shufflevector <4 x float> %212, <4 x float> %216, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
  %218 = bitcast <4 x float> %217 to <2 x i64>
  %219 = bitcast <4 x float> %35 to <16 x i8>
  %220 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %219, <16 x i8> %92) #4
  %221 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %92, <16 x i8> %219) #4
  %222 = or <16 x i8> %221, %220
  %223 = bitcast <4 x float> %203 to <16 x i8>
  %224 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %223, <16 x i8> %92) #4
  %225 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %92, <16 x i8> %223) #4
  %226 = or <16 x i8> %225, %224
  %227 = icmp ugt <16 x i8> %222, %226
  %228 = select <16 x i1> %227, <16 x i8> %222, <16 x i8> %226
  %229 = shl nsw i32 %1, 3
  %230 = sext i32 %229 to i64
  %231 = sub nsw i64 0, %230
  %232 = getelementptr inbounds i8, i8* %0, i64 %231
  %233 = bitcast i8* %232 to i64*
  %234 = load i64, i64* %233, align 1
  %235 = insertelement <2 x i64> undef, i64 %234, i32 0
  %236 = bitcast <2 x i64> %235 to <4 x float>
  %237 = getelementptr inbounds i8, i8* %0, i64 %206
  %238 = bitcast i8* %237 to <2 x float>*
  %239 = load <2 x float>, <2 x float>* %238, align 1
  %240 = shufflevector <2 x float> %239, <2 x float> undef, <4 x i32> <i32 0, i32 1, i32 undef, i32 undef>
  %241 = shufflevector <4 x float> %236, <4 x float> %240, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
  %242 = bitcast <4 x float> %217 to <16 x i8>
  %243 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %242, <16 x i8> %92) #4
  %244 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %92, <16 x i8> %242) #4
  %245 = or <16 x i8> %244, %243
  %246 = bitcast <4 x float> %241 to <16 x i8>
  %247 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %246, <16 x i8> %92) #4
  %248 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %92, <16 x i8> %246) #4
  %249 = or <16 x i8> %248, %247
  %250 = icmp ugt <16 x i8> %245, %249
  %251 = select <16 x i1> %250, <16 x i8> %245, <16 x i8> %249
  %252 = icmp ugt <16 x i8> %251, %228
  %253 = select <16 x i1> %252, <16 x i8> %251, <16 x i8> %228
  %254 = shufflevector <16 x i8> %253, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %255 = icmp ugt <16 x i8> %253, %254
  %256 = select <16 x i1> %255, <16 x i8> %253, <16 x i8> %254
  %257 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %256, <16 x i8> <i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1>) #4
  %258 = icmp eq <16 x i8> %257, zeroinitializer
  %259 = and <16 x i1> %189, %258
  %260 = sext <16 x i1> %259 to <16 x i8>
  %261 = shufflevector <16 x i8> %246, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %262 = shufflevector <16 x i8> %242, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %263 = shufflevector <16 x i8> %223, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %264 = shufflevector <16 x i8> %219, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %265 = shufflevector <16 x i8> %124, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %266 = shufflevector <16 x i8> %120, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %267 = shufflevector <16 x i8> %91, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %268 = shufflevector <16 x i8> %92, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %269 = shufflevector <16 x i8> %92, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %270 = shufflevector <16 x i8> %91, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %271 = shufflevector <16 x i8> %120, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %272 = shufflevector <16 x i8> %124, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %273 = shufflevector <16 x i8> %219, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %274 = shufflevector <16 x i8> %223, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %275 = shufflevector <16 x i8> %242, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %276 = shufflevector <16 x i8> %246, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %277 = bitcast <16 x i8> %262 to <8 x i16>
  %278 = bitcast <16 x i8> %263 to <8 x i16>
  %279 = bitcast <16 x i8> %264 to <8 x i16>
  %280 = bitcast <16 x i8> %265 to <8 x i16>
  %281 = bitcast <16 x i8> %275 to <8 x i16>
  %282 = bitcast <16 x i8> %274 to <8 x i16>
  %283 = bitcast <16 x i8> %273 to <8 x i16>
  %284 = bitcast <16 x i8> %272 to <8 x i16>
  %285 = bitcast <16 x i8> %266 to <8 x i16>
  %286 = bitcast <16 x i8> %267 to <8 x i16>
  %287 = add <8 x i16> %286, %285
  %288 = bitcast <16 x i8> %268 to <8 x i16>
  %289 = add <8 x i16> %287, %288
  %290 = bitcast <16 x i8> %271 to <8 x i16>
  %291 = bitcast <16 x i8> %270 to <8 x i16>
  %292 = add <8 x i16> %291, %290
  %293 = bitcast <16 x i8> %269 to <8 x i16>
  %294 = add <8 x i16> %292, %293
  %295 = add <8 x i16> %283, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %296 = add <8 x i16> %295, %279
  %297 = add <8 x i16> %296, %284
  %298 = add <8 x i16> %297, %280
  %299 = add <8 x i16> %298, %294
  %300 = add <8 x i16> %299, %289
  %301 = add <8 x i16> %300, %282
  %302 = add <8 x i16> %301, %278
  %303 = add <8 x i16> %302, %281
  %304 = add <8 x i16> %303, %277
  %305 = add <8 x i16> %289, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %306 = add <8 x i16> %305, %294
  %307 = bitcast <16 x i8> %261 to <8 x i16>
  %308 = add <8 x i16> %307, %288
  %309 = add <8 x i16> %308, %304
  %310 = lshr <8 x i16> %309, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %311 = bitcast <16 x i8> %276 to <8 x i16>
  %312 = add <8 x i16> %311, %293
  %313 = add <8 x i16> %312, %304
  %314 = lshr <8 x i16> %313, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %315 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %310, <8 x i16> %314) #4
  %316 = bitcast <16 x i8> %315 to <2 x i64>
  %317 = add <8 x i16> %288, %280
  %318 = add <8 x i16> %317, %306
  %319 = lshr <8 x i16> %318, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %320 = add <8 x i16> %293, %284
  %321 = add <8 x i16> %320, %306
  %322 = lshr <8 x i16> %321, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %323 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %319, <8 x i16> %322) #4
  %324 = bitcast <16 x i8> %323 to <2 x i64>
  %325 = shl <8 x i16> %307, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %326 = shl <8 x i16> %311, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %327 = shl <8 x i16> %280, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %328 = shl <8 x i16> %284, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %329 = sub <8 x i16> %304, %281
  %330 = add <8 x i16> %325, %286
  %331 = add <8 x i16> %330, %329
  %332 = lshr <8 x i16> %331, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %333 = add <8 x i16> %326, %291
  %334 = add <8 x i16> %333, %303
  %335 = lshr <8 x i16> %334, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %336 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %332, <8 x i16> %335) #4
  %337 = bitcast <16 x i8> %336 to <2 x i64>
  %338 = sub <8 x i16> %306, %285
  %339 = sub <8 x i16> %306, %290
  %340 = add <8 x i16> %327, %286
  %341 = add <8 x i16> %340, %339
  %342 = lshr <8 x i16> %341, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %343 = add <8 x i16> %328, %291
  %344 = add <8 x i16> %343, %338
  %345 = lshr <8 x i16> %344, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %346 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %342, <8 x i16> %345) #4
  %347 = bitcast <16 x i8> %346 to <2 x i64>
  %348 = mul <8 x i16> %307, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %349 = mul <8 x i16> %311, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %350 = mul <8 x i16> %280, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %351 = mul <8 x i16> %284, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %352 = sub <8 x i16> %329, %282
  %353 = sub <8 x i16> %303, %278
  %354 = add <8 x i16> %348, %285
  %355 = add <8 x i16> %354, %352
  %356 = lshr <8 x i16> %355, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %357 = add <8 x i16> %349, %290
  %358 = add <8 x i16> %357, %353
  %359 = lshr <8 x i16> %358, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %360 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %356, <8 x i16> %359) #4
  %361 = bitcast <16 x i8> %360 to <2 x i64>
  %362 = add <8 x i16> %350, %285
  %363 = sub <8 x i16> %362, %291
  %364 = add <8 x i16> %363, %339
  %365 = lshr <8 x i16> %364, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %366 = add <8 x i16> %351, %290
  %367 = sub <8 x i16> %366, %286
  %368 = add <8 x i16> %367, %338
  %369 = lshr <8 x i16> %368, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %370 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %365, <8 x i16> %369) #4
  %371 = bitcast <16 x i8> %370 to <2 x i64>
  %372 = shl <8 x i16> %307, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %373 = shl <8 x i16> %311, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %374 = sub <8 x i16> %352, %283
  %375 = sub <8 x i16> %353, %279
  %376 = add <8 x i16> %372, %280
  %377 = add <8 x i16> %376, %374
  %378 = lshr <8 x i16> %377, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %379 = add <8 x i16> %373, %284
  %380 = add <8 x i16> %379, %375
  %381 = lshr <8 x i16> %380, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %382 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %378, <8 x i16> %381) #4
  %383 = bitcast <16 x i8> %382 to <2 x i64>
  %384 = mul <8 x i16> %307, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  %385 = mul <8 x i16> %311, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  %386 = sub <8 x i16> %374, %284
  %387 = sub <8 x i16> %375, %280
  %388 = add <8 x i16> %384, %279
  %389 = add <8 x i16> %388, %386
  %390 = lshr <8 x i16> %389, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %391 = add <8 x i16> %385, %283
  %392 = add <8 x i16> %391, %387
  %393 = lshr <8 x i16> %392, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %394 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %390, <8 x i16> %393) #4
  %395 = bitcast <16 x i8> %394 to <2 x i64>
  %396 = mul <8 x i16> %307, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %397 = mul <8 x i16> %311, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %398 = sub <8 x i16> %386, %290
  %399 = sub <8 x i16> %387, %285
  %400 = add <8 x i16> %396, %278
  %401 = add <8 x i16> %400, %398
  %402 = lshr <8 x i16> %401, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %403 = add <8 x i16> %397, %282
  %404 = add <8 x i16> %403, %399
  %405 = lshr <8 x i16> %404, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %406 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %402, <8 x i16> %405) #4
  %407 = bitcast <16 x i8> %406 to <2 x i64>
  %408 = mul <8 x i16> %307, <i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7>
  %409 = mul <8 x i16> %311, <i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7>
  %410 = sub <8 x i16> %277, %291
  %411 = add <8 x i16> %410, %408
  %412 = add <8 x i16> %411, %398
  %413 = lshr <8 x i16> %412, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %414 = sub <8 x i16> %281, %286
  %415 = add <8 x i16> %414, %409
  %416 = add <8 x i16> %415, %399
  %417 = lshr <8 x i16> %416, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %418 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %413, <8 x i16> %417) #4
  %419 = bitcast <16 x i8> %418 to <2 x i64>
  %420 = bitcast <16 x i8> %190 to <4 x i32>
  %421 = shufflevector <4 x i32> %420, <4 x i32> undef, <4 x i32> <i32 0, i32 1, i32 0, i32 1>
  %422 = bitcast <4 x i32> %421 to <2 x i64>
  %423 = bitcast <16 x i8> %260 to <4 x i32>
  %424 = shufflevector <4 x i32> %423, <4 x i32> undef, <4 x i32> <i32 0, i32 1, i32 0, i32 1>
  %425 = bitcast <4 x i32> %424 to <2 x i64>
  %426 = xor <2 x i64> %422, <i64 -1, i64 -1>
  %427 = and <2 x i64> %426, %64
  %428 = and <2 x i64> %422, %371
  %429 = or <2 x i64> %428, %427
  %430 = and <2 x i64> %173, %426
  %431 = and <2 x i64> %422, %347
  %432 = or <2 x i64> %431, %430
  %433 = and <2 x i64> %161, %426
  %434 = and <2 x i64> %422, %324
  %435 = or <2 x i64> %434, %433
  %436 = xor <2 x i64> %425, <i64 -1, i64 -1>
  %437 = and <2 x i64> %436, %218
  %438 = and <2 x i64> %425, %419
  %439 = or <2 x i64> %438, %437
  %440 = extractelement <2 x i64> %439, i32 0
  store i64 %440, i64* %209, align 1
  %441 = bitcast <2 x i64> %439 to <4 x float>
  %442 = shufflevector <4 x float> %441, <4 x float> undef, <2 x i32> <i32 2, i32 3>
  store <2 x float> %442, <2 x float>* %214, align 1
  %443 = and <2 x i64> %436, %204
  %444 = and <2 x i64> %425, %407
  %445 = or <2 x i64> %444, %443
  %446 = extractelement <2 x i64> %445, i32 0
  store i64 %446, i64* %195, align 1
  %447 = bitcast <2 x i64> %445 to <4 x float>
  %448 = shufflevector <4 x float> %447, <4 x float> undef, <2 x i32> <i32 2, i32 3>
  store <2 x float> %448, <2 x float>* %200, align 1
  %449 = and <2 x i64> %436, %36
  %450 = and <2 x i64> %425, %395
  %451 = or <2 x i64> %450, %449
  %452 = extractelement <2 x i64> %451, i32 0
  store i64 %452, i64* %25, align 1
  %453 = bitcast <2 x i64> %451 to <4 x float>
  %454 = shufflevector <4 x float> %453, <4 x float> undef, <2 x i32> <i32 2, i32 3>
  store <2 x float> %454, <2 x float>* %32, align 1
  %455 = and <2 x i64> %436, %50
  %456 = and <2 x i64> %425, %383
  %457 = or <2 x i64> %456, %455
  %458 = extractelement <2 x i64> %457, i32 0
  store i64 %458, i64* %39, align 1
  %459 = bitcast <2 x i64> %457 to <4 x float>
  %460 = shufflevector <4 x float> %459, <4 x float> undef, <2 x i32> <i32 2, i32 3>
  store <2 x float> %460, <2 x float>* %46, align 1
  %461 = and <2 x i64> %429, %436
  %462 = and <2 x i64> %425, %361
  %463 = or <2 x i64> %461, %462
  %464 = extractelement <2 x i64> %463, i32 0
  store i64 %464, i64* %53, align 1
  %465 = bitcast <2 x i64> %463 to <4 x float>
  %466 = shufflevector <4 x float> %465, <4 x float> undef, <2 x i32> <i32 2, i32 3>
  store <2 x float> %466, <2 x float>* %60, align 1
  %467 = and <2 x i64> %432, %436
  %468 = and <2 x i64> %425, %337
  %469 = or <2 x i64> %467, %468
  %470 = extractelement <2 x i64> %469, i32 0
  store i64 %470, i64* %67, align 1
  %471 = bitcast <2 x i64> %469 to <4 x float>
  %472 = shufflevector <4 x float> %471, <4 x float> undef, <2 x i32> <i32 2, i32 3>
  store <2 x float> %472, <2 x float>* %73, align 1
  %473 = and <2 x i64> %435, %436
  %474 = and <2 x i64> %425, %316
  %475 = or <2 x i64> %473, %474
  %476 = extractelement <2 x i64> %475, i32 0
  store i64 %476, i64* %81, align 1
  %477 = bitcast <2 x i64> %475 to <4 x float>
  %478 = shufflevector <4 x float> %477, <4 x float> undef, <2 x i32> <i32 2, i32 3>
  store <2 x float> %478, <2 x float>* %85, align 1
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define hidden void @vpx_lpf_horizontal_16_dual_avx2(i8* nocapture, i32, i8* nocapture readonly, i8* nocapture readonly, i8* nocapture readonly) local_unnamed_addr #1 {
  %6 = load i8, i8* %4, align 1
  %7 = zext i8 %6 to i32
  %8 = insertelement <4 x i32> undef, i32 %7, i32 0
  %9 = bitcast <4 x i32> %8 to <16 x i8>
  %10 = shufflevector <16 x i8> %9, <16 x i8> undef, <16 x i32> zeroinitializer
  %11 = load i8, i8* %3, align 1
  %12 = zext i8 %11 to i32
  %13 = insertelement <4 x i32> undef, i32 %12, i32 0
  %14 = bitcast <4 x i32> %13 to <16 x i8>
  %15 = shufflevector <16 x i8> %14, <16 x i8> undef, <16 x i32> zeroinitializer
  %16 = load i8, i8* %2, align 1
  %17 = zext i8 %16 to i32
  %18 = insertelement <4 x i32> undef, i32 %17, i32 0
  %19 = bitcast <4 x i32> %18 to <16 x i8>
  %20 = shufflevector <16 x i8> %19, <16 x i8> undef, <16 x i32> zeroinitializer
  %21 = mul nsw i32 %1, 5
  %22 = sext i32 %21 to i64
  %23 = sub nsw i64 0, %22
  %24 = getelementptr inbounds i8, i8* %0, i64 %23
  %25 = bitcast i8* %24 to <2 x double>*
  %26 = load <2 x double>, <2 x double>* %25, align 1
  %27 = shufflevector <2 x double> %26, <2 x double> undef, <4 x i32> <i32 0, i32 1, i32 0, i32 1>
  %28 = bitcast <4 x double> %27 to <4 x i64>
  %29 = shl nsw i32 %1, 2
  %30 = sext i32 %29 to i64
  %31 = sub nsw i64 0, %30
  %32 = getelementptr inbounds i8, i8* %0, i64 %31
  %33 = bitcast i8* %32 to <2 x double>*
  %34 = load <2 x double>, <2 x double>* %33, align 1
  %35 = shufflevector <2 x double> %34, <2 x double> undef, <4 x i32> <i32 0, i32 1, i32 0, i32 1>
  %36 = bitcast <4 x double> %35 to <4 x i64>
  %37 = mul nsw i32 %1, 3
  %38 = sext i32 %37 to i64
  %39 = sub nsw i64 0, %38
  %40 = getelementptr inbounds i8, i8* %0, i64 %39
  %41 = bitcast i8* %40 to <2 x double>*
  %42 = load <2 x double>, <2 x double>* %41, align 1
  %43 = shufflevector <2 x double> %42, <2 x double> undef, <4 x i32> <i32 0, i32 1, i32 0, i32 1>
  %44 = bitcast <4 x double> %43 to <4 x i64>
  %45 = shl nsw i32 %1, 1
  %46 = sext i32 %45 to i64
  %47 = sub nsw i64 0, %46
  %48 = getelementptr inbounds i8, i8* %0, i64 %47
  %49 = bitcast i8* %48 to <2 x double>*
  %50 = load <2 x double>, <2 x double>* %49, align 1
  %51 = shufflevector <2 x double> %50, <2 x double> undef, <4 x i32> <i32 0, i32 1, i32 0, i32 1>
  %52 = bitcast <4 x double> %51 to <4 x i64>
  %53 = sext i32 %1 to i64
  %54 = sub nsw i64 0, %53
  %55 = getelementptr inbounds i8, i8* %0, i64 %54
  %56 = bitcast i8* %55 to <2 x double>*
  %57 = load <2 x double>, <2 x double>* %56, align 1
  %58 = shufflevector <2 x double> %57, <2 x double> undef, <4 x i32> <i32 0, i32 1, i32 0, i32 1>
  %59 = bitcast <4 x double> %58 to <4 x i64>
  %60 = bitcast i8* %0 to <2 x double>*
  %61 = load <2 x double>, <2 x double>* %60, align 1
  %62 = shufflevector <2 x double> %61, <2 x double> undef, <4 x i32> <i32 0, i32 1, i32 0, i32 1>
  %63 = bitcast <4 x double> %62 to <4 x i64>
  %64 = getelementptr inbounds i8, i8* %0, i64 %53
  %65 = bitcast i8* %64 to <2 x double>*
  %66 = load <2 x double>, <2 x double>* %65, align 1
  %67 = shufflevector <2 x double> %66, <2 x double> undef, <4 x i32> <i32 0, i32 1, i32 0, i32 1>
  %68 = bitcast <4 x double> %67 to <4 x i64>
  %69 = getelementptr inbounds i8, i8* %0, i64 %46
  %70 = bitcast i8* %69 to <2 x double>*
  %71 = load <2 x double>, <2 x double>* %70, align 1
  %72 = shufflevector <2 x double> %71, <2 x double> undef, <4 x i32> <i32 0, i32 1, i32 0, i32 1>
  %73 = bitcast <4 x double> %72 to <4 x i64>
  %74 = getelementptr inbounds i8, i8* %0, i64 %38
  %75 = bitcast i8* %74 to <2 x double>*
  %76 = load <2 x double>, <2 x double>* %75, align 1
  %77 = shufflevector <2 x double> %76, <2 x double> undef, <4 x i32> <i32 0, i32 1, i32 0, i32 1>
  %78 = bitcast <4 x double> %77 to <4 x i64>
  %79 = getelementptr inbounds i8, i8* %0, i64 %30
  %80 = bitcast i8* %79 to <2 x double>*
  %81 = load <2 x double>, <2 x double>* %80, align 1
  %82 = shufflevector <2 x double> %81, <2 x double> undef, <4 x i32> <i32 0, i32 1, i32 0, i32 1>
  %83 = bitcast <4 x double> %82 to <4 x i64>
  %84 = shufflevector <4 x i64> %28, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %85 = shufflevector <4 x i64> %36, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %86 = shufflevector <4 x i64> %44, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %87 = shufflevector <4 x i64> %52, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %88 = shufflevector <4 x i64> %59, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %89 = shufflevector <4 x i64> %63, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %90 = shufflevector <4 x i64> %68, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %91 = shufflevector <4 x i64> %73, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %92 = shufflevector <4 x i64> %78, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %93 = shufflevector <4 x i64> %83, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %94 = bitcast <2 x i64> %87 to <16 x i8>
  %95 = bitcast <2 x i64> %88 to <16 x i8>
  %96 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %94, <16 x i8> %95) #4
  %97 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %95, <16 x i8> %94) #4
  %98 = or <16 x i8> %97, %96
  %99 = bitcast <2 x i64> %90 to <16 x i8>
  %100 = bitcast <2 x i64> %89 to <16 x i8>
  %101 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %99, <16 x i8> %100) #4
  %102 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %100, <16 x i8> %99) #4
  %103 = or <16 x i8> %102, %101
  %104 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %95, <16 x i8> %100) #4
  %105 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %100, <16 x i8> %95) #4
  %106 = or <16 x i8> %105, %104
  %107 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %94, <16 x i8> %99) #4
  %108 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %99, <16 x i8> %94) #4
  %109 = or <16 x i8> %108, %107
  %110 = icmp ugt <16 x i8> %98, %103
  %111 = select <16 x i1> %110, <16 x i8> %98, <16 x i8> %103
  %112 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %111, <16 x i8> %10) #4
  %113 = icmp eq <16 x i8> %112, zeroinitializer
  %114 = sext <16 x i1> %113 to <16 x i8>
  %115 = tail call <16 x i8> @llvm.uadd.sat.v16i8(<16 x i8> %106, <16 x i8> %106) #4
  %116 = bitcast <16 x i8> %109 to <8 x i16>
  %117 = lshr <8 x i16> %116, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %118 = bitcast <8 x i16> %117 to <16 x i8>
  %119 = and <16 x i8> %118, <i8 127, i8 127, i8 127, i8 127, i8 127, i8 127, i8 127, i8 127, i8 127, i8 127, i8 127, i8 127, i8 127, i8 127, i8 127, i8 127>
  %120 = tail call <16 x i8> @llvm.uadd.sat.v16i8(<16 x i8> %115, <16 x i8> %119) #4
  %121 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %120, <16 x i8> %20) #4
  %122 = icmp ne <16 x i8> %121, zeroinitializer
  %123 = sext <16 x i1> %122 to <16 x i8>
  %124 = icmp ugt <16 x i8> %111, %123
  %125 = select <16 x i1> %124, <16 x i8> %111, <16 x i8> %123
  %126 = bitcast <2 x i64> %86 to <16 x i8>
  %127 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %126, <16 x i8> %94) #4
  %128 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %94, <16 x i8> %126) #4
  %129 = or <16 x i8> %128, %127
  %130 = bitcast <2 x i64> %85 to <16 x i8>
  %131 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %130, <16 x i8> %126) #4
  %132 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %126, <16 x i8> %130) #4
  %133 = or <16 x i8> %132, %131
  %134 = icmp ugt <16 x i8> %129, %133
  %135 = select <16 x i1> %134, <16 x i8> %129, <16 x i8> %133
  %136 = icmp ugt <16 x i8> %135, %125
  %137 = select <16 x i1> %136, <16 x i8> %135, <16 x i8> %125
  %138 = bitcast <2 x i64> %91 to <16 x i8>
  %139 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %138, <16 x i8> %99) #4
  %140 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %99, <16 x i8> %138) #4
  %141 = or <16 x i8> %140, %139
  %142 = bitcast <2 x i64> %92 to <16 x i8>
  %143 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %142, <16 x i8> %138) #4
  %144 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %138, <16 x i8> %142) #4
  %145 = or <16 x i8> %144, %143
  %146 = icmp ugt <16 x i8> %141, %145
  %147 = select <16 x i1> %146, <16 x i8> %141, <16 x i8> %145
  %148 = icmp ugt <16 x i8> %147, %137
  %149 = select <16 x i1> %148, <16 x i8> %147, <16 x i8> %137
  %150 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %149, <16 x i8> %15) #4
  %151 = icmp eq <16 x i8> %150, zeroinitializer
  %152 = xor <16 x i8> %94, <i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128>
  %153 = xor <16 x i8> %99, <i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128>
  %154 = tail call <16 x i8> @llvm.ssub.sat.v16i8(<16 x i8> %152, <16 x i8> %153) #4
  %155 = xor <16 x i8> %100, <i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128>
  %156 = xor <16 x i8> %95, <i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128>
  %157 = tail call <16 x i8> @llvm.ssub.sat.v16i8(<16 x i8> %155, <16 x i8> %156) #4
  %158 = xor <16 x i8> %114, <i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1>
  %159 = and <16 x i8> %154, %158
  %160 = tail call <16 x i8> @llvm.sadd.sat.v16i8(<16 x i8> %159, <16 x i8> %157) #4
  %161 = tail call <16 x i8> @llvm.sadd.sat.v16i8(<16 x i8> %160, <16 x i8> %157) #4
  %162 = tail call <16 x i8> @llvm.sadd.sat.v16i8(<16 x i8> %161, <16 x i8> %157) #4
  %163 = select <16 x i1> %151, <16 x i8> %162, <16 x i8> zeroinitializer
  %164 = tail call <16 x i8> @llvm.sadd.sat.v16i8(<16 x i8> %163, <16 x i8> <i8 4, i8 4, i8 4, i8 4, i8 4, i8 4, i8 4, i8 4, i8 4, i8 4, i8 4, i8 4, i8 4, i8 4, i8 4, i8 4>) #4
  %165 = tail call <16 x i8> @llvm.sadd.sat.v16i8(<16 x i8> %163, <16 x i8> <i8 3, i8 3, i8 3, i8 3, i8 3, i8 3, i8 3, i8 3, i8 3, i8 3, i8 3, i8 3, i8 3, i8 3, i8 3, i8 3>) #4
  %166 = ashr <16 x i8> %164, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %167 = bitcast <16 x i8> %166 to <2 x i64>
  %168 = bitcast <16 x i8> %164 to <8 x i16>
  %169 = lshr <8 x i16> %168, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %170 = bitcast <8 x i16> %169 to <2 x i64>
  %171 = and <2 x i64> %167, <i64 -2242545357980376864, i64 -2242545357980376864>
  %172 = and <2 x i64> %170, <i64 2242545357980376863, i64 2242545357980376863>
  %173 = or <2 x i64> %172, %171
  %174 = bitcast <2 x i64> %173 to <16 x i8>
  %175 = tail call <16 x i8> @llvm.ssub.sat.v16i8(<16 x i8> %155, <16 x i8> %174) #4
  %176 = bitcast <16 x i8> %175 to <2 x i64>
  %177 = xor <2 x i64> %176, <i64 -9187201950435737472, i64 -9187201950435737472>
  %178 = ashr <16 x i8> %165, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %179 = bitcast <16 x i8> %178 to <2 x i64>
  %180 = bitcast <16 x i8> %165 to <8 x i16>
  %181 = lshr <8 x i16> %180, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %182 = bitcast <8 x i16> %181 to <2 x i64>
  %183 = and <2 x i64> %179, <i64 -2242545357980376864, i64 -2242545357980376864>
  %184 = and <2 x i64> %182, <i64 2242545357980376863, i64 2242545357980376863>
  %185 = or <2 x i64> %184, %183
  %186 = bitcast <2 x i64> %185 to <16 x i8>
  %187 = tail call <16 x i8> @llvm.sadd.sat.v16i8(<16 x i8> %156, <16 x i8> %186) #4
  %188 = bitcast <16 x i8> %187 to <2 x i64>
  %189 = xor <2 x i64> %188, <i64 -9187201950435737472, i64 -9187201950435737472>
  %190 = tail call <16 x i8> @llvm.sadd.sat.v16i8(<16 x i8> %174, <16 x i8> <i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1>) #4
  %191 = ashr <16 x i8> %190, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %192 = bitcast <16 x i8> %191 to <2 x i64>
  %193 = bitcast <16 x i8> %190 to <8 x i16>
  %194 = lshr <8 x i16> %193, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %195 = bitcast <8 x i16> %194 to <2 x i64>
  %196 = and <2 x i64> %192, <i64 -9187201950435737472, i64 -9187201950435737472>
  %197 = and <2 x i64> %195, <i64 9187201950435737471, i64 9187201950435737471>
  %198 = or <2 x i64> %197, %196
  %199 = bitcast <2 x i64> %198 to <16 x i8>
  %200 = and <16 x i8> %199, %114
  %201 = tail call <16 x i8> @llvm.sadd.sat.v16i8(<16 x i8> %152, <16 x i8> %200) #4
  %202 = bitcast <16 x i8> %201 to <2 x i64>
  %203 = xor <2 x i64> %202, <i64 -9187201950435737472, i64 -9187201950435737472>
  %204 = tail call <16 x i8> @llvm.ssub.sat.v16i8(<16 x i8> %153, <16 x i8> %200) #4
  %205 = bitcast <16 x i8> %204 to <2 x i64>
  %206 = xor <2 x i64> %205, <i64 -9187201950435737472, i64 -9187201950435737472>
  %207 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %126, <16 x i8> %95) #4
  %208 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %95, <16 x i8> %126) #4
  %209 = or <16 x i8> %208, %207
  %210 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %138, <16 x i8> %100) #4
  %211 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %100, <16 x i8> %138) #4
  %212 = or <16 x i8> %211, %210
  %213 = icmp ugt <16 x i8> %209, %212
  %214 = select <16 x i1> %213, <16 x i8> %209, <16 x i8> %212
  %215 = icmp ugt <16 x i8> %214, %111
  %216 = select <16 x i1> %215, <16 x i8> %214, <16 x i8> %111
  %217 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %130, <16 x i8> %95) #4
  %218 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %95, <16 x i8> %130) #4
  %219 = or <16 x i8> %218, %217
  %220 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %142, <16 x i8> %100) #4
  %221 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %100, <16 x i8> %142) #4
  %222 = or <16 x i8> %221, %220
  %223 = icmp ugt <16 x i8> %219, %222
  %224 = select <16 x i1> %223, <16 x i8> %219, <16 x i8> %222
  %225 = icmp ugt <16 x i8> %224, %216
  %226 = select <16 x i1> %225, <16 x i8> %224, <16 x i8> %216
  %227 = bitcast <2 x i64> %84 to <16 x i8>
  %228 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %227, <16 x i8> %95) #4
  %229 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %95, <16 x i8> %227) #4
  %230 = or <16 x i8> %229, %228
  %231 = bitcast <2 x i64> %93 to <16 x i8>
  %232 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %231, <16 x i8> %100) #4
  %233 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %100, <16 x i8> %231) #4
  %234 = or <16 x i8> %233, %232
  %235 = icmp ugt <16 x i8> %230, %234
  %236 = select <16 x i1> %235, <16 x i8> %230, <16 x i8> %234
  %237 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %226, <16 x i8> <i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1>) #4
  %238 = icmp eq <16 x i8> %237, zeroinitializer
  %239 = and <16 x i1> %151, %238
  %240 = sext <16 x i1> %239 to <16 x i8>
  %241 = bitcast <16 x i8> %240 to <2 x i64>
  %242 = mul nsw i32 %1, 6
  %243 = sext i32 %242 to i64
  %244 = sub nsw i64 0, %243
  %245 = getelementptr inbounds i8, i8* %0, i64 %244
  %246 = bitcast i8* %245 to <2 x double>*
  %247 = load <2 x double>, <2 x double>* %246, align 1
  %248 = shufflevector <2 x double> %247, <2 x double> undef, <4 x i32> <i32 0, i32 1, i32 0, i32 1>
  %249 = bitcast <4 x double> %248 to <4 x i64>
  %250 = getelementptr inbounds i8, i8* %0, i64 %22
  %251 = bitcast i8* %250 to <2 x double>*
  %252 = load <2 x double>, <2 x double>* %251, align 1
  %253 = shufflevector <2 x double> %252, <2 x double> undef, <4 x i32> <i32 0, i32 1, i32 0, i32 1>
  %254 = bitcast <4 x double> %253 to <4 x i64>
  %255 = shufflevector <4 x i64> %249, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %256 = shufflevector <4 x i64> %254, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %257 = bitcast <2 x i64> %255 to <16 x i8>
  %258 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %257, <16 x i8> %95) #4
  %259 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %95, <16 x i8> %257) #4
  %260 = or <16 x i8> %259, %258
  %261 = bitcast <2 x i64> %256 to <16 x i8>
  %262 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %261, <16 x i8> %100) #4
  %263 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %100, <16 x i8> %261) #4
  %264 = or <16 x i8> %263, %262
  %265 = icmp ugt <16 x i8> %260, %264
  %266 = select <16 x i1> %265, <16 x i8> %260, <16 x i8> %264
  %267 = icmp ugt <16 x i8> %236, %266
  %268 = select <16 x i1> %267, <16 x i8> %236, <16 x i8> %266
  %269 = mul nsw i32 %1, 7
  %270 = sext i32 %269 to i64
  %271 = sub nsw i64 0, %270
  %272 = getelementptr inbounds i8, i8* %0, i64 %271
  %273 = bitcast i8* %272 to <2 x double>*
  %274 = load <2 x double>, <2 x double>* %273, align 1
  %275 = shufflevector <2 x double> %274, <2 x double> undef, <4 x i32> <i32 0, i32 1, i32 0, i32 1>
  %276 = bitcast <4 x double> %275 to <4 x i64>
  %277 = getelementptr inbounds i8, i8* %0, i64 %243
  %278 = bitcast i8* %277 to <2 x double>*
  %279 = load <2 x double>, <2 x double>* %278, align 1
  %280 = shufflevector <2 x double> %279, <2 x double> undef, <4 x i32> <i32 0, i32 1, i32 0, i32 1>
  %281 = bitcast <4 x double> %280 to <4 x i64>
  %282 = shufflevector <4 x i64> %276, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %283 = shufflevector <4 x i64> %281, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %284 = bitcast <2 x i64> %282 to <16 x i8>
  %285 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %284, <16 x i8> %95) #4
  %286 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %95, <16 x i8> %284) #4
  %287 = or <16 x i8> %286, %285
  %288 = bitcast <2 x i64> %283 to <16 x i8>
  %289 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %288, <16 x i8> %100) #4
  %290 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %100, <16 x i8> %288) #4
  %291 = or <16 x i8> %290, %289
  %292 = icmp ugt <16 x i8> %287, %291
  %293 = select <16 x i1> %292, <16 x i8> %287, <16 x i8> %291
  %294 = icmp ugt <16 x i8> %293, %268
  %295 = select <16 x i1> %294, <16 x i8> %293, <16 x i8> %268
  %296 = shl nsw i32 %1, 3
  %297 = sext i32 %296 to i64
  %298 = sub nsw i64 0, %297
  %299 = getelementptr inbounds i8, i8* %0, i64 %298
  %300 = bitcast i8* %299 to <2 x double>*
  %301 = load <2 x double>, <2 x double>* %300, align 1
  %302 = shufflevector <2 x double> %301, <2 x double> undef, <4 x i32> <i32 0, i32 1, i32 0, i32 1>
  %303 = bitcast <4 x double> %302 to <4 x i64>
  %304 = getelementptr inbounds i8, i8* %0, i64 %270
  %305 = bitcast i8* %304 to <2 x double>*
  %306 = load <2 x double>, <2 x double>* %305, align 1
  %307 = shufflevector <2 x double> %306, <2 x double> undef, <4 x i32> <i32 0, i32 1, i32 0, i32 1>
  %308 = bitcast <4 x double> %307 to <4 x i64>
  %309 = shufflevector <4 x i64> %303, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %310 = shufflevector <4 x i64> %308, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %311 = bitcast <2 x i64> %309 to <16 x i8>
  %312 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %311, <16 x i8> %95) #4
  %313 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %95, <16 x i8> %311) #4
  %314 = or <16 x i8> %313, %312
  %315 = bitcast <2 x i64> %310 to <16 x i8>
  %316 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %315, <16 x i8> %100) #4
  %317 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %100, <16 x i8> %315) #4
  %318 = or <16 x i8> %317, %316
  %319 = icmp ugt <16 x i8> %314, %318
  %320 = select <16 x i1> %319, <16 x i8> %314, <16 x i8> %318
  %321 = icmp ugt <16 x i8> %320, %295
  %322 = select <16 x i1> %321, <16 x i8> %320, <16 x i8> %295
  %323 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %322, <16 x i8> <i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1>) #4
  %324 = icmp eq <16 x i8> %323, zeroinitializer
  %325 = and <16 x i1> %239, %324
  %326 = sext <16 x i1> %325 to <16 x i8>
  %327 = bitcast <16 x i8> %326 to <2 x i64>
  %328 = bitcast <4 x double> %302 to <32 x i8>
  %329 = shufflevector <32 x i8> %328, <32 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <32 x i32> <i32 0, i32 32, i32 1, i32 32, i32 2, i32 32, i32 3, i32 32, i32 4, i32 32, i32 5, i32 32, i32 6, i32 32, i32 7, i32 32, i32 24, i32 48, i32 25, i32 48, i32 26, i32 48, i32 27, i32 48, i32 28, i32 48, i32 29, i32 48, i32 30, i32 48, i32 31, i32 48>
  %330 = bitcast <4 x double> %275 to <32 x i8>
  %331 = shufflevector <32 x i8> %330, <32 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <32 x i32> <i32 0, i32 32, i32 1, i32 32, i32 2, i32 32, i32 3, i32 32, i32 4, i32 32, i32 5, i32 32, i32 6, i32 32, i32 7, i32 32, i32 24, i32 48, i32 25, i32 48, i32 26, i32 48, i32 27, i32 48, i32 28, i32 48, i32 29, i32 48, i32 30, i32 48, i32 31, i32 48>
  %332 = bitcast <4 x double> %248 to <32 x i8>
  %333 = shufflevector <32 x i8> %332, <32 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <32 x i32> <i32 0, i32 32, i32 1, i32 32, i32 2, i32 32, i32 3, i32 32, i32 4, i32 32, i32 5, i32 32, i32 6, i32 32, i32 7, i32 32, i32 24, i32 48, i32 25, i32 48, i32 26, i32 48, i32 27, i32 48, i32 28, i32 48, i32 29, i32 48, i32 30, i32 48, i32 31, i32 48>
  %334 = bitcast <4 x double> %27 to <32 x i8>
  %335 = shufflevector <32 x i8> %334, <32 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <32 x i32> <i32 0, i32 32, i32 1, i32 32, i32 2, i32 32, i32 3, i32 32, i32 4, i32 32, i32 5, i32 32, i32 6, i32 32, i32 7, i32 32, i32 24, i32 48, i32 25, i32 48, i32 26, i32 48, i32 27, i32 48, i32 28, i32 48, i32 29, i32 48, i32 30, i32 48, i32 31, i32 48>
  %336 = bitcast <4 x double> %35 to <32 x i8>
  %337 = shufflevector <32 x i8> %336, <32 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <32 x i32> <i32 0, i32 32, i32 1, i32 32, i32 2, i32 32, i32 3, i32 32, i32 4, i32 32, i32 5, i32 32, i32 6, i32 32, i32 7, i32 32, i32 24, i32 48, i32 25, i32 48, i32 26, i32 48, i32 27, i32 48, i32 28, i32 48, i32 29, i32 48, i32 30, i32 48, i32 31, i32 48>
  %338 = bitcast <4 x double> %43 to <32 x i8>
  %339 = shufflevector <32 x i8> %338, <32 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <32 x i32> <i32 0, i32 32, i32 1, i32 32, i32 2, i32 32, i32 3, i32 32, i32 4, i32 32, i32 5, i32 32, i32 6, i32 32, i32 7, i32 32, i32 24, i32 48, i32 25, i32 48, i32 26, i32 48, i32 27, i32 48, i32 28, i32 48, i32 29, i32 48, i32 30, i32 48, i32 31, i32 48>
  %340 = bitcast <4 x double> %51 to <32 x i8>
  %341 = shufflevector <32 x i8> %340, <32 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <32 x i32> <i32 0, i32 32, i32 1, i32 32, i32 2, i32 32, i32 3, i32 32, i32 4, i32 32, i32 5, i32 32, i32 6, i32 32, i32 7, i32 32, i32 24, i32 48, i32 25, i32 48, i32 26, i32 48, i32 27, i32 48, i32 28, i32 48, i32 29, i32 48, i32 30, i32 48, i32 31, i32 48>
  %342 = bitcast <4 x double> %58 to <32 x i8>
  %343 = shufflevector <32 x i8> %342, <32 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <32 x i32> <i32 0, i32 32, i32 1, i32 32, i32 2, i32 32, i32 3, i32 32, i32 4, i32 32, i32 5, i32 32, i32 6, i32 32, i32 7, i32 32, i32 24, i32 48, i32 25, i32 48, i32 26, i32 48, i32 27, i32 48, i32 28, i32 48, i32 29, i32 48, i32 30, i32 48, i32 31, i32 48>
  %344 = bitcast <4 x double> %62 to <32 x i8>
  %345 = shufflevector <32 x i8> %344, <32 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <32 x i32> <i32 0, i32 32, i32 1, i32 32, i32 2, i32 32, i32 3, i32 32, i32 4, i32 32, i32 5, i32 32, i32 6, i32 32, i32 7, i32 32, i32 24, i32 48, i32 25, i32 48, i32 26, i32 48, i32 27, i32 48, i32 28, i32 48, i32 29, i32 48, i32 30, i32 48, i32 31, i32 48>
  %346 = bitcast <4 x double> %67 to <32 x i8>
  %347 = shufflevector <32 x i8> %346, <32 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <32 x i32> <i32 0, i32 32, i32 1, i32 32, i32 2, i32 32, i32 3, i32 32, i32 4, i32 32, i32 5, i32 32, i32 6, i32 32, i32 7, i32 32, i32 24, i32 48, i32 25, i32 48, i32 26, i32 48, i32 27, i32 48, i32 28, i32 48, i32 29, i32 48, i32 30, i32 48, i32 31, i32 48>
  %348 = bitcast <4 x double> %72 to <32 x i8>
  %349 = shufflevector <32 x i8> %348, <32 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <32 x i32> <i32 0, i32 32, i32 1, i32 32, i32 2, i32 32, i32 3, i32 32, i32 4, i32 32, i32 5, i32 32, i32 6, i32 32, i32 7, i32 32, i32 24, i32 48, i32 25, i32 48, i32 26, i32 48, i32 27, i32 48, i32 28, i32 48, i32 29, i32 48, i32 30, i32 48, i32 31, i32 48>
  %350 = bitcast <4 x double> %77 to <32 x i8>
  %351 = shufflevector <32 x i8> %350, <32 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <32 x i32> <i32 0, i32 32, i32 1, i32 32, i32 2, i32 32, i32 3, i32 32, i32 4, i32 32, i32 5, i32 32, i32 6, i32 32, i32 7, i32 32, i32 24, i32 48, i32 25, i32 48, i32 26, i32 48, i32 27, i32 48, i32 28, i32 48, i32 29, i32 48, i32 30, i32 48, i32 31, i32 48>
  %352 = bitcast <4 x double> %82 to <32 x i8>
  %353 = shufflevector <32 x i8> %352, <32 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <32 x i32> <i32 0, i32 32, i32 1, i32 32, i32 2, i32 32, i32 3, i32 32, i32 4, i32 32, i32 5, i32 32, i32 6, i32 32, i32 7, i32 32, i32 24, i32 48, i32 25, i32 48, i32 26, i32 48, i32 27, i32 48, i32 28, i32 48, i32 29, i32 48, i32 30, i32 48, i32 31, i32 48>
  %354 = bitcast <4 x double> %253 to <32 x i8>
  %355 = shufflevector <32 x i8> %354, <32 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <32 x i32> <i32 0, i32 32, i32 1, i32 32, i32 2, i32 32, i32 3, i32 32, i32 4, i32 32, i32 5, i32 32, i32 6, i32 32, i32 7, i32 32, i32 24, i32 48, i32 25, i32 48, i32 26, i32 48, i32 27, i32 48, i32 28, i32 48, i32 29, i32 48, i32 30, i32 48, i32 31, i32 48>
  %356 = bitcast <4 x double> %280 to <32 x i8>
  %357 = shufflevector <32 x i8> %356, <32 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <32 x i32> <i32 0, i32 32, i32 1, i32 32, i32 2, i32 32, i32 3, i32 32, i32 4, i32 32, i32 5, i32 32, i32 6, i32 32, i32 7, i32 32, i32 24, i32 48, i32 25, i32 48, i32 26, i32 48, i32 27, i32 48, i32 28, i32 48, i32 29, i32 48, i32 30, i32 48, i32 31, i32 48>
  %358 = bitcast <4 x double> %307 to <32 x i8>
  %359 = shufflevector <32 x i8> %358, <32 x i8> <i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <32 x i32> <i32 0, i32 32, i32 1, i32 32, i32 2, i32 32, i32 3, i32 32, i32 4, i32 32, i32 5, i32 32, i32 6, i32 32, i32 7, i32 32, i32 24, i32 48, i32 25, i32 48, i32 26, i32 48, i32 27, i32 48, i32 28, i32 48, i32 29, i32 48, i32 30, i32 48, i32 31, i32 48>
  %360 = bitcast <32 x i8> %331 to <16 x i16>
  %361 = bitcast <32 x i8> %333 to <16 x i16>
  %362 = bitcast <32 x i8> %335 to <16 x i16>
  %363 = bitcast <32 x i8> %337 to <16 x i16>
  %364 = bitcast <32 x i8> %357 to <16 x i16>
  %365 = bitcast <32 x i8> %355 to <16 x i16>
  %366 = bitcast <32 x i8> %353 to <16 x i16>
  %367 = bitcast <32 x i8> %351 to <16 x i16>
  %368 = bitcast <32 x i8> %339 to <16 x i16>
  %369 = bitcast <32 x i8> %341 to <16 x i16>
  %370 = add <16 x i16> %369, %368
  %371 = bitcast <32 x i8> %343 to <16 x i16>
  %372 = add <16 x i16> %370, %371
  %373 = bitcast <32 x i8> %349 to <16 x i16>
  %374 = bitcast <32 x i8> %347 to <16 x i16>
  %375 = bitcast <32 x i8> %345 to <16 x i16>
  %376 = add <16 x i16> %374, %375
  %377 = add <16 x i16> %376, %373
  %378 = add <16 x i16> %362, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %379 = add <16 x i16> %378, %363
  %380 = add <16 x i16> %379, %372
  %381 = add <16 x i16> %380, %367
  %382 = add <16 x i16> %381, %366
  %383 = add <16 x i16> %382, %377
  %384 = add <16 x i16> %383, %361
  %385 = add <16 x i16> %384, %365
  %386 = add <16 x i16> %385, %360
  %387 = add <16 x i16> %386, %364
  %388 = add <16 x i16> %372, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %389 = add <16 x i16> %388, %377
  %390 = bitcast <32 x i8> %329 to <16 x i16>
  %391 = add <16 x i16> %390, %371
  %392 = add <16 x i16> %391, %387
  %393 = lshr <16 x i16> %392, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %394 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %393, <16 x i16> undef) #4
  %395 = bitcast <32 x i8> %394 to <4 x i64>
  %396 = shufflevector <4 x i64> %395, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  %397 = bitcast <32 x i8> %359 to <16 x i16>
  %398 = add <16 x i16> %397, %375
  %399 = add <16 x i16> %398, %387
  %400 = lshr <16 x i16> %399, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %401 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %400, <16 x i16> undef) #4
  %402 = bitcast <32 x i8> %401 to <4 x i64>
  %403 = shufflevector <4 x i64> %402, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  %404 = add <16 x i16> %371, %363
  %405 = add <16 x i16> %404, %389
  %406 = lshr <16 x i16> %405, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %407 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %406, <16 x i16> undef) #4
  %408 = bitcast <32 x i8> %407 to <4 x i64>
  %409 = shufflevector <4 x i64> %408, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  %410 = add <16 x i16> %367, %375
  %411 = add <16 x i16> %410, %389
  %412 = lshr <16 x i16> %411, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %413 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %412, <16 x i16> undef) #4
  %414 = bitcast <32 x i8> %413 to <4 x i64>
  %415 = shufflevector <4 x i64> %414, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  %416 = shl <16 x i16> %390, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %417 = shl <16 x i16> %397, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %418 = shl <16 x i16> %363, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %419 = shl <16 x i16> %367, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %420 = sub <16 x i16> %387, %360
  %421 = add <16 x i16> %416, %369
  %422 = add <16 x i16> %421, %386
  %423 = lshr <16 x i16> %422, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %424 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %423, <16 x i16> undef) #4
  %425 = bitcast <32 x i8> %424 to <4 x i64>
  %426 = shufflevector <4 x i64> %425, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  %427 = add <16 x i16> %417, %374
  %428 = add <16 x i16> %427, %420
  %429 = lshr <16 x i16> %428, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %430 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %429, <16 x i16> undef) #4
  %431 = bitcast <32 x i8> %430 to <4 x i64>
  %432 = shufflevector <4 x i64> %431, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  %433 = sub <16 x i16> %389, %368
  %434 = sub <16 x i16> %389, %373
  %435 = add <16 x i16> %418, %369
  %436 = add <16 x i16> %435, %434
  %437 = lshr <16 x i16> %436, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %438 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %437, <16 x i16> undef) #4
  %439 = bitcast <32 x i8> %438 to <4 x i64>
  %440 = shufflevector <4 x i64> %439, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  %441 = add <16 x i16> %419, %374
  %442 = add <16 x i16> %441, %433
  %443 = lshr <16 x i16> %442, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %444 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %443, <16 x i16> undef) #4
  %445 = bitcast <32 x i8> %444 to <4 x i64>
  %446 = shufflevector <4 x i64> %445, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  %447 = mul <16 x i16> %390, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %448 = mul <16 x i16> %397, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %449 = mul <16 x i16> %363, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %450 = mul <16 x i16> %367, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %451 = sub <16 x i16> %386, %365
  %452 = sub <16 x i16> %420, %361
  %453 = add <16 x i16> %447, %368
  %454 = add <16 x i16> %453, %451
  %455 = lshr <16 x i16> %454, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %456 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %455, <16 x i16> undef) #4
  %457 = bitcast <32 x i8> %456 to <4 x i64>
  %458 = shufflevector <4 x i64> %457, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  %459 = add <16 x i16> %448, %373
  %460 = add <16 x i16> %459, %452
  %461 = lshr <16 x i16> %460, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %462 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %461, <16 x i16> undef) #4
  %463 = bitcast <32 x i8> %462 to <4 x i64>
  %464 = shufflevector <4 x i64> %463, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  %465 = add <16 x i16> %449, %368
  %466 = sub <16 x i16> %465, %374
  %467 = add <16 x i16> %466, %434
  %468 = lshr <16 x i16> %467, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %469 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %468, <16 x i16> undef) #4
  %470 = bitcast <32 x i8> %469 to <4 x i64>
  %471 = shufflevector <4 x i64> %470, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  %472 = sub <16 x i16> %373, %369
  %473 = add <16 x i16> %472, %450
  %474 = add <16 x i16> %473, %433
  %475 = lshr <16 x i16> %474, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %476 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %475, <16 x i16> undef) #4
  %477 = bitcast <32 x i8> %476 to <4 x i64>
  %478 = shufflevector <4 x i64> %477, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  %479 = shl <16 x i16> %390, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %480 = shl <16 x i16> %397, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %481 = sub <16 x i16> %451, %366
  %482 = sub <16 x i16> %452, %362
  %483 = add <16 x i16> %479, %363
  %484 = add <16 x i16> %483, %481
  %485 = lshr <16 x i16> %484, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %486 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %485, <16 x i16> undef) #4
  %487 = bitcast <32 x i8> %486 to <4 x i64>
  %488 = shufflevector <4 x i64> %487, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  %489 = add <16 x i16> %480, %367
  %490 = add <16 x i16> %489, %482
  %491 = lshr <16 x i16> %490, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %492 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %491, <16 x i16> undef) #4
  %493 = bitcast <32 x i8> %492 to <4 x i64>
  %494 = shufflevector <4 x i64> %493, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  %495 = mul <16 x i16> %390, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  %496 = mul <16 x i16> %397, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  %497 = sub <16 x i16> %481, %367
  %498 = sub <16 x i16> %482, %363
  %499 = add <16 x i16> %495, %362
  %500 = add <16 x i16> %499, %497
  %501 = lshr <16 x i16> %500, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %502 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %501, <16 x i16> undef) #4
  %503 = bitcast <32 x i8> %502 to <4 x i64>
  %504 = shufflevector <4 x i64> %503, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  %505 = add <16 x i16> %496, %366
  %506 = add <16 x i16> %505, %498
  %507 = lshr <16 x i16> %506, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %508 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %507, <16 x i16> undef) #4
  %509 = bitcast <32 x i8> %508 to <4 x i64>
  %510 = shufflevector <4 x i64> %509, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  %511 = mul <16 x i16> %390, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %512 = mul <16 x i16> %397, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %513 = sub <16 x i16> %497, %373
  %514 = sub <16 x i16> %498, %368
  %515 = add <16 x i16> %511, %361
  %516 = add <16 x i16> %515, %513
  %517 = lshr <16 x i16> %516, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %518 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %517, <16 x i16> undef) #4
  %519 = bitcast <32 x i8> %518 to <4 x i64>
  %520 = shufflevector <4 x i64> %519, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  %521 = add <16 x i16> %512, %365
  %522 = add <16 x i16> %521, %514
  %523 = lshr <16 x i16> %522, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %524 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %523, <16 x i16> undef) #4
  %525 = bitcast <32 x i8> %524 to <4 x i64>
  %526 = shufflevector <4 x i64> %525, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  %527 = mul <16 x i16> %390, <i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7>
  %528 = mul <16 x i16> %397, <i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7>
  %529 = sub <16 x i16> %360, %374
  %530 = add <16 x i16> %529, %527
  %531 = add <16 x i16> %530, %513
  %532 = lshr <16 x i16> %531, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %533 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %532, <16 x i16> undef) #4
  %534 = bitcast <32 x i8> %533 to <4 x i64>
  %535 = shufflevector <4 x i64> %534, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  %536 = sub <16 x i16> %364, %369
  %537 = add <16 x i16> %536, %528
  %538 = add <16 x i16> %537, %514
  %539 = lshr <16 x i16> %538, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %540 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %539, <16 x i16> undef) #4
  %541 = bitcast <32 x i8> %540 to <4 x i64>
  %542 = shufflevector <4 x i64> %541, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  %543 = xor <2 x i64> %241, <i64 -1, i64 -1>
  %544 = and <2 x i64> %86, %543
  %545 = and <2 x i64> %471, %241
  %546 = or <2 x i64> %544, %545
  %547 = and <2 x i64> %203, %543
  %548 = and <2 x i64> %440, %241
  %549 = or <2 x i64> %547, %548
  %550 = and <2 x i64> %189, %543
  %551 = and <2 x i64> %409, %241
  %552 = or <2 x i64> %550, %551
  %553 = and <2 x i64> %177, %543
  %554 = and <2 x i64> %415, %241
  %555 = or <2 x i64> %553, %554
  %556 = and <2 x i64> %206, %543
  %557 = and <2 x i64> %446, %241
  %558 = or <2 x i64> %556, %557
  %559 = and <2 x i64> %91, %543
  %560 = and <2 x i64> %478, %241
  %561 = or <2 x i64> %559, %560
  %562 = xor <2 x i64> %327, <i64 -1, i64 -1>
  %563 = and <2 x i64> %282, %562
  %564 = and <2 x i64> %535, %327
  %565 = or <2 x i64> %564, %563
  %566 = bitcast i8* %272 to <2 x i64>*
  store <2 x i64> %565, <2 x i64>* %566, align 1
  %567 = and <2 x i64> %255, %562
  %568 = and <2 x i64> %520, %327
  %569 = or <2 x i64> %568, %567
  %570 = bitcast i8* %245 to <2 x i64>*
  store <2 x i64> %569, <2 x i64>* %570, align 1
  %571 = and <2 x i64> %84, %562
  %572 = and <2 x i64> %504, %327
  %573 = or <2 x i64> %571, %572
  %574 = bitcast i8* %24 to <2 x i64>*
  store <2 x i64> %573, <2 x i64>* %574, align 1
  %575 = and <2 x i64> %85, %562
  %576 = and <2 x i64> %488, %327
  %577 = or <2 x i64> %575, %576
  %578 = bitcast i8* %32 to <2 x i64>*
  store <2 x i64> %577, <2 x i64>* %578, align 1
  %579 = and <2 x i64> %546, %562
  %580 = and <2 x i64> %458, %327
  %581 = or <2 x i64> %579, %580
  %582 = bitcast i8* %40 to <2 x i64>*
  store <2 x i64> %581, <2 x i64>* %582, align 1
  %583 = and <2 x i64> %549, %562
  %584 = and <2 x i64> %426, %327
  %585 = or <2 x i64> %583, %584
  %586 = bitcast i8* %48 to <2 x i64>*
  store <2 x i64> %585, <2 x i64>* %586, align 1
  %587 = and <2 x i64> %552, %562
  %588 = and <2 x i64> %396, %327
  %589 = or <2 x i64> %587, %588
  %590 = bitcast i8* %55 to <2 x i64>*
  store <2 x i64> %589, <2 x i64>* %590, align 1
  %591 = and <2 x i64> %555, %562
  %592 = and <2 x i64> %403, %327
  %593 = or <2 x i64> %591, %592
  %594 = bitcast i8* %0 to <2 x i64>*
  store <2 x i64> %593, <2 x i64>* %594, align 1
  %595 = and <2 x i64> %558, %562
  %596 = and <2 x i64> %432, %327
  %597 = or <2 x i64> %595, %596
  %598 = bitcast i8* %64 to <2 x i64>*
  store <2 x i64> %597, <2 x i64>* %598, align 1
  %599 = and <2 x i64> %561, %562
  %600 = and <2 x i64> %464, %327
  %601 = or <2 x i64> %599, %600
  %602 = bitcast i8* %69 to <2 x i64>*
  store <2 x i64> %601, <2 x i64>* %602, align 1
  %603 = and <2 x i64> %92, %562
  %604 = and <2 x i64> %494, %327
  %605 = or <2 x i64> %603, %604
  %606 = bitcast i8* %74 to <2 x i64>*
  store <2 x i64> %605, <2 x i64>* %606, align 1
  %607 = and <2 x i64> %93, %562
  %608 = and <2 x i64> %510, %327
  %609 = or <2 x i64> %608, %607
  %610 = bitcast i8* %79 to <2 x i64>*
  store <2 x i64> %609, <2 x i64>* %610, align 1
  %611 = and <2 x i64> %256, %562
  %612 = and <2 x i64> %526, %327
  %613 = or <2 x i64> %612, %611
  %614 = bitcast i8* %250 to <2 x i64>*
  store <2 x i64> %613, <2 x i64>* %614, align 1
  %615 = and <2 x i64> %283, %562
  %616 = and <2 x i64> %542, %327
  %617 = or <2 x i64> %616, %615
  %618 = bitcast i8* %277 to <2 x i64>*
  store <2 x i64> %617, <2 x i64>* %618, align 1
  ret void
}

; Function Attrs: nounwind readnone speculatable
declare <16 x i8> @llvm.usub.sat.v16i8(<16 x i8>, <16 x i8>) #2

; Function Attrs: nounwind readnone speculatable
declare <16 x i8> @llvm.uadd.sat.v16i8(<16 x i8>, <16 x i8>) #2

; Function Attrs: nounwind readnone speculatable
declare <16 x i8> @llvm.ssub.sat.v16i8(<16 x i8>, <16 x i8>) #2

; Function Attrs: nounwind readnone speculatable
declare <16 x i8> @llvm.sadd.sat.v16i8(<16 x i8>, <16 x i8>) #2

; Function Attrs: nounwind readnone
declare <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16>, <8 x i16>) #3

; Function Attrs: nounwind readnone
declare <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16>, <8 x i16>) #3

; Function Attrs: nounwind readnone
declare <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16>, <16 x i16>) #3

attributes #0 = { nofree nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="128" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+avx,+avx2,+cx8,+fxsr,+mmx,+popcnt,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { nofree nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="256" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+avx,+avx2,+cx8,+fxsr,+mmx,+popcnt,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #2 = { nounwind readnone speculatable }
attributes #3 = { nounwind readnone }
attributes #4 = { nounwind }

!llvm.module.flags = !{!0, !1}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 2}
