; ModuleID = '../../third_party/libvpx/source/libvpx/vpx_dsp/x86/highbd_idct8x8_add_sse4.c'
source_filename = "../../third_party/libvpx/source/libvpx/vpx_dsp/x86/highbd_idct8x8_add_sse4.c"
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

; Function Attrs: nounwind ssp uwtable
define hidden void @vpx_highbd_idct8x8_half1d_sse4_1(<2 x i64>*) local_unnamed_addr #0 {
  %2 = bitcast <2 x i64>* %0 to <4 x i32>*
  %3 = load <4 x i32>, <4 x i32>* %2, align 16
  %4 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %5 = bitcast <2 x i64>* %4 to <4 x i32>*
  %6 = load <4 x i32>, <4 x i32>* %5, align 16
  %7 = shufflevector <4 x i32> %3, <4 x i32> %6, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %8 = bitcast <4 x i32> %7 to <2 x i64>
  %9 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %10 = bitcast <2 x i64>* %9 to <4 x i32>*
  %11 = load <4 x i32>, <4 x i32>* %10, align 16
  %12 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %13 = bitcast <2 x i64>* %12 to <4 x i32>*
  %14 = load <4 x i32>, <4 x i32>* %13, align 16
  %15 = shufflevector <4 x i32> %11, <4 x i32> %14, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %16 = bitcast <4 x i32> %15 to <2 x i64>
  %17 = shufflevector <4 x i32> %3, <4 x i32> %6, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %18 = bitcast <4 x i32> %17 to <2 x i64>
  %19 = shufflevector <4 x i32> %11, <4 x i32> %14, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %20 = bitcast <4 x i32> %19 to <2 x i64>
  %21 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 4
  %22 = bitcast <2 x i64>* %21 to <4 x i32>*
  %23 = load <4 x i32>, <4 x i32>* %22, align 16
  %24 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 5
  %25 = bitcast <2 x i64>* %24 to <4 x i32>*
  %26 = load <4 x i32>, <4 x i32>* %25, align 16
  %27 = shufflevector <4 x i32> %23, <4 x i32> %26, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %28 = bitcast <4 x i32> %27 to <2 x i64>
  %29 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 6
  %30 = bitcast <2 x i64>* %29 to <4 x i32>*
  %31 = load <4 x i32>, <4 x i32>* %30, align 16
  %32 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 7
  %33 = bitcast <2 x i64>* %32 to <4 x i32>*
  %34 = load <4 x i32>, <4 x i32>* %33, align 16
  %35 = shufflevector <4 x i32> %31, <4 x i32> %34, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %36 = bitcast <4 x i32> %35 to <2 x i64>
  %37 = shufflevector <4 x i32> %23, <4 x i32> %26, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %38 = bitcast <4 x i32> %37 to <2 x i64>
  %39 = shufflevector <4 x i32> %31, <4 x i32> %34, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %40 = bitcast <4 x i32> %39 to <2 x i64>
  %41 = shufflevector <2 x i64> %8, <2 x i64> %16, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %41, <2 x i64>* %0, align 16
  %42 = shufflevector <2 x i64> %8, <2 x i64> %16, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %42, <2 x i64>* %4, align 16
  %43 = shufflevector <2 x i64> %18, <2 x i64> %20, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %43, <2 x i64>* %9, align 16
  %44 = shufflevector <2 x i64> %18, <2 x i64> %20, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %44, <2 x i64>* %12, align 16
  %45 = shufflevector <2 x i64> %28, <2 x i64> %36, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %45, <2 x i64>* %21, align 16
  %46 = shufflevector <2 x i64> %28, <2 x i64> %36, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %46, <2 x i64>* %24, align 16
  %47 = shufflevector <2 x i64> %38, <2 x i64> %40, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %47, <2 x i64>* %29, align 16
  %48 = shufflevector <2 x i64> %38, <2 x i64> %40, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %48, <2 x i64>* %32, align 16
  %49 = bitcast <2 x i64> %42 to <4 x i32>
  %50 = shufflevector <4 x i32> %49, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %51 = bitcast <4 x i32> %50 to <2 x i64>
  %52 = shufflevector <4 x i32> %49, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %53 = bitcast <4 x i32> %52 to <2 x i64>
  %54 = bitcast <2 x i64> %48 to <4 x i32>
  %55 = shufflevector <4 x i32> %54, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %56 = bitcast <4 x i32> %55 to <2 x i64>
  %57 = shufflevector <4 x i32> %54, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %58 = bitcast <4 x i32> %57 to <2 x i64>
  %59 = shl <2 x i64> %51, <i64 32, i64 32>
  %60 = ashr exact <2 x i64> %59, <i64 32, i64 32>
  %61 = mul nsw <2 x i64> %60, <i64 64276, i64 64276>
  %62 = shl <2 x i64> %53, <i64 32, i64 32>
  %63 = ashr exact <2 x i64> %62, <i64 32, i64 32>
  %64 = mul nsw <2 x i64> %63, <i64 64276, i64 64276>
  %65 = mul nsw <2 x i64> %60, <i64 12784, i64 12784>
  %66 = mul nsw <2 x i64> %63, <i64 12784, i64 12784>
  %67 = shl <2 x i64> %56, <i64 32, i64 32>
  %68 = ashr exact <2 x i64> %67, <i64 32, i64 32>
  %69 = mul nsw <2 x i64> %68, <i64 12784, i64 12784>
  %70 = shl <2 x i64> %58, <i64 32, i64 32>
  %71 = ashr exact <2 x i64> %70, <i64 32, i64 32>
  %72 = mul nsw <2 x i64> %71, <i64 12784, i64 12784>
  %73 = add nsw <2 x i64> %65, <i64 32768, i64 32768>
  %74 = mul nsw <2 x i64> %68, <i64 -64276, i64 -64276>
  %75 = add nsw <2 x i64> %73, %74
  %76 = bitcast <2 x i64> %75 to <16 x i8>
  %77 = shufflevector <16 x i8> %76, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %78 = add nsw <2 x i64> %66, <i64 32768, i64 32768>
  %79 = mul nsw <2 x i64> %71, <i64 -64276, i64 -64276>
  %80 = add nsw <2 x i64> %78, %79
  %81 = bitcast <2 x i64> %80 to <16 x i8>
  %82 = shufflevector <16 x i8> %81, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %83 = add nsw <2 x i64> %61, <i64 32768, i64 32768>
  %84 = add nsw <2 x i64> %83, %69
  %85 = bitcast <2 x i64> %84 to <16 x i8>
  %86 = shufflevector <16 x i8> %85, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %87 = add nsw <2 x i64> %64, <i64 32768, i64 32768>
  %88 = add nsw <2 x i64> %87, %72
  %89 = bitcast <2 x i64> %88 to <16 x i8>
  %90 = shufflevector <16 x i8> %89, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %91 = bitcast <16 x i8> %77 to <4 x i32>
  %92 = bitcast <16 x i8> %82 to <4 x i32>
  %93 = shufflevector <4 x i32> %91, <4 x i32> %92, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %94 = shufflevector <4 x i32> %91, <4 x i32> %92, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %95 = shufflevector <4 x i32> %93, <4 x i32> %94, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %96 = bitcast <16 x i8> %86 to <4 x i32>
  %97 = bitcast <16 x i8> %90 to <4 x i32>
  %98 = shufflevector <4 x i32> %96, <4 x i32> %97, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %99 = shufflevector <4 x i32> %96, <4 x i32> %97, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %100 = shufflevector <4 x i32> %98, <4 x i32> %99, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %101 = bitcast <2 x i64> %46 to <4 x i32>
  %102 = shufflevector <4 x i32> %101, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %103 = bitcast <4 x i32> %102 to <2 x i64>
  %104 = shufflevector <4 x i32> %101, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %105 = bitcast <4 x i32> %104 to <2 x i64>
  %106 = bitcast <2 x i64> %44 to <4 x i32>
  %107 = shufflevector <4 x i32> %106, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %108 = bitcast <4 x i32> %107 to <2 x i64>
  %109 = shufflevector <4 x i32> %106, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %110 = bitcast <4 x i32> %109 to <2 x i64>
  %111 = shl <2 x i64> %103, <i64 32, i64 32>
  %112 = ashr exact <2 x i64> %111, <i64 32, i64 32>
  %113 = mul nsw <2 x i64> %112, <i64 36408, i64 36408>
  %114 = shl <2 x i64> %105, <i64 32, i64 32>
  %115 = ashr exact <2 x i64> %114, <i64 32, i64 32>
  %116 = mul nsw <2 x i64> %115, <i64 36408, i64 36408>
  %117 = mul nsw <2 x i64> %112, <i64 54492, i64 54492>
  %118 = mul nsw <2 x i64> %115, <i64 54492, i64 54492>
  %119 = shl <2 x i64> %108, <i64 32, i64 32>
  %120 = ashr exact <2 x i64> %119, <i64 32, i64 32>
  %121 = mul nsw <2 x i64> %120, <i64 54492, i64 54492>
  %122 = shl <2 x i64> %110, <i64 32, i64 32>
  %123 = ashr exact <2 x i64> %122, <i64 32, i64 32>
  %124 = mul nsw <2 x i64> %123, <i64 54492, i64 54492>
  %125 = mul nsw <2 x i64> %120, <i64 -36408, i64 -36408>
  %126 = add nsw <2 x i64> %125, <i64 32768, i64 32768>
  %127 = add nsw <2 x i64> %126, %117
  %128 = bitcast <2 x i64> %127 to <16 x i8>
  %129 = shufflevector <16 x i8> %128, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %130 = mul nsw <2 x i64> %123, <i64 -36408, i64 -36408>
  %131 = add nsw <2 x i64> %130, <i64 32768, i64 32768>
  %132 = add nsw <2 x i64> %131, %118
  %133 = bitcast <2 x i64> %132 to <16 x i8>
  %134 = shufflevector <16 x i8> %133, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %135 = add nsw <2 x i64> %121, <i64 32768, i64 32768>
  %136 = add nsw <2 x i64> %135, %113
  %137 = bitcast <2 x i64> %136 to <16 x i8>
  %138 = shufflevector <16 x i8> %137, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %139 = add nsw <2 x i64> %124, <i64 32768, i64 32768>
  %140 = add nsw <2 x i64> %139, %116
  %141 = bitcast <2 x i64> %140 to <16 x i8>
  %142 = shufflevector <16 x i8> %141, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %143 = bitcast <16 x i8> %129 to <4 x i32>
  %144 = bitcast <16 x i8> %134 to <4 x i32>
  %145 = shufflevector <4 x i32> %143, <4 x i32> %144, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %146 = shufflevector <4 x i32> %143, <4 x i32> %144, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %147 = shufflevector <4 x i32> %145, <4 x i32> %146, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %148 = bitcast <16 x i8> %138 to <4 x i32>
  %149 = bitcast <16 x i8> %142 to <4 x i32>
  %150 = shufflevector <4 x i32> %148, <4 x i32> %149, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %151 = shufflevector <4 x i32> %148, <4 x i32> %149, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %152 = shufflevector <4 x i32> %150, <4 x i32> %151, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %153 = bitcast <2 x i64> %41 to <4 x i32>
  %154 = bitcast <2 x i64> %45 to <4 x i32>
  %155 = add <4 x i32> %154, %153
  %156 = shufflevector <4 x i32> %155, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %157 = bitcast <4 x i32> %156 to <2 x i64>
  %158 = shufflevector <4 x i32> %155, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %159 = bitcast <4 x i32> %158 to <2 x i64>
  %160 = shl <2 x i64> %157, <i64 32, i64 32>
  %161 = ashr exact <2 x i64> %160, <i64 32, i64 32>
  %162 = mul nsw <2 x i64> %161, <i64 46340, i64 46340>
  %163 = shl <2 x i64> %159, <i64 32, i64 32>
  %164 = ashr exact <2 x i64> %163, <i64 32, i64 32>
  %165 = mul nsw <2 x i64> %164, <i64 46340, i64 46340>
  %166 = add nsw <2 x i64> %162, <i64 32768, i64 32768>
  %167 = bitcast <2 x i64> %166 to <16 x i8>
  %168 = shufflevector <16 x i8> %167, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %169 = add nsw <2 x i64> %165, <i64 32768, i64 32768>
  %170 = bitcast <2 x i64> %169 to <16 x i8>
  %171 = shufflevector <16 x i8> %170, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %172 = bitcast <16 x i8> %168 to <4 x i32>
  %173 = bitcast <16 x i8> %171 to <4 x i32>
  %174 = shufflevector <4 x i32> %172, <4 x i32> %173, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %175 = shufflevector <4 x i32> %172, <4 x i32> %173, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %176 = shufflevector <4 x i32> %174, <4 x i32> %175, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %177 = sub <4 x i32> %153, %154
  %178 = shufflevector <4 x i32> %177, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %179 = bitcast <4 x i32> %178 to <2 x i64>
  %180 = shufflevector <4 x i32> %177, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %181 = bitcast <4 x i32> %180 to <2 x i64>
  %182 = shl <2 x i64> %179, <i64 32, i64 32>
  %183 = ashr exact <2 x i64> %182, <i64 32, i64 32>
  %184 = mul nsw <2 x i64> %183, <i64 46340, i64 46340>
  %185 = shl <2 x i64> %181, <i64 32, i64 32>
  %186 = ashr exact <2 x i64> %185, <i64 32, i64 32>
  %187 = mul nsw <2 x i64> %186, <i64 46340, i64 46340>
  %188 = add nsw <2 x i64> %184, <i64 32768, i64 32768>
  %189 = bitcast <2 x i64> %188 to <16 x i8>
  %190 = shufflevector <16 x i8> %189, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %191 = add nsw <2 x i64> %187, <i64 32768, i64 32768>
  %192 = bitcast <2 x i64> %191 to <16 x i8>
  %193 = shufflevector <16 x i8> %192, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %194 = bitcast <16 x i8> %190 to <4 x i32>
  %195 = bitcast <16 x i8> %193 to <4 x i32>
  %196 = shufflevector <4 x i32> %194, <4 x i32> %195, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %197 = shufflevector <4 x i32> %194, <4 x i32> %195, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %198 = shufflevector <4 x i32> %196, <4 x i32> %197, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %199 = bitcast <2 x i64> %43 to <4 x i32>
  %200 = shufflevector <4 x i32> %199, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %201 = bitcast <4 x i32> %200 to <2 x i64>
  %202 = shufflevector <4 x i32> %199, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %203 = bitcast <4 x i32> %202 to <2 x i64>
  %204 = bitcast <2 x i64> %47 to <4 x i32>
  %205 = shufflevector <4 x i32> %204, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %206 = bitcast <4 x i32> %205 to <2 x i64>
  %207 = shufflevector <4 x i32> %204, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %208 = bitcast <4 x i32> %207 to <2 x i64>
  %209 = shl <2 x i64> %201, <i64 32, i64 32>
  %210 = ashr exact <2 x i64> %209, <i64 32, i64 32>
  %211 = mul nsw <2 x i64> %210, <i64 60548, i64 60548>
  %212 = shl <2 x i64> %203, <i64 32, i64 32>
  %213 = ashr exact <2 x i64> %212, <i64 32, i64 32>
  %214 = mul nsw <2 x i64> %213, <i64 60548, i64 60548>
  %215 = mul nsw <2 x i64> %210, <i64 25080, i64 25080>
  %216 = mul nsw <2 x i64> %213, <i64 25080, i64 25080>
  %217 = shl <2 x i64> %206, <i64 32, i64 32>
  %218 = ashr exact <2 x i64> %217, <i64 32, i64 32>
  %219 = mul nsw <2 x i64> %218, <i64 25080, i64 25080>
  %220 = shl <2 x i64> %208, <i64 32, i64 32>
  %221 = ashr exact <2 x i64> %220, <i64 32, i64 32>
  %222 = mul nsw <2 x i64> %221, <i64 25080, i64 25080>
  %223 = add nsw <2 x i64> %215, <i64 32768, i64 32768>
  %224 = mul nsw <2 x i64> %218, <i64 -60548, i64 -60548>
  %225 = add nsw <2 x i64> %223, %224
  %226 = bitcast <2 x i64> %225 to <16 x i8>
  %227 = shufflevector <16 x i8> %226, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %228 = add nsw <2 x i64> %216, <i64 32768, i64 32768>
  %229 = mul nsw <2 x i64> %221, <i64 -60548, i64 -60548>
  %230 = add nsw <2 x i64> %228, %229
  %231 = bitcast <2 x i64> %230 to <16 x i8>
  %232 = shufflevector <16 x i8> %231, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %233 = add nsw <2 x i64> %211, <i64 32768, i64 32768>
  %234 = add nsw <2 x i64> %233, %219
  %235 = bitcast <2 x i64> %234 to <16 x i8>
  %236 = shufflevector <16 x i8> %235, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %237 = add nsw <2 x i64> %214, <i64 32768, i64 32768>
  %238 = add nsw <2 x i64> %237, %222
  %239 = bitcast <2 x i64> %238 to <16 x i8>
  %240 = shufflevector <16 x i8> %239, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %241 = bitcast <16 x i8> %227 to <4 x i32>
  %242 = bitcast <16 x i8> %232 to <4 x i32>
  %243 = shufflevector <4 x i32> %241, <4 x i32> %242, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %244 = shufflevector <4 x i32> %241, <4 x i32> %242, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %245 = shufflevector <4 x i32> %243, <4 x i32> %244, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %246 = bitcast <16 x i8> %236 to <4 x i32>
  %247 = bitcast <16 x i8> %240 to <4 x i32>
  %248 = shufflevector <4 x i32> %246, <4 x i32> %247, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %249 = shufflevector <4 x i32> %246, <4 x i32> %247, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %250 = shufflevector <4 x i32> %248, <4 x i32> %249, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %251 = add <4 x i32> %147, %95
  %252 = sub <4 x i32> %95, %147
  %253 = sub <4 x i32> %100, %152
  %254 = add <4 x i32> %100, %152
  %255 = add <4 x i32> %250, %176
  %256 = add <4 x i32> %198, %245
  %257 = sub <4 x i32> %198, %245
  %258 = sub <4 x i32> %176, %250
  %259 = add <4 x i32> %252, %253
  %260 = shufflevector <4 x i32> %259, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %261 = bitcast <4 x i32> %260 to <2 x i64>
  %262 = shufflevector <4 x i32> %259, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %263 = bitcast <4 x i32> %262 to <2 x i64>
  %264 = shl <2 x i64> %261, <i64 32, i64 32>
  %265 = ashr exact <2 x i64> %264, <i64 32, i64 32>
  %266 = mul nsw <2 x i64> %265, <i64 46340, i64 46340>
  %267 = shl <2 x i64> %263, <i64 32, i64 32>
  %268 = ashr exact <2 x i64> %267, <i64 32, i64 32>
  %269 = mul nsw <2 x i64> %268, <i64 46340, i64 46340>
  %270 = add nsw <2 x i64> %266, <i64 32768, i64 32768>
  %271 = bitcast <2 x i64> %270 to <16 x i8>
  %272 = shufflevector <16 x i8> %271, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %273 = add nsw <2 x i64> %269, <i64 32768, i64 32768>
  %274 = bitcast <2 x i64> %273 to <16 x i8>
  %275 = shufflevector <16 x i8> %274, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %276 = bitcast <16 x i8> %272 to <4 x i32>
  %277 = bitcast <16 x i8> %275 to <4 x i32>
  %278 = shufflevector <4 x i32> %276, <4 x i32> %277, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %279 = shufflevector <4 x i32> %276, <4 x i32> %277, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %280 = shufflevector <4 x i32> %278, <4 x i32> %279, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %281 = sub <4 x i32> %253, %252
  %282 = shufflevector <4 x i32> %281, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %283 = bitcast <4 x i32> %282 to <2 x i64>
  %284 = shufflevector <4 x i32> %281, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %285 = bitcast <4 x i32> %284 to <2 x i64>
  %286 = shl <2 x i64> %283, <i64 32, i64 32>
  %287 = ashr exact <2 x i64> %286, <i64 32, i64 32>
  %288 = mul nsw <2 x i64> %287, <i64 46340, i64 46340>
  %289 = shl <2 x i64> %285, <i64 32, i64 32>
  %290 = ashr exact <2 x i64> %289, <i64 32, i64 32>
  %291 = mul nsw <2 x i64> %290, <i64 46340, i64 46340>
  %292 = add nsw <2 x i64> %288, <i64 32768, i64 32768>
  %293 = bitcast <2 x i64> %292 to <16 x i8>
  %294 = shufflevector <16 x i8> %293, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %295 = add nsw <2 x i64> %291, <i64 32768, i64 32768>
  %296 = bitcast <2 x i64> %295 to <16 x i8>
  %297 = shufflevector <16 x i8> %296, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %298 = bitcast <16 x i8> %294 to <4 x i32>
  %299 = bitcast <16 x i8> %297 to <4 x i32>
  %300 = shufflevector <4 x i32> %298, <4 x i32> %299, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %301 = shufflevector <4 x i32> %298, <4 x i32> %299, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %302 = shufflevector <4 x i32> %300, <4 x i32> %301, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %303 = add <4 x i32> %254, %255
  store <4 x i32> %303, <4 x i32>* %2, align 16
  %304 = add <4 x i32> %280, %256
  store <4 x i32> %304, <4 x i32>* %5, align 16
  %305 = add <4 x i32> %302, %257
  store <4 x i32> %305, <4 x i32>* %10, align 16
  %306 = add <4 x i32> %251, %258
  store <4 x i32> %306, <4 x i32>* %13, align 16
  %307 = sub <4 x i32> %258, %251
  store <4 x i32> %307, <4 x i32>* %22, align 16
  %308 = sub <4 x i32> %257, %302
  store <4 x i32> %308, <4 x i32>* %25, align 16
  %309 = sub <4 x i32> %256, %280
  store <4 x i32> %309, <4 x i32>* %30, align 16
  %310 = sub <4 x i32> %255, %254
  store <4 x i32> %310, <4 x i32>* %33, align 16
  ret void
}

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #1

; Function Attrs: argmemonly nounwind
declare void @llvm.memset.p0i8.i64(i8* nocapture writeonly, i8, i64, i1 immarg) #1

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #1

; Function Attrs: nounwind ssp uwtable
define hidden void @vpx_highbd_idct8x8_64_add_sse4_1(i32* readonly, i16* nocapture, i32, i32) local_unnamed_addr #0 {
  %5 = alloca [16 x <2 x i64>], align 16
  %6 = alloca [8 x <2 x i64>], align 16
  %7 = bitcast [16 x <2 x i64>]* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %7) #5
  %8 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 8
  %9 = bitcast <2 x i64>* %8 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %9, i8 -86, i64 128, i1 false)
  %10 = bitcast i32* %0 to <2 x i64>*
  %11 = load <2 x i64>, <2 x i64>* %10, align 16
  %12 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 0
  store <2 x i64> %11, <2 x i64>* %12, align 16
  %13 = getelementptr inbounds i32, i32* %0, i64 4
  %14 = bitcast i32* %13 to <2 x i64>*
  %15 = load <2 x i64>, <2 x i64>* %14, align 16
  %16 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 4
  store <2 x i64> %15, <2 x i64>* %16, align 16
  %17 = getelementptr inbounds i32, i32* %0, i64 8
  %18 = bitcast i32* %17 to <2 x i64>*
  %19 = load <2 x i64>, <2 x i64>* %18, align 16
  %20 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 1
  store <2 x i64> %19, <2 x i64>* %20, align 16
  %21 = getelementptr inbounds i32, i32* %0, i64 12
  %22 = bitcast i32* %21 to <2 x i64>*
  %23 = load <2 x i64>, <2 x i64>* %22, align 16
  %24 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 5
  store <2 x i64> %23, <2 x i64>* %24, align 16
  %25 = getelementptr inbounds i32, i32* %0, i64 16
  %26 = bitcast i32* %25 to <2 x i64>*
  %27 = load <2 x i64>, <2 x i64>* %26, align 16
  %28 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 2
  store <2 x i64> %27, <2 x i64>* %28, align 16
  %29 = getelementptr inbounds i32, i32* %0, i64 20
  %30 = bitcast i32* %29 to <2 x i64>*
  %31 = load <2 x i64>, <2 x i64>* %30, align 16
  %32 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 6
  store <2 x i64> %31, <2 x i64>* %32, align 16
  %33 = getelementptr inbounds i32, i32* %0, i64 24
  %34 = bitcast i32* %33 to <2 x i64>*
  %35 = load <2 x i64>, <2 x i64>* %34, align 16
  %36 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 3
  store <2 x i64> %35, <2 x i64>* %36, align 16
  %37 = getelementptr inbounds i32, i32* %0, i64 28
  %38 = bitcast i32* %37 to <2 x i64>*
  %39 = load <2 x i64>, <2 x i64>* %38, align 16
  %40 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 7
  store <2 x i64> %39, <2 x i64>* %40, align 16
  %41 = icmp eq i32 %3, 8
  br i1 %41, label %42, label %148

42:                                               ; preds = %4
  %43 = bitcast [8 x <2 x i64>]* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 128, i8* nonnull %43) #5
  %44 = bitcast <2 x i64> %11 to <4 x i32>
  %45 = bitcast <2 x i64> %15 to <4 x i32>
  %46 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %44, <4 x i32> %45) #5
  %47 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 0
  %48 = bitcast [8 x <2 x i64>]* %6 to <8 x i16>*
  store <8 x i16> %46, <8 x i16>* %48, align 16
  %49 = bitcast <2 x i64> %19 to <4 x i32>
  %50 = bitcast <2 x i64> %23 to <4 x i32>
  %51 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %49, <4 x i32> %50) #5
  %52 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 1
  %53 = bitcast <2 x i64>* %52 to <8 x i16>*
  store <8 x i16> %51, <8 x i16>* %53, align 16
  %54 = bitcast <2 x i64> %27 to <4 x i32>
  %55 = bitcast <2 x i64> %31 to <4 x i32>
  %56 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %54, <4 x i32> %55) #5
  %57 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 2
  %58 = bitcast <2 x i64>* %57 to <8 x i16>*
  store <8 x i16> %56, <8 x i16>* %58, align 16
  %59 = bitcast <2 x i64> %35 to <4 x i32>
  %60 = bitcast <2 x i64> %39 to <4 x i32>
  %61 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %59, <4 x i32> %60) #5
  %62 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 3
  %63 = bitcast <2 x i64>* %62 to <8 x i16>*
  store <8 x i16> %61, <8 x i16>* %63, align 16
  %64 = getelementptr inbounds i32, i32* %0, i64 32
  %65 = bitcast i32* %64 to <2 x i64>*
  %66 = load <2 x i64>, <2 x i64>* %65, align 16
  %67 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 8
  store <2 x i64> %66, <2 x i64>* %67, align 16
  %68 = getelementptr inbounds i32, i32* %0, i64 36
  %69 = bitcast i32* %68 to <2 x i64>*
  %70 = load <2 x i64>, <2 x i64>* %69, align 16
  %71 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 12
  store <2 x i64> %70, <2 x i64>* %71, align 16
  %72 = getelementptr inbounds i32, i32* %0, i64 40
  %73 = bitcast i32* %72 to <2 x i64>*
  %74 = load <2 x i64>, <2 x i64>* %73, align 16
  %75 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 9
  store <2 x i64> %74, <2 x i64>* %75, align 16
  %76 = getelementptr inbounds i32, i32* %0, i64 44
  %77 = bitcast i32* %76 to <2 x i64>*
  %78 = load <2 x i64>, <2 x i64>* %77, align 16
  %79 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 13
  store <2 x i64> %78, <2 x i64>* %79, align 16
  %80 = getelementptr inbounds i32, i32* %0, i64 48
  %81 = bitcast i32* %80 to <2 x i64>*
  %82 = load <2 x i64>, <2 x i64>* %81, align 16
  %83 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 10
  store <2 x i64> %82, <2 x i64>* %83, align 16
  %84 = getelementptr inbounds i32, i32* %0, i64 52
  %85 = bitcast i32* %84 to <2 x i64>*
  %86 = load <2 x i64>, <2 x i64>* %85, align 16
  %87 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 14
  store <2 x i64> %86, <2 x i64>* %87, align 16
  %88 = getelementptr inbounds i32, i32* %0, i64 56
  %89 = bitcast i32* %88 to <2 x i64>*
  %90 = load <2 x i64>, <2 x i64>* %89, align 16
  %91 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 11
  store <2 x i64> %90, <2 x i64>* %91, align 16
  %92 = getelementptr inbounds i32, i32* %0, i64 60
  %93 = bitcast i32* %92 to <2 x i64>*
  %94 = load <2 x i64>, <2 x i64>* %93, align 16
  %95 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 15
  store <2 x i64> %94, <2 x i64>* %95, align 16
  %96 = bitcast <2 x i64> %66 to <4 x i32>
  %97 = bitcast <2 x i64> %70 to <4 x i32>
  %98 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %96, <4 x i32> %97) #5
  %99 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 4
  %100 = bitcast <2 x i64>* %99 to <8 x i16>*
  store <8 x i16> %98, <8 x i16>* %100, align 16
  %101 = bitcast <2 x i64> %74 to <4 x i32>
  %102 = bitcast <2 x i64> %78 to <4 x i32>
  %103 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %101, <4 x i32> %102) #5
  %104 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 5
  %105 = bitcast <2 x i64>* %104 to <8 x i16>*
  store <8 x i16> %103, <8 x i16>* %105, align 16
  %106 = bitcast <2 x i64> %82 to <4 x i32>
  %107 = bitcast <2 x i64> %86 to <4 x i32>
  %108 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %106, <4 x i32> %107) #5
  %109 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 6
  %110 = bitcast <2 x i64>* %109 to <8 x i16>*
  store <8 x i16> %108, <8 x i16>* %110, align 16
  %111 = bitcast <2 x i64> %90 to <4 x i32>
  %112 = bitcast <2 x i64> %94 to <4 x i32>
  %113 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %111, <4 x i32> %112) #5
  %114 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 7
  %115 = bitcast <2 x i64>* %114 to <8 x i16>*
  store <8 x i16> %113, <8 x i16>* %115, align 16
  call void @vpx_idct8_sse2(<2 x i64>* nonnull %47) #5
  call void @vpx_idct8_sse2(<2 x i64>* nonnull %47) #5
  %116 = load <8 x i16>, <8 x i16>* %48, align 16
  %117 = add <8 x i16> %116, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %118 = bitcast [16 x <2 x i64>]* %5 to <8 x i16>*
  %119 = load <8 x i16>, <8 x i16>* %53, align 16
  %120 = add <8 x i16> %119, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %121 = bitcast <2 x i64>* %20 to <8 x i16>*
  %122 = load <8 x i16>, <8 x i16>* %58, align 16
  %123 = add <8 x i16> %122, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %124 = bitcast <2 x i64>* %28 to <8 x i16>*
  %125 = load <8 x i16>, <8 x i16>* %63, align 16
  %126 = add <8 x i16> %125, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %127 = bitcast <2 x i64>* %36 to <8 x i16>*
  %128 = load <8 x i16>, <8 x i16>* %100, align 16
  %129 = add <8 x i16> %128, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %130 = bitcast <2 x i64>* %16 to <8 x i16>*
  %131 = load <8 x i16>, <8 x i16>* %105, align 16
  %132 = add <8 x i16> %131, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %133 = bitcast <2 x i64>* %24 to <8 x i16>*
  %134 = load <8 x i16>, <8 x i16>* %110, align 16
  %135 = add <8 x i16> %134, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %136 = bitcast <2 x i64>* %32 to <8 x i16>*
  %137 = load <8 x i16>, <8 x i16>* %115, align 16
  %138 = add <8 x i16> %137, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %139 = bitcast <2 x i64>* %40 to <8 x i16>*
  %140 = ashr <8 x i16> %117, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %140, <8 x i16>* %118, align 16
  %141 = ashr <8 x i16> %120, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %141, <8 x i16>* %121, align 16
  %142 = ashr <8 x i16> %123, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %142, <8 x i16>* %124, align 16
  %143 = ashr <8 x i16> %126, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %143, <8 x i16>* %127, align 16
  %144 = ashr <8 x i16> %129, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %144, <8 x i16>* %130, align 16
  %145 = ashr <8 x i16> %132, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %145, <8 x i16>* %133, align 16
  %146 = ashr <8 x i16> %135, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %146, <8 x i16>* %136, align 16
  %147 = ashr <8 x i16> %138, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %147, <8 x i16>* %139, align 16
  call void @llvm.lifetime.end.p0i8(i64 128, i8* nonnull %43) #5
  br label %269

148:                                              ; preds = %4
  call void @vpx_highbd_idct8x8_half1d_sse4_1(<2 x i64>* nonnull %12)
  %149 = getelementptr inbounds i32, i32* %0, i64 32
  %150 = bitcast i32* %149 to <2 x i64>*
  %151 = load <2 x i64>, <2 x i64>* %150, align 16
  %152 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 8
  store <2 x i64> %151, <2 x i64>* %152, align 16
  %153 = getelementptr inbounds i32, i32* %0, i64 36
  %154 = bitcast i32* %153 to <2 x i64>*
  %155 = load <2 x i64>, <2 x i64>* %154, align 16
  %156 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 12
  store <2 x i64> %155, <2 x i64>* %156, align 16
  %157 = getelementptr inbounds i32, i32* %0, i64 40
  %158 = bitcast i32* %157 to <2 x i64>*
  %159 = load <2 x i64>, <2 x i64>* %158, align 16
  %160 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 9
  store <2 x i64> %159, <2 x i64>* %160, align 16
  %161 = getelementptr inbounds i32, i32* %0, i64 44
  %162 = bitcast i32* %161 to <2 x i64>*
  %163 = load <2 x i64>, <2 x i64>* %162, align 16
  %164 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 13
  store <2 x i64> %163, <2 x i64>* %164, align 16
  %165 = getelementptr inbounds i32, i32* %0, i64 48
  %166 = bitcast i32* %165 to <2 x i64>*
  %167 = load <2 x i64>, <2 x i64>* %166, align 16
  %168 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 10
  store <2 x i64> %167, <2 x i64>* %168, align 16
  %169 = getelementptr inbounds i32, i32* %0, i64 52
  %170 = bitcast i32* %169 to <2 x i64>*
  %171 = load <2 x i64>, <2 x i64>* %170, align 16
  %172 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 14
  store <2 x i64> %171, <2 x i64>* %172, align 16
  %173 = getelementptr inbounds i32, i32* %0, i64 56
  %174 = bitcast i32* %173 to <2 x i64>*
  %175 = load <2 x i64>, <2 x i64>* %174, align 16
  %176 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 11
  store <2 x i64> %175, <2 x i64>* %176, align 16
  %177 = getelementptr inbounds i32, i32* %0, i64 60
  %178 = bitcast i32* %177 to <2 x i64>*
  %179 = load <2 x i64>, <2 x i64>* %178, align 16
  %180 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 15
  store <2 x i64> %179, <2 x i64>* %180, align 16
  call void @vpx_highbd_idct8x8_half1d_sse4_1(<2 x i64>* %152)
  %181 = load <2 x i64>, <2 x i64>* %16, align 16
  %182 = load <2 x i64>, <2 x i64>* %24, align 16
  %183 = load <2 x i64>, <2 x i64>* %32, align 16
  %184 = load <2 x i64>, <2 x i64>* %40, align 16
  %185 = load <2 x i64>, <2 x i64>* %152, align 16
  store <2 x i64> %185, <2 x i64>* %16, align 16
  %186 = load <2 x i64>, <2 x i64>* %160, align 16
  store <2 x i64> %186, <2 x i64>* %24, align 16
  %187 = load <2 x i64>, <2 x i64>* %168, align 16
  store <2 x i64> %187, <2 x i64>* %32, align 16
  %188 = load <2 x i64>, <2 x i64>* %176, align 16
  store <2 x i64> %188, <2 x i64>* %40, align 16
  call void @vpx_highbd_idct8x8_half1d_sse4_1(<2 x i64>* nonnull %12)
  store <2 x i64> %181, <2 x i64>* %152, align 16
  store <2 x i64> %182, <2 x i64>* %160, align 16
  store <2 x i64> %183, <2 x i64>* %168, align 16
  store <2 x i64> %184, <2 x i64>* %176, align 16
  call void @vpx_highbd_idct8x8_half1d_sse4_1(<2 x i64>* %152)
  %189 = bitcast [16 x <2 x i64>]* %5 to <4 x i32>*
  %190 = load <4 x i32>, <4 x i32>* %189, align 16
  %191 = bitcast <2 x i64>* %152 to <4 x i32>*
  %192 = load <4 x i32>, <4 x i32>* %191, align 16
  %193 = add <4 x i32> %190, <i32 16, i32 16, i32 16, i32 16>
  %194 = add <4 x i32> %192, <i32 16, i32 16, i32 16, i32 16>
  %195 = ashr <4 x i32> %193, <i32 5, i32 5, i32 5, i32 5>
  %196 = ashr <4 x i32> %194, <i32 5, i32 5, i32 5, i32 5>
  %197 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %195, <4 x i32> %196) #5
  %198 = bitcast [16 x <2 x i64>]* %5 to <8 x i16>*
  store <8 x i16> %197, <8 x i16>* %198, align 16
  %199 = bitcast <2 x i64>* %20 to <4 x i32>*
  %200 = load <4 x i32>, <4 x i32>* %199, align 16
  %201 = bitcast <2 x i64>* %160 to <4 x i32>*
  %202 = load <4 x i32>, <4 x i32>* %201, align 16
  %203 = add <4 x i32> %200, <i32 16, i32 16, i32 16, i32 16>
  %204 = add <4 x i32> %202, <i32 16, i32 16, i32 16, i32 16>
  %205 = ashr <4 x i32> %203, <i32 5, i32 5, i32 5, i32 5>
  %206 = ashr <4 x i32> %204, <i32 5, i32 5, i32 5, i32 5>
  %207 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %205, <4 x i32> %206) #5
  %208 = bitcast <2 x i64>* %20 to <8 x i16>*
  store <8 x i16> %207, <8 x i16>* %208, align 16
  %209 = bitcast <2 x i64>* %28 to <4 x i32>*
  %210 = load <4 x i32>, <4 x i32>* %209, align 16
  %211 = bitcast <2 x i64>* %168 to <4 x i32>*
  %212 = load <4 x i32>, <4 x i32>* %211, align 16
  %213 = add <4 x i32> %210, <i32 16, i32 16, i32 16, i32 16>
  %214 = add <4 x i32> %212, <i32 16, i32 16, i32 16, i32 16>
  %215 = ashr <4 x i32> %213, <i32 5, i32 5, i32 5, i32 5>
  %216 = ashr <4 x i32> %214, <i32 5, i32 5, i32 5, i32 5>
  %217 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %215, <4 x i32> %216) #5
  %218 = bitcast <2 x i64>* %28 to <8 x i16>*
  store <8 x i16> %217, <8 x i16>* %218, align 16
  %219 = bitcast <2 x i64>* %36 to <4 x i32>*
  %220 = load <4 x i32>, <4 x i32>* %219, align 16
  %221 = bitcast <2 x i64>* %176 to <4 x i32>*
  %222 = load <4 x i32>, <4 x i32>* %221, align 16
  %223 = add <4 x i32> %220, <i32 16, i32 16, i32 16, i32 16>
  %224 = add <4 x i32> %222, <i32 16, i32 16, i32 16, i32 16>
  %225 = ashr <4 x i32> %223, <i32 5, i32 5, i32 5, i32 5>
  %226 = ashr <4 x i32> %224, <i32 5, i32 5, i32 5, i32 5>
  %227 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %225, <4 x i32> %226) #5
  %228 = bitcast <2 x i64>* %36 to <8 x i16>*
  store <8 x i16> %227, <8 x i16>* %228, align 16
  %229 = bitcast <2 x i64>* %16 to <4 x i32>*
  %230 = load <4 x i32>, <4 x i32>* %229, align 16
  %231 = bitcast <2 x i64>* %156 to <4 x i32>*
  %232 = load <4 x i32>, <4 x i32>* %231, align 16
  %233 = add <4 x i32> %230, <i32 16, i32 16, i32 16, i32 16>
  %234 = add <4 x i32> %232, <i32 16, i32 16, i32 16, i32 16>
  %235 = ashr <4 x i32> %233, <i32 5, i32 5, i32 5, i32 5>
  %236 = ashr <4 x i32> %234, <i32 5, i32 5, i32 5, i32 5>
  %237 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %235, <4 x i32> %236) #5
  %238 = bitcast <2 x i64>* %16 to <8 x i16>*
  store <8 x i16> %237, <8 x i16>* %238, align 16
  %239 = bitcast <2 x i64>* %24 to <4 x i32>*
  %240 = load <4 x i32>, <4 x i32>* %239, align 16
  %241 = bitcast <2 x i64>* %164 to <4 x i32>*
  %242 = load <4 x i32>, <4 x i32>* %241, align 16
  %243 = add <4 x i32> %240, <i32 16, i32 16, i32 16, i32 16>
  %244 = add <4 x i32> %242, <i32 16, i32 16, i32 16, i32 16>
  %245 = ashr <4 x i32> %243, <i32 5, i32 5, i32 5, i32 5>
  %246 = ashr <4 x i32> %244, <i32 5, i32 5, i32 5, i32 5>
  %247 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %245, <4 x i32> %246) #5
  %248 = bitcast <2 x i64>* %24 to <8 x i16>*
  store <8 x i16> %247, <8 x i16>* %248, align 16
  %249 = bitcast <2 x i64>* %32 to <4 x i32>*
  %250 = load <4 x i32>, <4 x i32>* %249, align 16
  %251 = bitcast <2 x i64>* %172 to <4 x i32>*
  %252 = load <4 x i32>, <4 x i32>* %251, align 16
  %253 = add <4 x i32> %250, <i32 16, i32 16, i32 16, i32 16>
  %254 = add <4 x i32> %252, <i32 16, i32 16, i32 16, i32 16>
  %255 = ashr <4 x i32> %253, <i32 5, i32 5, i32 5, i32 5>
  %256 = ashr <4 x i32> %254, <i32 5, i32 5, i32 5, i32 5>
  %257 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %255, <4 x i32> %256) #5
  %258 = bitcast <2 x i64>* %32 to <8 x i16>*
  store <8 x i16> %257, <8 x i16>* %258, align 16
  %259 = bitcast <2 x i64>* %40 to <4 x i32>*
  %260 = load <4 x i32>, <4 x i32>* %259, align 16
  %261 = bitcast <2 x i64>* %180 to <4 x i32>*
  %262 = load <4 x i32>, <4 x i32>* %261, align 16
  %263 = add <4 x i32> %260, <i32 16, i32 16, i32 16, i32 16>
  %264 = add <4 x i32> %262, <i32 16, i32 16, i32 16, i32 16>
  %265 = ashr <4 x i32> %263, <i32 5, i32 5, i32 5, i32 5>
  %266 = ashr <4 x i32> %264, <i32 5, i32 5, i32 5, i32 5>
  %267 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %265, <4 x i32> %266) #5
  %268 = bitcast <2 x i64>* %40 to <8 x i16>*
  store <8 x i16> %267, <8 x i16>* %268, align 16
  br label %269

269:                                              ; preds = %148, %42
  %270 = phi <8 x i16> [ %267, %148 ], [ %147, %42 ]
  %271 = phi <8 x i16> [ %257, %148 ], [ %146, %42 ]
  %272 = phi <8 x i16> [ %247, %148 ], [ %145, %42 ]
  %273 = phi <8 x i16> [ %237, %148 ], [ %144, %42 ]
  %274 = phi <8 x i16> [ %227, %148 ], [ %143, %42 ]
  %275 = phi <8 x i16> [ %217, %148 ], [ %142, %42 ]
  %276 = phi <8 x i16> [ %207, %148 ], [ %141, %42 ]
  %277 = phi <8 x i16> [ %197, %148 ], [ %140, %42 ]
  %278 = bitcast i16* %1 to <8 x i16>*
  %279 = load <8 x i16>, <8 x i16>* %278, align 16
  %280 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>, i32 %3) #5
  %281 = add <8 x i16> %280, <i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1>
  %282 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %279, <8 x i16> %277) #5
  %283 = icmp sgt <8 x i16> %282, zeroinitializer
  %284 = select <8 x i1> %283, <8 x i16> %282, <8 x i16> zeroinitializer
  %285 = icmp slt <8 x i16> %284, %281
  %286 = select <8 x i1> %285, <8 x i16> %284, <8 x i16> %281
  store <8 x i16> %286, <8 x i16>* %278, align 16
  %287 = sext i32 %2 to i64
  %288 = getelementptr inbounds i16, i16* %1, i64 %287
  %289 = bitcast i16* %288 to <8 x i16>*
  %290 = load <8 x i16>, <8 x i16>* %289, align 16
  %291 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %290, <8 x i16> %276) #5
  %292 = icmp sgt <8 x i16> %291, zeroinitializer
  %293 = select <8 x i1> %292, <8 x i16> %291, <8 x i16> zeroinitializer
  %294 = icmp slt <8 x i16> %293, %281
  %295 = select <8 x i1> %294, <8 x i16> %293, <8 x i16> %281
  store <8 x i16> %295, <8 x i16>* %289, align 16
  %296 = getelementptr inbounds i16, i16* %288, i64 %287
  %297 = bitcast i16* %296 to <8 x i16>*
  %298 = load <8 x i16>, <8 x i16>* %297, align 16
  %299 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %298, <8 x i16> %275) #5
  %300 = icmp sgt <8 x i16> %299, zeroinitializer
  %301 = select <8 x i1> %300, <8 x i16> %299, <8 x i16> zeroinitializer
  %302 = icmp slt <8 x i16> %301, %281
  %303 = select <8 x i1> %302, <8 x i16> %301, <8 x i16> %281
  store <8 x i16> %303, <8 x i16>* %297, align 16
  %304 = getelementptr inbounds i16, i16* %296, i64 %287
  %305 = bitcast i16* %304 to <8 x i16>*
  %306 = load <8 x i16>, <8 x i16>* %305, align 16
  %307 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %306, <8 x i16> %274) #5
  %308 = icmp sgt <8 x i16> %307, zeroinitializer
  %309 = select <8 x i1> %308, <8 x i16> %307, <8 x i16> zeroinitializer
  %310 = icmp slt <8 x i16> %309, %281
  %311 = select <8 x i1> %310, <8 x i16> %309, <8 x i16> %281
  store <8 x i16> %311, <8 x i16>* %305, align 16
  %312 = getelementptr inbounds i16, i16* %304, i64 %287
  %313 = bitcast i16* %312 to <8 x i16>*
  %314 = load <8 x i16>, <8 x i16>* %313, align 16
  %315 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %314, <8 x i16> %273) #5
  %316 = icmp sgt <8 x i16> %315, zeroinitializer
  %317 = select <8 x i1> %316, <8 x i16> %315, <8 x i16> zeroinitializer
  %318 = icmp slt <8 x i16> %317, %281
  %319 = select <8 x i1> %318, <8 x i16> %317, <8 x i16> %281
  store <8 x i16> %319, <8 x i16>* %313, align 16
  %320 = getelementptr inbounds i16, i16* %312, i64 %287
  %321 = bitcast i16* %320 to <8 x i16>*
  %322 = load <8 x i16>, <8 x i16>* %321, align 16
  %323 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %322, <8 x i16> %272) #5
  %324 = icmp sgt <8 x i16> %323, zeroinitializer
  %325 = select <8 x i1> %324, <8 x i16> %323, <8 x i16> zeroinitializer
  %326 = icmp slt <8 x i16> %325, %281
  %327 = select <8 x i1> %326, <8 x i16> %325, <8 x i16> %281
  store <8 x i16> %327, <8 x i16>* %321, align 16
  %328 = getelementptr inbounds i16, i16* %320, i64 %287
  %329 = bitcast i16* %328 to <8 x i16>*
  %330 = load <8 x i16>, <8 x i16>* %329, align 16
  %331 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %330, <8 x i16> %271) #5
  %332 = icmp sgt <8 x i16> %331, zeroinitializer
  %333 = select <8 x i1> %332, <8 x i16> %331, <8 x i16> zeroinitializer
  %334 = icmp slt <8 x i16> %333, %281
  %335 = select <8 x i1> %334, <8 x i16> %333, <8 x i16> %281
  store <8 x i16> %335, <8 x i16>* %329, align 16
  %336 = getelementptr inbounds i16, i16* %328, i64 %287
  %337 = bitcast i16* %336 to <8 x i16>*
  %338 = load <8 x i16>, <8 x i16>* %337, align 16
  %339 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %338, <8 x i16> %270) #5
  %340 = icmp sgt <8 x i16> %339, zeroinitializer
  %341 = select <8 x i1> %340, <8 x i16> %339, <8 x i16> zeroinitializer
  %342 = icmp slt <8 x i16> %341, %281
  %343 = select <8 x i1> %342, <8 x i16> %341, <8 x i16> %281
  store <8 x i16> %343, <8 x i16>* %337, align 16
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %7) #5
  ret void
}

declare void @vpx_idct8_sse2(<2 x i64>*) local_unnamed_addr #2

; Function Attrs: nounwind ssp uwtable
define hidden void @vpx_highbd_idct8x8_12_add_sse4_1(i32* nocapture readonly, i16* nocapture, i32, i32) local_unnamed_addr #0 {
  %5 = alloca [16 x <2 x i64>], align 16
  %6 = bitcast [16 x <2 x i64>]* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %6) #5
  %7 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 4
  %8 = bitcast <2 x i64>* %7 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %8, i8 -86, i64 192, i1 false)
  %9 = bitcast i32* %0 to <2 x i64>*
  %10 = load <2 x i64>, <2 x i64>* %9, align 16
  %11 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 0
  store <2 x i64> %10, <2 x i64>* %11, align 16
  %12 = getelementptr inbounds i32, i32* %0, i64 8
  %13 = bitcast i32* %12 to <2 x i64>*
  %14 = load <2 x i64>, <2 x i64>* %13, align 16
  %15 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 1
  store <2 x i64> %14, <2 x i64>* %15, align 16
  %16 = getelementptr inbounds i32, i32* %0, i64 16
  %17 = bitcast i32* %16 to <2 x i64>*
  %18 = load <2 x i64>, <2 x i64>* %17, align 16
  %19 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 2
  store <2 x i64> %18, <2 x i64>* %19, align 16
  %20 = getelementptr inbounds i32, i32* %0, i64 24
  %21 = bitcast i32* %20 to <2 x i64>*
  %22 = load <2 x i64>, <2 x i64>* %21, align 16
  %23 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 3
  store <2 x i64> %22, <2 x i64>* %23, align 16
  %24 = icmp eq i32 %3, 8
  br i1 %24, label %25, label %170

25:                                               ; preds = %4
  %26 = bitcast <2 x i64> %10 to <4 x i32>
  %27 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %26, <4 x i32> undef) #5
  %28 = bitcast <2 x i64> %14 to <4 x i32>
  %29 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %28, <4 x i32> undef) #5
  %30 = bitcast <2 x i64> %18 to <4 x i32>
  %31 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %30, <4 x i32> undef) #5
  %32 = bitcast <2 x i64> %22 to <4 x i32>
  %33 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %32, <4 x i32> undef) #5
  %34 = shufflevector <8 x i16> %27, <8 x i16> %29, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %35 = shufflevector <8 x i16> %31, <8 x i16> %33, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %36 = bitcast <8 x i16> %34 to <4 x i32>
  %37 = bitcast <8 x i16> %35 to <4 x i32>
  %38 = shufflevector <4 x i32> %36, <4 x i32> %37, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %39 = shufflevector <4 x i32> %36, <4 x i32> %37, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %40 = bitcast <4 x i32> %38 to <2 x i64>
  %41 = shufflevector <2 x i64> %40, <2 x i64> undef, <2 x i32> zeroinitializer
  %42 = shufflevector <2 x i64> %40, <2 x i64> undef, <2 x i32> <i32 1, i32 1>
  %43 = bitcast <4 x i32> %39 to <2 x i64>
  %44 = shufflevector <2 x i64> %43, <2 x i64> undef, <2 x i32> zeroinitializer
  %45 = shufflevector <2 x i64> %43, <2 x i64> undef, <2 x i32> <i32 1, i32 1>
  %46 = bitcast <2 x i64> %42 to <8 x i16>
  %47 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %46, <8 x i16> <i16 6392, i16 6392, i16 6392, i16 6392, i16 32138, i16 32138, i16 32138, i16 32138>) #5
  %48 = bitcast <2 x i64> %45 to <8 x i16>
  %49 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %48, <8 x i16> <i16 -18204, i16 -18204, i16 -18204, i16 -18204, i16 27246, i16 27246, i16 27246, i16 27246>) #5
  %50 = bitcast <2 x i64> %41 to <8 x i16>
  %51 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %50, <8 x i16> <i16 23170, i16 23170, i16 23170, i16 23170, i16 23170, i16 23170, i16 23170, i16 23170>) #5
  %52 = bitcast <2 x i64> %44 to <8 x i16>
  %53 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %52, <8 x i16> <i16 30274, i16 30274, i16 30274, i16 30274, i16 12540, i16 12540, i16 12540, i16 12540>) #5
  %54 = add <8 x i16> %49, %47
  %55 = sub <8 x i16> %47, %49
  %56 = bitcast <8 x i16> %55 to <2 x i64>
  %57 = shufflevector <2 x i64> %56, <2 x i64> undef, <2 x i32> <i32 1, i32 undef>
  %58 = bitcast <2 x i64> %57 to <8 x i16>
  %59 = shufflevector <8 x i16> %58, <8 x i16> %55, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %60 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>, <8 x i16> %59) #5
  %61 = add <4 x i32> %60, <i32 8192, i32 8192, i32 8192, i32 8192>
  %62 = ashr <4 x i32> %61, <i32 14, i32 14, i32 14, i32 14>
  %63 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>, <8 x i16> %59) #5
  %64 = add <4 x i32> %63, <i32 8192, i32 8192, i32 8192, i32 8192>
  %65 = ashr <4 x i32> %64, <i32 14, i32 14, i32 14, i32 14>
  %66 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %62, <4 x i32> %65) #5
  %67 = add <8 x i16> %53, %51
  %68 = bitcast <8 x i16> %67 to <2 x i64>
  %69 = sub <8 x i16> %51, %53
  %70 = bitcast <8 x i16> %69 to <2 x i64>
  %71 = shufflevector <2 x i64> %70, <2 x i64> %68, <2 x i32> <i32 1, i32 3>
  %72 = shufflevector <2 x i64> %70, <2 x i64> %68, <2 x i32> <i32 0, i32 2>
  %73 = bitcast <2 x i64> %72 to <8 x i16>
  %74 = add <8 x i16> %54, %73
  %75 = bitcast <2 x i64> %71 to <8 x i16>
  %76 = add <8 x i16> %66, %75
  %77 = sub <8 x i16> %73, %54
  %78 = sub <8 x i16> %75, %66
  %79 = shufflevector <8 x i16> %74, <8 x i16> %76, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %80 = shufflevector <8 x i16> %76, <8 x i16> %74, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %81 = shufflevector <8 x i16> %77, <8 x i16> %78, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %82 = shufflevector <8 x i16> %78, <8 x i16> %77, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %83 = bitcast <8 x i16> %79 to <4 x i32>
  %84 = bitcast <8 x i16> %80 to <4 x i32>
  %85 = shufflevector <4 x i32> %83, <4 x i32> %84, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %86 = bitcast <4 x i32> %85 to <2 x i64>
  %87 = bitcast <8 x i16> %81 to <4 x i32>
  %88 = bitcast <8 x i16> %82 to <4 x i32>
  %89 = shufflevector <4 x i32> %87, <4 x i32> %88, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %90 = bitcast <4 x i32> %89 to <2 x i64>
  %91 = shufflevector <4 x i32> %83, <4 x i32> %84, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %92 = bitcast <4 x i32> %91 to <2 x i64>
  %93 = shufflevector <4 x i32> %87, <4 x i32> %88, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %94 = bitcast <4 x i32> %93 to <2 x i64>
  %95 = shufflevector <2 x i64> %86, <2 x i64> %90, <2 x i32> <i32 0, i32 2>
  %96 = shufflevector <2 x i64> %86, <2 x i64> %90, <2 x i32> <i32 1, i32 3>
  %97 = shufflevector <2 x i64> %92, <2 x i64> %94, <2 x i32> <i32 0, i32 2>
  %98 = shufflevector <2 x i64> %92, <2 x i64> %94, <2 x i32> <i32 1, i32 3>
  %99 = bitcast <2 x i64> %96 to <8 x i16>
  %100 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %99, <8 x i16> <i16 6392, i16 6392, i16 6392, i16 6392, i16 6392, i16 6392, i16 6392, i16 6392>) #5
  %101 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %99, <8 x i16> <i16 32138, i16 32138, i16 32138, i16 32138, i16 32138, i16 32138, i16 32138, i16 32138>) #5
  %102 = bitcast <2 x i64> %98 to <8 x i16>
  %103 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %102, <8 x i16> <i16 -18204, i16 -18204, i16 -18204, i16 -18204, i16 -18204, i16 -18204, i16 -18204, i16 -18204>) #5
  %104 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %102, <8 x i16> <i16 27246, i16 27246, i16 27246, i16 27246, i16 27246, i16 27246, i16 27246, i16 27246>) #5
  %105 = bitcast <2 x i64> %95 to <8 x i16>
  %106 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %105, <8 x i16> <i16 23170, i16 23170, i16 23170, i16 23170, i16 23170, i16 23170, i16 23170, i16 23170>) #5
  %107 = bitcast <2 x i64> %97 to <8 x i16>
  %108 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %107, <8 x i16> <i16 12540, i16 12540, i16 12540, i16 12540, i16 12540, i16 12540, i16 12540, i16 12540>) #5
  %109 = tail call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %107, <8 x i16> <i16 30274, i16 30274, i16 30274, i16 30274, i16 30274, i16 30274, i16 30274, i16 30274>) #5
  %110 = add <8 x i16> %103, %100
  %111 = sub <8 x i16> %100, %103
  %112 = sub <8 x i16> %101, %104
  %113 = add <8 x i16> %104, %101
  %114 = add <8 x i16> %109, %106
  %115 = add <8 x i16> %108, %106
  %116 = sub <8 x i16> %106, %108
  %117 = sub <8 x i16> %106, %109
  %118 = shufflevector <8 x i16> %112, <8 x i16> %111, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %119 = shufflevector <8 x i16> %112, <8 x i16> %111, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %120 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %118, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #5
  %121 = add <4 x i32> %120, <i32 8192, i32 8192, i32 8192, i32 8192>
  %122 = ashr <4 x i32> %121, <i32 14, i32 14, i32 14, i32 14>
  %123 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %119, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #5
  %124 = add <4 x i32> %123, <i32 8192, i32 8192, i32 8192, i32 8192>
  %125 = ashr <4 x i32> %124, <i32 14, i32 14, i32 14, i32 14>
  %126 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %122, <4 x i32> %125) #5
  %127 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %118, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #5
  %128 = add <4 x i32> %127, <i32 8192, i32 8192, i32 8192, i32 8192>
  %129 = ashr <4 x i32> %128, <i32 14, i32 14, i32 14, i32 14>
  %130 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %119, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #5
  %131 = add <4 x i32> %130, <i32 8192, i32 8192, i32 8192, i32 8192>
  %132 = ashr <4 x i32> %131, <i32 14, i32 14, i32 14, i32 14>
  %133 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %129, <4 x i32> %132) #5
  %134 = sub <8 x i16> %117, %110
  %135 = sub <8 x i16> %116, %126
  %136 = sub <8 x i16> %115, %133
  %137 = add <8 x i16> %113, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %138 = add <8 x i16> %137, %114
  %139 = bitcast [16 x <2 x i64>]* %5 to <8 x i16>*
  %140 = add <8 x i16> %115, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %141 = add <8 x i16> %140, %133
  %142 = bitcast <2 x i64>* %15 to <8 x i16>*
  %143 = add <8 x i16> %116, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %144 = add <8 x i16> %143, %126
  %145 = bitcast <2 x i64>* %19 to <8 x i16>*
  %146 = add <8 x i16> %110, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %147 = add <8 x i16> %146, %117
  %148 = bitcast <2 x i64>* %23 to <8 x i16>*
  %149 = add <8 x i16> %134, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %150 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 4
  %151 = bitcast <2 x i64>* %150 to <8 x i16>*
  %152 = add <8 x i16> %135, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %153 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 5
  %154 = bitcast <2 x i64>* %153 to <8 x i16>*
  %155 = add <8 x i16> %136, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %156 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 6
  %157 = bitcast <2 x i64>* %156 to <8 x i16>*
  %158 = sub <8 x i16> <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>, %113
  %159 = add <8 x i16> %158, %114
  %160 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 7
  %161 = bitcast <2 x i64>* %160 to <8 x i16>*
  %162 = ashr <8 x i16> %138, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %162, <8 x i16>* %139, align 16
  %163 = ashr <8 x i16> %141, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %163, <8 x i16>* %142, align 16
  %164 = ashr <8 x i16> %144, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %164, <8 x i16>* %145, align 16
  %165 = ashr <8 x i16> %147, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %165, <8 x i16>* %148, align 16
  %166 = ashr <8 x i16> %149, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %166, <8 x i16>* %151, align 16
  %167 = ashr <8 x i16> %152, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %167, <8 x i16>* %154, align 16
  %168 = ashr <8 x i16> %155, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %168, <8 x i16>* %157, align 16
  %169 = ashr <8 x i16> %159, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %169, <8 x i16>* %161, align 16
  br label %267

170:                                              ; preds = %4
  call fastcc void @highbd_idct8x8_12_half1d(<2 x i64>* nonnull %11)
  %171 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 4
  %172 = load <2 x i64>, <2 x i64>* %171, align 16
  %173 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 5
  %174 = load <2 x i64>, <2 x i64>* %173, align 16
  %175 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 6
  %176 = load <2 x i64>, <2 x i64>* %175, align 16
  %177 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 7
  %178 = load <2 x i64>, <2 x i64>* %177, align 16
  call fastcc void @highbd_idct8x8_12_half1d(<2 x i64>* nonnull %11)
  %179 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 8
  store <2 x i64> %172, <2 x i64>* %179, align 16
  %180 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 9
  store <2 x i64> %174, <2 x i64>* %180, align 16
  %181 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 10
  store <2 x i64> %176, <2 x i64>* %181, align 16
  %182 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 11
  store <2 x i64> %178, <2 x i64>* %182, align 16
  call fastcc void @highbd_idct8x8_12_half1d(<2 x i64>* %179)
  %183 = bitcast [16 x <2 x i64>]* %5 to <4 x i32>*
  %184 = load <4 x i32>, <4 x i32>* %183, align 16
  %185 = bitcast <2 x i64>* %179 to <4 x i32>*
  %186 = load <4 x i32>, <4 x i32>* %185, align 16
  %187 = add <4 x i32> %184, <i32 16, i32 16, i32 16, i32 16>
  %188 = add <4 x i32> %186, <i32 16, i32 16, i32 16, i32 16>
  %189 = ashr <4 x i32> %187, <i32 5, i32 5, i32 5, i32 5>
  %190 = ashr <4 x i32> %188, <i32 5, i32 5, i32 5, i32 5>
  %191 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %189, <4 x i32> %190) #5
  %192 = bitcast [16 x <2 x i64>]* %5 to <8 x i16>*
  store <8 x i16> %191, <8 x i16>* %192, align 16
  %193 = bitcast <2 x i64>* %15 to <4 x i32>*
  %194 = load <4 x i32>, <4 x i32>* %193, align 16
  %195 = bitcast <2 x i64>* %180 to <4 x i32>*
  %196 = load <4 x i32>, <4 x i32>* %195, align 16
  %197 = add <4 x i32> %194, <i32 16, i32 16, i32 16, i32 16>
  %198 = add <4 x i32> %196, <i32 16, i32 16, i32 16, i32 16>
  %199 = ashr <4 x i32> %197, <i32 5, i32 5, i32 5, i32 5>
  %200 = ashr <4 x i32> %198, <i32 5, i32 5, i32 5, i32 5>
  %201 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %199, <4 x i32> %200) #5
  %202 = bitcast <2 x i64>* %15 to <8 x i16>*
  store <8 x i16> %201, <8 x i16>* %202, align 16
  %203 = bitcast <2 x i64>* %19 to <4 x i32>*
  %204 = load <4 x i32>, <4 x i32>* %203, align 16
  %205 = bitcast <2 x i64>* %181 to <4 x i32>*
  %206 = load <4 x i32>, <4 x i32>* %205, align 16
  %207 = add <4 x i32> %204, <i32 16, i32 16, i32 16, i32 16>
  %208 = add <4 x i32> %206, <i32 16, i32 16, i32 16, i32 16>
  %209 = ashr <4 x i32> %207, <i32 5, i32 5, i32 5, i32 5>
  %210 = ashr <4 x i32> %208, <i32 5, i32 5, i32 5, i32 5>
  %211 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %209, <4 x i32> %210) #5
  %212 = bitcast <2 x i64>* %19 to <8 x i16>*
  store <8 x i16> %211, <8 x i16>* %212, align 16
  %213 = bitcast <2 x i64>* %23 to <4 x i32>*
  %214 = load <4 x i32>, <4 x i32>* %213, align 16
  %215 = bitcast <2 x i64>* %182 to <4 x i32>*
  %216 = load <4 x i32>, <4 x i32>* %215, align 16
  %217 = add <4 x i32> %214, <i32 16, i32 16, i32 16, i32 16>
  %218 = add <4 x i32> %216, <i32 16, i32 16, i32 16, i32 16>
  %219 = ashr <4 x i32> %217, <i32 5, i32 5, i32 5, i32 5>
  %220 = ashr <4 x i32> %218, <i32 5, i32 5, i32 5, i32 5>
  %221 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %219, <4 x i32> %220) #5
  %222 = bitcast <2 x i64>* %23 to <8 x i16>*
  store <8 x i16> %221, <8 x i16>* %222, align 16
  %223 = bitcast <2 x i64>* %171 to <4 x i32>*
  %224 = load <4 x i32>, <4 x i32>* %223, align 16
  %225 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 12
  %226 = bitcast <2 x i64>* %225 to <4 x i32>*
  %227 = load <4 x i32>, <4 x i32>* %226, align 16
  %228 = add <4 x i32> %224, <i32 16, i32 16, i32 16, i32 16>
  %229 = add <4 x i32> %227, <i32 16, i32 16, i32 16, i32 16>
  %230 = ashr <4 x i32> %228, <i32 5, i32 5, i32 5, i32 5>
  %231 = ashr <4 x i32> %229, <i32 5, i32 5, i32 5, i32 5>
  %232 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %230, <4 x i32> %231) #5
  %233 = bitcast <2 x i64>* %171 to <8 x i16>*
  store <8 x i16> %232, <8 x i16>* %233, align 16
  %234 = bitcast <2 x i64>* %173 to <4 x i32>*
  %235 = load <4 x i32>, <4 x i32>* %234, align 16
  %236 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 13
  %237 = bitcast <2 x i64>* %236 to <4 x i32>*
  %238 = load <4 x i32>, <4 x i32>* %237, align 16
  %239 = add <4 x i32> %235, <i32 16, i32 16, i32 16, i32 16>
  %240 = add <4 x i32> %238, <i32 16, i32 16, i32 16, i32 16>
  %241 = ashr <4 x i32> %239, <i32 5, i32 5, i32 5, i32 5>
  %242 = ashr <4 x i32> %240, <i32 5, i32 5, i32 5, i32 5>
  %243 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %241, <4 x i32> %242) #5
  %244 = bitcast <2 x i64>* %173 to <8 x i16>*
  store <8 x i16> %243, <8 x i16>* %244, align 16
  %245 = bitcast <2 x i64>* %175 to <4 x i32>*
  %246 = load <4 x i32>, <4 x i32>* %245, align 16
  %247 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 14
  %248 = bitcast <2 x i64>* %247 to <4 x i32>*
  %249 = load <4 x i32>, <4 x i32>* %248, align 16
  %250 = add <4 x i32> %246, <i32 16, i32 16, i32 16, i32 16>
  %251 = add <4 x i32> %249, <i32 16, i32 16, i32 16, i32 16>
  %252 = ashr <4 x i32> %250, <i32 5, i32 5, i32 5, i32 5>
  %253 = ashr <4 x i32> %251, <i32 5, i32 5, i32 5, i32 5>
  %254 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %252, <4 x i32> %253) #5
  %255 = bitcast <2 x i64>* %175 to <8 x i16>*
  store <8 x i16> %254, <8 x i16>* %255, align 16
  %256 = bitcast <2 x i64>* %177 to <4 x i32>*
  %257 = load <4 x i32>, <4 x i32>* %256, align 16
  %258 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 15
  %259 = bitcast <2 x i64>* %258 to <4 x i32>*
  %260 = load <4 x i32>, <4 x i32>* %259, align 16
  %261 = add <4 x i32> %257, <i32 16, i32 16, i32 16, i32 16>
  %262 = add <4 x i32> %260, <i32 16, i32 16, i32 16, i32 16>
  %263 = ashr <4 x i32> %261, <i32 5, i32 5, i32 5, i32 5>
  %264 = ashr <4 x i32> %262, <i32 5, i32 5, i32 5, i32 5>
  %265 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %263, <4 x i32> %264) #5
  %266 = bitcast <2 x i64>* %177 to <8 x i16>*
  store <8 x i16> %265, <8 x i16>* %266, align 16
  br label %267

267:                                              ; preds = %170, %25
  %268 = phi <8 x i16> [ %265, %170 ], [ %169, %25 ]
  %269 = phi <8 x i16> [ %254, %170 ], [ %168, %25 ]
  %270 = phi <8 x i16> [ %243, %170 ], [ %167, %25 ]
  %271 = phi <8 x i16> [ %232, %170 ], [ %166, %25 ]
  %272 = phi <8 x i16> [ %221, %170 ], [ %165, %25 ]
  %273 = phi <8 x i16> [ %211, %170 ], [ %164, %25 ]
  %274 = phi <8 x i16> [ %201, %170 ], [ %163, %25 ]
  %275 = phi <8 x i16> [ %191, %170 ], [ %162, %25 ]
  %276 = bitcast i16* %1 to <8 x i16>*
  %277 = load <8 x i16>, <8 x i16>* %276, align 16
  %278 = tail call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>, i32 %3) #5
  %279 = add <8 x i16> %278, <i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1>
  %280 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %277, <8 x i16> %275) #5
  %281 = icmp sgt <8 x i16> %280, zeroinitializer
  %282 = select <8 x i1> %281, <8 x i16> %280, <8 x i16> zeroinitializer
  %283 = icmp slt <8 x i16> %282, %279
  %284 = select <8 x i1> %283, <8 x i16> %282, <8 x i16> %279
  store <8 x i16> %284, <8 x i16>* %276, align 16
  %285 = sext i32 %2 to i64
  %286 = getelementptr inbounds i16, i16* %1, i64 %285
  %287 = bitcast i16* %286 to <8 x i16>*
  %288 = load <8 x i16>, <8 x i16>* %287, align 16
  %289 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %288, <8 x i16> %274) #5
  %290 = icmp sgt <8 x i16> %289, zeroinitializer
  %291 = select <8 x i1> %290, <8 x i16> %289, <8 x i16> zeroinitializer
  %292 = icmp slt <8 x i16> %291, %279
  %293 = select <8 x i1> %292, <8 x i16> %291, <8 x i16> %279
  store <8 x i16> %293, <8 x i16>* %287, align 16
  %294 = getelementptr inbounds i16, i16* %286, i64 %285
  %295 = bitcast i16* %294 to <8 x i16>*
  %296 = load <8 x i16>, <8 x i16>* %295, align 16
  %297 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %296, <8 x i16> %273) #5
  %298 = icmp sgt <8 x i16> %297, zeroinitializer
  %299 = select <8 x i1> %298, <8 x i16> %297, <8 x i16> zeroinitializer
  %300 = icmp slt <8 x i16> %299, %279
  %301 = select <8 x i1> %300, <8 x i16> %299, <8 x i16> %279
  store <8 x i16> %301, <8 x i16>* %295, align 16
  %302 = getelementptr inbounds i16, i16* %294, i64 %285
  %303 = bitcast i16* %302 to <8 x i16>*
  %304 = load <8 x i16>, <8 x i16>* %303, align 16
  %305 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %304, <8 x i16> %272) #5
  %306 = icmp sgt <8 x i16> %305, zeroinitializer
  %307 = select <8 x i1> %306, <8 x i16> %305, <8 x i16> zeroinitializer
  %308 = icmp slt <8 x i16> %307, %279
  %309 = select <8 x i1> %308, <8 x i16> %307, <8 x i16> %279
  store <8 x i16> %309, <8 x i16>* %303, align 16
  %310 = getelementptr inbounds i16, i16* %302, i64 %285
  %311 = bitcast i16* %310 to <8 x i16>*
  %312 = load <8 x i16>, <8 x i16>* %311, align 16
  %313 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %312, <8 x i16> %271) #5
  %314 = icmp sgt <8 x i16> %313, zeroinitializer
  %315 = select <8 x i1> %314, <8 x i16> %313, <8 x i16> zeroinitializer
  %316 = icmp slt <8 x i16> %315, %279
  %317 = select <8 x i1> %316, <8 x i16> %315, <8 x i16> %279
  store <8 x i16> %317, <8 x i16>* %311, align 16
  %318 = getelementptr inbounds i16, i16* %310, i64 %285
  %319 = bitcast i16* %318 to <8 x i16>*
  %320 = load <8 x i16>, <8 x i16>* %319, align 16
  %321 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %320, <8 x i16> %270) #5
  %322 = icmp sgt <8 x i16> %321, zeroinitializer
  %323 = select <8 x i1> %322, <8 x i16> %321, <8 x i16> zeroinitializer
  %324 = icmp slt <8 x i16> %323, %279
  %325 = select <8 x i1> %324, <8 x i16> %323, <8 x i16> %279
  store <8 x i16> %325, <8 x i16>* %319, align 16
  %326 = getelementptr inbounds i16, i16* %318, i64 %285
  %327 = bitcast i16* %326 to <8 x i16>*
  %328 = load <8 x i16>, <8 x i16>* %327, align 16
  %329 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %328, <8 x i16> %269) #5
  %330 = icmp sgt <8 x i16> %329, zeroinitializer
  %331 = select <8 x i1> %330, <8 x i16> %329, <8 x i16> zeroinitializer
  %332 = icmp slt <8 x i16> %331, %279
  %333 = select <8 x i1> %332, <8 x i16> %331, <8 x i16> %279
  store <8 x i16> %333, <8 x i16>* %327, align 16
  %334 = getelementptr inbounds i16, i16* %326, i64 %285
  %335 = bitcast i16* %334 to <8 x i16>*
  %336 = load <8 x i16>, <8 x i16>* %335, align 16
  %337 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %336, <8 x i16> %268) #5
  %338 = icmp sgt <8 x i16> %337, zeroinitializer
  %339 = select <8 x i1> %338, <8 x i16> %337, <8 x i16> zeroinitializer
  %340 = icmp slt <8 x i16> %339, %279
  %341 = select <8 x i1> %340, <8 x i16> %339, <8 x i16> %279
  store <8 x i16> %341, <8 x i16>* %335, align 16
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %6) #5
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal fastcc void @highbd_idct8x8_12_half1d(<2 x i64>* nocapture) unnamed_addr #0 {
  %2 = bitcast <2 x i64>* %0 to <4 x i32>*
  %3 = load <4 x i32>, <4 x i32>* %2, align 16
  %4 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %5 = bitcast <2 x i64>* %4 to <4 x i32>*
  %6 = load <4 x i32>, <4 x i32>* %5, align 16
  %7 = shufflevector <4 x i32> %3, <4 x i32> %6, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %8 = bitcast <4 x i32> %7 to <2 x i64>
  %9 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %10 = bitcast <2 x i64>* %9 to <4 x i32>*
  %11 = load <4 x i32>, <4 x i32>* %10, align 16
  %12 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %13 = bitcast <2 x i64>* %12 to <4 x i32>*
  %14 = load <4 x i32>, <4 x i32>* %13, align 16
  %15 = shufflevector <4 x i32> %11, <4 x i32> %14, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %16 = bitcast <4 x i32> %15 to <2 x i64>
  %17 = shufflevector <4 x i32> %3, <4 x i32> %6, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %18 = bitcast <4 x i32> %17 to <2 x i64>
  %19 = shufflevector <4 x i32> %11, <4 x i32> %14, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %20 = bitcast <4 x i32> %19 to <2 x i64>
  %21 = shufflevector <2 x i64> %8, <2 x i64> %16, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %21, <2 x i64>* %0, align 16
  %22 = shufflevector <2 x i64> %8, <2 x i64> %16, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %22, <2 x i64>* %4, align 16
  %23 = shufflevector <2 x i64> %18, <2 x i64> %20, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %23, <2 x i64>* %9, align 16
  %24 = shufflevector <2 x i64> %18, <2 x i64> %20, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %24, <2 x i64>* %12, align 16
  %25 = bitcast <2 x i64> %22 to <4 x i32>
  %26 = shufflevector <4 x i32> %25, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %27 = bitcast <4 x i32> %26 to <2 x i64>
  %28 = shufflevector <4 x i32> %25, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %29 = bitcast <4 x i32> %28 to <2 x i64>
  %30 = shl <2 x i64> %27, <i64 32, i64 32>
  %31 = ashr exact <2 x i64> %30, <i64 32, i64 32>
  %32 = mul nsw <2 x i64> %31, <i64 12784, i64 12784>
  %33 = shl <2 x i64> %29, <i64 32, i64 32>
  %34 = ashr exact <2 x i64> %33, <i64 32, i64 32>
  %35 = mul nsw <2 x i64> %34, <i64 12784, i64 12784>
  %36 = add nsw <2 x i64> %32, <i64 32768, i64 32768>
  %37 = bitcast <2 x i64> %36 to <16 x i8>
  %38 = shufflevector <16 x i8> %37, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %39 = add nsw <2 x i64> %35, <i64 32768, i64 32768>
  %40 = bitcast <2 x i64> %39 to <16 x i8>
  %41 = shufflevector <16 x i8> %40, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %42 = bitcast <16 x i8> %38 to <4 x i32>
  %43 = bitcast <16 x i8> %41 to <4 x i32>
  %44 = shufflevector <4 x i32> %42, <4 x i32> %43, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %45 = shufflevector <4 x i32> %42, <4 x i32> %43, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %46 = shufflevector <4 x i32> %44, <4 x i32> %45, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %47 = mul nsw <2 x i64> %31, <i64 64276, i64 64276>
  %48 = mul nsw <2 x i64> %34, <i64 64276, i64 64276>
  %49 = add nsw <2 x i64> %47, <i64 32768, i64 32768>
  %50 = bitcast <2 x i64> %49 to <16 x i8>
  %51 = shufflevector <16 x i8> %50, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %52 = add nsw <2 x i64> %48, <i64 32768, i64 32768>
  %53 = bitcast <2 x i64> %52 to <16 x i8>
  %54 = shufflevector <16 x i8> %53, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %55 = bitcast <16 x i8> %51 to <4 x i32>
  %56 = bitcast <16 x i8> %54 to <4 x i32>
  %57 = shufflevector <4 x i32> %55, <4 x i32> %56, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %58 = shufflevector <4 x i32> %55, <4 x i32> %56, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %59 = shufflevector <4 x i32> %57, <4 x i32> %58, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %60 = bitcast <2 x i64> %24 to <4 x i32>
  %61 = shufflevector <4 x i32> %60, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %62 = bitcast <4 x i32> %61 to <2 x i64>
  %63 = shufflevector <4 x i32> %60, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %64 = bitcast <4 x i32> %63 to <2 x i64>
  %65 = shl <2 x i64> %62, <i64 32, i64 32>
  %66 = ashr exact <2 x i64> %65, <i64 32, i64 32>
  %67 = mul nsw <2 x i64> %66, <i64 -36408, i64 -36408>
  %68 = shl <2 x i64> %64, <i64 32, i64 32>
  %69 = ashr exact <2 x i64> %68, <i64 32, i64 32>
  %70 = mul nsw <2 x i64> %69, <i64 -36408, i64 -36408>
  %71 = add nsw <2 x i64> %67, <i64 32768, i64 32768>
  %72 = bitcast <2 x i64> %71 to <16 x i8>
  %73 = shufflevector <16 x i8> %72, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %74 = add nsw <2 x i64> %70, <i64 32768, i64 32768>
  %75 = bitcast <2 x i64> %74 to <16 x i8>
  %76 = shufflevector <16 x i8> %75, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %77 = bitcast <16 x i8> %73 to <4 x i32>
  %78 = bitcast <16 x i8> %76 to <4 x i32>
  %79 = shufflevector <4 x i32> %77, <4 x i32> %78, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %80 = shufflevector <4 x i32> %77, <4 x i32> %78, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %81 = shufflevector <4 x i32> %79, <4 x i32> %80, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %82 = mul nsw <2 x i64> %66, <i64 54492, i64 54492>
  %83 = mul nsw <2 x i64> %69, <i64 54492, i64 54492>
  %84 = add nsw <2 x i64> %82, <i64 32768, i64 32768>
  %85 = bitcast <2 x i64> %84 to <16 x i8>
  %86 = shufflevector <16 x i8> %85, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %87 = add nsw <2 x i64> %83, <i64 32768, i64 32768>
  %88 = bitcast <2 x i64> %87 to <16 x i8>
  %89 = shufflevector <16 x i8> %88, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %90 = bitcast <16 x i8> %86 to <4 x i32>
  %91 = bitcast <16 x i8> %89 to <4 x i32>
  %92 = shufflevector <4 x i32> %90, <4 x i32> %91, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %93 = shufflevector <4 x i32> %90, <4 x i32> %91, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %94 = shufflevector <4 x i32> %92, <4 x i32> %93, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %95 = bitcast <2 x i64> %21 to <4 x i32>
  %96 = shufflevector <4 x i32> %95, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %97 = bitcast <4 x i32> %96 to <2 x i64>
  %98 = shufflevector <4 x i32> %95, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %99 = bitcast <4 x i32> %98 to <2 x i64>
  %100 = shl <2 x i64> %97, <i64 32, i64 32>
  %101 = ashr exact <2 x i64> %100, <i64 32, i64 32>
  %102 = mul nsw <2 x i64> %101, <i64 46340, i64 46340>
  %103 = shl <2 x i64> %99, <i64 32, i64 32>
  %104 = ashr exact <2 x i64> %103, <i64 32, i64 32>
  %105 = mul nsw <2 x i64> %104, <i64 46340, i64 46340>
  %106 = add nsw <2 x i64> %102, <i64 32768, i64 32768>
  %107 = bitcast <2 x i64> %106 to <16 x i8>
  %108 = shufflevector <16 x i8> %107, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %109 = add nsw <2 x i64> %105, <i64 32768, i64 32768>
  %110 = bitcast <2 x i64> %109 to <16 x i8>
  %111 = shufflevector <16 x i8> %110, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %112 = bitcast <16 x i8> %108 to <4 x i32>
  %113 = bitcast <16 x i8> %111 to <4 x i32>
  %114 = shufflevector <4 x i32> %112, <4 x i32> %113, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %115 = shufflevector <4 x i32> %112, <4 x i32> %113, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %116 = shufflevector <4 x i32> %114, <4 x i32> %115, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %117 = bitcast <2 x i64> %23 to <4 x i32>
  %118 = shufflevector <4 x i32> %117, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %119 = bitcast <4 x i32> %118 to <2 x i64>
  %120 = shufflevector <4 x i32> %117, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %121 = bitcast <4 x i32> %120 to <2 x i64>
  %122 = shl <2 x i64> %119, <i64 32, i64 32>
  %123 = ashr exact <2 x i64> %122, <i64 32, i64 32>
  %124 = mul nsw <2 x i64> %123, <i64 25080, i64 25080>
  %125 = shl <2 x i64> %121, <i64 32, i64 32>
  %126 = ashr exact <2 x i64> %125, <i64 32, i64 32>
  %127 = mul nsw <2 x i64> %126, <i64 25080, i64 25080>
  %128 = add nsw <2 x i64> %124, <i64 32768, i64 32768>
  %129 = bitcast <2 x i64> %128 to <16 x i8>
  %130 = shufflevector <16 x i8> %129, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %131 = add nsw <2 x i64> %127, <i64 32768, i64 32768>
  %132 = bitcast <2 x i64> %131 to <16 x i8>
  %133 = shufflevector <16 x i8> %132, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %134 = bitcast <16 x i8> %130 to <4 x i32>
  %135 = bitcast <16 x i8> %133 to <4 x i32>
  %136 = shufflevector <4 x i32> %134, <4 x i32> %135, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %137 = shufflevector <4 x i32> %134, <4 x i32> %135, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %138 = shufflevector <4 x i32> %136, <4 x i32> %137, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %139 = mul nsw <2 x i64> %123, <i64 60548, i64 60548>
  %140 = mul nsw <2 x i64> %126, <i64 60548, i64 60548>
  %141 = add nsw <2 x i64> %139, <i64 32768, i64 32768>
  %142 = bitcast <2 x i64> %141 to <16 x i8>
  %143 = shufflevector <16 x i8> %142, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %144 = add nsw <2 x i64> %140, <i64 32768, i64 32768>
  %145 = bitcast <2 x i64> %144 to <16 x i8>
  %146 = shufflevector <16 x i8> %145, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %147 = bitcast <16 x i8> %143 to <4 x i32>
  %148 = bitcast <16 x i8> %146 to <4 x i32>
  %149 = shufflevector <4 x i32> %147, <4 x i32> %148, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %150 = shufflevector <4 x i32> %147, <4 x i32> %148, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %151 = shufflevector <4 x i32> %149, <4 x i32> %150, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %152 = add <4 x i32> %81, %46
  %153 = sub <4 x i32> %46, %81
  %154 = sub <4 x i32> %59, %94
  %155 = add <4 x i32> %94, %59
  %156 = add <4 x i32> %151, %116
  %157 = add <4 x i32> %138, %116
  %158 = sub <4 x i32> %116, %138
  %159 = sub <4 x i32> %116, %151
  %160 = add <4 x i32> %153, %154
  %161 = shufflevector <4 x i32> %160, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %162 = bitcast <4 x i32> %161 to <2 x i64>
  %163 = shufflevector <4 x i32> %160, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %164 = bitcast <4 x i32> %163 to <2 x i64>
  %165 = shl <2 x i64> %162, <i64 32, i64 32>
  %166 = ashr exact <2 x i64> %165, <i64 32, i64 32>
  %167 = mul nsw <2 x i64> %166, <i64 46340, i64 46340>
  %168 = shl <2 x i64> %164, <i64 32, i64 32>
  %169 = ashr exact <2 x i64> %168, <i64 32, i64 32>
  %170 = mul nsw <2 x i64> %169, <i64 46340, i64 46340>
  %171 = add nsw <2 x i64> %167, <i64 32768, i64 32768>
  %172 = bitcast <2 x i64> %171 to <16 x i8>
  %173 = shufflevector <16 x i8> %172, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %174 = add nsw <2 x i64> %170, <i64 32768, i64 32768>
  %175 = bitcast <2 x i64> %174 to <16 x i8>
  %176 = shufflevector <16 x i8> %175, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %177 = bitcast <16 x i8> %173 to <4 x i32>
  %178 = bitcast <16 x i8> %176 to <4 x i32>
  %179 = shufflevector <4 x i32> %177, <4 x i32> %178, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %180 = shufflevector <4 x i32> %177, <4 x i32> %178, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %181 = shufflevector <4 x i32> %179, <4 x i32> %180, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %182 = sub <4 x i32> %154, %153
  %183 = shufflevector <4 x i32> %182, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %184 = bitcast <4 x i32> %183 to <2 x i64>
  %185 = shufflevector <4 x i32> %182, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %186 = bitcast <4 x i32> %185 to <2 x i64>
  %187 = shl <2 x i64> %184, <i64 32, i64 32>
  %188 = ashr exact <2 x i64> %187, <i64 32, i64 32>
  %189 = mul nsw <2 x i64> %188, <i64 46340, i64 46340>
  %190 = shl <2 x i64> %186, <i64 32, i64 32>
  %191 = ashr exact <2 x i64> %190, <i64 32, i64 32>
  %192 = mul nsw <2 x i64> %191, <i64 46340, i64 46340>
  %193 = add nsw <2 x i64> %189, <i64 32768, i64 32768>
  %194 = bitcast <2 x i64> %193 to <16 x i8>
  %195 = shufflevector <16 x i8> %194, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %196 = add nsw <2 x i64> %192, <i64 32768, i64 32768>
  %197 = bitcast <2 x i64> %196 to <16 x i8>
  %198 = shufflevector <16 x i8> %197, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %199 = bitcast <16 x i8> %195 to <4 x i32>
  %200 = bitcast <16 x i8> %198 to <4 x i32>
  %201 = shufflevector <4 x i32> %199, <4 x i32> %200, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %202 = shufflevector <4 x i32> %199, <4 x i32> %200, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %203 = shufflevector <4 x i32> %201, <4 x i32> %202, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %204 = add <4 x i32> %155, %156
  store <4 x i32> %204, <4 x i32>* %2, align 16
  %205 = add <4 x i32> %181, %157
  store <4 x i32> %205, <4 x i32>* %5, align 16
  %206 = add <4 x i32> %203, %158
  store <4 x i32> %206, <4 x i32>* %10, align 16
  %207 = add <4 x i32> %152, %159
  store <4 x i32> %207, <4 x i32>* %13, align 16
  %208 = sub <4 x i32> %159, %152
  %209 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 4
  %210 = bitcast <2 x i64>* %209 to <4 x i32>*
  store <4 x i32> %208, <4 x i32>* %210, align 16
  %211 = sub <4 x i32> %158, %203
  %212 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 5
  %213 = bitcast <2 x i64>* %212 to <4 x i32>*
  store <4 x i32> %211, <4 x i32>* %213, align 16
  %214 = sub <4 x i32> %157, %181
  %215 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 6
  %216 = bitcast <2 x i64>* %215 to <4 x i32>*
  store <4 x i32> %214, <4 x i32>* %216, align 16
  %217 = sub <4 x i32> %156, %155
  %218 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 7
  %219 = bitcast <2 x i64>* %218 to <4 x i32>*
  store <4 x i32> %217, <4 x i32>* %219, align 16
  ret void
}

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32>, <4 x i32>) #3

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16>, i32) #3

; Function Attrs: nounwind readnone speculatable
declare <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16>, <8 x i16>) #4

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16>, <8 x i16>) #3

; Function Attrs: nounwind readnone
declare <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16>, <8 x i16>) #3

attributes #0 = { nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="128" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+sse4.1,+ssse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { argmemonly nounwind }
attributes #2 = { "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+sse4.1,+ssse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #3 = { nounwind readnone }
attributes #4 = { nounwind readnone speculatable }
attributes #5 = { nounwind }

!llvm.module.flags = !{!0, !1}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 2}
