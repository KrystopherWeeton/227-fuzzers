; ModuleID = '../../third_party/libwebp/src/dsp/yuv_sse2.c'
source_filename = "../../third_party/libwebp/src/dsp/yuv_sse2.c"
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

@WebPSamplers = external local_unnamed_addr global [0 x void (i8*, i8*, i8*, i8*, i32)*], align 8
@WebPConvertARGBToY = external local_unnamed_addr global void (i32*, i8*, i32)*, align 8
@WebPConvertARGBToUV = external local_unnamed_addr global void (i32*, i8*, i8*, i32, i32)*, align 8
@WebPConvertRGB24ToY = external local_unnamed_addr global void (i8*, i8*, i32)*, align 8
@WebPConvertBGR24ToY = external local_unnamed_addr global void (i8*, i8*, i32)*, align 8
@WebPConvertRGBA32ToUV = external local_unnamed_addr global void (i16*, i8*, i8*, i32)*, align 8
@WebPSharpYUVUpdateY = external local_unnamed_addr global i64 (i16*, i16*, i16*, i32)*, align 8
@WebPSharpYUVUpdateRGB = external local_unnamed_addr global void (i16*, i16*, i16*, i32)*, align 8
@WebPSharpYUVFilterRow = external local_unnamed_addr global void (i16*, i16*, i32, i16*, i16*)*, align 8

; Function Attrs: nounwind ssp uwtable
define hidden void @VP8YuvToRgba32_SSE2(i8* nocapture readonly, i8* nocapture readonly, i8* nocapture readonly, i8* nocapture) local_unnamed_addr #0 {
  br label %5

5:                                                ; preds = %4, %5
  %6 = phi i64 [ 0, %4 ], [ %55, %5 ]
  %7 = phi i8* [ %3, %4 ], [ %56, %5 ]
  %8 = getelementptr inbounds i8, i8* %0, i64 %6
  %9 = getelementptr inbounds i8, i8* %1, i64 %6
  %10 = getelementptr inbounds i8, i8* %2, i64 %6
  %11 = bitcast i8* %8 to i64*
  %12 = load i64, i64* %11, align 1
  %13 = insertelement <2 x i64> undef, i64 %12, i32 0
  %14 = bitcast <2 x i64> %13 to <16 x i8>
  %15 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %14, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %16 = bitcast i8* %9 to i64*
  %17 = load i64, i64* %16, align 1
  %18 = insertelement <2 x i64> undef, i64 %17, i32 0
  %19 = bitcast <2 x i64> %18 to <16 x i8>
  %20 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %19, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %21 = bitcast i8* %10 to i64*
  %22 = load i64, i64* %21, align 1
  %23 = insertelement <2 x i64> undef, i64 %22, i32 0
  %24 = bitcast <2 x i64> %23 to <16 x i8>
  %25 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %24, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %26 = bitcast <16 x i8> %15 to <8 x i16>
  %27 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %26, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #7
  %28 = bitcast <16 x i8> %25 to <8 x i16>
  %29 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %28, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #7
  %30 = add <8 x i16> %27, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %31 = add <8 x i16> %30, %29
  %32 = bitcast <16 x i8> %20 to <8 x i16>
  %33 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %32, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #7
  %34 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %28, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #7
  %35 = add <8 x i16> %27, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %36 = sub <8 x i16> %35, %33
  %37 = sub <8 x i16> %36, %34
  %38 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %32, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #7
  %39 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %38, <8 x i16> %27) #7
  %40 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %39, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #7
  %41 = ashr <8 x i16> %31, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %42 = ashr <8 x i16> %37, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %43 = lshr <8 x i16> %40, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %44 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %41, <8 x i16> %43) #7
  %45 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %42, <8 x i16> <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>) #7
  %46 = shufflevector <16 x i8> %44, <16 x i8> %45, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %47 = shufflevector <16 x i8> %44, <16 x i8> %45, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %48 = bitcast <16 x i8> %46 to <8 x i16>
  %49 = bitcast <16 x i8> %47 to <8 x i16>
  %50 = shufflevector <8 x i16> %48, <8 x i16> %49, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %51 = shufflevector <8 x i16> %48, <8 x i16> %49, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %52 = bitcast i8* %7 to <8 x i16>*
  store <8 x i16> %50, <8 x i16>* %52, align 1
  %53 = getelementptr inbounds i8, i8* %7, i64 16
  %54 = bitcast i8* %53 to <8 x i16>*
  store <8 x i16> %51, <8 x i16>* %54, align 1
  %55 = add nuw nsw i64 %6, 8
  %56 = getelementptr inbounds i8, i8* %7, i64 32
  %57 = icmp ult i64 %55, 32
  br i1 %57, label %5, label %58

58:                                               ; preds = %5
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @VP8YuvToBgra32_SSE2(i8* nocapture readonly, i8* nocapture readonly, i8* nocapture readonly, i8* nocapture) local_unnamed_addr #0 {
  br label %5

5:                                                ; preds = %4, %5
  %6 = phi i64 [ 0, %4 ], [ %55, %5 ]
  %7 = phi i8* [ %3, %4 ], [ %56, %5 ]
  %8 = getelementptr inbounds i8, i8* %0, i64 %6
  %9 = getelementptr inbounds i8, i8* %1, i64 %6
  %10 = getelementptr inbounds i8, i8* %2, i64 %6
  %11 = bitcast i8* %8 to i64*
  %12 = load i64, i64* %11, align 1
  %13 = insertelement <2 x i64> undef, i64 %12, i32 0
  %14 = bitcast <2 x i64> %13 to <16 x i8>
  %15 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %14, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %16 = bitcast i8* %9 to i64*
  %17 = load i64, i64* %16, align 1
  %18 = insertelement <2 x i64> undef, i64 %17, i32 0
  %19 = bitcast <2 x i64> %18 to <16 x i8>
  %20 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %19, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %21 = bitcast i8* %10 to i64*
  %22 = load i64, i64* %21, align 1
  %23 = insertelement <2 x i64> undef, i64 %22, i32 0
  %24 = bitcast <2 x i64> %23 to <16 x i8>
  %25 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %24, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %26 = bitcast <16 x i8> %15 to <8 x i16>
  %27 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %26, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #7
  %28 = bitcast <16 x i8> %25 to <8 x i16>
  %29 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %28, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #7
  %30 = add <8 x i16> %27, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %31 = add <8 x i16> %30, %29
  %32 = bitcast <16 x i8> %20 to <8 x i16>
  %33 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %32, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #7
  %34 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %28, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #7
  %35 = add <8 x i16> %27, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %36 = sub <8 x i16> %35, %33
  %37 = sub <8 x i16> %36, %34
  %38 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %32, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #7
  %39 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %38, <8 x i16> %27) #7
  %40 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %39, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #7
  %41 = ashr <8 x i16> %31, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %42 = ashr <8 x i16> %37, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %43 = lshr <8 x i16> %40, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %44 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %43, <8 x i16> %41) #7
  %45 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %42, <8 x i16> <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>) #7
  %46 = shufflevector <16 x i8> %44, <16 x i8> %45, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %47 = shufflevector <16 x i8> %44, <16 x i8> %45, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %48 = bitcast <16 x i8> %46 to <8 x i16>
  %49 = bitcast <16 x i8> %47 to <8 x i16>
  %50 = shufflevector <8 x i16> %48, <8 x i16> %49, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %51 = shufflevector <8 x i16> %48, <8 x i16> %49, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %52 = bitcast i8* %7 to <8 x i16>*
  store <8 x i16> %50, <8 x i16>* %52, align 1
  %53 = getelementptr inbounds i8, i8* %7, i64 16
  %54 = bitcast i8* %53 to <8 x i16>*
  store <8 x i16> %51, <8 x i16>* %54, align 1
  %55 = add nuw nsw i64 %6, 8
  %56 = getelementptr inbounds i8, i8* %7, i64 32
  %57 = icmp ult i64 %55, 32
  br i1 %57, label %5, label %58

58:                                               ; preds = %5
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @VP8YuvToArgb32_SSE2(i8* nocapture readonly, i8* nocapture readonly, i8* nocapture readonly, i8* nocapture) local_unnamed_addr #0 {
  br label %5

5:                                                ; preds = %4, %5
  %6 = phi i64 [ 0, %4 ], [ %55, %5 ]
  %7 = phi i8* [ %3, %4 ], [ %56, %5 ]
  %8 = getelementptr inbounds i8, i8* %0, i64 %6
  %9 = getelementptr inbounds i8, i8* %1, i64 %6
  %10 = getelementptr inbounds i8, i8* %2, i64 %6
  %11 = bitcast i8* %8 to i64*
  %12 = load i64, i64* %11, align 1
  %13 = insertelement <2 x i64> undef, i64 %12, i32 0
  %14 = bitcast <2 x i64> %13 to <16 x i8>
  %15 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %14, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %16 = bitcast i8* %9 to i64*
  %17 = load i64, i64* %16, align 1
  %18 = insertelement <2 x i64> undef, i64 %17, i32 0
  %19 = bitcast <2 x i64> %18 to <16 x i8>
  %20 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %19, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %21 = bitcast i8* %10 to i64*
  %22 = load i64, i64* %21, align 1
  %23 = insertelement <2 x i64> undef, i64 %22, i32 0
  %24 = bitcast <2 x i64> %23 to <16 x i8>
  %25 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %24, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %26 = bitcast <16 x i8> %15 to <8 x i16>
  %27 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %26, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #7
  %28 = bitcast <16 x i8> %25 to <8 x i16>
  %29 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %28, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #7
  %30 = add <8 x i16> %27, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %31 = add <8 x i16> %30, %29
  %32 = bitcast <16 x i8> %20 to <8 x i16>
  %33 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %32, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #7
  %34 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %28, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #7
  %35 = add <8 x i16> %27, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %36 = sub <8 x i16> %35, %33
  %37 = sub <8 x i16> %36, %34
  %38 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %32, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #7
  %39 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %38, <8 x i16> %27) #7
  %40 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %39, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #7
  %41 = ashr <8 x i16> %31, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %42 = ashr <8 x i16> %37, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %43 = lshr <8 x i16> %40, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %44 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>, <8 x i16> %42) #7
  %45 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %41, <8 x i16> %43) #7
  %46 = shufflevector <16 x i8> %44, <16 x i8> %45, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %47 = shufflevector <16 x i8> %44, <16 x i8> %45, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %48 = bitcast <16 x i8> %46 to <8 x i16>
  %49 = bitcast <16 x i8> %47 to <8 x i16>
  %50 = shufflevector <8 x i16> %48, <8 x i16> %49, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %51 = shufflevector <8 x i16> %48, <8 x i16> %49, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %52 = bitcast i8* %7 to <8 x i16>*
  store <8 x i16> %50, <8 x i16>* %52, align 1
  %53 = getelementptr inbounds i8, i8* %7, i64 16
  %54 = bitcast i8* %53 to <8 x i16>*
  store <8 x i16> %51, <8 x i16>* %54, align 1
  %55 = add nuw nsw i64 %6, 8
  %56 = getelementptr inbounds i8, i8* %7, i64 32
  %57 = icmp ult i64 %55, 32
  br i1 %57, label %5, label %58

58:                                               ; preds = %5
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @VP8YuvToRgba444432_SSE2(i8* nocapture readonly, i8* nocapture readonly, i8* nocapture readonly, i8* nocapture) local_unnamed_addr #0 {
  br label %5

5:                                                ; preds = %4, %5
  %6 = phi i64 [ 0, %4 ], [ %56, %5 ]
  %7 = phi i8* [ %3, %4 ], [ %57, %5 ]
  %8 = getelementptr inbounds i8, i8* %0, i64 %6
  %9 = getelementptr inbounds i8, i8* %1, i64 %6
  %10 = getelementptr inbounds i8, i8* %2, i64 %6
  %11 = bitcast i8* %8 to i64*
  %12 = load i64, i64* %11, align 1
  %13 = insertelement <2 x i64> undef, i64 %12, i32 0
  %14 = bitcast <2 x i64> %13 to <16 x i8>
  %15 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %14, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %16 = bitcast i8* %9 to i64*
  %17 = load i64, i64* %16, align 1
  %18 = insertelement <2 x i64> undef, i64 %17, i32 0
  %19 = bitcast <2 x i64> %18 to <16 x i8>
  %20 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %19, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %21 = bitcast i8* %10 to i64*
  %22 = load i64, i64* %21, align 1
  %23 = insertelement <2 x i64> undef, i64 %22, i32 0
  %24 = bitcast <2 x i64> %23 to <16 x i8>
  %25 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %24, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %26 = bitcast <16 x i8> %15 to <8 x i16>
  %27 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %26, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #7
  %28 = bitcast <16 x i8> %25 to <8 x i16>
  %29 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %28, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #7
  %30 = add <8 x i16> %27, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %31 = add <8 x i16> %30, %29
  %32 = bitcast <16 x i8> %20 to <8 x i16>
  %33 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %32, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #7
  %34 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %28, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #7
  %35 = add <8 x i16> %27, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %36 = sub <8 x i16> %35, %33
  %37 = sub <8 x i16> %36, %34
  %38 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %32, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #7
  %39 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %38, <8 x i16> %27) #7
  %40 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %39, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #7
  %41 = ashr <8 x i16> %31, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %42 = ashr <8 x i16> %37, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %43 = lshr <8 x i16> %40, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %44 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %41, <8 x i16> %42) #7
  %45 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %43, <8 x i16> <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>) #7
  %46 = shufflevector <16 x i8> %44, <16 x i8> %45, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %47 = bitcast <16 x i8> %46 to <2 x i64>
  %48 = shufflevector <16 x i8> %44, <16 x i8> %45, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %49 = and <2 x i64> %47, <i64 -1085102592571150096, i64 -1085102592571150096>
  %50 = bitcast <16 x i8> %48 to <8 x i16>
  %51 = lshr <8 x i16> %50, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %52 = bitcast <8 x i16> %51 to <2 x i64>
  %53 = and <2 x i64> %52, <i64 1085102592571150095, i64 1085102592571150095>
  %54 = or <2 x i64> %53, %49
  %55 = bitcast i8* %7 to <2 x i64>*
  store <2 x i64> %54, <2 x i64>* %55, align 1
  %56 = add nuw nsw i64 %6, 8
  %57 = getelementptr inbounds i8, i8* %7, i64 16
  %58 = icmp ult i64 %56, 32
  br i1 %58, label %5, label %59

59:                                               ; preds = %5
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @VP8YuvToRgb56532_SSE2(i8* nocapture readonly, i8* nocapture readonly, i8* nocapture readonly, i8* nocapture) local_unnamed_addr #0 {
  br label %5

5:                                                ; preds = %4, %5
  %6 = phi i64 [ 0, %4 ], [ %66, %5 ]
  %7 = phi i8* [ %3, %4 ], [ %67, %5 ]
  %8 = getelementptr inbounds i8, i8* %0, i64 %6
  %9 = getelementptr inbounds i8, i8* %1, i64 %6
  %10 = getelementptr inbounds i8, i8* %2, i64 %6
  %11 = bitcast i8* %8 to i64*
  %12 = load i64, i64* %11, align 1
  %13 = insertelement <2 x i64> undef, i64 %12, i32 0
  %14 = bitcast <2 x i64> %13 to <16 x i8>
  %15 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %14, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %16 = bitcast i8* %9 to i64*
  %17 = load i64, i64* %16, align 1
  %18 = insertelement <2 x i64> undef, i64 %17, i32 0
  %19 = bitcast <2 x i64> %18 to <16 x i8>
  %20 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %19, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %21 = bitcast i8* %10 to i64*
  %22 = load i64, i64* %21, align 1
  %23 = insertelement <2 x i64> undef, i64 %22, i32 0
  %24 = bitcast <2 x i64> %23 to <16 x i8>
  %25 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %24, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %26 = bitcast <16 x i8> %15 to <8 x i16>
  %27 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %26, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #7
  %28 = bitcast <16 x i8> %25 to <8 x i16>
  %29 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %28, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #7
  %30 = add <8 x i16> %27, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %31 = add <8 x i16> %30, %29
  %32 = bitcast <16 x i8> %20 to <8 x i16>
  %33 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %32, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #7
  %34 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %28, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #7
  %35 = add <8 x i16> %27, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %36 = sub <8 x i16> %35, %33
  %37 = sub <8 x i16> %36, %34
  %38 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %32, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #7
  %39 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %38, <8 x i16> %27) #7
  %40 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %39, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #7
  %41 = ashr <8 x i16> %31, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %42 = ashr <8 x i16> %37, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %43 = lshr <8 x i16> %40, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %44 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %41, <8 x i16> undef) #7
  %45 = bitcast <16 x i8> %44 to <2 x i64>
  %46 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %42, <8 x i16> %42) #7
  %47 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %43, <8 x i16> %43) #7
  %48 = and <2 x i64> %45, <i64 -506381209866536712, i64 undef>
  %49 = bitcast <16 x i8> %47 to <8 x i16>
  %50 = lshr <8 x i16> %49, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %51 = bitcast <8 x i16> %50 to <2 x i64>
  %52 = and <2 x i64> %51, <i64 2242545357980376863, i64 undef>
  %53 = bitcast <16 x i8> %46 to <8 x i16>
  %54 = lshr <8 x i16> %53, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  %55 = bitcast <8 x i16> %54 to <2 x i64>
  %56 = and <2 x i64> %55, <i64 506381209866536711, i64 undef>
  %57 = shl <8 x i16> %53, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %58 = bitcast <8 x i16> %57 to <2 x i64>
  %59 = and <2 x i64> %58, <i64 -2242545357980376864, i64 undef>
  %60 = or <2 x i64> %56, %48
  %61 = or <2 x i64> %52, %59
  %62 = bitcast <2 x i64> %60 to <16 x i8>
  %63 = bitcast <2 x i64> %61 to <16 x i8>
  %64 = shufflevector <16 x i8> %62, <16 x i8> %63, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %65 = bitcast i8* %7 to <16 x i8>*
  store <16 x i8> %64, <16 x i8>* %65, align 1
  %66 = add nuw nsw i64 %6, 8
  %67 = getelementptr inbounds i8, i8* %7, i64 16
  %68 = icmp ult i64 %66, 32
  br i1 %68, label %5, label %69

69:                                               ; preds = %5
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @VP8YuvToRgb32_SSE2(i8* nocapture readonly, i8* nocapture readonly, i8* nocapture readonly, i8* nocapture) local_unnamed_addr #0 {
  %5 = bitcast i8* %0 to i64*
  %6 = load i64, i64* %5, align 1
  %7 = insertelement <2 x i64> undef, i64 %6, i32 0
  %8 = bitcast <2 x i64> %7 to <16 x i8>
  %9 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %8, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %10 = bitcast i8* %1 to i64*
  %11 = load i64, i64* %10, align 1
  %12 = insertelement <2 x i64> undef, i64 %11, i32 0
  %13 = bitcast <2 x i64> %12 to <16 x i8>
  %14 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %13, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %15 = bitcast i8* %2 to i64*
  %16 = load i64, i64* %15, align 1
  %17 = insertelement <2 x i64> undef, i64 %16, i32 0
  %18 = bitcast <2 x i64> %17 to <16 x i8>
  %19 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %18, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %20 = bitcast <16 x i8> %9 to <8 x i16>
  %21 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %20, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #7
  %22 = bitcast <16 x i8> %19 to <8 x i16>
  %23 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %22, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #7
  %24 = add <8 x i16> %21, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %25 = add <8 x i16> %24, %23
  %26 = bitcast <16 x i8> %14 to <8 x i16>
  %27 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %26, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #7
  %28 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %22, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #7
  %29 = add <8 x i16> %21, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %30 = sub <8 x i16> %29, %27
  %31 = sub <8 x i16> %30, %28
  %32 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %26, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #7
  %33 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %32, <8 x i16> %21) #7
  %34 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %33, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #7
  %35 = ashr <8 x i16> %25, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %36 = ashr <8 x i16> %31, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %37 = lshr <8 x i16> %34, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %38 = getelementptr inbounds i8, i8* %0, i64 8
  %39 = getelementptr inbounds i8, i8* %1, i64 8
  %40 = getelementptr inbounds i8, i8* %2, i64 8
  %41 = bitcast i8* %38 to i64*
  %42 = load i64, i64* %41, align 1
  %43 = insertelement <2 x i64> undef, i64 %42, i32 0
  %44 = bitcast <2 x i64> %43 to <16 x i8>
  %45 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %44, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %46 = bitcast i8* %39 to i64*
  %47 = load i64, i64* %46, align 1
  %48 = insertelement <2 x i64> undef, i64 %47, i32 0
  %49 = bitcast <2 x i64> %48 to <16 x i8>
  %50 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %49, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %51 = bitcast i8* %40 to i64*
  %52 = load i64, i64* %51, align 1
  %53 = insertelement <2 x i64> undef, i64 %52, i32 0
  %54 = bitcast <2 x i64> %53 to <16 x i8>
  %55 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %54, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %56 = bitcast <16 x i8> %45 to <8 x i16>
  %57 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %56, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #7
  %58 = bitcast <16 x i8> %55 to <8 x i16>
  %59 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %58, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #7
  %60 = add <8 x i16> %57, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %61 = add <8 x i16> %60, %59
  %62 = bitcast <16 x i8> %50 to <8 x i16>
  %63 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %62, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #7
  %64 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %58, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #7
  %65 = add <8 x i16> %57, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %66 = sub <8 x i16> %65, %63
  %67 = sub <8 x i16> %66, %64
  %68 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %62, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #7
  %69 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %68, <8 x i16> %57) #7
  %70 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %69, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #7
  %71 = ashr <8 x i16> %61, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %72 = ashr <8 x i16> %67, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %73 = lshr <8 x i16> %70, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %74 = getelementptr inbounds i8, i8* %0, i64 16
  %75 = getelementptr inbounds i8, i8* %1, i64 16
  %76 = getelementptr inbounds i8, i8* %2, i64 16
  %77 = bitcast i8* %74 to i64*
  %78 = load i64, i64* %77, align 1
  %79 = insertelement <2 x i64> undef, i64 %78, i32 0
  %80 = bitcast <2 x i64> %79 to <16 x i8>
  %81 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %80, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %82 = bitcast i8* %75 to i64*
  %83 = load i64, i64* %82, align 1
  %84 = insertelement <2 x i64> undef, i64 %83, i32 0
  %85 = bitcast <2 x i64> %84 to <16 x i8>
  %86 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %85, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %87 = bitcast i8* %76 to i64*
  %88 = load i64, i64* %87, align 1
  %89 = insertelement <2 x i64> undef, i64 %88, i32 0
  %90 = bitcast <2 x i64> %89 to <16 x i8>
  %91 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %90, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %92 = bitcast <16 x i8> %81 to <8 x i16>
  %93 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %92, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #7
  %94 = bitcast <16 x i8> %91 to <8 x i16>
  %95 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %94, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #7
  %96 = add <8 x i16> %93, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %97 = add <8 x i16> %96, %95
  %98 = bitcast <16 x i8> %86 to <8 x i16>
  %99 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %98, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #7
  %100 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %94, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #7
  %101 = add <8 x i16> %93, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %102 = sub <8 x i16> %101, %99
  %103 = sub <8 x i16> %102, %100
  %104 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %98, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #7
  %105 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %104, <8 x i16> %93) #7
  %106 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %105, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #7
  %107 = ashr <8 x i16> %97, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %108 = ashr <8 x i16> %103, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %109 = lshr <8 x i16> %106, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %110 = getelementptr inbounds i8, i8* %0, i64 24
  %111 = getelementptr inbounds i8, i8* %1, i64 24
  %112 = getelementptr inbounds i8, i8* %2, i64 24
  %113 = bitcast i8* %110 to i64*
  %114 = load i64, i64* %113, align 1
  %115 = insertelement <2 x i64> undef, i64 %114, i32 0
  %116 = bitcast <2 x i64> %115 to <16 x i8>
  %117 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %116, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %118 = bitcast i8* %111 to i64*
  %119 = load i64, i64* %118, align 1
  %120 = insertelement <2 x i64> undef, i64 %119, i32 0
  %121 = bitcast <2 x i64> %120 to <16 x i8>
  %122 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %121, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %123 = bitcast i8* %112 to i64*
  %124 = load i64, i64* %123, align 1
  %125 = insertelement <2 x i64> undef, i64 %124, i32 0
  %126 = bitcast <2 x i64> %125 to <16 x i8>
  %127 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %126, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %128 = bitcast <16 x i8> %117 to <8 x i16>
  %129 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %128, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #7
  %130 = bitcast <16 x i8> %127 to <8 x i16>
  %131 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %130, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #7
  %132 = add <8 x i16> %129, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %133 = add <8 x i16> %132, %131
  %134 = bitcast <16 x i8> %122 to <8 x i16>
  %135 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %134, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #7
  %136 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %130, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #7
  %137 = add <8 x i16> %129, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %138 = sub <8 x i16> %137, %135
  %139 = sub <8 x i16> %138, %136
  %140 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %134, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #7
  %141 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %140, <8 x i16> %129) #7
  %142 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %141, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #7
  %143 = ashr <8 x i16> %133, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %144 = ashr <8 x i16> %139, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %145 = lshr <8 x i16> %142, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %146 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %35, <8 x i16> %71) #7
  %147 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %107, <8 x i16> %143) #7
  %148 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %36, <8 x i16> %72) #7
  %149 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %108, <8 x i16> %144) #7
  %150 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %37, <8 x i16> %73) #7
  %151 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %109, <8 x i16> %145) #7
  %152 = bitcast <16 x i8> %146 to <8 x i16>
  %153 = bitcast <16 x i8> %147 to <8 x i16>
  %154 = and <8 x i16> %152, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %155 = and <8 x i16> %153, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %156 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %154, <8 x i16> %155) #7
  %157 = bitcast <16 x i8> %148 to <8 x i16>
  %158 = bitcast <16 x i8> %149 to <8 x i16>
  %159 = and <8 x i16> %157, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %160 = and <8 x i16> %158, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %161 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %159, <8 x i16> %160) #7
  %162 = bitcast <16 x i8> %150 to <8 x i16>
  %163 = bitcast <16 x i8> %151 to <8 x i16>
  %164 = and <8 x i16> %162, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %165 = and <8 x i16> %163, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %166 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %164, <8 x i16> %165) #7
  %167 = lshr <8 x i16> %152, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %168 = lshr <8 x i16> %153, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %169 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %167, <8 x i16> %168) #7
  %170 = lshr <8 x i16> %157, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %171 = lshr <8 x i16> %158, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %172 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %170, <8 x i16> %171) #7
  %173 = lshr <8 x i16> %162, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %174 = lshr <8 x i16> %163, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %175 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %173, <8 x i16> %174) #7
  %176 = bitcast <16 x i8> %156 to <8 x i16>
  %177 = and <8 x i16> %176, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %178 = bitcast <16 x i8> %161 to <8 x i16>
  %179 = and <8 x i16> %178, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %180 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %177, <8 x i16> %179) #7
  %181 = bitcast <16 x i8> %166 to <8 x i16>
  %182 = and <8 x i16> %181, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %183 = bitcast <16 x i8> %169 to <8 x i16>
  %184 = and <8 x i16> %183, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %185 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %182, <8 x i16> %184) #7
  %186 = bitcast <16 x i8> %172 to <8 x i16>
  %187 = and <8 x i16> %186, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %188 = bitcast <16 x i8> %175 to <8 x i16>
  %189 = and <8 x i16> %188, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %190 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %187, <8 x i16> %189) #7
  %191 = lshr <8 x i16> %176, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %192 = lshr <8 x i16> %178, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %193 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %191, <8 x i16> %192) #7
  %194 = lshr <8 x i16> %181, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %195 = lshr <8 x i16> %183, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %196 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %194, <8 x i16> %195) #7
  %197 = lshr <8 x i16> %186, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %198 = lshr <8 x i16> %188, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %199 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %197, <8 x i16> %198) #7
  %200 = bitcast <16 x i8> %180 to <8 x i16>
  %201 = bitcast <16 x i8> %185 to <8 x i16>
  %202 = and <8 x i16> %200, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %203 = and <8 x i16> %201, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %204 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %202, <8 x i16> %203) #7
  %205 = bitcast <16 x i8> %190 to <8 x i16>
  %206 = bitcast <16 x i8> %193 to <8 x i16>
  %207 = and <8 x i16> %205, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %208 = and <8 x i16> %206, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %209 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %207, <8 x i16> %208) #7
  %210 = bitcast <16 x i8> %196 to <8 x i16>
  %211 = and <8 x i16> %210, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %212 = bitcast <16 x i8> %199 to <8 x i16>
  %213 = and <8 x i16> %212, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %214 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %211, <8 x i16> %213) #7
  %215 = lshr <8 x i16> %200, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %216 = lshr <8 x i16> %201, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %217 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %215, <8 x i16> %216) #7
  %218 = lshr <8 x i16> %205, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %219 = lshr <8 x i16> %206, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %220 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %218, <8 x i16> %219) #7
  %221 = lshr <8 x i16> %210, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %222 = lshr <8 x i16> %212, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %223 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %221, <8 x i16> %222) #7
  %224 = bitcast <16 x i8> %204 to <8 x i16>
  %225 = and <8 x i16> %224, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %226 = bitcast <16 x i8> %209 to <8 x i16>
  %227 = and <8 x i16> %226, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %228 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %225, <8 x i16> %227) #7
  %229 = bitcast <16 x i8> %214 to <8 x i16>
  %230 = and <8 x i16> %229, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %231 = bitcast <16 x i8> %217 to <8 x i16>
  %232 = and <8 x i16> %231, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %233 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %230, <8 x i16> %232) #7
  %234 = bitcast <16 x i8> %220 to <8 x i16>
  %235 = and <8 x i16> %234, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %236 = bitcast <16 x i8> %223 to <8 x i16>
  %237 = and <8 x i16> %236, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %238 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %235, <8 x i16> %237) #7
  %239 = lshr <8 x i16> %224, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %240 = lshr <8 x i16> %226, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %241 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %239, <8 x i16> %240) #7
  %242 = lshr <8 x i16> %229, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %243 = lshr <8 x i16> %231, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %244 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %242, <8 x i16> %243) #7
  %245 = lshr <8 x i16> %234, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %246 = lshr <8 x i16> %236, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %247 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %245, <8 x i16> %246) #7
  %248 = bitcast <16 x i8> %228 to <8 x i16>
  %249 = and <8 x i16> %248, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %250 = bitcast <16 x i8> %233 to <8 x i16>
  %251 = and <8 x i16> %250, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %252 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %249, <8 x i16> %251) #7
  %253 = bitcast <16 x i8> %238 to <8 x i16>
  %254 = and <8 x i16> %253, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %255 = bitcast <16 x i8> %241 to <8 x i16>
  %256 = and <8 x i16> %255, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %257 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %254, <8 x i16> %256) #7
  %258 = bitcast <16 x i8> %244 to <8 x i16>
  %259 = and <8 x i16> %258, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %260 = bitcast <16 x i8> %247 to <8 x i16>
  %261 = and <8 x i16> %260, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %262 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %259, <8 x i16> %261) #7
  %263 = lshr <8 x i16> %248, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %264 = lshr <8 x i16> %250, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %265 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %263, <8 x i16> %264) #7
  %266 = lshr <8 x i16> %253, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %267 = lshr <8 x i16> %255, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %268 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %266, <8 x i16> %267) #7
  %269 = lshr <8 x i16> %258, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %270 = lshr <8 x i16> %260, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %271 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %269, <8 x i16> %270) #7
  %272 = bitcast i8* %3 to <16 x i8>*
  store <16 x i8> %252, <16 x i8>* %272, align 1
  %273 = getelementptr inbounds i8, i8* %3, i64 16
  %274 = bitcast i8* %273 to <16 x i8>*
  store <16 x i8> %257, <16 x i8>* %274, align 1
  %275 = getelementptr inbounds i8, i8* %3, i64 32
  %276 = bitcast i8* %275 to <16 x i8>*
  store <16 x i8> %262, <16 x i8>* %276, align 1
  %277 = getelementptr inbounds i8, i8* %3, i64 48
  %278 = bitcast i8* %277 to <16 x i8>*
  store <16 x i8> %265, <16 x i8>* %278, align 1
  %279 = getelementptr inbounds i8, i8* %3, i64 64
  %280 = bitcast i8* %279 to <16 x i8>*
  store <16 x i8> %268, <16 x i8>* %280, align 1
  %281 = getelementptr inbounds i8, i8* %3, i64 80
  %282 = bitcast i8* %281 to <16 x i8>*
  store <16 x i8> %271, <16 x i8>* %282, align 1
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @VP8YuvToBgr32_SSE2(i8* nocapture readonly, i8* nocapture readonly, i8* nocapture readonly, i8* nocapture) local_unnamed_addr #0 {
  %5 = bitcast i8* %0 to i64*
  %6 = load i64, i64* %5, align 1
  %7 = insertelement <2 x i64> undef, i64 %6, i32 0
  %8 = bitcast <2 x i64> %7 to <16 x i8>
  %9 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %8, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %10 = bitcast i8* %1 to i64*
  %11 = load i64, i64* %10, align 1
  %12 = insertelement <2 x i64> undef, i64 %11, i32 0
  %13 = bitcast <2 x i64> %12 to <16 x i8>
  %14 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %13, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %15 = bitcast i8* %2 to i64*
  %16 = load i64, i64* %15, align 1
  %17 = insertelement <2 x i64> undef, i64 %16, i32 0
  %18 = bitcast <2 x i64> %17 to <16 x i8>
  %19 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %18, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %20 = bitcast <16 x i8> %9 to <8 x i16>
  %21 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %20, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #7
  %22 = bitcast <16 x i8> %19 to <8 x i16>
  %23 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %22, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #7
  %24 = add <8 x i16> %21, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %25 = add <8 x i16> %24, %23
  %26 = bitcast <16 x i8> %14 to <8 x i16>
  %27 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %26, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #7
  %28 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %22, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #7
  %29 = add <8 x i16> %21, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %30 = sub <8 x i16> %29, %27
  %31 = sub <8 x i16> %30, %28
  %32 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %26, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #7
  %33 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %32, <8 x i16> %21) #7
  %34 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %33, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #7
  %35 = ashr <8 x i16> %25, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %36 = ashr <8 x i16> %31, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %37 = lshr <8 x i16> %34, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %38 = getelementptr inbounds i8, i8* %0, i64 8
  %39 = getelementptr inbounds i8, i8* %1, i64 8
  %40 = getelementptr inbounds i8, i8* %2, i64 8
  %41 = bitcast i8* %38 to i64*
  %42 = load i64, i64* %41, align 1
  %43 = insertelement <2 x i64> undef, i64 %42, i32 0
  %44 = bitcast <2 x i64> %43 to <16 x i8>
  %45 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %44, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %46 = bitcast i8* %39 to i64*
  %47 = load i64, i64* %46, align 1
  %48 = insertelement <2 x i64> undef, i64 %47, i32 0
  %49 = bitcast <2 x i64> %48 to <16 x i8>
  %50 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %49, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %51 = bitcast i8* %40 to i64*
  %52 = load i64, i64* %51, align 1
  %53 = insertelement <2 x i64> undef, i64 %52, i32 0
  %54 = bitcast <2 x i64> %53 to <16 x i8>
  %55 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %54, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %56 = bitcast <16 x i8> %45 to <8 x i16>
  %57 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %56, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #7
  %58 = bitcast <16 x i8> %55 to <8 x i16>
  %59 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %58, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #7
  %60 = add <8 x i16> %57, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %61 = add <8 x i16> %60, %59
  %62 = bitcast <16 x i8> %50 to <8 x i16>
  %63 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %62, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #7
  %64 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %58, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #7
  %65 = add <8 x i16> %57, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %66 = sub <8 x i16> %65, %63
  %67 = sub <8 x i16> %66, %64
  %68 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %62, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #7
  %69 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %68, <8 x i16> %57) #7
  %70 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %69, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #7
  %71 = ashr <8 x i16> %61, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %72 = ashr <8 x i16> %67, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %73 = lshr <8 x i16> %70, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %74 = getelementptr inbounds i8, i8* %0, i64 16
  %75 = getelementptr inbounds i8, i8* %1, i64 16
  %76 = getelementptr inbounds i8, i8* %2, i64 16
  %77 = bitcast i8* %74 to i64*
  %78 = load i64, i64* %77, align 1
  %79 = insertelement <2 x i64> undef, i64 %78, i32 0
  %80 = bitcast <2 x i64> %79 to <16 x i8>
  %81 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %80, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %82 = bitcast i8* %75 to i64*
  %83 = load i64, i64* %82, align 1
  %84 = insertelement <2 x i64> undef, i64 %83, i32 0
  %85 = bitcast <2 x i64> %84 to <16 x i8>
  %86 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %85, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %87 = bitcast i8* %76 to i64*
  %88 = load i64, i64* %87, align 1
  %89 = insertelement <2 x i64> undef, i64 %88, i32 0
  %90 = bitcast <2 x i64> %89 to <16 x i8>
  %91 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %90, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %92 = bitcast <16 x i8> %81 to <8 x i16>
  %93 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %92, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #7
  %94 = bitcast <16 x i8> %91 to <8 x i16>
  %95 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %94, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #7
  %96 = add <8 x i16> %93, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %97 = add <8 x i16> %96, %95
  %98 = bitcast <16 x i8> %86 to <8 x i16>
  %99 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %98, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #7
  %100 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %94, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #7
  %101 = add <8 x i16> %93, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %102 = sub <8 x i16> %101, %99
  %103 = sub <8 x i16> %102, %100
  %104 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %98, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #7
  %105 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %104, <8 x i16> %93) #7
  %106 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %105, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #7
  %107 = ashr <8 x i16> %97, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %108 = ashr <8 x i16> %103, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %109 = lshr <8 x i16> %106, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %110 = getelementptr inbounds i8, i8* %0, i64 24
  %111 = getelementptr inbounds i8, i8* %1, i64 24
  %112 = getelementptr inbounds i8, i8* %2, i64 24
  %113 = bitcast i8* %110 to i64*
  %114 = load i64, i64* %113, align 1
  %115 = insertelement <2 x i64> undef, i64 %114, i32 0
  %116 = bitcast <2 x i64> %115 to <16 x i8>
  %117 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %116, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %118 = bitcast i8* %111 to i64*
  %119 = load i64, i64* %118, align 1
  %120 = insertelement <2 x i64> undef, i64 %119, i32 0
  %121 = bitcast <2 x i64> %120 to <16 x i8>
  %122 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %121, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %123 = bitcast i8* %112 to i64*
  %124 = load i64, i64* %123, align 1
  %125 = insertelement <2 x i64> undef, i64 %124, i32 0
  %126 = bitcast <2 x i64> %125 to <16 x i8>
  %127 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %126, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %128 = bitcast <16 x i8> %117 to <8 x i16>
  %129 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %128, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #7
  %130 = bitcast <16 x i8> %127 to <8 x i16>
  %131 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %130, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #7
  %132 = add <8 x i16> %129, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %133 = add <8 x i16> %132, %131
  %134 = bitcast <16 x i8> %122 to <8 x i16>
  %135 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %134, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #7
  %136 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %130, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #7
  %137 = add <8 x i16> %129, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %138 = sub <8 x i16> %137, %135
  %139 = sub <8 x i16> %138, %136
  %140 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %134, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #7
  %141 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %140, <8 x i16> %129) #7
  %142 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %141, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #7
  %143 = ashr <8 x i16> %133, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %144 = ashr <8 x i16> %139, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %145 = lshr <8 x i16> %142, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %146 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %37, <8 x i16> %73) #7
  %147 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %109, <8 x i16> %145) #7
  %148 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %36, <8 x i16> %72) #7
  %149 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %108, <8 x i16> %144) #7
  %150 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %35, <8 x i16> %71) #7
  %151 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %107, <8 x i16> %143) #7
  %152 = bitcast <16 x i8> %146 to <8 x i16>
  %153 = bitcast <16 x i8> %147 to <8 x i16>
  %154 = and <8 x i16> %152, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %155 = and <8 x i16> %153, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %156 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %154, <8 x i16> %155) #7
  %157 = bitcast <16 x i8> %148 to <8 x i16>
  %158 = bitcast <16 x i8> %149 to <8 x i16>
  %159 = and <8 x i16> %157, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %160 = and <8 x i16> %158, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %161 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %159, <8 x i16> %160) #7
  %162 = bitcast <16 x i8> %150 to <8 x i16>
  %163 = bitcast <16 x i8> %151 to <8 x i16>
  %164 = and <8 x i16> %162, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %165 = and <8 x i16> %163, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %166 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %164, <8 x i16> %165) #7
  %167 = lshr <8 x i16> %152, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %168 = lshr <8 x i16> %153, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %169 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %167, <8 x i16> %168) #7
  %170 = lshr <8 x i16> %157, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %171 = lshr <8 x i16> %158, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %172 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %170, <8 x i16> %171) #7
  %173 = lshr <8 x i16> %162, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %174 = lshr <8 x i16> %163, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %175 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %173, <8 x i16> %174) #7
  %176 = bitcast <16 x i8> %156 to <8 x i16>
  %177 = and <8 x i16> %176, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %178 = bitcast <16 x i8> %161 to <8 x i16>
  %179 = and <8 x i16> %178, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %180 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %177, <8 x i16> %179) #7
  %181 = bitcast <16 x i8> %166 to <8 x i16>
  %182 = and <8 x i16> %181, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %183 = bitcast <16 x i8> %169 to <8 x i16>
  %184 = and <8 x i16> %183, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %185 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %182, <8 x i16> %184) #7
  %186 = bitcast <16 x i8> %172 to <8 x i16>
  %187 = and <8 x i16> %186, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %188 = bitcast <16 x i8> %175 to <8 x i16>
  %189 = and <8 x i16> %188, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %190 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %187, <8 x i16> %189) #7
  %191 = lshr <8 x i16> %176, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %192 = lshr <8 x i16> %178, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %193 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %191, <8 x i16> %192) #7
  %194 = lshr <8 x i16> %181, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %195 = lshr <8 x i16> %183, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %196 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %194, <8 x i16> %195) #7
  %197 = lshr <8 x i16> %186, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %198 = lshr <8 x i16> %188, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %199 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %197, <8 x i16> %198) #7
  %200 = bitcast <16 x i8> %180 to <8 x i16>
  %201 = bitcast <16 x i8> %185 to <8 x i16>
  %202 = and <8 x i16> %200, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %203 = and <8 x i16> %201, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %204 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %202, <8 x i16> %203) #7
  %205 = bitcast <16 x i8> %190 to <8 x i16>
  %206 = bitcast <16 x i8> %193 to <8 x i16>
  %207 = and <8 x i16> %205, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %208 = and <8 x i16> %206, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %209 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %207, <8 x i16> %208) #7
  %210 = bitcast <16 x i8> %196 to <8 x i16>
  %211 = and <8 x i16> %210, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %212 = bitcast <16 x i8> %199 to <8 x i16>
  %213 = and <8 x i16> %212, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %214 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %211, <8 x i16> %213) #7
  %215 = lshr <8 x i16> %200, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %216 = lshr <8 x i16> %201, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %217 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %215, <8 x i16> %216) #7
  %218 = lshr <8 x i16> %205, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %219 = lshr <8 x i16> %206, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %220 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %218, <8 x i16> %219) #7
  %221 = lshr <8 x i16> %210, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %222 = lshr <8 x i16> %212, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %223 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %221, <8 x i16> %222) #7
  %224 = bitcast <16 x i8> %204 to <8 x i16>
  %225 = and <8 x i16> %224, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %226 = bitcast <16 x i8> %209 to <8 x i16>
  %227 = and <8 x i16> %226, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %228 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %225, <8 x i16> %227) #7
  %229 = bitcast <16 x i8> %214 to <8 x i16>
  %230 = and <8 x i16> %229, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %231 = bitcast <16 x i8> %217 to <8 x i16>
  %232 = and <8 x i16> %231, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %233 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %230, <8 x i16> %232) #7
  %234 = bitcast <16 x i8> %220 to <8 x i16>
  %235 = and <8 x i16> %234, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %236 = bitcast <16 x i8> %223 to <8 x i16>
  %237 = and <8 x i16> %236, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %238 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %235, <8 x i16> %237) #7
  %239 = lshr <8 x i16> %224, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %240 = lshr <8 x i16> %226, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %241 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %239, <8 x i16> %240) #7
  %242 = lshr <8 x i16> %229, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %243 = lshr <8 x i16> %231, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %244 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %242, <8 x i16> %243) #7
  %245 = lshr <8 x i16> %234, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %246 = lshr <8 x i16> %236, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %247 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %245, <8 x i16> %246) #7
  %248 = bitcast <16 x i8> %228 to <8 x i16>
  %249 = and <8 x i16> %248, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %250 = bitcast <16 x i8> %233 to <8 x i16>
  %251 = and <8 x i16> %250, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %252 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %249, <8 x i16> %251) #7
  %253 = bitcast <16 x i8> %238 to <8 x i16>
  %254 = and <8 x i16> %253, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %255 = bitcast <16 x i8> %241 to <8 x i16>
  %256 = and <8 x i16> %255, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %257 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %254, <8 x i16> %256) #7
  %258 = bitcast <16 x i8> %244 to <8 x i16>
  %259 = and <8 x i16> %258, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %260 = bitcast <16 x i8> %247 to <8 x i16>
  %261 = and <8 x i16> %260, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %262 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %259, <8 x i16> %261) #7
  %263 = lshr <8 x i16> %248, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %264 = lshr <8 x i16> %250, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %265 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %263, <8 x i16> %264) #7
  %266 = lshr <8 x i16> %253, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %267 = lshr <8 x i16> %255, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %268 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %266, <8 x i16> %267) #7
  %269 = lshr <8 x i16> %258, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %270 = lshr <8 x i16> %260, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %271 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %269, <8 x i16> %270) #7
  %272 = bitcast i8* %3 to <16 x i8>*
  store <16 x i8> %252, <16 x i8>* %272, align 1
  %273 = getelementptr inbounds i8, i8* %3, i64 16
  %274 = bitcast i8* %273 to <16 x i8>*
  store <16 x i8> %257, <16 x i8>* %274, align 1
  %275 = getelementptr inbounds i8, i8* %3, i64 32
  %276 = bitcast i8* %275 to <16 x i8>*
  store <16 x i8> %262, <16 x i8>* %276, align 1
  %277 = getelementptr inbounds i8, i8* %3, i64 48
  %278 = bitcast i8* %277 to <16 x i8>*
  store <16 x i8> %265, <16 x i8>* %278, align 1
  %279 = getelementptr inbounds i8, i8* %3, i64 64
  %280 = bitcast i8* %279 to <16 x i8>*
  store <16 x i8> %268, <16 x i8>* %280, align 1
  %281 = getelementptr inbounds i8, i8* %3, i64 80
  %282 = bitcast i8* %281 to <16 x i8>*
  store <16 x i8> %271, <16 x i8>* %282, align 1
  ret void
}

; Function Attrs: nofree norecurse nounwind ssp uwtable writeonly
define hidden void @WebPInitSamplersSSE2() local_unnamed_addr #1 {
  store <2 x void (i8*, i8*, i8*, i8*, i32)*> <void (i8*, i8*, i8*, i8*, i32)* @YuvToRgbRow_SSE2, void (i8*, i8*, i8*, i8*, i32)* @YuvToRgbaRow_SSE2>, <2 x void (i8*, i8*, i8*, i8*, i32)*>* bitcast ([0 x void (i8*, i8*, i8*, i8*, i32)*]* @WebPSamplers to <2 x void (i8*, i8*, i8*, i8*, i32)*>*), align 8
  store <2 x void (i8*, i8*, i8*, i8*, i32)*> <void (i8*, i8*, i8*, i8*, i32)* @YuvToBgrRow_SSE2, void (i8*, i8*, i8*, i8*, i32)* @YuvToBgraRow_SSE2>, <2 x void (i8*, i8*, i8*, i8*, i32)*>* bitcast (void (i8*, i8*, i8*, i8*, i32)** getelementptr inbounds ([0 x void (i8*, i8*, i8*, i8*, i32)*], [0 x void (i8*, i8*, i8*, i8*, i32)*]* @WebPSamplers, i64 0, i64 2) to <2 x void (i8*, i8*, i8*, i8*, i32)*>*), align 8
  store void (i8*, i8*, i8*, i8*, i32)* @YuvToArgbRow_SSE2, void (i8*, i8*, i8*, i8*, i32)** getelementptr inbounds ([0 x void (i8*, i8*, i8*, i8*, i32)*], [0 x void (i8*, i8*, i8*, i8*, i32)*]* @WebPSamplers, i64 0, i64 4), align 8
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @YuvToRgbRow_SSE2(i8* nocapture readonly, i8* nocapture readonly, i8* nocapture readonly, i8* nocapture, i32) #0 {
  %6 = icmp slt i32 %4, 32
  br i1 %6, label %9, label %16

7:                                                ; preds = %16
  %8 = and i32 %4, -32
  br label %9

9:                                                ; preds = %7, %5
  %10 = phi i8* [ %1, %5 ], [ %309, %7 ]
  %11 = phi i8* [ %2, %5 ], [ %310, %7 ]
  %12 = phi i8* [ %3, %5 ], [ %311, %7 ]
  %13 = phi i8* [ %0, %5 ], [ %308, %7 ]
  %14 = phi i32 [ 0, %5 ], [ %8, %7 ]
  %15 = icmp slt i32 %14, %4
  br i1 %15, label %314, label %371

16:                                               ; preds = %5, %16
  %17 = phi i32 [ %312, %16 ], [ 32, %5 ]
  %18 = phi i8* [ %308, %16 ], [ %0, %5 ]
  %19 = phi i8* [ %311, %16 ], [ %3, %5 ]
  %20 = phi i8* [ %310, %16 ], [ %2, %5 ]
  %21 = phi i8* [ %309, %16 ], [ %1, %5 ]
  %22 = bitcast i8* %18 to i64*
  %23 = load i64, i64* %22, align 1
  %24 = insertelement <2 x i64> undef, i64 %23, i32 0
  %25 = bitcast <2 x i64> %24 to <16 x i8>
  %26 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %25, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %27 = bitcast i8* %21 to i32*
  %28 = load i32, i32* %27, align 4
  %29 = insertelement <4 x i32> undef, i32 %28, i32 0
  %30 = bitcast <4 x i32> %29 to <16 x i8>
  %31 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %30, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %32 = bitcast <16 x i8> %31 to <8 x i16>
  %33 = shufflevector <8 x i16> %32, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %34 = bitcast i8* %20 to i32*
  %35 = load i32, i32* %34, align 4
  %36 = insertelement <4 x i32> undef, i32 %35, i32 0
  %37 = bitcast <4 x i32> %36 to <16 x i8>
  %38 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %37, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %39 = bitcast <16 x i8> %38 to <8 x i16>
  %40 = shufflevector <8 x i16> %39, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %41 = bitcast <16 x i8> %26 to <8 x i16>
  %42 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %41, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #7
  %43 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %40, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #7
  %44 = add <8 x i16> %42, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %45 = add <8 x i16> %44, %43
  %46 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %33, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #7
  %47 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %40, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #7
  %48 = add <8 x i16> %42, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %49 = sub <8 x i16> %48, %46
  %50 = sub <8 x i16> %49, %47
  %51 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %33, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #7
  %52 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %51, <8 x i16> %42) #7
  %53 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %52, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #7
  %54 = ashr <8 x i16> %45, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %55 = ashr <8 x i16> %50, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %56 = lshr <8 x i16> %53, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %57 = getelementptr inbounds i8, i8* %18, i64 8
  %58 = getelementptr inbounds i8, i8* %21, i64 4
  %59 = getelementptr inbounds i8, i8* %20, i64 4
  %60 = bitcast i8* %57 to i64*
  %61 = load i64, i64* %60, align 1
  %62 = insertelement <2 x i64> undef, i64 %61, i32 0
  %63 = bitcast <2 x i64> %62 to <16 x i8>
  %64 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %63, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %65 = bitcast i8* %58 to i32*
  %66 = load i32, i32* %65, align 4
  %67 = insertelement <4 x i32> undef, i32 %66, i32 0
  %68 = bitcast <4 x i32> %67 to <16 x i8>
  %69 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %68, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %70 = bitcast <16 x i8> %69 to <8 x i16>
  %71 = shufflevector <8 x i16> %70, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %72 = bitcast i8* %59 to i32*
  %73 = load i32, i32* %72, align 4
  %74 = insertelement <4 x i32> undef, i32 %73, i32 0
  %75 = bitcast <4 x i32> %74 to <16 x i8>
  %76 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %75, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %77 = bitcast <16 x i8> %76 to <8 x i16>
  %78 = shufflevector <8 x i16> %77, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %79 = bitcast <16 x i8> %64 to <8 x i16>
  %80 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %79, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #7
  %81 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %78, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #7
  %82 = add <8 x i16> %80, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %83 = add <8 x i16> %82, %81
  %84 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %71, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #7
  %85 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %78, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #7
  %86 = add <8 x i16> %80, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %87 = sub <8 x i16> %86, %84
  %88 = sub <8 x i16> %87, %85
  %89 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %71, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #7
  %90 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %89, <8 x i16> %80) #7
  %91 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %90, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #7
  %92 = ashr <8 x i16> %83, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %93 = ashr <8 x i16> %88, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %94 = lshr <8 x i16> %91, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %95 = getelementptr inbounds i8, i8* %18, i64 16
  %96 = getelementptr inbounds i8, i8* %21, i64 8
  %97 = getelementptr inbounds i8, i8* %20, i64 8
  %98 = bitcast i8* %95 to i64*
  %99 = load i64, i64* %98, align 1
  %100 = insertelement <2 x i64> undef, i64 %99, i32 0
  %101 = bitcast <2 x i64> %100 to <16 x i8>
  %102 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %101, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %103 = bitcast i8* %96 to i32*
  %104 = load i32, i32* %103, align 4
  %105 = insertelement <4 x i32> undef, i32 %104, i32 0
  %106 = bitcast <4 x i32> %105 to <16 x i8>
  %107 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %106, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %108 = bitcast <16 x i8> %107 to <8 x i16>
  %109 = shufflevector <8 x i16> %108, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %110 = bitcast i8* %97 to i32*
  %111 = load i32, i32* %110, align 4
  %112 = insertelement <4 x i32> undef, i32 %111, i32 0
  %113 = bitcast <4 x i32> %112 to <16 x i8>
  %114 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %113, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %115 = bitcast <16 x i8> %114 to <8 x i16>
  %116 = shufflevector <8 x i16> %115, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %117 = bitcast <16 x i8> %102 to <8 x i16>
  %118 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %117, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #7
  %119 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %116, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #7
  %120 = add <8 x i16> %118, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %121 = add <8 x i16> %120, %119
  %122 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %109, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #7
  %123 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %116, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #7
  %124 = add <8 x i16> %118, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %125 = sub <8 x i16> %124, %122
  %126 = sub <8 x i16> %125, %123
  %127 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %109, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #7
  %128 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %127, <8 x i16> %118) #7
  %129 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %128, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #7
  %130 = ashr <8 x i16> %121, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %131 = ashr <8 x i16> %126, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %132 = lshr <8 x i16> %129, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %133 = getelementptr inbounds i8, i8* %18, i64 24
  %134 = getelementptr inbounds i8, i8* %21, i64 12
  %135 = getelementptr inbounds i8, i8* %20, i64 12
  %136 = bitcast i8* %133 to i64*
  %137 = load i64, i64* %136, align 1
  %138 = insertelement <2 x i64> undef, i64 %137, i32 0
  %139 = bitcast <2 x i64> %138 to <16 x i8>
  %140 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %139, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %141 = bitcast i8* %134 to i32*
  %142 = load i32, i32* %141, align 4
  %143 = insertelement <4 x i32> undef, i32 %142, i32 0
  %144 = bitcast <4 x i32> %143 to <16 x i8>
  %145 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %144, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %146 = bitcast <16 x i8> %145 to <8 x i16>
  %147 = shufflevector <8 x i16> %146, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %148 = bitcast i8* %135 to i32*
  %149 = load i32, i32* %148, align 4
  %150 = insertelement <4 x i32> undef, i32 %149, i32 0
  %151 = bitcast <4 x i32> %150 to <16 x i8>
  %152 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %151, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %153 = bitcast <16 x i8> %152 to <8 x i16>
  %154 = shufflevector <8 x i16> %153, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %155 = bitcast <16 x i8> %140 to <8 x i16>
  %156 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %155, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #7
  %157 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %154, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #7
  %158 = add <8 x i16> %156, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %159 = add <8 x i16> %158, %157
  %160 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %147, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #7
  %161 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %154, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #7
  %162 = add <8 x i16> %156, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %163 = sub <8 x i16> %162, %160
  %164 = sub <8 x i16> %163, %161
  %165 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %147, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #7
  %166 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %165, <8 x i16> %156) #7
  %167 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %166, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #7
  %168 = ashr <8 x i16> %159, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %169 = ashr <8 x i16> %164, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %170 = lshr <8 x i16> %167, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %171 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %54, <8 x i16> %92) #7
  %172 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %130, <8 x i16> %168) #7
  %173 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %55, <8 x i16> %93) #7
  %174 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %131, <8 x i16> %169) #7
  %175 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %56, <8 x i16> %94) #7
  %176 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %132, <8 x i16> %170) #7
  %177 = bitcast <16 x i8> %171 to <8 x i16>
  %178 = bitcast <16 x i8> %172 to <8 x i16>
  %179 = and <8 x i16> %177, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %180 = and <8 x i16> %178, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %181 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %179, <8 x i16> %180) #7
  %182 = bitcast <16 x i8> %173 to <8 x i16>
  %183 = bitcast <16 x i8> %174 to <8 x i16>
  %184 = and <8 x i16> %182, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %185 = and <8 x i16> %183, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %186 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %184, <8 x i16> %185) #7
  %187 = bitcast <16 x i8> %175 to <8 x i16>
  %188 = bitcast <16 x i8> %176 to <8 x i16>
  %189 = and <8 x i16> %187, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %190 = and <8 x i16> %188, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %191 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %189, <8 x i16> %190) #7
  %192 = lshr <8 x i16> %177, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %193 = lshr <8 x i16> %178, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %194 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %192, <8 x i16> %193) #7
  %195 = lshr <8 x i16> %182, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %196 = lshr <8 x i16> %183, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %197 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %195, <8 x i16> %196) #7
  %198 = lshr <8 x i16> %187, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %199 = lshr <8 x i16> %188, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %200 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %198, <8 x i16> %199) #7
  %201 = bitcast <16 x i8> %181 to <8 x i16>
  %202 = and <8 x i16> %201, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %203 = bitcast <16 x i8> %186 to <8 x i16>
  %204 = and <8 x i16> %203, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %205 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %202, <8 x i16> %204) #7
  %206 = bitcast <16 x i8> %191 to <8 x i16>
  %207 = and <8 x i16> %206, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %208 = bitcast <16 x i8> %194 to <8 x i16>
  %209 = and <8 x i16> %208, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %210 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %207, <8 x i16> %209) #7
  %211 = bitcast <16 x i8> %197 to <8 x i16>
  %212 = and <8 x i16> %211, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %213 = bitcast <16 x i8> %200 to <8 x i16>
  %214 = and <8 x i16> %213, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %215 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %212, <8 x i16> %214) #7
  %216 = lshr <8 x i16> %201, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %217 = lshr <8 x i16> %203, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %218 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %216, <8 x i16> %217) #7
  %219 = lshr <8 x i16> %206, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %220 = lshr <8 x i16> %208, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %221 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %219, <8 x i16> %220) #7
  %222 = lshr <8 x i16> %211, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %223 = lshr <8 x i16> %213, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %224 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %222, <8 x i16> %223) #7
  %225 = bitcast <16 x i8> %205 to <8 x i16>
  %226 = bitcast <16 x i8> %210 to <8 x i16>
  %227 = and <8 x i16> %225, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %228 = and <8 x i16> %226, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %229 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %227, <8 x i16> %228) #7
  %230 = bitcast <16 x i8> %215 to <8 x i16>
  %231 = bitcast <16 x i8> %218 to <8 x i16>
  %232 = and <8 x i16> %230, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %233 = and <8 x i16> %231, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %234 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %232, <8 x i16> %233) #7
  %235 = bitcast <16 x i8> %221 to <8 x i16>
  %236 = and <8 x i16> %235, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %237 = bitcast <16 x i8> %224 to <8 x i16>
  %238 = and <8 x i16> %237, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %239 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %236, <8 x i16> %238) #7
  %240 = lshr <8 x i16> %225, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %241 = lshr <8 x i16> %226, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %242 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %240, <8 x i16> %241) #7
  %243 = lshr <8 x i16> %230, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %244 = lshr <8 x i16> %231, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %245 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %243, <8 x i16> %244) #7
  %246 = lshr <8 x i16> %235, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %247 = lshr <8 x i16> %237, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %248 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %246, <8 x i16> %247) #7
  %249 = bitcast <16 x i8> %229 to <8 x i16>
  %250 = and <8 x i16> %249, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %251 = bitcast <16 x i8> %234 to <8 x i16>
  %252 = and <8 x i16> %251, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %253 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %250, <8 x i16> %252) #7
  %254 = bitcast <16 x i8> %239 to <8 x i16>
  %255 = and <8 x i16> %254, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %256 = bitcast <16 x i8> %242 to <8 x i16>
  %257 = and <8 x i16> %256, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %258 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %255, <8 x i16> %257) #7
  %259 = bitcast <16 x i8> %245 to <8 x i16>
  %260 = and <8 x i16> %259, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %261 = bitcast <16 x i8> %248 to <8 x i16>
  %262 = and <8 x i16> %261, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %263 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %260, <8 x i16> %262) #7
  %264 = lshr <8 x i16> %249, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %265 = lshr <8 x i16> %251, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %266 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %264, <8 x i16> %265) #7
  %267 = lshr <8 x i16> %254, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %268 = lshr <8 x i16> %256, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %269 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %267, <8 x i16> %268) #7
  %270 = lshr <8 x i16> %259, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %271 = lshr <8 x i16> %261, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %272 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %270, <8 x i16> %271) #7
  %273 = bitcast <16 x i8> %253 to <8 x i16>
  %274 = and <8 x i16> %273, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %275 = bitcast <16 x i8> %258 to <8 x i16>
  %276 = and <8 x i16> %275, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %277 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %274, <8 x i16> %276) #7
  %278 = bitcast <16 x i8> %263 to <8 x i16>
  %279 = and <8 x i16> %278, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %280 = bitcast <16 x i8> %266 to <8 x i16>
  %281 = and <8 x i16> %280, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %282 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %279, <8 x i16> %281) #7
  %283 = bitcast <16 x i8> %269 to <8 x i16>
  %284 = and <8 x i16> %283, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %285 = bitcast <16 x i8> %272 to <8 x i16>
  %286 = and <8 x i16> %285, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %287 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %284, <8 x i16> %286) #7
  %288 = lshr <8 x i16> %273, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %289 = lshr <8 x i16> %275, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %290 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %288, <8 x i16> %289) #7
  %291 = lshr <8 x i16> %278, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %292 = lshr <8 x i16> %280, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %293 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %291, <8 x i16> %292) #7
  %294 = lshr <8 x i16> %283, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %295 = lshr <8 x i16> %285, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %296 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %294, <8 x i16> %295) #7
  %297 = bitcast i8* %19 to <16 x i8>*
  store <16 x i8> %277, <16 x i8>* %297, align 1
  %298 = getelementptr inbounds i8, i8* %19, i64 16
  %299 = bitcast i8* %298 to <16 x i8>*
  store <16 x i8> %282, <16 x i8>* %299, align 1
  %300 = getelementptr inbounds i8, i8* %19, i64 32
  %301 = bitcast i8* %300 to <16 x i8>*
  store <16 x i8> %287, <16 x i8>* %301, align 1
  %302 = getelementptr inbounds i8, i8* %19, i64 48
  %303 = bitcast i8* %302 to <16 x i8>*
  store <16 x i8> %290, <16 x i8>* %303, align 1
  %304 = getelementptr inbounds i8, i8* %19, i64 64
  %305 = bitcast i8* %304 to <16 x i8>*
  store <16 x i8> %293, <16 x i8>* %305, align 1
  %306 = getelementptr inbounds i8, i8* %19, i64 80
  %307 = bitcast i8* %306 to <16 x i8>*
  store <16 x i8> %296, <16 x i8>* %307, align 1
  %308 = getelementptr inbounds i8, i8* %18, i64 32
  %309 = getelementptr inbounds i8, i8* %21, i64 16
  %310 = getelementptr inbounds i8, i8* %20, i64 16
  %311 = getelementptr inbounds i8, i8* %19, i64 96
  %312 = add nuw nsw i32 %17, 32
  %313 = icmp sgt i32 %312, %4
  br i1 %313, label %7, label %16

314:                                              ; preds = %9, %314
  %315 = phi i32 [ %369, %314 ], [ %14, %9 ]
  %316 = phi i8* [ %364, %314 ], [ %13, %9 ]
  %317 = phi i8* [ %363, %314 ], [ %12, %9 ]
  %318 = phi i8* [ %368, %314 ], [ %11, %9 ]
  %319 = phi i8* [ %367, %314 ], [ %10, %9 ]
  %320 = load i8, i8* %316, align 1
  %321 = zext i8 %320 to i32
  %322 = load i8, i8* %319, align 1
  %323 = zext i8 %322 to i32
  %324 = load i8, i8* %318, align 1
  %325 = zext i8 %324 to i32
  %326 = mul nuw nsw i32 %321, 19077
  %327 = lshr i32 %326, 8
  %328 = mul nuw nsw i32 %325, 26149
  %329 = lshr i32 %328, 8
  %330 = add nsw i32 %327, -14234
  %331 = add nsw i32 %330, %329
  %332 = icmp ult i32 %331, 16384
  %333 = lshr i32 %331, 6
  %334 = icmp slt i32 %331, 0
  %335 = select i1 %334, i32 0, i32 255
  %336 = select i1 %332, i32 %333, i32 %335
  %337 = trunc i32 %336 to i8
  store i8 %337, i8* %317, align 1
  %338 = mul nuw nsw i32 %323, 6419
  %339 = lshr i32 %338, 8
  %340 = mul nuw nsw i32 %325, 13320
  %341 = lshr i32 %340, 8
  %342 = add nuw nsw i32 %327, 8708
  %343 = sub nuw nsw i32 %342, %339
  %344 = sub nsw i32 %343, %341
  %345 = icmp ult i32 %344, 16384
  %346 = lshr i32 %344, 6
  %347 = icmp slt i32 %344, 0
  %348 = select i1 %347, i32 0, i32 255
  %349 = select i1 %345, i32 %346, i32 %348
  %350 = trunc i32 %349 to i8
  %351 = getelementptr inbounds i8, i8* %317, i64 1
  store i8 %350, i8* %351, align 1
  %352 = mul nuw nsw i32 %323, 33050
  %353 = lshr i32 %352, 8
  %354 = add nsw i32 %327, -17685
  %355 = add nsw i32 %354, %353
  %356 = icmp ult i32 %355, 16384
  %357 = lshr i32 %355, 6
  %358 = icmp slt i32 %355, 0
  %359 = select i1 %358, i32 0, i32 255
  %360 = select i1 %356, i32 %357, i32 %359
  %361 = trunc i32 %360 to i8
  %362 = getelementptr inbounds i8, i8* %317, i64 2
  store i8 %361, i8* %362, align 1
  %363 = getelementptr inbounds i8, i8* %317, i64 3
  %364 = getelementptr inbounds i8, i8* %316, i64 1
  %365 = and i32 %315, 1
  %366 = zext i32 %365 to i64
  %367 = getelementptr inbounds i8, i8* %319, i64 %366
  %368 = getelementptr inbounds i8, i8* %318, i64 %366
  %369 = add nuw nsw i32 %315, 1
  %370 = icmp eq i32 %369, %4
  br i1 %370, label %371, label %314

371:                                              ; preds = %314, %9
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @YuvToRgbaRow_SSE2(i8* nocapture readonly, i8* nocapture readonly, i8* nocapture readonly, i8* nocapture, i32) #0 {
  %6 = icmp slt i32 %4, 8
  br i1 %6, label %9, label %16

7:                                                ; preds = %16
  %8 = and i32 %4, -8
  br label %9

9:                                                ; preds = %7, %5
  %10 = phi i8* [ %1, %5 ], [ %69, %7 ]
  %11 = phi i8* [ %2, %5 ], [ %70, %7 ]
  %12 = phi i8* [ %3, %5 ], [ %71, %7 ]
  %13 = phi i8* [ %0, %5 ], [ %68, %7 ]
  %14 = phi i32 [ 0, %5 ], [ %8, %7 ]
  %15 = icmp slt i32 %14, %4
  br i1 %15, label %74, label %132

16:                                               ; preds = %5, %16
  %17 = phi i32 [ %72, %16 ], [ 8, %5 ]
  %18 = phi i8* [ %68, %16 ], [ %0, %5 ]
  %19 = phi i8* [ %71, %16 ], [ %3, %5 ]
  %20 = phi i8* [ %70, %16 ], [ %2, %5 ]
  %21 = phi i8* [ %69, %16 ], [ %1, %5 ]
  %22 = bitcast i8* %18 to i64*
  %23 = load i64, i64* %22, align 1
  %24 = insertelement <2 x i64> undef, i64 %23, i32 0
  %25 = bitcast <2 x i64> %24 to <16 x i8>
  %26 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %25, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %27 = bitcast i8* %21 to i32*
  %28 = load i32, i32* %27, align 4
  %29 = insertelement <4 x i32> undef, i32 %28, i32 0
  %30 = bitcast <4 x i32> %29 to <16 x i8>
  %31 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %30, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %32 = bitcast <16 x i8> %31 to <8 x i16>
  %33 = shufflevector <8 x i16> %32, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %34 = bitcast i8* %20 to i32*
  %35 = load i32, i32* %34, align 4
  %36 = insertelement <4 x i32> undef, i32 %35, i32 0
  %37 = bitcast <4 x i32> %36 to <16 x i8>
  %38 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %37, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %39 = bitcast <16 x i8> %38 to <8 x i16>
  %40 = shufflevector <8 x i16> %39, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %41 = bitcast <16 x i8> %26 to <8 x i16>
  %42 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %41, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #7
  %43 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %40, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #7
  %44 = add <8 x i16> %42, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %45 = add <8 x i16> %44, %43
  %46 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %33, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #7
  %47 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %40, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #7
  %48 = add <8 x i16> %42, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %49 = sub <8 x i16> %48, %46
  %50 = sub <8 x i16> %49, %47
  %51 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %33, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #7
  %52 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %51, <8 x i16> %42) #7
  %53 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %52, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #7
  %54 = ashr <8 x i16> %45, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %55 = ashr <8 x i16> %50, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %56 = lshr <8 x i16> %53, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %57 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %54, <8 x i16> %56) #7
  %58 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %55, <8 x i16> <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>) #7
  %59 = shufflevector <16 x i8> %57, <16 x i8> %58, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %60 = shufflevector <16 x i8> %57, <16 x i8> %58, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %61 = bitcast <16 x i8> %59 to <8 x i16>
  %62 = bitcast <16 x i8> %60 to <8 x i16>
  %63 = shufflevector <8 x i16> %61, <8 x i16> %62, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %64 = shufflevector <8 x i16> %61, <8 x i16> %62, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %65 = bitcast i8* %19 to <8 x i16>*
  store <8 x i16> %63, <8 x i16>* %65, align 1
  %66 = getelementptr inbounds i8, i8* %19, i64 16
  %67 = bitcast i8* %66 to <8 x i16>*
  store <8 x i16> %64, <8 x i16>* %67, align 1
  %68 = getelementptr inbounds i8, i8* %18, i64 8
  %69 = getelementptr inbounds i8, i8* %21, i64 4
  %70 = getelementptr inbounds i8, i8* %20, i64 4
  %71 = getelementptr inbounds i8, i8* %19, i64 32
  %72 = add nuw nsw i32 %17, 8
  %73 = icmp sgt i32 %72, %4
  br i1 %73, label %7, label %16

74:                                               ; preds = %9, %74
  %75 = phi i32 [ %130, %74 ], [ %14, %9 ]
  %76 = phi i8* [ %125, %74 ], [ %13, %9 ]
  %77 = phi i8* [ %124, %74 ], [ %12, %9 ]
  %78 = phi i8* [ %129, %74 ], [ %11, %9 ]
  %79 = phi i8* [ %128, %74 ], [ %10, %9 ]
  %80 = load i8, i8* %76, align 1
  %81 = load i8, i8* %79, align 1
  %82 = load i8, i8* %78, align 1
  %83 = zext i8 %80 to i32
  %84 = zext i8 %81 to i32
  %85 = zext i8 %82 to i32
  %86 = mul nuw nsw i32 %83, 19077
  %87 = lshr i32 %86, 8
  %88 = mul nuw nsw i32 %85, 26149
  %89 = lshr i32 %88, 8
  %90 = add nsw i32 %87, -14234
  %91 = add nsw i32 %90, %89
  %92 = icmp ult i32 %91, 16384
  %93 = lshr i32 %91, 6
  %94 = icmp slt i32 %91, 0
  %95 = select i1 %94, i32 0, i32 255
  %96 = select i1 %92, i32 %93, i32 %95
  %97 = trunc i32 %96 to i8
  store i8 %97, i8* %77, align 1
  %98 = mul nuw nsw i32 %84, 6419
  %99 = lshr i32 %98, 8
  %100 = mul nuw nsw i32 %85, 13320
  %101 = lshr i32 %100, 8
  %102 = add nuw nsw i32 %87, 8708
  %103 = sub nuw nsw i32 %102, %99
  %104 = sub nsw i32 %103, %101
  %105 = icmp ult i32 %104, 16384
  %106 = lshr i32 %104, 6
  %107 = icmp slt i32 %104, 0
  %108 = select i1 %107, i32 0, i32 255
  %109 = select i1 %105, i32 %106, i32 %108
  %110 = trunc i32 %109 to i8
  %111 = getelementptr inbounds i8, i8* %77, i64 1
  store i8 %110, i8* %111, align 1
  %112 = mul nuw nsw i32 %84, 33050
  %113 = lshr i32 %112, 8
  %114 = add nsw i32 %87, -17685
  %115 = add nsw i32 %114, %113
  %116 = icmp ult i32 %115, 16384
  %117 = lshr i32 %115, 6
  %118 = icmp slt i32 %115, 0
  %119 = select i1 %118, i32 0, i32 255
  %120 = select i1 %116, i32 %117, i32 %119
  %121 = trunc i32 %120 to i8
  %122 = getelementptr inbounds i8, i8* %77, i64 2
  store i8 %121, i8* %122, align 1
  %123 = getelementptr inbounds i8, i8* %77, i64 3
  store i8 -1, i8* %123, align 1
  %124 = getelementptr inbounds i8, i8* %77, i64 4
  %125 = getelementptr inbounds i8, i8* %76, i64 1
  %126 = and i32 %75, 1
  %127 = zext i32 %126 to i64
  %128 = getelementptr inbounds i8, i8* %79, i64 %127
  %129 = getelementptr inbounds i8, i8* %78, i64 %127
  %130 = add nuw nsw i32 %75, 1
  %131 = icmp eq i32 %130, %4
  br i1 %131, label %132, label %74

132:                                              ; preds = %74, %9
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @YuvToBgrRow_SSE2(i8* nocapture readonly, i8* nocapture readonly, i8* nocapture readonly, i8* nocapture, i32) #0 {
  %6 = icmp slt i32 %4, 32
  br i1 %6, label %9, label %16

7:                                                ; preds = %16
  %8 = and i32 %4, -32
  br label %9

9:                                                ; preds = %7, %5
  %10 = phi i8* [ %1, %5 ], [ %309, %7 ]
  %11 = phi i8* [ %2, %5 ], [ %310, %7 ]
  %12 = phi i8* [ %3, %5 ], [ %311, %7 ]
  %13 = phi i8* [ %0, %5 ], [ %308, %7 ]
  %14 = phi i32 [ 0, %5 ], [ %8, %7 ]
  %15 = icmp slt i32 %14, %4
  br i1 %15, label %314, label %371

16:                                               ; preds = %5, %16
  %17 = phi i32 [ %312, %16 ], [ 32, %5 ]
  %18 = phi i8* [ %308, %16 ], [ %0, %5 ]
  %19 = phi i8* [ %311, %16 ], [ %3, %5 ]
  %20 = phi i8* [ %310, %16 ], [ %2, %5 ]
  %21 = phi i8* [ %309, %16 ], [ %1, %5 ]
  %22 = bitcast i8* %18 to i64*
  %23 = load i64, i64* %22, align 1
  %24 = insertelement <2 x i64> undef, i64 %23, i32 0
  %25 = bitcast <2 x i64> %24 to <16 x i8>
  %26 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %25, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %27 = bitcast i8* %21 to i32*
  %28 = load i32, i32* %27, align 4
  %29 = insertelement <4 x i32> undef, i32 %28, i32 0
  %30 = bitcast <4 x i32> %29 to <16 x i8>
  %31 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %30, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %32 = bitcast <16 x i8> %31 to <8 x i16>
  %33 = shufflevector <8 x i16> %32, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %34 = bitcast i8* %20 to i32*
  %35 = load i32, i32* %34, align 4
  %36 = insertelement <4 x i32> undef, i32 %35, i32 0
  %37 = bitcast <4 x i32> %36 to <16 x i8>
  %38 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %37, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %39 = bitcast <16 x i8> %38 to <8 x i16>
  %40 = shufflevector <8 x i16> %39, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %41 = bitcast <16 x i8> %26 to <8 x i16>
  %42 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %41, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #7
  %43 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %40, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #7
  %44 = add <8 x i16> %42, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %45 = add <8 x i16> %44, %43
  %46 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %33, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #7
  %47 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %40, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #7
  %48 = add <8 x i16> %42, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %49 = sub <8 x i16> %48, %46
  %50 = sub <8 x i16> %49, %47
  %51 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %33, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #7
  %52 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %51, <8 x i16> %42) #7
  %53 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %52, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #7
  %54 = ashr <8 x i16> %45, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %55 = ashr <8 x i16> %50, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %56 = lshr <8 x i16> %53, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %57 = getelementptr inbounds i8, i8* %18, i64 8
  %58 = getelementptr inbounds i8, i8* %21, i64 4
  %59 = getelementptr inbounds i8, i8* %20, i64 4
  %60 = bitcast i8* %57 to i64*
  %61 = load i64, i64* %60, align 1
  %62 = insertelement <2 x i64> undef, i64 %61, i32 0
  %63 = bitcast <2 x i64> %62 to <16 x i8>
  %64 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %63, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %65 = bitcast i8* %58 to i32*
  %66 = load i32, i32* %65, align 4
  %67 = insertelement <4 x i32> undef, i32 %66, i32 0
  %68 = bitcast <4 x i32> %67 to <16 x i8>
  %69 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %68, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %70 = bitcast <16 x i8> %69 to <8 x i16>
  %71 = shufflevector <8 x i16> %70, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %72 = bitcast i8* %59 to i32*
  %73 = load i32, i32* %72, align 4
  %74 = insertelement <4 x i32> undef, i32 %73, i32 0
  %75 = bitcast <4 x i32> %74 to <16 x i8>
  %76 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %75, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %77 = bitcast <16 x i8> %76 to <8 x i16>
  %78 = shufflevector <8 x i16> %77, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %79 = bitcast <16 x i8> %64 to <8 x i16>
  %80 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %79, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #7
  %81 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %78, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #7
  %82 = add <8 x i16> %80, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %83 = add <8 x i16> %82, %81
  %84 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %71, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #7
  %85 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %78, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #7
  %86 = add <8 x i16> %80, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %87 = sub <8 x i16> %86, %84
  %88 = sub <8 x i16> %87, %85
  %89 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %71, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #7
  %90 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %89, <8 x i16> %80) #7
  %91 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %90, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #7
  %92 = ashr <8 x i16> %83, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %93 = ashr <8 x i16> %88, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %94 = lshr <8 x i16> %91, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %95 = getelementptr inbounds i8, i8* %18, i64 16
  %96 = getelementptr inbounds i8, i8* %21, i64 8
  %97 = getelementptr inbounds i8, i8* %20, i64 8
  %98 = bitcast i8* %95 to i64*
  %99 = load i64, i64* %98, align 1
  %100 = insertelement <2 x i64> undef, i64 %99, i32 0
  %101 = bitcast <2 x i64> %100 to <16 x i8>
  %102 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %101, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %103 = bitcast i8* %96 to i32*
  %104 = load i32, i32* %103, align 4
  %105 = insertelement <4 x i32> undef, i32 %104, i32 0
  %106 = bitcast <4 x i32> %105 to <16 x i8>
  %107 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %106, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %108 = bitcast <16 x i8> %107 to <8 x i16>
  %109 = shufflevector <8 x i16> %108, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %110 = bitcast i8* %97 to i32*
  %111 = load i32, i32* %110, align 4
  %112 = insertelement <4 x i32> undef, i32 %111, i32 0
  %113 = bitcast <4 x i32> %112 to <16 x i8>
  %114 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %113, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %115 = bitcast <16 x i8> %114 to <8 x i16>
  %116 = shufflevector <8 x i16> %115, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %117 = bitcast <16 x i8> %102 to <8 x i16>
  %118 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %117, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #7
  %119 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %116, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #7
  %120 = add <8 x i16> %118, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %121 = add <8 x i16> %120, %119
  %122 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %109, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #7
  %123 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %116, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #7
  %124 = add <8 x i16> %118, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %125 = sub <8 x i16> %124, %122
  %126 = sub <8 x i16> %125, %123
  %127 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %109, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #7
  %128 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %127, <8 x i16> %118) #7
  %129 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %128, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #7
  %130 = ashr <8 x i16> %121, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %131 = ashr <8 x i16> %126, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %132 = lshr <8 x i16> %129, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %133 = getelementptr inbounds i8, i8* %18, i64 24
  %134 = getelementptr inbounds i8, i8* %21, i64 12
  %135 = getelementptr inbounds i8, i8* %20, i64 12
  %136 = bitcast i8* %133 to i64*
  %137 = load i64, i64* %136, align 1
  %138 = insertelement <2 x i64> undef, i64 %137, i32 0
  %139 = bitcast <2 x i64> %138 to <16 x i8>
  %140 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %139, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %141 = bitcast i8* %134 to i32*
  %142 = load i32, i32* %141, align 4
  %143 = insertelement <4 x i32> undef, i32 %142, i32 0
  %144 = bitcast <4 x i32> %143 to <16 x i8>
  %145 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %144, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %146 = bitcast <16 x i8> %145 to <8 x i16>
  %147 = shufflevector <8 x i16> %146, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %148 = bitcast i8* %135 to i32*
  %149 = load i32, i32* %148, align 4
  %150 = insertelement <4 x i32> undef, i32 %149, i32 0
  %151 = bitcast <4 x i32> %150 to <16 x i8>
  %152 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %151, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %153 = bitcast <16 x i8> %152 to <8 x i16>
  %154 = shufflevector <8 x i16> %153, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %155 = bitcast <16 x i8> %140 to <8 x i16>
  %156 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %155, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #7
  %157 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %154, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #7
  %158 = add <8 x i16> %156, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %159 = add <8 x i16> %158, %157
  %160 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %147, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #7
  %161 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %154, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #7
  %162 = add <8 x i16> %156, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %163 = sub <8 x i16> %162, %160
  %164 = sub <8 x i16> %163, %161
  %165 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %147, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #7
  %166 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %165, <8 x i16> %156) #7
  %167 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %166, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #7
  %168 = ashr <8 x i16> %159, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %169 = ashr <8 x i16> %164, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %170 = lshr <8 x i16> %167, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %171 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %56, <8 x i16> %94) #7
  %172 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %132, <8 x i16> %170) #7
  %173 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %55, <8 x i16> %93) #7
  %174 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %131, <8 x i16> %169) #7
  %175 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %54, <8 x i16> %92) #7
  %176 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %130, <8 x i16> %168) #7
  %177 = bitcast <16 x i8> %171 to <8 x i16>
  %178 = bitcast <16 x i8> %172 to <8 x i16>
  %179 = and <8 x i16> %177, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %180 = and <8 x i16> %178, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %181 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %179, <8 x i16> %180) #7
  %182 = bitcast <16 x i8> %173 to <8 x i16>
  %183 = bitcast <16 x i8> %174 to <8 x i16>
  %184 = and <8 x i16> %182, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %185 = and <8 x i16> %183, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %186 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %184, <8 x i16> %185) #7
  %187 = bitcast <16 x i8> %175 to <8 x i16>
  %188 = bitcast <16 x i8> %176 to <8 x i16>
  %189 = and <8 x i16> %187, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %190 = and <8 x i16> %188, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %191 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %189, <8 x i16> %190) #7
  %192 = lshr <8 x i16> %177, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %193 = lshr <8 x i16> %178, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %194 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %192, <8 x i16> %193) #7
  %195 = lshr <8 x i16> %182, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %196 = lshr <8 x i16> %183, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %197 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %195, <8 x i16> %196) #7
  %198 = lshr <8 x i16> %187, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %199 = lshr <8 x i16> %188, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %200 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %198, <8 x i16> %199) #7
  %201 = bitcast <16 x i8> %181 to <8 x i16>
  %202 = and <8 x i16> %201, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %203 = bitcast <16 x i8> %186 to <8 x i16>
  %204 = and <8 x i16> %203, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %205 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %202, <8 x i16> %204) #7
  %206 = bitcast <16 x i8> %191 to <8 x i16>
  %207 = and <8 x i16> %206, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %208 = bitcast <16 x i8> %194 to <8 x i16>
  %209 = and <8 x i16> %208, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %210 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %207, <8 x i16> %209) #7
  %211 = bitcast <16 x i8> %197 to <8 x i16>
  %212 = and <8 x i16> %211, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %213 = bitcast <16 x i8> %200 to <8 x i16>
  %214 = and <8 x i16> %213, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %215 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %212, <8 x i16> %214) #7
  %216 = lshr <8 x i16> %201, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %217 = lshr <8 x i16> %203, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %218 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %216, <8 x i16> %217) #7
  %219 = lshr <8 x i16> %206, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %220 = lshr <8 x i16> %208, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %221 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %219, <8 x i16> %220) #7
  %222 = lshr <8 x i16> %211, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %223 = lshr <8 x i16> %213, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %224 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %222, <8 x i16> %223) #7
  %225 = bitcast <16 x i8> %205 to <8 x i16>
  %226 = bitcast <16 x i8> %210 to <8 x i16>
  %227 = and <8 x i16> %225, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %228 = and <8 x i16> %226, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %229 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %227, <8 x i16> %228) #7
  %230 = bitcast <16 x i8> %215 to <8 x i16>
  %231 = bitcast <16 x i8> %218 to <8 x i16>
  %232 = and <8 x i16> %230, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %233 = and <8 x i16> %231, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %234 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %232, <8 x i16> %233) #7
  %235 = bitcast <16 x i8> %221 to <8 x i16>
  %236 = and <8 x i16> %235, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %237 = bitcast <16 x i8> %224 to <8 x i16>
  %238 = and <8 x i16> %237, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %239 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %236, <8 x i16> %238) #7
  %240 = lshr <8 x i16> %225, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %241 = lshr <8 x i16> %226, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %242 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %240, <8 x i16> %241) #7
  %243 = lshr <8 x i16> %230, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %244 = lshr <8 x i16> %231, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %245 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %243, <8 x i16> %244) #7
  %246 = lshr <8 x i16> %235, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %247 = lshr <8 x i16> %237, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %248 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %246, <8 x i16> %247) #7
  %249 = bitcast <16 x i8> %229 to <8 x i16>
  %250 = and <8 x i16> %249, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %251 = bitcast <16 x i8> %234 to <8 x i16>
  %252 = and <8 x i16> %251, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %253 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %250, <8 x i16> %252) #7
  %254 = bitcast <16 x i8> %239 to <8 x i16>
  %255 = and <8 x i16> %254, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %256 = bitcast <16 x i8> %242 to <8 x i16>
  %257 = and <8 x i16> %256, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %258 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %255, <8 x i16> %257) #7
  %259 = bitcast <16 x i8> %245 to <8 x i16>
  %260 = and <8 x i16> %259, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %261 = bitcast <16 x i8> %248 to <8 x i16>
  %262 = and <8 x i16> %261, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %263 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %260, <8 x i16> %262) #7
  %264 = lshr <8 x i16> %249, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %265 = lshr <8 x i16> %251, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %266 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %264, <8 x i16> %265) #7
  %267 = lshr <8 x i16> %254, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %268 = lshr <8 x i16> %256, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %269 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %267, <8 x i16> %268) #7
  %270 = lshr <8 x i16> %259, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %271 = lshr <8 x i16> %261, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %272 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %270, <8 x i16> %271) #7
  %273 = bitcast <16 x i8> %253 to <8 x i16>
  %274 = and <8 x i16> %273, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %275 = bitcast <16 x i8> %258 to <8 x i16>
  %276 = and <8 x i16> %275, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %277 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %274, <8 x i16> %276) #7
  %278 = bitcast <16 x i8> %263 to <8 x i16>
  %279 = and <8 x i16> %278, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %280 = bitcast <16 x i8> %266 to <8 x i16>
  %281 = and <8 x i16> %280, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %282 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %279, <8 x i16> %281) #7
  %283 = bitcast <16 x i8> %269 to <8 x i16>
  %284 = and <8 x i16> %283, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %285 = bitcast <16 x i8> %272 to <8 x i16>
  %286 = and <8 x i16> %285, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %287 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %284, <8 x i16> %286) #7
  %288 = lshr <8 x i16> %273, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %289 = lshr <8 x i16> %275, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %290 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %288, <8 x i16> %289) #7
  %291 = lshr <8 x i16> %278, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %292 = lshr <8 x i16> %280, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %293 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %291, <8 x i16> %292) #7
  %294 = lshr <8 x i16> %283, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %295 = lshr <8 x i16> %285, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %296 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %294, <8 x i16> %295) #7
  %297 = bitcast i8* %19 to <16 x i8>*
  store <16 x i8> %277, <16 x i8>* %297, align 1
  %298 = getelementptr inbounds i8, i8* %19, i64 16
  %299 = bitcast i8* %298 to <16 x i8>*
  store <16 x i8> %282, <16 x i8>* %299, align 1
  %300 = getelementptr inbounds i8, i8* %19, i64 32
  %301 = bitcast i8* %300 to <16 x i8>*
  store <16 x i8> %287, <16 x i8>* %301, align 1
  %302 = getelementptr inbounds i8, i8* %19, i64 48
  %303 = bitcast i8* %302 to <16 x i8>*
  store <16 x i8> %290, <16 x i8>* %303, align 1
  %304 = getelementptr inbounds i8, i8* %19, i64 64
  %305 = bitcast i8* %304 to <16 x i8>*
  store <16 x i8> %293, <16 x i8>* %305, align 1
  %306 = getelementptr inbounds i8, i8* %19, i64 80
  %307 = bitcast i8* %306 to <16 x i8>*
  store <16 x i8> %296, <16 x i8>* %307, align 1
  %308 = getelementptr inbounds i8, i8* %18, i64 32
  %309 = getelementptr inbounds i8, i8* %21, i64 16
  %310 = getelementptr inbounds i8, i8* %20, i64 16
  %311 = getelementptr inbounds i8, i8* %19, i64 96
  %312 = add nuw nsw i32 %17, 32
  %313 = icmp sgt i32 %312, %4
  br i1 %313, label %7, label %16

314:                                              ; preds = %9, %314
  %315 = phi i32 [ %369, %314 ], [ %14, %9 ]
  %316 = phi i8* [ %364, %314 ], [ %13, %9 ]
  %317 = phi i8* [ %363, %314 ], [ %12, %9 ]
  %318 = phi i8* [ %368, %314 ], [ %11, %9 ]
  %319 = phi i8* [ %367, %314 ], [ %10, %9 ]
  %320 = load i8, i8* %316, align 1
  %321 = zext i8 %320 to i32
  %322 = load i8, i8* %319, align 1
  %323 = zext i8 %322 to i32
  %324 = load i8, i8* %318, align 1
  %325 = zext i8 %324 to i32
  %326 = mul nuw nsw i32 %321, 19077
  %327 = lshr i32 %326, 8
  %328 = mul nuw nsw i32 %323, 33050
  %329 = lshr i32 %328, 8
  %330 = add nsw i32 %327, -17685
  %331 = add nsw i32 %330, %329
  %332 = icmp ult i32 %331, 16384
  %333 = lshr i32 %331, 6
  %334 = icmp slt i32 %331, 0
  %335 = select i1 %334, i32 0, i32 255
  %336 = select i1 %332, i32 %333, i32 %335
  %337 = trunc i32 %336 to i8
  store i8 %337, i8* %317, align 1
  %338 = mul nuw nsw i32 %323, 6419
  %339 = lshr i32 %338, 8
  %340 = mul nuw nsw i32 %325, 13320
  %341 = lshr i32 %340, 8
  %342 = add nuw nsw i32 %327, 8708
  %343 = sub nuw nsw i32 %342, %339
  %344 = sub nsw i32 %343, %341
  %345 = icmp ult i32 %344, 16384
  %346 = lshr i32 %344, 6
  %347 = icmp slt i32 %344, 0
  %348 = select i1 %347, i32 0, i32 255
  %349 = select i1 %345, i32 %346, i32 %348
  %350 = trunc i32 %349 to i8
  %351 = getelementptr inbounds i8, i8* %317, i64 1
  store i8 %350, i8* %351, align 1
  %352 = mul nuw nsw i32 %325, 26149
  %353 = lshr i32 %352, 8
  %354 = add nsw i32 %327, -14234
  %355 = add nsw i32 %354, %353
  %356 = icmp ult i32 %355, 16384
  %357 = lshr i32 %355, 6
  %358 = icmp slt i32 %355, 0
  %359 = select i1 %358, i32 0, i32 255
  %360 = select i1 %356, i32 %357, i32 %359
  %361 = trunc i32 %360 to i8
  %362 = getelementptr inbounds i8, i8* %317, i64 2
  store i8 %361, i8* %362, align 1
  %363 = getelementptr inbounds i8, i8* %317, i64 3
  %364 = getelementptr inbounds i8, i8* %316, i64 1
  %365 = and i32 %315, 1
  %366 = zext i32 %365 to i64
  %367 = getelementptr inbounds i8, i8* %319, i64 %366
  %368 = getelementptr inbounds i8, i8* %318, i64 %366
  %369 = add nuw nsw i32 %315, 1
  %370 = icmp eq i32 %369, %4
  br i1 %370, label %371, label %314

371:                                              ; preds = %314, %9
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @YuvToBgraRow_SSE2(i8* nocapture readonly, i8* nocapture readonly, i8* nocapture readonly, i8* nocapture, i32) #0 {
  %6 = icmp slt i32 %4, 8
  br i1 %6, label %9, label %16

7:                                                ; preds = %16
  %8 = and i32 %4, -8
  br label %9

9:                                                ; preds = %7, %5
  %10 = phi i8* [ %1, %5 ], [ %69, %7 ]
  %11 = phi i8* [ %2, %5 ], [ %70, %7 ]
  %12 = phi i8* [ %3, %5 ], [ %71, %7 ]
  %13 = phi i8* [ %0, %5 ], [ %68, %7 ]
  %14 = phi i32 [ 0, %5 ], [ %8, %7 ]
  %15 = icmp slt i32 %14, %4
  br i1 %15, label %74, label %132

16:                                               ; preds = %5, %16
  %17 = phi i32 [ %72, %16 ], [ 8, %5 ]
  %18 = phi i8* [ %68, %16 ], [ %0, %5 ]
  %19 = phi i8* [ %71, %16 ], [ %3, %5 ]
  %20 = phi i8* [ %70, %16 ], [ %2, %5 ]
  %21 = phi i8* [ %69, %16 ], [ %1, %5 ]
  %22 = bitcast i8* %18 to i64*
  %23 = load i64, i64* %22, align 1
  %24 = insertelement <2 x i64> undef, i64 %23, i32 0
  %25 = bitcast <2 x i64> %24 to <16 x i8>
  %26 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %25, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %27 = bitcast i8* %21 to i32*
  %28 = load i32, i32* %27, align 4
  %29 = insertelement <4 x i32> undef, i32 %28, i32 0
  %30 = bitcast <4 x i32> %29 to <16 x i8>
  %31 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %30, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %32 = bitcast <16 x i8> %31 to <8 x i16>
  %33 = shufflevector <8 x i16> %32, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %34 = bitcast i8* %20 to i32*
  %35 = load i32, i32* %34, align 4
  %36 = insertelement <4 x i32> undef, i32 %35, i32 0
  %37 = bitcast <4 x i32> %36 to <16 x i8>
  %38 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %37, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %39 = bitcast <16 x i8> %38 to <8 x i16>
  %40 = shufflevector <8 x i16> %39, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %41 = bitcast <16 x i8> %26 to <8 x i16>
  %42 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %41, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #7
  %43 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %40, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #7
  %44 = add <8 x i16> %42, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %45 = add <8 x i16> %44, %43
  %46 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %33, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #7
  %47 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %40, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #7
  %48 = add <8 x i16> %42, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %49 = sub <8 x i16> %48, %46
  %50 = sub <8 x i16> %49, %47
  %51 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %33, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #7
  %52 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %51, <8 x i16> %42) #7
  %53 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %52, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #7
  %54 = ashr <8 x i16> %45, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %55 = ashr <8 x i16> %50, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %56 = lshr <8 x i16> %53, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %57 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %56, <8 x i16> %54) #7
  %58 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %55, <8 x i16> <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>) #7
  %59 = shufflevector <16 x i8> %57, <16 x i8> %58, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %60 = shufflevector <16 x i8> %57, <16 x i8> %58, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %61 = bitcast <16 x i8> %59 to <8 x i16>
  %62 = bitcast <16 x i8> %60 to <8 x i16>
  %63 = shufflevector <8 x i16> %61, <8 x i16> %62, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %64 = shufflevector <8 x i16> %61, <8 x i16> %62, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %65 = bitcast i8* %19 to <8 x i16>*
  store <8 x i16> %63, <8 x i16>* %65, align 1
  %66 = getelementptr inbounds i8, i8* %19, i64 16
  %67 = bitcast i8* %66 to <8 x i16>*
  store <8 x i16> %64, <8 x i16>* %67, align 1
  %68 = getelementptr inbounds i8, i8* %18, i64 8
  %69 = getelementptr inbounds i8, i8* %21, i64 4
  %70 = getelementptr inbounds i8, i8* %20, i64 4
  %71 = getelementptr inbounds i8, i8* %19, i64 32
  %72 = add nuw nsw i32 %17, 8
  %73 = icmp sgt i32 %72, %4
  br i1 %73, label %7, label %16

74:                                               ; preds = %9, %74
  %75 = phi i32 [ %130, %74 ], [ %14, %9 ]
  %76 = phi i8* [ %125, %74 ], [ %13, %9 ]
  %77 = phi i8* [ %124, %74 ], [ %12, %9 ]
  %78 = phi i8* [ %129, %74 ], [ %11, %9 ]
  %79 = phi i8* [ %128, %74 ], [ %10, %9 ]
  %80 = load i8, i8* %76, align 1
  %81 = load i8, i8* %79, align 1
  %82 = load i8, i8* %78, align 1
  %83 = zext i8 %80 to i32
  %84 = zext i8 %81 to i32
  %85 = zext i8 %82 to i32
  %86 = mul nuw nsw i32 %83, 19077
  %87 = lshr i32 %86, 8
  %88 = mul nuw nsw i32 %84, 33050
  %89 = lshr i32 %88, 8
  %90 = add nsw i32 %87, -17685
  %91 = add nsw i32 %90, %89
  %92 = icmp ult i32 %91, 16384
  %93 = lshr i32 %91, 6
  %94 = icmp slt i32 %91, 0
  %95 = select i1 %94, i32 0, i32 255
  %96 = select i1 %92, i32 %93, i32 %95
  %97 = trunc i32 %96 to i8
  store i8 %97, i8* %77, align 1
  %98 = mul nuw nsw i32 %84, 6419
  %99 = lshr i32 %98, 8
  %100 = mul nuw nsw i32 %85, 13320
  %101 = lshr i32 %100, 8
  %102 = add nuw nsw i32 %87, 8708
  %103 = sub nuw nsw i32 %102, %99
  %104 = sub nsw i32 %103, %101
  %105 = icmp ult i32 %104, 16384
  %106 = lshr i32 %104, 6
  %107 = icmp slt i32 %104, 0
  %108 = select i1 %107, i32 0, i32 255
  %109 = select i1 %105, i32 %106, i32 %108
  %110 = trunc i32 %109 to i8
  %111 = getelementptr inbounds i8, i8* %77, i64 1
  store i8 %110, i8* %111, align 1
  %112 = mul nuw nsw i32 %85, 26149
  %113 = lshr i32 %112, 8
  %114 = add nsw i32 %87, -14234
  %115 = add nsw i32 %114, %113
  %116 = icmp ult i32 %115, 16384
  %117 = lshr i32 %115, 6
  %118 = icmp slt i32 %115, 0
  %119 = select i1 %118, i32 0, i32 255
  %120 = select i1 %116, i32 %117, i32 %119
  %121 = trunc i32 %120 to i8
  %122 = getelementptr inbounds i8, i8* %77, i64 2
  store i8 %121, i8* %122, align 1
  %123 = getelementptr inbounds i8, i8* %77, i64 3
  store i8 -1, i8* %123, align 1
  %124 = getelementptr inbounds i8, i8* %77, i64 4
  %125 = getelementptr inbounds i8, i8* %76, i64 1
  %126 = and i32 %75, 1
  %127 = zext i32 %126 to i64
  %128 = getelementptr inbounds i8, i8* %79, i64 %127
  %129 = getelementptr inbounds i8, i8* %78, i64 %127
  %130 = add nuw nsw i32 %75, 1
  %131 = icmp eq i32 %130, %4
  br i1 %131, label %132, label %74

132:                                              ; preds = %74, %9
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @YuvToArgbRow_SSE2(i8* nocapture readonly, i8* nocapture readonly, i8* nocapture readonly, i8* nocapture, i32) #0 {
  %6 = icmp slt i32 %4, 8
  br i1 %6, label %9, label %16

7:                                                ; preds = %16
  %8 = and i32 %4, -8
  br label %9

9:                                                ; preds = %7, %5
  %10 = phi i8* [ %1, %5 ], [ %69, %7 ]
  %11 = phi i8* [ %2, %5 ], [ %70, %7 ]
  %12 = phi i8* [ %3, %5 ], [ %71, %7 ]
  %13 = phi i8* [ %0, %5 ], [ %68, %7 ]
  %14 = phi i32 [ 0, %5 ], [ %8, %7 ]
  %15 = icmp slt i32 %14, %4
  br i1 %15, label %74, label %132

16:                                               ; preds = %5, %16
  %17 = phi i32 [ %72, %16 ], [ 8, %5 ]
  %18 = phi i8* [ %68, %16 ], [ %0, %5 ]
  %19 = phi i8* [ %71, %16 ], [ %3, %5 ]
  %20 = phi i8* [ %70, %16 ], [ %2, %5 ]
  %21 = phi i8* [ %69, %16 ], [ %1, %5 ]
  %22 = bitcast i8* %18 to i64*
  %23 = load i64, i64* %22, align 1
  %24 = insertelement <2 x i64> undef, i64 %23, i32 0
  %25 = bitcast <2 x i64> %24 to <16 x i8>
  %26 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %25, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %27 = bitcast i8* %21 to i32*
  %28 = load i32, i32* %27, align 4
  %29 = insertelement <4 x i32> undef, i32 %28, i32 0
  %30 = bitcast <4 x i32> %29 to <16 x i8>
  %31 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %30, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %32 = bitcast <16 x i8> %31 to <8 x i16>
  %33 = shufflevector <8 x i16> %32, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %34 = bitcast i8* %20 to i32*
  %35 = load i32, i32* %34, align 4
  %36 = insertelement <4 x i32> undef, i32 %35, i32 0
  %37 = bitcast <4 x i32> %36 to <16 x i8>
  %38 = shufflevector <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i8> %37, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %39 = bitcast <16 x i8> %38 to <8 x i16>
  %40 = shufflevector <8 x i16> %39, <8 x i16> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
  %41 = bitcast <16 x i8> %26 to <8 x i16>
  %42 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %41, <8 x i16> <i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077, i16 19077>) #7
  %43 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %40, <8 x i16> <i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149, i16 26149>) #7
  %44 = add <8 x i16> %42, <i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234, i16 -14234>
  %45 = add <8 x i16> %44, %43
  %46 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %33, <8 x i16> <i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419, i16 6419>) #7
  %47 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %40, <8 x i16> <i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320, i16 13320>) #7
  %48 = add <8 x i16> %42, <i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708, i16 8708>
  %49 = sub <8 x i16> %48, %46
  %50 = sub <8 x i16> %49, %47
  %51 = tail call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %33, <8 x i16> <i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486, i16 -32486>) #7
  %52 = tail call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> %51, <8 x i16> %42) #7
  %53 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %52, <8 x i16> <i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685, i16 17685>) #7
  %54 = ashr <8 x i16> %45, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %55 = ashr <8 x i16> %50, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %56 = lshr <8 x i16> %53, <i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6, i16 6>
  %57 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>, <8 x i16> %55) #7
  %58 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %54, <8 x i16> %56) #7
  %59 = shufflevector <16 x i8> %57, <16 x i8> %58, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %60 = shufflevector <16 x i8> %57, <16 x i8> %58, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %61 = bitcast <16 x i8> %59 to <8 x i16>
  %62 = bitcast <16 x i8> %60 to <8 x i16>
  %63 = shufflevector <8 x i16> %61, <8 x i16> %62, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %64 = shufflevector <8 x i16> %61, <8 x i16> %62, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %65 = bitcast i8* %19 to <8 x i16>*
  store <8 x i16> %63, <8 x i16>* %65, align 1
  %66 = getelementptr inbounds i8, i8* %19, i64 16
  %67 = bitcast i8* %66 to <8 x i16>*
  store <8 x i16> %64, <8 x i16>* %67, align 1
  %68 = getelementptr inbounds i8, i8* %18, i64 8
  %69 = getelementptr inbounds i8, i8* %21, i64 4
  %70 = getelementptr inbounds i8, i8* %20, i64 4
  %71 = getelementptr inbounds i8, i8* %19, i64 32
  %72 = add nuw nsw i32 %17, 8
  %73 = icmp sgt i32 %72, %4
  br i1 %73, label %7, label %16

74:                                               ; preds = %9, %74
  %75 = phi i32 [ %130, %74 ], [ %14, %9 ]
  %76 = phi i8* [ %125, %74 ], [ %13, %9 ]
  %77 = phi i8* [ %124, %74 ], [ %12, %9 ]
  %78 = phi i8* [ %129, %74 ], [ %11, %9 ]
  %79 = phi i8* [ %128, %74 ], [ %10, %9 ]
  %80 = load i8, i8* %76, align 1
  %81 = load i8, i8* %79, align 1
  %82 = load i8, i8* %78, align 1
  store i8 -1, i8* %77, align 1
  %83 = zext i8 %80 to i32
  %84 = zext i8 %81 to i32
  %85 = zext i8 %82 to i32
  %86 = getelementptr inbounds i8, i8* %77, i64 1
  %87 = mul nuw nsw i32 %83, 19077
  %88 = lshr i32 %87, 8
  %89 = mul nuw nsw i32 %85, 26149
  %90 = lshr i32 %89, 8
  %91 = add nsw i32 %88, -14234
  %92 = add nsw i32 %91, %90
  %93 = icmp ult i32 %92, 16384
  %94 = lshr i32 %92, 6
  %95 = icmp slt i32 %92, 0
  %96 = select i1 %95, i32 0, i32 255
  %97 = select i1 %93, i32 %94, i32 %96
  %98 = trunc i32 %97 to i8
  store i8 %98, i8* %86, align 1
  %99 = mul nuw nsw i32 %84, 6419
  %100 = lshr i32 %99, 8
  %101 = mul nuw nsw i32 %85, 13320
  %102 = lshr i32 %101, 8
  %103 = add nuw nsw i32 %88, 8708
  %104 = sub nuw nsw i32 %103, %100
  %105 = sub nsw i32 %104, %102
  %106 = icmp ult i32 %105, 16384
  %107 = lshr i32 %105, 6
  %108 = icmp slt i32 %105, 0
  %109 = select i1 %108, i32 0, i32 255
  %110 = select i1 %106, i32 %107, i32 %109
  %111 = trunc i32 %110 to i8
  %112 = getelementptr inbounds i8, i8* %77, i64 2
  store i8 %111, i8* %112, align 1
  %113 = mul nuw nsw i32 %84, 33050
  %114 = lshr i32 %113, 8
  %115 = add nsw i32 %88, -17685
  %116 = add nsw i32 %115, %114
  %117 = icmp ult i32 %116, 16384
  %118 = lshr i32 %116, 6
  %119 = icmp slt i32 %116, 0
  %120 = select i1 %119, i32 0, i32 255
  %121 = select i1 %117, i32 %118, i32 %120
  %122 = trunc i32 %121 to i8
  %123 = getelementptr inbounds i8, i8* %77, i64 3
  store i8 %122, i8* %123, align 1
  %124 = getelementptr inbounds i8, i8* %77, i64 4
  %125 = getelementptr inbounds i8, i8* %76, i64 1
  %126 = and i32 %75, 1
  %127 = zext i32 %126 to i64
  %128 = getelementptr inbounds i8, i8* %79, i64 %127
  %129 = getelementptr inbounds i8, i8* %78, i64 %127
  %130 = add nuw nsw i32 %75, 1
  %131 = icmp eq i32 %130, %4
  br i1 %131, label %132, label %74

132:                                              ; preds = %74, %9
  ret void
}

; Function Attrs: nofree norecurse nounwind ssp uwtable writeonly
define hidden void @WebPInitConvertARGBToYUVSSE2() local_unnamed_addr #1 {
  store void (i32*, i8*, i32)* @ConvertARGBToY_SSE2, void (i32*, i8*, i32)** @WebPConvertARGBToY, align 8
  store void (i32*, i8*, i8*, i32, i32)* @ConvertARGBToUV_SSE2, void (i32*, i8*, i8*, i32, i32)** @WebPConvertARGBToUV, align 8
  store void (i8*, i8*, i32)* @ConvertRGB24ToY_SSE2, void (i8*, i8*, i32)** @WebPConvertRGB24ToY, align 8
  store void (i8*, i8*, i32)* @ConvertBGR24ToY_SSE2, void (i8*, i8*, i32)** @WebPConvertBGR24ToY, align 8
  store void (i16*, i8*, i8*, i32)* @ConvertRGBA32ToUV_SSE2, void (i16*, i8*, i8*, i32)** @WebPConvertRGBA32ToUV, align 8
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @ConvertARGBToY_SSE2(i32* nocapture readonly, i8* nocapture, i32) #0 {
  %4 = and i32 %2, -16
  %5 = icmp sgt i32 %4, 0
  br i1 %5, label %6, label %10

6:                                                ; preds = %3
  %7 = sext i32 %4 to i64
  br label %58

8:                                                ; preds = %58
  %9 = trunc i64 %139 to i32
  br label %10

10:                                               ; preds = %8, %3
  %11 = phi i32 [ 0, %3 ], [ %9, %8 ]
  %12 = icmp slt i32 %11, %2
  br i1 %12, label %13, label %161

13:                                               ; preds = %10
  %14 = zext i32 %11 to i64
  %15 = zext i32 %2 to i64
  %16 = sub nsw i64 %15, %14
  %17 = icmp ult i64 %16, 4
  br i1 %17, label %18, label %20

18:                                               ; preds = %56, %20, %13
  %19 = phi i64 [ %14, %20 ], [ %14, %13 ], [ %32, %56 ]
  br label %141

20:                                               ; preds = %13
  %21 = getelementptr i8, i8* %1, i64 %14
  %22 = getelementptr i8, i8* %1, i64 %15
  %23 = getelementptr i32, i32* %0, i64 %14
  %24 = bitcast i32* %23 to i8*
  %25 = getelementptr i32, i32* %0, i64 %15
  %26 = bitcast i32* %25 to i8*
  %27 = icmp ult i8* %21, %26
  %28 = icmp ugt i8* %22, %24
  %29 = and i1 %27, %28
  br i1 %29, label %18, label %30

30:                                               ; preds = %20
  %31 = and i64 %16, -4
  %32 = add nsw i64 %31, %14
  br label %33

33:                                               ; preds = %33, %30
  %34 = phi i64 [ 0, %30 ], [ %54, %33 ]
  %35 = add i64 %34, %14
  %36 = getelementptr inbounds i32, i32* %0, i64 %35
  %37 = bitcast i32* %36 to <4 x i32>*
  %38 = load <4 x i32>, <4 x i32>* %37, align 4, !alias.scope !2
  %39 = lshr <4 x i32> %38, <i32 16, i32 16, i32 16, i32 16>
  %40 = and <4 x i32> %39, <i32 255, i32 255, i32 255, i32 255>
  %41 = lshr <4 x i32> %38, <i32 8, i32 8, i32 8, i32 8>
  %42 = and <4 x i32> %41, <i32 255, i32 255, i32 255, i32 255>
  %43 = and <4 x i32> %38, <i32 255, i32 255, i32 255, i32 255>
  %44 = mul nuw nsw <4 x i32> %40, <i32 16839, i32 16839, i32 16839, i32 16839>
  %45 = mul nuw nsw <4 x i32> %42, <i32 33059, i32 33059, i32 33059, i32 33059>
  %46 = mul nuw nsw <4 x i32> %43, <i32 6420, i32 6420, i32 6420, i32 6420>
  %47 = add nuw nsw <4 x i32> %46, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %48 = add nuw nsw <4 x i32> %47, %44
  %49 = add nuw nsw <4 x i32> %48, %45
  %50 = lshr <4 x i32> %49, <i32 16, i32 16, i32 16, i32 16>
  %51 = trunc <4 x i32> %50 to <4 x i8>
  %52 = getelementptr inbounds i8, i8* %1, i64 %35
  %53 = bitcast i8* %52 to <4 x i8>*
  store <4 x i8> %51, <4 x i8>* %53, align 1, !alias.scope !5, !noalias !2
  %54 = add i64 %34, 4
  %55 = icmp eq i64 %54, %31
  br i1 %55, label %56, label %33, !llvm.loop !7

56:                                               ; preds = %33
  %57 = icmp eq i64 %16, %31
  br i1 %57, label %161, label %18

58:                                               ; preds = %6, %58
  %59 = phi i64 [ 0, %6 ], [ %139, %58 ]
  %60 = getelementptr inbounds i32, i32* %0, i64 %59
  %61 = bitcast i32* %60 to <16 x i8>*
  %62 = load <16 x i8>, <16 x i8>* %61, align 1
  %63 = getelementptr inbounds i32, i32* %60, i64 4
  %64 = bitcast i32* %63 to <16 x i8>*
  %65 = load <16 x i8>, <16 x i8>* %64, align 1
  %66 = getelementptr inbounds i32, i32* %60, i64 8
  %67 = bitcast i32* %66 to <16 x i8>*
  %68 = load <16 x i8>, <16 x i8>* %67, align 1
  %69 = getelementptr inbounds i32, i32* %60, i64 12
  %70 = bitcast i32* %69 to <16 x i8>*
  %71 = load <16 x i8>, <16 x i8>* %70, align 1
  %72 = shufflevector <16 x i8> %62, <16 x i8> %65, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %73 = shufflevector <16 x i8> %62, <16 x i8> %65, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %74 = shufflevector <16 x i8> %68, <16 x i8> %71, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %75 = shufflevector <16 x i8> %68, <16 x i8> %71, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %76 = shufflevector <16 x i8> %72, <16 x i8> %73, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %77 = shufflevector <16 x i8> %72, <16 x i8> %73, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %78 = shufflevector <16 x i8> %74, <16 x i8> %75, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %79 = shufflevector <16 x i8> %74, <16 x i8> %75, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %80 = shufflevector <16 x i8> %76, <16 x i8> %77, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %81 = bitcast <16 x i8> %80 to <2 x i64>
  %82 = shufflevector <16 x i8> %76, <16 x i8> %77, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %83 = bitcast <16 x i8> %82 to <2 x i64>
  %84 = shufflevector <16 x i8> %78, <16 x i8> %79, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %85 = bitcast <16 x i8> %84 to <2 x i64>
  %86 = shufflevector <16 x i8> %78, <16 x i8> %79, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %87 = bitcast <16 x i8> %86 to <2 x i64>
  %88 = shufflevector <2 x i64> %83, <2 x i64> %87, <2 x i32> <i32 0, i32 2>
  %89 = shufflevector <2 x i64> %81, <2 x i64> %85, <2 x i32> <i32 1, i32 3>
  %90 = shufflevector <2 x i64> %81, <2 x i64> %85, <2 x i32> <i32 0, i32 2>
  %91 = bitcast <2 x i64> %88 to <16 x i8>
  %92 = shufflevector <16 x i8> %91, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %93 = bitcast <16 x i8> %92 to <8 x i16>
  %94 = shufflevector <16 x i8> %91, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %95 = bitcast <16 x i8> %94 to <8 x i16>
  %96 = bitcast <2 x i64> %89 to <16 x i8>
  %97 = shufflevector <16 x i8> %96, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %98 = bitcast <16 x i8> %97 to <8 x i16>
  %99 = shufflevector <16 x i8> %96, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %100 = bitcast <16 x i8> %99 to <8 x i16>
  %101 = bitcast <2 x i64> %90 to <16 x i8>
  %102 = shufflevector <16 x i8> %101, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %103 = bitcast <16 x i8> %102 to <8 x i16>
  %104 = shufflevector <16 x i8> %101, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %105 = bitcast <16 x i8> %104 to <8 x i16>
  %106 = shufflevector <8 x i16> %93, <8 x i16> %98, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %107 = shufflevector <8 x i16> %93, <8 x i16> %98, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %108 = shufflevector <8 x i16> %98, <8 x i16> %103, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %109 = shufflevector <8 x i16> %98, <8 x i16> %103, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %110 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %106, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #7
  %111 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %107, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #7
  %112 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %108, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #7
  %113 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %109, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #7
  %114 = add <4 x i32> %110, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %115 = add <4 x i32> %114, %112
  %116 = add <4 x i32> %111, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %117 = add <4 x i32> %116, %113
  %118 = ashr <4 x i32> %115, <i32 16, i32 16, i32 16, i32 16>
  %119 = ashr <4 x i32> %117, <i32 16, i32 16, i32 16, i32 16>
  %120 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %118, <4 x i32> %119) #7
  %121 = shufflevector <8 x i16> %95, <8 x i16> %100, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %122 = shufflevector <8 x i16> %95, <8 x i16> %100, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %123 = shufflevector <8 x i16> %100, <8 x i16> %105, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %124 = shufflevector <8 x i16> %100, <8 x i16> %105, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %125 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %121, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #7
  %126 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %122, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #7
  %127 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %123, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #7
  %128 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %124, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #7
  %129 = add <4 x i32> %125, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %130 = add <4 x i32> %129, %127
  %131 = add <4 x i32> %126, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %132 = add <4 x i32> %131, %128
  %133 = ashr <4 x i32> %130, <i32 16, i32 16, i32 16, i32 16>
  %134 = ashr <4 x i32> %132, <i32 16, i32 16, i32 16, i32 16>
  %135 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %133, <4 x i32> %134) #7
  %136 = getelementptr inbounds i8, i8* %1, i64 %59
  %137 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %120, <8 x i16> %135) #7
  %138 = bitcast i8* %136 to <16 x i8>*
  store <16 x i8> %137, <16 x i8>* %138, align 1
  %139 = add nuw nsw i64 %59, 16
  %140 = icmp slt i64 %139, %7
  br i1 %140, label %58, label %8

141:                                              ; preds = %18, %141
  %142 = phi i64 [ %159, %141 ], [ %19, %18 ]
  %143 = getelementptr inbounds i32, i32* %0, i64 %142
  %144 = load i32, i32* %143, align 4
  %145 = lshr i32 %144, 16
  %146 = and i32 %145, 255
  %147 = lshr i32 %144, 8
  %148 = and i32 %147, 255
  %149 = and i32 %144, 255
  %150 = mul nuw nsw i32 %146, 16839
  %151 = mul nuw nsw i32 %148, 33059
  %152 = mul nuw nsw i32 %149, 6420
  %153 = add nuw nsw i32 %152, 1081344
  %154 = add nuw nsw i32 %153, %150
  %155 = add nuw nsw i32 %154, %151
  %156 = lshr i32 %155, 16
  %157 = trunc i32 %156 to i8
  %158 = getelementptr inbounds i8, i8* %1, i64 %142
  store i8 %157, i8* %158, align 1
  %159 = add nuw nsw i64 %142, 1
  %160 = icmp eq i64 %159, %15
  br i1 %160, label %161, label %141, !llvm.loop !9

161:                                              ; preds = %141, %56, %10
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @ConvertARGBToUV_SSE2(i32*, i8*, i8*, i32, i32) #0 {
  %6 = and i32 %3, -32
  %7 = icmp sgt i32 %6, 0
  br i1 %7, label %8, label %199

8:                                                ; preds = %5
  %9 = icmp eq i32 %4, 0
  %10 = sext i32 %6 to i64
  br label %11

11:                                               ; preds = %8, %189
  %12 = phi i64 [ 0, %8 ], [ %193, %189 ]
  %13 = phi i8* [ %1, %8 ], [ %194, %189 ]
  %14 = phi i8* [ %2, %8 ], [ %195, %189 ]
  %15 = getelementptr inbounds i32, i32* %0, i64 %12
  %16 = bitcast i32* %15 to <16 x i8>*
  %17 = load <16 x i8>, <16 x i8>* %16, align 1
  %18 = getelementptr inbounds i32, i32* %15, i64 4
  %19 = bitcast i32* %18 to <16 x i8>*
  %20 = load <16 x i8>, <16 x i8>* %19, align 1
  %21 = getelementptr inbounds i32, i32* %15, i64 8
  %22 = bitcast i32* %21 to <16 x i8>*
  %23 = load <16 x i8>, <16 x i8>* %22, align 1
  %24 = getelementptr inbounds i32, i32* %15, i64 12
  %25 = bitcast i32* %24 to <16 x i8>*
  %26 = load <16 x i8>, <16 x i8>* %25, align 1
  %27 = shufflevector <16 x i8> %17, <16 x i8> %20, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %28 = shufflevector <16 x i8> %17, <16 x i8> %20, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %29 = shufflevector <16 x i8> %23, <16 x i8> %26, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %30 = shufflevector <16 x i8> %23, <16 x i8> %26, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %31 = shufflevector <16 x i8> %27, <16 x i8> %28, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %32 = shufflevector <16 x i8> %27, <16 x i8> %28, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %33 = shufflevector <16 x i8> %29, <16 x i8> %30, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %34 = shufflevector <16 x i8> %29, <16 x i8> %30, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %35 = shufflevector <16 x i8> %31, <16 x i8> %32, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %36 = bitcast <16 x i8> %35 to <2 x i64>
  %37 = shufflevector <16 x i8> %31, <16 x i8> %32, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %38 = bitcast <16 x i8> %37 to <2 x i64>
  %39 = shufflevector <16 x i8> %33, <16 x i8> %34, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %40 = bitcast <16 x i8> %39 to <2 x i64>
  %41 = shufflevector <16 x i8> %33, <16 x i8> %34, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %42 = bitcast <16 x i8> %41 to <2 x i64>
  %43 = shufflevector <2 x i64> %38, <2 x i64> %42, <2 x i32> <i32 0, i32 2>
  %44 = shufflevector <2 x i64> %36, <2 x i64> %40, <2 x i32> <i32 1, i32 3>
  %45 = shufflevector <2 x i64> %36, <2 x i64> %40, <2 x i32> <i32 0, i32 2>
  %46 = bitcast <2 x i64> %43 to <16 x i8>
  %47 = shufflevector <16 x i8> %46, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %48 = bitcast <16 x i8> %47 to <8 x i16>
  %49 = shufflevector <16 x i8> %46, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %50 = bitcast <16 x i8> %49 to <8 x i16>
  %51 = bitcast <2 x i64> %44 to <16 x i8>
  %52 = shufflevector <16 x i8> %51, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %53 = bitcast <16 x i8> %52 to <8 x i16>
  %54 = shufflevector <16 x i8> %51, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %55 = bitcast <16 x i8> %54 to <8 x i16>
  %56 = bitcast <2 x i64> %45 to <16 x i8>
  %57 = shufflevector <16 x i8> %56, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %58 = bitcast <16 x i8> %57 to <8 x i16>
  %59 = shufflevector <16 x i8> %56, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %60 = bitcast <16 x i8> %59 to <8 x i16>
  %61 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %48, <8 x i16> <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>) #7
  %62 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %50, <8 x i16> <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>) #7
  %63 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %61, <4 x i32> %62) #7
  %64 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %53, <8 x i16> <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>) #7
  %65 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %55, <8 x i16> <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>) #7
  %66 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %64, <4 x i32> %65) #7
  %67 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %58, <8 x i16> <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>) #7
  %68 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %60, <8 x i16> <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>) #7
  %69 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %67, <4 x i32> %68) #7
  %70 = shufflevector <8 x i16> %63, <8 x i16> %66, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %71 = shufflevector <8 x i16> %63, <8 x i16> %66, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %72 = shufflevector <8 x i16> %66, <8 x i16> %69, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %73 = shufflevector <8 x i16> %66, <8 x i16> %69, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %74 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %70, <8 x i16> <i16 -9719, i16 -19081, i16 -9719, i16 -19081, i16 -9719, i16 -19081, i16 -9719, i16 -19081>) #7
  %75 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %71, <8 x i16> <i16 -9719, i16 -19081, i16 -9719, i16 -19081, i16 -9719, i16 -19081, i16 -9719, i16 -19081>) #7
  %76 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %72, <8 x i16> <i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800>) #7
  %77 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %73, <8 x i16> <i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800>) #7
  %78 = add <4 x i32> %74, <i32 33685504, i32 33685504, i32 33685504, i32 33685504>
  %79 = add <4 x i32> %78, %76
  %80 = add <4 x i32> %75, <i32 33685504, i32 33685504, i32 33685504, i32 33685504>
  %81 = add <4 x i32> %80, %77
  %82 = ashr <4 x i32> %79, <i32 18, i32 18, i32 18, i32 18>
  %83 = ashr <4 x i32> %81, <i32 18, i32 18, i32 18, i32 18>
  %84 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %82, <4 x i32> %83) #7
  %85 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %70, <8 x i16> <i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0>) #7
  %86 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %71, <8 x i16> <i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0>) #7
  %87 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %72, <8 x i16> <i16 -24116, i16 -4684, i16 -24116, i16 -4684, i16 -24116, i16 -4684, i16 -24116, i16 -4684>) #7
  %88 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %73, <8 x i16> <i16 -24116, i16 -4684, i16 -24116, i16 -4684, i16 -24116, i16 -4684, i16 -24116, i16 -4684>) #7
  %89 = add <4 x i32> %85, <i32 33685504, i32 33685504, i32 33685504, i32 33685504>
  %90 = add <4 x i32> %89, %87
  %91 = add <4 x i32> %86, <i32 33685504, i32 33685504, i32 33685504, i32 33685504>
  %92 = add <4 x i32> %91, %88
  %93 = ashr <4 x i32> %90, <i32 18, i32 18, i32 18, i32 18>
  %94 = ashr <4 x i32> %92, <i32 18, i32 18, i32 18, i32 18>
  %95 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %93, <4 x i32> %94) #7
  %96 = or i64 %12, 16
  %97 = getelementptr inbounds i32, i32* %0, i64 %96
  %98 = bitcast i32* %97 to <16 x i8>*
  %99 = load <16 x i8>, <16 x i8>* %98, align 1
  %100 = getelementptr inbounds i32, i32* %97, i64 4
  %101 = bitcast i32* %100 to <16 x i8>*
  %102 = load <16 x i8>, <16 x i8>* %101, align 1
  %103 = getelementptr inbounds i32, i32* %97, i64 8
  %104 = bitcast i32* %103 to <16 x i8>*
  %105 = load <16 x i8>, <16 x i8>* %104, align 1
  %106 = getelementptr inbounds i32, i32* %97, i64 12
  %107 = bitcast i32* %106 to <16 x i8>*
  %108 = load <16 x i8>, <16 x i8>* %107, align 1
  %109 = shufflevector <16 x i8> %99, <16 x i8> %102, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %110 = shufflevector <16 x i8> %99, <16 x i8> %102, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %111 = shufflevector <16 x i8> %105, <16 x i8> %108, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %112 = shufflevector <16 x i8> %105, <16 x i8> %108, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %113 = shufflevector <16 x i8> %109, <16 x i8> %110, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %114 = shufflevector <16 x i8> %109, <16 x i8> %110, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %115 = shufflevector <16 x i8> %111, <16 x i8> %112, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %116 = shufflevector <16 x i8> %111, <16 x i8> %112, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %117 = shufflevector <16 x i8> %113, <16 x i8> %114, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %118 = bitcast <16 x i8> %117 to <2 x i64>
  %119 = shufflevector <16 x i8> %113, <16 x i8> %114, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %120 = bitcast <16 x i8> %119 to <2 x i64>
  %121 = shufflevector <16 x i8> %115, <16 x i8> %116, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %122 = bitcast <16 x i8> %121 to <2 x i64>
  %123 = shufflevector <16 x i8> %115, <16 x i8> %116, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %124 = bitcast <16 x i8> %123 to <2 x i64>
  %125 = shufflevector <2 x i64> %120, <2 x i64> %124, <2 x i32> <i32 0, i32 2>
  %126 = shufflevector <2 x i64> %118, <2 x i64> %122, <2 x i32> <i32 1, i32 3>
  %127 = shufflevector <2 x i64> %118, <2 x i64> %122, <2 x i32> <i32 0, i32 2>
  %128 = bitcast <2 x i64> %125 to <16 x i8>
  %129 = shufflevector <16 x i8> %128, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %130 = bitcast <16 x i8> %129 to <8 x i16>
  %131 = shufflevector <16 x i8> %128, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %132 = bitcast <16 x i8> %131 to <8 x i16>
  %133 = bitcast <2 x i64> %126 to <16 x i8>
  %134 = shufflevector <16 x i8> %133, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %135 = bitcast <16 x i8> %134 to <8 x i16>
  %136 = shufflevector <16 x i8> %133, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %137 = bitcast <16 x i8> %136 to <8 x i16>
  %138 = bitcast <2 x i64> %127 to <16 x i8>
  %139 = shufflevector <16 x i8> %138, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %140 = bitcast <16 x i8> %139 to <8 x i16>
  %141 = shufflevector <16 x i8> %138, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %142 = bitcast <16 x i8> %141 to <8 x i16>
  %143 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %130, <8 x i16> <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>) #7
  %144 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %132, <8 x i16> <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>) #7
  %145 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %143, <4 x i32> %144) #7
  %146 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %135, <8 x i16> <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>) #7
  %147 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %137, <8 x i16> <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>) #7
  %148 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %146, <4 x i32> %147) #7
  %149 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %140, <8 x i16> <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>) #7
  %150 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %142, <8 x i16> <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>) #7
  %151 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %149, <4 x i32> %150) #7
  %152 = shufflevector <8 x i16> %145, <8 x i16> %148, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %153 = shufflevector <8 x i16> %145, <8 x i16> %148, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %154 = shufflevector <8 x i16> %148, <8 x i16> %151, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %155 = shufflevector <8 x i16> %148, <8 x i16> %151, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %156 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %152, <8 x i16> <i16 -9719, i16 -19081, i16 -9719, i16 -19081, i16 -9719, i16 -19081, i16 -9719, i16 -19081>) #7
  %157 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %153, <8 x i16> <i16 -9719, i16 -19081, i16 -9719, i16 -19081, i16 -9719, i16 -19081, i16 -9719, i16 -19081>) #7
  %158 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %154, <8 x i16> <i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800>) #7
  %159 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %155, <8 x i16> <i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800>) #7
  %160 = add <4 x i32> %156, <i32 33685504, i32 33685504, i32 33685504, i32 33685504>
  %161 = add <4 x i32> %160, %158
  %162 = add <4 x i32> %157, <i32 33685504, i32 33685504, i32 33685504, i32 33685504>
  %163 = add <4 x i32> %162, %159
  %164 = ashr <4 x i32> %161, <i32 18, i32 18, i32 18, i32 18>
  %165 = ashr <4 x i32> %163, <i32 18, i32 18, i32 18, i32 18>
  %166 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %164, <4 x i32> %165) #7
  %167 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %152, <8 x i16> <i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0>) #7
  %168 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %153, <8 x i16> <i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0>) #7
  %169 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %154, <8 x i16> <i16 -24116, i16 -4684, i16 -24116, i16 -4684, i16 -24116, i16 -4684, i16 -24116, i16 -4684>) #7
  %170 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %155, <8 x i16> <i16 -24116, i16 -4684, i16 -24116, i16 -4684, i16 -24116, i16 -4684, i16 -24116, i16 -4684>) #7
  %171 = add <4 x i32> %167, <i32 33685504, i32 33685504, i32 33685504, i32 33685504>
  %172 = add <4 x i32> %171, %169
  %173 = add <4 x i32> %168, <i32 33685504, i32 33685504, i32 33685504, i32 33685504>
  %174 = add <4 x i32> %173, %170
  %175 = ashr <4 x i32> %172, <i32 18, i32 18, i32 18, i32 18>
  %176 = ashr <4 x i32> %174, <i32 18, i32 18, i32 18, i32 18>
  %177 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %175, <4 x i32> %176) #7
  %178 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %84, <8 x i16> %166) #7
  %179 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %95, <8 x i16> %177) #7
  %180 = bitcast i8* %13 to <16 x i8>*
  br i1 %9, label %183, label %181

181:                                              ; preds = %11
  %182 = bitcast i8* %14 to <16 x i8>*
  br label %189

183:                                              ; preds = %11
  %184 = load <16 x i8>, <16 x i8>* %180, align 1
  %185 = bitcast i8* %14 to <16 x i8>*
  %186 = load <16 x i8>, <16 x i8>* %185, align 1
  %187 = tail call <16 x i8> @llvm.x86.sse2.pavg.b(<16 x i8> %178, <16 x i8> %184) #7
  %188 = tail call <16 x i8> @llvm.x86.sse2.pavg.b(<16 x i8> %179, <16 x i8> %186) #7
  br label %189

189:                                              ; preds = %181, %183
  %190 = phi <16 x i8>* [ %182, %181 ], [ %185, %183 ]
  %191 = phi <16 x i8> [ %178, %181 ], [ %187, %183 ]
  %192 = phi <16 x i8> [ %179, %181 ], [ %188, %183 ]
  store <16 x i8> %191, <16 x i8>* %180, align 1
  store <16 x i8> %192, <16 x i8>* %190, align 1
  %193 = add nuw nsw i64 %12, 32
  %194 = getelementptr inbounds i8, i8* %13, i64 16
  %195 = getelementptr inbounds i8, i8* %14, i64 16
  %196 = icmp slt i64 %193, %10
  br i1 %196, label %11, label %197

197:                                              ; preds = %189
  %198 = trunc i64 %193 to i32
  br label %199

199:                                              ; preds = %197, %5
  %200 = phi i32 [ 0, %5 ], [ %198, %197 ]
  %201 = phi i8* [ %2, %5 ], [ %195, %197 ]
  %202 = phi i8* [ %1, %5 ], [ %194, %197 ]
  %203 = icmp slt i32 %200, %3
  br i1 %203, label %204, label %208

204:                                              ; preds = %199
  %205 = zext i32 %200 to i64
  %206 = getelementptr inbounds i32, i32* %0, i64 %205
  %207 = sub nsw i32 %3, %200
  tail call void @WebPConvertARGBToUV_C(i32* %206, i8* %202, i8* %201, i32 %207, i32 %4) #7
  br label %208

208:                                              ; preds = %204, %199
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @ConvertRGB24ToY_SSE2(i8* nocapture readonly, i8* nocapture, i32) #0 {
  %4 = and i32 %2, -32
  %5 = icmp sgt i32 %4, 0
  br i1 %5, label %44, label %8

6:                                                ; preds = %44
  %7 = trunc i64 %187 to i32
  br label %8

8:                                                ; preds = %6, %3
  %9 = phi i32 [ 0, %3 ], [ %7, %6 ]
  %10 = phi i8* [ %0, %3 ], [ %189, %6 ]
  %11 = icmp slt i32 %9, %2
  br i1 %11, label %12, label %233

12:                                               ; preds = %8
  %13 = sext i32 %9 to i64
  %14 = sext i32 %2 to i64
  %15 = sub nsw i64 %14, %13
  %16 = xor i64 %13, -1
  %17 = and i64 %15, 1
  %18 = icmp eq i64 %17, 0
  br i1 %18, label %39, label %19

19:                                               ; preds = %12
  %20 = load i8, i8* %10, align 1
  %21 = zext i8 %20 to i32
  %22 = getelementptr inbounds i8, i8* %10, i64 1
  %23 = load i8, i8* %22, align 1
  %24 = zext i8 %23 to i32
  %25 = getelementptr inbounds i8, i8* %10, i64 2
  %26 = load i8, i8* %25, align 1
  %27 = zext i8 %26 to i32
  %28 = mul nuw nsw i32 %21, 16839
  %29 = mul nuw nsw i32 %24, 33059
  %30 = mul nuw nsw i32 %27, 6420
  %31 = add nuw nsw i32 %28, 1081344
  %32 = add nuw nsw i32 %31, %29
  %33 = add nuw nsw i32 %32, %30
  %34 = lshr i32 %33, 16
  %35 = trunc i32 %34 to i8
  %36 = getelementptr inbounds i8, i8* %1, i64 %13
  store i8 %35, i8* %36, align 1
  %37 = add nsw i64 %13, 1
  %38 = getelementptr inbounds i8, i8* %10, i64 3
  br label %39

39:                                               ; preds = %12, %19
  %40 = phi i64 [ %13, %12 ], [ %37, %19 ]
  %41 = phi i8* [ %10, %12 ], [ %38, %19 ]
  %42 = sub nsw i64 0, %14
  %43 = icmp eq i64 %16, %42
  br i1 %43, label %233, label %191

44:                                               ; preds = %3, %44
  %45 = phi i8* [ %189, %44 ], [ %0, %3 ]
  %46 = phi i64 [ %187, %44 ], [ 0, %3 ]
  %47 = bitcast i8* %45 to <16 x i8>*
  %48 = load <16 x i8>, <16 x i8>* %47, align 1
  %49 = getelementptr inbounds i8, i8* %45, i64 16
  %50 = bitcast i8* %49 to <16 x i8>*
  %51 = load <16 x i8>, <16 x i8>* %50, align 1
  %52 = getelementptr inbounds i8, i8* %45, i64 32
  %53 = bitcast i8* %52 to <16 x i8>*
  %54 = load <16 x i8>, <16 x i8>* %53, align 1
  %55 = getelementptr inbounds i8, i8* %45, i64 48
  %56 = bitcast i8* %55 to <16 x i8>*
  %57 = load <16 x i8>, <16 x i8>* %56, align 1
  %58 = getelementptr inbounds i8, i8* %45, i64 64
  %59 = bitcast i8* %58 to <16 x i8>*
  %60 = load <16 x i8>, <16 x i8>* %59, align 1
  %61 = getelementptr inbounds i8, i8* %45, i64 80
  %62 = bitcast i8* %61 to <16 x i8>*
  %63 = load <16 x i8>, <16 x i8>* %62, align 1
  %64 = shufflevector <16 x i8> %48, <16 x i8> %57, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %65 = shufflevector <16 x i8> %48, <16 x i8> %57, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %66 = shufflevector <16 x i8> %51, <16 x i8> %60, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %67 = shufflevector <16 x i8> %51, <16 x i8> %60, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %68 = shufflevector <16 x i8> %54, <16 x i8> %63, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %69 = shufflevector <16 x i8> %54, <16 x i8> %63, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %70 = shufflevector <16 x i8> %64, <16 x i8> %67, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %71 = shufflevector <16 x i8> %64, <16 x i8> %67, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %72 = shufflevector <16 x i8> %65, <16 x i8> %68, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %73 = shufflevector <16 x i8> %65, <16 x i8> %68, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %74 = shufflevector <16 x i8> %66, <16 x i8> %69, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %75 = shufflevector <16 x i8> %66, <16 x i8> %69, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %76 = shufflevector <16 x i8> %70, <16 x i8> %73, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %77 = shufflevector <16 x i8> %70, <16 x i8> %73, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %78 = shufflevector <16 x i8> %71, <16 x i8> %74, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %79 = shufflevector <16 x i8> %71, <16 x i8> %74, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %80 = shufflevector <16 x i8> %72, <16 x i8> %75, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %81 = shufflevector <16 x i8> %72, <16 x i8> %75, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %82 = shufflevector <16 x i8> %76, <16 x i8> %79, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %83 = shufflevector <16 x i8> %76, <16 x i8> %79, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %84 = shufflevector <16 x i8> %77, <16 x i8> %80, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %85 = shufflevector <16 x i8> %77, <16 x i8> %80, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %86 = shufflevector <16 x i8> %78, <16 x i8> %81, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %87 = shufflevector <16 x i8> %78, <16 x i8> %81, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %88 = shufflevector <16 x i8> %82, <16 x i8> %85, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %89 = shufflevector <16 x i8> %82, <16 x i8> %85, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %90 = shufflevector <16 x i8> %83, <16 x i8> %86, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %91 = shufflevector <16 x i8> %83, <16 x i8> %86, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %92 = shufflevector <16 x i8> %84, <16 x i8> %87, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %93 = shufflevector <16 x i8> %84, <16 x i8> %87, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %94 = shl i64 %46, 32
  %95 = ashr exact i64 %94, 32
  %96 = shufflevector <16 x i8> %88, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %97 = shufflevector <16 x i8> %90, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %98 = shufflevector <16 x i8> %92, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %99 = bitcast <16 x i8> %96 to <8 x i16>
  %100 = bitcast <16 x i8> %97 to <8 x i16>
  %101 = shufflevector <8 x i16> %99, <8 x i16> %100, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %102 = shufflevector <8 x i16> %99, <8 x i16> %100, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %103 = bitcast <16 x i8> %98 to <8 x i16>
  %104 = shufflevector <8 x i16> %100, <8 x i16> %103, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %105 = shufflevector <8 x i16> %100, <8 x i16> %103, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %106 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %101, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #7
  %107 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %102, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #7
  %108 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %104, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #7
  %109 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %105, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #7
  %110 = add <4 x i32> %106, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %111 = add <4 x i32> %110, %108
  %112 = add <4 x i32> %107, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %113 = add <4 x i32> %112, %109
  %114 = ashr <4 x i32> %111, <i32 16, i32 16, i32 16, i32 16>
  %115 = ashr <4 x i32> %113, <i32 16, i32 16, i32 16, i32 16>
  %116 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %114, <4 x i32> %115) #7
  %117 = shufflevector <16 x i8> %88, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %118 = shufflevector <16 x i8> %90, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %119 = shufflevector <16 x i8> %92, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %120 = bitcast <16 x i8> %117 to <8 x i16>
  %121 = bitcast <16 x i8> %118 to <8 x i16>
  %122 = shufflevector <8 x i16> %120, <8 x i16> %121, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %123 = shufflevector <8 x i16> %120, <8 x i16> %121, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %124 = bitcast <16 x i8> %119 to <8 x i16>
  %125 = shufflevector <8 x i16> %121, <8 x i16> %124, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %126 = shufflevector <8 x i16> %121, <8 x i16> %124, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %127 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %122, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #7
  %128 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %123, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #7
  %129 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %125, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #7
  %130 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %126, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #7
  %131 = add <4 x i32> %127, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %132 = add <4 x i32> %131, %129
  %133 = add <4 x i32> %128, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %134 = add <4 x i32> %133, %130
  %135 = ashr <4 x i32> %132, <i32 16, i32 16, i32 16, i32 16>
  %136 = ashr <4 x i32> %134, <i32 16, i32 16, i32 16, i32 16>
  %137 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %135, <4 x i32> %136) #7
  %138 = getelementptr inbounds i8, i8* %1, i64 %95
  %139 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %116, <8 x i16> %137) #7
  %140 = bitcast i8* %138 to <16 x i8>*
  store <16 x i8> %139, <16 x i8>* %140, align 1
  %141 = add nsw i64 %95, 16
  %142 = shufflevector <16 x i8> %89, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %143 = shufflevector <16 x i8> %91, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %144 = shufflevector <16 x i8> %93, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %145 = bitcast <16 x i8> %142 to <8 x i16>
  %146 = bitcast <16 x i8> %143 to <8 x i16>
  %147 = shufflevector <8 x i16> %145, <8 x i16> %146, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %148 = shufflevector <8 x i16> %145, <8 x i16> %146, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %149 = bitcast <16 x i8> %144 to <8 x i16>
  %150 = shufflevector <8 x i16> %146, <8 x i16> %149, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %151 = shufflevector <8 x i16> %146, <8 x i16> %149, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %152 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %147, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #7
  %153 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %148, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #7
  %154 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %150, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #7
  %155 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %151, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #7
  %156 = add <4 x i32> %152, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %157 = add <4 x i32> %156, %154
  %158 = add <4 x i32> %153, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %159 = add <4 x i32> %158, %155
  %160 = ashr <4 x i32> %157, <i32 16, i32 16, i32 16, i32 16>
  %161 = ashr <4 x i32> %159, <i32 16, i32 16, i32 16, i32 16>
  %162 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %160, <4 x i32> %161) #7
  %163 = shufflevector <16 x i8> %89, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %164 = shufflevector <16 x i8> %91, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %165 = shufflevector <16 x i8> %93, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %166 = bitcast <16 x i8> %163 to <8 x i16>
  %167 = bitcast <16 x i8> %164 to <8 x i16>
  %168 = shufflevector <8 x i16> %166, <8 x i16> %167, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %169 = shufflevector <8 x i16> %166, <8 x i16> %167, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %170 = bitcast <16 x i8> %165 to <8 x i16>
  %171 = shufflevector <8 x i16> %167, <8 x i16> %170, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %172 = shufflevector <8 x i16> %167, <8 x i16> %170, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %173 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %168, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #7
  %174 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %169, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #7
  %175 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %171, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #7
  %176 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %172, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #7
  %177 = add <4 x i32> %173, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %178 = add <4 x i32> %177, %175
  %179 = add <4 x i32> %174, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %180 = add <4 x i32> %179, %176
  %181 = ashr <4 x i32> %178, <i32 16, i32 16, i32 16, i32 16>
  %182 = ashr <4 x i32> %180, <i32 16, i32 16, i32 16, i32 16>
  %183 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %181, <4 x i32> %182) #7
  %184 = getelementptr inbounds i8, i8* %1, i64 %141
  %185 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %162, <8 x i16> %183) #7
  %186 = bitcast i8* %184 to <16 x i8>*
  store <16 x i8> %185, <16 x i8>* %186, align 1
  %187 = add nsw i64 %95, 32
  %188 = trunc i64 %187 to i32
  %189 = getelementptr inbounds i8, i8* %45, i64 96
  %190 = icmp sgt i32 %4, %188
  br i1 %190, label %44, label %6

191:                                              ; preds = %39, %191
  %192 = phi i64 [ %230, %191 ], [ %40, %39 ]
  %193 = phi i8* [ %231, %191 ], [ %41, %39 ]
  %194 = load i8, i8* %193, align 1
  %195 = zext i8 %194 to i32
  %196 = getelementptr inbounds i8, i8* %193, i64 1
  %197 = load i8, i8* %196, align 1
  %198 = zext i8 %197 to i32
  %199 = getelementptr inbounds i8, i8* %193, i64 2
  %200 = load i8, i8* %199, align 1
  %201 = zext i8 %200 to i32
  %202 = mul nuw nsw i32 %195, 16839
  %203 = mul nuw nsw i32 %198, 33059
  %204 = mul nuw nsw i32 %201, 6420
  %205 = add nuw nsw i32 %202, 1081344
  %206 = add nuw nsw i32 %205, %203
  %207 = add nuw nsw i32 %206, %204
  %208 = lshr i32 %207, 16
  %209 = trunc i32 %208 to i8
  %210 = getelementptr inbounds i8, i8* %1, i64 %192
  store i8 %209, i8* %210, align 1
  %211 = add nsw i64 %192, 1
  %212 = getelementptr inbounds i8, i8* %193, i64 3
  %213 = load i8, i8* %212, align 1
  %214 = zext i8 %213 to i32
  %215 = getelementptr inbounds i8, i8* %193, i64 4
  %216 = load i8, i8* %215, align 1
  %217 = zext i8 %216 to i32
  %218 = getelementptr inbounds i8, i8* %193, i64 5
  %219 = load i8, i8* %218, align 1
  %220 = zext i8 %219 to i32
  %221 = mul nuw nsw i32 %214, 16839
  %222 = mul nuw nsw i32 %217, 33059
  %223 = mul nuw nsw i32 %220, 6420
  %224 = add nuw nsw i32 %221, 1081344
  %225 = add nuw nsw i32 %224, %222
  %226 = add nuw nsw i32 %225, %223
  %227 = lshr i32 %226, 16
  %228 = trunc i32 %227 to i8
  %229 = getelementptr inbounds i8, i8* %1, i64 %211
  store i8 %228, i8* %229, align 1
  %230 = add nsw i64 %192, 2
  %231 = getelementptr inbounds i8, i8* %193, i64 6
  %232 = icmp eq i64 %230, %14
  br i1 %232, label %233, label %191

233:                                              ; preds = %39, %191, %8
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @ConvertBGR24ToY_SSE2(i8* nocapture readonly, i8* nocapture, i32) #0 {
  %4 = and i32 %2, -32
  %5 = icmp sgt i32 %4, 0
  br i1 %5, label %44, label %8

6:                                                ; preds = %44
  %7 = trunc i64 %187 to i32
  br label %8

8:                                                ; preds = %6, %3
  %9 = phi i32 [ 0, %3 ], [ %7, %6 ]
  %10 = phi i8* [ %0, %3 ], [ %189, %6 ]
  %11 = icmp slt i32 %9, %2
  br i1 %11, label %12, label %233

12:                                               ; preds = %8
  %13 = sext i32 %9 to i64
  %14 = sext i32 %2 to i64
  %15 = sub nsw i64 %14, %13
  %16 = xor i64 %13, -1
  %17 = and i64 %15, 1
  %18 = icmp eq i64 %17, 0
  br i1 %18, label %39, label %19

19:                                               ; preds = %12
  %20 = getelementptr inbounds i8, i8* %10, i64 2
  %21 = load i8, i8* %20, align 1
  %22 = zext i8 %21 to i32
  %23 = getelementptr inbounds i8, i8* %10, i64 1
  %24 = load i8, i8* %23, align 1
  %25 = zext i8 %24 to i32
  %26 = load i8, i8* %10, align 1
  %27 = zext i8 %26 to i32
  %28 = mul nuw nsw i32 %22, 16839
  %29 = mul nuw nsw i32 %25, 33059
  %30 = mul nuw nsw i32 %27, 6420
  %31 = add nuw nsw i32 %28, 1081344
  %32 = add nuw nsw i32 %31, %29
  %33 = add nuw nsw i32 %32, %30
  %34 = lshr i32 %33, 16
  %35 = trunc i32 %34 to i8
  %36 = getelementptr inbounds i8, i8* %1, i64 %13
  store i8 %35, i8* %36, align 1
  %37 = add nsw i64 %13, 1
  %38 = getelementptr inbounds i8, i8* %10, i64 3
  br label %39

39:                                               ; preds = %12, %19
  %40 = phi i64 [ %13, %12 ], [ %37, %19 ]
  %41 = phi i8* [ %10, %12 ], [ %38, %19 ]
  %42 = sub nsw i64 0, %14
  %43 = icmp eq i64 %16, %42
  br i1 %43, label %233, label %191

44:                                               ; preds = %3, %44
  %45 = phi i8* [ %189, %44 ], [ %0, %3 ]
  %46 = phi i64 [ %187, %44 ], [ 0, %3 ]
  %47 = bitcast i8* %45 to <16 x i8>*
  %48 = load <16 x i8>, <16 x i8>* %47, align 1
  %49 = getelementptr inbounds i8, i8* %45, i64 16
  %50 = bitcast i8* %49 to <16 x i8>*
  %51 = load <16 x i8>, <16 x i8>* %50, align 1
  %52 = getelementptr inbounds i8, i8* %45, i64 32
  %53 = bitcast i8* %52 to <16 x i8>*
  %54 = load <16 x i8>, <16 x i8>* %53, align 1
  %55 = getelementptr inbounds i8, i8* %45, i64 48
  %56 = bitcast i8* %55 to <16 x i8>*
  %57 = load <16 x i8>, <16 x i8>* %56, align 1
  %58 = getelementptr inbounds i8, i8* %45, i64 64
  %59 = bitcast i8* %58 to <16 x i8>*
  %60 = load <16 x i8>, <16 x i8>* %59, align 1
  %61 = getelementptr inbounds i8, i8* %45, i64 80
  %62 = bitcast i8* %61 to <16 x i8>*
  %63 = load <16 x i8>, <16 x i8>* %62, align 1
  %64 = shufflevector <16 x i8> %48, <16 x i8> %57, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %65 = shufflevector <16 x i8> %48, <16 x i8> %57, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %66 = shufflevector <16 x i8> %51, <16 x i8> %60, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %67 = shufflevector <16 x i8> %51, <16 x i8> %60, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %68 = shufflevector <16 x i8> %54, <16 x i8> %63, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %69 = shufflevector <16 x i8> %54, <16 x i8> %63, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %70 = shufflevector <16 x i8> %64, <16 x i8> %67, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %71 = shufflevector <16 x i8> %64, <16 x i8> %67, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %72 = shufflevector <16 x i8> %65, <16 x i8> %68, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %73 = shufflevector <16 x i8> %65, <16 x i8> %68, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %74 = shufflevector <16 x i8> %66, <16 x i8> %69, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %75 = shufflevector <16 x i8> %66, <16 x i8> %69, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %76 = shufflevector <16 x i8> %70, <16 x i8> %73, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %77 = shufflevector <16 x i8> %70, <16 x i8> %73, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %78 = shufflevector <16 x i8> %71, <16 x i8> %74, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %79 = shufflevector <16 x i8> %71, <16 x i8> %74, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %80 = shufflevector <16 x i8> %72, <16 x i8> %75, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %81 = shufflevector <16 x i8> %72, <16 x i8> %75, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %82 = shufflevector <16 x i8> %76, <16 x i8> %79, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %83 = shufflevector <16 x i8> %76, <16 x i8> %79, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %84 = shufflevector <16 x i8> %77, <16 x i8> %80, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %85 = shufflevector <16 x i8> %77, <16 x i8> %80, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %86 = shufflevector <16 x i8> %78, <16 x i8> %81, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %87 = shufflevector <16 x i8> %78, <16 x i8> %81, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %88 = shufflevector <16 x i8> %82, <16 x i8> %85, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %89 = shufflevector <16 x i8> %82, <16 x i8> %85, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %90 = shufflevector <16 x i8> %83, <16 x i8> %86, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %91 = shufflevector <16 x i8> %83, <16 x i8> %86, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %92 = shufflevector <16 x i8> %84, <16 x i8> %87, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %93 = shufflevector <16 x i8> %84, <16 x i8> %87, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %94 = shl i64 %46, 32
  %95 = ashr exact i64 %94, 32
  %96 = shufflevector <16 x i8> %88, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %97 = shufflevector <16 x i8> %90, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %98 = shufflevector <16 x i8> %92, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %99 = bitcast <16 x i8> %98 to <8 x i16>
  %100 = bitcast <16 x i8> %97 to <8 x i16>
  %101 = shufflevector <8 x i16> %99, <8 x i16> %100, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %102 = shufflevector <8 x i16> %99, <8 x i16> %100, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %103 = bitcast <16 x i8> %96 to <8 x i16>
  %104 = shufflevector <8 x i16> %100, <8 x i16> %103, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %105 = shufflevector <8 x i16> %100, <8 x i16> %103, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %106 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %101, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #7
  %107 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %102, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #7
  %108 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %104, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #7
  %109 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %105, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #7
  %110 = add <4 x i32> %106, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %111 = add <4 x i32> %110, %108
  %112 = add <4 x i32> %107, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %113 = add <4 x i32> %112, %109
  %114 = ashr <4 x i32> %111, <i32 16, i32 16, i32 16, i32 16>
  %115 = ashr <4 x i32> %113, <i32 16, i32 16, i32 16, i32 16>
  %116 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %114, <4 x i32> %115) #7
  %117 = shufflevector <16 x i8> %88, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %118 = shufflevector <16 x i8> %90, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %119 = shufflevector <16 x i8> %92, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %120 = bitcast <16 x i8> %119 to <8 x i16>
  %121 = bitcast <16 x i8> %118 to <8 x i16>
  %122 = shufflevector <8 x i16> %120, <8 x i16> %121, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %123 = shufflevector <8 x i16> %120, <8 x i16> %121, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %124 = bitcast <16 x i8> %117 to <8 x i16>
  %125 = shufflevector <8 x i16> %121, <8 x i16> %124, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %126 = shufflevector <8 x i16> %121, <8 x i16> %124, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %127 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %122, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #7
  %128 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %123, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #7
  %129 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %125, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #7
  %130 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %126, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #7
  %131 = add <4 x i32> %127, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %132 = add <4 x i32> %131, %129
  %133 = add <4 x i32> %128, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %134 = add <4 x i32> %133, %130
  %135 = ashr <4 x i32> %132, <i32 16, i32 16, i32 16, i32 16>
  %136 = ashr <4 x i32> %134, <i32 16, i32 16, i32 16, i32 16>
  %137 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %135, <4 x i32> %136) #7
  %138 = getelementptr inbounds i8, i8* %1, i64 %95
  %139 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %116, <8 x i16> %137) #7
  %140 = bitcast i8* %138 to <16 x i8>*
  store <16 x i8> %139, <16 x i8>* %140, align 1
  %141 = add nsw i64 %95, 16
  %142 = shufflevector <16 x i8> %89, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %143 = shufflevector <16 x i8> %91, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %144 = shufflevector <16 x i8> %93, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %145 = bitcast <16 x i8> %144 to <8 x i16>
  %146 = bitcast <16 x i8> %143 to <8 x i16>
  %147 = shufflevector <8 x i16> %145, <8 x i16> %146, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %148 = shufflevector <8 x i16> %145, <8 x i16> %146, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %149 = bitcast <16 x i8> %142 to <8 x i16>
  %150 = shufflevector <8 x i16> %146, <8 x i16> %149, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %151 = shufflevector <8 x i16> %146, <8 x i16> %149, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %152 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %147, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #7
  %153 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %148, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #7
  %154 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %150, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #7
  %155 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %151, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #7
  %156 = add <4 x i32> %152, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %157 = add <4 x i32> %156, %154
  %158 = add <4 x i32> %153, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %159 = add <4 x i32> %158, %155
  %160 = ashr <4 x i32> %157, <i32 16, i32 16, i32 16, i32 16>
  %161 = ashr <4 x i32> %159, <i32 16, i32 16, i32 16, i32 16>
  %162 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %160, <4 x i32> %161) #7
  %163 = shufflevector <16 x i8> %89, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %164 = shufflevector <16 x i8> %91, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %165 = shufflevector <16 x i8> %93, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %166 = bitcast <16 x i8> %165 to <8 x i16>
  %167 = bitcast <16 x i8> %164 to <8 x i16>
  %168 = shufflevector <8 x i16> %166, <8 x i16> %167, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %169 = shufflevector <8 x i16> %166, <8 x i16> %167, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %170 = bitcast <16 x i8> %163 to <8 x i16>
  %171 = shufflevector <8 x i16> %167, <8 x i16> %170, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %172 = shufflevector <8 x i16> %167, <8 x i16> %170, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %173 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %168, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #7
  %174 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %169, <8 x i16> <i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675, i16 16839, i16 16675>) #7
  %175 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %171, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #7
  %176 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %172, <8 x i16> <i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420, i16 16384, i16 6420>) #7
  %177 = add <4 x i32> %173, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %178 = add <4 x i32> %177, %175
  %179 = add <4 x i32> %174, <i32 1081344, i32 1081344, i32 1081344, i32 1081344>
  %180 = add <4 x i32> %179, %176
  %181 = ashr <4 x i32> %178, <i32 16, i32 16, i32 16, i32 16>
  %182 = ashr <4 x i32> %180, <i32 16, i32 16, i32 16, i32 16>
  %183 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %181, <4 x i32> %182) #7
  %184 = getelementptr inbounds i8, i8* %1, i64 %141
  %185 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %162, <8 x i16> %183) #7
  %186 = bitcast i8* %184 to <16 x i8>*
  store <16 x i8> %185, <16 x i8>* %186, align 1
  %187 = add nsw i64 %95, 32
  %188 = trunc i64 %187 to i32
  %189 = getelementptr inbounds i8, i8* %45, i64 96
  %190 = icmp sgt i32 %4, %188
  br i1 %190, label %44, label %6

191:                                              ; preds = %39, %191
  %192 = phi i64 [ %230, %191 ], [ %40, %39 ]
  %193 = phi i8* [ %231, %191 ], [ %41, %39 ]
  %194 = getelementptr inbounds i8, i8* %193, i64 2
  %195 = load i8, i8* %194, align 1
  %196 = zext i8 %195 to i32
  %197 = getelementptr inbounds i8, i8* %193, i64 1
  %198 = load i8, i8* %197, align 1
  %199 = zext i8 %198 to i32
  %200 = load i8, i8* %193, align 1
  %201 = zext i8 %200 to i32
  %202 = mul nuw nsw i32 %196, 16839
  %203 = mul nuw nsw i32 %199, 33059
  %204 = mul nuw nsw i32 %201, 6420
  %205 = add nuw nsw i32 %202, 1081344
  %206 = add nuw nsw i32 %205, %203
  %207 = add nuw nsw i32 %206, %204
  %208 = lshr i32 %207, 16
  %209 = trunc i32 %208 to i8
  %210 = getelementptr inbounds i8, i8* %1, i64 %192
  store i8 %209, i8* %210, align 1
  %211 = add nsw i64 %192, 1
  %212 = getelementptr inbounds i8, i8* %193, i64 3
  %213 = getelementptr inbounds i8, i8* %193, i64 5
  %214 = load i8, i8* %213, align 1
  %215 = zext i8 %214 to i32
  %216 = getelementptr inbounds i8, i8* %193, i64 4
  %217 = load i8, i8* %216, align 1
  %218 = zext i8 %217 to i32
  %219 = load i8, i8* %212, align 1
  %220 = zext i8 %219 to i32
  %221 = mul nuw nsw i32 %215, 16839
  %222 = mul nuw nsw i32 %218, 33059
  %223 = mul nuw nsw i32 %220, 6420
  %224 = add nuw nsw i32 %221, 1081344
  %225 = add nuw nsw i32 %224, %222
  %226 = add nuw nsw i32 %225, %223
  %227 = lshr i32 %226, 16
  %228 = trunc i32 %227 to i8
  %229 = getelementptr inbounds i8, i8* %1, i64 %211
  store i8 %228, i8* %229, align 1
  %230 = add nsw i64 %192, 2
  %231 = getelementptr inbounds i8, i8* %193, i64 6
  %232 = icmp eq i64 %230, %14
  br i1 %232, label %233, label %191

233:                                              ; preds = %39, %191, %8
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @ConvertRGBA32ToUV_SSE2(i16*, i8*, i8*, i32) #0 {
  %5 = and i32 %3, -16
  %6 = shl nsw i32 %5, 2
  %7 = sext i32 %6 to i64
  %8 = getelementptr inbounds i16, i16* %0, i64 %7
  %9 = icmp sgt i32 %5, 0
  br i1 %9, label %10, label %133

10:                                               ; preds = %4, %10
  %11 = phi i16* [ %131, %10 ], [ %0, %4 ]
  %12 = phi i8* [ %130, %10 ], [ %2, %4 ]
  %13 = phi i8* [ %129, %10 ], [ %1, %4 ]
  %14 = bitcast i16* %11 to <8 x i16>*
  %15 = load <8 x i16>, <8 x i16>* %14, align 1
  %16 = getelementptr inbounds i16, i16* %11, i64 8
  %17 = bitcast i16* %16 to <8 x i16>*
  %18 = load <8 x i16>, <8 x i16>* %17, align 1
  %19 = getelementptr inbounds i16, i16* %11, i64 16
  %20 = bitcast i16* %19 to <8 x i16>*
  %21 = load <8 x i16>, <8 x i16>* %20, align 1
  %22 = getelementptr inbounds i16, i16* %11, i64 24
  %23 = bitcast i16* %22 to <8 x i16>*
  %24 = load <8 x i16>, <8 x i16>* %23, align 1
  %25 = shufflevector <8 x i16> %15, <8 x i16> %18, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %26 = shufflevector <8 x i16> %15, <8 x i16> %18, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %27 = shufflevector <8 x i16> %21, <8 x i16> %24, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %28 = shufflevector <8 x i16> %21, <8 x i16> %24, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %29 = shufflevector <8 x i16> %25, <8 x i16> %26, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %30 = bitcast <8 x i16> %29 to <2 x i64>
  %31 = shufflevector <8 x i16> %25, <8 x i16> %26, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 undef, i32 undef, i32 undef, i32 undef>
  %32 = bitcast <8 x i16> %31 to <2 x i64>
  %33 = shufflevector <8 x i16> %27, <8 x i16> %28, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %34 = bitcast <8 x i16> %33 to <2 x i64>
  %35 = shufflevector <8 x i16> %27, <8 x i16> %28, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 undef, i32 undef, i32 undef, i32 undef>
  %36 = bitcast <8 x i16> %35 to <2 x i64>
  %37 = shufflevector <2 x i64> %30, <2 x i64> %34, <2 x i32> <i32 0, i32 2>
  %38 = shufflevector <2 x i64> %30, <2 x i64> %34, <2 x i32> <i32 1, i32 3>
  %39 = shufflevector <2 x i64> %32, <2 x i64> %36, <2 x i32> <i32 0, i32 2>
  %40 = bitcast <2 x i64> %37 to <8 x i16>
  %41 = bitcast <2 x i64> %38 to <8 x i16>
  %42 = shufflevector <8 x i16> %40, <8 x i16> %41, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %43 = shufflevector <8 x i16> %40, <8 x i16> %41, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %44 = bitcast <2 x i64> %39 to <8 x i16>
  %45 = shufflevector <8 x i16> %41, <8 x i16> %44, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %46 = shufflevector <8 x i16> %41, <8 x i16> %44, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %47 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %42, <8 x i16> <i16 -9719, i16 -19081, i16 -9719, i16 -19081, i16 -9719, i16 -19081, i16 -9719, i16 -19081>) #7
  %48 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %43, <8 x i16> <i16 -9719, i16 -19081, i16 -9719, i16 -19081, i16 -9719, i16 -19081, i16 -9719, i16 -19081>) #7
  %49 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %45, <8 x i16> <i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800>) #7
  %50 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %46, <8 x i16> <i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800>) #7
  %51 = add <4 x i32> %47, <i32 33685504, i32 33685504, i32 33685504, i32 33685504>
  %52 = add <4 x i32> %51, %49
  %53 = add <4 x i32> %48, <i32 33685504, i32 33685504, i32 33685504, i32 33685504>
  %54 = add <4 x i32> %53, %50
  %55 = ashr <4 x i32> %52, <i32 18, i32 18, i32 18, i32 18>
  %56 = ashr <4 x i32> %54, <i32 18, i32 18, i32 18, i32 18>
  %57 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %55, <4 x i32> %56) #7
  %58 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %42, <8 x i16> <i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0>) #7
  %59 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %43, <8 x i16> <i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0>) #7
  %60 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %45, <8 x i16> <i16 -24116, i16 -4684, i16 -24116, i16 -4684, i16 -24116, i16 -4684, i16 -24116, i16 -4684>) #7
  %61 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %46, <8 x i16> <i16 -24116, i16 -4684, i16 -24116, i16 -4684, i16 -24116, i16 -4684, i16 -24116, i16 -4684>) #7
  %62 = add <4 x i32> %58, <i32 33685504, i32 33685504, i32 33685504, i32 33685504>
  %63 = add <4 x i32> %62, %60
  %64 = add <4 x i32> %59, <i32 33685504, i32 33685504, i32 33685504, i32 33685504>
  %65 = add <4 x i32> %64, %61
  %66 = ashr <4 x i32> %63, <i32 18, i32 18, i32 18, i32 18>
  %67 = ashr <4 x i32> %65, <i32 18, i32 18, i32 18, i32 18>
  %68 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %66, <4 x i32> %67) #7
  %69 = getelementptr inbounds i16, i16* %11, i64 32
  %70 = bitcast i16* %69 to <8 x i16>*
  %71 = load <8 x i16>, <8 x i16>* %70, align 1
  %72 = getelementptr inbounds i16, i16* %11, i64 40
  %73 = bitcast i16* %72 to <8 x i16>*
  %74 = load <8 x i16>, <8 x i16>* %73, align 1
  %75 = getelementptr inbounds i16, i16* %11, i64 48
  %76 = bitcast i16* %75 to <8 x i16>*
  %77 = load <8 x i16>, <8 x i16>* %76, align 1
  %78 = getelementptr inbounds i16, i16* %11, i64 56
  %79 = bitcast i16* %78 to <8 x i16>*
  %80 = load <8 x i16>, <8 x i16>* %79, align 1
  %81 = shufflevector <8 x i16> %71, <8 x i16> %74, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %82 = shufflevector <8 x i16> %71, <8 x i16> %74, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %83 = shufflevector <8 x i16> %77, <8 x i16> %80, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %84 = shufflevector <8 x i16> %77, <8 x i16> %80, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %85 = shufflevector <8 x i16> %81, <8 x i16> %82, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %86 = bitcast <8 x i16> %85 to <2 x i64>
  %87 = shufflevector <8 x i16> %81, <8 x i16> %82, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 undef, i32 undef, i32 undef, i32 undef>
  %88 = bitcast <8 x i16> %87 to <2 x i64>
  %89 = shufflevector <8 x i16> %83, <8 x i16> %84, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %90 = bitcast <8 x i16> %89 to <2 x i64>
  %91 = shufflevector <8 x i16> %83, <8 x i16> %84, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 undef, i32 undef, i32 undef, i32 undef>
  %92 = bitcast <8 x i16> %91 to <2 x i64>
  %93 = shufflevector <2 x i64> %86, <2 x i64> %90, <2 x i32> <i32 0, i32 2>
  %94 = shufflevector <2 x i64> %86, <2 x i64> %90, <2 x i32> <i32 1, i32 3>
  %95 = shufflevector <2 x i64> %88, <2 x i64> %92, <2 x i32> <i32 0, i32 2>
  %96 = bitcast <2 x i64> %93 to <8 x i16>
  %97 = bitcast <2 x i64> %94 to <8 x i16>
  %98 = shufflevector <8 x i16> %96, <8 x i16> %97, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %99 = shufflevector <8 x i16> %96, <8 x i16> %97, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %100 = bitcast <2 x i64> %95 to <8 x i16>
  %101 = shufflevector <8 x i16> %97, <8 x i16> %100, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %102 = shufflevector <8 x i16> %97, <8 x i16> %100, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %103 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %98, <8 x i16> <i16 -9719, i16 -19081, i16 -9719, i16 -19081, i16 -9719, i16 -19081, i16 -9719, i16 -19081>) #7
  %104 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %99, <8 x i16> <i16 -9719, i16 -19081, i16 -9719, i16 -19081, i16 -9719, i16 -19081, i16 -9719, i16 -19081>) #7
  %105 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %101, <8 x i16> <i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800>) #7
  %106 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %102, <8 x i16> <i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800>) #7
  %107 = add <4 x i32> %103, <i32 33685504, i32 33685504, i32 33685504, i32 33685504>
  %108 = add <4 x i32> %107, %105
  %109 = add <4 x i32> %104, <i32 33685504, i32 33685504, i32 33685504, i32 33685504>
  %110 = add <4 x i32> %109, %106
  %111 = ashr <4 x i32> %108, <i32 18, i32 18, i32 18, i32 18>
  %112 = ashr <4 x i32> %110, <i32 18, i32 18, i32 18, i32 18>
  %113 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %111, <4 x i32> %112) #7
  %114 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %98, <8 x i16> <i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0>) #7
  %115 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %99, <8 x i16> <i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0, i16 28800, i16 0>) #7
  %116 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %101, <8 x i16> <i16 -24116, i16 -4684, i16 -24116, i16 -4684, i16 -24116, i16 -4684, i16 -24116, i16 -4684>) #7
  %117 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %102, <8 x i16> <i16 -24116, i16 -4684, i16 -24116, i16 -4684, i16 -24116, i16 -4684, i16 -24116, i16 -4684>) #7
  %118 = add <4 x i32> %114, <i32 33685504, i32 33685504, i32 33685504, i32 33685504>
  %119 = add <4 x i32> %118, %116
  %120 = add <4 x i32> %115, <i32 33685504, i32 33685504, i32 33685504, i32 33685504>
  %121 = add <4 x i32> %120, %117
  %122 = ashr <4 x i32> %119, <i32 18, i32 18, i32 18, i32 18>
  %123 = ashr <4 x i32> %121, <i32 18, i32 18, i32 18, i32 18>
  %124 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %122, <4 x i32> %123) #7
  %125 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %57, <8 x i16> %113) #7
  %126 = bitcast i8* %13 to <16 x i8>*
  store <16 x i8> %125, <16 x i8>* %126, align 1
  %127 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %68, <8 x i16> %124) #7
  %128 = bitcast i8* %12 to <16 x i8>*
  store <16 x i8> %127, <16 x i8>* %128, align 1
  %129 = getelementptr inbounds i8, i8* %13, i64 16
  %130 = getelementptr inbounds i8, i8* %12, i64 16
  %131 = getelementptr inbounds i16, i16* %11, i64 64
  %132 = icmp ult i16* %131, %8
  br i1 %132, label %10, label %133

133:                                              ; preds = %10, %4
  %134 = phi i8* [ %1, %4 ], [ %129, %10 ]
  %135 = phi i8* [ %2, %4 ], [ %130, %10 ]
  %136 = phi i16* [ %0, %4 ], [ %131, %10 ]
  %137 = icmp slt i32 %5, %3
  br i1 %137, label %138, label %140

138:                                              ; preds = %133
  %139 = sub nsw i32 %3, %5
  tail call void @WebPConvertRGBA32ToUV_C(i16* %136, i8* %134, i8* %135, i32 %139) #7
  br label %140

140:                                              ; preds = %138, %133
  ret void
}

; Function Attrs: nofree norecurse nounwind ssp uwtable writeonly
define hidden void @WebPInitSharpYUVSSE2() local_unnamed_addr #1 {
  store i64 (i16*, i16*, i16*, i32)* @SharpYUVUpdateY_SSE2, i64 (i16*, i16*, i16*, i32)** @WebPSharpYUVUpdateY, align 8
  store void (i16*, i16*, i16*, i32)* @SharpYUVUpdateRGB_SSE2, void (i16*, i16*, i16*, i32)** @WebPSharpYUVUpdateRGB, align 8
  store void (i16*, i16*, i32, i16*, i16*)* @SharpYUVFilterRow_SSE2, void (i16*, i16*, i32, i16*, i16*)** @WebPSharpYUVFilterRow, align 8
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal i64 @SharpYUVUpdateY_SSE2(i16* nocapture readonly, i16* nocapture readonly, i16* nocapture, i32) #0 {
  %5 = icmp slt i32 %3, 8
  br i1 %5, label %40, label %6

6:                                                ; preds = %4
  %7 = sext i32 %3 to i64
  br label %8

8:                                                ; preds = %6, %8
  %9 = phi i64 [ 0, %6 ], [ %33, %8 ]
  %10 = phi i64 [ 8, %6 ], [ %31, %8 ]
  %11 = phi <4 x i32> [ zeroinitializer, %6 ], [ %30, %8 ]
  %12 = getelementptr inbounds i16, i16* %0, i64 %9
  %13 = bitcast i16* %12 to <8 x i16>*
  %14 = load <8 x i16>, <8 x i16>* %13, align 1
  %15 = getelementptr inbounds i16, i16* %1, i64 %9
  %16 = bitcast i16* %15 to <8 x i16>*
  %17 = load <8 x i16>, <8 x i16>* %16, align 1
  %18 = getelementptr inbounds i16, i16* %2, i64 %9
  %19 = bitcast i16* %18 to <8 x i16>*
  %20 = load <8 x i16>, <8 x i16>* %19, align 1
  %21 = sub <8 x i16> %14, %17
  %22 = ashr <8 x i16> %21, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %23 = add <8 x i16> %21, %20
  %24 = icmp slt <8 x i16> %23, <i16 1023, i16 1023, i16 1023, i16 1023, i16 1023, i16 1023, i16 1023, i16 1023>
  %25 = select <8 x i1> %24, <8 x i16> %23, <8 x i16> <i16 1023, i16 1023, i16 1023, i16 1023, i16 1023, i16 1023, i16 1023, i16 1023>
  %26 = icmp sgt <8 x i16> %25, zeroinitializer
  %27 = select <8 x i1> %26, <8 x i16> %25, <8 x i16> zeroinitializer
  %28 = or <8 x i16> %22, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %29 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %21, <8 x i16> %28) #7
  store <8 x i16> %27, <8 x i16>* %19, align 1
  %30 = add <4 x i32> %29, %11
  %31 = add nuw nsw i64 %10, 8
  %32 = icmp sgt i64 %31, %7
  %33 = add nuw nsw i64 %9, 8
  br i1 %32, label %34, label %8

34:                                               ; preds = %8
  %35 = and i32 %3, -8
  %36 = extractelement <4 x i32> %30, i32 0
  %37 = extractelement <4 x i32> %30, i32 1
  %38 = extractelement <4 x i32> %30, i32 2
  %39 = extractelement <4 x i32> %30, i32 3
  br label %40

40:                                               ; preds = %34, %4
  %41 = phi i32 [ %36, %34 ], [ 0, %4 ]
  %42 = phi i32 [ %37, %34 ], [ 0, %4 ]
  %43 = phi i32 [ %38, %34 ], [ 0, %4 ]
  %44 = phi i32 [ %39, %34 ], [ 0, %4 ]
  %45 = phi i32 [ %35, %34 ], [ 0, %4 ]
  %46 = add i32 %43, %44
  %47 = add i32 %46, %42
  %48 = add i32 %47, %41
  %49 = zext i32 %48 to i64
  %50 = icmp slt i32 %45, %3
  br i1 %50, label %51, label %80

51:                                               ; preds = %40
  %52 = zext i32 %45 to i64
  %53 = zext i32 %3 to i64
  br label %54

54:                                               ; preds = %54, %51
  %55 = phi i64 [ %52, %51 ], [ %78, %54 ]
  %56 = phi i64 [ %49, %51 ], [ %77, %54 ]
  %57 = getelementptr inbounds i16, i16* %0, i64 %55
  %58 = load i16, i16* %57, align 2
  %59 = zext i16 %58 to i32
  %60 = getelementptr inbounds i16, i16* %1, i64 %55
  %61 = load i16, i16* %60, align 2
  %62 = zext i16 %61 to i32
  %63 = sub nsw i32 %59, %62
  %64 = getelementptr inbounds i16, i16* %2, i64 %55
  %65 = load i16, i16* %64, align 2
  %66 = zext i16 %65 to i32
  %67 = add nsw i32 %63, %66
  %68 = icmp slt i32 %67, 1023
  %69 = select i1 %68, i32 %67, i32 1023
  %70 = icmp sgt i32 %69, 0
  %71 = select i1 %70, i32 %69, i32 0
  %72 = trunc i32 %71 to i16
  store i16 %72, i16* %64, align 2
  %73 = icmp slt i32 %63, 0
  %74 = sub nsw i32 0, %63
  %75 = select i1 %73, i32 %74, i32 %63
  %76 = zext i32 %75 to i64
  %77 = add i64 %56, %76
  %78 = add nuw nsw i64 %55, 1
  %79 = icmp eq i64 %78, %53
  br i1 %79, label %80, label %54

80:                                               ; preds = %54, %40
  %81 = phi i64 [ %49, %40 ], [ %77, %54 ]
  ret i64 %81
}

; Function Attrs: nofree norecurse nounwind ssp uwtable
define internal void @SharpYUVUpdateRGB_SSE2(i16* nocapture readonly, i16* nocapture readonly, i16* nocapture, i32) #2 {
  %5 = icmp slt i32 %3, 8
  br i1 %5, label %10, label %6

6:                                                ; preds = %4
  %7 = sext i32 %3 to i64
  br label %86

8:                                                ; preds = %86
  %9 = and i32 %3, -8
  br label %10

10:                                               ; preds = %8, %4
  %11 = phi i32 [ 0, %4 ], [ %9, %8 ]
  %12 = icmp slt i32 %11, %3
  br i1 %12, label %13, label %124

13:                                               ; preds = %10
  %14 = zext i32 %11 to i64
  %15 = zext i32 %3 to i64
  %16 = sub nsw i64 %15, %14
  %17 = icmp ult i64 %16, 16
  br i1 %17, label %18, label %38

18:                                               ; preds = %84, %38, %13
  %19 = phi i64 [ %14, %38 ], [ %14, %13 ], [ %54, %84 ]
  %20 = sub nsw i64 %15, %19
  %21 = xor i64 %19, -1
  %22 = and i64 %20, 1
  %23 = icmp eq i64 %22, 0
  br i1 %23, label %34, label %24

24:                                               ; preds = %18
  %25 = getelementptr inbounds i16, i16* %0, i64 %19
  %26 = load i16, i16* %25, align 2
  %27 = getelementptr inbounds i16, i16* %1, i64 %19
  %28 = load i16, i16* %27, align 2
  %29 = sub i16 %26, %28
  %30 = getelementptr inbounds i16, i16* %2, i64 %19
  %31 = load i16, i16* %30, align 2
  %32 = add i16 %29, %31
  store i16 %32, i16* %30, align 2
  %33 = add nuw nsw i64 %19, 1
  br label %34

34:                                               ; preds = %18, %24
  %35 = phi i64 [ %19, %18 ], [ %33, %24 ]
  %36 = sub nsw i64 0, %15
  %37 = icmp eq i64 %21, %36
  br i1 %37, label %124, label %103

38:                                               ; preds = %13
  %39 = getelementptr i16, i16* %2, i64 %14
  %40 = getelementptr i16, i16* %2, i64 %15
  %41 = getelementptr i16, i16* %0, i64 %14
  %42 = getelementptr i16, i16* %0, i64 %15
  %43 = getelementptr i16, i16* %1, i64 %14
  %44 = getelementptr i16, i16* %1, i64 %15
  %45 = icmp ult i16* %39, %42
  %46 = icmp ult i16* %41, %40
  %47 = and i1 %45, %46
  %48 = icmp ult i16* %39, %44
  %49 = icmp ult i16* %43, %40
  %50 = and i1 %48, %49
  %51 = or i1 %47, %50
  br i1 %51, label %18, label %52

52:                                               ; preds = %38
  %53 = and i64 %16, -16
  %54 = add nsw i64 %53, %14
  br label %55

55:                                               ; preds = %55, %52
  %56 = phi i64 [ 0, %52 ], [ %82, %55 ]
  %57 = add i64 %56, %14
  %58 = getelementptr inbounds i16, i16* %0, i64 %57
  %59 = bitcast i16* %58 to <8 x i16>*
  %60 = load <8 x i16>, <8 x i16>* %59, align 2, !alias.scope !10
  %61 = getelementptr inbounds i16, i16* %58, i64 8
  %62 = bitcast i16* %61 to <8 x i16>*
  %63 = load <8 x i16>, <8 x i16>* %62, align 2, !alias.scope !10
  %64 = getelementptr inbounds i16, i16* %1, i64 %57
  %65 = bitcast i16* %64 to <8 x i16>*
  %66 = load <8 x i16>, <8 x i16>* %65, align 2, !alias.scope !13
  %67 = getelementptr inbounds i16, i16* %64, i64 8
  %68 = bitcast i16* %67 to <8 x i16>*
  %69 = load <8 x i16>, <8 x i16>* %68, align 2, !alias.scope !13
  %70 = sub <8 x i16> %60, %66
  %71 = sub <8 x i16> %63, %69
  %72 = getelementptr inbounds i16, i16* %2, i64 %57
  %73 = bitcast i16* %72 to <8 x i16>*
  %74 = load <8 x i16>, <8 x i16>* %73, align 2, !alias.scope !15, !noalias !17
  %75 = getelementptr inbounds i16, i16* %72, i64 8
  %76 = bitcast i16* %75 to <8 x i16>*
  %77 = load <8 x i16>, <8 x i16>* %76, align 2, !alias.scope !15, !noalias !17
  %78 = add <8 x i16> %70, %74
  %79 = add <8 x i16> %71, %77
  %80 = bitcast i16* %72 to <8 x i16>*
  store <8 x i16> %78, <8 x i16>* %80, align 2, !alias.scope !15, !noalias !17
  %81 = bitcast i16* %75 to <8 x i16>*
  store <8 x i16> %79, <8 x i16>* %81, align 2, !alias.scope !15, !noalias !17
  %82 = add i64 %56, 16
  %83 = icmp eq i64 %82, %53
  br i1 %83, label %84, label %55, !llvm.loop !18

84:                                               ; preds = %55
  %85 = icmp eq i64 %16, %53
  br i1 %85, label %124, label %18

86:                                               ; preds = %6, %86
  %87 = phi i64 [ 0, %6 ], [ %102, %86 ]
  %88 = phi i64 [ 8, %6 ], [ %100, %86 ]
  %89 = getelementptr inbounds i16, i16* %0, i64 %87
  %90 = bitcast i16* %89 to <8 x i16>*
  %91 = load <8 x i16>, <8 x i16>* %90, align 1
  %92 = getelementptr inbounds i16, i16* %1, i64 %87
  %93 = bitcast i16* %92 to <8 x i16>*
  %94 = load <8 x i16>, <8 x i16>* %93, align 1
  %95 = getelementptr inbounds i16, i16* %2, i64 %87
  %96 = bitcast i16* %95 to <8 x i16>*
  %97 = load <8 x i16>, <8 x i16>* %96, align 1
  %98 = sub <8 x i16> %91, %94
  %99 = add <8 x i16> %98, %97
  store <8 x i16> %99, <8 x i16>* %96, align 1
  %100 = add nuw nsw i64 %88, 8
  %101 = icmp sgt i64 %100, %7
  %102 = add nuw nsw i64 %87, 8
  br i1 %101, label %8, label %86

103:                                              ; preds = %34, %103
  %104 = phi i64 [ %122, %103 ], [ %35, %34 ]
  %105 = getelementptr inbounds i16, i16* %0, i64 %104
  %106 = load i16, i16* %105, align 2
  %107 = getelementptr inbounds i16, i16* %1, i64 %104
  %108 = load i16, i16* %107, align 2
  %109 = sub i16 %106, %108
  %110 = getelementptr inbounds i16, i16* %2, i64 %104
  %111 = load i16, i16* %110, align 2
  %112 = add i16 %109, %111
  store i16 %112, i16* %110, align 2
  %113 = add nuw nsw i64 %104, 1
  %114 = getelementptr inbounds i16, i16* %0, i64 %113
  %115 = load i16, i16* %114, align 2
  %116 = getelementptr inbounds i16, i16* %1, i64 %113
  %117 = load i16, i16* %116, align 2
  %118 = sub i16 %115, %117
  %119 = getelementptr inbounds i16, i16* %2, i64 %113
  %120 = load i16, i16* %119, align 2
  %121 = add i16 %118, %120
  store i16 %121, i16* %119, align 2
  %122 = add nuw nsw i64 %104, 2
  %123 = icmp eq i64 %122, %15
  br i1 %123, label %124, label %103, !llvm.loop !19

124:                                              ; preds = %34, %103, %84, %10
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define internal void @SharpYUVFilterRow_SSE2(i16* nocapture readonly, i16* nocapture readonly, i32, i16* nocapture readonly, i16* nocapture) #3 {
  %6 = icmp slt i32 %2, 8
  br i1 %6, label %11, label %7

7:                                                ; preds = %5
  %8 = sext i32 %2 to i64
  br label %110

9:                                                ; preds = %110
  %10 = and i32 %2, -8
  br label %11

11:                                               ; preds = %9, %5
  %12 = phi i32 [ 0, %5 ], [ %10, %9 ]
  %13 = icmp slt i32 %12, %2
  br i1 %13, label %14, label %217

14:                                               ; preds = %11
  %15 = zext i32 %12 to i64
  %16 = zext i32 %2 to i64
  %17 = sub nsw i64 %16, %15
  %18 = icmp ult i64 %17, 8
  br i1 %18, label %19, label %21

19:                                               ; preds = %108, %21, %14
  %20 = phi i64 [ %15, %21 ], [ %15, %14 ], [ %46, %108 ]
  br label %165

21:                                               ; preds = %14
  %22 = shl nuw nsw i64 %15, 1
  %23 = getelementptr i16, i16* %4, i64 %22
  %24 = shl nuw nsw i64 %16, 1
  %25 = getelementptr i16, i16* %4, i64 %24
  %26 = getelementptr i16, i16* %0, i64 %15
  %27 = add nuw nsw i64 %16, 1
  %28 = getelementptr i16, i16* %0, i64 %27
  %29 = getelementptr i16, i16* %1, i64 %15
  %30 = getelementptr i16, i16* %1, i64 %27
  %31 = getelementptr i16, i16* %3, i64 %22
  %32 = getelementptr i16, i16* %3, i64 %24
  %33 = icmp ult i16* %23, %28
  %34 = icmp ult i16* %26, %25
  %35 = and i1 %33, %34
  %36 = icmp ult i16* %23, %30
  %37 = icmp ult i16* %29, %25
  %38 = and i1 %36, %37
  %39 = or i1 %35, %38
  %40 = icmp ult i16* %23, %32
  %41 = icmp ult i16* %31, %25
  %42 = and i1 %40, %41
  %43 = or i1 %39, %42
  br i1 %43, label %19, label %44

44:                                               ; preds = %21
  %45 = and i64 %17, -8
  %46 = add nsw i64 %45, %15
  %47 = getelementptr inbounds i16, i16* %4, i64 -1
  br label %48

48:                                               ; preds = %48, %44
  %49 = phi i64 [ 0, %44 ], [ %106, %48 ]
  %50 = add i64 %49, %15
  %51 = getelementptr inbounds i16, i16* %0, i64 %50
  %52 = bitcast i16* %51 to <8 x i16>*
  %53 = load <8 x i16>, <8 x i16>* %52, align 2, !alias.scope !20
  %54 = sext <8 x i16> %53 to <8 x i32>
  %55 = or i64 %50, 1
  %56 = getelementptr inbounds i16, i16* %1, i64 %55
  %57 = bitcast i16* %56 to <8 x i16>*
  %58 = load <8 x i16>, <8 x i16>* %57, align 2, !alias.scope !23
  %59 = sext <8 x i16> %58 to <8 x i32>
  %60 = add nsw <8 x i32> %59, %54
  %61 = getelementptr inbounds i16, i16* %0, i64 %55
  %62 = bitcast i16* %61 to <8 x i16>*
  %63 = load <8 x i16>, <8 x i16>* %62, align 2, !alias.scope !20
  %64 = sext <8 x i16> %63 to <8 x i32>
  %65 = getelementptr inbounds i16, i16* %1, i64 %50
  %66 = bitcast i16* %65 to <8 x i16>*
  %67 = load <8 x i16>, <8 x i16>* %66, align 2, !alias.scope !23
  %68 = sext <8 x i16> %67 to <8 x i32>
  %69 = add nsw <8 x i32> %68, %64
  %70 = add nsw <8 x i32> %60, <i32 8, i32 8, i32 8, i32 8, i32 8, i32 8, i32 8, i32 8>
  %71 = add nsw <8 x i32> %70, %69
  %72 = shl nsw <8 x i32> %54, <i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3>
  %73 = shl nsw <8 x i32> %69, <i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1>
  %74 = add nsw <8 x i32> %73, %72
  %75 = add nsw <8 x i32> %74, %71
  %76 = ashr <8 x i32> %75, <i32 4, i32 4, i32 4, i32 4, i32 4, i32 4, i32 4, i32 4>
  %77 = shl nsw <8 x i32> %64, <i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3>
  %78 = shl nsw <8 x i32> %60, <i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1>
  %79 = add nsw <8 x i32> %77, %78
  %80 = add nsw <8 x i32> %79, %71
  %81 = ashr <8 x i32> %80, <i32 4, i32 4, i32 4, i32 4, i32 4, i32 4, i32 4, i32 4>
  %82 = shl nuw nsw i64 %50, 1
  %83 = getelementptr inbounds i16, i16* %3, i64 %82
  %84 = bitcast i16* %83 to <16 x i16>*
  %85 = load <16 x i16>, <16 x i16>* %84, align 2
  %86 = shufflevector <16 x i16> %85, <16 x i16> undef, <8 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14>
  %87 = shufflevector <16 x i16> %85, <16 x i16> undef, <8 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15>
  %88 = zext <8 x i16> %86 to <8 x i32>
  %89 = add nsw <8 x i32> %76, %88
  %90 = icmp slt <8 x i32> %89, <i32 1023, i32 1023, i32 1023, i32 1023, i32 1023, i32 1023, i32 1023, i32 1023>
  %91 = select <8 x i1> %90, <8 x i32> %89, <8 x i32> <i32 1023, i32 1023, i32 1023, i32 1023, i32 1023, i32 1023, i32 1023, i32 1023>
  %92 = icmp sgt <8 x i32> %91, zeroinitializer
  %93 = select <8 x i1> %92, <8 x i32> %91, <8 x i32> zeroinitializer
  %94 = trunc <8 x i32> %93 to <8 x i16>
  %95 = or i64 %82, 1
  %96 = zext <8 x i16> %87 to <8 x i32>
  %97 = add nsw <8 x i32> %81, %96
  %98 = icmp slt <8 x i32> %97, <i32 1023, i32 1023, i32 1023, i32 1023, i32 1023, i32 1023, i32 1023, i32 1023>
  %99 = select <8 x i1> %98, <8 x i32> %97, <8 x i32> <i32 1023, i32 1023, i32 1023, i32 1023, i32 1023, i32 1023, i32 1023, i32 1023>
  %100 = icmp sgt <8 x i32> %99, zeroinitializer
  %101 = select <8 x i1> %100, <8 x i32> %99, <8 x i32> zeroinitializer
  %102 = trunc <8 x i32> %101 to <8 x i16>
  %103 = getelementptr inbounds i16, i16* %47, i64 %95
  %104 = bitcast i16* %103 to <16 x i16>*
  %105 = shufflevector <8 x i16> %94, <8 x i16> %102, <16 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11, i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  store <16 x i16> %105, <16 x i16>* %104, align 2
  %106 = add i64 %49, 8
  %107 = icmp eq i64 %106, %45
  br i1 %107, label %108, label %48, !llvm.loop !25

108:                                              ; preds = %48
  %109 = icmp eq i64 %17, %45
  br i1 %109, label %217, label %19

110:                                              ; preds = %7, %110
  %111 = phi i64 [ 0, %7 ], [ %164, %110 ]
  %112 = phi i64 [ 8, %7 ], [ %162, %110 ]
  %113 = getelementptr inbounds i16, i16* %0, i64 %111
  %114 = bitcast i16* %113 to <8 x i16>*
  %115 = load <8 x i16>, <8 x i16>* %114, align 1
  %116 = getelementptr inbounds i16, i16* %113, i64 1
  %117 = bitcast i16* %116 to <8 x i16>*
  %118 = load <8 x i16>, <8 x i16>* %117, align 1
  %119 = getelementptr inbounds i16, i16* %1, i64 %111
  %120 = bitcast i16* %119 to <8 x i16>*
  %121 = load <8 x i16>, <8 x i16>* %120, align 1
  %122 = getelementptr inbounds i16, i16* %119, i64 1
  %123 = bitcast i16* %122 to <8 x i16>*
  %124 = load <8 x i16>, <8 x i16>* %123, align 1
  %125 = add <8 x i16> %124, %115
  %126 = add <8 x i16> %121, %118
  %127 = add <8 x i16> %126, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %128 = add <8 x i16> %127, %125
  %129 = shl <8 x i16> %125, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %130 = shl <8 x i16> %126, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %131 = add <8 x i16> %128, %129
  %132 = ashr <8 x i16> %131, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %133 = add <8 x i16> %128, %130
  %134 = ashr <8 x i16> %133, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %135 = add <8 x i16> %134, %115
  %136 = add <8 x i16> %132, %118
  %137 = ashr <8 x i16> %135, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %138 = ashr <8 x i16> %136, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %139 = shufflevector <8 x i16> %137, <8 x i16> %138, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %140 = shufflevector <8 x i16> %137, <8 x i16> %138, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %141 = shl nuw nsw i64 %111, 1
  %142 = getelementptr inbounds i16, i16* %3, i64 %141
  %143 = bitcast i16* %142 to <8 x i16>*
  %144 = load <8 x i16>, <8 x i16>* %143, align 1
  %145 = getelementptr inbounds i16, i16* %142, i64 8
  %146 = bitcast i16* %145 to <8 x i16>*
  %147 = load <8 x i16>, <8 x i16>* %146, align 1
  %148 = add <8 x i16> %139, %144
  %149 = add <8 x i16> %140, %147
  %150 = icmp slt <8 x i16> %148, <i16 1023, i16 1023, i16 1023, i16 1023, i16 1023, i16 1023, i16 1023, i16 1023>
  %151 = select <8 x i1> %150, <8 x i16> %148, <8 x i16> <i16 1023, i16 1023, i16 1023, i16 1023, i16 1023, i16 1023, i16 1023, i16 1023>
  %152 = icmp sgt <8 x i16> %151, zeroinitializer
  %153 = select <8 x i1> %152, <8 x i16> %151, <8 x i16> zeroinitializer
  %154 = icmp slt <8 x i16> %149, <i16 1023, i16 1023, i16 1023, i16 1023, i16 1023, i16 1023, i16 1023, i16 1023>
  %155 = select <8 x i1> %154, <8 x i16> %149, <8 x i16> <i16 1023, i16 1023, i16 1023, i16 1023, i16 1023, i16 1023, i16 1023, i16 1023>
  %156 = icmp sgt <8 x i16> %155, zeroinitializer
  %157 = select <8 x i1> %156, <8 x i16> %155, <8 x i16> zeroinitializer
  %158 = getelementptr inbounds i16, i16* %4, i64 %141
  %159 = bitcast i16* %158 to <8 x i16>*
  store <8 x i16> %153, <8 x i16>* %159, align 1
  %160 = getelementptr inbounds i16, i16* %158, i64 8
  %161 = bitcast i16* %160 to <8 x i16>*
  store <8 x i16> %157, <8 x i16>* %161, align 1
  %162 = add nuw nsw i64 %112, 8
  %163 = icmp sgt i64 %162, %8
  %164 = add nuw nsw i64 %111, 8
  br i1 %163, label %9, label %110

165:                                              ; preds = %19, %165
  %166 = phi i64 [ %170, %165 ], [ %20, %19 ]
  %167 = getelementptr inbounds i16, i16* %0, i64 %166
  %168 = load i16, i16* %167, align 2
  %169 = sext i16 %168 to i32
  %170 = add nuw nsw i64 %166, 1
  %171 = getelementptr inbounds i16, i16* %1, i64 %170
  %172 = load i16, i16* %171, align 2
  %173 = sext i16 %172 to i32
  %174 = add nsw i32 %173, %169
  %175 = getelementptr inbounds i16, i16* %0, i64 %170
  %176 = load i16, i16* %175, align 2
  %177 = sext i16 %176 to i32
  %178 = getelementptr inbounds i16, i16* %1, i64 %166
  %179 = load i16, i16* %178, align 2
  %180 = sext i16 %179 to i32
  %181 = add nsw i32 %180, %177
  %182 = add nsw i32 %174, 8
  %183 = add nsw i32 %182, %181
  %184 = shl nsw i32 %169, 3
  %185 = shl nsw i32 %181, 1
  %186 = add nsw i32 %185, %184
  %187 = add nsw i32 %186, %183
  %188 = ashr i32 %187, 4
  %189 = shl nsw i32 %177, 3
  %190 = shl nsw i32 %174, 1
  %191 = add nsw i32 %189, %190
  %192 = add nsw i32 %191, %183
  %193 = ashr i32 %192, 4
  %194 = shl nuw nsw i64 %166, 1
  %195 = getelementptr inbounds i16, i16* %3, i64 %194
  %196 = load i16, i16* %195, align 2
  %197 = zext i16 %196 to i32
  %198 = add nsw i32 %188, %197
  %199 = icmp slt i32 %198, 1023
  %200 = select i1 %199, i32 %198, i32 1023
  %201 = icmp sgt i32 %200, 0
  %202 = select i1 %201, i32 %200, i32 0
  %203 = trunc i32 %202 to i16
  %204 = getelementptr inbounds i16, i16* %4, i64 %194
  store i16 %203, i16* %204, align 2
  %205 = or i64 %194, 1
  %206 = getelementptr inbounds i16, i16* %3, i64 %205
  %207 = load i16, i16* %206, align 2
  %208 = zext i16 %207 to i32
  %209 = add nsw i32 %193, %208
  %210 = icmp slt i32 %209, 1023
  %211 = select i1 %210, i32 %209, i32 1023
  %212 = icmp sgt i32 %211, 0
  %213 = select i1 %212, i32 %211, i32 0
  %214 = trunc i32 %213 to i16
  %215 = getelementptr inbounds i16, i16* %4, i64 %205
  store i16 %214, i16* %215, align 2
  %216 = icmp eq i64 %170, %16
  br i1 %216, label %217, label %165, !llvm.loop !26

217:                                              ; preds = %165, %108, %11
  ret void
}

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16>, <8 x i16>) #4

; Function Attrs: nounwind readnone speculatable
declare <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16>, <8 x i16>) #5

; Function Attrs: nounwind readnone speculatable
declare <8 x i16> @llvm.usub.sat.v8i16(<8 x i16>, <8 x i16>) #5

; Function Attrs: nounwind readnone
declare <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16>, <8 x i16>) #4

; Function Attrs: nounwind readnone
declare <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16>, <8 x i16>) #4

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32>, <4 x i32>) #4

declare void @WebPConvertARGBToUV_C(i32*, i8*, i8*, i32, i32) local_unnamed_addr #6

; Function Attrs: nounwind readnone
declare <16 x i8> @llvm.x86.sse2.pavg.b(<16 x i8>, <16 x i8>) #4

declare void @WebPConvertRGBA32ToUV_C(i16*, i8*, i8*, i32) local_unnamed_addr #6

attributes #0 = { nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="128" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { nofree norecurse nounwind ssp uwtable writeonly "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #2 = { nofree norecurse nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="128" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #3 = { nofree nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="128" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #4 = { nounwind readnone }
attributes #5 = { nounwind readnone speculatable }
attributes #6 = { "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #7 = { nounwind }

!llvm.module.flags = !{!0, !1}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 2}
!2 = !{!3}
!3 = distinct !{!3, !4}
!4 = distinct !{!4, !"LVerDomain"}
!5 = !{!6}
!6 = distinct !{!6, !4}
!7 = distinct !{!7, !8}
!8 = !{!"llvm.loop.isvectorized", i32 1}
!9 = distinct !{!9, !8}
!10 = !{!11}
!11 = distinct !{!11, !12}
!12 = distinct !{!12, !"LVerDomain"}
!13 = !{!14}
!14 = distinct !{!14, !12}
!15 = !{!16}
!16 = distinct !{!16, !12}
!17 = !{!11, !14}
!18 = distinct !{!18, !8}
!19 = distinct !{!19, !8}
!20 = !{!21}
!21 = distinct !{!21, !22}
!22 = distinct !{!22, !"LVerDomain"}
!23 = !{!24}
!24 = distinct !{!24, !22}
!25 = distinct !{!25, !8}
!26 = distinct !{!26, !8}
