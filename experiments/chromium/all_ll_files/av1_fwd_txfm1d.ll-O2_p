; ModuleID = '../../third_party/libaom/source/libaom/av1/encoder/av1_fwd_txfm1d.c'
source_filename = "../../third_party/libaom/source/libaom/av1/encoder/av1_fwd_txfm1d.c"
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

module asm ".symver exp, exp@GLIBC_2.2.5"
module asm ".symver exp2, exp2@GLIBC_2.2.5"
module asm ".symver exp2f, exp2f@GLIBC_2.2.5"
module asm ".symver expf, expf@GLIBC_2.2.5"
module asm ".symver lgamma, lgamma@GLIBC_2.2.5"
module asm ".symver lgammaf, lgammaf@GLIBC_2.2.5"
module asm ".symver lgammal, lgammal@GLIBC_2.2.5"
module asm ".symver log, log@GLIBC_2.2.5"
module asm ".symver log2, log2@GLIBC_2.2.5"
module asm ".symver log2f, log2f@GLIBC_2.2.5"
module asm ".symver logf, logf@GLIBC_2.2.5"
module asm ".symver pow, pow@GLIBC_2.2.5"
module asm ".symver powf, powf@GLIBC_2.2.5"

@av1_cospi_arr_data = external local_unnamed_addr constant [7 x [64 x i32]], align 16
@av1_sinpi_arr_data = external local_unnamed_addr constant [7 x [5 x i32]], align 16

; Function Attrs: nounwind ssp uwtable
define hidden void @av1_fdct4(i32*, i32*, i8 signext, i8* nocapture readonly) local_unnamed_addr #0 {
  %5 = alloca <4 x i32>, align 16
  %6 = bitcast <4 x i32>* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 16, i8* nonnull %6) #3
  %7 = getelementptr inbounds <4 x i32>, <4 x i32>* %5, i64 0, i64 0
  %8 = getelementptr inbounds <4 x i32>, <4 x i32>* %5, i64 0, i64 1
  %9 = getelementptr inbounds <4 x i32>, <4 x i32>* %5, i64 0, i64 2
  %10 = getelementptr inbounds <4 x i32>, <4 x i32>* %5, i64 0, i64 3
  %11 = load i8, i8* %3, align 1
  tail call void @av1_range_check_buf(i32 0, i32* %0, i32* %0, i32 4, i8 signext %11) #3
  %12 = load i32, i32* %0, align 4
  %13 = getelementptr inbounds i32, i32* %0, i64 3
  %14 = load i32, i32* %13, align 4
  %15 = add nsw i32 %14, %12
  store i32 %15, i32* %1, align 4
  %16 = getelementptr inbounds i32, i32* %0, i64 1
  %17 = load i32, i32* %16, align 4
  %18 = getelementptr inbounds i32, i32* %0, i64 2
  %19 = load i32, i32* %18, align 4
  %20 = add nsw i32 %19, %17
  %21 = getelementptr inbounds i32, i32* %1, i64 1
  store i32 %20, i32* %21, align 4
  %22 = load i32, i32* %18, align 4
  %23 = load i32, i32* %16, align 4
  %24 = sub i32 %23, %22
  %25 = getelementptr inbounds i32, i32* %1, i64 2
  store i32 %24, i32* %25, align 4
  %26 = load i32, i32* %13, align 4
  %27 = load i32, i32* %0, align 4
  %28 = sub i32 %27, %26
  %29 = getelementptr inbounds i32, i32* %1, i64 3
  store i32 %28, i32* %29, align 4
  %30 = getelementptr inbounds i8, i8* %3, i64 1
  %31 = load i8, i8* %30, align 1
  tail call void @av1_range_check_buf(i32 1, i32* %0, i32* %1, i32 4, i8 signext %31) #3
  %32 = sext i8 %2 to i32
  %33 = add nsw i32 %32, -10
  %34 = sext i32 %33 to i64
  %35 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %34, i64 32
  %36 = load i32, i32* %35, align 16
  %37 = load i32, i32* %1, align 4
  %38 = load i32, i32* %21, align 4
  %39 = mul nsw i32 %37, %36
  %40 = sext i32 %39 to i64
  %41 = mul i32 %38, %36
  %42 = sext i32 %41 to i64
  %43 = add nsw i32 %32, -1
  %44 = zext i32 %43 to i64
  %45 = shl i64 1, %44
  %46 = add i64 %45, %40
  %47 = add i64 %46, %42
  %48 = zext i32 %32 to i64
  %49 = ashr i64 %47, %48
  %50 = trunc i64 %49 to i32
  store i32 %50, i32* %7, align 16
  %51 = sub i32 0, %41
  %52 = sext i32 %51 to i64
  %53 = add i64 %46, %52
  %54 = ashr i64 %53, %48
  %55 = trunc i64 %54 to i32
  store i32 %55, i32* %8, align 4
  %56 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %34, i64 48
  %57 = load i32, i32* %56, align 16
  %58 = load i32, i32* %25, align 4
  %59 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %34, i64 16
  %60 = load i32, i32* %59, align 16
  %61 = load i32, i32* %29, align 4
  %62 = mul nsw i32 %58, %57
  %63 = sext i32 %62 to i64
  %64 = mul nsw i32 %61, %60
  %65 = sext i32 %64 to i64
  %66 = add i64 %45, %63
  %67 = add i64 %66, %65
  %68 = ashr i64 %67, %48
  %69 = trunc i64 %68 to i32
  store i32 %69, i32* %9, align 8
  %70 = mul nsw i32 %61, %57
  %71 = sext i32 %70 to i64
  %72 = mul i32 %58, %60
  %73 = sub i32 0, %72
  %74 = sext i32 %73 to i64
  %75 = add i64 %45, %74
  %76 = add i64 %75, %71
  %77 = ashr i64 %76, %48
  %78 = trunc i64 %77 to i32
  store i32 %78, i32* %10, align 4
  %79 = getelementptr inbounds i8, i8* %3, i64 2
  %80 = load i8, i8* %79, align 1
  call void @av1_range_check_buf(i32 2, i32* %0, i32* nonnull %7, i32 4, i8 signext %80) #3
  %81 = load <4 x i32>, <4 x i32>* %5, align 16
  %82 = shufflevector <4 x i32> %81, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %83 = bitcast i32* %1 to <4 x i32>*
  store <4 x i32> %82, <4 x i32>* %83, align 4
  %84 = getelementptr inbounds i8, i8* %3, i64 3
  %85 = load i8, i8* %84, align 1
  call void @av1_range_check_buf(i32 3, i32* %0, i32* %1, i32 4, i8 signext %85) #3
  call void @llvm.lifetime.end.p0i8(i64 16, i8* nonnull %6) #3
  ret void
}

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #1

declare void @av1_range_check_buf(i32, i32*, i32*, i32, i8 signext) local_unnamed_addr #2

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #1

; Function Attrs: nounwind ssp uwtable
define hidden void @av1_fdct8(i32*, i32*, i8 signext, i8* nocapture readonly) local_unnamed_addr #0 {
  %5 = alloca [8 x i32], align 16
  %6 = bitcast [8 x i32]* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %6) #3
  %7 = getelementptr inbounds [8 x i32], [8 x i32]* %5, i64 0, i64 0
  %8 = getelementptr inbounds [8 x i32], [8 x i32]* %5, i64 0, i64 1
  %9 = getelementptr inbounds [8 x i32], [8 x i32]* %5, i64 0, i64 2
  %10 = getelementptr inbounds [8 x i32], [8 x i32]* %5, i64 0, i64 3
  %11 = getelementptr inbounds [8 x i32], [8 x i32]* %5, i64 0, i64 4
  %12 = getelementptr inbounds [8 x i32], [8 x i32]* %5, i64 0, i64 5
  %13 = getelementptr inbounds [8 x i32], [8 x i32]* %5, i64 0, i64 6
  %14 = getelementptr inbounds [8 x i32], [8 x i32]* %5, i64 0, i64 7
  %15 = load i8, i8* %3, align 1
  tail call void @av1_range_check_buf(i32 0, i32* %0, i32* %0, i32 8, i8 signext %15) #3
  %16 = load i32, i32* %0, align 4
  %17 = getelementptr inbounds i32, i32* %0, i64 7
  %18 = load i32, i32* %17, align 4
  %19 = add nsw i32 %18, %16
  store i32 %19, i32* %1, align 4
  %20 = getelementptr inbounds i32, i32* %0, i64 1
  %21 = load i32, i32* %20, align 4
  %22 = getelementptr inbounds i32, i32* %0, i64 6
  %23 = load i32, i32* %22, align 4
  %24 = add nsw i32 %23, %21
  %25 = getelementptr inbounds i32, i32* %1, i64 1
  store i32 %24, i32* %25, align 4
  %26 = getelementptr inbounds i32, i32* %0, i64 2
  %27 = load i32, i32* %26, align 4
  %28 = getelementptr inbounds i32, i32* %0, i64 5
  %29 = load i32, i32* %28, align 4
  %30 = add nsw i32 %29, %27
  %31 = getelementptr inbounds i32, i32* %1, i64 2
  store i32 %30, i32* %31, align 4
  %32 = getelementptr inbounds i32, i32* %0, i64 3
  %33 = load i32, i32* %32, align 4
  %34 = getelementptr inbounds i32, i32* %0, i64 4
  %35 = load i32, i32* %34, align 4
  %36 = add nsw i32 %35, %33
  %37 = getelementptr inbounds i32, i32* %1, i64 3
  store i32 %36, i32* %37, align 4
  %38 = load i32, i32* %34, align 4
  %39 = load i32, i32* %32, align 4
  %40 = sub i32 %39, %38
  %41 = getelementptr inbounds i32, i32* %1, i64 4
  store i32 %40, i32* %41, align 4
  %42 = load i32, i32* %28, align 4
  %43 = load i32, i32* %26, align 4
  %44 = sub i32 %43, %42
  %45 = getelementptr inbounds i32, i32* %1, i64 5
  store i32 %44, i32* %45, align 4
  %46 = load i32, i32* %22, align 4
  %47 = load i32, i32* %20, align 4
  %48 = sub i32 %47, %46
  %49 = getelementptr inbounds i32, i32* %1, i64 6
  store i32 %48, i32* %49, align 4
  %50 = load i32, i32* %17, align 4
  %51 = load i32, i32* %0, align 4
  %52 = sub i32 %51, %50
  %53 = getelementptr inbounds i32, i32* %1, i64 7
  store i32 %52, i32* %53, align 4
  %54 = getelementptr inbounds i8, i8* %3, i64 1
  %55 = load i8, i8* %54, align 1
  tail call void @av1_range_check_buf(i32 1, i32* %0, i32* %1, i32 8, i8 signext %55) #3
  %56 = sext i8 %2 to i32
  %57 = add nsw i32 %56, -10
  %58 = sext i32 %57 to i64
  %59 = bitcast i32* %1 to <4 x i32>*
  %60 = load <4 x i32>, <4 x i32>* %59, align 4
  %61 = shufflevector <4 x i32> %60, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %62 = add nsw <4 x i32> %61, %60
  %63 = sub <4 x i32> %61, %60
  %64 = shufflevector <4 x i32> %62, <4 x i32> %63, <4 x i32> <i32 0, i32 1, i32 6, i32 7>
  %65 = bitcast [8 x i32]* %5 to <4 x i32>*
  store <4 x i32> %64, <4 x i32>* %65, align 16
  %66 = load i32, i32* %41, align 4
  store i32 %66, i32* %11, align 16
  %67 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %58, i64 32
  %68 = load i32, i32* %67, align 16
  %69 = sub nsw i32 0, %68
  %70 = load i32, i32* %45, align 4
  %71 = load i32, i32* %49, align 4
  %72 = mul nsw i32 %70, %69
  %73 = sext i32 %72 to i64
  %74 = mul nsw i32 %71, %68
  %75 = sext i32 %74 to i64
  %76 = add nsw i32 %56, -1
  %77 = zext i32 %76 to i64
  %78 = shl i64 1, %77
  %79 = add i64 %78, %75
  %80 = add i64 %79, %73
  %81 = zext i32 %56 to i64
  %82 = ashr i64 %80, %81
  %83 = trunc i64 %82 to i32
  store i32 %83, i32* %12, align 4
  %84 = mul nsw i32 %70, %68
  %85 = sext i32 %84 to i64
  %86 = add i64 %79, %85
  %87 = ashr i64 %86, %81
  %88 = trunc i64 %87 to i32
  store i32 %88, i32* %13, align 8
  %89 = load i32, i32* %53, align 4
  store i32 %89, i32* %14, align 4
  %90 = getelementptr inbounds i8, i8* %3, i64 2
  %91 = load i8, i8* %90, align 1
  call void @av1_range_check_buf(i32 2, i32* %0, i32* nonnull %7, i32 8, i8 signext %91) #3
  %92 = load i32, i32* %7, align 16
  %93 = load i32, i32* %8, align 4
  %94 = mul nsw i32 %92, %68
  %95 = sext i32 %94 to i64
  %96 = mul nsw i32 %93, %68
  %97 = sext i32 %96 to i64
  %98 = add i64 %78, %95
  %99 = add i64 %98, %97
  %100 = ashr i64 %99, %81
  %101 = trunc i64 %100 to i32
  store i32 %101, i32* %1, align 4
  %102 = mul nsw i32 %93, %69
  %103 = sext i32 %102 to i64
  %104 = add i64 %98, %103
  %105 = ashr i64 %104, %81
  %106 = trunc i64 %105 to i32
  store i32 %106, i32* %25, align 4
  %107 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %58, i64 48
  %108 = load i32, i32* %107, align 16
  %109 = load i32, i32* %9, align 8
  %110 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %58, i64 16
  %111 = load i32, i32* %110, align 16
  %112 = load i32, i32* %10, align 4
  %113 = mul nsw i32 %109, %108
  %114 = sext i32 %113 to i64
  %115 = mul nsw i32 %112, %111
  %116 = sext i32 %115 to i64
  %117 = add i64 %78, %114
  %118 = add i64 %117, %116
  %119 = ashr i64 %118, %81
  %120 = trunc i64 %119 to i32
  store i32 %120, i32* %31, align 4
  %121 = mul nsw i32 %112, %108
  %122 = sext i32 %121 to i64
  %123 = mul i32 %109, %111
  %124 = sub i32 0, %123
  %125 = sext i32 %124 to i64
  %126 = add i64 %78, %125
  %127 = add i64 %126, %122
  %128 = ashr i64 %127, %81
  %129 = trunc i64 %128 to i32
  store i32 %129, i32* %37, align 4
  %130 = bitcast i32* %11 to <4 x i32>*
  %131 = load <4 x i32>, <4 x i32>* %130, align 16
  %132 = shufflevector <4 x i32> %131, <4 x i32> undef, <4 x i32> <i32 1, i32 0, i32 3, i32 2>
  %133 = add nsw <4 x i32> %132, %131
  %134 = sub <4 x i32> %132, %131
  %135 = shufflevector <4 x i32> %133, <4 x i32> %134, <4 x i32> <i32 0, i32 5, i32 6, i32 3>
  %136 = bitcast i32* %41 to <4 x i32>*
  store <4 x i32> %135, <4 x i32>* %136, align 4
  %137 = getelementptr inbounds i8, i8* %3, i64 3
  %138 = load i8, i8* %137, align 1
  call void @av1_range_check_buf(i32 3, i32* %0, i32* %1, i32 8, i8 signext %138) #3
  %139 = bitcast i32* %1 to <4 x i32>*
  %140 = load <4 x i32>, <4 x i32>* %139, align 4
  %141 = bitcast [8 x i32]* %5 to <4 x i32>*
  store <4 x i32> %140, <4 x i32>* %141, align 16
  %142 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %58, i64 56
  %143 = load i32, i32* %142, align 16
  %144 = load i32, i32* %41, align 4
  %145 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %58, i64 8
  %146 = load i32, i32* %145, align 16
  %147 = load i32, i32* %53, align 4
  %148 = mul nsw i32 %144, %143
  %149 = sext i32 %148 to i64
  %150 = mul nsw i32 %147, %146
  %151 = sext i32 %150 to i64
  %152 = add i64 %78, %149
  %153 = add i64 %152, %151
  %154 = ashr i64 %153, %81
  %155 = trunc i64 %154 to i32
  store i32 %155, i32* %11, align 16
  %156 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %58, i64 24
  %157 = load i32, i32* %156, align 16
  %158 = load i32, i32* %45, align 4
  %159 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %58, i64 40
  %160 = load i32, i32* %159, align 16
  %161 = load i32, i32* %49, align 4
  %162 = mul nsw i32 %158, %157
  %163 = sext i32 %162 to i64
  %164 = mul nsw i32 %161, %160
  %165 = sext i32 %164 to i64
  %166 = add i64 %78, %163
  %167 = add i64 %166, %165
  %168 = ashr i64 %167, %81
  %169 = trunc i64 %168 to i32
  store i32 %169, i32* %12, align 4
  %170 = mul nsw i32 %161, %157
  %171 = sext i32 %170 to i64
  %172 = mul i32 %158, %160
  %173 = sub i32 0, %172
  %174 = sext i32 %173 to i64
  %175 = add i64 %78, %174
  %176 = add i64 %175, %171
  %177 = ashr i64 %176, %81
  %178 = trunc i64 %177 to i32
  store i32 %178, i32* %13, align 8
  %179 = mul nsw i32 %147, %143
  %180 = sext i32 %179 to i64
  %181 = mul i32 %144, %146
  %182 = sub i32 0, %181
  %183 = sext i32 %182 to i64
  %184 = add i64 %78, %183
  %185 = add i64 %184, %180
  %186 = ashr i64 %185, %81
  %187 = trunc i64 %186 to i32
  store i32 %187, i32* %14, align 4
  %188 = getelementptr inbounds i8, i8* %3, i64 4
  %189 = load i8, i8* %188, align 1
  call void @av1_range_check_buf(i32 4, i32* %0, i32* nonnull %7, i32 8, i8 signext %189) #3
  %190 = load i32, i32* %7, align 16
  store i32 %190, i32* %1, align 4
  %191 = load i32, i32* %11, align 16
  store i32 %191, i32* %25, align 4
  %192 = load i32, i32* %9, align 8
  store i32 %192, i32* %31, align 4
  %193 = load i32, i32* %13, align 8
  store i32 %193, i32* %37, align 4
  %194 = load i32, i32* %8, align 4
  store i32 %194, i32* %41, align 4
  %195 = load i32, i32* %12, align 4
  store i32 %195, i32* %45, align 4
  %196 = load i32, i32* %10, align 4
  store i32 %196, i32* %49, align 4
  %197 = load i32, i32* %14, align 4
  store i32 %197, i32* %53, align 4
  %198 = getelementptr inbounds i8, i8* %3, i64 5
  %199 = load i8, i8* %198, align 1
  call void @av1_range_check_buf(i32 5, i32* %0, i32* %1, i32 8, i8 signext %199) #3
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %6) #3
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @av1_fdct16(i32*, i32*, i8 signext, i8* nocapture readonly) local_unnamed_addr #0 {
  %5 = alloca [16 x i32], align 16
  %6 = bitcast [16 x i32]* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %6) #3
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %6, i8 -86, i64 64, i1 false)
  %7 = load i8, i8* %3, align 1
  tail call void @av1_range_check_buf(i32 0, i32* %0, i32* %0, i32 16, i8 signext %7) #3
  %8 = load i32, i32* %0, align 4
  %9 = getelementptr inbounds i32, i32* %0, i64 15
  %10 = load i32, i32* %9, align 4
  %11 = add nsw i32 %10, %8
  store i32 %11, i32* %1, align 4
  %12 = getelementptr inbounds i32, i32* %0, i64 1
  %13 = load i32, i32* %12, align 4
  %14 = getelementptr inbounds i32, i32* %0, i64 14
  %15 = load i32, i32* %14, align 4
  %16 = add nsw i32 %15, %13
  %17 = getelementptr inbounds i32, i32* %1, i64 1
  store i32 %16, i32* %17, align 4
  %18 = getelementptr inbounds i32, i32* %0, i64 2
  %19 = load i32, i32* %18, align 4
  %20 = getelementptr inbounds i32, i32* %0, i64 13
  %21 = load i32, i32* %20, align 4
  %22 = add nsw i32 %21, %19
  %23 = getelementptr inbounds i32, i32* %1, i64 2
  store i32 %22, i32* %23, align 4
  %24 = getelementptr inbounds i32, i32* %0, i64 3
  %25 = load i32, i32* %24, align 4
  %26 = getelementptr inbounds i32, i32* %0, i64 12
  %27 = load i32, i32* %26, align 4
  %28 = add nsw i32 %27, %25
  %29 = getelementptr inbounds i32, i32* %1, i64 3
  store i32 %28, i32* %29, align 4
  %30 = getelementptr inbounds i32, i32* %0, i64 4
  %31 = load i32, i32* %30, align 4
  %32 = getelementptr inbounds i32, i32* %0, i64 11
  %33 = load i32, i32* %32, align 4
  %34 = add nsw i32 %33, %31
  %35 = getelementptr inbounds i32, i32* %1, i64 4
  store i32 %34, i32* %35, align 4
  %36 = getelementptr inbounds i32, i32* %0, i64 5
  %37 = load i32, i32* %36, align 4
  %38 = getelementptr inbounds i32, i32* %0, i64 10
  %39 = load i32, i32* %38, align 4
  %40 = add nsw i32 %39, %37
  %41 = getelementptr inbounds i32, i32* %1, i64 5
  store i32 %40, i32* %41, align 4
  %42 = getelementptr inbounds i32, i32* %0, i64 6
  %43 = load i32, i32* %42, align 4
  %44 = getelementptr inbounds i32, i32* %0, i64 9
  %45 = load i32, i32* %44, align 4
  %46 = add nsw i32 %45, %43
  %47 = getelementptr inbounds i32, i32* %1, i64 6
  store i32 %46, i32* %47, align 4
  %48 = getelementptr inbounds i32, i32* %0, i64 7
  %49 = load i32, i32* %48, align 4
  %50 = getelementptr inbounds i32, i32* %0, i64 8
  %51 = load i32, i32* %50, align 4
  %52 = add nsw i32 %51, %49
  %53 = getelementptr inbounds i32, i32* %1, i64 7
  store i32 %52, i32* %53, align 4
  %54 = load i32, i32* %50, align 4
  %55 = load i32, i32* %48, align 4
  %56 = sub i32 %55, %54
  %57 = getelementptr inbounds i32, i32* %1, i64 8
  store i32 %56, i32* %57, align 4
  %58 = load i32, i32* %44, align 4
  %59 = load i32, i32* %42, align 4
  %60 = sub i32 %59, %58
  %61 = getelementptr inbounds i32, i32* %1, i64 9
  store i32 %60, i32* %61, align 4
  %62 = load i32, i32* %38, align 4
  %63 = load i32, i32* %36, align 4
  %64 = sub i32 %63, %62
  %65 = getelementptr inbounds i32, i32* %1, i64 10
  store i32 %64, i32* %65, align 4
  %66 = load i32, i32* %32, align 4
  %67 = load i32, i32* %30, align 4
  %68 = sub i32 %67, %66
  %69 = getelementptr inbounds i32, i32* %1, i64 11
  store i32 %68, i32* %69, align 4
  %70 = load i32, i32* %26, align 4
  %71 = load i32, i32* %24, align 4
  %72 = sub i32 %71, %70
  %73 = getelementptr inbounds i32, i32* %1, i64 12
  store i32 %72, i32* %73, align 4
  %74 = load i32, i32* %20, align 4
  %75 = load i32, i32* %18, align 4
  %76 = sub i32 %75, %74
  %77 = getelementptr inbounds i32, i32* %1, i64 13
  store i32 %76, i32* %77, align 4
  %78 = load i32, i32* %14, align 4
  %79 = load i32, i32* %12, align 4
  %80 = sub i32 %79, %78
  %81 = getelementptr inbounds i32, i32* %1, i64 14
  store i32 %80, i32* %81, align 4
  %82 = load i32, i32* %9, align 4
  %83 = load i32, i32* %0, align 4
  %84 = sub i32 %83, %82
  %85 = getelementptr inbounds i32, i32* %1, i64 15
  store i32 %84, i32* %85, align 4
  %86 = getelementptr inbounds i8, i8* %3, i64 1
  %87 = load i8, i8* %86, align 1
  tail call void @av1_range_check_buf(i32 1, i32* %0, i32* %1, i32 16, i8 signext %87) #3
  %88 = sext i8 %2 to i32
  %89 = add nsw i32 %88, -10
  %90 = sext i32 %89 to i64
  %91 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 0
  %92 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 1
  %93 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 2
  %94 = bitcast i32* %1 to <4 x i32>*
  %95 = load <4 x i32>, <4 x i32>* %94, align 4
  %96 = bitcast i32* %35 to <4 x i32>*
  %97 = load <4 x i32>, <4 x i32>* %96, align 4
  %98 = shufflevector <4 x i32> %97, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %99 = add nsw <4 x i32> %98, %95
  %100 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 3
  %101 = bitcast [16 x i32]* %5 to <4 x i32>*
  store <4 x i32> %99, <4 x i32>* %101, align 16
  %102 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 4
  %103 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 5
  %104 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 6
  %105 = shufflevector <4 x i32> %95, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %106 = sub <4 x i32> %105, %97
  %107 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 7
  %108 = bitcast i32* %102 to <4 x i32>*
  store <4 x i32> %106, <4 x i32>* %108, align 16
  %109 = load i32, i32* %57, align 4
  %110 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 8
  store i32 %109, i32* %110, align 16
  %111 = load i32, i32* %61, align 4
  %112 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 9
  store i32 %111, i32* %112, align 4
  %113 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %90, i64 32
  %114 = load i32, i32* %113, align 16
  %115 = sub nsw i32 0, %114
  %116 = load i32, i32* %65, align 4
  %117 = load i32, i32* %77, align 4
  %118 = mul nsw i32 %116, %115
  %119 = sext i32 %118 to i64
  %120 = mul nsw i32 %117, %114
  %121 = sext i32 %120 to i64
  %122 = add nsw i32 %88, -1
  %123 = zext i32 %122 to i64
  %124 = shl i64 1, %123
  %125 = add i64 %124, %121
  %126 = add i64 %125, %119
  %127 = zext i32 %88 to i64
  %128 = ashr i64 %126, %127
  %129 = trunc i64 %128 to i32
  %130 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 10
  store i32 %129, i32* %130, align 8
  %131 = load i32, i32* %69, align 4
  %132 = load i32, i32* %73, align 4
  %133 = mul nsw i32 %131, %115
  %134 = sext i32 %133 to i64
  %135 = mul nsw i32 %132, %114
  %136 = sext i32 %135 to i64
  %137 = add i64 %124, %136
  %138 = add i64 %137, %134
  %139 = ashr i64 %138, %127
  %140 = trunc i64 %139 to i32
  %141 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 11
  store i32 %140, i32* %141, align 4
  %142 = mul nsw i32 %131, %114
  %143 = sext i32 %142 to i64
  %144 = add i64 %137, %143
  %145 = ashr i64 %144, %127
  %146 = trunc i64 %145 to i32
  %147 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 12
  store i32 %146, i32* %147, align 16
  %148 = mul nsw i32 %116, %114
  %149 = sext i32 %148 to i64
  %150 = add i64 %125, %149
  %151 = ashr i64 %150, %127
  %152 = trunc i64 %151 to i32
  %153 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 13
  store i32 %152, i32* %153, align 4
  %154 = load i32, i32* %81, align 4
  %155 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 14
  store i32 %154, i32* %155, align 8
  %156 = load i32, i32* %85, align 4
  %157 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 15
  store i32 %156, i32* %157, align 4
  %158 = getelementptr inbounds i8, i8* %3, i64 2
  %159 = load i8, i8* %158, align 1
  call void @av1_range_check_buf(i32 2, i32* %0, i32* nonnull %91, i32 16, i8 signext %159) #3
  %160 = bitcast [16 x i32]* %5 to <4 x i32>*
  %161 = load <4 x i32>, <4 x i32>* %160, align 16
  %162 = shufflevector <4 x i32> %161, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %163 = add nsw <4 x i32> %162, %161
  %164 = sub <4 x i32> %162, %161
  %165 = shufflevector <4 x i32> %163, <4 x i32> %164, <4 x i32> <i32 0, i32 1, i32 6, i32 7>
  %166 = bitcast i32* %1 to <4 x i32>*
  store <4 x i32> %165, <4 x i32>* %166, align 4
  %167 = load i32, i32* %102, align 16
  store i32 %167, i32* %35, align 4
  %168 = load i32, i32* %103, align 4
  %169 = load i32, i32* %104, align 8
  %170 = mul nsw i32 %168, %115
  %171 = sext i32 %170 to i64
  %172 = mul nsw i32 %169, %114
  %173 = sext i32 %172 to i64
  %174 = add i64 %124, %173
  %175 = add i64 %174, %171
  %176 = ashr i64 %175, %127
  %177 = trunc i64 %176 to i32
  store i32 %177, i32* %41, align 4
  %178 = mul nsw i32 %168, %114
  %179 = sext i32 %178 to i64
  %180 = add i64 %174, %179
  %181 = ashr i64 %180, %127
  %182 = trunc i64 %181 to i32
  store i32 %182, i32* %47, align 4
  %183 = load i32, i32* %107, align 4
  store i32 %183, i32* %53, align 4
  %184 = bitcast i32* %110 to <4 x i32>*
  %185 = load <4 x i32>, <4 x i32>* %184, align 16
  %186 = shufflevector <4 x i32> %185, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %187 = add nsw <4 x i32> %186, %185
  %188 = sub <4 x i32> %186, %185
  %189 = shufflevector <4 x i32> %187, <4 x i32> %188, <4 x i32> <i32 0, i32 1, i32 6, i32 7>
  %190 = bitcast i32* %57 to <4 x i32>*
  store <4 x i32> %189, <4 x i32>* %190, align 4
  %191 = bitcast i32* %147 to <4 x i32>*
  %192 = load <4 x i32>, <4 x i32>* %191, align 16
  %193 = shufflevector <4 x i32> %192, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %194 = sub <4 x i32> %193, %192
  %195 = add nsw <4 x i32> %193, %192
  %196 = shufflevector <4 x i32> %194, <4 x i32> %195, <4 x i32> <i32 0, i32 1, i32 6, i32 7>
  %197 = bitcast i32* %73 to <4 x i32>*
  store <4 x i32> %196, <4 x i32>* %197, align 4
  %198 = getelementptr inbounds i8, i8* %3, i64 3
  %199 = load i8, i8* %198, align 1
  call void @av1_range_check_buf(i32 3, i32* %0, i32* %1, i32 16, i8 signext %199) #3
  %200 = load i32, i32* %1, align 4
  %201 = load i32, i32* %17, align 4
  %202 = mul nsw i32 %200, %114
  %203 = sext i32 %202 to i64
  %204 = mul nsw i32 %201, %114
  %205 = sext i32 %204 to i64
  %206 = add i64 %124, %203
  %207 = add i64 %206, %205
  %208 = ashr i64 %207, %127
  %209 = trunc i64 %208 to i32
  store i32 %209, i32* %91, align 16
  %210 = mul nsw i32 %201, %115
  %211 = sext i32 %210 to i64
  %212 = add i64 %206, %211
  %213 = ashr i64 %212, %127
  %214 = trunc i64 %213 to i32
  store i32 %214, i32* %92, align 4
  %215 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %90, i64 48
  %216 = load i32, i32* %215, align 16
  %217 = load i32, i32* %23, align 4
  %218 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %90, i64 16
  %219 = load i32, i32* %218, align 16
  %220 = load i32, i32* %29, align 4
  %221 = mul nsw i32 %217, %216
  %222 = sext i32 %221 to i64
  %223 = mul nsw i32 %220, %219
  %224 = sext i32 %223 to i64
  %225 = add i64 %124, %222
  %226 = add i64 %225, %224
  %227 = ashr i64 %226, %127
  %228 = trunc i64 %227 to i32
  store i32 %228, i32* %93, align 8
  %229 = sub nsw i32 0, %219
  %230 = mul nsw i32 %220, %216
  %231 = sext i32 %230 to i64
  %232 = mul nsw i32 %217, %229
  %233 = sext i32 %232 to i64
  %234 = add i64 %124, %233
  %235 = add i64 %234, %231
  %236 = ashr i64 %235, %127
  %237 = trunc i64 %236 to i32
  store i32 %237, i32* %100, align 4
  %238 = bitcast i32* %35 to <4 x i32>*
  %239 = load <4 x i32>, <4 x i32>* %238, align 4
  %240 = shufflevector <4 x i32> %239, <4 x i32> undef, <4 x i32> <i32 1, i32 0, i32 3, i32 2>
  %241 = add nsw <4 x i32> %240, %239
  %242 = sub <4 x i32> %240, %239
  %243 = shufflevector <4 x i32> %241, <4 x i32> %242, <4 x i32> <i32 0, i32 5, i32 6, i32 3>
  %244 = bitcast i32* %102 to <4 x i32>*
  store <4 x i32> %243, <4 x i32>* %244, align 16
  %245 = load i32, i32* %57, align 4
  store i32 %245, i32* %110, align 16
  %246 = load i32, i32* %61, align 4
  %247 = load i32, i32* %81, align 4
  %248 = mul nsw i32 %246, %229
  %249 = sext i32 %248 to i64
  %250 = mul nsw i32 %247, %216
  %251 = sext i32 %250 to i64
  %252 = add i64 %124, %249
  %253 = add i64 %252, %251
  %254 = ashr i64 %253, %127
  %255 = trunc i64 %254 to i32
  store i32 %255, i32* %112, align 4
  %256 = load i32, i32* %65, align 4
  %257 = load i32, i32* %77, align 4
  %258 = mul i32 %216, %256
  %259 = sub i32 0, %258
  %260 = sext i32 %259 to i64
  %261 = mul nsw i32 %257, %229
  %262 = sext i32 %261 to i64
  %263 = add i64 %124, %260
  %264 = add i64 %263, %262
  %265 = ashr i64 %264, %127
  %266 = trunc i64 %265 to i32
  store i32 %266, i32* %130, align 8
  %267 = load i32, i32* %69, align 4
  store i32 %267, i32* %141, align 4
  %268 = load i32, i32* %73, align 4
  store i32 %268, i32* %147, align 16
  %269 = mul nsw i32 %257, %216
  %270 = sext i32 %269 to i64
  %271 = mul nsw i32 %256, %229
  %272 = sext i32 %271 to i64
  %273 = add i64 %124, %272
  %274 = add i64 %273, %270
  %275 = ashr i64 %274, %127
  %276 = trunc i64 %275 to i32
  store i32 %276, i32* %153, align 4
  %277 = mul nsw i32 %247, %219
  %278 = sext i32 %277 to i64
  %279 = mul nsw i32 %246, %216
  %280 = sext i32 %279 to i64
  %281 = add i64 %124, %280
  %282 = add i64 %281, %278
  %283 = ashr i64 %282, %127
  %284 = trunc i64 %283 to i32
  store i32 %284, i32* %155, align 8
  %285 = load i32, i32* %85, align 4
  store i32 %285, i32* %157, align 4
  %286 = getelementptr inbounds i8, i8* %3, i64 4
  %287 = load i8, i8* %286, align 1
  call void @av1_range_check_buf(i32 4, i32* %0, i32* nonnull %91, i32 16, i8 signext %287) #3
  %288 = bitcast [16 x i32]* %5 to <4 x i32>*
  %289 = load <4 x i32>, <4 x i32>* %288, align 16
  %290 = bitcast i32* %1 to <4 x i32>*
  store <4 x i32> %289, <4 x i32>* %290, align 4
  %291 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %90, i64 56
  %292 = load i32, i32* %291, align 16
  %293 = load i32, i32* %102, align 16
  %294 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %90, i64 8
  %295 = load i32, i32* %294, align 16
  %296 = load i32, i32* %107, align 4
  %297 = mul nsw i32 %293, %292
  %298 = sext i32 %297 to i64
  %299 = mul nsw i32 %296, %295
  %300 = sext i32 %299 to i64
  %301 = add i64 %124, %298
  %302 = add i64 %301, %300
  %303 = ashr i64 %302, %127
  %304 = trunc i64 %303 to i32
  store i32 %304, i32* %35, align 4
  %305 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %90, i64 24
  %306 = load i32, i32* %305, align 16
  %307 = load i32, i32* %103, align 4
  %308 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %90, i64 40
  %309 = load i32, i32* %308, align 16
  %310 = load i32, i32* %104, align 8
  %311 = mul nsw i32 %307, %306
  %312 = sext i32 %311 to i64
  %313 = mul nsw i32 %310, %309
  %314 = sext i32 %313 to i64
  %315 = add i64 %124, %312
  %316 = add i64 %315, %314
  %317 = ashr i64 %316, %127
  %318 = trunc i64 %317 to i32
  store i32 %318, i32* %41, align 4
  %319 = mul nsw i32 %310, %306
  %320 = sext i32 %319 to i64
  %321 = mul i32 %307, %309
  %322 = sub i32 0, %321
  %323 = sext i32 %322 to i64
  %324 = add i64 %124, %323
  %325 = add i64 %324, %320
  %326 = ashr i64 %325, %127
  %327 = trunc i64 %326 to i32
  store i32 %327, i32* %47, align 4
  %328 = mul nsw i32 %296, %292
  %329 = sext i32 %328 to i64
  %330 = mul i32 %293, %295
  %331 = sub i32 0, %330
  %332 = sext i32 %331 to i64
  %333 = add i64 %124, %332
  %334 = add i64 %333, %329
  %335 = ashr i64 %334, %127
  %336 = trunc i64 %335 to i32
  store i32 %336, i32* %53, align 4
  %337 = bitcast i32* %110 to <4 x i32>*
  %338 = load <4 x i32>, <4 x i32>* %337, align 16
  %339 = shufflevector <4 x i32> %338, <4 x i32> undef, <4 x i32> <i32 1, i32 0, i32 3, i32 2>
  %340 = add nsw <4 x i32> %339, %338
  %341 = sub <4 x i32> %339, %338
  %342 = shufflevector <4 x i32> %340, <4 x i32> %341, <4 x i32> <i32 0, i32 5, i32 6, i32 3>
  %343 = bitcast i32* %57 to <4 x i32>*
  store <4 x i32> %342, <4 x i32>* %343, align 4
  %344 = bitcast i32* %147 to <4 x i32>*
  %345 = load <4 x i32>, <4 x i32>* %344, align 16
  %346 = shufflevector <4 x i32> %345, <4 x i32> undef, <4 x i32> <i32 1, i32 0, i32 3, i32 2>
  %347 = add nsw <4 x i32> %346, %345
  %348 = sub <4 x i32> %346, %345
  %349 = shufflevector <4 x i32> %347, <4 x i32> %348, <4 x i32> <i32 0, i32 5, i32 6, i32 3>
  %350 = bitcast i32* %73 to <4 x i32>*
  store <4 x i32> %349, <4 x i32>* %350, align 4
  %351 = getelementptr inbounds i8, i8* %3, i64 5
  %352 = load i8, i8* %351, align 1
  call void @av1_range_check_buf(i32 5, i32* %0, i32* %1, i32 16, i8 signext %352) #3
  %353 = bitcast i32* %1 to <4 x i32>*
  %354 = load <4 x i32>, <4 x i32>* %353, align 4
  %355 = bitcast [16 x i32]* %5 to <4 x i32>*
  store <4 x i32> %354, <4 x i32>* %355, align 16
  %356 = bitcast i32* %35 to <4 x i32>*
  %357 = load <4 x i32>, <4 x i32>* %356, align 4
  %358 = bitcast i32* %102 to <4 x i32>*
  store <4 x i32> %357, <4 x i32>* %358, align 16
  %359 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %90, i64 60
  %360 = load i32, i32* %359, align 16
  %361 = load i32, i32* %57, align 4
  %362 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %90, i64 4
  %363 = load i32, i32* %362, align 16
  %364 = load i32, i32* %85, align 4
  %365 = mul nsw i32 %361, %360
  %366 = sext i32 %365 to i64
  %367 = mul nsw i32 %364, %363
  %368 = sext i32 %367 to i64
  %369 = add i64 %124, %366
  %370 = add i64 %369, %368
  %371 = ashr i64 %370, %127
  %372 = trunc i64 %371 to i32
  store i32 %372, i32* %110, align 16
  %373 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %90, i64 28
  %374 = load i32, i32* %373, align 16
  %375 = load i32, i32* %61, align 4
  %376 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %90, i64 36
  %377 = load i32, i32* %376, align 16
  %378 = load i32, i32* %81, align 4
  %379 = mul nsw i32 %375, %374
  %380 = sext i32 %379 to i64
  %381 = mul nsw i32 %378, %377
  %382 = sext i32 %381 to i64
  %383 = add i64 %124, %380
  %384 = add i64 %383, %382
  %385 = ashr i64 %384, %127
  %386 = trunc i64 %385 to i32
  store i32 %386, i32* %112, align 4
  %387 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %90, i64 44
  %388 = load i32, i32* %387, align 16
  %389 = load i32, i32* %65, align 4
  %390 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %90, i64 20
  %391 = load i32, i32* %390, align 16
  %392 = load i32, i32* %77, align 4
  %393 = mul nsw i32 %389, %388
  %394 = sext i32 %393 to i64
  %395 = mul nsw i32 %392, %391
  %396 = sext i32 %395 to i64
  %397 = add i64 %124, %394
  %398 = add i64 %397, %396
  %399 = ashr i64 %398, %127
  %400 = trunc i64 %399 to i32
  store i32 %400, i32* %130, align 8
  %401 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %90, i64 12
  %402 = load i32, i32* %401, align 16
  %403 = load i32, i32* %69, align 4
  %404 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %90, i64 52
  %405 = load i32, i32* %404, align 16
  %406 = load i32, i32* %73, align 4
  %407 = mul nsw i32 %403, %402
  %408 = sext i32 %407 to i64
  %409 = mul nsw i32 %406, %405
  %410 = sext i32 %409 to i64
  %411 = add i64 %124, %408
  %412 = add i64 %411, %410
  %413 = ashr i64 %412, %127
  %414 = trunc i64 %413 to i32
  store i32 %414, i32* %141, align 4
  %415 = mul nsw i32 %406, %402
  %416 = sext i32 %415 to i64
  %417 = mul i32 %403, %405
  %418 = sub i32 0, %417
  %419 = sext i32 %418 to i64
  %420 = add i64 %124, %419
  %421 = add i64 %420, %416
  %422 = ashr i64 %421, %127
  %423 = trunc i64 %422 to i32
  store i32 %423, i32* %147, align 16
  %424 = mul nsw i32 %392, %388
  %425 = sext i32 %424 to i64
  %426 = mul i32 %389, %391
  %427 = sub i32 0, %426
  %428 = sext i32 %427 to i64
  %429 = add i64 %124, %428
  %430 = add i64 %429, %425
  %431 = ashr i64 %430, %127
  %432 = trunc i64 %431 to i32
  store i32 %432, i32* %153, align 4
  %433 = mul nsw i32 %378, %374
  %434 = sext i32 %433 to i64
  %435 = mul i32 %375, %377
  %436 = sub i32 0, %435
  %437 = sext i32 %436 to i64
  %438 = add i64 %124, %437
  %439 = add i64 %438, %434
  %440 = ashr i64 %439, %127
  %441 = trunc i64 %440 to i32
  store i32 %441, i32* %155, align 8
  %442 = mul nsw i32 %364, %360
  %443 = sext i32 %442 to i64
  %444 = mul i32 %361, %363
  %445 = sub i32 0, %444
  %446 = sext i32 %445 to i64
  %447 = add i64 %124, %446
  %448 = add i64 %447, %443
  %449 = ashr i64 %448, %127
  %450 = trunc i64 %449 to i32
  store i32 %450, i32* %157, align 4
  %451 = getelementptr inbounds i8, i8* %3, i64 6
  %452 = load i8, i8* %451, align 1
  call void @av1_range_check_buf(i32 6, i32* %0, i32* nonnull %91, i32 16, i8 signext %452) #3
  %453 = load i32, i32* %91, align 16
  store i32 %453, i32* %1, align 4
  %454 = load i32, i32* %110, align 16
  store i32 %454, i32* %17, align 4
  %455 = load i32, i32* %102, align 16
  store i32 %455, i32* %23, align 4
  %456 = load i32, i32* %147, align 16
  store i32 %456, i32* %29, align 4
  %457 = load i32, i32* %93, align 8
  store i32 %457, i32* %35, align 4
  %458 = load i32, i32* %130, align 8
  store i32 %458, i32* %41, align 4
  %459 = load i32, i32* %104, align 8
  store i32 %459, i32* %47, align 4
  %460 = load i32, i32* %155, align 8
  store i32 %460, i32* %53, align 4
  %461 = load i32, i32* %92, align 4
  store i32 %461, i32* %57, align 4
  %462 = load i32, i32* %112, align 4
  store i32 %462, i32* %61, align 4
  %463 = load i32, i32* %103, align 4
  store i32 %463, i32* %65, align 4
  %464 = load i32, i32* %153, align 4
  store i32 %464, i32* %69, align 4
  %465 = load i32, i32* %100, align 4
  store i32 %465, i32* %73, align 4
  %466 = load i32, i32* %141, align 4
  store i32 %466, i32* %77, align 4
  %467 = load i32, i32* %107, align 4
  store i32 %467, i32* %81, align 4
  %468 = load i32, i32* %157, align 4
  store i32 %468, i32* %85, align 4
  %469 = getelementptr inbounds i8, i8* %3, i64 7
  %470 = load i8, i8* %469, align 1
  call void @av1_range_check_buf(i32 7, i32* %0, i32* %1, i32 16, i8 signext %470) #3
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %6) #3
  ret void
}

; Function Attrs: argmemonly nounwind
declare void @llvm.memset.p0i8.i64(i8* nocapture writeonly, i8, i64, i1 immarg) #1

; Function Attrs: nounwind ssp uwtable
define hidden void @av1_fdct32(i32*, i32*, i8 signext, i8* nocapture readonly) local_unnamed_addr #0 {
  %5 = alloca [32 x i32], align 16
  %6 = bitcast [32 x i32]* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 128, i8* nonnull %6) #3
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %6, i8 -86, i64 128, i1 false)
  %7 = load i8, i8* %3, align 1
  tail call void @av1_range_check_buf(i32 0, i32* %0, i32* %0, i32 32, i8 signext %7) #3
  %8 = load i32, i32* %0, align 4
  %9 = getelementptr inbounds i32, i32* %0, i64 31
  %10 = load i32, i32* %9, align 4
  %11 = add nsw i32 %10, %8
  store i32 %11, i32* %1, align 4
  %12 = getelementptr inbounds i32, i32* %0, i64 1
  %13 = load i32, i32* %12, align 4
  %14 = getelementptr inbounds i32, i32* %0, i64 30
  %15 = load i32, i32* %14, align 4
  %16 = add nsw i32 %15, %13
  %17 = getelementptr inbounds i32, i32* %1, i64 1
  store i32 %16, i32* %17, align 4
  %18 = getelementptr inbounds i32, i32* %0, i64 2
  %19 = load i32, i32* %18, align 4
  %20 = getelementptr inbounds i32, i32* %0, i64 29
  %21 = load i32, i32* %20, align 4
  %22 = add nsw i32 %21, %19
  %23 = getelementptr inbounds i32, i32* %1, i64 2
  store i32 %22, i32* %23, align 4
  %24 = getelementptr inbounds i32, i32* %0, i64 3
  %25 = load i32, i32* %24, align 4
  %26 = getelementptr inbounds i32, i32* %0, i64 28
  %27 = load i32, i32* %26, align 4
  %28 = add nsw i32 %27, %25
  %29 = getelementptr inbounds i32, i32* %1, i64 3
  store i32 %28, i32* %29, align 4
  %30 = getelementptr inbounds i32, i32* %0, i64 4
  %31 = load i32, i32* %30, align 4
  %32 = getelementptr inbounds i32, i32* %0, i64 27
  %33 = load i32, i32* %32, align 4
  %34 = add nsw i32 %33, %31
  %35 = getelementptr inbounds i32, i32* %1, i64 4
  store i32 %34, i32* %35, align 4
  %36 = getelementptr inbounds i32, i32* %0, i64 5
  %37 = load i32, i32* %36, align 4
  %38 = getelementptr inbounds i32, i32* %0, i64 26
  %39 = load i32, i32* %38, align 4
  %40 = add nsw i32 %39, %37
  %41 = getelementptr inbounds i32, i32* %1, i64 5
  store i32 %40, i32* %41, align 4
  %42 = getelementptr inbounds i32, i32* %0, i64 6
  %43 = load i32, i32* %42, align 4
  %44 = getelementptr inbounds i32, i32* %0, i64 25
  %45 = load i32, i32* %44, align 4
  %46 = add nsw i32 %45, %43
  %47 = getelementptr inbounds i32, i32* %1, i64 6
  store i32 %46, i32* %47, align 4
  %48 = getelementptr inbounds i32, i32* %0, i64 7
  %49 = load i32, i32* %48, align 4
  %50 = getelementptr inbounds i32, i32* %0, i64 24
  %51 = load i32, i32* %50, align 4
  %52 = add nsw i32 %51, %49
  %53 = getelementptr inbounds i32, i32* %1, i64 7
  store i32 %52, i32* %53, align 4
  %54 = getelementptr inbounds i32, i32* %0, i64 8
  %55 = load i32, i32* %54, align 4
  %56 = getelementptr inbounds i32, i32* %0, i64 23
  %57 = load i32, i32* %56, align 4
  %58 = add nsw i32 %57, %55
  %59 = getelementptr inbounds i32, i32* %1, i64 8
  store i32 %58, i32* %59, align 4
  %60 = getelementptr inbounds i32, i32* %0, i64 9
  %61 = load i32, i32* %60, align 4
  %62 = getelementptr inbounds i32, i32* %0, i64 22
  %63 = load i32, i32* %62, align 4
  %64 = add nsw i32 %63, %61
  %65 = getelementptr inbounds i32, i32* %1, i64 9
  store i32 %64, i32* %65, align 4
  %66 = getelementptr inbounds i32, i32* %0, i64 10
  %67 = load i32, i32* %66, align 4
  %68 = getelementptr inbounds i32, i32* %0, i64 21
  %69 = load i32, i32* %68, align 4
  %70 = add nsw i32 %69, %67
  %71 = getelementptr inbounds i32, i32* %1, i64 10
  store i32 %70, i32* %71, align 4
  %72 = getelementptr inbounds i32, i32* %0, i64 11
  %73 = load i32, i32* %72, align 4
  %74 = getelementptr inbounds i32, i32* %0, i64 20
  %75 = load i32, i32* %74, align 4
  %76 = add nsw i32 %75, %73
  %77 = getelementptr inbounds i32, i32* %1, i64 11
  store i32 %76, i32* %77, align 4
  %78 = getelementptr inbounds i32, i32* %0, i64 12
  %79 = load i32, i32* %78, align 4
  %80 = getelementptr inbounds i32, i32* %0, i64 19
  %81 = load i32, i32* %80, align 4
  %82 = add nsw i32 %81, %79
  %83 = getelementptr inbounds i32, i32* %1, i64 12
  store i32 %82, i32* %83, align 4
  %84 = getelementptr inbounds i32, i32* %0, i64 13
  %85 = load i32, i32* %84, align 4
  %86 = getelementptr inbounds i32, i32* %0, i64 18
  %87 = load i32, i32* %86, align 4
  %88 = add nsw i32 %87, %85
  %89 = getelementptr inbounds i32, i32* %1, i64 13
  store i32 %88, i32* %89, align 4
  %90 = getelementptr inbounds i32, i32* %0, i64 14
  %91 = load i32, i32* %90, align 4
  %92 = getelementptr inbounds i32, i32* %0, i64 17
  %93 = load i32, i32* %92, align 4
  %94 = add nsw i32 %93, %91
  %95 = getelementptr inbounds i32, i32* %1, i64 14
  store i32 %94, i32* %95, align 4
  %96 = getelementptr inbounds i32, i32* %0, i64 15
  %97 = load i32, i32* %96, align 4
  %98 = getelementptr inbounds i32, i32* %0, i64 16
  %99 = load i32, i32* %98, align 4
  %100 = add nsw i32 %99, %97
  %101 = getelementptr inbounds i32, i32* %1, i64 15
  store i32 %100, i32* %101, align 4
  %102 = load i32, i32* %98, align 4
  %103 = load i32, i32* %96, align 4
  %104 = sub i32 %103, %102
  %105 = getelementptr inbounds i32, i32* %1, i64 16
  store i32 %104, i32* %105, align 4
  %106 = load i32, i32* %92, align 4
  %107 = load i32, i32* %90, align 4
  %108 = sub i32 %107, %106
  %109 = getelementptr inbounds i32, i32* %1, i64 17
  store i32 %108, i32* %109, align 4
  %110 = load i32, i32* %86, align 4
  %111 = load i32, i32* %84, align 4
  %112 = sub i32 %111, %110
  %113 = getelementptr inbounds i32, i32* %1, i64 18
  store i32 %112, i32* %113, align 4
  %114 = load i32, i32* %80, align 4
  %115 = load i32, i32* %78, align 4
  %116 = sub i32 %115, %114
  %117 = getelementptr inbounds i32, i32* %1, i64 19
  store i32 %116, i32* %117, align 4
  %118 = load i32, i32* %74, align 4
  %119 = load i32, i32* %72, align 4
  %120 = sub i32 %119, %118
  %121 = getelementptr inbounds i32, i32* %1, i64 20
  store i32 %120, i32* %121, align 4
  %122 = load i32, i32* %68, align 4
  %123 = load i32, i32* %66, align 4
  %124 = sub i32 %123, %122
  %125 = getelementptr inbounds i32, i32* %1, i64 21
  store i32 %124, i32* %125, align 4
  %126 = load i32, i32* %62, align 4
  %127 = load i32, i32* %60, align 4
  %128 = sub i32 %127, %126
  %129 = getelementptr inbounds i32, i32* %1, i64 22
  store i32 %128, i32* %129, align 4
  %130 = load i32, i32* %56, align 4
  %131 = load i32, i32* %54, align 4
  %132 = sub i32 %131, %130
  %133 = getelementptr inbounds i32, i32* %1, i64 23
  store i32 %132, i32* %133, align 4
  %134 = load i32, i32* %50, align 4
  %135 = load i32, i32* %48, align 4
  %136 = sub i32 %135, %134
  %137 = getelementptr inbounds i32, i32* %1, i64 24
  store i32 %136, i32* %137, align 4
  %138 = load i32, i32* %44, align 4
  %139 = load i32, i32* %42, align 4
  %140 = sub i32 %139, %138
  %141 = getelementptr inbounds i32, i32* %1, i64 25
  store i32 %140, i32* %141, align 4
  %142 = load i32, i32* %38, align 4
  %143 = load i32, i32* %36, align 4
  %144 = sub i32 %143, %142
  %145 = getelementptr inbounds i32, i32* %1, i64 26
  store i32 %144, i32* %145, align 4
  %146 = load i32, i32* %32, align 4
  %147 = load i32, i32* %30, align 4
  %148 = sub i32 %147, %146
  %149 = getelementptr inbounds i32, i32* %1, i64 27
  store i32 %148, i32* %149, align 4
  %150 = load i32, i32* %26, align 4
  %151 = load i32, i32* %24, align 4
  %152 = sub i32 %151, %150
  %153 = getelementptr inbounds i32, i32* %1, i64 28
  store i32 %152, i32* %153, align 4
  %154 = load i32, i32* %20, align 4
  %155 = load i32, i32* %18, align 4
  %156 = sub i32 %155, %154
  %157 = getelementptr inbounds i32, i32* %1, i64 29
  store i32 %156, i32* %157, align 4
  %158 = load i32, i32* %14, align 4
  %159 = load i32, i32* %12, align 4
  %160 = sub i32 %159, %158
  %161 = getelementptr inbounds i32, i32* %1, i64 30
  store i32 %160, i32* %161, align 4
  %162 = load i32, i32* %9, align 4
  %163 = load i32, i32* %0, align 4
  %164 = sub i32 %163, %162
  %165 = getelementptr inbounds i32, i32* %1, i64 31
  store i32 %164, i32* %165, align 4
  %166 = getelementptr inbounds i8, i8* %3, i64 1
  %167 = load i8, i8* %166, align 1
  tail call void @av1_range_check_buf(i32 1, i32* %0, i32* %1, i32 32, i8 signext %167) #3
  %168 = sext i8 %2 to i32
  %169 = add nsw i32 %168, -10
  %170 = sext i32 %169 to i64
  %171 = getelementptr inbounds [32 x i32], [32 x i32]* %5, i64 0, i64 0
  %172 = getelementptr inbounds [32 x i32], [32 x i32]* %5, i64 0, i64 1
  %173 = getelementptr inbounds [32 x i32], [32 x i32]* %5, i64 0, i64 2
  %174 = bitcast i32* %1 to <4 x i32>*
  %175 = load <4 x i32>, <4 x i32>* %174, align 4
  %176 = bitcast i32* %83 to <4 x i32>*
  %177 = load <4 x i32>, <4 x i32>* %176, align 4
  %178 = shufflevector <4 x i32> %177, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %179 = add nsw <4 x i32> %178, %175
  %180 = getelementptr inbounds [32 x i32], [32 x i32]* %5, i64 0, i64 3
  %181 = bitcast [32 x i32]* %5 to <4 x i32>*
  store <4 x i32> %179, <4 x i32>* %181, align 16
  %182 = getelementptr inbounds [32 x i32], [32 x i32]* %5, i64 0, i64 4
  %183 = getelementptr inbounds [32 x i32], [32 x i32]* %5, i64 0, i64 5
  %184 = getelementptr inbounds [32 x i32], [32 x i32]* %5, i64 0, i64 6
  %185 = bitcast i32* %35 to <4 x i32>*
  %186 = load <4 x i32>, <4 x i32>* %185, align 4
  %187 = bitcast i32* %59 to <4 x i32>*
  %188 = load <4 x i32>, <4 x i32>* %187, align 4
  %189 = shufflevector <4 x i32> %188, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %190 = add nsw <4 x i32> %189, %186
  %191 = getelementptr inbounds [32 x i32], [32 x i32]* %5, i64 0, i64 7
  %192 = bitcast i32* %182 to <4 x i32>*
  store <4 x i32> %190, <4 x i32>* %192, align 16
  %193 = getelementptr inbounds [32 x i32], [32 x i32]* %5, i64 0, i64 8
  %194 = getelementptr inbounds [32 x i32], [32 x i32]* %5, i64 0, i64 9
  %195 = getelementptr inbounds [32 x i32], [32 x i32]* %5, i64 0, i64 10
  %196 = shufflevector <4 x i32> %186, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %197 = sub <4 x i32> %196, %188
  %198 = getelementptr inbounds [32 x i32], [32 x i32]* %5, i64 0, i64 11
  %199 = bitcast i32* %193 to <4 x i32>*
  store <4 x i32> %197, <4 x i32>* %199, align 16
  %200 = getelementptr inbounds [32 x i32], [32 x i32]* %5, i64 0, i64 12
  %201 = getelementptr inbounds [32 x i32], [32 x i32]* %5, i64 0, i64 13
  %202 = getelementptr inbounds [32 x i32], [32 x i32]* %5, i64 0, i64 14
  %203 = shufflevector <4 x i32> %175, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %204 = sub <4 x i32> %203, %177
  %205 = getelementptr inbounds [32 x i32], [32 x i32]* %5, i64 0, i64 15
  %206 = bitcast i32* %200 to <4 x i32>*
  store <4 x i32> %204, <4 x i32>* %206, align 16
  %207 = getelementptr inbounds [32 x i32], [32 x i32]* %5, i64 0, i64 16
  %208 = getelementptr inbounds [32 x i32], [32 x i32]* %5, i64 0, i64 17
  %209 = getelementptr inbounds [32 x i32], [32 x i32]* %5, i64 0, i64 18
  %210 = bitcast i32* %105 to <4 x i32>*
  %211 = load <4 x i32>, <4 x i32>* %210, align 4
  %212 = getelementptr inbounds [32 x i32], [32 x i32]* %5, i64 0, i64 19
  %213 = bitcast i32* %207 to <4 x i32>*
  store <4 x i32> %211, <4 x i32>* %213, align 16
  %214 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %170, i64 32
  %215 = load i32, i32* %214, align 16
  %216 = sub nsw i32 0, %215
  %217 = load i32, i32* %121, align 4
  %218 = load i32, i32* %149, align 4
  %219 = mul nsw i32 %217, %216
  %220 = sext i32 %219 to i64
  %221 = mul nsw i32 %218, %215
  %222 = sext i32 %221 to i64
  %223 = add nsw i32 %168, -1
  %224 = zext i32 %223 to i64
  %225 = shl i64 1, %224
  %226 = add i64 %225, %222
  %227 = add i64 %226, %220
  %228 = zext i32 %168 to i64
  %229 = ashr i64 %227, %228
  %230 = trunc i64 %229 to i32
  %231 = getelementptr inbounds [32 x i32], [32 x i32]* %5, i64 0, i64 20
  store i32 %230, i32* %231, align 16
  %232 = load i32, i32* %125, align 4
  %233 = load i32, i32* %145, align 4
  %234 = mul nsw i32 %232, %216
  %235 = sext i32 %234 to i64
  %236 = mul nsw i32 %233, %215
  %237 = sext i32 %236 to i64
  %238 = add i64 %225, %237
  %239 = add i64 %238, %235
  %240 = ashr i64 %239, %228
  %241 = trunc i64 %240 to i32
  %242 = getelementptr inbounds [32 x i32], [32 x i32]* %5, i64 0, i64 21
  store i32 %241, i32* %242, align 4
  %243 = load i32, i32* %129, align 4
  %244 = load i32, i32* %141, align 4
  %245 = mul nsw i32 %243, %216
  %246 = sext i32 %245 to i64
  %247 = mul nsw i32 %244, %215
  %248 = sext i32 %247 to i64
  %249 = add i64 %225, %248
  %250 = add i64 %249, %246
  %251 = ashr i64 %250, %228
  %252 = trunc i64 %251 to i32
  %253 = getelementptr inbounds [32 x i32], [32 x i32]* %5, i64 0, i64 22
  store i32 %252, i32* %253, align 8
  %254 = load i32, i32* %133, align 4
  %255 = load i32, i32* %137, align 4
  %256 = mul nsw i32 %254, %216
  %257 = sext i32 %256 to i64
  %258 = mul nsw i32 %255, %215
  %259 = sext i32 %258 to i64
  %260 = add i64 %225, %259
  %261 = add i64 %260, %257
  %262 = ashr i64 %261, %228
  %263 = trunc i64 %262 to i32
  %264 = getelementptr inbounds [32 x i32], [32 x i32]* %5, i64 0, i64 23
  store i32 %263, i32* %264, align 4
  %265 = mul nsw i32 %254, %215
  %266 = sext i32 %265 to i64
  %267 = add i64 %260, %266
  %268 = ashr i64 %267, %228
  %269 = trunc i64 %268 to i32
  %270 = getelementptr inbounds [32 x i32], [32 x i32]* %5, i64 0, i64 24
  store i32 %269, i32* %270, align 16
  %271 = mul nsw i32 %243, %215
  %272 = sext i32 %271 to i64
  %273 = add i64 %249, %272
  %274 = ashr i64 %273, %228
  %275 = trunc i64 %274 to i32
  %276 = getelementptr inbounds [32 x i32], [32 x i32]* %5, i64 0, i64 25
  store i32 %275, i32* %276, align 4
  %277 = mul nsw i32 %232, %215
  %278 = sext i32 %277 to i64
  %279 = add i64 %238, %278
  %280 = ashr i64 %279, %228
  %281 = trunc i64 %280 to i32
  %282 = getelementptr inbounds [32 x i32], [32 x i32]* %5, i64 0, i64 26
  store i32 %281, i32* %282, align 8
  %283 = mul nsw i32 %217, %215
  %284 = sext i32 %283 to i64
  %285 = add i64 %226, %284
  %286 = ashr i64 %285, %228
  %287 = trunc i64 %286 to i32
  %288 = getelementptr inbounds [32 x i32], [32 x i32]* %5, i64 0, i64 27
  store i32 %287, i32* %288, align 4
  %289 = getelementptr inbounds [32 x i32], [32 x i32]* %5, i64 0, i64 28
  %290 = getelementptr inbounds [32 x i32], [32 x i32]* %5, i64 0, i64 29
  %291 = getelementptr inbounds [32 x i32], [32 x i32]* %5, i64 0, i64 30
  %292 = bitcast i32* %153 to <4 x i32>*
  %293 = load <4 x i32>, <4 x i32>* %292, align 4
  %294 = getelementptr inbounds [32 x i32], [32 x i32]* %5, i64 0, i64 31
  %295 = bitcast i32* %289 to <4 x i32>*
  store <4 x i32> %293, <4 x i32>* %295, align 16
  %296 = getelementptr inbounds i8, i8* %3, i64 2
  %297 = load i8, i8* %296, align 1
  call void @av1_range_check_buf(i32 2, i32* %0, i32* nonnull %171, i32 32, i8 signext %297) #3
  %298 = bitcast [32 x i32]* %5 to <4 x i32>*
  %299 = load <4 x i32>, <4 x i32>* %298, align 16
  %300 = bitcast i32* %182 to <4 x i32>*
  %301 = load <4 x i32>, <4 x i32>* %300, align 16
  %302 = shufflevector <4 x i32> %301, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %303 = add nsw <4 x i32> %302, %299
  %304 = bitcast i32* %1 to <4 x i32>*
  store <4 x i32> %303, <4 x i32>* %304, align 4
  %305 = shufflevector <4 x i32> %299, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %306 = sub <4 x i32> %305, %301
  %307 = bitcast i32* %35 to <4 x i32>*
  store <4 x i32> %306, <4 x i32>* %307, align 4
  %308 = load i32, i32* %193, align 16
  store i32 %308, i32* %59, align 4
  %309 = load i32, i32* %194, align 4
  store i32 %309, i32* %65, align 4
  %310 = load i32, i32* %195, align 8
  %311 = load i32, i32* %201, align 4
  %312 = mul nsw i32 %310, %216
  %313 = sext i32 %312 to i64
  %314 = mul nsw i32 %311, %215
  %315 = sext i32 %314 to i64
  %316 = add i64 %225, %315
  %317 = add i64 %316, %313
  %318 = ashr i64 %317, %228
  %319 = trunc i64 %318 to i32
  store i32 %319, i32* %71, align 4
  %320 = load i32, i32* %198, align 4
  %321 = load i32, i32* %200, align 16
  %322 = mul nsw i32 %320, %216
  %323 = sext i32 %322 to i64
  %324 = mul nsw i32 %321, %215
  %325 = sext i32 %324 to i64
  %326 = add i64 %225, %325
  %327 = add i64 %326, %323
  %328 = ashr i64 %327, %228
  %329 = trunc i64 %328 to i32
  store i32 %329, i32* %77, align 4
  %330 = mul nsw i32 %320, %215
  %331 = sext i32 %330 to i64
  %332 = add i64 %326, %331
  %333 = ashr i64 %332, %228
  %334 = trunc i64 %333 to i32
  store i32 %334, i32* %83, align 4
  %335 = mul nsw i32 %310, %215
  %336 = sext i32 %335 to i64
  %337 = add i64 %316, %336
  %338 = ashr i64 %337, %228
  %339 = trunc i64 %338 to i32
  store i32 %339, i32* %89, align 4
  %340 = load i32, i32* %202, align 8
  store i32 %340, i32* %95, align 4
  %341 = load i32, i32* %205, align 4
  store i32 %341, i32* %101, align 4
  %342 = bitcast i32* %207 to <4 x i32>*
  %343 = load <4 x i32>, <4 x i32>* %342, align 16
  %344 = bitcast i32* %231 to <4 x i32>*
  %345 = load <4 x i32>, <4 x i32>* %344, align 16
  %346 = shufflevector <4 x i32> %345, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %347 = add nsw <4 x i32> %346, %343
  %348 = bitcast i32* %105 to <4 x i32>*
  store <4 x i32> %347, <4 x i32>* %348, align 4
  %349 = shufflevector <4 x i32> %343, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %350 = sub <4 x i32> %349, %345
  %351 = bitcast i32* %121 to <4 x i32>*
  store <4 x i32> %350, <4 x i32>* %351, align 4
  %352 = bitcast i32* %270 to <4 x i32>*
  %353 = load <4 x i32>, <4 x i32>* %352, align 16
  %354 = bitcast i32* %289 to <4 x i32>*
  %355 = load <4 x i32>, <4 x i32>* %354, align 16
  %356 = shufflevector <4 x i32> %355, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %357 = sub <4 x i32> %356, %353
  %358 = bitcast i32* %137 to <4 x i32>*
  store <4 x i32> %357, <4 x i32>* %358, align 4
  %359 = shufflevector <4 x i32> %353, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %360 = add nsw <4 x i32> %355, %359
  %361 = bitcast i32* %153 to <4 x i32>*
  store <4 x i32> %360, <4 x i32>* %361, align 4
  %362 = getelementptr inbounds i8, i8* %3, i64 3
  %363 = load i8, i8* %362, align 1
  call void @av1_range_check_buf(i32 3, i32* %0, i32* %1, i32 32, i8 signext %363) #3
  %364 = bitcast i32* %1 to <4 x i32>*
  %365 = load <4 x i32>, <4 x i32>* %364, align 4
  %366 = shufflevector <4 x i32> %365, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %367 = add nsw <4 x i32> %366, %365
  %368 = sub <4 x i32> %366, %365
  %369 = shufflevector <4 x i32> %367, <4 x i32> %368, <4 x i32> <i32 0, i32 1, i32 6, i32 7>
  %370 = bitcast [32 x i32]* %5 to <4 x i32>*
  store <4 x i32> %369, <4 x i32>* %370, align 16
  %371 = load i32, i32* %35, align 4
  store i32 %371, i32* %182, align 16
  %372 = load i32, i32* %41, align 4
  %373 = load i32, i32* %47, align 4
  %374 = mul nsw i32 %372, %216
  %375 = sext i32 %374 to i64
  %376 = mul nsw i32 %373, %215
  %377 = sext i32 %376 to i64
  %378 = add i64 %225, %377
  %379 = add i64 %378, %375
  %380 = ashr i64 %379, %228
  %381 = trunc i64 %380 to i32
  store i32 %381, i32* %183, align 4
  %382 = mul nsw i32 %372, %215
  %383 = sext i32 %382 to i64
  %384 = add i64 %378, %383
  %385 = ashr i64 %384, %228
  %386 = trunc i64 %385 to i32
  store i32 %386, i32* %184, align 8
  %387 = load i32, i32* %53, align 4
  store i32 %387, i32* %191, align 4
  %388 = bitcast i32* %59 to <4 x i32>*
  %389 = load <4 x i32>, <4 x i32>* %388, align 4
  %390 = shufflevector <4 x i32> %389, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %391 = add nsw <4 x i32> %390, %389
  %392 = sub <4 x i32> %390, %389
  %393 = shufflevector <4 x i32> %391, <4 x i32> %392, <4 x i32> <i32 0, i32 1, i32 6, i32 7>
  %394 = bitcast i32* %193 to <4 x i32>*
  store <4 x i32> %393, <4 x i32>* %394, align 16
  %395 = bitcast i32* %83 to <4 x i32>*
  %396 = load <4 x i32>, <4 x i32>* %395, align 4
  %397 = shufflevector <4 x i32> %396, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %398 = sub <4 x i32> %397, %396
  %399 = add nsw <4 x i32> %397, %396
  %400 = shufflevector <4 x i32> %398, <4 x i32> %399, <4 x i32> <i32 0, i32 1, i32 6, i32 7>
  %401 = bitcast i32* %200 to <4 x i32>*
  store <4 x i32> %400, <4 x i32>* %401, align 16
  %402 = load i32, i32* %105, align 4
  store i32 %402, i32* %207, align 16
  %403 = load i32, i32* %109, align 4
  store i32 %403, i32* %208, align 4
  %404 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %170, i64 16
  %405 = load i32, i32* %404, align 16
  %406 = sub nsw i32 0, %405
  %407 = load i32, i32* %113, align 4
  %408 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %170, i64 48
  %409 = load i32, i32* %408, align 16
  %410 = load i32, i32* %157, align 4
  %411 = mul nsw i32 %407, %406
  %412 = sext i32 %411 to i64
  %413 = mul nsw i32 %410, %409
  %414 = sext i32 %413 to i64
  %415 = add i64 %225, %412
  %416 = add i64 %415, %414
  %417 = ashr i64 %416, %228
  %418 = trunc i64 %417 to i32
  store i32 %418, i32* %209, align 8
  %419 = load i32, i32* %117, align 4
  %420 = load i32, i32* %153, align 4
  %421 = mul nsw i32 %419, %406
  %422 = sext i32 %421 to i64
  %423 = mul nsw i32 %420, %409
  %424 = sext i32 %423 to i64
  %425 = add i64 %225, %422
  %426 = add i64 %425, %424
  %427 = ashr i64 %426, %228
  %428 = trunc i64 %427 to i32
  store i32 %428, i32* %212, align 4
  %429 = sub nsw i32 0, %409
  %430 = load i32, i32* %121, align 4
  %431 = load i32, i32* %149, align 4
  %432 = mul nsw i32 %430, %429
  %433 = sext i32 %432 to i64
  %434 = mul nsw i32 %431, %406
  %435 = sext i32 %434 to i64
  %436 = add i64 %225, %433
  %437 = add i64 %436, %435
  %438 = ashr i64 %437, %228
  %439 = trunc i64 %438 to i32
  store i32 %439, i32* %231, align 16
  %440 = load i32, i32* %125, align 4
  %441 = load i32, i32* %145, align 4
  %442 = mul nsw i32 %440, %429
  %443 = sext i32 %442 to i64
  %444 = mul nsw i32 %441, %406
  %445 = sext i32 %444 to i64
  %446 = add i64 %225, %443
  %447 = add i64 %446, %445
  %448 = ashr i64 %447, %228
  %449 = trunc i64 %448 to i32
  store i32 %449, i32* %242, align 4
  %450 = bitcast i32* %129 to <4 x i32>*
  %451 = load <4 x i32>, <4 x i32>* %450, align 4
  %452 = bitcast i32* %253 to <4 x i32>*
  store <4 x i32> %451, <4 x i32>* %452, align 8
  %453 = mul nsw i32 %441, %409
  %454 = sext i32 %453 to i64
  %455 = mul nsw i32 %440, %406
  %456 = sext i32 %455 to i64
  %457 = add i64 %225, %456
  %458 = add i64 %457, %454
  %459 = ashr i64 %458, %228
  %460 = trunc i64 %459 to i32
  store i32 %460, i32* %282, align 8
  %461 = mul nsw i32 %431, %409
  %462 = sext i32 %461 to i64
  %463 = mul nsw i32 %430, %406
  %464 = sext i32 %463 to i64
  %465 = add i64 %225, %464
  %466 = add i64 %465, %462
  %467 = ashr i64 %466, %228
  %468 = trunc i64 %467 to i32
  store i32 %468, i32* %288, align 4
  %469 = mul nsw i32 %420, %405
  %470 = sext i32 %469 to i64
  %471 = mul nsw i32 %419, %409
  %472 = sext i32 %471 to i64
  %473 = add i64 %225, %472
  %474 = add i64 %473, %470
  %475 = ashr i64 %474, %228
  %476 = trunc i64 %475 to i32
  store i32 %476, i32* %289, align 16
  %477 = mul nsw i32 %410, %405
  %478 = sext i32 %477 to i64
  %479 = mul nsw i32 %409, %407
  %480 = sext i32 %479 to i64
  %481 = add i64 %225, %480
  %482 = add i64 %481, %478
  %483 = ashr i64 %482, %228
  %484 = trunc i64 %483 to i32
  store i32 %484, i32* %290, align 4
  %485 = load i32, i32* %161, align 4
  store i32 %485, i32* %291, align 8
  %486 = load i32, i32* %165, align 4
  store i32 %486, i32* %294, align 4
  %487 = getelementptr inbounds i8, i8* %3, i64 4
  %488 = load i8, i8* %487, align 1
  call void @av1_range_check_buf(i32 4, i32* %0, i32* nonnull %171, i32 32, i8 signext %488) #3
  %489 = load i32, i32* %171, align 16
  %490 = load i32, i32* %172, align 4
  %491 = mul nsw i32 %489, %215
  %492 = sext i32 %491 to i64
  %493 = mul nsw i32 %490, %215
  %494 = sext i32 %493 to i64
  %495 = add i64 %225, %492
  %496 = add i64 %495, %494
  %497 = ashr i64 %496, %228
  %498 = trunc i64 %497 to i32
  store i32 %498, i32* %1, align 4
  %499 = mul nsw i32 %490, %216
  %500 = sext i32 %499 to i64
  %501 = add i64 %495, %500
  %502 = ashr i64 %501, %228
  %503 = trunc i64 %502 to i32
  store i32 %503, i32* %17, align 4
  %504 = load i32, i32* %173, align 8
  %505 = load i32, i32* %180, align 4
  %506 = mul nsw i32 %504, %409
  %507 = sext i32 %506 to i64
  %508 = mul nsw i32 %505, %405
  %509 = sext i32 %508 to i64
  %510 = add i64 %225, %507
  %511 = add i64 %510, %509
  %512 = ashr i64 %511, %228
  %513 = trunc i64 %512 to i32
  store i32 %513, i32* %23, align 4
  %514 = mul nsw i32 %505, %409
  %515 = sext i32 %514 to i64
  %516 = mul nsw i32 %504, %406
  %517 = sext i32 %516 to i64
  %518 = add i64 %225, %517
  %519 = add i64 %518, %515
  %520 = ashr i64 %519, %228
  %521 = trunc i64 %520 to i32
  store i32 %521, i32* %29, align 4
  %522 = bitcast i32* %182 to <4 x i32>*
  %523 = load <4 x i32>, <4 x i32>* %522, align 16
  %524 = shufflevector <4 x i32> %523, <4 x i32> undef, <4 x i32> <i32 1, i32 0, i32 3, i32 2>
  %525 = add nsw <4 x i32> %524, %523
  %526 = sub <4 x i32> %524, %523
  %527 = shufflevector <4 x i32> %525, <4 x i32> %526, <4 x i32> <i32 0, i32 5, i32 6, i32 3>
  %528 = bitcast i32* %35 to <4 x i32>*
  store <4 x i32> %527, <4 x i32>* %528, align 4
  %529 = load i32, i32* %193, align 16
  store i32 %529, i32* %59, align 4
  %530 = load i32, i32* %194, align 4
  %531 = load i32, i32* %202, align 8
  %532 = mul nsw i32 %530, %406
  %533 = sext i32 %532 to i64
  %534 = mul nsw i32 %531, %409
  %535 = sext i32 %534 to i64
  %536 = add i64 %225, %533
  %537 = add i64 %536, %535
  %538 = ashr i64 %537, %228
  %539 = trunc i64 %538 to i32
  store i32 %539, i32* %65, align 4
  %540 = load i32, i32* %195, align 8
  %541 = load i32, i32* %201, align 4
  %542 = mul nsw i32 %540, %429
  %543 = sext i32 %542 to i64
  %544 = mul nsw i32 %541, %406
  %545 = sext i32 %544 to i64
  %546 = add i64 %225, %543
  %547 = add i64 %546, %545
  %548 = ashr i64 %547, %228
  %549 = trunc i64 %548 to i32
  store i32 %549, i32* %71, align 4
  %550 = load i32, i32* %198, align 4
  store i32 %550, i32* %77, align 4
  %551 = load i32, i32* %200, align 16
  store i32 %551, i32* %83, align 4
  %552 = mul nsw i32 %541, %409
  %553 = sext i32 %552 to i64
  %554 = mul nsw i32 %540, %406
  %555 = sext i32 %554 to i64
  %556 = add i64 %225, %555
  %557 = add i64 %556, %553
  %558 = ashr i64 %557, %228
  %559 = trunc i64 %558 to i32
  store i32 %559, i32* %89, align 4
  %560 = mul nsw i32 %531, %405
  %561 = sext i32 %560 to i64
  %562 = mul nsw i32 %530, %409
  %563 = sext i32 %562 to i64
  %564 = add i64 %225, %563
  %565 = add i64 %564, %561
  %566 = ashr i64 %565, %228
  %567 = trunc i64 %566 to i32
  store i32 %567, i32* %95, align 4
  %568 = load i32, i32* %205, align 4
  store i32 %568, i32* %101, align 4
  %569 = bitcast i32* %207 to <4 x i32>*
  %570 = load <4 x i32>, <4 x i32>* %569, align 16
  %571 = shufflevector <4 x i32> %570, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %572 = add nsw <4 x i32> %571, %570
  %573 = sub <4 x i32> %571, %570
  %574 = shufflevector <4 x i32> %572, <4 x i32> %573, <4 x i32> <i32 0, i32 1, i32 6, i32 7>
  %575 = bitcast i32* %105 to <4 x i32>*
  store <4 x i32> %574, <4 x i32>* %575, align 4
  %576 = bitcast i32* %231 to <4 x i32>*
  %577 = load <4 x i32>, <4 x i32>* %576, align 16
  %578 = shufflevector <4 x i32> %577, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %579 = sub <4 x i32> %578, %577
  %580 = add nsw <4 x i32> %578, %577
  %581 = shufflevector <4 x i32> %579, <4 x i32> %580, <4 x i32> <i32 0, i32 1, i32 6, i32 7>
  %582 = bitcast i32* %121 to <4 x i32>*
  store <4 x i32> %581, <4 x i32>* %582, align 4
  %583 = bitcast i32* %270 to <4 x i32>*
  %584 = load <4 x i32>, <4 x i32>* %583, align 16
  %585 = shufflevector <4 x i32> %584, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %586 = add nsw <4 x i32> %585, %584
  %587 = sub <4 x i32> %585, %584
  %588 = shufflevector <4 x i32> %586, <4 x i32> %587, <4 x i32> <i32 0, i32 1, i32 6, i32 7>
  %589 = bitcast i32* %137 to <4 x i32>*
  store <4 x i32> %588, <4 x i32>* %589, align 4
  %590 = bitcast i32* %289 to <4 x i32>*
  %591 = load <4 x i32>, <4 x i32>* %590, align 16
  %592 = shufflevector <4 x i32> %591, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %593 = sub <4 x i32> %592, %591
  %594 = add nsw <4 x i32> %592, %591
  %595 = shufflevector <4 x i32> %593, <4 x i32> %594, <4 x i32> <i32 0, i32 1, i32 6, i32 7>
  %596 = bitcast i32* %153 to <4 x i32>*
  store <4 x i32> %595, <4 x i32>* %596, align 4
  %597 = getelementptr inbounds i8, i8* %3, i64 5
  %598 = load i8, i8* %597, align 1
  call void @av1_range_check_buf(i32 5, i32* %0, i32* %1, i32 32, i8 signext %598) #3
  %599 = bitcast i32* %1 to <4 x i32>*
  %600 = load <4 x i32>, <4 x i32>* %599, align 4
  %601 = bitcast [32 x i32]* %5 to <4 x i32>*
  store <4 x i32> %600, <4 x i32>* %601, align 16
  %602 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %170, i64 56
  %603 = load i32, i32* %602, align 16
  %604 = load i32, i32* %35, align 4
  %605 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %170, i64 8
  %606 = load i32, i32* %605, align 16
  %607 = load i32, i32* %53, align 4
  %608 = mul nsw i32 %604, %603
  %609 = sext i32 %608 to i64
  %610 = mul nsw i32 %607, %606
  %611 = sext i32 %610 to i64
  %612 = add i64 %225, %609
  %613 = add i64 %612, %611
  %614 = ashr i64 %613, %228
  %615 = trunc i64 %614 to i32
  store i32 %615, i32* %182, align 16
  %616 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %170, i64 24
  %617 = load i32, i32* %616, align 16
  %618 = load i32, i32* %41, align 4
  %619 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %170, i64 40
  %620 = load i32, i32* %619, align 16
  %621 = load i32, i32* %47, align 4
  %622 = mul nsw i32 %618, %617
  %623 = sext i32 %622 to i64
  %624 = mul nsw i32 %621, %620
  %625 = sext i32 %624 to i64
  %626 = add i64 %225, %623
  %627 = add i64 %626, %625
  %628 = ashr i64 %627, %228
  %629 = trunc i64 %628 to i32
  store i32 %629, i32* %183, align 4
  %630 = sub nsw i32 0, %620
  %631 = mul nsw i32 %621, %617
  %632 = sext i32 %631 to i64
  %633 = mul nsw i32 %618, %630
  %634 = sext i32 %633 to i64
  %635 = add i64 %225, %634
  %636 = add i64 %635, %632
  %637 = ashr i64 %636, %228
  %638 = trunc i64 %637 to i32
  store i32 %638, i32* %184, align 8
  %639 = sub nsw i32 0, %606
  %640 = mul nsw i32 %607, %603
  %641 = sext i32 %640 to i64
  %642 = mul nsw i32 %604, %639
  %643 = sext i32 %642 to i64
  %644 = add i64 %225, %643
  %645 = add i64 %644, %641
  %646 = ashr i64 %645, %228
  %647 = trunc i64 %646 to i32
  store i32 %647, i32* %191, align 4
  %648 = bitcast i32* %59 to <4 x i32>*
  %649 = load <4 x i32>, <4 x i32>* %648, align 4
  %650 = shufflevector <4 x i32> %649, <4 x i32> undef, <4 x i32> <i32 1, i32 0, i32 3, i32 2>
  %651 = add nsw <4 x i32> %650, %649
  %652 = sub <4 x i32> %650, %649
  %653 = shufflevector <4 x i32> %651, <4 x i32> %652, <4 x i32> <i32 0, i32 5, i32 6, i32 3>
  %654 = bitcast i32* %193 to <4 x i32>*
  store <4 x i32> %653, <4 x i32>* %654, align 16
  %655 = bitcast i32* %83 to <4 x i32>*
  %656 = load <4 x i32>, <4 x i32>* %655, align 4
  %657 = shufflevector <4 x i32> %656, <4 x i32> undef, <4 x i32> <i32 1, i32 0, i32 3, i32 2>
  %658 = add nsw <4 x i32> %657, %656
  %659 = sub <4 x i32> %657, %656
  %660 = shufflevector <4 x i32> %658, <4 x i32> %659, <4 x i32> <i32 0, i32 5, i32 6, i32 3>
  %661 = bitcast i32* %200 to <4 x i32>*
  store <4 x i32> %660, <4 x i32>* %661, align 16
  %662 = load i32, i32* %105, align 4
  store i32 %662, i32* %207, align 16
  %663 = load i32, i32* %109, align 4
  %664 = load i32, i32* %161, align 4
  %665 = mul nsw i32 %663, %639
  %666 = sext i32 %665 to i64
  %667 = mul nsw i32 %664, %603
  %668 = sext i32 %667 to i64
  %669 = add i64 %225, %666
  %670 = add i64 %669, %668
  %671 = ashr i64 %670, %228
  %672 = trunc i64 %671 to i32
  store i32 %672, i32* %208, align 4
  %673 = load i32, i32* %113, align 4
  %674 = load i32, i32* %157, align 4
  %675 = mul i32 %603, %673
  %676 = sub i32 0, %675
  %677 = sext i32 %676 to i64
  %678 = mul nsw i32 %674, %639
  %679 = sext i32 %678 to i64
  %680 = add i64 %225, %677
  %681 = add i64 %680, %679
  %682 = ashr i64 %681, %228
  %683 = trunc i64 %682 to i32
  store i32 %683, i32* %209, align 8
  %684 = load i32, i32* %117, align 4
  store i32 %684, i32* %212, align 4
  %685 = load i32, i32* %121, align 4
  store i32 %685, i32* %231, align 16
  %686 = load i32, i32* %125, align 4
  %687 = load i32, i32* %145, align 4
  %688 = mul nsw i32 %686, %630
  %689 = sext i32 %688 to i64
  %690 = mul nsw i32 %687, %617
  %691 = sext i32 %690 to i64
  %692 = add i64 %225, %689
  %693 = add i64 %692, %691
  %694 = ashr i64 %693, %228
  %695 = trunc i64 %694 to i32
  store i32 %695, i32* %242, align 4
  %696 = load i32, i32* %129, align 4
  %697 = load i32, i32* %141, align 4
  %698 = mul i32 %617, %696
  %699 = sub i32 0, %698
  %700 = sext i32 %699 to i64
  %701 = mul nsw i32 %697, %630
  %702 = sext i32 %701 to i64
  %703 = add i64 %225, %700
  %704 = add i64 %703, %702
  %705 = ashr i64 %704, %228
  %706 = trunc i64 %705 to i32
  store i32 %706, i32* %253, align 8
  %707 = load i32, i32* %133, align 4
  store i32 %707, i32* %264, align 4
  %708 = load i32, i32* %137, align 4
  store i32 %708, i32* %270, align 16
  %709 = mul nsw i32 %697, %617
  %710 = sext i32 %709 to i64
  %711 = mul nsw i32 %696, %630
  %712 = sext i32 %711 to i64
  %713 = add i64 %225, %712
  %714 = add i64 %713, %710
  %715 = ashr i64 %714, %228
  %716 = trunc i64 %715 to i32
  store i32 %716, i32* %276, align 4
  %717 = mul nsw i32 %687, %620
  %718 = sext i32 %717 to i64
  %719 = mul nsw i32 %686, %617
  %720 = sext i32 %719 to i64
  %721 = add i64 %225, %720
  %722 = add i64 %721, %718
  %723 = ashr i64 %722, %228
  %724 = trunc i64 %723 to i32
  store i32 %724, i32* %282, align 8
  %725 = load i32, i32* %149, align 4
  store i32 %725, i32* %288, align 4
  %726 = load i32, i32* %153, align 4
  store i32 %726, i32* %289, align 16
  %727 = mul nsw i32 %674, %603
  %728 = sext i32 %727 to i64
  %729 = mul nsw i32 %673, %639
  %730 = sext i32 %729 to i64
  %731 = add i64 %225, %730
  %732 = add i64 %731, %728
  %733 = ashr i64 %732, %228
  %734 = trunc i64 %733 to i32
  store i32 %734, i32* %290, align 4
  %735 = mul nsw i32 %664, %606
  %736 = sext i32 %735 to i64
  %737 = mul nsw i32 %663, %603
  %738 = sext i32 %737 to i64
  %739 = add i64 %225, %738
  %740 = add i64 %739, %736
  %741 = ashr i64 %740, %228
  %742 = trunc i64 %741 to i32
  store i32 %742, i32* %291, align 8
  %743 = load i32, i32* %165, align 4
  store i32 %743, i32* %294, align 4
  %744 = getelementptr inbounds i8, i8* %3, i64 6
  %745 = load i8, i8* %744, align 1
  call void @av1_range_check_buf(i32 6, i32* %0, i32* nonnull %171, i32 32, i8 signext %745) #3
  %746 = bitcast [32 x i32]* %5 to <4 x i32>*
  %747 = load <4 x i32>, <4 x i32>* %746, align 16
  %748 = bitcast i32* %1 to <4 x i32>*
  store <4 x i32> %747, <4 x i32>* %748, align 4
  %749 = bitcast i32* %182 to <4 x i32>*
  %750 = load <4 x i32>, <4 x i32>* %749, align 16
  %751 = bitcast i32* %35 to <4 x i32>*
  store <4 x i32> %750, <4 x i32>* %751, align 4
  %752 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %170, i64 60
  %753 = load i32, i32* %752, align 16
  %754 = load i32, i32* %193, align 16
  %755 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %170, i64 4
  %756 = load i32, i32* %755, align 16
  %757 = load i32, i32* %205, align 4
  %758 = mul nsw i32 %754, %753
  %759 = sext i32 %758 to i64
  %760 = mul nsw i32 %757, %756
  %761 = sext i32 %760 to i64
  %762 = add i64 %225, %759
  %763 = add i64 %762, %761
  %764 = ashr i64 %763, %228
  %765 = trunc i64 %764 to i32
  store i32 %765, i32* %59, align 4
  %766 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %170, i64 28
  %767 = load i32, i32* %766, align 16
  %768 = load i32, i32* %194, align 4
  %769 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %170, i64 36
  %770 = load i32, i32* %769, align 16
  %771 = load i32, i32* %202, align 8
  %772 = mul nsw i32 %768, %767
  %773 = sext i32 %772 to i64
  %774 = mul nsw i32 %771, %770
  %775 = sext i32 %774 to i64
  %776 = add i64 %225, %773
  %777 = add i64 %776, %775
  %778 = ashr i64 %777, %228
  %779 = trunc i64 %778 to i32
  store i32 %779, i32* %65, align 4
  %780 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %170, i64 44
  %781 = load i32, i32* %780, align 16
  %782 = load i32, i32* %195, align 8
  %783 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %170, i64 20
  %784 = load i32, i32* %783, align 16
  %785 = load i32, i32* %201, align 4
  %786 = mul nsw i32 %782, %781
  %787 = sext i32 %786 to i64
  %788 = mul nsw i32 %785, %784
  %789 = sext i32 %788 to i64
  %790 = add i64 %225, %787
  %791 = add i64 %790, %789
  %792 = ashr i64 %791, %228
  %793 = trunc i64 %792 to i32
  store i32 %793, i32* %71, align 4
  %794 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %170, i64 12
  %795 = load i32, i32* %794, align 16
  %796 = load i32, i32* %198, align 4
  %797 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %170, i64 52
  %798 = load i32, i32* %797, align 16
  %799 = load i32, i32* %200, align 16
  %800 = mul nsw i32 %796, %795
  %801 = sext i32 %800 to i64
  %802 = mul nsw i32 %799, %798
  %803 = sext i32 %802 to i64
  %804 = add i64 %225, %801
  %805 = add i64 %804, %803
  %806 = ashr i64 %805, %228
  %807 = trunc i64 %806 to i32
  store i32 %807, i32* %77, align 4
  %808 = mul nsw i32 %799, %795
  %809 = sext i32 %808 to i64
  %810 = mul i32 %796, %798
  %811 = sub i32 0, %810
  %812 = sext i32 %811 to i64
  %813 = add i64 %225, %812
  %814 = add i64 %813, %809
  %815 = ashr i64 %814, %228
  %816 = trunc i64 %815 to i32
  store i32 %816, i32* %83, align 4
  %817 = mul nsw i32 %785, %781
  %818 = sext i32 %817 to i64
  %819 = mul i32 %782, %784
  %820 = sub i32 0, %819
  %821 = sext i32 %820 to i64
  %822 = add i64 %225, %821
  %823 = add i64 %822, %818
  %824 = ashr i64 %823, %228
  %825 = trunc i64 %824 to i32
  store i32 %825, i32* %89, align 4
  %826 = mul nsw i32 %771, %767
  %827 = sext i32 %826 to i64
  %828 = mul i32 %768, %770
  %829 = sub i32 0, %828
  %830 = sext i32 %829 to i64
  %831 = add i64 %225, %830
  %832 = add i64 %831, %827
  %833 = ashr i64 %832, %228
  %834 = trunc i64 %833 to i32
  store i32 %834, i32* %95, align 4
  %835 = mul nsw i32 %757, %753
  %836 = sext i32 %835 to i64
  %837 = mul i32 %754, %756
  %838 = sub i32 0, %837
  %839 = sext i32 %838 to i64
  %840 = add i64 %225, %839
  %841 = add i64 %840, %836
  %842 = ashr i64 %841, %228
  %843 = trunc i64 %842 to i32
  store i32 %843, i32* %101, align 4
  %844 = bitcast i32* %207 to <4 x i32>*
  %845 = load <4 x i32>, <4 x i32>* %844, align 16
  %846 = shufflevector <4 x i32> %845, <4 x i32> undef, <4 x i32> <i32 1, i32 0, i32 3, i32 2>
  %847 = add nsw <4 x i32> %846, %845
  %848 = sub <4 x i32> %846, %845
  %849 = shufflevector <4 x i32> %847, <4 x i32> %848, <4 x i32> <i32 0, i32 5, i32 6, i32 3>
  %850 = bitcast i32* %105 to <4 x i32>*
  store <4 x i32> %849, <4 x i32>* %850, align 4
  %851 = bitcast i32* %231 to <4 x i32>*
  %852 = load <4 x i32>, <4 x i32>* %851, align 16
  %853 = shufflevector <4 x i32> %852, <4 x i32> undef, <4 x i32> <i32 1, i32 0, i32 3, i32 2>
  %854 = add nsw <4 x i32> %853, %852
  %855 = sub <4 x i32> %853, %852
  %856 = shufflevector <4 x i32> %854, <4 x i32> %855, <4 x i32> <i32 0, i32 5, i32 6, i32 3>
  %857 = bitcast i32* %121 to <4 x i32>*
  store <4 x i32> %856, <4 x i32>* %857, align 4
  %858 = bitcast i32* %270 to <4 x i32>*
  %859 = load <4 x i32>, <4 x i32>* %858, align 16
  %860 = shufflevector <4 x i32> %859, <4 x i32> undef, <4 x i32> <i32 1, i32 0, i32 3, i32 2>
  %861 = add nsw <4 x i32> %860, %859
  %862 = sub <4 x i32> %860, %859
  %863 = shufflevector <4 x i32> %861, <4 x i32> %862, <4 x i32> <i32 0, i32 5, i32 6, i32 3>
  %864 = bitcast i32* %137 to <4 x i32>*
  store <4 x i32> %863, <4 x i32>* %864, align 4
  %865 = bitcast i32* %289 to <4 x i32>*
  %866 = load <4 x i32>, <4 x i32>* %865, align 16
  %867 = shufflevector <4 x i32> %866, <4 x i32> undef, <4 x i32> <i32 1, i32 0, i32 3, i32 2>
  %868 = add nsw <4 x i32> %867, %866
  %869 = sub <4 x i32> %867, %866
  %870 = shufflevector <4 x i32> %868, <4 x i32> %869, <4 x i32> <i32 0, i32 5, i32 6, i32 3>
  %871 = bitcast i32* %153 to <4 x i32>*
  store <4 x i32> %870, <4 x i32>* %871, align 4
  %872 = getelementptr inbounds i8, i8* %3, i64 7
  %873 = load i8, i8* %872, align 1
  call void @av1_range_check_buf(i32 7, i32* %0, i32* %1, i32 32, i8 signext %873) #3
  %874 = bitcast i32* %1 to <4 x i32>*
  %875 = load <4 x i32>, <4 x i32>* %874, align 4
  %876 = bitcast [32 x i32]* %5 to <4 x i32>*
  store <4 x i32> %875, <4 x i32>* %876, align 16
  %877 = bitcast i32* %35 to <4 x i32>*
  %878 = load <4 x i32>, <4 x i32>* %877, align 4
  %879 = bitcast i32* %182 to <4 x i32>*
  store <4 x i32> %878, <4 x i32>* %879, align 16
  %880 = bitcast i32* %59 to <4 x i32>*
  %881 = load <4 x i32>, <4 x i32>* %880, align 4
  %882 = bitcast i32* %193 to <4 x i32>*
  store <4 x i32> %881, <4 x i32>* %882, align 16
  %883 = bitcast i32* %83 to <4 x i32>*
  %884 = load <4 x i32>, <4 x i32>* %883, align 4
  %885 = bitcast i32* %200 to <4 x i32>*
  store <4 x i32> %884, <4 x i32>* %885, align 16
  %886 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %170, i64 62
  %887 = load i32, i32* %886, align 8
  %888 = load i32, i32* %105, align 4
  %889 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %170, i64 2
  %890 = load i32, i32* %889, align 8
  %891 = load i32, i32* %165, align 4
  %892 = mul nsw i32 %888, %887
  %893 = sext i32 %892 to i64
  %894 = mul nsw i32 %891, %890
  %895 = sext i32 %894 to i64
  %896 = add i64 %225, %893
  %897 = add i64 %896, %895
  %898 = ashr i64 %897, %228
  %899 = trunc i64 %898 to i32
  store i32 %899, i32* %207, align 16
  %900 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %170, i64 30
  %901 = load i32, i32* %900, align 8
  %902 = load i32, i32* %109, align 4
  %903 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %170, i64 34
  %904 = load i32, i32* %903, align 8
  %905 = load i32, i32* %161, align 4
  %906 = mul nsw i32 %902, %901
  %907 = sext i32 %906 to i64
  %908 = mul nsw i32 %905, %904
  %909 = sext i32 %908 to i64
  %910 = add i64 %225, %907
  %911 = add i64 %910, %909
  %912 = ashr i64 %911, %228
  %913 = trunc i64 %912 to i32
  store i32 %913, i32* %208, align 4
  %914 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %170, i64 46
  %915 = load i32, i32* %914, align 8
  %916 = load i32, i32* %113, align 4
  %917 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %170, i64 18
  %918 = load i32, i32* %917, align 8
  %919 = load i32, i32* %157, align 4
  %920 = mul nsw i32 %916, %915
  %921 = sext i32 %920 to i64
  %922 = mul nsw i32 %919, %918
  %923 = sext i32 %922 to i64
  %924 = add i64 %225, %921
  %925 = add i64 %924, %923
  %926 = ashr i64 %925, %228
  %927 = trunc i64 %926 to i32
  store i32 %927, i32* %209, align 8
  %928 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %170, i64 14
  %929 = load i32, i32* %928, align 8
  %930 = load i32, i32* %117, align 4
  %931 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %170, i64 50
  %932 = load i32, i32* %931, align 8
  %933 = load i32, i32* %153, align 4
  %934 = mul nsw i32 %930, %929
  %935 = sext i32 %934 to i64
  %936 = mul nsw i32 %933, %932
  %937 = sext i32 %936 to i64
  %938 = add i64 %225, %935
  %939 = add i64 %938, %937
  %940 = ashr i64 %939, %228
  %941 = trunc i64 %940 to i32
  store i32 %941, i32* %212, align 4
  %942 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %170, i64 54
  %943 = load i32, i32* %942, align 8
  %944 = load i32, i32* %121, align 4
  %945 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %170, i64 10
  %946 = load i32, i32* %945, align 8
  %947 = load i32, i32* %149, align 4
  %948 = mul nsw i32 %944, %943
  %949 = sext i32 %948 to i64
  %950 = mul nsw i32 %947, %946
  %951 = sext i32 %950 to i64
  %952 = add i64 %225, %949
  %953 = add i64 %952, %951
  %954 = ashr i64 %953, %228
  %955 = trunc i64 %954 to i32
  store i32 %955, i32* %231, align 16
  %956 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %170, i64 22
  %957 = load i32, i32* %956, align 8
  %958 = load i32, i32* %125, align 4
  %959 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %170, i64 42
  %960 = load i32, i32* %959, align 8
  %961 = load i32, i32* %145, align 4
  %962 = mul nsw i32 %958, %957
  %963 = sext i32 %962 to i64
  %964 = mul nsw i32 %961, %960
  %965 = sext i32 %964 to i64
  %966 = add i64 %225, %963
  %967 = add i64 %966, %965
  %968 = ashr i64 %967, %228
  %969 = trunc i64 %968 to i32
  store i32 %969, i32* %242, align 4
  %970 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %170, i64 38
  %971 = load i32, i32* %970, align 8
  %972 = load i32, i32* %129, align 4
  %973 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %170, i64 26
  %974 = load i32, i32* %973, align 8
  %975 = load i32, i32* %141, align 4
  %976 = mul nsw i32 %972, %971
  %977 = sext i32 %976 to i64
  %978 = mul nsw i32 %975, %974
  %979 = sext i32 %978 to i64
  %980 = add i64 %225, %977
  %981 = add i64 %980, %979
  %982 = ashr i64 %981, %228
  %983 = trunc i64 %982 to i32
  store i32 %983, i32* %253, align 8
  %984 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %170, i64 6
  %985 = load i32, i32* %984, align 8
  %986 = load i32, i32* %133, align 4
  %987 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %170, i64 58
  %988 = load i32, i32* %987, align 8
  %989 = load i32, i32* %137, align 4
  %990 = mul nsw i32 %986, %985
  %991 = sext i32 %990 to i64
  %992 = mul nsw i32 %989, %988
  %993 = sext i32 %992 to i64
  %994 = add i64 %225, %991
  %995 = add i64 %994, %993
  %996 = ashr i64 %995, %228
  %997 = trunc i64 %996 to i32
  store i32 %997, i32* %264, align 4
  %998 = mul nsw i32 %989, %985
  %999 = sext i32 %998 to i64
  %1000 = mul i32 %986, %988
  %1001 = sub i32 0, %1000
  %1002 = sext i32 %1001 to i64
  %1003 = add i64 %225, %1002
  %1004 = add i64 %1003, %999
  %1005 = ashr i64 %1004, %228
  %1006 = trunc i64 %1005 to i32
  store i32 %1006, i32* %270, align 16
  %1007 = mul nsw i32 %975, %971
  %1008 = sext i32 %1007 to i64
  %1009 = mul i32 %972, %974
  %1010 = sub i32 0, %1009
  %1011 = sext i32 %1010 to i64
  %1012 = add i64 %225, %1011
  %1013 = add i64 %1012, %1008
  %1014 = ashr i64 %1013, %228
  %1015 = trunc i64 %1014 to i32
  store i32 %1015, i32* %276, align 4
  %1016 = mul nsw i32 %961, %957
  %1017 = sext i32 %1016 to i64
  %1018 = mul i32 %958, %960
  %1019 = sub i32 0, %1018
  %1020 = sext i32 %1019 to i64
  %1021 = add i64 %225, %1020
  %1022 = add i64 %1021, %1017
  %1023 = ashr i64 %1022, %228
  %1024 = trunc i64 %1023 to i32
  store i32 %1024, i32* %282, align 8
  %1025 = mul nsw i32 %947, %943
  %1026 = sext i32 %1025 to i64
  %1027 = mul i32 %944, %946
  %1028 = sub i32 0, %1027
  %1029 = sext i32 %1028 to i64
  %1030 = add i64 %225, %1029
  %1031 = add i64 %1030, %1026
  %1032 = ashr i64 %1031, %228
  %1033 = trunc i64 %1032 to i32
  store i32 %1033, i32* %288, align 4
  %1034 = mul nsw i32 %933, %929
  %1035 = sext i32 %1034 to i64
  %1036 = mul i32 %930, %932
  %1037 = sub i32 0, %1036
  %1038 = sext i32 %1037 to i64
  %1039 = add i64 %225, %1038
  %1040 = add i64 %1039, %1035
  %1041 = ashr i64 %1040, %228
  %1042 = trunc i64 %1041 to i32
  store i32 %1042, i32* %289, align 16
  %1043 = mul nsw i32 %919, %915
  %1044 = sext i32 %1043 to i64
  %1045 = mul i32 %916, %918
  %1046 = sub i32 0, %1045
  %1047 = sext i32 %1046 to i64
  %1048 = add i64 %225, %1047
  %1049 = add i64 %1048, %1044
  %1050 = ashr i64 %1049, %228
  %1051 = trunc i64 %1050 to i32
  store i32 %1051, i32* %290, align 4
  %1052 = mul nsw i32 %905, %901
  %1053 = sext i32 %1052 to i64
  %1054 = mul i32 %902, %904
  %1055 = sub i32 0, %1054
  %1056 = sext i32 %1055 to i64
  %1057 = add i64 %225, %1056
  %1058 = add i64 %1057, %1053
  %1059 = ashr i64 %1058, %228
  %1060 = trunc i64 %1059 to i32
  store i32 %1060, i32* %291, align 8
  %1061 = mul nsw i32 %891, %887
  %1062 = sext i32 %1061 to i64
  %1063 = mul i32 %888, %890
  %1064 = sub i32 0, %1063
  %1065 = sext i32 %1064 to i64
  %1066 = add i64 %225, %1065
  %1067 = add i64 %1066, %1062
  %1068 = ashr i64 %1067, %228
  %1069 = trunc i64 %1068 to i32
  store i32 %1069, i32* %294, align 4
  %1070 = getelementptr inbounds i8, i8* %3, i64 8
  %1071 = load i8, i8* %1070, align 1
  call void @av1_range_check_buf(i32 8, i32* %0, i32* nonnull %171, i32 32, i8 signext %1071) #3
  %1072 = load i32, i32* %171, align 16
  store i32 %1072, i32* %1, align 4
  %1073 = load i32, i32* %207, align 16
  store i32 %1073, i32* %17, align 4
  %1074 = load i32, i32* %193, align 16
  store i32 %1074, i32* %23, align 4
  %1075 = load i32, i32* %270, align 16
  store i32 %1075, i32* %29, align 4
  %1076 = load i32, i32* %182, align 16
  store i32 %1076, i32* %35, align 4
  %1077 = load i32, i32* %231, align 16
  store i32 %1077, i32* %41, align 4
  %1078 = load i32, i32* %200, align 16
  store i32 %1078, i32* %47, align 4
  %1079 = load i32, i32* %289, align 16
  store i32 %1079, i32* %53, align 4
  %1080 = load i32, i32* %173, align 8
  store i32 %1080, i32* %59, align 4
  %1081 = load i32, i32* %209, align 8
  store i32 %1081, i32* %65, align 4
  %1082 = load i32, i32* %195, align 8
  store i32 %1082, i32* %71, align 4
  %1083 = load i32, i32* %282, align 8
  store i32 %1083, i32* %77, align 4
  %1084 = load i32, i32* %184, align 8
  store i32 %1084, i32* %83, align 4
  %1085 = load i32, i32* %253, align 8
  store i32 %1085, i32* %89, align 4
  %1086 = load i32, i32* %202, align 8
  store i32 %1086, i32* %95, align 4
  %1087 = load i32, i32* %291, align 8
  store i32 %1087, i32* %101, align 4
  %1088 = load i32, i32* %172, align 4
  store i32 %1088, i32* %105, align 4
  %1089 = load i32, i32* %208, align 4
  store i32 %1089, i32* %109, align 4
  %1090 = load i32, i32* %194, align 4
  store i32 %1090, i32* %113, align 4
  %1091 = load i32, i32* %276, align 4
  store i32 %1091, i32* %117, align 4
  %1092 = load i32, i32* %183, align 4
  store i32 %1092, i32* %121, align 4
  %1093 = load i32, i32* %242, align 4
  store i32 %1093, i32* %125, align 4
  %1094 = load i32, i32* %201, align 4
  store i32 %1094, i32* %129, align 4
  %1095 = load i32, i32* %290, align 4
  store i32 %1095, i32* %133, align 4
  %1096 = load i32, i32* %180, align 4
  store i32 %1096, i32* %137, align 4
  %1097 = load i32, i32* %212, align 4
  store i32 %1097, i32* %141, align 4
  %1098 = load i32, i32* %198, align 4
  store i32 %1098, i32* %145, align 4
  %1099 = load i32, i32* %288, align 4
  store i32 %1099, i32* %149, align 4
  %1100 = load i32, i32* %191, align 4
  store i32 %1100, i32* %153, align 4
  %1101 = load i32, i32* %264, align 4
  store i32 %1101, i32* %157, align 4
  %1102 = load i32, i32* %205, align 4
  store i32 %1102, i32* %161, align 4
  %1103 = load i32, i32* %294, align 4
  store i32 %1103, i32* %165, align 4
  %1104 = getelementptr inbounds i8, i8* %3, i64 9
  %1105 = load i8, i8* %1104, align 1
  call void @av1_range_check_buf(i32 9, i32* %0, i32* %1, i32 32, i8 signext %1105) #3
  call void @llvm.lifetime.end.p0i8(i64 128, i8* nonnull %6) #3
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @av1_fadst4(i32*, i32*, i8 signext, i8* nocapture readonly) local_unnamed_addr #0 {
  %5 = sext i8 %2 to i32
  %6 = add nsw i32 %5, -10
  %7 = sext i32 %6 to i64
  %8 = load i8, i8* %3, align 1
  tail call void @av1_range_check_buf(i32 0, i32* %0, i32* %0, i32 4, i8 signext %8) #3
  %9 = load i32, i32* %0, align 4
  %10 = getelementptr inbounds i32, i32* %0, i64 1
  %11 = load i32, i32* %10, align 4
  %12 = getelementptr inbounds i32, i32* %0, i64 2
  %13 = load i32, i32* %12, align 4
  %14 = getelementptr inbounds i32, i32* %0, i64 3
  %15 = load i32, i32* %14, align 4
  %16 = or i32 %11, %9
  %17 = or i32 %16, %13
  %18 = or i32 %17, %15
  %19 = icmp eq i32 %18, 0
  br i1 %19, label %20, label %22

20:                                               ; preds = %4
  %21 = bitcast i32* %1 to i8*
  call void @llvm.memset.p0i8.i64(i8* align 4 %21, i8 0, i64 16, i1 false)
  br label %74

22:                                               ; preds = %4
  %23 = getelementptr inbounds [7 x [5 x i32]], [7 x [5 x i32]]* @av1_sinpi_arr_data, i64 0, i64 %7, i64 1
  %24 = load i32, i32* %23, align 4
  %25 = mul nsw i32 %24, %9
  %26 = getelementptr inbounds [7 x [5 x i32]], [7 x [5 x i32]]* @av1_sinpi_arr_data, i64 0, i64 %7, i64 4
  %27 = load i32, i32* %26, align 4
  %28 = mul nsw i32 %27, %9
  %29 = getelementptr inbounds [7 x [5 x i32]], [7 x [5 x i32]]* @av1_sinpi_arr_data, i64 0, i64 %7, i64 2
  %30 = load i32, i32* %29, align 4
  %31 = mul nsw i32 %30, %11
  %32 = mul nsw i32 %24, %11
  %33 = getelementptr inbounds [7 x [5 x i32]], [7 x [5 x i32]]* @av1_sinpi_arr_data, i64 0, i64 %7, i64 3
  %34 = load i32, i32* %33, align 4
  %35 = mul nsw i32 %34, %13
  %36 = mul nsw i32 %27, %15
  %37 = mul nsw i32 %30, %15
  %38 = add nsw i32 %11, %9
  %39 = sub i32 %38, %15
  %40 = mul nsw i32 %34, %39
  %41 = sub nsw i32 %28, %32
  %42 = add i32 %36, %25
  %43 = add i32 %42, %31
  %44 = add nsw i32 %41, %37
  %45 = add nsw i32 %43, %35
  %46 = sub nsw i32 %44, %35
  %47 = add i32 %35, %44
  %48 = sub i32 %47, %43
  %49 = getelementptr inbounds i8, i8* %3, i64 6
  %50 = sext i32 %45 to i64
  %51 = add nsw i32 %5, -1
  %52 = zext i32 %51 to i64
  %53 = shl i64 1, %52
  %54 = add nsw i64 %53, %50
  %55 = zext i32 %5 to i64
  %56 = ashr i64 %54, %55
  %57 = trunc i64 %56 to i32
  store i32 %57, i32* %1, align 4
  %58 = sext i32 %40 to i64
  %59 = add nsw i64 %53, %58
  %60 = ashr i64 %59, %55
  %61 = trunc i64 %60 to i32
  %62 = getelementptr inbounds i32, i32* %1, i64 1
  store i32 %61, i32* %62, align 4
  %63 = sext i32 %46 to i64
  %64 = add nsw i64 %53, %63
  %65 = ashr i64 %64, %55
  %66 = trunc i64 %65 to i32
  %67 = getelementptr inbounds i32, i32* %1, i64 2
  store i32 %66, i32* %67, align 4
  %68 = sext i32 %48 to i64
  %69 = add nsw i64 %53, %68
  %70 = ashr i64 %69, %55
  %71 = trunc i64 %70 to i32
  %72 = getelementptr inbounds i32, i32* %1, i64 3
  store i32 %71, i32* %72, align 4
  %73 = load i8, i8* %49, align 1
  tail call void @av1_range_check_buf(i32 6, i32* %0, i32* %1, i32 4, i8 signext %73) #3
  br label %74

74:                                               ; preds = %22, %20
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @av1_fadst8(i32*, i32*, i8 signext, i8* nocapture readonly) local_unnamed_addr #0 {
  %5 = alloca [8 x i32], align 16
  %6 = bitcast [8 x i32]* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %6) #3
  %7 = getelementptr inbounds [8 x i32], [8 x i32]* %5, i64 0, i64 0
  %8 = getelementptr inbounds [8 x i32], [8 x i32]* %5, i64 0, i64 1
  %9 = getelementptr inbounds [8 x i32], [8 x i32]* %5, i64 0, i64 2
  %10 = getelementptr inbounds [8 x i32], [8 x i32]* %5, i64 0, i64 3
  %11 = getelementptr inbounds [8 x i32], [8 x i32]* %5, i64 0, i64 4
  %12 = getelementptr inbounds [8 x i32], [8 x i32]* %5, i64 0, i64 5
  %13 = getelementptr inbounds [8 x i32], [8 x i32]* %5, i64 0, i64 6
  %14 = getelementptr inbounds [8 x i32], [8 x i32]* %5, i64 0, i64 7
  %15 = load i8, i8* %3, align 1
  tail call void @av1_range_check_buf(i32 0, i32* %0, i32* %0, i32 8, i8 signext %15) #3
  %16 = load i32, i32* %0, align 4
  store i32 %16, i32* %1, align 4
  %17 = getelementptr inbounds i32, i32* %0, i64 7
  %18 = load i32, i32* %17, align 4
  %19 = sub nsw i32 0, %18
  %20 = getelementptr inbounds i32, i32* %1, i64 1
  store i32 %19, i32* %20, align 4
  %21 = getelementptr inbounds i32, i32* %0, i64 3
  %22 = load i32, i32* %21, align 4
  %23 = sub nsw i32 0, %22
  %24 = getelementptr inbounds i32, i32* %1, i64 2
  store i32 %23, i32* %24, align 4
  %25 = getelementptr inbounds i32, i32* %0, i64 4
  %26 = load i32, i32* %25, align 4
  %27 = getelementptr inbounds i32, i32* %1, i64 3
  store i32 %26, i32* %27, align 4
  %28 = getelementptr inbounds i32, i32* %0, i64 1
  %29 = load i32, i32* %28, align 4
  %30 = sub nsw i32 0, %29
  %31 = getelementptr inbounds i32, i32* %1, i64 4
  store i32 %30, i32* %31, align 4
  %32 = getelementptr inbounds i32, i32* %0, i64 6
  %33 = load i32, i32* %32, align 4
  %34 = getelementptr inbounds i32, i32* %1, i64 5
  store i32 %33, i32* %34, align 4
  %35 = getelementptr inbounds i32, i32* %0, i64 2
  %36 = load i32, i32* %35, align 4
  %37 = getelementptr inbounds i32, i32* %1, i64 6
  store i32 %36, i32* %37, align 4
  %38 = getelementptr inbounds i32, i32* %0, i64 5
  %39 = load i32, i32* %38, align 4
  %40 = sub nsw i32 0, %39
  %41 = getelementptr inbounds i32, i32* %1, i64 7
  store i32 %40, i32* %41, align 4
  %42 = getelementptr inbounds i8, i8* %3, i64 1
  %43 = load i8, i8* %42, align 1
  tail call void @av1_range_check_buf(i32 1, i32* %0, i32* %1, i32 8, i8 signext %43) #3
  %44 = sext i8 %2 to i32
  %45 = add nsw i32 %44, -10
  %46 = sext i32 %45 to i64
  %47 = load i32, i32* %1, align 4
  store i32 %47, i32* %7, align 16
  %48 = load i32, i32* %20, align 4
  store i32 %48, i32* %8, align 4
  %49 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %46, i64 32
  %50 = load i32, i32* %49, align 16
  %51 = load i32, i32* %24, align 4
  %52 = load i32, i32* %27, align 4
  %53 = mul nsw i32 %51, %50
  %54 = sext i32 %53 to i64
  %55 = mul nsw i32 %52, %50
  %56 = sext i32 %55 to i64
  %57 = add nsw i32 %44, -1
  %58 = zext i32 %57 to i64
  %59 = shl i64 1, %58
  %60 = add i64 %59, %54
  %61 = add i64 %60, %56
  %62 = zext i32 %44 to i64
  %63 = ashr i64 %61, %62
  %64 = trunc i64 %63 to i32
  store i32 %64, i32* %9, align 8
  %65 = sub nsw i32 0, %50
  %66 = mul nsw i32 %52, %65
  %67 = sext i32 %66 to i64
  %68 = add i64 %60, %67
  %69 = ashr i64 %68, %62
  %70 = trunc i64 %69 to i32
  store i32 %70, i32* %10, align 4
  %71 = load i32, i32* %31, align 4
  store i32 %71, i32* %11, align 16
  %72 = load i32, i32* %34, align 4
  store i32 %72, i32* %12, align 4
  %73 = load i32, i32* %37, align 4
  %74 = load i32, i32* %41, align 4
  %75 = mul nsw i32 %73, %50
  %76 = sext i32 %75 to i64
  %77 = mul nsw i32 %74, %50
  %78 = sext i32 %77 to i64
  %79 = add i64 %59, %76
  %80 = add i64 %79, %78
  %81 = ashr i64 %80, %62
  %82 = trunc i64 %81 to i32
  store i32 %82, i32* %13, align 8
  %83 = mul nsw i32 %74, %65
  %84 = sext i32 %83 to i64
  %85 = add i64 %79, %84
  %86 = ashr i64 %85, %62
  %87 = trunc i64 %86 to i32
  store i32 %87, i32* %14, align 4
  %88 = getelementptr inbounds i8, i8* %3, i64 2
  %89 = load i8, i8* %88, align 1
  call void @av1_range_check_buf(i32 2, i32* %0, i32* nonnull %7, i32 8, i8 signext %89) #3
  %90 = bitcast [8 x i32]* %5 to <4 x i32>*
  %91 = load <4 x i32>, <4 x i32>* %90, align 16
  %92 = shufflevector <4 x i32> %91, <4 x i32> undef, <4 x i32> <i32 2, i32 3, i32 0, i32 1>
  %93 = add nsw <4 x i32> %92, %91
  %94 = sub nsw <4 x i32> %92, %91
  %95 = shufflevector <4 x i32> %93, <4 x i32> %94, <4 x i32> <i32 0, i32 1, i32 6, i32 7>
  %96 = bitcast i32* %1 to <4 x i32>*
  store <4 x i32> %95, <4 x i32>* %96, align 4
  %97 = bitcast i32* %11 to <4 x i32>*
  %98 = load <4 x i32>, <4 x i32>* %97, align 16
  %99 = shufflevector <4 x i32> %98, <4 x i32> undef, <4 x i32> <i32 2, i32 3, i32 0, i32 1>
  %100 = add nsw <4 x i32> %99, %98
  %101 = sub nsw <4 x i32> %99, %98
  %102 = shufflevector <4 x i32> %100, <4 x i32> %101, <4 x i32> <i32 0, i32 1, i32 6, i32 7>
  %103 = bitcast i32* %31 to <4 x i32>*
  store <4 x i32> %102, <4 x i32>* %103, align 4
  %104 = getelementptr inbounds i8, i8* %3, i64 3
  %105 = load i8, i8* %104, align 1
  call void @av1_range_check_buf(i32 3, i32* %0, i32* %1, i32 8, i8 signext %105) #3
  %106 = bitcast i32* %1 to <4 x i32>*
  %107 = load <4 x i32>, <4 x i32>* %106, align 4
  %108 = bitcast [8 x i32]* %5 to <4 x i32>*
  store <4 x i32> %107, <4 x i32>* %108, align 16
  %109 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %46, i64 16
  %110 = load i32, i32* %109, align 16
  %111 = load i32, i32* %31, align 4
  %112 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %46, i64 48
  %113 = load i32, i32* %112, align 16
  %114 = load i32, i32* %34, align 4
  %115 = mul nsw i32 %111, %110
  %116 = sext i32 %115 to i64
  %117 = mul nsw i32 %114, %113
  %118 = sext i32 %117 to i64
  %119 = add i64 %59, %116
  %120 = add i64 %119, %118
  %121 = ashr i64 %120, %62
  %122 = trunc i64 %121 to i32
  store i32 %122, i32* %11, align 16
  %123 = mul nsw i32 %113, %111
  %124 = sext i32 %123 to i64
  %125 = mul i32 %110, %114
  %126 = sub i32 0, %125
  %127 = sext i32 %126 to i64
  %128 = add i64 %59, %124
  %129 = add i64 %128, %127
  %130 = ashr i64 %129, %62
  %131 = trunc i64 %130 to i32
  store i32 %131, i32* %12, align 4
  %132 = load i32, i32* %37, align 4
  %133 = load i32, i32* %41, align 4
  %134 = mul i32 %113, %132
  %135 = sub i32 0, %134
  %136 = sext i32 %135 to i64
  %137 = mul nsw i32 %133, %110
  %138 = sext i32 %137 to i64
  %139 = add i64 %59, %136
  %140 = add i64 %139, %138
  %141 = ashr i64 %140, %62
  %142 = trunc i64 %141 to i32
  store i32 %142, i32* %13, align 8
  %143 = mul nsw i32 %132, %110
  %144 = sext i32 %143 to i64
  %145 = mul nsw i32 %133, %113
  %146 = sext i32 %145 to i64
  %147 = add i64 %59, %144
  %148 = add i64 %147, %146
  %149 = ashr i64 %148, %62
  %150 = trunc i64 %149 to i32
  store i32 %150, i32* %14, align 4
  %151 = getelementptr inbounds i8, i8* %3, i64 4
  %152 = load i8, i8* %151, align 1
  call void @av1_range_check_buf(i32 4, i32* %0, i32* nonnull %7, i32 8, i8 signext %152) #3
  %153 = bitcast [8 x i32]* %5 to <4 x i32>*
  %154 = load <4 x i32>, <4 x i32>* %153, align 16
  %155 = bitcast i32* %11 to <4 x i32>*
  %156 = load <4 x i32>, <4 x i32>* %155, align 16
  %157 = add nsw <4 x i32> %156, %154
  %158 = bitcast i32* %1 to <4 x i32>*
  store <4 x i32> %157, <4 x i32>* %158, align 4
  %159 = sub nsw <4 x i32> %154, %156
  %160 = bitcast i32* %31 to <4 x i32>*
  store <4 x i32> %159, <4 x i32>* %160, align 4
  %161 = getelementptr inbounds i8, i8* %3, i64 5
  %162 = load i8, i8* %161, align 1
  call void @av1_range_check_buf(i32 5, i32* %0, i32* %1, i32 8, i8 signext %162) #3
  %163 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %46, i64 4
  %164 = load i32, i32* %163, align 16
  %165 = load i32, i32* %1, align 4
  %166 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %46, i64 60
  %167 = load i32, i32* %166, align 16
  %168 = load i32, i32* %20, align 4
  %169 = mul nsw i32 %165, %164
  %170 = sext i32 %169 to i64
  %171 = mul nsw i32 %168, %167
  %172 = sext i32 %171 to i64
  %173 = add i64 %59, %170
  %174 = add i64 %173, %172
  %175 = ashr i64 %174, %62
  %176 = trunc i64 %175 to i32
  store i32 %176, i32* %7, align 16
  %177 = mul nsw i32 %167, %165
  %178 = sext i32 %177 to i64
  %179 = mul i32 %164, %168
  %180 = sub i32 0, %179
  %181 = sext i32 %180 to i64
  %182 = add i64 %59, %178
  %183 = add i64 %182, %181
  %184 = ashr i64 %183, %62
  %185 = trunc i64 %184 to i32
  store i32 %185, i32* %8, align 4
  %186 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %46, i64 20
  %187 = load i32, i32* %186, align 16
  %188 = load i32, i32* %24, align 4
  %189 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %46, i64 44
  %190 = load i32, i32* %189, align 16
  %191 = load i32, i32* %27, align 4
  %192 = mul nsw i32 %188, %187
  %193 = sext i32 %192 to i64
  %194 = mul nsw i32 %191, %190
  %195 = sext i32 %194 to i64
  %196 = add i64 %59, %193
  %197 = add i64 %196, %195
  %198 = ashr i64 %197, %62
  %199 = trunc i64 %198 to i32
  store i32 %199, i32* %9, align 8
  %200 = mul nsw i32 %190, %188
  %201 = sext i32 %200 to i64
  %202 = mul i32 %187, %191
  %203 = sub i32 0, %202
  %204 = sext i32 %203 to i64
  %205 = add i64 %59, %201
  %206 = add i64 %205, %204
  %207 = ashr i64 %206, %62
  %208 = trunc i64 %207 to i32
  store i32 %208, i32* %10, align 4
  %209 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %46, i64 36
  %210 = load i32, i32* %209, align 16
  %211 = load i32, i32* %31, align 4
  %212 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %46, i64 28
  %213 = load i32, i32* %212, align 16
  %214 = load i32, i32* %34, align 4
  %215 = mul nsw i32 %211, %210
  %216 = sext i32 %215 to i64
  %217 = mul nsw i32 %214, %213
  %218 = sext i32 %217 to i64
  %219 = add i64 %59, %216
  %220 = add i64 %219, %218
  %221 = ashr i64 %220, %62
  %222 = trunc i64 %221 to i32
  store i32 %222, i32* %11, align 16
  %223 = mul nsw i32 %213, %211
  %224 = sext i32 %223 to i64
  %225 = mul i32 %210, %214
  %226 = sub i32 0, %225
  %227 = sext i32 %226 to i64
  %228 = add i64 %59, %224
  %229 = add i64 %228, %227
  %230 = ashr i64 %229, %62
  %231 = trunc i64 %230 to i32
  store i32 %231, i32* %12, align 4
  %232 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %46, i64 52
  %233 = load i32, i32* %232, align 16
  %234 = load i32, i32* %37, align 4
  %235 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %46, i64 12
  %236 = load i32, i32* %235, align 16
  %237 = load i32, i32* %41, align 4
  %238 = mul nsw i32 %234, %233
  %239 = sext i32 %238 to i64
  %240 = mul nsw i32 %237, %236
  %241 = sext i32 %240 to i64
  %242 = add i64 %59, %239
  %243 = add i64 %242, %241
  %244 = ashr i64 %243, %62
  %245 = trunc i64 %244 to i32
  store i32 %245, i32* %13, align 8
  %246 = mul nsw i32 %236, %234
  %247 = sext i32 %246 to i64
  %248 = mul i32 %233, %237
  %249 = sub i32 0, %248
  %250 = sext i32 %249 to i64
  %251 = add i64 %59, %247
  %252 = add i64 %251, %250
  %253 = ashr i64 %252, %62
  %254 = trunc i64 %253 to i32
  store i32 %254, i32* %14, align 4
  %255 = getelementptr inbounds i8, i8* %3, i64 6
  %256 = load i8, i8* %255, align 1
  call void @av1_range_check_buf(i32 6, i32* %0, i32* nonnull %7, i32 8, i8 signext %256) #3
  %257 = load i32, i32* %8, align 4
  store i32 %257, i32* %1, align 4
  %258 = bitcast i32* %10 to <4 x i32>*
  %259 = load <4 x i32>, <4 x i32>* %258, align 4
  %260 = shufflevector <4 x i32> %259, <4 x i32> undef, <4 x i32> <i32 3, i32 0, i32 1, i32 2>
  %261 = bitcast i32* %20 to <4 x i32>*
  store <4 x i32> %260, <4 x i32>* %261, align 4
  %262 = load i32, i32* %9, align 8
  store i32 %262, i32* %34, align 4
  %263 = load i32, i32* %14, align 4
  store i32 %263, i32* %37, align 4
  %264 = load i32, i32* %7, align 16
  store i32 %264, i32* %41, align 4
  %265 = getelementptr inbounds i8, i8* %3, i64 7
  %266 = load i8, i8* %265, align 1
  call void @av1_range_check_buf(i32 7, i32* %0, i32* %1, i32 8, i8 signext %266) #3
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %6) #3
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @av1_fadst16(i32*, i32*, i8 signext, i8* nocapture readonly) local_unnamed_addr #0 {
  %5 = alloca [16 x i32], align 16
  %6 = bitcast [16 x i32]* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %6) #3
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %6, i8 -86, i64 64, i1 false)
  %7 = load i8, i8* %3, align 1
  tail call void @av1_range_check_buf(i32 0, i32* %0, i32* %0, i32 16, i8 signext %7) #3
  %8 = load i32, i32* %0, align 4
  store i32 %8, i32* %1, align 4
  %9 = getelementptr inbounds i32, i32* %0, i64 15
  %10 = load i32, i32* %9, align 4
  %11 = sub nsw i32 0, %10
  %12 = getelementptr inbounds i32, i32* %1, i64 1
  store i32 %11, i32* %12, align 4
  %13 = getelementptr inbounds i32, i32* %0, i64 7
  %14 = load i32, i32* %13, align 4
  %15 = sub nsw i32 0, %14
  %16 = getelementptr inbounds i32, i32* %1, i64 2
  store i32 %15, i32* %16, align 4
  %17 = getelementptr inbounds i32, i32* %0, i64 8
  %18 = load i32, i32* %17, align 4
  %19 = getelementptr inbounds i32, i32* %1, i64 3
  store i32 %18, i32* %19, align 4
  %20 = getelementptr inbounds i32, i32* %0, i64 3
  %21 = load i32, i32* %20, align 4
  %22 = sub nsw i32 0, %21
  %23 = getelementptr inbounds i32, i32* %1, i64 4
  store i32 %22, i32* %23, align 4
  %24 = getelementptr inbounds i32, i32* %0, i64 12
  %25 = load i32, i32* %24, align 4
  %26 = getelementptr inbounds i32, i32* %1, i64 5
  store i32 %25, i32* %26, align 4
  %27 = getelementptr inbounds i32, i32* %0, i64 4
  %28 = load i32, i32* %27, align 4
  %29 = getelementptr inbounds i32, i32* %1, i64 6
  store i32 %28, i32* %29, align 4
  %30 = getelementptr inbounds i32, i32* %0, i64 11
  %31 = load i32, i32* %30, align 4
  %32 = sub nsw i32 0, %31
  %33 = getelementptr inbounds i32, i32* %1, i64 7
  store i32 %32, i32* %33, align 4
  %34 = getelementptr inbounds i32, i32* %0, i64 1
  %35 = load i32, i32* %34, align 4
  %36 = sub nsw i32 0, %35
  %37 = getelementptr inbounds i32, i32* %1, i64 8
  store i32 %36, i32* %37, align 4
  %38 = getelementptr inbounds i32, i32* %0, i64 14
  %39 = load i32, i32* %38, align 4
  %40 = getelementptr inbounds i32, i32* %1, i64 9
  store i32 %39, i32* %40, align 4
  %41 = getelementptr inbounds i32, i32* %0, i64 6
  %42 = load i32, i32* %41, align 4
  %43 = getelementptr inbounds i32, i32* %1, i64 10
  store i32 %42, i32* %43, align 4
  %44 = getelementptr inbounds i32, i32* %0, i64 9
  %45 = load i32, i32* %44, align 4
  %46 = sub nsw i32 0, %45
  %47 = getelementptr inbounds i32, i32* %1, i64 11
  store i32 %46, i32* %47, align 4
  %48 = getelementptr inbounds i32, i32* %0, i64 2
  %49 = load i32, i32* %48, align 4
  %50 = getelementptr inbounds i32, i32* %1, i64 12
  store i32 %49, i32* %50, align 4
  %51 = getelementptr inbounds i32, i32* %0, i64 13
  %52 = load i32, i32* %51, align 4
  %53 = sub nsw i32 0, %52
  %54 = getelementptr inbounds i32, i32* %1, i64 13
  store i32 %53, i32* %54, align 4
  %55 = getelementptr inbounds i32, i32* %0, i64 5
  %56 = load i32, i32* %55, align 4
  %57 = sub nsw i32 0, %56
  %58 = getelementptr inbounds i32, i32* %1, i64 14
  store i32 %57, i32* %58, align 4
  %59 = getelementptr inbounds i32, i32* %0, i64 10
  %60 = load i32, i32* %59, align 4
  %61 = getelementptr inbounds i32, i32* %1, i64 15
  store i32 %60, i32* %61, align 4
  %62 = getelementptr inbounds i8, i8* %3, i64 1
  %63 = load i8, i8* %62, align 1
  tail call void @av1_range_check_buf(i32 1, i32* %0, i32* %1, i32 16, i8 signext %63) #3
  %64 = sext i8 %2 to i32
  %65 = add nsw i32 %64, -10
  %66 = sext i32 %65 to i64
  %67 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 0
  %68 = load i32, i32* %1, align 4
  store i32 %68, i32* %67, align 16
  %69 = load i32, i32* %12, align 4
  %70 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 1
  store i32 %69, i32* %70, align 4
  %71 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %66, i64 32
  %72 = load i32, i32* %71, align 16
  %73 = load i32, i32* %16, align 4
  %74 = load i32, i32* %19, align 4
  %75 = mul nsw i32 %73, %72
  %76 = sext i32 %75 to i64
  %77 = mul nsw i32 %74, %72
  %78 = sext i32 %77 to i64
  %79 = add nsw i32 %64, -1
  %80 = zext i32 %79 to i64
  %81 = shl i64 1, %80
  %82 = add i64 %81, %76
  %83 = add i64 %82, %78
  %84 = zext i32 %64 to i64
  %85 = ashr i64 %83, %84
  %86 = trunc i64 %85 to i32
  %87 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 2
  store i32 %86, i32* %87, align 8
  %88 = sub nsw i32 0, %72
  %89 = mul nsw i32 %74, %88
  %90 = sext i32 %89 to i64
  %91 = add i64 %82, %90
  %92 = ashr i64 %91, %84
  %93 = trunc i64 %92 to i32
  %94 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 3
  store i32 %93, i32* %94, align 4
  %95 = load i32, i32* %23, align 4
  %96 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 4
  store i32 %95, i32* %96, align 16
  %97 = load i32, i32* %26, align 4
  %98 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 5
  store i32 %97, i32* %98, align 4
  %99 = load i32, i32* %29, align 4
  %100 = load i32, i32* %33, align 4
  %101 = mul nsw i32 %99, %72
  %102 = sext i32 %101 to i64
  %103 = mul nsw i32 %100, %72
  %104 = sext i32 %103 to i64
  %105 = add i64 %81, %102
  %106 = add i64 %105, %104
  %107 = ashr i64 %106, %84
  %108 = trunc i64 %107 to i32
  %109 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 6
  store i32 %108, i32* %109, align 8
  %110 = mul nsw i32 %100, %88
  %111 = sext i32 %110 to i64
  %112 = add i64 %105, %111
  %113 = ashr i64 %112, %84
  %114 = trunc i64 %113 to i32
  %115 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 7
  store i32 %114, i32* %115, align 4
  %116 = load i32, i32* %37, align 4
  %117 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 8
  store i32 %116, i32* %117, align 16
  %118 = load i32, i32* %40, align 4
  %119 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 9
  store i32 %118, i32* %119, align 4
  %120 = load i32, i32* %43, align 4
  %121 = load i32, i32* %47, align 4
  %122 = mul nsw i32 %120, %72
  %123 = sext i32 %122 to i64
  %124 = mul nsw i32 %121, %72
  %125 = sext i32 %124 to i64
  %126 = add i64 %81, %123
  %127 = add i64 %126, %125
  %128 = ashr i64 %127, %84
  %129 = trunc i64 %128 to i32
  %130 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 10
  store i32 %129, i32* %130, align 8
  %131 = mul nsw i32 %121, %88
  %132 = sext i32 %131 to i64
  %133 = add i64 %126, %132
  %134 = ashr i64 %133, %84
  %135 = trunc i64 %134 to i32
  %136 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 11
  store i32 %135, i32* %136, align 4
  %137 = load i32, i32* %50, align 4
  %138 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 12
  store i32 %137, i32* %138, align 16
  %139 = load i32, i32* %54, align 4
  %140 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 13
  store i32 %139, i32* %140, align 4
  %141 = load i32, i32* %58, align 4
  %142 = load i32, i32* %61, align 4
  %143 = mul nsw i32 %141, %72
  %144 = sext i32 %143 to i64
  %145 = mul nsw i32 %142, %72
  %146 = sext i32 %145 to i64
  %147 = add i64 %81, %144
  %148 = add i64 %147, %146
  %149 = ashr i64 %148, %84
  %150 = trunc i64 %149 to i32
  %151 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 14
  store i32 %150, i32* %151, align 8
  %152 = mul nsw i32 %142, %88
  %153 = sext i32 %152 to i64
  %154 = add i64 %147, %153
  %155 = ashr i64 %154, %84
  %156 = trunc i64 %155 to i32
  %157 = getelementptr inbounds [16 x i32], [16 x i32]* %5, i64 0, i64 15
  store i32 %156, i32* %157, align 4
  %158 = getelementptr inbounds i8, i8* %3, i64 2
  %159 = load i8, i8* %158, align 1
  call void @av1_range_check_buf(i32 2, i32* %0, i32* nonnull %67, i32 16, i8 signext %159) #3
  %160 = bitcast [16 x i32]* %5 to <4 x i32>*
  %161 = load <4 x i32>, <4 x i32>* %160, align 16
  %162 = shufflevector <4 x i32> %161, <4 x i32> undef, <4 x i32> <i32 2, i32 3, i32 0, i32 1>
  %163 = add nsw <4 x i32> %162, %161
  %164 = sub nsw <4 x i32> %162, %161
  %165 = shufflevector <4 x i32> %163, <4 x i32> %164, <4 x i32> <i32 0, i32 1, i32 6, i32 7>
  %166 = bitcast i32* %1 to <4 x i32>*
  store <4 x i32> %165, <4 x i32>* %166, align 4
  %167 = bitcast i32* %96 to <4 x i32>*
  %168 = load <4 x i32>, <4 x i32>* %167, align 16
  %169 = shufflevector <4 x i32> %168, <4 x i32> undef, <4 x i32> <i32 2, i32 3, i32 0, i32 1>
  %170 = add nsw <4 x i32> %169, %168
  %171 = sub nsw <4 x i32> %169, %168
  %172 = shufflevector <4 x i32> %170, <4 x i32> %171, <4 x i32> <i32 0, i32 1, i32 6, i32 7>
  %173 = bitcast i32* %23 to <4 x i32>*
  store <4 x i32> %172, <4 x i32>* %173, align 4
  %174 = bitcast i32* %117 to <4 x i32>*
  %175 = load <4 x i32>, <4 x i32>* %174, align 16
  %176 = shufflevector <4 x i32> %175, <4 x i32> undef, <4 x i32> <i32 2, i32 3, i32 0, i32 1>
  %177 = add nsw <4 x i32> %176, %175
  %178 = sub nsw <4 x i32> %176, %175
  %179 = shufflevector <4 x i32> %177, <4 x i32> %178, <4 x i32> <i32 0, i32 1, i32 6, i32 7>
  %180 = bitcast i32* %37 to <4 x i32>*
  store <4 x i32> %179, <4 x i32>* %180, align 4
  %181 = bitcast i32* %138 to <4 x i32>*
  %182 = load <4 x i32>, <4 x i32>* %181, align 16
  %183 = shufflevector <4 x i32> %182, <4 x i32> undef, <4 x i32> <i32 2, i32 3, i32 0, i32 1>
  %184 = add nsw <4 x i32> %183, %182
  %185 = sub nsw <4 x i32> %183, %182
  %186 = shufflevector <4 x i32> %184, <4 x i32> %185, <4 x i32> <i32 0, i32 1, i32 6, i32 7>
  %187 = bitcast i32* %50 to <4 x i32>*
  store <4 x i32> %186, <4 x i32>* %187, align 4
  %188 = getelementptr inbounds i8, i8* %3, i64 3
  %189 = load i8, i8* %188, align 1
  call void @av1_range_check_buf(i32 3, i32* %0, i32* %1, i32 16, i8 signext %189) #3
  %190 = bitcast i32* %1 to <4 x i32>*
  %191 = load <4 x i32>, <4 x i32>* %190, align 4
  %192 = bitcast [16 x i32]* %5 to <4 x i32>*
  store <4 x i32> %191, <4 x i32>* %192, align 16
  %193 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %66, i64 16
  %194 = load i32, i32* %193, align 16
  %195 = load i32, i32* %23, align 4
  %196 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %66, i64 48
  %197 = load i32, i32* %196, align 16
  %198 = load i32, i32* %26, align 4
  %199 = mul nsw i32 %195, %194
  %200 = sext i32 %199 to i64
  %201 = mul nsw i32 %198, %197
  %202 = sext i32 %201 to i64
  %203 = add i64 %81, %200
  %204 = add i64 %203, %202
  %205 = ashr i64 %204, %84
  %206 = trunc i64 %205 to i32
  store i32 %206, i32* %96, align 16
  %207 = sub nsw i32 0, %194
  %208 = mul nsw i32 %197, %195
  %209 = sext i32 %208 to i64
  %210 = mul nsw i32 %198, %207
  %211 = sext i32 %210 to i64
  %212 = add i64 %81, %209
  %213 = add i64 %212, %211
  %214 = ashr i64 %213, %84
  %215 = trunc i64 %214 to i32
  store i32 %215, i32* %98, align 4
  %216 = sub nsw i32 0, %197
  %217 = load i32, i32* %29, align 4
  %218 = load i32, i32* %33, align 4
  %219 = mul nsw i32 %217, %216
  %220 = sext i32 %219 to i64
  %221 = mul nsw i32 %218, %194
  %222 = sext i32 %221 to i64
  %223 = add i64 %81, %220
  %224 = add i64 %223, %222
  %225 = ashr i64 %224, %84
  %226 = trunc i64 %225 to i32
  store i32 %226, i32* %109, align 8
  %227 = mul nsw i32 %217, %194
  %228 = sext i32 %227 to i64
  %229 = mul nsw i32 %218, %197
  %230 = sext i32 %229 to i64
  %231 = add i64 %81, %228
  %232 = add i64 %231, %230
  %233 = ashr i64 %232, %84
  %234 = trunc i64 %233 to i32
  store i32 %234, i32* %115, align 4
  %235 = bitcast i32* %37 to <4 x i32>*
  %236 = load <4 x i32>, <4 x i32>* %235, align 4
  %237 = bitcast i32* %117 to <4 x i32>*
  store <4 x i32> %236, <4 x i32>* %237, align 16
  %238 = load i32, i32* %50, align 4
  %239 = load i32, i32* %54, align 4
  %240 = mul nsw i32 %238, %194
  %241 = sext i32 %240 to i64
  %242 = mul nsw i32 %239, %197
  %243 = sext i32 %242 to i64
  %244 = add i64 %81, %241
  %245 = add i64 %244, %243
  %246 = ashr i64 %245, %84
  %247 = trunc i64 %246 to i32
  store i32 %247, i32* %138, align 16
  %248 = mul nsw i32 %238, %197
  %249 = sext i32 %248 to i64
  %250 = mul nsw i32 %239, %207
  %251 = sext i32 %250 to i64
  %252 = add i64 %81, %249
  %253 = add i64 %252, %251
  %254 = ashr i64 %253, %84
  %255 = trunc i64 %254 to i32
  store i32 %255, i32* %140, align 4
  %256 = load i32, i32* %58, align 4
  %257 = load i32, i32* %61, align 4
  %258 = mul nsw i32 %256, %216
  %259 = sext i32 %258 to i64
  %260 = mul nsw i32 %257, %194
  %261 = sext i32 %260 to i64
  %262 = add i64 %81, %259
  %263 = add i64 %262, %261
  %264 = ashr i64 %263, %84
  %265 = trunc i64 %264 to i32
  store i32 %265, i32* %151, align 8
  %266 = mul nsw i32 %256, %194
  %267 = sext i32 %266 to i64
  %268 = mul nsw i32 %257, %197
  %269 = sext i32 %268 to i64
  %270 = add i64 %81, %267
  %271 = add i64 %270, %269
  %272 = ashr i64 %271, %84
  %273 = trunc i64 %272 to i32
  store i32 %273, i32* %157, align 4
  %274 = getelementptr inbounds i8, i8* %3, i64 4
  %275 = load i8, i8* %274, align 1
  call void @av1_range_check_buf(i32 4, i32* %0, i32* nonnull %67, i32 16, i8 signext %275) #3
  %276 = bitcast [16 x i32]* %5 to <4 x i32>*
  %277 = load <4 x i32>, <4 x i32>* %276, align 16
  %278 = bitcast i32* %96 to <4 x i32>*
  %279 = load <4 x i32>, <4 x i32>* %278, align 16
  %280 = add nsw <4 x i32> %279, %277
  %281 = bitcast i32* %1 to <4 x i32>*
  store <4 x i32> %280, <4 x i32>* %281, align 4
  %282 = sub nsw <4 x i32> %277, %279
  %283 = bitcast i32* %23 to <4 x i32>*
  store <4 x i32> %282, <4 x i32>* %283, align 4
  %284 = bitcast i32* %117 to <4 x i32>*
  %285 = load <4 x i32>, <4 x i32>* %284, align 16
  %286 = bitcast i32* %138 to <4 x i32>*
  %287 = load <4 x i32>, <4 x i32>* %286, align 16
  %288 = add nsw <4 x i32> %287, %285
  %289 = bitcast i32* %37 to <4 x i32>*
  store <4 x i32> %288, <4 x i32>* %289, align 4
  %290 = sub nsw <4 x i32> %285, %287
  %291 = bitcast i32* %50 to <4 x i32>*
  store <4 x i32> %290, <4 x i32>* %291, align 4
  %292 = getelementptr inbounds i8, i8* %3, i64 5
  %293 = load i8, i8* %292, align 1
  call void @av1_range_check_buf(i32 5, i32* %0, i32* %1, i32 16, i8 signext %293) #3
  %294 = bitcast i32* %1 to <4 x i32>*
  %295 = load <4 x i32>, <4 x i32>* %294, align 4
  %296 = bitcast [16 x i32]* %5 to <4 x i32>*
  store <4 x i32> %295, <4 x i32>* %296, align 16
  %297 = bitcast i32* %23 to <4 x i32>*
  %298 = load <4 x i32>, <4 x i32>* %297, align 4
  %299 = bitcast i32* %96 to <4 x i32>*
  store <4 x i32> %298, <4 x i32>* %299, align 16
  %300 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %66, i64 8
  %301 = load i32, i32* %300, align 16
  %302 = load i32, i32* %37, align 4
  %303 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %66, i64 56
  %304 = load i32, i32* %303, align 16
  %305 = load i32, i32* %40, align 4
  %306 = mul nsw i32 %302, %301
  %307 = sext i32 %306 to i64
  %308 = mul nsw i32 %305, %304
  %309 = sext i32 %308 to i64
  %310 = add i64 %81, %307
  %311 = add i64 %310, %309
  %312 = ashr i64 %311, %84
  %313 = trunc i64 %312 to i32
  store i32 %313, i32* %117, align 16
  %314 = mul nsw i32 %304, %302
  %315 = sext i32 %314 to i64
  %316 = mul i32 %301, %305
  %317 = sub i32 0, %316
  %318 = sext i32 %317 to i64
  %319 = add i64 %81, %315
  %320 = add i64 %319, %318
  %321 = ashr i64 %320, %84
  %322 = trunc i64 %321 to i32
  store i32 %322, i32* %119, align 4
  %323 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %66, i64 40
  %324 = load i32, i32* %323, align 16
  %325 = load i32, i32* %43, align 4
  %326 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %66, i64 24
  %327 = load i32, i32* %326, align 16
  %328 = load i32, i32* %47, align 4
  %329 = mul nsw i32 %325, %324
  %330 = sext i32 %329 to i64
  %331 = mul nsw i32 %328, %327
  %332 = sext i32 %331 to i64
  %333 = add i64 %81, %330
  %334 = add i64 %333, %332
  %335 = ashr i64 %334, %84
  %336 = trunc i64 %335 to i32
  store i32 %336, i32* %130, align 8
  %337 = mul nsw i32 %327, %325
  %338 = sext i32 %337 to i64
  %339 = mul i32 %324, %328
  %340 = sub i32 0, %339
  %341 = sext i32 %340 to i64
  %342 = add i64 %81, %338
  %343 = add i64 %342, %341
  %344 = ashr i64 %343, %84
  %345 = trunc i64 %344 to i32
  store i32 %345, i32* %136, align 4
  %346 = load i32, i32* %50, align 4
  %347 = load i32, i32* %54, align 4
  %348 = mul i32 %304, %346
  %349 = sub i32 0, %348
  %350 = sext i32 %349 to i64
  %351 = mul nsw i32 %347, %301
  %352 = sext i32 %351 to i64
  %353 = add i64 %81, %350
  %354 = add i64 %353, %352
  %355 = ashr i64 %354, %84
  %356 = trunc i64 %355 to i32
  store i32 %356, i32* %138, align 16
  %357 = mul nsw i32 %346, %301
  %358 = sext i32 %357 to i64
  %359 = mul nsw i32 %347, %304
  %360 = sext i32 %359 to i64
  %361 = add i64 %81, %358
  %362 = add i64 %361, %360
  %363 = ashr i64 %362, %84
  %364 = trunc i64 %363 to i32
  store i32 %364, i32* %140, align 4
  %365 = load i32, i32* %58, align 4
  %366 = load i32, i32* %61, align 4
  %367 = mul i32 %327, %365
  %368 = sub i32 0, %367
  %369 = sext i32 %368 to i64
  %370 = mul nsw i32 %366, %324
  %371 = sext i32 %370 to i64
  %372 = add i64 %81, %369
  %373 = add i64 %372, %371
  %374 = ashr i64 %373, %84
  %375 = trunc i64 %374 to i32
  store i32 %375, i32* %151, align 8
  %376 = mul nsw i32 %365, %324
  %377 = sext i32 %376 to i64
  %378 = mul nsw i32 %366, %327
  %379 = sext i32 %378 to i64
  %380 = add i64 %81, %377
  %381 = add i64 %380, %379
  %382 = ashr i64 %381, %84
  %383 = trunc i64 %382 to i32
  store i32 %383, i32* %157, align 4
  %384 = getelementptr inbounds i8, i8* %3, i64 6
  %385 = load i8, i8* %384, align 1
  call void @av1_range_check_buf(i32 6, i32* %0, i32* nonnull %67, i32 16, i8 signext %385) #3
  %386 = bitcast [16 x i32]* %5 to <4 x i32>*
  %387 = load <4 x i32>, <4 x i32>* %386, align 16
  %388 = bitcast i32* %117 to <4 x i32>*
  %389 = load <4 x i32>, <4 x i32>* %388, align 16
  %390 = add nsw <4 x i32> %389, %387
  %391 = bitcast i32* %1 to <4 x i32>*
  store <4 x i32> %390, <4 x i32>* %391, align 4
  %392 = bitcast i32* %96 to <4 x i32>*
  %393 = load <4 x i32>, <4 x i32>* %392, align 16
  %394 = bitcast i32* %138 to <4 x i32>*
  %395 = load <4 x i32>, <4 x i32>* %394, align 16
  %396 = add nsw <4 x i32> %395, %393
  %397 = bitcast i32* %23 to <4 x i32>*
  store <4 x i32> %396, <4 x i32>* %397, align 4
  %398 = sub nsw <4 x i32> %387, %389
  %399 = bitcast i32* %37 to <4 x i32>*
  store <4 x i32> %398, <4 x i32>* %399, align 4
  %400 = sub nsw <4 x i32> %393, %395
  %401 = bitcast i32* %50 to <4 x i32>*
  store <4 x i32> %400, <4 x i32>* %401, align 4
  %402 = getelementptr inbounds i8, i8* %3, i64 7
  %403 = load i8, i8* %402, align 1
  call void @av1_range_check_buf(i32 7, i32* %0, i32* %1, i32 16, i8 signext %403) #3
  %404 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %66, i64 2
  %405 = load i32, i32* %404, align 8
  %406 = load i32, i32* %1, align 4
  %407 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %66, i64 62
  %408 = load i32, i32* %407, align 8
  %409 = load i32, i32* %12, align 4
  %410 = mul nsw i32 %406, %405
  %411 = sext i32 %410 to i64
  %412 = mul nsw i32 %409, %408
  %413 = sext i32 %412 to i64
  %414 = add i64 %81, %411
  %415 = add i64 %414, %413
  %416 = ashr i64 %415, %84
  %417 = trunc i64 %416 to i32
  store i32 %417, i32* %67, align 16
  %418 = mul nsw i32 %408, %406
  %419 = sext i32 %418 to i64
  %420 = mul i32 %405, %409
  %421 = sub i32 0, %420
  %422 = sext i32 %421 to i64
  %423 = add i64 %81, %419
  %424 = add i64 %423, %422
  %425 = ashr i64 %424, %84
  %426 = trunc i64 %425 to i32
  store i32 %426, i32* %70, align 4
  %427 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %66, i64 10
  %428 = load i32, i32* %427, align 8
  %429 = load i32, i32* %16, align 4
  %430 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %66, i64 54
  %431 = load i32, i32* %430, align 8
  %432 = load i32, i32* %19, align 4
  %433 = mul nsw i32 %429, %428
  %434 = sext i32 %433 to i64
  %435 = mul nsw i32 %432, %431
  %436 = sext i32 %435 to i64
  %437 = add i64 %81, %434
  %438 = add i64 %437, %436
  %439 = ashr i64 %438, %84
  %440 = trunc i64 %439 to i32
  store i32 %440, i32* %87, align 8
  %441 = mul nsw i32 %431, %429
  %442 = sext i32 %441 to i64
  %443 = mul i32 %428, %432
  %444 = sub i32 0, %443
  %445 = sext i32 %444 to i64
  %446 = add i64 %81, %442
  %447 = add i64 %446, %445
  %448 = ashr i64 %447, %84
  %449 = trunc i64 %448 to i32
  store i32 %449, i32* %94, align 4
  %450 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %66, i64 18
  %451 = load i32, i32* %450, align 8
  %452 = load i32, i32* %23, align 4
  %453 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %66, i64 46
  %454 = load i32, i32* %453, align 8
  %455 = load i32, i32* %26, align 4
  %456 = mul nsw i32 %452, %451
  %457 = sext i32 %456 to i64
  %458 = mul nsw i32 %455, %454
  %459 = sext i32 %458 to i64
  %460 = add i64 %81, %457
  %461 = add i64 %460, %459
  %462 = ashr i64 %461, %84
  %463 = trunc i64 %462 to i32
  store i32 %463, i32* %96, align 16
  %464 = mul nsw i32 %454, %452
  %465 = sext i32 %464 to i64
  %466 = mul i32 %451, %455
  %467 = sub i32 0, %466
  %468 = sext i32 %467 to i64
  %469 = add i64 %81, %465
  %470 = add i64 %469, %468
  %471 = ashr i64 %470, %84
  %472 = trunc i64 %471 to i32
  store i32 %472, i32* %98, align 4
  %473 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %66, i64 26
  %474 = load i32, i32* %473, align 8
  %475 = load i32, i32* %29, align 4
  %476 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %66, i64 38
  %477 = load i32, i32* %476, align 8
  %478 = load i32, i32* %33, align 4
  %479 = mul nsw i32 %475, %474
  %480 = sext i32 %479 to i64
  %481 = mul nsw i32 %478, %477
  %482 = sext i32 %481 to i64
  %483 = add i64 %81, %480
  %484 = add i64 %483, %482
  %485 = ashr i64 %484, %84
  %486 = trunc i64 %485 to i32
  store i32 %486, i32* %109, align 8
  %487 = mul nsw i32 %477, %475
  %488 = sext i32 %487 to i64
  %489 = mul i32 %474, %478
  %490 = sub i32 0, %489
  %491 = sext i32 %490 to i64
  %492 = add i64 %81, %488
  %493 = add i64 %492, %491
  %494 = ashr i64 %493, %84
  %495 = trunc i64 %494 to i32
  store i32 %495, i32* %115, align 4
  %496 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %66, i64 34
  %497 = load i32, i32* %496, align 8
  %498 = load i32, i32* %37, align 4
  %499 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %66, i64 30
  %500 = load i32, i32* %499, align 8
  %501 = load i32, i32* %40, align 4
  %502 = mul nsw i32 %498, %497
  %503 = sext i32 %502 to i64
  %504 = mul nsw i32 %501, %500
  %505 = sext i32 %504 to i64
  %506 = add i64 %81, %503
  %507 = add i64 %506, %505
  %508 = ashr i64 %507, %84
  %509 = trunc i64 %508 to i32
  store i32 %509, i32* %117, align 16
  %510 = mul nsw i32 %500, %498
  %511 = sext i32 %510 to i64
  %512 = mul i32 %497, %501
  %513 = sub i32 0, %512
  %514 = sext i32 %513 to i64
  %515 = add i64 %81, %511
  %516 = add i64 %515, %514
  %517 = ashr i64 %516, %84
  %518 = trunc i64 %517 to i32
  store i32 %518, i32* %119, align 4
  %519 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %66, i64 42
  %520 = load i32, i32* %519, align 8
  %521 = load i32, i32* %43, align 4
  %522 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %66, i64 22
  %523 = load i32, i32* %522, align 8
  %524 = load i32, i32* %47, align 4
  %525 = mul nsw i32 %521, %520
  %526 = sext i32 %525 to i64
  %527 = mul nsw i32 %524, %523
  %528 = sext i32 %527 to i64
  %529 = add i64 %81, %526
  %530 = add i64 %529, %528
  %531 = ashr i64 %530, %84
  %532 = trunc i64 %531 to i32
  store i32 %532, i32* %130, align 8
  %533 = mul nsw i32 %523, %521
  %534 = sext i32 %533 to i64
  %535 = mul i32 %520, %524
  %536 = sub i32 0, %535
  %537 = sext i32 %536 to i64
  %538 = add i64 %81, %534
  %539 = add i64 %538, %537
  %540 = ashr i64 %539, %84
  %541 = trunc i64 %540 to i32
  store i32 %541, i32* %136, align 4
  %542 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %66, i64 50
  %543 = load i32, i32* %542, align 8
  %544 = load i32, i32* %50, align 4
  %545 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %66, i64 14
  %546 = load i32, i32* %545, align 8
  %547 = load i32, i32* %54, align 4
  %548 = mul nsw i32 %544, %543
  %549 = sext i32 %548 to i64
  %550 = mul nsw i32 %547, %546
  %551 = sext i32 %550 to i64
  %552 = add i64 %81, %549
  %553 = add i64 %552, %551
  %554 = ashr i64 %553, %84
  %555 = trunc i64 %554 to i32
  store i32 %555, i32* %138, align 16
  %556 = mul nsw i32 %546, %544
  %557 = sext i32 %556 to i64
  %558 = mul i32 %543, %547
  %559 = sub i32 0, %558
  %560 = sext i32 %559 to i64
  %561 = add i64 %81, %557
  %562 = add i64 %561, %560
  %563 = ashr i64 %562, %84
  %564 = trunc i64 %563 to i32
  store i32 %564, i32* %140, align 4
  %565 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %66, i64 58
  %566 = load i32, i32* %565, align 8
  %567 = load i32, i32* %58, align 4
  %568 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %66, i64 6
  %569 = load i32, i32* %568, align 8
  %570 = load i32, i32* %61, align 4
  %571 = mul nsw i32 %567, %566
  %572 = sext i32 %571 to i64
  %573 = mul nsw i32 %570, %569
  %574 = sext i32 %573 to i64
  %575 = add i64 %81, %572
  %576 = add i64 %575, %574
  %577 = ashr i64 %576, %84
  %578 = trunc i64 %577 to i32
  store i32 %578, i32* %151, align 8
  %579 = mul nsw i32 %569, %567
  %580 = sext i32 %579 to i64
  %581 = mul i32 %566, %570
  %582 = sub i32 0, %581
  %583 = sext i32 %582 to i64
  %584 = add i64 %81, %580
  %585 = add i64 %584, %583
  %586 = ashr i64 %585, %84
  %587 = trunc i64 %586 to i32
  store i32 %587, i32* %157, align 4
  %588 = getelementptr inbounds i8, i8* %3, i64 8
  %589 = load i8, i8* %588, align 1
  call void @av1_range_check_buf(i32 8, i32* %0, i32* nonnull %67, i32 16, i8 signext %589) #3
  %590 = load i32, i32* %70, align 4
  store i32 %590, i32* %1, align 4
  %591 = load i32, i32* %151, align 8
  store i32 %591, i32* %12, align 4
  %592 = load i32, i32* %94, align 4
  store i32 %592, i32* %16, align 4
  %593 = load i32, i32* %138, align 16
  store i32 %593, i32* %19, align 4
  %594 = load i32, i32* %98, align 4
  store i32 %594, i32* %23, align 4
  %595 = bitcast i32* %115 to <4 x i32>*
  %596 = load <4 x i32>, <4 x i32>* %595, align 4
  %597 = shufflevector <4 x i32> %596, <4 x i32> undef, <4 x i32> <i32 3, i32 0, i32 1, i32 2>
  %598 = bitcast i32* %26 to <4 x i32>*
  store <4 x i32> %597, <4 x i32>* %598, align 4
  %599 = load i32, i32* %109, align 8
  store i32 %599, i32* %40, align 4
  %600 = load i32, i32* %136, align 4
  store i32 %600, i32* %43, align 4
  %601 = load i32, i32* %96, align 16
  store i32 %601, i32* %47, align 4
  %602 = load i32, i32* %140, align 4
  store i32 %602, i32* %50, align 4
  %603 = load i32, i32* %87, align 8
  store i32 %603, i32* %54, align 4
  %604 = load i32, i32* %157, align 4
  store i32 %604, i32* %58, align 4
  %605 = load i32, i32* %67, align 16
  store i32 %605, i32* %61, align 4
  %606 = getelementptr inbounds i8, i8* %3, i64 9
  %607 = load i8, i8* %606, align 1
  call void @av1_range_check_buf(i32 9, i32* %0, i32* %1, i32 16, i8 signext %607) #3
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %6) #3
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @av1_fidentity4_c(i32*, i32*, i8 signext, i8* nocapture readonly) local_unnamed_addr #0 {
  %5 = load i32, i32* %0, align 4
  %6 = sext i32 %5 to i64
  %7 = mul nsw i64 %6, 5793
  %8 = add nsw i64 %7, 2048
  %9 = lshr i64 %8, 12
  %10 = trunc i64 %9 to i32
  store i32 %10, i32* %1, align 4
  %11 = getelementptr inbounds i32, i32* %0, i64 1
  %12 = load i32, i32* %11, align 4
  %13 = sext i32 %12 to i64
  %14 = mul nsw i64 %13, 5793
  %15 = add nsw i64 %14, 2048
  %16 = lshr i64 %15, 12
  %17 = trunc i64 %16 to i32
  %18 = getelementptr inbounds i32, i32* %1, i64 1
  store i32 %17, i32* %18, align 4
  %19 = getelementptr inbounds i32, i32* %0, i64 2
  %20 = load i32, i32* %19, align 4
  %21 = sext i32 %20 to i64
  %22 = mul nsw i64 %21, 5793
  %23 = add nsw i64 %22, 2048
  %24 = lshr i64 %23, 12
  %25 = trunc i64 %24 to i32
  %26 = getelementptr inbounds i32, i32* %1, i64 2
  store i32 %25, i32* %26, align 4
  %27 = getelementptr inbounds i32, i32* %0, i64 3
  %28 = load i32, i32* %27, align 4
  %29 = sext i32 %28 to i64
  %30 = mul nsw i64 %29, 5793
  %31 = add nsw i64 %30, 2048
  %32 = lshr i64 %31, 12
  %33 = trunc i64 %32 to i32
  %34 = getelementptr inbounds i32, i32* %1, i64 3
  store i32 %33, i32* %34, align 4
  %35 = load i8, i8* %3, align 1
  tail call void @av1_range_check_buf(i32 0, i32* %0, i32* %1, i32 4, i8 signext %35) #3
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @av1_fidentity8_c(i32*, i32*, i8 signext, i8* nocapture readonly) local_unnamed_addr #0 {
  %5 = load i32, i32* %0, align 4
  %6 = shl nsw i32 %5, 1
  store i32 %6, i32* %1, align 4
  %7 = getelementptr inbounds i32, i32* %0, i64 1
  %8 = load i32, i32* %7, align 4
  %9 = shl nsw i32 %8, 1
  %10 = getelementptr inbounds i32, i32* %1, i64 1
  store i32 %9, i32* %10, align 4
  %11 = getelementptr inbounds i32, i32* %0, i64 2
  %12 = load i32, i32* %11, align 4
  %13 = shl nsw i32 %12, 1
  %14 = getelementptr inbounds i32, i32* %1, i64 2
  store i32 %13, i32* %14, align 4
  %15 = getelementptr inbounds i32, i32* %0, i64 3
  %16 = load i32, i32* %15, align 4
  %17 = shl nsw i32 %16, 1
  %18 = getelementptr inbounds i32, i32* %1, i64 3
  store i32 %17, i32* %18, align 4
  %19 = getelementptr inbounds i32, i32* %0, i64 4
  %20 = load i32, i32* %19, align 4
  %21 = shl nsw i32 %20, 1
  %22 = getelementptr inbounds i32, i32* %1, i64 4
  store i32 %21, i32* %22, align 4
  %23 = getelementptr inbounds i32, i32* %0, i64 5
  %24 = load i32, i32* %23, align 4
  %25 = shl nsw i32 %24, 1
  %26 = getelementptr inbounds i32, i32* %1, i64 5
  store i32 %25, i32* %26, align 4
  %27 = getelementptr inbounds i32, i32* %0, i64 6
  %28 = load i32, i32* %27, align 4
  %29 = shl nsw i32 %28, 1
  %30 = getelementptr inbounds i32, i32* %1, i64 6
  store i32 %29, i32* %30, align 4
  %31 = getelementptr inbounds i32, i32* %0, i64 7
  %32 = load i32, i32* %31, align 4
  %33 = shl nsw i32 %32, 1
  %34 = getelementptr inbounds i32, i32* %1, i64 7
  store i32 %33, i32* %34, align 4
  %35 = load i8, i8* %3, align 1
  tail call void @av1_range_check_buf(i32 0, i32* %0, i32* %1, i32 8, i8 signext %35) #3
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @av1_fidentity16_c(i32*, i32*, i8 signext, i8* nocapture readonly) local_unnamed_addr #0 {
  %5 = load i32, i32* %0, align 4
  %6 = sext i32 %5 to i64
  %7 = mul nsw i64 %6, 11586
  %8 = add nsw i64 %7, 2048
  %9 = lshr i64 %8, 12
  %10 = trunc i64 %9 to i32
  store i32 %10, i32* %1, align 4
  %11 = getelementptr inbounds i32, i32* %0, i64 1
  %12 = load i32, i32* %11, align 4
  %13 = sext i32 %12 to i64
  %14 = mul nsw i64 %13, 11586
  %15 = add nsw i64 %14, 2048
  %16 = lshr i64 %15, 12
  %17 = trunc i64 %16 to i32
  %18 = getelementptr inbounds i32, i32* %1, i64 1
  store i32 %17, i32* %18, align 4
  %19 = getelementptr inbounds i32, i32* %0, i64 2
  %20 = load i32, i32* %19, align 4
  %21 = sext i32 %20 to i64
  %22 = mul nsw i64 %21, 11586
  %23 = add nsw i64 %22, 2048
  %24 = lshr i64 %23, 12
  %25 = trunc i64 %24 to i32
  %26 = getelementptr inbounds i32, i32* %1, i64 2
  store i32 %25, i32* %26, align 4
  %27 = getelementptr inbounds i32, i32* %0, i64 3
  %28 = load i32, i32* %27, align 4
  %29 = sext i32 %28 to i64
  %30 = mul nsw i64 %29, 11586
  %31 = add nsw i64 %30, 2048
  %32 = lshr i64 %31, 12
  %33 = trunc i64 %32 to i32
  %34 = getelementptr inbounds i32, i32* %1, i64 3
  store i32 %33, i32* %34, align 4
  %35 = getelementptr inbounds i32, i32* %0, i64 4
  %36 = load i32, i32* %35, align 4
  %37 = sext i32 %36 to i64
  %38 = mul nsw i64 %37, 11586
  %39 = add nsw i64 %38, 2048
  %40 = lshr i64 %39, 12
  %41 = trunc i64 %40 to i32
  %42 = getelementptr inbounds i32, i32* %1, i64 4
  store i32 %41, i32* %42, align 4
  %43 = getelementptr inbounds i32, i32* %0, i64 5
  %44 = load i32, i32* %43, align 4
  %45 = sext i32 %44 to i64
  %46 = mul nsw i64 %45, 11586
  %47 = add nsw i64 %46, 2048
  %48 = lshr i64 %47, 12
  %49 = trunc i64 %48 to i32
  %50 = getelementptr inbounds i32, i32* %1, i64 5
  store i32 %49, i32* %50, align 4
  %51 = getelementptr inbounds i32, i32* %0, i64 6
  %52 = load i32, i32* %51, align 4
  %53 = sext i32 %52 to i64
  %54 = mul nsw i64 %53, 11586
  %55 = add nsw i64 %54, 2048
  %56 = lshr i64 %55, 12
  %57 = trunc i64 %56 to i32
  %58 = getelementptr inbounds i32, i32* %1, i64 6
  store i32 %57, i32* %58, align 4
  %59 = getelementptr inbounds i32, i32* %0, i64 7
  %60 = load i32, i32* %59, align 4
  %61 = sext i32 %60 to i64
  %62 = mul nsw i64 %61, 11586
  %63 = add nsw i64 %62, 2048
  %64 = lshr i64 %63, 12
  %65 = trunc i64 %64 to i32
  %66 = getelementptr inbounds i32, i32* %1, i64 7
  store i32 %65, i32* %66, align 4
  %67 = getelementptr inbounds i32, i32* %0, i64 8
  %68 = load i32, i32* %67, align 4
  %69 = sext i32 %68 to i64
  %70 = mul nsw i64 %69, 11586
  %71 = add nsw i64 %70, 2048
  %72 = lshr i64 %71, 12
  %73 = trunc i64 %72 to i32
  %74 = getelementptr inbounds i32, i32* %1, i64 8
  store i32 %73, i32* %74, align 4
  %75 = getelementptr inbounds i32, i32* %0, i64 9
  %76 = load i32, i32* %75, align 4
  %77 = sext i32 %76 to i64
  %78 = mul nsw i64 %77, 11586
  %79 = add nsw i64 %78, 2048
  %80 = lshr i64 %79, 12
  %81 = trunc i64 %80 to i32
  %82 = getelementptr inbounds i32, i32* %1, i64 9
  store i32 %81, i32* %82, align 4
  %83 = getelementptr inbounds i32, i32* %0, i64 10
  %84 = load i32, i32* %83, align 4
  %85 = sext i32 %84 to i64
  %86 = mul nsw i64 %85, 11586
  %87 = add nsw i64 %86, 2048
  %88 = lshr i64 %87, 12
  %89 = trunc i64 %88 to i32
  %90 = getelementptr inbounds i32, i32* %1, i64 10
  store i32 %89, i32* %90, align 4
  %91 = getelementptr inbounds i32, i32* %0, i64 11
  %92 = load i32, i32* %91, align 4
  %93 = sext i32 %92 to i64
  %94 = mul nsw i64 %93, 11586
  %95 = add nsw i64 %94, 2048
  %96 = lshr i64 %95, 12
  %97 = trunc i64 %96 to i32
  %98 = getelementptr inbounds i32, i32* %1, i64 11
  store i32 %97, i32* %98, align 4
  %99 = getelementptr inbounds i32, i32* %0, i64 12
  %100 = load i32, i32* %99, align 4
  %101 = sext i32 %100 to i64
  %102 = mul nsw i64 %101, 11586
  %103 = add nsw i64 %102, 2048
  %104 = lshr i64 %103, 12
  %105 = trunc i64 %104 to i32
  %106 = getelementptr inbounds i32, i32* %1, i64 12
  store i32 %105, i32* %106, align 4
  %107 = getelementptr inbounds i32, i32* %0, i64 13
  %108 = load i32, i32* %107, align 4
  %109 = sext i32 %108 to i64
  %110 = mul nsw i64 %109, 11586
  %111 = add nsw i64 %110, 2048
  %112 = lshr i64 %111, 12
  %113 = trunc i64 %112 to i32
  %114 = getelementptr inbounds i32, i32* %1, i64 13
  store i32 %113, i32* %114, align 4
  %115 = getelementptr inbounds i32, i32* %0, i64 14
  %116 = load i32, i32* %115, align 4
  %117 = sext i32 %116 to i64
  %118 = mul nsw i64 %117, 11586
  %119 = add nsw i64 %118, 2048
  %120 = lshr i64 %119, 12
  %121 = trunc i64 %120 to i32
  %122 = getelementptr inbounds i32, i32* %1, i64 14
  store i32 %121, i32* %122, align 4
  %123 = getelementptr inbounds i32, i32* %0, i64 15
  %124 = load i32, i32* %123, align 4
  %125 = sext i32 %124 to i64
  %126 = mul nsw i64 %125, 11586
  %127 = add nsw i64 %126, 2048
  %128 = lshr i64 %127, 12
  %129 = trunc i64 %128 to i32
  %130 = getelementptr inbounds i32, i32* %1, i64 15
  store i32 %129, i32* %130, align 4
  %131 = load i8, i8* %3, align 1
  tail call void @av1_range_check_buf(i32 0, i32* %0, i32* %1, i32 16, i8 signext %131) #3
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @av1_fidentity32_c(i32*, i32*, i8 signext, i8* nocapture readonly) local_unnamed_addr #0 {
  %5 = getelementptr i32, i32* %1, i64 32
  %6 = getelementptr i32, i32* %0, i64 32
  %7 = icmp ugt i32* %6, %1
  %8 = icmp ugt i32* %5, %0
  %9 = and i1 %7, %8
  br i1 %9, label %59, label %10

10:                                               ; preds = %4
  %11 = bitcast i32* %0 to <4 x i32>*
  %12 = load <4 x i32>, <4 x i32>* %11, align 4, !alias.scope !2
  %13 = shl nsw <4 x i32> %12, <i32 2, i32 2, i32 2, i32 2>
  %14 = bitcast i32* %1 to <4 x i32>*
  store <4 x i32> %13, <4 x i32>* %14, align 4, !alias.scope !5, !noalias !2
  %15 = getelementptr inbounds i32, i32* %0, i64 4
  %16 = bitcast i32* %15 to <4 x i32>*
  %17 = load <4 x i32>, <4 x i32>* %16, align 4, !alias.scope !2
  %18 = shl nsw <4 x i32> %17, <i32 2, i32 2, i32 2, i32 2>
  %19 = getelementptr inbounds i32, i32* %1, i64 4
  %20 = bitcast i32* %19 to <4 x i32>*
  store <4 x i32> %18, <4 x i32>* %20, align 4, !alias.scope !5, !noalias !2
  %21 = getelementptr inbounds i32, i32* %0, i64 8
  %22 = bitcast i32* %21 to <4 x i32>*
  %23 = load <4 x i32>, <4 x i32>* %22, align 4, !alias.scope !2
  %24 = shl nsw <4 x i32> %23, <i32 2, i32 2, i32 2, i32 2>
  %25 = getelementptr inbounds i32, i32* %1, i64 8
  %26 = bitcast i32* %25 to <4 x i32>*
  store <4 x i32> %24, <4 x i32>* %26, align 4, !alias.scope !5, !noalias !2
  %27 = getelementptr inbounds i32, i32* %0, i64 12
  %28 = bitcast i32* %27 to <4 x i32>*
  %29 = load <4 x i32>, <4 x i32>* %28, align 4, !alias.scope !2
  %30 = shl nsw <4 x i32> %29, <i32 2, i32 2, i32 2, i32 2>
  %31 = getelementptr inbounds i32, i32* %1, i64 12
  %32 = bitcast i32* %31 to <4 x i32>*
  store <4 x i32> %30, <4 x i32>* %32, align 4, !alias.scope !5, !noalias !2
  %33 = getelementptr inbounds i32, i32* %0, i64 16
  %34 = bitcast i32* %33 to <4 x i32>*
  %35 = load <4 x i32>, <4 x i32>* %34, align 4, !alias.scope !2
  %36 = shl nsw <4 x i32> %35, <i32 2, i32 2, i32 2, i32 2>
  %37 = getelementptr inbounds i32, i32* %1, i64 16
  %38 = bitcast i32* %37 to <4 x i32>*
  store <4 x i32> %36, <4 x i32>* %38, align 4, !alias.scope !5, !noalias !2
  %39 = getelementptr inbounds i32, i32* %0, i64 20
  %40 = bitcast i32* %39 to <4 x i32>*
  %41 = load <4 x i32>, <4 x i32>* %40, align 4, !alias.scope !2
  %42 = shl nsw <4 x i32> %41, <i32 2, i32 2, i32 2, i32 2>
  %43 = getelementptr inbounds i32, i32* %1, i64 20
  %44 = bitcast i32* %43 to <4 x i32>*
  store <4 x i32> %42, <4 x i32>* %44, align 4, !alias.scope !5, !noalias !2
  %45 = getelementptr inbounds i32, i32* %0, i64 24
  %46 = bitcast i32* %45 to <4 x i32>*
  %47 = load <4 x i32>, <4 x i32>* %46, align 4, !alias.scope !2
  %48 = shl nsw <4 x i32> %47, <i32 2, i32 2, i32 2, i32 2>
  %49 = getelementptr inbounds i32, i32* %1, i64 24
  %50 = bitcast i32* %49 to <4 x i32>*
  store <4 x i32> %48, <4 x i32>* %50, align 4, !alias.scope !5, !noalias !2
  %51 = getelementptr inbounds i32, i32* %0, i64 28
  %52 = bitcast i32* %51 to <4 x i32>*
  %53 = load <4 x i32>, <4 x i32>* %52, align 4, !alias.scope !2
  %54 = shl nsw <4 x i32> %53, <i32 2, i32 2, i32 2, i32 2>
  %55 = getelementptr inbounds i32, i32* %1, i64 28
  %56 = bitcast i32* %55 to <4 x i32>*
  store <4 x i32> %54, <4 x i32>* %56, align 4, !alias.scope !5, !noalias !2
  br label %57

57:                                               ; preds = %59, %10
  %58 = load i8, i8* %3, align 1
  tail call void @av1_range_check_buf(i32 0, i32* %0, i32* %1, i32 32, i8 signext %58) #3
  ret void

59:                                               ; preds = %4, %59
  %60 = phi i64 [ %80, %59 ], [ 0, %4 ]
  %61 = getelementptr inbounds i32, i32* %0, i64 %60
  %62 = load i32, i32* %61, align 4
  %63 = shl nsw i32 %62, 2
  %64 = getelementptr inbounds i32, i32* %1, i64 %60
  store i32 %63, i32* %64, align 4
  %65 = or i64 %60, 1
  %66 = getelementptr inbounds i32, i32* %0, i64 %65
  %67 = load i32, i32* %66, align 4
  %68 = shl nsw i32 %67, 2
  %69 = getelementptr inbounds i32, i32* %1, i64 %65
  store i32 %68, i32* %69, align 4
  %70 = or i64 %60, 2
  %71 = getelementptr inbounds i32, i32* %0, i64 %70
  %72 = load i32, i32* %71, align 4
  %73 = shl nsw i32 %72, 2
  %74 = getelementptr inbounds i32, i32* %1, i64 %70
  store i32 %73, i32* %74, align 4
  %75 = or i64 %60, 3
  %76 = getelementptr inbounds i32, i32* %0, i64 %75
  %77 = load i32, i32* %76, align 4
  %78 = shl nsw i32 %77, 2
  %79 = getelementptr inbounds i32, i32* %1, i64 %75
  store i32 %78, i32* %79, align 4
  %80 = add nuw nsw i64 %60, 4
  %81 = icmp eq i64 %80, 32
  br i1 %81, label %57, label %59, !llvm.loop !7
}

; Function Attrs: nounwind ssp uwtable
define hidden void @av1_fdct64(i32*, i32*, i8 signext, i8* nocapture readonly) local_unnamed_addr #0 {
  %5 = alloca [64 x i32], align 16
  %6 = bitcast [64 x i32]* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %6) #3
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %6, i8 -86, i64 256, i1 false)
  %7 = load i8, i8* %3, align 1
  tail call void @av1_range_check_buf(i32 0, i32* %0, i32* %0, i32 64, i8 signext %7) #3
  %8 = load i32, i32* %0, align 4
  %9 = getelementptr inbounds i32, i32* %0, i64 63
  %10 = load i32, i32* %9, align 4
  %11 = add nsw i32 %10, %8
  store i32 %11, i32* %1, align 4
  %12 = getelementptr inbounds i32, i32* %0, i64 1
  %13 = load i32, i32* %12, align 4
  %14 = getelementptr inbounds i32, i32* %0, i64 62
  %15 = load i32, i32* %14, align 4
  %16 = add nsw i32 %15, %13
  %17 = getelementptr inbounds i32, i32* %1, i64 1
  store i32 %16, i32* %17, align 4
  %18 = getelementptr inbounds i32, i32* %0, i64 2
  %19 = load i32, i32* %18, align 4
  %20 = getelementptr inbounds i32, i32* %0, i64 61
  %21 = load i32, i32* %20, align 4
  %22 = add nsw i32 %21, %19
  %23 = getelementptr inbounds i32, i32* %1, i64 2
  store i32 %22, i32* %23, align 4
  %24 = getelementptr inbounds i32, i32* %0, i64 3
  %25 = load i32, i32* %24, align 4
  %26 = getelementptr inbounds i32, i32* %0, i64 60
  %27 = load i32, i32* %26, align 4
  %28 = add nsw i32 %27, %25
  %29 = getelementptr inbounds i32, i32* %1, i64 3
  store i32 %28, i32* %29, align 4
  %30 = getelementptr inbounds i32, i32* %0, i64 4
  %31 = load i32, i32* %30, align 4
  %32 = getelementptr inbounds i32, i32* %0, i64 59
  %33 = load i32, i32* %32, align 4
  %34 = add nsw i32 %33, %31
  %35 = getelementptr inbounds i32, i32* %1, i64 4
  store i32 %34, i32* %35, align 4
  %36 = getelementptr inbounds i32, i32* %0, i64 5
  %37 = load i32, i32* %36, align 4
  %38 = getelementptr inbounds i32, i32* %0, i64 58
  %39 = load i32, i32* %38, align 4
  %40 = add nsw i32 %39, %37
  %41 = getelementptr inbounds i32, i32* %1, i64 5
  store i32 %40, i32* %41, align 4
  %42 = getelementptr inbounds i32, i32* %0, i64 6
  %43 = load i32, i32* %42, align 4
  %44 = getelementptr inbounds i32, i32* %0, i64 57
  %45 = load i32, i32* %44, align 4
  %46 = add nsw i32 %45, %43
  %47 = getelementptr inbounds i32, i32* %1, i64 6
  store i32 %46, i32* %47, align 4
  %48 = getelementptr inbounds i32, i32* %0, i64 7
  %49 = load i32, i32* %48, align 4
  %50 = getelementptr inbounds i32, i32* %0, i64 56
  %51 = load i32, i32* %50, align 4
  %52 = add nsw i32 %51, %49
  %53 = getelementptr inbounds i32, i32* %1, i64 7
  store i32 %52, i32* %53, align 4
  %54 = getelementptr inbounds i32, i32* %0, i64 8
  %55 = load i32, i32* %54, align 4
  %56 = getelementptr inbounds i32, i32* %0, i64 55
  %57 = load i32, i32* %56, align 4
  %58 = add nsw i32 %57, %55
  %59 = getelementptr inbounds i32, i32* %1, i64 8
  store i32 %58, i32* %59, align 4
  %60 = getelementptr inbounds i32, i32* %0, i64 9
  %61 = load i32, i32* %60, align 4
  %62 = getelementptr inbounds i32, i32* %0, i64 54
  %63 = load i32, i32* %62, align 4
  %64 = add nsw i32 %63, %61
  %65 = getelementptr inbounds i32, i32* %1, i64 9
  store i32 %64, i32* %65, align 4
  %66 = getelementptr inbounds i32, i32* %0, i64 10
  %67 = load i32, i32* %66, align 4
  %68 = getelementptr inbounds i32, i32* %0, i64 53
  %69 = load i32, i32* %68, align 4
  %70 = add nsw i32 %69, %67
  %71 = getelementptr inbounds i32, i32* %1, i64 10
  store i32 %70, i32* %71, align 4
  %72 = getelementptr inbounds i32, i32* %0, i64 11
  %73 = load i32, i32* %72, align 4
  %74 = getelementptr inbounds i32, i32* %0, i64 52
  %75 = load i32, i32* %74, align 4
  %76 = add nsw i32 %75, %73
  %77 = getelementptr inbounds i32, i32* %1, i64 11
  store i32 %76, i32* %77, align 4
  %78 = getelementptr inbounds i32, i32* %0, i64 12
  %79 = load i32, i32* %78, align 4
  %80 = getelementptr inbounds i32, i32* %0, i64 51
  %81 = load i32, i32* %80, align 4
  %82 = add nsw i32 %81, %79
  %83 = getelementptr inbounds i32, i32* %1, i64 12
  store i32 %82, i32* %83, align 4
  %84 = getelementptr inbounds i32, i32* %0, i64 13
  %85 = load i32, i32* %84, align 4
  %86 = getelementptr inbounds i32, i32* %0, i64 50
  %87 = load i32, i32* %86, align 4
  %88 = add nsw i32 %87, %85
  %89 = getelementptr inbounds i32, i32* %1, i64 13
  store i32 %88, i32* %89, align 4
  %90 = getelementptr inbounds i32, i32* %0, i64 14
  %91 = load i32, i32* %90, align 4
  %92 = getelementptr inbounds i32, i32* %0, i64 49
  %93 = load i32, i32* %92, align 4
  %94 = add nsw i32 %93, %91
  %95 = getelementptr inbounds i32, i32* %1, i64 14
  store i32 %94, i32* %95, align 4
  %96 = getelementptr inbounds i32, i32* %0, i64 15
  %97 = load i32, i32* %96, align 4
  %98 = getelementptr inbounds i32, i32* %0, i64 48
  %99 = load i32, i32* %98, align 4
  %100 = add nsw i32 %99, %97
  %101 = getelementptr inbounds i32, i32* %1, i64 15
  store i32 %100, i32* %101, align 4
  %102 = getelementptr inbounds i32, i32* %0, i64 16
  %103 = load i32, i32* %102, align 4
  %104 = getelementptr inbounds i32, i32* %0, i64 47
  %105 = load i32, i32* %104, align 4
  %106 = add nsw i32 %105, %103
  %107 = getelementptr inbounds i32, i32* %1, i64 16
  store i32 %106, i32* %107, align 4
  %108 = getelementptr inbounds i32, i32* %0, i64 17
  %109 = load i32, i32* %108, align 4
  %110 = getelementptr inbounds i32, i32* %0, i64 46
  %111 = load i32, i32* %110, align 4
  %112 = add nsw i32 %111, %109
  %113 = getelementptr inbounds i32, i32* %1, i64 17
  store i32 %112, i32* %113, align 4
  %114 = getelementptr inbounds i32, i32* %0, i64 18
  %115 = load i32, i32* %114, align 4
  %116 = getelementptr inbounds i32, i32* %0, i64 45
  %117 = load i32, i32* %116, align 4
  %118 = add nsw i32 %117, %115
  %119 = getelementptr inbounds i32, i32* %1, i64 18
  store i32 %118, i32* %119, align 4
  %120 = getelementptr inbounds i32, i32* %0, i64 19
  %121 = load i32, i32* %120, align 4
  %122 = getelementptr inbounds i32, i32* %0, i64 44
  %123 = load i32, i32* %122, align 4
  %124 = add nsw i32 %123, %121
  %125 = getelementptr inbounds i32, i32* %1, i64 19
  store i32 %124, i32* %125, align 4
  %126 = getelementptr inbounds i32, i32* %0, i64 20
  %127 = load i32, i32* %126, align 4
  %128 = getelementptr inbounds i32, i32* %0, i64 43
  %129 = load i32, i32* %128, align 4
  %130 = add nsw i32 %129, %127
  %131 = getelementptr inbounds i32, i32* %1, i64 20
  store i32 %130, i32* %131, align 4
  %132 = getelementptr inbounds i32, i32* %0, i64 21
  %133 = load i32, i32* %132, align 4
  %134 = getelementptr inbounds i32, i32* %0, i64 42
  %135 = load i32, i32* %134, align 4
  %136 = add nsw i32 %135, %133
  %137 = getelementptr inbounds i32, i32* %1, i64 21
  store i32 %136, i32* %137, align 4
  %138 = getelementptr inbounds i32, i32* %0, i64 22
  %139 = load i32, i32* %138, align 4
  %140 = getelementptr inbounds i32, i32* %0, i64 41
  %141 = load i32, i32* %140, align 4
  %142 = add nsw i32 %141, %139
  %143 = getelementptr inbounds i32, i32* %1, i64 22
  store i32 %142, i32* %143, align 4
  %144 = getelementptr inbounds i32, i32* %0, i64 23
  %145 = load i32, i32* %144, align 4
  %146 = getelementptr inbounds i32, i32* %0, i64 40
  %147 = load i32, i32* %146, align 4
  %148 = add nsw i32 %147, %145
  %149 = getelementptr inbounds i32, i32* %1, i64 23
  store i32 %148, i32* %149, align 4
  %150 = getelementptr inbounds i32, i32* %0, i64 24
  %151 = load i32, i32* %150, align 4
  %152 = getelementptr inbounds i32, i32* %0, i64 39
  %153 = load i32, i32* %152, align 4
  %154 = add nsw i32 %153, %151
  %155 = getelementptr inbounds i32, i32* %1, i64 24
  store i32 %154, i32* %155, align 4
  %156 = getelementptr inbounds i32, i32* %0, i64 25
  %157 = load i32, i32* %156, align 4
  %158 = getelementptr inbounds i32, i32* %0, i64 38
  %159 = load i32, i32* %158, align 4
  %160 = add nsw i32 %159, %157
  %161 = getelementptr inbounds i32, i32* %1, i64 25
  store i32 %160, i32* %161, align 4
  %162 = getelementptr inbounds i32, i32* %0, i64 26
  %163 = load i32, i32* %162, align 4
  %164 = getelementptr inbounds i32, i32* %0, i64 37
  %165 = load i32, i32* %164, align 4
  %166 = add nsw i32 %165, %163
  %167 = getelementptr inbounds i32, i32* %1, i64 26
  store i32 %166, i32* %167, align 4
  %168 = getelementptr inbounds i32, i32* %0, i64 27
  %169 = load i32, i32* %168, align 4
  %170 = getelementptr inbounds i32, i32* %0, i64 36
  %171 = load i32, i32* %170, align 4
  %172 = add nsw i32 %171, %169
  %173 = getelementptr inbounds i32, i32* %1, i64 27
  store i32 %172, i32* %173, align 4
  %174 = getelementptr inbounds i32, i32* %0, i64 28
  %175 = load i32, i32* %174, align 4
  %176 = getelementptr inbounds i32, i32* %0, i64 35
  %177 = load i32, i32* %176, align 4
  %178 = add nsw i32 %177, %175
  %179 = getelementptr inbounds i32, i32* %1, i64 28
  store i32 %178, i32* %179, align 4
  %180 = getelementptr inbounds i32, i32* %0, i64 29
  %181 = load i32, i32* %180, align 4
  %182 = getelementptr inbounds i32, i32* %0, i64 34
  %183 = load i32, i32* %182, align 4
  %184 = add nsw i32 %183, %181
  %185 = getelementptr inbounds i32, i32* %1, i64 29
  store i32 %184, i32* %185, align 4
  %186 = getelementptr inbounds i32, i32* %0, i64 30
  %187 = load i32, i32* %186, align 4
  %188 = getelementptr inbounds i32, i32* %0, i64 33
  %189 = load i32, i32* %188, align 4
  %190 = add nsw i32 %189, %187
  %191 = getelementptr inbounds i32, i32* %1, i64 30
  store i32 %190, i32* %191, align 4
  %192 = getelementptr inbounds i32, i32* %0, i64 31
  %193 = load i32, i32* %192, align 4
  %194 = getelementptr inbounds i32, i32* %0, i64 32
  %195 = load i32, i32* %194, align 4
  %196 = add nsw i32 %195, %193
  %197 = getelementptr inbounds i32, i32* %1, i64 31
  store i32 %196, i32* %197, align 4
  %198 = load i32, i32* %194, align 4
  %199 = load i32, i32* %192, align 4
  %200 = sub i32 %199, %198
  %201 = getelementptr inbounds i32, i32* %1, i64 32
  store i32 %200, i32* %201, align 4
  %202 = load i32, i32* %188, align 4
  %203 = load i32, i32* %186, align 4
  %204 = sub i32 %203, %202
  %205 = getelementptr inbounds i32, i32* %1, i64 33
  store i32 %204, i32* %205, align 4
  %206 = load i32, i32* %182, align 4
  %207 = load i32, i32* %180, align 4
  %208 = sub i32 %207, %206
  %209 = getelementptr inbounds i32, i32* %1, i64 34
  store i32 %208, i32* %209, align 4
  %210 = load i32, i32* %176, align 4
  %211 = load i32, i32* %174, align 4
  %212 = sub i32 %211, %210
  %213 = getelementptr inbounds i32, i32* %1, i64 35
  store i32 %212, i32* %213, align 4
  %214 = load i32, i32* %170, align 4
  %215 = load i32, i32* %168, align 4
  %216 = sub i32 %215, %214
  %217 = getelementptr inbounds i32, i32* %1, i64 36
  store i32 %216, i32* %217, align 4
  %218 = load i32, i32* %164, align 4
  %219 = load i32, i32* %162, align 4
  %220 = sub i32 %219, %218
  %221 = getelementptr inbounds i32, i32* %1, i64 37
  store i32 %220, i32* %221, align 4
  %222 = load i32, i32* %158, align 4
  %223 = load i32, i32* %156, align 4
  %224 = sub i32 %223, %222
  %225 = getelementptr inbounds i32, i32* %1, i64 38
  store i32 %224, i32* %225, align 4
  %226 = load i32, i32* %152, align 4
  %227 = load i32, i32* %150, align 4
  %228 = sub i32 %227, %226
  %229 = getelementptr inbounds i32, i32* %1, i64 39
  store i32 %228, i32* %229, align 4
  %230 = load i32, i32* %146, align 4
  %231 = load i32, i32* %144, align 4
  %232 = sub i32 %231, %230
  %233 = getelementptr inbounds i32, i32* %1, i64 40
  store i32 %232, i32* %233, align 4
  %234 = load i32, i32* %140, align 4
  %235 = load i32, i32* %138, align 4
  %236 = sub i32 %235, %234
  %237 = getelementptr inbounds i32, i32* %1, i64 41
  store i32 %236, i32* %237, align 4
  %238 = load i32, i32* %134, align 4
  %239 = load i32, i32* %132, align 4
  %240 = sub i32 %239, %238
  %241 = getelementptr inbounds i32, i32* %1, i64 42
  store i32 %240, i32* %241, align 4
  %242 = load i32, i32* %128, align 4
  %243 = load i32, i32* %126, align 4
  %244 = sub i32 %243, %242
  %245 = getelementptr inbounds i32, i32* %1, i64 43
  store i32 %244, i32* %245, align 4
  %246 = load i32, i32* %122, align 4
  %247 = load i32, i32* %120, align 4
  %248 = sub i32 %247, %246
  %249 = getelementptr inbounds i32, i32* %1, i64 44
  store i32 %248, i32* %249, align 4
  %250 = load i32, i32* %116, align 4
  %251 = load i32, i32* %114, align 4
  %252 = sub i32 %251, %250
  %253 = getelementptr inbounds i32, i32* %1, i64 45
  store i32 %252, i32* %253, align 4
  %254 = load i32, i32* %110, align 4
  %255 = load i32, i32* %108, align 4
  %256 = sub i32 %255, %254
  %257 = getelementptr inbounds i32, i32* %1, i64 46
  store i32 %256, i32* %257, align 4
  %258 = load i32, i32* %104, align 4
  %259 = load i32, i32* %102, align 4
  %260 = sub i32 %259, %258
  %261 = getelementptr inbounds i32, i32* %1, i64 47
  store i32 %260, i32* %261, align 4
  %262 = load i32, i32* %98, align 4
  %263 = load i32, i32* %96, align 4
  %264 = sub i32 %263, %262
  %265 = getelementptr inbounds i32, i32* %1, i64 48
  store i32 %264, i32* %265, align 4
  %266 = load i32, i32* %92, align 4
  %267 = load i32, i32* %90, align 4
  %268 = sub i32 %267, %266
  %269 = getelementptr inbounds i32, i32* %1, i64 49
  store i32 %268, i32* %269, align 4
  %270 = load i32, i32* %86, align 4
  %271 = load i32, i32* %84, align 4
  %272 = sub i32 %271, %270
  %273 = getelementptr inbounds i32, i32* %1, i64 50
  store i32 %272, i32* %273, align 4
  %274 = load i32, i32* %80, align 4
  %275 = load i32, i32* %78, align 4
  %276 = sub i32 %275, %274
  %277 = getelementptr inbounds i32, i32* %1, i64 51
  store i32 %276, i32* %277, align 4
  %278 = load i32, i32* %74, align 4
  %279 = load i32, i32* %72, align 4
  %280 = sub i32 %279, %278
  %281 = getelementptr inbounds i32, i32* %1, i64 52
  store i32 %280, i32* %281, align 4
  %282 = load i32, i32* %68, align 4
  %283 = load i32, i32* %66, align 4
  %284 = sub i32 %283, %282
  %285 = getelementptr inbounds i32, i32* %1, i64 53
  store i32 %284, i32* %285, align 4
  %286 = load i32, i32* %62, align 4
  %287 = load i32, i32* %60, align 4
  %288 = sub i32 %287, %286
  %289 = getelementptr inbounds i32, i32* %1, i64 54
  store i32 %288, i32* %289, align 4
  %290 = load i32, i32* %56, align 4
  %291 = load i32, i32* %54, align 4
  %292 = sub i32 %291, %290
  %293 = getelementptr inbounds i32, i32* %1, i64 55
  store i32 %292, i32* %293, align 4
  %294 = load i32, i32* %50, align 4
  %295 = load i32, i32* %48, align 4
  %296 = sub i32 %295, %294
  %297 = getelementptr inbounds i32, i32* %1, i64 56
  store i32 %296, i32* %297, align 4
  %298 = load i32, i32* %44, align 4
  %299 = load i32, i32* %42, align 4
  %300 = sub i32 %299, %298
  %301 = getelementptr inbounds i32, i32* %1, i64 57
  store i32 %300, i32* %301, align 4
  %302 = load i32, i32* %38, align 4
  %303 = load i32, i32* %36, align 4
  %304 = sub i32 %303, %302
  %305 = getelementptr inbounds i32, i32* %1, i64 58
  store i32 %304, i32* %305, align 4
  %306 = load i32, i32* %32, align 4
  %307 = load i32, i32* %30, align 4
  %308 = sub i32 %307, %306
  %309 = getelementptr inbounds i32, i32* %1, i64 59
  store i32 %308, i32* %309, align 4
  %310 = load i32, i32* %26, align 4
  %311 = load i32, i32* %24, align 4
  %312 = sub i32 %311, %310
  %313 = getelementptr inbounds i32, i32* %1, i64 60
  store i32 %312, i32* %313, align 4
  %314 = load i32, i32* %20, align 4
  %315 = load i32, i32* %18, align 4
  %316 = sub i32 %315, %314
  %317 = getelementptr inbounds i32, i32* %1, i64 61
  store i32 %316, i32* %317, align 4
  %318 = load i32, i32* %14, align 4
  %319 = load i32, i32* %12, align 4
  %320 = sub i32 %319, %318
  %321 = getelementptr inbounds i32, i32* %1, i64 62
  store i32 %320, i32* %321, align 4
  %322 = load i32, i32* %9, align 4
  %323 = load i32, i32* %0, align 4
  %324 = sub i32 %323, %322
  %325 = getelementptr inbounds i32, i32* %1, i64 63
  store i32 %324, i32* %325, align 4
  %326 = getelementptr inbounds i8, i8* %3, i64 1
  %327 = load i8, i8* %326, align 1
  tail call void @av1_range_check_buf(i32 1, i32* %0, i32* %1, i32 64, i8 signext %327) #3
  %328 = sext i8 %2 to i32
  %329 = add nsw i32 %328, -10
  %330 = sext i32 %329 to i64
  %331 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 0
  %332 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 1
  %333 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 2
  %334 = bitcast i32* %1 to <4 x i32>*
  %335 = load <4 x i32>, <4 x i32>* %334, align 4
  %336 = bitcast i32* %179 to <4 x i32>*
  %337 = load <4 x i32>, <4 x i32>* %336, align 4
  %338 = shufflevector <4 x i32> %337, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %339 = add nsw <4 x i32> %338, %335
  %340 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 3
  %341 = bitcast [64 x i32]* %5 to <4 x i32>*
  store <4 x i32> %339, <4 x i32>* %341, align 16
  %342 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 4
  %343 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 5
  %344 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 6
  %345 = bitcast i32* %35 to <4 x i32>*
  %346 = load <4 x i32>, <4 x i32>* %345, align 4
  %347 = bitcast i32* %155 to <4 x i32>*
  %348 = load <4 x i32>, <4 x i32>* %347, align 4
  %349 = shufflevector <4 x i32> %348, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %350 = add nsw <4 x i32> %349, %346
  %351 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 7
  %352 = bitcast i32* %342 to <4 x i32>*
  store <4 x i32> %350, <4 x i32>* %352, align 16
  %353 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 8
  %354 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 9
  %355 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 10
  %356 = bitcast i32* %59 to <4 x i32>*
  %357 = load <4 x i32>, <4 x i32>* %356, align 4
  %358 = bitcast i32* %131 to <4 x i32>*
  %359 = load <4 x i32>, <4 x i32>* %358, align 4
  %360 = shufflevector <4 x i32> %359, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %361 = add nsw <4 x i32> %360, %357
  %362 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 11
  %363 = bitcast i32* %353 to <4 x i32>*
  store <4 x i32> %361, <4 x i32>* %363, align 16
  %364 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 12
  %365 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 13
  %366 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 14
  %367 = bitcast i32* %83 to <4 x i32>*
  %368 = load <4 x i32>, <4 x i32>* %367, align 4
  %369 = bitcast i32* %107 to <4 x i32>*
  %370 = load <4 x i32>, <4 x i32>* %369, align 4
  %371 = shufflevector <4 x i32> %370, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %372 = add nsw <4 x i32> %371, %368
  %373 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 15
  %374 = bitcast i32* %364 to <4 x i32>*
  store <4 x i32> %372, <4 x i32>* %374, align 16
  %375 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 16
  %376 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 17
  %377 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 18
  %378 = shufflevector <4 x i32> %368, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %379 = sub <4 x i32> %378, %370
  %380 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 19
  %381 = bitcast i32* %375 to <4 x i32>*
  store <4 x i32> %379, <4 x i32>* %381, align 16
  %382 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 20
  %383 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 21
  %384 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 22
  %385 = shufflevector <4 x i32> %357, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %386 = sub <4 x i32> %385, %359
  %387 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 23
  %388 = bitcast i32* %382 to <4 x i32>*
  store <4 x i32> %386, <4 x i32>* %388, align 16
  %389 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 24
  %390 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 25
  %391 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 26
  %392 = shufflevector <4 x i32> %346, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %393 = sub <4 x i32> %392, %348
  %394 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 27
  %395 = bitcast i32* %389 to <4 x i32>*
  store <4 x i32> %393, <4 x i32>* %395, align 16
  %396 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 28
  %397 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 29
  %398 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 30
  %399 = shufflevector <4 x i32> %335, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %400 = sub <4 x i32> %399, %337
  %401 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 31
  %402 = bitcast i32* %396 to <4 x i32>*
  store <4 x i32> %400, <4 x i32>* %402, align 16
  %403 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 32
  %404 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 33
  %405 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 34
  %406 = bitcast i32* %201 to <4 x i32>*
  %407 = load <4 x i32>, <4 x i32>* %406, align 4
  %408 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 35
  %409 = bitcast i32* %403 to <4 x i32>*
  store <4 x i32> %407, <4 x i32>* %409, align 16
  %410 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 36
  %411 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 37
  %412 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 38
  %413 = bitcast i32* %217 to <4 x i32>*
  %414 = load <4 x i32>, <4 x i32>* %413, align 4
  %415 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 39
  %416 = bitcast i32* %410 to <4 x i32>*
  store <4 x i32> %414, <4 x i32>* %416, align 16
  %417 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 32
  %418 = load i32, i32* %417, align 16
  %419 = sub nsw i32 0, %418
  %420 = load i32, i32* %233, align 4
  %421 = load i32, i32* %293, align 4
  %422 = mul nsw i32 %420, %419
  %423 = sext i32 %422 to i64
  %424 = mul nsw i32 %421, %418
  %425 = sext i32 %424 to i64
  %426 = add nsw i32 %328, -1
  %427 = zext i32 %426 to i64
  %428 = shl i64 1, %427
  %429 = add i64 %428, %425
  %430 = add i64 %429, %423
  %431 = zext i32 %328 to i64
  %432 = ashr i64 %430, %431
  %433 = trunc i64 %432 to i32
  %434 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 40
  store i32 %433, i32* %434, align 16
  %435 = load i32, i32* %237, align 4
  %436 = load i32, i32* %289, align 4
  %437 = mul nsw i32 %435, %419
  %438 = sext i32 %437 to i64
  %439 = mul nsw i32 %436, %418
  %440 = sext i32 %439 to i64
  %441 = add i64 %428, %440
  %442 = add i64 %441, %438
  %443 = ashr i64 %442, %431
  %444 = trunc i64 %443 to i32
  %445 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 41
  store i32 %444, i32* %445, align 4
  %446 = load i32, i32* %241, align 4
  %447 = load i32, i32* %285, align 4
  %448 = mul nsw i32 %446, %419
  %449 = sext i32 %448 to i64
  %450 = mul nsw i32 %447, %418
  %451 = sext i32 %450 to i64
  %452 = add i64 %428, %451
  %453 = add i64 %452, %449
  %454 = ashr i64 %453, %431
  %455 = trunc i64 %454 to i32
  %456 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 42
  store i32 %455, i32* %456, align 8
  %457 = load i32, i32* %245, align 4
  %458 = load i32, i32* %281, align 4
  %459 = mul nsw i32 %457, %419
  %460 = sext i32 %459 to i64
  %461 = mul nsw i32 %458, %418
  %462 = sext i32 %461 to i64
  %463 = add i64 %428, %462
  %464 = add i64 %463, %460
  %465 = ashr i64 %464, %431
  %466 = trunc i64 %465 to i32
  %467 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 43
  store i32 %466, i32* %467, align 4
  %468 = load i32, i32* %249, align 4
  %469 = load i32, i32* %277, align 4
  %470 = mul nsw i32 %468, %419
  %471 = sext i32 %470 to i64
  %472 = mul nsw i32 %469, %418
  %473 = sext i32 %472 to i64
  %474 = add i64 %428, %473
  %475 = add i64 %474, %471
  %476 = ashr i64 %475, %431
  %477 = trunc i64 %476 to i32
  %478 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 44
  store i32 %477, i32* %478, align 16
  %479 = load i32, i32* %253, align 4
  %480 = load i32, i32* %273, align 4
  %481 = mul nsw i32 %479, %419
  %482 = sext i32 %481 to i64
  %483 = mul nsw i32 %480, %418
  %484 = sext i32 %483 to i64
  %485 = add i64 %428, %484
  %486 = add i64 %485, %482
  %487 = ashr i64 %486, %431
  %488 = trunc i64 %487 to i32
  %489 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 45
  store i32 %488, i32* %489, align 4
  %490 = load i32, i32* %257, align 4
  %491 = load i32, i32* %269, align 4
  %492 = mul nsw i32 %490, %419
  %493 = sext i32 %492 to i64
  %494 = mul nsw i32 %491, %418
  %495 = sext i32 %494 to i64
  %496 = add i64 %428, %495
  %497 = add i64 %496, %493
  %498 = ashr i64 %497, %431
  %499 = trunc i64 %498 to i32
  %500 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 46
  store i32 %499, i32* %500, align 8
  %501 = load i32, i32* %261, align 4
  %502 = load i32, i32* %265, align 4
  %503 = mul nsw i32 %501, %419
  %504 = sext i32 %503 to i64
  %505 = mul nsw i32 %502, %418
  %506 = sext i32 %505 to i64
  %507 = add i64 %428, %506
  %508 = add i64 %507, %504
  %509 = ashr i64 %508, %431
  %510 = trunc i64 %509 to i32
  %511 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 47
  store i32 %510, i32* %511, align 4
  %512 = mul nsw i32 %501, %418
  %513 = sext i32 %512 to i64
  %514 = add i64 %507, %513
  %515 = ashr i64 %514, %431
  %516 = trunc i64 %515 to i32
  %517 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 48
  store i32 %516, i32* %517, align 16
  %518 = mul nsw i32 %490, %418
  %519 = sext i32 %518 to i64
  %520 = add i64 %496, %519
  %521 = ashr i64 %520, %431
  %522 = trunc i64 %521 to i32
  %523 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 49
  store i32 %522, i32* %523, align 4
  %524 = mul nsw i32 %479, %418
  %525 = sext i32 %524 to i64
  %526 = add i64 %485, %525
  %527 = ashr i64 %526, %431
  %528 = trunc i64 %527 to i32
  %529 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 50
  store i32 %528, i32* %529, align 8
  %530 = mul nsw i32 %468, %418
  %531 = sext i32 %530 to i64
  %532 = add i64 %474, %531
  %533 = ashr i64 %532, %431
  %534 = trunc i64 %533 to i32
  %535 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 51
  store i32 %534, i32* %535, align 4
  %536 = mul nsw i32 %457, %418
  %537 = sext i32 %536 to i64
  %538 = add i64 %463, %537
  %539 = ashr i64 %538, %431
  %540 = trunc i64 %539 to i32
  %541 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 52
  store i32 %540, i32* %541, align 16
  %542 = mul nsw i32 %446, %418
  %543 = sext i32 %542 to i64
  %544 = add i64 %452, %543
  %545 = ashr i64 %544, %431
  %546 = trunc i64 %545 to i32
  %547 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 53
  store i32 %546, i32* %547, align 4
  %548 = mul nsw i32 %435, %418
  %549 = sext i32 %548 to i64
  %550 = add i64 %441, %549
  %551 = ashr i64 %550, %431
  %552 = trunc i64 %551 to i32
  %553 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 54
  store i32 %552, i32* %553, align 8
  %554 = mul nsw i32 %420, %418
  %555 = sext i32 %554 to i64
  %556 = add i64 %429, %555
  %557 = ashr i64 %556, %431
  %558 = trunc i64 %557 to i32
  %559 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 55
  store i32 %558, i32* %559, align 4
  %560 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 56
  %561 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 57
  %562 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 58
  %563 = bitcast i32* %297 to <4 x i32>*
  %564 = load <4 x i32>, <4 x i32>* %563, align 4
  %565 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 59
  %566 = bitcast i32* %560 to <4 x i32>*
  store <4 x i32> %564, <4 x i32>* %566, align 16
  %567 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 60
  %568 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 61
  %569 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 62
  %570 = bitcast i32* %313 to <4 x i32>*
  %571 = load <4 x i32>, <4 x i32>* %570, align 4
  %572 = getelementptr inbounds [64 x i32], [64 x i32]* %5, i64 0, i64 63
  %573 = bitcast i32* %567 to <4 x i32>*
  store <4 x i32> %571, <4 x i32>* %573, align 16
  %574 = getelementptr inbounds i8, i8* %3, i64 2
  %575 = load i8, i8* %574, align 1
  call void @av1_range_check_buf(i32 2, i32* %0, i32* nonnull %331, i32 64, i8 signext %575) #3
  %576 = bitcast [64 x i32]* %5 to <4 x i32>*
  %577 = load <4 x i32>, <4 x i32>* %576, align 16
  %578 = bitcast i32* %364 to <4 x i32>*
  %579 = load <4 x i32>, <4 x i32>* %578, align 16
  %580 = shufflevector <4 x i32> %579, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %581 = add nsw <4 x i32> %580, %577
  %582 = bitcast i32* %1 to <4 x i32>*
  store <4 x i32> %581, <4 x i32>* %582, align 4
  %583 = bitcast i32* %342 to <4 x i32>*
  %584 = load <4 x i32>, <4 x i32>* %583, align 16
  %585 = bitcast i32* %353 to <4 x i32>*
  %586 = load <4 x i32>, <4 x i32>* %585, align 16
  %587 = shufflevector <4 x i32> %586, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %588 = add nsw <4 x i32> %587, %584
  %589 = bitcast i32* %35 to <4 x i32>*
  store <4 x i32> %588, <4 x i32>* %589, align 4
  %590 = shufflevector <4 x i32> %584, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %591 = sub <4 x i32> %590, %586
  %592 = bitcast i32* %59 to <4 x i32>*
  store <4 x i32> %591, <4 x i32>* %592, align 4
  %593 = shufflevector <4 x i32> %577, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %594 = sub <4 x i32> %593, %579
  %595 = bitcast i32* %83 to <4 x i32>*
  store <4 x i32> %594, <4 x i32>* %595, align 4
  %596 = bitcast i32* %375 to <4 x i32>*
  %597 = load <4 x i32>, <4 x i32>* %596, align 16
  %598 = bitcast i32* %107 to <4 x i32>*
  store <4 x i32> %597, <4 x i32>* %598, align 4
  %599 = load i32, i32* %382, align 16
  %600 = load i32, i32* %394, align 4
  %601 = mul nsw i32 %599, %419
  %602 = sext i32 %601 to i64
  %603 = mul nsw i32 %600, %418
  %604 = sext i32 %603 to i64
  %605 = add i64 %428, %604
  %606 = add i64 %605, %602
  %607 = ashr i64 %606, %431
  %608 = trunc i64 %607 to i32
  store i32 %608, i32* %131, align 4
  %609 = load i32, i32* %383, align 4
  %610 = load i32, i32* %391, align 8
  %611 = mul nsw i32 %609, %419
  %612 = sext i32 %611 to i64
  %613 = mul nsw i32 %610, %418
  %614 = sext i32 %613 to i64
  %615 = add i64 %428, %614
  %616 = add i64 %615, %612
  %617 = ashr i64 %616, %431
  %618 = trunc i64 %617 to i32
  store i32 %618, i32* %137, align 4
  %619 = load i32, i32* %384, align 8
  %620 = load i32, i32* %390, align 4
  %621 = mul nsw i32 %619, %419
  %622 = sext i32 %621 to i64
  %623 = mul nsw i32 %620, %418
  %624 = sext i32 %623 to i64
  %625 = add i64 %428, %624
  %626 = add i64 %625, %622
  %627 = ashr i64 %626, %431
  %628 = trunc i64 %627 to i32
  store i32 %628, i32* %143, align 4
  %629 = load i32, i32* %387, align 4
  %630 = load i32, i32* %389, align 16
  %631 = mul nsw i32 %629, %419
  %632 = sext i32 %631 to i64
  %633 = mul nsw i32 %630, %418
  %634 = sext i32 %633 to i64
  %635 = add i64 %428, %634
  %636 = add i64 %635, %632
  %637 = ashr i64 %636, %431
  %638 = trunc i64 %637 to i32
  store i32 %638, i32* %149, align 4
  %639 = mul nsw i32 %629, %418
  %640 = sext i32 %639 to i64
  %641 = add i64 %635, %640
  %642 = ashr i64 %641, %431
  %643 = trunc i64 %642 to i32
  store i32 %643, i32* %155, align 4
  %644 = mul nsw i32 %619, %418
  %645 = sext i32 %644 to i64
  %646 = add i64 %625, %645
  %647 = ashr i64 %646, %431
  %648 = trunc i64 %647 to i32
  store i32 %648, i32* %161, align 4
  %649 = mul nsw i32 %609, %418
  %650 = sext i32 %649 to i64
  %651 = add i64 %615, %650
  %652 = ashr i64 %651, %431
  %653 = trunc i64 %652 to i32
  store i32 %653, i32* %167, align 4
  %654 = mul nsw i32 %599, %418
  %655 = sext i32 %654 to i64
  %656 = add i64 %605, %655
  %657 = ashr i64 %656, %431
  %658 = trunc i64 %657 to i32
  store i32 %658, i32* %173, align 4
  %659 = bitcast i32* %396 to <4 x i32>*
  %660 = load <4 x i32>, <4 x i32>* %659, align 16
  %661 = bitcast i32* %179 to <4 x i32>*
  store <4 x i32> %660, <4 x i32>* %661, align 4
  %662 = bitcast i32* %403 to <4 x i32>*
  %663 = load <4 x i32>, <4 x i32>* %662, align 16
  %664 = bitcast i32* %478 to <4 x i32>*
  %665 = load <4 x i32>, <4 x i32>* %664, align 16
  %666 = shufflevector <4 x i32> %665, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %667 = add nsw <4 x i32> %666, %663
  %668 = bitcast i32* %201 to <4 x i32>*
  store <4 x i32> %667, <4 x i32>* %668, align 4
  %669 = bitcast i32* %410 to <4 x i32>*
  %670 = load <4 x i32>, <4 x i32>* %669, align 16
  %671 = bitcast i32* %434 to <4 x i32>*
  %672 = load <4 x i32>, <4 x i32>* %671, align 16
  %673 = shufflevector <4 x i32> %672, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %674 = add nsw <4 x i32> %673, %670
  %675 = bitcast i32* %217 to <4 x i32>*
  store <4 x i32> %674, <4 x i32>* %675, align 4
  %676 = shufflevector <4 x i32> %670, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %677 = sub <4 x i32> %676, %672
  %678 = bitcast i32* %233 to <4 x i32>*
  store <4 x i32> %677, <4 x i32>* %678, align 4
  %679 = shufflevector <4 x i32> %663, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %680 = sub <4 x i32> %679, %665
  %681 = bitcast i32* %249 to <4 x i32>*
  store <4 x i32> %680, <4 x i32>* %681, align 4
  %682 = bitcast i32* %517 to <4 x i32>*
  %683 = load <4 x i32>, <4 x i32>* %682, align 16
  %684 = bitcast i32* %567 to <4 x i32>*
  %685 = load <4 x i32>, <4 x i32>* %684, align 16
  %686 = shufflevector <4 x i32> %685, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %687 = sub <4 x i32> %686, %683
  %688 = bitcast i32* %265 to <4 x i32>*
  store <4 x i32> %687, <4 x i32>* %688, align 4
  %689 = bitcast i32* %541 to <4 x i32>*
  %690 = load <4 x i32>, <4 x i32>* %689, align 16
  %691 = bitcast i32* %560 to <4 x i32>*
  %692 = load <4 x i32>, <4 x i32>* %691, align 16
  %693 = shufflevector <4 x i32> %692, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %694 = sub <4 x i32> %693, %690
  %695 = bitcast i32* %281 to <4 x i32>*
  store <4 x i32> %694, <4 x i32>* %695, align 4
  %696 = shufflevector <4 x i32> %690, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %697 = add nsw <4 x i32> %692, %696
  %698 = bitcast i32* %297 to <4 x i32>*
  store <4 x i32> %697, <4 x i32>* %698, align 4
  %699 = shufflevector <4 x i32> %683, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %700 = add nsw <4 x i32> %685, %699
  %701 = bitcast i32* %313 to <4 x i32>*
  store <4 x i32> %700, <4 x i32>* %701, align 4
  %702 = getelementptr inbounds i8, i8* %3, i64 3
  %703 = load i8, i8* %702, align 1
  call void @av1_range_check_buf(i32 3, i32* %0, i32* %1, i32 64, i8 signext %703) #3
  %704 = bitcast i32* %1 to <4 x i32>*
  %705 = load <4 x i32>, <4 x i32>* %704, align 4
  %706 = bitcast i32* %35 to <4 x i32>*
  %707 = load <4 x i32>, <4 x i32>* %706, align 4
  %708 = shufflevector <4 x i32> %707, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %709 = add nsw <4 x i32> %708, %705
  %710 = bitcast [64 x i32]* %5 to <4 x i32>*
  store <4 x i32> %709, <4 x i32>* %710, align 16
  %711 = shufflevector <4 x i32> %705, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %712 = sub <4 x i32> %711, %707
  %713 = bitcast i32* %342 to <4 x i32>*
  store <4 x i32> %712, <4 x i32>* %713, align 16
  %714 = load i32, i32* %59, align 4
  store i32 %714, i32* %353, align 16
  %715 = load i32, i32* %65, align 4
  store i32 %715, i32* %354, align 4
  %716 = load i32, i32* %71, align 4
  %717 = load i32, i32* %89, align 4
  %718 = mul nsw i32 %716, %419
  %719 = sext i32 %718 to i64
  %720 = mul nsw i32 %717, %418
  %721 = sext i32 %720 to i64
  %722 = add i64 %428, %721
  %723 = add i64 %722, %719
  %724 = ashr i64 %723, %431
  %725 = trunc i64 %724 to i32
  store i32 %725, i32* %355, align 8
  %726 = load i32, i32* %77, align 4
  %727 = load i32, i32* %83, align 4
  %728 = mul nsw i32 %726, %419
  %729 = sext i32 %728 to i64
  %730 = mul nsw i32 %727, %418
  %731 = sext i32 %730 to i64
  %732 = add i64 %428, %731
  %733 = add i64 %732, %729
  %734 = ashr i64 %733, %431
  %735 = trunc i64 %734 to i32
  store i32 %735, i32* %362, align 4
  %736 = mul nsw i32 %726, %418
  %737 = sext i32 %736 to i64
  %738 = add i64 %732, %737
  %739 = ashr i64 %738, %431
  %740 = trunc i64 %739 to i32
  store i32 %740, i32* %364, align 16
  %741 = mul nsw i32 %716, %418
  %742 = sext i32 %741 to i64
  %743 = add i64 %722, %742
  %744 = ashr i64 %743, %431
  %745 = trunc i64 %744 to i32
  store i32 %745, i32* %365, align 4
  %746 = load i32, i32* %95, align 4
  store i32 %746, i32* %366, align 8
  %747 = load i32, i32* %101, align 4
  store i32 %747, i32* %373, align 4
  %748 = bitcast i32* %107 to <4 x i32>*
  %749 = load <4 x i32>, <4 x i32>* %748, align 4
  %750 = bitcast i32* %131 to <4 x i32>*
  %751 = load <4 x i32>, <4 x i32>* %750, align 4
  %752 = shufflevector <4 x i32> %751, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %753 = add nsw <4 x i32> %752, %749
  %754 = bitcast i32* %375 to <4 x i32>*
  store <4 x i32> %753, <4 x i32>* %754, align 16
  %755 = shufflevector <4 x i32> %749, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %756 = sub <4 x i32> %755, %751
  %757 = bitcast i32* %382 to <4 x i32>*
  store <4 x i32> %756, <4 x i32>* %757, align 16
  %758 = bitcast i32* %155 to <4 x i32>*
  %759 = load <4 x i32>, <4 x i32>* %758, align 4
  %760 = bitcast i32* %179 to <4 x i32>*
  %761 = load <4 x i32>, <4 x i32>* %760, align 4
  %762 = shufflevector <4 x i32> %761, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %763 = sub <4 x i32> %762, %759
  %764 = bitcast i32* %389 to <4 x i32>*
  store <4 x i32> %763, <4 x i32>* %764, align 16
  %765 = shufflevector <4 x i32> %759, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %766 = add nsw <4 x i32> %761, %765
  %767 = bitcast i32* %396 to <4 x i32>*
  store <4 x i32> %766, <4 x i32>* %767, align 16
  %768 = bitcast i32* %201 to <4 x i32>*
  %769 = load <4 x i32>, <4 x i32>* %768, align 4
  %770 = bitcast i32* %403 to <4 x i32>*
  store <4 x i32> %769, <4 x i32>* %770, align 16
  %771 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 16
  %772 = load i32, i32* %771, align 16
  %773 = sub nsw i32 0, %772
  %774 = load i32, i32* %217, align 4
  %775 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 48
  %776 = load i32, i32* %775, align 16
  %777 = load i32, i32* %309, align 4
  %778 = mul nsw i32 %774, %773
  %779 = sext i32 %778 to i64
  %780 = mul nsw i32 %777, %776
  %781 = sext i32 %780 to i64
  %782 = add i64 %428, %779
  %783 = add i64 %782, %781
  %784 = ashr i64 %783, %431
  %785 = trunc i64 %784 to i32
  store i32 %785, i32* %410, align 16
  %786 = load i32, i32* %221, align 4
  %787 = load i32, i32* %305, align 4
  %788 = mul nsw i32 %786, %773
  %789 = sext i32 %788 to i64
  %790 = mul nsw i32 %787, %776
  %791 = sext i32 %790 to i64
  %792 = add i64 %428, %789
  %793 = add i64 %792, %791
  %794 = ashr i64 %793, %431
  %795 = trunc i64 %794 to i32
  store i32 %795, i32* %411, align 4
  %796 = load i32, i32* %225, align 4
  %797 = load i32, i32* %301, align 4
  %798 = mul nsw i32 %796, %773
  %799 = sext i32 %798 to i64
  %800 = mul nsw i32 %797, %776
  %801 = sext i32 %800 to i64
  %802 = add i64 %428, %799
  %803 = add i64 %802, %801
  %804 = ashr i64 %803, %431
  %805 = trunc i64 %804 to i32
  store i32 %805, i32* %412, align 8
  %806 = load i32, i32* %229, align 4
  %807 = load i32, i32* %297, align 4
  %808 = mul nsw i32 %806, %773
  %809 = sext i32 %808 to i64
  %810 = mul nsw i32 %807, %776
  %811 = sext i32 %810 to i64
  %812 = add i64 %428, %809
  %813 = add i64 %812, %811
  %814 = ashr i64 %813, %431
  %815 = trunc i64 %814 to i32
  store i32 %815, i32* %415, align 4
  %816 = sub nsw i32 0, %776
  %817 = load i32, i32* %233, align 4
  %818 = load i32, i32* %293, align 4
  %819 = mul nsw i32 %817, %816
  %820 = sext i32 %819 to i64
  %821 = mul nsw i32 %818, %773
  %822 = sext i32 %821 to i64
  %823 = add i64 %428, %820
  %824 = add i64 %823, %822
  %825 = ashr i64 %824, %431
  %826 = trunc i64 %825 to i32
  store i32 %826, i32* %434, align 16
  %827 = load i32, i32* %237, align 4
  %828 = load i32, i32* %289, align 4
  %829 = mul nsw i32 %827, %816
  %830 = sext i32 %829 to i64
  %831 = mul nsw i32 %828, %773
  %832 = sext i32 %831 to i64
  %833 = add i64 %428, %830
  %834 = add i64 %833, %832
  %835 = ashr i64 %834, %431
  %836 = trunc i64 %835 to i32
  store i32 %836, i32* %445, align 4
  %837 = load i32, i32* %241, align 4
  %838 = load i32, i32* %285, align 4
  %839 = mul nsw i32 %837, %816
  %840 = sext i32 %839 to i64
  %841 = mul nsw i32 %838, %773
  %842 = sext i32 %841 to i64
  %843 = add i64 %428, %840
  %844 = add i64 %843, %842
  %845 = ashr i64 %844, %431
  %846 = trunc i64 %845 to i32
  store i32 %846, i32* %456, align 8
  %847 = load i32, i32* %245, align 4
  %848 = load i32, i32* %281, align 4
  %849 = mul nsw i32 %847, %816
  %850 = sext i32 %849 to i64
  %851 = mul nsw i32 %848, %773
  %852 = sext i32 %851 to i64
  %853 = add i64 %428, %850
  %854 = add i64 %853, %852
  %855 = ashr i64 %854, %431
  %856 = trunc i64 %855 to i32
  store i32 %856, i32* %467, align 4
  %857 = bitcast i32* %249 to <4 x i32>*
  %858 = load <4 x i32>, <4 x i32>* %857, align 4
  %859 = bitcast i32* %478 to <4 x i32>*
  store <4 x i32> %858, <4 x i32>* %859, align 16
  %860 = bitcast i32* %265 to <4 x i32>*
  %861 = load <4 x i32>, <4 x i32>* %860, align 4
  %862 = bitcast i32* %517 to <4 x i32>*
  store <4 x i32> %861, <4 x i32>* %862, align 16
  %863 = mul nsw i32 %848, %776
  %864 = sext i32 %863 to i64
  %865 = mul nsw i32 %847, %773
  %866 = sext i32 %865 to i64
  %867 = add i64 %428, %866
  %868 = add i64 %867, %864
  %869 = ashr i64 %868, %431
  %870 = trunc i64 %869 to i32
  store i32 %870, i32* %541, align 16
  %871 = mul nsw i32 %838, %776
  %872 = sext i32 %871 to i64
  %873 = mul nsw i32 %837, %773
  %874 = sext i32 %873 to i64
  %875 = add i64 %428, %874
  %876 = add i64 %875, %872
  %877 = ashr i64 %876, %431
  %878 = trunc i64 %877 to i32
  store i32 %878, i32* %547, align 4
  %879 = mul nsw i32 %828, %776
  %880 = sext i32 %879 to i64
  %881 = mul nsw i32 %827, %773
  %882 = sext i32 %881 to i64
  %883 = add i64 %428, %882
  %884 = add i64 %883, %880
  %885 = ashr i64 %884, %431
  %886 = trunc i64 %885 to i32
  store i32 %886, i32* %553, align 8
  %887 = mul nsw i32 %818, %776
  %888 = sext i32 %887 to i64
  %889 = mul nsw i32 %817, %773
  %890 = sext i32 %889 to i64
  %891 = add i64 %428, %890
  %892 = add i64 %891, %888
  %893 = ashr i64 %892, %431
  %894 = trunc i64 %893 to i32
  store i32 %894, i32* %559, align 4
  %895 = mul nsw i32 %807, %772
  %896 = sext i32 %895 to i64
  %897 = mul nsw i32 %806, %776
  %898 = sext i32 %897 to i64
  %899 = add i64 %428, %898
  %900 = add i64 %899, %896
  %901 = ashr i64 %900, %431
  %902 = trunc i64 %901 to i32
  store i32 %902, i32* %560, align 16
  %903 = mul nsw i32 %797, %772
  %904 = sext i32 %903 to i64
  %905 = mul nsw i32 %796, %776
  %906 = sext i32 %905 to i64
  %907 = add i64 %428, %906
  %908 = add i64 %907, %904
  %909 = ashr i64 %908, %431
  %910 = trunc i64 %909 to i32
  store i32 %910, i32* %561, align 4
  %911 = mul nsw i32 %787, %772
  %912 = sext i32 %911 to i64
  %913 = mul nsw i32 %786, %776
  %914 = sext i32 %913 to i64
  %915 = add i64 %428, %914
  %916 = add i64 %915, %912
  %917 = ashr i64 %916, %431
  %918 = trunc i64 %917 to i32
  store i32 %918, i32* %562, align 8
  %919 = mul nsw i32 %777, %772
  %920 = sext i32 %919 to i64
  %921 = mul nsw i32 %776, %774
  %922 = sext i32 %921 to i64
  %923 = add i64 %428, %922
  %924 = add i64 %923, %920
  %925 = ashr i64 %924, %431
  %926 = trunc i64 %925 to i32
  store i32 %926, i32* %565, align 4
  %927 = bitcast i32* %313 to <4 x i32>*
  %928 = load <4 x i32>, <4 x i32>* %927, align 4
  %929 = bitcast i32* %567 to <4 x i32>*
  store <4 x i32> %928, <4 x i32>* %929, align 16
  %930 = getelementptr inbounds i8, i8* %3, i64 4
  %931 = load i8, i8* %930, align 1
  call void @av1_range_check_buf(i32 4, i32* %0, i32* nonnull %331, i32 64, i8 signext %931) #3
  %932 = bitcast [64 x i32]* %5 to <4 x i32>*
  %933 = load <4 x i32>, <4 x i32>* %932, align 16
  %934 = shufflevector <4 x i32> %933, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %935 = add nsw <4 x i32> %934, %933
  %936 = sub <4 x i32> %934, %933
  %937 = shufflevector <4 x i32> %935, <4 x i32> %936, <4 x i32> <i32 0, i32 1, i32 6, i32 7>
  %938 = bitcast i32* %1 to <4 x i32>*
  store <4 x i32> %937, <4 x i32>* %938, align 4
  %939 = load i32, i32* %342, align 16
  store i32 %939, i32* %35, align 4
  %940 = load i32, i32* %343, align 4
  %941 = load i32, i32* %344, align 8
  %942 = mul nsw i32 %940, %419
  %943 = sext i32 %942 to i64
  %944 = mul nsw i32 %941, %418
  %945 = sext i32 %944 to i64
  %946 = add i64 %428, %945
  %947 = add i64 %946, %943
  %948 = ashr i64 %947, %431
  %949 = trunc i64 %948 to i32
  store i32 %949, i32* %41, align 4
  %950 = mul nsw i32 %940, %418
  %951 = sext i32 %950 to i64
  %952 = add i64 %946, %951
  %953 = ashr i64 %952, %431
  %954 = trunc i64 %953 to i32
  store i32 %954, i32* %47, align 4
  %955 = load i32, i32* %351, align 4
  store i32 %955, i32* %53, align 4
  %956 = bitcast i32* %353 to <4 x i32>*
  %957 = load <4 x i32>, <4 x i32>* %956, align 16
  %958 = shufflevector <4 x i32> %957, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %959 = add nsw <4 x i32> %958, %957
  %960 = sub <4 x i32> %958, %957
  %961 = shufflevector <4 x i32> %959, <4 x i32> %960, <4 x i32> <i32 0, i32 1, i32 6, i32 7>
  %962 = bitcast i32* %59 to <4 x i32>*
  store <4 x i32> %961, <4 x i32>* %962, align 4
  %963 = bitcast i32* %364 to <4 x i32>*
  %964 = load <4 x i32>, <4 x i32>* %963, align 16
  %965 = shufflevector <4 x i32> %964, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %966 = sub <4 x i32> %965, %964
  %967 = add nsw <4 x i32> %965, %964
  %968 = shufflevector <4 x i32> %966, <4 x i32> %967, <4 x i32> <i32 0, i32 1, i32 6, i32 7>
  %969 = bitcast i32* %83 to <4 x i32>*
  store <4 x i32> %968, <4 x i32>* %969, align 4
  %970 = load i32, i32* %375, align 16
  store i32 %970, i32* %107, align 4
  %971 = load i32, i32* %376, align 4
  store i32 %971, i32* %113, align 4
  %972 = load i32, i32* %377, align 8
  %973 = load i32, i32* %397, align 4
  %974 = mul nsw i32 %972, %773
  %975 = sext i32 %974 to i64
  %976 = mul nsw i32 %973, %776
  %977 = sext i32 %976 to i64
  %978 = add i64 %428, %975
  %979 = add i64 %978, %977
  %980 = ashr i64 %979, %431
  %981 = trunc i64 %980 to i32
  store i32 %981, i32* %119, align 4
  %982 = load i32, i32* %380, align 4
  %983 = load i32, i32* %396, align 16
  %984 = mul nsw i32 %982, %773
  %985 = sext i32 %984 to i64
  %986 = mul nsw i32 %983, %776
  %987 = sext i32 %986 to i64
  %988 = add i64 %428, %985
  %989 = add i64 %988, %987
  %990 = ashr i64 %989, %431
  %991 = trunc i64 %990 to i32
  store i32 %991, i32* %125, align 4
  %992 = load i32, i32* %382, align 16
  %993 = load i32, i32* %394, align 4
  %994 = mul nsw i32 %992, %816
  %995 = sext i32 %994 to i64
  %996 = mul nsw i32 %993, %773
  %997 = sext i32 %996 to i64
  %998 = add i64 %428, %995
  %999 = add i64 %998, %997
  %1000 = ashr i64 %999, %431
  %1001 = trunc i64 %1000 to i32
  store i32 %1001, i32* %131, align 4
  %1002 = load i32, i32* %383, align 4
  %1003 = load i32, i32* %391, align 8
  %1004 = mul nsw i32 %1002, %816
  %1005 = sext i32 %1004 to i64
  %1006 = mul nsw i32 %1003, %773
  %1007 = sext i32 %1006 to i64
  %1008 = add i64 %428, %1005
  %1009 = add i64 %1008, %1007
  %1010 = ashr i64 %1009, %431
  %1011 = trunc i64 %1010 to i32
  store i32 %1011, i32* %137, align 4
  %1012 = bitcast i32* %384 to <4 x i32>*
  %1013 = load <4 x i32>, <4 x i32>* %1012, align 8
  %1014 = bitcast i32* %143 to <4 x i32>*
  store <4 x i32> %1013, <4 x i32>* %1014, align 4
  %1015 = mul nsw i32 %1003, %776
  %1016 = sext i32 %1015 to i64
  %1017 = mul nsw i32 %1002, %773
  %1018 = sext i32 %1017 to i64
  %1019 = add i64 %428, %1018
  %1020 = add i64 %1019, %1016
  %1021 = ashr i64 %1020, %431
  %1022 = trunc i64 %1021 to i32
  store i32 %1022, i32* %167, align 4
  %1023 = mul nsw i32 %993, %776
  %1024 = sext i32 %1023 to i64
  %1025 = mul nsw i32 %992, %773
  %1026 = sext i32 %1025 to i64
  %1027 = add i64 %428, %1024
  %1028 = add i64 %1027, %1026
  %1029 = ashr i64 %1028, %431
  %1030 = trunc i64 %1029 to i32
  store i32 %1030, i32* %173, align 4
  %1031 = mul nsw i32 %983, %772
  %1032 = sext i32 %1031 to i64
  %1033 = mul nsw i32 %982, %776
  %1034 = sext i32 %1033 to i64
  %1035 = add i64 %428, %1032
  %1036 = add i64 %1035, %1034
  %1037 = ashr i64 %1036, %431
  %1038 = trunc i64 %1037 to i32
  store i32 %1038, i32* %179, align 4
  %1039 = mul nsw i32 %973, %772
  %1040 = sext i32 %1039 to i64
  %1041 = mul nsw i32 %972, %776
  %1042 = sext i32 %1041 to i64
  %1043 = add i64 %428, %1040
  %1044 = add i64 %1043, %1042
  %1045 = ashr i64 %1044, %431
  %1046 = trunc i64 %1045 to i32
  store i32 %1046, i32* %185, align 4
  %1047 = load i32, i32* %398, align 8
  store i32 %1047, i32* %191, align 4
  %1048 = load i32, i32* %401, align 4
  store i32 %1048, i32* %197, align 4
  %1049 = bitcast i32* %403 to <4 x i32>*
  %1050 = load <4 x i32>, <4 x i32>* %1049, align 16
  %1051 = bitcast i32* %410 to <4 x i32>*
  %1052 = load <4 x i32>, <4 x i32>* %1051, align 16
  %1053 = shufflevector <4 x i32> %1052, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %1054 = add nsw <4 x i32> %1053, %1050
  %1055 = bitcast i32* %201 to <4 x i32>*
  store <4 x i32> %1054, <4 x i32>* %1055, align 4
  %1056 = shufflevector <4 x i32> %1050, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %1057 = sub <4 x i32> %1056, %1052
  %1058 = bitcast i32* %217 to <4 x i32>*
  store <4 x i32> %1057, <4 x i32>* %1058, align 4
  %1059 = bitcast i32* %434 to <4 x i32>*
  %1060 = load <4 x i32>, <4 x i32>* %1059, align 16
  %1061 = bitcast i32* %478 to <4 x i32>*
  %1062 = load <4 x i32>, <4 x i32>* %1061, align 16
  %1063 = shufflevector <4 x i32> %1062, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %1064 = sub <4 x i32> %1063, %1060
  %1065 = bitcast i32* %233 to <4 x i32>*
  store <4 x i32> %1064, <4 x i32>* %1065, align 4
  %1066 = shufflevector <4 x i32> %1062, <4 x i32> %1060, <4 x i32> <i32 0, i32 6, i32 5, i32 4>
  %1067 = shufflevector <4 x i32> %1060, <4 x i32> %1062, <4 x i32> <i32 3, i32 5, i32 6, i32 7>
  %1068 = add nsw <4 x i32> %1066, %1067
  %1069 = bitcast i32* %249 to <4 x i32>*
  store <4 x i32> %1068, <4 x i32>* %1069, align 4
  %1070 = bitcast i32* %517 to <4 x i32>*
  %1071 = load <4 x i32>, <4 x i32>* %1070, align 16
  %1072 = bitcast i32* %541 to <4 x i32>*
  %1073 = load <4 x i32>, <4 x i32>* %1072, align 16
  %1074 = shufflevector <4 x i32> %1073, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %1075 = add nsw <4 x i32> %1074, %1071
  %1076 = bitcast i32* %265 to <4 x i32>*
  store <4 x i32> %1075, <4 x i32>* %1076, align 4
  %1077 = shufflevector <4 x i32> %1071, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %1078 = sub <4 x i32> %1077, %1073
  %1079 = bitcast i32* %281 to <4 x i32>*
  store <4 x i32> %1078, <4 x i32>* %1079, align 4
  %1080 = bitcast i32* %560 to <4 x i32>*
  %1081 = load <4 x i32>, <4 x i32>* %1080, align 16
  %1082 = bitcast i32* %567 to <4 x i32>*
  %1083 = load <4 x i32>, <4 x i32>* %1082, align 16
  %1084 = shufflevector <4 x i32> %1083, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %1085 = sub <4 x i32> %1084, %1081
  %1086 = bitcast i32* %297 to <4 x i32>*
  store <4 x i32> %1085, <4 x i32>* %1086, align 4
  %1087 = shufflevector <4 x i32> %1083, <4 x i32> %1081, <4 x i32> <i32 0, i32 6, i32 5, i32 4>
  %1088 = shufflevector <4 x i32> %1081, <4 x i32> %1083, <4 x i32> <i32 3, i32 5, i32 6, i32 7>
  %1089 = add nsw <4 x i32> %1087, %1088
  %1090 = bitcast i32* %313 to <4 x i32>*
  store <4 x i32> %1089, <4 x i32>* %1090, align 4
  %1091 = getelementptr inbounds i8, i8* %3, i64 5
  %1092 = load i8, i8* %1091, align 1
  call void @av1_range_check_buf(i32 5, i32* %0, i32* %1, i32 64, i8 signext %1092) #3
  %1093 = load i32, i32* %1, align 4
  %1094 = load i32, i32* %17, align 4
  %1095 = mul nsw i32 %1093, %418
  %1096 = sext i32 %1095 to i64
  %1097 = mul nsw i32 %1094, %418
  %1098 = sext i32 %1097 to i64
  %1099 = add i64 %428, %1096
  %1100 = add i64 %1099, %1098
  %1101 = ashr i64 %1100, %431
  %1102 = trunc i64 %1101 to i32
  store i32 %1102, i32* %331, align 16
  %1103 = mul nsw i32 %1094, %419
  %1104 = sext i32 %1103 to i64
  %1105 = add i64 %428, %1104
  %1106 = add i64 %1105, %1096
  %1107 = ashr i64 %1106, %431
  %1108 = trunc i64 %1107 to i32
  store i32 %1108, i32* %332, align 4
  %1109 = load i32, i32* %23, align 4
  %1110 = load i32, i32* %29, align 4
  %1111 = mul nsw i32 %1109, %776
  %1112 = sext i32 %1111 to i64
  %1113 = mul nsw i32 %1110, %772
  %1114 = sext i32 %1113 to i64
  %1115 = add i64 %428, %1112
  %1116 = add i64 %1115, %1114
  %1117 = ashr i64 %1116, %431
  %1118 = trunc i64 %1117 to i32
  store i32 %1118, i32* %333, align 8
  %1119 = mul nsw i32 %1110, %776
  %1120 = sext i32 %1119 to i64
  %1121 = mul nsw i32 %1109, %773
  %1122 = sext i32 %1121 to i64
  %1123 = add i64 %428, %1120
  %1124 = add i64 %1123, %1122
  %1125 = ashr i64 %1124, %431
  %1126 = trunc i64 %1125 to i32
  store i32 %1126, i32* %340, align 4
  %1127 = bitcast i32* %35 to <4 x i32>*
  %1128 = load <4 x i32>, <4 x i32>* %1127, align 4
  %1129 = shufflevector <4 x i32> %1128, <4 x i32> undef, <4 x i32> <i32 1, i32 0, i32 3, i32 2>
  %1130 = add nsw <4 x i32> %1129, %1128
  %1131 = sub <4 x i32> %1129, %1128
  %1132 = shufflevector <4 x i32> %1130, <4 x i32> %1131, <4 x i32> <i32 0, i32 5, i32 6, i32 3>
  %1133 = bitcast i32* %342 to <4 x i32>*
  store <4 x i32> %1132, <4 x i32>* %1133, align 16
  %1134 = load i32, i32* %59, align 4
  store i32 %1134, i32* %353, align 16
  %1135 = load i32, i32* %65, align 4
  %1136 = load i32, i32* %95, align 4
  %1137 = mul nsw i32 %1135, %773
  %1138 = sext i32 %1137 to i64
  %1139 = mul nsw i32 %1136, %776
  %1140 = sext i32 %1139 to i64
  %1141 = add i64 %428, %1138
  %1142 = add i64 %1141, %1140
  %1143 = ashr i64 %1142, %431
  %1144 = trunc i64 %1143 to i32
  store i32 %1144, i32* %354, align 4
  %1145 = load i32, i32* %71, align 4
  %1146 = load i32, i32* %89, align 4
  %1147 = mul nsw i32 %1145, %816
  %1148 = sext i32 %1147 to i64
  %1149 = mul nsw i32 %1146, %773
  %1150 = sext i32 %1149 to i64
  %1151 = add i64 %428, %1148
  %1152 = add i64 %1151, %1150
  %1153 = ashr i64 %1152, %431
  %1154 = trunc i64 %1153 to i32
  store i32 %1154, i32* %355, align 8
  %1155 = load i32, i32* %77, align 4
  store i32 %1155, i32* %362, align 4
  %1156 = load i32, i32* %83, align 4
  store i32 %1156, i32* %364, align 16
  %1157 = mul nsw i32 %1146, %776
  %1158 = sext i32 %1157 to i64
  %1159 = mul nsw i32 %1145, %773
  %1160 = sext i32 %1159 to i64
  %1161 = add i64 %428, %1158
  %1162 = add i64 %1161, %1160
  %1163 = ashr i64 %1162, %431
  %1164 = trunc i64 %1163 to i32
  store i32 %1164, i32* %365, align 4
  %1165 = mul nsw i32 %1136, %772
  %1166 = sext i32 %1165 to i64
  %1167 = mul nsw i32 %1135, %776
  %1168 = sext i32 %1167 to i64
  %1169 = add i64 %428, %1166
  %1170 = add i64 %1169, %1168
  %1171 = ashr i64 %1170, %431
  %1172 = trunc i64 %1171 to i32
  store i32 %1172, i32* %366, align 8
  %1173 = load i32, i32* %101, align 4
  store i32 %1173, i32* %373, align 4
  %1174 = bitcast i32* %107 to <4 x i32>*
  %1175 = load <4 x i32>, <4 x i32>* %1174, align 4
  %1176 = shufflevector <4 x i32> %1175, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %1177 = add nsw <4 x i32> %1176, %1175
  %1178 = sub <4 x i32> %1176, %1175
  %1179 = shufflevector <4 x i32> %1177, <4 x i32> %1178, <4 x i32> <i32 0, i32 1, i32 6, i32 7>
  %1180 = bitcast i32* %375 to <4 x i32>*
  store <4 x i32> %1179, <4 x i32>* %1180, align 16
  %1181 = bitcast i32* %131 to <4 x i32>*
  %1182 = load <4 x i32>, <4 x i32>* %1181, align 4
  %1183 = shufflevector <4 x i32> %1182, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %1184 = sub <4 x i32> %1183, %1182
  %1185 = add nsw <4 x i32> %1183, %1182
  %1186 = shufflevector <4 x i32> %1184, <4 x i32> %1185, <4 x i32> <i32 0, i32 1, i32 6, i32 7>
  %1187 = bitcast i32* %382 to <4 x i32>*
  store <4 x i32> %1186, <4 x i32>* %1187, align 16
  %1188 = bitcast i32* %155 to <4 x i32>*
  %1189 = load <4 x i32>, <4 x i32>* %1188, align 4
  %1190 = shufflevector <4 x i32> %1189, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %1191 = add nsw <4 x i32> %1190, %1189
  %1192 = sub <4 x i32> %1190, %1189
  %1193 = shufflevector <4 x i32> %1191, <4 x i32> %1192, <4 x i32> <i32 0, i32 1, i32 6, i32 7>
  %1194 = bitcast i32* %389 to <4 x i32>*
  store <4 x i32> %1193, <4 x i32>* %1194, align 16
  %1195 = bitcast i32* %179 to <4 x i32>*
  %1196 = load <4 x i32>, <4 x i32>* %1195, align 4
  %1197 = shufflevector <4 x i32> %1196, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %1198 = sub <4 x i32> %1197, %1196
  %1199 = add nsw <4 x i32> %1197, %1196
  %1200 = shufflevector <4 x i32> %1198, <4 x i32> %1199, <4 x i32> <i32 0, i32 1, i32 6, i32 7>
  %1201 = bitcast i32* %396 to <4 x i32>*
  store <4 x i32> %1200, <4 x i32>* %1201, align 16
  %1202 = load i32, i32* %201, align 4
  store i32 %1202, i32* %403, align 16
  %1203 = load i32, i32* %205, align 4
  store i32 %1203, i32* %404, align 4
  %1204 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 8
  %1205 = load i32, i32* %1204, align 16
  %1206 = sub nsw i32 0, %1205
  %1207 = load i32, i32* %209, align 4
  %1208 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 56
  %1209 = load i32, i32* %1208, align 16
  %1210 = load i32, i32* %317, align 4
  %1211 = mul nsw i32 %1207, %1206
  %1212 = sext i32 %1211 to i64
  %1213 = mul nsw i32 %1210, %1209
  %1214 = sext i32 %1213 to i64
  %1215 = add i64 %428, %1212
  %1216 = add i64 %1215, %1214
  %1217 = ashr i64 %1216, %431
  %1218 = trunc i64 %1217 to i32
  store i32 %1218, i32* %405, align 8
  %1219 = load i32, i32* %213, align 4
  %1220 = load i32, i32* %313, align 4
  %1221 = mul nsw i32 %1219, %1206
  %1222 = sext i32 %1221 to i64
  %1223 = mul nsw i32 %1220, %1209
  %1224 = sext i32 %1223 to i64
  %1225 = add i64 %428, %1222
  %1226 = add i64 %1225, %1224
  %1227 = ashr i64 %1226, %431
  %1228 = trunc i64 %1227 to i32
  store i32 %1228, i32* %408, align 4
  %1229 = sub nsw i32 0, %1209
  %1230 = load i32, i32* %217, align 4
  %1231 = load i32, i32* %309, align 4
  %1232 = mul nsw i32 %1230, %1229
  %1233 = sext i32 %1232 to i64
  %1234 = mul nsw i32 %1231, %1206
  %1235 = sext i32 %1234 to i64
  %1236 = add i64 %428, %1233
  %1237 = add i64 %1236, %1235
  %1238 = ashr i64 %1237, %431
  %1239 = trunc i64 %1238 to i32
  store i32 %1239, i32* %410, align 16
  %1240 = load i32, i32* %221, align 4
  %1241 = load i32, i32* %305, align 4
  %1242 = mul nsw i32 %1240, %1229
  %1243 = sext i32 %1242 to i64
  %1244 = mul nsw i32 %1241, %1206
  %1245 = sext i32 %1244 to i64
  %1246 = add i64 %428, %1243
  %1247 = add i64 %1246, %1245
  %1248 = ashr i64 %1247, %431
  %1249 = trunc i64 %1248 to i32
  store i32 %1249, i32* %411, align 4
  %1250 = bitcast i32* %225 to <4 x i32>*
  %1251 = load <4 x i32>, <4 x i32>* %1250, align 4
  %1252 = bitcast i32* %412 to <4 x i32>*
  store <4 x i32> %1251, <4 x i32>* %1252, align 8
  %1253 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 40
  %1254 = load i32, i32* %1253, align 16
  %1255 = sub nsw i32 0, %1254
  %1256 = load i32, i32* %241, align 4
  %1257 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 24
  %1258 = load i32, i32* %1257, align 16
  %1259 = load i32, i32* %285, align 4
  %1260 = mul nsw i32 %1256, %1255
  %1261 = sext i32 %1260 to i64
  %1262 = mul nsw i32 %1259, %1258
  %1263 = sext i32 %1262 to i64
  %1264 = add i64 %428, %1261
  %1265 = add i64 %1264, %1263
  %1266 = ashr i64 %1265, %431
  %1267 = trunc i64 %1266 to i32
  store i32 %1267, i32* %456, align 8
  %1268 = load i32, i32* %245, align 4
  %1269 = load i32, i32* %281, align 4
  %1270 = mul nsw i32 %1268, %1255
  %1271 = sext i32 %1270 to i64
  %1272 = mul nsw i32 %1269, %1258
  %1273 = sext i32 %1272 to i64
  %1274 = add i64 %428, %1271
  %1275 = add i64 %1274, %1273
  %1276 = ashr i64 %1275, %431
  %1277 = trunc i64 %1276 to i32
  store i32 %1277, i32* %467, align 4
  %1278 = sub nsw i32 0, %1258
  %1279 = load i32, i32* %249, align 4
  %1280 = load i32, i32* %277, align 4
  %1281 = mul nsw i32 %1279, %1278
  %1282 = sext i32 %1281 to i64
  %1283 = mul nsw i32 %1280, %1255
  %1284 = sext i32 %1283 to i64
  %1285 = add i64 %428, %1282
  %1286 = add i64 %1285, %1284
  %1287 = ashr i64 %1286, %431
  %1288 = trunc i64 %1287 to i32
  store i32 %1288, i32* %478, align 16
  %1289 = load i32, i32* %253, align 4
  %1290 = load i32, i32* %273, align 4
  %1291 = mul nsw i32 %1289, %1278
  %1292 = sext i32 %1291 to i64
  %1293 = mul nsw i32 %1290, %1255
  %1294 = sext i32 %1293 to i64
  %1295 = add i64 %428, %1292
  %1296 = add i64 %1295, %1294
  %1297 = ashr i64 %1296, %431
  %1298 = trunc i64 %1297 to i32
  store i32 %1298, i32* %489, align 4
  %1299 = load i32, i32* %257, align 4
  store i32 %1299, i32* %500, align 8
  %1300 = load i32, i32* %261, align 4
  store i32 %1300, i32* %511, align 4
  %1301 = load i32, i32* %265, align 4
  store i32 %1301, i32* %517, align 16
  %1302 = load i32, i32* %269, align 4
  store i32 %1302, i32* %523, align 4
  %1303 = mul nsw i32 %1290, %1258
  %1304 = sext i32 %1303 to i64
  %1305 = mul nsw i32 %1289, %1255
  %1306 = sext i32 %1305 to i64
  %1307 = add i64 %428, %1304
  %1308 = add i64 %1307, %1306
  %1309 = ashr i64 %1308, %431
  %1310 = trunc i64 %1309 to i32
  store i32 %1310, i32* %529, align 8
  %1311 = mul nsw i32 %1280, %1258
  %1312 = sext i32 %1311 to i64
  %1313 = mul nsw i32 %1279, %1255
  %1314 = sext i32 %1313 to i64
  %1315 = add i64 %428, %1312
  %1316 = add i64 %1315, %1314
  %1317 = ashr i64 %1316, %431
  %1318 = trunc i64 %1317 to i32
  store i32 %1318, i32* %535, align 4
  %1319 = mul nsw i32 %1269, %1254
  %1320 = sext i32 %1319 to i64
  %1321 = mul nsw i32 %1268, %1258
  %1322 = sext i32 %1321 to i64
  %1323 = add i64 %428, %1320
  %1324 = add i64 %1323, %1322
  %1325 = ashr i64 %1324, %431
  %1326 = trunc i64 %1325 to i32
  store i32 %1326, i32* %541, align 16
  %1327 = mul nsw i32 %1259, %1254
  %1328 = sext i32 %1327 to i64
  %1329 = mul nsw i32 %1256, %1258
  %1330 = sext i32 %1329 to i64
  %1331 = add i64 %428, %1328
  %1332 = add i64 %1331, %1330
  %1333 = ashr i64 %1332, %431
  %1334 = trunc i64 %1333 to i32
  store i32 %1334, i32* %547, align 4
  %1335 = bitcast i32* %289 to <4 x i32>*
  %1336 = load <4 x i32>, <4 x i32>* %1335, align 4
  %1337 = bitcast i32* %553 to <4 x i32>*
  store <4 x i32> %1336, <4 x i32>* %1337, align 8
  %1338 = load i32, i32* %305, align 4
  %1339 = load i32, i32* %221, align 4
  %1340 = mul nsw i32 %1338, %1209
  %1341 = sext i32 %1340 to i64
  %1342 = mul nsw i32 %1339, %1206
  %1343 = sext i32 %1342 to i64
  %1344 = add i64 %428, %1341
  %1345 = add i64 %1344, %1343
  %1346 = ashr i64 %1345, %431
  %1347 = trunc i64 %1346 to i32
  store i32 %1347, i32* %562, align 8
  %1348 = load i32, i32* %309, align 4
  %1349 = load i32, i32* %217, align 4
  %1350 = mul nsw i32 %1348, %1209
  %1351 = sext i32 %1350 to i64
  %1352 = mul nsw i32 %1349, %1206
  %1353 = sext i32 %1352 to i64
  %1354 = add i64 %428, %1351
  %1355 = add i64 %1354, %1353
  %1356 = ashr i64 %1355, %431
  %1357 = trunc i64 %1356 to i32
  store i32 %1357, i32* %565, align 4
  %1358 = load i32, i32* %313, align 4
  %1359 = load i32, i32* %213, align 4
  %1360 = mul nsw i32 %1358, %1205
  %1361 = sext i32 %1360 to i64
  %1362 = mul nsw i32 %1359, %1209
  %1363 = sext i32 %1362 to i64
  %1364 = add i64 %428, %1361
  %1365 = add i64 %1364, %1363
  %1366 = ashr i64 %1365, %431
  %1367 = trunc i64 %1366 to i32
  store i32 %1367, i32* %567, align 16
  %1368 = load i32, i32* %317, align 4
  %1369 = load i32, i32* %209, align 4
  %1370 = mul nsw i32 %1368, %1205
  %1371 = sext i32 %1370 to i64
  %1372 = mul nsw i32 %1369, %1209
  %1373 = sext i32 %1372 to i64
  %1374 = add i64 %428, %1371
  %1375 = add i64 %1374, %1373
  %1376 = ashr i64 %1375, %431
  %1377 = trunc i64 %1376 to i32
  store i32 %1377, i32* %568, align 4
  %1378 = load i32, i32* %321, align 4
  store i32 %1378, i32* %569, align 8
  %1379 = load i32, i32* %325, align 4
  store i32 %1379, i32* %572, align 4
  %1380 = getelementptr inbounds i8, i8* %3, i64 6
  %1381 = load i8, i8* %1380, align 1
  call void @av1_range_check_buf(i32 6, i32* %0, i32* nonnull %331, i32 64, i8 signext %1381) #3
  %1382 = bitcast [64 x i32]* %5 to <4 x i32>*
  %1383 = load <4 x i32>, <4 x i32>* %1382, align 16
  %1384 = bitcast i32* %1 to <4 x i32>*
  store <4 x i32> %1383, <4 x i32>* %1384, align 4
  %1385 = load i32, i32* %342, align 16
  %1386 = load i32, i32* %351, align 4
  %1387 = mul nsw i32 %1385, %1209
  %1388 = sext i32 %1387 to i64
  %1389 = mul nsw i32 %1386, %1205
  %1390 = sext i32 %1389 to i64
  %1391 = add i64 %428, %1388
  %1392 = add i64 %1391, %1390
  %1393 = ashr i64 %1392, %431
  %1394 = trunc i64 %1393 to i32
  store i32 %1394, i32* %35, align 4
  %1395 = load i32, i32* %343, align 4
  %1396 = load i32, i32* %344, align 8
  %1397 = mul nsw i32 %1395, %1258
  %1398 = sext i32 %1397 to i64
  %1399 = mul nsw i32 %1396, %1254
  %1400 = sext i32 %1399 to i64
  %1401 = add i64 %428, %1398
  %1402 = add i64 %1401, %1400
  %1403 = ashr i64 %1402, %431
  %1404 = trunc i64 %1403 to i32
  store i32 %1404, i32* %41, align 4
  %1405 = mul nsw i32 %1396, %1258
  %1406 = sext i32 %1405 to i64
  %1407 = mul nsw i32 %1395, %1255
  %1408 = sext i32 %1407 to i64
  %1409 = add i64 %428, %1406
  %1410 = add i64 %1409, %1408
  %1411 = ashr i64 %1410, %431
  %1412 = trunc i64 %1411 to i32
  store i32 %1412, i32* %47, align 4
  %1413 = mul nsw i32 %1386, %1209
  %1414 = sext i32 %1413 to i64
  %1415 = mul nsw i32 %1385, %1206
  %1416 = sext i32 %1415 to i64
  %1417 = add i64 %428, %1414
  %1418 = add i64 %1417, %1416
  %1419 = ashr i64 %1418, %431
  %1420 = trunc i64 %1419 to i32
  store i32 %1420, i32* %53, align 4
  %1421 = bitcast i32* %353 to <4 x i32>*
  %1422 = load <4 x i32>, <4 x i32>* %1421, align 16
  %1423 = shufflevector <4 x i32> %1422, <4 x i32> undef, <4 x i32> <i32 1, i32 0, i32 3, i32 2>
  %1424 = add nsw <4 x i32> %1423, %1422
  %1425 = sub <4 x i32> %1423, %1422
  %1426 = shufflevector <4 x i32> %1424, <4 x i32> %1425, <4 x i32> <i32 0, i32 5, i32 6, i32 3>
  %1427 = bitcast i32* %59 to <4 x i32>*
  store <4 x i32> %1426, <4 x i32>* %1427, align 4
  %1428 = bitcast i32* %364 to <4 x i32>*
  %1429 = load <4 x i32>, <4 x i32>* %1428, align 16
  %1430 = shufflevector <4 x i32> %1429, <4 x i32> undef, <4 x i32> <i32 1, i32 0, i32 3, i32 2>
  %1431 = add nsw <4 x i32> %1430, %1429
  %1432 = sub <4 x i32> %1430, %1429
  %1433 = shufflevector <4 x i32> %1431, <4 x i32> %1432, <4 x i32> <i32 0, i32 5, i32 6, i32 3>
  %1434 = bitcast i32* %83 to <4 x i32>*
  store <4 x i32> %1433, <4 x i32>* %1434, align 4
  %1435 = load i32, i32* %375, align 16
  store i32 %1435, i32* %107, align 4
  %1436 = load i32, i32* %376, align 4
  %1437 = load i32, i32* %398, align 8
  %1438 = mul nsw i32 %1436, %1206
  %1439 = sext i32 %1438 to i64
  %1440 = mul nsw i32 %1437, %1209
  %1441 = sext i32 %1440 to i64
  %1442 = add i64 %428, %1439
  %1443 = add i64 %1442, %1441
  %1444 = ashr i64 %1443, %431
  %1445 = trunc i64 %1444 to i32
  store i32 %1445, i32* %113, align 4
  %1446 = load i32, i32* %377, align 8
  %1447 = load i32, i32* %397, align 4
  %1448 = mul nsw i32 %1446, %1229
  %1449 = sext i32 %1448 to i64
  %1450 = mul nsw i32 %1447, %1206
  %1451 = sext i32 %1450 to i64
  %1452 = add i64 %428, %1449
  %1453 = add i64 %1452, %1451
  %1454 = ashr i64 %1453, %431
  %1455 = trunc i64 %1454 to i32
  store i32 %1455, i32* %119, align 4
  %1456 = load i32, i32* %380, align 4
  store i32 %1456, i32* %125, align 4
  %1457 = load i32, i32* %382, align 16
  store i32 %1457, i32* %131, align 4
  %1458 = load i32, i32* %383, align 4
  %1459 = load i32, i32* %391, align 8
  %1460 = mul nsw i32 %1458, %1255
  %1461 = sext i32 %1460 to i64
  %1462 = mul nsw i32 %1459, %1258
  %1463 = sext i32 %1462 to i64
  %1464 = add i64 %428, %1461
  %1465 = add i64 %1464, %1463
  %1466 = ashr i64 %1465, %431
  %1467 = trunc i64 %1466 to i32
  store i32 %1467, i32* %137, align 4
  %1468 = load i32, i32* %384, align 8
  %1469 = load i32, i32* %390, align 4
  %1470 = mul nsw i32 %1468, %1278
  %1471 = sext i32 %1470 to i64
  %1472 = mul nsw i32 %1469, %1255
  %1473 = sext i32 %1472 to i64
  %1474 = add i64 %428, %1471
  %1475 = add i64 %1474, %1473
  %1476 = ashr i64 %1475, %431
  %1477 = trunc i64 %1476 to i32
  store i32 %1477, i32* %143, align 4
  %1478 = load i32, i32* %387, align 4
  store i32 %1478, i32* %149, align 4
  %1479 = load i32, i32* %389, align 16
  store i32 %1479, i32* %155, align 4
  %1480 = mul nsw i32 %1469, %1258
  %1481 = sext i32 %1480 to i64
  %1482 = mul nsw i32 %1468, %1255
  %1483 = sext i32 %1482 to i64
  %1484 = add i64 %428, %1481
  %1485 = add i64 %1484, %1483
  %1486 = ashr i64 %1485, %431
  %1487 = trunc i64 %1486 to i32
  store i32 %1487, i32* %161, align 4
  %1488 = mul nsw i32 %1459, %1254
  %1489 = sext i32 %1488 to i64
  %1490 = mul nsw i32 %1458, %1258
  %1491 = sext i32 %1490 to i64
  %1492 = add i64 %428, %1489
  %1493 = add i64 %1492, %1491
  %1494 = ashr i64 %1493, %431
  %1495 = trunc i64 %1494 to i32
  store i32 %1495, i32* %167, align 4
  %1496 = load i32, i32* %394, align 4
  store i32 %1496, i32* %173, align 4
  %1497 = load i32, i32* %396, align 16
  store i32 %1497, i32* %179, align 4
  %1498 = mul nsw i32 %1447, %1209
  %1499 = sext i32 %1498 to i64
  %1500 = mul nsw i32 %1446, %1206
  %1501 = sext i32 %1500 to i64
  %1502 = add i64 %428, %1499
  %1503 = add i64 %1502, %1501
  %1504 = ashr i64 %1503, %431
  %1505 = trunc i64 %1504 to i32
  store i32 %1505, i32* %185, align 4
  %1506 = mul nsw i32 %1437, %1205
  %1507 = sext i32 %1506 to i64
  %1508 = mul nsw i32 %1436, %1209
  %1509 = sext i32 %1508 to i64
  %1510 = add i64 %428, %1507
  %1511 = add i64 %1510, %1509
  %1512 = ashr i64 %1511, %431
  %1513 = trunc i64 %1512 to i32
  store i32 %1513, i32* %191, align 4
  %1514 = load i32, i32* %401, align 4
  store i32 %1514, i32* %197, align 4
  %1515 = bitcast i32* %403 to <4 x i32>*
  %1516 = load <4 x i32>, <4 x i32>* %1515, align 16
  %1517 = shufflevector <4 x i32> %1516, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %1518 = add nsw <4 x i32> %1517, %1516
  %1519 = sub <4 x i32> %1517, %1516
  %1520 = shufflevector <4 x i32> %1518, <4 x i32> %1519, <4 x i32> <i32 0, i32 1, i32 6, i32 7>
  %1521 = bitcast i32* %201 to <4 x i32>*
  store <4 x i32> %1520, <4 x i32>* %1521, align 4
  %1522 = bitcast i32* %410 to <4 x i32>*
  %1523 = load <4 x i32>, <4 x i32>* %1522, align 16
  %1524 = shufflevector <4 x i32> %1523, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %1525 = sub <4 x i32> %1524, %1523
  %1526 = add nsw <4 x i32> %1524, %1523
  %1527 = shufflevector <4 x i32> %1525, <4 x i32> %1526, <4 x i32> <i32 0, i32 1, i32 6, i32 7>
  %1528 = bitcast i32* %217 to <4 x i32>*
  store <4 x i32> %1527, <4 x i32>* %1528, align 4
  %1529 = bitcast i32* %434 to <4 x i32>*
  %1530 = load <4 x i32>, <4 x i32>* %1529, align 16
  %1531 = shufflevector <4 x i32> %1530, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %1532 = add nsw <4 x i32> %1531, %1530
  %1533 = sub <4 x i32> %1531, %1530
  %1534 = shufflevector <4 x i32> %1532, <4 x i32> %1533, <4 x i32> <i32 0, i32 1, i32 6, i32 7>
  %1535 = bitcast i32* %233 to <4 x i32>*
  store <4 x i32> %1534, <4 x i32>* %1535, align 4
  %1536 = bitcast i32* %478 to <4 x i32>*
  %1537 = load <4 x i32>, <4 x i32>* %1536, align 16
  %1538 = shufflevector <4 x i32> %1537, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %1539 = sub <4 x i32> %1538, %1537
  %1540 = add nsw <4 x i32> %1538, %1537
  %1541 = shufflevector <4 x i32> %1539, <4 x i32> %1540, <4 x i32> <i32 0, i32 1, i32 6, i32 7>
  %1542 = bitcast i32* %249 to <4 x i32>*
  store <4 x i32> %1541, <4 x i32>* %1542, align 4
  %1543 = bitcast i32* %517 to <4 x i32>*
  %1544 = load <4 x i32>, <4 x i32>* %1543, align 16
  %1545 = shufflevector <4 x i32> %1544, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %1546 = add nsw <4 x i32> %1545, %1544
  %1547 = sub <4 x i32> %1545, %1544
  %1548 = shufflevector <4 x i32> %1546, <4 x i32> %1547, <4 x i32> <i32 0, i32 1, i32 6, i32 7>
  %1549 = bitcast i32* %265 to <4 x i32>*
  store <4 x i32> %1548, <4 x i32>* %1549, align 4
  %1550 = bitcast i32* %541 to <4 x i32>*
  %1551 = load <4 x i32>, <4 x i32>* %1550, align 16
  %1552 = shufflevector <4 x i32> %1551, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %1553 = sub <4 x i32> %1552, %1551
  %1554 = add nsw <4 x i32> %1552, %1551
  %1555 = shufflevector <4 x i32> %1553, <4 x i32> %1554, <4 x i32> <i32 0, i32 1, i32 6, i32 7>
  %1556 = bitcast i32* %281 to <4 x i32>*
  store <4 x i32> %1555, <4 x i32>* %1556, align 4
  %1557 = bitcast i32* %560 to <4 x i32>*
  %1558 = load <4 x i32>, <4 x i32>* %1557, align 16
  %1559 = shufflevector <4 x i32> %1558, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %1560 = add nsw <4 x i32> %1559, %1558
  %1561 = sub <4 x i32> %1559, %1558
  %1562 = shufflevector <4 x i32> %1560, <4 x i32> %1561, <4 x i32> <i32 0, i32 1, i32 6, i32 7>
  %1563 = bitcast i32* %297 to <4 x i32>*
  store <4 x i32> %1562, <4 x i32>* %1563, align 4
  %1564 = bitcast i32* %567 to <4 x i32>*
  %1565 = load <4 x i32>, <4 x i32>* %1564, align 16
  %1566 = shufflevector <4 x i32> %1565, <4 x i32> undef, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  %1567 = sub <4 x i32> %1566, %1565
  %1568 = add nsw <4 x i32> %1566, %1565
  %1569 = shufflevector <4 x i32> %1567, <4 x i32> %1568, <4 x i32> <i32 0, i32 1, i32 6, i32 7>
  %1570 = bitcast i32* %313 to <4 x i32>*
  store <4 x i32> %1569, <4 x i32>* %1570, align 4
  %1571 = getelementptr inbounds i8, i8* %3, i64 7
  %1572 = load i8, i8* %1571, align 1
  call void @av1_range_check_buf(i32 7, i32* %0, i32* %1, i32 64, i8 signext %1572) #3
  %1573 = bitcast i32* %1 to <4 x i32>*
  %1574 = load <4 x i32>, <4 x i32>* %1573, align 4
  %1575 = bitcast [64 x i32]* %5 to <4 x i32>*
  store <4 x i32> %1574, <4 x i32>* %1575, align 16
  %1576 = bitcast i32* %35 to <4 x i32>*
  %1577 = load <4 x i32>, <4 x i32>* %1576, align 4
  %1578 = bitcast i32* %342 to <4 x i32>*
  store <4 x i32> %1577, <4 x i32>* %1578, align 16
  %1579 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 60
  %1580 = load i32, i32* %1579, align 16
  %1581 = load i32, i32* %59, align 4
  %1582 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 4
  %1583 = load i32, i32* %1582, align 16
  %1584 = load i32, i32* %101, align 4
  %1585 = mul nsw i32 %1581, %1580
  %1586 = sext i32 %1585 to i64
  %1587 = mul nsw i32 %1584, %1583
  %1588 = sext i32 %1587 to i64
  %1589 = add i64 %428, %1586
  %1590 = add i64 %1589, %1588
  %1591 = ashr i64 %1590, %431
  %1592 = trunc i64 %1591 to i32
  store i32 %1592, i32* %353, align 16
  %1593 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 28
  %1594 = load i32, i32* %1593, align 16
  %1595 = load i32, i32* %65, align 4
  %1596 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 36
  %1597 = load i32, i32* %1596, align 16
  %1598 = load i32, i32* %95, align 4
  %1599 = mul nsw i32 %1595, %1594
  %1600 = sext i32 %1599 to i64
  %1601 = mul nsw i32 %1598, %1597
  %1602 = sext i32 %1601 to i64
  %1603 = add i64 %428, %1600
  %1604 = add i64 %1603, %1602
  %1605 = ashr i64 %1604, %431
  %1606 = trunc i64 %1605 to i32
  store i32 %1606, i32* %354, align 4
  %1607 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 44
  %1608 = load i32, i32* %1607, align 16
  %1609 = load i32, i32* %71, align 4
  %1610 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 20
  %1611 = load i32, i32* %1610, align 16
  %1612 = load i32, i32* %89, align 4
  %1613 = mul nsw i32 %1609, %1608
  %1614 = sext i32 %1613 to i64
  %1615 = mul nsw i32 %1612, %1611
  %1616 = sext i32 %1615 to i64
  %1617 = add i64 %428, %1614
  %1618 = add i64 %1617, %1616
  %1619 = ashr i64 %1618, %431
  %1620 = trunc i64 %1619 to i32
  store i32 %1620, i32* %355, align 8
  %1621 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 12
  %1622 = load i32, i32* %1621, align 16
  %1623 = load i32, i32* %77, align 4
  %1624 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 52
  %1625 = load i32, i32* %1624, align 16
  %1626 = load i32, i32* %83, align 4
  %1627 = mul nsw i32 %1623, %1622
  %1628 = sext i32 %1627 to i64
  %1629 = mul nsw i32 %1626, %1625
  %1630 = sext i32 %1629 to i64
  %1631 = add i64 %428, %1628
  %1632 = add i64 %1631, %1630
  %1633 = ashr i64 %1632, %431
  %1634 = trunc i64 %1633 to i32
  store i32 %1634, i32* %362, align 4
  %1635 = sub nsw i32 0, %1625
  %1636 = mul nsw i32 %1626, %1622
  %1637 = sext i32 %1636 to i64
  %1638 = mul nsw i32 %1623, %1635
  %1639 = sext i32 %1638 to i64
  %1640 = add i64 %428, %1637
  %1641 = add i64 %1640, %1639
  %1642 = ashr i64 %1641, %431
  %1643 = trunc i64 %1642 to i32
  store i32 %1643, i32* %364, align 16
  %1644 = sub nsw i32 0, %1611
  %1645 = mul nsw i32 %1612, %1608
  %1646 = sext i32 %1645 to i64
  %1647 = mul nsw i32 %1609, %1644
  %1648 = sext i32 %1647 to i64
  %1649 = add i64 %428, %1646
  %1650 = add i64 %1649, %1648
  %1651 = ashr i64 %1650, %431
  %1652 = trunc i64 %1651 to i32
  store i32 %1652, i32* %365, align 4
  %1653 = sub nsw i32 0, %1597
  %1654 = mul nsw i32 %1598, %1594
  %1655 = sext i32 %1654 to i64
  %1656 = mul nsw i32 %1595, %1653
  %1657 = sext i32 %1656 to i64
  %1658 = add i64 %428, %1655
  %1659 = add i64 %1658, %1657
  %1660 = ashr i64 %1659, %431
  %1661 = trunc i64 %1660 to i32
  store i32 %1661, i32* %366, align 8
  %1662 = sub nsw i32 0, %1583
  %1663 = mul nsw i32 %1584, %1580
  %1664 = sext i32 %1663 to i64
  %1665 = mul nsw i32 %1581, %1662
  %1666 = sext i32 %1665 to i64
  %1667 = add i64 %428, %1664
  %1668 = add i64 %1667, %1666
  %1669 = ashr i64 %1668, %431
  %1670 = trunc i64 %1669 to i32
  store i32 %1670, i32* %373, align 4
  %1671 = bitcast i32* %107 to <4 x i32>*
  %1672 = load <4 x i32>, <4 x i32>* %1671, align 4
  %1673 = shufflevector <4 x i32> %1672, <4 x i32> undef, <4 x i32> <i32 1, i32 0, i32 3, i32 2>
  %1674 = add nsw <4 x i32> %1673, %1672
  %1675 = sub <4 x i32> %1673, %1672
  %1676 = shufflevector <4 x i32> %1674, <4 x i32> %1675, <4 x i32> <i32 0, i32 5, i32 6, i32 3>
  %1677 = bitcast i32* %375 to <4 x i32>*
  store <4 x i32> %1676, <4 x i32>* %1677, align 16
  %1678 = bitcast i32* %131 to <4 x i32>*
  %1679 = load <4 x i32>, <4 x i32>* %1678, align 4
  %1680 = shufflevector <4 x i32> %1679, <4 x i32> undef, <4 x i32> <i32 1, i32 0, i32 3, i32 2>
  %1681 = add nsw <4 x i32> %1680, %1679
  %1682 = sub <4 x i32> %1680, %1679
  %1683 = shufflevector <4 x i32> %1681, <4 x i32> %1682, <4 x i32> <i32 0, i32 5, i32 6, i32 3>
  %1684 = bitcast i32* %382 to <4 x i32>*
  store <4 x i32> %1683, <4 x i32>* %1684, align 16
  %1685 = bitcast i32* %155 to <4 x i32>*
  %1686 = load <4 x i32>, <4 x i32>* %1685, align 4
  %1687 = shufflevector <4 x i32> %1686, <4 x i32> undef, <4 x i32> <i32 1, i32 0, i32 3, i32 2>
  %1688 = add nsw <4 x i32> %1687, %1686
  %1689 = sub <4 x i32> %1687, %1686
  %1690 = shufflevector <4 x i32> %1688, <4 x i32> %1689, <4 x i32> <i32 0, i32 5, i32 6, i32 3>
  %1691 = bitcast i32* %389 to <4 x i32>*
  store <4 x i32> %1690, <4 x i32>* %1691, align 16
  %1692 = bitcast i32* %179 to <4 x i32>*
  %1693 = load <4 x i32>, <4 x i32>* %1692, align 4
  %1694 = shufflevector <4 x i32> %1693, <4 x i32> undef, <4 x i32> <i32 1, i32 0, i32 3, i32 2>
  %1695 = add nsw <4 x i32> %1694, %1693
  %1696 = sub <4 x i32> %1694, %1693
  %1697 = shufflevector <4 x i32> %1695, <4 x i32> %1696, <4 x i32> <i32 0, i32 5, i32 6, i32 3>
  %1698 = bitcast i32* %396 to <4 x i32>*
  store <4 x i32> %1697, <4 x i32>* %1698, align 16
  %1699 = load i32, i32* %201, align 4
  store i32 %1699, i32* %403, align 16
  %1700 = load i32, i32* %205, align 4
  %1701 = load i32, i32* %321, align 4
  %1702 = mul nsw i32 %1700, %1662
  %1703 = sext i32 %1702 to i64
  %1704 = mul nsw i32 %1701, %1580
  %1705 = sext i32 %1704 to i64
  %1706 = add i64 %428, %1703
  %1707 = add i64 %1706, %1705
  %1708 = ashr i64 %1707, %431
  %1709 = trunc i64 %1708 to i32
  store i32 %1709, i32* %404, align 4
  %1710 = load i32, i32* %209, align 4
  %1711 = load i32, i32* %317, align 4
  %1712 = mul i32 %1580, %1710
  %1713 = sub i32 0, %1712
  %1714 = sext i32 %1713 to i64
  %1715 = mul nsw i32 %1711, %1662
  %1716 = sext i32 %1715 to i64
  %1717 = add i64 %428, %1714
  %1718 = add i64 %1717, %1716
  %1719 = ashr i64 %1718, %431
  %1720 = trunc i64 %1719 to i32
  store i32 %1720, i32* %405, align 8
  %1721 = load i32, i32* %213, align 4
  store i32 %1721, i32* %408, align 4
  %1722 = load i32, i32* %217, align 4
  store i32 %1722, i32* %410, align 16
  %1723 = load i32, i32* %221, align 4
  %1724 = load i32, i32* %305, align 4
  %1725 = mul nsw i32 %1723, %1653
  %1726 = sext i32 %1725 to i64
  %1727 = mul nsw i32 %1724, %1594
  %1728 = sext i32 %1727 to i64
  %1729 = add i64 %428, %1726
  %1730 = add i64 %1729, %1728
  %1731 = ashr i64 %1730, %431
  %1732 = trunc i64 %1731 to i32
  store i32 %1732, i32* %411, align 4
  %1733 = load i32, i32* %225, align 4
  %1734 = load i32, i32* %301, align 4
  %1735 = mul i32 %1594, %1733
  %1736 = sub i32 0, %1735
  %1737 = sext i32 %1736 to i64
  %1738 = mul nsw i32 %1734, %1653
  %1739 = sext i32 %1738 to i64
  %1740 = add i64 %428, %1737
  %1741 = add i64 %1740, %1739
  %1742 = ashr i64 %1741, %431
  %1743 = trunc i64 %1742 to i32
  store i32 %1743, i32* %412, align 8
  %1744 = load i32, i32* %229, align 4
  store i32 %1744, i32* %415, align 4
  %1745 = load i32, i32* %233, align 4
  store i32 %1745, i32* %434, align 16
  %1746 = load i32, i32* %237, align 4
  %1747 = load i32, i32* %289, align 4
  %1748 = mul nsw i32 %1746, %1644
  %1749 = sext i32 %1748 to i64
  %1750 = mul nsw i32 %1747, %1608
  %1751 = sext i32 %1750 to i64
  %1752 = add i64 %428, %1749
  %1753 = add i64 %1752, %1751
  %1754 = ashr i64 %1753, %431
  %1755 = trunc i64 %1754 to i32
  store i32 %1755, i32* %445, align 4
  %1756 = load i32, i32* %241, align 4
  %1757 = load i32, i32* %285, align 4
  %1758 = mul i32 %1608, %1756
  %1759 = sub i32 0, %1758
  %1760 = sext i32 %1759 to i64
  %1761 = mul nsw i32 %1757, %1644
  %1762 = sext i32 %1761 to i64
  %1763 = add i64 %428, %1760
  %1764 = add i64 %1763, %1762
  %1765 = ashr i64 %1764, %431
  %1766 = trunc i64 %1765 to i32
  store i32 %1766, i32* %456, align 8
  %1767 = load i32, i32* %245, align 4
  store i32 %1767, i32* %467, align 4
  %1768 = load i32, i32* %249, align 4
  store i32 %1768, i32* %478, align 16
  %1769 = load i32, i32* %253, align 4
  %1770 = load i32, i32* %273, align 4
  %1771 = mul nsw i32 %1769, %1635
  %1772 = sext i32 %1771 to i64
  %1773 = mul nsw i32 %1770, %1622
  %1774 = sext i32 %1773 to i64
  %1775 = add i64 %428, %1772
  %1776 = add i64 %1775, %1774
  %1777 = ashr i64 %1776, %431
  %1778 = trunc i64 %1777 to i32
  store i32 %1778, i32* %489, align 4
  %1779 = load i32, i32* %257, align 4
  %1780 = load i32, i32* %269, align 4
  %1781 = mul i32 %1622, %1779
  %1782 = sub i32 0, %1781
  %1783 = sext i32 %1782 to i64
  %1784 = mul nsw i32 %1780, %1635
  %1785 = sext i32 %1784 to i64
  %1786 = add i64 %428, %1783
  %1787 = add i64 %1786, %1785
  %1788 = ashr i64 %1787, %431
  %1789 = trunc i64 %1788 to i32
  store i32 %1789, i32* %500, align 8
  %1790 = load i32, i32* %261, align 4
  store i32 %1790, i32* %511, align 4
  %1791 = load i32, i32* %265, align 4
  store i32 %1791, i32* %517, align 16
  %1792 = mul nsw i32 %1780, %1622
  %1793 = sext i32 %1792 to i64
  %1794 = mul nsw i32 %1779, %1635
  %1795 = sext i32 %1794 to i64
  %1796 = add i64 %428, %1793
  %1797 = add i64 %1796, %1795
  %1798 = ashr i64 %1797, %431
  %1799 = trunc i64 %1798 to i32
  store i32 %1799, i32* %523, align 4
  %1800 = mul nsw i32 %1770, %1625
  %1801 = sext i32 %1800 to i64
  %1802 = mul nsw i32 %1769, %1622
  %1803 = sext i32 %1802 to i64
  %1804 = add i64 %428, %1801
  %1805 = add i64 %1804, %1803
  %1806 = ashr i64 %1805, %431
  %1807 = trunc i64 %1806 to i32
  store i32 %1807, i32* %529, align 8
  %1808 = load i32, i32* %277, align 4
  store i32 %1808, i32* %535, align 4
  %1809 = load i32, i32* %281, align 4
  store i32 %1809, i32* %541, align 16
  %1810 = mul nsw i32 %1757, %1608
  %1811 = sext i32 %1810 to i64
  %1812 = mul nsw i32 %1756, %1644
  %1813 = sext i32 %1812 to i64
  %1814 = add i64 %428, %1811
  %1815 = add i64 %1814, %1813
  %1816 = ashr i64 %1815, %431
  %1817 = trunc i64 %1816 to i32
  store i32 %1817, i32* %547, align 4
  %1818 = mul nsw i32 %1747, %1611
  %1819 = sext i32 %1818 to i64
  %1820 = mul nsw i32 %1746, %1608
  %1821 = sext i32 %1820 to i64
  %1822 = add i64 %428, %1819
  %1823 = add i64 %1822, %1821
  %1824 = ashr i64 %1823, %431
  %1825 = trunc i64 %1824 to i32
  store i32 %1825, i32* %553, align 8
  %1826 = load i32, i32* %293, align 4
  store i32 %1826, i32* %559, align 4
  %1827 = load i32, i32* %297, align 4
  store i32 %1827, i32* %560, align 16
  %1828 = load i32, i32* %301, align 4
  %1829 = load i32, i32* %225, align 4
  %1830 = mul nsw i32 %1828, %1594
  %1831 = sext i32 %1830 to i64
  %1832 = mul nsw i32 %1829, %1653
  %1833 = sext i32 %1832 to i64
  %1834 = add i64 %428, %1831
  %1835 = add i64 %1834, %1833
  %1836 = ashr i64 %1835, %431
  %1837 = trunc i64 %1836 to i32
  store i32 %1837, i32* %561, align 4
  %1838 = load i32, i32* %305, align 4
  %1839 = load i32, i32* %221, align 4
  %1840 = mul nsw i32 %1838, %1597
  %1841 = sext i32 %1840 to i64
  %1842 = mul nsw i32 %1839, %1594
  %1843 = sext i32 %1842 to i64
  %1844 = add i64 %428, %1841
  %1845 = add i64 %1844, %1843
  %1846 = ashr i64 %1845, %431
  %1847 = trunc i64 %1846 to i32
  store i32 %1847, i32* %562, align 8
  %1848 = load i32, i32* %309, align 4
  store i32 %1848, i32* %565, align 4
  %1849 = load i32, i32* %313, align 4
  store i32 %1849, i32* %567, align 16
  %1850 = load i32, i32* %317, align 4
  %1851 = load i32, i32* %209, align 4
  %1852 = mul nsw i32 %1850, %1580
  %1853 = sext i32 %1852 to i64
  %1854 = mul nsw i32 %1851, %1662
  %1855 = sext i32 %1854 to i64
  %1856 = add i64 %428, %1853
  %1857 = add i64 %1856, %1855
  %1858 = ashr i64 %1857, %431
  %1859 = trunc i64 %1858 to i32
  store i32 %1859, i32* %568, align 4
  %1860 = load i32, i32* %321, align 4
  %1861 = load i32, i32* %205, align 4
  %1862 = mul nsw i32 %1860, %1583
  %1863 = sext i32 %1862 to i64
  %1864 = mul nsw i32 %1861, %1580
  %1865 = sext i32 %1864 to i64
  %1866 = add i64 %428, %1863
  %1867 = add i64 %1866, %1865
  %1868 = ashr i64 %1867, %431
  %1869 = trunc i64 %1868 to i32
  store i32 %1869, i32* %569, align 8
  %1870 = load i32, i32* %325, align 4
  store i32 %1870, i32* %572, align 4
  %1871 = getelementptr inbounds i8, i8* %3, i64 8
  %1872 = load i8, i8* %1871, align 1
  call void @av1_range_check_buf(i32 8, i32* %0, i32* nonnull %331, i32 64, i8 signext %1872) #3
  %1873 = bitcast [64 x i32]* %5 to <4 x i32>*
  %1874 = load <4 x i32>, <4 x i32>* %1873, align 16
  %1875 = bitcast i32* %1 to <4 x i32>*
  store <4 x i32> %1874, <4 x i32>* %1875, align 4
  %1876 = bitcast i32* %342 to <4 x i32>*
  %1877 = load <4 x i32>, <4 x i32>* %1876, align 16
  %1878 = bitcast i32* %35 to <4 x i32>*
  store <4 x i32> %1877, <4 x i32>* %1878, align 4
  %1879 = bitcast i32* %353 to <4 x i32>*
  %1880 = load <4 x i32>, <4 x i32>* %1879, align 16
  %1881 = bitcast i32* %59 to <4 x i32>*
  store <4 x i32> %1880, <4 x i32>* %1881, align 4
  %1882 = bitcast i32* %364 to <4 x i32>*
  %1883 = load <4 x i32>, <4 x i32>* %1882, align 16
  %1884 = bitcast i32* %83 to <4 x i32>*
  store <4 x i32> %1883, <4 x i32>* %1884, align 4
  %1885 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 62
  %1886 = load i32, i32* %1885, align 8
  %1887 = load i32, i32* %375, align 16
  %1888 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 2
  %1889 = load i32, i32* %1888, align 8
  %1890 = load i32, i32* %401, align 4
  %1891 = mul nsw i32 %1887, %1886
  %1892 = sext i32 %1891 to i64
  %1893 = mul nsw i32 %1890, %1889
  %1894 = sext i32 %1893 to i64
  %1895 = add i64 %428, %1892
  %1896 = add i64 %1895, %1894
  %1897 = ashr i64 %1896, %431
  %1898 = trunc i64 %1897 to i32
  store i32 %1898, i32* %107, align 4
  %1899 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 30
  %1900 = load i32, i32* %1899, align 8
  %1901 = load i32, i32* %376, align 4
  %1902 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 34
  %1903 = load i32, i32* %1902, align 8
  %1904 = load i32, i32* %398, align 8
  %1905 = mul nsw i32 %1901, %1900
  %1906 = sext i32 %1905 to i64
  %1907 = mul nsw i32 %1904, %1903
  %1908 = sext i32 %1907 to i64
  %1909 = add i64 %428, %1906
  %1910 = add i64 %1909, %1908
  %1911 = ashr i64 %1910, %431
  %1912 = trunc i64 %1911 to i32
  store i32 %1912, i32* %113, align 4
  %1913 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 46
  %1914 = load i32, i32* %1913, align 8
  %1915 = load i32, i32* %377, align 8
  %1916 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 18
  %1917 = load i32, i32* %1916, align 8
  %1918 = load i32, i32* %397, align 4
  %1919 = mul nsw i32 %1915, %1914
  %1920 = sext i32 %1919 to i64
  %1921 = mul nsw i32 %1918, %1917
  %1922 = sext i32 %1921 to i64
  %1923 = add i64 %428, %1920
  %1924 = add i64 %1923, %1922
  %1925 = ashr i64 %1924, %431
  %1926 = trunc i64 %1925 to i32
  store i32 %1926, i32* %119, align 4
  %1927 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 14
  %1928 = load i32, i32* %1927, align 8
  %1929 = load i32, i32* %380, align 4
  %1930 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 50
  %1931 = load i32, i32* %1930, align 8
  %1932 = load i32, i32* %396, align 16
  %1933 = mul nsw i32 %1929, %1928
  %1934 = sext i32 %1933 to i64
  %1935 = mul nsw i32 %1932, %1931
  %1936 = sext i32 %1935 to i64
  %1937 = add i64 %428, %1934
  %1938 = add i64 %1937, %1936
  %1939 = ashr i64 %1938, %431
  %1940 = trunc i64 %1939 to i32
  store i32 %1940, i32* %125, align 4
  %1941 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 54
  %1942 = load i32, i32* %1941, align 8
  %1943 = load i32, i32* %382, align 16
  %1944 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 10
  %1945 = load i32, i32* %1944, align 8
  %1946 = load i32, i32* %394, align 4
  %1947 = mul nsw i32 %1943, %1942
  %1948 = sext i32 %1947 to i64
  %1949 = mul nsw i32 %1946, %1945
  %1950 = sext i32 %1949 to i64
  %1951 = add i64 %428, %1948
  %1952 = add i64 %1951, %1950
  %1953 = ashr i64 %1952, %431
  %1954 = trunc i64 %1953 to i32
  store i32 %1954, i32* %131, align 4
  %1955 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 22
  %1956 = load i32, i32* %1955, align 8
  %1957 = load i32, i32* %383, align 4
  %1958 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 42
  %1959 = load i32, i32* %1958, align 8
  %1960 = load i32, i32* %391, align 8
  %1961 = mul nsw i32 %1957, %1956
  %1962 = sext i32 %1961 to i64
  %1963 = mul nsw i32 %1960, %1959
  %1964 = sext i32 %1963 to i64
  %1965 = add i64 %428, %1962
  %1966 = add i64 %1965, %1964
  %1967 = ashr i64 %1966, %431
  %1968 = trunc i64 %1967 to i32
  store i32 %1968, i32* %137, align 4
  %1969 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 38
  %1970 = load i32, i32* %1969, align 8
  %1971 = load i32, i32* %384, align 8
  %1972 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 26
  %1973 = load i32, i32* %1972, align 8
  %1974 = load i32, i32* %390, align 4
  %1975 = mul nsw i32 %1971, %1970
  %1976 = sext i32 %1975 to i64
  %1977 = mul nsw i32 %1974, %1973
  %1978 = sext i32 %1977 to i64
  %1979 = add i64 %428, %1976
  %1980 = add i64 %1979, %1978
  %1981 = ashr i64 %1980, %431
  %1982 = trunc i64 %1981 to i32
  store i32 %1982, i32* %143, align 4
  %1983 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 6
  %1984 = load i32, i32* %1983, align 8
  %1985 = load i32, i32* %387, align 4
  %1986 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 58
  %1987 = load i32, i32* %1986, align 8
  %1988 = load i32, i32* %389, align 16
  %1989 = mul nsw i32 %1985, %1984
  %1990 = sext i32 %1989 to i64
  %1991 = mul nsw i32 %1988, %1987
  %1992 = sext i32 %1991 to i64
  %1993 = add i64 %428, %1990
  %1994 = add i64 %1993, %1992
  %1995 = ashr i64 %1994, %431
  %1996 = trunc i64 %1995 to i32
  store i32 %1996, i32* %149, align 4
  %1997 = mul nsw i32 %1988, %1984
  %1998 = sext i32 %1997 to i64
  %1999 = mul i32 %1987, %1985
  %2000 = sub i32 0, %1999
  %2001 = sext i32 %2000 to i64
  %2002 = add i64 %428, %1998
  %2003 = add i64 %2002, %2001
  %2004 = ashr i64 %2003, %431
  %2005 = trunc i64 %2004 to i32
  store i32 %2005, i32* %155, align 4
  %2006 = mul nsw i32 %1974, %1970
  %2007 = sext i32 %2006 to i64
  %2008 = mul i32 %1973, %1971
  %2009 = sub i32 0, %2008
  %2010 = sext i32 %2009 to i64
  %2011 = add i64 %428, %2007
  %2012 = add i64 %2011, %2010
  %2013 = ashr i64 %2012, %431
  %2014 = trunc i64 %2013 to i32
  store i32 %2014, i32* %161, align 4
  %2015 = mul nsw i32 %1960, %1956
  %2016 = sext i32 %2015 to i64
  %2017 = mul i32 %1959, %1957
  %2018 = sub i32 0, %2017
  %2019 = sext i32 %2018 to i64
  %2020 = add i64 %428, %2016
  %2021 = add i64 %2020, %2019
  %2022 = ashr i64 %2021, %431
  %2023 = trunc i64 %2022 to i32
  store i32 %2023, i32* %167, align 4
  %2024 = mul nsw i32 %1946, %1942
  %2025 = sext i32 %2024 to i64
  %2026 = mul i32 %1945, %1943
  %2027 = sub i32 0, %2026
  %2028 = sext i32 %2027 to i64
  %2029 = add i64 %428, %2025
  %2030 = add i64 %2029, %2028
  %2031 = ashr i64 %2030, %431
  %2032 = trunc i64 %2031 to i32
  store i32 %2032, i32* %173, align 4
  %2033 = load i32, i32* %396, align 16
  %2034 = load i32, i32* %380, align 4
  %2035 = mul nsw i32 %2033, %1928
  %2036 = sext i32 %2035 to i64
  %2037 = mul i32 %1931, %2034
  %2038 = sub i32 0, %2037
  %2039 = sext i32 %2038 to i64
  %2040 = add i64 %428, %2036
  %2041 = add i64 %2040, %2039
  %2042 = ashr i64 %2041, %431
  %2043 = trunc i64 %2042 to i32
  store i32 %2043, i32* %179, align 4
  %2044 = load i32, i32* %397, align 4
  %2045 = load i32, i32* %377, align 8
  %2046 = mul nsw i32 %2044, %1914
  %2047 = sext i32 %2046 to i64
  %2048 = mul i32 %1917, %2045
  %2049 = sub i32 0, %2048
  %2050 = sext i32 %2049 to i64
  %2051 = add i64 %428, %2047
  %2052 = add i64 %2051, %2050
  %2053 = ashr i64 %2052, %431
  %2054 = trunc i64 %2053 to i32
  store i32 %2054, i32* %185, align 4
  %2055 = load i32, i32* %398, align 8
  %2056 = load i32, i32* %376, align 4
  %2057 = mul nsw i32 %2055, %1900
  %2058 = sext i32 %2057 to i64
  %2059 = mul i32 %1903, %2056
  %2060 = sub i32 0, %2059
  %2061 = sext i32 %2060 to i64
  %2062 = add i64 %428, %2058
  %2063 = add i64 %2062, %2061
  %2064 = ashr i64 %2063, %431
  %2065 = trunc i64 %2064 to i32
  store i32 %2065, i32* %191, align 4
  %2066 = load i32, i32* %401, align 4
  %2067 = load i32, i32* %375, align 16
  %2068 = mul nsw i32 %2066, %1886
  %2069 = sext i32 %2068 to i64
  %2070 = mul i32 %1889, %2067
  %2071 = sub i32 0, %2070
  %2072 = sext i32 %2071 to i64
  %2073 = add i64 %428, %2069
  %2074 = add i64 %2073, %2072
  %2075 = ashr i64 %2074, %431
  %2076 = trunc i64 %2075 to i32
  store i32 %2076, i32* %197, align 4
  %2077 = bitcast i32* %403 to <4 x i32>*
  %2078 = load <4 x i32>, <4 x i32>* %2077, align 16
  %2079 = shufflevector <4 x i32> %2078, <4 x i32> undef, <4 x i32> <i32 1, i32 0, i32 3, i32 2>
  %2080 = add nsw <4 x i32> %2079, %2078
  %2081 = sub <4 x i32> %2079, %2078
  %2082 = shufflevector <4 x i32> %2080, <4 x i32> %2081, <4 x i32> <i32 0, i32 5, i32 6, i32 3>
  %2083 = bitcast i32* %201 to <4 x i32>*
  store <4 x i32> %2082, <4 x i32>* %2083, align 4
  %2084 = bitcast i32* %410 to <4 x i32>*
  %2085 = load <4 x i32>, <4 x i32>* %2084, align 16
  %2086 = shufflevector <4 x i32> %2085, <4 x i32> undef, <4 x i32> <i32 1, i32 0, i32 3, i32 2>
  %2087 = add nsw <4 x i32> %2086, %2085
  %2088 = sub <4 x i32> %2086, %2085
  %2089 = shufflevector <4 x i32> %2087, <4 x i32> %2088, <4 x i32> <i32 0, i32 5, i32 6, i32 3>
  %2090 = bitcast i32* %217 to <4 x i32>*
  store <4 x i32> %2089, <4 x i32>* %2090, align 4
  %2091 = bitcast i32* %434 to <4 x i32>*
  %2092 = load <4 x i32>, <4 x i32>* %2091, align 16
  %2093 = shufflevector <4 x i32> %2092, <4 x i32> undef, <4 x i32> <i32 1, i32 0, i32 3, i32 2>
  %2094 = add nsw <4 x i32> %2093, %2092
  %2095 = sub <4 x i32> %2093, %2092
  %2096 = shufflevector <4 x i32> %2094, <4 x i32> %2095, <4 x i32> <i32 0, i32 5, i32 6, i32 3>
  %2097 = bitcast i32* %233 to <4 x i32>*
  store <4 x i32> %2096, <4 x i32>* %2097, align 4
  %2098 = bitcast i32* %478 to <4 x i32>*
  %2099 = load <4 x i32>, <4 x i32>* %2098, align 16
  %2100 = shufflevector <4 x i32> %2099, <4 x i32> undef, <4 x i32> <i32 1, i32 0, i32 3, i32 2>
  %2101 = add nsw <4 x i32> %2100, %2099
  %2102 = sub <4 x i32> %2100, %2099
  %2103 = shufflevector <4 x i32> %2101, <4 x i32> %2102, <4 x i32> <i32 0, i32 5, i32 6, i32 3>
  %2104 = bitcast i32* %249 to <4 x i32>*
  store <4 x i32> %2103, <4 x i32>* %2104, align 4
  %2105 = bitcast i32* %517 to <4 x i32>*
  %2106 = load <4 x i32>, <4 x i32>* %2105, align 16
  %2107 = shufflevector <4 x i32> %2106, <4 x i32> undef, <4 x i32> <i32 1, i32 0, i32 3, i32 2>
  %2108 = add nsw <4 x i32> %2107, %2106
  %2109 = sub <4 x i32> %2107, %2106
  %2110 = shufflevector <4 x i32> %2108, <4 x i32> %2109, <4 x i32> <i32 0, i32 5, i32 6, i32 3>
  %2111 = bitcast i32* %265 to <4 x i32>*
  store <4 x i32> %2110, <4 x i32>* %2111, align 4
  %2112 = bitcast i32* %541 to <4 x i32>*
  %2113 = load <4 x i32>, <4 x i32>* %2112, align 16
  %2114 = shufflevector <4 x i32> %2113, <4 x i32> undef, <4 x i32> <i32 1, i32 0, i32 3, i32 2>
  %2115 = add nsw <4 x i32> %2114, %2113
  %2116 = sub <4 x i32> %2114, %2113
  %2117 = shufflevector <4 x i32> %2115, <4 x i32> %2116, <4 x i32> <i32 0, i32 5, i32 6, i32 3>
  %2118 = bitcast i32* %281 to <4 x i32>*
  store <4 x i32> %2117, <4 x i32>* %2118, align 4
  %2119 = bitcast i32* %560 to <4 x i32>*
  %2120 = load <4 x i32>, <4 x i32>* %2119, align 16
  %2121 = shufflevector <4 x i32> %2120, <4 x i32> undef, <4 x i32> <i32 1, i32 0, i32 3, i32 2>
  %2122 = add nsw <4 x i32> %2121, %2120
  %2123 = sub <4 x i32> %2121, %2120
  %2124 = shufflevector <4 x i32> %2122, <4 x i32> %2123, <4 x i32> <i32 0, i32 5, i32 6, i32 3>
  %2125 = bitcast i32* %297 to <4 x i32>*
  store <4 x i32> %2124, <4 x i32>* %2125, align 4
  %2126 = bitcast i32* %567 to <4 x i32>*
  %2127 = load <4 x i32>, <4 x i32>* %2126, align 16
  %2128 = shufflevector <4 x i32> %2127, <4 x i32> undef, <4 x i32> <i32 1, i32 0, i32 3, i32 2>
  %2129 = add nsw <4 x i32> %2128, %2127
  %2130 = sub <4 x i32> %2128, %2127
  %2131 = shufflevector <4 x i32> %2129, <4 x i32> %2130, <4 x i32> <i32 0, i32 5, i32 6, i32 3>
  %2132 = bitcast i32* %313 to <4 x i32>*
  store <4 x i32> %2131, <4 x i32>* %2132, align 4
  %2133 = getelementptr inbounds i8, i8* %3, i64 9
  %2134 = load i8, i8* %2133, align 1
  call void @av1_range_check_buf(i32 9, i32* %0, i32* %1, i32 64, i8 signext %2134) #3
  %2135 = bitcast i32* %1 to <4 x i32>*
  %2136 = load <4 x i32>, <4 x i32>* %2135, align 4
  %2137 = bitcast [64 x i32]* %5 to <4 x i32>*
  store <4 x i32> %2136, <4 x i32>* %2137, align 16
  %2138 = bitcast i32* %35 to <4 x i32>*
  %2139 = load <4 x i32>, <4 x i32>* %2138, align 4
  %2140 = bitcast i32* %342 to <4 x i32>*
  store <4 x i32> %2139, <4 x i32>* %2140, align 16
  %2141 = bitcast i32* %59 to <4 x i32>*
  %2142 = load <4 x i32>, <4 x i32>* %2141, align 4
  %2143 = bitcast i32* %353 to <4 x i32>*
  store <4 x i32> %2142, <4 x i32>* %2143, align 16
  %2144 = bitcast i32* %83 to <4 x i32>*
  %2145 = load <4 x i32>, <4 x i32>* %2144, align 4
  %2146 = bitcast i32* %364 to <4 x i32>*
  store <4 x i32> %2145, <4 x i32>* %2146, align 16
  %2147 = bitcast i32* %107 to <4 x i32>*
  %2148 = load <4 x i32>, <4 x i32>* %2147, align 4
  %2149 = bitcast i32* %375 to <4 x i32>*
  store <4 x i32> %2148, <4 x i32>* %2149, align 16
  %2150 = bitcast i32* %131 to <4 x i32>*
  %2151 = load <4 x i32>, <4 x i32>* %2150, align 4
  %2152 = bitcast i32* %382 to <4 x i32>*
  store <4 x i32> %2151, <4 x i32>* %2152, align 16
  %2153 = bitcast i32* %155 to <4 x i32>*
  %2154 = load <4 x i32>, <4 x i32>* %2153, align 4
  %2155 = bitcast i32* %389 to <4 x i32>*
  store <4 x i32> %2154, <4 x i32>* %2155, align 16
  %2156 = bitcast i32* %179 to <4 x i32>*
  %2157 = load <4 x i32>, <4 x i32>* %2156, align 4
  %2158 = bitcast i32* %396 to <4 x i32>*
  store <4 x i32> %2157, <4 x i32>* %2158, align 16
  %2159 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 63
  %2160 = load i32, i32* %2159, align 4
  %2161 = load i32, i32* %201, align 4
  %2162 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 1
  %2163 = load i32, i32* %2162, align 4
  %2164 = load i32, i32* %325, align 4
  %2165 = mul nsw i32 %2161, %2160
  %2166 = sext i32 %2165 to i64
  %2167 = mul nsw i32 %2164, %2163
  %2168 = sext i32 %2167 to i64
  %2169 = add i64 %428, %2166
  %2170 = add i64 %2169, %2168
  %2171 = ashr i64 %2170, %431
  %2172 = trunc i64 %2171 to i32
  store i32 %2172, i32* %403, align 16
  %2173 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 31
  %2174 = load i32, i32* %2173, align 4
  %2175 = load i32, i32* %205, align 4
  %2176 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 33
  %2177 = load i32, i32* %2176, align 4
  %2178 = load i32, i32* %321, align 4
  %2179 = mul nsw i32 %2175, %2174
  %2180 = sext i32 %2179 to i64
  %2181 = mul nsw i32 %2178, %2177
  %2182 = sext i32 %2181 to i64
  %2183 = add i64 %428, %2180
  %2184 = add i64 %2183, %2182
  %2185 = ashr i64 %2184, %431
  %2186 = trunc i64 %2185 to i32
  store i32 %2186, i32* %404, align 4
  %2187 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 47
  %2188 = load i32, i32* %2187, align 4
  %2189 = load i32, i32* %209, align 4
  %2190 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 17
  %2191 = load i32, i32* %2190, align 4
  %2192 = load i32, i32* %317, align 4
  %2193 = mul nsw i32 %2189, %2188
  %2194 = sext i32 %2193 to i64
  %2195 = mul nsw i32 %2192, %2191
  %2196 = sext i32 %2195 to i64
  %2197 = add i64 %428, %2194
  %2198 = add i64 %2197, %2196
  %2199 = ashr i64 %2198, %431
  %2200 = trunc i64 %2199 to i32
  store i32 %2200, i32* %405, align 8
  %2201 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 15
  %2202 = load i32, i32* %2201, align 4
  %2203 = load i32, i32* %213, align 4
  %2204 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 49
  %2205 = load i32, i32* %2204, align 4
  %2206 = load i32, i32* %313, align 4
  %2207 = mul nsw i32 %2203, %2202
  %2208 = sext i32 %2207 to i64
  %2209 = mul nsw i32 %2206, %2205
  %2210 = sext i32 %2209 to i64
  %2211 = add i64 %428, %2208
  %2212 = add i64 %2211, %2210
  %2213 = ashr i64 %2212, %431
  %2214 = trunc i64 %2213 to i32
  store i32 %2214, i32* %408, align 4
  %2215 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 55
  %2216 = load i32, i32* %2215, align 4
  %2217 = load i32, i32* %217, align 4
  %2218 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 9
  %2219 = load i32, i32* %2218, align 4
  %2220 = load i32, i32* %309, align 4
  %2221 = mul nsw i32 %2217, %2216
  %2222 = sext i32 %2221 to i64
  %2223 = mul nsw i32 %2220, %2219
  %2224 = sext i32 %2223 to i64
  %2225 = add i64 %428, %2222
  %2226 = add i64 %2225, %2224
  %2227 = ashr i64 %2226, %431
  %2228 = trunc i64 %2227 to i32
  store i32 %2228, i32* %410, align 16
  %2229 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 23
  %2230 = load i32, i32* %2229, align 4
  %2231 = load i32, i32* %221, align 4
  %2232 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 41
  %2233 = load i32, i32* %2232, align 4
  %2234 = load i32, i32* %305, align 4
  %2235 = mul nsw i32 %2231, %2230
  %2236 = sext i32 %2235 to i64
  %2237 = mul nsw i32 %2234, %2233
  %2238 = sext i32 %2237 to i64
  %2239 = add i64 %428, %2236
  %2240 = add i64 %2239, %2238
  %2241 = ashr i64 %2240, %431
  %2242 = trunc i64 %2241 to i32
  store i32 %2242, i32* %411, align 4
  %2243 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 39
  %2244 = load i32, i32* %2243, align 4
  %2245 = load i32, i32* %225, align 4
  %2246 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 25
  %2247 = load i32, i32* %2246, align 4
  %2248 = load i32, i32* %301, align 4
  %2249 = mul nsw i32 %2245, %2244
  %2250 = sext i32 %2249 to i64
  %2251 = mul nsw i32 %2248, %2247
  %2252 = sext i32 %2251 to i64
  %2253 = add i64 %428, %2250
  %2254 = add i64 %2253, %2252
  %2255 = ashr i64 %2254, %431
  %2256 = trunc i64 %2255 to i32
  store i32 %2256, i32* %412, align 8
  %2257 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 7
  %2258 = load i32, i32* %2257, align 4
  %2259 = load i32, i32* %229, align 4
  %2260 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 57
  %2261 = load i32, i32* %2260, align 4
  %2262 = load i32, i32* %297, align 4
  %2263 = mul nsw i32 %2259, %2258
  %2264 = sext i32 %2263 to i64
  %2265 = mul nsw i32 %2262, %2261
  %2266 = sext i32 %2265 to i64
  %2267 = add i64 %428, %2264
  %2268 = add i64 %2267, %2266
  %2269 = ashr i64 %2268, %431
  %2270 = trunc i64 %2269 to i32
  store i32 %2270, i32* %415, align 4
  %2271 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 59
  %2272 = load i32, i32* %2271, align 4
  %2273 = load i32, i32* %233, align 4
  %2274 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 5
  %2275 = load i32, i32* %2274, align 4
  %2276 = load i32, i32* %293, align 4
  %2277 = mul nsw i32 %2273, %2272
  %2278 = sext i32 %2277 to i64
  %2279 = mul nsw i32 %2276, %2275
  %2280 = sext i32 %2279 to i64
  %2281 = add i64 %428, %2278
  %2282 = add i64 %2281, %2280
  %2283 = ashr i64 %2282, %431
  %2284 = trunc i64 %2283 to i32
  store i32 %2284, i32* %434, align 16
  %2285 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 27
  %2286 = load i32, i32* %2285, align 4
  %2287 = load i32, i32* %237, align 4
  %2288 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 37
  %2289 = load i32, i32* %2288, align 4
  %2290 = load i32, i32* %289, align 4
  %2291 = mul nsw i32 %2287, %2286
  %2292 = sext i32 %2291 to i64
  %2293 = mul nsw i32 %2290, %2289
  %2294 = sext i32 %2293 to i64
  %2295 = add i64 %428, %2292
  %2296 = add i64 %2295, %2294
  %2297 = ashr i64 %2296, %431
  %2298 = trunc i64 %2297 to i32
  store i32 %2298, i32* %445, align 4
  %2299 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 43
  %2300 = load i32, i32* %2299, align 4
  %2301 = load i32, i32* %241, align 4
  %2302 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 21
  %2303 = load i32, i32* %2302, align 4
  %2304 = load i32, i32* %285, align 4
  %2305 = mul nsw i32 %2301, %2300
  %2306 = sext i32 %2305 to i64
  %2307 = mul nsw i32 %2304, %2303
  %2308 = sext i32 %2307 to i64
  %2309 = add i64 %428, %2306
  %2310 = add i64 %2309, %2308
  %2311 = ashr i64 %2310, %431
  %2312 = trunc i64 %2311 to i32
  store i32 %2312, i32* %456, align 8
  %2313 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 11
  %2314 = load i32, i32* %2313, align 4
  %2315 = load i32, i32* %245, align 4
  %2316 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 53
  %2317 = load i32, i32* %2316, align 4
  %2318 = load i32, i32* %281, align 4
  %2319 = mul nsw i32 %2315, %2314
  %2320 = sext i32 %2319 to i64
  %2321 = mul nsw i32 %2318, %2317
  %2322 = sext i32 %2321 to i64
  %2323 = add i64 %428, %2320
  %2324 = add i64 %2323, %2322
  %2325 = ashr i64 %2324, %431
  %2326 = trunc i64 %2325 to i32
  store i32 %2326, i32* %467, align 4
  %2327 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 51
  %2328 = load i32, i32* %2327, align 4
  %2329 = load i32, i32* %249, align 4
  %2330 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 13
  %2331 = load i32, i32* %2330, align 4
  %2332 = load i32, i32* %277, align 4
  %2333 = mul nsw i32 %2329, %2328
  %2334 = sext i32 %2333 to i64
  %2335 = mul nsw i32 %2332, %2331
  %2336 = sext i32 %2335 to i64
  %2337 = add i64 %428, %2334
  %2338 = add i64 %2337, %2336
  %2339 = ashr i64 %2338, %431
  %2340 = trunc i64 %2339 to i32
  store i32 %2340, i32* %478, align 16
  %2341 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 19
  %2342 = load i32, i32* %2341, align 4
  %2343 = load i32, i32* %253, align 4
  %2344 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 45
  %2345 = load i32, i32* %2344, align 4
  %2346 = load i32, i32* %273, align 4
  %2347 = mul nsw i32 %2343, %2342
  %2348 = sext i32 %2347 to i64
  %2349 = mul nsw i32 %2346, %2345
  %2350 = sext i32 %2349 to i64
  %2351 = add i64 %428, %2348
  %2352 = add i64 %2351, %2350
  %2353 = ashr i64 %2352, %431
  %2354 = trunc i64 %2353 to i32
  store i32 %2354, i32* %489, align 4
  %2355 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 35
  %2356 = load i32, i32* %2355, align 4
  %2357 = load i32, i32* %257, align 4
  %2358 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 29
  %2359 = load i32, i32* %2358, align 4
  %2360 = load i32, i32* %269, align 4
  %2361 = mul nsw i32 %2357, %2356
  %2362 = sext i32 %2361 to i64
  %2363 = mul nsw i32 %2360, %2359
  %2364 = sext i32 %2363 to i64
  %2365 = add i64 %428, %2362
  %2366 = add i64 %2365, %2364
  %2367 = ashr i64 %2366, %431
  %2368 = trunc i64 %2367 to i32
  store i32 %2368, i32* %500, align 8
  %2369 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 3
  %2370 = load i32, i32* %2369, align 4
  %2371 = load i32, i32* %261, align 4
  %2372 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %330, i64 61
  %2373 = load i32, i32* %2372, align 4
  %2374 = load i32, i32* %265, align 4
  %2375 = mul nsw i32 %2371, %2370
  %2376 = sext i32 %2375 to i64
  %2377 = mul nsw i32 %2374, %2373
  %2378 = sext i32 %2377 to i64
  %2379 = add i64 %428, %2376
  %2380 = add i64 %2379, %2378
  %2381 = ashr i64 %2380, %431
  %2382 = trunc i64 %2381 to i32
  store i32 %2382, i32* %511, align 4
  %2383 = mul nsw i32 %2374, %2370
  %2384 = sext i32 %2383 to i64
  %2385 = mul i32 %2373, %2371
  %2386 = sub i32 0, %2385
  %2387 = sext i32 %2386 to i64
  %2388 = add i64 %428, %2384
  %2389 = add i64 %2388, %2387
  %2390 = ashr i64 %2389, %431
  %2391 = trunc i64 %2390 to i32
  store i32 %2391, i32* %517, align 16
  %2392 = mul nsw i32 %2360, %2356
  %2393 = sext i32 %2392 to i64
  %2394 = mul i32 %2359, %2357
  %2395 = sub i32 0, %2394
  %2396 = sext i32 %2395 to i64
  %2397 = add i64 %428, %2393
  %2398 = add i64 %2397, %2396
  %2399 = ashr i64 %2398, %431
  %2400 = trunc i64 %2399 to i32
  store i32 %2400, i32* %523, align 4
  %2401 = mul nsw i32 %2346, %2342
  %2402 = sext i32 %2401 to i64
  %2403 = mul i32 %2345, %2343
  %2404 = sub i32 0, %2403
  %2405 = sext i32 %2404 to i64
  %2406 = add i64 %428, %2402
  %2407 = add i64 %2406, %2405
  %2408 = ashr i64 %2407, %431
  %2409 = trunc i64 %2408 to i32
  store i32 %2409, i32* %529, align 8
  %2410 = mul nsw i32 %2332, %2328
  %2411 = sext i32 %2410 to i64
  %2412 = mul i32 %2331, %2329
  %2413 = sub i32 0, %2412
  %2414 = sext i32 %2413 to i64
  %2415 = add i64 %428, %2411
  %2416 = add i64 %2415, %2414
  %2417 = ashr i64 %2416, %431
  %2418 = trunc i64 %2417 to i32
  store i32 %2418, i32* %535, align 4
  %2419 = load i32, i32* %281, align 4
  %2420 = load i32, i32* %245, align 4
  %2421 = mul nsw i32 %2419, %2314
  %2422 = sext i32 %2421 to i64
  %2423 = mul i32 %2317, %2420
  %2424 = sub i32 0, %2423
  %2425 = sext i32 %2424 to i64
  %2426 = add i64 %428, %2422
  %2427 = add i64 %2426, %2425
  %2428 = ashr i64 %2427, %431
  %2429 = trunc i64 %2428 to i32
  store i32 %2429, i32* %541, align 16
  %2430 = load i32, i32* %285, align 4
  %2431 = load i32, i32* %241, align 4
  %2432 = mul nsw i32 %2430, %2300
  %2433 = sext i32 %2432 to i64
  %2434 = mul i32 %2303, %2431
  %2435 = sub i32 0, %2434
  %2436 = sext i32 %2435 to i64
  %2437 = add i64 %428, %2433
  %2438 = add i64 %2437, %2436
  %2439 = ashr i64 %2438, %431
  %2440 = trunc i64 %2439 to i32
  store i32 %2440, i32* %547, align 4
  %2441 = load i32, i32* %289, align 4
  %2442 = load i32, i32* %237, align 4
  %2443 = mul nsw i32 %2441, %2286
  %2444 = sext i32 %2443 to i64
  %2445 = mul i32 %2289, %2442
  %2446 = sub i32 0, %2445
  %2447 = sext i32 %2446 to i64
  %2448 = add i64 %428, %2444
  %2449 = add i64 %2448, %2447
  %2450 = ashr i64 %2449, %431
  %2451 = trunc i64 %2450 to i32
  store i32 %2451, i32* %553, align 8
  %2452 = load i32, i32* %293, align 4
  %2453 = load i32, i32* %233, align 4
  %2454 = mul nsw i32 %2452, %2272
  %2455 = sext i32 %2454 to i64
  %2456 = mul i32 %2275, %2453
  %2457 = sub i32 0, %2456
  %2458 = sext i32 %2457 to i64
  %2459 = add i64 %428, %2455
  %2460 = add i64 %2459, %2458
  %2461 = ashr i64 %2460, %431
  %2462 = trunc i64 %2461 to i32
  store i32 %2462, i32* %559, align 4
  %2463 = load i32, i32* %297, align 4
  %2464 = load i32, i32* %229, align 4
  %2465 = mul nsw i32 %2463, %2258
  %2466 = sext i32 %2465 to i64
  %2467 = mul i32 %2261, %2464
  %2468 = sub i32 0, %2467
  %2469 = sext i32 %2468 to i64
  %2470 = add i64 %428, %2466
  %2471 = add i64 %2470, %2469
  %2472 = ashr i64 %2471, %431
  %2473 = trunc i64 %2472 to i32
  store i32 %2473, i32* %560, align 16
  %2474 = load i32, i32* %301, align 4
  %2475 = load i32, i32* %225, align 4
  %2476 = mul nsw i32 %2474, %2244
  %2477 = sext i32 %2476 to i64
  %2478 = mul i32 %2247, %2475
  %2479 = sub i32 0, %2478
  %2480 = sext i32 %2479 to i64
  %2481 = add i64 %428, %2477
  %2482 = add i64 %2481, %2480
  %2483 = ashr i64 %2482, %431
  %2484 = trunc i64 %2483 to i32
  store i32 %2484, i32* %561, align 4
  %2485 = load i32, i32* %305, align 4
  %2486 = load i32, i32* %221, align 4
  %2487 = mul nsw i32 %2485, %2230
  %2488 = sext i32 %2487 to i64
  %2489 = mul i32 %2233, %2486
  %2490 = sub i32 0, %2489
  %2491 = sext i32 %2490 to i64
  %2492 = add i64 %428, %2488
  %2493 = add i64 %2492, %2491
  %2494 = ashr i64 %2493, %431
  %2495 = trunc i64 %2494 to i32
  store i32 %2495, i32* %562, align 8
  %2496 = load i32, i32* %309, align 4
  %2497 = load i32, i32* %217, align 4
  %2498 = mul nsw i32 %2496, %2216
  %2499 = sext i32 %2498 to i64
  %2500 = mul i32 %2219, %2497
  %2501 = sub i32 0, %2500
  %2502 = sext i32 %2501 to i64
  %2503 = add i64 %428, %2499
  %2504 = add i64 %2503, %2502
  %2505 = ashr i64 %2504, %431
  %2506 = trunc i64 %2505 to i32
  store i32 %2506, i32* %565, align 4
  %2507 = load i32, i32* %313, align 4
  %2508 = load i32, i32* %213, align 4
  %2509 = mul nsw i32 %2507, %2202
  %2510 = sext i32 %2509 to i64
  %2511 = mul i32 %2205, %2508
  %2512 = sub i32 0, %2511
  %2513 = sext i32 %2512 to i64
  %2514 = add i64 %428, %2510
  %2515 = add i64 %2514, %2513
  %2516 = ashr i64 %2515, %431
  %2517 = trunc i64 %2516 to i32
  store i32 %2517, i32* %567, align 16
  %2518 = load i32, i32* %317, align 4
  %2519 = load i32, i32* %209, align 4
  %2520 = mul nsw i32 %2518, %2188
  %2521 = sext i32 %2520 to i64
  %2522 = mul i32 %2191, %2519
  %2523 = sub i32 0, %2522
  %2524 = sext i32 %2523 to i64
  %2525 = add i64 %428, %2521
  %2526 = add i64 %2525, %2524
  %2527 = ashr i64 %2526, %431
  %2528 = trunc i64 %2527 to i32
  store i32 %2528, i32* %568, align 4
  %2529 = load i32, i32* %321, align 4
  %2530 = load i32, i32* %205, align 4
  %2531 = mul nsw i32 %2529, %2174
  %2532 = sext i32 %2531 to i64
  %2533 = mul i32 %2177, %2530
  %2534 = sub i32 0, %2533
  %2535 = sext i32 %2534 to i64
  %2536 = add i64 %428, %2532
  %2537 = add i64 %2536, %2535
  %2538 = ashr i64 %2537, %431
  %2539 = trunc i64 %2538 to i32
  store i32 %2539, i32* %569, align 8
  %2540 = load i32, i32* %325, align 4
  %2541 = load i32, i32* %201, align 4
  %2542 = mul nsw i32 %2540, %2160
  %2543 = sext i32 %2542 to i64
  %2544 = mul i32 %2163, %2541
  %2545 = sub i32 0, %2544
  %2546 = sext i32 %2545 to i64
  %2547 = add i64 %428, %2543
  %2548 = add i64 %2547, %2546
  %2549 = ashr i64 %2548, %431
  %2550 = trunc i64 %2549 to i32
  store i32 %2550, i32* %572, align 4
  %2551 = getelementptr inbounds i8, i8* %3, i64 10
  %2552 = load i8, i8* %2551, align 1
  call void @av1_range_check_buf(i32 10, i32* %0, i32* nonnull %331, i32 64, i8 signext %2552) #3
  %2553 = load i32, i32* %331, align 16
  store i32 %2553, i32* %1, align 4
  %2554 = load i32, i32* %403, align 16
  store i32 %2554, i32* %17, align 4
  %2555 = load i32, i32* %375, align 16
  store i32 %2555, i32* %23, align 4
  %2556 = load i32, i32* %517, align 16
  store i32 %2556, i32* %29, align 4
  %2557 = load i32, i32* %353, align 16
  store i32 %2557, i32* %35, align 4
  %2558 = load i32, i32* %434, align 16
  store i32 %2558, i32* %41, align 4
  %2559 = load i32, i32* %389, align 16
  store i32 %2559, i32* %47, align 4
  %2560 = load i32, i32* %560, align 16
  store i32 %2560, i32* %53, align 4
  %2561 = load i32, i32* %342, align 16
  store i32 %2561, i32* %59, align 4
  %2562 = load i32, i32* %410, align 16
  store i32 %2562, i32* %65, align 4
  %2563 = load i32, i32* %382, align 16
  store i32 %2563, i32* %71, align 4
  %2564 = load i32, i32* %541, align 16
  store i32 %2564, i32* %77, align 4
  %2565 = load i32, i32* %364, align 16
  store i32 %2565, i32* %83, align 4
  %2566 = load i32, i32* %478, align 16
  store i32 %2566, i32* %89, align 4
  %2567 = load i32, i32* %396, align 16
  store i32 %2567, i32* %95, align 4
  %2568 = load i32, i32* %567, align 16
  store i32 %2568, i32* %101, align 4
  %2569 = load i32, i32* %333, align 8
  store i32 %2569, i32* %107, align 4
  %2570 = load i32, i32* %405, align 8
  store i32 %2570, i32* %113, align 4
  %2571 = load i32, i32* %377, align 8
  store i32 %2571, i32* %119, align 4
  %2572 = load i32, i32* %529, align 8
  store i32 %2572, i32* %125, align 4
  %2573 = load i32, i32* %355, align 8
  store i32 %2573, i32* %131, align 4
  %2574 = load i32, i32* %456, align 8
  store i32 %2574, i32* %137, align 4
  %2575 = load i32, i32* %391, align 8
  store i32 %2575, i32* %143, align 4
  %2576 = load i32, i32* %562, align 8
  store i32 %2576, i32* %149, align 4
  %2577 = load i32, i32* %344, align 8
  store i32 %2577, i32* %155, align 4
  %2578 = load i32, i32* %412, align 8
  store i32 %2578, i32* %161, align 4
  %2579 = load i32, i32* %384, align 8
  store i32 %2579, i32* %167, align 4
  %2580 = load i32, i32* %553, align 8
  store i32 %2580, i32* %173, align 4
  %2581 = load i32, i32* %366, align 8
  store i32 %2581, i32* %179, align 4
  %2582 = load i32, i32* %500, align 8
  store i32 %2582, i32* %185, align 4
  %2583 = load i32, i32* %398, align 8
  store i32 %2583, i32* %191, align 4
  %2584 = load i32, i32* %569, align 8
  store i32 %2584, i32* %197, align 4
  %2585 = load i32, i32* %332, align 4
  store i32 %2585, i32* %201, align 4
  %2586 = load i32, i32* %404, align 4
  store i32 %2586, i32* %205, align 4
  %2587 = load i32, i32* %376, align 4
  store i32 %2587, i32* %209, align 4
  %2588 = load i32, i32* %523, align 4
  store i32 %2588, i32* %213, align 4
  %2589 = load i32, i32* %354, align 4
  store i32 %2589, i32* %217, align 4
  %2590 = load i32, i32* %445, align 4
  store i32 %2590, i32* %221, align 4
  %2591 = load i32, i32* %390, align 4
  store i32 %2591, i32* %225, align 4
  %2592 = load i32, i32* %561, align 4
  store i32 %2592, i32* %229, align 4
  %2593 = load i32, i32* %343, align 4
  store i32 %2593, i32* %233, align 4
  %2594 = load i32, i32* %411, align 4
  store i32 %2594, i32* %237, align 4
  %2595 = load i32, i32* %383, align 4
  store i32 %2595, i32* %241, align 4
  %2596 = load i32, i32* %547, align 4
  store i32 %2596, i32* %245, align 4
  %2597 = load i32, i32* %365, align 4
  store i32 %2597, i32* %249, align 4
  %2598 = load i32, i32* %489, align 4
  store i32 %2598, i32* %253, align 4
  %2599 = load i32, i32* %397, align 4
  store i32 %2599, i32* %257, align 4
  %2600 = load i32, i32* %568, align 4
  store i32 %2600, i32* %261, align 4
  %2601 = load i32, i32* %340, align 4
  store i32 %2601, i32* %265, align 4
  %2602 = load i32, i32* %408, align 4
  store i32 %2602, i32* %269, align 4
  %2603 = load i32, i32* %380, align 4
  store i32 %2603, i32* %273, align 4
  %2604 = load i32, i32* %535, align 4
  store i32 %2604, i32* %277, align 4
  %2605 = load i32, i32* %362, align 4
  store i32 %2605, i32* %281, align 4
  %2606 = load i32, i32* %467, align 4
  store i32 %2606, i32* %285, align 4
  %2607 = load i32, i32* %394, align 4
  store i32 %2607, i32* %289, align 4
  %2608 = load i32, i32* %565, align 4
  store i32 %2608, i32* %293, align 4
  %2609 = load i32, i32* %351, align 4
  store i32 %2609, i32* %297, align 4
  %2610 = load i32, i32* %415, align 4
  store i32 %2610, i32* %301, align 4
  %2611 = load i32, i32* %387, align 4
  store i32 %2611, i32* %305, align 4
  %2612 = load i32, i32* %559, align 4
  store i32 %2612, i32* %309, align 4
  %2613 = load i32, i32* %373, align 4
  store i32 %2613, i32* %313, align 4
  %2614 = load i32, i32* %511, align 4
  store i32 %2614, i32* %317, align 4
  %2615 = load i32, i32* %401, align 4
  store i32 %2615, i32* %321, align 4
  %2616 = load i32, i32* %572, align 4
  store i32 %2616, i32* %325, align 4
  %2617 = getelementptr inbounds i8, i8* %3, i64 11
  %2618 = load i8, i8* %2617, align 1
  call void @av1_range_check_buf(i32 11, i32* %0, i32* %1, i32 64, i8 signext %2618) #3
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %6) #3
  ret void
}

attributes #0 = { nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { argmemonly nounwind }
attributes #2 = { "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #3 = { nounwind }

!llvm.module.flags = !{!0, !1}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 2}
!2 = !{!3}
!3 = distinct !{!3, !4}
!4 = distinct !{!4, !"LVerDomain"}
!5 = !{!6}
!6 = distinct !{!6, !4}
!7 = distinct !{!7, !8}
!8 = !{!"llvm.loop.isvectorized", i32 1}
