; ModuleID = '../../third_party/libvpx/source/libvpx/vpx_dsp/x86/fwd_txfm_sse2.c'
source_filename = "../../third_party/libvpx/source/libvpx/vpx_dsp/x86/fwd_txfm_sse2.c"
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

; Function Attrs: nofree nounwind ssp uwtable
define hidden void @vpx_fdct4x4_1_sse2(i16* nocapture readonly, i32* nocapture, i32) local_unnamed_addr #0 {
  %4 = bitcast i16* %0 to i64*
  %5 = load i64, i64* %4, align 1
  %6 = insertelement <2 x i64> undef, i64 %5, i32 0
  %7 = sext i32 %2 to i64
  %8 = getelementptr inbounds i16, i16* %0, i64 %7
  %9 = bitcast i16* %8 to i64*
  %10 = load i64, i64* %9, align 1
  %11 = insertelement <2 x i64> undef, i64 %10, i32 0
  %12 = shl nsw i32 %2, 1
  %13 = sext i32 %12 to i64
  %14 = getelementptr inbounds i16, i16* %0, i64 %13
  %15 = bitcast i16* %14 to i64*
  %16 = load i64, i64* %15, align 1
  %17 = insertelement <2 x i64> %11, i64 %16, i32 1
  %18 = mul nsw i32 %2, 3
  %19 = sext i32 %18 to i64
  %20 = getelementptr inbounds i16, i16* %0, i64 %19
  %21 = bitcast i16* %20 to i64*
  %22 = load i64, i64* %21, align 1
  %23 = insertelement <2 x i64> %6, i64 %22, i32 1
  %24 = bitcast <2 x i64> %23 to <8 x i16>
  %25 = bitcast <2 x i64> %17 to <8 x i16>
  %26 = add <8 x i16> %24, %25
  %27 = shufflevector <8 x i16> <i16 0, i16 0, i16 0, i16 0, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i16> %26, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %28 = shufflevector <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 0, i16 0, i16 0, i16 0>, <8 x i16> %26, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %29 = bitcast <8 x i16> %27 to <4 x i32>
  %30 = ashr <4 x i32> %29, <i32 16, i32 16, i32 16, i32 16>
  %31 = bitcast <8 x i16> %28 to <4 x i32>
  %32 = ashr <4 x i32> %31, <i32 16, i32 16, i32 16, i32 16>
  %33 = add nsw <4 x i32> %32, %30
  %34 = shufflevector <4 x i32> %33, <4 x i32> <i32 0, i32 0, i32 undef, i32 undef>, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %35 = shufflevector <4 x i32> %33, <4 x i32> <i32 undef, i32 undef, i32 0, i32 0>, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %36 = add <4 x i32> %35, %34
  %37 = bitcast <4 x i32> %36 to <16 x i8>
  %38 = shufflevector <16 x i8> %37, <16 x i8> undef, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %39 = bitcast <16 x i8> %38 to <4 x i32>
  %40 = add <4 x i32> %36, %39
  %41 = extractelement <4 x i32> %40, i32 0
  %42 = shl i32 %41, 1
  store i32 %42, i32* %1, align 4
  ret void
}

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #1

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #1

; Function Attrs: nofree nounwind ssp uwtable
define hidden void @vpx_fdct8x8_1_sse2(i16* nocapture readonly, i32* nocapture, i32) local_unnamed_addr #0 {
  %4 = bitcast i16* %0 to <8 x i16>*
  %5 = load <8 x i16>, <8 x i16>* %4, align 16
  %6 = sext i32 %2 to i64
  %7 = getelementptr inbounds i16, i16* %0, i64 %6
  %8 = bitcast i16* %7 to <8 x i16>*
  %9 = load <8 x i16>, <8 x i16>* %8, align 16
  %10 = shl nsw i32 %2, 1
  %11 = sext i32 %10 to i64
  %12 = getelementptr inbounds i16, i16* %0, i64 %11
  %13 = bitcast i16* %12 to <8 x i16>*
  %14 = load <8 x i16>, <8 x i16>* %13, align 16
  %15 = mul nsw i32 %2, 3
  %16 = sext i32 %15 to i64
  %17 = getelementptr inbounds i16, i16* %0, i64 %16
  %18 = bitcast i16* %17 to <8 x i16>*
  %19 = load <8 x i16>, <8 x i16>* %18, align 16
  %20 = shl nsw i32 %2, 2
  %21 = sext i32 %20 to i64
  %22 = getelementptr inbounds i16, i16* %0, i64 %21
  %23 = bitcast i16* %22 to <8 x i16>*
  %24 = load <8 x i16>, <8 x i16>* %23, align 16
  %25 = mul nsw i32 %2, 5
  %26 = sext i32 %25 to i64
  %27 = getelementptr inbounds i16, i16* %0, i64 %26
  %28 = bitcast i16* %27 to <8 x i16>*
  %29 = load <8 x i16>, <8 x i16>* %28, align 16
  %30 = mul nsw i32 %2, 6
  %31 = sext i32 %30 to i64
  %32 = getelementptr inbounds i16, i16* %0, i64 %31
  %33 = bitcast i16* %32 to <8 x i16>*
  %34 = load <8 x i16>, <8 x i16>* %33, align 16
  %35 = mul nsw i32 %2, 7
  %36 = sext i32 %35 to i64
  %37 = getelementptr inbounds i16, i16* %0, i64 %36
  %38 = bitcast i16* %37 to <8 x i16>*
  %39 = load <8 x i16>, <8 x i16>* %38, align 16
  %40 = add <8 x i16> %9, %5
  %41 = add <8 x i16> %40, %14
  %42 = add <8 x i16> %41, %19
  %43 = add <8 x i16> %42, %24
  %44 = add <8 x i16> %43, %29
  %45 = add <8 x i16> %44, %34
  %46 = add <8 x i16> %45, %39
  %47 = shufflevector <8 x i16> <i16 0, i16 0, i16 0, i16 0, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i16> %46, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %48 = shufflevector <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 0, i16 0, i16 0, i16 0>, <8 x i16> %46, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %49 = bitcast <8 x i16> %47 to <4 x i32>
  %50 = ashr <4 x i32> %49, <i32 16, i32 16, i32 16, i32 16>
  %51 = bitcast <8 x i16> %48 to <4 x i32>
  %52 = ashr <4 x i32> %51, <i32 16, i32 16, i32 16, i32 16>
  %53 = add nsw <4 x i32> %52, %50
  %54 = shufflevector <4 x i32> %53, <4 x i32> <i32 0, i32 0, i32 undef, i32 undef>, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %55 = shufflevector <4 x i32> %53, <4 x i32> <i32 undef, i32 undef, i32 0, i32 0>, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %56 = add <4 x i32> %55, %54
  %57 = bitcast <4 x i32> %56 to <16 x i8>
  %58 = shufflevector <16 x i8> %57, <16 x i8> undef, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %59 = bitcast <16 x i8> %58 to <4 x i32>
  %60 = add <4 x i32> %56, %59
  %61 = extractelement <4 x i32> %60, i32 0
  store i32 %61, i32* %1, align 4
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define hidden void @vpx_fdct16x16_1_sse2(i16* nocapture readonly, i32* nocapture, i32) local_unnamed_addr #0 {
  %4 = sext i32 %2 to i64
  %5 = shl nsw i32 %2, 1
  %6 = sext i32 %5 to i64
  %7 = mul nsw i32 %2, 3
  %8 = sext i32 %7 to i64
  %9 = shl nsw i32 %2, 2
  %10 = sext i32 %9 to i64
  %11 = mul nsw i32 %2, 5
  %12 = sext i32 %11 to i64
  %13 = mul nsw i32 %2, 6
  %14 = sext i32 %13 to i64
  %15 = mul nsw i32 %2, 7
  %16 = sext i32 %15 to i64
  %17 = shl nsw i32 %2, 3
  %18 = sext i32 %17 to i64
  %19 = bitcast i16* %0 to <8 x i16>*
  %20 = load <8 x i16>, <8 x i16>* %19, align 16
  %21 = getelementptr inbounds i16, i16* %0, i64 8
  %22 = bitcast i16* %21 to <8 x i16>*
  %23 = load <8 x i16>, <8 x i16>* %22, align 16
  %24 = getelementptr inbounds i16, i16* %0, i64 %4
  %25 = bitcast i16* %24 to <8 x i16>*
  %26 = load <8 x i16>, <8 x i16>* %25, align 16
  %27 = getelementptr inbounds i16, i16* %24, i64 8
  %28 = bitcast i16* %27 to <8 x i16>*
  %29 = load <8 x i16>, <8 x i16>* %28, align 16
  %30 = getelementptr inbounds i16, i16* %0, i64 %6
  %31 = bitcast i16* %30 to <8 x i16>*
  %32 = load <8 x i16>, <8 x i16>* %31, align 16
  %33 = getelementptr inbounds i16, i16* %30, i64 8
  %34 = bitcast i16* %33 to <8 x i16>*
  %35 = load <8 x i16>, <8 x i16>* %34, align 16
  %36 = getelementptr inbounds i16, i16* %0, i64 %8
  %37 = bitcast i16* %36 to <8 x i16>*
  %38 = load <8 x i16>, <8 x i16>* %37, align 16
  %39 = getelementptr inbounds i16, i16* %36, i64 8
  %40 = bitcast i16* %39 to <8 x i16>*
  %41 = load <8 x i16>, <8 x i16>* %40, align 16
  %42 = getelementptr inbounds i16, i16* %0, i64 %10
  %43 = bitcast i16* %42 to <8 x i16>*
  %44 = load <8 x i16>, <8 x i16>* %43, align 16
  %45 = getelementptr inbounds i16, i16* %42, i64 8
  %46 = bitcast i16* %45 to <8 x i16>*
  %47 = load <8 x i16>, <8 x i16>* %46, align 16
  %48 = getelementptr inbounds i16, i16* %0, i64 %12
  %49 = bitcast i16* %48 to <8 x i16>*
  %50 = load <8 x i16>, <8 x i16>* %49, align 16
  %51 = getelementptr inbounds i16, i16* %48, i64 8
  %52 = bitcast i16* %51 to <8 x i16>*
  %53 = load <8 x i16>, <8 x i16>* %52, align 16
  %54 = getelementptr inbounds i16, i16* %0, i64 %14
  %55 = bitcast i16* %54 to <8 x i16>*
  %56 = load <8 x i16>, <8 x i16>* %55, align 16
  %57 = getelementptr inbounds i16, i16* %54, i64 8
  %58 = bitcast i16* %57 to <8 x i16>*
  %59 = load <8 x i16>, <8 x i16>* %58, align 16
  %60 = getelementptr inbounds i16, i16* %0, i64 %16
  %61 = bitcast i16* %60 to <8 x i16>*
  %62 = load <8 x i16>, <8 x i16>* %61, align 16
  %63 = getelementptr inbounds i16, i16* %60, i64 8
  %64 = bitcast i16* %63 to <8 x i16>*
  %65 = load <8 x i16>, <8 x i16>* %64, align 16
  %66 = add <8 x i16> %20, %23
  %67 = add <8 x i16> %66, %26
  %68 = add <8 x i16> %67, %29
  %69 = add <8 x i16> %68, %32
  %70 = add <8 x i16> %69, %35
  %71 = add <8 x i16> %70, %38
  %72 = add <8 x i16> %71, %41
  %73 = add <8 x i16> %72, %44
  %74 = add <8 x i16> %73, %47
  %75 = add <8 x i16> %74, %50
  %76 = add <8 x i16> %75, %53
  %77 = add <8 x i16> %76, %56
  %78 = add <8 x i16> %77, %59
  %79 = add <8 x i16> %78, %62
  %80 = add <8 x i16> %79, %65
  %81 = getelementptr inbounds i16, i16* %0, i64 %18
  %82 = bitcast i16* %81 to <8 x i16>*
  %83 = load <8 x i16>, <8 x i16>* %82, align 16
  %84 = getelementptr inbounds i16, i16* %81, i64 8
  %85 = bitcast i16* %84 to <8 x i16>*
  %86 = load <8 x i16>, <8 x i16>* %85, align 16
  %87 = getelementptr inbounds i16, i16* %81, i64 %4
  %88 = bitcast i16* %87 to <8 x i16>*
  %89 = load <8 x i16>, <8 x i16>* %88, align 16
  %90 = getelementptr inbounds i16, i16* %87, i64 8
  %91 = bitcast i16* %90 to <8 x i16>*
  %92 = load <8 x i16>, <8 x i16>* %91, align 16
  %93 = getelementptr inbounds i16, i16* %81, i64 %6
  %94 = bitcast i16* %93 to <8 x i16>*
  %95 = load <8 x i16>, <8 x i16>* %94, align 16
  %96 = getelementptr inbounds i16, i16* %93, i64 8
  %97 = bitcast i16* %96 to <8 x i16>*
  %98 = load <8 x i16>, <8 x i16>* %97, align 16
  %99 = getelementptr inbounds i16, i16* %81, i64 %8
  %100 = bitcast i16* %99 to <8 x i16>*
  %101 = load <8 x i16>, <8 x i16>* %100, align 16
  %102 = getelementptr inbounds i16, i16* %99, i64 8
  %103 = bitcast i16* %102 to <8 x i16>*
  %104 = load <8 x i16>, <8 x i16>* %103, align 16
  %105 = getelementptr inbounds i16, i16* %81, i64 %10
  %106 = bitcast i16* %105 to <8 x i16>*
  %107 = load <8 x i16>, <8 x i16>* %106, align 16
  %108 = getelementptr inbounds i16, i16* %105, i64 8
  %109 = bitcast i16* %108 to <8 x i16>*
  %110 = load <8 x i16>, <8 x i16>* %109, align 16
  %111 = getelementptr inbounds i16, i16* %81, i64 %12
  %112 = bitcast i16* %111 to <8 x i16>*
  %113 = load <8 x i16>, <8 x i16>* %112, align 16
  %114 = getelementptr inbounds i16, i16* %111, i64 8
  %115 = bitcast i16* %114 to <8 x i16>*
  %116 = load <8 x i16>, <8 x i16>* %115, align 16
  %117 = getelementptr inbounds i16, i16* %81, i64 %14
  %118 = bitcast i16* %117 to <8 x i16>*
  %119 = load <8 x i16>, <8 x i16>* %118, align 16
  %120 = getelementptr inbounds i16, i16* %117, i64 8
  %121 = bitcast i16* %120 to <8 x i16>*
  %122 = load <8 x i16>, <8 x i16>* %121, align 16
  %123 = getelementptr inbounds i16, i16* %81, i64 %16
  %124 = bitcast i16* %123 to <8 x i16>*
  %125 = load <8 x i16>, <8 x i16>* %124, align 16
  %126 = getelementptr inbounds i16, i16* %123, i64 8
  %127 = bitcast i16* %126 to <8 x i16>*
  %128 = load <8 x i16>, <8 x i16>* %127, align 16
  %129 = add <8 x i16> %83, %80
  %130 = add <8 x i16> %129, %86
  %131 = add <8 x i16> %130, %89
  %132 = add <8 x i16> %131, %92
  %133 = add <8 x i16> %132, %95
  %134 = add <8 x i16> %133, %98
  %135 = add <8 x i16> %134, %101
  %136 = add <8 x i16> %135, %104
  %137 = add <8 x i16> %136, %107
  %138 = add <8 x i16> %137, %110
  %139 = add <8 x i16> %138, %113
  %140 = add <8 x i16> %139, %116
  %141 = add <8 x i16> %140, %119
  %142 = add <8 x i16> %141, %122
  %143 = add <8 x i16> %142, %125
  %144 = add <8 x i16> %143, %128
  %145 = shufflevector <8 x i16> <i16 0, i16 0, i16 0, i16 0, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i16> %144, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %146 = shufflevector <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 0, i16 0, i16 0, i16 0>, <8 x i16> %144, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %147 = bitcast <8 x i16> %145 to <4 x i32>
  %148 = ashr <4 x i32> %147, <i32 16, i32 16, i32 16, i32 16>
  %149 = bitcast <8 x i16> %146 to <4 x i32>
  %150 = ashr <4 x i32> %149, <i32 16, i32 16, i32 16, i32 16>
  %151 = add nsw <4 x i32> %150, %148
  %152 = shufflevector <4 x i32> %151, <4 x i32> <i32 0, i32 0, i32 undef, i32 undef>, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %153 = shufflevector <4 x i32> %151, <4 x i32> <i32 undef, i32 undef, i32 0, i32 0>, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %154 = add <4 x i32> %153, %152
  %155 = bitcast <4 x i32> %154 to <16 x i8>
  %156 = shufflevector <16 x i8> %155, <16 x i8> undef, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %157 = bitcast <16 x i8> %156 to <4 x i32>
  %158 = add <4 x i32> %154, %157
  %159 = extractelement <4 x i32> %158, i32 0
  %160 = ashr i32 %159, 1
  store i32 %160, i32* %1, align 4
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define hidden void @vpx_fdct32x32_1_sse2(i16* nocapture readonly, i32* nocapture, i32) local_unnamed_addr #0 {
  %4 = sext i32 %2 to i64
  br label %5

5:                                                ; preds = %5, %3
  %6 = phi i32 [ 0, %3 ], [ %73, %5 ]
  %7 = phi <8 x i16> [ zeroinitializer, %3 ], [ %72, %5 ]
  %8 = phi i16* [ %0, %3 ], [ %56, %5 ]
  %9 = bitcast i16* %8 to <8 x i16>*
  %10 = load <8 x i16>, <8 x i16>* %9, align 16
  %11 = getelementptr inbounds i16, i16* %8, i64 8
  %12 = bitcast i16* %11 to <8 x i16>*
  %13 = load <8 x i16>, <8 x i16>* %12, align 16
  %14 = getelementptr inbounds i16, i16* %8, i64 16
  %15 = bitcast i16* %14 to <8 x i16>*
  %16 = load <8 x i16>, <8 x i16>* %15, align 16
  %17 = getelementptr inbounds i16, i16* %8, i64 24
  %18 = bitcast i16* %17 to <8 x i16>*
  %19 = load <8 x i16>, <8 x i16>* %18, align 16
  %20 = getelementptr inbounds i16, i16* %8, i64 %4
  %21 = bitcast i16* %20 to <8 x i16>*
  %22 = load <8 x i16>, <8 x i16>* %21, align 16
  %23 = getelementptr inbounds i16, i16* %20, i64 8
  %24 = bitcast i16* %23 to <8 x i16>*
  %25 = load <8 x i16>, <8 x i16>* %24, align 16
  %26 = getelementptr inbounds i16, i16* %20, i64 16
  %27 = bitcast i16* %26 to <8 x i16>*
  %28 = load <8 x i16>, <8 x i16>* %27, align 16
  %29 = getelementptr inbounds i16, i16* %20, i64 24
  %30 = bitcast i16* %29 to <8 x i16>*
  %31 = load <8 x i16>, <8 x i16>* %30, align 16
  %32 = getelementptr inbounds i16, i16* %20, i64 %4
  %33 = bitcast i16* %32 to <8 x i16>*
  %34 = load <8 x i16>, <8 x i16>* %33, align 16
  %35 = getelementptr inbounds i16, i16* %32, i64 8
  %36 = bitcast i16* %35 to <8 x i16>*
  %37 = load <8 x i16>, <8 x i16>* %36, align 16
  %38 = getelementptr inbounds i16, i16* %32, i64 16
  %39 = bitcast i16* %38 to <8 x i16>*
  %40 = load <8 x i16>, <8 x i16>* %39, align 16
  %41 = getelementptr inbounds i16, i16* %32, i64 24
  %42 = bitcast i16* %41 to <8 x i16>*
  %43 = load <8 x i16>, <8 x i16>* %42, align 16
  %44 = getelementptr inbounds i16, i16* %32, i64 %4
  %45 = bitcast i16* %44 to <8 x i16>*
  %46 = load <8 x i16>, <8 x i16>* %45, align 16
  %47 = getelementptr inbounds i16, i16* %44, i64 8
  %48 = bitcast i16* %47 to <8 x i16>*
  %49 = load <8 x i16>, <8 x i16>* %48, align 16
  %50 = getelementptr inbounds i16, i16* %44, i64 16
  %51 = bitcast i16* %50 to <8 x i16>*
  %52 = load <8 x i16>, <8 x i16>* %51, align 16
  %53 = getelementptr inbounds i16, i16* %44, i64 24
  %54 = bitcast i16* %53 to <8 x i16>*
  %55 = load <8 x i16>, <8 x i16>* %54, align 16
  %56 = getelementptr inbounds i16, i16* %44, i64 %4
  %57 = add <8 x i16> %10, %7
  %58 = add <8 x i16> %57, %13
  %59 = add <8 x i16> %58, %16
  %60 = add <8 x i16> %59, %19
  %61 = add <8 x i16> %60, %22
  %62 = add <8 x i16> %61, %25
  %63 = add <8 x i16> %62, %28
  %64 = add <8 x i16> %63, %31
  %65 = add <8 x i16> %64, %34
  %66 = add <8 x i16> %65, %37
  %67 = add <8 x i16> %66, %40
  %68 = add <8 x i16> %67, %43
  %69 = add <8 x i16> %68, %46
  %70 = add <8 x i16> %69, %49
  %71 = add <8 x i16> %70, %52
  %72 = add <8 x i16> %71, %55
  %73 = add nuw nsw i32 %6, 1
  %74 = icmp eq i32 %73, 8
  br i1 %74, label %75, label %5

75:                                               ; preds = %5
  %76 = shufflevector <8 x i16> <i16 0, i16 0, i16 0, i16 0, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i16> %72, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %77 = shufflevector <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 0, i16 0, i16 0, i16 0>, <8 x i16> %72, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %78 = bitcast <8 x i16> %76 to <4 x i32>
  %79 = ashr <4 x i32> %78, <i32 16, i32 16, i32 16, i32 16>
  %80 = bitcast <8 x i16> %77 to <4 x i32>
  %81 = ashr <4 x i32> %80, <i32 16, i32 16, i32 16, i32 16>
  %82 = add nsw <4 x i32> %81, %79
  %83 = shufflevector <4 x i32> %82, <4 x i32> <i32 0, i32 0, i32 undef, i32 undef>, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %84 = shufflevector <4 x i32> %82, <4 x i32> <i32 undef, i32 undef, i32 0, i32 0>, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %85 = add <4 x i32> %84, %83
  %86 = bitcast <4 x i32> %85 to <16 x i8>
  %87 = shufflevector <16 x i8> %86, <16 x i8> undef, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %88 = bitcast <16 x i8> %87 to <4 x i32>
  %89 = add <4 x i32> %85, %88
  %90 = extractelement <4 x i32> %89, i32 0
  %91 = ashr i32 %90, 3
  store i32 %91, i32* %1, align 4
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @vpx_fdct4x4_sse2(i16* nocapture readonly, i32* nocapture, i32) local_unnamed_addr #2 {
  %4 = bitcast i16* %0 to i64*
  %5 = load i64, i64* %4, align 1
  %6 = insertelement <2 x i64> undef, i64 %5, i32 0
  %7 = sext i32 %2 to i64
  %8 = getelementptr inbounds i16, i16* %0, i64 %7
  %9 = bitcast i16* %8 to i64*
  %10 = load i64, i64* %9, align 1
  %11 = insertelement <2 x i64> undef, i64 %10, i32 0
  %12 = shl nsw i32 %2, 1
  %13 = sext i32 %12 to i64
  %14 = getelementptr inbounds i16, i16* %0, i64 %13
  %15 = bitcast i16* %14 to i64*
  %16 = load i64, i64* %15, align 1
  %17 = insertelement <2 x i64> %11, i64 %16, i32 1
  %18 = mul nsw i32 %2, 3
  %19 = sext i32 %18 to i64
  %20 = getelementptr inbounds i16, i16* %0, i64 %19
  %21 = bitcast i16* %20 to i64*
  %22 = load i64, i64* %21, align 1
  %23 = insertelement <2 x i64> %6, i64 %22, i32 1
  %24 = bitcast <2 x i64> %23 to <8 x i16>
  %25 = shl <8 x i16> %24, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %26 = bitcast <2 x i64> %17 to <8 x i16>
  %27 = shl <8 x i16> %26, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %28 = icmp eq <8 x i16> %25, <i16 0, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %29 = zext <8 x i1> %28 to <8 x i16>
  %30 = sub <8 x i16> %25, %29
  %31 = add <8 x i16> %30, <i16 1, i16 0, i16 0, i16 0, i16 0, i16 0, i16 0, i16 0>
  %32 = shufflevector <8 x i16> %31, <8 x i16> %27, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %33 = shufflevector <8 x i16> %31, <8 x i16> %27, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %34 = bitcast <8 x i16> %32 to <4 x i32>
  %35 = shufflevector <4 x i32> %34, <4 x i32> undef, <4 x i32> <i32 0, i32 1, i32 3, i32 2>
  %36 = bitcast <8 x i16> %33 to <4 x i32>
  %37 = shufflevector <4 x i32> %36, <4 x i32> undef, <4 x i32> <i32 0, i32 1, i32 3, i32 2>
  %38 = bitcast <4 x i32> %35 to <8 x i16>
  %39 = bitcast <4 x i32> %37 to <8 x i16>
  %40 = add <8 x i16> %39, %38
  %41 = sub <8 x i16> %38, %39
  %42 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %40, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %43 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %40, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %44 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %41, <8 x i16> <i16 15137, i16 6270, i16 15137, i16 6270, i16 6270, i16 -15137, i16 6270, i16 -15137>) #6
  %45 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %41, <8 x i16> <i16 6270, i16 -15137, i16 6270, i16 -15137, i16 15137, i16 6270, i16 15137, i16 6270>) #6
  %46 = add <4 x i32> %42, <i32 8192, i32 8192, i32 8192, i32 8192>
  %47 = add <4 x i32> %44, <i32 8192, i32 8192, i32 8192, i32 8192>
  %48 = add <4 x i32> %43, <i32 8192, i32 8192, i32 8192, i32 8192>
  %49 = add <4 x i32> %45, <i32 8192, i32 8192, i32 8192, i32 8192>
  %50 = ashr <4 x i32> %46, <i32 14, i32 14, i32 14, i32 14>
  %51 = ashr <4 x i32> %47, <i32 14, i32 14, i32 14, i32 14>
  %52 = ashr <4 x i32> %48, <i32 14, i32 14, i32 14, i32 14>
  %53 = ashr <4 x i32> %49, <i32 14, i32 14, i32 14, i32 14>
  %54 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %50, <4 x i32> %51) #6
  %55 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %52, <4 x i32> %53) #6
  %56 = bitcast <8 x i16> %54 to <4 x i32>
  %57 = shufflevector <4 x i32> %56, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %58 = bitcast <8 x i16> %55 to <4 x i32>
  %59 = shufflevector <4 x i32> %58, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 0, i32 2>
  %60 = bitcast <4 x i32> %57 to <8 x i16>
  %61 = bitcast <4 x i32> %59 to <8 x i16>
  %62 = add <8 x i16> %61, %60
  %63 = sub <8 x i16> %60, %61
  %64 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %62, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %65 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %62, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %66 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %63, <8 x i16> <i16 15137, i16 6270, i16 15137, i16 6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270>) #6
  %67 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %63, <8 x i16> <i16 6270, i16 -15137, i16 6270, i16 -15137, i16 -6270, i16 15137, i16 -6270, i16 15137>) #6
  %68 = add <4 x i32> %64, <i32 24576, i32 24576, i32 24576, i32 24576>
  %69 = add <4 x i32> %65, <i32 24576, i32 24576, i32 24576, i32 24576>
  %70 = add <4 x i32> %66, <i32 24576, i32 24576, i32 24576, i32 24576>
  %71 = add <4 x i32> %67, <i32 24576, i32 24576, i32 24576, i32 24576>
  %72 = ashr <4 x i32> %68, <i32 16, i32 16, i32 16, i32 16>
  %73 = ashr <4 x i32> %69, <i32 16, i32 16, i32 16, i32 16>
  %74 = ashr <4 x i32> %70, <i32 16, i32 16, i32 16, i32 16>
  %75 = ashr <4 x i32> %71, <i32 16, i32 16, i32 16, i32 16>
  %76 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %72, <4 x i32> %73) #6
  %77 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %74, <4 x i32> %75) #6
  %78 = shufflevector <8 x i16> %76, <8 x i16> %77, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %79 = shufflevector <8 x i16> %76, <8 x i16> %77, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %80 = bitcast <8 x i16> %78 to <4 x i32>
  %81 = bitcast <8 x i16> %79 to <4 x i32>
  %82 = shufflevector <4 x i32> %80, <4 x i32> %81, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %83 = shufflevector <4 x i32> %80, <4 x i32> %81, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %84 = bitcast <4 x i32> %82 to <8 x i16>
  %85 = ashr <8 x i16> %84, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %86 = shufflevector <8 x i16> %84, <8 x i16> %85, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %87 = shufflevector <8 x i16> %84, <8 x i16> %85, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %88 = bitcast i32* %1 to <8 x i16>*
  store <8 x i16> %86, <8 x i16>* %88, align 1
  %89 = getelementptr inbounds i32, i32* %1, i64 4
  %90 = bitcast i32* %89 to <8 x i16>*
  store <8 x i16> %87, <8 x i16>* %90, align 1
  %91 = getelementptr inbounds i32, i32* %1, i64 8
  %92 = bitcast <4 x i32> %83 to <8 x i16>
  %93 = ashr <8 x i16> %92, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %94 = shufflevector <8 x i16> %92, <8 x i16> %93, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %95 = shufflevector <8 x i16> %92, <8 x i16> %93, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %96 = bitcast i32* %91 to <8 x i16>*
  store <8 x i16> %94, <8 x i16>* %96, align 1
  %97 = getelementptr inbounds i32, i32* %1, i64 12
  %98 = bitcast i32* %97 to <8 x i16>*
  store <8 x i16> %95, <8 x i16>* %98, align 1
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @vpx_fdct8x8_sse2(i16* nocapture readonly, i32* nocapture, i32) local_unnamed_addr #2 {
  %4 = bitcast i16* %0 to <8 x i16>*
  %5 = load <8 x i16>, <8 x i16>* %4, align 16
  %6 = sext i32 %2 to i64
  %7 = getelementptr inbounds i16, i16* %0, i64 %6
  %8 = bitcast i16* %7 to <8 x i16>*
  %9 = load <8 x i16>, <8 x i16>* %8, align 16
  %10 = shl nsw i32 %2, 1
  %11 = sext i32 %10 to i64
  %12 = getelementptr inbounds i16, i16* %0, i64 %11
  %13 = bitcast i16* %12 to <8 x i16>*
  %14 = load <8 x i16>, <8 x i16>* %13, align 16
  %15 = mul nsw i32 %2, 3
  %16 = sext i32 %15 to i64
  %17 = getelementptr inbounds i16, i16* %0, i64 %16
  %18 = bitcast i16* %17 to <8 x i16>*
  %19 = load <8 x i16>, <8 x i16>* %18, align 16
  %20 = shl nsw i32 %2, 2
  %21 = sext i32 %20 to i64
  %22 = getelementptr inbounds i16, i16* %0, i64 %21
  %23 = bitcast i16* %22 to <8 x i16>*
  %24 = load <8 x i16>, <8 x i16>* %23, align 16
  %25 = mul nsw i32 %2, 5
  %26 = sext i32 %25 to i64
  %27 = getelementptr inbounds i16, i16* %0, i64 %26
  %28 = bitcast i16* %27 to <8 x i16>*
  %29 = load <8 x i16>, <8 x i16>* %28, align 16
  %30 = mul nsw i32 %2, 6
  %31 = sext i32 %30 to i64
  %32 = getelementptr inbounds i16, i16* %0, i64 %31
  %33 = bitcast i16* %32 to <8 x i16>*
  %34 = load <8 x i16>, <8 x i16>* %33, align 16
  %35 = mul nsw i32 %2, 7
  %36 = sext i32 %35 to i64
  %37 = getelementptr inbounds i16, i16* %0, i64 %36
  %38 = bitcast i16* %37 to <8 x i16>*
  %39 = load <8 x i16>, <8 x i16>* %38, align 16
  %40 = shl <8 x i16> %5, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %41 = shl <8 x i16> %9, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %42 = bitcast <8 x i16> %41 to <2 x i64>
  %43 = shl <8 x i16> %14, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %44 = bitcast <8 x i16> %43 to <2 x i64>
  %45 = shl <8 x i16> %19, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %46 = bitcast <8 x i16> %45 to <2 x i64>
  %47 = shl <8 x i16> %24, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %48 = bitcast <8 x i16> %47 to <2 x i64>
  %49 = shl <8 x i16> %29, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %50 = bitcast <8 x i16> %49 to <2 x i64>
  %51 = shl <8 x i16> %34, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %52 = bitcast <8 x i16> %51 to <2 x i64>
  %53 = shl <8 x i16> %39, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %54 = bitcast <8 x i16> %53 to <2 x i64>
  br label %55

55:                                               ; preds = %55, %3
  %56 = phi <8 x i16> [ %40, %3 ], [ %209, %55 ]
  %57 = phi i32 [ 0, %3 ], [ %208, %55 ]
  %58 = phi <2 x i64> [ %54, %3 ], [ %207, %55 ]
  %59 = phi <2 x i64> [ %52, %3 ], [ %206, %55 ]
  %60 = phi <2 x i64> [ %50, %3 ], [ %205, %55 ]
  %61 = phi <2 x i64> [ %48, %3 ], [ %204, %55 ]
  %62 = phi <2 x i64> [ %46, %3 ], [ %203, %55 ]
  %63 = phi <2 x i64> [ %44, %3 ], [ %202, %55 ]
  %64 = phi <2 x i64> [ %42, %3 ], [ %201, %55 ]
  %65 = bitcast <2 x i64> %58 to <8 x i16>
  %66 = add <8 x i16> %56, %65
  %67 = bitcast <2 x i64> %64 to <8 x i16>
  %68 = bitcast <2 x i64> %59 to <8 x i16>
  %69 = add <8 x i16> %68, %67
  %70 = bitcast <2 x i64> %63 to <8 x i16>
  %71 = bitcast <2 x i64> %60 to <8 x i16>
  %72 = add <8 x i16> %71, %70
  %73 = bitcast <2 x i64> %62 to <8 x i16>
  %74 = bitcast <2 x i64> %61 to <8 x i16>
  %75 = add <8 x i16> %74, %73
  %76 = sub <8 x i16> %73, %74
  %77 = sub <8 x i16> %70, %71
  %78 = sub <8 x i16> %67, %68
  %79 = sub <8 x i16> %56, %65
  %80 = add <8 x i16> %66, %75
  %81 = add <8 x i16> %69, %72
  %82 = sub <8 x i16> %69, %72
  %83 = sub <8 x i16> %66, %75
  %84 = shufflevector <8 x i16> %80, <8 x i16> %81, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %85 = shufflevector <8 x i16> %80, <8 x i16> %81, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %86 = shufflevector <8 x i16> %82, <8 x i16> %83, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %87 = shufflevector <8 x i16> %82, <8 x i16> %83, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %88 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %84, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %89 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %85, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %90 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %84, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %91 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %85, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %92 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %86, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %93 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %87, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %94 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %86, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %95 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %87, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %96 = add <4 x i32> %88, <i32 8192, i32 8192, i32 8192, i32 8192>
  %97 = add <4 x i32> %89, <i32 8192, i32 8192, i32 8192, i32 8192>
  %98 = add <4 x i32> %90, <i32 8192, i32 8192, i32 8192, i32 8192>
  %99 = add <4 x i32> %91, <i32 8192, i32 8192, i32 8192, i32 8192>
  %100 = add <4 x i32> %92, <i32 8192, i32 8192, i32 8192, i32 8192>
  %101 = add <4 x i32> %93, <i32 8192, i32 8192, i32 8192, i32 8192>
  %102 = add <4 x i32> %94, <i32 8192, i32 8192, i32 8192, i32 8192>
  %103 = add <4 x i32> %95, <i32 8192, i32 8192, i32 8192, i32 8192>
  %104 = ashr <4 x i32> %96, <i32 14, i32 14, i32 14, i32 14>
  %105 = ashr <4 x i32> %97, <i32 14, i32 14, i32 14, i32 14>
  %106 = ashr <4 x i32> %98, <i32 14, i32 14, i32 14, i32 14>
  %107 = ashr <4 x i32> %99, <i32 14, i32 14, i32 14, i32 14>
  %108 = ashr <4 x i32> %100, <i32 14, i32 14, i32 14, i32 14>
  %109 = ashr <4 x i32> %101, <i32 14, i32 14, i32 14, i32 14>
  %110 = ashr <4 x i32> %102, <i32 14, i32 14, i32 14, i32 14>
  %111 = ashr <4 x i32> %103, <i32 14, i32 14, i32 14, i32 14>
  %112 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %104, <4 x i32> %105) #6
  %113 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %106, <4 x i32> %107) #6
  %114 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %108, <4 x i32> %109) #6
  %115 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %110, <4 x i32> %111) #6
  %116 = shufflevector <8 x i16> %78, <8 x i16> %77, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %117 = shufflevector <8 x i16> %78, <8 x i16> %77, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %118 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %116, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %119 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %117, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %120 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %116, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %121 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %117, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %122 = add <4 x i32> %118, <i32 8192, i32 8192, i32 8192, i32 8192>
  %123 = add <4 x i32> %119, <i32 8192, i32 8192, i32 8192, i32 8192>
  %124 = add <4 x i32> %120, <i32 8192, i32 8192, i32 8192, i32 8192>
  %125 = add <4 x i32> %121, <i32 8192, i32 8192, i32 8192, i32 8192>
  %126 = ashr <4 x i32> %122, <i32 14, i32 14, i32 14, i32 14>
  %127 = ashr <4 x i32> %123, <i32 14, i32 14, i32 14, i32 14>
  %128 = ashr <4 x i32> %124, <i32 14, i32 14, i32 14, i32 14>
  %129 = ashr <4 x i32> %125, <i32 14, i32 14, i32 14, i32 14>
  %130 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %126, <4 x i32> %127) #6
  %131 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %128, <4 x i32> %129) #6
  %132 = add <8 x i16> %130, %76
  %133 = sub <8 x i16> %76, %130
  %134 = sub <8 x i16> %79, %131
  %135 = add <8 x i16> %131, %79
  %136 = shufflevector <8 x i16> %132, <8 x i16> %135, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %137 = shufflevector <8 x i16> %132, <8 x i16> %135, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %138 = shufflevector <8 x i16> %133, <8 x i16> %134, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %139 = shufflevector <8 x i16> %133, <8 x i16> %134, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %140 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %136, <8 x i16> <i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069>) #6
  %141 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %137, <8 x i16> <i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069>) #6
  %142 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %136, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %143 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %137, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %144 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %138, <8 x i16> <i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102>) #6
  %145 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %139, <8 x i16> <i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102>) #6
  %146 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %138, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %147 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %139, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %148 = add <4 x i32> %140, <i32 8192, i32 8192, i32 8192, i32 8192>
  %149 = add <4 x i32> %141, <i32 8192, i32 8192, i32 8192, i32 8192>
  %150 = add <4 x i32> %142, <i32 8192, i32 8192, i32 8192, i32 8192>
  %151 = add <4 x i32> %143, <i32 8192, i32 8192, i32 8192, i32 8192>
  %152 = add <4 x i32> %144, <i32 8192, i32 8192, i32 8192, i32 8192>
  %153 = add <4 x i32> %145, <i32 8192, i32 8192, i32 8192, i32 8192>
  %154 = add <4 x i32> %146, <i32 8192, i32 8192, i32 8192, i32 8192>
  %155 = add <4 x i32> %147, <i32 8192, i32 8192, i32 8192, i32 8192>
  %156 = ashr <4 x i32> %148, <i32 14, i32 14, i32 14, i32 14>
  %157 = ashr <4 x i32> %149, <i32 14, i32 14, i32 14, i32 14>
  %158 = ashr <4 x i32> %150, <i32 14, i32 14, i32 14, i32 14>
  %159 = ashr <4 x i32> %151, <i32 14, i32 14, i32 14, i32 14>
  %160 = ashr <4 x i32> %152, <i32 14, i32 14, i32 14, i32 14>
  %161 = ashr <4 x i32> %153, <i32 14, i32 14, i32 14, i32 14>
  %162 = ashr <4 x i32> %154, <i32 14, i32 14, i32 14, i32 14>
  %163 = ashr <4 x i32> %155, <i32 14, i32 14, i32 14, i32 14>
  %164 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %156, <4 x i32> %157) #6
  %165 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %158, <4 x i32> %159) #6
  %166 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %160, <4 x i32> %161) #6
  %167 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %162, <4 x i32> %163) #6
  %168 = shufflevector <8 x i16> %112, <8 x i16> %164, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %169 = shufflevector <8 x i16> %114, <8 x i16> %167, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %170 = shufflevector <8 x i16> %112, <8 x i16> %164, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %171 = shufflevector <8 x i16> %114, <8 x i16> %167, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %172 = shufflevector <8 x i16> %113, <8 x i16> %166, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %173 = shufflevector <8 x i16> %115, <8 x i16> %165, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %174 = shufflevector <8 x i16> %113, <8 x i16> %166, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %175 = shufflevector <8 x i16> %115, <8 x i16> %165, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %176 = bitcast <8 x i16> %168 to <4 x i32>
  %177 = bitcast <8 x i16> %169 to <4 x i32>
  %178 = shufflevector <4 x i32> %176, <4 x i32> %177, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %179 = bitcast <4 x i32> %178 to <2 x i64>
  %180 = bitcast <8 x i16> %170 to <4 x i32>
  %181 = bitcast <8 x i16> %171 to <4 x i32>
  %182 = shufflevector <4 x i32> %180, <4 x i32> %181, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %183 = bitcast <4 x i32> %182 to <2 x i64>
  %184 = shufflevector <4 x i32> %176, <4 x i32> %177, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %185 = bitcast <4 x i32> %184 to <2 x i64>
  %186 = shufflevector <4 x i32> %180, <4 x i32> %181, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %187 = bitcast <4 x i32> %186 to <2 x i64>
  %188 = bitcast <8 x i16> %172 to <4 x i32>
  %189 = bitcast <8 x i16> %173 to <4 x i32>
  %190 = shufflevector <4 x i32> %188, <4 x i32> %189, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %191 = bitcast <4 x i32> %190 to <2 x i64>
  %192 = bitcast <8 x i16> %174 to <4 x i32>
  %193 = bitcast <8 x i16> %175 to <4 x i32>
  %194 = shufflevector <4 x i32> %192, <4 x i32> %193, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %195 = bitcast <4 x i32> %194 to <2 x i64>
  %196 = shufflevector <4 x i32> %188, <4 x i32> %189, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %197 = bitcast <4 x i32> %196 to <2 x i64>
  %198 = shufflevector <4 x i32> %192, <4 x i32> %193, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %199 = bitcast <4 x i32> %198 to <2 x i64>
  %200 = shufflevector <2 x i64> %179, <2 x i64> %191, <2 x i32> <i32 0, i32 2>
  %201 = shufflevector <2 x i64> %179, <2 x i64> %191, <2 x i32> <i32 1, i32 3>
  %202 = shufflevector <2 x i64> %185, <2 x i64> %197, <2 x i32> <i32 0, i32 2>
  %203 = shufflevector <2 x i64> %185, <2 x i64> %197, <2 x i32> <i32 1, i32 3>
  %204 = shufflevector <2 x i64> %183, <2 x i64> %195, <2 x i32> <i32 0, i32 2>
  %205 = shufflevector <2 x i64> %183, <2 x i64> %195, <2 x i32> <i32 1, i32 3>
  %206 = shufflevector <2 x i64> %187, <2 x i64> %199, <2 x i32> <i32 0, i32 2>
  %207 = shufflevector <2 x i64> %187, <2 x i64> %199, <2 x i32> <i32 1, i32 3>
  %208 = add nuw nsw i32 %57, 1
  %209 = bitcast <2 x i64> %200 to <8 x i16>
  %210 = icmp eq i32 %208, 2
  br i1 %210, label %211, label %55

211:                                              ; preds = %55
  %212 = ashr <8 x i16> %209, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %213 = bitcast <2 x i64> %201 to <8 x i16>
  %214 = ashr <8 x i16> %213, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %215 = bitcast <2 x i64> %202 to <8 x i16>
  %216 = ashr <8 x i16> %215, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %217 = bitcast <2 x i64> %203 to <8 x i16>
  %218 = ashr <8 x i16> %217, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %219 = bitcast <2 x i64> %204 to <8 x i16>
  %220 = ashr <8 x i16> %219, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %221 = bitcast <2 x i64> %205 to <8 x i16>
  %222 = ashr <8 x i16> %221, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %223 = bitcast <2 x i64> %206 to <8 x i16>
  %224 = ashr <8 x i16> %223, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %225 = bitcast <2 x i64> %207 to <8 x i16>
  %226 = ashr <8 x i16> %225, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %227 = sub <8 x i16> %209, %212
  %228 = sub <8 x i16> %213, %214
  %229 = sub <8 x i16> %215, %216
  %230 = sub <8 x i16> %217, %218
  %231 = sub <8 x i16> %219, %220
  %232 = sub <8 x i16> %221, %222
  %233 = sub <8 x i16> %223, %224
  %234 = sub <8 x i16> %225, %226
  %235 = ashr <8 x i16> %227, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %236 = ashr <8 x i16> %228, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %237 = ashr <8 x i16> %229, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %238 = ashr <8 x i16> %230, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %239 = ashr <8 x i16> %231, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %240 = ashr <8 x i16> %232, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %241 = ashr <8 x i16> %233, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %242 = ashr <8 x i16> %234, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %243 = ashr <8 x i16> %227, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %244 = shufflevector <8 x i16> %235, <8 x i16> %243, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %245 = shufflevector <8 x i16> %235, <8 x i16> %243, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %246 = bitcast i32* %1 to <8 x i16>*
  store <8 x i16> %244, <8 x i16>* %246, align 16
  %247 = getelementptr inbounds i32, i32* %1, i64 4
  %248 = bitcast i32* %247 to <8 x i16>*
  store <8 x i16> %245, <8 x i16>* %248, align 16
  %249 = getelementptr inbounds i32, i32* %1, i64 8
  %250 = ashr <8 x i16> %228, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %251 = shufflevector <8 x i16> %236, <8 x i16> %250, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %252 = shufflevector <8 x i16> %236, <8 x i16> %250, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %253 = bitcast i32* %249 to <8 x i16>*
  store <8 x i16> %251, <8 x i16>* %253, align 16
  %254 = getelementptr inbounds i32, i32* %1, i64 12
  %255 = bitcast i32* %254 to <8 x i16>*
  store <8 x i16> %252, <8 x i16>* %255, align 16
  %256 = getelementptr inbounds i32, i32* %1, i64 16
  %257 = ashr <8 x i16> %229, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %258 = shufflevector <8 x i16> %237, <8 x i16> %257, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %259 = shufflevector <8 x i16> %237, <8 x i16> %257, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %260 = bitcast i32* %256 to <8 x i16>*
  store <8 x i16> %258, <8 x i16>* %260, align 16
  %261 = getelementptr inbounds i32, i32* %1, i64 20
  %262 = bitcast i32* %261 to <8 x i16>*
  store <8 x i16> %259, <8 x i16>* %262, align 16
  %263 = getelementptr inbounds i32, i32* %1, i64 24
  %264 = ashr <8 x i16> %230, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %265 = shufflevector <8 x i16> %238, <8 x i16> %264, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %266 = shufflevector <8 x i16> %238, <8 x i16> %264, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %267 = bitcast i32* %263 to <8 x i16>*
  store <8 x i16> %265, <8 x i16>* %267, align 16
  %268 = getelementptr inbounds i32, i32* %1, i64 28
  %269 = bitcast i32* %268 to <8 x i16>*
  store <8 x i16> %266, <8 x i16>* %269, align 16
  %270 = getelementptr inbounds i32, i32* %1, i64 32
  %271 = ashr <8 x i16> %231, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %272 = shufflevector <8 x i16> %239, <8 x i16> %271, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %273 = shufflevector <8 x i16> %239, <8 x i16> %271, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %274 = bitcast i32* %270 to <8 x i16>*
  store <8 x i16> %272, <8 x i16>* %274, align 16
  %275 = getelementptr inbounds i32, i32* %1, i64 36
  %276 = bitcast i32* %275 to <8 x i16>*
  store <8 x i16> %273, <8 x i16>* %276, align 16
  %277 = getelementptr inbounds i32, i32* %1, i64 40
  %278 = ashr <8 x i16> %232, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %279 = shufflevector <8 x i16> %240, <8 x i16> %278, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %280 = shufflevector <8 x i16> %240, <8 x i16> %278, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %281 = bitcast i32* %277 to <8 x i16>*
  store <8 x i16> %279, <8 x i16>* %281, align 16
  %282 = getelementptr inbounds i32, i32* %1, i64 44
  %283 = bitcast i32* %282 to <8 x i16>*
  store <8 x i16> %280, <8 x i16>* %283, align 16
  %284 = getelementptr inbounds i32, i32* %1, i64 48
  %285 = ashr <8 x i16> %233, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %286 = shufflevector <8 x i16> %241, <8 x i16> %285, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %287 = shufflevector <8 x i16> %241, <8 x i16> %285, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %288 = bitcast i32* %284 to <8 x i16>*
  store <8 x i16> %286, <8 x i16>* %288, align 16
  %289 = getelementptr inbounds i32, i32* %1, i64 52
  %290 = bitcast i32* %289 to <8 x i16>*
  store <8 x i16> %287, <8 x i16>* %290, align 16
  %291 = getelementptr inbounds i32, i32* %1, i64 56
  %292 = ashr <8 x i16> %234, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %293 = shufflevector <8 x i16> %242, <8 x i16> %292, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %294 = shufflevector <8 x i16> %242, <8 x i16> %292, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %295 = bitcast i32* %291 to <8 x i16>*
  store <8 x i16> %293, <8 x i16>* %295, align 16
  %296 = getelementptr inbounds i32, i32* %1, i64 60
  %297 = bitcast i32* %296 to <8 x i16>*
  store <8 x i16> %294, <8 x i16>* %297, align 16
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @vpx_fdct16x16_sse2(i16* readonly, i32* nocapture, i32) local_unnamed_addr #2 {
  %4 = alloca [256 x i16], align 16
  %5 = bitcast [256 x i16]* %4 to i8*
  call void @llvm.lifetime.start.p0i8(i64 512, i8* nonnull %5) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %5, i8 -86, i64 512, i1 false)
  %6 = getelementptr inbounds [256 x i16], [256 x i16]* %4, i64 0, i64 0
  %7 = sext i32 %2 to i64
  %8 = shl nsw i32 %2, 1
  %9 = sext i32 %8 to i64
  %10 = mul nsw i32 %2, 3
  %11 = sext i32 %10 to i64
  %12 = shl nsw i32 %2, 2
  %13 = sext i32 %12 to i64
  %14 = mul nsw i32 %2, 5
  %15 = sext i32 %14 to i64
  %16 = mul nsw i32 %2, 6
  %17 = sext i32 %16 to i64
  %18 = mul nsw i32 %2, 7
  %19 = sext i32 %18 to i64
  %20 = shl nsw i32 %2, 3
  %21 = sext i32 %20 to i64
  %22 = mul nsw i32 %2, 9
  %23 = sext i32 %22 to i64
  %24 = mul nsw i32 %2, 10
  %25 = sext i32 %24 to i64
  %26 = mul nsw i32 %2, 11
  %27 = sext i32 %26 to i64
  %28 = mul nsw i32 %2, 12
  %29 = sext i32 %28 to i64
  %30 = mul nsw i32 %2, 13
  %31 = sext i32 %30 to i64
  %32 = mul nsw i32 %2, 14
  %33 = sext i32 %32 to i64
  %34 = mul nsw i32 %2, 15
  %35 = sext i32 %34 to i64
  br label %36

36:                                               ; preds = %715, %3
  %37 = phi i32 [ 0, %3 ], [ %716, %715 ]
  %38 = phi i16* [ %0, %3 ], [ %6, %715 ]
  %39 = phi i16* [ %6, %3 ], [ %712, %715 ]
  %40 = phi i32* [ %1, %3 ], [ %711, %715 ]
  %41 = icmp eq i32 %37, 0
  br label %42

42:                                               ; preds = %36, %708
  %43 = phi i16* [ %38, %36 ], [ %208, %708 ]
  %44 = phi i16* [ %39, %36 ], [ %712, %708 ]
  %45 = phi i32* [ %40, %36 ], [ %711, %708 ]
  %46 = phi i32 [ 0, %36 ], [ %713, %708 ]
  %47 = bitcast i16* %43 to <2 x i64>*
  %48 = load <2 x i64>, <2 x i64>* %47, align 16
  br i1 %41, label %49, label %112

49:                                               ; preds = %42
  %50 = getelementptr inbounds i16, i16* %43, i64 %7
  %51 = bitcast i16* %50 to <8 x i16>*
  %52 = load <8 x i16>, <8 x i16>* %51, align 16
  %53 = getelementptr inbounds i16, i16* %43, i64 %9
  %54 = bitcast i16* %53 to <8 x i16>*
  %55 = load <8 x i16>, <8 x i16>* %54, align 16
  %56 = getelementptr inbounds i16, i16* %43, i64 %11
  %57 = bitcast i16* %56 to <8 x i16>*
  %58 = load <8 x i16>, <8 x i16>* %57, align 16
  %59 = getelementptr inbounds i16, i16* %43, i64 %13
  %60 = bitcast i16* %59 to <8 x i16>*
  %61 = load <8 x i16>, <8 x i16>* %60, align 16
  %62 = getelementptr inbounds i16, i16* %43, i64 %15
  %63 = bitcast i16* %62 to <8 x i16>*
  %64 = load <8 x i16>, <8 x i16>* %63, align 16
  %65 = getelementptr inbounds i16, i16* %43, i64 %17
  %66 = bitcast i16* %65 to <8 x i16>*
  %67 = load <8 x i16>, <8 x i16>* %66, align 16
  %68 = getelementptr inbounds i16, i16* %43, i64 %19
  %69 = bitcast i16* %68 to <8 x i16>*
  %70 = load <8 x i16>, <8 x i16>* %69, align 16
  %71 = getelementptr inbounds i16, i16* %43, i64 %21
  %72 = bitcast i16* %71 to <8 x i16>*
  %73 = load <8 x i16>, <8 x i16>* %72, align 16
  %74 = getelementptr inbounds i16, i16* %43, i64 %23
  %75 = bitcast i16* %74 to <8 x i16>*
  %76 = load <8 x i16>, <8 x i16>* %75, align 16
  %77 = getelementptr inbounds i16, i16* %43, i64 %25
  %78 = bitcast i16* %77 to <8 x i16>*
  %79 = load <8 x i16>, <8 x i16>* %78, align 16
  %80 = getelementptr inbounds i16, i16* %43, i64 %27
  %81 = bitcast i16* %80 to <8 x i16>*
  %82 = load <8 x i16>, <8 x i16>* %81, align 16
  %83 = getelementptr inbounds i16, i16* %43, i64 %29
  %84 = bitcast i16* %83 to <8 x i16>*
  %85 = load <8 x i16>, <8 x i16>* %84, align 16
  %86 = getelementptr inbounds i16, i16* %43, i64 %31
  %87 = bitcast i16* %86 to <8 x i16>*
  %88 = load <8 x i16>, <8 x i16>* %87, align 16
  %89 = getelementptr inbounds i16, i16* %43, i64 %33
  %90 = bitcast i16* %89 to <8 x i16>*
  %91 = load <8 x i16>, <8 x i16>* %90, align 16
  %92 = getelementptr inbounds i16, i16* %43, i64 %35
  %93 = bitcast i16* %92 to <8 x i16>*
  %94 = load <8 x i16>, <8 x i16>* %93, align 16
  %95 = bitcast <2 x i64> %48 to <8 x i16>
  %96 = shl <8 x i16> %95, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %97 = shl <8 x i16> %52, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %98 = shl <8 x i16> %55, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %99 = shl <8 x i16> %58, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %100 = shl <8 x i16> %61, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %101 = shl <8 x i16> %64, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %102 = shl <8 x i16> %67, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %103 = shl <8 x i16> %70, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %104 = shl <8 x i16> %73, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %105 = shl <8 x i16> %76, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %106 = shl <8 x i16> %79, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %107 = shl <8 x i16> %82, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %108 = shl <8 x i16> %85, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %109 = shl <8 x i16> %88, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %110 = shl <8 x i16> %91, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %111 = shl <8 x i16> %94, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  br label %191

112:                                              ; preds = %42
  %113 = getelementptr inbounds i16, i16* %43, i64 16
  %114 = bitcast i16* %113 to <8 x i16>*
  %115 = load <8 x i16>, <8 x i16>* %114, align 16
  %116 = getelementptr inbounds i16, i16* %43, i64 32
  %117 = bitcast i16* %116 to <8 x i16>*
  %118 = load <8 x i16>, <8 x i16>* %117, align 16
  %119 = getelementptr inbounds i16, i16* %43, i64 48
  %120 = bitcast i16* %119 to <8 x i16>*
  %121 = load <8 x i16>, <8 x i16>* %120, align 16
  %122 = getelementptr inbounds i16, i16* %43, i64 64
  %123 = bitcast i16* %122 to <8 x i16>*
  %124 = load <8 x i16>, <8 x i16>* %123, align 16
  %125 = getelementptr inbounds i16, i16* %43, i64 80
  %126 = bitcast i16* %125 to <8 x i16>*
  %127 = load <8 x i16>, <8 x i16>* %126, align 16
  %128 = getelementptr inbounds i16, i16* %43, i64 96
  %129 = bitcast i16* %128 to <8 x i16>*
  %130 = load <8 x i16>, <8 x i16>* %129, align 16
  %131 = getelementptr inbounds i16, i16* %43, i64 112
  %132 = bitcast i16* %131 to <8 x i16>*
  %133 = load <8 x i16>, <8 x i16>* %132, align 16
  %134 = getelementptr inbounds i16, i16* %43, i64 128
  %135 = bitcast i16* %134 to <8 x i16>*
  %136 = load <8 x i16>, <8 x i16>* %135, align 16
  %137 = getelementptr inbounds i16, i16* %43, i64 144
  %138 = bitcast i16* %137 to <8 x i16>*
  %139 = load <8 x i16>, <8 x i16>* %138, align 16
  %140 = getelementptr inbounds i16, i16* %43, i64 160
  %141 = bitcast i16* %140 to <8 x i16>*
  %142 = load <8 x i16>, <8 x i16>* %141, align 16
  %143 = getelementptr inbounds i16, i16* %43, i64 176
  %144 = bitcast i16* %143 to <8 x i16>*
  %145 = load <8 x i16>, <8 x i16>* %144, align 16
  %146 = getelementptr inbounds i16, i16* %43, i64 192
  %147 = bitcast i16* %146 to <8 x i16>*
  %148 = load <8 x i16>, <8 x i16>* %147, align 16
  %149 = getelementptr inbounds i16, i16* %43, i64 208
  %150 = bitcast i16* %149 to <8 x i16>*
  %151 = load <8 x i16>, <8 x i16>* %150, align 16
  %152 = getelementptr inbounds i16, i16* %43, i64 224
  %153 = bitcast i16* %152 to <8 x i16>*
  %154 = load <8 x i16>, <8 x i16>* %153, align 16
  %155 = getelementptr inbounds i16, i16* %43, i64 240
  %156 = bitcast i16* %155 to <8 x i16>*
  %157 = load <8 x i16>, <8 x i16>* %156, align 16
  %158 = bitcast <2 x i64> %48 to <8 x i16>
  %159 = add <8 x i16> %158, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %160 = add <8 x i16> %115, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %161 = add <8 x i16> %118, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %162 = add <8 x i16> %121, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %163 = add <8 x i16> %124, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %164 = add <8 x i16> %127, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %165 = add <8 x i16> %130, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %166 = add <8 x i16> %133, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %167 = add <8 x i16> %136, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %168 = add <8 x i16> %139, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %169 = add <8 x i16> %142, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %170 = add <8 x i16> %145, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %171 = add <8 x i16> %148, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %172 = add <8 x i16> %151, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %173 = add <8 x i16> %154, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %174 = add <8 x i16> %157, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %175 = ashr <8 x i16> %159, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %176 = ashr <8 x i16> %160, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %177 = ashr <8 x i16> %161, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %178 = ashr <8 x i16> %162, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %179 = ashr <8 x i16> %163, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %180 = ashr <8 x i16> %164, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %181 = ashr <8 x i16> %165, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %182 = ashr <8 x i16> %166, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %183 = ashr <8 x i16> %167, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %184 = ashr <8 x i16> %168, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %185 = ashr <8 x i16> %169, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %186 = ashr <8 x i16> %170, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %187 = ashr <8 x i16> %171, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %188 = ashr <8 x i16> %172, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %189 = ashr <8 x i16> %173, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %190 = ashr <8 x i16> %174, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  br label %191

191:                                              ; preds = %112, %49
  %192 = phi <8 x i16> [ %108, %49 ], [ %187, %112 ]
  %193 = phi <8 x i16> [ %109, %49 ], [ %188, %112 ]
  %194 = phi <8 x i16> [ %110, %49 ], [ %189, %112 ]
  %195 = phi <8 x i16> [ %111, %49 ], [ %190, %112 ]
  %196 = phi <8 x i16> [ %107, %49 ], [ %186, %112 ]
  %197 = phi <8 x i16> [ %106, %49 ], [ %185, %112 ]
  %198 = phi <8 x i16> [ %105, %49 ], [ %184, %112 ]
  %199 = phi <8 x i16> [ %104, %49 ], [ %183, %112 ]
  %200 = phi <8 x i16> [ %103, %49 ], [ %182, %112 ]
  %201 = phi <8 x i16> [ %102, %49 ], [ %181, %112 ]
  %202 = phi <8 x i16> [ %101, %49 ], [ %180, %112 ]
  %203 = phi <8 x i16> [ %100, %49 ], [ %179, %112 ]
  %204 = phi <8 x i16> [ %99, %49 ], [ %178, %112 ]
  %205 = phi <8 x i16> [ %98, %49 ], [ %177, %112 ]
  %206 = phi <8 x i16> [ %97, %49 ], [ %176, %112 ]
  %207 = phi <8 x i16> [ %96, %49 ], [ %175, %112 ]
  %208 = getelementptr inbounds i16, i16* %43, i64 8
  %209 = add <8 x i16> %207, %195
  %210 = add <8 x i16> %206, %194
  %211 = add <8 x i16> %205, %193
  %212 = add <8 x i16> %204, %192
  %213 = add <8 x i16> %203, %196
  %214 = add <8 x i16> %202, %197
  %215 = add <8 x i16> %201, %198
  %216 = add <8 x i16> %200, %199
  %217 = sub <8 x i16> %200, %199
  %218 = sub <8 x i16> %201, %198
  %219 = sub <8 x i16> %202, %197
  %220 = sub <8 x i16> %203, %196
  %221 = sub <8 x i16> %204, %192
  %222 = sub <8 x i16> %205, %193
  %223 = sub <8 x i16> %206, %194
  %224 = sub <8 x i16> %207, %195
  %225 = add <8 x i16> %209, %216
  %226 = add <8 x i16> %210, %215
  %227 = add <8 x i16> %211, %214
  %228 = add <8 x i16> %212, %213
  %229 = sub <8 x i16> %212, %213
  %230 = sub <8 x i16> %211, %214
  %231 = sub <8 x i16> %210, %215
  %232 = sub <8 x i16> %209, %216
  %233 = add <8 x i16> %225, %228
  %234 = add <8 x i16> %226, %227
  %235 = sub <8 x i16> %226, %227
  %236 = sub <8 x i16> %225, %228
  %237 = shufflevector <8 x i16> %233, <8 x i16> %234, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %238 = shufflevector <8 x i16> %233, <8 x i16> %234, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %239 = shufflevector <8 x i16> %235, <8 x i16> %236, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %240 = shufflevector <8 x i16> %235, <8 x i16> %236, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %241 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %237, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %242 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %238, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %243 = add <4 x i32> %241, <i32 8192, i32 8192, i32 8192, i32 8192>
  %244 = add <4 x i32> %242, <i32 8192, i32 8192, i32 8192, i32 8192>
  %245 = ashr <4 x i32> %243, <i32 14, i32 14, i32 14, i32 14>
  %246 = ashr <4 x i32> %244, <i32 14, i32 14, i32 14, i32 14>
  %247 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %245, <4 x i32> %246) #6
  %248 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %237, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %249 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %238, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %250 = add <4 x i32> %248, <i32 8192, i32 8192, i32 8192, i32 8192>
  %251 = add <4 x i32> %249, <i32 8192, i32 8192, i32 8192, i32 8192>
  %252 = ashr <4 x i32> %250, <i32 14, i32 14, i32 14, i32 14>
  %253 = ashr <4 x i32> %251, <i32 14, i32 14, i32 14, i32 14>
  %254 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %252, <4 x i32> %253) #6
  %255 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %239, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %256 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %240, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %257 = add <4 x i32> %255, <i32 8192, i32 8192, i32 8192, i32 8192>
  %258 = add <4 x i32> %256, <i32 8192, i32 8192, i32 8192, i32 8192>
  %259 = ashr <4 x i32> %257, <i32 14, i32 14, i32 14, i32 14>
  %260 = ashr <4 x i32> %258, <i32 14, i32 14, i32 14, i32 14>
  %261 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %259, <4 x i32> %260) #6
  %262 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %239, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %263 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %240, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %264 = add <4 x i32> %262, <i32 8192, i32 8192, i32 8192, i32 8192>
  %265 = add <4 x i32> %263, <i32 8192, i32 8192, i32 8192, i32 8192>
  %266 = ashr <4 x i32> %264, <i32 14, i32 14, i32 14, i32 14>
  %267 = ashr <4 x i32> %265, <i32 14, i32 14, i32 14, i32 14>
  %268 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %266, <4 x i32> %267) #6
  %269 = shufflevector <8 x i16> %231, <8 x i16> %230, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %270 = shufflevector <8 x i16> %231, <8 x i16> %230, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %271 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %269, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %272 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %270, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %273 = add <4 x i32> %271, <i32 8192, i32 8192, i32 8192, i32 8192>
  %274 = add <4 x i32> %272, <i32 8192, i32 8192, i32 8192, i32 8192>
  %275 = ashr <4 x i32> %273, <i32 14, i32 14, i32 14, i32 14>
  %276 = ashr <4 x i32> %274, <i32 14, i32 14, i32 14, i32 14>
  %277 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %275, <4 x i32> %276) #6
  %278 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %269, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %279 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %270, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %280 = add <4 x i32> %278, <i32 8192, i32 8192, i32 8192, i32 8192>
  %281 = add <4 x i32> %279, <i32 8192, i32 8192, i32 8192, i32 8192>
  %282 = ashr <4 x i32> %280, <i32 14, i32 14, i32 14, i32 14>
  %283 = ashr <4 x i32> %281, <i32 14, i32 14, i32 14, i32 14>
  %284 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %282, <4 x i32> %283) #6
  %285 = add <8 x i16> %277, %229
  %286 = sub <8 x i16> %229, %277
  %287 = sub <8 x i16> %232, %284
  %288 = add <8 x i16> %284, %232
  %289 = shufflevector <8 x i16> %285, <8 x i16> %288, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %290 = shufflevector <8 x i16> %285, <8 x i16> %288, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %291 = shufflevector <8 x i16> %286, <8 x i16> %287, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %292 = shufflevector <8 x i16> %286, <8 x i16> %287, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %293 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %289, <8 x i16> <i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069>) #6
  %294 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %290, <8 x i16> <i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069>) #6
  %295 = add <4 x i32> %293, <i32 8192, i32 8192, i32 8192, i32 8192>
  %296 = add <4 x i32> %294, <i32 8192, i32 8192, i32 8192, i32 8192>
  %297 = ashr <4 x i32> %295, <i32 14, i32 14, i32 14, i32 14>
  %298 = ashr <4 x i32> %296, <i32 14, i32 14, i32 14, i32 14>
  %299 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %297, <4 x i32> %298) #6
  %300 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %289, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %301 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %290, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %302 = add <4 x i32> %300, <i32 8192, i32 8192, i32 8192, i32 8192>
  %303 = add <4 x i32> %301, <i32 8192, i32 8192, i32 8192, i32 8192>
  %304 = ashr <4 x i32> %302, <i32 14, i32 14, i32 14, i32 14>
  %305 = ashr <4 x i32> %303, <i32 14, i32 14, i32 14, i32 14>
  %306 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %304, <4 x i32> %305) #6
  %307 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %291, <8 x i16> <i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102>) #6
  %308 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %292, <8 x i16> <i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102>) #6
  %309 = add <4 x i32> %307, <i32 8192, i32 8192, i32 8192, i32 8192>
  %310 = add <4 x i32> %308, <i32 8192, i32 8192, i32 8192, i32 8192>
  %311 = ashr <4 x i32> %309, <i32 14, i32 14, i32 14, i32 14>
  %312 = ashr <4 x i32> %310, <i32 14, i32 14, i32 14, i32 14>
  %313 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %311, <4 x i32> %312) #6
  %314 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %291, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %315 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %292, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %316 = add <4 x i32> %314, <i32 8192, i32 8192, i32 8192, i32 8192>
  %317 = add <4 x i32> %315, <i32 8192, i32 8192, i32 8192, i32 8192>
  %318 = ashr <4 x i32> %316, <i32 14, i32 14, i32 14, i32 14>
  %319 = ashr <4 x i32> %317, <i32 14, i32 14, i32 14, i32 14>
  %320 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %318, <4 x i32> %319) #6
  %321 = shufflevector <8 x i16> %222, <8 x i16> %219, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %322 = shufflevector <8 x i16> %222, <8 x i16> %219, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %323 = shufflevector <8 x i16> %221, <8 x i16> %220, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %324 = shufflevector <8 x i16> %221, <8 x i16> %220, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %325 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %321, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %326 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %322, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %327 = add <4 x i32> %325, <i32 8192, i32 8192, i32 8192, i32 8192>
  %328 = add <4 x i32> %326, <i32 8192, i32 8192, i32 8192, i32 8192>
  %329 = ashr <4 x i32> %327, <i32 14, i32 14, i32 14, i32 14>
  %330 = ashr <4 x i32> %328, <i32 14, i32 14, i32 14, i32 14>
  %331 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %329, <4 x i32> %330) #6
  %332 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %323, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %333 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %324, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %334 = add <4 x i32> %332, <i32 8192, i32 8192, i32 8192, i32 8192>
  %335 = add <4 x i32> %333, <i32 8192, i32 8192, i32 8192, i32 8192>
  %336 = ashr <4 x i32> %334, <i32 14, i32 14, i32 14, i32 14>
  %337 = ashr <4 x i32> %335, <i32 14, i32 14, i32 14, i32 14>
  %338 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %336, <4 x i32> %337) #6
  %339 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %321, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %340 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %322, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %341 = add <4 x i32> %339, <i32 8192, i32 8192, i32 8192, i32 8192>
  %342 = add <4 x i32> %340, <i32 8192, i32 8192, i32 8192, i32 8192>
  %343 = ashr <4 x i32> %341, <i32 14, i32 14, i32 14, i32 14>
  %344 = ashr <4 x i32> %342, <i32 14, i32 14, i32 14, i32 14>
  %345 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %343, <4 x i32> %344) #6
  %346 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %323, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %347 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %324, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %348 = add <4 x i32> %346, <i32 8192, i32 8192, i32 8192, i32 8192>
  %349 = add <4 x i32> %347, <i32 8192, i32 8192, i32 8192, i32 8192>
  %350 = ashr <4 x i32> %348, <i32 14, i32 14, i32 14, i32 14>
  %351 = ashr <4 x i32> %349, <i32 14, i32 14, i32 14, i32 14>
  %352 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %350, <4 x i32> %351) #6
  %353 = add <8 x i16> %338, %217
  %354 = add <8 x i16> %331, %218
  %355 = sub <8 x i16> %218, %331
  %356 = sub <8 x i16> %217, %338
  %357 = sub <8 x i16> %224, %352
  %358 = sub <8 x i16> %223, %345
  %359 = add <8 x i16> %345, %223
  %360 = add <8 x i16> %352, %224
  %361 = shufflevector <8 x i16> %354, <8 x i16> %359, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %362 = shufflevector <8 x i16> %354, <8 x i16> %359, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %363 = shufflevector <8 x i16> %355, <8 x i16> %358, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %364 = shufflevector <8 x i16> %355, <8 x i16> %358, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %365 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %361, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %366 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %362, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %367 = add <4 x i32> %365, <i32 8192, i32 8192, i32 8192, i32 8192>
  %368 = add <4 x i32> %366, <i32 8192, i32 8192, i32 8192, i32 8192>
  %369 = ashr <4 x i32> %367, <i32 14, i32 14, i32 14, i32 14>
  %370 = ashr <4 x i32> %368, <i32 14, i32 14, i32 14, i32 14>
  %371 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %369, <4 x i32> %370) #6
  %372 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %363, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %373 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %364, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %374 = add <4 x i32> %372, <i32 8192, i32 8192, i32 8192, i32 8192>
  %375 = add <4 x i32> %373, <i32 8192, i32 8192, i32 8192, i32 8192>
  %376 = ashr <4 x i32> %374, <i32 14, i32 14, i32 14, i32 14>
  %377 = ashr <4 x i32> %375, <i32 14, i32 14, i32 14, i32 14>
  %378 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %376, <4 x i32> %377) #6
  %379 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %361, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %380 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %362, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %381 = add <4 x i32> %379, <i32 8192, i32 8192, i32 8192, i32 8192>
  %382 = add <4 x i32> %380, <i32 8192, i32 8192, i32 8192, i32 8192>
  %383 = ashr <4 x i32> %381, <i32 14, i32 14, i32 14, i32 14>
  %384 = ashr <4 x i32> %382, <i32 14, i32 14, i32 14, i32 14>
  %385 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %383, <4 x i32> %384) #6
  %386 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %363, <8 x i16> <i16 15137, i16 -6270, i16 15137, i16 -6270, i16 15137, i16 -6270, i16 15137, i16 -6270>) #6
  %387 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %364, <8 x i16> <i16 15137, i16 -6270, i16 15137, i16 -6270, i16 15137, i16 -6270, i16 15137, i16 -6270>) #6
  %388 = add <4 x i32> %386, <i32 8192, i32 8192, i32 8192, i32 8192>
  %389 = add <4 x i32> %387, <i32 8192, i32 8192, i32 8192, i32 8192>
  %390 = ashr <4 x i32> %388, <i32 14, i32 14, i32 14, i32 14>
  %391 = ashr <4 x i32> %389, <i32 14, i32 14, i32 14, i32 14>
  %392 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %390, <4 x i32> %391) #6
  %393 = add <8 x i16> %371, %353
  %394 = sub <8 x i16> %353, %371
  %395 = add <8 x i16> %378, %356
  %396 = sub <8 x i16> %356, %378
  %397 = sub <8 x i16> %357, %392
  %398 = add <8 x i16> %392, %357
  %399 = sub <8 x i16> %360, %385
  %400 = add <8 x i16> %385, %360
  %401 = shufflevector <8 x i16> %393, <8 x i16> %400, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %402 = shufflevector <8 x i16> %393, <8 x i16> %400, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %403 = shufflevector <8 x i16> %394, <8 x i16> %399, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %404 = shufflevector <8 x i16> %394, <8 x i16> %399, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %405 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %401, <8 x i16> <i16 1606, i16 16305, i16 1606, i16 16305, i16 1606, i16 16305, i16 1606, i16 16305>) #6
  %406 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %402, <8 x i16> <i16 1606, i16 16305, i16 1606, i16 16305, i16 1606, i16 16305, i16 1606, i16 16305>) #6
  %407 = add <4 x i32> %405, <i32 8192, i32 8192, i32 8192, i32 8192>
  %408 = add <4 x i32> %406, <i32 8192, i32 8192, i32 8192, i32 8192>
  %409 = ashr <4 x i32> %407, <i32 14, i32 14, i32 14, i32 14>
  %410 = ashr <4 x i32> %408, <i32 14, i32 14, i32 14, i32 14>
  %411 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %409, <4 x i32> %410) #6
  %412 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %403, <8 x i16> <i16 12665, i16 10394, i16 12665, i16 10394, i16 12665, i16 10394, i16 12665, i16 10394>) #6
  %413 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %404, <8 x i16> <i16 12665, i16 10394, i16 12665, i16 10394, i16 12665, i16 10394, i16 12665, i16 10394>) #6
  %414 = add <4 x i32> %412, <i32 8192, i32 8192, i32 8192, i32 8192>
  %415 = add <4 x i32> %413, <i32 8192, i32 8192, i32 8192, i32 8192>
  %416 = ashr <4 x i32> %414, <i32 14, i32 14, i32 14, i32 14>
  %417 = ashr <4 x i32> %415, <i32 14, i32 14, i32 14, i32 14>
  %418 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %416, <4 x i32> %417) #6
  %419 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %401, <8 x i16> <i16 -16305, i16 1606, i16 -16305, i16 1606, i16 -16305, i16 1606, i16 -16305, i16 1606>) #6
  %420 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %402, <8 x i16> <i16 -16305, i16 1606, i16 -16305, i16 1606, i16 -16305, i16 1606, i16 -16305, i16 1606>) #6
  %421 = add <4 x i32> %419, <i32 8192, i32 8192, i32 8192, i32 8192>
  %422 = add <4 x i32> %420, <i32 8192, i32 8192, i32 8192, i32 8192>
  %423 = ashr <4 x i32> %421, <i32 14, i32 14, i32 14, i32 14>
  %424 = ashr <4 x i32> %422, <i32 14, i32 14, i32 14, i32 14>
  %425 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %423, <4 x i32> %424) #6
  %426 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %403, <8 x i16> <i16 -10394, i16 12665, i16 -10394, i16 12665, i16 -10394, i16 12665, i16 -10394, i16 12665>) #6
  %427 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %404, <8 x i16> <i16 -10394, i16 12665, i16 -10394, i16 12665, i16 -10394, i16 12665, i16 -10394, i16 12665>) #6
  %428 = add <4 x i32> %426, <i32 8192, i32 8192, i32 8192, i32 8192>
  %429 = add <4 x i32> %427, <i32 8192, i32 8192, i32 8192, i32 8192>
  %430 = ashr <4 x i32> %428, <i32 14, i32 14, i32 14, i32 14>
  %431 = ashr <4 x i32> %429, <i32 14, i32 14, i32 14, i32 14>
  %432 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %430, <4 x i32> %431) #6
  %433 = shufflevector <8 x i16> %395, <8 x i16> %398, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %434 = shufflevector <8 x i16> %395, <8 x i16> %398, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %435 = shufflevector <8 x i16> %396, <8 x i16> %397, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %436 = shufflevector <8 x i16> %396, <8 x i16> %397, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %437 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %433, <8 x i16> <i16 7723, i16 14449, i16 7723, i16 14449, i16 7723, i16 14449, i16 7723, i16 14449>) #6
  %438 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %434, <8 x i16> <i16 7723, i16 14449, i16 7723, i16 14449, i16 7723, i16 14449, i16 7723, i16 14449>) #6
  %439 = add <4 x i32> %437, <i32 8192, i32 8192, i32 8192, i32 8192>
  %440 = add <4 x i32> %438, <i32 8192, i32 8192, i32 8192, i32 8192>
  %441 = ashr <4 x i32> %439, <i32 14, i32 14, i32 14, i32 14>
  %442 = ashr <4 x i32> %440, <i32 14, i32 14, i32 14, i32 14>
  %443 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %441, <4 x i32> %442) #6
  %444 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %435, <8 x i16> <i16 15679, i16 4756, i16 15679, i16 4756, i16 15679, i16 4756, i16 15679, i16 4756>) #6
  %445 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %436, <8 x i16> <i16 15679, i16 4756, i16 15679, i16 4756, i16 15679, i16 4756, i16 15679, i16 4756>) #6
  %446 = add <4 x i32> %444, <i32 8192, i32 8192, i32 8192, i32 8192>
  %447 = add <4 x i32> %445, <i32 8192, i32 8192, i32 8192, i32 8192>
  %448 = ashr <4 x i32> %446, <i32 14, i32 14, i32 14, i32 14>
  %449 = ashr <4 x i32> %447, <i32 14, i32 14, i32 14, i32 14>
  %450 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %448, <4 x i32> %449) #6
  %451 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %433, <8 x i16> <i16 -14449, i16 7723, i16 -14449, i16 7723, i16 -14449, i16 7723, i16 -14449, i16 7723>) #6
  %452 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %434, <8 x i16> <i16 -14449, i16 7723, i16 -14449, i16 7723, i16 -14449, i16 7723, i16 -14449, i16 7723>) #6
  %453 = add <4 x i32> %451, <i32 8192, i32 8192, i32 8192, i32 8192>
  %454 = add <4 x i32> %452, <i32 8192, i32 8192, i32 8192, i32 8192>
  %455 = ashr <4 x i32> %453, <i32 14, i32 14, i32 14, i32 14>
  %456 = ashr <4 x i32> %454, <i32 14, i32 14, i32 14, i32 14>
  %457 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %455, <4 x i32> %456) #6
  %458 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %435, <8 x i16> <i16 -4756, i16 15679, i16 -4756, i16 15679, i16 -4756, i16 15679, i16 -4756, i16 15679>) #6
  %459 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %436, <8 x i16> <i16 -4756, i16 15679, i16 -4756, i16 15679, i16 -4756, i16 15679, i16 -4756, i16 15679>) #6
  %460 = add <4 x i32> %458, <i32 8192, i32 8192, i32 8192, i32 8192>
  %461 = add <4 x i32> %459, <i32 8192, i32 8192, i32 8192, i32 8192>
  %462 = ashr <4 x i32> %460, <i32 14, i32 14, i32 14, i32 14>
  %463 = ashr <4 x i32> %461, <i32 14, i32 14, i32 14, i32 14>
  %464 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %462, <4 x i32> %463) #6
  %465 = shufflevector <8 x i16> %247, <8 x i16> %411, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %466 = shufflevector <8 x i16> %299, <8 x i16> %464, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %467 = shufflevector <8 x i16> %247, <8 x i16> %411, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %468 = shufflevector <8 x i16> %299, <8 x i16> %464, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %469 = shufflevector <8 x i16> %261, <8 x i16> %443, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %470 = shufflevector <8 x i16> %320, <8 x i16> %432, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %471 = shufflevector <8 x i16> %261, <8 x i16> %443, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %472 = shufflevector <8 x i16> %320, <8 x i16> %432, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %473 = bitcast <8 x i16> %465 to <4 x i32>
  %474 = bitcast <8 x i16> %466 to <4 x i32>
  %475 = shufflevector <4 x i32> %473, <4 x i32> %474, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %476 = bitcast <4 x i32> %475 to <2 x i64>
  %477 = bitcast <8 x i16> %467 to <4 x i32>
  %478 = bitcast <8 x i16> %468 to <4 x i32>
  %479 = shufflevector <4 x i32> %477, <4 x i32> %478, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %480 = bitcast <4 x i32> %479 to <2 x i64>
  %481 = shufflevector <4 x i32> %473, <4 x i32> %474, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %482 = bitcast <4 x i32> %481 to <2 x i64>
  %483 = shufflevector <4 x i32> %477, <4 x i32> %478, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %484 = bitcast <4 x i32> %483 to <2 x i64>
  %485 = bitcast <8 x i16> %469 to <4 x i32>
  %486 = bitcast <8 x i16> %470 to <4 x i32>
  %487 = shufflevector <4 x i32> %485, <4 x i32> %486, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %488 = bitcast <4 x i32> %487 to <2 x i64>
  %489 = bitcast <8 x i16> %471 to <4 x i32>
  %490 = bitcast <8 x i16> %472 to <4 x i32>
  %491 = shufflevector <4 x i32> %489, <4 x i32> %490, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %492 = bitcast <4 x i32> %491 to <2 x i64>
  %493 = shufflevector <4 x i32> %485, <4 x i32> %486, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %494 = bitcast <4 x i32> %493 to <2 x i64>
  %495 = shufflevector <4 x i32> %489, <4 x i32> %490, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %496 = bitcast <4 x i32> %495 to <2 x i64>
  %497 = shufflevector <2 x i64> %476, <2 x i64> %488, <2 x i32> <i32 0, i32 2>
  %498 = shufflevector <2 x i64> %476, <2 x i64> %488, <2 x i32> <i32 1, i32 3>
  %499 = shufflevector <2 x i64> %482, <2 x i64> %494, <2 x i32> <i32 0, i32 2>
  %500 = shufflevector <2 x i64> %482, <2 x i64> %494, <2 x i32> <i32 1, i32 3>
  %501 = shufflevector <2 x i64> %480, <2 x i64> %492, <2 x i32> <i32 0, i32 2>
  %502 = shufflevector <2 x i64> %480, <2 x i64> %492, <2 x i32> <i32 1, i32 3>
  %503 = shufflevector <2 x i64> %484, <2 x i64> %496, <2 x i32> <i32 0, i32 2>
  %504 = shufflevector <2 x i64> %484, <2 x i64> %496, <2 x i32> <i32 1, i32 3>
  br i1 %41, label %505, label %521

505:                                              ; preds = %191
  %506 = bitcast i16* %44 to <2 x i64>*
  store <2 x i64> %497, <2 x i64>* %506, align 1
  %507 = getelementptr inbounds i16, i16* %44, i64 16
  %508 = bitcast i16* %507 to <2 x i64>*
  store <2 x i64> %498, <2 x i64>* %508, align 1
  %509 = getelementptr inbounds i16, i16* %44, i64 32
  %510 = bitcast i16* %509 to <2 x i64>*
  store <2 x i64> %499, <2 x i64>* %510, align 1
  %511 = getelementptr inbounds i16, i16* %44, i64 48
  %512 = bitcast i16* %511 to <2 x i64>*
  store <2 x i64> %500, <2 x i64>* %512, align 1
  %513 = getelementptr inbounds i16, i16* %44, i64 64
  %514 = bitcast i16* %513 to <2 x i64>*
  store <2 x i64> %501, <2 x i64>* %514, align 1
  %515 = getelementptr inbounds i16, i16* %44, i64 80
  %516 = bitcast i16* %515 to <2 x i64>*
  store <2 x i64> %502, <2 x i64>* %516, align 1
  %517 = getelementptr inbounds i16, i16* %44, i64 96
  %518 = bitcast i16* %517 to <2 x i64>*
  store <2 x i64> %503, <2 x i64>* %518, align 1
  %519 = getelementptr inbounds i16, i16* %44, i64 112
  %520 = bitcast i16* %519 to <2 x i64>*
  store <2 x i64> %504, <2 x i64>* %520, align 1
  br label %585

521:                                              ; preds = %191
  %522 = bitcast <2 x i64> %497 to <8 x i16>
  %523 = ashr <8 x i16> %522, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %524 = shufflevector <8 x i16> %522, <8 x i16> %523, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %525 = shufflevector <8 x i16> %522, <8 x i16> %523, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %526 = bitcast i32* %45 to <8 x i16>*
  store <8 x i16> %524, <8 x i16>* %526, align 1
  %527 = getelementptr inbounds i32, i32* %45, i64 4
  %528 = bitcast i32* %527 to <8 x i16>*
  store <8 x i16> %525, <8 x i16>* %528, align 1
  %529 = getelementptr inbounds i32, i32* %45, i64 16
  %530 = bitcast <2 x i64> %498 to <8 x i16>
  %531 = ashr <8 x i16> %530, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %532 = shufflevector <8 x i16> %530, <8 x i16> %531, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %533 = shufflevector <8 x i16> %530, <8 x i16> %531, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %534 = bitcast i32* %529 to <8 x i16>*
  store <8 x i16> %532, <8 x i16>* %534, align 1
  %535 = getelementptr inbounds i32, i32* %45, i64 20
  %536 = bitcast i32* %535 to <8 x i16>*
  store <8 x i16> %533, <8 x i16>* %536, align 1
  %537 = getelementptr inbounds i32, i32* %45, i64 32
  %538 = bitcast <2 x i64> %499 to <8 x i16>
  %539 = ashr <8 x i16> %538, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %540 = shufflevector <8 x i16> %538, <8 x i16> %539, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %541 = shufflevector <8 x i16> %538, <8 x i16> %539, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %542 = bitcast i32* %537 to <8 x i16>*
  store <8 x i16> %540, <8 x i16>* %542, align 1
  %543 = getelementptr inbounds i32, i32* %45, i64 36
  %544 = bitcast i32* %543 to <8 x i16>*
  store <8 x i16> %541, <8 x i16>* %544, align 1
  %545 = getelementptr inbounds i32, i32* %45, i64 48
  %546 = bitcast <2 x i64> %500 to <8 x i16>
  %547 = ashr <8 x i16> %546, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %548 = shufflevector <8 x i16> %546, <8 x i16> %547, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %549 = shufflevector <8 x i16> %546, <8 x i16> %547, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %550 = bitcast i32* %545 to <8 x i16>*
  store <8 x i16> %548, <8 x i16>* %550, align 1
  %551 = getelementptr inbounds i32, i32* %45, i64 52
  %552 = bitcast i32* %551 to <8 x i16>*
  store <8 x i16> %549, <8 x i16>* %552, align 1
  %553 = getelementptr inbounds i32, i32* %45, i64 64
  %554 = bitcast <2 x i64> %501 to <8 x i16>
  %555 = ashr <8 x i16> %554, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %556 = shufflevector <8 x i16> %554, <8 x i16> %555, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %557 = shufflevector <8 x i16> %554, <8 x i16> %555, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %558 = bitcast i32* %553 to <8 x i16>*
  store <8 x i16> %556, <8 x i16>* %558, align 1
  %559 = getelementptr inbounds i32, i32* %45, i64 68
  %560 = bitcast i32* %559 to <8 x i16>*
  store <8 x i16> %557, <8 x i16>* %560, align 1
  %561 = getelementptr inbounds i32, i32* %45, i64 80
  %562 = bitcast <2 x i64> %502 to <8 x i16>
  %563 = ashr <8 x i16> %562, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %564 = shufflevector <8 x i16> %562, <8 x i16> %563, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %565 = shufflevector <8 x i16> %562, <8 x i16> %563, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %566 = bitcast i32* %561 to <8 x i16>*
  store <8 x i16> %564, <8 x i16>* %566, align 1
  %567 = getelementptr inbounds i32, i32* %45, i64 84
  %568 = bitcast i32* %567 to <8 x i16>*
  store <8 x i16> %565, <8 x i16>* %568, align 1
  %569 = getelementptr inbounds i32, i32* %45, i64 96
  %570 = bitcast <2 x i64> %503 to <8 x i16>
  %571 = ashr <8 x i16> %570, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %572 = shufflevector <8 x i16> %570, <8 x i16> %571, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %573 = shufflevector <8 x i16> %570, <8 x i16> %571, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %574 = bitcast i32* %569 to <8 x i16>*
  store <8 x i16> %572, <8 x i16>* %574, align 1
  %575 = getelementptr inbounds i32, i32* %45, i64 100
  %576 = bitcast i32* %575 to <8 x i16>*
  store <8 x i16> %573, <8 x i16>* %576, align 1
  %577 = getelementptr inbounds i32, i32* %45, i64 112
  %578 = bitcast <2 x i64> %504 to <8 x i16>
  %579 = ashr <8 x i16> %578, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %580 = shufflevector <8 x i16> %578, <8 x i16> %579, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %581 = shufflevector <8 x i16> %578, <8 x i16> %579, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %582 = bitcast i32* %577 to <8 x i16>*
  store <8 x i16> %580, <8 x i16>* %582, align 1
  %583 = getelementptr inbounds i32, i32* %45, i64 116
  %584 = bitcast i32* %583 to <8 x i16>*
  store <8 x i16> %581, <8 x i16>* %584, align 1
  br label %585

585:                                              ; preds = %505, %521
  %586 = shufflevector <8 x i16> %254, <8 x i16> %418, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %587 = shufflevector <8 x i16> %313, <8 x i16> %457, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %588 = shufflevector <8 x i16> %254, <8 x i16> %418, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %589 = shufflevector <8 x i16> %313, <8 x i16> %457, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %590 = shufflevector <8 x i16> %268, <8 x i16> %450, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %591 = shufflevector <8 x i16> %306, <8 x i16> %425, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %592 = shufflevector <8 x i16> %268, <8 x i16> %450, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %593 = shufflevector <8 x i16> %306, <8 x i16> %425, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %594 = bitcast <8 x i16> %586 to <4 x i32>
  %595 = bitcast <8 x i16> %587 to <4 x i32>
  %596 = shufflevector <4 x i32> %594, <4 x i32> %595, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %597 = bitcast <4 x i32> %596 to <2 x i64>
  %598 = bitcast <8 x i16> %588 to <4 x i32>
  %599 = bitcast <8 x i16> %589 to <4 x i32>
  %600 = shufflevector <4 x i32> %598, <4 x i32> %599, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %601 = bitcast <4 x i32> %600 to <2 x i64>
  %602 = shufflevector <4 x i32> %594, <4 x i32> %595, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %603 = bitcast <4 x i32> %602 to <2 x i64>
  %604 = shufflevector <4 x i32> %598, <4 x i32> %599, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %605 = bitcast <4 x i32> %604 to <2 x i64>
  %606 = bitcast <8 x i16> %590 to <4 x i32>
  %607 = bitcast <8 x i16> %591 to <4 x i32>
  %608 = shufflevector <4 x i32> %606, <4 x i32> %607, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %609 = bitcast <4 x i32> %608 to <2 x i64>
  %610 = bitcast <8 x i16> %592 to <4 x i32>
  %611 = bitcast <8 x i16> %593 to <4 x i32>
  %612 = shufflevector <4 x i32> %610, <4 x i32> %611, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %613 = bitcast <4 x i32> %612 to <2 x i64>
  %614 = shufflevector <4 x i32> %606, <4 x i32> %607, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %615 = bitcast <4 x i32> %614 to <2 x i64>
  %616 = shufflevector <4 x i32> %610, <4 x i32> %611, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %617 = bitcast <4 x i32> %616 to <2 x i64>
  %618 = shufflevector <2 x i64> %597, <2 x i64> %609, <2 x i32> <i32 0, i32 2>
  %619 = shufflevector <2 x i64> %597, <2 x i64> %609, <2 x i32> <i32 1, i32 3>
  %620 = shufflevector <2 x i64> %603, <2 x i64> %615, <2 x i32> <i32 0, i32 2>
  %621 = shufflevector <2 x i64> %603, <2 x i64> %615, <2 x i32> <i32 1, i32 3>
  %622 = shufflevector <2 x i64> %601, <2 x i64> %613, <2 x i32> <i32 0, i32 2>
  %623 = shufflevector <2 x i64> %601, <2 x i64> %613, <2 x i32> <i32 1, i32 3>
  %624 = shufflevector <2 x i64> %605, <2 x i64> %617, <2 x i32> <i32 0, i32 2>
  %625 = shufflevector <2 x i64> %605, <2 x i64> %617, <2 x i32> <i32 1, i32 3>
  br i1 %41, label %626, label %643

626:                                              ; preds = %585
  %627 = getelementptr inbounds i16, i16* %44, i64 8
  %628 = bitcast i16* %627 to <2 x i64>*
  store <2 x i64> %618, <2 x i64>* %628, align 1
  %629 = getelementptr inbounds i16, i16* %44, i64 24
  %630 = bitcast i16* %629 to <2 x i64>*
  store <2 x i64> %619, <2 x i64>* %630, align 1
  %631 = getelementptr inbounds i16, i16* %44, i64 40
  %632 = bitcast i16* %631 to <2 x i64>*
  store <2 x i64> %620, <2 x i64>* %632, align 1
  %633 = getelementptr inbounds i16, i16* %44, i64 56
  %634 = bitcast i16* %633 to <2 x i64>*
  store <2 x i64> %621, <2 x i64>* %634, align 1
  %635 = getelementptr inbounds i16, i16* %44, i64 72
  %636 = bitcast i16* %635 to <2 x i64>*
  store <2 x i64> %622, <2 x i64>* %636, align 1
  %637 = getelementptr inbounds i16, i16* %44, i64 88
  %638 = bitcast i16* %637 to <2 x i64>*
  store <2 x i64> %623, <2 x i64>* %638, align 1
  %639 = getelementptr inbounds i16, i16* %44, i64 104
  %640 = bitcast i16* %639 to <2 x i64>*
  store <2 x i64> %624, <2 x i64>* %640, align 1
  %641 = getelementptr inbounds i16, i16* %44, i64 120
  %642 = bitcast i16* %641 to <2 x i64>*
  store <2 x i64> %625, <2 x i64>* %642, align 1
  br label %708

643:                                              ; preds = %585
  %644 = getelementptr inbounds i32, i32* %45, i64 8
  %645 = bitcast <2 x i64> %618 to <8 x i16>
  %646 = ashr <8 x i16> %645, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %647 = shufflevector <8 x i16> %645, <8 x i16> %646, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %648 = shufflevector <8 x i16> %645, <8 x i16> %646, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %649 = bitcast i32* %644 to <8 x i16>*
  store <8 x i16> %647, <8 x i16>* %649, align 1
  %650 = getelementptr inbounds i32, i32* %45, i64 12
  %651 = bitcast i32* %650 to <8 x i16>*
  store <8 x i16> %648, <8 x i16>* %651, align 1
  %652 = getelementptr inbounds i32, i32* %45, i64 24
  %653 = bitcast <2 x i64> %619 to <8 x i16>
  %654 = ashr <8 x i16> %653, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %655 = shufflevector <8 x i16> %653, <8 x i16> %654, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %656 = shufflevector <8 x i16> %653, <8 x i16> %654, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %657 = bitcast i32* %652 to <8 x i16>*
  store <8 x i16> %655, <8 x i16>* %657, align 1
  %658 = getelementptr inbounds i32, i32* %45, i64 28
  %659 = bitcast i32* %658 to <8 x i16>*
  store <8 x i16> %656, <8 x i16>* %659, align 1
  %660 = getelementptr inbounds i32, i32* %45, i64 40
  %661 = bitcast <2 x i64> %620 to <8 x i16>
  %662 = ashr <8 x i16> %661, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %663 = shufflevector <8 x i16> %661, <8 x i16> %662, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %664 = shufflevector <8 x i16> %661, <8 x i16> %662, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %665 = bitcast i32* %660 to <8 x i16>*
  store <8 x i16> %663, <8 x i16>* %665, align 1
  %666 = getelementptr inbounds i32, i32* %45, i64 44
  %667 = bitcast i32* %666 to <8 x i16>*
  store <8 x i16> %664, <8 x i16>* %667, align 1
  %668 = getelementptr inbounds i32, i32* %45, i64 56
  %669 = bitcast <2 x i64> %621 to <8 x i16>
  %670 = ashr <8 x i16> %669, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %671 = shufflevector <8 x i16> %669, <8 x i16> %670, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %672 = shufflevector <8 x i16> %669, <8 x i16> %670, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %673 = bitcast i32* %668 to <8 x i16>*
  store <8 x i16> %671, <8 x i16>* %673, align 1
  %674 = getelementptr inbounds i32, i32* %45, i64 60
  %675 = bitcast i32* %674 to <8 x i16>*
  store <8 x i16> %672, <8 x i16>* %675, align 1
  %676 = getelementptr inbounds i32, i32* %45, i64 72
  %677 = bitcast <2 x i64> %622 to <8 x i16>
  %678 = ashr <8 x i16> %677, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %679 = shufflevector <8 x i16> %677, <8 x i16> %678, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %680 = shufflevector <8 x i16> %677, <8 x i16> %678, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %681 = bitcast i32* %676 to <8 x i16>*
  store <8 x i16> %679, <8 x i16>* %681, align 1
  %682 = getelementptr inbounds i32, i32* %45, i64 76
  %683 = bitcast i32* %682 to <8 x i16>*
  store <8 x i16> %680, <8 x i16>* %683, align 1
  %684 = getelementptr inbounds i32, i32* %45, i64 88
  %685 = bitcast <2 x i64> %623 to <8 x i16>
  %686 = ashr <8 x i16> %685, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %687 = shufflevector <8 x i16> %685, <8 x i16> %686, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %688 = shufflevector <8 x i16> %685, <8 x i16> %686, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %689 = bitcast i32* %684 to <8 x i16>*
  store <8 x i16> %687, <8 x i16>* %689, align 1
  %690 = getelementptr inbounds i32, i32* %45, i64 92
  %691 = bitcast i32* %690 to <8 x i16>*
  store <8 x i16> %688, <8 x i16>* %691, align 1
  %692 = getelementptr inbounds i32, i32* %45, i64 104
  %693 = bitcast <2 x i64> %624 to <8 x i16>
  %694 = ashr <8 x i16> %693, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %695 = shufflevector <8 x i16> %693, <8 x i16> %694, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %696 = shufflevector <8 x i16> %693, <8 x i16> %694, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %697 = bitcast i32* %692 to <8 x i16>*
  store <8 x i16> %695, <8 x i16>* %697, align 1
  %698 = getelementptr inbounds i32, i32* %45, i64 108
  %699 = bitcast i32* %698 to <8 x i16>*
  store <8 x i16> %696, <8 x i16>* %699, align 1
  %700 = getelementptr inbounds i32, i32* %45, i64 120
  %701 = bitcast <2 x i64> %625 to <8 x i16>
  %702 = ashr <8 x i16> %701, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %703 = shufflevector <8 x i16> %701, <8 x i16> %702, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %704 = shufflevector <8 x i16> %701, <8 x i16> %702, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %705 = bitcast i32* %700 to <8 x i16>*
  store <8 x i16> %703, <8 x i16>* %705, align 1
  %706 = getelementptr inbounds i32, i32* %45, i64 124
  %707 = bitcast i32* %706 to <8 x i16>*
  store <8 x i16> %704, <8 x i16>* %707, align 1
  br label %708

708:                                              ; preds = %626, %643
  %709 = getelementptr inbounds i16, i16* %44, i64 128
  %710 = getelementptr inbounds i32, i32* %45, i64 128
  %711 = select i1 %41, i32* %45, i32* %710
  %712 = select i1 %41, i16* %709, i16* %44
  %713 = add nuw nsw i32 %46, 8
  %714 = icmp ult i32 %713, 16
  br i1 %714, label %42, label %715

715:                                              ; preds = %708
  %716 = add nuw nsw i32 %37, 1
  %717 = icmp eq i32 %716, 2
  br i1 %717, label %718, label %36

718:                                              ; preds = %715
  call void @llvm.lifetime.end.p0i8(i64 512, i8* nonnull %5) #6
  ret void
}

; Function Attrs: argmemonly nounwind
declare void @llvm.memset.p0i8.i64(i8* nocapture writeonly, i8, i64, i1 immarg) #1

; Function Attrs: nounwind ssp uwtable
define hidden void @vpx_fdct32x32_rd_sse2(i16* nocapture readonly, i32* nocapture, i32) local_unnamed_addr #2 {
  %4 = alloca [1024 x i16], align 16
  %5 = alloca [32 x <2 x i64>], align 16
  %6 = shl nsw i32 %2, 1
  %7 = mul nsw i32 %2, 3
  %8 = bitcast [1024 x i16]* %4 to i8*
  call void @llvm.lifetime.start.p0i8(i64 2048, i8* nonnull %8) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %8, i8 -86, i64 2048, i1 false)
  %9 = bitcast [32 x <2 x i64>]* %5 to i8*
  %10 = mul nsw i32 %2, 31
  %11 = sext i32 %10 to i64
  %12 = sext i32 %2 to i64
  %13 = sext i32 %6 to i64
  %14 = sext i32 %7 to i64
  %15 = sub nsw i64 0, %14
  %16 = sub nsw i64 0, %13
  %17 = sub nsw i64 0, %12
  %18 = shl nsw i32 %2, 2
  %19 = sext i32 %18 to i64
  %20 = mul nsw i32 %2, 27
  %21 = sext i32 %20 to i64
  %22 = shl nsw i32 %2, 3
  %23 = sext i32 %22 to i64
  %24 = mul nsw i32 %2, 23
  %25 = sext i32 %24 to i64
  %26 = mul nsw i32 %2, 12
  %27 = sext i32 %26 to i64
  %28 = mul nsw i32 %2, 19
  %29 = sext i32 %28 to i64
  %30 = bitcast [32 x <2 x i64>]* %5 to <8 x i16>*
  %31 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 16
  %32 = bitcast <2 x i64>* %31 to <8 x i16>*
  %33 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 8
  %34 = bitcast <2 x i64>* %33 to <8 x i16>*
  %35 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 24
  %36 = bitcast <2 x i64>* %35 to <8 x i16>*
  %37 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 4
  %38 = bitcast <2 x i64>* %37 to <8 x i16>*
  %39 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 20
  %40 = bitcast <2 x i64>* %39 to <8 x i16>*
  %41 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 12
  %42 = bitcast <2 x i64>* %41 to <8 x i16>*
  %43 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 28
  %44 = bitcast <2 x i64>* %43 to <8 x i16>*
  %45 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 2
  %46 = bitcast <2 x i64>* %45 to <8 x i16>*
  %47 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 18
  %48 = bitcast <2 x i64>* %47 to <8 x i16>*
  %49 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 10
  %50 = bitcast <2 x i64>* %49 to <8 x i16>*
  %51 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 26
  %52 = bitcast <2 x i64>* %51 to <8 x i16>*
  %53 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 6
  %54 = bitcast <2 x i64>* %53 to <8 x i16>*
  %55 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 22
  %56 = bitcast <2 x i64>* %55 to <8 x i16>*
  %57 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 14
  %58 = bitcast <2 x i64>* %57 to <8 x i16>*
  %59 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 30
  %60 = bitcast <2 x i64>* %59 to <8 x i16>*
  %61 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 1
  %62 = bitcast <2 x i64>* %61 to <8 x i16>*
  %63 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 17
  %64 = bitcast <2 x i64>* %63 to <8 x i16>*
  %65 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 9
  %66 = bitcast <2 x i64>* %65 to <8 x i16>*
  %67 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 25
  %68 = bitcast <2 x i64>* %67 to <8 x i16>*
  %69 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 7
  %70 = bitcast <2 x i64>* %69 to <8 x i16>*
  %71 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 23
  %72 = bitcast <2 x i64>* %71 to <8 x i16>*
  %73 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 15
  %74 = bitcast <2 x i64>* %73 to <8 x i16>*
  %75 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 31
  %76 = bitcast <2 x i64>* %75 to <8 x i16>*
  %77 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 5
  %78 = bitcast <2 x i64>* %77 to <8 x i16>*
  %79 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 21
  %80 = bitcast <2 x i64>* %79 to <8 x i16>*
  %81 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 13
  %82 = bitcast <2 x i64>* %81 to <8 x i16>*
  %83 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 29
  %84 = bitcast <2 x i64>* %83 to <8 x i16>*
  %85 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 3
  %86 = bitcast <2 x i64>* %85 to <8 x i16>*
  %87 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 19
  %88 = bitcast <2 x i64>* %87 to <8 x i16>*
  %89 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 11
  %90 = bitcast <2 x i64>* %89 to <8 x i16>*
  %91 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 27
  %92 = bitcast <2 x i64>* %91 to <8 x i16>*
  br label %93

93:                                               ; preds = %1417, %3
  %94 = phi i32 [ 0, %3 ], [ %1418, %1417 ]
  %95 = icmp eq i32 %94, 0
  %96 = icmp eq i32 %94, 1
  br label %97

97:                                               ; preds = %93, %1414
  %98 = phi i64 [ 0, %93 ], [ %1415, %1414 ]
  call void @llvm.lifetime.start.p0i8(i64 512, i8* nonnull %9) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %9, i8 -86, i64 512, i1 false)
  br i1 %95, label %99, label %260

99:                                               ; preds = %97
  %100 = getelementptr inbounds i16, i16* %0, i64 %98
  %101 = getelementptr inbounds i16, i16* %100, i64 %11
  %102 = bitcast i16* %100 to <8 x i16>*
  %103 = load <8 x i16>, <8 x i16>* %102, align 1
  %104 = getelementptr inbounds i16, i16* %100, i64 %12
  %105 = bitcast i16* %104 to <8 x i16>*
  %106 = load <8 x i16>, <8 x i16>* %105, align 1
  %107 = getelementptr inbounds i16, i16* %100, i64 %13
  %108 = bitcast i16* %107 to <8 x i16>*
  %109 = load <8 x i16>, <8 x i16>* %108, align 1
  %110 = getelementptr inbounds i16, i16* %100, i64 %14
  %111 = bitcast i16* %110 to <8 x i16>*
  %112 = load <8 x i16>, <8 x i16>* %111, align 1
  %113 = getelementptr inbounds i16, i16* %101, i64 %15
  %114 = bitcast i16* %113 to <8 x i16>*
  %115 = load <8 x i16>, <8 x i16>* %114, align 1
  %116 = getelementptr inbounds i16, i16* %101, i64 %16
  %117 = bitcast i16* %116 to <8 x i16>*
  %118 = load <8 x i16>, <8 x i16>* %117, align 1
  %119 = getelementptr inbounds i16, i16* %101, i64 %17
  %120 = bitcast i16* %119 to <8 x i16>*
  %121 = load <8 x i16>, <8 x i16>* %120, align 1
  %122 = bitcast i16* %101 to <8 x i16>*
  %123 = load <8 x i16>, <8 x i16>* %122, align 1
  %124 = add <8 x i16> %123, %103
  %125 = add <8 x i16> %121, %106
  %126 = add <8 x i16> %118, %109
  %127 = add <8 x i16> %115, %112
  %128 = sub <8 x i16> %112, %115
  %129 = sub <8 x i16> %109, %118
  %130 = sub <8 x i16> %106, %121
  %131 = sub <8 x i16> %103, %123
  %132 = shl <8 x i16> %124, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %133 = shl <8 x i16> %125, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %134 = shl <8 x i16> %126, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %135 = shl <8 x i16> %127, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %136 = shl <8 x i16> %128, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %137 = shl <8 x i16> %129, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %138 = shl <8 x i16> %130, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %139 = shl <8 x i16> %131, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %140 = getelementptr inbounds i16, i16* %100, i64 %19
  %141 = getelementptr inbounds i16, i16* %100, i64 %21
  %142 = bitcast i16* %140 to <8 x i16>*
  %143 = load <8 x i16>, <8 x i16>* %142, align 1
  %144 = getelementptr inbounds i16, i16* %140, i64 %12
  %145 = bitcast i16* %144 to <8 x i16>*
  %146 = load <8 x i16>, <8 x i16>* %145, align 1
  %147 = getelementptr inbounds i16, i16* %140, i64 %13
  %148 = bitcast i16* %147 to <8 x i16>*
  %149 = load <8 x i16>, <8 x i16>* %148, align 1
  %150 = getelementptr inbounds i16, i16* %140, i64 %14
  %151 = bitcast i16* %150 to <8 x i16>*
  %152 = load <8 x i16>, <8 x i16>* %151, align 1
  %153 = getelementptr inbounds i16, i16* %141, i64 %15
  %154 = bitcast i16* %153 to <8 x i16>*
  %155 = load <8 x i16>, <8 x i16>* %154, align 1
  %156 = getelementptr inbounds i16, i16* %141, i64 %16
  %157 = bitcast i16* %156 to <8 x i16>*
  %158 = load <8 x i16>, <8 x i16>* %157, align 1
  %159 = getelementptr inbounds i16, i16* %141, i64 %17
  %160 = bitcast i16* %159 to <8 x i16>*
  %161 = load <8 x i16>, <8 x i16>* %160, align 1
  %162 = bitcast i16* %141 to <8 x i16>*
  %163 = load <8 x i16>, <8 x i16>* %162, align 1
  %164 = add <8 x i16> %163, %143
  %165 = add <8 x i16> %161, %146
  %166 = add <8 x i16> %158, %149
  %167 = add <8 x i16> %155, %152
  %168 = sub <8 x i16> %152, %155
  %169 = sub <8 x i16> %149, %158
  %170 = sub <8 x i16> %146, %161
  %171 = sub <8 x i16> %143, %163
  %172 = shl <8 x i16> %164, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %173 = shl <8 x i16> %165, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %174 = shl <8 x i16> %166, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %175 = shl <8 x i16> %167, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %176 = shl <8 x i16> %168, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %177 = shl <8 x i16> %169, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %178 = shl <8 x i16> %170, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %179 = shl <8 x i16> %171, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %180 = getelementptr inbounds i16, i16* %100, i64 %23
  %181 = getelementptr inbounds i16, i16* %100, i64 %25
  %182 = bitcast i16* %180 to <8 x i16>*
  %183 = load <8 x i16>, <8 x i16>* %182, align 1
  %184 = getelementptr inbounds i16, i16* %180, i64 %12
  %185 = bitcast i16* %184 to <8 x i16>*
  %186 = load <8 x i16>, <8 x i16>* %185, align 1
  %187 = getelementptr inbounds i16, i16* %180, i64 %13
  %188 = bitcast i16* %187 to <8 x i16>*
  %189 = load <8 x i16>, <8 x i16>* %188, align 1
  %190 = getelementptr inbounds i16, i16* %180, i64 %14
  %191 = bitcast i16* %190 to <8 x i16>*
  %192 = load <8 x i16>, <8 x i16>* %191, align 1
  %193 = getelementptr inbounds i16, i16* %181, i64 %15
  %194 = bitcast i16* %193 to <8 x i16>*
  %195 = load <8 x i16>, <8 x i16>* %194, align 1
  %196 = getelementptr inbounds i16, i16* %181, i64 %16
  %197 = bitcast i16* %196 to <8 x i16>*
  %198 = load <8 x i16>, <8 x i16>* %197, align 1
  %199 = getelementptr inbounds i16, i16* %181, i64 %17
  %200 = bitcast i16* %199 to <8 x i16>*
  %201 = load <8 x i16>, <8 x i16>* %200, align 1
  %202 = bitcast i16* %181 to <8 x i16>*
  %203 = load <8 x i16>, <8 x i16>* %202, align 1
  %204 = add <8 x i16> %203, %183
  %205 = add <8 x i16> %201, %186
  %206 = add <8 x i16> %198, %189
  %207 = add <8 x i16> %195, %192
  %208 = sub <8 x i16> %192, %195
  %209 = sub <8 x i16> %189, %198
  %210 = sub <8 x i16> %186, %201
  %211 = sub <8 x i16> %183, %203
  %212 = shl <8 x i16> %204, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %213 = shl <8 x i16> %205, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %214 = shl <8 x i16> %206, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %215 = shl <8 x i16> %207, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %216 = shl <8 x i16> %208, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %217 = shl <8 x i16> %209, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %218 = shl <8 x i16> %210, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %219 = shl <8 x i16> %211, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %220 = getelementptr inbounds i16, i16* %100, i64 %27
  %221 = getelementptr inbounds i16, i16* %100, i64 %29
  %222 = bitcast i16* %220 to <8 x i16>*
  %223 = load <8 x i16>, <8 x i16>* %222, align 1
  %224 = getelementptr inbounds i16, i16* %220, i64 %12
  %225 = bitcast i16* %224 to <8 x i16>*
  %226 = load <8 x i16>, <8 x i16>* %225, align 1
  %227 = getelementptr inbounds i16, i16* %220, i64 %13
  %228 = bitcast i16* %227 to <8 x i16>*
  %229 = load <8 x i16>, <8 x i16>* %228, align 1
  %230 = getelementptr inbounds i16, i16* %220, i64 %14
  %231 = bitcast i16* %230 to <8 x i16>*
  %232 = load <8 x i16>, <8 x i16>* %231, align 1
  %233 = getelementptr inbounds i16, i16* %221, i64 %15
  %234 = bitcast i16* %233 to <8 x i16>*
  %235 = load <8 x i16>, <8 x i16>* %234, align 1
  %236 = getelementptr inbounds i16, i16* %221, i64 %16
  %237 = bitcast i16* %236 to <8 x i16>*
  %238 = load <8 x i16>, <8 x i16>* %237, align 1
  %239 = getelementptr inbounds i16, i16* %221, i64 %17
  %240 = bitcast i16* %239 to <8 x i16>*
  %241 = load <8 x i16>, <8 x i16>* %240, align 1
  %242 = bitcast i16* %221 to <8 x i16>*
  %243 = load <8 x i16>, <8 x i16>* %242, align 1
  %244 = add <8 x i16> %243, %223
  %245 = add <8 x i16> %241, %226
  %246 = add <8 x i16> %238, %229
  %247 = add <8 x i16> %235, %232
  %248 = sub <8 x i16> %232, %235
  %249 = sub <8 x i16> %229, %238
  %250 = sub <8 x i16> %226, %241
  %251 = sub <8 x i16> %223, %243
  %252 = shl <8 x i16> %244, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %253 = shl <8 x i16> %245, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %254 = shl <8 x i16> %246, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %255 = shl <8 x i16> %247, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %256 = shl <8 x i16> %248, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %257 = shl <8 x i16> %249, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %258 = shl <8 x i16> %250, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %259 = shl <8 x i16> %251, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  br label %389

260:                                              ; preds = %97
  %261 = getelementptr inbounds [1024 x i16], [1024 x i16]* %4, i64 0, i64 %98
  %262 = bitcast i16* %261 to <8 x i16>*
  %263 = load <8 x i16>, <8 x i16>* %262, align 16
  %264 = getelementptr inbounds i16, i16* %261, i64 32
  %265 = bitcast i16* %264 to <8 x i16>*
  %266 = load <8 x i16>, <8 x i16>* %265, align 16
  %267 = getelementptr inbounds i16, i16* %261, i64 64
  %268 = bitcast i16* %267 to <8 x i16>*
  %269 = load <8 x i16>, <8 x i16>* %268, align 16
  %270 = getelementptr inbounds i16, i16* %261, i64 96
  %271 = bitcast i16* %270 to <8 x i16>*
  %272 = load <8 x i16>, <8 x i16>* %271, align 16
  %273 = getelementptr inbounds i16, i16* %261, i64 896
  %274 = bitcast i16* %273 to <8 x i16>*
  %275 = load <8 x i16>, <8 x i16>* %274, align 16
  %276 = getelementptr inbounds i16, i16* %261, i64 928
  %277 = bitcast i16* %276 to <8 x i16>*
  %278 = load <8 x i16>, <8 x i16>* %277, align 16
  %279 = getelementptr inbounds i16, i16* %261, i64 960
  %280 = bitcast i16* %279 to <8 x i16>*
  %281 = load <8 x i16>, <8 x i16>* %280, align 16
  %282 = getelementptr inbounds i16, i16* %261, i64 992
  %283 = bitcast i16* %282 to <8 x i16>*
  %284 = load <8 x i16>, <8 x i16>* %283, align 16
  %285 = add <8 x i16> %284, %263
  %286 = add <8 x i16> %281, %266
  %287 = add <8 x i16> %278, %269
  %288 = add <8 x i16> %275, %272
  %289 = sub <8 x i16> %272, %275
  %290 = sub <8 x i16> %269, %278
  %291 = sub <8 x i16> %266, %281
  %292 = sub <8 x i16> %263, %284
  %293 = getelementptr inbounds i16, i16* %261, i64 128
  %294 = bitcast i16* %293 to <8 x i16>*
  %295 = load <8 x i16>, <8 x i16>* %294, align 16
  %296 = getelementptr inbounds i16, i16* %261, i64 160
  %297 = bitcast i16* %296 to <8 x i16>*
  %298 = load <8 x i16>, <8 x i16>* %297, align 16
  %299 = getelementptr inbounds i16, i16* %261, i64 192
  %300 = bitcast i16* %299 to <8 x i16>*
  %301 = load <8 x i16>, <8 x i16>* %300, align 16
  %302 = getelementptr inbounds i16, i16* %261, i64 224
  %303 = bitcast i16* %302 to <8 x i16>*
  %304 = load <8 x i16>, <8 x i16>* %303, align 16
  %305 = getelementptr inbounds i16, i16* %261, i64 768
  %306 = bitcast i16* %305 to <8 x i16>*
  %307 = load <8 x i16>, <8 x i16>* %306, align 16
  %308 = getelementptr inbounds i16, i16* %261, i64 800
  %309 = bitcast i16* %308 to <8 x i16>*
  %310 = load <8 x i16>, <8 x i16>* %309, align 16
  %311 = getelementptr inbounds i16, i16* %261, i64 832
  %312 = bitcast i16* %311 to <8 x i16>*
  %313 = load <8 x i16>, <8 x i16>* %312, align 16
  %314 = getelementptr inbounds i16, i16* %261, i64 864
  %315 = bitcast i16* %314 to <8 x i16>*
  %316 = load <8 x i16>, <8 x i16>* %315, align 16
  %317 = add <8 x i16> %316, %295
  %318 = add <8 x i16> %313, %298
  %319 = add <8 x i16> %310, %301
  %320 = add <8 x i16> %307, %304
  %321 = sub <8 x i16> %304, %307
  %322 = sub <8 x i16> %301, %310
  %323 = sub <8 x i16> %298, %313
  %324 = sub <8 x i16> %295, %316
  %325 = getelementptr inbounds i16, i16* %261, i64 256
  %326 = bitcast i16* %325 to <8 x i16>*
  %327 = load <8 x i16>, <8 x i16>* %326, align 16
  %328 = getelementptr inbounds i16, i16* %261, i64 288
  %329 = bitcast i16* %328 to <8 x i16>*
  %330 = load <8 x i16>, <8 x i16>* %329, align 16
  %331 = getelementptr inbounds i16, i16* %261, i64 320
  %332 = bitcast i16* %331 to <8 x i16>*
  %333 = load <8 x i16>, <8 x i16>* %332, align 16
  %334 = getelementptr inbounds i16, i16* %261, i64 352
  %335 = bitcast i16* %334 to <8 x i16>*
  %336 = load <8 x i16>, <8 x i16>* %335, align 16
  %337 = getelementptr inbounds i16, i16* %261, i64 640
  %338 = bitcast i16* %337 to <8 x i16>*
  %339 = load <8 x i16>, <8 x i16>* %338, align 16
  %340 = getelementptr inbounds i16, i16* %261, i64 672
  %341 = bitcast i16* %340 to <8 x i16>*
  %342 = load <8 x i16>, <8 x i16>* %341, align 16
  %343 = getelementptr inbounds i16, i16* %261, i64 704
  %344 = bitcast i16* %343 to <8 x i16>*
  %345 = load <8 x i16>, <8 x i16>* %344, align 16
  %346 = getelementptr inbounds i16, i16* %261, i64 736
  %347 = bitcast i16* %346 to <8 x i16>*
  %348 = load <8 x i16>, <8 x i16>* %347, align 16
  %349 = add <8 x i16> %348, %327
  %350 = add <8 x i16> %345, %330
  %351 = add <8 x i16> %342, %333
  %352 = add <8 x i16> %339, %336
  %353 = sub <8 x i16> %336, %339
  %354 = sub <8 x i16> %333, %342
  %355 = sub <8 x i16> %330, %345
  %356 = sub <8 x i16> %327, %348
  %357 = getelementptr inbounds i16, i16* %261, i64 384
  %358 = bitcast i16* %357 to <8 x i16>*
  %359 = load <8 x i16>, <8 x i16>* %358, align 16
  %360 = getelementptr inbounds i16, i16* %261, i64 416
  %361 = bitcast i16* %360 to <8 x i16>*
  %362 = load <8 x i16>, <8 x i16>* %361, align 16
  %363 = getelementptr inbounds i16, i16* %261, i64 448
  %364 = bitcast i16* %363 to <8 x i16>*
  %365 = load <8 x i16>, <8 x i16>* %364, align 16
  %366 = getelementptr inbounds i16, i16* %261, i64 480
  %367 = bitcast i16* %366 to <8 x i16>*
  %368 = load <8 x i16>, <8 x i16>* %367, align 16
  %369 = getelementptr inbounds i16, i16* %261, i64 512
  %370 = bitcast i16* %369 to <8 x i16>*
  %371 = load <8 x i16>, <8 x i16>* %370, align 16
  %372 = getelementptr inbounds i16, i16* %261, i64 544
  %373 = bitcast i16* %372 to <8 x i16>*
  %374 = load <8 x i16>, <8 x i16>* %373, align 16
  %375 = getelementptr inbounds i16, i16* %261, i64 576
  %376 = bitcast i16* %375 to <8 x i16>*
  %377 = load <8 x i16>, <8 x i16>* %376, align 16
  %378 = getelementptr inbounds i16, i16* %261, i64 608
  %379 = bitcast i16* %378 to <8 x i16>*
  %380 = load <8 x i16>, <8 x i16>* %379, align 16
  %381 = add <8 x i16> %380, %359
  %382 = add <8 x i16> %377, %362
  %383 = add <8 x i16> %374, %365
  %384 = add <8 x i16> %371, %368
  %385 = sub <8 x i16> %368, %371
  %386 = sub <8 x i16> %365, %374
  %387 = sub <8 x i16> %362, %377
  %388 = sub <8 x i16> %359, %380
  br label %389

389:                                              ; preds = %260, %99
  %390 = phi <8 x i16> [ %139, %99 ], [ %292, %260 ]
  %391 = phi <8 x i16> [ %138, %99 ], [ %291, %260 ]
  %392 = phi <8 x i16> [ %137, %99 ], [ %290, %260 ]
  %393 = phi <8 x i16> [ %136, %99 ], [ %289, %260 ]
  %394 = phi <8 x i16> [ %179, %99 ], [ %324, %260 ]
  %395 = phi <8 x i16> [ %178, %99 ], [ %323, %260 ]
  %396 = phi <8 x i16> [ %177, %99 ], [ %322, %260 ]
  %397 = phi <8 x i16> [ %176, %99 ], [ %321, %260 ]
  %398 = phi <8 x i16> [ %219, %99 ], [ %356, %260 ]
  %399 = phi <8 x i16> [ %218, %99 ], [ %355, %260 ]
  %400 = phi <8 x i16> [ %217, %99 ], [ %354, %260 ]
  %401 = phi <8 x i16> [ %216, %99 ], [ %353, %260 ]
  %402 = phi <8 x i16> [ %259, %99 ], [ %388, %260 ]
  %403 = phi <8 x i16> [ %258, %99 ], [ %387, %260 ]
  %404 = phi <8 x i16> [ %257, %99 ], [ %386, %260 ]
  %405 = phi <8 x i16> [ %256, %99 ], [ %385, %260 ]
  %406 = phi <8 x i16> [ %255, %99 ], [ %384, %260 ]
  %407 = phi <8 x i16> [ %254, %99 ], [ %383, %260 ]
  %408 = phi <8 x i16> [ %253, %99 ], [ %382, %260 ]
  %409 = phi <8 x i16> [ %252, %99 ], [ %381, %260 ]
  %410 = phi <8 x i16> [ %215, %99 ], [ %352, %260 ]
  %411 = phi <8 x i16> [ %214, %99 ], [ %351, %260 ]
  %412 = phi <8 x i16> [ %213, %99 ], [ %350, %260 ]
  %413 = phi <8 x i16> [ %212, %99 ], [ %349, %260 ]
  %414 = phi <8 x i16> [ %175, %99 ], [ %320, %260 ]
  %415 = phi <8 x i16> [ %174, %99 ], [ %319, %260 ]
  %416 = phi <8 x i16> [ %173, %99 ], [ %318, %260 ]
  %417 = phi <8 x i16> [ %172, %99 ], [ %317, %260 ]
  %418 = phi <8 x i16> [ %135, %99 ], [ %288, %260 ]
  %419 = phi <8 x i16> [ %134, %99 ], [ %287, %260 ]
  %420 = phi <8 x i16> [ %133, %99 ], [ %286, %260 ]
  %421 = phi <8 x i16> [ %132, %99 ], [ %285, %260 ]
  %422 = add <8 x i16> %421, %406
  %423 = add <8 x i16> %420, %407
  %424 = add <8 x i16> %419, %408
  %425 = add <8 x i16> %418, %409
  %426 = add <8 x i16> %417, %410
  %427 = add <8 x i16> %416, %411
  %428 = add <8 x i16> %415, %412
  %429 = add <8 x i16> %414, %413
  %430 = sub <8 x i16> %414, %413
  %431 = sub <8 x i16> %415, %412
  %432 = sub <8 x i16> %416, %411
  %433 = sub <8 x i16> %417, %410
  %434 = sub <8 x i16> %418, %409
  %435 = sub <8 x i16> %419, %408
  %436 = sub <8 x i16> %420, %407
  %437 = sub <8 x i16> %421, %406
  %438 = shufflevector <8 x i16> %394, <8 x i16> %401, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %439 = shufflevector <8 x i16> %394, <8 x i16> %401, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %440 = shufflevector <8 x i16> %395, <8 x i16> %400, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %441 = shufflevector <8 x i16> %395, <8 x i16> %400, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %442 = shufflevector <8 x i16> %396, <8 x i16> %399, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %443 = shufflevector <8 x i16> %396, <8 x i16> %399, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %444 = shufflevector <8 x i16> %397, <8 x i16> %398, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %445 = shufflevector <8 x i16> %397, <8 x i16> %398, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %446 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %438, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %447 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %439, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %448 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %440, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %449 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %441, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %450 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %442, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %451 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %443, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %452 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %444, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %453 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %445, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %454 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %444, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %455 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %445, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %456 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %442, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %457 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %443, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %458 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %440, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %459 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %441, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %460 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %438, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %461 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %439, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %462 = add <4 x i32> %446, <i32 8192, i32 8192, i32 8192, i32 8192>
  %463 = add <4 x i32> %447, <i32 8192, i32 8192, i32 8192, i32 8192>
  %464 = add <4 x i32> %448, <i32 8192, i32 8192, i32 8192, i32 8192>
  %465 = add <4 x i32> %449, <i32 8192, i32 8192, i32 8192, i32 8192>
  %466 = add <4 x i32> %450, <i32 8192, i32 8192, i32 8192, i32 8192>
  %467 = add <4 x i32> %451, <i32 8192, i32 8192, i32 8192, i32 8192>
  %468 = add <4 x i32> %452, <i32 8192, i32 8192, i32 8192, i32 8192>
  %469 = add <4 x i32> %453, <i32 8192, i32 8192, i32 8192, i32 8192>
  %470 = add <4 x i32> %454, <i32 8192, i32 8192, i32 8192, i32 8192>
  %471 = add <4 x i32> %455, <i32 8192, i32 8192, i32 8192, i32 8192>
  %472 = add <4 x i32> %456, <i32 8192, i32 8192, i32 8192, i32 8192>
  %473 = add <4 x i32> %457, <i32 8192, i32 8192, i32 8192, i32 8192>
  %474 = add <4 x i32> %458, <i32 8192, i32 8192, i32 8192, i32 8192>
  %475 = add <4 x i32> %459, <i32 8192, i32 8192, i32 8192, i32 8192>
  %476 = add <4 x i32> %460, <i32 8192, i32 8192, i32 8192, i32 8192>
  %477 = add <4 x i32> %461, <i32 8192, i32 8192, i32 8192, i32 8192>
  %478 = ashr <4 x i32> %462, <i32 14, i32 14, i32 14, i32 14>
  %479 = ashr <4 x i32> %463, <i32 14, i32 14, i32 14, i32 14>
  %480 = ashr <4 x i32> %464, <i32 14, i32 14, i32 14, i32 14>
  %481 = ashr <4 x i32> %465, <i32 14, i32 14, i32 14, i32 14>
  %482 = ashr <4 x i32> %466, <i32 14, i32 14, i32 14, i32 14>
  %483 = ashr <4 x i32> %467, <i32 14, i32 14, i32 14, i32 14>
  %484 = ashr <4 x i32> %468, <i32 14, i32 14, i32 14, i32 14>
  %485 = ashr <4 x i32> %469, <i32 14, i32 14, i32 14, i32 14>
  %486 = ashr <4 x i32> %470, <i32 14, i32 14, i32 14, i32 14>
  %487 = ashr <4 x i32> %471, <i32 14, i32 14, i32 14, i32 14>
  %488 = ashr <4 x i32> %472, <i32 14, i32 14, i32 14, i32 14>
  %489 = ashr <4 x i32> %473, <i32 14, i32 14, i32 14, i32 14>
  %490 = ashr <4 x i32> %474, <i32 14, i32 14, i32 14, i32 14>
  %491 = ashr <4 x i32> %475, <i32 14, i32 14, i32 14, i32 14>
  %492 = ashr <4 x i32> %476, <i32 14, i32 14, i32 14, i32 14>
  %493 = ashr <4 x i32> %477, <i32 14, i32 14, i32 14, i32 14>
  %494 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %478, <4 x i32> %479) #6
  %495 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %480, <4 x i32> %481) #6
  %496 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %482, <4 x i32> %483) #6
  %497 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %484, <4 x i32> %485) #6
  %498 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %486, <4 x i32> %487) #6
  %499 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %488, <4 x i32> %489) #6
  %500 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %490, <4 x i32> %491) #6
  %501 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %492, <4 x i32> %493) #6
  br i1 %96, label %502, label %631

502:                                              ; preds = %389
  %503 = lshr <8 x i16> %422, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %504 = lshr <8 x i16> %423, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %505 = lshr <8 x i16> %424, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %506 = lshr <8 x i16> %425, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %507 = lshr <8 x i16> %426, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %508 = lshr <8 x i16> %427, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %509 = lshr <8 x i16> %428, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %510 = lshr <8 x i16> %429, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %511 = lshr <8 x i16> %430, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %512 = lshr <8 x i16> %431, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %513 = lshr <8 x i16> %432, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %514 = lshr <8 x i16> %433, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %515 = lshr <8 x i16> %434, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %516 = lshr <8 x i16> %435, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %517 = lshr <8 x i16> %436, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %518 = lshr <8 x i16> %437, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %519 = lshr <8 x i16> %405, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %520 = lshr <8 x i16> %404, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %521 = lshr <8 x i16> %403, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %522 = lshr <8 x i16> %402, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %523 = lshr <8 x i16> %494, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %524 = lshr <8 x i16> %495, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %525 = lshr <8 x i16> %496, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %526 = lshr <8 x i16> %497, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %527 = lshr <8 x i16> %498, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %528 = lshr <8 x i16> %499, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %529 = lshr <8 x i16> %500, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %530 = lshr <8 x i16> %501, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %531 = lshr <8 x i16> %393, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %532 = lshr <8 x i16> %392, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %533 = lshr <8 x i16> %391, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %534 = lshr <8 x i16> %390, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %535 = add <8 x i16> %422, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %536 = add <8 x i16> %535, %503
  %537 = add <8 x i16> %423, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %538 = add <8 x i16> %537, %504
  %539 = add <8 x i16> %424, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %540 = add <8 x i16> %539, %505
  %541 = add <8 x i16> %425, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %542 = add <8 x i16> %541, %506
  %543 = add <8 x i16> %426, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %544 = add <8 x i16> %543, %507
  %545 = add <8 x i16> %427, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %546 = add <8 x i16> %545, %508
  %547 = add <8 x i16> %428, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %548 = add <8 x i16> %547, %509
  %549 = add <8 x i16> %429, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %550 = add <8 x i16> %549, %510
  %551 = add <8 x i16> %430, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %552 = add <8 x i16> %551, %511
  %553 = add <8 x i16> %431, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %554 = add <8 x i16> %553, %512
  %555 = add <8 x i16> %432, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %556 = add <8 x i16> %555, %513
  %557 = add <8 x i16> %433, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %558 = add <8 x i16> %557, %514
  %559 = add <8 x i16> %434, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %560 = add <8 x i16> %559, %515
  %561 = add <8 x i16> %435, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %562 = add <8 x i16> %561, %516
  %563 = add <8 x i16> %436, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %564 = add <8 x i16> %563, %517
  %565 = add <8 x i16> %437, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %566 = add <8 x i16> %565, %518
  %567 = add <8 x i16> %405, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %568 = add <8 x i16> %567, %519
  %569 = add <8 x i16> %404, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %570 = add <8 x i16> %569, %520
  %571 = add <8 x i16> %403, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %572 = add <8 x i16> %571, %521
  %573 = add <8 x i16> %402, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %574 = add <8 x i16> %573, %522
  %575 = add <8 x i16> %494, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %576 = add <8 x i16> %575, %523
  %577 = add <8 x i16> %495, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %578 = add <8 x i16> %577, %524
  %579 = add <8 x i16> %496, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %580 = add <8 x i16> %579, %525
  %581 = add <8 x i16> %497, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %582 = add <8 x i16> %581, %526
  %583 = add <8 x i16> %498, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %584 = add <8 x i16> %583, %527
  %585 = add <8 x i16> %499, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %586 = add <8 x i16> %585, %528
  %587 = add <8 x i16> %500, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %588 = add <8 x i16> %587, %529
  %589 = add <8 x i16> %501, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %590 = add <8 x i16> %589, %530
  %591 = add <8 x i16> %393, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %592 = add <8 x i16> %591, %531
  %593 = add <8 x i16> %392, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %594 = add <8 x i16> %593, %532
  %595 = add <8 x i16> %391, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %596 = add <8 x i16> %595, %533
  %597 = add <8 x i16> %390, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %598 = add <8 x i16> %597, %534
  %599 = ashr <8 x i16> %536, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %600 = ashr <8 x i16> %538, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %601 = ashr <8 x i16> %540, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %602 = ashr <8 x i16> %542, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %603 = ashr <8 x i16> %544, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %604 = ashr <8 x i16> %546, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %605 = ashr <8 x i16> %548, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %606 = ashr <8 x i16> %550, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %607 = ashr <8 x i16> %552, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %608 = ashr <8 x i16> %554, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %609 = ashr <8 x i16> %556, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %610 = ashr <8 x i16> %558, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %611 = ashr <8 x i16> %560, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %612 = ashr <8 x i16> %562, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %613 = ashr <8 x i16> %564, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %614 = ashr <8 x i16> %566, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %615 = ashr <8 x i16> %568, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %616 = ashr <8 x i16> %570, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %617 = ashr <8 x i16> %572, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %618 = ashr <8 x i16> %574, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %619 = ashr <8 x i16> %576, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %620 = ashr <8 x i16> %578, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %621 = ashr <8 x i16> %580, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %622 = ashr <8 x i16> %582, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %623 = ashr <8 x i16> %584, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %624 = ashr <8 x i16> %586, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %625 = ashr <8 x i16> %588, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %626 = ashr <8 x i16> %590, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %627 = ashr <8 x i16> %592, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %628 = ashr <8 x i16> %594, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %629 = ashr <8 x i16> %596, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %630 = ashr <8 x i16> %598, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  br label %631

631:                                              ; preds = %502, %389
  %632 = phi <8 x i16> [ %630, %502 ], [ %390, %389 ]
  %633 = phi <8 x i16> [ %629, %502 ], [ %391, %389 ]
  %634 = phi <8 x i16> [ %628, %502 ], [ %392, %389 ]
  %635 = phi <8 x i16> [ %627, %502 ], [ %393, %389 ]
  %636 = phi <8 x i16> [ %618, %502 ], [ %402, %389 ]
  %637 = phi <8 x i16> [ %617, %502 ], [ %403, %389 ]
  %638 = phi <8 x i16> [ %616, %502 ], [ %404, %389 ]
  %639 = phi <8 x i16> [ %615, %502 ], [ %405, %389 ]
  %640 = phi <8 x i16> [ %599, %502 ], [ %422, %389 ]
  %641 = phi <8 x i16> [ %600, %502 ], [ %423, %389 ]
  %642 = phi <8 x i16> [ %601, %502 ], [ %424, %389 ]
  %643 = phi <8 x i16> [ %602, %502 ], [ %425, %389 ]
  %644 = phi <8 x i16> [ %603, %502 ], [ %426, %389 ]
  %645 = phi <8 x i16> [ %604, %502 ], [ %427, %389 ]
  %646 = phi <8 x i16> [ %605, %502 ], [ %428, %389 ]
  %647 = phi <8 x i16> [ %606, %502 ], [ %429, %389 ]
  %648 = phi <8 x i16> [ %607, %502 ], [ %430, %389 ]
  %649 = phi <8 x i16> [ %608, %502 ], [ %431, %389 ]
  %650 = phi <8 x i16> [ %609, %502 ], [ %432, %389 ]
  %651 = phi <8 x i16> [ %610, %502 ], [ %433, %389 ]
  %652 = phi <8 x i16> [ %611, %502 ], [ %434, %389 ]
  %653 = phi <8 x i16> [ %612, %502 ], [ %435, %389 ]
  %654 = phi <8 x i16> [ %613, %502 ], [ %436, %389 ]
  %655 = phi <8 x i16> [ %614, %502 ], [ %437, %389 ]
  %656 = phi <8 x i16> [ %619, %502 ], [ %494, %389 ]
  %657 = phi <8 x i16> [ %620, %502 ], [ %495, %389 ]
  %658 = phi <8 x i16> [ %621, %502 ], [ %496, %389 ]
  %659 = phi <8 x i16> [ %622, %502 ], [ %497, %389 ]
  %660 = phi <8 x i16> [ %623, %502 ], [ %498, %389 ]
  %661 = phi <8 x i16> [ %624, %502 ], [ %499, %389 ]
  %662 = phi <8 x i16> [ %625, %502 ], [ %500, %389 ]
  %663 = phi <8 x i16> [ %626, %502 ], [ %501, %389 ]
  %664 = add <8 x i16> %647, %640
  %665 = add <8 x i16> %646, %641
  %666 = add <8 x i16> %645, %642
  %667 = add <8 x i16> %644, %643
  %668 = sub <8 x i16> %643, %644
  %669 = sub <8 x i16> %642, %645
  %670 = sub <8 x i16> %641, %646
  %671 = sub <8 x i16> %640, %647
  %672 = shufflevector <8 x i16> %653, <8 x i16> %650, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %673 = shufflevector <8 x i16> %653, <8 x i16> %650, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %674 = shufflevector <8 x i16> %652, <8 x i16> %651, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %675 = shufflevector <8 x i16> %652, <8 x i16> %651, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %676 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %672, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %677 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %673, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %678 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %674, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %679 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %675, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %680 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %674, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %681 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %675, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %682 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %672, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %683 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %673, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %684 = add <4 x i32> %676, <i32 8192, i32 8192, i32 8192, i32 8192>
  %685 = add <4 x i32> %677, <i32 8192, i32 8192, i32 8192, i32 8192>
  %686 = add <4 x i32> %678, <i32 8192, i32 8192, i32 8192, i32 8192>
  %687 = add <4 x i32> %679, <i32 8192, i32 8192, i32 8192, i32 8192>
  %688 = add <4 x i32> %680, <i32 8192, i32 8192, i32 8192, i32 8192>
  %689 = add <4 x i32> %681, <i32 8192, i32 8192, i32 8192, i32 8192>
  %690 = add <4 x i32> %682, <i32 8192, i32 8192, i32 8192, i32 8192>
  %691 = add <4 x i32> %683, <i32 8192, i32 8192, i32 8192, i32 8192>
  %692 = ashr <4 x i32> %684, <i32 14, i32 14, i32 14, i32 14>
  %693 = ashr <4 x i32> %685, <i32 14, i32 14, i32 14, i32 14>
  %694 = ashr <4 x i32> %686, <i32 14, i32 14, i32 14, i32 14>
  %695 = ashr <4 x i32> %687, <i32 14, i32 14, i32 14, i32 14>
  %696 = ashr <4 x i32> %688, <i32 14, i32 14, i32 14, i32 14>
  %697 = ashr <4 x i32> %689, <i32 14, i32 14, i32 14, i32 14>
  %698 = ashr <4 x i32> %690, <i32 14, i32 14, i32 14, i32 14>
  %699 = ashr <4 x i32> %691, <i32 14, i32 14, i32 14, i32 14>
  %700 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %692, <4 x i32> %693) #6
  %701 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %694, <4 x i32> %695) #6
  %702 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %696, <4 x i32> %697) #6
  %703 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %698, <4 x i32> %699) #6
  %704 = add <8 x i16> %659, %639
  %705 = add <8 x i16> %658, %638
  %706 = add <8 x i16> %657, %637
  %707 = add <8 x i16> %656, %636
  %708 = sub <8 x i16> %636, %656
  %709 = sub <8 x i16> %637, %657
  %710 = sub <8 x i16> %638, %658
  %711 = sub <8 x i16> %639, %659
  %712 = sub <8 x i16> %632, %660
  %713 = sub <8 x i16> %633, %661
  %714 = sub <8 x i16> %634, %662
  %715 = sub <8 x i16> %635, %663
  %716 = add <8 x i16> %663, %635
  %717 = add <8 x i16> %662, %634
  %718 = add <8 x i16> %661, %633
  %719 = add <8 x i16> %660, %632
  %720 = add <8 x i16> %664, %667
  %721 = add <8 x i16> %665, %666
  %722 = sub <8 x i16> %665, %666
  %723 = sub <8 x i16> %664, %667
  %724 = add <8 x i16> %701, %648
  %725 = add <8 x i16> %700, %649
  %726 = sub <8 x i16> %649, %700
  %727 = sub <8 x i16> %648, %701
  %728 = sub <8 x i16> %655, %702
  %729 = sub <8 x i16> %654, %703
  %730 = add <8 x i16> %703, %654
  %731 = add <8 x i16> %702, %655
  %732 = shufflevector <8 x i16> %670, <8 x i16> %669, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %733 = shufflevector <8 x i16> %670, <8 x i16> %669, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %734 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %732, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %735 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %733, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %736 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %732, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %737 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %733, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %738 = add <4 x i32> %734, <i32 8192, i32 8192, i32 8192, i32 8192>
  %739 = add <4 x i32> %735, <i32 8192, i32 8192, i32 8192, i32 8192>
  %740 = add <4 x i32> %736, <i32 8192, i32 8192, i32 8192, i32 8192>
  %741 = add <4 x i32> %737, <i32 8192, i32 8192, i32 8192, i32 8192>
  %742 = ashr <4 x i32> %738, <i32 14, i32 14, i32 14, i32 14>
  %743 = ashr <4 x i32> %739, <i32 14, i32 14, i32 14, i32 14>
  %744 = ashr <4 x i32> %740, <i32 14, i32 14, i32 14, i32 14>
  %745 = ashr <4 x i32> %741, <i32 14, i32 14, i32 14, i32 14>
  %746 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %742, <4 x i32> %743) #6
  %747 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %744, <4 x i32> %745) #6
  %748 = shufflevector <8 x i16> %706, <8 x i16> %717, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %749 = shufflevector <8 x i16> %706, <8 x i16> %717, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %750 = shufflevector <8 x i16> %707, <8 x i16> %716, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %751 = shufflevector <8 x i16> %707, <8 x i16> %716, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %752 = shufflevector <8 x i16> %708, <8 x i16> %715, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %753 = shufflevector <8 x i16> %708, <8 x i16> %715, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %754 = shufflevector <8 x i16> %709, <8 x i16> %714, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %755 = shufflevector <8 x i16> %709, <8 x i16> %714, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %756 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %748, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %757 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %749, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %758 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %750, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %759 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %751, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %760 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %752, <8 x i16> <i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137>) #6
  %761 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %753, <8 x i16> <i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137>) #6
  %762 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %754, <8 x i16> <i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137>) #6
  %763 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %755, <8 x i16> <i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137>) #6
  %764 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %754, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %765 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %755, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %766 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %752, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %767 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %753, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %768 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %750, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %769 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %751, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %770 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %748, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %771 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %749, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %772 = add <4 x i32> %756, <i32 8192, i32 8192, i32 8192, i32 8192>
  %773 = add <4 x i32> %757, <i32 8192, i32 8192, i32 8192, i32 8192>
  %774 = add <4 x i32> %758, <i32 8192, i32 8192, i32 8192, i32 8192>
  %775 = add <4 x i32> %759, <i32 8192, i32 8192, i32 8192, i32 8192>
  %776 = add <4 x i32> %760, <i32 8192, i32 8192, i32 8192, i32 8192>
  %777 = add <4 x i32> %761, <i32 8192, i32 8192, i32 8192, i32 8192>
  %778 = add <4 x i32> %762, <i32 8192, i32 8192, i32 8192, i32 8192>
  %779 = add <4 x i32> %763, <i32 8192, i32 8192, i32 8192, i32 8192>
  %780 = add <4 x i32> %764, <i32 8192, i32 8192, i32 8192, i32 8192>
  %781 = add <4 x i32> %765, <i32 8192, i32 8192, i32 8192, i32 8192>
  %782 = add <4 x i32> %766, <i32 8192, i32 8192, i32 8192, i32 8192>
  %783 = add <4 x i32> %767, <i32 8192, i32 8192, i32 8192, i32 8192>
  %784 = add <4 x i32> %768, <i32 8192, i32 8192, i32 8192, i32 8192>
  %785 = add <4 x i32> %769, <i32 8192, i32 8192, i32 8192, i32 8192>
  %786 = add <4 x i32> %770, <i32 8192, i32 8192, i32 8192, i32 8192>
  %787 = add <4 x i32> %771, <i32 8192, i32 8192, i32 8192, i32 8192>
  %788 = ashr <4 x i32> %772, <i32 14, i32 14, i32 14, i32 14>
  %789 = ashr <4 x i32> %773, <i32 14, i32 14, i32 14, i32 14>
  %790 = ashr <4 x i32> %774, <i32 14, i32 14, i32 14, i32 14>
  %791 = ashr <4 x i32> %775, <i32 14, i32 14, i32 14, i32 14>
  %792 = ashr <4 x i32> %776, <i32 14, i32 14, i32 14, i32 14>
  %793 = ashr <4 x i32> %777, <i32 14, i32 14, i32 14, i32 14>
  %794 = ashr <4 x i32> %778, <i32 14, i32 14, i32 14, i32 14>
  %795 = ashr <4 x i32> %779, <i32 14, i32 14, i32 14, i32 14>
  %796 = ashr <4 x i32> %780, <i32 14, i32 14, i32 14, i32 14>
  %797 = ashr <4 x i32> %781, <i32 14, i32 14, i32 14, i32 14>
  %798 = ashr <4 x i32> %782, <i32 14, i32 14, i32 14, i32 14>
  %799 = ashr <4 x i32> %783, <i32 14, i32 14, i32 14, i32 14>
  %800 = ashr <4 x i32> %784, <i32 14, i32 14, i32 14, i32 14>
  %801 = ashr <4 x i32> %785, <i32 14, i32 14, i32 14, i32 14>
  %802 = ashr <4 x i32> %786, <i32 14, i32 14, i32 14, i32 14>
  %803 = ashr <4 x i32> %787, <i32 14, i32 14, i32 14, i32 14>
  %804 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %788, <4 x i32> %789) #6
  %805 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %790, <4 x i32> %791) #6
  %806 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %792, <4 x i32> %793) #6
  %807 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %794, <4 x i32> %795) #6
  %808 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %796, <4 x i32> %797) #6
  %809 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %798, <4 x i32> %799) #6
  %810 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %800, <4 x i32> %801) #6
  %811 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %802, <4 x i32> %803) #6
  %812 = add <8 x i16> %746, %668
  %813 = sub <8 x i16> %668, %746
  %814 = sub <8 x i16> %671, %747
  %815 = add <8 x i16> %747, %671
  %816 = shufflevector <8 x i16> %720, <8 x i16> %721, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %817 = shufflevector <8 x i16> %720, <8 x i16> %721, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %818 = shufflevector <8 x i16> %722, <8 x i16> %723, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %819 = shufflevector <8 x i16> %722, <8 x i16> %723, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %820 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %816, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %821 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %817, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %822 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %816, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %823 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %817, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %824 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %818, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %825 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %819, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %826 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %818, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %827 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %819, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %828 = add <4 x i32> %820, <i32 8192, i32 8192, i32 8192, i32 8192>
  %829 = add <4 x i32> %821, <i32 8192, i32 8192, i32 8192, i32 8192>
  %830 = add <4 x i32> %822, <i32 8192, i32 8192, i32 8192, i32 8192>
  %831 = add <4 x i32> %823, <i32 8192, i32 8192, i32 8192, i32 8192>
  %832 = add <4 x i32> %824, <i32 8192, i32 8192, i32 8192, i32 8192>
  %833 = add <4 x i32> %825, <i32 8192, i32 8192, i32 8192, i32 8192>
  %834 = add <4 x i32> %826, <i32 8192, i32 8192, i32 8192, i32 8192>
  %835 = add <4 x i32> %827, <i32 8192, i32 8192, i32 8192, i32 8192>
  %836 = ashr <4 x i32> %828, <i32 14, i32 14, i32 14, i32 14>
  %837 = ashr <4 x i32> %829, <i32 14, i32 14, i32 14, i32 14>
  %838 = ashr <4 x i32> %830, <i32 14, i32 14, i32 14, i32 14>
  %839 = ashr <4 x i32> %831, <i32 14, i32 14, i32 14, i32 14>
  %840 = ashr <4 x i32> %832, <i32 14, i32 14, i32 14, i32 14>
  %841 = ashr <4 x i32> %833, <i32 14, i32 14, i32 14, i32 14>
  %842 = ashr <4 x i32> %834, <i32 14, i32 14, i32 14, i32 14>
  %843 = ashr <4 x i32> %835, <i32 14, i32 14, i32 14, i32 14>
  %844 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %836, <4 x i32> %837) #6
  store <8 x i16> %844, <8 x i16>* %30, align 16
  %845 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %838, <4 x i32> %839) #6
  store <8 x i16> %845, <8 x i16>* %32, align 16
  %846 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %840, <4 x i32> %841) #6
  store <8 x i16> %846, <8 x i16>* %34, align 16
  %847 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %842, <4 x i32> %843) #6
  store <8 x i16> %847, <8 x i16>* %36, align 16
  %848 = shufflevector <8 x i16> %725, <8 x i16> %730, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %849 = shufflevector <8 x i16> %725, <8 x i16> %730, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %850 = shufflevector <8 x i16> %726, <8 x i16> %729, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %851 = shufflevector <8 x i16> %726, <8 x i16> %729, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %852 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %848, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %853 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %849, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %854 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %850, <8 x i16> <i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137>) #6
  %855 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %851, <8 x i16> <i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137>) #6
  %856 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %850, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %857 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %851, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %858 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %848, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %859 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %849, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %860 = add <4 x i32> %852, <i32 8192, i32 8192, i32 8192, i32 8192>
  %861 = add <4 x i32> %853, <i32 8192, i32 8192, i32 8192, i32 8192>
  %862 = add <4 x i32> %854, <i32 8192, i32 8192, i32 8192, i32 8192>
  %863 = add <4 x i32> %855, <i32 8192, i32 8192, i32 8192, i32 8192>
  %864 = add <4 x i32> %856, <i32 8192, i32 8192, i32 8192, i32 8192>
  %865 = add <4 x i32> %857, <i32 8192, i32 8192, i32 8192, i32 8192>
  %866 = add <4 x i32> %858, <i32 8192, i32 8192, i32 8192, i32 8192>
  %867 = add <4 x i32> %859, <i32 8192, i32 8192, i32 8192, i32 8192>
  %868 = ashr <4 x i32> %860, <i32 14, i32 14, i32 14, i32 14>
  %869 = ashr <4 x i32> %861, <i32 14, i32 14, i32 14, i32 14>
  %870 = ashr <4 x i32> %862, <i32 14, i32 14, i32 14, i32 14>
  %871 = ashr <4 x i32> %863, <i32 14, i32 14, i32 14, i32 14>
  %872 = ashr <4 x i32> %864, <i32 14, i32 14, i32 14, i32 14>
  %873 = ashr <4 x i32> %865, <i32 14, i32 14, i32 14, i32 14>
  %874 = ashr <4 x i32> %866, <i32 14, i32 14, i32 14, i32 14>
  %875 = ashr <4 x i32> %867, <i32 14, i32 14, i32 14, i32 14>
  %876 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %868, <4 x i32> %869) #6
  %877 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %870, <4 x i32> %871) #6
  %878 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %872, <4 x i32> %873) #6
  %879 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %874, <4 x i32> %875) #6
  %880 = add <8 x i16> %805, %704
  %881 = add <8 x i16> %804, %705
  %882 = sub <8 x i16> %705, %804
  %883 = sub <8 x i16> %704, %805
  %884 = sub <8 x i16> %711, %806
  %885 = sub <8 x i16> %710, %807
  %886 = add <8 x i16> %807, %710
  %887 = add <8 x i16> %806, %711
  %888 = add <8 x i16> %809, %712
  %889 = add <8 x i16> %808, %713
  %890 = sub <8 x i16> %713, %808
  %891 = sub <8 x i16> %712, %809
  %892 = sub <8 x i16> %719, %810
  %893 = sub <8 x i16> %718, %811
  %894 = add <8 x i16> %811, %718
  %895 = add <8 x i16> %810, %719
  %896 = shufflevector <8 x i16> %812, <8 x i16> %815, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %897 = shufflevector <8 x i16> %812, <8 x i16> %815, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %898 = shufflevector <8 x i16> %813, <8 x i16> %814, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %899 = shufflevector <8 x i16> %813, <8 x i16> %814, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %900 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %896, <8 x i16> <i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069>) #6
  %901 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %897, <8 x i16> <i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069>) #6
  %902 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %898, <8 x i16> <i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102>) #6
  %903 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %899, <8 x i16> <i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102>) #6
  %904 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %898, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %905 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %899, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %906 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %896, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %907 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %897, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %908 = add <4 x i32> %900, <i32 8192, i32 8192, i32 8192, i32 8192>
  %909 = add <4 x i32> %901, <i32 8192, i32 8192, i32 8192, i32 8192>
  %910 = add <4 x i32> %902, <i32 8192, i32 8192, i32 8192, i32 8192>
  %911 = add <4 x i32> %903, <i32 8192, i32 8192, i32 8192, i32 8192>
  %912 = add <4 x i32> %904, <i32 8192, i32 8192, i32 8192, i32 8192>
  %913 = add <4 x i32> %905, <i32 8192, i32 8192, i32 8192, i32 8192>
  %914 = add <4 x i32> %906, <i32 8192, i32 8192, i32 8192, i32 8192>
  %915 = add <4 x i32> %907, <i32 8192, i32 8192, i32 8192, i32 8192>
  %916 = ashr <4 x i32> %908, <i32 14, i32 14, i32 14, i32 14>
  %917 = ashr <4 x i32> %909, <i32 14, i32 14, i32 14, i32 14>
  %918 = ashr <4 x i32> %910, <i32 14, i32 14, i32 14, i32 14>
  %919 = ashr <4 x i32> %911, <i32 14, i32 14, i32 14, i32 14>
  %920 = ashr <4 x i32> %912, <i32 14, i32 14, i32 14, i32 14>
  %921 = ashr <4 x i32> %913, <i32 14, i32 14, i32 14, i32 14>
  %922 = ashr <4 x i32> %914, <i32 14, i32 14, i32 14, i32 14>
  %923 = ashr <4 x i32> %915, <i32 14, i32 14, i32 14, i32 14>
  %924 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %916, <4 x i32> %917) #6
  store <8 x i16> %924, <8 x i16>* %38, align 16
  %925 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %918, <4 x i32> %919) #6
  store <8 x i16> %925, <8 x i16>* %40, align 16
  %926 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %920, <4 x i32> %921) #6
  store <8 x i16> %926, <8 x i16>* %42, align 16
  %927 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %922, <4 x i32> %923) #6
  store <8 x i16> %927, <8 x i16>* %44, align 16
  %928 = add <8 x i16> %876, %724
  %929 = sub <8 x i16> %724, %876
  %930 = sub <8 x i16> %727, %877
  %931 = add <8 x i16> %877, %727
  %932 = add <8 x i16> %878, %728
  %933 = sub <8 x i16> %728, %878
  %934 = sub <8 x i16> %731, %879
  %935 = add <8 x i16> %879, %731
  %936 = shufflevector <8 x i16> %881, <8 x i16> %894, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %937 = shufflevector <8 x i16> %881, <8 x i16> %894, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %938 = shufflevector <8 x i16> %882, <8 x i16> %893, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %939 = shufflevector <8 x i16> %882, <8 x i16> %893, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %940 = shufflevector <8 x i16> %885, <8 x i16> %890, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %941 = shufflevector <8 x i16> %885, <8 x i16> %890, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %942 = shufflevector <8 x i16> %886, <8 x i16> %889, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %943 = shufflevector <8 x i16> %886, <8 x i16> %889, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %944 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %936, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %945 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %937, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %946 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %938, <8 x i16> <i16 -3196, i16 -16069, i16 -3196, i16 -16069, i16 -3196, i16 -16069, i16 -3196, i16 -16069>) #6
  %947 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %939, <8 x i16> <i16 -3196, i16 -16069, i16 -3196, i16 -16069, i16 -3196, i16 -16069, i16 -3196, i16 -16069>) #6
  %948 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %940, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %949 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %941, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %950 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %942, <8 x i16> <i16 -13623, i16 -9102, i16 -13623, i16 -9102, i16 -13623, i16 -9102, i16 -13623, i16 -9102>) #6
  %951 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %943, <8 x i16> <i16 -13623, i16 -9102, i16 -13623, i16 -9102, i16 -13623, i16 -9102, i16 -13623, i16 -9102>) #6
  %952 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %942, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %953 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %943, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %954 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %940, <8 x i16> <i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102>) #6
  %955 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %941, <8 x i16> <i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102>) #6
  %956 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %938, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %957 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %939, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %958 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %936, <8 x i16> <i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069>) #6
  %959 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %937, <8 x i16> <i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069>) #6
  %960 = add <4 x i32> %944, <i32 8192, i32 8192, i32 8192, i32 8192>
  %961 = add <4 x i32> %945, <i32 8192, i32 8192, i32 8192, i32 8192>
  %962 = add <4 x i32> %946, <i32 8192, i32 8192, i32 8192, i32 8192>
  %963 = add <4 x i32> %947, <i32 8192, i32 8192, i32 8192, i32 8192>
  %964 = add <4 x i32> %948, <i32 8192, i32 8192, i32 8192, i32 8192>
  %965 = add <4 x i32> %949, <i32 8192, i32 8192, i32 8192, i32 8192>
  %966 = add <4 x i32> %950, <i32 8192, i32 8192, i32 8192, i32 8192>
  %967 = add <4 x i32> %951, <i32 8192, i32 8192, i32 8192, i32 8192>
  %968 = ashr <4 x i32> %960, <i32 14, i32 14, i32 14, i32 14>
  %969 = ashr <4 x i32> %961, <i32 14, i32 14, i32 14, i32 14>
  %970 = ashr <4 x i32> %962, <i32 14, i32 14, i32 14, i32 14>
  %971 = ashr <4 x i32> %963, <i32 14, i32 14, i32 14, i32 14>
  %972 = ashr <4 x i32> %964, <i32 14, i32 14, i32 14, i32 14>
  %973 = ashr <4 x i32> %965, <i32 14, i32 14, i32 14, i32 14>
  %974 = ashr <4 x i32> %966, <i32 14, i32 14, i32 14, i32 14>
  %975 = ashr <4 x i32> %967, <i32 14, i32 14, i32 14, i32 14>
  %976 = add <4 x i32> %952, <i32 8192, i32 8192, i32 8192, i32 8192>
  %977 = add <4 x i32> %953, <i32 8192, i32 8192, i32 8192, i32 8192>
  %978 = add <4 x i32> %954, <i32 8192, i32 8192, i32 8192, i32 8192>
  %979 = add <4 x i32> %955, <i32 8192, i32 8192, i32 8192, i32 8192>
  %980 = add <4 x i32> %956, <i32 8192, i32 8192, i32 8192, i32 8192>
  %981 = add <4 x i32> %957, <i32 8192, i32 8192, i32 8192, i32 8192>
  %982 = add <4 x i32> %958, <i32 8192, i32 8192, i32 8192, i32 8192>
  %983 = add <4 x i32> %959, <i32 8192, i32 8192, i32 8192, i32 8192>
  %984 = ashr <4 x i32> %976, <i32 14, i32 14, i32 14, i32 14>
  %985 = ashr <4 x i32> %977, <i32 14, i32 14, i32 14, i32 14>
  %986 = ashr <4 x i32> %978, <i32 14, i32 14, i32 14, i32 14>
  %987 = ashr <4 x i32> %979, <i32 14, i32 14, i32 14, i32 14>
  %988 = ashr <4 x i32> %980, <i32 14, i32 14, i32 14, i32 14>
  %989 = ashr <4 x i32> %981, <i32 14, i32 14, i32 14, i32 14>
  %990 = ashr <4 x i32> %982, <i32 14, i32 14, i32 14, i32 14>
  %991 = ashr <4 x i32> %983, <i32 14, i32 14, i32 14, i32 14>
  %992 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %968, <4 x i32> %969) #6
  %993 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %970, <4 x i32> %971) #6
  %994 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %972, <4 x i32> %973) #6
  %995 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %974, <4 x i32> %975) #6
  %996 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %984, <4 x i32> %985) #6
  %997 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %986, <4 x i32> %987) #6
  %998 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %988, <4 x i32> %989) #6
  %999 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %990, <4 x i32> %991) #6
  %1000 = shufflevector <8 x i16> %928, <8 x i16> %935, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1001 = shufflevector <8 x i16> %928, <8 x i16> %935, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1002 = shufflevector <8 x i16> %929, <8 x i16> %934, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1003 = shufflevector <8 x i16> %929, <8 x i16> %934, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1004 = shufflevector <8 x i16> %930, <8 x i16> %933, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1005 = shufflevector <8 x i16> %930, <8 x i16> %933, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1006 = shufflevector <8 x i16> %931, <8 x i16> %932, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1007 = shufflevector <8 x i16> %931, <8 x i16> %932, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1008 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1000, <8 x i16> <i16 1606, i16 16305, i16 1606, i16 16305, i16 1606, i16 16305, i16 1606, i16 16305>) #6
  %1009 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1001, <8 x i16> <i16 1606, i16 16305, i16 1606, i16 16305, i16 1606, i16 16305, i16 1606, i16 16305>) #6
  %1010 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1002, <8 x i16> <i16 12665, i16 10394, i16 12665, i16 10394, i16 12665, i16 10394, i16 12665, i16 10394>) #6
  %1011 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1003, <8 x i16> <i16 12665, i16 10394, i16 12665, i16 10394, i16 12665, i16 10394, i16 12665, i16 10394>) #6
  %1012 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1004, <8 x i16> <i16 7723, i16 14449, i16 7723, i16 14449, i16 7723, i16 14449, i16 7723, i16 14449>) #6
  %1013 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1005, <8 x i16> <i16 7723, i16 14449, i16 7723, i16 14449, i16 7723, i16 14449, i16 7723, i16 14449>) #6
  %1014 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1006, <8 x i16> <i16 15679, i16 4756, i16 15679, i16 4756, i16 15679, i16 4756, i16 15679, i16 4756>) #6
  %1015 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1007, <8 x i16> <i16 15679, i16 4756, i16 15679, i16 4756, i16 15679, i16 4756, i16 15679, i16 4756>) #6
  %1016 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1006, <8 x i16> <i16 -4756, i16 15679, i16 -4756, i16 15679, i16 -4756, i16 15679, i16 -4756, i16 15679>) #6
  %1017 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1007, <8 x i16> <i16 -4756, i16 15679, i16 -4756, i16 15679, i16 -4756, i16 15679, i16 -4756, i16 15679>) #6
  %1018 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1004, <8 x i16> <i16 -14449, i16 7723, i16 -14449, i16 7723, i16 -14449, i16 7723, i16 -14449, i16 7723>) #6
  %1019 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1005, <8 x i16> <i16 -14449, i16 7723, i16 -14449, i16 7723, i16 -14449, i16 7723, i16 -14449, i16 7723>) #6
  %1020 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1002, <8 x i16> <i16 -10394, i16 12665, i16 -10394, i16 12665, i16 -10394, i16 12665, i16 -10394, i16 12665>) #6
  %1021 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1003, <8 x i16> <i16 -10394, i16 12665, i16 -10394, i16 12665, i16 -10394, i16 12665, i16 -10394, i16 12665>) #6
  %1022 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1000, <8 x i16> <i16 -16305, i16 1606, i16 -16305, i16 1606, i16 -16305, i16 1606, i16 -16305, i16 1606>) #6
  %1023 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1001, <8 x i16> <i16 -16305, i16 1606, i16 -16305, i16 1606, i16 -16305, i16 1606, i16 -16305, i16 1606>) #6
  %1024 = add <4 x i32> %1008, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1025 = add <4 x i32> %1009, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1026 = add <4 x i32> %1010, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1027 = add <4 x i32> %1011, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1028 = add <4 x i32> %1012, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1029 = add <4 x i32> %1013, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1030 = add <4 x i32> %1014, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1031 = add <4 x i32> %1015, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1032 = add <4 x i32> %1016, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1033 = add <4 x i32> %1017, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1034 = add <4 x i32> %1018, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1035 = add <4 x i32> %1019, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1036 = add <4 x i32> %1020, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1037 = add <4 x i32> %1021, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1038 = add <4 x i32> %1022, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1039 = add <4 x i32> %1023, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1040 = ashr <4 x i32> %1024, <i32 14, i32 14, i32 14, i32 14>
  %1041 = ashr <4 x i32> %1025, <i32 14, i32 14, i32 14, i32 14>
  %1042 = ashr <4 x i32> %1026, <i32 14, i32 14, i32 14, i32 14>
  %1043 = ashr <4 x i32> %1027, <i32 14, i32 14, i32 14, i32 14>
  %1044 = ashr <4 x i32> %1028, <i32 14, i32 14, i32 14, i32 14>
  %1045 = ashr <4 x i32> %1029, <i32 14, i32 14, i32 14, i32 14>
  %1046 = ashr <4 x i32> %1030, <i32 14, i32 14, i32 14, i32 14>
  %1047 = ashr <4 x i32> %1031, <i32 14, i32 14, i32 14, i32 14>
  %1048 = ashr <4 x i32> %1032, <i32 14, i32 14, i32 14, i32 14>
  %1049 = ashr <4 x i32> %1033, <i32 14, i32 14, i32 14, i32 14>
  %1050 = ashr <4 x i32> %1034, <i32 14, i32 14, i32 14, i32 14>
  %1051 = ashr <4 x i32> %1035, <i32 14, i32 14, i32 14, i32 14>
  %1052 = ashr <4 x i32> %1036, <i32 14, i32 14, i32 14, i32 14>
  %1053 = ashr <4 x i32> %1037, <i32 14, i32 14, i32 14, i32 14>
  %1054 = ashr <4 x i32> %1038, <i32 14, i32 14, i32 14, i32 14>
  %1055 = ashr <4 x i32> %1039, <i32 14, i32 14, i32 14, i32 14>
  %1056 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1040, <4 x i32> %1041) #6
  store <8 x i16> %1056, <8 x i16>* %46, align 16
  %1057 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1042, <4 x i32> %1043) #6
  store <8 x i16> %1057, <8 x i16>* %48, align 16
  %1058 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1044, <4 x i32> %1045) #6
  store <8 x i16> %1058, <8 x i16>* %50, align 16
  %1059 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1046, <4 x i32> %1047) #6
  store <8 x i16> %1059, <8 x i16>* %52, align 16
  %1060 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1048, <4 x i32> %1049) #6
  store <8 x i16> %1060, <8 x i16>* %54, align 16
  %1061 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1050, <4 x i32> %1051) #6
  store <8 x i16> %1061, <8 x i16>* %56, align 16
  %1062 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1052, <4 x i32> %1053) #6
  store <8 x i16> %1062, <8 x i16>* %58, align 16
  %1063 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1054, <4 x i32> %1055) #6
  store <8 x i16> %1063, <8 x i16>* %60, align 16
  %1064 = add <8 x i16> %992, %880
  %1065 = sub <8 x i16> %880, %992
  %1066 = sub <8 x i16> %883, %993
  %1067 = add <8 x i16> %993, %883
  %1068 = add <8 x i16> %994, %884
  %1069 = sub <8 x i16> %884, %994
  %1070 = sub <8 x i16> %887, %995
  %1071 = add <8 x i16> %995, %887
  %1072 = add <8 x i16> %996, %888
  %1073 = sub <8 x i16> %888, %996
  %1074 = sub <8 x i16> %891, %997
  %1075 = add <8 x i16> %997, %891
  %1076 = add <8 x i16> %998, %892
  %1077 = sub <8 x i16> %892, %998
  %1078 = sub <8 x i16> %895, %999
  %1079 = add <8 x i16> %999, %895
  %1080 = shufflevector <8 x i16> %1064, <8 x i16> %1079, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1081 = shufflevector <8 x i16> %1064, <8 x i16> %1079, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1082 = shufflevector <8 x i16> %1065, <8 x i16> %1078, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1083 = shufflevector <8 x i16> %1065, <8 x i16> %1078, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1084 = shufflevector <8 x i16> %1066, <8 x i16> %1077, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1085 = shufflevector <8 x i16> %1066, <8 x i16> %1077, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1086 = shufflevector <8 x i16> %1067, <8 x i16> %1076, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1087 = shufflevector <8 x i16> %1067, <8 x i16> %1076, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1088 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1080, <8 x i16> <i16 804, i16 16364, i16 804, i16 16364, i16 804, i16 16364, i16 804, i16 16364>) #6
  %1089 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1081, <8 x i16> <i16 804, i16 16364, i16 804, i16 16364, i16 804, i16 16364, i16 804, i16 16364>) #6
  %1090 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1082, <8 x i16> <i16 12140, i16 11003, i16 12140, i16 11003, i16 12140, i16 11003, i16 12140, i16 11003>) #6
  %1091 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1083, <8 x i16> <i16 12140, i16 11003, i16 12140, i16 11003, i16 12140, i16 11003, i16 12140, i16 11003>) #6
  %1092 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1084, <8 x i16> <i16 7005, i16 14811, i16 7005, i16 14811, i16 7005, i16 14811, i16 7005, i16 14811>) #6
  %1093 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1085, <8 x i16> <i16 7005, i16 14811, i16 7005, i16 14811, i16 7005, i16 14811, i16 7005, i16 14811>) #6
  %1094 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1086, <8 x i16> <i16 15426, i16 5520, i16 15426, i16 5520, i16 15426, i16 5520, i16 15426, i16 5520>) #6
  %1095 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1087, <8 x i16> <i16 15426, i16 5520, i16 15426, i16 5520, i16 15426, i16 5520, i16 15426, i16 5520>) #6
  %1096 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1086, <8 x i16> <i16 -5520, i16 15426, i16 -5520, i16 15426, i16 -5520, i16 15426, i16 -5520, i16 15426>) #6
  %1097 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1087, <8 x i16> <i16 -5520, i16 15426, i16 -5520, i16 15426, i16 -5520, i16 15426, i16 -5520, i16 15426>) #6
  %1098 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1084, <8 x i16> <i16 -14811, i16 7005, i16 -14811, i16 7005, i16 -14811, i16 7005, i16 -14811, i16 7005>) #6
  %1099 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1085, <8 x i16> <i16 -14811, i16 7005, i16 -14811, i16 7005, i16 -14811, i16 7005, i16 -14811, i16 7005>) #6
  %1100 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1082, <8 x i16> <i16 -11003, i16 12140, i16 -11003, i16 12140, i16 -11003, i16 12140, i16 -11003, i16 12140>) #6
  %1101 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1083, <8 x i16> <i16 -11003, i16 12140, i16 -11003, i16 12140, i16 -11003, i16 12140, i16 -11003, i16 12140>) #6
  %1102 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1080, <8 x i16> <i16 -16364, i16 804, i16 -16364, i16 804, i16 -16364, i16 804, i16 -16364, i16 804>) #6
  %1103 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1081, <8 x i16> <i16 -16364, i16 804, i16 -16364, i16 804, i16 -16364, i16 804, i16 -16364, i16 804>) #6
  %1104 = add <4 x i32> %1088, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1105 = add <4 x i32> %1089, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1106 = add <4 x i32> %1090, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1107 = add <4 x i32> %1091, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1108 = add <4 x i32> %1092, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1109 = add <4 x i32> %1093, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1110 = add <4 x i32> %1094, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1111 = add <4 x i32> %1095, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1112 = add <4 x i32> %1096, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1113 = add <4 x i32> %1097, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1114 = add <4 x i32> %1098, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1115 = add <4 x i32> %1099, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1116 = add <4 x i32> %1100, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1117 = add <4 x i32> %1101, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1118 = add <4 x i32> %1102, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1119 = add <4 x i32> %1103, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1120 = ashr <4 x i32> %1104, <i32 14, i32 14, i32 14, i32 14>
  %1121 = ashr <4 x i32> %1105, <i32 14, i32 14, i32 14, i32 14>
  %1122 = ashr <4 x i32> %1106, <i32 14, i32 14, i32 14, i32 14>
  %1123 = ashr <4 x i32> %1107, <i32 14, i32 14, i32 14, i32 14>
  %1124 = ashr <4 x i32> %1108, <i32 14, i32 14, i32 14, i32 14>
  %1125 = ashr <4 x i32> %1109, <i32 14, i32 14, i32 14, i32 14>
  %1126 = ashr <4 x i32> %1110, <i32 14, i32 14, i32 14, i32 14>
  %1127 = ashr <4 x i32> %1111, <i32 14, i32 14, i32 14, i32 14>
  %1128 = ashr <4 x i32> %1112, <i32 14, i32 14, i32 14, i32 14>
  %1129 = ashr <4 x i32> %1113, <i32 14, i32 14, i32 14, i32 14>
  %1130 = ashr <4 x i32> %1114, <i32 14, i32 14, i32 14, i32 14>
  %1131 = ashr <4 x i32> %1115, <i32 14, i32 14, i32 14, i32 14>
  %1132 = ashr <4 x i32> %1116, <i32 14, i32 14, i32 14, i32 14>
  %1133 = ashr <4 x i32> %1117, <i32 14, i32 14, i32 14, i32 14>
  %1134 = ashr <4 x i32> %1118, <i32 14, i32 14, i32 14, i32 14>
  %1135 = ashr <4 x i32> %1119, <i32 14, i32 14, i32 14, i32 14>
  %1136 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1120, <4 x i32> %1121) #6
  store <8 x i16> %1136, <8 x i16>* %62, align 16
  %1137 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1122, <4 x i32> %1123) #6
  store <8 x i16> %1137, <8 x i16>* %64, align 16
  %1138 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1124, <4 x i32> %1125) #6
  store <8 x i16> %1138, <8 x i16>* %66, align 16
  %1139 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1126, <4 x i32> %1127) #6
  store <8 x i16> %1139, <8 x i16>* %68, align 16
  %1140 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1128, <4 x i32> %1129) #6
  store <8 x i16> %1140, <8 x i16>* %70, align 16
  %1141 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1130, <4 x i32> %1131) #6
  store <8 x i16> %1141, <8 x i16>* %72, align 16
  %1142 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1132, <4 x i32> %1133) #6
  store <8 x i16> %1142, <8 x i16>* %74, align 16
  %1143 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1134, <4 x i32> %1135) #6
  store <8 x i16> %1143, <8 x i16>* %76, align 16
  %1144 = shufflevector <8 x i16> %1068, <8 x i16> %1075, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1145 = shufflevector <8 x i16> %1068, <8 x i16> %1075, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1146 = shufflevector <8 x i16> %1069, <8 x i16> %1074, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1147 = shufflevector <8 x i16> %1069, <8 x i16> %1074, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1148 = shufflevector <8 x i16> %1070, <8 x i16> %1073, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1149 = shufflevector <8 x i16> %1070, <8 x i16> %1073, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1150 = shufflevector <8 x i16> %1071, <8 x i16> %1072, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1151 = shufflevector <8 x i16> %1071, <8 x i16> %1072, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1152 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1144, <8 x i16> <i16 3981, i16 15893, i16 3981, i16 15893, i16 3981, i16 15893, i16 3981, i16 15893>) #6
  %1153 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1145, <8 x i16> <i16 3981, i16 15893, i16 3981, i16 15893, i16 3981, i16 15893, i16 3981, i16 15893>) #6
  %1154 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1146, <8 x i16> <i16 14053, i16 8423, i16 14053, i16 8423, i16 14053, i16 8423, i16 14053, i16 8423>) #6
  %1155 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1147, <8 x i16> <i16 14053, i16 8423, i16 14053, i16 8423, i16 14053, i16 8423, i16 14053, i16 8423>) #6
  %1156 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1148, <8 x i16> <i16 9760, i16 13160, i16 9760, i16 13160, i16 9760, i16 13160, i16 9760, i16 13160>) #6
  %1157 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1149, <8 x i16> <i16 9760, i16 13160, i16 9760, i16 13160, i16 9760, i16 13160, i16 9760, i16 13160>) #6
  %1158 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1150, <8 x i16> <i16 16207, i16 2404, i16 16207, i16 2404, i16 16207, i16 2404, i16 16207, i16 2404>) #6
  %1159 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1151, <8 x i16> <i16 16207, i16 2404, i16 16207, i16 2404, i16 16207, i16 2404, i16 16207, i16 2404>) #6
  %1160 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1150, <8 x i16> <i16 -2404, i16 16207, i16 -2404, i16 16207, i16 -2404, i16 16207, i16 -2404, i16 16207>) #6
  %1161 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1151, <8 x i16> <i16 -2404, i16 16207, i16 -2404, i16 16207, i16 -2404, i16 16207, i16 -2404, i16 16207>) #6
  %1162 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1148, <8 x i16> <i16 -13160, i16 9760, i16 -13160, i16 9760, i16 -13160, i16 9760, i16 -13160, i16 9760>) #6
  %1163 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1149, <8 x i16> <i16 -13160, i16 9760, i16 -13160, i16 9760, i16 -13160, i16 9760, i16 -13160, i16 9760>) #6
  %1164 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1146, <8 x i16> <i16 -8423, i16 14053, i16 -8423, i16 14053, i16 -8423, i16 14053, i16 -8423, i16 14053>) #6
  %1165 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1147, <8 x i16> <i16 -8423, i16 14053, i16 -8423, i16 14053, i16 -8423, i16 14053, i16 -8423, i16 14053>) #6
  %1166 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1144, <8 x i16> <i16 -15893, i16 3981, i16 -15893, i16 3981, i16 -15893, i16 3981, i16 -15893, i16 3981>) #6
  %1167 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1145, <8 x i16> <i16 -15893, i16 3981, i16 -15893, i16 3981, i16 -15893, i16 3981, i16 -15893, i16 3981>) #6
  %1168 = add <4 x i32> %1152, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1169 = add <4 x i32> %1153, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1170 = add <4 x i32> %1154, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1171 = add <4 x i32> %1155, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1172 = add <4 x i32> %1156, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1173 = add <4 x i32> %1157, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1174 = add <4 x i32> %1158, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1175 = add <4 x i32> %1159, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1176 = add <4 x i32> %1160, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1177 = add <4 x i32> %1161, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1178 = add <4 x i32> %1162, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1179 = add <4 x i32> %1163, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1180 = add <4 x i32> %1164, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1181 = add <4 x i32> %1165, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1182 = add <4 x i32> %1166, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1183 = add <4 x i32> %1167, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1184 = ashr <4 x i32> %1168, <i32 14, i32 14, i32 14, i32 14>
  %1185 = ashr <4 x i32> %1169, <i32 14, i32 14, i32 14, i32 14>
  %1186 = ashr <4 x i32> %1170, <i32 14, i32 14, i32 14, i32 14>
  %1187 = ashr <4 x i32> %1171, <i32 14, i32 14, i32 14, i32 14>
  %1188 = ashr <4 x i32> %1172, <i32 14, i32 14, i32 14, i32 14>
  %1189 = ashr <4 x i32> %1173, <i32 14, i32 14, i32 14, i32 14>
  %1190 = ashr <4 x i32> %1174, <i32 14, i32 14, i32 14, i32 14>
  %1191 = ashr <4 x i32> %1175, <i32 14, i32 14, i32 14, i32 14>
  %1192 = ashr <4 x i32> %1176, <i32 14, i32 14, i32 14, i32 14>
  %1193 = ashr <4 x i32> %1177, <i32 14, i32 14, i32 14, i32 14>
  %1194 = ashr <4 x i32> %1178, <i32 14, i32 14, i32 14, i32 14>
  %1195 = ashr <4 x i32> %1179, <i32 14, i32 14, i32 14, i32 14>
  %1196 = ashr <4 x i32> %1180, <i32 14, i32 14, i32 14, i32 14>
  %1197 = ashr <4 x i32> %1181, <i32 14, i32 14, i32 14, i32 14>
  %1198 = ashr <4 x i32> %1182, <i32 14, i32 14, i32 14, i32 14>
  %1199 = ashr <4 x i32> %1183, <i32 14, i32 14, i32 14, i32 14>
  %1200 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1184, <4 x i32> %1185) #6
  store <8 x i16> %1200, <8 x i16>* %78, align 16
  %1201 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1186, <4 x i32> %1187) #6
  store <8 x i16> %1201, <8 x i16>* %80, align 16
  %1202 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1188, <4 x i32> %1189) #6
  store <8 x i16> %1202, <8 x i16>* %82, align 16
  %1203 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1190, <4 x i32> %1191) #6
  store <8 x i16> %1203, <8 x i16>* %84, align 16
  %1204 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1192, <4 x i32> %1193) #6
  store <8 x i16> %1204, <8 x i16>* %86, align 16
  %1205 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1194, <4 x i32> %1195) #6
  store <8 x i16> %1205, <8 x i16>* %88, align 16
  %1206 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1196, <4 x i32> %1197) #6
  store <8 x i16> %1206, <8 x i16>* %90, align 16
  %1207 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1198, <4 x i32> %1199) #6
  store <8 x i16> %1207, <8 x i16>* %92, align 16
  %1208 = shl nsw i64 %98, 5
  %1209 = getelementptr inbounds [1024 x i16], [1024 x i16]* %4, i64 0, i64 %1208
  %1210 = getelementptr inbounds i32, i32* %1, i64 %1208
  br label %1211

1211:                                             ; preds = %1409, %631
  %1212 = phi i64 [ 0, %631 ], [ %1412, %1409 ]
  %1213 = phi i32* [ %1210, %631 ], [ %1411, %1409 ]
  %1214 = phi i16* [ %1209, %631 ], [ %1410, %1409 ]
  %1215 = shl nsw i64 %1212, 3
  %1216 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 %1215
  %1217 = bitcast <2 x i64>* %1216 to <8 x i16>*
  %1218 = load <8 x i16>, <8 x i16>* %1217, align 16
  %1219 = getelementptr inbounds <2 x i64>, <2 x i64>* %1216, i64 1
  %1220 = bitcast <2 x i64>* %1219 to <8 x i16>*
  %1221 = load <8 x i16>, <8 x i16>* %1220, align 16
  %1222 = shufflevector <8 x i16> %1218, <8 x i16> %1221, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1223 = getelementptr inbounds <2 x i64>, <2 x i64>* %1216, i64 2
  %1224 = bitcast <2 x i64>* %1223 to <8 x i16>*
  %1225 = load <8 x i16>, <8 x i16>* %1224, align 16
  %1226 = getelementptr inbounds <2 x i64>, <2 x i64>* %1216, i64 3
  %1227 = bitcast <2 x i64>* %1226 to <8 x i16>*
  %1228 = load <8 x i16>, <8 x i16>* %1227, align 16
  %1229 = shufflevector <8 x i16> %1225, <8 x i16> %1228, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1230 = shufflevector <8 x i16> %1218, <8 x i16> %1221, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1231 = shufflevector <8 x i16> %1225, <8 x i16> %1228, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1232 = getelementptr inbounds <2 x i64>, <2 x i64>* %1216, i64 4
  %1233 = bitcast <2 x i64>* %1232 to <8 x i16>*
  %1234 = load <8 x i16>, <8 x i16>* %1233, align 16
  %1235 = getelementptr inbounds <2 x i64>, <2 x i64>* %1216, i64 5
  %1236 = bitcast <2 x i64>* %1235 to <8 x i16>*
  %1237 = load <8 x i16>, <8 x i16>* %1236, align 16
  %1238 = shufflevector <8 x i16> %1234, <8 x i16> %1237, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1239 = getelementptr inbounds <2 x i64>, <2 x i64>* %1216, i64 6
  %1240 = bitcast <2 x i64>* %1239 to <8 x i16>*
  %1241 = load <8 x i16>, <8 x i16>* %1240, align 16
  %1242 = getelementptr inbounds <2 x i64>, <2 x i64>* %1216, i64 7
  %1243 = bitcast <2 x i64>* %1242 to <8 x i16>*
  %1244 = load <8 x i16>, <8 x i16>* %1243, align 16
  %1245 = shufflevector <8 x i16> %1241, <8 x i16> %1244, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1246 = shufflevector <8 x i16> %1234, <8 x i16> %1237, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1247 = shufflevector <8 x i16> %1241, <8 x i16> %1244, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1248 = bitcast <8 x i16> %1222 to <4 x i32>
  %1249 = bitcast <8 x i16> %1229 to <4 x i32>
  %1250 = shufflevector <4 x i32> %1248, <4 x i32> %1249, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1251 = bitcast <4 x i32> %1250 to <2 x i64>
  %1252 = bitcast <8 x i16> %1230 to <4 x i32>
  %1253 = bitcast <8 x i16> %1231 to <4 x i32>
  %1254 = shufflevector <4 x i32> %1252, <4 x i32> %1253, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1255 = bitcast <4 x i32> %1254 to <2 x i64>
  %1256 = shufflevector <4 x i32> %1248, <4 x i32> %1249, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1257 = bitcast <4 x i32> %1256 to <2 x i64>
  %1258 = shufflevector <4 x i32> %1252, <4 x i32> %1253, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1259 = bitcast <4 x i32> %1258 to <2 x i64>
  %1260 = bitcast <8 x i16> %1238 to <4 x i32>
  %1261 = bitcast <8 x i16> %1245 to <4 x i32>
  %1262 = shufflevector <4 x i32> %1260, <4 x i32> %1261, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1263 = bitcast <4 x i32> %1262 to <2 x i64>
  %1264 = bitcast <8 x i16> %1246 to <4 x i32>
  %1265 = bitcast <8 x i16> %1247 to <4 x i32>
  %1266 = shufflevector <4 x i32> %1264, <4 x i32> %1265, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1267 = bitcast <4 x i32> %1266 to <2 x i64>
  %1268 = shufflevector <4 x i32> %1260, <4 x i32> %1261, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1269 = bitcast <4 x i32> %1268 to <2 x i64>
  %1270 = shufflevector <4 x i32> %1264, <4 x i32> %1265, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1271 = bitcast <4 x i32> %1270 to <2 x i64>
  %1272 = shufflevector <2 x i64> %1251, <2 x i64> %1263, <2 x i32> <i32 0, i32 2>
  %1273 = shufflevector <2 x i64> %1251, <2 x i64> %1263, <2 x i32> <i32 1, i32 3>
  %1274 = shufflevector <2 x i64> %1257, <2 x i64> %1269, <2 x i32> <i32 0, i32 2>
  %1275 = shufflevector <2 x i64> %1257, <2 x i64> %1269, <2 x i32> <i32 1, i32 3>
  %1276 = shufflevector <2 x i64> %1255, <2 x i64> %1267, <2 x i32> <i32 0, i32 2>
  %1277 = shufflevector <2 x i64> %1255, <2 x i64> %1267, <2 x i32> <i32 1, i32 3>
  %1278 = shufflevector <2 x i64> %1259, <2 x i64> %1271, <2 x i32> <i32 0, i32 2>
  %1279 = shufflevector <2 x i64> %1259, <2 x i64> %1271, <2 x i32> <i32 1, i32 3>
  %1280 = bitcast <2 x i64> %1272 to <8 x i16>
  br i1 %95, label %1281, label %1345

1281:                                             ; preds = %1211
  %1282 = icmp sgt <8 x i16> %1280, zeroinitializer
  %1283 = bitcast <2 x i64> %1273 to <8 x i16>
  %1284 = icmp sgt <8 x i16> %1283, zeroinitializer
  %1285 = bitcast <2 x i64> %1274 to <8 x i16>
  %1286 = icmp sgt <8 x i16> %1285, zeroinitializer
  %1287 = bitcast <2 x i64> %1275 to <8 x i16>
  %1288 = icmp sgt <8 x i16> %1287, zeroinitializer
  %1289 = bitcast <2 x i64> %1276 to <8 x i16>
  %1290 = icmp sgt <8 x i16> %1289, zeroinitializer
  %1291 = bitcast <2 x i64> %1277 to <8 x i16>
  %1292 = icmp sgt <8 x i16> %1291, zeroinitializer
  %1293 = bitcast <2 x i64> %1278 to <8 x i16>
  %1294 = icmp sgt <8 x i16> %1293, zeroinitializer
  %1295 = bitcast <2 x i64> %1279 to <8 x i16>
  %1296 = icmp sgt <8 x i16> %1295, zeroinitializer
  %1297 = zext <8 x i1> %1282 to <8 x i16>
  %1298 = zext <8 x i1> %1284 to <8 x i16>
  %1299 = zext <8 x i1> %1286 to <8 x i16>
  %1300 = zext <8 x i1> %1288 to <8 x i16>
  %1301 = zext <8 x i1> %1290 to <8 x i16>
  %1302 = zext <8 x i1> %1292 to <8 x i16>
  %1303 = zext <8 x i1> %1294 to <8 x i16>
  %1304 = zext <8 x i1> %1296 to <8 x i16>
  %1305 = add <8 x i16> %1280, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1306 = add <8 x i16> %1305, %1297
  %1307 = add <8 x i16> %1283, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1308 = add <8 x i16> %1307, %1298
  %1309 = add <8 x i16> %1285, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1310 = add <8 x i16> %1309, %1299
  %1311 = add <8 x i16> %1287, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1312 = add <8 x i16> %1311, %1300
  %1313 = add <8 x i16> %1289, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1314 = add <8 x i16> %1313, %1301
  %1315 = add <8 x i16> %1291, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1316 = add <8 x i16> %1315, %1302
  %1317 = add <8 x i16> %1293, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1318 = add <8 x i16> %1317, %1303
  %1319 = add <8 x i16> %1295, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1320 = add <8 x i16> %1319, %1304
  %1321 = ashr <8 x i16> %1306, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1322 = ashr <8 x i16> %1308, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1323 = ashr <8 x i16> %1310, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1324 = ashr <8 x i16> %1312, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1325 = ashr <8 x i16> %1314, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1326 = ashr <8 x i16> %1316, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1327 = ashr <8 x i16> %1318, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1328 = ashr <8 x i16> %1320, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1329 = bitcast i16* %1214 to <8 x i16>*
  store <8 x i16> %1321, <8 x i16>* %1329, align 1
  %1330 = getelementptr inbounds i16, i16* %1214, i64 32
  %1331 = bitcast i16* %1330 to <8 x i16>*
  store <8 x i16> %1322, <8 x i16>* %1331, align 1
  %1332 = getelementptr inbounds i16, i16* %1214, i64 64
  %1333 = bitcast i16* %1332 to <8 x i16>*
  store <8 x i16> %1323, <8 x i16>* %1333, align 1
  %1334 = getelementptr inbounds i16, i16* %1214, i64 96
  %1335 = bitcast i16* %1334 to <8 x i16>*
  store <8 x i16> %1324, <8 x i16>* %1335, align 1
  %1336 = getelementptr inbounds i16, i16* %1214, i64 128
  %1337 = bitcast i16* %1336 to <8 x i16>*
  store <8 x i16> %1325, <8 x i16>* %1337, align 1
  %1338 = getelementptr inbounds i16, i16* %1214, i64 160
  %1339 = bitcast i16* %1338 to <8 x i16>*
  store <8 x i16> %1326, <8 x i16>* %1339, align 1
  %1340 = getelementptr inbounds i16, i16* %1214, i64 192
  %1341 = bitcast i16* %1340 to <8 x i16>*
  store <8 x i16> %1327, <8 x i16>* %1341, align 1
  %1342 = getelementptr inbounds i16, i16* %1214, i64 224
  %1343 = bitcast i16* %1342 to <8 x i16>*
  store <8 x i16> %1328, <8 x i16>* %1343, align 1
  %1344 = getelementptr inbounds i16, i16* %1214, i64 8
  br label %1409

1345:                                             ; preds = %1211
  %1346 = ashr <8 x i16> %1280, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1347 = shufflevector <8 x i16> %1280, <8 x i16> %1346, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1348 = shufflevector <8 x i16> %1280, <8 x i16> %1346, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1349 = bitcast i32* %1213 to <8 x i16>*
  store <8 x i16> %1347, <8 x i16>* %1349, align 1
  %1350 = getelementptr inbounds i32, i32* %1213, i64 4
  %1351 = bitcast i32* %1350 to <8 x i16>*
  store <8 x i16> %1348, <8 x i16>* %1351, align 1
  %1352 = getelementptr inbounds i32, i32* %1213, i64 32
  %1353 = bitcast <2 x i64> %1273 to <8 x i16>
  %1354 = ashr <8 x i16> %1353, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1355 = shufflevector <8 x i16> %1353, <8 x i16> %1354, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1356 = shufflevector <8 x i16> %1353, <8 x i16> %1354, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1357 = bitcast i32* %1352 to <8 x i16>*
  store <8 x i16> %1355, <8 x i16>* %1357, align 1
  %1358 = getelementptr inbounds i32, i32* %1213, i64 36
  %1359 = bitcast i32* %1358 to <8 x i16>*
  store <8 x i16> %1356, <8 x i16>* %1359, align 1
  %1360 = getelementptr inbounds i32, i32* %1213, i64 64
  %1361 = bitcast <2 x i64> %1274 to <8 x i16>
  %1362 = ashr <8 x i16> %1361, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1363 = shufflevector <8 x i16> %1361, <8 x i16> %1362, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1364 = shufflevector <8 x i16> %1361, <8 x i16> %1362, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1365 = bitcast i32* %1360 to <8 x i16>*
  store <8 x i16> %1363, <8 x i16>* %1365, align 1
  %1366 = getelementptr inbounds i32, i32* %1213, i64 68
  %1367 = bitcast i32* %1366 to <8 x i16>*
  store <8 x i16> %1364, <8 x i16>* %1367, align 1
  %1368 = getelementptr inbounds i32, i32* %1213, i64 96
  %1369 = bitcast <2 x i64> %1275 to <8 x i16>
  %1370 = ashr <8 x i16> %1369, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1371 = shufflevector <8 x i16> %1369, <8 x i16> %1370, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1372 = shufflevector <8 x i16> %1369, <8 x i16> %1370, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1373 = bitcast i32* %1368 to <8 x i16>*
  store <8 x i16> %1371, <8 x i16>* %1373, align 1
  %1374 = getelementptr inbounds i32, i32* %1213, i64 100
  %1375 = bitcast i32* %1374 to <8 x i16>*
  store <8 x i16> %1372, <8 x i16>* %1375, align 1
  %1376 = getelementptr inbounds i32, i32* %1213, i64 128
  %1377 = bitcast <2 x i64> %1276 to <8 x i16>
  %1378 = ashr <8 x i16> %1377, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1379 = shufflevector <8 x i16> %1377, <8 x i16> %1378, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1380 = shufflevector <8 x i16> %1377, <8 x i16> %1378, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1381 = bitcast i32* %1376 to <8 x i16>*
  store <8 x i16> %1379, <8 x i16>* %1381, align 1
  %1382 = getelementptr inbounds i32, i32* %1213, i64 132
  %1383 = bitcast i32* %1382 to <8 x i16>*
  store <8 x i16> %1380, <8 x i16>* %1383, align 1
  %1384 = getelementptr inbounds i32, i32* %1213, i64 160
  %1385 = bitcast <2 x i64> %1277 to <8 x i16>
  %1386 = ashr <8 x i16> %1385, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1387 = shufflevector <8 x i16> %1385, <8 x i16> %1386, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1388 = shufflevector <8 x i16> %1385, <8 x i16> %1386, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1389 = bitcast i32* %1384 to <8 x i16>*
  store <8 x i16> %1387, <8 x i16>* %1389, align 1
  %1390 = getelementptr inbounds i32, i32* %1213, i64 164
  %1391 = bitcast i32* %1390 to <8 x i16>*
  store <8 x i16> %1388, <8 x i16>* %1391, align 1
  %1392 = getelementptr inbounds i32, i32* %1213, i64 192
  %1393 = bitcast <2 x i64> %1278 to <8 x i16>
  %1394 = ashr <8 x i16> %1393, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1395 = shufflevector <8 x i16> %1393, <8 x i16> %1394, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1396 = shufflevector <8 x i16> %1393, <8 x i16> %1394, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1397 = bitcast i32* %1392 to <8 x i16>*
  store <8 x i16> %1395, <8 x i16>* %1397, align 1
  %1398 = getelementptr inbounds i32, i32* %1213, i64 196
  %1399 = bitcast i32* %1398 to <8 x i16>*
  store <8 x i16> %1396, <8 x i16>* %1399, align 1
  %1400 = getelementptr inbounds i32, i32* %1213, i64 224
  %1401 = bitcast <2 x i64> %1279 to <8 x i16>
  %1402 = ashr <8 x i16> %1401, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1403 = shufflevector <8 x i16> %1401, <8 x i16> %1402, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1404 = shufflevector <8 x i16> %1401, <8 x i16> %1402, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1405 = bitcast i32* %1400 to <8 x i16>*
  store <8 x i16> %1403, <8 x i16>* %1405, align 1
  %1406 = getelementptr inbounds i32, i32* %1213, i64 228
  %1407 = bitcast i32* %1406 to <8 x i16>*
  store <8 x i16> %1404, <8 x i16>* %1407, align 1
  %1408 = getelementptr inbounds i32, i32* %1213, i64 8
  br label %1409

1409:                                             ; preds = %1345, %1281
  %1410 = phi i16* [ %1344, %1281 ], [ %1214, %1345 ]
  %1411 = phi i32* [ %1213, %1281 ], [ %1408, %1345 ]
  %1412 = add nuw nsw i64 %1212, 1
  %1413 = icmp eq i64 %1412, 4
  br i1 %1413, label %1414, label %1211

1414:                                             ; preds = %1409
  call void @llvm.lifetime.end.p0i8(i64 512, i8* nonnull %9) #6
  %1415 = add nuw nsw i64 %98, 8
  %1416 = icmp ult i64 %1415, 32
  br i1 %1416, label %97, label %1417

1417:                                             ; preds = %1414
  %1418 = add nuw nsw i32 %94, 1
  %1419 = icmp eq i32 %1418, 2
  br i1 %1419, label %1420, label %93

1420:                                             ; preds = %1417
  call void @llvm.lifetime.end.p0i8(i64 2048, i8* nonnull %8) #6
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @vpx_fdct32x32_sse2(i16* nocapture readonly, i32* nocapture, i32) local_unnamed_addr #2 {
  %4 = alloca [1024 x i16], align 16
  %5 = alloca [32 x <2 x i64>], align 16
  %6 = shl nsw i32 %2, 1
  %7 = mul nsw i32 %2, 3
  %8 = bitcast [1024 x i16]* %4 to i8*
  call void @llvm.lifetime.start.p0i8(i64 2048, i8* nonnull %8) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %8, i8 -86, i64 2048, i1 false)
  %9 = bitcast [32 x <2 x i64>]* %5 to i8*
  %10 = mul nsw i32 %2, 31
  %11 = sext i32 %10 to i64
  %12 = sext i32 %2 to i64
  %13 = sext i32 %6 to i64
  %14 = sext i32 %7 to i64
  %15 = sub nsw i64 0, %14
  %16 = sub nsw i64 0, %13
  %17 = sub nsw i64 0, %12
  %18 = shl nsw i32 %2, 2
  %19 = sext i32 %18 to i64
  %20 = mul nsw i32 %2, 27
  %21 = sext i32 %20 to i64
  %22 = shl nsw i32 %2, 3
  %23 = sext i32 %22 to i64
  %24 = mul nsw i32 %2, 23
  %25 = sext i32 %24 to i64
  %26 = mul nsw i32 %2, 12
  %27 = sext i32 %26 to i64
  %28 = mul nsw i32 %2, 19
  %29 = sext i32 %28 to i64
  %30 = bitcast [32 x <2 x i64>]* %5 to <8 x i16>*
  %31 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 16
  %32 = bitcast <2 x i64>* %31 to <8 x i16>*
  %33 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 8
  %34 = bitcast <2 x i64>* %33 to <8 x i16>*
  %35 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 24
  %36 = bitcast <2 x i64>* %35 to <8 x i16>*
  %37 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 4
  %38 = bitcast <2 x i64>* %37 to <8 x i16>*
  %39 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 20
  %40 = bitcast <2 x i64>* %39 to <8 x i16>*
  %41 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 12
  %42 = bitcast <2 x i64>* %41 to <8 x i16>*
  %43 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 28
  %44 = bitcast <2 x i64>* %43 to <8 x i16>*
  %45 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 2
  %46 = bitcast <2 x i64>* %45 to <8 x i16>*
  %47 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 18
  %48 = bitcast <2 x i64>* %47 to <8 x i16>*
  %49 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 10
  %50 = bitcast <2 x i64>* %49 to <8 x i16>*
  %51 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 26
  %52 = bitcast <2 x i64>* %51 to <8 x i16>*
  %53 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 6
  %54 = bitcast <2 x i64>* %53 to <8 x i16>*
  %55 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 22
  %56 = bitcast <2 x i64>* %55 to <8 x i16>*
  %57 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 14
  %58 = bitcast <2 x i64>* %57 to <8 x i16>*
  %59 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 30
  %60 = bitcast <2 x i64>* %59 to <8 x i16>*
  %61 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 1
  %62 = bitcast <2 x i64>* %61 to <8 x i16>*
  %63 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 17
  %64 = bitcast <2 x i64>* %63 to <8 x i16>*
  %65 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 9
  %66 = bitcast <2 x i64>* %65 to <8 x i16>*
  %67 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 25
  %68 = bitcast <2 x i64>* %67 to <8 x i16>*
  %69 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 7
  %70 = bitcast <2 x i64>* %69 to <8 x i16>*
  %71 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 23
  %72 = bitcast <2 x i64>* %71 to <8 x i16>*
  %73 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 15
  %74 = bitcast <2 x i64>* %73 to <8 x i16>*
  %75 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 31
  %76 = bitcast <2 x i64>* %75 to <8 x i16>*
  %77 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 5
  %78 = bitcast <2 x i64>* %77 to <8 x i16>*
  %79 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 21
  %80 = bitcast <2 x i64>* %79 to <8 x i16>*
  %81 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 13
  %82 = bitcast <2 x i64>* %81 to <8 x i16>*
  %83 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 29
  %84 = bitcast <2 x i64>* %83 to <8 x i16>*
  %85 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 3
  %86 = bitcast <2 x i64>* %85 to <8 x i16>*
  %87 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 19
  %88 = bitcast <2 x i64>* %87 to <8 x i16>*
  %89 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 11
  %90 = bitcast <2 x i64>* %89 to <8 x i16>*
  %91 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 27
  %92 = bitcast <2 x i64>* %91 to <8 x i16>*
  br label %93

93:                                               ; preds = %3922, %3
  %94 = phi i32 [ 0, %3 ], [ %3923, %3922 ]
  %95 = icmp eq i32 %94, 0
  br label %96

96:                                               ; preds = %93, %3919
  %97 = phi i64 [ 0, %93 ], [ %3920, %3919 ]
  call void @llvm.lifetime.start.p0i8(i64 512, i8* nonnull %9) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %9, i8 -86, i64 512, i1 false)
  br i1 %95, label %98, label %259

98:                                               ; preds = %96
  %99 = getelementptr inbounds i16, i16* %0, i64 %97
  %100 = getelementptr inbounds i16, i16* %99, i64 %11
  %101 = bitcast i16* %99 to <8 x i16>*
  %102 = load <8 x i16>, <8 x i16>* %101, align 1
  %103 = getelementptr inbounds i16, i16* %99, i64 %12
  %104 = bitcast i16* %103 to <8 x i16>*
  %105 = load <8 x i16>, <8 x i16>* %104, align 1
  %106 = getelementptr inbounds i16, i16* %99, i64 %13
  %107 = bitcast i16* %106 to <8 x i16>*
  %108 = load <8 x i16>, <8 x i16>* %107, align 1
  %109 = getelementptr inbounds i16, i16* %99, i64 %14
  %110 = bitcast i16* %109 to <8 x i16>*
  %111 = load <8 x i16>, <8 x i16>* %110, align 1
  %112 = getelementptr inbounds i16, i16* %100, i64 %15
  %113 = bitcast i16* %112 to <8 x i16>*
  %114 = load <8 x i16>, <8 x i16>* %113, align 1
  %115 = getelementptr inbounds i16, i16* %100, i64 %16
  %116 = bitcast i16* %115 to <8 x i16>*
  %117 = load <8 x i16>, <8 x i16>* %116, align 1
  %118 = getelementptr inbounds i16, i16* %100, i64 %17
  %119 = bitcast i16* %118 to <8 x i16>*
  %120 = load <8 x i16>, <8 x i16>* %119, align 1
  %121 = bitcast i16* %100 to <8 x i16>*
  %122 = load <8 x i16>, <8 x i16>* %121, align 1
  %123 = add <8 x i16> %122, %102
  %124 = add <8 x i16> %120, %105
  %125 = add <8 x i16> %117, %108
  %126 = add <8 x i16> %114, %111
  %127 = sub <8 x i16> %111, %114
  %128 = sub <8 x i16> %108, %117
  %129 = sub <8 x i16> %105, %120
  %130 = sub <8 x i16> %102, %122
  %131 = shl <8 x i16> %123, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %132 = shl <8 x i16> %124, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %133 = shl <8 x i16> %125, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %134 = shl <8 x i16> %126, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %135 = shl <8 x i16> %127, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %136 = shl <8 x i16> %128, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %137 = shl <8 x i16> %129, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %138 = shl <8 x i16> %130, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %139 = getelementptr inbounds i16, i16* %99, i64 %19
  %140 = getelementptr inbounds i16, i16* %99, i64 %21
  %141 = bitcast i16* %139 to <8 x i16>*
  %142 = load <8 x i16>, <8 x i16>* %141, align 1
  %143 = getelementptr inbounds i16, i16* %139, i64 %12
  %144 = bitcast i16* %143 to <8 x i16>*
  %145 = load <8 x i16>, <8 x i16>* %144, align 1
  %146 = getelementptr inbounds i16, i16* %139, i64 %13
  %147 = bitcast i16* %146 to <8 x i16>*
  %148 = load <8 x i16>, <8 x i16>* %147, align 1
  %149 = getelementptr inbounds i16, i16* %139, i64 %14
  %150 = bitcast i16* %149 to <8 x i16>*
  %151 = load <8 x i16>, <8 x i16>* %150, align 1
  %152 = getelementptr inbounds i16, i16* %140, i64 %15
  %153 = bitcast i16* %152 to <8 x i16>*
  %154 = load <8 x i16>, <8 x i16>* %153, align 1
  %155 = getelementptr inbounds i16, i16* %140, i64 %16
  %156 = bitcast i16* %155 to <8 x i16>*
  %157 = load <8 x i16>, <8 x i16>* %156, align 1
  %158 = getelementptr inbounds i16, i16* %140, i64 %17
  %159 = bitcast i16* %158 to <8 x i16>*
  %160 = load <8 x i16>, <8 x i16>* %159, align 1
  %161 = bitcast i16* %140 to <8 x i16>*
  %162 = load <8 x i16>, <8 x i16>* %161, align 1
  %163 = add <8 x i16> %162, %142
  %164 = add <8 x i16> %160, %145
  %165 = add <8 x i16> %157, %148
  %166 = add <8 x i16> %154, %151
  %167 = sub <8 x i16> %151, %154
  %168 = sub <8 x i16> %148, %157
  %169 = sub <8 x i16> %145, %160
  %170 = sub <8 x i16> %142, %162
  %171 = shl <8 x i16> %163, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %172 = shl <8 x i16> %164, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %173 = shl <8 x i16> %165, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %174 = shl <8 x i16> %166, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %175 = shl <8 x i16> %167, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %176 = shl <8 x i16> %168, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %177 = shl <8 x i16> %169, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %178 = shl <8 x i16> %170, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %179 = getelementptr inbounds i16, i16* %99, i64 %23
  %180 = getelementptr inbounds i16, i16* %99, i64 %25
  %181 = bitcast i16* %179 to <8 x i16>*
  %182 = load <8 x i16>, <8 x i16>* %181, align 1
  %183 = getelementptr inbounds i16, i16* %179, i64 %12
  %184 = bitcast i16* %183 to <8 x i16>*
  %185 = load <8 x i16>, <8 x i16>* %184, align 1
  %186 = getelementptr inbounds i16, i16* %179, i64 %13
  %187 = bitcast i16* %186 to <8 x i16>*
  %188 = load <8 x i16>, <8 x i16>* %187, align 1
  %189 = getelementptr inbounds i16, i16* %179, i64 %14
  %190 = bitcast i16* %189 to <8 x i16>*
  %191 = load <8 x i16>, <8 x i16>* %190, align 1
  %192 = getelementptr inbounds i16, i16* %180, i64 %15
  %193 = bitcast i16* %192 to <8 x i16>*
  %194 = load <8 x i16>, <8 x i16>* %193, align 1
  %195 = getelementptr inbounds i16, i16* %180, i64 %16
  %196 = bitcast i16* %195 to <8 x i16>*
  %197 = load <8 x i16>, <8 x i16>* %196, align 1
  %198 = getelementptr inbounds i16, i16* %180, i64 %17
  %199 = bitcast i16* %198 to <8 x i16>*
  %200 = load <8 x i16>, <8 x i16>* %199, align 1
  %201 = bitcast i16* %180 to <8 x i16>*
  %202 = load <8 x i16>, <8 x i16>* %201, align 1
  %203 = add <8 x i16> %202, %182
  %204 = add <8 x i16> %200, %185
  %205 = add <8 x i16> %197, %188
  %206 = add <8 x i16> %194, %191
  %207 = sub <8 x i16> %191, %194
  %208 = sub <8 x i16> %188, %197
  %209 = sub <8 x i16> %185, %200
  %210 = sub <8 x i16> %182, %202
  %211 = shl <8 x i16> %203, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %212 = shl <8 x i16> %204, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %213 = shl <8 x i16> %205, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %214 = shl <8 x i16> %206, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %215 = shl <8 x i16> %207, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %216 = shl <8 x i16> %208, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %217 = shl <8 x i16> %209, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %218 = shl <8 x i16> %210, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %219 = getelementptr inbounds i16, i16* %99, i64 %27
  %220 = getelementptr inbounds i16, i16* %99, i64 %29
  %221 = bitcast i16* %219 to <8 x i16>*
  %222 = load <8 x i16>, <8 x i16>* %221, align 1
  %223 = getelementptr inbounds i16, i16* %219, i64 %12
  %224 = bitcast i16* %223 to <8 x i16>*
  %225 = load <8 x i16>, <8 x i16>* %224, align 1
  %226 = getelementptr inbounds i16, i16* %219, i64 %13
  %227 = bitcast i16* %226 to <8 x i16>*
  %228 = load <8 x i16>, <8 x i16>* %227, align 1
  %229 = getelementptr inbounds i16, i16* %219, i64 %14
  %230 = bitcast i16* %229 to <8 x i16>*
  %231 = load <8 x i16>, <8 x i16>* %230, align 1
  %232 = getelementptr inbounds i16, i16* %220, i64 %15
  %233 = bitcast i16* %232 to <8 x i16>*
  %234 = load <8 x i16>, <8 x i16>* %233, align 1
  %235 = getelementptr inbounds i16, i16* %220, i64 %16
  %236 = bitcast i16* %235 to <8 x i16>*
  %237 = load <8 x i16>, <8 x i16>* %236, align 1
  %238 = getelementptr inbounds i16, i16* %220, i64 %17
  %239 = bitcast i16* %238 to <8 x i16>*
  %240 = load <8 x i16>, <8 x i16>* %239, align 1
  %241 = bitcast i16* %220 to <8 x i16>*
  %242 = load <8 x i16>, <8 x i16>* %241, align 1
  %243 = add <8 x i16> %242, %222
  %244 = add <8 x i16> %240, %225
  %245 = add <8 x i16> %237, %228
  %246 = add <8 x i16> %234, %231
  %247 = sub <8 x i16> %231, %234
  %248 = sub <8 x i16> %228, %237
  %249 = sub <8 x i16> %225, %240
  %250 = sub <8 x i16> %222, %242
  %251 = shl <8 x i16> %243, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %252 = shl <8 x i16> %244, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %253 = shl <8 x i16> %245, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %254 = shl <8 x i16> %246, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %255 = shl <8 x i16> %247, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %256 = shl <8 x i16> %248, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %257 = shl <8 x i16> %249, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %258 = shl <8 x i16> %250, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  br label %388

259:                                              ; preds = %96
  %260 = getelementptr inbounds [1024 x i16], [1024 x i16]* %4, i64 0, i64 %97
  %261 = bitcast i16* %260 to <8 x i16>*
  %262 = load <8 x i16>, <8 x i16>* %261, align 16
  %263 = getelementptr inbounds i16, i16* %260, i64 32
  %264 = bitcast i16* %263 to <8 x i16>*
  %265 = load <8 x i16>, <8 x i16>* %264, align 16
  %266 = getelementptr inbounds i16, i16* %260, i64 64
  %267 = bitcast i16* %266 to <8 x i16>*
  %268 = load <8 x i16>, <8 x i16>* %267, align 16
  %269 = getelementptr inbounds i16, i16* %260, i64 96
  %270 = bitcast i16* %269 to <8 x i16>*
  %271 = load <8 x i16>, <8 x i16>* %270, align 16
  %272 = getelementptr inbounds i16, i16* %260, i64 896
  %273 = bitcast i16* %272 to <8 x i16>*
  %274 = load <8 x i16>, <8 x i16>* %273, align 16
  %275 = getelementptr inbounds i16, i16* %260, i64 928
  %276 = bitcast i16* %275 to <8 x i16>*
  %277 = load <8 x i16>, <8 x i16>* %276, align 16
  %278 = getelementptr inbounds i16, i16* %260, i64 960
  %279 = bitcast i16* %278 to <8 x i16>*
  %280 = load <8 x i16>, <8 x i16>* %279, align 16
  %281 = getelementptr inbounds i16, i16* %260, i64 992
  %282 = bitcast i16* %281 to <8 x i16>*
  %283 = load <8 x i16>, <8 x i16>* %282, align 16
  %284 = add <8 x i16> %283, %262
  %285 = add <8 x i16> %280, %265
  %286 = add <8 x i16> %277, %268
  %287 = add <8 x i16> %274, %271
  %288 = sub <8 x i16> %271, %274
  %289 = sub <8 x i16> %268, %277
  %290 = sub <8 x i16> %265, %280
  %291 = sub <8 x i16> %262, %283
  %292 = getelementptr inbounds i16, i16* %260, i64 128
  %293 = bitcast i16* %292 to <8 x i16>*
  %294 = load <8 x i16>, <8 x i16>* %293, align 16
  %295 = getelementptr inbounds i16, i16* %260, i64 160
  %296 = bitcast i16* %295 to <8 x i16>*
  %297 = load <8 x i16>, <8 x i16>* %296, align 16
  %298 = getelementptr inbounds i16, i16* %260, i64 192
  %299 = bitcast i16* %298 to <8 x i16>*
  %300 = load <8 x i16>, <8 x i16>* %299, align 16
  %301 = getelementptr inbounds i16, i16* %260, i64 224
  %302 = bitcast i16* %301 to <8 x i16>*
  %303 = load <8 x i16>, <8 x i16>* %302, align 16
  %304 = getelementptr inbounds i16, i16* %260, i64 768
  %305 = bitcast i16* %304 to <8 x i16>*
  %306 = load <8 x i16>, <8 x i16>* %305, align 16
  %307 = getelementptr inbounds i16, i16* %260, i64 800
  %308 = bitcast i16* %307 to <8 x i16>*
  %309 = load <8 x i16>, <8 x i16>* %308, align 16
  %310 = getelementptr inbounds i16, i16* %260, i64 832
  %311 = bitcast i16* %310 to <8 x i16>*
  %312 = load <8 x i16>, <8 x i16>* %311, align 16
  %313 = getelementptr inbounds i16, i16* %260, i64 864
  %314 = bitcast i16* %313 to <8 x i16>*
  %315 = load <8 x i16>, <8 x i16>* %314, align 16
  %316 = add <8 x i16> %315, %294
  %317 = add <8 x i16> %312, %297
  %318 = add <8 x i16> %309, %300
  %319 = add <8 x i16> %306, %303
  %320 = sub <8 x i16> %303, %306
  %321 = sub <8 x i16> %300, %309
  %322 = sub <8 x i16> %297, %312
  %323 = sub <8 x i16> %294, %315
  %324 = getelementptr inbounds i16, i16* %260, i64 256
  %325 = bitcast i16* %324 to <8 x i16>*
  %326 = load <8 x i16>, <8 x i16>* %325, align 16
  %327 = getelementptr inbounds i16, i16* %260, i64 288
  %328 = bitcast i16* %327 to <8 x i16>*
  %329 = load <8 x i16>, <8 x i16>* %328, align 16
  %330 = getelementptr inbounds i16, i16* %260, i64 320
  %331 = bitcast i16* %330 to <8 x i16>*
  %332 = load <8 x i16>, <8 x i16>* %331, align 16
  %333 = getelementptr inbounds i16, i16* %260, i64 352
  %334 = bitcast i16* %333 to <8 x i16>*
  %335 = load <8 x i16>, <8 x i16>* %334, align 16
  %336 = getelementptr inbounds i16, i16* %260, i64 640
  %337 = bitcast i16* %336 to <8 x i16>*
  %338 = load <8 x i16>, <8 x i16>* %337, align 16
  %339 = getelementptr inbounds i16, i16* %260, i64 672
  %340 = bitcast i16* %339 to <8 x i16>*
  %341 = load <8 x i16>, <8 x i16>* %340, align 16
  %342 = getelementptr inbounds i16, i16* %260, i64 704
  %343 = bitcast i16* %342 to <8 x i16>*
  %344 = load <8 x i16>, <8 x i16>* %343, align 16
  %345 = getelementptr inbounds i16, i16* %260, i64 736
  %346 = bitcast i16* %345 to <8 x i16>*
  %347 = load <8 x i16>, <8 x i16>* %346, align 16
  %348 = add <8 x i16> %347, %326
  %349 = add <8 x i16> %344, %329
  %350 = add <8 x i16> %341, %332
  %351 = add <8 x i16> %338, %335
  %352 = sub <8 x i16> %335, %338
  %353 = sub <8 x i16> %332, %341
  %354 = sub <8 x i16> %329, %344
  %355 = sub <8 x i16> %326, %347
  %356 = getelementptr inbounds i16, i16* %260, i64 384
  %357 = bitcast i16* %356 to <8 x i16>*
  %358 = load <8 x i16>, <8 x i16>* %357, align 16
  %359 = getelementptr inbounds i16, i16* %260, i64 416
  %360 = bitcast i16* %359 to <8 x i16>*
  %361 = load <8 x i16>, <8 x i16>* %360, align 16
  %362 = getelementptr inbounds i16, i16* %260, i64 448
  %363 = bitcast i16* %362 to <8 x i16>*
  %364 = load <8 x i16>, <8 x i16>* %363, align 16
  %365 = getelementptr inbounds i16, i16* %260, i64 480
  %366 = bitcast i16* %365 to <8 x i16>*
  %367 = load <8 x i16>, <8 x i16>* %366, align 16
  %368 = getelementptr inbounds i16, i16* %260, i64 512
  %369 = bitcast i16* %368 to <8 x i16>*
  %370 = load <8 x i16>, <8 x i16>* %369, align 16
  %371 = getelementptr inbounds i16, i16* %260, i64 544
  %372 = bitcast i16* %371 to <8 x i16>*
  %373 = load <8 x i16>, <8 x i16>* %372, align 16
  %374 = getelementptr inbounds i16, i16* %260, i64 576
  %375 = bitcast i16* %374 to <8 x i16>*
  %376 = load <8 x i16>, <8 x i16>* %375, align 16
  %377 = getelementptr inbounds i16, i16* %260, i64 608
  %378 = bitcast i16* %377 to <8 x i16>*
  %379 = load <8 x i16>, <8 x i16>* %378, align 16
  %380 = add <8 x i16> %379, %358
  %381 = add <8 x i16> %376, %361
  %382 = add <8 x i16> %373, %364
  %383 = add <8 x i16> %370, %367
  %384 = sub <8 x i16> %367, %370
  %385 = sub <8 x i16> %364, %373
  %386 = sub <8 x i16> %361, %376
  %387 = sub <8 x i16> %358, %379
  br label %388

388:                                              ; preds = %259, %98
  %389 = phi <8 x i16> [ %138, %98 ], [ %291, %259 ]
  %390 = phi <8 x i16> [ %137, %98 ], [ %290, %259 ]
  %391 = phi <8 x i16> [ %136, %98 ], [ %289, %259 ]
  %392 = phi <8 x i16> [ %135, %98 ], [ %288, %259 ]
  %393 = phi <8 x i16> [ %178, %98 ], [ %323, %259 ]
  %394 = phi <8 x i16> [ %177, %98 ], [ %322, %259 ]
  %395 = phi <8 x i16> [ %176, %98 ], [ %321, %259 ]
  %396 = phi <8 x i16> [ %175, %98 ], [ %320, %259 ]
  %397 = phi <8 x i16> [ %218, %98 ], [ %355, %259 ]
  %398 = phi <8 x i16> [ %217, %98 ], [ %354, %259 ]
  %399 = phi <8 x i16> [ %216, %98 ], [ %353, %259 ]
  %400 = phi <8 x i16> [ %215, %98 ], [ %352, %259 ]
  %401 = phi <8 x i16> [ %258, %98 ], [ %387, %259 ]
  %402 = phi <8 x i16> [ %257, %98 ], [ %386, %259 ]
  %403 = phi <8 x i16> [ %256, %98 ], [ %385, %259 ]
  %404 = phi <8 x i16> [ %255, %98 ], [ %384, %259 ]
  %405 = phi <8 x i16> [ %254, %98 ], [ %383, %259 ]
  %406 = phi <8 x i16> [ %253, %98 ], [ %382, %259 ]
  %407 = phi <8 x i16> [ %252, %98 ], [ %381, %259 ]
  %408 = phi <8 x i16> [ %251, %98 ], [ %380, %259 ]
  %409 = phi <8 x i16> [ %214, %98 ], [ %351, %259 ]
  %410 = phi <8 x i16> [ %213, %98 ], [ %350, %259 ]
  %411 = phi <8 x i16> [ %212, %98 ], [ %349, %259 ]
  %412 = phi <8 x i16> [ %211, %98 ], [ %348, %259 ]
  %413 = phi <8 x i16> [ %174, %98 ], [ %319, %259 ]
  %414 = phi <8 x i16> [ %173, %98 ], [ %318, %259 ]
  %415 = phi <8 x i16> [ %172, %98 ], [ %317, %259 ]
  %416 = phi <8 x i16> [ %171, %98 ], [ %316, %259 ]
  %417 = phi <8 x i16> [ %134, %98 ], [ %287, %259 ]
  %418 = phi <8 x i16> [ %133, %98 ], [ %286, %259 ]
  %419 = phi <8 x i16> [ %132, %98 ], [ %285, %259 ]
  %420 = phi <8 x i16> [ %131, %98 ], [ %284, %259 ]
  %421 = add <8 x i16> %420, %405
  %422 = add <8 x i16> %419, %406
  %423 = add <8 x i16> %418, %407
  %424 = add <8 x i16> %417, %408
  %425 = add <8 x i16> %416, %409
  %426 = add <8 x i16> %415, %410
  %427 = add <8 x i16> %414, %411
  %428 = add <8 x i16> %413, %412
  %429 = sub <8 x i16> %413, %412
  %430 = sub <8 x i16> %414, %411
  %431 = sub <8 x i16> %415, %410
  %432 = sub <8 x i16> %416, %409
  %433 = sub <8 x i16> %417, %408
  %434 = sub <8 x i16> %418, %407
  %435 = sub <8 x i16> %419, %406
  %436 = sub <8 x i16> %420, %405
  %437 = shufflevector <8 x i16> %393, <8 x i16> %400, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %438 = shufflevector <8 x i16> %393, <8 x i16> %400, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %439 = shufflevector <8 x i16> %394, <8 x i16> %399, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %440 = shufflevector <8 x i16> %394, <8 x i16> %399, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %441 = shufflevector <8 x i16> %395, <8 x i16> %398, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %442 = shufflevector <8 x i16> %395, <8 x i16> %398, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %443 = shufflevector <8 x i16> %396, <8 x i16> %397, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %444 = shufflevector <8 x i16> %396, <8 x i16> %397, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %445 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %437, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %446 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %438, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %447 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %439, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %448 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %440, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %449 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %441, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %450 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %442, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %451 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %443, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %452 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %444, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %453 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %443, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %454 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %444, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %455 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %441, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %456 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %442, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %457 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %439, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %458 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %440, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %459 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %437, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %460 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %438, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %461 = add <4 x i32> %445, <i32 8192, i32 8192, i32 8192, i32 8192>
  %462 = add <4 x i32> %446, <i32 8192, i32 8192, i32 8192, i32 8192>
  %463 = add <4 x i32> %447, <i32 8192, i32 8192, i32 8192, i32 8192>
  %464 = add <4 x i32> %448, <i32 8192, i32 8192, i32 8192, i32 8192>
  %465 = add <4 x i32> %449, <i32 8192, i32 8192, i32 8192, i32 8192>
  %466 = add <4 x i32> %450, <i32 8192, i32 8192, i32 8192, i32 8192>
  %467 = add <4 x i32> %451, <i32 8192, i32 8192, i32 8192, i32 8192>
  %468 = add <4 x i32> %452, <i32 8192, i32 8192, i32 8192, i32 8192>
  %469 = add <4 x i32> %453, <i32 8192, i32 8192, i32 8192, i32 8192>
  %470 = add <4 x i32> %454, <i32 8192, i32 8192, i32 8192, i32 8192>
  %471 = add <4 x i32> %455, <i32 8192, i32 8192, i32 8192, i32 8192>
  %472 = add <4 x i32> %456, <i32 8192, i32 8192, i32 8192, i32 8192>
  %473 = add <4 x i32> %457, <i32 8192, i32 8192, i32 8192, i32 8192>
  %474 = add <4 x i32> %458, <i32 8192, i32 8192, i32 8192, i32 8192>
  %475 = add <4 x i32> %459, <i32 8192, i32 8192, i32 8192, i32 8192>
  %476 = add <4 x i32> %460, <i32 8192, i32 8192, i32 8192, i32 8192>
  %477 = ashr <4 x i32> %461, <i32 14, i32 14, i32 14, i32 14>
  %478 = ashr <4 x i32> %462, <i32 14, i32 14, i32 14, i32 14>
  %479 = ashr <4 x i32> %463, <i32 14, i32 14, i32 14, i32 14>
  %480 = ashr <4 x i32> %464, <i32 14, i32 14, i32 14, i32 14>
  %481 = ashr <4 x i32> %465, <i32 14, i32 14, i32 14, i32 14>
  %482 = ashr <4 x i32> %466, <i32 14, i32 14, i32 14, i32 14>
  %483 = ashr <4 x i32> %467, <i32 14, i32 14, i32 14, i32 14>
  %484 = ashr <4 x i32> %468, <i32 14, i32 14, i32 14, i32 14>
  %485 = ashr <4 x i32> %469, <i32 14, i32 14, i32 14, i32 14>
  %486 = ashr <4 x i32> %470, <i32 14, i32 14, i32 14, i32 14>
  %487 = ashr <4 x i32> %471, <i32 14, i32 14, i32 14, i32 14>
  %488 = ashr <4 x i32> %472, <i32 14, i32 14, i32 14, i32 14>
  %489 = ashr <4 x i32> %473, <i32 14, i32 14, i32 14, i32 14>
  %490 = ashr <4 x i32> %474, <i32 14, i32 14, i32 14, i32 14>
  %491 = ashr <4 x i32> %475, <i32 14, i32 14, i32 14, i32 14>
  %492 = ashr <4 x i32> %476, <i32 14, i32 14, i32 14, i32 14>
  %493 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %477, <4 x i32> %478) #6
  %494 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %479, <4 x i32> %480) #6
  %495 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %481, <4 x i32> %482) #6
  %496 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %483, <4 x i32> %484) #6
  %497 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %485, <4 x i32> %486) #6
  %498 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %487, <4 x i32> %488) #6
  %499 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %489, <4 x i32> %490) #6
  %500 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %491, <4 x i32> %492) #6
  br i1 %95, label %501, label %1022

501:                                              ; preds = %388
  %502 = add <8 x i16> %421, %428
  %503 = add <8 x i16> %422, %427
  %504 = add <8 x i16> %423, %426
  %505 = add <8 x i16> %424, %425
  %506 = sub <8 x i16> %424, %425
  %507 = sub <8 x i16> %423, %426
  %508 = sub <8 x i16> %422, %427
  %509 = sub <8 x i16> %421, %428
  %510 = shufflevector <8 x i16> %434, <8 x i16> %431, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %511 = shufflevector <8 x i16> %434, <8 x i16> %431, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %512 = shufflevector <8 x i16> %433, <8 x i16> %432, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %513 = shufflevector <8 x i16> %433, <8 x i16> %432, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %514 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %510, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %515 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %511, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %516 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %512, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %517 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %513, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %518 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %512, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %519 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %513, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %520 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %510, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %521 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %511, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %522 = add <4 x i32> %514, <i32 8192, i32 8192, i32 8192, i32 8192>
  %523 = add <4 x i32> %515, <i32 8192, i32 8192, i32 8192, i32 8192>
  %524 = add <4 x i32> %516, <i32 8192, i32 8192, i32 8192, i32 8192>
  %525 = add <4 x i32> %517, <i32 8192, i32 8192, i32 8192, i32 8192>
  %526 = add <4 x i32> %518, <i32 8192, i32 8192, i32 8192, i32 8192>
  %527 = add <4 x i32> %519, <i32 8192, i32 8192, i32 8192, i32 8192>
  %528 = add <4 x i32> %520, <i32 8192, i32 8192, i32 8192, i32 8192>
  %529 = add <4 x i32> %521, <i32 8192, i32 8192, i32 8192, i32 8192>
  %530 = ashr <4 x i32> %522, <i32 14, i32 14, i32 14, i32 14>
  %531 = ashr <4 x i32> %523, <i32 14, i32 14, i32 14, i32 14>
  %532 = ashr <4 x i32> %524, <i32 14, i32 14, i32 14, i32 14>
  %533 = ashr <4 x i32> %525, <i32 14, i32 14, i32 14, i32 14>
  %534 = ashr <4 x i32> %526, <i32 14, i32 14, i32 14, i32 14>
  %535 = ashr <4 x i32> %527, <i32 14, i32 14, i32 14, i32 14>
  %536 = ashr <4 x i32> %528, <i32 14, i32 14, i32 14, i32 14>
  %537 = ashr <4 x i32> %529, <i32 14, i32 14, i32 14, i32 14>
  %538 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %530, <4 x i32> %531) #6
  %539 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %532, <4 x i32> %533) #6
  %540 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %534, <4 x i32> %535) #6
  %541 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %536, <4 x i32> %537) #6
  %542 = add <8 x i16> %496, %404
  %543 = add <8 x i16> %495, %403
  %544 = add <8 x i16> %494, %402
  %545 = add <8 x i16> %493, %401
  %546 = sub <8 x i16> %401, %493
  %547 = sub <8 x i16> %402, %494
  %548 = sub <8 x i16> %403, %495
  %549 = sub <8 x i16> %404, %496
  %550 = sub <8 x i16> %389, %497
  %551 = sub <8 x i16> %390, %498
  %552 = sub <8 x i16> %391, %499
  %553 = sub <8 x i16> %392, %500
  %554 = add <8 x i16> %500, %392
  %555 = add <8 x i16> %499, %391
  %556 = add <8 x i16> %498, %390
  %557 = add <8 x i16> %497, %389
  %558 = add <8 x i16> %502, %505
  %559 = add <8 x i16> %503, %504
  %560 = sub <8 x i16> %503, %504
  %561 = sub <8 x i16> %502, %505
  %562 = add <8 x i16> %539, %429
  %563 = add <8 x i16> %538, %430
  %564 = sub <8 x i16> %430, %538
  %565 = sub <8 x i16> %429, %539
  %566 = sub <8 x i16> %436, %540
  %567 = sub <8 x i16> %435, %541
  %568 = add <8 x i16> %541, %435
  %569 = add <8 x i16> %540, %436
  %570 = shufflevector <8 x i16> %508, <8 x i16> %507, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %571 = shufflevector <8 x i16> %508, <8 x i16> %507, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %572 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %570, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %573 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %571, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %574 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %570, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %575 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %571, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %576 = add <4 x i32> %572, <i32 8192, i32 8192, i32 8192, i32 8192>
  %577 = add <4 x i32> %573, <i32 8192, i32 8192, i32 8192, i32 8192>
  %578 = add <4 x i32> %574, <i32 8192, i32 8192, i32 8192, i32 8192>
  %579 = add <4 x i32> %575, <i32 8192, i32 8192, i32 8192, i32 8192>
  %580 = ashr <4 x i32> %576, <i32 14, i32 14, i32 14, i32 14>
  %581 = ashr <4 x i32> %577, <i32 14, i32 14, i32 14, i32 14>
  %582 = ashr <4 x i32> %578, <i32 14, i32 14, i32 14, i32 14>
  %583 = ashr <4 x i32> %579, <i32 14, i32 14, i32 14, i32 14>
  %584 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %580, <4 x i32> %581) #6
  %585 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %582, <4 x i32> %583) #6
  %586 = shufflevector <8 x i16> %544, <8 x i16> %555, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %587 = shufflevector <8 x i16> %544, <8 x i16> %555, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %588 = shufflevector <8 x i16> %545, <8 x i16> %554, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %589 = shufflevector <8 x i16> %545, <8 x i16> %554, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %590 = shufflevector <8 x i16> %546, <8 x i16> %553, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %591 = shufflevector <8 x i16> %546, <8 x i16> %553, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %592 = shufflevector <8 x i16> %547, <8 x i16> %552, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %593 = shufflevector <8 x i16> %547, <8 x i16> %552, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %594 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %586, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %595 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %587, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %596 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %588, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %597 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %589, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %598 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %590, <8 x i16> <i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137>) #6
  %599 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %591, <8 x i16> <i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137>) #6
  %600 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %592, <8 x i16> <i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137>) #6
  %601 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %593, <8 x i16> <i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137>) #6
  %602 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %592, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %603 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %593, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %604 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %590, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %605 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %591, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %606 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %588, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %607 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %589, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %608 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %586, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %609 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %587, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %610 = add <4 x i32> %594, <i32 8192, i32 8192, i32 8192, i32 8192>
  %611 = add <4 x i32> %595, <i32 8192, i32 8192, i32 8192, i32 8192>
  %612 = add <4 x i32> %596, <i32 8192, i32 8192, i32 8192, i32 8192>
  %613 = add <4 x i32> %597, <i32 8192, i32 8192, i32 8192, i32 8192>
  %614 = add <4 x i32> %598, <i32 8192, i32 8192, i32 8192, i32 8192>
  %615 = add <4 x i32> %599, <i32 8192, i32 8192, i32 8192, i32 8192>
  %616 = add <4 x i32> %600, <i32 8192, i32 8192, i32 8192, i32 8192>
  %617 = add <4 x i32> %601, <i32 8192, i32 8192, i32 8192, i32 8192>
  %618 = add <4 x i32> %602, <i32 8192, i32 8192, i32 8192, i32 8192>
  %619 = add <4 x i32> %603, <i32 8192, i32 8192, i32 8192, i32 8192>
  %620 = add <4 x i32> %604, <i32 8192, i32 8192, i32 8192, i32 8192>
  %621 = add <4 x i32> %605, <i32 8192, i32 8192, i32 8192, i32 8192>
  %622 = add <4 x i32> %606, <i32 8192, i32 8192, i32 8192, i32 8192>
  %623 = add <4 x i32> %607, <i32 8192, i32 8192, i32 8192, i32 8192>
  %624 = add <4 x i32> %608, <i32 8192, i32 8192, i32 8192, i32 8192>
  %625 = add <4 x i32> %609, <i32 8192, i32 8192, i32 8192, i32 8192>
  %626 = ashr <4 x i32> %610, <i32 14, i32 14, i32 14, i32 14>
  %627 = ashr <4 x i32> %611, <i32 14, i32 14, i32 14, i32 14>
  %628 = ashr <4 x i32> %612, <i32 14, i32 14, i32 14, i32 14>
  %629 = ashr <4 x i32> %613, <i32 14, i32 14, i32 14, i32 14>
  %630 = ashr <4 x i32> %614, <i32 14, i32 14, i32 14, i32 14>
  %631 = ashr <4 x i32> %615, <i32 14, i32 14, i32 14, i32 14>
  %632 = ashr <4 x i32> %616, <i32 14, i32 14, i32 14, i32 14>
  %633 = ashr <4 x i32> %617, <i32 14, i32 14, i32 14, i32 14>
  %634 = ashr <4 x i32> %618, <i32 14, i32 14, i32 14, i32 14>
  %635 = ashr <4 x i32> %619, <i32 14, i32 14, i32 14, i32 14>
  %636 = ashr <4 x i32> %620, <i32 14, i32 14, i32 14, i32 14>
  %637 = ashr <4 x i32> %621, <i32 14, i32 14, i32 14, i32 14>
  %638 = ashr <4 x i32> %622, <i32 14, i32 14, i32 14, i32 14>
  %639 = ashr <4 x i32> %623, <i32 14, i32 14, i32 14, i32 14>
  %640 = ashr <4 x i32> %624, <i32 14, i32 14, i32 14, i32 14>
  %641 = ashr <4 x i32> %625, <i32 14, i32 14, i32 14, i32 14>
  %642 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %626, <4 x i32> %627) #6
  %643 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %628, <4 x i32> %629) #6
  %644 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %630, <4 x i32> %631) #6
  %645 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %632, <4 x i32> %633) #6
  %646 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %634, <4 x i32> %635) #6
  %647 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %636, <4 x i32> %637) #6
  %648 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %638, <4 x i32> %639) #6
  %649 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %640, <4 x i32> %641) #6
  %650 = add <8 x i16> %584, %506
  %651 = sub <8 x i16> %506, %584
  %652 = sub <8 x i16> %509, %585
  %653 = add <8 x i16> %585, %509
  %654 = shufflevector <8 x i16> %558, <8 x i16> %559, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %655 = shufflevector <8 x i16> %558, <8 x i16> %559, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %656 = shufflevector <8 x i16> %560, <8 x i16> %561, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %657 = shufflevector <8 x i16> %560, <8 x i16> %561, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %658 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %654, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %659 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %655, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %660 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %654, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %661 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %655, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %662 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %656, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %663 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %657, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %664 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %656, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %665 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %657, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %666 = add <4 x i32> %658, <i32 8192, i32 8192, i32 8192, i32 8192>
  %667 = add <4 x i32> %659, <i32 8192, i32 8192, i32 8192, i32 8192>
  %668 = add <4 x i32> %660, <i32 8192, i32 8192, i32 8192, i32 8192>
  %669 = add <4 x i32> %661, <i32 8192, i32 8192, i32 8192, i32 8192>
  %670 = add <4 x i32> %662, <i32 8192, i32 8192, i32 8192, i32 8192>
  %671 = add <4 x i32> %663, <i32 8192, i32 8192, i32 8192, i32 8192>
  %672 = add <4 x i32> %664, <i32 8192, i32 8192, i32 8192, i32 8192>
  %673 = add <4 x i32> %665, <i32 8192, i32 8192, i32 8192, i32 8192>
  %674 = ashr <4 x i32> %666, <i32 14, i32 14, i32 14, i32 14>
  %675 = ashr <4 x i32> %667, <i32 14, i32 14, i32 14, i32 14>
  %676 = ashr <4 x i32> %668, <i32 14, i32 14, i32 14, i32 14>
  %677 = ashr <4 x i32> %669, <i32 14, i32 14, i32 14, i32 14>
  %678 = ashr <4 x i32> %670, <i32 14, i32 14, i32 14, i32 14>
  %679 = ashr <4 x i32> %671, <i32 14, i32 14, i32 14, i32 14>
  %680 = ashr <4 x i32> %672, <i32 14, i32 14, i32 14, i32 14>
  %681 = ashr <4 x i32> %673, <i32 14, i32 14, i32 14, i32 14>
  %682 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %674, <4 x i32> %675) #6
  store <8 x i16> %682, <8 x i16>* %30, align 16
  %683 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %676, <4 x i32> %677) #6
  store <8 x i16> %683, <8 x i16>* %32, align 16
  %684 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %678, <4 x i32> %679) #6
  store <8 x i16> %684, <8 x i16>* %34, align 16
  %685 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %680, <4 x i32> %681) #6
  store <8 x i16> %685, <8 x i16>* %36, align 16
  %686 = shufflevector <8 x i16> %563, <8 x i16> %568, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %687 = shufflevector <8 x i16> %563, <8 x i16> %568, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %688 = shufflevector <8 x i16> %564, <8 x i16> %567, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %689 = shufflevector <8 x i16> %564, <8 x i16> %567, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %690 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %686, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %691 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %687, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %692 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %688, <8 x i16> <i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137>) #6
  %693 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %689, <8 x i16> <i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137>) #6
  %694 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %688, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %695 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %689, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %696 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %686, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %697 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %687, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %698 = add <4 x i32> %690, <i32 8192, i32 8192, i32 8192, i32 8192>
  %699 = add <4 x i32> %691, <i32 8192, i32 8192, i32 8192, i32 8192>
  %700 = add <4 x i32> %692, <i32 8192, i32 8192, i32 8192, i32 8192>
  %701 = add <4 x i32> %693, <i32 8192, i32 8192, i32 8192, i32 8192>
  %702 = add <4 x i32> %694, <i32 8192, i32 8192, i32 8192, i32 8192>
  %703 = add <4 x i32> %695, <i32 8192, i32 8192, i32 8192, i32 8192>
  %704 = add <4 x i32> %696, <i32 8192, i32 8192, i32 8192, i32 8192>
  %705 = add <4 x i32> %697, <i32 8192, i32 8192, i32 8192, i32 8192>
  %706 = ashr <4 x i32> %698, <i32 14, i32 14, i32 14, i32 14>
  %707 = ashr <4 x i32> %699, <i32 14, i32 14, i32 14, i32 14>
  %708 = ashr <4 x i32> %700, <i32 14, i32 14, i32 14, i32 14>
  %709 = ashr <4 x i32> %701, <i32 14, i32 14, i32 14, i32 14>
  %710 = ashr <4 x i32> %702, <i32 14, i32 14, i32 14, i32 14>
  %711 = ashr <4 x i32> %703, <i32 14, i32 14, i32 14, i32 14>
  %712 = ashr <4 x i32> %704, <i32 14, i32 14, i32 14, i32 14>
  %713 = ashr <4 x i32> %705, <i32 14, i32 14, i32 14, i32 14>
  %714 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %706, <4 x i32> %707) #6
  %715 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %708, <4 x i32> %709) #6
  %716 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %710, <4 x i32> %711) #6
  %717 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %712, <4 x i32> %713) #6
  %718 = add <8 x i16> %643, %542
  %719 = add <8 x i16> %642, %543
  %720 = sub <8 x i16> %543, %642
  %721 = sub <8 x i16> %542, %643
  %722 = sub <8 x i16> %549, %644
  %723 = sub <8 x i16> %548, %645
  %724 = add <8 x i16> %645, %548
  %725 = add <8 x i16> %644, %549
  %726 = add <8 x i16> %647, %550
  %727 = add <8 x i16> %646, %551
  %728 = sub <8 x i16> %551, %646
  %729 = sub <8 x i16> %550, %647
  %730 = sub <8 x i16> %557, %648
  %731 = sub <8 x i16> %556, %649
  %732 = add <8 x i16> %649, %556
  %733 = add <8 x i16> %648, %557
  %734 = shufflevector <8 x i16> %650, <8 x i16> %653, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %735 = shufflevector <8 x i16> %650, <8 x i16> %653, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %736 = shufflevector <8 x i16> %651, <8 x i16> %652, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %737 = shufflevector <8 x i16> %651, <8 x i16> %652, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %738 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %734, <8 x i16> <i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069>) #6
  %739 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %735, <8 x i16> <i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069>) #6
  %740 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %736, <8 x i16> <i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102>) #6
  %741 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %737, <8 x i16> <i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102>) #6
  %742 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %736, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %743 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %737, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %744 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %734, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %745 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %735, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %746 = add <4 x i32> %738, <i32 8192, i32 8192, i32 8192, i32 8192>
  %747 = add <4 x i32> %739, <i32 8192, i32 8192, i32 8192, i32 8192>
  %748 = add <4 x i32> %740, <i32 8192, i32 8192, i32 8192, i32 8192>
  %749 = add <4 x i32> %741, <i32 8192, i32 8192, i32 8192, i32 8192>
  %750 = add <4 x i32> %742, <i32 8192, i32 8192, i32 8192, i32 8192>
  %751 = add <4 x i32> %743, <i32 8192, i32 8192, i32 8192, i32 8192>
  %752 = add <4 x i32> %744, <i32 8192, i32 8192, i32 8192, i32 8192>
  %753 = add <4 x i32> %745, <i32 8192, i32 8192, i32 8192, i32 8192>
  %754 = ashr <4 x i32> %746, <i32 14, i32 14, i32 14, i32 14>
  %755 = ashr <4 x i32> %747, <i32 14, i32 14, i32 14, i32 14>
  %756 = ashr <4 x i32> %748, <i32 14, i32 14, i32 14, i32 14>
  %757 = ashr <4 x i32> %749, <i32 14, i32 14, i32 14, i32 14>
  %758 = ashr <4 x i32> %750, <i32 14, i32 14, i32 14, i32 14>
  %759 = ashr <4 x i32> %751, <i32 14, i32 14, i32 14, i32 14>
  %760 = ashr <4 x i32> %752, <i32 14, i32 14, i32 14, i32 14>
  %761 = ashr <4 x i32> %753, <i32 14, i32 14, i32 14, i32 14>
  %762 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %754, <4 x i32> %755) #6
  store <8 x i16> %762, <8 x i16>* %38, align 16
  %763 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %756, <4 x i32> %757) #6
  store <8 x i16> %763, <8 x i16>* %40, align 16
  %764 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %758, <4 x i32> %759) #6
  store <8 x i16> %764, <8 x i16>* %42, align 16
  %765 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %760, <4 x i32> %761) #6
  store <8 x i16> %765, <8 x i16>* %44, align 16
  %766 = add <8 x i16> %714, %562
  %767 = sub <8 x i16> %562, %714
  %768 = sub <8 x i16> %565, %715
  %769 = add <8 x i16> %715, %565
  %770 = add <8 x i16> %716, %566
  %771 = sub <8 x i16> %566, %716
  %772 = sub <8 x i16> %569, %717
  %773 = add <8 x i16> %717, %569
  %774 = shufflevector <8 x i16> %719, <8 x i16> %732, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %775 = shufflevector <8 x i16> %719, <8 x i16> %732, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %776 = shufflevector <8 x i16> %720, <8 x i16> %731, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %777 = shufflevector <8 x i16> %720, <8 x i16> %731, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %778 = shufflevector <8 x i16> %723, <8 x i16> %728, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %779 = shufflevector <8 x i16> %723, <8 x i16> %728, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %780 = shufflevector <8 x i16> %724, <8 x i16> %727, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %781 = shufflevector <8 x i16> %724, <8 x i16> %727, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %782 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %774, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %783 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %775, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %784 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %776, <8 x i16> <i16 -3196, i16 -16069, i16 -3196, i16 -16069, i16 -3196, i16 -16069, i16 -3196, i16 -16069>) #6
  %785 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %777, <8 x i16> <i16 -3196, i16 -16069, i16 -3196, i16 -16069, i16 -3196, i16 -16069, i16 -3196, i16 -16069>) #6
  %786 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %778, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %787 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %779, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %788 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %780, <8 x i16> <i16 -13623, i16 -9102, i16 -13623, i16 -9102, i16 -13623, i16 -9102, i16 -13623, i16 -9102>) #6
  %789 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %781, <8 x i16> <i16 -13623, i16 -9102, i16 -13623, i16 -9102, i16 -13623, i16 -9102, i16 -13623, i16 -9102>) #6
  %790 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %780, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %791 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %781, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %792 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %778, <8 x i16> <i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102>) #6
  %793 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %779, <8 x i16> <i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102>) #6
  %794 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %776, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %795 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %777, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %796 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %774, <8 x i16> <i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069>) #6
  %797 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %775, <8 x i16> <i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069>) #6
  %798 = add <4 x i32> %782, <i32 8192, i32 8192, i32 8192, i32 8192>
  %799 = add <4 x i32> %783, <i32 8192, i32 8192, i32 8192, i32 8192>
  %800 = add <4 x i32> %784, <i32 8192, i32 8192, i32 8192, i32 8192>
  %801 = add <4 x i32> %785, <i32 8192, i32 8192, i32 8192, i32 8192>
  %802 = add <4 x i32> %786, <i32 8192, i32 8192, i32 8192, i32 8192>
  %803 = add <4 x i32> %787, <i32 8192, i32 8192, i32 8192, i32 8192>
  %804 = add <4 x i32> %788, <i32 8192, i32 8192, i32 8192, i32 8192>
  %805 = add <4 x i32> %789, <i32 8192, i32 8192, i32 8192, i32 8192>
  %806 = ashr <4 x i32> %798, <i32 14, i32 14, i32 14, i32 14>
  %807 = ashr <4 x i32> %799, <i32 14, i32 14, i32 14, i32 14>
  %808 = ashr <4 x i32> %800, <i32 14, i32 14, i32 14, i32 14>
  %809 = ashr <4 x i32> %801, <i32 14, i32 14, i32 14, i32 14>
  %810 = ashr <4 x i32> %802, <i32 14, i32 14, i32 14, i32 14>
  %811 = ashr <4 x i32> %803, <i32 14, i32 14, i32 14, i32 14>
  %812 = ashr <4 x i32> %804, <i32 14, i32 14, i32 14, i32 14>
  %813 = ashr <4 x i32> %805, <i32 14, i32 14, i32 14, i32 14>
  %814 = add <4 x i32> %790, <i32 8192, i32 8192, i32 8192, i32 8192>
  %815 = add <4 x i32> %791, <i32 8192, i32 8192, i32 8192, i32 8192>
  %816 = add <4 x i32> %792, <i32 8192, i32 8192, i32 8192, i32 8192>
  %817 = add <4 x i32> %793, <i32 8192, i32 8192, i32 8192, i32 8192>
  %818 = add <4 x i32> %794, <i32 8192, i32 8192, i32 8192, i32 8192>
  %819 = add <4 x i32> %795, <i32 8192, i32 8192, i32 8192, i32 8192>
  %820 = add <4 x i32> %796, <i32 8192, i32 8192, i32 8192, i32 8192>
  %821 = add <4 x i32> %797, <i32 8192, i32 8192, i32 8192, i32 8192>
  %822 = ashr <4 x i32> %814, <i32 14, i32 14, i32 14, i32 14>
  %823 = ashr <4 x i32> %815, <i32 14, i32 14, i32 14, i32 14>
  %824 = ashr <4 x i32> %816, <i32 14, i32 14, i32 14, i32 14>
  %825 = ashr <4 x i32> %817, <i32 14, i32 14, i32 14, i32 14>
  %826 = ashr <4 x i32> %818, <i32 14, i32 14, i32 14, i32 14>
  %827 = ashr <4 x i32> %819, <i32 14, i32 14, i32 14, i32 14>
  %828 = ashr <4 x i32> %820, <i32 14, i32 14, i32 14, i32 14>
  %829 = ashr <4 x i32> %821, <i32 14, i32 14, i32 14, i32 14>
  %830 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %806, <4 x i32> %807) #6
  %831 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %808, <4 x i32> %809) #6
  %832 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %810, <4 x i32> %811) #6
  %833 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %812, <4 x i32> %813) #6
  %834 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %822, <4 x i32> %823) #6
  %835 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %824, <4 x i32> %825) #6
  %836 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %826, <4 x i32> %827) #6
  %837 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %828, <4 x i32> %829) #6
  %838 = shufflevector <8 x i16> %766, <8 x i16> %773, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %839 = shufflevector <8 x i16> %766, <8 x i16> %773, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %840 = shufflevector <8 x i16> %767, <8 x i16> %772, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %841 = shufflevector <8 x i16> %767, <8 x i16> %772, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %842 = shufflevector <8 x i16> %768, <8 x i16> %771, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %843 = shufflevector <8 x i16> %768, <8 x i16> %771, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %844 = shufflevector <8 x i16> %769, <8 x i16> %770, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %845 = shufflevector <8 x i16> %769, <8 x i16> %770, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %846 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %838, <8 x i16> <i16 1606, i16 16305, i16 1606, i16 16305, i16 1606, i16 16305, i16 1606, i16 16305>) #6
  %847 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %839, <8 x i16> <i16 1606, i16 16305, i16 1606, i16 16305, i16 1606, i16 16305, i16 1606, i16 16305>) #6
  %848 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %840, <8 x i16> <i16 12665, i16 10394, i16 12665, i16 10394, i16 12665, i16 10394, i16 12665, i16 10394>) #6
  %849 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %841, <8 x i16> <i16 12665, i16 10394, i16 12665, i16 10394, i16 12665, i16 10394, i16 12665, i16 10394>) #6
  %850 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %842, <8 x i16> <i16 7723, i16 14449, i16 7723, i16 14449, i16 7723, i16 14449, i16 7723, i16 14449>) #6
  %851 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %843, <8 x i16> <i16 7723, i16 14449, i16 7723, i16 14449, i16 7723, i16 14449, i16 7723, i16 14449>) #6
  %852 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %844, <8 x i16> <i16 15679, i16 4756, i16 15679, i16 4756, i16 15679, i16 4756, i16 15679, i16 4756>) #6
  %853 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %845, <8 x i16> <i16 15679, i16 4756, i16 15679, i16 4756, i16 15679, i16 4756, i16 15679, i16 4756>) #6
  %854 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %844, <8 x i16> <i16 -4756, i16 15679, i16 -4756, i16 15679, i16 -4756, i16 15679, i16 -4756, i16 15679>) #6
  %855 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %845, <8 x i16> <i16 -4756, i16 15679, i16 -4756, i16 15679, i16 -4756, i16 15679, i16 -4756, i16 15679>) #6
  %856 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %842, <8 x i16> <i16 -14449, i16 7723, i16 -14449, i16 7723, i16 -14449, i16 7723, i16 -14449, i16 7723>) #6
  %857 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %843, <8 x i16> <i16 -14449, i16 7723, i16 -14449, i16 7723, i16 -14449, i16 7723, i16 -14449, i16 7723>) #6
  %858 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %840, <8 x i16> <i16 -10394, i16 12665, i16 -10394, i16 12665, i16 -10394, i16 12665, i16 -10394, i16 12665>) #6
  %859 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %841, <8 x i16> <i16 -10394, i16 12665, i16 -10394, i16 12665, i16 -10394, i16 12665, i16 -10394, i16 12665>) #6
  %860 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %838, <8 x i16> <i16 -16305, i16 1606, i16 -16305, i16 1606, i16 -16305, i16 1606, i16 -16305, i16 1606>) #6
  %861 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %839, <8 x i16> <i16 -16305, i16 1606, i16 -16305, i16 1606, i16 -16305, i16 1606, i16 -16305, i16 1606>) #6
  %862 = add <4 x i32> %846, <i32 8192, i32 8192, i32 8192, i32 8192>
  %863 = add <4 x i32> %847, <i32 8192, i32 8192, i32 8192, i32 8192>
  %864 = add <4 x i32> %848, <i32 8192, i32 8192, i32 8192, i32 8192>
  %865 = add <4 x i32> %849, <i32 8192, i32 8192, i32 8192, i32 8192>
  %866 = add <4 x i32> %850, <i32 8192, i32 8192, i32 8192, i32 8192>
  %867 = add <4 x i32> %851, <i32 8192, i32 8192, i32 8192, i32 8192>
  %868 = add <4 x i32> %852, <i32 8192, i32 8192, i32 8192, i32 8192>
  %869 = add <4 x i32> %853, <i32 8192, i32 8192, i32 8192, i32 8192>
  %870 = add <4 x i32> %854, <i32 8192, i32 8192, i32 8192, i32 8192>
  %871 = add <4 x i32> %855, <i32 8192, i32 8192, i32 8192, i32 8192>
  %872 = add <4 x i32> %856, <i32 8192, i32 8192, i32 8192, i32 8192>
  %873 = add <4 x i32> %857, <i32 8192, i32 8192, i32 8192, i32 8192>
  %874 = add <4 x i32> %858, <i32 8192, i32 8192, i32 8192, i32 8192>
  %875 = add <4 x i32> %859, <i32 8192, i32 8192, i32 8192, i32 8192>
  %876 = add <4 x i32> %860, <i32 8192, i32 8192, i32 8192, i32 8192>
  %877 = add <4 x i32> %861, <i32 8192, i32 8192, i32 8192, i32 8192>
  %878 = ashr <4 x i32> %862, <i32 14, i32 14, i32 14, i32 14>
  %879 = ashr <4 x i32> %863, <i32 14, i32 14, i32 14, i32 14>
  %880 = ashr <4 x i32> %864, <i32 14, i32 14, i32 14, i32 14>
  %881 = ashr <4 x i32> %865, <i32 14, i32 14, i32 14, i32 14>
  %882 = ashr <4 x i32> %866, <i32 14, i32 14, i32 14, i32 14>
  %883 = ashr <4 x i32> %867, <i32 14, i32 14, i32 14, i32 14>
  %884 = ashr <4 x i32> %868, <i32 14, i32 14, i32 14, i32 14>
  %885 = ashr <4 x i32> %869, <i32 14, i32 14, i32 14, i32 14>
  %886 = ashr <4 x i32> %870, <i32 14, i32 14, i32 14, i32 14>
  %887 = ashr <4 x i32> %871, <i32 14, i32 14, i32 14, i32 14>
  %888 = ashr <4 x i32> %872, <i32 14, i32 14, i32 14, i32 14>
  %889 = ashr <4 x i32> %873, <i32 14, i32 14, i32 14, i32 14>
  %890 = ashr <4 x i32> %874, <i32 14, i32 14, i32 14, i32 14>
  %891 = ashr <4 x i32> %875, <i32 14, i32 14, i32 14, i32 14>
  %892 = ashr <4 x i32> %876, <i32 14, i32 14, i32 14, i32 14>
  %893 = ashr <4 x i32> %877, <i32 14, i32 14, i32 14, i32 14>
  %894 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %878, <4 x i32> %879) #6
  store <8 x i16> %894, <8 x i16>* %46, align 16
  %895 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %880, <4 x i32> %881) #6
  store <8 x i16> %895, <8 x i16>* %48, align 16
  %896 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %882, <4 x i32> %883) #6
  store <8 x i16> %896, <8 x i16>* %50, align 16
  %897 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %884, <4 x i32> %885) #6
  store <8 x i16> %897, <8 x i16>* %52, align 16
  %898 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %886, <4 x i32> %887) #6
  store <8 x i16> %898, <8 x i16>* %54, align 16
  %899 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %888, <4 x i32> %889) #6
  store <8 x i16> %899, <8 x i16>* %56, align 16
  %900 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %890, <4 x i32> %891) #6
  store <8 x i16> %900, <8 x i16>* %58, align 16
  %901 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %892, <4 x i32> %893) #6
  store <8 x i16> %901, <8 x i16>* %60, align 16
  %902 = add <8 x i16> %830, %718
  %903 = sub <8 x i16> %718, %830
  %904 = sub <8 x i16> %721, %831
  %905 = add <8 x i16> %831, %721
  %906 = add <8 x i16> %832, %722
  %907 = sub <8 x i16> %722, %832
  %908 = sub <8 x i16> %725, %833
  %909 = add <8 x i16> %833, %725
  %910 = add <8 x i16> %834, %726
  %911 = sub <8 x i16> %726, %834
  %912 = sub <8 x i16> %729, %835
  %913 = add <8 x i16> %835, %729
  %914 = add <8 x i16> %836, %730
  %915 = sub <8 x i16> %730, %836
  %916 = sub <8 x i16> %733, %837
  %917 = add <8 x i16> %837, %733
  %918 = shufflevector <8 x i16> %902, <8 x i16> %917, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %919 = shufflevector <8 x i16> %902, <8 x i16> %917, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %920 = shufflevector <8 x i16> %903, <8 x i16> %916, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %921 = shufflevector <8 x i16> %903, <8 x i16> %916, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %922 = shufflevector <8 x i16> %904, <8 x i16> %915, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %923 = shufflevector <8 x i16> %904, <8 x i16> %915, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %924 = shufflevector <8 x i16> %905, <8 x i16> %914, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %925 = shufflevector <8 x i16> %905, <8 x i16> %914, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %926 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %918, <8 x i16> <i16 804, i16 16364, i16 804, i16 16364, i16 804, i16 16364, i16 804, i16 16364>) #6
  %927 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %919, <8 x i16> <i16 804, i16 16364, i16 804, i16 16364, i16 804, i16 16364, i16 804, i16 16364>) #6
  %928 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %920, <8 x i16> <i16 12140, i16 11003, i16 12140, i16 11003, i16 12140, i16 11003, i16 12140, i16 11003>) #6
  %929 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %921, <8 x i16> <i16 12140, i16 11003, i16 12140, i16 11003, i16 12140, i16 11003, i16 12140, i16 11003>) #6
  %930 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %922, <8 x i16> <i16 7005, i16 14811, i16 7005, i16 14811, i16 7005, i16 14811, i16 7005, i16 14811>) #6
  %931 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %923, <8 x i16> <i16 7005, i16 14811, i16 7005, i16 14811, i16 7005, i16 14811, i16 7005, i16 14811>) #6
  %932 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %924, <8 x i16> <i16 15426, i16 5520, i16 15426, i16 5520, i16 15426, i16 5520, i16 15426, i16 5520>) #6
  %933 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %925, <8 x i16> <i16 15426, i16 5520, i16 15426, i16 5520, i16 15426, i16 5520, i16 15426, i16 5520>) #6
  %934 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %924, <8 x i16> <i16 -5520, i16 15426, i16 -5520, i16 15426, i16 -5520, i16 15426, i16 -5520, i16 15426>) #6
  %935 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %925, <8 x i16> <i16 -5520, i16 15426, i16 -5520, i16 15426, i16 -5520, i16 15426, i16 -5520, i16 15426>) #6
  %936 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %922, <8 x i16> <i16 -14811, i16 7005, i16 -14811, i16 7005, i16 -14811, i16 7005, i16 -14811, i16 7005>) #6
  %937 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %923, <8 x i16> <i16 -14811, i16 7005, i16 -14811, i16 7005, i16 -14811, i16 7005, i16 -14811, i16 7005>) #6
  %938 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %920, <8 x i16> <i16 -11003, i16 12140, i16 -11003, i16 12140, i16 -11003, i16 12140, i16 -11003, i16 12140>) #6
  %939 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %921, <8 x i16> <i16 -11003, i16 12140, i16 -11003, i16 12140, i16 -11003, i16 12140, i16 -11003, i16 12140>) #6
  %940 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %918, <8 x i16> <i16 -16364, i16 804, i16 -16364, i16 804, i16 -16364, i16 804, i16 -16364, i16 804>) #6
  %941 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %919, <8 x i16> <i16 -16364, i16 804, i16 -16364, i16 804, i16 -16364, i16 804, i16 -16364, i16 804>) #6
  %942 = add <4 x i32> %926, <i32 8192, i32 8192, i32 8192, i32 8192>
  %943 = add <4 x i32> %927, <i32 8192, i32 8192, i32 8192, i32 8192>
  %944 = add <4 x i32> %928, <i32 8192, i32 8192, i32 8192, i32 8192>
  %945 = add <4 x i32> %929, <i32 8192, i32 8192, i32 8192, i32 8192>
  %946 = add <4 x i32> %930, <i32 8192, i32 8192, i32 8192, i32 8192>
  %947 = add <4 x i32> %931, <i32 8192, i32 8192, i32 8192, i32 8192>
  %948 = add <4 x i32> %932, <i32 8192, i32 8192, i32 8192, i32 8192>
  %949 = add <4 x i32> %933, <i32 8192, i32 8192, i32 8192, i32 8192>
  %950 = add <4 x i32> %934, <i32 8192, i32 8192, i32 8192, i32 8192>
  %951 = add <4 x i32> %935, <i32 8192, i32 8192, i32 8192, i32 8192>
  %952 = add <4 x i32> %936, <i32 8192, i32 8192, i32 8192, i32 8192>
  %953 = add <4 x i32> %937, <i32 8192, i32 8192, i32 8192, i32 8192>
  %954 = add <4 x i32> %938, <i32 8192, i32 8192, i32 8192, i32 8192>
  %955 = add <4 x i32> %939, <i32 8192, i32 8192, i32 8192, i32 8192>
  %956 = add <4 x i32> %940, <i32 8192, i32 8192, i32 8192, i32 8192>
  %957 = add <4 x i32> %941, <i32 8192, i32 8192, i32 8192, i32 8192>
  %958 = ashr <4 x i32> %942, <i32 14, i32 14, i32 14, i32 14>
  %959 = ashr <4 x i32> %943, <i32 14, i32 14, i32 14, i32 14>
  %960 = ashr <4 x i32> %944, <i32 14, i32 14, i32 14, i32 14>
  %961 = ashr <4 x i32> %945, <i32 14, i32 14, i32 14, i32 14>
  %962 = ashr <4 x i32> %946, <i32 14, i32 14, i32 14, i32 14>
  %963 = ashr <4 x i32> %947, <i32 14, i32 14, i32 14, i32 14>
  %964 = ashr <4 x i32> %948, <i32 14, i32 14, i32 14, i32 14>
  %965 = ashr <4 x i32> %949, <i32 14, i32 14, i32 14, i32 14>
  %966 = ashr <4 x i32> %950, <i32 14, i32 14, i32 14, i32 14>
  %967 = ashr <4 x i32> %951, <i32 14, i32 14, i32 14, i32 14>
  %968 = ashr <4 x i32> %952, <i32 14, i32 14, i32 14, i32 14>
  %969 = ashr <4 x i32> %953, <i32 14, i32 14, i32 14, i32 14>
  %970 = ashr <4 x i32> %954, <i32 14, i32 14, i32 14, i32 14>
  %971 = ashr <4 x i32> %955, <i32 14, i32 14, i32 14, i32 14>
  %972 = ashr <4 x i32> %956, <i32 14, i32 14, i32 14, i32 14>
  %973 = ashr <4 x i32> %957, <i32 14, i32 14, i32 14, i32 14>
  %974 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %958, <4 x i32> %959) #6
  store <8 x i16> %974, <8 x i16>* %62, align 16
  %975 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %960, <4 x i32> %961) #6
  store <8 x i16> %975, <8 x i16>* %64, align 16
  %976 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %962, <4 x i32> %963) #6
  store <8 x i16> %976, <8 x i16>* %66, align 16
  %977 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %964, <4 x i32> %965) #6
  store <8 x i16> %977, <8 x i16>* %68, align 16
  %978 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %966, <4 x i32> %967) #6
  store <8 x i16> %978, <8 x i16>* %70, align 16
  %979 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %968, <4 x i32> %969) #6
  store <8 x i16> %979, <8 x i16>* %72, align 16
  %980 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %970, <4 x i32> %971) #6
  store <8 x i16> %980, <8 x i16>* %74, align 16
  %981 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %972, <4 x i32> %973) #6
  store <8 x i16> %981, <8 x i16>* %76, align 16
  %982 = shufflevector <8 x i16> %906, <8 x i16> %913, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %983 = shufflevector <8 x i16> %906, <8 x i16> %913, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %984 = shufflevector <8 x i16> %907, <8 x i16> %912, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %985 = shufflevector <8 x i16> %907, <8 x i16> %912, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %986 = shufflevector <8 x i16> %908, <8 x i16> %911, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %987 = shufflevector <8 x i16> %908, <8 x i16> %911, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %988 = shufflevector <8 x i16> %909, <8 x i16> %910, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %989 = shufflevector <8 x i16> %909, <8 x i16> %910, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %990 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %982, <8 x i16> <i16 3981, i16 15893, i16 3981, i16 15893, i16 3981, i16 15893, i16 3981, i16 15893>) #6
  %991 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %983, <8 x i16> <i16 3981, i16 15893, i16 3981, i16 15893, i16 3981, i16 15893, i16 3981, i16 15893>) #6
  %992 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %984, <8 x i16> <i16 14053, i16 8423, i16 14053, i16 8423, i16 14053, i16 8423, i16 14053, i16 8423>) #6
  %993 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %985, <8 x i16> <i16 14053, i16 8423, i16 14053, i16 8423, i16 14053, i16 8423, i16 14053, i16 8423>) #6
  %994 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %986, <8 x i16> <i16 9760, i16 13160, i16 9760, i16 13160, i16 9760, i16 13160, i16 9760, i16 13160>) #6
  %995 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %987, <8 x i16> <i16 9760, i16 13160, i16 9760, i16 13160, i16 9760, i16 13160, i16 9760, i16 13160>) #6
  %996 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %988, <8 x i16> <i16 16207, i16 2404, i16 16207, i16 2404, i16 16207, i16 2404, i16 16207, i16 2404>) #6
  %997 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %989, <8 x i16> <i16 16207, i16 2404, i16 16207, i16 2404, i16 16207, i16 2404, i16 16207, i16 2404>) #6
  %998 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %988, <8 x i16> <i16 -2404, i16 16207, i16 -2404, i16 16207, i16 -2404, i16 16207, i16 -2404, i16 16207>) #6
  %999 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %989, <8 x i16> <i16 -2404, i16 16207, i16 -2404, i16 16207, i16 -2404, i16 16207, i16 -2404, i16 16207>) #6
  %1000 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %986, <8 x i16> <i16 -13160, i16 9760, i16 -13160, i16 9760, i16 -13160, i16 9760, i16 -13160, i16 9760>) #6
  %1001 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %987, <8 x i16> <i16 -13160, i16 9760, i16 -13160, i16 9760, i16 -13160, i16 9760, i16 -13160, i16 9760>) #6
  %1002 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %984, <8 x i16> <i16 -8423, i16 14053, i16 -8423, i16 14053, i16 -8423, i16 14053, i16 -8423, i16 14053>) #6
  %1003 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %985, <8 x i16> <i16 -8423, i16 14053, i16 -8423, i16 14053, i16 -8423, i16 14053, i16 -8423, i16 14053>) #6
  %1004 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %982, <8 x i16> <i16 -15893, i16 3981, i16 -15893, i16 3981, i16 -15893, i16 3981, i16 -15893, i16 3981>) #6
  %1005 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %983, <8 x i16> <i16 -15893, i16 3981, i16 -15893, i16 3981, i16 -15893, i16 3981, i16 -15893, i16 3981>) #6
  %1006 = add <4 x i32> %990, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1007 = add <4 x i32> %991, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1008 = add <4 x i32> %992, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1009 = add <4 x i32> %993, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1010 = add <4 x i32> %994, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1011 = add <4 x i32> %995, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1012 = add <4 x i32> %996, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1013 = add <4 x i32> %997, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1014 = add <4 x i32> %998, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1015 = add <4 x i32> %999, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1016 = add <4 x i32> %1000, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1017 = add <4 x i32> %1001, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1018 = add <4 x i32> %1002, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1019 = add <4 x i32> %1003, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1020 = add <4 x i32> %1004, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1021 = add <4 x i32> %1005, <i32 8192, i32 8192, i32 8192, i32 8192>
  br label %3671

1022:                                             ; preds = %388
  %1023 = shufflevector <8 x i16> %421, <8 x i16> %428, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1024 = shufflevector <8 x i16> %421, <8 x i16> %428, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1025 = shufflevector <8 x i16> %422, <8 x i16> %427, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1026 = shufflevector <8 x i16> %422, <8 x i16> %427, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1027 = shufflevector <8 x i16> %423, <8 x i16> %426, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1028 = shufflevector <8 x i16> %423, <8 x i16> %426, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1029 = shufflevector <8 x i16> %424, <8 x i16> %425, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1030 = shufflevector <8 x i16> %424, <8 x i16> %425, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1031 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1023, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %1032 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1024, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %1033 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1025, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %1034 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1026, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %1035 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1027, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %1036 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1028, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %1037 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1029, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %1038 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1030, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %1039 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1029, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %1040 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1030, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %1041 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1027, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %1042 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1028, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %1043 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1025, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %1044 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1026, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %1045 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1023, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %1046 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1024, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %1047 = shufflevector <8 x i16> %434, <8 x i16> %431, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1048 = shufflevector <8 x i16> %434, <8 x i16> %431, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1049 = shufflevector <8 x i16> %433, <8 x i16> %432, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1050 = shufflevector <8 x i16> %433, <8 x i16> %432, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1051 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1047, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %1052 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1048, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %1053 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1049, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %1054 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1050, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %1055 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1049, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %1056 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1050, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %1057 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1047, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %1058 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1048, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %1059 = add <4 x i32> %1051, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1060 = add <4 x i32> %1052, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1061 = add <4 x i32> %1053, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1062 = add <4 x i32> %1054, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1063 = add <4 x i32> %1055, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1064 = add <4 x i32> %1056, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1065 = add <4 x i32> %1057, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1066 = add <4 x i32> %1058, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1067 = ashr <4 x i32> %1059, <i32 14, i32 14, i32 14, i32 14>
  %1068 = ashr <4 x i32> %1060, <i32 14, i32 14, i32 14, i32 14>
  %1069 = ashr <4 x i32> %1061, <i32 14, i32 14, i32 14, i32 14>
  %1070 = ashr <4 x i32> %1062, <i32 14, i32 14, i32 14, i32 14>
  %1071 = ashr <4 x i32> %1063, <i32 14, i32 14, i32 14, i32 14>
  %1072 = ashr <4 x i32> %1064, <i32 14, i32 14, i32 14, i32 14>
  %1073 = ashr <4 x i32> %1065, <i32 14, i32 14, i32 14, i32 14>
  %1074 = ashr <4 x i32> %1066, <i32 14, i32 14, i32 14, i32 14>
  %1075 = shufflevector <8 x i16> %404, <8 x i16> %496, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1076 = shufflevector <8 x i16> %404, <8 x i16> %496, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1077 = shufflevector <8 x i16> %403, <8 x i16> %495, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1078 = shufflevector <8 x i16> %403, <8 x i16> %495, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1079 = shufflevector <8 x i16> %402, <8 x i16> %494, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1080 = shufflevector <8 x i16> %402, <8 x i16> %494, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1081 = shufflevector <8 x i16> %401, <8 x i16> %493, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1082 = shufflevector <8 x i16> %401, <8 x i16> %493, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1083 = shufflevector <8 x i16> %392, <8 x i16> %500, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1084 = shufflevector <8 x i16> %392, <8 x i16> %500, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1085 = shufflevector <8 x i16> %391, <8 x i16> %499, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1086 = shufflevector <8 x i16> %391, <8 x i16> %499, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1087 = shufflevector <8 x i16> %390, <8 x i16> %498, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1088 = shufflevector <8 x i16> %390, <8 x i16> %498, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1089 = shufflevector <8 x i16> %389, <8 x i16> %497, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1090 = shufflevector <8 x i16> %389, <8 x i16> %497, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1091 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1075, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %1092 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1076, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %1093 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1077, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %1094 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1078, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %1095 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1079, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %1096 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1080, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %1097 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1081, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %1098 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1082, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %1099 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1081, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %1100 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1082, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %1101 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1079, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %1102 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1080, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %1103 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1077, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %1104 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1078, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %1105 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1075, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %1106 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1076, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %1107 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1089, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %1108 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1090, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %1109 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1087, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %1110 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1088, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %1111 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1085, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %1112 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1086, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %1113 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1083, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %1114 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1084, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %1115 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1083, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %1116 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1084, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %1117 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1085, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %1118 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1086, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %1119 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1087, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %1120 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1088, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %1121 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1089, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %1122 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1090, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %1123 = ashr <8 x i16> %429, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1124 = ashr <8 x i16> %430, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1125 = ashr <8 x i16> %435, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1126 = ashr <8 x i16> %436, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1127 = shufflevector <8 x i16> %429, <8 x i16> %1123, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1128 = shufflevector <8 x i16> %429, <8 x i16> %1123, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1129 = shufflevector <8 x i16> %430, <8 x i16> %1124, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1130 = shufflevector <8 x i16> %430, <8 x i16> %1124, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1131 = shufflevector <8 x i16> %435, <8 x i16> %1125, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1132 = shufflevector <8 x i16> %435, <8 x i16> %1125, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1133 = shufflevector <8 x i16> %436, <8 x i16> %1126, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1134 = shufflevector <8 x i16> %436, <8 x i16> %1126, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1135 = add <4 x i32> %1037, %1031
  %1136 = add <4 x i32> %1038, %1032
  %1137 = add <4 x i32> %1035, %1033
  %1138 = add <4 x i32> %1036, %1034
  %1139 = sub <4 x i32> %1033, %1035
  %1140 = sub <4 x i32> %1034, %1036
  %1141 = sub <4 x i32> %1031, %1037
  %1142 = sub <4 x i32> %1032, %1038
  %1143 = bitcast <8 x i16> %1127 to <4 x i32>
  %1144 = add <4 x i32> %1069, %1143
  %1145 = bitcast <8 x i16> %1128 to <4 x i32>
  %1146 = add <4 x i32> %1070, %1145
  %1147 = bitcast <8 x i16> %1129 to <4 x i32>
  %1148 = add <4 x i32> %1067, %1147
  %1149 = bitcast <8 x i16> %1130 to <4 x i32>
  %1150 = add <4 x i32> %1068, %1149
  %1151 = sub <4 x i32> %1147, %1067
  %1152 = sub <4 x i32> %1149, %1068
  %1153 = sub <4 x i32> %1143, %1069
  %1154 = sub <4 x i32> %1145, %1070
  %1155 = bitcast <8 x i16> %1133 to <4 x i32>
  %1156 = sub <4 x i32> %1155, %1071
  %1157 = bitcast <8 x i16> %1134 to <4 x i32>
  %1158 = sub <4 x i32> %1157, %1072
  %1159 = bitcast <8 x i16> %1131 to <4 x i32>
  %1160 = sub <4 x i32> %1159, %1073
  %1161 = bitcast <8 x i16> %1132 to <4 x i32>
  %1162 = sub <4 x i32> %1161, %1074
  %1163 = add <4 x i32> %1073, %1159
  %1164 = add <4 x i32> %1074, %1161
  %1165 = add <4 x i32> %1071, %1155
  %1166 = add <4 x i32> %1072, %1157
  %1167 = shufflevector <4 x i32> %1043, <4 x i32> %1041, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1168 = bitcast <4 x i32> %1167 to <2 x i64>
  %1169 = shufflevector <4 x i32> %1043, <4 x i32> %1041, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1170 = bitcast <4 x i32> %1169 to <2 x i64>
  %1171 = shufflevector <4 x i32> %1044, <4 x i32> %1042, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1172 = bitcast <4 x i32> %1171 to <2 x i64>
  %1173 = shufflevector <4 x i32> %1044, <4 x i32> %1042, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1174 = bitcast <4 x i32> %1173 to <2 x i64>
  %1175 = and <2 x i64> %1168, <i64 4294967295, i64 4294967295>
  %1176 = mul nuw nsw <2 x i64> %1175, <i64 11585, i64 11585>
  %1177 = lshr <2 x i64> %1168, <i64 32, i64 32>
  %1178 = mul nuw <2 x i64> %1177, <i64 4294955711, i64 4294955711>
  %1179 = add <2 x i64> %1178, %1176
  %1180 = and <2 x i64> %1170, <i64 4294967295, i64 4294967295>
  %1181 = mul nuw nsw <2 x i64> %1180, <i64 11585, i64 11585>
  %1182 = lshr <2 x i64> %1170, <i64 32, i64 32>
  %1183 = mul nuw <2 x i64> %1182, <i64 4294955711, i64 4294955711>
  %1184 = add <2 x i64> %1183, %1181
  %1185 = and <2 x i64> %1172, <i64 4294967295, i64 4294967295>
  %1186 = mul nuw nsw <2 x i64> %1185, <i64 11585, i64 11585>
  %1187 = lshr <2 x i64> %1172, <i64 32, i64 32>
  %1188 = mul nuw <2 x i64> %1187, <i64 4294955711, i64 4294955711>
  %1189 = add <2 x i64> %1188, %1186
  %1190 = and <2 x i64> %1174, <i64 4294967295, i64 4294967295>
  %1191 = mul nuw nsw <2 x i64> %1190, <i64 11585, i64 11585>
  %1192 = lshr <2 x i64> %1174, <i64 32, i64 32>
  %1193 = mul nuw <2 x i64> %1192, <i64 4294955711, i64 4294955711>
  %1194 = add <2 x i64> %1193, %1191
  %1195 = mul nuw nsw <2 x i64> %1177, <i64 11585, i64 11585>
  %1196 = add nuw nsw <2 x i64> %1195, %1176
  %1197 = mul nuw nsw <2 x i64> %1182, <i64 11585, i64 11585>
  %1198 = add nuw nsw <2 x i64> %1197, %1181
  %1199 = mul nuw nsw <2 x i64> %1187, <i64 11585, i64 11585>
  %1200 = add nuw nsw <2 x i64> %1199, %1186
  %1201 = mul nuw nsw <2 x i64> %1192, <i64 11585, i64 11585>
  %1202 = add nuw nsw <2 x i64> %1201, %1191
  %1203 = bitcast <2 x i64> %1179 to <4 x i32>
  %1204 = shufflevector <4 x i32> %1203, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1205 = bitcast <4 x i32> %1204 to <2 x i64>
  %1206 = bitcast <2 x i64> %1184 to <4 x i32>
  %1207 = shufflevector <4 x i32> %1206, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1208 = bitcast <4 x i32> %1207 to <2 x i64>
  %1209 = shufflevector <2 x i64> %1205, <2 x i64> %1208, <2 x i32> <i32 0, i32 2>
  %1210 = bitcast <2 x i64> %1189 to <4 x i32>
  %1211 = shufflevector <4 x i32> %1210, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1212 = bitcast <4 x i32> %1211 to <2 x i64>
  %1213 = bitcast <2 x i64> %1194 to <4 x i32>
  %1214 = shufflevector <4 x i32> %1213, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1215 = bitcast <4 x i32> %1214 to <2 x i64>
  %1216 = shufflevector <2 x i64> %1212, <2 x i64> %1215, <2 x i32> <i32 0, i32 2>
  %1217 = bitcast <2 x i64> %1196 to <4 x i32>
  %1218 = shufflevector <4 x i32> %1217, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1219 = bitcast <4 x i32> %1218 to <2 x i64>
  %1220 = bitcast <2 x i64> %1198 to <4 x i32>
  %1221 = shufflevector <4 x i32> %1220, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1222 = bitcast <4 x i32> %1221 to <2 x i64>
  %1223 = shufflevector <2 x i64> %1219, <2 x i64> %1222, <2 x i32> <i32 0, i32 2>
  %1224 = bitcast <2 x i64> %1200 to <4 x i32>
  %1225 = shufflevector <4 x i32> %1224, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1226 = bitcast <4 x i32> %1225 to <2 x i64>
  %1227 = bitcast <2 x i64> %1202 to <4 x i32>
  %1228 = shufflevector <4 x i32> %1227, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1229 = bitcast <4 x i32> %1228 to <2 x i64>
  %1230 = shufflevector <2 x i64> %1226, <2 x i64> %1229, <2 x i32> <i32 0, i32 2>
  %1231 = bitcast <2 x i64> %1209 to <4 x i32>
  %1232 = add <4 x i32> %1231, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1233 = bitcast <2 x i64> %1216 to <4 x i32>
  %1234 = add <4 x i32> %1233, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1235 = bitcast <2 x i64> %1223 to <4 x i32>
  %1236 = add <4 x i32> %1235, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1237 = bitcast <2 x i64> %1230 to <4 x i32>
  %1238 = add <4 x i32> %1237, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1239 = ashr <4 x i32> %1232, <i32 14, i32 14, i32 14, i32 14>
  %1240 = ashr <4 x i32> %1234, <i32 14, i32 14, i32 14, i32 14>
  %1241 = ashr <4 x i32> %1236, <i32 14, i32 14, i32 14, i32 14>
  %1242 = ashr <4 x i32> %1238, <i32 14, i32 14, i32 14, i32 14>
  %1243 = shufflevector <4 x i32> %1095, <4 x i32> %1117, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1244 = bitcast <4 x i32> %1243 to <2 x i64>
  %1245 = shufflevector <4 x i32> %1095, <4 x i32> %1117, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1246 = bitcast <4 x i32> %1245 to <2 x i64>
  %1247 = shufflevector <4 x i32> %1096, <4 x i32> %1118, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1248 = bitcast <4 x i32> %1247 to <2 x i64>
  %1249 = shufflevector <4 x i32> %1096, <4 x i32> %1118, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1250 = bitcast <4 x i32> %1249 to <2 x i64>
  %1251 = shufflevector <4 x i32> %1097, <4 x i32> %1115, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1252 = bitcast <4 x i32> %1251 to <2 x i64>
  %1253 = shufflevector <4 x i32> %1097, <4 x i32> %1115, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1254 = bitcast <4 x i32> %1253 to <2 x i64>
  %1255 = shufflevector <4 x i32> %1098, <4 x i32> %1116, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1256 = bitcast <4 x i32> %1255 to <2 x i64>
  %1257 = shufflevector <4 x i32> %1098, <4 x i32> %1116, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1258 = bitcast <4 x i32> %1257 to <2 x i64>
  %1259 = shufflevector <4 x i32> %1099, <4 x i32> %1113, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1260 = bitcast <4 x i32> %1259 to <2 x i64>
  %1261 = shufflevector <4 x i32> %1099, <4 x i32> %1113, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1262 = bitcast <4 x i32> %1261 to <2 x i64>
  %1263 = shufflevector <4 x i32> %1100, <4 x i32> %1114, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1264 = bitcast <4 x i32> %1263 to <2 x i64>
  %1265 = shufflevector <4 x i32> %1100, <4 x i32> %1114, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1266 = bitcast <4 x i32> %1265 to <2 x i64>
  %1267 = shufflevector <4 x i32> %1101, <4 x i32> %1111, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1268 = bitcast <4 x i32> %1267 to <2 x i64>
  %1269 = shufflevector <4 x i32> %1101, <4 x i32> %1111, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1270 = bitcast <4 x i32> %1269 to <2 x i64>
  %1271 = shufflevector <4 x i32> %1102, <4 x i32> %1112, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1272 = bitcast <4 x i32> %1271 to <2 x i64>
  %1273 = shufflevector <4 x i32> %1102, <4 x i32> %1112, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1274 = bitcast <4 x i32> %1273 to <2 x i64>
  %1275 = and <2 x i64> %1244, <i64 4294967295, i64 4294967295>
  %1276 = mul nuw <2 x i64> %1275, <i64 4294952159, i64 4294952159>
  %1277 = lshr <2 x i64> %1244, <i64 32, i64 32>
  %1278 = mul nuw nsw <2 x i64> %1277, <i64 6270, i64 6270>
  %1279 = add <2 x i64> %1278, %1276
  %1280 = and <2 x i64> %1246, <i64 4294967295, i64 4294967295>
  %1281 = mul nuw <2 x i64> %1280, <i64 4294952159, i64 4294952159>
  %1282 = lshr <2 x i64> %1246, <i64 32, i64 32>
  %1283 = mul nuw nsw <2 x i64> %1282, <i64 6270, i64 6270>
  %1284 = add <2 x i64> %1283, %1281
  %1285 = and <2 x i64> %1248, <i64 4294967295, i64 4294967295>
  %1286 = mul nuw <2 x i64> %1285, <i64 4294952159, i64 4294952159>
  %1287 = lshr <2 x i64> %1248, <i64 32, i64 32>
  %1288 = mul nuw nsw <2 x i64> %1287, <i64 6270, i64 6270>
  %1289 = add <2 x i64> %1288, %1286
  %1290 = and <2 x i64> %1250, <i64 4294967295, i64 4294967295>
  %1291 = mul nuw <2 x i64> %1290, <i64 4294952159, i64 4294952159>
  %1292 = lshr <2 x i64> %1250, <i64 32, i64 32>
  %1293 = mul nuw nsw <2 x i64> %1292, <i64 6270, i64 6270>
  %1294 = add <2 x i64> %1293, %1291
  %1295 = and <2 x i64> %1252, <i64 4294967295, i64 4294967295>
  %1296 = mul nuw <2 x i64> %1295, <i64 4294952159, i64 4294952159>
  %1297 = lshr <2 x i64> %1252, <i64 32, i64 32>
  %1298 = mul nuw nsw <2 x i64> %1297, <i64 6270, i64 6270>
  %1299 = add <2 x i64> %1298, %1296
  %1300 = and <2 x i64> %1254, <i64 4294967295, i64 4294967295>
  %1301 = mul nuw <2 x i64> %1300, <i64 4294952159, i64 4294952159>
  %1302 = lshr <2 x i64> %1254, <i64 32, i64 32>
  %1303 = mul nuw nsw <2 x i64> %1302, <i64 6270, i64 6270>
  %1304 = add <2 x i64> %1303, %1301
  %1305 = and <2 x i64> %1256, <i64 4294967295, i64 4294967295>
  %1306 = mul nuw <2 x i64> %1305, <i64 4294952159, i64 4294952159>
  %1307 = lshr <2 x i64> %1256, <i64 32, i64 32>
  %1308 = mul nuw nsw <2 x i64> %1307, <i64 6270, i64 6270>
  %1309 = add <2 x i64> %1308, %1306
  %1310 = and <2 x i64> %1258, <i64 4294967295, i64 4294967295>
  %1311 = mul nuw <2 x i64> %1310, <i64 4294952159, i64 4294952159>
  %1312 = lshr <2 x i64> %1258, <i64 32, i64 32>
  %1313 = mul nuw nsw <2 x i64> %1312, <i64 6270, i64 6270>
  %1314 = add <2 x i64> %1313, %1311
  %1315 = and <2 x i64> %1260, <i64 4294967295, i64 4294967295>
  %1316 = mul nuw <2 x i64> %1315, <i64 4294961026, i64 4294961026>
  %1317 = lshr <2 x i64> %1260, <i64 32, i64 32>
  %1318 = mul nuw <2 x i64> %1317, <i64 4294952159, i64 4294952159>
  %1319 = add <2 x i64> %1318, %1316
  %1320 = and <2 x i64> %1262, <i64 4294967295, i64 4294967295>
  %1321 = mul nuw <2 x i64> %1320, <i64 4294961026, i64 4294961026>
  %1322 = lshr <2 x i64> %1262, <i64 32, i64 32>
  %1323 = mul nuw <2 x i64> %1322, <i64 4294952159, i64 4294952159>
  %1324 = add <2 x i64> %1323, %1321
  %1325 = and <2 x i64> %1264, <i64 4294967295, i64 4294967295>
  %1326 = mul nuw <2 x i64> %1325, <i64 4294961026, i64 4294961026>
  %1327 = lshr <2 x i64> %1264, <i64 32, i64 32>
  %1328 = mul nuw <2 x i64> %1327, <i64 4294952159, i64 4294952159>
  %1329 = add <2 x i64> %1328, %1326
  %1330 = and <2 x i64> %1266, <i64 4294967295, i64 4294967295>
  %1331 = mul nuw <2 x i64> %1330, <i64 4294961026, i64 4294961026>
  %1332 = lshr <2 x i64> %1266, <i64 32, i64 32>
  %1333 = mul nuw <2 x i64> %1332, <i64 4294952159, i64 4294952159>
  %1334 = add <2 x i64> %1333, %1331
  %1335 = and <2 x i64> %1268, <i64 4294967295, i64 4294967295>
  %1336 = mul nuw <2 x i64> %1335, <i64 4294961026, i64 4294961026>
  %1337 = lshr <2 x i64> %1268, <i64 32, i64 32>
  %1338 = mul nuw <2 x i64> %1337, <i64 4294952159, i64 4294952159>
  %1339 = add <2 x i64> %1338, %1336
  %1340 = and <2 x i64> %1270, <i64 4294967295, i64 4294967295>
  %1341 = mul nuw <2 x i64> %1340, <i64 4294961026, i64 4294961026>
  %1342 = lshr <2 x i64> %1270, <i64 32, i64 32>
  %1343 = mul nuw <2 x i64> %1342, <i64 4294952159, i64 4294952159>
  %1344 = add <2 x i64> %1343, %1341
  %1345 = and <2 x i64> %1272, <i64 4294967295, i64 4294967295>
  %1346 = mul nuw <2 x i64> %1345, <i64 4294961026, i64 4294961026>
  %1347 = lshr <2 x i64> %1272, <i64 32, i64 32>
  %1348 = mul nuw <2 x i64> %1347, <i64 4294952159, i64 4294952159>
  %1349 = add <2 x i64> %1348, %1346
  %1350 = and <2 x i64> %1274, <i64 4294967295, i64 4294967295>
  %1351 = mul nuw <2 x i64> %1350, <i64 4294961026, i64 4294961026>
  %1352 = lshr <2 x i64> %1274, <i64 32, i64 32>
  %1353 = mul nuw <2 x i64> %1352, <i64 4294952159, i64 4294952159>
  %1354 = add <2 x i64> %1353, %1351
  %1355 = mul nuw <2 x i64> %1335, <i64 4294952159, i64 4294952159>
  %1356 = mul nuw nsw <2 x i64> %1337, <i64 6270, i64 6270>
  %1357 = add <2 x i64> %1356, %1355
  %1358 = mul nuw <2 x i64> %1340, <i64 4294952159, i64 4294952159>
  %1359 = mul nuw nsw <2 x i64> %1342, <i64 6270, i64 6270>
  %1360 = add <2 x i64> %1359, %1358
  %1361 = mul nuw <2 x i64> %1345, <i64 4294952159, i64 4294952159>
  %1362 = mul nuw nsw <2 x i64> %1347, <i64 6270, i64 6270>
  %1363 = add <2 x i64> %1362, %1361
  %1364 = mul nuw <2 x i64> %1350, <i64 4294952159, i64 4294952159>
  %1365 = mul nuw nsw <2 x i64> %1352, <i64 6270, i64 6270>
  %1366 = add <2 x i64> %1365, %1364
  %1367 = mul nuw <2 x i64> %1315, <i64 4294952159, i64 4294952159>
  %1368 = mul nuw nsw <2 x i64> %1317, <i64 6270, i64 6270>
  %1369 = add <2 x i64> %1368, %1367
  %1370 = mul nuw <2 x i64> %1320, <i64 4294952159, i64 4294952159>
  %1371 = mul nuw nsw <2 x i64> %1322, <i64 6270, i64 6270>
  %1372 = add <2 x i64> %1371, %1370
  %1373 = mul nuw <2 x i64> %1325, <i64 4294952159, i64 4294952159>
  %1374 = mul nuw nsw <2 x i64> %1327, <i64 6270, i64 6270>
  %1375 = add <2 x i64> %1374, %1373
  %1376 = mul nuw <2 x i64> %1330, <i64 4294952159, i64 4294952159>
  %1377 = mul nuw nsw <2 x i64> %1332, <i64 6270, i64 6270>
  %1378 = add <2 x i64> %1377, %1376
  %1379 = mul nuw nsw <2 x i64> %1295, <i64 6270, i64 6270>
  %1380 = mul nuw nsw <2 x i64> %1297, <i64 15137, i64 15137>
  %1381 = add nuw nsw <2 x i64> %1380, %1379
  %1382 = mul nuw nsw <2 x i64> %1300, <i64 6270, i64 6270>
  %1383 = mul nuw nsw <2 x i64> %1302, <i64 15137, i64 15137>
  %1384 = add nuw nsw <2 x i64> %1383, %1382
  %1385 = mul nuw nsw <2 x i64> %1305, <i64 6270, i64 6270>
  %1386 = mul nuw nsw <2 x i64> %1307, <i64 15137, i64 15137>
  %1387 = add nuw nsw <2 x i64> %1386, %1385
  %1388 = mul nuw nsw <2 x i64> %1310, <i64 6270, i64 6270>
  %1389 = mul nuw nsw <2 x i64> %1312, <i64 15137, i64 15137>
  %1390 = add nuw nsw <2 x i64> %1389, %1388
  %1391 = mul nuw nsw <2 x i64> %1275, <i64 6270, i64 6270>
  %1392 = mul nuw nsw <2 x i64> %1277, <i64 15137, i64 15137>
  %1393 = add nuw nsw <2 x i64> %1392, %1391
  %1394 = mul nuw nsw <2 x i64> %1280, <i64 6270, i64 6270>
  %1395 = mul nuw nsw <2 x i64> %1282, <i64 15137, i64 15137>
  %1396 = add nuw nsw <2 x i64> %1395, %1394
  %1397 = mul nuw nsw <2 x i64> %1285, <i64 6270, i64 6270>
  %1398 = mul nuw nsw <2 x i64> %1287, <i64 15137, i64 15137>
  %1399 = add nuw nsw <2 x i64> %1398, %1397
  %1400 = mul nuw nsw <2 x i64> %1290, <i64 6270, i64 6270>
  %1401 = mul nuw nsw <2 x i64> %1292, <i64 15137, i64 15137>
  %1402 = add nuw nsw <2 x i64> %1401, %1400
  %1403 = bitcast <2 x i64> %1279 to <4 x i32>
  %1404 = shufflevector <4 x i32> %1403, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1405 = bitcast <4 x i32> %1404 to <2 x i64>
  %1406 = bitcast <2 x i64> %1284 to <4 x i32>
  %1407 = shufflevector <4 x i32> %1406, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1408 = bitcast <4 x i32> %1407 to <2 x i64>
  %1409 = shufflevector <2 x i64> %1405, <2 x i64> %1408, <2 x i32> <i32 0, i32 2>
  %1410 = bitcast <2 x i64> %1289 to <4 x i32>
  %1411 = shufflevector <4 x i32> %1410, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1412 = bitcast <4 x i32> %1411 to <2 x i64>
  %1413 = bitcast <2 x i64> %1294 to <4 x i32>
  %1414 = shufflevector <4 x i32> %1413, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1415 = bitcast <4 x i32> %1414 to <2 x i64>
  %1416 = shufflevector <2 x i64> %1412, <2 x i64> %1415, <2 x i32> <i32 0, i32 2>
  %1417 = bitcast <2 x i64> %1299 to <4 x i32>
  %1418 = shufflevector <4 x i32> %1417, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1419 = bitcast <4 x i32> %1418 to <2 x i64>
  %1420 = bitcast <2 x i64> %1304 to <4 x i32>
  %1421 = shufflevector <4 x i32> %1420, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1422 = bitcast <4 x i32> %1421 to <2 x i64>
  %1423 = shufflevector <2 x i64> %1419, <2 x i64> %1422, <2 x i32> <i32 0, i32 2>
  %1424 = bitcast <2 x i64> %1309 to <4 x i32>
  %1425 = shufflevector <4 x i32> %1424, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1426 = bitcast <4 x i32> %1425 to <2 x i64>
  %1427 = bitcast <2 x i64> %1314 to <4 x i32>
  %1428 = shufflevector <4 x i32> %1427, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1429 = bitcast <4 x i32> %1428 to <2 x i64>
  %1430 = shufflevector <2 x i64> %1426, <2 x i64> %1429, <2 x i32> <i32 0, i32 2>
  %1431 = bitcast <2 x i64> %1319 to <4 x i32>
  %1432 = shufflevector <4 x i32> %1431, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1433 = bitcast <4 x i32> %1432 to <2 x i64>
  %1434 = bitcast <2 x i64> %1324 to <4 x i32>
  %1435 = shufflevector <4 x i32> %1434, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1436 = bitcast <4 x i32> %1435 to <2 x i64>
  %1437 = shufflevector <2 x i64> %1433, <2 x i64> %1436, <2 x i32> <i32 0, i32 2>
  %1438 = bitcast <2 x i64> %1329 to <4 x i32>
  %1439 = shufflevector <4 x i32> %1438, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1440 = bitcast <4 x i32> %1439 to <2 x i64>
  %1441 = bitcast <2 x i64> %1334 to <4 x i32>
  %1442 = shufflevector <4 x i32> %1441, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1443 = bitcast <4 x i32> %1442 to <2 x i64>
  %1444 = shufflevector <2 x i64> %1440, <2 x i64> %1443, <2 x i32> <i32 0, i32 2>
  %1445 = bitcast <2 x i64> %1339 to <4 x i32>
  %1446 = shufflevector <4 x i32> %1445, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1447 = bitcast <4 x i32> %1446 to <2 x i64>
  %1448 = bitcast <2 x i64> %1344 to <4 x i32>
  %1449 = shufflevector <4 x i32> %1448, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1450 = bitcast <4 x i32> %1449 to <2 x i64>
  %1451 = shufflevector <2 x i64> %1447, <2 x i64> %1450, <2 x i32> <i32 0, i32 2>
  %1452 = bitcast <2 x i64> %1349 to <4 x i32>
  %1453 = shufflevector <4 x i32> %1452, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1454 = bitcast <4 x i32> %1453 to <2 x i64>
  %1455 = bitcast <2 x i64> %1354 to <4 x i32>
  %1456 = shufflevector <4 x i32> %1455, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1457 = bitcast <4 x i32> %1456 to <2 x i64>
  %1458 = shufflevector <2 x i64> %1454, <2 x i64> %1457, <2 x i32> <i32 0, i32 2>
  %1459 = bitcast <2 x i64> %1357 to <4 x i32>
  %1460 = shufflevector <4 x i32> %1459, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1461 = bitcast <4 x i32> %1460 to <2 x i64>
  %1462 = bitcast <2 x i64> %1360 to <4 x i32>
  %1463 = shufflevector <4 x i32> %1462, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1464 = bitcast <4 x i32> %1463 to <2 x i64>
  %1465 = shufflevector <2 x i64> %1461, <2 x i64> %1464, <2 x i32> <i32 0, i32 2>
  %1466 = bitcast <2 x i64> %1363 to <4 x i32>
  %1467 = shufflevector <4 x i32> %1466, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1468 = bitcast <4 x i32> %1467 to <2 x i64>
  %1469 = bitcast <2 x i64> %1366 to <4 x i32>
  %1470 = shufflevector <4 x i32> %1469, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1471 = bitcast <4 x i32> %1470 to <2 x i64>
  %1472 = shufflevector <2 x i64> %1468, <2 x i64> %1471, <2 x i32> <i32 0, i32 2>
  %1473 = bitcast <2 x i64> %1369 to <4 x i32>
  %1474 = shufflevector <4 x i32> %1473, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1475 = bitcast <4 x i32> %1474 to <2 x i64>
  %1476 = bitcast <2 x i64> %1372 to <4 x i32>
  %1477 = shufflevector <4 x i32> %1476, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1478 = bitcast <4 x i32> %1477 to <2 x i64>
  %1479 = shufflevector <2 x i64> %1475, <2 x i64> %1478, <2 x i32> <i32 0, i32 2>
  %1480 = bitcast <2 x i64> %1375 to <4 x i32>
  %1481 = shufflevector <4 x i32> %1480, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1482 = bitcast <4 x i32> %1481 to <2 x i64>
  %1483 = bitcast <2 x i64> %1378 to <4 x i32>
  %1484 = shufflevector <4 x i32> %1483, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1485 = bitcast <4 x i32> %1484 to <2 x i64>
  %1486 = shufflevector <2 x i64> %1482, <2 x i64> %1485, <2 x i32> <i32 0, i32 2>
  %1487 = bitcast <2 x i64> %1381 to <4 x i32>
  %1488 = shufflevector <4 x i32> %1487, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1489 = bitcast <4 x i32> %1488 to <2 x i64>
  %1490 = bitcast <2 x i64> %1384 to <4 x i32>
  %1491 = shufflevector <4 x i32> %1490, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1492 = bitcast <4 x i32> %1491 to <2 x i64>
  %1493 = shufflevector <2 x i64> %1489, <2 x i64> %1492, <2 x i32> <i32 0, i32 2>
  %1494 = bitcast <2 x i64> %1387 to <4 x i32>
  %1495 = shufflevector <4 x i32> %1494, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1496 = bitcast <4 x i32> %1495 to <2 x i64>
  %1497 = bitcast <2 x i64> %1390 to <4 x i32>
  %1498 = shufflevector <4 x i32> %1497, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1499 = bitcast <4 x i32> %1498 to <2 x i64>
  %1500 = shufflevector <2 x i64> %1496, <2 x i64> %1499, <2 x i32> <i32 0, i32 2>
  %1501 = bitcast <2 x i64> %1393 to <4 x i32>
  %1502 = shufflevector <4 x i32> %1501, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1503 = bitcast <4 x i32> %1502 to <2 x i64>
  %1504 = bitcast <2 x i64> %1396 to <4 x i32>
  %1505 = shufflevector <4 x i32> %1504, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1506 = bitcast <4 x i32> %1505 to <2 x i64>
  %1507 = shufflevector <2 x i64> %1503, <2 x i64> %1506, <2 x i32> <i32 0, i32 2>
  %1508 = bitcast <2 x i64> %1399 to <4 x i32>
  %1509 = shufflevector <4 x i32> %1508, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1510 = bitcast <4 x i32> %1509 to <2 x i64>
  %1511 = bitcast <2 x i64> %1402 to <4 x i32>
  %1512 = shufflevector <4 x i32> %1511, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1513 = bitcast <4 x i32> %1512 to <2 x i64>
  %1514 = shufflevector <2 x i64> %1510, <2 x i64> %1513, <2 x i32> <i32 0, i32 2>
  %1515 = bitcast <2 x i64> %1409 to <4 x i32>
  %1516 = add <4 x i32> %1515, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1517 = bitcast <2 x i64> %1416 to <4 x i32>
  %1518 = add <4 x i32> %1517, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1519 = bitcast <2 x i64> %1423 to <4 x i32>
  %1520 = add <4 x i32> %1519, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1521 = bitcast <2 x i64> %1430 to <4 x i32>
  %1522 = add <4 x i32> %1521, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1523 = bitcast <2 x i64> %1437 to <4 x i32>
  %1524 = add <4 x i32> %1523, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1525 = bitcast <2 x i64> %1444 to <4 x i32>
  %1526 = add <4 x i32> %1525, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1527 = bitcast <2 x i64> %1451 to <4 x i32>
  %1528 = add <4 x i32> %1527, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1529 = bitcast <2 x i64> %1458 to <4 x i32>
  %1530 = add <4 x i32> %1529, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1531 = bitcast <2 x i64> %1465 to <4 x i32>
  %1532 = add <4 x i32> %1531, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1533 = bitcast <2 x i64> %1472 to <4 x i32>
  %1534 = add <4 x i32> %1533, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1535 = bitcast <2 x i64> %1479 to <4 x i32>
  %1536 = add <4 x i32> %1535, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1537 = bitcast <2 x i64> %1486 to <4 x i32>
  %1538 = add <4 x i32> %1537, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1539 = bitcast <2 x i64> %1493 to <4 x i32>
  %1540 = add <4 x i32> %1539, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1541 = bitcast <2 x i64> %1500 to <4 x i32>
  %1542 = add <4 x i32> %1541, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1543 = bitcast <2 x i64> %1507 to <4 x i32>
  %1544 = add <4 x i32> %1543, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1545 = bitcast <2 x i64> %1514 to <4 x i32>
  %1546 = add <4 x i32> %1545, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1547 = ashr <4 x i32> %1516, <i32 14, i32 14, i32 14, i32 14>
  %1548 = ashr <4 x i32> %1518, <i32 14, i32 14, i32 14, i32 14>
  %1549 = ashr <4 x i32> %1520, <i32 14, i32 14, i32 14, i32 14>
  %1550 = ashr <4 x i32> %1522, <i32 14, i32 14, i32 14, i32 14>
  %1551 = ashr <4 x i32> %1524, <i32 14, i32 14, i32 14, i32 14>
  %1552 = ashr <4 x i32> %1526, <i32 14, i32 14, i32 14, i32 14>
  %1553 = ashr <4 x i32> %1528, <i32 14, i32 14, i32 14, i32 14>
  %1554 = ashr <4 x i32> %1530, <i32 14, i32 14, i32 14, i32 14>
  %1555 = ashr <4 x i32> %1532, <i32 14, i32 14, i32 14, i32 14>
  %1556 = ashr <4 x i32> %1534, <i32 14, i32 14, i32 14, i32 14>
  %1557 = ashr <4 x i32> %1536, <i32 14, i32 14, i32 14, i32 14>
  %1558 = ashr <4 x i32> %1538, <i32 14, i32 14, i32 14, i32 14>
  %1559 = ashr <4 x i32> %1540, <i32 14, i32 14, i32 14, i32 14>
  %1560 = ashr <4 x i32> %1542, <i32 14, i32 14, i32 14, i32 14>
  %1561 = ashr <4 x i32> %1544, <i32 14, i32 14, i32 14, i32 14>
  %1562 = ashr <4 x i32> %1546, <i32 14, i32 14, i32 14, i32 14>
  %1563 = add <4 x i32> %1239, %1039
  %1564 = add <4 x i32> %1240, %1040
  %1565 = sub <4 x i32> %1039, %1239
  %1566 = sub <4 x i32> %1040, %1240
  %1567 = sub <4 x i32> %1045, %1241
  %1568 = sub <4 x i32> %1046, %1242
  %1569 = add <4 x i32> %1241, %1045
  %1570 = add <4 x i32> %1242, %1046
  %1571 = shufflevector <4 x i32> %1135, <4 x i32> %1137, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1572 = bitcast <4 x i32> %1571 to <2 x i64>
  %1573 = shufflevector <4 x i32> %1135, <4 x i32> %1137, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1574 = bitcast <4 x i32> %1573 to <2 x i64>
  %1575 = shufflevector <4 x i32> %1136, <4 x i32> %1138, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1576 = bitcast <4 x i32> %1575 to <2 x i64>
  %1577 = shufflevector <4 x i32> %1136, <4 x i32> %1138, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1578 = bitcast <4 x i32> %1577 to <2 x i64>
  %1579 = shufflevector <4 x i32> %1139, <4 x i32> %1141, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1580 = bitcast <4 x i32> %1579 to <2 x i64>
  %1581 = shufflevector <4 x i32> %1139, <4 x i32> %1141, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1582 = bitcast <4 x i32> %1581 to <2 x i64>
  %1583 = shufflevector <4 x i32> %1140, <4 x i32> %1142, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1584 = bitcast <4 x i32> %1583 to <2 x i64>
  %1585 = shufflevector <4 x i32> %1140, <4 x i32> %1142, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1586 = bitcast <4 x i32> %1585 to <2 x i64>
  %1587 = and <2 x i64> %1572, <i64 4294967295, i64 4294967295>
  %1588 = mul nuw nsw <2 x i64> %1587, <i64 11585, i64 11585>
  %1589 = lshr <2 x i64> %1572, <i64 32, i64 32>
  %1590 = mul nuw nsw <2 x i64> %1589, <i64 11585, i64 11585>
  %1591 = add nuw nsw <2 x i64> %1590, %1588
  %1592 = and <2 x i64> %1574, <i64 4294967295, i64 4294967295>
  %1593 = mul nuw nsw <2 x i64> %1592, <i64 11585, i64 11585>
  %1594 = lshr <2 x i64> %1574, <i64 32, i64 32>
  %1595 = mul nuw nsw <2 x i64> %1594, <i64 11585, i64 11585>
  %1596 = add nuw nsw <2 x i64> %1595, %1593
  %1597 = and <2 x i64> %1576, <i64 4294967295, i64 4294967295>
  %1598 = mul nuw nsw <2 x i64> %1597, <i64 11585, i64 11585>
  %1599 = lshr <2 x i64> %1576, <i64 32, i64 32>
  %1600 = mul nuw nsw <2 x i64> %1599, <i64 11585, i64 11585>
  %1601 = add nuw nsw <2 x i64> %1600, %1598
  %1602 = and <2 x i64> %1578, <i64 4294967295, i64 4294967295>
  %1603 = mul nuw nsw <2 x i64> %1602, <i64 11585, i64 11585>
  %1604 = lshr <2 x i64> %1578, <i64 32, i64 32>
  %1605 = mul nuw nsw <2 x i64> %1604, <i64 11585, i64 11585>
  %1606 = add nuw nsw <2 x i64> %1605, %1603
  %1607 = mul nuw <2 x i64> %1589, <i64 4294955711, i64 4294955711>
  %1608 = add <2 x i64> %1607, %1588
  %1609 = mul nuw <2 x i64> %1594, <i64 4294955711, i64 4294955711>
  %1610 = add <2 x i64> %1609, %1593
  %1611 = mul nuw <2 x i64> %1599, <i64 4294955711, i64 4294955711>
  %1612 = add <2 x i64> %1611, %1598
  %1613 = mul nuw <2 x i64> %1604, <i64 4294955711, i64 4294955711>
  %1614 = add <2 x i64> %1613, %1603
  %1615 = and <2 x i64> %1580, <i64 4294967295, i64 4294967295>
  %1616 = mul nuw nsw <2 x i64> %1615, <i64 6270, i64 6270>
  %1617 = lshr <2 x i64> %1580, <i64 32, i64 32>
  %1618 = mul nuw nsw <2 x i64> %1617, <i64 15137, i64 15137>
  %1619 = add nuw nsw <2 x i64> %1618, %1616
  %1620 = and <2 x i64> %1582, <i64 4294967295, i64 4294967295>
  %1621 = mul nuw nsw <2 x i64> %1620, <i64 6270, i64 6270>
  %1622 = lshr <2 x i64> %1582, <i64 32, i64 32>
  %1623 = mul nuw nsw <2 x i64> %1622, <i64 15137, i64 15137>
  %1624 = add nuw nsw <2 x i64> %1623, %1621
  %1625 = and <2 x i64> %1584, <i64 4294967295, i64 4294967295>
  %1626 = mul nuw nsw <2 x i64> %1625, <i64 6270, i64 6270>
  %1627 = lshr <2 x i64> %1584, <i64 32, i64 32>
  %1628 = mul nuw nsw <2 x i64> %1627, <i64 15137, i64 15137>
  %1629 = add nuw nsw <2 x i64> %1628, %1626
  %1630 = and <2 x i64> %1586, <i64 4294967295, i64 4294967295>
  %1631 = mul nuw nsw <2 x i64> %1630, <i64 6270, i64 6270>
  %1632 = lshr <2 x i64> %1586, <i64 32, i64 32>
  %1633 = mul nuw nsw <2 x i64> %1632, <i64 15137, i64 15137>
  %1634 = add nuw nsw <2 x i64> %1633, %1631
  %1635 = mul nuw <2 x i64> %1615, <i64 4294952159, i64 4294952159>
  %1636 = mul nuw nsw <2 x i64> %1617, <i64 6270, i64 6270>
  %1637 = add <2 x i64> %1636, %1635
  %1638 = mul nuw <2 x i64> %1620, <i64 4294952159, i64 4294952159>
  %1639 = mul nuw nsw <2 x i64> %1622, <i64 6270, i64 6270>
  %1640 = add <2 x i64> %1639, %1638
  %1641 = mul nuw <2 x i64> %1625, <i64 4294952159, i64 4294952159>
  %1642 = mul nuw nsw <2 x i64> %1627, <i64 6270, i64 6270>
  %1643 = add <2 x i64> %1642, %1641
  %1644 = mul nuw <2 x i64> %1630, <i64 4294952159, i64 4294952159>
  %1645 = mul nuw nsw <2 x i64> %1632, <i64 6270, i64 6270>
  %1646 = add <2 x i64> %1645, %1644
  %1647 = bitcast <2 x i64> %1591 to <4 x i32>
  %1648 = shufflevector <4 x i32> %1647, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1649 = bitcast <4 x i32> %1648 to <2 x i64>
  %1650 = bitcast <2 x i64> %1596 to <4 x i32>
  %1651 = shufflevector <4 x i32> %1650, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1652 = bitcast <4 x i32> %1651 to <2 x i64>
  %1653 = shufflevector <2 x i64> %1649, <2 x i64> %1652, <2 x i32> <i32 0, i32 2>
  %1654 = bitcast <2 x i64> %1601 to <4 x i32>
  %1655 = shufflevector <4 x i32> %1654, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1656 = bitcast <4 x i32> %1655 to <2 x i64>
  %1657 = bitcast <2 x i64> %1606 to <4 x i32>
  %1658 = shufflevector <4 x i32> %1657, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1659 = bitcast <4 x i32> %1658 to <2 x i64>
  %1660 = shufflevector <2 x i64> %1656, <2 x i64> %1659, <2 x i32> <i32 0, i32 2>
  %1661 = bitcast <2 x i64> %1608 to <4 x i32>
  %1662 = shufflevector <4 x i32> %1661, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1663 = bitcast <4 x i32> %1662 to <2 x i64>
  %1664 = bitcast <2 x i64> %1610 to <4 x i32>
  %1665 = shufflevector <4 x i32> %1664, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1666 = bitcast <4 x i32> %1665 to <2 x i64>
  %1667 = shufflevector <2 x i64> %1663, <2 x i64> %1666, <2 x i32> <i32 0, i32 2>
  %1668 = bitcast <2 x i64> %1612 to <4 x i32>
  %1669 = shufflevector <4 x i32> %1668, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1670 = bitcast <4 x i32> %1669 to <2 x i64>
  %1671 = bitcast <2 x i64> %1614 to <4 x i32>
  %1672 = shufflevector <4 x i32> %1671, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1673 = bitcast <4 x i32> %1672 to <2 x i64>
  %1674 = shufflevector <2 x i64> %1670, <2 x i64> %1673, <2 x i32> <i32 0, i32 2>
  %1675 = bitcast <2 x i64> %1619 to <4 x i32>
  %1676 = shufflevector <4 x i32> %1675, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1677 = bitcast <4 x i32> %1676 to <2 x i64>
  %1678 = bitcast <2 x i64> %1624 to <4 x i32>
  %1679 = shufflevector <4 x i32> %1678, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1680 = bitcast <4 x i32> %1679 to <2 x i64>
  %1681 = shufflevector <2 x i64> %1677, <2 x i64> %1680, <2 x i32> <i32 0, i32 2>
  %1682 = bitcast <2 x i64> %1629 to <4 x i32>
  %1683 = shufflevector <4 x i32> %1682, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1684 = bitcast <4 x i32> %1683 to <2 x i64>
  %1685 = bitcast <2 x i64> %1634 to <4 x i32>
  %1686 = shufflevector <4 x i32> %1685, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1687 = bitcast <4 x i32> %1686 to <2 x i64>
  %1688 = shufflevector <2 x i64> %1684, <2 x i64> %1687, <2 x i32> <i32 0, i32 2>
  %1689 = bitcast <2 x i64> %1637 to <4 x i32>
  %1690 = shufflevector <4 x i32> %1689, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1691 = bitcast <4 x i32> %1690 to <2 x i64>
  %1692 = bitcast <2 x i64> %1640 to <4 x i32>
  %1693 = shufflevector <4 x i32> %1692, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1694 = bitcast <4 x i32> %1693 to <2 x i64>
  %1695 = shufflevector <2 x i64> %1691, <2 x i64> %1694, <2 x i32> <i32 0, i32 2>
  %1696 = bitcast <2 x i64> %1643 to <4 x i32>
  %1697 = shufflevector <4 x i32> %1696, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1698 = bitcast <4 x i32> %1697 to <2 x i64>
  %1699 = bitcast <2 x i64> %1646 to <4 x i32>
  %1700 = shufflevector <4 x i32> %1699, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1701 = bitcast <4 x i32> %1700 to <2 x i64>
  %1702 = shufflevector <2 x i64> %1698, <2 x i64> %1701, <2 x i32> <i32 0, i32 2>
  %1703 = bitcast <2 x i64> %1653 to <4 x i32>
  %1704 = add <4 x i32> %1703, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1705 = bitcast <2 x i64> %1660 to <4 x i32>
  %1706 = add <4 x i32> %1705, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1707 = bitcast <2 x i64> %1667 to <4 x i32>
  %1708 = add <4 x i32> %1707, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1709 = bitcast <2 x i64> %1674 to <4 x i32>
  %1710 = add <4 x i32> %1709, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1711 = bitcast <2 x i64> %1681 to <4 x i32>
  %1712 = add <4 x i32> %1711, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1713 = bitcast <2 x i64> %1688 to <4 x i32>
  %1714 = add <4 x i32> %1713, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1715 = bitcast <2 x i64> %1695 to <4 x i32>
  %1716 = add <4 x i32> %1715, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1717 = bitcast <2 x i64> %1702 to <4 x i32>
  %1718 = add <4 x i32> %1717, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1719 = ashr <4 x i32> %1704, <i32 14, i32 14, i32 14, i32 14>
  %1720 = ashr <4 x i32> %1706, <i32 14, i32 14, i32 14, i32 14>
  %1721 = ashr <4 x i32> %1708, <i32 14, i32 14, i32 14, i32 14>
  %1722 = ashr <4 x i32> %1710, <i32 14, i32 14, i32 14, i32 14>
  %1723 = ashr <4 x i32> %1712, <i32 14, i32 14, i32 14, i32 14>
  %1724 = ashr <4 x i32> %1714, <i32 14, i32 14, i32 14, i32 14>
  %1725 = ashr <4 x i32> %1716, <i32 14, i32 14, i32 14, i32 14>
  %1726 = ashr <4 x i32> %1718, <i32 14, i32 14, i32 14, i32 14>
  %1727 = lshr <4 x i32> %1704, <i32 31, i32 31, i32 31, i32 31>
  %1728 = lshr <4 x i32> %1706, <i32 31, i32 31, i32 31, i32 31>
  %1729 = lshr <4 x i32> %1708, <i32 31, i32 31, i32 31, i32 31>
  %1730 = lshr <4 x i32> %1710, <i32 31, i32 31, i32 31, i32 31>
  %1731 = lshr <4 x i32> %1712, <i32 31, i32 31, i32 31, i32 31>
  %1732 = lshr <4 x i32> %1714, <i32 31, i32 31, i32 31, i32 31>
  %1733 = lshr <4 x i32> %1716, <i32 31, i32 31, i32 31, i32 31>
  %1734 = lshr <4 x i32> %1718, <i32 31, i32 31, i32 31, i32 31>
  %1735 = add nuw nsw <4 x i32> %1727, <i32 1, i32 1, i32 1, i32 1>
  %1736 = add nsw <4 x i32> %1735, %1719
  %1737 = add nuw nsw <4 x i32> %1728, <i32 1, i32 1, i32 1, i32 1>
  %1738 = add nsw <4 x i32> %1737, %1720
  %1739 = add nuw nsw <4 x i32> %1729, <i32 1, i32 1, i32 1, i32 1>
  %1740 = add nsw <4 x i32> %1739, %1721
  %1741 = add nuw nsw <4 x i32> %1730, <i32 1, i32 1, i32 1, i32 1>
  %1742 = add nsw <4 x i32> %1741, %1722
  %1743 = add nuw nsw <4 x i32> %1731, <i32 1, i32 1, i32 1, i32 1>
  %1744 = add nsw <4 x i32> %1743, %1723
  %1745 = add nuw nsw <4 x i32> %1732, <i32 1, i32 1, i32 1, i32 1>
  %1746 = add nsw <4 x i32> %1745, %1724
  %1747 = add nuw nsw <4 x i32> %1733, <i32 1, i32 1, i32 1, i32 1>
  %1748 = add nsw <4 x i32> %1747, %1725
  %1749 = add nuw nsw <4 x i32> %1734, <i32 1, i32 1, i32 1, i32 1>
  %1750 = add nsw <4 x i32> %1749, %1726
  %1751 = ashr <4 x i32> %1736, <i32 2, i32 2, i32 2, i32 2>
  %1752 = ashr <4 x i32> %1738, <i32 2, i32 2, i32 2, i32 2>
  %1753 = ashr <4 x i32> %1740, <i32 2, i32 2, i32 2, i32 2>
  %1754 = ashr <4 x i32> %1742, <i32 2, i32 2, i32 2, i32 2>
  %1755 = ashr <4 x i32> %1744, <i32 2, i32 2, i32 2, i32 2>
  %1756 = ashr <4 x i32> %1746, <i32 2, i32 2, i32 2, i32 2>
  %1757 = ashr <4 x i32> %1748, <i32 2, i32 2, i32 2, i32 2>
  %1758 = ashr <4 x i32> %1750, <i32 2, i32 2, i32 2, i32 2>
  %1759 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1751, <4 x i32> %1752) #6
  store <8 x i16> %1759, <8 x i16>* %30, align 16
  %1760 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1753, <4 x i32> %1754) #6
  store <8 x i16> %1760, <8 x i16>* %32, align 16
  %1761 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1755, <4 x i32> %1756) #6
  store <8 x i16> %1761, <8 x i16>* %34, align 16
  %1762 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1757, <4 x i32> %1758) #6
  store <8 x i16> %1762, <8 x i16>* %36, align 16
  %1763 = shufflevector <4 x i32> %1148, <4 x i32> %1163, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1764 = bitcast <4 x i32> %1763 to <2 x i64>
  %1765 = shufflevector <4 x i32> %1148, <4 x i32> %1163, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1766 = bitcast <4 x i32> %1765 to <2 x i64>
  %1767 = shufflevector <4 x i32> %1150, <4 x i32> %1164, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1768 = bitcast <4 x i32> %1767 to <2 x i64>
  %1769 = shufflevector <4 x i32> %1150, <4 x i32> %1164, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1770 = bitcast <4 x i32> %1769 to <2 x i64>
  %1771 = shufflevector <4 x i32> %1151, <4 x i32> %1160, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1772 = bitcast <4 x i32> %1771 to <2 x i64>
  %1773 = shufflevector <4 x i32> %1151, <4 x i32> %1160, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1774 = bitcast <4 x i32> %1773 to <2 x i64>
  %1775 = shufflevector <4 x i32> %1152, <4 x i32> %1162, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1776 = bitcast <4 x i32> %1775 to <2 x i64>
  %1777 = shufflevector <4 x i32> %1152, <4 x i32> %1162, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1778 = bitcast <4 x i32> %1777 to <2 x i64>
  %1779 = and <2 x i64> %1764, <i64 4294967295, i64 4294967295>
  %1780 = mul nuw <2 x i64> %1779, <i64 4294952159, i64 4294952159>
  %1781 = lshr <2 x i64> %1764, <i64 32, i64 32>
  %1782 = mul nuw nsw <2 x i64> %1781, <i64 6270, i64 6270>
  %1783 = add <2 x i64> %1782, %1780
  %1784 = and <2 x i64> %1766, <i64 4294967295, i64 4294967295>
  %1785 = mul nuw <2 x i64> %1784, <i64 4294952159, i64 4294952159>
  %1786 = lshr <2 x i64> %1766, <i64 32, i64 32>
  %1787 = mul nuw nsw <2 x i64> %1786, <i64 6270, i64 6270>
  %1788 = add <2 x i64> %1787, %1785
  %1789 = and <2 x i64> %1768, <i64 4294967295, i64 4294967295>
  %1790 = mul nuw <2 x i64> %1789, <i64 4294952159, i64 4294952159>
  %1791 = lshr <2 x i64> %1768, <i64 32, i64 32>
  %1792 = mul nuw nsw <2 x i64> %1791, <i64 6270, i64 6270>
  %1793 = add <2 x i64> %1792, %1790
  %1794 = and <2 x i64> %1770, <i64 4294967295, i64 4294967295>
  %1795 = mul nuw <2 x i64> %1794, <i64 4294952159, i64 4294952159>
  %1796 = lshr <2 x i64> %1770, <i64 32, i64 32>
  %1797 = mul nuw nsw <2 x i64> %1796, <i64 6270, i64 6270>
  %1798 = add <2 x i64> %1797, %1795
  %1799 = and <2 x i64> %1772, <i64 4294967295, i64 4294967295>
  %1800 = mul nuw <2 x i64> %1799, <i64 4294961026, i64 4294961026>
  %1801 = lshr <2 x i64> %1772, <i64 32, i64 32>
  %1802 = mul nuw <2 x i64> %1801, <i64 4294952159, i64 4294952159>
  %1803 = add <2 x i64> %1802, %1800
  %1804 = and <2 x i64> %1774, <i64 4294967295, i64 4294967295>
  %1805 = mul nuw <2 x i64> %1804, <i64 4294961026, i64 4294961026>
  %1806 = lshr <2 x i64> %1774, <i64 32, i64 32>
  %1807 = mul nuw <2 x i64> %1806, <i64 4294952159, i64 4294952159>
  %1808 = add <2 x i64> %1807, %1805
  %1809 = and <2 x i64> %1776, <i64 4294967295, i64 4294967295>
  %1810 = mul nuw <2 x i64> %1809, <i64 4294961026, i64 4294961026>
  %1811 = lshr <2 x i64> %1776, <i64 32, i64 32>
  %1812 = mul nuw <2 x i64> %1811, <i64 4294952159, i64 4294952159>
  %1813 = add <2 x i64> %1812, %1810
  %1814 = and <2 x i64> %1778, <i64 4294967295, i64 4294967295>
  %1815 = mul nuw <2 x i64> %1814, <i64 4294961026, i64 4294961026>
  %1816 = lshr <2 x i64> %1778, <i64 32, i64 32>
  %1817 = mul nuw <2 x i64> %1816, <i64 4294952159, i64 4294952159>
  %1818 = add <2 x i64> %1817, %1815
  %1819 = mul nuw <2 x i64> %1799, <i64 4294952159, i64 4294952159>
  %1820 = mul nuw nsw <2 x i64> %1801, <i64 6270, i64 6270>
  %1821 = add <2 x i64> %1820, %1819
  %1822 = mul nuw <2 x i64> %1804, <i64 4294952159, i64 4294952159>
  %1823 = mul nuw nsw <2 x i64> %1806, <i64 6270, i64 6270>
  %1824 = add <2 x i64> %1823, %1822
  %1825 = mul nuw <2 x i64> %1809, <i64 4294952159, i64 4294952159>
  %1826 = mul nuw nsw <2 x i64> %1811, <i64 6270, i64 6270>
  %1827 = add <2 x i64> %1826, %1825
  %1828 = mul nuw <2 x i64> %1814, <i64 4294952159, i64 4294952159>
  %1829 = mul nuw nsw <2 x i64> %1816, <i64 6270, i64 6270>
  %1830 = add <2 x i64> %1829, %1828
  %1831 = mul nuw nsw <2 x i64> %1779, <i64 6270, i64 6270>
  %1832 = mul nuw nsw <2 x i64> %1781, <i64 15137, i64 15137>
  %1833 = add nuw nsw <2 x i64> %1832, %1831
  %1834 = mul nuw nsw <2 x i64> %1784, <i64 6270, i64 6270>
  %1835 = mul nuw nsw <2 x i64> %1786, <i64 15137, i64 15137>
  %1836 = add nuw nsw <2 x i64> %1835, %1834
  %1837 = mul nuw nsw <2 x i64> %1789, <i64 6270, i64 6270>
  %1838 = mul nuw nsw <2 x i64> %1791, <i64 15137, i64 15137>
  %1839 = add nuw nsw <2 x i64> %1838, %1837
  %1840 = mul nuw nsw <2 x i64> %1794, <i64 6270, i64 6270>
  %1841 = mul nuw nsw <2 x i64> %1796, <i64 15137, i64 15137>
  %1842 = add nuw nsw <2 x i64> %1841, %1840
  %1843 = bitcast <2 x i64> %1783 to <4 x i32>
  %1844 = shufflevector <4 x i32> %1843, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1845 = bitcast <4 x i32> %1844 to <2 x i64>
  %1846 = bitcast <2 x i64> %1788 to <4 x i32>
  %1847 = shufflevector <4 x i32> %1846, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1848 = bitcast <4 x i32> %1847 to <2 x i64>
  %1849 = shufflevector <2 x i64> %1845, <2 x i64> %1848, <2 x i32> <i32 0, i32 2>
  %1850 = bitcast <2 x i64> %1793 to <4 x i32>
  %1851 = shufflevector <4 x i32> %1850, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1852 = bitcast <4 x i32> %1851 to <2 x i64>
  %1853 = bitcast <2 x i64> %1798 to <4 x i32>
  %1854 = shufflevector <4 x i32> %1853, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1855 = bitcast <4 x i32> %1854 to <2 x i64>
  %1856 = shufflevector <2 x i64> %1852, <2 x i64> %1855, <2 x i32> <i32 0, i32 2>
  %1857 = bitcast <2 x i64> %1803 to <4 x i32>
  %1858 = shufflevector <4 x i32> %1857, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1859 = bitcast <4 x i32> %1858 to <2 x i64>
  %1860 = bitcast <2 x i64> %1808 to <4 x i32>
  %1861 = shufflevector <4 x i32> %1860, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1862 = bitcast <4 x i32> %1861 to <2 x i64>
  %1863 = shufflevector <2 x i64> %1859, <2 x i64> %1862, <2 x i32> <i32 0, i32 2>
  %1864 = bitcast <2 x i64> %1813 to <4 x i32>
  %1865 = shufflevector <4 x i32> %1864, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1866 = bitcast <4 x i32> %1865 to <2 x i64>
  %1867 = bitcast <2 x i64> %1818 to <4 x i32>
  %1868 = shufflevector <4 x i32> %1867, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1869 = bitcast <4 x i32> %1868 to <2 x i64>
  %1870 = shufflevector <2 x i64> %1866, <2 x i64> %1869, <2 x i32> <i32 0, i32 2>
  %1871 = bitcast <2 x i64> %1821 to <4 x i32>
  %1872 = shufflevector <4 x i32> %1871, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1873 = bitcast <4 x i32> %1872 to <2 x i64>
  %1874 = bitcast <2 x i64> %1824 to <4 x i32>
  %1875 = shufflevector <4 x i32> %1874, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1876 = bitcast <4 x i32> %1875 to <2 x i64>
  %1877 = shufflevector <2 x i64> %1873, <2 x i64> %1876, <2 x i32> <i32 0, i32 2>
  %1878 = bitcast <2 x i64> %1827 to <4 x i32>
  %1879 = shufflevector <4 x i32> %1878, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1880 = bitcast <4 x i32> %1879 to <2 x i64>
  %1881 = bitcast <2 x i64> %1830 to <4 x i32>
  %1882 = shufflevector <4 x i32> %1881, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1883 = bitcast <4 x i32> %1882 to <2 x i64>
  %1884 = shufflevector <2 x i64> %1880, <2 x i64> %1883, <2 x i32> <i32 0, i32 2>
  %1885 = bitcast <2 x i64> %1833 to <4 x i32>
  %1886 = shufflevector <4 x i32> %1885, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1887 = bitcast <4 x i32> %1886 to <2 x i64>
  %1888 = bitcast <2 x i64> %1836 to <4 x i32>
  %1889 = shufflevector <4 x i32> %1888, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1890 = bitcast <4 x i32> %1889 to <2 x i64>
  %1891 = shufflevector <2 x i64> %1887, <2 x i64> %1890, <2 x i32> <i32 0, i32 2>
  %1892 = bitcast <2 x i64> %1839 to <4 x i32>
  %1893 = shufflevector <4 x i32> %1892, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1894 = bitcast <4 x i32> %1893 to <2 x i64>
  %1895 = bitcast <2 x i64> %1842 to <4 x i32>
  %1896 = shufflevector <4 x i32> %1895, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %1897 = bitcast <4 x i32> %1896 to <2 x i64>
  %1898 = shufflevector <2 x i64> %1894, <2 x i64> %1897, <2 x i32> <i32 0, i32 2>
  %1899 = bitcast <2 x i64> %1849 to <4 x i32>
  %1900 = add <4 x i32> %1899, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1901 = bitcast <2 x i64> %1856 to <4 x i32>
  %1902 = add <4 x i32> %1901, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1903 = bitcast <2 x i64> %1863 to <4 x i32>
  %1904 = add <4 x i32> %1903, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1905 = bitcast <2 x i64> %1870 to <4 x i32>
  %1906 = add <4 x i32> %1905, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1907 = bitcast <2 x i64> %1877 to <4 x i32>
  %1908 = add <4 x i32> %1907, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1909 = bitcast <2 x i64> %1884 to <4 x i32>
  %1910 = add <4 x i32> %1909, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1911 = bitcast <2 x i64> %1891 to <4 x i32>
  %1912 = add <4 x i32> %1911, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1913 = bitcast <2 x i64> %1898 to <4 x i32>
  %1914 = add <4 x i32> %1913, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1915 = ashr <4 x i32> %1900, <i32 14, i32 14, i32 14, i32 14>
  %1916 = ashr <4 x i32> %1902, <i32 14, i32 14, i32 14, i32 14>
  %1917 = ashr <4 x i32> %1904, <i32 14, i32 14, i32 14, i32 14>
  %1918 = ashr <4 x i32> %1906, <i32 14, i32 14, i32 14, i32 14>
  %1919 = ashr <4 x i32> %1908, <i32 14, i32 14, i32 14, i32 14>
  %1920 = ashr <4 x i32> %1910, <i32 14, i32 14, i32 14, i32 14>
  %1921 = ashr <4 x i32> %1912, <i32 14, i32 14, i32 14, i32 14>
  %1922 = ashr <4 x i32> %1914, <i32 14, i32 14, i32 14, i32 14>
  %1923 = add <4 x i32> %1549, %1091
  %1924 = add <4 x i32> %1550, %1092
  %1925 = add <4 x i32> %1547, %1093
  %1926 = add <4 x i32> %1548, %1094
  %1927 = sub <4 x i32> %1093, %1547
  %1928 = sub <4 x i32> %1094, %1548
  %1929 = sub <4 x i32> %1091, %1549
  %1930 = sub <4 x i32> %1092, %1550
  %1931 = sub <4 x i32> %1105, %1551
  %1932 = sub <4 x i32> %1106, %1552
  %1933 = sub <4 x i32> %1103, %1553
  %1934 = sub <4 x i32> %1104, %1554
  %1935 = add <4 x i32> %1553, %1103
  %1936 = add <4 x i32> %1554, %1104
  %1937 = add <4 x i32> %1551, %1105
  %1938 = add <4 x i32> %1552, %1106
  %1939 = add <4 x i32> %1557, %1107
  %1940 = add <4 x i32> %1558, %1108
  %1941 = add <4 x i32> %1555, %1109
  %1942 = add <4 x i32> %1556, %1110
  %1943 = sub <4 x i32> %1109, %1555
  %1944 = sub <4 x i32> %1110, %1556
  %1945 = sub <4 x i32> %1107, %1557
  %1946 = sub <4 x i32> %1108, %1558
  %1947 = sub <4 x i32> %1121, %1559
  %1948 = sub <4 x i32> %1122, %1560
  %1949 = sub <4 x i32> %1119, %1561
  %1950 = sub <4 x i32> %1120, %1562
  %1951 = add <4 x i32> %1561, %1119
  %1952 = add <4 x i32> %1562, %1120
  %1953 = add <4 x i32> %1559, %1121
  %1954 = add <4 x i32> %1560, %1122
  %1955 = shufflevector <4 x i32> %1563, <4 x i32> %1569, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1956 = bitcast <4 x i32> %1955 to <2 x i64>
  %1957 = shufflevector <4 x i32> %1563, <4 x i32> %1569, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1958 = bitcast <4 x i32> %1957 to <2 x i64>
  %1959 = shufflevector <4 x i32> %1564, <4 x i32> %1570, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1960 = bitcast <4 x i32> %1959 to <2 x i64>
  %1961 = shufflevector <4 x i32> %1564, <4 x i32> %1570, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1962 = bitcast <4 x i32> %1961 to <2 x i64>
  %1963 = shufflevector <4 x i32> %1565, <4 x i32> %1567, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1964 = bitcast <4 x i32> %1963 to <2 x i64>
  %1965 = shufflevector <4 x i32> %1565, <4 x i32> %1567, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1966 = bitcast <4 x i32> %1965 to <2 x i64>
  %1967 = shufflevector <4 x i32> %1566, <4 x i32> %1568, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1968 = bitcast <4 x i32> %1967 to <2 x i64>
  %1969 = shufflevector <4 x i32> %1566, <4 x i32> %1568, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1970 = bitcast <4 x i32> %1969 to <2 x i64>
  %1971 = and <2 x i64> %1956, <i64 4294967295, i64 4294967295>
  %1972 = mul nuw nsw <2 x i64> %1971, <i64 3196, i64 3196>
  %1973 = lshr <2 x i64> %1956, <i64 32, i64 32>
  %1974 = mul nuw nsw <2 x i64> %1973, <i64 16069, i64 16069>
  %1975 = add nuw nsw <2 x i64> %1974, %1972
  %1976 = and <2 x i64> %1958, <i64 4294967295, i64 4294967295>
  %1977 = mul nuw nsw <2 x i64> %1976, <i64 3196, i64 3196>
  %1978 = lshr <2 x i64> %1958, <i64 32, i64 32>
  %1979 = mul nuw nsw <2 x i64> %1978, <i64 16069, i64 16069>
  %1980 = add nuw nsw <2 x i64> %1979, %1977
  %1981 = and <2 x i64> %1960, <i64 4294967295, i64 4294967295>
  %1982 = mul nuw nsw <2 x i64> %1981, <i64 3196, i64 3196>
  %1983 = lshr <2 x i64> %1960, <i64 32, i64 32>
  %1984 = mul nuw nsw <2 x i64> %1983, <i64 16069, i64 16069>
  %1985 = add nuw nsw <2 x i64> %1984, %1982
  %1986 = and <2 x i64> %1962, <i64 4294967295, i64 4294967295>
  %1987 = mul nuw nsw <2 x i64> %1986, <i64 3196, i64 3196>
  %1988 = lshr <2 x i64> %1962, <i64 32, i64 32>
  %1989 = mul nuw nsw <2 x i64> %1988, <i64 16069, i64 16069>
  %1990 = add nuw nsw <2 x i64> %1989, %1987
  %1991 = and <2 x i64> %1964, <i64 4294967295, i64 4294967295>
  %1992 = mul nuw nsw <2 x i64> %1991, <i64 13623, i64 13623>
  %1993 = lshr <2 x i64> %1964, <i64 32, i64 32>
  %1994 = mul nuw nsw <2 x i64> %1993, <i64 9102, i64 9102>
  %1995 = add nuw nsw <2 x i64> %1994, %1992
  %1996 = and <2 x i64> %1966, <i64 4294967295, i64 4294967295>
  %1997 = mul nuw nsw <2 x i64> %1996, <i64 13623, i64 13623>
  %1998 = lshr <2 x i64> %1966, <i64 32, i64 32>
  %1999 = mul nuw nsw <2 x i64> %1998, <i64 9102, i64 9102>
  %2000 = add nuw nsw <2 x i64> %1999, %1997
  %2001 = and <2 x i64> %1968, <i64 4294967295, i64 4294967295>
  %2002 = mul nuw nsw <2 x i64> %2001, <i64 13623, i64 13623>
  %2003 = lshr <2 x i64> %1968, <i64 32, i64 32>
  %2004 = mul nuw nsw <2 x i64> %2003, <i64 9102, i64 9102>
  %2005 = add nuw nsw <2 x i64> %2004, %2002
  %2006 = and <2 x i64> %1970, <i64 4294967295, i64 4294967295>
  %2007 = mul nuw nsw <2 x i64> %2006, <i64 13623, i64 13623>
  %2008 = lshr <2 x i64> %1970, <i64 32, i64 32>
  %2009 = mul nuw nsw <2 x i64> %2008, <i64 9102, i64 9102>
  %2010 = add nuw nsw <2 x i64> %2009, %2007
  %2011 = mul nuw <2 x i64> %1991, <i64 4294958194, i64 4294958194>
  %2012 = mul nuw nsw <2 x i64> %1993, <i64 13623, i64 13623>
  %2013 = add <2 x i64> %2012, %2011
  %2014 = mul nuw <2 x i64> %1996, <i64 4294958194, i64 4294958194>
  %2015 = mul nuw nsw <2 x i64> %1998, <i64 13623, i64 13623>
  %2016 = add <2 x i64> %2015, %2014
  %2017 = mul nuw <2 x i64> %2001, <i64 4294958194, i64 4294958194>
  %2018 = mul nuw nsw <2 x i64> %2003, <i64 13623, i64 13623>
  %2019 = add <2 x i64> %2018, %2017
  %2020 = mul nuw <2 x i64> %2006, <i64 4294958194, i64 4294958194>
  %2021 = mul nuw nsw <2 x i64> %2008, <i64 13623, i64 13623>
  %2022 = add <2 x i64> %2021, %2020
  %2023 = mul nuw <2 x i64> %1971, <i64 4294951227, i64 4294951227>
  %2024 = mul nuw nsw <2 x i64> %1973, <i64 3196, i64 3196>
  %2025 = add <2 x i64> %2024, %2023
  %2026 = mul nuw <2 x i64> %1976, <i64 4294951227, i64 4294951227>
  %2027 = mul nuw nsw <2 x i64> %1978, <i64 3196, i64 3196>
  %2028 = add <2 x i64> %2027, %2026
  %2029 = mul nuw <2 x i64> %1981, <i64 4294951227, i64 4294951227>
  %2030 = mul nuw nsw <2 x i64> %1983, <i64 3196, i64 3196>
  %2031 = add <2 x i64> %2030, %2029
  %2032 = mul nuw <2 x i64> %1986, <i64 4294951227, i64 4294951227>
  %2033 = mul nuw nsw <2 x i64> %1988, <i64 3196, i64 3196>
  %2034 = add <2 x i64> %2033, %2032
  %2035 = bitcast <2 x i64> %1975 to <4 x i32>
  %2036 = shufflevector <4 x i32> %2035, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2037 = bitcast <4 x i32> %2036 to <2 x i64>
  %2038 = bitcast <2 x i64> %1980 to <4 x i32>
  %2039 = shufflevector <4 x i32> %2038, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2040 = bitcast <4 x i32> %2039 to <2 x i64>
  %2041 = shufflevector <2 x i64> %2037, <2 x i64> %2040, <2 x i32> <i32 0, i32 2>
  %2042 = bitcast <2 x i64> %1985 to <4 x i32>
  %2043 = shufflevector <4 x i32> %2042, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2044 = bitcast <4 x i32> %2043 to <2 x i64>
  %2045 = bitcast <2 x i64> %1990 to <4 x i32>
  %2046 = shufflevector <4 x i32> %2045, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2047 = bitcast <4 x i32> %2046 to <2 x i64>
  %2048 = shufflevector <2 x i64> %2044, <2 x i64> %2047, <2 x i32> <i32 0, i32 2>
  %2049 = bitcast <2 x i64> %1995 to <4 x i32>
  %2050 = shufflevector <4 x i32> %2049, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2051 = bitcast <4 x i32> %2050 to <2 x i64>
  %2052 = bitcast <2 x i64> %2000 to <4 x i32>
  %2053 = shufflevector <4 x i32> %2052, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2054 = bitcast <4 x i32> %2053 to <2 x i64>
  %2055 = shufflevector <2 x i64> %2051, <2 x i64> %2054, <2 x i32> <i32 0, i32 2>
  %2056 = bitcast <2 x i64> %2005 to <4 x i32>
  %2057 = shufflevector <4 x i32> %2056, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2058 = bitcast <4 x i32> %2057 to <2 x i64>
  %2059 = bitcast <2 x i64> %2010 to <4 x i32>
  %2060 = shufflevector <4 x i32> %2059, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2061 = bitcast <4 x i32> %2060 to <2 x i64>
  %2062 = shufflevector <2 x i64> %2058, <2 x i64> %2061, <2 x i32> <i32 0, i32 2>
  %2063 = bitcast <2 x i64> %2013 to <4 x i32>
  %2064 = shufflevector <4 x i32> %2063, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2065 = bitcast <4 x i32> %2064 to <2 x i64>
  %2066 = bitcast <2 x i64> %2016 to <4 x i32>
  %2067 = shufflevector <4 x i32> %2066, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2068 = bitcast <4 x i32> %2067 to <2 x i64>
  %2069 = shufflevector <2 x i64> %2065, <2 x i64> %2068, <2 x i32> <i32 0, i32 2>
  %2070 = bitcast <2 x i64> %2019 to <4 x i32>
  %2071 = shufflevector <4 x i32> %2070, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2072 = bitcast <4 x i32> %2071 to <2 x i64>
  %2073 = bitcast <2 x i64> %2022 to <4 x i32>
  %2074 = shufflevector <4 x i32> %2073, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2075 = bitcast <4 x i32> %2074 to <2 x i64>
  %2076 = shufflevector <2 x i64> %2072, <2 x i64> %2075, <2 x i32> <i32 0, i32 2>
  %2077 = bitcast <2 x i64> %2025 to <4 x i32>
  %2078 = shufflevector <4 x i32> %2077, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2079 = bitcast <4 x i32> %2078 to <2 x i64>
  %2080 = bitcast <2 x i64> %2028 to <4 x i32>
  %2081 = shufflevector <4 x i32> %2080, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2082 = bitcast <4 x i32> %2081 to <2 x i64>
  %2083 = shufflevector <2 x i64> %2079, <2 x i64> %2082, <2 x i32> <i32 0, i32 2>
  %2084 = bitcast <2 x i64> %2031 to <4 x i32>
  %2085 = shufflevector <4 x i32> %2084, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2086 = bitcast <4 x i32> %2085 to <2 x i64>
  %2087 = bitcast <2 x i64> %2034 to <4 x i32>
  %2088 = shufflevector <4 x i32> %2087, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2089 = bitcast <4 x i32> %2088 to <2 x i64>
  %2090 = shufflevector <2 x i64> %2086, <2 x i64> %2089, <2 x i32> <i32 0, i32 2>
  %2091 = bitcast <2 x i64> %2041 to <4 x i32>
  %2092 = add <4 x i32> %2091, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2093 = bitcast <2 x i64> %2048 to <4 x i32>
  %2094 = add <4 x i32> %2093, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2095 = bitcast <2 x i64> %2055 to <4 x i32>
  %2096 = add <4 x i32> %2095, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2097 = bitcast <2 x i64> %2062 to <4 x i32>
  %2098 = add <4 x i32> %2097, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2099 = bitcast <2 x i64> %2069 to <4 x i32>
  %2100 = add <4 x i32> %2099, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2101 = bitcast <2 x i64> %2076 to <4 x i32>
  %2102 = add <4 x i32> %2101, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2103 = bitcast <2 x i64> %2083 to <4 x i32>
  %2104 = add <4 x i32> %2103, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2105 = bitcast <2 x i64> %2090 to <4 x i32>
  %2106 = add <4 x i32> %2105, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2107 = ashr <4 x i32> %2092, <i32 14, i32 14, i32 14, i32 14>
  %2108 = ashr <4 x i32> %2094, <i32 14, i32 14, i32 14, i32 14>
  %2109 = ashr <4 x i32> %2096, <i32 14, i32 14, i32 14, i32 14>
  %2110 = ashr <4 x i32> %2098, <i32 14, i32 14, i32 14, i32 14>
  %2111 = ashr <4 x i32> %2100, <i32 14, i32 14, i32 14, i32 14>
  %2112 = ashr <4 x i32> %2102, <i32 14, i32 14, i32 14, i32 14>
  %2113 = ashr <4 x i32> %2104, <i32 14, i32 14, i32 14, i32 14>
  %2114 = ashr <4 x i32> %2106, <i32 14, i32 14, i32 14, i32 14>
  %2115 = lshr <4 x i32> %2092, <i32 31, i32 31, i32 31, i32 31>
  %2116 = lshr <4 x i32> %2094, <i32 31, i32 31, i32 31, i32 31>
  %2117 = lshr <4 x i32> %2096, <i32 31, i32 31, i32 31, i32 31>
  %2118 = lshr <4 x i32> %2098, <i32 31, i32 31, i32 31, i32 31>
  %2119 = lshr <4 x i32> %2100, <i32 31, i32 31, i32 31, i32 31>
  %2120 = lshr <4 x i32> %2102, <i32 31, i32 31, i32 31, i32 31>
  %2121 = lshr <4 x i32> %2104, <i32 31, i32 31, i32 31, i32 31>
  %2122 = lshr <4 x i32> %2106, <i32 31, i32 31, i32 31, i32 31>
  %2123 = add nuw nsw <4 x i32> %2115, <i32 1, i32 1, i32 1, i32 1>
  %2124 = add nsw <4 x i32> %2123, %2107
  %2125 = add nuw nsw <4 x i32> %2116, <i32 1, i32 1, i32 1, i32 1>
  %2126 = add nsw <4 x i32> %2125, %2108
  %2127 = add nuw nsw <4 x i32> %2117, <i32 1, i32 1, i32 1, i32 1>
  %2128 = add nsw <4 x i32> %2127, %2109
  %2129 = add nuw nsw <4 x i32> %2118, <i32 1, i32 1, i32 1, i32 1>
  %2130 = add nsw <4 x i32> %2129, %2110
  %2131 = add nuw nsw <4 x i32> %2119, <i32 1, i32 1, i32 1, i32 1>
  %2132 = add nsw <4 x i32> %2131, %2111
  %2133 = add nuw nsw <4 x i32> %2120, <i32 1, i32 1, i32 1, i32 1>
  %2134 = add nsw <4 x i32> %2133, %2112
  %2135 = add nuw nsw <4 x i32> %2121, <i32 1, i32 1, i32 1, i32 1>
  %2136 = add nsw <4 x i32> %2135, %2113
  %2137 = add nuw nsw <4 x i32> %2122, <i32 1, i32 1, i32 1, i32 1>
  %2138 = add nsw <4 x i32> %2137, %2114
  %2139 = ashr <4 x i32> %2124, <i32 2, i32 2, i32 2, i32 2>
  %2140 = ashr <4 x i32> %2126, <i32 2, i32 2, i32 2, i32 2>
  %2141 = ashr <4 x i32> %2128, <i32 2, i32 2, i32 2, i32 2>
  %2142 = ashr <4 x i32> %2130, <i32 2, i32 2, i32 2, i32 2>
  %2143 = ashr <4 x i32> %2132, <i32 2, i32 2, i32 2, i32 2>
  %2144 = ashr <4 x i32> %2134, <i32 2, i32 2, i32 2, i32 2>
  %2145 = ashr <4 x i32> %2136, <i32 2, i32 2, i32 2, i32 2>
  %2146 = ashr <4 x i32> %2138, <i32 2, i32 2, i32 2, i32 2>
  %2147 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2139, <4 x i32> %2140) #6
  store <8 x i16> %2147, <8 x i16>* %38, align 16
  %2148 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2141, <4 x i32> %2142) #6
  store <8 x i16> %2148, <8 x i16>* %40, align 16
  %2149 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2143, <4 x i32> %2144) #6
  store <8 x i16> %2149, <8 x i16>* %42, align 16
  %2150 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2145, <4 x i32> %2146) #6
  store <8 x i16> %2150, <8 x i16>* %44, align 16
  %2151 = add <4 x i32> %1915, %1144
  %2152 = add <4 x i32> %1916, %1146
  %2153 = sub <4 x i32> %1144, %1915
  %2154 = sub <4 x i32> %1146, %1916
  %2155 = sub <4 x i32> %1153, %1917
  %2156 = sub <4 x i32> %1154, %1918
  %2157 = add <4 x i32> %1917, %1153
  %2158 = add <4 x i32> %1918, %1154
  %2159 = add <4 x i32> %1919, %1156
  %2160 = add <4 x i32> %1920, %1158
  %2161 = sub <4 x i32> %1156, %1919
  %2162 = sub <4 x i32> %1158, %1920
  %2163 = sub <4 x i32> %1165, %1921
  %2164 = sub <4 x i32> %1166, %1922
  %2165 = add <4 x i32> %1921, %1165
  %2166 = add <4 x i32> %1922, %1166
  %2167 = shufflevector <4 x i32> %1925, <4 x i32> %1951, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2168 = bitcast <4 x i32> %2167 to <2 x i64>
  %2169 = shufflevector <4 x i32> %1925, <4 x i32> %1951, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2170 = bitcast <4 x i32> %2169 to <2 x i64>
  %2171 = shufflevector <4 x i32> %1926, <4 x i32> %1952, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2172 = bitcast <4 x i32> %2171 to <2 x i64>
  %2173 = shufflevector <4 x i32> %1926, <4 x i32> %1952, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2174 = bitcast <4 x i32> %2173 to <2 x i64>
  %2175 = shufflevector <4 x i32> %1927, <4 x i32> %1949, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2176 = bitcast <4 x i32> %2175 to <2 x i64>
  %2177 = shufflevector <4 x i32> %1927, <4 x i32> %1949, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2178 = bitcast <4 x i32> %2177 to <2 x i64>
  %2179 = shufflevector <4 x i32> %1928, <4 x i32> %1950, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2180 = bitcast <4 x i32> %2179 to <2 x i64>
  %2181 = shufflevector <4 x i32> %1928, <4 x i32> %1950, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2182 = bitcast <4 x i32> %2181 to <2 x i64>
  %2183 = shufflevector <4 x i32> %1933, <4 x i32> %1943, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2184 = bitcast <4 x i32> %2183 to <2 x i64>
  %2185 = shufflevector <4 x i32> %1933, <4 x i32> %1943, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2186 = bitcast <4 x i32> %2185 to <2 x i64>
  %2187 = shufflevector <4 x i32> %1934, <4 x i32> %1944, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2188 = bitcast <4 x i32> %2187 to <2 x i64>
  %2189 = shufflevector <4 x i32> %1934, <4 x i32> %1944, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2190 = bitcast <4 x i32> %2189 to <2 x i64>
  %2191 = shufflevector <4 x i32> %1935, <4 x i32> %1941, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2192 = bitcast <4 x i32> %2191 to <2 x i64>
  %2193 = shufflevector <4 x i32> %1935, <4 x i32> %1941, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2194 = bitcast <4 x i32> %2193 to <2 x i64>
  %2195 = shufflevector <4 x i32> %1936, <4 x i32> %1942, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2196 = bitcast <4 x i32> %2195 to <2 x i64>
  %2197 = shufflevector <4 x i32> %1936, <4 x i32> %1942, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2198 = bitcast <4 x i32> %2197 to <2 x i64>
  %2199 = and <2 x i64> %2168, <i64 4294967295, i64 4294967295>
  %2200 = mul nuw <2 x i64> %2199, <i64 4294951227, i64 4294951227>
  %2201 = lshr <2 x i64> %2168, <i64 32, i64 32>
  %2202 = mul nuw nsw <2 x i64> %2201, <i64 3196, i64 3196>
  %2203 = add <2 x i64> %2202, %2200
  %2204 = and <2 x i64> %2170, <i64 4294967295, i64 4294967295>
  %2205 = mul nuw <2 x i64> %2204, <i64 4294951227, i64 4294951227>
  %2206 = lshr <2 x i64> %2170, <i64 32, i64 32>
  %2207 = mul nuw nsw <2 x i64> %2206, <i64 3196, i64 3196>
  %2208 = add <2 x i64> %2207, %2205
  %2209 = and <2 x i64> %2172, <i64 4294967295, i64 4294967295>
  %2210 = mul nuw <2 x i64> %2209, <i64 4294951227, i64 4294951227>
  %2211 = lshr <2 x i64> %2172, <i64 32, i64 32>
  %2212 = mul nuw nsw <2 x i64> %2211, <i64 3196, i64 3196>
  %2213 = add <2 x i64> %2212, %2210
  %2214 = and <2 x i64> %2174, <i64 4294967295, i64 4294967295>
  %2215 = mul nuw <2 x i64> %2214, <i64 4294951227, i64 4294951227>
  %2216 = lshr <2 x i64> %2174, <i64 32, i64 32>
  %2217 = mul nuw nsw <2 x i64> %2216, <i64 3196, i64 3196>
  %2218 = add <2 x i64> %2217, %2215
  %2219 = and <2 x i64> %2176, <i64 4294967295, i64 4294967295>
  %2220 = mul nuw <2 x i64> %2219, <i64 4294964100, i64 4294964100>
  %2221 = lshr <2 x i64> %2176, <i64 32, i64 32>
  %2222 = mul nuw <2 x i64> %2221, <i64 4294951227, i64 4294951227>
  %2223 = add <2 x i64> %2222, %2220
  %2224 = and <2 x i64> %2178, <i64 4294967295, i64 4294967295>
  %2225 = mul nuw <2 x i64> %2224, <i64 4294964100, i64 4294964100>
  %2226 = lshr <2 x i64> %2178, <i64 32, i64 32>
  %2227 = mul nuw <2 x i64> %2226, <i64 4294951227, i64 4294951227>
  %2228 = add <2 x i64> %2227, %2225
  %2229 = and <2 x i64> %2180, <i64 4294967295, i64 4294967295>
  %2230 = mul nuw <2 x i64> %2229, <i64 4294964100, i64 4294964100>
  %2231 = lshr <2 x i64> %2180, <i64 32, i64 32>
  %2232 = mul nuw <2 x i64> %2231, <i64 4294951227, i64 4294951227>
  %2233 = add <2 x i64> %2232, %2230
  %2234 = and <2 x i64> %2182, <i64 4294967295, i64 4294967295>
  %2235 = mul nuw <2 x i64> %2234, <i64 4294964100, i64 4294964100>
  %2236 = lshr <2 x i64> %2182, <i64 32, i64 32>
  %2237 = mul nuw <2 x i64> %2236, <i64 4294951227, i64 4294951227>
  %2238 = add <2 x i64> %2237, %2235
  %2239 = and <2 x i64> %2184, <i64 4294967295, i64 4294967295>
  %2240 = mul nuw <2 x i64> %2239, <i64 4294958194, i64 4294958194>
  %2241 = lshr <2 x i64> %2184, <i64 32, i64 32>
  %2242 = mul nuw nsw <2 x i64> %2241, <i64 13623, i64 13623>
  %2243 = add <2 x i64> %2242, %2240
  %2244 = and <2 x i64> %2186, <i64 4294967295, i64 4294967295>
  %2245 = mul nuw <2 x i64> %2244, <i64 4294958194, i64 4294958194>
  %2246 = lshr <2 x i64> %2186, <i64 32, i64 32>
  %2247 = mul nuw nsw <2 x i64> %2246, <i64 13623, i64 13623>
  %2248 = add <2 x i64> %2247, %2245
  %2249 = and <2 x i64> %2188, <i64 4294967295, i64 4294967295>
  %2250 = mul nuw <2 x i64> %2249, <i64 4294958194, i64 4294958194>
  %2251 = lshr <2 x i64> %2188, <i64 32, i64 32>
  %2252 = mul nuw nsw <2 x i64> %2251, <i64 13623, i64 13623>
  %2253 = add <2 x i64> %2252, %2250
  %2254 = and <2 x i64> %2190, <i64 4294967295, i64 4294967295>
  %2255 = mul nuw <2 x i64> %2254, <i64 4294958194, i64 4294958194>
  %2256 = lshr <2 x i64> %2190, <i64 32, i64 32>
  %2257 = mul nuw nsw <2 x i64> %2256, <i64 13623, i64 13623>
  %2258 = add <2 x i64> %2257, %2255
  %2259 = and <2 x i64> %2192, <i64 4294967295, i64 4294967295>
  %2260 = mul nuw <2 x i64> %2259, <i64 4294953673, i64 4294953673>
  %2261 = lshr <2 x i64> %2192, <i64 32, i64 32>
  %2262 = mul nuw <2 x i64> %2261, <i64 4294958194, i64 4294958194>
  %2263 = add <2 x i64> %2262, %2260
  %2264 = and <2 x i64> %2194, <i64 4294967295, i64 4294967295>
  %2265 = mul nuw <2 x i64> %2264, <i64 4294953673, i64 4294953673>
  %2266 = lshr <2 x i64> %2194, <i64 32, i64 32>
  %2267 = mul nuw <2 x i64> %2266, <i64 4294958194, i64 4294958194>
  %2268 = add <2 x i64> %2267, %2265
  %2269 = and <2 x i64> %2196, <i64 4294967295, i64 4294967295>
  %2270 = mul nuw <2 x i64> %2269, <i64 4294953673, i64 4294953673>
  %2271 = lshr <2 x i64> %2196, <i64 32, i64 32>
  %2272 = mul nuw <2 x i64> %2271, <i64 4294958194, i64 4294958194>
  %2273 = add <2 x i64> %2272, %2270
  %2274 = and <2 x i64> %2198, <i64 4294967295, i64 4294967295>
  %2275 = mul nuw <2 x i64> %2274, <i64 4294953673, i64 4294953673>
  %2276 = lshr <2 x i64> %2198, <i64 32, i64 32>
  %2277 = mul nuw <2 x i64> %2276, <i64 4294958194, i64 4294958194>
  %2278 = add <2 x i64> %2277, %2275
  %2279 = mul nuw <2 x i64> %2259, <i64 4294958194, i64 4294958194>
  %2280 = mul nuw nsw <2 x i64> %2261, <i64 13623, i64 13623>
  %2281 = add <2 x i64> %2280, %2279
  %2282 = mul nuw <2 x i64> %2264, <i64 4294958194, i64 4294958194>
  %2283 = mul nuw nsw <2 x i64> %2266, <i64 13623, i64 13623>
  %2284 = add <2 x i64> %2283, %2282
  %2285 = mul nuw <2 x i64> %2269, <i64 4294958194, i64 4294958194>
  %2286 = mul nuw nsw <2 x i64> %2271, <i64 13623, i64 13623>
  %2287 = add <2 x i64> %2286, %2285
  %2288 = mul nuw <2 x i64> %2274, <i64 4294958194, i64 4294958194>
  %2289 = mul nuw nsw <2 x i64> %2276, <i64 13623, i64 13623>
  %2290 = add <2 x i64> %2289, %2288
  %2291 = mul nuw nsw <2 x i64> %2239, <i64 13623, i64 13623>
  %2292 = mul nuw nsw <2 x i64> %2241, <i64 9102, i64 9102>
  %2293 = add nuw nsw <2 x i64> %2292, %2291
  %2294 = mul nuw nsw <2 x i64> %2244, <i64 13623, i64 13623>
  %2295 = mul nuw nsw <2 x i64> %2246, <i64 9102, i64 9102>
  %2296 = add nuw nsw <2 x i64> %2295, %2294
  %2297 = mul nuw nsw <2 x i64> %2249, <i64 13623, i64 13623>
  %2298 = mul nuw nsw <2 x i64> %2251, <i64 9102, i64 9102>
  %2299 = add nuw nsw <2 x i64> %2298, %2297
  %2300 = mul nuw nsw <2 x i64> %2254, <i64 13623, i64 13623>
  %2301 = mul nuw nsw <2 x i64> %2256, <i64 9102, i64 9102>
  %2302 = add nuw nsw <2 x i64> %2301, %2300
  %2303 = mul nuw <2 x i64> %2219, <i64 4294951227, i64 4294951227>
  %2304 = mul nuw nsw <2 x i64> %2221, <i64 3196, i64 3196>
  %2305 = add <2 x i64> %2304, %2303
  %2306 = mul nuw <2 x i64> %2224, <i64 4294951227, i64 4294951227>
  %2307 = mul nuw nsw <2 x i64> %2226, <i64 3196, i64 3196>
  %2308 = add <2 x i64> %2307, %2306
  %2309 = mul nuw <2 x i64> %2229, <i64 4294951227, i64 4294951227>
  %2310 = mul nuw nsw <2 x i64> %2231, <i64 3196, i64 3196>
  %2311 = add <2 x i64> %2310, %2309
  %2312 = mul nuw <2 x i64> %2234, <i64 4294951227, i64 4294951227>
  %2313 = mul nuw nsw <2 x i64> %2236, <i64 3196, i64 3196>
  %2314 = add <2 x i64> %2313, %2312
  %2315 = mul nuw nsw <2 x i64> %2199, <i64 3196, i64 3196>
  %2316 = mul nuw nsw <2 x i64> %2201, <i64 16069, i64 16069>
  %2317 = add nuw nsw <2 x i64> %2316, %2315
  %2318 = mul nuw nsw <2 x i64> %2204, <i64 3196, i64 3196>
  %2319 = mul nuw nsw <2 x i64> %2206, <i64 16069, i64 16069>
  %2320 = add nuw nsw <2 x i64> %2319, %2318
  %2321 = mul nuw nsw <2 x i64> %2209, <i64 3196, i64 3196>
  %2322 = mul nuw nsw <2 x i64> %2211, <i64 16069, i64 16069>
  %2323 = add nuw nsw <2 x i64> %2322, %2321
  %2324 = mul nuw nsw <2 x i64> %2214, <i64 3196, i64 3196>
  %2325 = mul nuw nsw <2 x i64> %2216, <i64 16069, i64 16069>
  %2326 = add nuw nsw <2 x i64> %2325, %2324
  %2327 = bitcast <2 x i64> %2203 to <4 x i32>
  %2328 = shufflevector <4 x i32> %2327, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2329 = bitcast <4 x i32> %2328 to <2 x i64>
  %2330 = bitcast <2 x i64> %2208 to <4 x i32>
  %2331 = shufflevector <4 x i32> %2330, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2332 = bitcast <4 x i32> %2331 to <2 x i64>
  %2333 = shufflevector <2 x i64> %2329, <2 x i64> %2332, <2 x i32> <i32 0, i32 2>
  %2334 = bitcast <2 x i64> %2213 to <4 x i32>
  %2335 = shufflevector <4 x i32> %2334, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2336 = bitcast <4 x i32> %2335 to <2 x i64>
  %2337 = bitcast <2 x i64> %2218 to <4 x i32>
  %2338 = shufflevector <4 x i32> %2337, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2339 = bitcast <4 x i32> %2338 to <2 x i64>
  %2340 = shufflevector <2 x i64> %2336, <2 x i64> %2339, <2 x i32> <i32 0, i32 2>
  %2341 = bitcast <2 x i64> %2223 to <4 x i32>
  %2342 = shufflevector <4 x i32> %2341, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2343 = bitcast <4 x i32> %2342 to <2 x i64>
  %2344 = bitcast <2 x i64> %2228 to <4 x i32>
  %2345 = shufflevector <4 x i32> %2344, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2346 = bitcast <4 x i32> %2345 to <2 x i64>
  %2347 = shufflevector <2 x i64> %2343, <2 x i64> %2346, <2 x i32> <i32 0, i32 2>
  %2348 = bitcast <2 x i64> %2233 to <4 x i32>
  %2349 = shufflevector <4 x i32> %2348, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2350 = bitcast <4 x i32> %2349 to <2 x i64>
  %2351 = bitcast <2 x i64> %2238 to <4 x i32>
  %2352 = shufflevector <4 x i32> %2351, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2353 = bitcast <4 x i32> %2352 to <2 x i64>
  %2354 = shufflevector <2 x i64> %2350, <2 x i64> %2353, <2 x i32> <i32 0, i32 2>
  %2355 = bitcast <2 x i64> %2243 to <4 x i32>
  %2356 = shufflevector <4 x i32> %2355, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2357 = bitcast <4 x i32> %2356 to <2 x i64>
  %2358 = bitcast <2 x i64> %2248 to <4 x i32>
  %2359 = shufflevector <4 x i32> %2358, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2360 = bitcast <4 x i32> %2359 to <2 x i64>
  %2361 = shufflevector <2 x i64> %2357, <2 x i64> %2360, <2 x i32> <i32 0, i32 2>
  %2362 = bitcast <2 x i64> %2253 to <4 x i32>
  %2363 = shufflevector <4 x i32> %2362, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2364 = bitcast <4 x i32> %2363 to <2 x i64>
  %2365 = bitcast <2 x i64> %2258 to <4 x i32>
  %2366 = shufflevector <4 x i32> %2365, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2367 = bitcast <4 x i32> %2366 to <2 x i64>
  %2368 = shufflevector <2 x i64> %2364, <2 x i64> %2367, <2 x i32> <i32 0, i32 2>
  %2369 = bitcast <2 x i64> %2263 to <4 x i32>
  %2370 = shufflevector <4 x i32> %2369, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2371 = bitcast <4 x i32> %2370 to <2 x i64>
  %2372 = bitcast <2 x i64> %2268 to <4 x i32>
  %2373 = shufflevector <4 x i32> %2372, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2374 = bitcast <4 x i32> %2373 to <2 x i64>
  %2375 = shufflevector <2 x i64> %2371, <2 x i64> %2374, <2 x i32> <i32 0, i32 2>
  %2376 = bitcast <2 x i64> %2273 to <4 x i32>
  %2377 = shufflevector <4 x i32> %2376, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2378 = bitcast <4 x i32> %2377 to <2 x i64>
  %2379 = bitcast <2 x i64> %2278 to <4 x i32>
  %2380 = shufflevector <4 x i32> %2379, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2381 = bitcast <4 x i32> %2380 to <2 x i64>
  %2382 = shufflevector <2 x i64> %2378, <2 x i64> %2381, <2 x i32> <i32 0, i32 2>
  %2383 = bitcast <2 x i64> %2281 to <4 x i32>
  %2384 = shufflevector <4 x i32> %2383, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2385 = bitcast <4 x i32> %2384 to <2 x i64>
  %2386 = bitcast <2 x i64> %2284 to <4 x i32>
  %2387 = shufflevector <4 x i32> %2386, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2388 = bitcast <4 x i32> %2387 to <2 x i64>
  %2389 = shufflevector <2 x i64> %2385, <2 x i64> %2388, <2 x i32> <i32 0, i32 2>
  %2390 = bitcast <2 x i64> %2287 to <4 x i32>
  %2391 = shufflevector <4 x i32> %2390, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2392 = bitcast <4 x i32> %2391 to <2 x i64>
  %2393 = bitcast <2 x i64> %2290 to <4 x i32>
  %2394 = shufflevector <4 x i32> %2393, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2395 = bitcast <4 x i32> %2394 to <2 x i64>
  %2396 = shufflevector <2 x i64> %2392, <2 x i64> %2395, <2 x i32> <i32 0, i32 2>
  %2397 = bitcast <2 x i64> %2293 to <4 x i32>
  %2398 = shufflevector <4 x i32> %2397, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2399 = bitcast <4 x i32> %2398 to <2 x i64>
  %2400 = bitcast <2 x i64> %2296 to <4 x i32>
  %2401 = shufflevector <4 x i32> %2400, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2402 = bitcast <4 x i32> %2401 to <2 x i64>
  %2403 = shufflevector <2 x i64> %2399, <2 x i64> %2402, <2 x i32> <i32 0, i32 2>
  %2404 = bitcast <2 x i64> %2299 to <4 x i32>
  %2405 = shufflevector <4 x i32> %2404, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2406 = bitcast <4 x i32> %2405 to <2 x i64>
  %2407 = bitcast <2 x i64> %2302 to <4 x i32>
  %2408 = shufflevector <4 x i32> %2407, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2409 = bitcast <4 x i32> %2408 to <2 x i64>
  %2410 = shufflevector <2 x i64> %2406, <2 x i64> %2409, <2 x i32> <i32 0, i32 2>
  %2411 = bitcast <2 x i64> %2305 to <4 x i32>
  %2412 = shufflevector <4 x i32> %2411, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2413 = bitcast <4 x i32> %2412 to <2 x i64>
  %2414 = bitcast <2 x i64> %2308 to <4 x i32>
  %2415 = shufflevector <4 x i32> %2414, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2416 = bitcast <4 x i32> %2415 to <2 x i64>
  %2417 = shufflevector <2 x i64> %2413, <2 x i64> %2416, <2 x i32> <i32 0, i32 2>
  %2418 = bitcast <2 x i64> %2311 to <4 x i32>
  %2419 = shufflevector <4 x i32> %2418, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2420 = bitcast <4 x i32> %2419 to <2 x i64>
  %2421 = bitcast <2 x i64> %2314 to <4 x i32>
  %2422 = shufflevector <4 x i32> %2421, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2423 = bitcast <4 x i32> %2422 to <2 x i64>
  %2424 = shufflevector <2 x i64> %2420, <2 x i64> %2423, <2 x i32> <i32 0, i32 2>
  %2425 = bitcast <2 x i64> %2317 to <4 x i32>
  %2426 = shufflevector <4 x i32> %2425, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2427 = bitcast <4 x i32> %2426 to <2 x i64>
  %2428 = bitcast <2 x i64> %2320 to <4 x i32>
  %2429 = shufflevector <4 x i32> %2428, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2430 = bitcast <4 x i32> %2429 to <2 x i64>
  %2431 = shufflevector <2 x i64> %2427, <2 x i64> %2430, <2 x i32> <i32 0, i32 2>
  %2432 = bitcast <2 x i64> %2323 to <4 x i32>
  %2433 = shufflevector <4 x i32> %2432, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2434 = bitcast <4 x i32> %2433 to <2 x i64>
  %2435 = bitcast <2 x i64> %2326 to <4 x i32>
  %2436 = shufflevector <4 x i32> %2435, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2437 = bitcast <4 x i32> %2436 to <2 x i64>
  %2438 = shufflevector <2 x i64> %2434, <2 x i64> %2437, <2 x i32> <i32 0, i32 2>
  %2439 = bitcast <2 x i64> %2333 to <4 x i32>
  %2440 = add <4 x i32> %2439, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2441 = bitcast <2 x i64> %2340 to <4 x i32>
  %2442 = add <4 x i32> %2441, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2443 = bitcast <2 x i64> %2347 to <4 x i32>
  %2444 = add <4 x i32> %2443, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2445 = bitcast <2 x i64> %2354 to <4 x i32>
  %2446 = add <4 x i32> %2445, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2447 = bitcast <2 x i64> %2361 to <4 x i32>
  %2448 = add <4 x i32> %2447, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2449 = bitcast <2 x i64> %2368 to <4 x i32>
  %2450 = add <4 x i32> %2449, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2451 = bitcast <2 x i64> %2375 to <4 x i32>
  %2452 = add <4 x i32> %2451, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2453 = bitcast <2 x i64> %2382 to <4 x i32>
  %2454 = add <4 x i32> %2453, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2455 = bitcast <2 x i64> %2389 to <4 x i32>
  %2456 = add <4 x i32> %2455, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2457 = bitcast <2 x i64> %2396 to <4 x i32>
  %2458 = add <4 x i32> %2457, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2459 = bitcast <2 x i64> %2403 to <4 x i32>
  %2460 = add <4 x i32> %2459, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2461 = bitcast <2 x i64> %2410 to <4 x i32>
  %2462 = add <4 x i32> %2461, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2463 = bitcast <2 x i64> %2417 to <4 x i32>
  %2464 = add <4 x i32> %2463, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2465 = bitcast <2 x i64> %2424 to <4 x i32>
  %2466 = add <4 x i32> %2465, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2467 = bitcast <2 x i64> %2431 to <4 x i32>
  %2468 = add <4 x i32> %2467, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2469 = bitcast <2 x i64> %2438 to <4 x i32>
  %2470 = add <4 x i32> %2469, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2471 = ashr <4 x i32> %2440, <i32 14, i32 14, i32 14, i32 14>
  %2472 = ashr <4 x i32> %2442, <i32 14, i32 14, i32 14, i32 14>
  %2473 = ashr <4 x i32> %2444, <i32 14, i32 14, i32 14, i32 14>
  %2474 = ashr <4 x i32> %2446, <i32 14, i32 14, i32 14, i32 14>
  %2475 = ashr <4 x i32> %2448, <i32 14, i32 14, i32 14, i32 14>
  %2476 = ashr <4 x i32> %2450, <i32 14, i32 14, i32 14, i32 14>
  %2477 = ashr <4 x i32> %2452, <i32 14, i32 14, i32 14, i32 14>
  %2478 = ashr <4 x i32> %2454, <i32 14, i32 14, i32 14, i32 14>
  %2479 = ashr <4 x i32> %2456, <i32 14, i32 14, i32 14, i32 14>
  %2480 = ashr <4 x i32> %2458, <i32 14, i32 14, i32 14, i32 14>
  %2481 = ashr <4 x i32> %2460, <i32 14, i32 14, i32 14, i32 14>
  %2482 = ashr <4 x i32> %2462, <i32 14, i32 14, i32 14, i32 14>
  %2483 = ashr <4 x i32> %2464, <i32 14, i32 14, i32 14, i32 14>
  %2484 = ashr <4 x i32> %2466, <i32 14, i32 14, i32 14, i32 14>
  %2485 = ashr <4 x i32> %2468, <i32 14, i32 14, i32 14, i32 14>
  %2486 = ashr <4 x i32> %2470, <i32 14, i32 14, i32 14, i32 14>
  %2487 = shufflevector <4 x i32> %2151, <4 x i32> %2165, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2488 = bitcast <4 x i32> %2487 to <2 x i64>
  %2489 = shufflevector <4 x i32> %2151, <4 x i32> %2165, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2490 = bitcast <4 x i32> %2489 to <2 x i64>
  %2491 = shufflevector <4 x i32> %2152, <4 x i32> %2166, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2492 = bitcast <4 x i32> %2491 to <2 x i64>
  %2493 = shufflevector <4 x i32> %2152, <4 x i32> %2166, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2494 = bitcast <4 x i32> %2493 to <2 x i64>
  %2495 = shufflevector <4 x i32> %2153, <4 x i32> %2163, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2496 = bitcast <4 x i32> %2495 to <2 x i64>
  %2497 = shufflevector <4 x i32> %2153, <4 x i32> %2163, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2498 = bitcast <4 x i32> %2497 to <2 x i64>
  %2499 = shufflevector <4 x i32> %2154, <4 x i32> %2164, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2500 = bitcast <4 x i32> %2499 to <2 x i64>
  %2501 = shufflevector <4 x i32> %2154, <4 x i32> %2164, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2502 = bitcast <4 x i32> %2501 to <2 x i64>
  %2503 = shufflevector <4 x i32> %2155, <4 x i32> %2161, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2504 = bitcast <4 x i32> %2503 to <2 x i64>
  %2505 = shufflevector <4 x i32> %2155, <4 x i32> %2161, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2506 = bitcast <4 x i32> %2505 to <2 x i64>
  %2507 = shufflevector <4 x i32> %2156, <4 x i32> %2162, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2508 = bitcast <4 x i32> %2507 to <2 x i64>
  %2509 = shufflevector <4 x i32> %2156, <4 x i32> %2162, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2510 = bitcast <4 x i32> %2509 to <2 x i64>
  %2511 = shufflevector <4 x i32> %2157, <4 x i32> %2159, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2512 = bitcast <4 x i32> %2511 to <2 x i64>
  %2513 = shufflevector <4 x i32> %2157, <4 x i32> %2159, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2514 = bitcast <4 x i32> %2513 to <2 x i64>
  %2515 = shufflevector <4 x i32> %2158, <4 x i32> %2160, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2516 = bitcast <4 x i32> %2515 to <2 x i64>
  %2517 = shufflevector <4 x i32> %2158, <4 x i32> %2160, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2518 = bitcast <4 x i32> %2517 to <2 x i64>
  %2519 = and <2 x i64> %2488, <i64 4294967295, i64 4294967295>
  %2520 = mul nuw nsw <2 x i64> %2519, <i64 1606, i64 1606>
  %2521 = lshr <2 x i64> %2488, <i64 32, i64 32>
  %2522 = mul nuw nsw <2 x i64> %2521, <i64 16305, i64 16305>
  %2523 = add nuw nsw <2 x i64> %2522, %2520
  %2524 = and <2 x i64> %2490, <i64 4294967295, i64 4294967295>
  %2525 = mul nuw nsw <2 x i64> %2524, <i64 1606, i64 1606>
  %2526 = lshr <2 x i64> %2490, <i64 32, i64 32>
  %2527 = mul nuw nsw <2 x i64> %2526, <i64 16305, i64 16305>
  %2528 = add nuw nsw <2 x i64> %2527, %2525
  %2529 = and <2 x i64> %2492, <i64 4294967295, i64 4294967295>
  %2530 = mul nuw nsw <2 x i64> %2529, <i64 1606, i64 1606>
  %2531 = lshr <2 x i64> %2492, <i64 32, i64 32>
  %2532 = mul nuw nsw <2 x i64> %2531, <i64 16305, i64 16305>
  %2533 = add nuw nsw <2 x i64> %2532, %2530
  %2534 = and <2 x i64> %2494, <i64 4294967295, i64 4294967295>
  %2535 = mul nuw nsw <2 x i64> %2534, <i64 1606, i64 1606>
  %2536 = lshr <2 x i64> %2494, <i64 32, i64 32>
  %2537 = mul nuw nsw <2 x i64> %2536, <i64 16305, i64 16305>
  %2538 = add nuw nsw <2 x i64> %2537, %2535
  %2539 = and <2 x i64> %2496, <i64 4294967295, i64 4294967295>
  %2540 = mul nuw nsw <2 x i64> %2539, <i64 12665, i64 12665>
  %2541 = lshr <2 x i64> %2496, <i64 32, i64 32>
  %2542 = mul nuw nsw <2 x i64> %2541, <i64 10394, i64 10394>
  %2543 = add nuw nsw <2 x i64> %2542, %2540
  %2544 = and <2 x i64> %2498, <i64 4294967295, i64 4294967295>
  %2545 = mul nuw nsw <2 x i64> %2544, <i64 12665, i64 12665>
  %2546 = lshr <2 x i64> %2498, <i64 32, i64 32>
  %2547 = mul nuw nsw <2 x i64> %2546, <i64 10394, i64 10394>
  %2548 = add nuw nsw <2 x i64> %2547, %2545
  %2549 = and <2 x i64> %2500, <i64 4294967295, i64 4294967295>
  %2550 = mul nuw nsw <2 x i64> %2549, <i64 12665, i64 12665>
  %2551 = lshr <2 x i64> %2500, <i64 32, i64 32>
  %2552 = mul nuw nsw <2 x i64> %2551, <i64 10394, i64 10394>
  %2553 = add nuw nsw <2 x i64> %2552, %2550
  %2554 = and <2 x i64> %2502, <i64 4294967295, i64 4294967295>
  %2555 = mul nuw nsw <2 x i64> %2554, <i64 12665, i64 12665>
  %2556 = lshr <2 x i64> %2502, <i64 32, i64 32>
  %2557 = mul nuw nsw <2 x i64> %2556, <i64 10394, i64 10394>
  %2558 = add nuw nsw <2 x i64> %2557, %2555
  %2559 = and <2 x i64> %2504, <i64 4294967295, i64 4294967295>
  %2560 = mul nuw nsw <2 x i64> %2559, <i64 7723, i64 7723>
  %2561 = lshr <2 x i64> %2504, <i64 32, i64 32>
  %2562 = mul nuw nsw <2 x i64> %2561, <i64 14449, i64 14449>
  %2563 = add nuw nsw <2 x i64> %2562, %2560
  %2564 = and <2 x i64> %2506, <i64 4294967295, i64 4294967295>
  %2565 = mul nuw nsw <2 x i64> %2564, <i64 7723, i64 7723>
  %2566 = lshr <2 x i64> %2506, <i64 32, i64 32>
  %2567 = mul nuw nsw <2 x i64> %2566, <i64 14449, i64 14449>
  %2568 = add nuw nsw <2 x i64> %2567, %2565
  %2569 = and <2 x i64> %2508, <i64 4294967295, i64 4294967295>
  %2570 = mul nuw nsw <2 x i64> %2569, <i64 7723, i64 7723>
  %2571 = lshr <2 x i64> %2508, <i64 32, i64 32>
  %2572 = mul nuw nsw <2 x i64> %2571, <i64 14449, i64 14449>
  %2573 = add nuw nsw <2 x i64> %2572, %2570
  %2574 = and <2 x i64> %2510, <i64 4294967295, i64 4294967295>
  %2575 = mul nuw nsw <2 x i64> %2574, <i64 7723, i64 7723>
  %2576 = lshr <2 x i64> %2510, <i64 32, i64 32>
  %2577 = mul nuw nsw <2 x i64> %2576, <i64 14449, i64 14449>
  %2578 = add nuw nsw <2 x i64> %2577, %2575
  %2579 = and <2 x i64> %2512, <i64 4294967295, i64 4294967295>
  %2580 = mul nuw nsw <2 x i64> %2579, <i64 15679, i64 15679>
  %2581 = lshr <2 x i64> %2512, <i64 32, i64 32>
  %2582 = mul nuw nsw <2 x i64> %2581, <i64 4756, i64 4756>
  %2583 = add nuw nsw <2 x i64> %2582, %2580
  %2584 = and <2 x i64> %2514, <i64 4294967295, i64 4294967295>
  %2585 = mul nuw nsw <2 x i64> %2584, <i64 15679, i64 15679>
  %2586 = lshr <2 x i64> %2514, <i64 32, i64 32>
  %2587 = mul nuw nsw <2 x i64> %2586, <i64 4756, i64 4756>
  %2588 = add nuw nsw <2 x i64> %2587, %2585
  %2589 = and <2 x i64> %2516, <i64 4294967295, i64 4294967295>
  %2590 = mul nuw nsw <2 x i64> %2589, <i64 15679, i64 15679>
  %2591 = lshr <2 x i64> %2516, <i64 32, i64 32>
  %2592 = mul nuw nsw <2 x i64> %2591, <i64 4756, i64 4756>
  %2593 = add nuw nsw <2 x i64> %2592, %2590
  %2594 = and <2 x i64> %2518, <i64 4294967295, i64 4294967295>
  %2595 = mul nuw nsw <2 x i64> %2594, <i64 15679, i64 15679>
  %2596 = lshr <2 x i64> %2518, <i64 32, i64 32>
  %2597 = mul nuw nsw <2 x i64> %2596, <i64 4756, i64 4756>
  %2598 = add nuw nsw <2 x i64> %2597, %2595
  %2599 = mul nuw <2 x i64> %2579, <i64 4294962540, i64 4294962540>
  %2600 = mul nuw nsw <2 x i64> %2581, <i64 15679, i64 15679>
  %2601 = add <2 x i64> %2600, %2599
  %2602 = mul nuw <2 x i64> %2584, <i64 4294962540, i64 4294962540>
  %2603 = mul nuw nsw <2 x i64> %2586, <i64 15679, i64 15679>
  %2604 = add <2 x i64> %2603, %2602
  %2605 = mul nuw <2 x i64> %2589, <i64 4294962540, i64 4294962540>
  %2606 = mul nuw nsw <2 x i64> %2591, <i64 15679, i64 15679>
  %2607 = add <2 x i64> %2606, %2605
  %2608 = mul nuw <2 x i64> %2594, <i64 4294962540, i64 4294962540>
  %2609 = mul nuw nsw <2 x i64> %2596, <i64 15679, i64 15679>
  %2610 = add <2 x i64> %2609, %2608
  %2611 = mul nuw <2 x i64> %2559, <i64 4294952847, i64 4294952847>
  %2612 = mul nuw nsw <2 x i64> %2561, <i64 7723, i64 7723>
  %2613 = add <2 x i64> %2612, %2611
  %2614 = mul nuw <2 x i64> %2564, <i64 4294952847, i64 4294952847>
  %2615 = mul nuw nsw <2 x i64> %2566, <i64 7723, i64 7723>
  %2616 = add <2 x i64> %2615, %2614
  %2617 = mul nuw <2 x i64> %2569, <i64 4294952847, i64 4294952847>
  %2618 = mul nuw nsw <2 x i64> %2571, <i64 7723, i64 7723>
  %2619 = add <2 x i64> %2618, %2617
  %2620 = mul nuw <2 x i64> %2574, <i64 4294952847, i64 4294952847>
  %2621 = mul nuw nsw <2 x i64> %2576, <i64 7723, i64 7723>
  %2622 = add <2 x i64> %2621, %2620
  %2623 = mul nuw <2 x i64> %2539, <i64 4294956902, i64 4294956902>
  %2624 = mul nuw nsw <2 x i64> %2541, <i64 12665, i64 12665>
  %2625 = add <2 x i64> %2624, %2623
  %2626 = mul nuw <2 x i64> %2544, <i64 4294956902, i64 4294956902>
  %2627 = mul nuw nsw <2 x i64> %2546, <i64 12665, i64 12665>
  %2628 = add <2 x i64> %2627, %2626
  %2629 = mul nuw <2 x i64> %2549, <i64 4294956902, i64 4294956902>
  %2630 = mul nuw nsw <2 x i64> %2551, <i64 12665, i64 12665>
  %2631 = add <2 x i64> %2630, %2629
  %2632 = mul nuw <2 x i64> %2554, <i64 4294956902, i64 4294956902>
  %2633 = mul nuw nsw <2 x i64> %2556, <i64 12665, i64 12665>
  %2634 = add <2 x i64> %2633, %2632
  %2635 = mul nuw <2 x i64> %2519, <i64 4294950991, i64 4294950991>
  %2636 = mul nuw nsw <2 x i64> %2521, <i64 1606, i64 1606>
  %2637 = add <2 x i64> %2636, %2635
  %2638 = mul nuw <2 x i64> %2524, <i64 4294950991, i64 4294950991>
  %2639 = mul nuw nsw <2 x i64> %2526, <i64 1606, i64 1606>
  %2640 = add <2 x i64> %2639, %2638
  %2641 = mul nuw <2 x i64> %2529, <i64 4294950991, i64 4294950991>
  %2642 = mul nuw nsw <2 x i64> %2531, <i64 1606, i64 1606>
  %2643 = add <2 x i64> %2642, %2641
  %2644 = mul nuw <2 x i64> %2534, <i64 4294950991, i64 4294950991>
  %2645 = mul nuw nsw <2 x i64> %2536, <i64 1606, i64 1606>
  %2646 = add <2 x i64> %2645, %2644
  %2647 = bitcast <2 x i64> %2523 to <4 x i32>
  %2648 = shufflevector <4 x i32> %2647, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2649 = bitcast <4 x i32> %2648 to <2 x i64>
  %2650 = bitcast <2 x i64> %2528 to <4 x i32>
  %2651 = shufflevector <4 x i32> %2650, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2652 = bitcast <4 x i32> %2651 to <2 x i64>
  %2653 = shufflevector <2 x i64> %2649, <2 x i64> %2652, <2 x i32> <i32 0, i32 2>
  %2654 = bitcast <2 x i64> %2533 to <4 x i32>
  %2655 = shufflevector <4 x i32> %2654, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2656 = bitcast <4 x i32> %2655 to <2 x i64>
  %2657 = bitcast <2 x i64> %2538 to <4 x i32>
  %2658 = shufflevector <4 x i32> %2657, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2659 = bitcast <4 x i32> %2658 to <2 x i64>
  %2660 = shufflevector <2 x i64> %2656, <2 x i64> %2659, <2 x i32> <i32 0, i32 2>
  %2661 = bitcast <2 x i64> %2543 to <4 x i32>
  %2662 = shufflevector <4 x i32> %2661, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2663 = bitcast <4 x i32> %2662 to <2 x i64>
  %2664 = bitcast <2 x i64> %2548 to <4 x i32>
  %2665 = shufflevector <4 x i32> %2664, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2666 = bitcast <4 x i32> %2665 to <2 x i64>
  %2667 = shufflevector <2 x i64> %2663, <2 x i64> %2666, <2 x i32> <i32 0, i32 2>
  %2668 = bitcast <2 x i64> %2553 to <4 x i32>
  %2669 = shufflevector <4 x i32> %2668, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2670 = bitcast <4 x i32> %2669 to <2 x i64>
  %2671 = bitcast <2 x i64> %2558 to <4 x i32>
  %2672 = shufflevector <4 x i32> %2671, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2673 = bitcast <4 x i32> %2672 to <2 x i64>
  %2674 = shufflevector <2 x i64> %2670, <2 x i64> %2673, <2 x i32> <i32 0, i32 2>
  %2675 = bitcast <2 x i64> %2563 to <4 x i32>
  %2676 = shufflevector <4 x i32> %2675, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2677 = bitcast <4 x i32> %2676 to <2 x i64>
  %2678 = bitcast <2 x i64> %2568 to <4 x i32>
  %2679 = shufflevector <4 x i32> %2678, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2680 = bitcast <4 x i32> %2679 to <2 x i64>
  %2681 = shufflevector <2 x i64> %2677, <2 x i64> %2680, <2 x i32> <i32 0, i32 2>
  %2682 = bitcast <2 x i64> %2573 to <4 x i32>
  %2683 = shufflevector <4 x i32> %2682, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2684 = bitcast <4 x i32> %2683 to <2 x i64>
  %2685 = bitcast <2 x i64> %2578 to <4 x i32>
  %2686 = shufflevector <4 x i32> %2685, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2687 = bitcast <4 x i32> %2686 to <2 x i64>
  %2688 = shufflevector <2 x i64> %2684, <2 x i64> %2687, <2 x i32> <i32 0, i32 2>
  %2689 = bitcast <2 x i64> %2583 to <4 x i32>
  %2690 = shufflevector <4 x i32> %2689, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2691 = bitcast <4 x i32> %2690 to <2 x i64>
  %2692 = bitcast <2 x i64> %2588 to <4 x i32>
  %2693 = shufflevector <4 x i32> %2692, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2694 = bitcast <4 x i32> %2693 to <2 x i64>
  %2695 = shufflevector <2 x i64> %2691, <2 x i64> %2694, <2 x i32> <i32 0, i32 2>
  %2696 = bitcast <2 x i64> %2593 to <4 x i32>
  %2697 = shufflevector <4 x i32> %2696, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2698 = bitcast <4 x i32> %2697 to <2 x i64>
  %2699 = bitcast <2 x i64> %2598 to <4 x i32>
  %2700 = shufflevector <4 x i32> %2699, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2701 = bitcast <4 x i32> %2700 to <2 x i64>
  %2702 = shufflevector <2 x i64> %2698, <2 x i64> %2701, <2 x i32> <i32 0, i32 2>
  %2703 = bitcast <2 x i64> %2601 to <4 x i32>
  %2704 = shufflevector <4 x i32> %2703, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2705 = bitcast <4 x i32> %2704 to <2 x i64>
  %2706 = bitcast <2 x i64> %2604 to <4 x i32>
  %2707 = shufflevector <4 x i32> %2706, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2708 = bitcast <4 x i32> %2707 to <2 x i64>
  %2709 = shufflevector <2 x i64> %2705, <2 x i64> %2708, <2 x i32> <i32 0, i32 2>
  %2710 = bitcast <2 x i64> %2607 to <4 x i32>
  %2711 = shufflevector <4 x i32> %2710, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2712 = bitcast <4 x i32> %2711 to <2 x i64>
  %2713 = bitcast <2 x i64> %2610 to <4 x i32>
  %2714 = shufflevector <4 x i32> %2713, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2715 = bitcast <4 x i32> %2714 to <2 x i64>
  %2716 = shufflevector <2 x i64> %2712, <2 x i64> %2715, <2 x i32> <i32 0, i32 2>
  %2717 = bitcast <2 x i64> %2613 to <4 x i32>
  %2718 = shufflevector <4 x i32> %2717, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2719 = bitcast <4 x i32> %2718 to <2 x i64>
  %2720 = bitcast <2 x i64> %2616 to <4 x i32>
  %2721 = shufflevector <4 x i32> %2720, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2722 = bitcast <4 x i32> %2721 to <2 x i64>
  %2723 = shufflevector <2 x i64> %2719, <2 x i64> %2722, <2 x i32> <i32 0, i32 2>
  %2724 = bitcast <2 x i64> %2619 to <4 x i32>
  %2725 = shufflevector <4 x i32> %2724, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2726 = bitcast <4 x i32> %2725 to <2 x i64>
  %2727 = bitcast <2 x i64> %2622 to <4 x i32>
  %2728 = shufflevector <4 x i32> %2727, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2729 = bitcast <4 x i32> %2728 to <2 x i64>
  %2730 = shufflevector <2 x i64> %2726, <2 x i64> %2729, <2 x i32> <i32 0, i32 2>
  %2731 = bitcast <2 x i64> %2625 to <4 x i32>
  %2732 = shufflevector <4 x i32> %2731, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2733 = bitcast <4 x i32> %2732 to <2 x i64>
  %2734 = bitcast <2 x i64> %2628 to <4 x i32>
  %2735 = shufflevector <4 x i32> %2734, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2736 = bitcast <4 x i32> %2735 to <2 x i64>
  %2737 = shufflevector <2 x i64> %2733, <2 x i64> %2736, <2 x i32> <i32 0, i32 2>
  %2738 = bitcast <2 x i64> %2631 to <4 x i32>
  %2739 = shufflevector <4 x i32> %2738, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2740 = bitcast <4 x i32> %2739 to <2 x i64>
  %2741 = bitcast <2 x i64> %2634 to <4 x i32>
  %2742 = shufflevector <4 x i32> %2741, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2743 = bitcast <4 x i32> %2742 to <2 x i64>
  %2744 = shufflevector <2 x i64> %2740, <2 x i64> %2743, <2 x i32> <i32 0, i32 2>
  %2745 = bitcast <2 x i64> %2637 to <4 x i32>
  %2746 = shufflevector <4 x i32> %2745, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2747 = bitcast <4 x i32> %2746 to <2 x i64>
  %2748 = bitcast <2 x i64> %2640 to <4 x i32>
  %2749 = shufflevector <4 x i32> %2748, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2750 = bitcast <4 x i32> %2749 to <2 x i64>
  %2751 = shufflevector <2 x i64> %2747, <2 x i64> %2750, <2 x i32> <i32 0, i32 2>
  %2752 = bitcast <2 x i64> %2643 to <4 x i32>
  %2753 = shufflevector <4 x i32> %2752, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2754 = bitcast <4 x i32> %2753 to <2 x i64>
  %2755 = bitcast <2 x i64> %2646 to <4 x i32>
  %2756 = shufflevector <4 x i32> %2755, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %2757 = bitcast <4 x i32> %2756 to <2 x i64>
  %2758 = shufflevector <2 x i64> %2754, <2 x i64> %2757, <2 x i32> <i32 0, i32 2>
  %2759 = bitcast <2 x i64> %2653 to <4 x i32>
  %2760 = add <4 x i32> %2759, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2761 = bitcast <2 x i64> %2660 to <4 x i32>
  %2762 = add <4 x i32> %2761, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2763 = bitcast <2 x i64> %2667 to <4 x i32>
  %2764 = add <4 x i32> %2763, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2765 = bitcast <2 x i64> %2674 to <4 x i32>
  %2766 = add <4 x i32> %2765, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2767 = bitcast <2 x i64> %2681 to <4 x i32>
  %2768 = add <4 x i32> %2767, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2769 = bitcast <2 x i64> %2688 to <4 x i32>
  %2770 = add <4 x i32> %2769, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2771 = bitcast <2 x i64> %2695 to <4 x i32>
  %2772 = add <4 x i32> %2771, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2773 = bitcast <2 x i64> %2702 to <4 x i32>
  %2774 = add <4 x i32> %2773, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2775 = bitcast <2 x i64> %2709 to <4 x i32>
  %2776 = add <4 x i32> %2775, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2777 = bitcast <2 x i64> %2716 to <4 x i32>
  %2778 = add <4 x i32> %2777, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2779 = bitcast <2 x i64> %2723 to <4 x i32>
  %2780 = add <4 x i32> %2779, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2781 = bitcast <2 x i64> %2730 to <4 x i32>
  %2782 = add <4 x i32> %2781, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2783 = bitcast <2 x i64> %2737 to <4 x i32>
  %2784 = add <4 x i32> %2783, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2785 = bitcast <2 x i64> %2744 to <4 x i32>
  %2786 = add <4 x i32> %2785, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2787 = bitcast <2 x i64> %2751 to <4 x i32>
  %2788 = add <4 x i32> %2787, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2789 = bitcast <2 x i64> %2758 to <4 x i32>
  %2790 = add <4 x i32> %2789, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2791 = ashr <4 x i32> %2760, <i32 14, i32 14, i32 14, i32 14>
  %2792 = ashr <4 x i32> %2762, <i32 14, i32 14, i32 14, i32 14>
  %2793 = ashr <4 x i32> %2764, <i32 14, i32 14, i32 14, i32 14>
  %2794 = ashr <4 x i32> %2766, <i32 14, i32 14, i32 14, i32 14>
  %2795 = ashr <4 x i32> %2768, <i32 14, i32 14, i32 14, i32 14>
  %2796 = ashr <4 x i32> %2770, <i32 14, i32 14, i32 14, i32 14>
  %2797 = ashr <4 x i32> %2772, <i32 14, i32 14, i32 14, i32 14>
  %2798 = ashr <4 x i32> %2774, <i32 14, i32 14, i32 14, i32 14>
  %2799 = ashr <4 x i32> %2776, <i32 14, i32 14, i32 14, i32 14>
  %2800 = ashr <4 x i32> %2778, <i32 14, i32 14, i32 14, i32 14>
  %2801 = ashr <4 x i32> %2780, <i32 14, i32 14, i32 14, i32 14>
  %2802 = ashr <4 x i32> %2782, <i32 14, i32 14, i32 14, i32 14>
  %2803 = ashr <4 x i32> %2784, <i32 14, i32 14, i32 14, i32 14>
  %2804 = ashr <4 x i32> %2786, <i32 14, i32 14, i32 14, i32 14>
  %2805 = ashr <4 x i32> %2788, <i32 14, i32 14, i32 14, i32 14>
  %2806 = ashr <4 x i32> %2790, <i32 14, i32 14, i32 14, i32 14>
  %2807 = lshr <4 x i32> %2760, <i32 31, i32 31, i32 31, i32 31>
  %2808 = lshr <4 x i32> %2762, <i32 31, i32 31, i32 31, i32 31>
  %2809 = lshr <4 x i32> %2764, <i32 31, i32 31, i32 31, i32 31>
  %2810 = lshr <4 x i32> %2766, <i32 31, i32 31, i32 31, i32 31>
  %2811 = lshr <4 x i32> %2768, <i32 31, i32 31, i32 31, i32 31>
  %2812 = lshr <4 x i32> %2770, <i32 31, i32 31, i32 31, i32 31>
  %2813 = lshr <4 x i32> %2772, <i32 31, i32 31, i32 31, i32 31>
  %2814 = lshr <4 x i32> %2774, <i32 31, i32 31, i32 31, i32 31>
  %2815 = lshr <4 x i32> %2776, <i32 31, i32 31, i32 31, i32 31>
  %2816 = lshr <4 x i32> %2778, <i32 31, i32 31, i32 31, i32 31>
  %2817 = lshr <4 x i32> %2780, <i32 31, i32 31, i32 31, i32 31>
  %2818 = lshr <4 x i32> %2782, <i32 31, i32 31, i32 31, i32 31>
  %2819 = lshr <4 x i32> %2784, <i32 31, i32 31, i32 31, i32 31>
  %2820 = lshr <4 x i32> %2786, <i32 31, i32 31, i32 31, i32 31>
  %2821 = lshr <4 x i32> %2788, <i32 31, i32 31, i32 31, i32 31>
  %2822 = lshr <4 x i32> %2790, <i32 31, i32 31, i32 31, i32 31>
  %2823 = add nuw nsw <4 x i32> %2807, <i32 1, i32 1, i32 1, i32 1>
  %2824 = add nsw <4 x i32> %2823, %2791
  %2825 = add nuw nsw <4 x i32> %2808, <i32 1, i32 1, i32 1, i32 1>
  %2826 = add nsw <4 x i32> %2825, %2792
  %2827 = add nuw nsw <4 x i32> %2809, <i32 1, i32 1, i32 1, i32 1>
  %2828 = add nsw <4 x i32> %2827, %2793
  %2829 = add nuw nsw <4 x i32> %2810, <i32 1, i32 1, i32 1, i32 1>
  %2830 = add nsw <4 x i32> %2829, %2794
  %2831 = add nuw nsw <4 x i32> %2811, <i32 1, i32 1, i32 1, i32 1>
  %2832 = add nsw <4 x i32> %2831, %2795
  %2833 = add nuw nsw <4 x i32> %2812, <i32 1, i32 1, i32 1, i32 1>
  %2834 = add nsw <4 x i32> %2833, %2796
  %2835 = add nuw nsw <4 x i32> %2813, <i32 1, i32 1, i32 1, i32 1>
  %2836 = add nsw <4 x i32> %2835, %2797
  %2837 = add nuw nsw <4 x i32> %2814, <i32 1, i32 1, i32 1, i32 1>
  %2838 = add nsw <4 x i32> %2837, %2798
  %2839 = add nuw nsw <4 x i32> %2815, <i32 1, i32 1, i32 1, i32 1>
  %2840 = add nsw <4 x i32> %2839, %2799
  %2841 = add nuw nsw <4 x i32> %2816, <i32 1, i32 1, i32 1, i32 1>
  %2842 = add nsw <4 x i32> %2841, %2800
  %2843 = add nuw nsw <4 x i32> %2817, <i32 1, i32 1, i32 1, i32 1>
  %2844 = add nsw <4 x i32> %2843, %2801
  %2845 = add nuw nsw <4 x i32> %2818, <i32 1, i32 1, i32 1, i32 1>
  %2846 = add nsw <4 x i32> %2845, %2802
  %2847 = add nuw nsw <4 x i32> %2819, <i32 1, i32 1, i32 1, i32 1>
  %2848 = add nsw <4 x i32> %2847, %2803
  %2849 = add nuw nsw <4 x i32> %2820, <i32 1, i32 1, i32 1, i32 1>
  %2850 = add nsw <4 x i32> %2849, %2804
  %2851 = add nuw nsw <4 x i32> %2821, <i32 1, i32 1, i32 1, i32 1>
  %2852 = add nsw <4 x i32> %2851, %2805
  %2853 = add nuw nsw <4 x i32> %2822, <i32 1, i32 1, i32 1, i32 1>
  %2854 = add nsw <4 x i32> %2853, %2806
  %2855 = ashr <4 x i32> %2824, <i32 2, i32 2, i32 2, i32 2>
  %2856 = ashr <4 x i32> %2826, <i32 2, i32 2, i32 2, i32 2>
  %2857 = ashr <4 x i32> %2828, <i32 2, i32 2, i32 2, i32 2>
  %2858 = ashr <4 x i32> %2830, <i32 2, i32 2, i32 2, i32 2>
  %2859 = ashr <4 x i32> %2832, <i32 2, i32 2, i32 2, i32 2>
  %2860 = ashr <4 x i32> %2834, <i32 2, i32 2, i32 2, i32 2>
  %2861 = ashr <4 x i32> %2836, <i32 2, i32 2, i32 2, i32 2>
  %2862 = ashr <4 x i32> %2838, <i32 2, i32 2, i32 2, i32 2>
  %2863 = ashr <4 x i32> %2840, <i32 2, i32 2, i32 2, i32 2>
  %2864 = ashr <4 x i32> %2842, <i32 2, i32 2, i32 2, i32 2>
  %2865 = ashr <4 x i32> %2844, <i32 2, i32 2, i32 2, i32 2>
  %2866 = ashr <4 x i32> %2846, <i32 2, i32 2, i32 2, i32 2>
  %2867 = ashr <4 x i32> %2848, <i32 2, i32 2, i32 2, i32 2>
  %2868 = ashr <4 x i32> %2850, <i32 2, i32 2, i32 2, i32 2>
  %2869 = ashr <4 x i32> %2852, <i32 2, i32 2, i32 2, i32 2>
  %2870 = ashr <4 x i32> %2854, <i32 2, i32 2, i32 2, i32 2>
  %2871 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2855, <4 x i32> %2856) #6
  store <8 x i16> %2871, <8 x i16>* %46, align 16
  %2872 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2857, <4 x i32> %2858) #6
  store <8 x i16> %2872, <8 x i16>* %48, align 16
  %2873 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2859, <4 x i32> %2860) #6
  store <8 x i16> %2873, <8 x i16>* %50, align 16
  %2874 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2861, <4 x i32> %2862) #6
  store <8 x i16> %2874, <8 x i16>* %52, align 16
  %2875 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2863, <4 x i32> %2864) #6
  store <8 x i16> %2875, <8 x i16>* %54, align 16
  %2876 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2865, <4 x i32> %2866) #6
  store <8 x i16> %2876, <8 x i16>* %56, align 16
  %2877 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2867, <4 x i32> %2868) #6
  store <8 x i16> %2877, <8 x i16>* %58, align 16
  %2878 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2869, <4 x i32> %2870) #6
  store <8 x i16> %2878, <8 x i16>* %60, align 16
  %2879 = add <4 x i32> %2471, %1923
  %2880 = add <4 x i32> %2472, %1924
  %2881 = sub <4 x i32> %1923, %2471
  %2882 = sub <4 x i32> %1924, %2472
  %2883 = sub <4 x i32> %1929, %2473
  %2884 = sub <4 x i32> %1930, %2474
  %2885 = add <4 x i32> %2473, %1929
  %2886 = add <4 x i32> %2474, %1930
  %2887 = add <4 x i32> %2475, %1931
  %2888 = add <4 x i32> %2476, %1932
  %2889 = sub <4 x i32> %1931, %2475
  %2890 = sub <4 x i32> %1932, %2476
  %2891 = sub <4 x i32> %1937, %2477
  %2892 = sub <4 x i32> %1938, %2478
  %2893 = add <4 x i32> %2477, %1937
  %2894 = add <4 x i32> %2478, %1938
  %2895 = add <4 x i32> %2479, %1939
  %2896 = add <4 x i32> %2480, %1940
  %2897 = sub <4 x i32> %1939, %2479
  %2898 = sub <4 x i32> %1940, %2480
  %2899 = sub <4 x i32> %1945, %2481
  %2900 = sub <4 x i32> %1946, %2482
  %2901 = add <4 x i32> %2481, %1945
  %2902 = add <4 x i32> %2482, %1946
  %2903 = add <4 x i32> %2483, %1947
  %2904 = add <4 x i32> %2484, %1948
  %2905 = sub <4 x i32> %1947, %2483
  %2906 = sub <4 x i32> %1948, %2484
  %2907 = sub <4 x i32> %1953, %2485
  %2908 = sub <4 x i32> %1954, %2486
  %2909 = add <4 x i32> %2485, %1953
  %2910 = add <4 x i32> %2486, %1954
  %2911 = shufflevector <4 x i32> %2879, <4 x i32> %2909, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2912 = bitcast <4 x i32> %2911 to <2 x i64>
  %2913 = shufflevector <4 x i32> %2879, <4 x i32> %2909, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2914 = bitcast <4 x i32> %2913 to <2 x i64>
  %2915 = shufflevector <4 x i32> %2880, <4 x i32> %2910, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2916 = bitcast <4 x i32> %2915 to <2 x i64>
  %2917 = shufflevector <4 x i32> %2880, <4 x i32> %2910, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2918 = bitcast <4 x i32> %2917 to <2 x i64>
  %2919 = shufflevector <4 x i32> %2881, <4 x i32> %2907, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2920 = bitcast <4 x i32> %2919 to <2 x i64>
  %2921 = shufflevector <4 x i32> %2881, <4 x i32> %2907, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2922 = bitcast <4 x i32> %2921 to <2 x i64>
  %2923 = shufflevector <4 x i32> %2882, <4 x i32> %2908, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2924 = bitcast <4 x i32> %2923 to <2 x i64>
  %2925 = shufflevector <4 x i32> %2882, <4 x i32> %2908, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2926 = bitcast <4 x i32> %2925 to <2 x i64>
  %2927 = shufflevector <4 x i32> %2883, <4 x i32> %2905, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2928 = bitcast <4 x i32> %2927 to <2 x i64>
  %2929 = shufflevector <4 x i32> %2883, <4 x i32> %2905, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2930 = bitcast <4 x i32> %2929 to <2 x i64>
  %2931 = shufflevector <4 x i32> %2884, <4 x i32> %2906, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2932 = bitcast <4 x i32> %2931 to <2 x i64>
  %2933 = shufflevector <4 x i32> %2884, <4 x i32> %2906, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2934 = bitcast <4 x i32> %2933 to <2 x i64>
  %2935 = shufflevector <4 x i32> %2885, <4 x i32> %2903, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2936 = bitcast <4 x i32> %2935 to <2 x i64>
  %2937 = shufflevector <4 x i32> %2885, <4 x i32> %2903, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2938 = bitcast <4 x i32> %2937 to <2 x i64>
  %2939 = shufflevector <4 x i32> %2886, <4 x i32> %2904, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2940 = bitcast <4 x i32> %2939 to <2 x i64>
  %2941 = shufflevector <4 x i32> %2886, <4 x i32> %2904, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2942 = bitcast <4 x i32> %2941 to <2 x i64>
  %2943 = and <2 x i64> %2912, <i64 4294967295, i64 4294967295>
  %2944 = mul nuw nsw <2 x i64> %2943, <i64 804, i64 804>
  %2945 = lshr <2 x i64> %2912, <i64 32, i64 32>
  %2946 = mul nuw nsw <2 x i64> %2945, <i64 16364, i64 16364>
  %2947 = add nuw nsw <2 x i64> %2946, %2944
  %2948 = and <2 x i64> %2914, <i64 4294967295, i64 4294967295>
  %2949 = mul nuw nsw <2 x i64> %2948, <i64 804, i64 804>
  %2950 = lshr <2 x i64> %2914, <i64 32, i64 32>
  %2951 = mul nuw nsw <2 x i64> %2950, <i64 16364, i64 16364>
  %2952 = add nuw nsw <2 x i64> %2951, %2949
  %2953 = and <2 x i64> %2916, <i64 4294967295, i64 4294967295>
  %2954 = mul nuw nsw <2 x i64> %2953, <i64 804, i64 804>
  %2955 = lshr <2 x i64> %2916, <i64 32, i64 32>
  %2956 = mul nuw nsw <2 x i64> %2955, <i64 16364, i64 16364>
  %2957 = add nuw nsw <2 x i64> %2956, %2954
  %2958 = and <2 x i64> %2918, <i64 4294967295, i64 4294967295>
  %2959 = mul nuw nsw <2 x i64> %2958, <i64 804, i64 804>
  %2960 = lshr <2 x i64> %2918, <i64 32, i64 32>
  %2961 = mul nuw nsw <2 x i64> %2960, <i64 16364, i64 16364>
  %2962 = add nuw nsw <2 x i64> %2961, %2959
  %2963 = and <2 x i64> %2920, <i64 4294967295, i64 4294967295>
  %2964 = mul nuw nsw <2 x i64> %2963, <i64 12140, i64 12140>
  %2965 = lshr <2 x i64> %2920, <i64 32, i64 32>
  %2966 = mul nuw nsw <2 x i64> %2965, <i64 11003, i64 11003>
  %2967 = add nuw nsw <2 x i64> %2966, %2964
  %2968 = and <2 x i64> %2922, <i64 4294967295, i64 4294967295>
  %2969 = mul nuw nsw <2 x i64> %2968, <i64 12140, i64 12140>
  %2970 = lshr <2 x i64> %2922, <i64 32, i64 32>
  %2971 = mul nuw nsw <2 x i64> %2970, <i64 11003, i64 11003>
  %2972 = add nuw nsw <2 x i64> %2971, %2969
  %2973 = and <2 x i64> %2924, <i64 4294967295, i64 4294967295>
  %2974 = mul nuw nsw <2 x i64> %2973, <i64 12140, i64 12140>
  %2975 = lshr <2 x i64> %2924, <i64 32, i64 32>
  %2976 = mul nuw nsw <2 x i64> %2975, <i64 11003, i64 11003>
  %2977 = add nuw nsw <2 x i64> %2976, %2974
  %2978 = and <2 x i64> %2926, <i64 4294967295, i64 4294967295>
  %2979 = mul nuw nsw <2 x i64> %2978, <i64 12140, i64 12140>
  %2980 = lshr <2 x i64> %2926, <i64 32, i64 32>
  %2981 = mul nuw nsw <2 x i64> %2980, <i64 11003, i64 11003>
  %2982 = add nuw nsw <2 x i64> %2981, %2979
  %2983 = and <2 x i64> %2928, <i64 4294967295, i64 4294967295>
  %2984 = mul nuw nsw <2 x i64> %2983, <i64 7005, i64 7005>
  %2985 = lshr <2 x i64> %2928, <i64 32, i64 32>
  %2986 = mul nuw nsw <2 x i64> %2985, <i64 14811, i64 14811>
  %2987 = add nuw nsw <2 x i64> %2986, %2984
  %2988 = and <2 x i64> %2930, <i64 4294967295, i64 4294967295>
  %2989 = mul nuw nsw <2 x i64> %2988, <i64 7005, i64 7005>
  %2990 = lshr <2 x i64> %2930, <i64 32, i64 32>
  %2991 = mul nuw nsw <2 x i64> %2990, <i64 14811, i64 14811>
  %2992 = add nuw nsw <2 x i64> %2991, %2989
  %2993 = and <2 x i64> %2932, <i64 4294967295, i64 4294967295>
  %2994 = mul nuw nsw <2 x i64> %2993, <i64 7005, i64 7005>
  %2995 = lshr <2 x i64> %2932, <i64 32, i64 32>
  %2996 = mul nuw nsw <2 x i64> %2995, <i64 14811, i64 14811>
  %2997 = add nuw nsw <2 x i64> %2996, %2994
  %2998 = and <2 x i64> %2934, <i64 4294967295, i64 4294967295>
  %2999 = mul nuw nsw <2 x i64> %2998, <i64 7005, i64 7005>
  %3000 = lshr <2 x i64> %2934, <i64 32, i64 32>
  %3001 = mul nuw nsw <2 x i64> %3000, <i64 14811, i64 14811>
  %3002 = add nuw nsw <2 x i64> %3001, %2999
  %3003 = and <2 x i64> %2936, <i64 4294967295, i64 4294967295>
  %3004 = mul nuw nsw <2 x i64> %3003, <i64 15426, i64 15426>
  %3005 = lshr <2 x i64> %2936, <i64 32, i64 32>
  %3006 = mul nuw nsw <2 x i64> %3005, <i64 5520, i64 5520>
  %3007 = add nuw nsw <2 x i64> %3006, %3004
  %3008 = and <2 x i64> %2938, <i64 4294967295, i64 4294967295>
  %3009 = mul nuw nsw <2 x i64> %3008, <i64 15426, i64 15426>
  %3010 = lshr <2 x i64> %2938, <i64 32, i64 32>
  %3011 = mul nuw nsw <2 x i64> %3010, <i64 5520, i64 5520>
  %3012 = add nuw nsw <2 x i64> %3011, %3009
  %3013 = and <2 x i64> %2940, <i64 4294967295, i64 4294967295>
  %3014 = mul nuw nsw <2 x i64> %3013, <i64 15426, i64 15426>
  %3015 = lshr <2 x i64> %2940, <i64 32, i64 32>
  %3016 = mul nuw nsw <2 x i64> %3015, <i64 5520, i64 5520>
  %3017 = add nuw nsw <2 x i64> %3016, %3014
  %3018 = and <2 x i64> %2942, <i64 4294967295, i64 4294967295>
  %3019 = mul nuw nsw <2 x i64> %3018, <i64 15426, i64 15426>
  %3020 = lshr <2 x i64> %2942, <i64 32, i64 32>
  %3021 = mul nuw nsw <2 x i64> %3020, <i64 5520, i64 5520>
  %3022 = add nuw nsw <2 x i64> %3021, %3019
  %3023 = mul nuw <2 x i64> %3003, <i64 4294961776, i64 4294961776>
  %3024 = mul nuw nsw <2 x i64> %3005, <i64 15426, i64 15426>
  %3025 = add <2 x i64> %3024, %3023
  %3026 = mul nuw <2 x i64> %3008, <i64 4294961776, i64 4294961776>
  %3027 = mul nuw nsw <2 x i64> %3010, <i64 15426, i64 15426>
  %3028 = add <2 x i64> %3027, %3026
  %3029 = mul nuw <2 x i64> %3013, <i64 4294961776, i64 4294961776>
  %3030 = mul nuw nsw <2 x i64> %3015, <i64 15426, i64 15426>
  %3031 = add <2 x i64> %3030, %3029
  %3032 = mul nuw <2 x i64> %3018, <i64 4294961776, i64 4294961776>
  %3033 = mul nuw nsw <2 x i64> %3020, <i64 15426, i64 15426>
  %3034 = add <2 x i64> %3033, %3032
  %3035 = mul nuw <2 x i64> %2983, <i64 4294952485, i64 4294952485>
  %3036 = mul nuw nsw <2 x i64> %2985, <i64 7005, i64 7005>
  %3037 = add <2 x i64> %3036, %3035
  %3038 = mul nuw <2 x i64> %2988, <i64 4294952485, i64 4294952485>
  %3039 = mul nuw nsw <2 x i64> %2990, <i64 7005, i64 7005>
  %3040 = add <2 x i64> %3039, %3038
  %3041 = mul nuw <2 x i64> %2993, <i64 4294952485, i64 4294952485>
  %3042 = mul nuw nsw <2 x i64> %2995, <i64 7005, i64 7005>
  %3043 = add <2 x i64> %3042, %3041
  %3044 = mul nuw <2 x i64> %2998, <i64 4294952485, i64 4294952485>
  %3045 = mul nuw nsw <2 x i64> %3000, <i64 7005, i64 7005>
  %3046 = add <2 x i64> %3045, %3044
  %3047 = mul nuw <2 x i64> %2963, <i64 4294956293, i64 4294956293>
  %3048 = mul nuw nsw <2 x i64> %2965, <i64 12140, i64 12140>
  %3049 = add <2 x i64> %3048, %3047
  %3050 = mul nuw <2 x i64> %2968, <i64 4294956293, i64 4294956293>
  %3051 = mul nuw nsw <2 x i64> %2970, <i64 12140, i64 12140>
  %3052 = add <2 x i64> %3051, %3050
  %3053 = mul nuw <2 x i64> %2973, <i64 4294956293, i64 4294956293>
  %3054 = mul nuw nsw <2 x i64> %2975, <i64 12140, i64 12140>
  %3055 = add <2 x i64> %3054, %3053
  %3056 = mul nuw <2 x i64> %2978, <i64 4294956293, i64 4294956293>
  %3057 = mul nuw nsw <2 x i64> %2980, <i64 12140, i64 12140>
  %3058 = add <2 x i64> %3057, %3056
  %3059 = mul nuw <2 x i64> %2943, <i64 4294950932, i64 4294950932>
  %3060 = mul nuw nsw <2 x i64> %2945, <i64 804, i64 804>
  %3061 = add <2 x i64> %3060, %3059
  %3062 = mul nuw <2 x i64> %2948, <i64 4294950932, i64 4294950932>
  %3063 = mul nuw nsw <2 x i64> %2950, <i64 804, i64 804>
  %3064 = add <2 x i64> %3063, %3062
  %3065 = mul nuw <2 x i64> %2953, <i64 4294950932, i64 4294950932>
  %3066 = mul nuw nsw <2 x i64> %2955, <i64 804, i64 804>
  %3067 = add <2 x i64> %3066, %3065
  %3068 = mul nuw <2 x i64> %2958, <i64 4294950932, i64 4294950932>
  %3069 = mul nuw nsw <2 x i64> %2960, <i64 804, i64 804>
  %3070 = add <2 x i64> %3069, %3068
  %3071 = bitcast <2 x i64> %2947 to <4 x i32>
  %3072 = shufflevector <4 x i32> %3071, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3073 = bitcast <4 x i32> %3072 to <2 x i64>
  %3074 = bitcast <2 x i64> %2952 to <4 x i32>
  %3075 = shufflevector <4 x i32> %3074, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3076 = bitcast <4 x i32> %3075 to <2 x i64>
  %3077 = shufflevector <2 x i64> %3073, <2 x i64> %3076, <2 x i32> <i32 0, i32 2>
  %3078 = bitcast <2 x i64> %2957 to <4 x i32>
  %3079 = shufflevector <4 x i32> %3078, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3080 = bitcast <4 x i32> %3079 to <2 x i64>
  %3081 = bitcast <2 x i64> %2962 to <4 x i32>
  %3082 = shufflevector <4 x i32> %3081, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3083 = bitcast <4 x i32> %3082 to <2 x i64>
  %3084 = shufflevector <2 x i64> %3080, <2 x i64> %3083, <2 x i32> <i32 0, i32 2>
  %3085 = bitcast <2 x i64> %2967 to <4 x i32>
  %3086 = shufflevector <4 x i32> %3085, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3087 = bitcast <4 x i32> %3086 to <2 x i64>
  %3088 = bitcast <2 x i64> %2972 to <4 x i32>
  %3089 = shufflevector <4 x i32> %3088, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3090 = bitcast <4 x i32> %3089 to <2 x i64>
  %3091 = shufflevector <2 x i64> %3087, <2 x i64> %3090, <2 x i32> <i32 0, i32 2>
  %3092 = bitcast <2 x i64> %2977 to <4 x i32>
  %3093 = shufflevector <4 x i32> %3092, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3094 = bitcast <4 x i32> %3093 to <2 x i64>
  %3095 = bitcast <2 x i64> %2982 to <4 x i32>
  %3096 = shufflevector <4 x i32> %3095, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3097 = bitcast <4 x i32> %3096 to <2 x i64>
  %3098 = shufflevector <2 x i64> %3094, <2 x i64> %3097, <2 x i32> <i32 0, i32 2>
  %3099 = bitcast <2 x i64> %2987 to <4 x i32>
  %3100 = shufflevector <4 x i32> %3099, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3101 = bitcast <4 x i32> %3100 to <2 x i64>
  %3102 = bitcast <2 x i64> %2992 to <4 x i32>
  %3103 = shufflevector <4 x i32> %3102, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3104 = bitcast <4 x i32> %3103 to <2 x i64>
  %3105 = shufflevector <2 x i64> %3101, <2 x i64> %3104, <2 x i32> <i32 0, i32 2>
  %3106 = bitcast <2 x i64> %2997 to <4 x i32>
  %3107 = shufflevector <4 x i32> %3106, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3108 = bitcast <4 x i32> %3107 to <2 x i64>
  %3109 = bitcast <2 x i64> %3002 to <4 x i32>
  %3110 = shufflevector <4 x i32> %3109, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3111 = bitcast <4 x i32> %3110 to <2 x i64>
  %3112 = shufflevector <2 x i64> %3108, <2 x i64> %3111, <2 x i32> <i32 0, i32 2>
  %3113 = bitcast <2 x i64> %3007 to <4 x i32>
  %3114 = shufflevector <4 x i32> %3113, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3115 = bitcast <4 x i32> %3114 to <2 x i64>
  %3116 = bitcast <2 x i64> %3012 to <4 x i32>
  %3117 = shufflevector <4 x i32> %3116, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3118 = bitcast <4 x i32> %3117 to <2 x i64>
  %3119 = shufflevector <2 x i64> %3115, <2 x i64> %3118, <2 x i32> <i32 0, i32 2>
  %3120 = bitcast <2 x i64> %3017 to <4 x i32>
  %3121 = shufflevector <4 x i32> %3120, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3122 = bitcast <4 x i32> %3121 to <2 x i64>
  %3123 = bitcast <2 x i64> %3022 to <4 x i32>
  %3124 = shufflevector <4 x i32> %3123, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3125 = bitcast <4 x i32> %3124 to <2 x i64>
  %3126 = shufflevector <2 x i64> %3122, <2 x i64> %3125, <2 x i32> <i32 0, i32 2>
  %3127 = bitcast <2 x i64> %3025 to <4 x i32>
  %3128 = shufflevector <4 x i32> %3127, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3129 = bitcast <4 x i32> %3128 to <2 x i64>
  %3130 = bitcast <2 x i64> %3028 to <4 x i32>
  %3131 = shufflevector <4 x i32> %3130, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3132 = bitcast <4 x i32> %3131 to <2 x i64>
  %3133 = shufflevector <2 x i64> %3129, <2 x i64> %3132, <2 x i32> <i32 0, i32 2>
  %3134 = bitcast <2 x i64> %3031 to <4 x i32>
  %3135 = shufflevector <4 x i32> %3134, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3136 = bitcast <4 x i32> %3135 to <2 x i64>
  %3137 = bitcast <2 x i64> %3034 to <4 x i32>
  %3138 = shufflevector <4 x i32> %3137, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3139 = bitcast <4 x i32> %3138 to <2 x i64>
  %3140 = shufflevector <2 x i64> %3136, <2 x i64> %3139, <2 x i32> <i32 0, i32 2>
  %3141 = bitcast <2 x i64> %3037 to <4 x i32>
  %3142 = shufflevector <4 x i32> %3141, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3143 = bitcast <4 x i32> %3142 to <2 x i64>
  %3144 = bitcast <2 x i64> %3040 to <4 x i32>
  %3145 = shufflevector <4 x i32> %3144, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3146 = bitcast <4 x i32> %3145 to <2 x i64>
  %3147 = shufflevector <2 x i64> %3143, <2 x i64> %3146, <2 x i32> <i32 0, i32 2>
  %3148 = bitcast <2 x i64> %3043 to <4 x i32>
  %3149 = shufflevector <4 x i32> %3148, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3150 = bitcast <4 x i32> %3149 to <2 x i64>
  %3151 = bitcast <2 x i64> %3046 to <4 x i32>
  %3152 = shufflevector <4 x i32> %3151, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3153 = bitcast <4 x i32> %3152 to <2 x i64>
  %3154 = shufflevector <2 x i64> %3150, <2 x i64> %3153, <2 x i32> <i32 0, i32 2>
  %3155 = bitcast <2 x i64> %3049 to <4 x i32>
  %3156 = shufflevector <4 x i32> %3155, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3157 = bitcast <4 x i32> %3156 to <2 x i64>
  %3158 = bitcast <2 x i64> %3052 to <4 x i32>
  %3159 = shufflevector <4 x i32> %3158, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3160 = bitcast <4 x i32> %3159 to <2 x i64>
  %3161 = shufflevector <2 x i64> %3157, <2 x i64> %3160, <2 x i32> <i32 0, i32 2>
  %3162 = bitcast <2 x i64> %3055 to <4 x i32>
  %3163 = shufflevector <4 x i32> %3162, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3164 = bitcast <4 x i32> %3163 to <2 x i64>
  %3165 = bitcast <2 x i64> %3058 to <4 x i32>
  %3166 = shufflevector <4 x i32> %3165, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3167 = bitcast <4 x i32> %3166 to <2 x i64>
  %3168 = shufflevector <2 x i64> %3164, <2 x i64> %3167, <2 x i32> <i32 0, i32 2>
  %3169 = bitcast <2 x i64> %3061 to <4 x i32>
  %3170 = shufflevector <4 x i32> %3169, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3171 = bitcast <4 x i32> %3170 to <2 x i64>
  %3172 = bitcast <2 x i64> %3064 to <4 x i32>
  %3173 = shufflevector <4 x i32> %3172, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3174 = bitcast <4 x i32> %3173 to <2 x i64>
  %3175 = shufflevector <2 x i64> %3171, <2 x i64> %3174, <2 x i32> <i32 0, i32 2>
  %3176 = bitcast <2 x i64> %3067 to <4 x i32>
  %3177 = shufflevector <4 x i32> %3176, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3178 = bitcast <4 x i32> %3177 to <2 x i64>
  %3179 = bitcast <2 x i64> %3070 to <4 x i32>
  %3180 = shufflevector <4 x i32> %3179, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3181 = bitcast <4 x i32> %3180 to <2 x i64>
  %3182 = shufflevector <2 x i64> %3178, <2 x i64> %3181, <2 x i32> <i32 0, i32 2>
  %3183 = bitcast <2 x i64> %3077 to <4 x i32>
  %3184 = add <4 x i32> %3183, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3185 = bitcast <2 x i64> %3084 to <4 x i32>
  %3186 = add <4 x i32> %3185, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3187 = bitcast <2 x i64> %3091 to <4 x i32>
  %3188 = add <4 x i32> %3187, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3189 = bitcast <2 x i64> %3098 to <4 x i32>
  %3190 = add <4 x i32> %3189, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3191 = bitcast <2 x i64> %3105 to <4 x i32>
  %3192 = add <4 x i32> %3191, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3193 = bitcast <2 x i64> %3112 to <4 x i32>
  %3194 = add <4 x i32> %3193, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3195 = bitcast <2 x i64> %3119 to <4 x i32>
  %3196 = add <4 x i32> %3195, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3197 = bitcast <2 x i64> %3126 to <4 x i32>
  %3198 = add <4 x i32> %3197, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3199 = bitcast <2 x i64> %3133 to <4 x i32>
  %3200 = add <4 x i32> %3199, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3201 = bitcast <2 x i64> %3140 to <4 x i32>
  %3202 = add <4 x i32> %3201, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3203 = bitcast <2 x i64> %3147 to <4 x i32>
  %3204 = add <4 x i32> %3203, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3205 = bitcast <2 x i64> %3154 to <4 x i32>
  %3206 = add <4 x i32> %3205, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3207 = bitcast <2 x i64> %3161 to <4 x i32>
  %3208 = add <4 x i32> %3207, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3209 = bitcast <2 x i64> %3168 to <4 x i32>
  %3210 = add <4 x i32> %3209, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3211 = bitcast <2 x i64> %3175 to <4 x i32>
  %3212 = add <4 x i32> %3211, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3213 = bitcast <2 x i64> %3182 to <4 x i32>
  %3214 = add <4 x i32> %3213, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3215 = ashr <4 x i32> %3184, <i32 14, i32 14, i32 14, i32 14>
  %3216 = ashr <4 x i32> %3186, <i32 14, i32 14, i32 14, i32 14>
  %3217 = ashr <4 x i32> %3188, <i32 14, i32 14, i32 14, i32 14>
  %3218 = ashr <4 x i32> %3190, <i32 14, i32 14, i32 14, i32 14>
  %3219 = ashr <4 x i32> %3192, <i32 14, i32 14, i32 14, i32 14>
  %3220 = ashr <4 x i32> %3194, <i32 14, i32 14, i32 14, i32 14>
  %3221 = ashr <4 x i32> %3196, <i32 14, i32 14, i32 14, i32 14>
  %3222 = ashr <4 x i32> %3198, <i32 14, i32 14, i32 14, i32 14>
  %3223 = ashr <4 x i32> %3200, <i32 14, i32 14, i32 14, i32 14>
  %3224 = ashr <4 x i32> %3202, <i32 14, i32 14, i32 14, i32 14>
  %3225 = ashr <4 x i32> %3204, <i32 14, i32 14, i32 14, i32 14>
  %3226 = ashr <4 x i32> %3206, <i32 14, i32 14, i32 14, i32 14>
  %3227 = ashr <4 x i32> %3208, <i32 14, i32 14, i32 14, i32 14>
  %3228 = ashr <4 x i32> %3210, <i32 14, i32 14, i32 14, i32 14>
  %3229 = ashr <4 x i32> %3212, <i32 14, i32 14, i32 14, i32 14>
  %3230 = ashr <4 x i32> %3214, <i32 14, i32 14, i32 14, i32 14>
  %3231 = lshr <4 x i32> %3184, <i32 31, i32 31, i32 31, i32 31>
  %3232 = lshr <4 x i32> %3186, <i32 31, i32 31, i32 31, i32 31>
  %3233 = lshr <4 x i32> %3188, <i32 31, i32 31, i32 31, i32 31>
  %3234 = lshr <4 x i32> %3190, <i32 31, i32 31, i32 31, i32 31>
  %3235 = lshr <4 x i32> %3192, <i32 31, i32 31, i32 31, i32 31>
  %3236 = lshr <4 x i32> %3194, <i32 31, i32 31, i32 31, i32 31>
  %3237 = lshr <4 x i32> %3196, <i32 31, i32 31, i32 31, i32 31>
  %3238 = lshr <4 x i32> %3198, <i32 31, i32 31, i32 31, i32 31>
  %3239 = lshr <4 x i32> %3200, <i32 31, i32 31, i32 31, i32 31>
  %3240 = lshr <4 x i32> %3202, <i32 31, i32 31, i32 31, i32 31>
  %3241 = lshr <4 x i32> %3204, <i32 31, i32 31, i32 31, i32 31>
  %3242 = lshr <4 x i32> %3206, <i32 31, i32 31, i32 31, i32 31>
  %3243 = lshr <4 x i32> %3208, <i32 31, i32 31, i32 31, i32 31>
  %3244 = lshr <4 x i32> %3210, <i32 31, i32 31, i32 31, i32 31>
  %3245 = lshr <4 x i32> %3212, <i32 31, i32 31, i32 31, i32 31>
  %3246 = lshr <4 x i32> %3214, <i32 31, i32 31, i32 31, i32 31>
  %3247 = add nuw nsw <4 x i32> %3231, <i32 1, i32 1, i32 1, i32 1>
  %3248 = add nsw <4 x i32> %3247, %3215
  %3249 = add nuw nsw <4 x i32> %3232, <i32 1, i32 1, i32 1, i32 1>
  %3250 = add nsw <4 x i32> %3249, %3216
  %3251 = add nuw nsw <4 x i32> %3233, <i32 1, i32 1, i32 1, i32 1>
  %3252 = add nsw <4 x i32> %3251, %3217
  %3253 = add nuw nsw <4 x i32> %3234, <i32 1, i32 1, i32 1, i32 1>
  %3254 = add nsw <4 x i32> %3253, %3218
  %3255 = add nuw nsw <4 x i32> %3235, <i32 1, i32 1, i32 1, i32 1>
  %3256 = add nsw <4 x i32> %3255, %3219
  %3257 = add nuw nsw <4 x i32> %3236, <i32 1, i32 1, i32 1, i32 1>
  %3258 = add nsw <4 x i32> %3257, %3220
  %3259 = add nuw nsw <4 x i32> %3237, <i32 1, i32 1, i32 1, i32 1>
  %3260 = add nsw <4 x i32> %3259, %3221
  %3261 = add nuw nsw <4 x i32> %3238, <i32 1, i32 1, i32 1, i32 1>
  %3262 = add nsw <4 x i32> %3261, %3222
  %3263 = add nuw nsw <4 x i32> %3239, <i32 1, i32 1, i32 1, i32 1>
  %3264 = add nsw <4 x i32> %3263, %3223
  %3265 = add nuw nsw <4 x i32> %3240, <i32 1, i32 1, i32 1, i32 1>
  %3266 = add nsw <4 x i32> %3265, %3224
  %3267 = add nuw nsw <4 x i32> %3241, <i32 1, i32 1, i32 1, i32 1>
  %3268 = add nsw <4 x i32> %3267, %3225
  %3269 = add nuw nsw <4 x i32> %3242, <i32 1, i32 1, i32 1, i32 1>
  %3270 = add nsw <4 x i32> %3269, %3226
  %3271 = add nuw nsw <4 x i32> %3243, <i32 1, i32 1, i32 1, i32 1>
  %3272 = add nsw <4 x i32> %3271, %3227
  %3273 = add nuw nsw <4 x i32> %3244, <i32 1, i32 1, i32 1, i32 1>
  %3274 = add nsw <4 x i32> %3273, %3228
  %3275 = add nuw nsw <4 x i32> %3245, <i32 1, i32 1, i32 1, i32 1>
  %3276 = add nsw <4 x i32> %3275, %3229
  %3277 = add nuw nsw <4 x i32> %3246, <i32 1, i32 1, i32 1, i32 1>
  %3278 = add nsw <4 x i32> %3277, %3230
  %3279 = ashr <4 x i32> %3248, <i32 2, i32 2, i32 2, i32 2>
  %3280 = ashr <4 x i32> %3250, <i32 2, i32 2, i32 2, i32 2>
  %3281 = ashr <4 x i32> %3252, <i32 2, i32 2, i32 2, i32 2>
  %3282 = ashr <4 x i32> %3254, <i32 2, i32 2, i32 2, i32 2>
  %3283 = ashr <4 x i32> %3256, <i32 2, i32 2, i32 2, i32 2>
  %3284 = ashr <4 x i32> %3258, <i32 2, i32 2, i32 2, i32 2>
  %3285 = ashr <4 x i32> %3260, <i32 2, i32 2, i32 2, i32 2>
  %3286 = ashr <4 x i32> %3262, <i32 2, i32 2, i32 2, i32 2>
  %3287 = ashr <4 x i32> %3264, <i32 2, i32 2, i32 2, i32 2>
  %3288 = ashr <4 x i32> %3266, <i32 2, i32 2, i32 2, i32 2>
  %3289 = ashr <4 x i32> %3268, <i32 2, i32 2, i32 2, i32 2>
  %3290 = ashr <4 x i32> %3270, <i32 2, i32 2, i32 2, i32 2>
  %3291 = ashr <4 x i32> %3272, <i32 2, i32 2, i32 2, i32 2>
  %3292 = ashr <4 x i32> %3274, <i32 2, i32 2, i32 2, i32 2>
  %3293 = ashr <4 x i32> %3276, <i32 2, i32 2, i32 2, i32 2>
  %3294 = ashr <4 x i32> %3278, <i32 2, i32 2, i32 2, i32 2>
  %3295 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3279, <4 x i32> %3280) #6
  store <8 x i16> %3295, <8 x i16>* %62, align 16
  %3296 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3281, <4 x i32> %3282) #6
  store <8 x i16> %3296, <8 x i16>* %64, align 16
  %3297 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3283, <4 x i32> %3284) #6
  store <8 x i16> %3297, <8 x i16>* %66, align 16
  %3298 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3285, <4 x i32> %3286) #6
  store <8 x i16> %3298, <8 x i16>* %68, align 16
  %3299 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3287, <4 x i32> %3288) #6
  store <8 x i16> %3299, <8 x i16>* %70, align 16
  %3300 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3289, <4 x i32> %3290) #6
  store <8 x i16> %3300, <8 x i16>* %72, align 16
  %3301 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3291, <4 x i32> %3292) #6
  store <8 x i16> %3301, <8 x i16>* %74, align 16
  %3302 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3293, <4 x i32> %3294) #6
  store <8 x i16> %3302, <8 x i16>* %76, align 16
  %3303 = shufflevector <4 x i32> %2887, <4 x i32> %2901, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %3304 = bitcast <4 x i32> %3303 to <2 x i64>
  %3305 = shufflevector <4 x i32> %2887, <4 x i32> %2901, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %3306 = bitcast <4 x i32> %3305 to <2 x i64>
  %3307 = shufflevector <4 x i32> %2888, <4 x i32> %2902, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %3308 = bitcast <4 x i32> %3307 to <2 x i64>
  %3309 = shufflevector <4 x i32> %2888, <4 x i32> %2902, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %3310 = bitcast <4 x i32> %3309 to <2 x i64>
  %3311 = shufflevector <4 x i32> %2889, <4 x i32> %2899, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %3312 = bitcast <4 x i32> %3311 to <2 x i64>
  %3313 = shufflevector <4 x i32> %2889, <4 x i32> %2899, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %3314 = bitcast <4 x i32> %3313 to <2 x i64>
  %3315 = shufflevector <4 x i32> %2890, <4 x i32> %2900, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %3316 = bitcast <4 x i32> %3315 to <2 x i64>
  %3317 = shufflevector <4 x i32> %2890, <4 x i32> %2900, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %3318 = bitcast <4 x i32> %3317 to <2 x i64>
  %3319 = shufflevector <4 x i32> %2891, <4 x i32> %2897, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %3320 = bitcast <4 x i32> %3319 to <2 x i64>
  %3321 = shufflevector <4 x i32> %2891, <4 x i32> %2897, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %3322 = bitcast <4 x i32> %3321 to <2 x i64>
  %3323 = shufflevector <4 x i32> %2892, <4 x i32> %2898, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %3324 = bitcast <4 x i32> %3323 to <2 x i64>
  %3325 = shufflevector <4 x i32> %2892, <4 x i32> %2898, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %3326 = bitcast <4 x i32> %3325 to <2 x i64>
  %3327 = shufflevector <4 x i32> %2893, <4 x i32> %2895, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %3328 = bitcast <4 x i32> %3327 to <2 x i64>
  %3329 = shufflevector <4 x i32> %2893, <4 x i32> %2895, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %3330 = bitcast <4 x i32> %3329 to <2 x i64>
  %3331 = shufflevector <4 x i32> %2894, <4 x i32> %2896, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %3332 = bitcast <4 x i32> %3331 to <2 x i64>
  %3333 = shufflevector <4 x i32> %2894, <4 x i32> %2896, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %3334 = bitcast <4 x i32> %3333 to <2 x i64>
  %3335 = and <2 x i64> %3304, <i64 4294967295, i64 4294967295>
  %3336 = mul nuw nsw <2 x i64> %3335, <i64 3981, i64 3981>
  %3337 = lshr <2 x i64> %3304, <i64 32, i64 32>
  %3338 = mul nuw nsw <2 x i64> %3337, <i64 15893, i64 15893>
  %3339 = add nuw nsw <2 x i64> %3338, %3336
  %3340 = and <2 x i64> %3306, <i64 4294967295, i64 4294967295>
  %3341 = mul nuw nsw <2 x i64> %3340, <i64 3981, i64 3981>
  %3342 = lshr <2 x i64> %3306, <i64 32, i64 32>
  %3343 = mul nuw nsw <2 x i64> %3342, <i64 15893, i64 15893>
  %3344 = add nuw nsw <2 x i64> %3343, %3341
  %3345 = and <2 x i64> %3308, <i64 4294967295, i64 4294967295>
  %3346 = mul nuw nsw <2 x i64> %3345, <i64 3981, i64 3981>
  %3347 = lshr <2 x i64> %3308, <i64 32, i64 32>
  %3348 = mul nuw nsw <2 x i64> %3347, <i64 15893, i64 15893>
  %3349 = add nuw nsw <2 x i64> %3348, %3346
  %3350 = and <2 x i64> %3310, <i64 4294967295, i64 4294967295>
  %3351 = mul nuw nsw <2 x i64> %3350, <i64 3981, i64 3981>
  %3352 = lshr <2 x i64> %3310, <i64 32, i64 32>
  %3353 = mul nuw nsw <2 x i64> %3352, <i64 15893, i64 15893>
  %3354 = add nuw nsw <2 x i64> %3353, %3351
  %3355 = and <2 x i64> %3312, <i64 4294967295, i64 4294967295>
  %3356 = mul nuw nsw <2 x i64> %3355, <i64 14053, i64 14053>
  %3357 = lshr <2 x i64> %3312, <i64 32, i64 32>
  %3358 = mul nuw nsw <2 x i64> %3357, <i64 8423, i64 8423>
  %3359 = add nuw nsw <2 x i64> %3358, %3356
  %3360 = and <2 x i64> %3314, <i64 4294967295, i64 4294967295>
  %3361 = mul nuw nsw <2 x i64> %3360, <i64 14053, i64 14053>
  %3362 = lshr <2 x i64> %3314, <i64 32, i64 32>
  %3363 = mul nuw nsw <2 x i64> %3362, <i64 8423, i64 8423>
  %3364 = add nuw nsw <2 x i64> %3363, %3361
  %3365 = and <2 x i64> %3316, <i64 4294967295, i64 4294967295>
  %3366 = mul nuw nsw <2 x i64> %3365, <i64 14053, i64 14053>
  %3367 = lshr <2 x i64> %3316, <i64 32, i64 32>
  %3368 = mul nuw nsw <2 x i64> %3367, <i64 8423, i64 8423>
  %3369 = add nuw nsw <2 x i64> %3368, %3366
  %3370 = and <2 x i64> %3318, <i64 4294967295, i64 4294967295>
  %3371 = mul nuw nsw <2 x i64> %3370, <i64 14053, i64 14053>
  %3372 = lshr <2 x i64> %3318, <i64 32, i64 32>
  %3373 = mul nuw nsw <2 x i64> %3372, <i64 8423, i64 8423>
  %3374 = add nuw nsw <2 x i64> %3373, %3371
  %3375 = and <2 x i64> %3320, <i64 4294967295, i64 4294967295>
  %3376 = mul nuw nsw <2 x i64> %3375, <i64 9760, i64 9760>
  %3377 = lshr <2 x i64> %3320, <i64 32, i64 32>
  %3378 = mul nuw nsw <2 x i64> %3377, <i64 13160, i64 13160>
  %3379 = add nuw nsw <2 x i64> %3378, %3376
  %3380 = and <2 x i64> %3322, <i64 4294967295, i64 4294967295>
  %3381 = mul nuw nsw <2 x i64> %3380, <i64 9760, i64 9760>
  %3382 = lshr <2 x i64> %3322, <i64 32, i64 32>
  %3383 = mul nuw nsw <2 x i64> %3382, <i64 13160, i64 13160>
  %3384 = add nuw nsw <2 x i64> %3383, %3381
  %3385 = and <2 x i64> %3324, <i64 4294967295, i64 4294967295>
  %3386 = mul nuw nsw <2 x i64> %3385, <i64 9760, i64 9760>
  %3387 = lshr <2 x i64> %3324, <i64 32, i64 32>
  %3388 = mul nuw nsw <2 x i64> %3387, <i64 13160, i64 13160>
  %3389 = add nuw nsw <2 x i64> %3388, %3386
  %3390 = and <2 x i64> %3326, <i64 4294967295, i64 4294967295>
  %3391 = mul nuw nsw <2 x i64> %3390, <i64 9760, i64 9760>
  %3392 = lshr <2 x i64> %3326, <i64 32, i64 32>
  %3393 = mul nuw nsw <2 x i64> %3392, <i64 13160, i64 13160>
  %3394 = add nuw nsw <2 x i64> %3393, %3391
  %3395 = and <2 x i64> %3328, <i64 4294967295, i64 4294967295>
  %3396 = mul nuw nsw <2 x i64> %3395, <i64 16207, i64 16207>
  %3397 = lshr <2 x i64> %3328, <i64 32, i64 32>
  %3398 = mul nuw nsw <2 x i64> %3397, <i64 2404, i64 2404>
  %3399 = add nuw nsw <2 x i64> %3398, %3396
  %3400 = and <2 x i64> %3330, <i64 4294967295, i64 4294967295>
  %3401 = mul nuw nsw <2 x i64> %3400, <i64 16207, i64 16207>
  %3402 = lshr <2 x i64> %3330, <i64 32, i64 32>
  %3403 = mul nuw nsw <2 x i64> %3402, <i64 2404, i64 2404>
  %3404 = add nuw nsw <2 x i64> %3403, %3401
  %3405 = and <2 x i64> %3332, <i64 4294967295, i64 4294967295>
  %3406 = mul nuw nsw <2 x i64> %3405, <i64 16207, i64 16207>
  %3407 = lshr <2 x i64> %3332, <i64 32, i64 32>
  %3408 = mul nuw nsw <2 x i64> %3407, <i64 2404, i64 2404>
  %3409 = add nuw nsw <2 x i64> %3408, %3406
  %3410 = and <2 x i64> %3334, <i64 4294967295, i64 4294967295>
  %3411 = mul nuw nsw <2 x i64> %3410, <i64 16207, i64 16207>
  %3412 = lshr <2 x i64> %3334, <i64 32, i64 32>
  %3413 = mul nuw nsw <2 x i64> %3412, <i64 2404, i64 2404>
  %3414 = add nuw nsw <2 x i64> %3413, %3411
  %3415 = mul nuw <2 x i64> %3395, <i64 4294964892, i64 4294964892>
  %3416 = mul nuw nsw <2 x i64> %3397, <i64 16207, i64 16207>
  %3417 = add <2 x i64> %3416, %3415
  %3418 = mul nuw <2 x i64> %3400, <i64 4294964892, i64 4294964892>
  %3419 = mul nuw nsw <2 x i64> %3402, <i64 16207, i64 16207>
  %3420 = add <2 x i64> %3419, %3418
  %3421 = mul nuw <2 x i64> %3405, <i64 4294964892, i64 4294964892>
  %3422 = mul nuw nsw <2 x i64> %3407, <i64 16207, i64 16207>
  %3423 = add <2 x i64> %3422, %3421
  %3424 = mul nuw <2 x i64> %3410, <i64 4294964892, i64 4294964892>
  %3425 = mul nuw nsw <2 x i64> %3412, <i64 16207, i64 16207>
  %3426 = add <2 x i64> %3425, %3424
  %3427 = mul nuw <2 x i64> %3375, <i64 4294954136, i64 4294954136>
  %3428 = mul nuw nsw <2 x i64> %3377, <i64 9760, i64 9760>
  %3429 = add <2 x i64> %3428, %3427
  %3430 = mul nuw <2 x i64> %3380, <i64 4294954136, i64 4294954136>
  %3431 = mul nuw nsw <2 x i64> %3382, <i64 9760, i64 9760>
  %3432 = add <2 x i64> %3431, %3430
  %3433 = mul nuw <2 x i64> %3385, <i64 4294954136, i64 4294954136>
  %3434 = mul nuw nsw <2 x i64> %3387, <i64 9760, i64 9760>
  %3435 = add <2 x i64> %3434, %3433
  %3436 = mul nuw <2 x i64> %3390, <i64 4294954136, i64 4294954136>
  %3437 = mul nuw nsw <2 x i64> %3392, <i64 9760, i64 9760>
  %3438 = add <2 x i64> %3437, %3436
  %3439 = mul nuw <2 x i64> %3355, <i64 4294958873, i64 4294958873>
  %3440 = mul nuw nsw <2 x i64> %3357, <i64 14053, i64 14053>
  %3441 = add <2 x i64> %3440, %3439
  %3442 = mul nuw <2 x i64> %3360, <i64 4294958873, i64 4294958873>
  %3443 = mul nuw nsw <2 x i64> %3362, <i64 14053, i64 14053>
  %3444 = add <2 x i64> %3443, %3442
  %3445 = mul nuw <2 x i64> %3365, <i64 4294958873, i64 4294958873>
  %3446 = mul nuw nsw <2 x i64> %3367, <i64 14053, i64 14053>
  %3447 = add <2 x i64> %3446, %3445
  %3448 = mul nuw <2 x i64> %3370, <i64 4294958873, i64 4294958873>
  %3449 = mul nuw nsw <2 x i64> %3372, <i64 14053, i64 14053>
  %3450 = add <2 x i64> %3449, %3448
  %3451 = mul nuw <2 x i64> %3335, <i64 4294951403, i64 4294951403>
  %3452 = mul nuw nsw <2 x i64> %3337, <i64 3981, i64 3981>
  %3453 = add <2 x i64> %3452, %3451
  %3454 = mul nuw <2 x i64> %3340, <i64 4294951403, i64 4294951403>
  %3455 = mul nuw nsw <2 x i64> %3342, <i64 3981, i64 3981>
  %3456 = add <2 x i64> %3455, %3454
  %3457 = mul nuw <2 x i64> %3345, <i64 4294951403, i64 4294951403>
  %3458 = mul nuw nsw <2 x i64> %3347, <i64 3981, i64 3981>
  %3459 = add <2 x i64> %3458, %3457
  %3460 = mul nuw <2 x i64> %3350, <i64 4294951403, i64 4294951403>
  %3461 = mul nuw nsw <2 x i64> %3352, <i64 3981, i64 3981>
  %3462 = add <2 x i64> %3461, %3460
  %3463 = bitcast <2 x i64> %3339 to <4 x i32>
  %3464 = shufflevector <4 x i32> %3463, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3465 = bitcast <4 x i32> %3464 to <2 x i64>
  %3466 = bitcast <2 x i64> %3344 to <4 x i32>
  %3467 = shufflevector <4 x i32> %3466, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3468 = bitcast <4 x i32> %3467 to <2 x i64>
  %3469 = shufflevector <2 x i64> %3465, <2 x i64> %3468, <2 x i32> <i32 0, i32 2>
  %3470 = bitcast <2 x i64> %3349 to <4 x i32>
  %3471 = shufflevector <4 x i32> %3470, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3472 = bitcast <4 x i32> %3471 to <2 x i64>
  %3473 = bitcast <2 x i64> %3354 to <4 x i32>
  %3474 = shufflevector <4 x i32> %3473, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3475 = bitcast <4 x i32> %3474 to <2 x i64>
  %3476 = shufflevector <2 x i64> %3472, <2 x i64> %3475, <2 x i32> <i32 0, i32 2>
  %3477 = bitcast <2 x i64> %3359 to <4 x i32>
  %3478 = shufflevector <4 x i32> %3477, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3479 = bitcast <4 x i32> %3478 to <2 x i64>
  %3480 = bitcast <2 x i64> %3364 to <4 x i32>
  %3481 = shufflevector <4 x i32> %3480, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3482 = bitcast <4 x i32> %3481 to <2 x i64>
  %3483 = shufflevector <2 x i64> %3479, <2 x i64> %3482, <2 x i32> <i32 0, i32 2>
  %3484 = bitcast <2 x i64> %3369 to <4 x i32>
  %3485 = shufflevector <4 x i32> %3484, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3486 = bitcast <4 x i32> %3485 to <2 x i64>
  %3487 = bitcast <2 x i64> %3374 to <4 x i32>
  %3488 = shufflevector <4 x i32> %3487, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3489 = bitcast <4 x i32> %3488 to <2 x i64>
  %3490 = shufflevector <2 x i64> %3486, <2 x i64> %3489, <2 x i32> <i32 0, i32 2>
  %3491 = bitcast <2 x i64> %3379 to <4 x i32>
  %3492 = shufflevector <4 x i32> %3491, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3493 = bitcast <4 x i32> %3492 to <2 x i64>
  %3494 = bitcast <2 x i64> %3384 to <4 x i32>
  %3495 = shufflevector <4 x i32> %3494, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3496 = bitcast <4 x i32> %3495 to <2 x i64>
  %3497 = shufflevector <2 x i64> %3493, <2 x i64> %3496, <2 x i32> <i32 0, i32 2>
  %3498 = bitcast <2 x i64> %3389 to <4 x i32>
  %3499 = shufflevector <4 x i32> %3498, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3500 = bitcast <4 x i32> %3499 to <2 x i64>
  %3501 = bitcast <2 x i64> %3394 to <4 x i32>
  %3502 = shufflevector <4 x i32> %3501, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3503 = bitcast <4 x i32> %3502 to <2 x i64>
  %3504 = shufflevector <2 x i64> %3500, <2 x i64> %3503, <2 x i32> <i32 0, i32 2>
  %3505 = bitcast <2 x i64> %3399 to <4 x i32>
  %3506 = shufflevector <4 x i32> %3505, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3507 = bitcast <4 x i32> %3506 to <2 x i64>
  %3508 = bitcast <2 x i64> %3404 to <4 x i32>
  %3509 = shufflevector <4 x i32> %3508, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3510 = bitcast <4 x i32> %3509 to <2 x i64>
  %3511 = shufflevector <2 x i64> %3507, <2 x i64> %3510, <2 x i32> <i32 0, i32 2>
  %3512 = bitcast <2 x i64> %3409 to <4 x i32>
  %3513 = shufflevector <4 x i32> %3512, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3514 = bitcast <4 x i32> %3513 to <2 x i64>
  %3515 = bitcast <2 x i64> %3414 to <4 x i32>
  %3516 = shufflevector <4 x i32> %3515, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3517 = bitcast <4 x i32> %3516 to <2 x i64>
  %3518 = shufflevector <2 x i64> %3514, <2 x i64> %3517, <2 x i32> <i32 0, i32 2>
  %3519 = bitcast <2 x i64> %3417 to <4 x i32>
  %3520 = shufflevector <4 x i32> %3519, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3521 = bitcast <4 x i32> %3520 to <2 x i64>
  %3522 = bitcast <2 x i64> %3420 to <4 x i32>
  %3523 = shufflevector <4 x i32> %3522, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3524 = bitcast <4 x i32> %3523 to <2 x i64>
  %3525 = shufflevector <2 x i64> %3521, <2 x i64> %3524, <2 x i32> <i32 0, i32 2>
  %3526 = bitcast <2 x i64> %3423 to <4 x i32>
  %3527 = shufflevector <4 x i32> %3526, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3528 = bitcast <4 x i32> %3527 to <2 x i64>
  %3529 = bitcast <2 x i64> %3426 to <4 x i32>
  %3530 = shufflevector <4 x i32> %3529, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3531 = bitcast <4 x i32> %3530 to <2 x i64>
  %3532 = shufflevector <2 x i64> %3528, <2 x i64> %3531, <2 x i32> <i32 0, i32 2>
  %3533 = bitcast <2 x i64> %3429 to <4 x i32>
  %3534 = shufflevector <4 x i32> %3533, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3535 = bitcast <4 x i32> %3534 to <2 x i64>
  %3536 = bitcast <2 x i64> %3432 to <4 x i32>
  %3537 = shufflevector <4 x i32> %3536, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3538 = bitcast <4 x i32> %3537 to <2 x i64>
  %3539 = shufflevector <2 x i64> %3535, <2 x i64> %3538, <2 x i32> <i32 0, i32 2>
  %3540 = bitcast <2 x i64> %3435 to <4 x i32>
  %3541 = shufflevector <4 x i32> %3540, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3542 = bitcast <4 x i32> %3541 to <2 x i64>
  %3543 = bitcast <2 x i64> %3438 to <4 x i32>
  %3544 = shufflevector <4 x i32> %3543, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3545 = bitcast <4 x i32> %3544 to <2 x i64>
  %3546 = shufflevector <2 x i64> %3542, <2 x i64> %3545, <2 x i32> <i32 0, i32 2>
  %3547 = bitcast <2 x i64> %3441 to <4 x i32>
  %3548 = shufflevector <4 x i32> %3547, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3549 = bitcast <4 x i32> %3548 to <2 x i64>
  %3550 = bitcast <2 x i64> %3444 to <4 x i32>
  %3551 = shufflevector <4 x i32> %3550, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3552 = bitcast <4 x i32> %3551 to <2 x i64>
  %3553 = shufflevector <2 x i64> %3549, <2 x i64> %3552, <2 x i32> <i32 0, i32 2>
  %3554 = bitcast <2 x i64> %3447 to <4 x i32>
  %3555 = shufflevector <4 x i32> %3554, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3556 = bitcast <4 x i32> %3555 to <2 x i64>
  %3557 = bitcast <2 x i64> %3450 to <4 x i32>
  %3558 = shufflevector <4 x i32> %3557, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3559 = bitcast <4 x i32> %3558 to <2 x i64>
  %3560 = shufflevector <2 x i64> %3556, <2 x i64> %3559, <2 x i32> <i32 0, i32 2>
  %3561 = bitcast <2 x i64> %3453 to <4 x i32>
  %3562 = shufflevector <4 x i32> %3561, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3563 = bitcast <4 x i32> %3562 to <2 x i64>
  %3564 = bitcast <2 x i64> %3456 to <4 x i32>
  %3565 = shufflevector <4 x i32> %3564, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3566 = bitcast <4 x i32> %3565 to <2 x i64>
  %3567 = shufflevector <2 x i64> %3563, <2 x i64> %3566, <2 x i32> <i32 0, i32 2>
  %3568 = bitcast <2 x i64> %3459 to <4 x i32>
  %3569 = shufflevector <4 x i32> %3568, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3570 = bitcast <4 x i32> %3569 to <2 x i64>
  %3571 = bitcast <2 x i64> %3462 to <4 x i32>
  %3572 = shufflevector <4 x i32> %3571, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3573 = bitcast <4 x i32> %3572 to <2 x i64>
  %3574 = shufflevector <2 x i64> %3570, <2 x i64> %3573, <2 x i32> <i32 0, i32 2>
  %3575 = bitcast <2 x i64> %3469 to <4 x i32>
  %3576 = add <4 x i32> %3575, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3577 = bitcast <2 x i64> %3476 to <4 x i32>
  %3578 = add <4 x i32> %3577, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3579 = bitcast <2 x i64> %3483 to <4 x i32>
  %3580 = add <4 x i32> %3579, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3581 = bitcast <2 x i64> %3490 to <4 x i32>
  %3582 = add <4 x i32> %3581, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3583 = bitcast <2 x i64> %3497 to <4 x i32>
  %3584 = add <4 x i32> %3583, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3585 = bitcast <2 x i64> %3504 to <4 x i32>
  %3586 = add <4 x i32> %3585, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3587 = bitcast <2 x i64> %3511 to <4 x i32>
  %3588 = add <4 x i32> %3587, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3589 = bitcast <2 x i64> %3518 to <4 x i32>
  %3590 = add <4 x i32> %3589, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3591 = bitcast <2 x i64> %3525 to <4 x i32>
  %3592 = add <4 x i32> %3591, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3593 = bitcast <2 x i64> %3532 to <4 x i32>
  %3594 = add <4 x i32> %3593, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3595 = bitcast <2 x i64> %3539 to <4 x i32>
  %3596 = add <4 x i32> %3595, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3597 = bitcast <2 x i64> %3546 to <4 x i32>
  %3598 = add <4 x i32> %3597, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3599 = bitcast <2 x i64> %3553 to <4 x i32>
  %3600 = add <4 x i32> %3599, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3601 = bitcast <2 x i64> %3560 to <4 x i32>
  %3602 = add <4 x i32> %3601, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3603 = bitcast <2 x i64> %3567 to <4 x i32>
  %3604 = add <4 x i32> %3603, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3605 = bitcast <2 x i64> %3574 to <4 x i32>
  %3606 = add <4 x i32> %3605, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3607 = ashr <4 x i32> %3576, <i32 14, i32 14, i32 14, i32 14>
  %3608 = ashr <4 x i32> %3578, <i32 14, i32 14, i32 14, i32 14>
  %3609 = ashr <4 x i32> %3580, <i32 14, i32 14, i32 14, i32 14>
  %3610 = ashr <4 x i32> %3582, <i32 14, i32 14, i32 14, i32 14>
  %3611 = ashr <4 x i32> %3584, <i32 14, i32 14, i32 14, i32 14>
  %3612 = ashr <4 x i32> %3586, <i32 14, i32 14, i32 14, i32 14>
  %3613 = ashr <4 x i32> %3588, <i32 14, i32 14, i32 14, i32 14>
  %3614 = ashr <4 x i32> %3590, <i32 14, i32 14, i32 14, i32 14>
  %3615 = ashr <4 x i32> %3592, <i32 14, i32 14, i32 14, i32 14>
  %3616 = ashr <4 x i32> %3594, <i32 14, i32 14, i32 14, i32 14>
  %3617 = ashr <4 x i32> %3596, <i32 14, i32 14, i32 14, i32 14>
  %3618 = ashr <4 x i32> %3598, <i32 14, i32 14, i32 14, i32 14>
  %3619 = ashr <4 x i32> %3600, <i32 14, i32 14, i32 14, i32 14>
  %3620 = ashr <4 x i32> %3602, <i32 14, i32 14, i32 14, i32 14>
  %3621 = ashr <4 x i32> %3604, <i32 14, i32 14, i32 14, i32 14>
  %3622 = ashr <4 x i32> %3606, <i32 14, i32 14, i32 14, i32 14>
  %3623 = ashr <4 x i32> %3576, <i32 31, i32 31, i32 31, i32 31>
  %3624 = ashr <4 x i32> %3578, <i32 31, i32 31, i32 31, i32 31>
  %3625 = ashr <4 x i32> %3580, <i32 31, i32 31, i32 31, i32 31>
  %3626 = ashr <4 x i32> %3582, <i32 31, i32 31, i32 31, i32 31>
  %3627 = ashr <4 x i32> %3584, <i32 31, i32 31, i32 31, i32 31>
  %3628 = ashr <4 x i32> %3586, <i32 31, i32 31, i32 31, i32 31>
  %3629 = ashr <4 x i32> %3588, <i32 31, i32 31, i32 31, i32 31>
  %3630 = ashr <4 x i32> %3590, <i32 31, i32 31, i32 31, i32 31>
  %3631 = ashr <4 x i32> %3592, <i32 31, i32 31, i32 31, i32 31>
  %3632 = ashr <4 x i32> %3594, <i32 31, i32 31, i32 31, i32 31>
  %3633 = ashr <4 x i32> %3596, <i32 31, i32 31, i32 31, i32 31>
  %3634 = ashr <4 x i32> %3598, <i32 31, i32 31, i32 31, i32 31>
  %3635 = ashr <4 x i32> %3600, <i32 31, i32 31, i32 31, i32 31>
  %3636 = ashr <4 x i32> %3602, <i32 31, i32 31, i32 31, i32 31>
  %3637 = ashr <4 x i32> %3604, <i32 31, i32 31, i32 31, i32 31>
  %3638 = ashr <4 x i32> %3606, <i32 31, i32 31, i32 31, i32 31>
  %3639 = sub nsw <4 x i32> %3607, %3623
  %3640 = sub nsw <4 x i32> %3608, %3624
  %3641 = sub nsw <4 x i32> %3609, %3625
  %3642 = sub nsw <4 x i32> %3610, %3626
  %3643 = sub nsw <4 x i32> %3611, %3627
  %3644 = sub nsw <4 x i32> %3612, %3628
  %3645 = sub nsw <4 x i32> %3613, %3629
  %3646 = sub nsw <4 x i32> %3614, %3630
  %3647 = sub nsw <4 x i32> %3615, %3631
  %3648 = sub nsw <4 x i32> %3616, %3632
  %3649 = sub nsw <4 x i32> %3617, %3633
  %3650 = sub nsw <4 x i32> %3618, %3634
  %3651 = sub nsw <4 x i32> %3619, %3635
  %3652 = sub nsw <4 x i32> %3620, %3636
  %3653 = sub nsw <4 x i32> %3621, %3637
  %3654 = sub nsw <4 x i32> %3622, %3638
  %3655 = add nsw <4 x i32> %3639, <i32 1, i32 1, i32 1, i32 1>
  %3656 = add nsw <4 x i32> %3640, <i32 1, i32 1, i32 1, i32 1>
  %3657 = add nsw <4 x i32> %3641, <i32 1, i32 1, i32 1, i32 1>
  %3658 = add nsw <4 x i32> %3642, <i32 1, i32 1, i32 1, i32 1>
  %3659 = add nsw <4 x i32> %3643, <i32 1, i32 1, i32 1, i32 1>
  %3660 = add nsw <4 x i32> %3644, <i32 1, i32 1, i32 1, i32 1>
  %3661 = add nsw <4 x i32> %3645, <i32 1, i32 1, i32 1, i32 1>
  %3662 = add nsw <4 x i32> %3646, <i32 1, i32 1, i32 1, i32 1>
  %3663 = add nsw <4 x i32> %3647, <i32 1, i32 1, i32 1, i32 1>
  %3664 = add nsw <4 x i32> %3648, <i32 1, i32 1, i32 1, i32 1>
  %3665 = add nsw <4 x i32> %3649, <i32 1, i32 1, i32 1, i32 1>
  %3666 = add nsw <4 x i32> %3650, <i32 1, i32 1, i32 1, i32 1>
  %3667 = add nsw <4 x i32> %3651, <i32 1, i32 1, i32 1, i32 1>
  %3668 = add nsw <4 x i32> %3652, <i32 1, i32 1, i32 1, i32 1>
  %3669 = add nsw <4 x i32> %3653, <i32 1, i32 1, i32 1, i32 1>
  %3670 = add nsw <4 x i32> %3654, <i32 1, i32 1, i32 1, i32 1>
  br label %3671

3671:                                             ; preds = %1022, %501
  %3672 = phi <4 x i32> [ <i32 2, i32 2, i32 2, i32 2>, %1022 ], [ <i32 14, i32 14, i32 14, i32 14>, %501 ]
  %3673 = phi <4 x i32> [ %3655, %1022 ], [ %1006, %501 ]
  %3674 = phi <4 x i32> [ %3656, %1022 ], [ %1007, %501 ]
  %3675 = phi <4 x i32> [ %3657, %1022 ], [ %1008, %501 ]
  %3676 = phi <4 x i32> [ %3658, %1022 ], [ %1009, %501 ]
  %3677 = phi <4 x i32> [ %3659, %1022 ], [ %1010, %501 ]
  %3678 = phi <4 x i32> [ %3660, %1022 ], [ %1011, %501 ]
  %3679 = phi <4 x i32> [ %3661, %1022 ], [ %1012, %501 ]
  %3680 = phi <4 x i32> [ %3662, %1022 ], [ %1013, %501 ]
  %3681 = phi <4 x i32> [ %3663, %1022 ], [ %1014, %501 ]
  %3682 = phi <4 x i32> [ %3664, %1022 ], [ %1015, %501 ]
  %3683 = phi <4 x i32> [ %3665, %1022 ], [ %1016, %501 ]
  %3684 = phi <4 x i32> [ %3666, %1022 ], [ %1017, %501 ]
  %3685 = phi <4 x i32> [ %3667, %1022 ], [ %1018, %501 ]
  %3686 = phi <4 x i32> [ %3668, %1022 ], [ %1019, %501 ]
  %3687 = phi <4 x i32> [ %3669, %1022 ], [ %1020, %501 ]
  %3688 = phi <4 x i32> [ %3670, %1022 ], [ %1021, %501 ]
  %3689 = ashr <4 x i32> %3673, %3672
  %3690 = ashr <4 x i32> %3674, %3672
  %3691 = ashr <4 x i32> %3675, %3672
  %3692 = ashr <4 x i32> %3676, %3672
  %3693 = ashr <4 x i32> %3677, %3672
  %3694 = ashr <4 x i32> %3678, %3672
  %3695 = ashr <4 x i32> %3679, %3672
  %3696 = ashr <4 x i32> %3680, %3672
  %3697 = ashr <4 x i32> %3681, %3672
  %3698 = ashr <4 x i32> %3682, %3672
  %3699 = ashr <4 x i32> %3683, %3672
  %3700 = ashr <4 x i32> %3684, %3672
  %3701 = ashr <4 x i32> %3685, %3672
  %3702 = ashr <4 x i32> %3686, %3672
  %3703 = ashr <4 x i32> %3687, %3672
  %3704 = ashr <4 x i32> %3688, %3672
  %3705 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3689, <4 x i32> %3690) #6
  store <8 x i16> %3705, <8 x i16>* %78, align 16
  %3706 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3691, <4 x i32> %3692) #6
  store <8 x i16> %3706, <8 x i16>* %80, align 16
  %3707 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3693, <4 x i32> %3694) #6
  store <8 x i16> %3707, <8 x i16>* %82, align 16
  %3708 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3695, <4 x i32> %3696) #6
  store <8 x i16> %3708, <8 x i16>* %84, align 16
  %3709 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3697, <4 x i32> %3698) #6
  store <8 x i16> %3709, <8 x i16>* %86, align 16
  %3710 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3699, <4 x i32> %3700) #6
  store <8 x i16> %3710, <8 x i16>* %88, align 16
  %3711 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3701, <4 x i32> %3702) #6
  store <8 x i16> %3711, <8 x i16>* %90, align 16
  %3712 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3703, <4 x i32> %3704) #6
  store <8 x i16> %3712, <8 x i16>* %92, align 16
  %3713 = shl nsw i64 %97, 5
  %3714 = getelementptr inbounds [1024 x i16], [1024 x i16]* %4, i64 0, i64 %3713
  %3715 = getelementptr inbounds i32, i32* %1, i64 %3713
  br label %3716

3716:                                             ; preds = %3914, %3671
  %3717 = phi i64 [ 0, %3671 ], [ %3917, %3914 ]
  %3718 = phi i32* [ %3715, %3671 ], [ %3916, %3914 ]
  %3719 = phi i16* [ %3714, %3671 ], [ %3915, %3914 ]
  %3720 = shl nsw i64 %3717, 3
  %3721 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %5, i64 0, i64 %3720
  %3722 = bitcast <2 x i64>* %3721 to <8 x i16>*
  %3723 = load <8 x i16>, <8 x i16>* %3722, align 16
  %3724 = getelementptr inbounds <2 x i64>, <2 x i64>* %3721, i64 1
  %3725 = bitcast <2 x i64>* %3724 to <8 x i16>*
  %3726 = load <8 x i16>, <8 x i16>* %3725, align 16
  %3727 = shufflevector <8 x i16> %3723, <8 x i16> %3726, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3728 = getelementptr inbounds <2 x i64>, <2 x i64>* %3721, i64 2
  %3729 = bitcast <2 x i64>* %3728 to <8 x i16>*
  %3730 = load <8 x i16>, <8 x i16>* %3729, align 16
  %3731 = getelementptr inbounds <2 x i64>, <2 x i64>* %3721, i64 3
  %3732 = bitcast <2 x i64>* %3731 to <8 x i16>*
  %3733 = load <8 x i16>, <8 x i16>* %3732, align 16
  %3734 = shufflevector <8 x i16> %3730, <8 x i16> %3733, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3735 = shufflevector <8 x i16> %3723, <8 x i16> %3726, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3736 = shufflevector <8 x i16> %3730, <8 x i16> %3733, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3737 = getelementptr inbounds <2 x i64>, <2 x i64>* %3721, i64 4
  %3738 = bitcast <2 x i64>* %3737 to <8 x i16>*
  %3739 = load <8 x i16>, <8 x i16>* %3738, align 16
  %3740 = getelementptr inbounds <2 x i64>, <2 x i64>* %3721, i64 5
  %3741 = bitcast <2 x i64>* %3740 to <8 x i16>*
  %3742 = load <8 x i16>, <8 x i16>* %3741, align 16
  %3743 = shufflevector <8 x i16> %3739, <8 x i16> %3742, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3744 = getelementptr inbounds <2 x i64>, <2 x i64>* %3721, i64 6
  %3745 = bitcast <2 x i64>* %3744 to <8 x i16>*
  %3746 = load <8 x i16>, <8 x i16>* %3745, align 16
  %3747 = getelementptr inbounds <2 x i64>, <2 x i64>* %3721, i64 7
  %3748 = bitcast <2 x i64>* %3747 to <8 x i16>*
  %3749 = load <8 x i16>, <8 x i16>* %3748, align 16
  %3750 = shufflevector <8 x i16> %3746, <8 x i16> %3749, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3751 = shufflevector <8 x i16> %3739, <8 x i16> %3742, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3752 = shufflevector <8 x i16> %3746, <8 x i16> %3749, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3753 = bitcast <8 x i16> %3727 to <4 x i32>
  %3754 = bitcast <8 x i16> %3734 to <4 x i32>
  %3755 = shufflevector <4 x i32> %3753, <4 x i32> %3754, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %3756 = bitcast <4 x i32> %3755 to <2 x i64>
  %3757 = bitcast <8 x i16> %3735 to <4 x i32>
  %3758 = bitcast <8 x i16> %3736 to <4 x i32>
  %3759 = shufflevector <4 x i32> %3757, <4 x i32> %3758, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %3760 = bitcast <4 x i32> %3759 to <2 x i64>
  %3761 = shufflevector <4 x i32> %3753, <4 x i32> %3754, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %3762 = bitcast <4 x i32> %3761 to <2 x i64>
  %3763 = shufflevector <4 x i32> %3757, <4 x i32> %3758, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %3764 = bitcast <4 x i32> %3763 to <2 x i64>
  %3765 = bitcast <8 x i16> %3743 to <4 x i32>
  %3766 = bitcast <8 x i16> %3750 to <4 x i32>
  %3767 = shufflevector <4 x i32> %3765, <4 x i32> %3766, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %3768 = bitcast <4 x i32> %3767 to <2 x i64>
  %3769 = bitcast <8 x i16> %3751 to <4 x i32>
  %3770 = bitcast <8 x i16> %3752 to <4 x i32>
  %3771 = shufflevector <4 x i32> %3769, <4 x i32> %3770, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %3772 = bitcast <4 x i32> %3771 to <2 x i64>
  %3773 = shufflevector <4 x i32> %3765, <4 x i32> %3766, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %3774 = bitcast <4 x i32> %3773 to <2 x i64>
  %3775 = shufflevector <4 x i32> %3769, <4 x i32> %3770, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %3776 = bitcast <4 x i32> %3775 to <2 x i64>
  %3777 = shufflevector <2 x i64> %3756, <2 x i64> %3768, <2 x i32> <i32 0, i32 2>
  %3778 = shufflevector <2 x i64> %3756, <2 x i64> %3768, <2 x i32> <i32 1, i32 3>
  %3779 = shufflevector <2 x i64> %3762, <2 x i64> %3774, <2 x i32> <i32 0, i32 2>
  %3780 = shufflevector <2 x i64> %3762, <2 x i64> %3774, <2 x i32> <i32 1, i32 3>
  %3781 = shufflevector <2 x i64> %3760, <2 x i64> %3772, <2 x i32> <i32 0, i32 2>
  %3782 = shufflevector <2 x i64> %3760, <2 x i64> %3772, <2 x i32> <i32 1, i32 3>
  %3783 = shufflevector <2 x i64> %3764, <2 x i64> %3776, <2 x i32> <i32 0, i32 2>
  %3784 = shufflevector <2 x i64> %3764, <2 x i64> %3776, <2 x i32> <i32 1, i32 3>
  %3785 = bitcast <2 x i64> %3777 to <8 x i16>
  br i1 %95, label %3786, label %3850

3786:                                             ; preds = %3716
  %3787 = icmp sgt <8 x i16> %3785, zeroinitializer
  %3788 = bitcast <2 x i64> %3778 to <8 x i16>
  %3789 = icmp sgt <8 x i16> %3788, zeroinitializer
  %3790 = bitcast <2 x i64> %3779 to <8 x i16>
  %3791 = icmp sgt <8 x i16> %3790, zeroinitializer
  %3792 = bitcast <2 x i64> %3780 to <8 x i16>
  %3793 = icmp sgt <8 x i16> %3792, zeroinitializer
  %3794 = bitcast <2 x i64> %3781 to <8 x i16>
  %3795 = icmp sgt <8 x i16> %3794, zeroinitializer
  %3796 = bitcast <2 x i64> %3782 to <8 x i16>
  %3797 = icmp sgt <8 x i16> %3796, zeroinitializer
  %3798 = bitcast <2 x i64> %3783 to <8 x i16>
  %3799 = icmp sgt <8 x i16> %3798, zeroinitializer
  %3800 = bitcast <2 x i64> %3784 to <8 x i16>
  %3801 = icmp sgt <8 x i16> %3800, zeroinitializer
  %3802 = zext <8 x i1> %3787 to <8 x i16>
  %3803 = zext <8 x i1> %3789 to <8 x i16>
  %3804 = zext <8 x i1> %3791 to <8 x i16>
  %3805 = zext <8 x i1> %3793 to <8 x i16>
  %3806 = zext <8 x i1> %3795 to <8 x i16>
  %3807 = zext <8 x i1> %3797 to <8 x i16>
  %3808 = zext <8 x i1> %3799 to <8 x i16>
  %3809 = zext <8 x i1> %3801 to <8 x i16>
  %3810 = add <8 x i16> %3785, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %3811 = add <8 x i16> %3810, %3802
  %3812 = add <8 x i16> %3788, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %3813 = add <8 x i16> %3812, %3803
  %3814 = add <8 x i16> %3790, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %3815 = add <8 x i16> %3814, %3804
  %3816 = add <8 x i16> %3792, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %3817 = add <8 x i16> %3816, %3805
  %3818 = add <8 x i16> %3794, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %3819 = add <8 x i16> %3818, %3806
  %3820 = add <8 x i16> %3796, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %3821 = add <8 x i16> %3820, %3807
  %3822 = add <8 x i16> %3798, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %3823 = add <8 x i16> %3822, %3808
  %3824 = add <8 x i16> %3800, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %3825 = add <8 x i16> %3824, %3809
  %3826 = ashr <8 x i16> %3811, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3827 = ashr <8 x i16> %3813, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3828 = ashr <8 x i16> %3815, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3829 = ashr <8 x i16> %3817, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3830 = ashr <8 x i16> %3819, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3831 = ashr <8 x i16> %3821, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3832 = ashr <8 x i16> %3823, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3833 = ashr <8 x i16> %3825, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3834 = bitcast i16* %3719 to <8 x i16>*
  store <8 x i16> %3826, <8 x i16>* %3834, align 1
  %3835 = getelementptr inbounds i16, i16* %3719, i64 32
  %3836 = bitcast i16* %3835 to <8 x i16>*
  store <8 x i16> %3827, <8 x i16>* %3836, align 1
  %3837 = getelementptr inbounds i16, i16* %3719, i64 64
  %3838 = bitcast i16* %3837 to <8 x i16>*
  store <8 x i16> %3828, <8 x i16>* %3838, align 1
  %3839 = getelementptr inbounds i16, i16* %3719, i64 96
  %3840 = bitcast i16* %3839 to <8 x i16>*
  store <8 x i16> %3829, <8 x i16>* %3840, align 1
  %3841 = getelementptr inbounds i16, i16* %3719, i64 128
  %3842 = bitcast i16* %3841 to <8 x i16>*
  store <8 x i16> %3830, <8 x i16>* %3842, align 1
  %3843 = getelementptr inbounds i16, i16* %3719, i64 160
  %3844 = bitcast i16* %3843 to <8 x i16>*
  store <8 x i16> %3831, <8 x i16>* %3844, align 1
  %3845 = getelementptr inbounds i16, i16* %3719, i64 192
  %3846 = bitcast i16* %3845 to <8 x i16>*
  store <8 x i16> %3832, <8 x i16>* %3846, align 1
  %3847 = getelementptr inbounds i16, i16* %3719, i64 224
  %3848 = bitcast i16* %3847 to <8 x i16>*
  store <8 x i16> %3833, <8 x i16>* %3848, align 1
  %3849 = getelementptr inbounds i16, i16* %3719, i64 8
  br label %3914

3850:                                             ; preds = %3716
  %3851 = ashr <8 x i16> %3785, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %3852 = shufflevector <8 x i16> %3785, <8 x i16> %3851, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3853 = shufflevector <8 x i16> %3785, <8 x i16> %3851, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3854 = bitcast i32* %3718 to <8 x i16>*
  store <8 x i16> %3852, <8 x i16>* %3854, align 1
  %3855 = getelementptr inbounds i32, i32* %3718, i64 4
  %3856 = bitcast i32* %3855 to <8 x i16>*
  store <8 x i16> %3853, <8 x i16>* %3856, align 1
  %3857 = getelementptr inbounds i32, i32* %3718, i64 32
  %3858 = bitcast <2 x i64> %3778 to <8 x i16>
  %3859 = ashr <8 x i16> %3858, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %3860 = shufflevector <8 x i16> %3858, <8 x i16> %3859, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3861 = shufflevector <8 x i16> %3858, <8 x i16> %3859, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3862 = bitcast i32* %3857 to <8 x i16>*
  store <8 x i16> %3860, <8 x i16>* %3862, align 1
  %3863 = getelementptr inbounds i32, i32* %3718, i64 36
  %3864 = bitcast i32* %3863 to <8 x i16>*
  store <8 x i16> %3861, <8 x i16>* %3864, align 1
  %3865 = getelementptr inbounds i32, i32* %3718, i64 64
  %3866 = bitcast <2 x i64> %3779 to <8 x i16>
  %3867 = ashr <8 x i16> %3866, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %3868 = shufflevector <8 x i16> %3866, <8 x i16> %3867, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3869 = shufflevector <8 x i16> %3866, <8 x i16> %3867, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3870 = bitcast i32* %3865 to <8 x i16>*
  store <8 x i16> %3868, <8 x i16>* %3870, align 1
  %3871 = getelementptr inbounds i32, i32* %3718, i64 68
  %3872 = bitcast i32* %3871 to <8 x i16>*
  store <8 x i16> %3869, <8 x i16>* %3872, align 1
  %3873 = getelementptr inbounds i32, i32* %3718, i64 96
  %3874 = bitcast <2 x i64> %3780 to <8 x i16>
  %3875 = ashr <8 x i16> %3874, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %3876 = shufflevector <8 x i16> %3874, <8 x i16> %3875, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3877 = shufflevector <8 x i16> %3874, <8 x i16> %3875, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3878 = bitcast i32* %3873 to <8 x i16>*
  store <8 x i16> %3876, <8 x i16>* %3878, align 1
  %3879 = getelementptr inbounds i32, i32* %3718, i64 100
  %3880 = bitcast i32* %3879 to <8 x i16>*
  store <8 x i16> %3877, <8 x i16>* %3880, align 1
  %3881 = getelementptr inbounds i32, i32* %3718, i64 128
  %3882 = bitcast <2 x i64> %3781 to <8 x i16>
  %3883 = ashr <8 x i16> %3882, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %3884 = shufflevector <8 x i16> %3882, <8 x i16> %3883, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3885 = shufflevector <8 x i16> %3882, <8 x i16> %3883, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3886 = bitcast i32* %3881 to <8 x i16>*
  store <8 x i16> %3884, <8 x i16>* %3886, align 1
  %3887 = getelementptr inbounds i32, i32* %3718, i64 132
  %3888 = bitcast i32* %3887 to <8 x i16>*
  store <8 x i16> %3885, <8 x i16>* %3888, align 1
  %3889 = getelementptr inbounds i32, i32* %3718, i64 160
  %3890 = bitcast <2 x i64> %3782 to <8 x i16>
  %3891 = ashr <8 x i16> %3890, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %3892 = shufflevector <8 x i16> %3890, <8 x i16> %3891, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3893 = shufflevector <8 x i16> %3890, <8 x i16> %3891, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3894 = bitcast i32* %3889 to <8 x i16>*
  store <8 x i16> %3892, <8 x i16>* %3894, align 1
  %3895 = getelementptr inbounds i32, i32* %3718, i64 164
  %3896 = bitcast i32* %3895 to <8 x i16>*
  store <8 x i16> %3893, <8 x i16>* %3896, align 1
  %3897 = getelementptr inbounds i32, i32* %3718, i64 192
  %3898 = bitcast <2 x i64> %3783 to <8 x i16>
  %3899 = ashr <8 x i16> %3898, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %3900 = shufflevector <8 x i16> %3898, <8 x i16> %3899, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3901 = shufflevector <8 x i16> %3898, <8 x i16> %3899, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3902 = bitcast i32* %3897 to <8 x i16>*
  store <8 x i16> %3900, <8 x i16>* %3902, align 1
  %3903 = getelementptr inbounds i32, i32* %3718, i64 196
  %3904 = bitcast i32* %3903 to <8 x i16>*
  store <8 x i16> %3901, <8 x i16>* %3904, align 1
  %3905 = getelementptr inbounds i32, i32* %3718, i64 224
  %3906 = bitcast <2 x i64> %3784 to <8 x i16>
  %3907 = ashr <8 x i16> %3906, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %3908 = shufflevector <8 x i16> %3906, <8 x i16> %3907, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3909 = shufflevector <8 x i16> %3906, <8 x i16> %3907, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3910 = bitcast i32* %3905 to <8 x i16>*
  store <8 x i16> %3908, <8 x i16>* %3910, align 1
  %3911 = getelementptr inbounds i32, i32* %3718, i64 228
  %3912 = bitcast i32* %3911 to <8 x i16>*
  store <8 x i16> %3909, <8 x i16>* %3912, align 1
  %3913 = getelementptr inbounds i32, i32* %3718, i64 8
  br label %3914

3914:                                             ; preds = %3850, %3786
  %3915 = phi i16* [ %3849, %3786 ], [ %3719, %3850 ]
  %3916 = phi i32* [ %3718, %3786 ], [ %3913, %3850 ]
  %3917 = add nuw nsw i64 %3717, 1
  %3918 = icmp eq i64 %3917, 4
  br i1 %3918, label %3919, label %3716

3919:                                             ; preds = %3914
  call void @llvm.lifetime.end.p0i8(i64 512, i8* nonnull %9) #6
  %3920 = add nuw nsw i64 %97, 8
  %3921 = icmp ult i64 %3920, 32
  br i1 %3921, label %96, label %3922

3922:                                             ; preds = %3919
  %3923 = add nuw nsw i32 %94, 1
  %3924 = icmp eq i32 %3923, 2
  br i1 %3924, label %3925, label %93

3925:                                             ; preds = %3922
  call void @llvm.lifetime.end.p0i8(i64 2048, i8* nonnull %8) #6
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @vpx_highbd_fdct4x4_sse2(i16*, i32*, i32) local_unnamed_addr #2 {
  %4 = bitcast i16* %0 to i64*
  %5 = load i64, i64* %4, align 1
  %6 = insertelement <2 x i64> undef, i64 %5, i32 0
  %7 = sext i32 %2 to i64
  %8 = getelementptr inbounds i16, i16* %0, i64 %7
  %9 = bitcast i16* %8 to i64*
  %10 = load i64, i64* %9, align 1
  %11 = insertelement <2 x i64> undef, i64 %10, i32 0
  %12 = shl nsw i32 %2, 1
  %13 = sext i32 %12 to i64
  %14 = getelementptr inbounds i16, i16* %0, i64 %13
  %15 = bitcast i16* %14 to i64*
  %16 = load i64, i64* %15, align 1
  %17 = insertelement <2 x i64> %11, i64 %16, i32 1
  %18 = mul nsw i32 %2, 3
  %19 = sext i32 %18 to i64
  %20 = getelementptr inbounds i16, i16* %0, i64 %19
  %21 = bitcast i16* %20 to i64*
  %22 = load i64, i64* %21, align 1
  %23 = insertelement <2 x i64> %6, i64 %22, i32 1
  %24 = bitcast <2 x i64> %23 to <8 x i16>
  %25 = icmp sgt <8 x i16> %24, <i16 1023, i16 1023, i16 1023, i16 1023, i16 1023, i16 1023, i16 1023, i16 1023>
  %26 = icmp slt <8 x i16> %24, <i16 -1024, i16 -1024, i16 -1024, i16 -1024, i16 -1024, i16 -1024, i16 -1024, i16 -1024>
  %27 = xor <8 x i1> %26, %25
  %28 = bitcast <2 x i64> %17 to <8 x i16>
  %29 = icmp sgt <8 x i16> %28, <i16 1023, i16 1023, i16 1023, i16 1023, i16 1023, i16 1023, i16 1023, i16 1023>
  %30 = icmp slt <8 x i16> %28, <i16 -1024, i16 -1024, i16 -1024, i16 -1024, i16 -1024, i16 -1024, i16 -1024, i16 -1024>
  %31 = xor <8 x i1> %30, %29
  %32 = or <8 x i1> %27, %31
  %33 = sext <8 x i1> %32 to <8 x i16>
  %34 = bitcast <8 x i16> %33 to <16 x i8>
  %35 = icmp slt <16 x i8> %34, zeroinitializer
  %36 = bitcast <16 x i1> %35 to i16
  %37 = icmp eq i16 %36, 0
  br i1 %37, label %39, label %38

38:                                               ; preds = %3
  tail call void @vpx_highbd_fdct4x4_c(i16* %0, i32* %1, i32 %2) #6
  br label %149

39:                                               ; preds = %3
  %40 = shl <8 x i16> %24, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %41 = shl <8 x i16> %28, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %42 = icmp eq <8 x i16> %40, <i16 0, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %43 = zext <8 x i1> %42 to <8 x i16>
  %44 = sub <8 x i16> %40, %43
  %45 = add <8 x i16> %44, <i16 1, i16 0, i16 0, i16 0, i16 0, i16 0, i16 0, i16 0>
  %46 = shufflevector <8 x i16> %45, <8 x i16> %41, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %47 = shufflevector <8 x i16> %45, <8 x i16> %41, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %48 = bitcast <8 x i16> %46 to <4 x i32>
  %49 = shufflevector <4 x i32> %48, <4 x i32> undef, <4 x i32> <i32 0, i32 1, i32 3, i32 2>
  %50 = bitcast <8 x i16> %47 to <4 x i32>
  %51 = shufflevector <4 x i32> %50, <4 x i32> undef, <4 x i32> <i32 0, i32 1, i32 3, i32 2>
  %52 = bitcast <4 x i32> %49 to <8 x i16>
  %53 = bitcast <4 x i32> %51 to <8 x i16>
  %54 = add <8 x i16> %53, %52
  %55 = sub <8 x i16> %52, %53
  %56 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %54, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %57 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %54, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %58 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %55, <8 x i16> <i16 15137, i16 6270, i16 15137, i16 6270, i16 6270, i16 -15137, i16 6270, i16 -15137>) #6
  %59 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %55, <8 x i16> <i16 6270, i16 -15137, i16 6270, i16 -15137, i16 15137, i16 6270, i16 15137, i16 6270>) #6
  %60 = add <4 x i32> %56, <i32 8192, i32 8192, i32 8192, i32 8192>
  %61 = add <4 x i32> %58, <i32 8192, i32 8192, i32 8192, i32 8192>
  %62 = add <4 x i32> %57, <i32 8192, i32 8192, i32 8192, i32 8192>
  %63 = add <4 x i32> %59, <i32 8192, i32 8192, i32 8192, i32 8192>
  %64 = ashr <4 x i32> %60, <i32 14, i32 14, i32 14, i32 14>
  %65 = ashr <4 x i32> %61, <i32 14, i32 14, i32 14, i32 14>
  %66 = ashr <4 x i32> %62, <i32 14, i32 14, i32 14, i32 14>
  %67 = ashr <4 x i32> %63, <i32 14, i32 14, i32 14, i32 14>
  %68 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %64, <4 x i32> %65) #6
  %69 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %66, <4 x i32> %67) #6
  %70 = add <8 x i16> %68, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %71 = icmp ult <8 x i16> %70, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %72 = add <8 x i16> %69, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %73 = icmp ult <8 x i16> %72, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %74 = or <8 x i1> %73, %71
  %75 = sext <8 x i1> %74 to <8 x i16>
  %76 = bitcast <8 x i16> %75 to <16 x i8>
  %77 = icmp slt <16 x i8> %76, zeroinitializer
  %78 = bitcast <16 x i1> %77 to i16
  %79 = icmp eq i16 %78, 0
  br i1 %79, label %81, label %80

80:                                               ; preds = %39
  tail call void @vpx_highbd_fdct4x4_c(i16* %0, i32* %1, i32 %2) #6
  br label %149

81:                                               ; preds = %39
  %82 = bitcast <8 x i16> %68 to <4 x i32>
  %83 = shufflevector <4 x i32> %82, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %84 = bitcast <8 x i16> %69 to <4 x i32>
  %85 = shufflevector <4 x i32> %84, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 0, i32 2>
  %86 = bitcast <4 x i32> %83 to <8 x i16>
  %87 = bitcast <4 x i32> %85 to <8 x i16>
  %88 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %86, <8 x i16> %87) #6
  %89 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %86, <8 x i16> %87) #6
  %90 = add <8 x i16> %88, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %91 = icmp ult <8 x i16> %90, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %92 = add <8 x i16> %89, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %93 = icmp ult <8 x i16> %92, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %94 = or <8 x i1> %93, %91
  %95 = sext <8 x i1> %94 to <8 x i16>
  %96 = bitcast <8 x i16> %95 to <16 x i8>
  %97 = icmp slt <16 x i8> %96, zeroinitializer
  %98 = bitcast <16 x i1> %97 to i16
  %99 = icmp eq i16 %98, 0
  br i1 %99, label %101, label %100

100:                                              ; preds = %81
  tail call void @vpx_highbd_fdct4x4_c(i16* %0, i32* %1, i32 %2) #6
  br label %149

101:                                              ; preds = %81
  %102 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %88, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %103 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %88, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %104 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %89, <8 x i16> <i16 15137, i16 6270, i16 15137, i16 6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270>) #6
  %105 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %89, <8 x i16> <i16 6270, i16 -15137, i16 6270, i16 -15137, i16 -6270, i16 15137, i16 -6270, i16 15137>) #6
  %106 = add <4 x i32> %102, <i32 24576, i32 24576, i32 24576, i32 24576>
  %107 = add <4 x i32> %103, <i32 24576, i32 24576, i32 24576, i32 24576>
  %108 = add <4 x i32> %104, <i32 24576, i32 24576, i32 24576, i32 24576>
  %109 = add <4 x i32> %105, <i32 24576, i32 24576, i32 24576, i32 24576>
  %110 = ashr <4 x i32> %106, <i32 16, i32 16, i32 16, i32 16>
  %111 = ashr <4 x i32> %107, <i32 16, i32 16, i32 16, i32 16>
  %112 = ashr <4 x i32> %108, <i32 16, i32 16, i32 16, i32 16>
  %113 = ashr <4 x i32> %109, <i32 16, i32 16, i32 16, i32 16>
  %114 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %110, <4 x i32> %111) #6
  %115 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %112, <4 x i32> %113) #6
  %116 = add <8 x i16> %114, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %117 = icmp ult <8 x i16> %116, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %118 = add <8 x i16> %115, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %119 = icmp ult <8 x i16> %118, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %120 = or <8 x i1> %119, %117
  %121 = sext <8 x i1> %120 to <8 x i16>
  %122 = bitcast <8 x i16> %121 to <16 x i8>
  %123 = icmp slt <16 x i8> %122, zeroinitializer
  %124 = bitcast <16 x i1> %123 to i16
  %125 = icmp eq i16 %124, 0
  br i1 %125, label %127, label %126

126:                                              ; preds = %101
  tail call void @vpx_highbd_fdct4x4_c(i16* %0, i32* %1, i32 %2) #6
  br label %149

127:                                              ; preds = %101
  %128 = shufflevector <8 x i16> %114, <8 x i16> %115, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %129 = shufflevector <8 x i16> %114, <8 x i16> %115, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %130 = bitcast <8 x i16> %128 to <4 x i32>
  %131 = bitcast <8 x i16> %129 to <4 x i32>
  %132 = shufflevector <4 x i32> %130, <4 x i32> %131, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %133 = shufflevector <4 x i32> %130, <4 x i32> %131, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %134 = bitcast <4 x i32> %132 to <8 x i16>
  %135 = ashr <8 x i16> %134, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %136 = shufflevector <8 x i16> %134, <8 x i16> %135, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %137 = shufflevector <8 x i16> %134, <8 x i16> %135, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %138 = bitcast i32* %1 to <8 x i16>*
  store <8 x i16> %136, <8 x i16>* %138, align 1
  %139 = getelementptr inbounds i32, i32* %1, i64 4
  %140 = bitcast i32* %139 to <8 x i16>*
  store <8 x i16> %137, <8 x i16>* %140, align 1
  %141 = getelementptr inbounds i32, i32* %1, i64 8
  %142 = bitcast <4 x i32> %133 to <8 x i16>
  %143 = ashr <8 x i16> %142, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %144 = shufflevector <8 x i16> %142, <8 x i16> %143, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %145 = shufflevector <8 x i16> %142, <8 x i16> %143, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %146 = bitcast i32* %141 to <8 x i16>*
  store <8 x i16> %144, <8 x i16>* %146, align 1
  %147 = getelementptr inbounds i32, i32* %1, i64 12
  %148 = bitcast i32* %147 to <8 x i16>*
  store <8 x i16> %145, <8 x i16>* %148, align 1
  br label %149

149:                                              ; preds = %126, %100, %80, %127, %38
  ret void
}

declare void @vpx_highbd_fdct4x4_c(i16*, i32*, i32) local_unnamed_addr #3

; Function Attrs: nounwind ssp uwtable
define hidden void @vpx_highbd_fdct8x8_sse2(i16*, i32*, i32) local_unnamed_addr #2 {
  %4 = bitcast i16* %0 to <8 x i16>*
  %5 = load <8 x i16>, <8 x i16>* %4, align 16
  %6 = sext i32 %2 to i64
  %7 = getelementptr inbounds i16, i16* %0, i64 %6
  %8 = bitcast i16* %7 to <8 x i16>*
  %9 = load <8 x i16>, <8 x i16>* %8, align 16
  %10 = shl nsw i32 %2, 1
  %11 = sext i32 %10 to i64
  %12 = getelementptr inbounds i16, i16* %0, i64 %11
  %13 = bitcast i16* %12 to <8 x i16>*
  %14 = load <8 x i16>, <8 x i16>* %13, align 16
  %15 = mul nsw i32 %2, 3
  %16 = sext i32 %15 to i64
  %17 = getelementptr inbounds i16, i16* %0, i64 %16
  %18 = bitcast i16* %17 to <8 x i16>*
  %19 = load <8 x i16>, <8 x i16>* %18, align 16
  %20 = shl nsw i32 %2, 2
  %21 = sext i32 %20 to i64
  %22 = getelementptr inbounds i16, i16* %0, i64 %21
  %23 = bitcast i16* %22 to <8 x i16>*
  %24 = load <8 x i16>, <8 x i16>* %23, align 16
  %25 = mul nsw i32 %2, 5
  %26 = sext i32 %25 to i64
  %27 = getelementptr inbounds i16, i16* %0, i64 %26
  %28 = bitcast i16* %27 to <8 x i16>*
  %29 = load <8 x i16>, <8 x i16>* %28, align 16
  %30 = mul nsw i32 %2, 6
  %31 = sext i32 %30 to i64
  %32 = getelementptr inbounds i16, i16* %0, i64 %31
  %33 = bitcast i16* %32 to <8 x i16>*
  %34 = load <8 x i16>, <8 x i16>* %33, align 16
  %35 = mul nsw i32 %2, 7
  %36 = sext i32 %35 to i64
  %37 = getelementptr inbounds i16, i16* %0, i64 %36
  %38 = bitcast i16* %37 to <8 x i16>*
  %39 = load <8 x i16>, <8 x i16>* %38, align 16
  %40 = shl <8 x i16> %5, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %41 = shl <8 x i16> %9, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %42 = bitcast <8 x i16> %41 to <2 x i64>
  %43 = shl <8 x i16> %14, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %44 = bitcast <8 x i16> %43 to <2 x i64>
  %45 = shl <8 x i16> %19, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %46 = bitcast <8 x i16> %45 to <2 x i64>
  %47 = shl <8 x i16> %24, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %48 = bitcast <8 x i16> %47 to <2 x i64>
  %49 = shl <8 x i16> %29, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %50 = bitcast <8 x i16> %49 to <2 x i64>
  %51 = shl <8 x i16> %34, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %52 = bitcast <8 x i16> %51 to <2 x i64>
  %53 = shl <8 x i16> %39, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %54 = bitcast <8 x i16> %53 to <2 x i64>
  br label %55

55:                                               ; preds = %289, %3
  %56 = phi <8 x i16> [ %40, %3 ], [ %331, %289 ]
  %57 = phi i32 [ 0, %3 ], [ %330, %289 ]
  %58 = phi <2 x i64> [ %54, %3 ], [ %329, %289 ]
  %59 = phi <2 x i64> [ %52, %3 ], [ %328, %289 ]
  %60 = phi <2 x i64> [ %50, %3 ], [ %327, %289 ]
  %61 = phi <2 x i64> [ %48, %3 ], [ %326, %289 ]
  %62 = phi <2 x i64> [ %46, %3 ], [ %325, %289 ]
  %63 = phi <2 x i64> [ %44, %3 ], [ %324, %289 ]
  %64 = phi <2 x i64> [ %42, %3 ], [ %323, %289 ]
  %65 = bitcast <2 x i64> %58 to <8 x i16>
  %66 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %56, <8 x i16> %65) #6
  %67 = bitcast <2 x i64> %64 to <8 x i16>
  %68 = bitcast <2 x i64> %59 to <8 x i16>
  %69 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %67, <8 x i16> %68) #6
  %70 = bitcast <2 x i64> %63 to <8 x i16>
  %71 = bitcast <2 x i64> %60 to <8 x i16>
  %72 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %70, <8 x i16> %71) #6
  %73 = bitcast <2 x i64> %62 to <8 x i16>
  %74 = bitcast <2 x i64> %61 to <8 x i16>
  %75 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %73, <8 x i16> %74) #6
  %76 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %73, <8 x i16> %74) #6
  %77 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %70, <8 x i16> %71) #6
  %78 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %67, <8 x i16> %68) #6
  %79 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %56, <8 x i16> %65) #6
  %80 = icmp eq i32 %57, 1
  br i1 %80, label %81, label %117

81:                                               ; preds = %55
  %82 = add <8 x i16> %66, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %83 = icmp ult <8 x i16> %82, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %84 = add <8 x i16> %69, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %85 = icmp ult <8 x i16> %84, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %86 = add <8 x i16> %72, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %87 = icmp ult <8 x i16> %86, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %88 = add <8 x i16> %75, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %89 = icmp ult <8 x i16> %88, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %90 = or <8 x i1> %87, %89
  %91 = or <8 x i1> %90, %85
  %92 = or <8 x i1> %91, %83
  %93 = sext <8 x i1> %92 to <8 x i16>
  %94 = bitcast <8 x i16> %93 to <16 x i8>
  %95 = icmp slt <16 x i8> %94, zeroinitializer
  %96 = bitcast <16 x i1> %95 to i16
  %97 = zext i16 %96 to i32
  %98 = add <8 x i16> %76, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %99 = icmp ult <8 x i16> %98, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %100 = add <8 x i16> %77, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %101 = icmp ult <8 x i16> %100, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %102 = add <8 x i16> %78, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %103 = icmp ult <8 x i16> %102, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %104 = add <8 x i16> %79, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %105 = icmp ult <8 x i16> %104, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %106 = or <8 x i1> %101, %99
  %107 = or <8 x i1> %106, %103
  %108 = or <8 x i1> %107, %105
  %109 = sext <8 x i1> %108 to <8 x i16>
  %110 = bitcast <8 x i16> %109 to <16 x i8>
  %111 = icmp slt <16 x i8> %110, zeroinitializer
  %112 = bitcast <16 x i1> %111 to i16
  %113 = zext i16 %112 to i32
  %114 = sub nsw i32 0, %97
  %115 = icmp eq i32 %113, %114
  br i1 %115, label %117, label %116

116:                                              ; preds = %81
  tail call void @vpx_highbd_fdct8x8_c(i16* %0, i32* %1, i32 %2) #6
  br label %420

117:                                              ; preds = %81, %55
  %118 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %66, <8 x i16> %75) #6
  %119 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %69, <8 x i16> %72) #6
  %120 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %69, <8 x i16> %72) #6
  %121 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %66, <8 x i16> %75) #6
  %122 = add <8 x i16> %118, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %123 = icmp ult <8 x i16> %122, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %124 = add <8 x i16> %119, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %125 = icmp ult <8 x i16> %124, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %126 = add <8 x i16> %120, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %127 = icmp ult <8 x i16> %126, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %128 = add <8 x i16> %121, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %129 = icmp ult <8 x i16> %128, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %130 = or <8 x i1> %127, %125
  %131 = or <8 x i1> %130, %123
  %132 = or <8 x i1> %131, %129
  %133 = sext <8 x i1> %132 to <8 x i16>
  %134 = bitcast <8 x i16> %133 to <16 x i8>
  %135 = icmp slt <16 x i8> %134, zeroinitializer
  %136 = bitcast <16 x i1> %135 to i16
  %137 = icmp eq i16 %136, 0
  br i1 %137, label %139, label %138

138:                                              ; preds = %117
  tail call void @vpx_highbd_fdct8x8_c(i16* %0, i32* %1, i32 %2) #6
  br label %420

139:                                              ; preds = %117
  %140 = shufflevector <8 x i16> %118, <8 x i16> %119, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %141 = shufflevector <8 x i16> %118, <8 x i16> %119, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %142 = shufflevector <8 x i16> %120, <8 x i16> %121, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %143 = shufflevector <8 x i16> %120, <8 x i16> %121, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %144 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %140, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %145 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %141, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %146 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %140, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %147 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %141, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %148 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %142, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %149 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %143, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %150 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %142, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %151 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %143, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %152 = add <4 x i32> %144, <i32 8192, i32 8192, i32 8192, i32 8192>
  %153 = add <4 x i32> %145, <i32 8192, i32 8192, i32 8192, i32 8192>
  %154 = add <4 x i32> %146, <i32 8192, i32 8192, i32 8192, i32 8192>
  %155 = add <4 x i32> %147, <i32 8192, i32 8192, i32 8192, i32 8192>
  %156 = add <4 x i32> %148, <i32 8192, i32 8192, i32 8192, i32 8192>
  %157 = add <4 x i32> %149, <i32 8192, i32 8192, i32 8192, i32 8192>
  %158 = add <4 x i32> %150, <i32 8192, i32 8192, i32 8192, i32 8192>
  %159 = add <4 x i32> %151, <i32 8192, i32 8192, i32 8192, i32 8192>
  %160 = ashr <4 x i32> %152, <i32 14, i32 14, i32 14, i32 14>
  %161 = ashr <4 x i32> %153, <i32 14, i32 14, i32 14, i32 14>
  %162 = ashr <4 x i32> %154, <i32 14, i32 14, i32 14, i32 14>
  %163 = ashr <4 x i32> %155, <i32 14, i32 14, i32 14, i32 14>
  %164 = ashr <4 x i32> %156, <i32 14, i32 14, i32 14, i32 14>
  %165 = ashr <4 x i32> %157, <i32 14, i32 14, i32 14, i32 14>
  %166 = ashr <4 x i32> %158, <i32 14, i32 14, i32 14, i32 14>
  %167 = ashr <4 x i32> %159, <i32 14, i32 14, i32 14, i32 14>
  %168 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %160, <4 x i32> %161) #6
  %169 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %162, <4 x i32> %163) #6
  %170 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %164, <4 x i32> %165) #6
  %171 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %166, <4 x i32> %167) #6
  %172 = add <8 x i16> %168, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %173 = icmp ult <8 x i16> %172, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %174 = add <8 x i16> %169, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %175 = icmp ult <8 x i16> %174, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %176 = add <8 x i16> %170, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %177 = icmp ult <8 x i16> %176, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %178 = add <8 x i16> %171, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %179 = icmp ult <8 x i16> %178, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %180 = or <8 x i1> %175, %173
  %181 = or <8 x i1> %180, %177
  %182 = or <8 x i1> %181, %179
  %183 = sext <8 x i1> %182 to <8 x i16>
  %184 = bitcast <8 x i16> %183 to <16 x i8>
  %185 = icmp slt <16 x i8> %184, zeroinitializer
  %186 = bitcast <16 x i1> %185 to i16
  %187 = icmp eq i16 %186, 0
  br i1 %187, label %189, label %188

188:                                              ; preds = %139
  tail call void @vpx_highbd_fdct8x8_c(i16* %0, i32* %1, i32 %2) #6
  br label %420

189:                                              ; preds = %139
  %190 = shufflevector <8 x i16> %78, <8 x i16> %77, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %191 = shufflevector <8 x i16> %78, <8 x i16> %77, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %192 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %190, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %193 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %191, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %194 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %190, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %195 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %191, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %196 = add <4 x i32> %192, <i32 8192, i32 8192, i32 8192, i32 8192>
  %197 = add <4 x i32> %193, <i32 8192, i32 8192, i32 8192, i32 8192>
  %198 = add <4 x i32> %194, <i32 8192, i32 8192, i32 8192, i32 8192>
  %199 = add <4 x i32> %195, <i32 8192, i32 8192, i32 8192, i32 8192>
  %200 = ashr <4 x i32> %196, <i32 14, i32 14, i32 14, i32 14>
  %201 = ashr <4 x i32> %197, <i32 14, i32 14, i32 14, i32 14>
  %202 = ashr <4 x i32> %198, <i32 14, i32 14, i32 14, i32 14>
  %203 = ashr <4 x i32> %199, <i32 14, i32 14, i32 14, i32 14>
  %204 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %200, <4 x i32> %201) #6
  %205 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %202, <4 x i32> %203) #6
  %206 = add <8 x i16> %204, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %207 = icmp ult <8 x i16> %206, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %208 = add <8 x i16> %205, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %209 = icmp ult <8 x i16> %208, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %210 = or <8 x i1> %209, %207
  %211 = sext <8 x i1> %210 to <8 x i16>
  %212 = bitcast <8 x i16> %211 to <16 x i8>
  %213 = icmp slt <16 x i8> %212, zeroinitializer
  %214 = bitcast <16 x i1> %213 to i16
  %215 = icmp eq i16 %214, 0
  br i1 %215, label %217, label %216

216:                                              ; preds = %189
  tail call void @vpx_highbd_fdct8x8_c(i16* %0, i32* %1, i32 %2) #6
  br label %420

217:                                              ; preds = %189
  %218 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %76, <8 x i16> %204) #6
  %219 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %76, <8 x i16> %204) #6
  %220 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %79, <8 x i16> %205) #6
  %221 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %79, <8 x i16> %205) #6
  %222 = add <8 x i16> %218, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %223 = icmp ult <8 x i16> %222, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %224 = add <8 x i16> %219, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %225 = icmp ult <8 x i16> %224, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %226 = add <8 x i16> %220, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %227 = icmp ult <8 x i16> %226, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %228 = add <8 x i16> %221, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %229 = icmp ult <8 x i16> %228, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %230 = or <8 x i1> %225, %223
  %231 = or <8 x i1> %230, %227
  %232 = or <8 x i1> %231, %229
  %233 = sext <8 x i1> %232 to <8 x i16>
  %234 = bitcast <8 x i16> %233 to <16 x i8>
  %235 = icmp slt <16 x i8> %234, zeroinitializer
  %236 = bitcast <16 x i1> %235 to i16
  %237 = icmp eq i16 %236, 0
  br i1 %237, label %239, label %238

238:                                              ; preds = %217
  tail call void @vpx_highbd_fdct8x8_c(i16* %0, i32* %1, i32 %2) #6
  br label %420

239:                                              ; preds = %217
  %240 = shufflevector <8 x i16> %218, <8 x i16> %221, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %241 = shufflevector <8 x i16> %218, <8 x i16> %221, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %242 = shufflevector <8 x i16> %219, <8 x i16> %220, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %243 = shufflevector <8 x i16> %219, <8 x i16> %220, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %244 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %240, <8 x i16> <i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069>) #6
  %245 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %241, <8 x i16> <i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069>) #6
  %246 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %240, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %247 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %241, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %248 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %242, <8 x i16> <i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102>) #6
  %249 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %243, <8 x i16> <i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102>) #6
  %250 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %242, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %251 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %243, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %252 = add <4 x i32> %244, <i32 8192, i32 8192, i32 8192, i32 8192>
  %253 = add <4 x i32> %245, <i32 8192, i32 8192, i32 8192, i32 8192>
  %254 = add <4 x i32> %246, <i32 8192, i32 8192, i32 8192, i32 8192>
  %255 = add <4 x i32> %247, <i32 8192, i32 8192, i32 8192, i32 8192>
  %256 = add <4 x i32> %248, <i32 8192, i32 8192, i32 8192, i32 8192>
  %257 = add <4 x i32> %249, <i32 8192, i32 8192, i32 8192, i32 8192>
  %258 = add <4 x i32> %250, <i32 8192, i32 8192, i32 8192, i32 8192>
  %259 = add <4 x i32> %251, <i32 8192, i32 8192, i32 8192, i32 8192>
  %260 = ashr <4 x i32> %252, <i32 14, i32 14, i32 14, i32 14>
  %261 = ashr <4 x i32> %253, <i32 14, i32 14, i32 14, i32 14>
  %262 = ashr <4 x i32> %254, <i32 14, i32 14, i32 14, i32 14>
  %263 = ashr <4 x i32> %255, <i32 14, i32 14, i32 14, i32 14>
  %264 = ashr <4 x i32> %256, <i32 14, i32 14, i32 14, i32 14>
  %265 = ashr <4 x i32> %257, <i32 14, i32 14, i32 14, i32 14>
  %266 = ashr <4 x i32> %258, <i32 14, i32 14, i32 14, i32 14>
  %267 = ashr <4 x i32> %259, <i32 14, i32 14, i32 14, i32 14>
  %268 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %260, <4 x i32> %261) #6
  %269 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %262, <4 x i32> %263) #6
  %270 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %264, <4 x i32> %265) #6
  %271 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %266, <4 x i32> %267) #6
  %272 = add <8 x i16> %268, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %273 = icmp ult <8 x i16> %272, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %274 = add <8 x i16> %269, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %275 = icmp ult <8 x i16> %274, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %276 = add <8 x i16> %270, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %277 = icmp ult <8 x i16> %276, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %278 = add <8 x i16> %271, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %279 = icmp ult <8 x i16> %278, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %280 = or <8 x i1> %275, %273
  %281 = or <8 x i1> %280, %277
  %282 = or <8 x i1> %281, %279
  %283 = sext <8 x i1> %282 to <8 x i16>
  %284 = bitcast <8 x i16> %283 to <16 x i8>
  %285 = icmp slt <16 x i8> %284, zeroinitializer
  %286 = bitcast <16 x i1> %285 to i16
  %287 = icmp eq i16 %286, 0
  br i1 %287, label %289, label %288

288:                                              ; preds = %239
  tail call void @vpx_highbd_fdct8x8_c(i16* %0, i32* %1, i32 %2) #6
  br label %420

289:                                              ; preds = %239
  %290 = shufflevector <8 x i16> %168, <8 x i16> %268, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %291 = shufflevector <8 x i16> %170, <8 x i16> %271, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %292 = shufflevector <8 x i16> %168, <8 x i16> %268, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %293 = shufflevector <8 x i16> %170, <8 x i16> %271, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %294 = shufflevector <8 x i16> %169, <8 x i16> %270, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %295 = shufflevector <8 x i16> %171, <8 x i16> %269, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %296 = shufflevector <8 x i16> %169, <8 x i16> %270, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %297 = shufflevector <8 x i16> %171, <8 x i16> %269, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %298 = bitcast <8 x i16> %290 to <4 x i32>
  %299 = bitcast <8 x i16> %291 to <4 x i32>
  %300 = shufflevector <4 x i32> %298, <4 x i32> %299, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %301 = bitcast <4 x i32> %300 to <2 x i64>
  %302 = bitcast <8 x i16> %292 to <4 x i32>
  %303 = bitcast <8 x i16> %293 to <4 x i32>
  %304 = shufflevector <4 x i32> %302, <4 x i32> %303, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %305 = bitcast <4 x i32> %304 to <2 x i64>
  %306 = shufflevector <4 x i32> %298, <4 x i32> %299, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %307 = bitcast <4 x i32> %306 to <2 x i64>
  %308 = shufflevector <4 x i32> %302, <4 x i32> %303, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %309 = bitcast <4 x i32> %308 to <2 x i64>
  %310 = bitcast <8 x i16> %294 to <4 x i32>
  %311 = bitcast <8 x i16> %295 to <4 x i32>
  %312 = shufflevector <4 x i32> %310, <4 x i32> %311, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %313 = bitcast <4 x i32> %312 to <2 x i64>
  %314 = bitcast <8 x i16> %296 to <4 x i32>
  %315 = bitcast <8 x i16> %297 to <4 x i32>
  %316 = shufflevector <4 x i32> %314, <4 x i32> %315, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %317 = bitcast <4 x i32> %316 to <2 x i64>
  %318 = shufflevector <4 x i32> %310, <4 x i32> %311, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %319 = bitcast <4 x i32> %318 to <2 x i64>
  %320 = shufflevector <4 x i32> %314, <4 x i32> %315, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %321 = bitcast <4 x i32> %320 to <2 x i64>
  %322 = shufflevector <2 x i64> %301, <2 x i64> %313, <2 x i32> <i32 0, i32 2>
  %323 = shufflevector <2 x i64> %301, <2 x i64> %313, <2 x i32> <i32 1, i32 3>
  %324 = shufflevector <2 x i64> %307, <2 x i64> %319, <2 x i32> <i32 0, i32 2>
  %325 = shufflevector <2 x i64> %307, <2 x i64> %319, <2 x i32> <i32 1, i32 3>
  %326 = shufflevector <2 x i64> %305, <2 x i64> %317, <2 x i32> <i32 0, i32 2>
  %327 = shufflevector <2 x i64> %305, <2 x i64> %317, <2 x i32> <i32 1, i32 3>
  %328 = shufflevector <2 x i64> %309, <2 x i64> %321, <2 x i32> <i32 0, i32 2>
  %329 = shufflevector <2 x i64> %309, <2 x i64> %321, <2 x i32> <i32 1, i32 3>
  %330 = add nuw nsw i32 %57, 1
  %331 = bitcast <2 x i64> %322 to <8 x i16>
  %332 = icmp eq i32 %330, 2
  br i1 %332, label %333, label %55

333:                                              ; preds = %289
  %334 = ashr <8 x i16> %331, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %335 = bitcast <2 x i64> %323 to <8 x i16>
  %336 = ashr <8 x i16> %335, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %337 = bitcast <2 x i64> %324 to <8 x i16>
  %338 = ashr <8 x i16> %337, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %339 = bitcast <2 x i64> %325 to <8 x i16>
  %340 = ashr <8 x i16> %339, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %341 = bitcast <2 x i64> %326 to <8 x i16>
  %342 = ashr <8 x i16> %341, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %343 = bitcast <2 x i64> %327 to <8 x i16>
  %344 = ashr <8 x i16> %343, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %345 = bitcast <2 x i64> %328 to <8 x i16>
  %346 = ashr <8 x i16> %345, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %347 = bitcast <2 x i64> %329 to <8 x i16>
  %348 = ashr <8 x i16> %347, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %349 = sub <8 x i16> %331, %334
  %350 = sub <8 x i16> %335, %336
  %351 = sub <8 x i16> %337, %338
  %352 = sub <8 x i16> %339, %340
  %353 = sub <8 x i16> %341, %342
  %354 = sub <8 x i16> %343, %344
  %355 = sub <8 x i16> %345, %346
  %356 = sub <8 x i16> %347, %348
  %357 = ashr <8 x i16> %349, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %358 = ashr <8 x i16> %350, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %359 = ashr <8 x i16> %351, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %360 = ashr <8 x i16> %352, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %361 = ashr <8 x i16> %353, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %362 = ashr <8 x i16> %354, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %363 = ashr <8 x i16> %355, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %364 = ashr <8 x i16> %356, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %365 = ashr <8 x i16> %349, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %366 = shufflevector <8 x i16> %357, <8 x i16> %365, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %367 = shufflevector <8 x i16> %357, <8 x i16> %365, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %368 = bitcast i32* %1 to <8 x i16>*
  store <8 x i16> %366, <8 x i16>* %368, align 16
  %369 = getelementptr inbounds i32, i32* %1, i64 4
  %370 = bitcast i32* %369 to <8 x i16>*
  store <8 x i16> %367, <8 x i16>* %370, align 16
  %371 = getelementptr inbounds i32, i32* %1, i64 8
  %372 = ashr <8 x i16> %350, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %373 = shufflevector <8 x i16> %358, <8 x i16> %372, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %374 = shufflevector <8 x i16> %358, <8 x i16> %372, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %375 = bitcast i32* %371 to <8 x i16>*
  store <8 x i16> %373, <8 x i16>* %375, align 16
  %376 = getelementptr inbounds i32, i32* %1, i64 12
  %377 = bitcast i32* %376 to <8 x i16>*
  store <8 x i16> %374, <8 x i16>* %377, align 16
  %378 = getelementptr inbounds i32, i32* %1, i64 16
  %379 = ashr <8 x i16> %351, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %380 = shufflevector <8 x i16> %359, <8 x i16> %379, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %381 = shufflevector <8 x i16> %359, <8 x i16> %379, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %382 = bitcast i32* %378 to <8 x i16>*
  store <8 x i16> %380, <8 x i16>* %382, align 16
  %383 = getelementptr inbounds i32, i32* %1, i64 20
  %384 = bitcast i32* %383 to <8 x i16>*
  store <8 x i16> %381, <8 x i16>* %384, align 16
  %385 = getelementptr inbounds i32, i32* %1, i64 24
  %386 = ashr <8 x i16> %352, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %387 = shufflevector <8 x i16> %360, <8 x i16> %386, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %388 = shufflevector <8 x i16> %360, <8 x i16> %386, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %389 = bitcast i32* %385 to <8 x i16>*
  store <8 x i16> %387, <8 x i16>* %389, align 16
  %390 = getelementptr inbounds i32, i32* %1, i64 28
  %391 = bitcast i32* %390 to <8 x i16>*
  store <8 x i16> %388, <8 x i16>* %391, align 16
  %392 = getelementptr inbounds i32, i32* %1, i64 32
  %393 = ashr <8 x i16> %353, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %394 = shufflevector <8 x i16> %361, <8 x i16> %393, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %395 = shufflevector <8 x i16> %361, <8 x i16> %393, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %396 = bitcast i32* %392 to <8 x i16>*
  store <8 x i16> %394, <8 x i16>* %396, align 16
  %397 = getelementptr inbounds i32, i32* %1, i64 36
  %398 = bitcast i32* %397 to <8 x i16>*
  store <8 x i16> %395, <8 x i16>* %398, align 16
  %399 = getelementptr inbounds i32, i32* %1, i64 40
  %400 = ashr <8 x i16> %354, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %401 = shufflevector <8 x i16> %362, <8 x i16> %400, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %402 = shufflevector <8 x i16> %362, <8 x i16> %400, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %403 = bitcast i32* %399 to <8 x i16>*
  store <8 x i16> %401, <8 x i16>* %403, align 16
  %404 = getelementptr inbounds i32, i32* %1, i64 44
  %405 = bitcast i32* %404 to <8 x i16>*
  store <8 x i16> %402, <8 x i16>* %405, align 16
  %406 = getelementptr inbounds i32, i32* %1, i64 48
  %407 = ashr <8 x i16> %355, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %408 = shufflevector <8 x i16> %363, <8 x i16> %407, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %409 = shufflevector <8 x i16> %363, <8 x i16> %407, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %410 = bitcast i32* %406 to <8 x i16>*
  store <8 x i16> %408, <8 x i16>* %410, align 16
  %411 = getelementptr inbounds i32, i32* %1, i64 52
  %412 = bitcast i32* %411 to <8 x i16>*
  store <8 x i16> %409, <8 x i16>* %412, align 16
  %413 = getelementptr inbounds i32, i32* %1, i64 56
  %414 = ashr <8 x i16> %356, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %415 = shufflevector <8 x i16> %364, <8 x i16> %414, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %416 = shufflevector <8 x i16> %364, <8 x i16> %414, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %417 = bitcast i32* %413 to <8 x i16>*
  store <8 x i16> %415, <8 x i16>* %417, align 16
  %418 = getelementptr inbounds i32, i32* %1, i64 60
  %419 = bitcast i32* %418 to <8 x i16>*
  store <8 x i16> %416, <8 x i16>* %419, align 16
  br label %420

420:                                              ; preds = %238, %216, %138, %188, %288, %116, %333
  ret void
}

declare void @vpx_highbd_fdct8x8_c(i16*, i32*, i32) local_unnamed_addr #3

; Function Attrs: nounwind ssp uwtable
define hidden void @vpx_highbd_fdct16x16_sse2(i16*, i32*, i32) local_unnamed_addr #2 {
  %4 = alloca [256 x i16], align 16
  %5 = bitcast [256 x i16]* %4 to i8*
  call void @llvm.lifetime.start.p0i8(i64 512, i8* nonnull %5) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %5, i8 -86, i64 512, i1 false)
  %6 = getelementptr inbounds [256 x i16], [256 x i16]* %4, i64 0, i64 0
  %7 = sext i32 %2 to i64
  %8 = shl nsw i32 %2, 1
  %9 = sext i32 %8 to i64
  %10 = mul nsw i32 %2, 3
  %11 = sext i32 %10 to i64
  %12 = shl nsw i32 %2, 2
  %13 = sext i32 %12 to i64
  %14 = mul nsw i32 %2, 5
  %15 = sext i32 %14 to i64
  %16 = mul nsw i32 %2, 6
  %17 = sext i32 %16 to i64
  %18 = mul nsw i32 %2, 7
  %19 = sext i32 %18 to i64
  %20 = shl nsw i32 %2, 3
  %21 = sext i32 %20 to i64
  %22 = mul nsw i32 %2, 9
  %23 = sext i32 %22 to i64
  %24 = mul nsw i32 %2, 10
  %25 = sext i32 %24 to i64
  %26 = mul nsw i32 %2, 11
  %27 = sext i32 %26 to i64
  %28 = mul nsw i32 %2, 12
  %29 = sext i32 %28 to i64
  %30 = mul nsw i32 %2, 13
  %31 = sext i32 %30 to i64
  %32 = mul nsw i32 %2, 14
  %33 = sext i32 %32 to i64
  %34 = mul nsw i32 %2, 15
  %35 = sext i32 %34 to i64
  br label %36

36:                                               ; preds = %1037, %3
  %37 = phi i32* [ %1, %3 ], [ %1034, %1037 ]
  %38 = phi i16* [ %6, %3 ], [ %1033, %1037 ]
  %39 = phi i16* [ %0, %3 ], [ %6, %1037 ]
  %40 = phi i32 [ 0, %3 ], [ %1038, %1037 ]
  %41 = icmp eq i32 %40, 0
  br label %42

42:                                               ; preds = %36, %1030
  %43 = phi i32 [ 0, %36 ], [ %1035, %1030 ]
  %44 = phi i32* [ %37, %36 ], [ %1034, %1030 ]
  %45 = phi i16* [ %38, %36 ], [ %1033, %1030 ]
  %46 = phi i16* [ %39, %36 ], [ %208, %1030 ]
  %47 = bitcast i16* %46 to <2 x i64>*
  %48 = load <2 x i64>, <2 x i64>* %47, align 16
  br i1 %41, label %49, label %112

49:                                               ; preds = %42
  %50 = getelementptr inbounds i16, i16* %46, i64 %7
  %51 = bitcast i16* %50 to <8 x i16>*
  %52 = load <8 x i16>, <8 x i16>* %51, align 16
  %53 = getelementptr inbounds i16, i16* %46, i64 %9
  %54 = bitcast i16* %53 to <8 x i16>*
  %55 = load <8 x i16>, <8 x i16>* %54, align 16
  %56 = getelementptr inbounds i16, i16* %46, i64 %11
  %57 = bitcast i16* %56 to <8 x i16>*
  %58 = load <8 x i16>, <8 x i16>* %57, align 16
  %59 = getelementptr inbounds i16, i16* %46, i64 %13
  %60 = bitcast i16* %59 to <8 x i16>*
  %61 = load <8 x i16>, <8 x i16>* %60, align 16
  %62 = getelementptr inbounds i16, i16* %46, i64 %15
  %63 = bitcast i16* %62 to <8 x i16>*
  %64 = load <8 x i16>, <8 x i16>* %63, align 16
  %65 = getelementptr inbounds i16, i16* %46, i64 %17
  %66 = bitcast i16* %65 to <8 x i16>*
  %67 = load <8 x i16>, <8 x i16>* %66, align 16
  %68 = getelementptr inbounds i16, i16* %46, i64 %19
  %69 = bitcast i16* %68 to <8 x i16>*
  %70 = load <8 x i16>, <8 x i16>* %69, align 16
  %71 = getelementptr inbounds i16, i16* %46, i64 %21
  %72 = bitcast i16* %71 to <8 x i16>*
  %73 = load <8 x i16>, <8 x i16>* %72, align 16
  %74 = getelementptr inbounds i16, i16* %46, i64 %23
  %75 = bitcast i16* %74 to <8 x i16>*
  %76 = load <8 x i16>, <8 x i16>* %75, align 16
  %77 = getelementptr inbounds i16, i16* %46, i64 %25
  %78 = bitcast i16* %77 to <8 x i16>*
  %79 = load <8 x i16>, <8 x i16>* %78, align 16
  %80 = getelementptr inbounds i16, i16* %46, i64 %27
  %81 = bitcast i16* %80 to <8 x i16>*
  %82 = load <8 x i16>, <8 x i16>* %81, align 16
  %83 = getelementptr inbounds i16, i16* %46, i64 %29
  %84 = bitcast i16* %83 to <8 x i16>*
  %85 = load <8 x i16>, <8 x i16>* %84, align 16
  %86 = getelementptr inbounds i16, i16* %46, i64 %31
  %87 = bitcast i16* %86 to <8 x i16>*
  %88 = load <8 x i16>, <8 x i16>* %87, align 16
  %89 = getelementptr inbounds i16, i16* %46, i64 %33
  %90 = bitcast i16* %89 to <8 x i16>*
  %91 = load <8 x i16>, <8 x i16>* %90, align 16
  %92 = getelementptr inbounds i16, i16* %46, i64 %35
  %93 = bitcast i16* %92 to <8 x i16>*
  %94 = load <8 x i16>, <8 x i16>* %93, align 16
  %95 = bitcast <2 x i64> %48 to <8 x i16>
  %96 = shl <8 x i16> %95, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %97 = shl <8 x i16> %52, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %98 = shl <8 x i16> %55, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %99 = shl <8 x i16> %58, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %100 = shl <8 x i16> %61, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %101 = shl <8 x i16> %64, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %102 = shl <8 x i16> %67, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %103 = shl <8 x i16> %70, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %104 = shl <8 x i16> %73, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %105 = shl <8 x i16> %76, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %106 = shl <8 x i16> %79, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %107 = shl <8 x i16> %82, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %108 = shl <8 x i16> %85, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %109 = shl <8 x i16> %88, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %110 = shl <8 x i16> %91, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %111 = shl <8 x i16> %94, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  br label %191

112:                                              ; preds = %42
  %113 = getelementptr inbounds i16, i16* %46, i64 16
  %114 = bitcast i16* %113 to <8 x i16>*
  %115 = load <8 x i16>, <8 x i16>* %114, align 16
  %116 = getelementptr inbounds i16, i16* %46, i64 32
  %117 = bitcast i16* %116 to <8 x i16>*
  %118 = load <8 x i16>, <8 x i16>* %117, align 16
  %119 = getelementptr inbounds i16, i16* %46, i64 48
  %120 = bitcast i16* %119 to <8 x i16>*
  %121 = load <8 x i16>, <8 x i16>* %120, align 16
  %122 = getelementptr inbounds i16, i16* %46, i64 64
  %123 = bitcast i16* %122 to <8 x i16>*
  %124 = load <8 x i16>, <8 x i16>* %123, align 16
  %125 = getelementptr inbounds i16, i16* %46, i64 80
  %126 = bitcast i16* %125 to <8 x i16>*
  %127 = load <8 x i16>, <8 x i16>* %126, align 16
  %128 = getelementptr inbounds i16, i16* %46, i64 96
  %129 = bitcast i16* %128 to <8 x i16>*
  %130 = load <8 x i16>, <8 x i16>* %129, align 16
  %131 = getelementptr inbounds i16, i16* %46, i64 112
  %132 = bitcast i16* %131 to <8 x i16>*
  %133 = load <8 x i16>, <8 x i16>* %132, align 16
  %134 = getelementptr inbounds i16, i16* %46, i64 128
  %135 = bitcast i16* %134 to <8 x i16>*
  %136 = load <8 x i16>, <8 x i16>* %135, align 16
  %137 = getelementptr inbounds i16, i16* %46, i64 144
  %138 = bitcast i16* %137 to <8 x i16>*
  %139 = load <8 x i16>, <8 x i16>* %138, align 16
  %140 = getelementptr inbounds i16, i16* %46, i64 160
  %141 = bitcast i16* %140 to <8 x i16>*
  %142 = load <8 x i16>, <8 x i16>* %141, align 16
  %143 = getelementptr inbounds i16, i16* %46, i64 176
  %144 = bitcast i16* %143 to <8 x i16>*
  %145 = load <8 x i16>, <8 x i16>* %144, align 16
  %146 = getelementptr inbounds i16, i16* %46, i64 192
  %147 = bitcast i16* %146 to <8 x i16>*
  %148 = load <8 x i16>, <8 x i16>* %147, align 16
  %149 = getelementptr inbounds i16, i16* %46, i64 208
  %150 = bitcast i16* %149 to <8 x i16>*
  %151 = load <8 x i16>, <8 x i16>* %150, align 16
  %152 = getelementptr inbounds i16, i16* %46, i64 224
  %153 = bitcast i16* %152 to <8 x i16>*
  %154 = load <8 x i16>, <8 x i16>* %153, align 16
  %155 = getelementptr inbounds i16, i16* %46, i64 240
  %156 = bitcast i16* %155 to <8 x i16>*
  %157 = load <8 x i16>, <8 x i16>* %156, align 16
  %158 = bitcast <2 x i64> %48 to <8 x i16>
  %159 = add <8 x i16> %158, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %160 = add <8 x i16> %115, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %161 = add <8 x i16> %118, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %162 = add <8 x i16> %121, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %163 = add <8 x i16> %124, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %164 = add <8 x i16> %127, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %165 = add <8 x i16> %130, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %166 = add <8 x i16> %133, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %167 = add <8 x i16> %136, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %168 = add <8 x i16> %139, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %169 = add <8 x i16> %142, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %170 = add <8 x i16> %145, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %171 = add <8 x i16> %148, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %172 = add <8 x i16> %151, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %173 = add <8 x i16> %154, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %174 = add <8 x i16> %157, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %175 = ashr <8 x i16> %159, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %176 = ashr <8 x i16> %160, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %177 = ashr <8 x i16> %161, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %178 = ashr <8 x i16> %162, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %179 = ashr <8 x i16> %163, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %180 = ashr <8 x i16> %164, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %181 = ashr <8 x i16> %165, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %182 = ashr <8 x i16> %166, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %183 = ashr <8 x i16> %167, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %184 = ashr <8 x i16> %168, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %185 = ashr <8 x i16> %169, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %186 = ashr <8 x i16> %170, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %187 = ashr <8 x i16> %171, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %188 = ashr <8 x i16> %172, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %189 = ashr <8 x i16> %173, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %190 = ashr <8 x i16> %174, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  br label %191

191:                                              ; preds = %112, %49
  %192 = phi <8 x i16> [ %96, %49 ], [ %175, %112 ]
  %193 = phi <8 x i16> [ %97, %49 ], [ %176, %112 ]
  %194 = phi <8 x i16> [ %98, %49 ], [ %177, %112 ]
  %195 = phi <8 x i16> [ %99, %49 ], [ %178, %112 ]
  %196 = phi <8 x i16> [ %100, %49 ], [ %179, %112 ]
  %197 = phi <8 x i16> [ %101, %49 ], [ %180, %112 ]
  %198 = phi <8 x i16> [ %102, %49 ], [ %181, %112 ]
  %199 = phi <8 x i16> [ %103, %49 ], [ %182, %112 ]
  %200 = phi <8 x i16> [ %104, %49 ], [ %183, %112 ]
  %201 = phi <8 x i16> [ %105, %49 ], [ %184, %112 ]
  %202 = phi <8 x i16> [ %106, %49 ], [ %185, %112 ]
  %203 = phi <8 x i16> [ %107, %49 ], [ %186, %112 ]
  %204 = phi <8 x i16> [ %108, %49 ], [ %187, %112 ]
  %205 = phi <8 x i16> [ %109, %49 ], [ %188, %112 ]
  %206 = phi <8 x i16> [ %110, %49 ], [ %189, %112 ]
  %207 = phi <8 x i16> [ %111, %49 ], [ %190, %112 ]
  %208 = getelementptr inbounds i16, i16* %46, i64 8
  %209 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %192, <8 x i16> %207) #6
  %210 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %193, <8 x i16> %206) #6
  %211 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %194, <8 x i16> %205) #6
  %212 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %195, <8 x i16> %204) #6
  %213 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %196, <8 x i16> %203) #6
  %214 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %197, <8 x i16> %202) #6
  %215 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %198, <8 x i16> %201) #6
  %216 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %199, <8 x i16> %200) #6
  %217 = add <8 x i16> %209, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %218 = icmp ult <8 x i16> %217, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %219 = add <8 x i16> %210, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %220 = icmp ult <8 x i16> %219, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %221 = add <8 x i16> %211, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %222 = icmp ult <8 x i16> %221, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %223 = add <8 x i16> %212, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %224 = icmp ult <8 x i16> %223, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %225 = or <8 x i1> %222, %224
  %226 = or <8 x i1> %225, %220
  %227 = or <8 x i1> %226, %218
  %228 = sext <8 x i1> %227 to <8 x i16>
  %229 = bitcast <8 x i16> %228 to <16 x i8>
  %230 = icmp slt <16 x i8> %229, zeroinitializer
  %231 = bitcast <16 x i1> %230 to i16
  %232 = zext i16 %231 to i32
  %233 = add <8 x i16> %213, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %234 = icmp ult <8 x i16> %233, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %235 = add <8 x i16> %214, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %236 = icmp ult <8 x i16> %235, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %237 = add <8 x i16> %215, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %238 = icmp ult <8 x i16> %237, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %239 = add <8 x i16> %216, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %240 = icmp ult <8 x i16> %239, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %241 = or <8 x i1> %238, %240
  %242 = or <8 x i1> %241, %236
  %243 = or <8 x i1> %242, %234
  %244 = sext <8 x i1> %243 to <8 x i16>
  %245 = bitcast <8 x i16> %244 to <16 x i8>
  %246 = icmp slt <16 x i8> %245, zeroinitializer
  %247 = bitcast <16 x i1> %246 to i16
  %248 = zext i16 %247 to i32
  %249 = sub nsw i32 0, %232
  %250 = icmp eq i32 %248, %249
  br i1 %250, label %251, label %1040

251:                                              ; preds = %191
  %252 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %199, <8 x i16> %200) #6
  %253 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %198, <8 x i16> %201) #6
  %254 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %197, <8 x i16> %202) #6
  %255 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %196, <8 x i16> %203) #6
  %256 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %195, <8 x i16> %204) #6
  %257 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %194, <8 x i16> %205) #6
  %258 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %193, <8 x i16> %206) #6
  %259 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %192, <8 x i16> %207) #6
  %260 = add <8 x i16> %252, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %261 = icmp ult <8 x i16> %260, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %262 = add <8 x i16> %253, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %263 = icmp ult <8 x i16> %262, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %264 = add <8 x i16> %254, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %265 = icmp ult <8 x i16> %264, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %266 = add <8 x i16> %255, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %267 = icmp ult <8 x i16> %266, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %268 = or <8 x i1> %263, %261
  %269 = or <8 x i1> %268, %265
  %270 = or <8 x i1> %269, %267
  %271 = sext <8 x i1> %270 to <8 x i16>
  %272 = bitcast <8 x i16> %271 to <16 x i8>
  %273 = icmp slt <16 x i8> %272, zeroinitializer
  %274 = bitcast <16 x i1> %273 to i16
  %275 = zext i16 %274 to i32
  %276 = add <8 x i16> %256, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %277 = icmp ult <8 x i16> %276, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %278 = add <8 x i16> %257, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %279 = icmp ult <8 x i16> %278, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %280 = add <8 x i16> %258, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %281 = icmp ult <8 x i16> %280, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %282 = add <8 x i16> %259, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %283 = icmp ult <8 x i16> %282, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %284 = or <8 x i1> %279, %277
  %285 = or <8 x i1> %284, %281
  %286 = or <8 x i1> %285, %283
  %287 = sext <8 x i1> %286 to <8 x i16>
  %288 = bitcast <8 x i16> %287 to <16 x i8>
  %289 = icmp slt <16 x i8> %288, zeroinitializer
  %290 = bitcast <16 x i1> %289 to i16
  %291 = zext i16 %290 to i32
  %292 = sub nsw i32 0, %275
  %293 = icmp eq i32 %291, %292
  br i1 %293, label %294, label %1040

294:                                              ; preds = %251
  %295 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %209, <8 x i16> %216) #6
  %296 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %210, <8 x i16> %215) #6
  %297 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %211, <8 x i16> %214) #6
  %298 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %212, <8 x i16> %213) #6
  %299 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %212, <8 x i16> %213) #6
  %300 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %211, <8 x i16> %214) #6
  %301 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %210, <8 x i16> %215) #6
  %302 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %209, <8 x i16> %216) #6
  %303 = add <8 x i16> %295, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %304 = icmp ult <8 x i16> %303, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %305 = add <8 x i16> %296, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %306 = icmp ult <8 x i16> %305, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %307 = add <8 x i16> %297, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %308 = icmp ult <8 x i16> %307, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %309 = add <8 x i16> %298, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %310 = icmp ult <8 x i16> %309, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %311 = or <8 x i1> %308, %310
  %312 = or <8 x i1> %311, %306
  %313 = or <8 x i1> %312, %304
  %314 = sext <8 x i1> %313 to <8 x i16>
  %315 = bitcast <8 x i16> %314 to <16 x i8>
  %316 = icmp slt <16 x i8> %315, zeroinitializer
  %317 = bitcast <16 x i1> %316 to i16
  %318 = zext i16 %317 to i32
  %319 = add <8 x i16> %299, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %320 = icmp ult <8 x i16> %319, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %321 = add <8 x i16> %300, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %322 = icmp ult <8 x i16> %321, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %323 = add <8 x i16> %301, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %324 = icmp ult <8 x i16> %323, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %325 = add <8 x i16> %302, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %326 = icmp ult <8 x i16> %325, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %327 = or <8 x i1> %322, %320
  %328 = or <8 x i1> %327, %324
  %329 = or <8 x i1> %328, %326
  %330 = sext <8 x i1> %329 to <8 x i16>
  %331 = bitcast <8 x i16> %330 to <16 x i8>
  %332 = icmp slt <16 x i8> %331, zeroinitializer
  %333 = bitcast <16 x i1> %332 to i16
  %334 = zext i16 %333 to i32
  %335 = sub nsw i32 0, %318
  %336 = icmp eq i32 %334, %335
  br i1 %336, label %337, label %1040

337:                                              ; preds = %294
  %338 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %295, <8 x i16> %298) #6
  %339 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %296, <8 x i16> %297) #6
  %340 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %296, <8 x i16> %297) #6
  %341 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %295, <8 x i16> %298) #6
  %342 = add <8 x i16> %338, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %343 = icmp ult <8 x i16> %342, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %344 = add <8 x i16> %339, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %345 = icmp ult <8 x i16> %344, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %346 = add <8 x i16> %340, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %347 = icmp ult <8 x i16> %346, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %348 = add <8 x i16> %341, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %349 = icmp ult <8 x i16> %348, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %350 = or <8 x i1> %347, %345
  %351 = or <8 x i1> %350, %343
  %352 = or <8 x i1> %351, %349
  %353 = sext <8 x i1> %352 to <8 x i16>
  %354 = bitcast <8 x i16> %353 to <16 x i8>
  %355 = icmp slt <16 x i8> %354, zeroinitializer
  %356 = bitcast <16 x i1> %355 to i16
  %357 = icmp eq i16 %356, 0
  br i1 %357, label %358, label %1040

358:                                              ; preds = %337
  %359 = shufflevector <8 x i16> %338, <8 x i16> %339, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %360 = shufflevector <8 x i16> %338, <8 x i16> %339, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %361 = shufflevector <8 x i16> %340, <8 x i16> %341, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %362 = shufflevector <8 x i16> %340, <8 x i16> %341, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %363 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %359, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %364 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %360, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %365 = add <4 x i32> %363, <i32 8192, i32 8192, i32 8192, i32 8192>
  %366 = add <4 x i32> %364, <i32 8192, i32 8192, i32 8192, i32 8192>
  %367 = ashr <4 x i32> %365, <i32 14, i32 14, i32 14, i32 14>
  %368 = ashr <4 x i32> %366, <i32 14, i32 14, i32 14, i32 14>
  %369 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %367, <4 x i32> %368) #6
  %370 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %359, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %371 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %360, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %372 = add <4 x i32> %370, <i32 8192, i32 8192, i32 8192, i32 8192>
  %373 = add <4 x i32> %371, <i32 8192, i32 8192, i32 8192, i32 8192>
  %374 = ashr <4 x i32> %372, <i32 14, i32 14, i32 14, i32 14>
  %375 = ashr <4 x i32> %373, <i32 14, i32 14, i32 14, i32 14>
  %376 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %374, <4 x i32> %375) #6
  %377 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %361, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %378 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %362, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %379 = add <4 x i32> %377, <i32 8192, i32 8192, i32 8192, i32 8192>
  %380 = add <4 x i32> %378, <i32 8192, i32 8192, i32 8192, i32 8192>
  %381 = ashr <4 x i32> %379, <i32 14, i32 14, i32 14, i32 14>
  %382 = ashr <4 x i32> %380, <i32 14, i32 14, i32 14, i32 14>
  %383 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %381, <4 x i32> %382) #6
  %384 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %361, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %385 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %362, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %386 = add <4 x i32> %384, <i32 8192, i32 8192, i32 8192, i32 8192>
  %387 = add <4 x i32> %385, <i32 8192, i32 8192, i32 8192, i32 8192>
  %388 = ashr <4 x i32> %386, <i32 14, i32 14, i32 14, i32 14>
  %389 = ashr <4 x i32> %387, <i32 14, i32 14, i32 14, i32 14>
  %390 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %388, <4 x i32> %389) #6
  %391 = add <8 x i16> %369, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %392 = icmp ult <8 x i16> %391, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %393 = add <8 x i16> %376, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %394 = icmp ult <8 x i16> %393, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %395 = add <8 x i16> %383, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %396 = icmp ult <8 x i16> %395, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %397 = add <8 x i16> %390, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %398 = icmp ult <8 x i16> %397, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %399 = or <8 x i1> %394, %392
  %400 = or <8 x i1> %399, %396
  %401 = or <8 x i1> %400, %398
  %402 = sext <8 x i1> %401 to <8 x i16>
  %403 = bitcast <8 x i16> %402 to <16 x i8>
  %404 = icmp slt <16 x i8> %403, zeroinitializer
  %405 = bitcast <16 x i1> %404 to i16
  %406 = icmp eq i16 %405, 0
  br i1 %406, label %407, label %1040

407:                                              ; preds = %358
  %408 = shufflevector <8 x i16> %301, <8 x i16> %300, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %409 = shufflevector <8 x i16> %301, <8 x i16> %300, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %410 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %408, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %411 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %409, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %412 = add <4 x i32> %410, <i32 8192, i32 8192, i32 8192, i32 8192>
  %413 = add <4 x i32> %411, <i32 8192, i32 8192, i32 8192, i32 8192>
  %414 = ashr <4 x i32> %412, <i32 14, i32 14, i32 14, i32 14>
  %415 = ashr <4 x i32> %413, <i32 14, i32 14, i32 14, i32 14>
  %416 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %414, <4 x i32> %415) #6
  %417 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %408, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %418 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %409, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %419 = add <4 x i32> %417, <i32 8192, i32 8192, i32 8192, i32 8192>
  %420 = add <4 x i32> %418, <i32 8192, i32 8192, i32 8192, i32 8192>
  %421 = ashr <4 x i32> %419, <i32 14, i32 14, i32 14, i32 14>
  %422 = ashr <4 x i32> %420, <i32 14, i32 14, i32 14, i32 14>
  %423 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %421, <4 x i32> %422) #6
  %424 = add <8 x i16> %416, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %425 = icmp ult <8 x i16> %424, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %426 = add <8 x i16> %423, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %427 = icmp ult <8 x i16> %426, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %428 = or <8 x i1> %427, %425
  %429 = sext <8 x i1> %428 to <8 x i16>
  %430 = bitcast <8 x i16> %429 to <16 x i8>
  %431 = icmp slt <16 x i8> %430, zeroinitializer
  %432 = bitcast <16 x i1> %431 to i16
  %433 = icmp eq i16 %432, 0
  br i1 %433, label %434, label %1040

434:                                              ; preds = %407
  %435 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %299, <8 x i16> %416) #6
  %436 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %299, <8 x i16> %416) #6
  %437 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %302, <8 x i16> %423) #6
  %438 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %302, <8 x i16> %423) #6
  %439 = add <8 x i16> %435, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %440 = icmp ult <8 x i16> %439, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %441 = add <8 x i16> %436, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %442 = icmp ult <8 x i16> %441, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %443 = add <8 x i16> %437, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %444 = icmp ult <8 x i16> %443, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %445 = add <8 x i16> %438, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %446 = icmp ult <8 x i16> %445, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %447 = or <8 x i1> %442, %440
  %448 = or <8 x i1> %447, %444
  %449 = or <8 x i1> %448, %446
  %450 = sext <8 x i1> %449 to <8 x i16>
  %451 = bitcast <8 x i16> %450 to <16 x i8>
  %452 = icmp slt <16 x i8> %451, zeroinitializer
  %453 = bitcast <16 x i1> %452 to i16
  %454 = icmp eq i16 %453, 0
  br i1 %454, label %455, label %1040

455:                                              ; preds = %434
  %456 = shufflevector <8 x i16> %435, <8 x i16> %438, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %457 = shufflevector <8 x i16> %435, <8 x i16> %438, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %458 = shufflevector <8 x i16> %436, <8 x i16> %437, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %459 = shufflevector <8 x i16> %436, <8 x i16> %437, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %460 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %456, <8 x i16> <i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069>) #6
  %461 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %457, <8 x i16> <i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069>) #6
  %462 = add <4 x i32> %460, <i32 8192, i32 8192, i32 8192, i32 8192>
  %463 = add <4 x i32> %461, <i32 8192, i32 8192, i32 8192, i32 8192>
  %464 = ashr <4 x i32> %462, <i32 14, i32 14, i32 14, i32 14>
  %465 = ashr <4 x i32> %463, <i32 14, i32 14, i32 14, i32 14>
  %466 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %464, <4 x i32> %465) #6
  %467 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %456, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %468 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %457, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %469 = add <4 x i32> %467, <i32 8192, i32 8192, i32 8192, i32 8192>
  %470 = add <4 x i32> %468, <i32 8192, i32 8192, i32 8192, i32 8192>
  %471 = ashr <4 x i32> %469, <i32 14, i32 14, i32 14, i32 14>
  %472 = ashr <4 x i32> %470, <i32 14, i32 14, i32 14, i32 14>
  %473 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %471, <4 x i32> %472) #6
  %474 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %458, <8 x i16> <i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102>) #6
  %475 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %459, <8 x i16> <i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102>) #6
  %476 = add <4 x i32> %474, <i32 8192, i32 8192, i32 8192, i32 8192>
  %477 = add <4 x i32> %475, <i32 8192, i32 8192, i32 8192, i32 8192>
  %478 = ashr <4 x i32> %476, <i32 14, i32 14, i32 14, i32 14>
  %479 = ashr <4 x i32> %477, <i32 14, i32 14, i32 14, i32 14>
  %480 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %478, <4 x i32> %479) #6
  %481 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %458, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %482 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %459, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %483 = add <4 x i32> %481, <i32 8192, i32 8192, i32 8192, i32 8192>
  %484 = add <4 x i32> %482, <i32 8192, i32 8192, i32 8192, i32 8192>
  %485 = ashr <4 x i32> %483, <i32 14, i32 14, i32 14, i32 14>
  %486 = ashr <4 x i32> %484, <i32 14, i32 14, i32 14, i32 14>
  %487 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %485, <4 x i32> %486) #6
  %488 = add <8 x i16> %466, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %489 = icmp ult <8 x i16> %488, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %490 = add <8 x i16> %473, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %491 = icmp ult <8 x i16> %490, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %492 = add <8 x i16> %480, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %493 = icmp ult <8 x i16> %492, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %494 = add <8 x i16> %487, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %495 = icmp ult <8 x i16> %494, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %496 = or <8 x i1> %491, %489
  %497 = or <8 x i1> %496, %493
  %498 = or <8 x i1> %497, %495
  %499 = sext <8 x i1> %498 to <8 x i16>
  %500 = bitcast <8 x i16> %499 to <16 x i8>
  %501 = icmp slt <16 x i8> %500, zeroinitializer
  %502 = bitcast <16 x i1> %501 to i16
  %503 = icmp eq i16 %502, 0
  br i1 %503, label %504, label %1040

504:                                              ; preds = %455
  %505 = shufflevector <8 x i16> %257, <8 x i16> %254, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %506 = shufflevector <8 x i16> %257, <8 x i16> %254, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %507 = shufflevector <8 x i16> %256, <8 x i16> %255, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %508 = shufflevector <8 x i16> %256, <8 x i16> %255, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %509 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %505, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %510 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %506, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %511 = add <4 x i32> %509, <i32 8192, i32 8192, i32 8192, i32 8192>
  %512 = add <4 x i32> %510, <i32 8192, i32 8192, i32 8192, i32 8192>
  %513 = ashr <4 x i32> %511, <i32 14, i32 14, i32 14, i32 14>
  %514 = ashr <4 x i32> %512, <i32 14, i32 14, i32 14, i32 14>
  %515 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %513, <4 x i32> %514) #6
  %516 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %507, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %517 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %508, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %518 = add <4 x i32> %516, <i32 8192, i32 8192, i32 8192, i32 8192>
  %519 = add <4 x i32> %517, <i32 8192, i32 8192, i32 8192, i32 8192>
  %520 = ashr <4 x i32> %518, <i32 14, i32 14, i32 14, i32 14>
  %521 = ashr <4 x i32> %519, <i32 14, i32 14, i32 14, i32 14>
  %522 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %520, <4 x i32> %521) #6
  %523 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %505, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %524 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %506, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %525 = add <4 x i32> %523, <i32 8192, i32 8192, i32 8192, i32 8192>
  %526 = add <4 x i32> %524, <i32 8192, i32 8192, i32 8192, i32 8192>
  %527 = ashr <4 x i32> %525, <i32 14, i32 14, i32 14, i32 14>
  %528 = ashr <4 x i32> %526, <i32 14, i32 14, i32 14, i32 14>
  %529 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %527, <4 x i32> %528) #6
  %530 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %507, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %531 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %508, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %532 = add <4 x i32> %530, <i32 8192, i32 8192, i32 8192, i32 8192>
  %533 = add <4 x i32> %531, <i32 8192, i32 8192, i32 8192, i32 8192>
  %534 = ashr <4 x i32> %532, <i32 14, i32 14, i32 14, i32 14>
  %535 = ashr <4 x i32> %533, <i32 14, i32 14, i32 14, i32 14>
  %536 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %534, <4 x i32> %535) #6
  %537 = add <8 x i16> %515, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %538 = icmp ult <8 x i16> %537, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %539 = add <8 x i16> %522, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %540 = icmp ult <8 x i16> %539, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %541 = add <8 x i16> %529, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %542 = icmp ult <8 x i16> %541, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %543 = add <8 x i16> %536, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %544 = icmp ult <8 x i16> %543, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %545 = or <8 x i1> %540, %538
  %546 = or <8 x i1> %545, %542
  %547 = or <8 x i1> %546, %544
  %548 = sext <8 x i1> %547 to <8 x i16>
  %549 = bitcast <8 x i16> %548 to <16 x i8>
  %550 = icmp slt <16 x i8> %549, zeroinitializer
  %551 = bitcast <16 x i1> %550 to i16
  %552 = icmp eq i16 %551, 0
  br i1 %552, label %553, label %1040

553:                                              ; preds = %504
  %554 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %252, <8 x i16> %522) #6
  %555 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %253, <8 x i16> %515) #6
  %556 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %253, <8 x i16> %515) #6
  %557 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %252, <8 x i16> %522) #6
  %558 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %259, <8 x i16> %536) #6
  %559 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %258, <8 x i16> %529) #6
  %560 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %258, <8 x i16> %529) #6
  %561 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %259, <8 x i16> %536) #6
  %562 = add <8 x i16> %554, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %563 = icmp ult <8 x i16> %562, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %564 = add <8 x i16> %555, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %565 = icmp ult <8 x i16> %564, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %566 = add <8 x i16> %556, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %567 = icmp ult <8 x i16> %566, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %568 = add <8 x i16> %557, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %569 = icmp ult <8 x i16> %568, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %570 = or <8 x i1> %567, %565
  %571 = or <8 x i1> %570, %563
  %572 = or <8 x i1> %571, %569
  %573 = sext <8 x i1> %572 to <8 x i16>
  %574 = bitcast <8 x i16> %573 to <16 x i8>
  %575 = icmp slt <16 x i8> %574, zeroinitializer
  %576 = bitcast <16 x i1> %575 to i16
  %577 = zext i16 %576 to i32
  %578 = add <8 x i16> %558, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %579 = icmp ult <8 x i16> %578, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %580 = add <8 x i16> %559, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %581 = icmp ult <8 x i16> %580, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %582 = add <8 x i16> %560, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %583 = icmp ult <8 x i16> %582, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %584 = add <8 x i16> %561, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %585 = icmp ult <8 x i16> %584, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %586 = or <8 x i1> %583, %581
  %587 = or <8 x i1> %586, %579
  %588 = or <8 x i1> %587, %585
  %589 = sext <8 x i1> %588 to <8 x i16>
  %590 = bitcast <8 x i16> %589 to <16 x i8>
  %591 = icmp slt <16 x i8> %590, zeroinitializer
  %592 = bitcast <16 x i1> %591 to i16
  %593 = zext i16 %592 to i32
  %594 = sub nsw i32 0, %577
  %595 = icmp eq i32 %593, %594
  br i1 %595, label %596, label %1040

596:                                              ; preds = %553
  %597 = shufflevector <8 x i16> %555, <8 x i16> %560, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %598 = shufflevector <8 x i16> %555, <8 x i16> %560, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %599 = shufflevector <8 x i16> %556, <8 x i16> %559, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %600 = shufflevector <8 x i16> %556, <8 x i16> %559, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %601 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %597, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %602 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %598, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %603 = add <4 x i32> %601, <i32 8192, i32 8192, i32 8192, i32 8192>
  %604 = add <4 x i32> %602, <i32 8192, i32 8192, i32 8192, i32 8192>
  %605 = ashr <4 x i32> %603, <i32 14, i32 14, i32 14, i32 14>
  %606 = ashr <4 x i32> %604, <i32 14, i32 14, i32 14, i32 14>
  %607 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %605, <4 x i32> %606) #6
  %608 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %599, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %609 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %600, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %610 = add <4 x i32> %608, <i32 8192, i32 8192, i32 8192, i32 8192>
  %611 = add <4 x i32> %609, <i32 8192, i32 8192, i32 8192, i32 8192>
  %612 = ashr <4 x i32> %610, <i32 14, i32 14, i32 14, i32 14>
  %613 = ashr <4 x i32> %611, <i32 14, i32 14, i32 14, i32 14>
  %614 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %612, <4 x i32> %613) #6
  %615 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %597, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %616 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %598, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %617 = add <4 x i32> %615, <i32 8192, i32 8192, i32 8192, i32 8192>
  %618 = add <4 x i32> %616, <i32 8192, i32 8192, i32 8192, i32 8192>
  %619 = ashr <4 x i32> %617, <i32 14, i32 14, i32 14, i32 14>
  %620 = ashr <4 x i32> %618, <i32 14, i32 14, i32 14, i32 14>
  %621 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %619, <4 x i32> %620) #6
  %622 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %599, <8 x i16> <i16 15137, i16 -6270, i16 15137, i16 -6270, i16 15137, i16 -6270, i16 15137, i16 -6270>) #6
  %623 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %600, <8 x i16> <i16 15137, i16 -6270, i16 15137, i16 -6270, i16 15137, i16 -6270, i16 15137, i16 -6270>) #6
  %624 = add <4 x i32> %622, <i32 8192, i32 8192, i32 8192, i32 8192>
  %625 = add <4 x i32> %623, <i32 8192, i32 8192, i32 8192, i32 8192>
  %626 = ashr <4 x i32> %624, <i32 14, i32 14, i32 14, i32 14>
  %627 = ashr <4 x i32> %625, <i32 14, i32 14, i32 14, i32 14>
  %628 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %626, <4 x i32> %627) #6
  %629 = add <8 x i16> %607, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %630 = icmp ult <8 x i16> %629, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %631 = add <8 x i16> %614, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %632 = icmp ult <8 x i16> %631, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %633 = add <8 x i16> %621, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %634 = icmp ult <8 x i16> %633, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %635 = add <8 x i16> %628, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %636 = icmp ult <8 x i16> %635, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %637 = or <8 x i1> %632, %630
  %638 = or <8 x i1> %637, %634
  %639 = or <8 x i1> %638, %636
  %640 = sext <8 x i1> %639 to <8 x i16>
  %641 = bitcast <8 x i16> %640 to <16 x i8>
  %642 = icmp slt <16 x i8> %641, zeroinitializer
  %643 = bitcast <16 x i1> %642 to i16
  %644 = icmp eq i16 %643, 0
  br i1 %644, label %645, label %1040

645:                                              ; preds = %596
  %646 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %554, <8 x i16> %607) #6
  %647 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %554, <8 x i16> %607) #6
  %648 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %557, <8 x i16> %614) #6
  %649 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %557, <8 x i16> %614) #6
  %650 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %558, <8 x i16> %628) #6
  %651 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %558, <8 x i16> %628) #6
  %652 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %561, <8 x i16> %621) #6
  %653 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %561, <8 x i16> %621) #6
  %654 = add <8 x i16> %646, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %655 = icmp ult <8 x i16> %654, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %656 = add <8 x i16> %647, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %657 = icmp ult <8 x i16> %656, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %658 = add <8 x i16> %648, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %659 = icmp ult <8 x i16> %658, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %660 = add <8 x i16> %649, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %661 = icmp ult <8 x i16> %660, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %662 = or <8 x i1> %657, %655
  %663 = or <8 x i1> %662, %659
  %664 = or <8 x i1> %663, %661
  %665 = sext <8 x i1> %664 to <8 x i16>
  %666 = bitcast <8 x i16> %665 to <16 x i8>
  %667 = icmp slt <16 x i8> %666, zeroinitializer
  %668 = bitcast <16 x i1> %667 to i16
  %669 = zext i16 %668 to i32
  %670 = add <8 x i16> %650, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %671 = icmp ult <8 x i16> %670, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %672 = add <8 x i16> %651, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %673 = icmp ult <8 x i16> %672, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %674 = add <8 x i16> %652, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %675 = icmp ult <8 x i16> %674, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %676 = add <8 x i16> %653, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %677 = icmp ult <8 x i16> %676, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %678 = or <8 x i1> %677, %675
  %679 = or <8 x i1> %678, %671
  %680 = or <8 x i1> %679, %673
  %681 = sext <8 x i1> %680 to <8 x i16>
  %682 = bitcast <8 x i16> %681 to <16 x i8>
  %683 = icmp slt <16 x i8> %682, zeroinitializer
  %684 = bitcast <16 x i1> %683 to i16
  %685 = zext i16 %684 to i32
  %686 = sub nsw i32 0, %669
  %687 = icmp eq i32 %685, %686
  br i1 %687, label %688, label %1040

688:                                              ; preds = %645
  %689 = shufflevector <8 x i16> %646, <8 x i16> %653, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %690 = shufflevector <8 x i16> %646, <8 x i16> %653, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %691 = shufflevector <8 x i16> %647, <8 x i16> %652, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %692 = shufflevector <8 x i16> %647, <8 x i16> %652, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %693 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %689, <8 x i16> <i16 1606, i16 16305, i16 1606, i16 16305, i16 1606, i16 16305, i16 1606, i16 16305>) #6
  %694 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %690, <8 x i16> <i16 1606, i16 16305, i16 1606, i16 16305, i16 1606, i16 16305, i16 1606, i16 16305>) #6
  %695 = add <4 x i32> %693, <i32 8192, i32 8192, i32 8192, i32 8192>
  %696 = add <4 x i32> %694, <i32 8192, i32 8192, i32 8192, i32 8192>
  %697 = ashr <4 x i32> %695, <i32 14, i32 14, i32 14, i32 14>
  %698 = ashr <4 x i32> %696, <i32 14, i32 14, i32 14, i32 14>
  %699 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %697, <4 x i32> %698) #6
  %700 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %691, <8 x i16> <i16 12665, i16 10394, i16 12665, i16 10394, i16 12665, i16 10394, i16 12665, i16 10394>) #6
  %701 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %692, <8 x i16> <i16 12665, i16 10394, i16 12665, i16 10394, i16 12665, i16 10394, i16 12665, i16 10394>) #6
  %702 = add <4 x i32> %700, <i32 8192, i32 8192, i32 8192, i32 8192>
  %703 = add <4 x i32> %701, <i32 8192, i32 8192, i32 8192, i32 8192>
  %704 = ashr <4 x i32> %702, <i32 14, i32 14, i32 14, i32 14>
  %705 = ashr <4 x i32> %703, <i32 14, i32 14, i32 14, i32 14>
  %706 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %704, <4 x i32> %705) #6
  %707 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %689, <8 x i16> <i16 -16305, i16 1606, i16 -16305, i16 1606, i16 -16305, i16 1606, i16 -16305, i16 1606>) #6
  %708 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %690, <8 x i16> <i16 -16305, i16 1606, i16 -16305, i16 1606, i16 -16305, i16 1606, i16 -16305, i16 1606>) #6
  %709 = add <4 x i32> %707, <i32 8192, i32 8192, i32 8192, i32 8192>
  %710 = add <4 x i32> %708, <i32 8192, i32 8192, i32 8192, i32 8192>
  %711 = ashr <4 x i32> %709, <i32 14, i32 14, i32 14, i32 14>
  %712 = ashr <4 x i32> %710, <i32 14, i32 14, i32 14, i32 14>
  %713 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %711, <4 x i32> %712) #6
  %714 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %691, <8 x i16> <i16 -10394, i16 12665, i16 -10394, i16 12665, i16 -10394, i16 12665, i16 -10394, i16 12665>) #6
  %715 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %692, <8 x i16> <i16 -10394, i16 12665, i16 -10394, i16 12665, i16 -10394, i16 12665, i16 -10394, i16 12665>) #6
  %716 = add <4 x i32> %714, <i32 8192, i32 8192, i32 8192, i32 8192>
  %717 = add <4 x i32> %715, <i32 8192, i32 8192, i32 8192, i32 8192>
  %718 = ashr <4 x i32> %716, <i32 14, i32 14, i32 14, i32 14>
  %719 = ashr <4 x i32> %717, <i32 14, i32 14, i32 14, i32 14>
  %720 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %718, <4 x i32> %719) #6
  %721 = add <8 x i16> %699, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %722 = icmp ult <8 x i16> %721, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %723 = add <8 x i16> %706, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %724 = icmp ult <8 x i16> %723, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %725 = add <8 x i16> %713, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %726 = icmp ult <8 x i16> %725, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %727 = add <8 x i16> %720, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %728 = icmp ult <8 x i16> %727, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %729 = or <8 x i1> %724, %722
  %730 = or <8 x i1> %729, %726
  %731 = or <8 x i1> %730, %728
  %732 = sext <8 x i1> %731 to <8 x i16>
  %733 = bitcast <8 x i16> %732 to <16 x i8>
  %734 = icmp slt <16 x i8> %733, zeroinitializer
  %735 = bitcast <16 x i1> %734 to i16
  %736 = icmp eq i16 %735, 0
  br i1 %736, label %737, label %1040

737:                                              ; preds = %688
  %738 = shufflevector <8 x i16> %648, <8 x i16> %651, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %739 = shufflevector <8 x i16> %648, <8 x i16> %651, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %740 = shufflevector <8 x i16> %649, <8 x i16> %650, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %741 = shufflevector <8 x i16> %649, <8 x i16> %650, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %742 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %738, <8 x i16> <i16 7723, i16 14449, i16 7723, i16 14449, i16 7723, i16 14449, i16 7723, i16 14449>) #6
  %743 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %739, <8 x i16> <i16 7723, i16 14449, i16 7723, i16 14449, i16 7723, i16 14449, i16 7723, i16 14449>) #6
  %744 = add <4 x i32> %742, <i32 8192, i32 8192, i32 8192, i32 8192>
  %745 = add <4 x i32> %743, <i32 8192, i32 8192, i32 8192, i32 8192>
  %746 = ashr <4 x i32> %744, <i32 14, i32 14, i32 14, i32 14>
  %747 = ashr <4 x i32> %745, <i32 14, i32 14, i32 14, i32 14>
  %748 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %746, <4 x i32> %747) #6
  %749 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %740, <8 x i16> <i16 15679, i16 4756, i16 15679, i16 4756, i16 15679, i16 4756, i16 15679, i16 4756>) #6
  %750 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %741, <8 x i16> <i16 15679, i16 4756, i16 15679, i16 4756, i16 15679, i16 4756, i16 15679, i16 4756>) #6
  %751 = add <4 x i32> %749, <i32 8192, i32 8192, i32 8192, i32 8192>
  %752 = add <4 x i32> %750, <i32 8192, i32 8192, i32 8192, i32 8192>
  %753 = ashr <4 x i32> %751, <i32 14, i32 14, i32 14, i32 14>
  %754 = ashr <4 x i32> %752, <i32 14, i32 14, i32 14, i32 14>
  %755 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %753, <4 x i32> %754) #6
  %756 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %738, <8 x i16> <i16 -14449, i16 7723, i16 -14449, i16 7723, i16 -14449, i16 7723, i16 -14449, i16 7723>) #6
  %757 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %739, <8 x i16> <i16 -14449, i16 7723, i16 -14449, i16 7723, i16 -14449, i16 7723, i16 -14449, i16 7723>) #6
  %758 = add <4 x i32> %756, <i32 8192, i32 8192, i32 8192, i32 8192>
  %759 = add <4 x i32> %757, <i32 8192, i32 8192, i32 8192, i32 8192>
  %760 = ashr <4 x i32> %758, <i32 14, i32 14, i32 14, i32 14>
  %761 = ashr <4 x i32> %759, <i32 14, i32 14, i32 14, i32 14>
  %762 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %760, <4 x i32> %761) #6
  %763 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %740, <8 x i16> <i16 -4756, i16 15679, i16 -4756, i16 15679, i16 -4756, i16 15679, i16 -4756, i16 15679>) #6
  %764 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %741, <8 x i16> <i16 -4756, i16 15679, i16 -4756, i16 15679, i16 -4756, i16 15679, i16 -4756, i16 15679>) #6
  %765 = add <4 x i32> %763, <i32 8192, i32 8192, i32 8192, i32 8192>
  %766 = add <4 x i32> %764, <i32 8192, i32 8192, i32 8192, i32 8192>
  %767 = ashr <4 x i32> %765, <i32 14, i32 14, i32 14, i32 14>
  %768 = ashr <4 x i32> %766, <i32 14, i32 14, i32 14, i32 14>
  %769 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %767, <4 x i32> %768) #6
  %770 = add <8 x i16> %748, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %771 = icmp ult <8 x i16> %770, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %772 = add <8 x i16> %755, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %773 = icmp ult <8 x i16> %772, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %774 = add <8 x i16> %762, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %775 = icmp ult <8 x i16> %774, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %776 = add <8 x i16> %769, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %777 = icmp ult <8 x i16> %776, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %778 = or <8 x i1> %773, %771
  %779 = or <8 x i1> %778, %775
  %780 = or <8 x i1> %779, %777
  %781 = sext <8 x i1> %780 to <8 x i16>
  %782 = bitcast <8 x i16> %781 to <16 x i8>
  %783 = icmp slt <16 x i8> %782, zeroinitializer
  %784 = bitcast <16 x i1> %783 to i16
  %785 = icmp eq i16 %784, 0
  br i1 %785, label %786, label %1040

786:                                              ; preds = %737
  %787 = shufflevector <8 x i16> %369, <8 x i16> %699, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %788 = shufflevector <8 x i16> %466, <8 x i16> %769, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %789 = shufflevector <8 x i16> %369, <8 x i16> %699, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %790 = shufflevector <8 x i16> %466, <8 x i16> %769, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %791 = shufflevector <8 x i16> %383, <8 x i16> %748, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %792 = shufflevector <8 x i16> %487, <8 x i16> %720, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %793 = shufflevector <8 x i16> %383, <8 x i16> %748, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %794 = shufflevector <8 x i16> %487, <8 x i16> %720, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %795 = bitcast <8 x i16> %787 to <4 x i32>
  %796 = bitcast <8 x i16> %788 to <4 x i32>
  %797 = shufflevector <4 x i32> %795, <4 x i32> %796, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %798 = bitcast <4 x i32> %797 to <2 x i64>
  %799 = bitcast <8 x i16> %789 to <4 x i32>
  %800 = bitcast <8 x i16> %790 to <4 x i32>
  %801 = shufflevector <4 x i32> %799, <4 x i32> %800, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %802 = bitcast <4 x i32> %801 to <2 x i64>
  %803 = shufflevector <4 x i32> %795, <4 x i32> %796, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %804 = bitcast <4 x i32> %803 to <2 x i64>
  %805 = shufflevector <4 x i32> %799, <4 x i32> %800, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %806 = bitcast <4 x i32> %805 to <2 x i64>
  %807 = bitcast <8 x i16> %791 to <4 x i32>
  %808 = bitcast <8 x i16> %792 to <4 x i32>
  %809 = shufflevector <4 x i32> %807, <4 x i32> %808, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %810 = bitcast <4 x i32> %809 to <2 x i64>
  %811 = bitcast <8 x i16> %793 to <4 x i32>
  %812 = bitcast <8 x i16> %794 to <4 x i32>
  %813 = shufflevector <4 x i32> %811, <4 x i32> %812, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %814 = bitcast <4 x i32> %813 to <2 x i64>
  %815 = shufflevector <4 x i32> %807, <4 x i32> %808, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %816 = bitcast <4 x i32> %815 to <2 x i64>
  %817 = shufflevector <4 x i32> %811, <4 x i32> %812, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %818 = bitcast <4 x i32> %817 to <2 x i64>
  %819 = shufflevector <2 x i64> %798, <2 x i64> %810, <2 x i32> <i32 0, i32 2>
  %820 = shufflevector <2 x i64> %798, <2 x i64> %810, <2 x i32> <i32 1, i32 3>
  %821 = shufflevector <2 x i64> %804, <2 x i64> %816, <2 x i32> <i32 0, i32 2>
  %822 = shufflevector <2 x i64> %804, <2 x i64> %816, <2 x i32> <i32 1, i32 3>
  %823 = shufflevector <2 x i64> %802, <2 x i64> %814, <2 x i32> <i32 0, i32 2>
  %824 = shufflevector <2 x i64> %802, <2 x i64> %814, <2 x i32> <i32 1, i32 3>
  %825 = shufflevector <2 x i64> %806, <2 x i64> %818, <2 x i32> <i32 0, i32 2>
  %826 = shufflevector <2 x i64> %806, <2 x i64> %818, <2 x i32> <i32 1, i32 3>
  br i1 %41, label %827, label %843

827:                                              ; preds = %786
  %828 = bitcast i16* %45 to <2 x i64>*
  store <2 x i64> %819, <2 x i64>* %828, align 1
  %829 = getelementptr inbounds i16, i16* %45, i64 16
  %830 = bitcast i16* %829 to <2 x i64>*
  store <2 x i64> %820, <2 x i64>* %830, align 1
  %831 = getelementptr inbounds i16, i16* %45, i64 32
  %832 = bitcast i16* %831 to <2 x i64>*
  store <2 x i64> %821, <2 x i64>* %832, align 1
  %833 = getelementptr inbounds i16, i16* %45, i64 48
  %834 = bitcast i16* %833 to <2 x i64>*
  store <2 x i64> %822, <2 x i64>* %834, align 1
  %835 = getelementptr inbounds i16, i16* %45, i64 64
  %836 = bitcast i16* %835 to <2 x i64>*
  store <2 x i64> %823, <2 x i64>* %836, align 1
  %837 = getelementptr inbounds i16, i16* %45, i64 80
  %838 = bitcast i16* %837 to <2 x i64>*
  store <2 x i64> %824, <2 x i64>* %838, align 1
  %839 = getelementptr inbounds i16, i16* %45, i64 96
  %840 = bitcast i16* %839 to <2 x i64>*
  store <2 x i64> %825, <2 x i64>* %840, align 1
  %841 = getelementptr inbounds i16, i16* %45, i64 112
  %842 = bitcast i16* %841 to <2 x i64>*
  store <2 x i64> %826, <2 x i64>* %842, align 1
  br label %907

843:                                              ; preds = %786
  %844 = bitcast <2 x i64> %819 to <8 x i16>
  %845 = ashr <8 x i16> %844, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %846 = shufflevector <8 x i16> %844, <8 x i16> %845, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %847 = shufflevector <8 x i16> %844, <8 x i16> %845, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %848 = bitcast i32* %44 to <8 x i16>*
  store <8 x i16> %846, <8 x i16>* %848, align 1
  %849 = getelementptr inbounds i32, i32* %44, i64 4
  %850 = bitcast i32* %849 to <8 x i16>*
  store <8 x i16> %847, <8 x i16>* %850, align 1
  %851 = getelementptr inbounds i32, i32* %44, i64 16
  %852 = bitcast <2 x i64> %820 to <8 x i16>
  %853 = ashr <8 x i16> %852, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %854 = shufflevector <8 x i16> %852, <8 x i16> %853, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %855 = shufflevector <8 x i16> %852, <8 x i16> %853, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %856 = bitcast i32* %851 to <8 x i16>*
  store <8 x i16> %854, <8 x i16>* %856, align 1
  %857 = getelementptr inbounds i32, i32* %44, i64 20
  %858 = bitcast i32* %857 to <8 x i16>*
  store <8 x i16> %855, <8 x i16>* %858, align 1
  %859 = getelementptr inbounds i32, i32* %44, i64 32
  %860 = bitcast <2 x i64> %821 to <8 x i16>
  %861 = ashr <8 x i16> %860, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %862 = shufflevector <8 x i16> %860, <8 x i16> %861, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %863 = shufflevector <8 x i16> %860, <8 x i16> %861, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %864 = bitcast i32* %859 to <8 x i16>*
  store <8 x i16> %862, <8 x i16>* %864, align 1
  %865 = getelementptr inbounds i32, i32* %44, i64 36
  %866 = bitcast i32* %865 to <8 x i16>*
  store <8 x i16> %863, <8 x i16>* %866, align 1
  %867 = getelementptr inbounds i32, i32* %44, i64 48
  %868 = bitcast <2 x i64> %822 to <8 x i16>
  %869 = ashr <8 x i16> %868, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %870 = shufflevector <8 x i16> %868, <8 x i16> %869, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %871 = shufflevector <8 x i16> %868, <8 x i16> %869, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %872 = bitcast i32* %867 to <8 x i16>*
  store <8 x i16> %870, <8 x i16>* %872, align 1
  %873 = getelementptr inbounds i32, i32* %44, i64 52
  %874 = bitcast i32* %873 to <8 x i16>*
  store <8 x i16> %871, <8 x i16>* %874, align 1
  %875 = getelementptr inbounds i32, i32* %44, i64 64
  %876 = bitcast <2 x i64> %823 to <8 x i16>
  %877 = ashr <8 x i16> %876, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %878 = shufflevector <8 x i16> %876, <8 x i16> %877, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %879 = shufflevector <8 x i16> %876, <8 x i16> %877, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %880 = bitcast i32* %875 to <8 x i16>*
  store <8 x i16> %878, <8 x i16>* %880, align 1
  %881 = getelementptr inbounds i32, i32* %44, i64 68
  %882 = bitcast i32* %881 to <8 x i16>*
  store <8 x i16> %879, <8 x i16>* %882, align 1
  %883 = getelementptr inbounds i32, i32* %44, i64 80
  %884 = bitcast <2 x i64> %824 to <8 x i16>
  %885 = ashr <8 x i16> %884, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %886 = shufflevector <8 x i16> %884, <8 x i16> %885, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %887 = shufflevector <8 x i16> %884, <8 x i16> %885, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %888 = bitcast i32* %883 to <8 x i16>*
  store <8 x i16> %886, <8 x i16>* %888, align 1
  %889 = getelementptr inbounds i32, i32* %44, i64 84
  %890 = bitcast i32* %889 to <8 x i16>*
  store <8 x i16> %887, <8 x i16>* %890, align 1
  %891 = getelementptr inbounds i32, i32* %44, i64 96
  %892 = bitcast <2 x i64> %825 to <8 x i16>
  %893 = ashr <8 x i16> %892, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %894 = shufflevector <8 x i16> %892, <8 x i16> %893, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %895 = shufflevector <8 x i16> %892, <8 x i16> %893, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %896 = bitcast i32* %891 to <8 x i16>*
  store <8 x i16> %894, <8 x i16>* %896, align 1
  %897 = getelementptr inbounds i32, i32* %44, i64 100
  %898 = bitcast i32* %897 to <8 x i16>*
  store <8 x i16> %895, <8 x i16>* %898, align 1
  %899 = getelementptr inbounds i32, i32* %44, i64 112
  %900 = bitcast <2 x i64> %826 to <8 x i16>
  %901 = ashr <8 x i16> %900, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %902 = shufflevector <8 x i16> %900, <8 x i16> %901, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %903 = shufflevector <8 x i16> %900, <8 x i16> %901, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %904 = bitcast i32* %899 to <8 x i16>*
  store <8 x i16> %902, <8 x i16>* %904, align 1
  %905 = getelementptr inbounds i32, i32* %44, i64 116
  %906 = bitcast i32* %905 to <8 x i16>*
  store <8 x i16> %903, <8 x i16>* %906, align 1
  br label %907

907:                                              ; preds = %827, %843
  %908 = shufflevector <8 x i16> %376, <8 x i16> %706, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %909 = shufflevector <8 x i16> %480, <8 x i16> %762, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %910 = shufflevector <8 x i16> %376, <8 x i16> %706, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %911 = shufflevector <8 x i16> %480, <8 x i16> %762, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %912 = shufflevector <8 x i16> %390, <8 x i16> %755, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %913 = shufflevector <8 x i16> %473, <8 x i16> %713, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %914 = shufflevector <8 x i16> %390, <8 x i16> %755, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %915 = shufflevector <8 x i16> %473, <8 x i16> %713, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %916 = bitcast <8 x i16> %908 to <4 x i32>
  %917 = bitcast <8 x i16> %909 to <4 x i32>
  %918 = shufflevector <4 x i32> %916, <4 x i32> %917, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %919 = bitcast <4 x i32> %918 to <2 x i64>
  %920 = bitcast <8 x i16> %910 to <4 x i32>
  %921 = bitcast <8 x i16> %911 to <4 x i32>
  %922 = shufflevector <4 x i32> %920, <4 x i32> %921, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %923 = bitcast <4 x i32> %922 to <2 x i64>
  %924 = shufflevector <4 x i32> %916, <4 x i32> %917, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %925 = bitcast <4 x i32> %924 to <2 x i64>
  %926 = shufflevector <4 x i32> %920, <4 x i32> %921, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %927 = bitcast <4 x i32> %926 to <2 x i64>
  %928 = bitcast <8 x i16> %912 to <4 x i32>
  %929 = bitcast <8 x i16> %913 to <4 x i32>
  %930 = shufflevector <4 x i32> %928, <4 x i32> %929, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %931 = bitcast <4 x i32> %930 to <2 x i64>
  %932 = bitcast <8 x i16> %914 to <4 x i32>
  %933 = bitcast <8 x i16> %915 to <4 x i32>
  %934 = shufflevector <4 x i32> %932, <4 x i32> %933, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %935 = bitcast <4 x i32> %934 to <2 x i64>
  %936 = shufflevector <4 x i32> %928, <4 x i32> %929, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %937 = bitcast <4 x i32> %936 to <2 x i64>
  %938 = shufflevector <4 x i32> %932, <4 x i32> %933, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %939 = bitcast <4 x i32> %938 to <2 x i64>
  %940 = shufflevector <2 x i64> %919, <2 x i64> %931, <2 x i32> <i32 0, i32 2>
  %941 = shufflevector <2 x i64> %919, <2 x i64> %931, <2 x i32> <i32 1, i32 3>
  %942 = shufflevector <2 x i64> %925, <2 x i64> %937, <2 x i32> <i32 0, i32 2>
  %943 = shufflevector <2 x i64> %925, <2 x i64> %937, <2 x i32> <i32 1, i32 3>
  %944 = shufflevector <2 x i64> %923, <2 x i64> %935, <2 x i32> <i32 0, i32 2>
  %945 = shufflevector <2 x i64> %923, <2 x i64> %935, <2 x i32> <i32 1, i32 3>
  %946 = shufflevector <2 x i64> %927, <2 x i64> %939, <2 x i32> <i32 0, i32 2>
  %947 = shufflevector <2 x i64> %927, <2 x i64> %939, <2 x i32> <i32 1, i32 3>
  br i1 %41, label %948, label %965

948:                                              ; preds = %907
  %949 = getelementptr inbounds i16, i16* %45, i64 8
  %950 = bitcast i16* %949 to <2 x i64>*
  store <2 x i64> %940, <2 x i64>* %950, align 1
  %951 = getelementptr inbounds i16, i16* %45, i64 24
  %952 = bitcast i16* %951 to <2 x i64>*
  store <2 x i64> %941, <2 x i64>* %952, align 1
  %953 = getelementptr inbounds i16, i16* %45, i64 40
  %954 = bitcast i16* %953 to <2 x i64>*
  store <2 x i64> %942, <2 x i64>* %954, align 1
  %955 = getelementptr inbounds i16, i16* %45, i64 56
  %956 = bitcast i16* %955 to <2 x i64>*
  store <2 x i64> %943, <2 x i64>* %956, align 1
  %957 = getelementptr inbounds i16, i16* %45, i64 72
  %958 = bitcast i16* %957 to <2 x i64>*
  store <2 x i64> %944, <2 x i64>* %958, align 1
  %959 = getelementptr inbounds i16, i16* %45, i64 88
  %960 = bitcast i16* %959 to <2 x i64>*
  store <2 x i64> %945, <2 x i64>* %960, align 1
  %961 = getelementptr inbounds i16, i16* %45, i64 104
  %962 = bitcast i16* %961 to <2 x i64>*
  store <2 x i64> %946, <2 x i64>* %962, align 1
  %963 = getelementptr inbounds i16, i16* %45, i64 120
  %964 = bitcast i16* %963 to <2 x i64>*
  store <2 x i64> %947, <2 x i64>* %964, align 1
  br label %1030

965:                                              ; preds = %907
  %966 = getelementptr inbounds i32, i32* %44, i64 8
  %967 = bitcast <2 x i64> %940 to <8 x i16>
  %968 = ashr <8 x i16> %967, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %969 = shufflevector <8 x i16> %967, <8 x i16> %968, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %970 = shufflevector <8 x i16> %967, <8 x i16> %968, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %971 = bitcast i32* %966 to <8 x i16>*
  store <8 x i16> %969, <8 x i16>* %971, align 1
  %972 = getelementptr inbounds i32, i32* %44, i64 12
  %973 = bitcast i32* %972 to <8 x i16>*
  store <8 x i16> %970, <8 x i16>* %973, align 1
  %974 = getelementptr inbounds i32, i32* %44, i64 24
  %975 = bitcast <2 x i64> %941 to <8 x i16>
  %976 = ashr <8 x i16> %975, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %977 = shufflevector <8 x i16> %975, <8 x i16> %976, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %978 = shufflevector <8 x i16> %975, <8 x i16> %976, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %979 = bitcast i32* %974 to <8 x i16>*
  store <8 x i16> %977, <8 x i16>* %979, align 1
  %980 = getelementptr inbounds i32, i32* %44, i64 28
  %981 = bitcast i32* %980 to <8 x i16>*
  store <8 x i16> %978, <8 x i16>* %981, align 1
  %982 = getelementptr inbounds i32, i32* %44, i64 40
  %983 = bitcast <2 x i64> %942 to <8 x i16>
  %984 = ashr <8 x i16> %983, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %985 = shufflevector <8 x i16> %983, <8 x i16> %984, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %986 = shufflevector <8 x i16> %983, <8 x i16> %984, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %987 = bitcast i32* %982 to <8 x i16>*
  store <8 x i16> %985, <8 x i16>* %987, align 1
  %988 = getelementptr inbounds i32, i32* %44, i64 44
  %989 = bitcast i32* %988 to <8 x i16>*
  store <8 x i16> %986, <8 x i16>* %989, align 1
  %990 = getelementptr inbounds i32, i32* %44, i64 56
  %991 = bitcast <2 x i64> %943 to <8 x i16>
  %992 = ashr <8 x i16> %991, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %993 = shufflevector <8 x i16> %991, <8 x i16> %992, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %994 = shufflevector <8 x i16> %991, <8 x i16> %992, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %995 = bitcast i32* %990 to <8 x i16>*
  store <8 x i16> %993, <8 x i16>* %995, align 1
  %996 = getelementptr inbounds i32, i32* %44, i64 60
  %997 = bitcast i32* %996 to <8 x i16>*
  store <8 x i16> %994, <8 x i16>* %997, align 1
  %998 = getelementptr inbounds i32, i32* %44, i64 72
  %999 = bitcast <2 x i64> %944 to <8 x i16>
  %1000 = ashr <8 x i16> %999, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1001 = shufflevector <8 x i16> %999, <8 x i16> %1000, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1002 = shufflevector <8 x i16> %999, <8 x i16> %1000, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1003 = bitcast i32* %998 to <8 x i16>*
  store <8 x i16> %1001, <8 x i16>* %1003, align 1
  %1004 = getelementptr inbounds i32, i32* %44, i64 76
  %1005 = bitcast i32* %1004 to <8 x i16>*
  store <8 x i16> %1002, <8 x i16>* %1005, align 1
  %1006 = getelementptr inbounds i32, i32* %44, i64 88
  %1007 = bitcast <2 x i64> %945 to <8 x i16>
  %1008 = ashr <8 x i16> %1007, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1009 = shufflevector <8 x i16> %1007, <8 x i16> %1008, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1010 = shufflevector <8 x i16> %1007, <8 x i16> %1008, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1011 = bitcast i32* %1006 to <8 x i16>*
  store <8 x i16> %1009, <8 x i16>* %1011, align 1
  %1012 = getelementptr inbounds i32, i32* %44, i64 92
  %1013 = bitcast i32* %1012 to <8 x i16>*
  store <8 x i16> %1010, <8 x i16>* %1013, align 1
  %1014 = getelementptr inbounds i32, i32* %44, i64 104
  %1015 = bitcast <2 x i64> %946 to <8 x i16>
  %1016 = ashr <8 x i16> %1015, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1017 = shufflevector <8 x i16> %1015, <8 x i16> %1016, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1018 = shufflevector <8 x i16> %1015, <8 x i16> %1016, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1019 = bitcast i32* %1014 to <8 x i16>*
  store <8 x i16> %1017, <8 x i16>* %1019, align 1
  %1020 = getelementptr inbounds i32, i32* %44, i64 108
  %1021 = bitcast i32* %1020 to <8 x i16>*
  store <8 x i16> %1018, <8 x i16>* %1021, align 1
  %1022 = getelementptr inbounds i32, i32* %44, i64 120
  %1023 = bitcast <2 x i64> %947 to <8 x i16>
  %1024 = ashr <8 x i16> %1023, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1025 = shufflevector <8 x i16> %1023, <8 x i16> %1024, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1026 = shufflevector <8 x i16> %1023, <8 x i16> %1024, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1027 = bitcast i32* %1022 to <8 x i16>*
  store <8 x i16> %1025, <8 x i16>* %1027, align 1
  %1028 = getelementptr inbounds i32, i32* %44, i64 124
  %1029 = bitcast i32* %1028 to <8 x i16>*
  store <8 x i16> %1026, <8 x i16>* %1029, align 1
  br label %1030

1030:                                             ; preds = %948, %965
  %1031 = getelementptr inbounds i16, i16* %45, i64 128
  %1032 = getelementptr inbounds i32, i32* %44, i64 128
  %1033 = select i1 %41, i16* %1031, i16* %45
  %1034 = select i1 %41, i32* %44, i32* %1032
  %1035 = add nuw nsw i32 %43, 8
  %1036 = icmp ult i32 %1035, 16
  br i1 %1036, label %42, label %1037

1037:                                             ; preds = %1030
  %1038 = add nuw nsw i32 %40, 1
  %1039 = icmp eq i32 %1038, 2
  br i1 %1039, label %1041, label %36

1040:                                             ; preds = %737, %688, %645, %596, %553, %504, %455, %434, %407, %358, %337, %294, %251, %191
  tail call void @vpx_highbd_fdct16x16_c(i16* %0, i32* %1, i32 %2) #6
  br label %1041

1041:                                             ; preds = %1037, %1040
  call void @llvm.lifetime.end.p0i8(i64 512, i8* nonnull %5) #6
  ret void
}

declare void @vpx_highbd_fdct16x16_c(i16*, i32*, i32) local_unnamed_addr #3

; Function Attrs: nounwind ssp uwtable
define hidden void @vpx_highbd_fdct32x32_rd_sse2(i16*, i32*, i32) local_unnamed_addr #2 {
  %4 = alloca [32 x i64], align 16
  %5 = alloca [32 x i64], align 16
  %6 = alloca [1024 x i16], align 16
  %7 = alloca [32 x <2 x i64>], align 16
  %8 = shl nsw i32 %2, 1
  %9 = mul nsw i32 %2, 3
  %10 = bitcast [1024 x i16]* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 2048, i8* nonnull %10) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %10, i8 -86, i64 2048, i1 false)
  %11 = bitcast [32 x <2 x i64>]* %7 to i8*
  %12 = mul nsw i32 %2, 31
  %13 = sext i32 %12 to i64
  %14 = sext i32 %2 to i64
  %15 = sext i32 %8 to i64
  %16 = sext i32 %9 to i64
  %17 = sub nsw i64 0, %16
  %18 = sub nsw i64 0, %15
  %19 = sub nsw i64 0, %14
  %20 = shl nsw i32 %2, 2
  %21 = sext i32 %20 to i64
  %22 = mul nsw i32 %2, 27
  %23 = sext i32 %22 to i64
  %24 = shl nsw i32 %2, 3
  %25 = sext i32 %24 to i64
  %26 = mul nsw i32 %2, 23
  %27 = sext i32 %26 to i64
  %28 = mul nsw i32 %2, 12
  %29 = sext i32 %28 to i64
  %30 = mul nsw i32 %2, 19
  %31 = sext i32 %30 to i64
  %32 = bitcast [32 x <2 x i64>]* %7 to <8 x i16>*
  %33 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 16
  %34 = bitcast <2 x i64>* %33 to <8 x i16>*
  %35 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 8
  %36 = bitcast <2 x i64>* %35 to <8 x i16>*
  %37 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 24
  %38 = bitcast <2 x i64>* %37 to <8 x i16>*
  %39 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 4
  %40 = bitcast <2 x i64>* %39 to <8 x i16>*
  %41 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 20
  %42 = bitcast <2 x i64>* %41 to <8 x i16>*
  %43 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 12
  %44 = bitcast <2 x i64>* %43 to <8 x i16>*
  %45 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 28
  %46 = bitcast <2 x i64>* %45 to <8 x i16>*
  %47 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 2
  %48 = bitcast <2 x i64>* %47 to <8 x i16>*
  %49 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 18
  %50 = bitcast <2 x i64>* %49 to <8 x i16>*
  %51 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 10
  %52 = bitcast <2 x i64>* %51 to <8 x i16>*
  %53 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 26
  %54 = bitcast <2 x i64>* %53 to <8 x i16>*
  %55 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 6
  %56 = bitcast <2 x i64>* %55 to <8 x i16>*
  %57 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 22
  %58 = bitcast <2 x i64>* %57 to <8 x i16>*
  %59 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 14
  %60 = bitcast <2 x i64>* %59 to <8 x i16>*
  %61 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 30
  %62 = bitcast <2 x i64>* %61 to <8 x i16>*
  %63 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 1
  %64 = bitcast <2 x i64>* %63 to <8 x i16>*
  %65 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 17
  %66 = bitcast <2 x i64>* %65 to <8 x i16>*
  %67 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 9
  %68 = bitcast <2 x i64>* %67 to <8 x i16>*
  %69 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 25
  %70 = bitcast <2 x i64>* %69 to <8 x i16>*
  %71 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 7
  %72 = bitcast <2 x i64>* %71 to <8 x i16>*
  %73 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 23
  %74 = bitcast <2 x i64>* %73 to <8 x i16>*
  %75 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 15
  %76 = bitcast <2 x i64>* %75 to <8 x i16>*
  %77 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 31
  %78 = bitcast <2 x i64>* %77 to <8 x i16>*
  %79 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 5
  %80 = bitcast <2 x i64>* %79 to <8 x i16>*
  %81 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 21
  %82 = bitcast <2 x i64>* %81 to <8 x i16>*
  %83 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 13
  %84 = bitcast <2 x i64>* %83 to <8 x i16>*
  %85 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 29
  %86 = bitcast <2 x i64>* %85 to <8 x i16>*
  %87 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 3
  %88 = bitcast <2 x i64>* %87 to <8 x i16>*
  %89 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 19
  %90 = bitcast <2 x i64>* %89 to <8 x i16>*
  %91 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 11
  %92 = bitcast <2 x i64>* %91 to <8 x i16>*
  %93 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 27
  %94 = bitcast <2 x i64>* %93 to <8 x i16>*
  br label %95

95:                                               ; preds = %6137, %3
  %96 = phi i32 [ 0, %3 ], [ %6138, %6137 ]
  %97 = icmp eq i32 %96, 0
  %98 = icmp eq i32 %96, 1
  br label %99

99:                                               ; preds = %95, %6133
  %100 = phi i64 [ 0, %95 ], [ %6134, %6133 ]
  call void @llvm.lifetime.start.p0i8(i64 512, i8* nonnull %11) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %11, i8 -86, i64 512, i1 false)
  br i1 %97, label %101, label %262

101:                                              ; preds = %99
  %102 = getelementptr inbounds i16, i16* %0, i64 %100
  %103 = getelementptr inbounds i16, i16* %102, i64 %13
  %104 = bitcast i16* %102 to <8 x i16>*
  %105 = load <8 x i16>, <8 x i16>* %104, align 1
  %106 = getelementptr inbounds i16, i16* %102, i64 %14
  %107 = bitcast i16* %106 to <8 x i16>*
  %108 = load <8 x i16>, <8 x i16>* %107, align 1
  %109 = getelementptr inbounds i16, i16* %102, i64 %15
  %110 = bitcast i16* %109 to <8 x i16>*
  %111 = load <8 x i16>, <8 x i16>* %110, align 1
  %112 = getelementptr inbounds i16, i16* %102, i64 %16
  %113 = bitcast i16* %112 to <8 x i16>*
  %114 = load <8 x i16>, <8 x i16>* %113, align 1
  %115 = getelementptr inbounds i16, i16* %103, i64 %17
  %116 = bitcast i16* %115 to <8 x i16>*
  %117 = load <8 x i16>, <8 x i16>* %116, align 1
  %118 = getelementptr inbounds i16, i16* %103, i64 %18
  %119 = bitcast i16* %118 to <8 x i16>*
  %120 = load <8 x i16>, <8 x i16>* %119, align 1
  %121 = getelementptr inbounds i16, i16* %103, i64 %19
  %122 = bitcast i16* %121 to <8 x i16>*
  %123 = load <8 x i16>, <8 x i16>* %122, align 1
  %124 = bitcast i16* %103 to <8 x i16>*
  %125 = load <8 x i16>, <8 x i16>* %124, align 1
  %126 = add <8 x i16> %125, %105
  %127 = add <8 x i16> %123, %108
  %128 = add <8 x i16> %120, %111
  %129 = add <8 x i16> %117, %114
  %130 = sub <8 x i16> %114, %117
  %131 = sub <8 x i16> %111, %120
  %132 = sub <8 x i16> %108, %123
  %133 = sub <8 x i16> %105, %125
  %134 = shl <8 x i16> %126, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %135 = shl <8 x i16> %127, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %136 = shl <8 x i16> %128, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %137 = shl <8 x i16> %129, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %138 = shl <8 x i16> %130, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %139 = shl <8 x i16> %131, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %140 = shl <8 x i16> %132, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %141 = shl <8 x i16> %133, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %142 = getelementptr inbounds i16, i16* %102, i64 %21
  %143 = getelementptr inbounds i16, i16* %102, i64 %23
  %144 = bitcast i16* %142 to <8 x i16>*
  %145 = load <8 x i16>, <8 x i16>* %144, align 1
  %146 = getelementptr inbounds i16, i16* %142, i64 %14
  %147 = bitcast i16* %146 to <8 x i16>*
  %148 = load <8 x i16>, <8 x i16>* %147, align 1
  %149 = getelementptr inbounds i16, i16* %142, i64 %15
  %150 = bitcast i16* %149 to <8 x i16>*
  %151 = load <8 x i16>, <8 x i16>* %150, align 1
  %152 = getelementptr inbounds i16, i16* %142, i64 %16
  %153 = bitcast i16* %152 to <8 x i16>*
  %154 = load <8 x i16>, <8 x i16>* %153, align 1
  %155 = getelementptr inbounds i16, i16* %143, i64 %17
  %156 = bitcast i16* %155 to <8 x i16>*
  %157 = load <8 x i16>, <8 x i16>* %156, align 1
  %158 = getelementptr inbounds i16, i16* %143, i64 %18
  %159 = bitcast i16* %158 to <8 x i16>*
  %160 = load <8 x i16>, <8 x i16>* %159, align 1
  %161 = getelementptr inbounds i16, i16* %143, i64 %19
  %162 = bitcast i16* %161 to <8 x i16>*
  %163 = load <8 x i16>, <8 x i16>* %162, align 1
  %164 = bitcast i16* %143 to <8 x i16>*
  %165 = load <8 x i16>, <8 x i16>* %164, align 1
  %166 = add <8 x i16> %165, %145
  %167 = add <8 x i16> %163, %148
  %168 = add <8 x i16> %160, %151
  %169 = add <8 x i16> %157, %154
  %170 = sub <8 x i16> %154, %157
  %171 = sub <8 x i16> %151, %160
  %172 = sub <8 x i16> %148, %163
  %173 = sub <8 x i16> %145, %165
  %174 = shl <8 x i16> %166, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %175 = shl <8 x i16> %167, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %176 = shl <8 x i16> %168, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %177 = shl <8 x i16> %169, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %178 = shl <8 x i16> %170, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %179 = shl <8 x i16> %171, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %180 = shl <8 x i16> %172, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %181 = shl <8 x i16> %173, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %182 = getelementptr inbounds i16, i16* %102, i64 %25
  %183 = getelementptr inbounds i16, i16* %102, i64 %27
  %184 = bitcast i16* %182 to <8 x i16>*
  %185 = load <8 x i16>, <8 x i16>* %184, align 1
  %186 = getelementptr inbounds i16, i16* %182, i64 %14
  %187 = bitcast i16* %186 to <8 x i16>*
  %188 = load <8 x i16>, <8 x i16>* %187, align 1
  %189 = getelementptr inbounds i16, i16* %182, i64 %15
  %190 = bitcast i16* %189 to <8 x i16>*
  %191 = load <8 x i16>, <8 x i16>* %190, align 1
  %192 = getelementptr inbounds i16, i16* %182, i64 %16
  %193 = bitcast i16* %192 to <8 x i16>*
  %194 = load <8 x i16>, <8 x i16>* %193, align 1
  %195 = getelementptr inbounds i16, i16* %183, i64 %17
  %196 = bitcast i16* %195 to <8 x i16>*
  %197 = load <8 x i16>, <8 x i16>* %196, align 1
  %198 = getelementptr inbounds i16, i16* %183, i64 %18
  %199 = bitcast i16* %198 to <8 x i16>*
  %200 = load <8 x i16>, <8 x i16>* %199, align 1
  %201 = getelementptr inbounds i16, i16* %183, i64 %19
  %202 = bitcast i16* %201 to <8 x i16>*
  %203 = load <8 x i16>, <8 x i16>* %202, align 1
  %204 = bitcast i16* %183 to <8 x i16>*
  %205 = load <8 x i16>, <8 x i16>* %204, align 1
  %206 = add <8 x i16> %205, %185
  %207 = add <8 x i16> %203, %188
  %208 = add <8 x i16> %200, %191
  %209 = add <8 x i16> %197, %194
  %210 = sub <8 x i16> %194, %197
  %211 = sub <8 x i16> %191, %200
  %212 = sub <8 x i16> %188, %203
  %213 = sub <8 x i16> %185, %205
  %214 = shl <8 x i16> %206, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %215 = shl <8 x i16> %207, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %216 = shl <8 x i16> %208, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %217 = shl <8 x i16> %209, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %218 = shl <8 x i16> %210, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %219 = shl <8 x i16> %211, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %220 = shl <8 x i16> %212, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %221 = shl <8 x i16> %213, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %222 = getelementptr inbounds i16, i16* %102, i64 %29
  %223 = getelementptr inbounds i16, i16* %102, i64 %31
  %224 = bitcast i16* %222 to <8 x i16>*
  %225 = load <8 x i16>, <8 x i16>* %224, align 1
  %226 = getelementptr inbounds i16, i16* %222, i64 %14
  %227 = bitcast i16* %226 to <8 x i16>*
  %228 = load <8 x i16>, <8 x i16>* %227, align 1
  %229 = getelementptr inbounds i16, i16* %222, i64 %15
  %230 = bitcast i16* %229 to <8 x i16>*
  %231 = load <8 x i16>, <8 x i16>* %230, align 1
  %232 = getelementptr inbounds i16, i16* %222, i64 %16
  %233 = bitcast i16* %232 to <8 x i16>*
  %234 = load <8 x i16>, <8 x i16>* %233, align 1
  %235 = getelementptr inbounds i16, i16* %223, i64 %17
  %236 = bitcast i16* %235 to <8 x i16>*
  %237 = load <8 x i16>, <8 x i16>* %236, align 1
  %238 = getelementptr inbounds i16, i16* %223, i64 %18
  %239 = bitcast i16* %238 to <8 x i16>*
  %240 = load <8 x i16>, <8 x i16>* %239, align 1
  %241 = getelementptr inbounds i16, i16* %223, i64 %19
  %242 = bitcast i16* %241 to <8 x i16>*
  %243 = load <8 x i16>, <8 x i16>* %242, align 1
  %244 = bitcast i16* %223 to <8 x i16>*
  %245 = load <8 x i16>, <8 x i16>* %244, align 1
  %246 = add <8 x i16> %245, %225
  %247 = add <8 x i16> %243, %228
  %248 = add <8 x i16> %240, %231
  %249 = add <8 x i16> %237, %234
  %250 = sub <8 x i16> %234, %237
  %251 = sub <8 x i16> %231, %240
  %252 = sub <8 x i16> %228, %243
  %253 = sub <8 x i16> %225, %245
  %254 = shl <8 x i16> %246, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %255 = shl <8 x i16> %247, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %256 = shl <8 x i16> %248, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %257 = shl <8 x i16> %249, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %258 = shl <8 x i16> %250, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %259 = shl <8 x i16> %251, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %260 = shl <8 x i16> %252, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %261 = shl <8 x i16> %253, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  br label %1138

262:                                              ; preds = %99
  %263 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %100
  %264 = bitcast i16* %263 to <8 x i16>*
  %265 = load <8 x i16>, <8 x i16>* %264, align 16
  %266 = getelementptr inbounds i16, i16* %263, i64 32
  %267 = bitcast i16* %266 to <8 x i16>*
  %268 = load <8 x i16>, <8 x i16>* %267, align 16
  %269 = getelementptr inbounds i16, i16* %263, i64 64
  %270 = bitcast i16* %269 to <8 x i16>*
  %271 = load <8 x i16>, <8 x i16>* %270, align 16
  %272 = getelementptr inbounds i16, i16* %263, i64 96
  %273 = bitcast i16* %272 to <8 x i16>*
  %274 = load <8 x i16>, <8 x i16>* %273, align 16
  %275 = getelementptr inbounds i16, i16* %263, i64 896
  %276 = bitcast i16* %275 to <8 x i16>*
  %277 = load <8 x i16>, <8 x i16>* %276, align 16
  %278 = getelementptr inbounds i16, i16* %263, i64 928
  %279 = bitcast i16* %278 to <8 x i16>*
  %280 = load <8 x i16>, <8 x i16>* %279, align 16
  %281 = getelementptr inbounds i16, i16* %263, i64 960
  %282 = bitcast i16* %281 to <8 x i16>*
  %283 = load <8 x i16>, <8 x i16>* %282, align 16
  %284 = getelementptr inbounds i16, i16* %263, i64 992
  %285 = bitcast i16* %284 to <8 x i16>*
  %286 = load <8 x i16>, <8 x i16>* %285, align 16
  %287 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %265, <8 x i16> %286) #6
  %288 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %268, <8 x i16> %283) #6
  %289 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %271, <8 x i16> %280) #6
  %290 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %274, <8 x i16> %277) #6
  %291 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %274, <8 x i16> %277) #6
  %292 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %271, <8 x i16> %280) #6
  %293 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %268, <8 x i16> %283) #6
  %294 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %265, <8 x i16> %286) #6
  %295 = add <8 x i16> %287, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %296 = icmp ult <8 x i16> %295, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %297 = add <8 x i16> %288, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %298 = icmp ult <8 x i16> %297, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %299 = add <8 x i16> %289, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %300 = icmp ult <8 x i16> %299, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %301 = add <8 x i16> %290, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %302 = icmp ult <8 x i16> %301, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %303 = or <8 x i1> %300, %302
  %304 = or <8 x i1> %303, %298
  %305 = or <8 x i1> %304, %296
  %306 = sext <8 x i1> %305 to <8 x i16>
  %307 = bitcast <8 x i16> %306 to <16 x i8>
  %308 = icmp slt <16 x i8> %307, zeroinitializer
  %309 = bitcast <16 x i1> %308 to i16
  %310 = zext i16 %309 to i32
  %311 = add <8 x i16> %291, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %312 = icmp ult <8 x i16> %311, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %313 = add <8 x i16> %292, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %314 = icmp ult <8 x i16> %313, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %315 = add <8 x i16> %293, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %316 = icmp ult <8 x i16> %315, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %317 = add <8 x i16> %294, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %318 = icmp ult <8 x i16> %317, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %319 = or <8 x i1> %314, %312
  %320 = or <8 x i1> %319, %316
  %321 = or <8 x i1> %320, %318
  %322 = sext <8 x i1> %321 to <8 x i16>
  %323 = bitcast <8 x i16> %322 to <16 x i8>
  %324 = icmp slt <16 x i8> %323, zeroinitializer
  %325 = bitcast <16 x i1> %324 to i16
  %326 = zext i16 %325 to i32
  %327 = sub nsw i32 0, %310
  %328 = icmp eq i32 %326, %327
  br i1 %328, label %481, label %329

329:                                              ; preds = %262
  %330 = bitcast [32 x i64]* %4 to i8*
  %331 = bitcast [32 x i64]* %5 to i8*
  %332 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %333 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %334 = bitcast [32 x i64]* %5 to <2 x i64>*
  %335 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %336 = bitcast i64* %335 to <2 x i64>*
  %337 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %338 = bitcast i64* %337 to <2 x i64>*
  %339 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %340 = bitcast i64* %339 to <2 x i64>*
  %341 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %342 = bitcast i64* %341 to <2 x i64>*
  %343 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %344 = bitcast i64* %343 to <2 x i64>*
  %345 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %346 = bitcast i64* %345 to <2 x i64>*
  %347 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %348 = bitcast i64* %347 to <2 x i64>*
  %349 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %350 = bitcast i64* %349 to <2 x i64>*
  %351 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %352 = bitcast i64* %351 to <2 x i64>*
  %353 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %354 = bitcast i64* %353 to <2 x i64>*
  %355 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %356 = bitcast i64* %355 to <2 x i64>*
  %357 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %358 = bitcast i64* %357 to <2 x i64>*
  %359 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %360 = bitcast i64* %359 to <2 x i64>*
  %361 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %362 = bitcast i64* %361 to <2 x i64>*
  %363 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %364 = bitcast i64* %363 to <2 x i64>*
  br label %365

365:                                              ; preds = %398, %329
  %366 = phi i64 [ 0, %329 ], [ %479, %398 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %330) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %330, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %331) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %331, i8 -86, i64 256, i1 false) #6
  br label %367

367:                                              ; preds = %367, %365
  %368 = phi i64 [ 0, %365 ], [ %396, %367 ]
  %369 = shl i64 %368, 5
  %370 = add nuw nsw i64 %369, %366
  %371 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %370
  %372 = load i16, i16* %371, align 2
  %373 = sext i16 %372 to i64
  %374 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %368
  store i64 %373, i64* %374, align 16
  %375 = or i64 %368, 1
  %376 = shl i64 %375, 5
  %377 = add nuw nsw i64 %376, %366
  %378 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %377
  %379 = load i16, i16* %378, align 2
  %380 = sext i16 %379 to i64
  %381 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %375
  store i64 %380, i64* %381, align 8
  %382 = or i64 %368, 2
  %383 = shl i64 %382, 5
  %384 = add nuw nsw i64 %383, %366
  %385 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %384
  %386 = load i16, i16* %385, align 2
  %387 = sext i16 %386 to i64
  %388 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %382
  store i64 %387, i64* %388, align 16
  %389 = or i64 %368, 3
  %390 = shl i64 %389, 5
  %391 = add nuw nsw i64 %390, %366
  %392 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %391
  %393 = load i16, i16* %392, align 2
  %394 = sext i16 %393 to i64
  %395 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %389
  store i64 %394, i64* %395, align 8
  %396 = add nuw nsw i64 %368, 4
  %397 = icmp eq i64 %396, 32
  br i1 %397, label %398, label %367

398:                                              ; preds = %367
  call void @vpx_fdct32(i64* nonnull %332, i64* nonnull %333, i32 1) #6
  %399 = shl i64 %366, 5
  %400 = load <2 x i64>, <2 x i64>* %334, align 16
  %401 = trunc <2 x i64> %400 to <2 x i32>
  %402 = getelementptr inbounds i32, i32* %1, i64 %399
  %403 = bitcast i32* %402 to <2 x i32>*
  store <2 x i32> %401, <2 x i32>* %403, align 4
  %404 = load <2 x i64>, <2 x i64>* %336, align 16
  %405 = trunc <2 x i64> %404 to <2 x i32>
  %406 = or i64 %399, 2
  %407 = getelementptr inbounds i32, i32* %1, i64 %406
  %408 = bitcast i32* %407 to <2 x i32>*
  store <2 x i32> %405, <2 x i32>* %408, align 4
  %409 = load <2 x i64>, <2 x i64>* %338, align 16
  %410 = trunc <2 x i64> %409 to <2 x i32>
  %411 = or i64 %399, 4
  %412 = getelementptr inbounds i32, i32* %1, i64 %411
  %413 = bitcast i32* %412 to <2 x i32>*
  store <2 x i32> %410, <2 x i32>* %413, align 4
  %414 = load <2 x i64>, <2 x i64>* %340, align 16
  %415 = trunc <2 x i64> %414 to <2 x i32>
  %416 = or i64 %399, 6
  %417 = getelementptr inbounds i32, i32* %1, i64 %416
  %418 = bitcast i32* %417 to <2 x i32>*
  store <2 x i32> %415, <2 x i32>* %418, align 4
  %419 = load <2 x i64>, <2 x i64>* %342, align 16
  %420 = trunc <2 x i64> %419 to <2 x i32>
  %421 = or i64 %399, 8
  %422 = getelementptr inbounds i32, i32* %1, i64 %421
  %423 = bitcast i32* %422 to <2 x i32>*
  store <2 x i32> %420, <2 x i32>* %423, align 4
  %424 = load <2 x i64>, <2 x i64>* %344, align 16
  %425 = trunc <2 x i64> %424 to <2 x i32>
  %426 = or i64 %399, 10
  %427 = getelementptr inbounds i32, i32* %1, i64 %426
  %428 = bitcast i32* %427 to <2 x i32>*
  store <2 x i32> %425, <2 x i32>* %428, align 4
  %429 = load <2 x i64>, <2 x i64>* %346, align 16
  %430 = trunc <2 x i64> %429 to <2 x i32>
  %431 = or i64 %399, 12
  %432 = getelementptr inbounds i32, i32* %1, i64 %431
  %433 = bitcast i32* %432 to <2 x i32>*
  store <2 x i32> %430, <2 x i32>* %433, align 4
  %434 = load <2 x i64>, <2 x i64>* %348, align 16
  %435 = trunc <2 x i64> %434 to <2 x i32>
  %436 = or i64 %399, 14
  %437 = getelementptr inbounds i32, i32* %1, i64 %436
  %438 = bitcast i32* %437 to <2 x i32>*
  store <2 x i32> %435, <2 x i32>* %438, align 4
  %439 = load <2 x i64>, <2 x i64>* %350, align 16
  %440 = trunc <2 x i64> %439 to <2 x i32>
  %441 = or i64 %399, 16
  %442 = getelementptr inbounds i32, i32* %1, i64 %441
  %443 = bitcast i32* %442 to <2 x i32>*
  store <2 x i32> %440, <2 x i32>* %443, align 4
  %444 = load <2 x i64>, <2 x i64>* %352, align 16
  %445 = trunc <2 x i64> %444 to <2 x i32>
  %446 = or i64 %399, 18
  %447 = getelementptr inbounds i32, i32* %1, i64 %446
  %448 = bitcast i32* %447 to <2 x i32>*
  store <2 x i32> %445, <2 x i32>* %448, align 4
  %449 = load <2 x i64>, <2 x i64>* %354, align 16
  %450 = trunc <2 x i64> %449 to <2 x i32>
  %451 = or i64 %399, 20
  %452 = getelementptr inbounds i32, i32* %1, i64 %451
  %453 = bitcast i32* %452 to <2 x i32>*
  store <2 x i32> %450, <2 x i32>* %453, align 4
  %454 = load <2 x i64>, <2 x i64>* %356, align 16
  %455 = trunc <2 x i64> %454 to <2 x i32>
  %456 = or i64 %399, 22
  %457 = getelementptr inbounds i32, i32* %1, i64 %456
  %458 = bitcast i32* %457 to <2 x i32>*
  store <2 x i32> %455, <2 x i32>* %458, align 4
  %459 = load <2 x i64>, <2 x i64>* %358, align 16
  %460 = trunc <2 x i64> %459 to <2 x i32>
  %461 = or i64 %399, 24
  %462 = getelementptr inbounds i32, i32* %1, i64 %461
  %463 = bitcast i32* %462 to <2 x i32>*
  store <2 x i32> %460, <2 x i32>* %463, align 4
  %464 = load <2 x i64>, <2 x i64>* %360, align 16
  %465 = trunc <2 x i64> %464 to <2 x i32>
  %466 = or i64 %399, 26
  %467 = getelementptr inbounds i32, i32* %1, i64 %466
  %468 = bitcast i32* %467 to <2 x i32>*
  store <2 x i32> %465, <2 x i32>* %468, align 4
  %469 = load <2 x i64>, <2 x i64>* %362, align 16
  %470 = trunc <2 x i64> %469 to <2 x i32>
  %471 = or i64 %399, 28
  %472 = getelementptr inbounds i32, i32* %1, i64 %471
  %473 = bitcast i32* %472 to <2 x i32>*
  store <2 x i32> %470, <2 x i32>* %473, align 4
  %474 = load <2 x i64>, <2 x i64>* %364, align 16
  %475 = trunc <2 x i64> %474 to <2 x i32>
  %476 = or i64 %399, 30
  %477 = getelementptr inbounds i32, i32* %1, i64 %476
  %478 = bitcast i32* %477 to <2 x i32>*
  store <2 x i32> %475, <2 x i32>* %478, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %331) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %330) #6
  %479 = add nuw nsw i64 %366, 1
  %480 = icmp eq i64 %479, 32
  br i1 %480, label %6136, label %365

481:                                              ; preds = %262
  %482 = getelementptr inbounds i16, i16* %263, i64 128
  %483 = bitcast i16* %482 to <8 x i16>*
  %484 = load <8 x i16>, <8 x i16>* %483, align 16
  %485 = getelementptr inbounds i16, i16* %263, i64 160
  %486 = bitcast i16* %485 to <8 x i16>*
  %487 = load <8 x i16>, <8 x i16>* %486, align 16
  %488 = getelementptr inbounds i16, i16* %263, i64 192
  %489 = bitcast i16* %488 to <8 x i16>*
  %490 = load <8 x i16>, <8 x i16>* %489, align 16
  %491 = getelementptr inbounds i16, i16* %263, i64 224
  %492 = bitcast i16* %491 to <8 x i16>*
  %493 = load <8 x i16>, <8 x i16>* %492, align 16
  %494 = getelementptr inbounds i16, i16* %263, i64 768
  %495 = bitcast i16* %494 to <8 x i16>*
  %496 = load <8 x i16>, <8 x i16>* %495, align 16
  %497 = getelementptr inbounds i16, i16* %263, i64 800
  %498 = bitcast i16* %497 to <8 x i16>*
  %499 = load <8 x i16>, <8 x i16>* %498, align 16
  %500 = getelementptr inbounds i16, i16* %263, i64 832
  %501 = bitcast i16* %500 to <8 x i16>*
  %502 = load <8 x i16>, <8 x i16>* %501, align 16
  %503 = getelementptr inbounds i16, i16* %263, i64 864
  %504 = bitcast i16* %503 to <8 x i16>*
  %505 = load <8 x i16>, <8 x i16>* %504, align 16
  %506 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %484, <8 x i16> %505) #6
  %507 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %487, <8 x i16> %502) #6
  %508 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %490, <8 x i16> %499) #6
  %509 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %493, <8 x i16> %496) #6
  %510 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %493, <8 x i16> %496) #6
  %511 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %490, <8 x i16> %499) #6
  %512 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %487, <8 x i16> %502) #6
  %513 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %484, <8 x i16> %505) #6
  %514 = add <8 x i16> %506, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %515 = icmp ult <8 x i16> %514, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %516 = add <8 x i16> %507, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %517 = icmp ult <8 x i16> %516, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %518 = add <8 x i16> %508, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %519 = icmp ult <8 x i16> %518, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %520 = add <8 x i16> %509, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %521 = icmp ult <8 x i16> %520, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %522 = or <8 x i1> %519, %521
  %523 = or <8 x i1> %522, %517
  %524 = or <8 x i1> %523, %515
  %525 = sext <8 x i1> %524 to <8 x i16>
  %526 = bitcast <8 x i16> %525 to <16 x i8>
  %527 = icmp slt <16 x i8> %526, zeroinitializer
  %528 = bitcast <16 x i1> %527 to i16
  %529 = zext i16 %528 to i32
  %530 = add <8 x i16> %510, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %531 = icmp ult <8 x i16> %530, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %532 = add <8 x i16> %511, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %533 = icmp ult <8 x i16> %532, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %534 = add <8 x i16> %512, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %535 = icmp ult <8 x i16> %534, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %536 = add <8 x i16> %513, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %537 = icmp ult <8 x i16> %536, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %538 = or <8 x i1> %533, %531
  %539 = or <8 x i1> %538, %535
  %540 = or <8 x i1> %539, %537
  %541 = sext <8 x i1> %540 to <8 x i16>
  %542 = bitcast <8 x i16> %541 to <16 x i8>
  %543 = icmp slt <16 x i8> %542, zeroinitializer
  %544 = bitcast <16 x i1> %543 to i16
  %545 = zext i16 %544 to i32
  %546 = sub nsw i32 0, %529
  %547 = icmp eq i32 %545, %546
  br i1 %547, label %700, label %548

548:                                              ; preds = %481
  %549 = bitcast [32 x i64]* %4 to i8*
  %550 = bitcast [32 x i64]* %5 to i8*
  %551 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %552 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %553 = bitcast [32 x i64]* %5 to <2 x i64>*
  %554 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %555 = bitcast i64* %554 to <2 x i64>*
  %556 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %557 = bitcast i64* %556 to <2 x i64>*
  %558 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %559 = bitcast i64* %558 to <2 x i64>*
  %560 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %561 = bitcast i64* %560 to <2 x i64>*
  %562 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %563 = bitcast i64* %562 to <2 x i64>*
  %564 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %565 = bitcast i64* %564 to <2 x i64>*
  %566 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %567 = bitcast i64* %566 to <2 x i64>*
  %568 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %569 = bitcast i64* %568 to <2 x i64>*
  %570 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %571 = bitcast i64* %570 to <2 x i64>*
  %572 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %573 = bitcast i64* %572 to <2 x i64>*
  %574 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %575 = bitcast i64* %574 to <2 x i64>*
  %576 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %577 = bitcast i64* %576 to <2 x i64>*
  %578 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %579 = bitcast i64* %578 to <2 x i64>*
  %580 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %581 = bitcast i64* %580 to <2 x i64>*
  %582 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %583 = bitcast i64* %582 to <2 x i64>*
  br label %584

584:                                              ; preds = %617, %548
  %585 = phi i64 [ 0, %548 ], [ %698, %617 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %549) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %549, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %550) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %550, i8 -86, i64 256, i1 false) #6
  br label %586

586:                                              ; preds = %586, %584
  %587 = phi i64 [ 0, %584 ], [ %615, %586 ]
  %588 = shl i64 %587, 5
  %589 = add nuw nsw i64 %588, %585
  %590 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %589
  %591 = load i16, i16* %590, align 2
  %592 = sext i16 %591 to i64
  %593 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %587
  store i64 %592, i64* %593, align 16
  %594 = or i64 %587, 1
  %595 = shl i64 %594, 5
  %596 = add nuw nsw i64 %595, %585
  %597 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %596
  %598 = load i16, i16* %597, align 2
  %599 = sext i16 %598 to i64
  %600 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %594
  store i64 %599, i64* %600, align 8
  %601 = or i64 %587, 2
  %602 = shl i64 %601, 5
  %603 = add nuw nsw i64 %602, %585
  %604 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %603
  %605 = load i16, i16* %604, align 2
  %606 = sext i16 %605 to i64
  %607 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %601
  store i64 %606, i64* %607, align 16
  %608 = or i64 %587, 3
  %609 = shl i64 %608, 5
  %610 = add nuw nsw i64 %609, %585
  %611 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %610
  %612 = load i16, i16* %611, align 2
  %613 = sext i16 %612 to i64
  %614 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %608
  store i64 %613, i64* %614, align 8
  %615 = add nuw nsw i64 %587, 4
  %616 = icmp eq i64 %615, 32
  br i1 %616, label %617, label %586

617:                                              ; preds = %586
  call void @vpx_fdct32(i64* nonnull %551, i64* nonnull %552, i32 1) #6
  %618 = shl i64 %585, 5
  %619 = load <2 x i64>, <2 x i64>* %553, align 16
  %620 = trunc <2 x i64> %619 to <2 x i32>
  %621 = getelementptr inbounds i32, i32* %1, i64 %618
  %622 = bitcast i32* %621 to <2 x i32>*
  store <2 x i32> %620, <2 x i32>* %622, align 4
  %623 = load <2 x i64>, <2 x i64>* %555, align 16
  %624 = trunc <2 x i64> %623 to <2 x i32>
  %625 = or i64 %618, 2
  %626 = getelementptr inbounds i32, i32* %1, i64 %625
  %627 = bitcast i32* %626 to <2 x i32>*
  store <2 x i32> %624, <2 x i32>* %627, align 4
  %628 = load <2 x i64>, <2 x i64>* %557, align 16
  %629 = trunc <2 x i64> %628 to <2 x i32>
  %630 = or i64 %618, 4
  %631 = getelementptr inbounds i32, i32* %1, i64 %630
  %632 = bitcast i32* %631 to <2 x i32>*
  store <2 x i32> %629, <2 x i32>* %632, align 4
  %633 = load <2 x i64>, <2 x i64>* %559, align 16
  %634 = trunc <2 x i64> %633 to <2 x i32>
  %635 = or i64 %618, 6
  %636 = getelementptr inbounds i32, i32* %1, i64 %635
  %637 = bitcast i32* %636 to <2 x i32>*
  store <2 x i32> %634, <2 x i32>* %637, align 4
  %638 = load <2 x i64>, <2 x i64>* %561, align 16
  %639 = trunc <2 x i64> %638 to <2 x i32>
  %640 = or i64 %618, 8
  %641 = getelementptr inbounds i32, i32* %1, i64 %640
  %642 = bitcast i32* %641 to <2 x i32>*
  store <2 x i32> %639, <2 x i32>* %642, align 4
  %643 = load <2 x i64>, <2 x i64>* %563, align 16
  %644 = trunc <2 x i64> %643 to <2 x i32>
  %645 = or i64 %618, 10
  %646 = getelementptr inbounds i32, i32* %1, i64 %645
  %647 = bitcast i32* %646 to <2 x i32>*
  store <2 x i32> %644, <2 x i32>* %647, align 4
  %648 = load <2 x i64>, <2 x i64>* %565, align 16
  %649 = trunc <2 x i64> %648 to <2 x i32>
  %650 = or i64 %618, 12
  %651 = getelementptr inbounds i32, i32* %1, i64 %650
  %652 = bitcast i32* %651 to <2 x i32>*
  store <2 x i32> %649, <2 x i32>* %652, align 4
  %653 = load <2 x i64>, <2 x i64>* %567, align 16
  %654 = trunc <2 x i64> %653 to <2 x i32>
  %655 = or i64 %618, 14
  %656 = getelementptr inbounds i32, i32* %1, i64 %655
  %657 = bitcast i32* %656 to <2 x i32>*
  store <2 x i32> %654, <2 x i32>* %657, align 4
  %658 = load <2 x i64>, <2 x i64>* %569, align 16
  %659 = trunc <2 x i64> %658 to <2 x i32>
  %660 = or i64 %618, 16
  %661 = getelementptr inbounds i32, i32* %1, i64 %660
  %662 = bitcast i32* %661 to <2 x i32>*
  store <2 x i32> %659, <2 x i32>* %662, align 4
  %663 = load <2 x i64>, <2 x i64>* %571, align 16
  %664 = trunc <2 x i64> %663 to <2 x i32>
  %665 = or i64 %618, 18
  %666 = getelementptr inbounds i32, i32* %1, i64 %665
  %667 = bitcast i32* %666 to <2 x i32>*
  store <2 x i32> %664, <2 x i32>* %667, align 4
  %668 = load <2 x i64>, <2 x i64>* %573, align 16
  %669 = trunc <2 x i64> %668 to <2 x i32>
  %670 = or i64 %618, 20
  %671 = getelementptr inbounds i32, i32* %1, i64 %670
  %672 = bitcast i32* %671 to <2 x i32>*
  store <2 x i32> %669, <2 x i32>* %672, align 4
  %673 = load <2 x i64>, <2 x i64>* %575, align 16
  %674 = trunc <2 x i64> %673 to <2 x i32>
  %675 = or i64 %618, 22
  %676 = getelementptr inbounds i32, i32* %1, i64 %675
  %677 = bitcast i32* %676 to <2 x i32>*
  store <2 x i32> %674, <2 x i32>* %677, align 4
  %678 = load <2 x i64>, <2 x i64>* %577, align 16
  %679 = trunc <2 x i64> %678 to <2 x i32>
  %680 = or i64 %618, 24
  %681 = getelementptr inbounds i32, i32* %1, i64 %680
  %682 = bitcast i32* %681 to <2 x i32>*
  store <2 x i32> %679, <2 x i32>* %682, align 4
  %683 = load <2 x i64>, <2 x i64>* %579, align 16
  %684 = trunc <2 x i64> %683 to <2 x i32>
  %685 = or i64 %618, 26
  %686 = getelementptr inbounds i32, i32* %1, i64 %685
  %687 = bitcast i32* %686 to <2 x i32>*
  store <2 x i32> %684, <2 x i32>* %687, align 4
  %688 = load <2 x i64>, <2 x i64>* %581, align 16
  %689 = trunc <2 x i64> %688 to <2 x i32>
  %690 = or i64 %618, 28
  %691 = getelementptr inbounds i32, i32* %1, i64 %690
  %692 = bitcast i32* %691 to <2 x i32>*
  store <2 x i32> %689, <2 x i32>* %692, align 4
  %693 = load <2 x i64>, <2 x i64>* %583, align 16
  %694 = trunc <2 x i64> %693 to <2 x i32>
  %695 = or i64 %618, 30
  %696 = getelementptr inbounds i32, i32* %1, i64 %695
  %697 = bitcast i32* %696 to <2 x i32>*
  store <2 x i32> %694, <2 x i32>* %697, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %550) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %549) #6
  %698 = add nuw nsw i64 %585, 1
  %699 = icmp eq i64 %698, 32
  br i1 %699, label %6136, label %584

700:                                              ; preds = %481
  %701 = getelementptr inbounds i16, i16* %263, i64 256
  %702 = bitcast i16* %701 to <8 x i16>*
  %703 = load <8 x i16>, <8 x i16>* %702, align 16
  %704 = getelementptr inbounds i16, i16* %263, i64 288
  %705 = bitcast i16* %704 to <8 x i16>*
  %706 = load <8 x i16>, <8 x i16>* %705, align 16
  %707 = getelementptr inbounds i16, i16* %263, i64 320
  %708 = bitcast i16* %707 to <8 x i16>*
  %709 = load <8 x i16>, <8 x i16>* %708, align 16
  %710 = getelementptr inbounds i16, i16* %263, i64 352
  %711 = bitcast i16* %710 to <8 x i16>*
  %712 = load <8 x i16>, <8 x i16>* %711, align 16
  %713 = getelementptr inbounds i16, i16* %263, i64 640
  %714 = bitcast i16* %713 to <8 x i16>*
  %715 = load <8 x i16>, <8 x i16>* %714, align 16
  %716 = getelementptr inbounds i16, i16* %263, i64 672
  %717 = bitcast i16* %716 to <8 x i16>*
  %718 = load <8 x i16>, <8 x i16>* %717, align 16
  %719 = getelementptr inbounds i16, i16* %263, i64 704
  %720 = bitcast i16* %719 to <8 x i16>*
  %721 = load <8 x i16>, <8 x i16>* %720, align 16
  %722 = getelementptr inbounds i16, i16* %263, i64 736
  %723 = bitcast i16* %722 to <8 x i16>*
  %724 = load <8 x i16>, <8 x i16>* %723, align 16
  %725 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %703, <8 x i16> %724) #6
  %726 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %706, <8 x i16> %721) #6
  %727 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %709, <8 x i16> %718) #6
  %728 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %712, <8 x i16> %715) #6
  %729 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %712, <8 x i16> %715) #6
  %730 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %709, <8 x i16> %718) #6
  %731 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %706, <8 x i16> %721) #6
  %732 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %703, <8 x i16> %724) #6
  %733 = add <8 x i16> %725, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %734 = icmp ult <8 x i16> %733, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %735 = add <8 x i16> %726, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %736 = icmp ult <8 x i16> %735, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %737 = add <8 x i16> %727, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %738 = icmp ult <8 x i16> %737, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %739 = add <8 x i16> %728, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %740 = icmp ult <8 x i16> %739, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %741 = or <8 x i1> %738, %740
  %742 = or <8 x i1> %741, %736
  %743 = or <8 x i1> %742, %734
  %744 = sext <8 x i1> %743 to <8 x i16>
  %745 = bitcast <8 x i16> %744 to <16 x i8>
  %746 = icmp slt <16 x i8> %745, zeroinitializer
  %747 = bitcast <16 x i1> %746 to i16
  %748 = zext i16 %747 to i32
  %749 = add <8 x i16> %729, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %750 = icmp ult <8 x i16> %749, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %751 = add <8 x i16> %730, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %752 = icmp ult <8 x i16> %751, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %753 = add <8 x i16> %731, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %754 = icmp ult <8 x i16> %753, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %755 = add <8 x i16> %732, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %756 = icmp ult <8 x i16> %755, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %757 = or <8 x i1> %752, %750
  %758 = or <8 x i1> %757, %754
  %759 = or <8 x i1> %758, %756
  %760 = sext <8 x i1> %759 to <8 x i16>
  %761 = bitcast <8 x i16> %760 to <16 x i8>
  %762 = icmp slt <16 x i8> %761, zeroinitializer
  %763 = bitcast <16 x i1> %762 to i16
  %764 = zext i16 %763 to i32
  %765 = sub nsw i32 0, %748
  %766 = icmp eq i32 %764, %765
  br i1 %766, label %919, label %767

767:                                              ; preds = %700
  %768 = bitcast [32 x i64]* %4 to i8*
  %769 = bitcast [32 x i64]* %5 to i8*
  %770 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %771 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %772 = bitcast [32 x i64]* %5 to <2 x i64>*
  %773 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %774 = bitcast i64* %773 to <2 x i64>*
  %775 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %776 = bitcast i64* %775 to <2 x i64>*
  %777 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %778 = bitcast i64* %777 to <2 x i64>*
  %779 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %780 = bitcast i64* %779 to <2 x i64>*
  %781 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %782 = bitcast i64* %781 to <2 x i64>*
  %783 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %784 = bitcast i64* %783 to <2 x i64>*
  %785 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %786 = bitcast i64* %785 to <2 x i64>*
  %787 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %788 = bitcast i64* %787 to <2 x i64>*
  %789 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %790 = bitcast i64* %789 to <2 x i64>*
  %791 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %792 = bitcast i64* %791 to <2 x i64>*
  %793 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %794 = bitcast i64* %793 to <2 x i64>*
  %795 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %796 = bitcast i64* %795 to <2 x i64>*
  %797 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %798 = bitcast i64* %797 to <2 x i64>*
  %799 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %800 = bitcast i64* %799 to <2 x i64>*
  %801 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %802 = bitcast i64* %801 to <2 x i64>*
  br label %803

803:                                              ; preds = %836, %767
  %804 = phi i64 [ 0, %767 ], [ %917, %836 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %768) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %768, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %769) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %769, i8 -86, i64 256, i1 false) #6
  br label %805

805:                                              ; preds = %805, %803
  %806 = phi i64 [ 0, %803 ], [ %834, %805 ]
  %807 = shl i64 %806, 5
  %808 = add nuw nsw i64 %807, %804
  %809 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %808
  %810 = load i16, i16* %809, align 2
  %811 = sext i16 %810 to i64
  %812 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %806
  store i64 %811, i64* %812, align 16
  %813 = or i64 %806, 1
  %814 = shl i64 %813, 5
  %815 = add nuw nsw i64 %814, %804
  %816 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %815
  %817 = load i16, i16* %816, align 2
  %818 = sext i16 %817 to i64
  %819 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %813
  store i64 %818, i64* %819, align 8
  %820 = or i64 %806, 2
  %821 = shl i64 %820, 5
  %822 = add nuw nsw i64 %821, %804
  %823 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %822
  %824 = load i16, i16* %823, align 2
  %825 = sext i16 %824 to i64
  %826 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %820
  store i64 %825, i64* %826, align 16
  %827 = or i64 %806, 3
  %828 = shl i64 %827, 5
  %829 = add nuw nsw i64 %828, %804
  %830 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %829
  %831 = load i16, i16* %830, align 2
  %832 = sext i16 %831 to i64
  %833 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %827
  store i64 %832, i64* %833, align 8
  %834 = add nuw nsw i64 %806, 4
  %835 = icmp eq i64 %834, 32
  br i1 %835, label %836, label %805

836:                                              ; preds = %805
  call void @vpx_fdct32(i64* nonnull %770, i64* nonnull %771, i32 1) #6
  %837 = shl i64 %804, 5
  %838 = load <2 x i64>, <2 x i64>* %772, align 16
  %839 = trunc <2 x i64> %838 to <2 x i32>
  %840 = getelementptr inbounds i32, i32* %1, i64 %837
  %841 = bitcast i32* %840 to <2 x i32>*
  store <2 x i32> %839, <2 x i32>* %841, align 4
  %842 = load <2 x i64>, <2 x i64>* %774, align 16
  %843 = trunc <2 x i64> %842 to <2 x i32>
  %844 = or i64 %837, 2
  %845 = getelementptr inbounds i32, i32* %1, i64 %844
  %846 = bitcast i32* %845 to <2 x i32>*
  store <2 x i32> %843, <2 x i32>* %846, align 4
  %847 = load <2 x i64>, <2 x i64>* %776, align 16
  %848 = trunc <2 x i64> %847 to <2 x i32>
  %849 = or i64 %837, 4
  %850 = getelementptr inbounds i32, i32* %1, i64 %849
  %851 = bitcast i32* %850 to <2 x i32>*
  store <2 x i32> %848, <2 x i32>* %851, align 4
  %852 = load <2 x i64>, <2 x i64>* %778, align 16
  %853 = trunc <2 x i64> %852 to <2 x i32>
  %854 = or i64 %837, 6
  %855 = getelementptr inbounds i32, i32* %1, i64 %854
  %856 = bitcast i32* %855 to <2 x i32>*
  store <2 x i32> %853, <2 x i32>* %856, align 4
  %857 = load <2 x i64>, <2 x i64>* %780, align 16
  %858 = trunc <2 x i64> %857 to <2 x i32>
  %859 = or i64 %837, 8
  %860 = getelementptr inbounds i32, i32* %1, i64 %859
  %861 = bitcast i32* %860 to <2 x i32>*
  store <2 x i32> %858, <2 x i32>* %861, align 4
  %862 = load <2 x i64>, <2 x i64>* %782, align 16
  %863 = trunc <2 x i64> %862 to <2 x i32>
  %864 = or i64 %837, 10
  %865 = getelementptr inbounds i32, i32* %1, i64 %864
  %866 = bitcast i32* %865 to <2 x i32>*
  store <2 x i32> %863, <2 x i32>* %866, align 4
  %867 = load <2 x i64>, <2 x i64>* %784, align 16
  %868 = trunc <2 x i64> %867 to <2 x i32>
  %869 = or i64 %837, 12
  %870 = getelementptr inbounds i32, i32* %1, i64 %869
  %871 = bitcast i32* %870 to <2 x i32>*
  store <2 x i32> %868, <2 x i32>* %871, align 4
  %872 = load <2 x i64>, <2 x i64>* %786, align 16
  %873 = trunc <2 x i64> %872 to <2 x i32>
  %874 = or i64 %837, 14
  %875 = getelementptr inbounds i32, i32* %1, i64 %874
  %876 = bitcast i32* %875 to <2 x i32>*
  store <2 x i32> %873, <2 x i32>* %876, align 4
  %877 = load <2 x i64>, <2 x i64>* %788, align 16
  %878 = trunc <2 x i64> %877 to <2 x i32>
  %879 = or i64 %837, 16
  %880 = getelementptr inbounds i32, i32* %1, i64 %879
  %881 = bitcast i32* %880 to <2 x i32>*
  store <2 x i32> %878, <2 x i32>* %881, align 4
  %882 = load <2 x i64>, <2 x i64>* %790, align 16
  %883 = trunc <2 x i64> %882 to <2 x i32>
  %884 = or i64 %837, 18
  %885 = getelementptr inbounds i32, i32* %1, i64 %884
  %886 = bitcast i32* %885 to <2 x i32>*
  store <2 x i32> %883, <2 x i32>* %886, align 4
  %887 = load <2 x i64>, <2 x i64>* %792, align 16
  %888 = trunc <2 x i64> %887 to <2 x i32>
  %889 = or i64 %837, 20
  %890 = getelementptr inbounds i32, i32* %1, i64 %889
  %891 = bitcast i32* %890 to <2 x i32>*
  store <2 x i32> %888, <2 x i32>* %891, align 4
  %892 = load <2 x i64>, <2 x i64>* %794, align 16
  %893 = trunc <2 x i64> %892 to <2 x i32>
  %894 = or i64 %837, 22
  %895 = getelementptr inbounds i32, i32* %1, i64 %894
  %896 = bitcast i32* %895 to <2 x i32>*
  store <2 x i32> %893, <2 x i32>* %896, align 4
  %897 = load <2 x i64>, <2 x i64>* %796, align 16
  %898 = trunc <2 x i64> %897 to <2 x i32>
  %899 = or i64 %837, 24
  %900 = getelementptr inbounds i32, i32* %1, i64 %899
  %901 = bitcast i32* %900 to <2 x i32>*
  store <2 x i32> %898, <2 x i32>* %901, align 4
  %902 = load <2 x i64>, <2 x i64>* %798, align 16
  %903 = trunc <2 x i64> %902 to <2 x i32>
  %904 = or i64 %837, 26
  %905 = getelementptr inbounds i32, i32* %1, i64 %904
  %906 = bitcast i32* %905 to <2 x i32>*
  store <2 x i32> %903, <2 x i32>* %906, align 4
  %907 = load <2 x i64>, <2 x i64>* %800, align 16
  %908 = trunc <2 x i64> %907 to <2 x i32>
  %909 = or i64 %837, 28
  %910 = getelementptr inbounds i32, i32* %1, i64 %909
  %911 = bitcast i32* %910 to <2 x i32>*
  store <2 x i32> %908, <2 x i32>* %911, align 4
  %912 = load <2 x i64>, <2 x i64>* %802, align 16
  %913 = trunc <2 x i64> %912 to <2 x i32>
  %914 = or i64 %837, 30
  %915 = getelementptr inbounds i32, i32* %1, i64 %914
  %916 = bitcast i32* %915 to <2 x i32>*
  store <2 x i32> %913, <2 x i32>* %916, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %769) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %768) #6
  %917 = add nuw nsw i64 %804, 1
  %918 = icmp eq i64 %917, 32
  br i1 %918, label %6136, label %803

919:                                              ; preds = %700
  %920 = getelementptr inbounds i16, i16* %263, i64 384
  %921 = bitcast i16* %920 to <8 x i16>*
  %922 = load <8 x i16>, <8 x i16>* %921, align 16
  %923 = getelementptr inbounds i16, i16* %263, i64 416
  %924 = bitcast i16* %923 to <8 x i16>*
  %925 = load <8 x i16>, <8 x i16>* %924, align 16
  %926 = getelementptr inbounds i16, i16* %263, i64 448
  %927 = bitcast i16* %926 to <8 x i16>*
  %928 = load <8 x i16>, <8 x i16>* %927, align 16
  %929 = getelementptr inbounds i16, i16* %263, i64 480
  %930 = bitcast i16* %929 to <8 x i16>*
  %931 = load <8 x i16>, <8 x i16>* %930, align 16
  %932 = getelementptr inbounds i16, i16* %263, i64 512
  %933 = bitcast i16* %932 to <8 x i16>*
  %934 = load <8 x i16>, <8 x i16>* %933, align 16
  %935 = getelementptr inbounds i16, i16* %263, i64 544
  %936 = bitcast i16* %935 to <8 x i16>*
  %937 = load <8 x i16>, <8 x i16>* %936, align 16
  %938 = getelementptr inbounds i16, i16* %263, i64 576
  %939 = bitcast i16* %938 to <8 x i16>*
  %940 = load <8 x i16>, <8 x i16>* %939, align 16
  %941 = getelementptr inbounds i16, i16* %263, i64 608
  %942 = bitcast i16* %941 to <8 x i16>*
  %943 = load <8 x i16>, <8 x i16>* %942, align 16
  %944 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %922, <8 x i16> %943) #6
  %945 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %925, <8 x i16> %940) #6
  %946 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %928, <8 x i16> %937) #6
  %947 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %931, <8 x i16> %934) #6
  %948 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %931, <8 x i16> %934) #6
  %949 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %928, <8 x i16> %937) #6
  %950 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %925, <8 x i16> %940) #6
  %951 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %922, <8 x i16> %943) #6
  %952 = add <8 x i16> %944, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %953 = icmp ult <8 x i16> %952, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %954 = add <8 x i16> %945, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %955 = icmp ult <8 x i16> %954, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %956 = add <8 x i16> %946, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %957 = icmp ult <8 x i16> %956, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %958 = add <8 x i16> %947, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %959 = icmp ult <8 x i16> %958, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %960 = or <8 x i1> %957, %959
  %961 = or <8 x i1> %960, %955
  %962 = or <8 x i1> %961, %953
  %963 = sext <8 x i1> %962 to <8 x i16>
  %964 = bitcast <8 x i16> %963 to <16 x i8>
  %965 = icmp slt <16 x i8> %964, zeroinitializer
  %966 = bitcast <16 x i1> %965 to i16
  %967 = zext i16 %966 to i32
  %968 = add <8 x i16> %948, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %969 = icmp ult <8 x i16> %968, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %970 = add <8 x i16> %949, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %971 = icmp ult <8 x i16> %970, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %972 = add <8 x i16> %950, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %973 = icmp ult <8 x i16> %972, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %974 = add <8 x i16> %951, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %975 = icmp ult <8 x i16> %974, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %976 = or <8 x i1> %971, %969
  %977 = or <8 x i1> %976, %973
  %978 = or <8 x i1> %977, %975
  %979 = sext <8 x i1> %978 to <8 x i16>
  %980 = bitcast <8 x i16> %979 to <16 x i8>
  %981 = icmp slt <16 x i8> %980, zeroinitializer
  %982 = bitcast <16 x i1> %981 to i16
  %983 = zext i16 %982 to i32
  %984 = sub nsw i32 0, %967
  %985 = icmp eq i32 %983, %984
  br i1 %985, label %1138, label %986

986:                                              ; preds = %919
  %987 = bitcast [32 x i64]* %4 to i8*
  %988 = bitcast [32 x i64]* %5 to i8*
  %989 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %990 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %991 = bitcast [32 x i64]* %5 to <2 x i64>*
  %992 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %993 = bitcast i64* %992 to <2 x i64>*
  %994 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %995 = bitcast i64* %994 to <2 x i64>*
  %996 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %997 = bitcast i64* %996 to <2 x i64>*
  %998 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %999 = bitcast i64* %998 to <2 x i64>*
  %1000 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %1001 = bitcast i64* %1000 to <2 x i64>*
  %1002 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %1003 = bitcast i64* %1002 to <2 x i64>*
  %1004 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %1005 = bitcast i64* %1004 to <2 x i64>*
  %1006 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %1007 = bitcast i64* %1006 to <2 x i64>*
  %1008 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %1009 = bitcast i64* %1008 to <2 x i64>*
  %1010 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %1011 = bitcast i64* %1010 to <2 x i64>*
  %1012 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %1013 = bitcast i64* %1012 to <2 x i64>*
  %1014 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %1015 = bitcast i64* %1014 to <2 x i64>*
  %1016 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %1017 = bitcast i64* %1016 to <2 x i64>*
  %1018 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %1019 = bitcast i64* %1018 to <2 x i64>*
  %1020 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %1021 = bitcast i64* %1020 to <2 x i64>*
  br label %1022

1022:                                             ; preds = %1055, %986
  %1023 = phi i64 [ 0, %986 ], [ %1136, %1055 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %987) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %987, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %988) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %988, i8 -86, i64 256, i1 false) #6
  br label %1024

1024:                                             ; preds = %1024, %1022
  %1025 = phi i64 [ 0, %1022 ], [ %1053, %1024 ]
  %1026 = shl i64 %1025, 5
  %1027 = add nuw nsw i64 %1026, %1023
  %1028 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %1027
  %1029 = load i16, i16* %1028, align 2
  %1030 = sext i16 %1029 to i64
  %1031 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %1025
  store i64 %1030, i64* %1031, align 16
  %1032 = or i64 %1025, 1
  %1033 = shl i64 %1032, 5
  %1034 = add nuw nsw i64 %1033, %1023
  %1035 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %1034
  %1036 = load i16, i16* %1035, align 2
  %1037 = sext i16 %1036 to i64
  %1038 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %1032
  store i64 %1037, i64* %1038, align 8
  %1039 = or i64 %1025, 2
  %1040 = shl i64 %1039, 5
  %1041 = add nuw nsw i64 %1040, %1023
  %1042 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %1041
  %1043 = load i16, i16* %1042, align 2
  %1044 = sext i16 %1043 to i64
  %1045 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %1039
  store i64 %1044, i64* %1045, align 16
  %1046 = or i64 %1025, 3
  %1047 = shl i64 %1046, 5
  %1048 = add nuw nsw i64 %1047, %1023
  %1049 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %1048
  %1050 = load i16, i16* %1049, align 2
  %1051 = sext i16 %1050 to i64
  %1052 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %1046
  store i64 %1051, i64* %1052, align 8
  %1053 = add nuw nsw i64 %1025, 4
  %1054 = icmp eq i64 %1053, 32
  br i1 %1054, label %1055, label %1024

1055:                                             ; preds = %1024
  call void @vpx_fdct32(i64* nonnull %989, i64* nonnull %990, i32 1) #6
  %1056 = shl i64 %1023, 5
  %1057 = load <2 x i64>, <2 x i64>* %991, align 16
  %1058 = trunc <2 x i64> %1057 to <2 x i32>
  %1059 = getelementptr inbounds i32, i32* %1, i64 %1056
  %1060 = bitcast i32* %1059 to <2 x i32>*
  store <2 x i32> %1058, <2 x i32>* %1060, align 4
  %1061 = load <2 x i64>, <2 x i64>* %993, align 16
  %1062 = trunc <2 x i64> %1061 to <2 x i32>
  %1063 = or i64 %1056, 2
  %1064 = getelementptr inbounds i32, i32* %1, i64 %1063
  %1065 = bitcast i32* %1064 to <2 x i32>*
  store <2 x i32> %1062, <2 x i32>* %1065, align 4
  %1066 = load <2 x i64>, <2 x i64>* %995, align 16
  %1067 = trunc <2 x i64> %1066 to <2 x i32>
  %1068 = or i64 %1056, 4
  %1069 = getelementptr inbounds i32, i32* %1, i64 %1068
  %1070 = bitcast i32* %1069 to <2 x i32>*
  store <2 x i32> %1067, <2 x i32>* %1070, align 4
  %1071 = load <2 x i64>, <2 x i64>* %997, align 16
  %1072 = trunc <2 x i64> %1071 to <2 x i32>
  %1073 = or i64 %1056, 6
  %1074 = getelementptr inbounds i32, i32* %1, i64 %1073
  %1075 = bitcast i32* %1074 to <2 x i32>*
  store <2 x i32> %1072, <2 x i32>* %1075, align 4
  %1076 = load <2 x i64>, <2 x i64>* %999, align 16
  %1077 = trunc <2 x i64> %1076 to <2 x i32>
  %1078 = or i64 %1056, 8
  %1079 = getelementptr inbounds i32, i32* %1, i64 %1078
  %1080 = bitcast i32* %1079 to <2 x i32>*
  store <2 x i32> %1077, <2 x i32>* %1080, align 4
  %1081 = load <2 x i64>, <2 x i64>* %1001, align 16
  %1082 = trunc <2 x i64> %1081 to <2 x i32>
  %1083 = or i64 %1056, 10
  %1084 = getelementptr inbounds i32, i32* %1, i64 %1083
  %1085 = bitcast i32* %1084 to <2 x i32>*
  store <2 x i32> %1082, <2 x i32>* %1085, align 4
  %1086 = load <2 x i64>, <2 x i64>* %1003, align 16
  %1087 = trunc <2 x i64> %1086 to <2 x i32>
  %1088 = or i64 %1056, 12
  %1089 = getelementptr inbounds i32, i32* %1, i64 %1088
  %1090 = bitcast i32* %1089 to <2 x i32>*
  store <2 x i32> %1087, <2 x i32>* %1090, align 4
  %1091 = load <2 x i64>, <2 x i64>* %1005, align 16
  %1092 = trunc <2 x i64> %1091 to <2 x i32>
  %1093 = or i64 %1056, 14
  %1094 = getelementptr inbounds i32, i32* %1, i64 %1093
  %1095 = bitcast i32* %1094 to <2 x i32>*
  store <2 x i32> %1092, <2 x i32>* %1095, align 4
  %1096 = load <2 x i64>, <2 x i64>* %1007, align 16
  %1097 = trunc <2 x i64> %1096 to <2 x i32>
  %1098 = or i64 %1056, 16
  %1099 = getelementptr inbounds i32, i32* %1, i64 %1098
  %1100 = bitcast i32* %1099 to <2 x i32>*
  store <2 x i32> %1097, <2 x i32>* %1100, align 4
  %1101 = load <2 x i64>, <2 x i64>* %1009, align 16
  %1102 = trunc <2 x i64> %1101 to <2 x i32>
  %1103 = or i64 %1056, 18
  %1104 = getelementptr inbounds i32, i32* %1, i64 %1103
  %1105 = bitcast i32* %1104 to <2 x i32>*
  store <2 x i32> %1102, <2 x i32>* %1105, align 4
  %1106 = load <2 x i64>, <2 x i64>* %1011, align 16
  %1107 = trunc <2 x i64> %1106 to <2 x i32>
  %1108 = or i64 %1056, 20
  %1109 = getelementptr inbounds i32, i32* %1, i64 %1108
  %1110 = bitcast i32* %1109 to <2 x i32>*
  store <2 x i32> %1107, <2 x i32>* %1110, align 4
  %1111 = load <2 x i64>, <2 x i64>* %1013, align 16
  %1112 = trunc <2 x i64> %1111 to <2 x i32>
  %1113 = or i64 %1056, 22
  %1114 = getelementptr inbounds i32, i32* %1, i64 %1113
  %1115 = bitcast i32* %1114 to <2 x i32>*
  store <2 x i32> %1112, <2 x i32>* %1115, align 4
  %1116 = load <2 x i64>, <2 x i64>* %1015, align 16
  %1117 = trunc <2 x i64> %1116 to <2 x i32>
  %1118 = or i64 %1056, 24
  %1119 = getelementptr inbounds i32, i32* %1, i64 %1118
  %1120 = bitcast i32* %1119 to <2 x i32>*
  store <2 x i32> %1117, <2 x i32>* %1120, align 4
  %1121 = load <2 x i64>, <2 x i64>* %1017, align 16
  %1122 = trunc <2 x i64> %1121 to <2 x i32>
  %1123 = or i64 %1056, 26
  %1124 = getelementptr inbounds i32, i32* %1, i64 %1123
  %1125 = bitcast i32* %1124 to <2 x i32>*
  store <2 x i32> %1122, <2 x i32>* %1125, align 4
  %1126 = load <2 x i64>, <2 x i64>* %1019, align 16
  %1127 = trunc <2 x i64> %1126 to <2 x i32>
  %1128 = or i64 %1056, 28
  %1129 = getelementptr inbounds i32, i32* %1, i64 %1128
  %1130 = bitcast i32* %1129 to <2 x i32>*
  store <2 x i32> %1127, <2 x i32>* %1130, align 4
  %1131 = load <2 x i64>, <2 x i64>* %1021, align 16
  %1132 = trunc <2 x i64> %1131 to <2 x i32>
  %1133 = or i64 %1056, 30
  %1134 = getelementptr inbounds i32, i32* %1, i64 %1133
  %1135 = bitcast i32* %1134 to <2 x i32>*
  store <2 x i32> %1132, <2 x i32>* %1135, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %988) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %987) #6
  %1136 = add nuw nsw i64 %1023, 1
  %1137 = icmp eq i64 %1136, 32
  br i1 %1137, label %6136, label %1022

1138:                                             ; preds = %919, %101
  %1139 = phi <8 x i16> [ %141, %101 ], [ %294, %919 ]
  %1140 = phi <8 x i16> [ %140, %101 ], [ %293, %919 ]
  %1141 = phi <8 x i16> [ %139, %101 ], [ %292, %919 ]
  %1142 = phi <8 x i16> [ %138, %101 ], [ %291, %919 ]
  %1143 = phi <8 x i16> [ %181, %101 ], [ %513, %919 ]
  %1144 = phi <8 x i16> [ %180, %101 ], [ %512, %919 ]
  %1145 = phi <8 x i16> [ %179, %101 ], [ %511, %919 ]
  %1146 = phi <8 x i16> [ %178, %101 ], [ %510, %919 ]
  %1147 = phi <8 x i16> [ %221, %101 ], [ %732, %919 ]
  %1148 = phi <8 x i16> [ %220, %101 ], [ %731, %919 ]
  %1149 = phi <8 x i16> [ %219, %101 ], [ %730, %919 ]
  %1150 = phi <8 x i16> [ %218, %101 ], [ %729, %919 ]
  %1151 = phi <8 x i16> [ %261, %101 ], [ %951, %919 ]
  %1152 = phi <8 x i16> [ %260, %101 ], [ %950, %919 ]
  %1153 = phi <8 x i16> [ %259, %101 ], [ %949, %919 ]
  %1154 = phi <8 x i16> [ %258, %101 ], [ %948, %919 ]
  %1155 = phi <8 x i16> [ %257, %101 ], [ %947, %919 ]
  %1156 = phi <8 x i16> [ %256, %101 ], [ %946, %919 ]
  %1157 = phi <8 x i16> [ %255, %101 ], [ %945, %919 ]
  %1158 = phi <8 x i16> [ %254, %101 ], [ %944, %919 ]
  %1159 = phi <8 x i16> [ %217, %101 ], [ %728, %919 ]
  %1160 = phi <8 x i16> [ %216, %101 ], [ %727, %919 ]
  %1161 = phi <8 x i16> [ %215, %101 ], [ %726, %919 ]
  %1162 = phi <8 x i16> [ %214, %101 ], [ %725, %919 ]
  %1163 = phi <8 x i16> [ %177, %101 ], [ %509, %919 ]
  %1164 = phi <8 x i16> [ %176, %101 ], [ %508, %919 ]
  %1165 = phi <8 x i16> [ %175, %101 ], [ %507, %919 ]
  %1166 = phi <8 x i16> [ %174, %101 ], [ %506, %919 ]
  %1167 = phi <8 x i16> [ %137, %101 ], [ %290, %919 ]
  %1168 = phi <8 x i16> [ %136, %101 ], [ %289, %919 ]
  %1169 = phi <8 x i16> [ %135, %101 ], [ %288, %919 ]
  %1170 = phi <8 x i16> [ %134, %101 ], [ %287, %919 ]
  %1171 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1170, <8 x i16> %1155) #6
  %1172 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1169, <8 x i16> %1156) #6
  %1173 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1168, <8 x i16> %1157) #6
  %1174 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1167, <8 x i16> %1158) #6
  %1175 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1166, <8 x i16> %1159) #6
  %1176 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1165, <8 x i16> %1160) #6
  %1177 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1164, <8 x i16> %1161) #6
  %1178 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1163, <8 x i16> %1162) #6
  %1179 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1163, <8 x i16> %1162) #6
  %1180 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1164, <8 x i16> %1161) #6
  %1181 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1165, <8 x i16> %1160) #6
  %1182 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1166, <8 x i16> %1159) #6
  %1183 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1167, <8 x i16> %1158) #6
  %1184 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1168, <8 x i16> %1157) #6
  %1185 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1169, <8 x i16> %1156) #6
  %1186 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1170, <8 x i16> %1155) #6
  %1187 = add <8 x i16> %1171, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1188 = icmp ult <8 x i16> %1187, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1189 = add <8 x i16> %1172, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1190 = icmp ult <8 x i16> %1189, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1191 = add <8 x i16> %1173, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1192 = icmp ult <8 x i16> %1191, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1193 = add <8 x i16> %1174, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1194 = icmp ult <8 x i16> %1193, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1195 = or <8 x i1> %1192, %1194
  %1196 = or <8 x i1> %1195, %1190
  %1197 = or <8 x i1> %1196, %1188
  %1198 = sext <8 x i1> %1197 to <8 x i16>
  %1199 = bitcast <8 x i16> %1198 to <16 x i8>
  %1200 = icmp slt <16 x i8> %1199, zeroinitializer
  %1201 = bitcast <16 x i1> %1200 to i16
  %1202 = zext i16 %1201 to i32
  %1203 = add <8 x i16> %1175, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1204 = icmp ult <8 x i16> %1203, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1205 = add <8 x i16> %1176, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1206 = icmp ult <8 x i16> %1205, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1207 = add <8 x i16> %1177, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1208 = icmp ult <8 x i16> %1207, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1209 = add <8 x i16> %1178, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1210 = icmp ult <8 x i16> %1209, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1211 = or <8 x i1> %1208, %1210
  %1212 = or <8 x i1> %1211, %1206
  %1213 = or <8 x i1> %1212, %1204
  %1214 = sext <8 x i1> %1213 to <8 x i16>
  %1215 = bitcast <8 x i16> %1214 to <16 x i8>
  %1216 = icmp slt <16 x i8> %1215, zeroinitializer
  %1217 = bitcast <16 x i1> %1216 to i16
  %1218 = zext i16 %1217 to i32
  %1219 = icmp eq i16 %1201, 0
  br i1 %1219, label %1220, label %1255

1220:                                             ; preds = %1138
  %1221 = add <8 x i16> %1179, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1222 = icmp ult <8 x i16> %1221, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1223 = add <8 x i16> %1180, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1224 = icmp ult <8 x i16> %1223, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1225 = add <8 x i16> %1181, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1226 = icmp ult <8 x i16> %1225, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1227 = add <8 x i16> %1182, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1228 = icmp ult <8 x i16> %1227, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1229 = or <8 x i1> %1224, %1222
  %1230 = or <8 x i1> %1229, %1226
  %1231 = or <8 x i1> %1230, %1228
  %1232 = sext <8 x i1> %1231 to <8 x i16>
  %1233 = bitcast <8 x i16> %1232 to <16 x i8>
  %1234 = icmp slt <16 x i8> %1233, zeroinitializer
  %1235 = bitcast <16 x i1> %1234 to i16
  %1236 = zext i16 %1235 to i32
  %1237 = icmp eq i16 %1217, 0
  br i1 %1237, label %1238, label %1255

1238:                                             ; preds = %1220
  %1239 = add <8 x i16> %1183, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1240 = icmp ult <8 x i16> %1239, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1241 = add <8 x i16> %1184, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1242 = icmp ult <8 x i16> %1241, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1243 = add <8 x i16> %1185, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1244 = icmp ult <8 x i16> %1243, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1245 = add <8 x i16> %1186, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1246 = icmp ult <8 x i16> %1245, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1247 = or <8 x i1> %1242, %1240
  %1248 = or <8 x i1> %1247, %1244
  %1249 = or <8 x i1> %1248, %1246
  %1250 = sext <8 x i1> %1249 to <8 x i16>
  %1251 = bitcast <8 x i16> %1250 to <16 x i8>
  %1252 = icmp slt <16 x i8> %1251, zeroinitializer
  %1253 = bitcast <16 x i1> %1252 to i16
  %1254 = zext i16 %1253 to i32
  br label %1255

1255:                                             ; preds = %1138, %1220, %1238
  %1256 = phi i32 [ %1202, %1138 ], [ %1236, %1220 ], [ %1236, %1238 ]
  %1257 = phi i32 [ %1218, %1138 ], [ %1218, %1220 ], [ %1254, %1238 ]
  %1258 = sub nsw i32 0, %1256
  %1259 = icmp eq i32 %1257, %1258
  br i1 %1259, label %1414, label %1260

1260:                                             ; preds = %1255
  br i1 %97, label %1261, label %1262

1261:                                             ; preds = %1260
  tail call void @vpx_highbd_fdct32x32_rd_c(i16* %0, i32* %1, i32 %2) #6
  br label %6136

1262:                                             ; preds = %1260
  %1263 = bitcast [32 x i64]* %4 to i8*
  %1264 = bitcast [32 x i64]* %5 to i8*
  %1265 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %1266 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %1267 = bitcast [32 x i64]* %5 to <2 x i64>*
  %1268 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %1269 = bitcast i64* %1268 to <2 x i64>*
  %1270 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %1271 = bitcast i64* %1270 to <2 x i64>*
  %1272 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %1273 = bitcast i64* %1272 to <2 x i64>*
  %1274 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %1275 = bitcast i64* %1274 to <2 x i64>*
  %1276 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %1277 = bitcast i64* %1276 to <2 x i64>*
  %1278 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %1279 = bitcast i64* %1278 to <2 x i64>*
  %1280 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %1281 = bitcast i64* %1280 to <2 x i64>*
  %1282 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %1283 = bitcast i64* %1282 to <2 x i64>*
  %1284 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %1285 = bitcast i64* %1284 to <2 x i64>*
  %1286 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %1287 = bitcast i64* %1286 to <2 x i64>*
  %1288 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %1289 = bitcast i64* %1288 to <2 x i64>*
  %1290 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %1291 = bitcast i64* %1290 to <2 x i64>*
  %1292 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %1293 = bitcast i64* %1292 to <2 x i64>*
  %1294 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %1295 = bitcast i64* %1294 to <2 x i64>*
  %1296 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %1297 = bitcast i64* %1296 to <2 x i64>*
  br label %1298

1298:                                             ; preds = %1331, %1262
  %1299 = phi i64 [ 0, %1262 ], [ %1412, %1331 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %1263) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %1263, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %1264) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %1264, i8 -86, i64 256, i1 false) #6
  br label %1300

1300:                                             ; preds = %1300, %1298
  %1301 = phi i64 [ 0, %1298 ], [ %1329, %1300 ]
  %1302 = shl i64 %1301, 5
  %1303 = add nuw nsw i64 %1302, %1299
  %1304 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %1303
  %1305 = load i16, i16* %1304, align 2
  %1306 = sext i16 %1305 to i64
  %1307 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %1301
  store i64 %1306, i64* %1307, align 16
  %1308 = or i64 %1301, 1
  %1309 = shl i64 %1308, 5
  %1310 = add nuw nsw i64 %1309, %1299
  %1311 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %1310
  %1312 = load i16, i16* %1311, align 2
  %1313 = sext i16 %1312 to i64
  %1314 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %1308
  store i64 %1313, i64* %1314, align 8
  %1315 = or i64 %1301, 2
  %1316 = shl i64 %1315, 5
  %1317 = add nuw nsw i64 %1316, %1299
  %1318 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %1317
  %1319 = load i16, i16* %1318, align 2
  %1320 = sext i16 %1319 to i64
  %1321 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %1315
  store i64 %1320, i64* %1321, align 16
  %1322 = or i64 %1301, 3
  %1323 = shl i64 %1322, 5
  %1324 = add nuw nsw i64 %1323, %1299
  %1325 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %1324
  %1326 = load i16, i16* %1325, align 2
  %1327 = sext i16 %1326 to i64
  %1328 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %1322
  store i64 %1327, i64* %1328, align 8
  %1329 = add nuw nsw i64 %1301, 4
  %1330 = icmp eq i64 %1329, 32
  br i1 %1330, label %1331, label %1300

1331:                                             ; preds = %1300
  call void @vpx_fdct32(i64* nonnull %1265, i64* nonnull %1266, i32 1) #6
  %1332 = shl i64 %1299, 5
  %1333 = load <2 x i64>, <2 x i64>* %1267, align 16
  %1334 = trunc <2 x i64> %1333 to <2 x i32>
  %1335 = getelementptr inbounds i32, i32* %1, i64 %1332
  %1336 = bitcast i32* %1335 to <2 x i32>*
  store <2 x i32> %1334, <2 x i32>* %1336, align 4
  %1337 = load <2 x i64>, <2 x i64>* %1269, align 16
  %1338 = trunc <2 x i64> %1337 to <2 x i32>
  %1339 = or i64 %1332, 2
  %1340 = getelementptr inbounds i32, i32* %1, i64 %1339
  %1341 = bitcast i32* %1340 to <2 x i32>*
  store <2 x i32> %1338, <2 x i32>* %1341, align 4
  %1342 = load <2 x i64>, <2 x i64>* %1271, align 16
  %1343 = trunc <2 x i64> %1342 to <2 x i32>
  %1344 = or i64 %1332, 4
  %1345 = getelementptr inbounds i32, i32* %1, i64 %1344
  %1346 = bitcast i32* %1345 to <2 x i32>*
  store <2 x i32> %1343, <2 x i32>* %1346, align 4
  %1347 = load <2 x i64>, <2 x i64>* %1273, align 16
  %1348 = trunc <2 x i64> %1347 to <2 x i32>
  %1349 = or i64 %1332, 6
  %1350 = getelementptr inbounds i32, i32* %1, i64 %1349
  %1351 = bitcast i32* %1350 to <2 x i32>*
  store <2 x i32> %1348, <2 x i32>* %1351, align 4
  %1352 = load <2 x i64>, <2 x i64>* %1275, align 16
  %1353 = trunc <2 x i64> %1352 to <2 x i32>
  %1354 = or i64 %1332, 8
  %1355 = getelementptr inbounds i32, i32* %1, i64 %1354
  %1356 = bitcast i32* %1355 to <2 x i32>*
  store <2 x i32> %1353, <2 x i32>* %1356, align 4
  %1357 = load <2 x i64>, <2 x i64>* %1277, align 16
  %1358 = trunc <2 x i64> %1357 to <2 x i32>
  %1359 = or i64 %1332, 10
  %1360 = getelementptr inbounds i32, i32* %1, i64 %1359
  %1361 = bitcast i32* %1360 to <2 x i32>*
  store <2 x i32> %1358, <2 x i32>* %1361, align 4
  %1362 = load <2 x i64>, <2 x i64>* %1279, align 16
  %1363 = trunc <2 x i64> %1362 to <2 x i32>
  %1364 = or i64 %1332, 12
  %1365 = getelementptr inbounds i32, i32* %1, i64 %1364
  %1366 = bitcast i32* %1365 to <2 x i32>*
  store <2 x i32> %1363, <2 x i32>* %1366, align 4
  %1367 = load <2 x i64>, <2 x i64>* %1281, align 16
  %1368 = trunc <2 x i64> %1367 to <2 x i32>
  %1369 = or i64 %1332, 14
  %1370 = getelementptr inbounds i32, i32* %1, i64 %1369
  %1371 = bitcast i32* %1370 to <2 x i32>*
  store <2 x i32> %1368, <2 x i32>* %1371, align 4
  %1372 = load <2 x i64>, <2 x i64>* %1283, align 16
  %1373 = trunc <2 x i64> %1372 to <2 x i32>
  %1374 = or i64 %1332, 16
  %1375 = getelementptr inbounds i32, i32* %1, i64 %1374
  %1376 = bitcast i32* %1375 to <2 x i32>*
  store <2 x i32> %1373, <2 x i32>* %1376, align 4
  %1377 = load <2 x i64>, <2 x i64>* %1285, align 16
  %1378 = trunc <2 x i64> %1377 to <2 x i32>
  %1379 = or i64 %1332, 18
  %1380 = getelementptr inbounds i32, i32* %1, i64 %1379
  %1381 = bitcast i32* %1380 to <2 x i32>*
  store <2 x i32> %1378, <2 x i32>* %1381, align 4
  %1382 = load <2 x i64>, <2 x i64>* %1287, align 16
  %1383 = trunc <2 x i64> %1382 to <2 x i32>
  %1384 = or i64 %1332, 20
  %1385 = getelementptr inbounds i32, i32* %1, i64 %1384
  %1386 = bitcast i32* %1385 to <2 x i32>*
  store <2 x i32> %1383, <2 x i32>* %1386, align 4
  %1387 = load <2 x i64>, <2 x i64>* %1289, align 16
  %1388 = trunc <2 x i64> %1387 to <2 x i32>
  %1389 = or i64 %1332, 22
  %1390 = getelementptr inbounds i32, i32* %1, i64 %1389
  %1391 = bitcast i32* %1390 to <2 x i32>*
  store <2 x i32> %1388, <2 x i32>* %1391, align 4
  %1392 = load <2 x i64>, <2 x i64>* %1291, align 16
  %1393 = trunc <2 x i64> %1392 to <2 x i32>
  %1394 = or i64 %1332, 24
  %1395 = getelementptr inbounds i32, i32* %1, i64 %1394
  %1396 = bitcast i32* %1395 to <2 x i32>*
  store <2 x i32> %1393, <2 x i32>* %1396, align 4
  %1397 = load <2 x i64>, <2 x i64>* %1293, align 16
  %1398 = trunc <2 x i64> %1397 to <2 x i32>
  %1399 = or i64 %1332, 26
  %1400 = getelementptr inbounds i32, i32* %1, i64 %1399
  %1401 = bitcast i32* %1400 to <2 x i32>*
  store <2 x i32> %1398, <2 x i32>* %1401, align 4
  %1402 = load <2 x i64>, <2 x i64>* %1295, align 16
  %1403 = trunc <2 x i64> %1402 to <2 x i32>
  %1404 = or i64 %1332, 28
  %1405 = getelementptr inbounds i32, i32* %1, i64 %1404
  %1406 = bitcast i32* %1405 to <2 x i32>*
  store <2 x i32> %1403, <2 x i32>* %1406, align 4
  %1407 = load <2 x i64>, <2 x i64>* %1297, align 16
  %1408 = trunc <2 x i64> %1407 to <2 x i32>
  %1409 = or i64 %1332, 30
  %1410 = getelementptr inbounds i32, i32* %1, i64 %1409
  %1411 = bitcast i32* %1410 to <2 x i32>*
  store <2 x i32> %1408, <2 x i32>* %1411, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %1264) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %1263) #6
  %1412 = add nuw nsw i64 %1299, 1
  %1413 = icmp eq i64 %1412, 32
  br i1 %1413, label %6136, label %1298

1414:                                             ; preds = %1255
  %1415 = shufflevector <8 x i16> %1143, <8 x i16> %1150, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1416 = shufflevector <8 x i16> %1143, <8 x i16> %1150, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1417 = shufflevector <8 x i16> %1144, <8 x i16> %1149, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1418 = shufflevector <8 x i16> %1144, <8 x i16> %1149, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1419 = shufflevector <8 x i16> %1145, <8 x i16> %1148, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1420 = shufflevector <8 x i16> %1145, <8 x i16> %1148, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1421 = shufflevector <8 x i16> %1146, <8 x i16> %1147, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1422 = shufflevector <8 x i16> %1146, <8 x i16> %1147, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1423 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1415, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %1424 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1416, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %1425 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1417, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %1426 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1418, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %1427 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1419, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %1428 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1420, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %1429 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1421, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %1430 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1422, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %1431 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1421, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %1432 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1422, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %1433 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1419, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %1434 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1420, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %1435 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1417, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %1436 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1418, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %1437 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1415, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %1438 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1416, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %1439 = add <4 x i32> %1423, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1440 = add <4 x i32> %1424, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1441 = add <4 x i32> %1425, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1442 = add <4 x i32> %1426, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1443 = add <4 x i32> %1427, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1444 = add <4 x i32> %1428, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1445 = add <4 x i32> %1429, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1446 = add <4 x i32> %1430, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1447 = add <4 x i32> %1431, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1448 = add <4 x i32> %1432, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1449 = add <4 x i32> %1433, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1450 = add <4 x i32> %1434, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1451 = add <4 x i32> %1435, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1452 = add <4 x i32> %1436, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1453 = add <4 x i32> %1437, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1454 = add <4 x i32> %1438, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1455 = ashr <4 x i32> %1439, <i32 14, i32 14, i32 14, i32 14>
  %1456 = ashr <4 x i32> %1440, <i32 14, i32 14, i32 14, i32 14>
  %1457 = ashr <4 x i32> %1441, <i32 14, i32 14, i32 14, i32 14>
  %1458 = ashr <4 x i32> %1442, <i32 14, i32 14, i32 14, i32 14>
  %1459 = ashr <4 x i32> %1443, <i32 14, i32 14, i32 14, i32 14>
  %1460 = ashr <4 x i32> %1444, <i32 14, i32 14, i32 14, i32 14>
  %1461 = ashr <4 x i32> %1445, <i32 14, i32 14, i32 14, i32 14>
  %1462 = ashr <4 x i32> %1446, <i32 14, i32 14, i32 14, i32 14>
  %1463 = ashr <4 x i32> %1447, <i32 14, i32 14, i32 14, i32 14>
  %1464 = ashr <4 x i32> %1448, <i32 14, i32 14, i32 14, i32 14>
  %1465 = ashr <4 x i32> %1449, <i32 14, i32 14, i32 14, i32 14>
  %1466 = ashr <4 x i32> %1450, <i32 14, i32 14, i32 14, i32 14>
  %1467 = ashr <4 x i32> %1451, <i32 14, i32 14, i32 14, i32 14>
  %1468 = ashr <4 x i32> %1452, <i32 14, i32 14, i32 14, i32 14>
  %1469 = ashr <4 x i32> %1453, <i32 14, i32 14, i32 14, i32 14>
  %1470 = ashr <4 x i32> %1454, <i32 14, i32 14, i32 14, i32 14>
  %1471 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1455, <4 x i32> %1456) #6
  %1472 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1457, <4 x i32> %1458) #6
  %1473 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1459, <4 x i32> %1460) #6
  %1474 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1461, <4 x i32> %1462) #6
  %1475 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1463, <4 x i32> %1464) #6
  %1476 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1465, <4 x i32> %1466) #6
  %1477 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1467, <4 x i32> %1468) #6
  %1478 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1469, <4 x i32> %1470) #6
  %1479 = add <8 x i16> %1471, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1480 = icmp ult <8 x i16> %1479, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1481 = add <8 x i16> %1472, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1482 = icmp ult <8 x i16> %1481, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1483 = add <8 x i16> %1473, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1484 = icmp ult <8 x i16> %1483, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1485 = add <8 x i16> %1474, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1486 = icmp ult <8 x i16> %1485, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1487 = or <8 x i1> %1482, %1480
  %1488 = or <8 x i1> %1487, %1484
  %1489 = or <8 x i1> %1488, %1486
  %1490 = sext <8 x i1> %1489 to <8 x i16>
  %1491 = bitcast <8 x i16> %1490 to <16 x i8>
  %1492 = icmp slt <16 x i8> %1491, zeroinitializer
  %1493 = bitcast <16 x i1> %1492 to i16
  %1494 = zext i16 %1493 to i32
  %1495 = add <8 x i16> %1475, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1496 = icmp ult <8 x i16> %1495, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1497 = add <8 x i16> %1476, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1498 = icmp ult <8 x i16> %1497, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1499 = add <8 x i16> %1477, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1500 = icmp ult <8 x i16> %1499, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1501 = add <8 x i16> %1478, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1502 = icmp ult <8 x i16> %1501, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1503 = or <8 x i1> %1498, %1496
  %1504 = or <8 x i1> %1503, %1500
  %1505 = or <8 x i1> %1504, %1502
  %1506 = sext <8 x i1> %1505 to <8 x i16>
  %1507 = bitcast <8 x i16> %1506 to <16 x i8>
  %1508 = icmp slt <16 x i8> %1507, zeroinitializer
  %1509 = bitcast <16 x i1> %1508 to i16
  %1510 = zext i16 %1509 to i32
  %1511 = sub nsw i32 0, %1494
  %1512 = icmp eq i32 %1510, %1511
  br i1 %1512, label %1667, label %1513

1513:                                             ; preds = %1414
  br i1 %97, label %1514, label %1515

1514:                                             ; preds = %1513
  tail call void @vpx_highbd_fdct32x32_rd_c(i16* %0, i32* %1, i32 %2) #6
  br label %6136

1515:                                             ; preds = %1513
  %1516 = bitcast [32 x i64]* %4 to i8*
  %1517 = bitcast [32 x i64]* %5 to i8*
  %1518 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %1519 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %1520 = bitcast [32 x i64]* %5 to <2 x i64>*
  %1521 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %1522 = bitcast i64* %1521 to <2 x i64>*
  %1523 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %1524 = bitcast i64* %1523 to <2 x i64>*
  %1525 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %1526 = bitcast i64* %1525 to <2 x i64>*
  %1527 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %1528 = bitcast i64* %1527 to <2 x i64>*
  %1529 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %1530 = bitcast i64* %1529 to <2 x i64>*
  %1531 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %1532 = bitcast i64* %1531 to <2 x i64>*
  %1533 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %1534 = bitcast i64* %1533 to <2 x i64>*
  %1535 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %1536 = bitcast i64* %1535 to <2 x i64>*
  %1537 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %1538 = bitcast i64* %1537 to <2 x i64>*
  %1539 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %1540 = bitcast i64* %1539 to <2 x i64>*
  %1541 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %1542 = bitcast i64* %1541 to <2 x i64>*
  %1543 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %1544 = bitcast i64* %1543 to <2 x i64>*
  %1545 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %1546 = bitcast i64* %1545 to <2 x i64>*
  %1547 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %1548 = bitcast i64* %1547 to <2 x i64>*
  %1549 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %1550 = bitcast i64* %1549 to <2 x i64>*
  br label %1551

1551:                                             ; preds = %1584, %1515
  %1552 = phi i64 [ 0, %1515 ], [ %1665, %1584 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %1516) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %1516, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %1517) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %1517, i8 -86, i64 256, i1 false) #6
  br label %1553

1553:                                             ; preds = %1553, %1551
  %1554 = phi i64 [ 0, %1551 ], [ %1582, %1553 ]
  %1555 = shl i64 %1554, 5
  %1556 = add nuw nsw i64 %1555, %1552
  %1557 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %1556
  %1558 = load i16, i16* %1557, align 2
  %1559 = sext i16 %1558 to i64
  %1560 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %1554
  store i64 %1559, i64* %1560, align 16
  %1561 = or i64 %1554, 1
  %1562 = shl i64 %1561, 5
  %1563 = add nuw nsw i64 %1562, %1552
  %1564 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %1563
  %1565 = load i16, i16* %1564, align 2
  %1566 = sext i16 %1565 to i64
  %1567 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %1561
  store i64 %1566, i64* %1567, align 8
  %1568 = or i64 %1554, 2
  %1569 = shl i64 %1568, 5
  %1570 = add nuw nsw i64 %1569, %1552
  %1571 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %1570
  %1572 = load i16, i16* %1571, align 2
  %1573 = sext i16 %1572 to i64
  %1574 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %1568
  store i64 %1573, i64* %1574, align 16
  %1575 = or i64 %1554, 3
  %1576 = shl i64 %1575, 5
  %1577 = add nuw nsw i64 %1576, %1552
  %1578 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %1577
  %1579 = load i16, i16* %1578, align 2
  %1580 = sext i16 %1579 to i64
  %1581 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %1575
  store i64 %1580, i64* %1581, align 8
  %1582 = add nuw nsw i64 %1554, 4
  %1583 = icmp eq i64 %1582, 32
  br i1 %1583, label %1584, label %1553

1584:                                             ; preds = %1553
  call void @vpx_fdct32(i64* nonnull %1518, i64* nonnull %1519, i32 1) #6
  %1585 = shl i64 %1552, 5
  %1586 = load <2 x i64>, <2 x i64>* %1520, align 16
  %1587 = trunc <2 x i64> %1586 to <2 x i32>
  %1588 = getelementptr inbounds i32, i32* %1, i64 %1585
  %1589 = bitcast i32* %1588 to <2 x i32>*
  store <2 x i32> %1587, <2 x i32>* %1589, align 4
  %1590 = load <2 x i64>, <2 x i64>* %1522, align 16
  %1591 = trunc <2 x i64> %1590 to <2 x i32>
  %1592 = or i64 %1585, 2
  %1593 = getelementptr inbounds i32, i32* %1, i64 %1592
  %1594 = bitcast i32* %1593 to <2 x i32>*
  store <2 x i32> %1591, <2 x i32>* %1594, align 4
  %1595 = load <2 x i64>, <2 x i64>* %1524, align 16
  %1596 = trunc <2 x i64> %1595 to <2 x i32>
  %1597 = or i64 %1585, 4
  %1598 = getelementptr inbounds i32, i32* %1, i64 %1597
  %1599 = bitcast i32* %1598 to <2 x i32>*
  store <2 x i32> %1596, <2 x i32>* %1599, align 4
  %1600 = load <2 x i64>, <2 x i64>* %1526, align 16
  %1601 = trunc <2 x i64> %1600 to <2 x i32>
  %1602 = or i64 %1585, 6
  %1603 = getelementptr inbounds i32, i32* %1, i64 %1602
  %1604 = bitcast i32* %1603 to <2 x i32>*
  store <2 x i32> %1601, <2 x i32>* %1604, align 4
  %1605 = load <2 x i64>, <2 x i64>* %1528, align 16
  %1606 = trunc <2 x i64> %1605 to <2 x i32>
  %1607 = or i64 %1585, 8
  %1608 = getelementptr inbounds i32, i32* %1, i64 %1607
  %1609 = bitcast i32* %1608 to <2 x i32>*
  store <2 x i32> %1606, <2 x i32>* %1609, align 4
  %1610 = load <2 x i64>, <2 x i64>* %1530, align 16
  %1611 = trunc <2 x i64> %1610 to <2 x i32>
  %1612 = or i64 %1585, 10
  %1613 = getelementptr inbounds i32, i32* %1, i64 %1612
  %1614 = bitcast i32* %1613 to <2 x i32>*
  store <2 x i32> %1611, <2 x i32>* %1614, align 4
  %1615 = load <2 x i64>, <2 x i64>* %1532, align 16
  %1616 = trunc <2 x i64> %1615 to <2 x i32>
  %1617 = or i64 %1585, 12
  %1618 = getelementptr inbounds i32, i32* %1, i64 %1617
  %1619 = bitcast i32* %1618 to <2 x i32>*
  store <2 x i32> %1616, <2 x i32>* %1619, align 4
  %1620 = load <2 x i64>, <2 x i64>* %1534, align 16
  %1621 = trunc <2 x i64> %1620 to <2 x i32>
  %1622 = or i64 %1585, 14
  %1623 = getelementptr inbounds i32, i32* %1, i64 %1622
  %1624 = bitcast i32* %1623 to <2 x i32>*
  store <2 x i32> %1621, <2 x i32>* %1624, align 4
  %1625 = load <2 x i64>, <2 x i64>* %1536, align 16
  %1626 = trunc <2 x i64> %1625 to <2 x i32>
  %1627 = or i64 %1585, 16
  %1628 = getelementptr inbounds i32, i32* %1, i64 %1627
  %1629 = bitcast i32* %1628 to <2 x i32>*
  store <2 x i32> %1626, <2 x i32>* %1629, align 4
  %1630 = load <2 x i64>, <2 x i64>* %1538, align 16
  %1631 = trunc <2 x i64> %1630 to <2 x i32>
  %1632 = or i64 %1585, 18
  %1633 = getelementptr inbounds i32, i32* %1, i64 %1632
  %1634 = bitcast i32* %1633 to <2 x i32>*
  store <2 x i32> %1631, <2 x i32>* %1634, align 4
  %1635 = load <2 x i64>, <2 x i64>* %1540, align 16
  %1636 = trunc <2 x i64> %1635 to <2 x i32>
  %1637 = or i64 %1585, 20
  %1638 = getelementptr inbounds i32, i32* %1, i64 %1637
  %1639 = bitcast i32* %1638 to <2 x i32>*
  store <2 x i32> %1636, <2 x i32>* %1639, align 4
  %1640 = load <2 x i64>, <2 x i64>* %1542, align 16
  %1641 = trunc <2 x i64> %1640 to <2 x i32>
  %1642 = or i64 %1585, 22
  %1643 = getelementptr inbounds i32, i32* %1, i64 %1642
  %1644 = bitcast i32* %1643 to <2 x i32>*
  store <2 x i32> %1641, <2 x i32>* %1644, align 4
  %1645 = load <2 x i64>, <2 x i64>* %1544, align 16
  %1646 = trunc <2 x i64> %1645 to <2 x i32>
  %1647 = or i64 %1585, 24
  %1648 = getelementptr inbounds i32, i32* %1, i64 %1647
  %1649 = bitcast i32* %1648 to <2 x i32>*
  store <2 x i32> %1646, <2 x i32>* %1649, align 4
  %1650 = load <2 x i64>, <2 x i64>* %1546, align 16
  %1651 = trunc <2 x i64> %1650 to <2 x i32>
  %1652 = or i64 %1585, 26
  %1653 = getelementptr inbounds i32, i32* %1, i64 %1652
  %1654 = bitcast i32* %1653 to <2 x i32>*
  store <2 x i32> %1651, <2 x i32>* %1654, align 4
  %1655 = load <2 x i64>, <2 x i64>* %1548, align 16
  %1656 = trunc <2 x i64> %1655 to <2 x i32>
  %1657 = or i64 %1585, 28
  %1658 = getelementptr inbounds i32, i32* %1, i64 %1657
  %1659 = bitcast i32* %1658 to <2 x i32>*
  store <2 x i32> %1656, <2 x i32>* %1659, align 4
  %1660 = load <2 x i64>, <2 x i64>* %1550, align 16
  %1661 = trunc <2 x i64> %1660 to <2 x i32>
  %1662 = or i64 %1585, 30
  %1663 = getelementptr inbounds i32, i32* %1, i64 %1662
  %1664 = bitcast i32* %1663 to <2 x i32>*
  store <2 x i32> %1661, <2 x i32>* %1664, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %1517) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %1516) #6
  %1665 = add nuw nsw i64 %1552, 1
  %1666 = icmp eq i64 %1665, 32
  br i1 %1666, label %6136, label %1551

1667:                                             ; preds = %1414
  br i1 %98, label %1668, label %2095

1668:                                             ; preds = %1667
  %1669 = ashr <8 x i16> %1171, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1670 = ashr <8 x i16> %1172, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1671 = ashr <8 x i16> %1173, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1672 = ashr <8 x i16> %1174, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1673 = ashr <8 x i16> %1175, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1674 = ashr <8 x i16> %1176, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1675 = ashr <8 x i16> %1177, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1676 = ashr <8 x i16> %1178, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1677 = ashr <8 x i16> %1179, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1678 = ashr <8 x i16> %1180, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1679 = ashr <8 x i16> %1181, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1680 = ashr <8 x i16> %1182, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1681 = ashr <8 x i16> %1183, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1682 = ashr <8 x i16> %1184, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1683 = ashr <8 x i16> %1185, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1684 = ashr <8 x i16> %1186, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1685 = ashr <8 x i16> %1154, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1686 = ashr <8 x i16> %1153, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1687 = ashr <8 x i16> %1152, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1688 = ashr <8 x i16> %1151, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1689 = ashr <8 x i16> %1471, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1690 = ashr <8 x i16> %1472, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1691 = ashr <8 x i16> %1473, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1692 = ashr <8 x i16> %1474, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1693 = ashr <8 x i16> %1475, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1694 = ashr <8 x i16> %1476, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1695 = ashr <8 x i16> %1477, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1696 = ashr <8 x i16> %1478, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1697 = ashr <8 x i16> %1142, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1698 = ashr <8 x i16> %1141, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1699 = ashr <8 x i16> %1140, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1700 = ashr <8 x i16> %1139, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1701 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1171, <8 x i16> %1669) #6
  %1702 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1172, <8 x i16> %1670) #6
  %1703 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1173, <8 x i16> %1671) #6
  %1704 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1174, <8 x i16> %1672) #6
  %1705 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1175, <8 x i16> %1673) #6
  %1706 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1176, <8 x i16> %1674) #6
  %1707 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1177, <8 x i16> %1675) #6
  %1708 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1178, <8 x i16> %1676) #6
  %1709 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1179, <8 x i16> %1677) #6
  %1710 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1180, <8 x i16> %1678) #6
  %1711 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1181, <8 x i16> %1679) #6
  %1712 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1182, <8 x i16> %1680) #6
  %1713 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1183, <8 x i16> %1681) #6
  %1714 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1184, <8 x i16> %1682) #6
  %1715 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1185, <8 x i16> %1683) #6
  %1716 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1186, <8 x i16> %1684) #6
  %1717 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1154, <8 x i16> %1685) #6
  %1718 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1153, <8 x i16> %1686) #6
  %1719 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1152, <8 x i16> %1687) #6
  %1720 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1151, <8 x i16> %1688) #6
  %1721 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1471, <8 x i16> %1689) #6
  %1722 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1472, <8 x i16> %1690) #6
  %1723 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1473, <8 x i16> %1691) #6
  %1724 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1474, <8 x i16> %1692) #6
  %1725 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1475, <8 x i16> %1693) #6
  %1726 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1476, <8 x i16> %1694) #6
  %1727 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1477, <8 x i16> %1695) #6
  %1728 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1478, <8 x i16> %1696) #6
  %1729 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1142, <8 x i16> %1697) #6
  %1730 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1141, <8 x i16> %1698) #6
  %1731 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1140, <8 x i16> %1699) #6
  %1732 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1139, <8 x i16> %1700) #6
  %1733 = add <8 x i16> %1701, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1734 = icmp ult <8 x i16> %1733, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1735 = add <8 x i16> %1702, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1736 = icmp ult <8 x i16> %1735, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1737 = add <8 x i16> %1703, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1738 = icmp ult <8 x i16> %1737, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1739 = add <8 x i16> %1704, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1740 = icmp ult <8 x i16> %1739, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1741 = or <8 x i1> %1738, %1740
  %1742 = or <8 x i1> %1741, %1736
  %1743 = or <8 x i1> %1742, %1734
  %1744 = sext <8 x i1> %1743 to <8 x i16>
  %1745 = bitcast <8 x i16> %1744 to <16 x i8>
  %1746 = icmp slt <16 x i8> %1745, zeroinitializer
  %1747 = bitcast <16 x i1> %1746 to i16
  %1748 = zext i16 %1747 to i32
  %1749 = add <8 x i16> %1705, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1750 = icmp ult <8 x i16> %1749, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1751 = add <8 x i16> %1706, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1752 = icmp ult <8 x i16> %1751, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1753 = add <8 x i16> %1707, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1754 = icmp ult <8 x i16> %1753, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1755 = add <8 x i16> %1708, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1756 = icmp ult <8 x i16> %1755, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1757 = or <8 x i1> %1754, %1756
  %1758 = or <8 x i1> %1757, %1752
  %1759 = or <8 x i1> %1758, %1750
  %1760 = sext <8 x i1> %1759 to <8 x i16>
  %1761 = bitcast <8 x i16> %1760 to <16 x i8>
  %1762 = icmp slt <16 x i8> %1761, zeroinitializer
  %1763 = bitcast <16 x i1> %1762 to i16
  %1764 = zext i16 %1763 to i32
  %1765 = icmp eq i16 %1747, 0
  br i1 %1765, label %1766, label %1873

1766:                                             ; preds = %1668
  %1767 = add <8 x i16> %1709, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1768 = icmp ult <8 x i16> %1767, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1769 = add <8 x i16> %1710, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1770 = icmp ult <8 x i16> %1769, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1771 = add <8 x i16> %1711, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1772 = icmp ult <8 x i16> %1771, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1773 = add <8 x i16> %1712, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1774 = icmp ult <8 x i16> %1773, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1775 = or <8 x i1> %1770, %1768
  %1776 = or <8 x i1> %1775, %1772
  %1777 = or <8 x i1> %1776, %1774
  %1778 = sext <8 x i1> %1777 to <8 x i16>
  %1779 = bitcast <8 x i16> %1778 to <16 x i8>
  %1780 = icmp slt <16 x i8> %1779, zeroinitializer
  %1781 = bitcast <16 x i1> %1780 to i16
  %1782 = zext i16 %1781 to i32
  %1783 = icmp eq i16 %1763, 0
  br i1 %1783, label %1784, label %1873

1784:                                             ; preds = %1766
  %1785 = add <8 x i16> %1713, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1786 = icmp ult <8 x i16> %1785, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1787 = add <8 x i16> %1714, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1788 = icmp ult <8 x i16> %1787, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1789 = add <8 x i16> %1715, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1790 = icmp ult <8 x i16> %1789, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1791 = add <8 x i16> %1716, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1792 = icmp ult <8 x i16> %1791, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1793 = or <8 x i1> %1788, %1786
  %1794 = or <8 x i1> %1793, %1790
  %1795 = or <8 x i1> %1794, %1792
  %1796 = sext <8 x i1> %1795 to <8 x i16>
  %1797 = bitcast <8 x i16> %1796 to <16 x i8>
  %1798 = icmp slt <16 x i8> %1797, zeroinitializer
  %1799 = bitcast <16 x i1> %1798 to i16
  %1800 = zext i16 %1799 to i32
  %1801 = icmp eq i16 %1781, 0
  br i1 %1801, label %1802, label %1873

1802:                                             ; preds = %1784
  %1803 = add <8 x i16> %1717, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1804 = icmp ult <8 x i16> %1803, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1805 = add <8 x i16> %1718, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1806 = icmp ult <8 x i16> %1805, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1807 = add <8 x i16> %1719, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1808 = icmp ult <8 x i16> %1807, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1809 = add <8 x i16> %1720, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1810 = icmp ult <8 x i16> %1809, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1811 = or <8 x i1> %1808, %1810
  %1812 = or <8 x i1> %1811, %1806
  %1813 = or <8 x i1> %1812, %1804
  %1814 = sext <8 x i1> %1813 to <8 x i16>
  %1815 = bitcast <8 x i16> %1814 to <16 x i8>
  %1816 = icmp slt <16 x i8> %1815, zeroinitializer
  %1817 = bitcast <16 x i1> %1816 to i16
  %1818 = zext i16 %1817 to i32
  %1819 = icmp eq i16 %1799, 0
  br i1 %1819, label %1820, label %1873

1820:                                             ; preds = %1802
  %1821 = add <8 x i16> %1721, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1822 = icmp ult <8 x i16> %1821, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1823 = add <8 x i16> %1722, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1824 = icmp ult <8 x i16> %1823, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1825 = add <8 x i16> %1723, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1826 = icmp ult <8 x i16> %1825, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1827 = add <8 x i16> %1724, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1828 = icmp ult <8 x i16> %1827, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1829 = or <8 x i1> %1824, %1822
  %1830 = or <8 x i1> %1829, %1826
  %1831 = or <8 x i1> %1830, %1828
  %1832 = sext <8 x i1> %1831 to <8 x i16>
  %1833 = bitcast <8 x i16> %1832 to <16 x i8>
  %1834 = icmp slt <16 x i8> %1833, zeroinitializer
  %1835 = bitcast <16 x i1> %1834 to i16
  %1836 = zext i16 %1835 to i32
  %1837 = icmp eq i16 %1817, 0
  br i1 %1837, label %1838, label %1873

1838:                                             ; preds = %1820
  %1839 = add <8 x i16> %1725, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1840 = icmp ult <8 x i16> %1839, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1841 = add <8 x i16> %1726, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1842 = icmp ult <8 x i16> %1841, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1843 = add <8 x i16> %1727, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1844 = icmp ult <8 x i16> %1843, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1845 = add <8 x i16> %1728, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1846 = icmp ult <8 x i16> %1845, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1847 = or <8 x i1> %1842, %1840
  %1848 = or <8 x i1> %1847, %1844
  %1849 = or <8 x i1> %1848, %1846
  %1850 = sext <8 x i1> %1849 to <8 x i16>
  %1851 = bitcast <8 x i16> %1850 to <16 x i8>
  %1852 = icmp slt <16 x i8> %1851, zeroinitializer
  %1853 = bitcast <16 x i1> %1852 to i16
  %1854 = zext i16 %1853 to i32
  %1855 = icmp eq i16 %1835, 0
  br i1 %1855, label %1856, label %1873

1856:                                             ; preds = %1838
  %1857 = add <8 x i16> %1729, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1858 = icmp ult <8 x i16> %1857, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1859 = add <8 x i16> %1730, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1860 = icmp ult <8 x i16> %1859, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1861 = add <8 x i16> %1731, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1862 = icmp ult <8 x i16> %1861, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1863 = add <8 x i16> %1732, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1864 = icmp ult <8 x i16> %1863, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1865 = or <8 x i1> %1862, %1864
  %1866 = or <8 x i1> %1865, %1860
  %1867 = or <8 x i1> %1866, %1858
  %1868 = sext <8 x i1> %1867 to <8 x i16>
  %1869 = bitcast <8 x i16> %1868 to <16 x i8>
  %1870 = icmp slt <16 x i8> %1869, zeroinitializer
  %1871 = bitcast <16 x i1> %1870 to i16
  %1872 = zext i16 %1871 to i32
  br label %1873

1873:                                             ; preds = %1668, %1766, %1784, %1802, %1820, %1838, %1856
  %1874 = phi i32 [ %1748, %1668 ], [ %1782, %1766 ], [ %1782, %1784 ], [ %1818, %1802 ], [ %1818, %1820 ], [ %1854, %1838 ], [ %1854, %1856 ]
  %1875 = phi i32 [ %1764, %1668 ], [ %1764, %1766 ], [ %1800, %1784 ], [ %1800, %1802 ], [ %1836, %1820 ], [ %1836, %1838 ], [ %1872, %1856 ]
  %1876 = sub nsw i32 0, %1874
  %1877 = icmp eq i32 %1875, %1876
  br i1 %1877, label %2030, label %1878

1878:                                             ; preds = %1873
  %1879 = bitcast [32 x i64]* %4 to i8*
  %1880 = bitcast [32 x i64]* %5 to i8*
  %1881 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %1882 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %1883 = bitcast [32 x i64]* %5 to <2 x i64>*
  %1884 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %1885 = bitcast i64* %1884 to <2 x i64>*
  %1886 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %1887 = bitcast i64* %1886 to <2 x i64>*
  %1888 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %1889 = bitcast i64* %1888 to <2 x i64>*
  %1890 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %1891 = bitcast i64* %1890 to <2 x i64>*
  %1892 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %1893 = bitcast i64* %1892 to <2 x i64>*
  %1894 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %1895 = bitcast i64* %1894 to <2 x i64>*
  %1896 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %1897 = bitcast i64* %1896 to <2 x i64>*
  %1898 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %1899 = bitcast i64* %1898 to <2 x i64>*
  %1900 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %1901 = bitcast i64* %1900 to <2 x i64>*
  %1902 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %1903 = bitcast i64* %1902 to <2 x i64>*
  %1904 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %1905 = bitcast i64* %1904 to <2 x i64>*
  %1906 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %1907 = bitcast i64* %1906 to <2 x i64>*
  %1908 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %1909 = bitcast i64* %1908 to <2 x i64>*
  %1910 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %1911 = bitcast i64* %1910 to <2 x i64>*
  %1912 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %1913 = bitcast i64* %1912 to <2 x i64>*
  br label %1914

1914:                                             ; preds = %1947, %1878
  %1915 = phi i64 [ 0, %1878 ], [ %2028, %1947 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %1879) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %1879, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %1880) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %1880, i8 -86, i64 256, i1 false) #6
  br label %1916

1916:                                             ; preds = %1916, %1914
  %1917 = phi i64 [ 0, %1914 ], [ %1945, %1916 ]
  %1918 = shl i64 %1917, 5
  %1919 = add nuw nsw i64 %1918, %1915
  %1920 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %1919
  %1921 = load i16, i16* %1920, align 2
  %1922 = sext i16 %1921 to i64
  %1923 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %1917
  store i64 %1922, i64* %1923, align 16
  %1924 = or i64 %1917, 1
  %1925 = shl i64 %1924, 5
  %1926 = add nuw nsw i64 %1925, %1915
  %1927 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %1926
  %1928 = load i16, i16* %1927, align 2
  %1929 = sext i16 %1928 to i64
  %1930 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %1924
  store i64 %1929, i64* %1930, align 8
  %1931 = or i64 %1917, 2
  %1932 = shl i64 %1931, 5
  %1933 = add nuw nsw i64 %1932, %1915
  %1934 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %1933
  %1935 = load i16, i16* %1934, align 2
  %1936 = sext i16 %1935 to i64
  %1937 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %1931
  store i64 %1936, i64* %1937, align 16
  %1938 = or i64 %1917, 3
  %1939 = shl i64 %1938, 5
  %1940 = add nuw nsw i64 %1939, %1915
  %1941 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %1940
  %1942 = load i16, i16* %1941, align 2
  %1943 = sext i16 %1942 to i64
  %1944 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %1938
  store i64 %1943, i64* %1944, align 8
  %1945 = add nuw nsw i64 %1917, 4
  %1946 = icmp eq i64 %1945, 32
  br i1 %1946, label %1947, label %1916

1947:                                             ; preds = %1916
  call void @vpx_fdct32(i64* nonnull %1881, i64* nonnull %1882, i32 1) #6
  %1948 = shl i64 %1915, 5
  %1949 = load <2 x i64>, <2 x i64>* %1883, align 16
  %1950 = trunc <2 x i64> %1949 to <2 x i32>
  %1951 = getelementptr inbounds i32, i32* %1, i64 %1948
  %1952 = bitcast i32* %1951 to <2 x i32>*
  store <2 x i32> %1950, <2 x i32>* %1952, align 4
  %1953 = load <2 x i64>, <2 x i64>* %1885, align 16
  %1954 = trunc <2 x i64> %1953 to <2 x i32>
  %1955 = or i64 %1948, 2
  %1956 = getelementptr inbounds i32, i32* %1, i64 %1955
  %1957 = bitcast i32* %1956 to <2 x i32>*
  store <2 x i32> %1954, <2 x i32>* %1957, align 4
  %1958 = load <2 x i64>, <2 x i64>* %1887, align 16
  %1959 = trunc <2 x i64> %1958 to <2 x i32>
  %1960 = or i64 %1948, 4
  %1961 = getelementptr inbounds i32, i32* %1, i64 %1960
  %1962 = bitcast i32* %1961 to <2 x i32>*
  store <2 x i32> %1959, <2 x i32>* %1962, align 4
  %1963 = load <2 x i64>, <2 x i64>* %1889, align 16
  %1964 = trunc <2 x i64> %1963 to <2 x i32>
  %1965 = or i64 %1948, 6
  %1966 = getelementptr inbounds i32, i32* %1, i64 %1965
  %1967 = bitcast i32* %1966 to <2 x i32>*
  store <2 x i32> %1964, <2 x i32>* %1967, align 4
  %1968 = load <2 x i64>, <2 x i64>* %1891, align 16
  %1969 = trunc <2 x i64> %1968 to <2 x i32>
  %1970 = or i64 %1948, 8
  %1971 = getelementptr inbounds i32, i32* %1, i64 %1970
  %1972 = bitcast i32* %1971 to <2 x i32>*
  store <2 x i32> %1969, <2 x i32>* %1972, align 4
  %1973 = load <2 x i64>, <2 x i64>* %1893, align 16
  %1974 = trunc <2 x i64> %1973 to <2 x i32>
  %1975 = or i64 %1948, 10
  %1976 = getelementptr inbounds i32, i32* %1, i64 %1975
  %1977 = bitcast i32* %1976 to <2 x i32>*
  store <2 x i32> %1974, <2 x i32>* %1977, align 4
  %1978 = load <2 x i64>, <2 x i64>* %1895, align 16
  %1979 = trunc <2 x i64> %1978 to <2 x i32>
  %1980 = or i64 %1948, 12
  %1981 = getelementptr inbounds i32, i32* %1, i64 %1980
  %1982 = bitcast i32* %1981 to <2 x i32>*
  store <2 x i32> %1979, <2 x i32>* %1982, align 4
  %1983 = load <2 x i64>, <2 x i64>* %1897, align 16
  %1984 = trunc <2 x i64> %1983 to <2 x i32>
  %1985 = or i64 %1948, 14
  %1986 = getelementptr inbounds i32, i32* %1, i64 %1985
  %1987 = bitcast i32* %1986 to <2 x i32>*
  store <2 x i32> %1984, <2 x i32>* %1987, align 4
  %1988 = load <2 x i64>, <2 x i64>* %1899, align 16
  %1989 = trunc <2 x i64> %1988 to <2 x i32>
  %1990 = or i64 %1948, 16
  %1991 = getelementptr inbounds i32, i32* %1, i64 %1990
  %1992 = bitcast i32* %1991 to <2 x i32>*
  store <2 x i32> %1989, <2 x i32>* %1992, align 4
  %1993 = load <2 x i64>, <2 x i64>* %1901, align 16
  %1994 = trunc <2 x i64> %1993 to <2 x i32>
  %1995 = or i64 %1948, 18
  %1996 = getelementptr inbounds i32, i32* %1, i64 %1995
  %1997 = bitcast i32* %1996 to <2 x i32>*
  store <2 x i32> %1994, <2 x i32>* %1997, align 4
  %1998 = load <2 x i64>, <2 x i64>* %1903, align 16
  %1999 = trunc <2 x i64> %1998 to <2 x i32>
  %2000 = or i64 %1948, 20
  %2001 = getelementptr inbounds i32, i32* %1, i64 %2000
  %2002 = bitcast i32* %2001 to <2 x i32>*
  store <2 x i32> %1999, <2 x i32>* %2002, align 4
  %2003 = load <2 x i64>, <2 x i64>* %1905, align 16
  %2004 = trunc <2 x i64> %2003 to <2 x i32>
  %2005 = or i64 %1948, 22
  %2006 = getelementptr inbounds i32, i32* %1, i64 %2005
  %2007 = bitcast i32* %2006 to <2 x i32>*
  store <2 x i32> %2004, <2 x i32>* %2007, align 4
  %2008 = load <2 x i64>, <2 x i64>* %1907, align 16
  %2009 = trunc <2 x i64> %2008 to <2 x i32>
  %2010 = or i64 %1948, 24
  %2011 = getelementptr inbounds i32, i32* %1, i64 %2010
  %2012 = bitcast i32* %2011 to <2 x i32>*
  store <2 x i32> %2009, <2 x i32>* %2012, align 4
  %2013 = load <2 x i64>, <2 x i64>* %1909, align 16
  %2014 = trunc <2 x i64> %2013 to <2 x i32>
  %2015 = or i64 %1948, 26
  %2016 = getelementptr inbounds i32, i32* %1, i64 %2015
  %2017 = bitcast i32* %2016 to <2 x i32>*
  store <2 x i32> %2014, <2 x i32>* %2017, align 4
  %2018 = load <2 x i64>, <2 x i64>* %1911, align 16
  %2019 = trunc <2 x i64> %2018 to <2 x i32>
  %2020 = or i64 %1948, 28
  %2021 = getelementptr inbounds i32, i32* %1, i64 %2020
  %2022 = bitcast i32* %2021 to <2 x i32>*
  store <2 x i32> %2019, <2 x i32>* %2022, align 4
  %2023 = load <2 x i64>, <2 x i64>* %1913, align 16
  %2024 = trunc <2 x i64> %2023 to <2 x i32>
  %2025 = or i64 %1948, 30
  %2026 = getelementptr inbounds i32, i32* %1, i64 %2025
  %2027 = bitcast i32* %2026 to <2 x i32>*
  store <2 x i32> %2024, <2 x i32>* %2027, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %1880) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %1879) #6
  %2028 = add nuw nsw i64 %1915, 1
  %2029 = icmp eq i64 %2028, 32
  br i1 %2029, label %6136, label %1914

2030:                                             ; preds = %1873
  %2031 = add <8 x i16> %1701, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2032 = add <8 x i16> %1702, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2033 = add <8 x i16> %1703, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2034 = add <8 x i16> %1704, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2035 = add <8 x i16> %1705, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2036 = add <8 x i16> %1706, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2037 = add <8 x i16> %1707, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2038 = add <8 x i16> %1708, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2039 = add <8 x i16> %1709, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2040 = add <8 x i16> %1710, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2041 = add <8 x i16> %1711, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2042 = add <8 x i16> %1712, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2043 = add <8 x i16> %1713, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2044 = add <8 x i16> %1714, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2045 = add <8 x i16> %1715, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2046 = add <8 x i16> %1716, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2047 = add <8 x i16> %1717, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2048 = add <8 x i16> %1718, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2049 = add <8 x i16> %1719, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2050 = add <8 x i16> %1720, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2051 = add <8 x i16> %1721, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2052 = add <8 x i16> %1722, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2053 = add <8 x i16> %1723, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2054 = add <8 x i16> %1724, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2055 = add <8 x i16> %1725, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2056 = add <8 x i16> %1726, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2057 = add <8 x i16> %1727, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2058 = add <8 x i16> %1728, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2059 = add <8 x i16> %1729, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2060 = add <8 x i16> %1730, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2061 = add <8 x i16> %1731, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2062 = add <8 x i16> %1732, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2063 = ashr <8 x i16> %2031, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2064 = ashr <8 x i16> %2032, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2065 = ashr <8 x i16> %2033, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2066 = ashr <8 x i16> %2034, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2067 = ashr <8 x i16> %2035, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2068 = ashr <8 x i16> %2036, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2069 = ashr <8 x i16> %2037, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2070 = ashr <8 x i16> %2038, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2071 = ashr <8 x i16> %2039, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2072 = ashr <8 x i16> %2040, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2073 = ashr <8 x i16> %2041, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2074 = ashr <8 x i16> %2042, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2075 = ashr <8 x i16> %2043, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2076 = ashr <8 x i16> %2044, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2077 = ashr <8 x i16> %2045, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2078 = ashr <8 x i16> %2046, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2079 = ashr <8 x i16> %2047, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2080 = ashr <8 x i16> %2048, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2081 = ashr <8 x i16> %2049, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2082 = ashr <8 x i16> %2050, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2083 = ashr <8 x i16> %2051, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2084 = ashr <8 x i16> %2052, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2085 = ashr <8 x i16> %2053, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2086 = ashr <8 x i16> %2054, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2087 = ashr <8 x i16> %2055, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2088 = ashr <8 x i16> %2056, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2089 = ashr <8 x i16> %2057, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2090 = ashr <8 x i16> %2058, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2091 = ashr <8 x i16> %2059, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2092 = ashr <8 x i16> %2060, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2093 = ashr <8 x i16> %2061, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2094 = ashr <8 x i16> %2062, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  br label %2095

2095:                                             ; preds = %2030, %1667
  %2096 = phi <8 x i16> [ %1139, %1667 ], [ %2094, %2030 ]
  %2097 = phi <8 x i16> [ %1140, %1667 ], [ %2093, %2030 ]
  %2098 = phi <8 x i16> [ %1141, %1667 ], [ %2092, %2030 ]
  %2099 = phi <8 x i16> [ %1142, %1667 ], [ %2091, %2030 ]
  %2100 = phi <8 x i16> [ %1151, %1667 ], [ %2082, %2030 ]
  %2101 = phi <8 x i16> [ %1152, %1667 ], [ %2081, %2030 ]
  %2102 = phi <8 x i16> [ %1153, %1667 ], [ %2080, %2030 ]
  %2103 = phi <8 x i16> [ %1154, %1667 ], [ %2079, %2030 ]
  %2104 = phi <8 x i16> [ %1478, %1667 ], [ %2090, %2030 ]
  %2105 = phi <8 x i16> [ %1477, %1667 ], [ %2089, %2030 ]
  %2106 = phi <8 x i16> [ %1476, %1667 ], [ %2088, %2030 ]
  %2107 = phi <8 x i16> [ %1475, %1667 ], [ %2087, %2030 ]
  %2108 = phi <8 x i16> [ %1474, %1667 ], [ %2086, %2030 ]
  %2109 = phi <8 x i16> [ %1473, %1667 ], [ %2085, %2030 ]
  %2110 = phi <8 x i16> [ %1472, %1667 ], [ %2084, %2030 ]
  %2111 = phi <8 x i16> [ %1471, %1667 ], [ %2083, %2030 ]
  %2112 = phi <8 x i16> [ %1186, %1667 ], [ %2078, %2030 ]
  %2113 = phi <8 x i16> [ %1185, %1667 ], [ %2077, %2030 ]
  %2114 = phi <8 x i16> [ %1184, %1667 ], [ %2076, %2030 ]
  %2115 = phi <8 x i16> [ %1183, %1667 ], [ %2075, %2030 ]
  %2116 = phi <8 x i16> [ %1182, %1667 ], [ %2074, %2030 ]
  %2117 = phi <8 x i16> [ %1181, %1667 ], [ %2073, %2030 ]
  %2118 = phi <8 x i16> [ %1180, %1667 ], [ %2072, %2030 ]
  %2119 = phi <8 x i16> [ %1179, %1667 ], [ %2071, %2030 ]
  %2120 = phi <8 x i16> [ %1178, %1667 ], [ %2070, %2030 ]
  %2121 = phi <8 x i16> [ %1177, %1667 ], [ %2069, %2030 ]
  %2122 = phi <8 x i16> [ %1176, %1667 ], [ %2068, %2030 ]
  %2123 = phi <8 x i16> [ %1175, %1667 ], [ %2067, %2030 ]
  %2124 = phi <8 x i16> [ %1174, %1667 ], [ %2066, %2030 ]
  %2125 = phi <8 x i16> [ %1173, %1667 ], [ %2065, %2030 ]
  %2126 = phi <8 x i16> [ %1172, %1667 ], [ %2064, %2030 ]
  %2127 = phi <8 x i16> [ %1171, %1667 ], [ %2063, %2030 ]
  %2128 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2120, <8 x i16> %2127) #6
  %2129 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2121, <8 x i16> %2126) #6
  %2130 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2122, <8 x i16> %2125) #6
  %2131 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2123, <8 x i16> %2124) #6
  %2132 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2124, <8 x i16> %2123) #6
  %2133 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2125, <8 x i16> %2122) #6
  %2134 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2126, <8 x i16> %2121) #6
  %2135 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2127, <8 x i16> %2120) #6
  %2136 = add <8 x i16> %2128, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2137 = icmp ult <8 x i16> %2136, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2138 = add <8 x i16> %2129, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2139 = icmp ult <8 x i16> %2138, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2140 = add <8 x i16> %2130, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2141 = icmp ult <8 x i16> %2140, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2142 = add <8 x i16> %2131, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2143 = icmp ult <8 x i16> %2142, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2144 = or <8 x i1> %2141, %2143
  %2145 = or <8 x i1> %2144, %2139
  %2146 = or <8 x i1> %2145, %2137
  %2147 = sext <8 x i1> %2146 to <8 x i16>
  %2148 = bitcast <8 x i16> %2147 to <16 x i8>
  %2149 = icmp slt <16 x i8> %2148, zeroinitializer
  %2150 = bitcast <16 x i1> %2149 to i16
  %2151 = zext i16 %2150 to i32
  %2152 = add <8 x i16> %2132, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2153 = icmp ult <8 x i16> %2152, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2154 = add <8 x i16> %2133, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2155 = icmp ult <8 x i16> %2154, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2156 = add <8 x i16> %2134, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2157 = icmp ult <8 x i16> %2156, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2158 = add <8 x i16> %2135, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2159 = icmp ult <8 x i16> %2158, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2160 = or <8 x i1> %2155, %2153
  %2161 = or <8 x i1> %2160, %2157
  %2162 = or <8 x i1> %2161, %2159
  %2163 = sext <8 x i1> %2162 to <8 x i16>
  %2164 = bitcast <8 x i16> %2163 to <16 x i8>
  %2165 = icmp slt <16 x i8> %2164, zeroinitializer
  %2166 = bitcast <16 x i1> %2165 to i16
  %2167 = zext i16 %2166 to i32
  %2168 = sub nsw i32 0, %2151
  %2169 = icmp eq i32 %2167, %2168
  br i1 %2169, label %2324, label %2170

2170:                                             ; preds = %2095
  br i1 %97, label %2171, label %2172

2171:                                             ; preds = %2170
  tail call void @vpx_highbd_fdct32x32_rd_c(i16* %0, i32* %1, i32 %2) #6
  br label %6136

2172:                                             ; preds = %2170
  %2173 = bitcast [32 x i64]* %4 to i8*
  %2174 = bitcast [32 x i64]* %5 to i8*
  %2175 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %2176 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %2177 = bitcast [32 x i64]* %5 to <2 x i64>*
  %2178 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %2179 = bitcast i64* %2178 to <2 x i64>*
  %2180 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %2181 = bitcast i64* %2180 to <2 x i64>*
  %2182 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %2183 = bitcast i64* %2182 to <2 x i64>*
  %2184 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %2185 = bitcast i64* %2184 to <2 x i64>*
  %2186 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %2187 = bitcast i64* %2186 to <2 x i64>*
  %2188 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %2189 = bitcast i64* %2188 to <2 x i64>*
  %2190 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %2191 = bitcast i64* %2190 to <2 x i64>*
  %2192 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %2193 = bitcast i64* %2192 to <2 x i64>*
  %2194 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %2195 = bitcast i64* %2194 to <2 x i64>*
  %2196 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %2197 = bitcast i64* %2196 to <2 x i64>*
  %2198 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %2199 = bitcast i64* %2198 to <2 x i64>*
  %2200 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %2201 = bitcast i64* %2200 to <2 x i64>*
  %2202 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %2203 = bitcast i64* %2202 to <2 x i64>*
  %2204 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %2205 = bitcast i64* %2204 to <2 x i64>*
  %2206 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %2207 = bitcast i64* %2206 to <2 x i64>*
  br label %2208

2208:                                             ; preds = %2241, %2172
  %2209 = phi i64 [ 0, %2172 ], [ %2322, %2241 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %2173) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %2173, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %2174) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %2174, i8 -86, i64 256, i1 false) #6
  br label %2210

2210:                                             ; preds = %2210, %2208
  %2211 = phi i64 [ 0, %2208 ], [ %2239, %2210 ]
  %2212 = shl i64 %2211, 5
  %2213 = add nuw nsw i64 %2212, %2209
  %2214 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %2213
  %2215 = load i16, i16* %2214, align 2
  %2216 = sext i16 %2215 to i64
  %2217 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %2211
  store i64 %2216, i64* %2217, align 16
  %2218 = or i64 %2211, 1
  %2219 = shl i64 %2218, 5
  %2220 = add nuw nsw i64 %2219, %2209
  %2221 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %2220
  %2222 = load i16, i16* %2221, align 2
  %2223 = sext i16 %2222 to i64
  %2224 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %2218
  store i64 %2223, i64* %2224, align 8
  %2225 = or i64 %2211, 2
  %2226 = shl i64 %2225, 5
  %2227 = add nuw nsw i64 %2226, %2209
  %2228 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %2227
  %2229 = load i16, i16* %2228, align 2
  %2230 = sext i16 %2229 to i64
  %2231 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %2225
  store i64 %2230, i64* %2231, align 16
  %2232 = or i64 %2211, 3
  %2233 = shl i64 %2232, 5
  %2234 = add nuw nsw i64 %2233, %2209
  %2235 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %2234
  %2236 = load i16, i16* %2235, align 2
  %2237 = sext i16 %2236 to i64
  %2238 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %2232
  store i64 %2237, i64* %2238, align 8
  %2239 = add nuw nsw i64 %2211, 4
  %2240 = icmp eq i64 %2239, 32
  br i1 %2240, label %2241, label %2210

2241:                                             ; preds = %2210
  call void @vpx_fdct32(i64* nonnull %2175, i64* nonnull %2176, i32 1) #6
  %2242 = shl i64 %2209, 5
  %2243 = load <2 x i64>, <2 x i64>* %2177, align 16
  %2244 = trunc <2 x i64> %2243 to <2 x i32>
  %2245 = getelementptr inbounds i32, i32* %1, i64 %2242
  %2246 = bitcast i32* %2245 to <2 x i32>*
  store <2 x i32> %2244, <2 x i32>* %2246, align 4
  %2247 = load <2 x i64>, <2 x i64>* %2179, align 16
  %2248 = trunc <2 x i64> %2247 to <2 x i32>
  %2249 = or i64 %2242, 2
  %2250 = getelementptr inbounds i32, i32* %1, i64 %2249
  %2251 = bitcast i32* %2250 to <2 x i32>*
  store <2 x i32> %2248, <2 x i32>* %2251, align 4
  %2252 = load <2 x i64>, <2 x i64>* %2181, align 16
  %2253 = trunc <2 x i64> %2252 to <2 x i32>
  %2254 = or i64 %2242, 4
  %2255 = getelementptr inbounds i32, i32* %1, i64 %2254
  %2256 = bitcast i32* %2255 to <2 x i32>*
  store <2 x i32> %2253, <2 x i32>* %2256, align 4
  %2257 = load <2 x i64>, <2 x i64>* %2183, align 16
  %2258 = trunc <2 x i64> %2257 to <2 x i32>
  %2259 = or i64 %2242, 6
  %2260 = getelementptr inbounds i32, i32* %1, i64 %2259
  %2261 = bitcast i32* %2260 to <2 x i32>*
  store <2 x i32> %2258, <2 x i32>* %2261, align 4
  %2262 = load <2 x i64>, <2 x i64>* %2185, align 16
  %2263 = trunc <2 x i64> %2262 to <2 x i32>
  %2264 = or i64 %2242, 8
  %2265 = getelementptr inbounds i32, i32* %1, i64 %2264
  %2266 = bitcast i32* %2265 to <2 x i32>*
  store <2 x i32> %2263, <2 x i32>* %2266, align 4
  %2267 = load <2 x i64>, <2 x i64>* %2187, align 16
  %2268 = trunc <2 x i64> %2267 to <2 x i32>
  %2269 = or i64 %2242, 10
  %2270 = getelementptr inbounds i32, i32* %1, i64 %2269
  %2271 = bitcast i32* %2270 to <2 x i32>*
  store <2 x i32> %2268, <2 x i32>* %2271, align 4
  %2272 = load <2 x i64>, <2 x i64>* %2189, align 16
  %2273 = trunc <2 x i64> %2272 to <2 x i32>
  %2274 = or i64 %2242, 12
  %2275 = getelementptr inbounds i32, i32* %1, i64 %2274
  %2276 = bitcast i32* %2275 to <2 x i32>*
  store <2 x i32> %2273, <2 x i32>* %2276, align 4
  %2277 = load <2 x i64>, <2 x i64>* %2191, align 16
  %2278 = trunc <2 x i64> %2277 to <2 x i32>
  %2279 = or i64 %2242, 14
  %2280 = getelementptr inbounds i32, i32* %1, i64 %2279
  %2281 = bitcast i32* %2280 to <2 x i32>*
  store <2 x i32> %2278, <2 x i32>* %2281, align 4
  %2282 = load <2 x i64>, <2 x i64>* %2193, align 16
  %2283 = trunc <2 x i64> %2282 to <2 x i32>
  %2284 = or i64 %2242, 16
  %2285 = getelementptr inbounds i32, i32* %1, i64 %2284
  %2286 = bitcast i32* %2285 to <2 x i32>*
  store <2 x i32> %2283, <2 x i32>* %2286, align 4
  %2287 = load <2 x i64>, <2 x i64>* %2195, align 16
  %2288 = trunc <2 x i64> %2287 to <2 x i32>
  %2289 = or i64 %2242, 18
  %2290 = getelementptr inbounds i32, i32* %1, i64 %2289
  %2291 = bitcast i32* %2290 to <2 x i32>*
  store <2 x i32> %2288, <2 x i32>* %2291, align 4
  %2292 = load <2 x i64>, <2 x i64>* %2197, align 16
  %2293 = trunc <2 x i64> %2292 to <2 x i32>
  %2294 = or i64 %2242, 20
  %2295 = getelementptr inbounds i32, i32* %1, i64 %2294
  %2296 = bitcast i32* %2295 to <2 x i32>*
  store <2 x i32> %2293, <2 x i32>* %2296, align 4
  %2297 = load <2 x i64>, <2 x i64>* %2199, align 16
  %2298 = trunc <2 x i64> %2297 to <2 x i32>
  %2299 = or i64 %2242, 22
  %2300 = getelementptr inbounds i32, i32* %1, i64 %2299
  %2301 = bitcast i32* %2300 to <2 x i32>*
  store <2 x i32> %2298, <2 x i32>* %2301, align 4
  %2302 = load <2 x i64>, <2 x i64>* %2201, align 16
  %2303 = trunc <2 x i64> %2302 to <2 x i32>
  %2304 = or i64 %2242, 24
  %2305 = getelementptr inbounds i32, i32* %1, i64 %2304
  %2306 = bitcast i32* %2305 to <2 x i32>*
  store <2 x i32> %2303, <2 x i32>* %2306, align 4
  %2307 = load <2 x i64>, <2 x i64>* %2203, align 16
  %2308 = trunc <2 x i64> %2307 to <2 x i32>
  %2309 = or i64 %2242, 26
  %2310 = getelementptr inbounds i32, i32* %1, i64 %2309
  %2311 = bitcast i32* %2310 to <2 x i32>*
  store <2 x i32> %2308, <2 x i32>* %2311, align 4
  %2312 = load <2 x i64>, <2 x i64>* %2205, align 16
  %2313 = trunc <2 x i64> %2312 to <2 x i32>
  %2314 = or i64 %2242, 28
  %2315 = getelementptr inbounds i32, i32* %1, i64 %2314
  %2316 = bitcast i32* %2315 to <2 x i32>*
  store <2 x i32> %2313, <2 x i32>* %2316, align 4
  %2317 = load <2 x i64>, <2 x i64>* %2207, align 16
  %2318 = trunc <2 x i64> %2317 to <2 x i32>
  %2319 = or i64 %2242, 30
  %2320 = getelementptr inbounds i32, i32* %1, i64 %2319
  %2321 = bitcast i32* %2320 to <2 x i32>*
  store <2 x i32> %2318, <2 x i32>* %2321, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %2174) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %2173) #6
  %2322 = add nuw nsw i64 %2209, 1
  %2323 = icmp eq i64 %2322, 32
  br i1 %2323, label %6136, label %2208

2324:                                             ; preds = %2095
  %2325 = shufflevector <8 x i16> %2114, <8 x i16> %2117, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2326 = shufflevector <8 x i16> %2114, <8 x i16> %2117, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2327 = shufflevector <8 x i16> %2115, <8 x i16> %2116, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2328 = shufflevector <8 x i16> %2115, <8 x i16> %2116, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2329 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2325, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %2330 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2326, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %2331 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2327, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %2332 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2328, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %2333 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2327, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %2334 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2328, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %2335 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2325, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %2336 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2326, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %2337 = add <4 x i32> %2329, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2338 = add <4 x i32> %2330, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2339 = add <4 x i32> %2331, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2340 = add <4 x i32> %2332, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2341 = add <4 x i32> %2333, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2342 = add <4 x i32> %2334, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2343 = add <4 x i32> %2335, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2344 = add <4 x i32> %2336, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2345 = ashr <4 x i32> %2337, <i32 14, i32 14, i32 14, i32 14>
  %2346 = ashr <4 x i32> %2338, <i32 14, i32 14, i32 14, i32 14>
  %2347 = ashr <4 x i32> %2339, <i32 14, i32 14, i32 14, i32 14>
  %2348 = ashr <4 x i32> %2340, <i32 14, i32 14, i32 14, i32 14>
  %2349 = ashr <4 x i32> %2341, <i32 14, i32 14, i32 14, i32 14>
  %2350 = ashr <4 x i32> %2342, <i32 14, i32 14, i32 14, i32 14>
  %2351 = ashr <4 x i32> %2343, <i32 14, i32 14, i32 14, i32 14>
  %2352 = ashr <4 x i32> %2344, <i32 14, i32 14, i32 14, i32 14>
  %2353 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2345, <4 x i32> %2346) #6
  %2354 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2347, <4 x i32> %2348) #6
  %2355 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2349, <4 x i32> %2350) #6
  %2356 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2351, <4 x i32> %2352) #6
  %2357 = add <8 x i16> %2353, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2358 = icmp ult <8 x i16> %2357, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2359 = add <8 x i16> %2354, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2360 = icmp ult <8 x i16> %2359, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2361 = add <8 x i16> %2355, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2362 = icmp ult <8 x i16> %2361, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2363 = add <8 x i16> %2356, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2364 = icmp ult <8 x i16> %2363, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2365 = or <8 x i1> %2360, %2358
  %2366 = or <8 x i1> %2365, %2362
  %2367 = or <8 x i1> %2366, %2364
  %2368 = sext <8 x i1> %2367 to <8 x i16>
  %2369 = bitcast <8 x i16> %2368 to <16 x i8>
  %2370 = icmp slt <16 x i8> %2369, zeroinitializer
  %2371 = bitcast <16 x i1> %2370 to i16
  %2372 = icmp eq i16 %2371, 0
  br i1 %2372, label %2527, label %2373

2373:                                             ; preds = %2324
  br i1 %97, label %2374, label %2375

2374:                                             ; preds = %2373
  tail call void @vpx_highbd_fdct32x32_rd_c(i16* %0, i32* %1, i32 %2) #6
  br label %6136

2375:                                             ; preds = %2373
  %2376 = bitcast [32 x i64]* %4 to i8*
  %2377 = bitcast [32 x i64]* %5 to i8*
  %2378 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %2379 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %2380 = bitcast [32 x i64]* %5 to <2 x i64>*
  %2381 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %2382 = bitcast i64* %2381 to <2 x i64>*
  %2383 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %2384 = bitcast i64* %2383 to <2 x i64>*
  %2385 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %2386 = bitcast i64* %2385 to <2 x i64>*
  %2387 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %2388 = bitcast i64* %2387 to <2 x i64>*
  %2389 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %2390 = bitcast i64* %2389 to <2 x i64>*
  %2391 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %2392 = bitcast i64* %2391 to <2 x i64>*
  %2393 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %2394 = bitcast i64* %2393 to <2 x i64>*
  %2395 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %2396 = bitcast i64* %2395 to <2 x i64>*
  %2397 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %2398 = bitcast i64* %2397 to <2 x i64>*
  %2399 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %2400 = bitcast i64* %2399 to <2 x i64>*
  %2401 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %2402 = bitcast i64* %2401 to <2 x i64>*
  %2403 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %2404 = bitcast i64* %2403 to <2 x i64>*
  %2405 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %2406 = bitcast i64* %2405 to <2 x i64>*
  %2407 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %2408 = bitcast i64* %2407 to <2 x i64>*
  %2409 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %2410 = bitcast i64* %2409 to <2 x i64>*
  br label %2411

2411:                                             ; preds = %2444, %2375
  %2412 = phi i64 [ 0, %2375 ], [ %2525, %2444 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %2376) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %2376, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %2377) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %2377, i8 -86, i64 256, i1 false) #6
  br label %2413

2413:                                             ; preds = %2413, %2411
  %2414 = phi i64 [ 0, %2411 ], [ %2442, %2413 ]
  %2415 = shl i64 %2414, 5
  %2416 = add nuw nsw i64 %2415, %2412
  %2417 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %2416
  %2418 = load i16, i16* %2417, align 2
  %2419 = sext i16 %2418 to i64
  %2420 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %2414
  store i64 %2419, i64* %2420, align 16
  %2421 = or i64 %2414, 1
  %2422 = shl i64 %2421, 5
  %2423 = add nuw nsw i64 %2422, %2412
  %2424 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %2423
  %2425 = load i16, i16* %2424, align 2
  %2426 = sext i16 %2425 to i64
  %2427 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %2421
  store i64 %2426, i64* %2427, align 8
  %2428 = or i64 %2414, 2
  %2429 = shl i64 %2428, 5
  %2430 = add nuw nsw i64 %2429, %2412
  %2431 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %2430
  %2432 = load i16, i16* %2431, align 2
  %2433 = sext i16 %2432 to i64
  %2434 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %2428
  store i64 %2433, i64* %2434, align 16
  %2435 = or i64 %2414, 3
  %2436 = shl i64 %2435, 5
  %2437 = add nuw nsw i64 %2436, %2412
  %2438 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %2437
  %2439 = load i16, i16* %2438, align 2
  %2440 = sext i16 %2439 to i64
  %2441 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %2435
  store i64 %2440, i64* %2441, align 8
  %2442 = add nuw nsw i64 %2414, 4
  %2443 = icmp eq i64 %2442, 32
  br i1 %2443, label %2444, label %2413

2444:                                             ; preds = %2413
  call void @vpx_fdct32(i64* nonnull %2378, i64* nonnull %2379, i32 1) #6
  %2445 = shl i64 %2412, 5
  %2446 = load <2 x i64>, <2 x i64>* %2380, align 16
  %2447 = trunc <2 x i64> %2446 to <2 x i32>
  %2448 = getelementptr inbounds i32, i32* %1, i64 %2445
  %2449 = bitcast i32* %2448 to <2 x i32>*
  store <2 x i32> %2447, <2 x i32>* %2449, align 4
  %2450 = load <2 x i64>, <2 x i64>* %2382, align 16
  %2451 = trunc <2 x i64> %2450 to <2 x i32>
  %2452 = or i64 %2445, 2
  %2453 = getelementptr inbounds i32, i32* %1, i64 %2452
  %2454 = bitcast i32* %2453 to <2 x i32>*
  store <2 x i32> %2451, <2 x i32>* %2454, align 4
  %2455 = load <2 x i64>, <2 x i64>* %2384, align 16
  %2456 = trunc <2 x i64> %2455 to <2 x i32>
  %2457 = or i64 %2445, 4
  %2458 = getelementptr inbounds i32, i32* %1, i64 %2457
  %2459 = bitcast i32* %2458 to <2 x i32>*
  store <2 x i32> %2456, <2 x i32>* %2459, align 4
  %2460 = load <2 x i64>, <2 x i64>* %2386, align 16
  %2461 = trunc <2 x i64> %2460 to <2 x i32>
  %2462 = or i64 %2445, 6
  %2463 = getelementptr inbounds i32, i32* %1, i64 %2462
  %2464 = bitcast i32* %2463 to <2 x i32>*
  store <2 x i32> %2461, <2 x i32>* %2464, align 4
  %2465 = load <2 x i64>, <2 x i64>* %2388, align 16
  %2466 = trunc <2 x i64> %2465 to <2 x i32>
  %2467 = or i64 %2445, 8
  %2468 = getelementptr inbounds i32, i32* %1, i64 %2467
  %2469 = bitcast i32* %2468 to <2 x i32>*
  store <2 x i32> %2466, <2 x i32>* %2469, align 4
  %2470 = load <2 x i64>, <2 x i64>* %2390, align 16
  %2471 = trunc <2 x i64> %2470 to <2 x i32>
  %2472 = or i64 %2445, 10
  %2473 = getelementptr inbounds i32, i32* %1, i64 %2472
  %2474 = bitcast i32* %2473 to <2 x i32>*
  store <2 x i32> %2471, <2 x i32>* %2474, align 4
  %2475 = load <2 x i64>, <2 x i64>* %2392, align 16
  %2476 = trunc <2 x i64> %2475 to <2 x i32>
  %2477 = or i64 %2445, 12
  %2478 = getelementptr inbounds i32, i32* %1, i64 %2477
  %2479 = bitcast i32* %2478 to <2 x i32>*
  store <2 x i32> %2476, <2 x i32>* %2479, align 4
  %2480 = load <2 x i64>, <2 x i64>* %2394, align 16
  %2481 = trunc <2 x i64> %2480 to <2 x i32>
  %2482 = or i64 %2445, 14
  %2483 = getelementptr inbounds i32, i32* %1, i64 %2482
  %2484 = bitcast i32* %2483 to <2 x i32>*
  store <2 x i32> %2481, <2 x i32>* %2484, align 4
  %2485 = load <2 x i64>, <2 x i64>* %2396, align 16
  %2486 = trunc <2 x i64> %2485 to <2 x i32>
  %2487 = or i64 %2445, 16
  %2488 = getelementptr inbounds i32, i32* %1, i64 %2487
  %2489 = bitcast i32* %2488 to <2 x i32>*
  store <2 x i32> %2486, <2 x i32>* %2489, align 4
  %2490 = load <2 x i64>, <2 x i64>* %2398, align 16
  %2491 = trunc <2 x i64> %2490 to <2 x i32>
  %2492 = or i64 %2445, 18
  %2493 = getelementptr inbounds i32, i32* %1, i64 %2492
  %2494 = bitcast i32* %2493 to <2 x i32>*
  store <2 x i32> %2491, <2 x i32>* %2494, align 4
  %2495 = load <2 x i64>, <2 x i64>* %2400, align 16
  %2496 = trunc <2 x i64> %2495 to <2 x i32>
  %2497 = or i64 %2445, 20
  %2498 = getelementptr inbounds i32, i32* %1, i64 %2497
  %2499 = bitcast i32* %2498 to <2 x i32>*
  store <2 x i32> %2496, <2 x i32>* %2499, align 4
  %2500 = load <2 x i64>, <2 x i64>* %2402, align 16
  %2501 = trunc <2 x i64> %2500 to <2 x i32>
  %2502 = or i64 %2445, 22
  %2503 = getelementptr inbounds i32, i32* %1, i64 %2502
  %2504 = bitcast i32* %2503 to <2 x i32>*
  store <2 x i32> %2501, <2 x i32>* %2504, align 4
  %2505 = load <2 x i64>, <2 x i64>* %2404, align 16
  %2506 = trunc <2 x i64> %2505 to <2 x i32>
  %2507 = or i64 %2445, 24
  %2508 = getelementptr inbounds i32, i32* %1, i64 %2507
  %2509 = bitcast i32* %2508 to <2 x i32>*
  store <2 x i32> %2506, <2 x i32>* %2509, align 4
  %2510 = load <2 x i64>, <2 x i64>* %2406, align 16
  %2511 = trunc <2 x i64> %2510 to <2 x i32>
  %2512 = or i64 %2445, 26
  %2513 = getelementptr inbounds i32, i32* %1, i64 %2512
  %2514 = bitcast i32* %2513 to <2 x i32>*
  store <2 x i32> %2511, <2 x i32>* %2514, align 4
  %2515 = load <2 x i64>, <2 x i64>* %2408, align 16
  %2516 = trunc <2 x i64> %2515 to <2 x i32>
  %2517 = or i64 %2445, 28
  %2518 = getelementptr inbounds i32, i32* %1, i64 %2517
  %2519 = bitcast i32* %2518 to <2 x i32>*
  store <2 x i32> %2516, <2 x i32>* %2519, align 4
  %2520 = load <2 x i64>, <2 x i64>* %2410, align 16
  %2521 = trunc <2 x i64> %2520 to <2 x i32>
  %2522 = or i64 %2445, 30
  %2523 = getelementptr inbounds i32, i32* %1, i64 %2522
  %2524 = bitcast i32* %2523 to <2 x i32>*
  store <2 x i32> %2521, <2 x i32>* %2524, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %2377) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %2376) #6
  %2525 = add nuw nsw i64 %2412, 1
  %2526 = icmp eq i64 %2525, 32
  br i1 %2526, label %6136, label %2411

2527:                                             ; preds = %2324
  %2528 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2108, <8 x i16> %2103) #6
  %2529 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2109, <8 x i16> %2102) #6
  %2530 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2110, <8 x i16> %2101) #6
  %2531 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2111, <8 x i16> %2100) #6
  %2532 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2100, <8 x i16> %2111) #6
  %2533 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2101, <8 x i16> %2110) #6
  %2534 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2102, <8 x i16> %2109) #6
  %2535 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2103, <8 x i16> %2108) #6
  %2536 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2096, <8 x i16> %2107) #6
  %2537 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2097, <8 x i16> %2106) #6
  %2538 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2098, <8 x i16> %2105) #6
  %2539 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2099, <8 x i16> %2104) #6
  %2540 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2104, <8 x i16> %2099) #6
  %2541 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2105, <8 x i16> %2098) #6
  %2542 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2106, <8 x i16> %2097) #6
  %2543 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2107, <8 x i16> %2096) #6
  %2544 = add <8 x i16> %2528, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2545 = icmp ult <8 x i16> %2544, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2546 = add <8 x i16> %2529, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2547 = icmp ult <8 x i16> %2546, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2548 = add <8 x i16> %2530, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2549 = icmp ult <8 x i16> %2548, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2550 = add <8 x i16> %2531, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2551 = icmp ult <8 x i16> %2550, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2552 = or <8 x i1> %2547, %2545
  %2553 = or <8 x i1> %2552, %2549
  %2554 = or <8 x i1> %2553, %2551
  %2555 = sext <8 x i1> %2554 to <8 x i16>
  %2556 = bitcast <8 x i16> %2555 to <16 x i8>
  %2557 = icmp slt <16 x i8> %2556, zeroinitializer
  %2558 = bitcast <16 x i1> %2557 to i16
  %2559 = zext i16 %2558 to i32
  %2560 = add <8 x i16> %2532, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2561 = icmp ult <8 x i16> %2560, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2562 = add <8 x i16> %2533, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2563 = icmp ult <8 x i16> %2562, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2564 = add <8 x i16> %2534, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2565 = icmp ult <8 x i16> %2564, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2566 = add <8 x i16> %2535, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2567 = icmp ult <8 x i16> %2566, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2568 = or <8 x i1> %2565, %2567
  %2569 = or <8 x i1> %2568, %2563
  %2570 = or <8 x i1> %2569, %2561
  %2571 = sext <8 x i1> %2570 to <8 x i16>
  %2572 = bitcast <8 x i16> %2571 to <16 x i8>
  %2573 = icmp slt <16 x i8> %2572, zeroinitializer
  %2574 = bitcast <16 x i1> %2573 to i16
  %2575 = zext i16 %2574 to i32
  %2576 = icmp eq i16 %2558, 0
  br i1 %2576, label %2577, label %2612

2577:                                             ; preds = %2527
  %2578 = add <8 x i16> %2536, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2579 = icmp ult <8 x i16> %2578, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2580 = add <8 x i16> %2537, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2581 = icmp ult <8 x i16> %2580, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2582 = add <8 x i16> %2538, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2583 = icmp ult <8 x i16> %2582, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2584 = add <8 x i16> %2539, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2585 = icmp ult <8 x i16> %2584, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2586 = or <8 x i1> %2583, %2585
  %2587 = or <8 x i1> %2586, %2581
  %2588 = or <8 x i1> %2587, %2579
  %2589 = sext <8 x i1> %2588 to <8 x i16>
  %2590 = bitcast <8 x i16> %2589 to <16 x i8>
  %2591 = icmp slt <16 x i8> %2590, zeroinitializer
  %2592 = bitcast <16 x i1> %2591 to i16
  %2593 = zext i16 %2592 to i32
  %2594 = icmp eq i16 %2574, 0
  br i1 %2594, label %2595, label %2612

2595:                                             ; preds = %2577
  %2596 = add <8 x i16> %2540, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2597 = icmp ult <8 x i16> %2596, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2598 = add <8 x i16> %2541, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2599 = icmp ult <8 x i16> %2598, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2600 = add <8 x i16> %2542, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2601 = icmp ult <8 x i16> %2600, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2602 = add <8 x i16> %2543, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2603 = icmp ult <8 x i16> %2602, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2604 = or <8 x i1> %2599, %2597
  %2605 = or <8 x i1> %2604, %2601
  %2606 = or <8 x i1> %2605, %2603
  %2607 = sext <8 x i1> %2606 to <8 x i16>
  %2608 = bitcast <8 x i16> %2607 to <16 x i8>
  %2609 = icmp slt <16 x i8> %2608, zeroinitializer
  %2610 = bitcast <16 x i1> %2609 to i16
  %2611 = zext i16 %2610 to i32
  br label %2612

2612:                                             ; preds = %2527, %2577, %2595
  %2613 = phi i32 [ %2559, %2527 ], [ %2593, %2577 ], [ %2593, %2595 ]
  %2614 = phi i32 [ %2575, %2527 ], [ %2575, %2577 ], [ %2611, %2595 ]
  %2615 = sub nsw i32 0, %2613
  %2616 = icmp eq i32 %2614, %2615
  br i1 %2616, label %2771, label %2617

2617:                                             ; preds = %2612
  br i1 %97, label %2618, label %2619

2618:                                             ; preds = %2617
  tail call void @vpx_highbd_fdct32x32_rd_c(i16* %0, i32* %1, i32 %2) #6
  br label %6136

2619:                                             ; preds = %2617
  %2620 = bitcast [32 x i64]* %4 to i8*
  %2621 = bitcast [32 x i64]* %5 to i8*
  %2622 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %2623 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %2624 = bitcast [32 x i64]* %5 to <2 x i64>*
  %2625 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %2626 = bitcast i64* %2625 to <2 x i64>*
  %2627 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %2628 = bitcast i64* %2627 to <2 x i64>*
  %2629 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %2630 = bitcast i64* %2629 to <2 x i64>*
  %2631 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %2632 = bitcast i64* %2631 to <2 x i64>*
  %2633 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %2634 = bitcast i64* %2633 to <2 x i64>*
  %2635 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %2636 = bitcast i64* %2635 to <2 x i64>*
  %2637 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %2638 = bitcast i64* %2637 to <2 x i64>*
  %2639 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %2640 = bitcast i64* %2639 to <2 x i64>*
  %2641 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %2642 = bitcast i64* %2641 to <2 x i64>*
  %2643 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %2644 = bitcast i64* %2643 to <2 x i64>*
  %2645 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %2646 = bitcast i64* %2645 to <2 x i64>*
  %2647 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %2648 = bitcast i64* %2647 to <2 x i64>*
  %2649 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %2650 = bitcast i64* %2649 to <2 x i64>*
  %2651 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %2652 = bitcast i64* %2651 to <2 x i64>*
  %2653 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %2654 = bitcast i64* %2653 to <2 x i64>*
  br label %2655

2655:                                             ; preds = %2688, %2619
  %2656 = phi i64 [ 0, %2619 ], [ %2769, %2688 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %2620) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %2620, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %2621) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %2621, i8 -86, i64 256, i1 false) #6
  br label %2657

2657:                                             ; preds = %2657, %2655
  %2658 = phi i64 [ 0, %2655 ], [ %2686, %2657 ]
  %2659 = shl i64 %2658, 5
  %2660 = add nuw nsw i64 %2659, %2656
  %2661 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %2660
  %2662 = load i16, i16* %2661, align 2
  %2663 = sext i16 %2662 to i64
  %2664 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %2658
  store i64 %2663, i64* %2664, align 16
  %2665 = or i64 %2658, 1
  %2666 = shl i64 %2665, 5
  %2667 = add nuw nsw i64 %2666, %2656
  %2668 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %2667
  %2669 = load i16, i16* %2668, align 2
  %2670 = sext i16 %2669 to i64
  %2671 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %2665
  store i64 %2670, i64* %2671, align 8
  %2672 = or i64 %2658, 2
  %2673 = shl i64 %2672, 5
  %2674 = add nuw nsw i64 %2673, %2656
  %2675 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %2674
  %2676 = load i16, i16* %2675, align 2
  %2677 = sext i16 %2676 to i64
  %2678 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %2672
  store i64 %2677, i64* %2678, align 16
  %2679 = or i64 %2658, 3
  %2680 = shl i64 %2679, 5
  %2681 = add nuw nsw i64 %2680, %2656
  %2682 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %2681
  %2683 = load i16, i16* %2682, align 2
  %2684 = sext i16 %2683 to i64
  %2685 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %2679
  store i64 %2684, i64* %2685, align 8
  %2686 = add nuw nsw i64 %2658, 4
  %2687 = icmp eq i64 %2686, 32
  br i1 %2687, label %2688, label %2657

2688:                                             ; preds = %2657
  call void @vpx_fdct32(i64* nonnull %2622, i64* nonnull %2623, i32 1) #6
  %2689 = shl i64 %2656, 5
  %2690 = load <2 x i64>, <2 x i64>* %2624, align 16
  %2691 = trunc <2 x i64> %2690 to <2 x i32>
  %2692 = getelementptr inbounds i32, i32* %1, i64 %2689
  %2693 = bitcast i32* %2692 to <2 x i32>*
  store <2 x i32> %2691, <2 x i32>* %2693, align 4
  %2694 = load <2 x i64>, <2 x i64>* %2626, align 16
  %2695 = trunc <2 x i64> %2694 to <2 x i32>
  %2696 = or i64 %2689, 2
  %2697 = getelementptr inbounds i32, i32* %1, i64 %2696
  %2698 = bitcast i32* %2697 to <2 x i32>*
  store <2 x i32> %2695, <2 x i32>* %2698, align 4
  %2699 = load <2 x i64>, <2 x i64>* %2628, align 16
  %2700 = trunc <2 x i64> %2699 to <2 x i32>
  %2701 = or i64 %2689, 4
  %2702 = getelementptr inbounds i32, i32* %1, i64 %2701
  %2703 = bitcast i32* %2702 to <2 x i32>*
  store <2 x i32> %2700, <2 x i32>* %2703, align 4
  %2704 = load <2 x i64>, <2 x i64>* %2630, align 16
  %2705 = trunc <2 x i64> %2704 to <2 x i32>
  %2706 = or i64 %2689, 6
  %2707 = getelementptr inbounds i32, i32* %1, i64 %2706
  %2708 = bitcast i32* %2707 to <2 x i32>*
  store <2 x i32> %2705, <2 x i32>* %2708, align 4
  %2709 = load <2 x i64>, <2 x i64>* %2632, align 16
  %2710 = trunc <2 x i64> %2709 to <2 x i32>
  %2711 = or i64 %2689, 8
  %2712 = getelementptr inbounds i32, i32* %1, i64 %2711
  %2713 = bitcast i32* %2712 to <2 x i32>*
  store <2 x i32> %2710, <2 x i32>* %2713, align 4
  %2714 = load <2 x i64>, <2 x i64>* %2634, align 16
  %2715 = trunc <2 x i64> %2714 to <2 x i32>
  %2716 = or i64 %2689, 10
  %2717 = getelementptr inbounds i32, i32* %1, i64 %2716
  %2718 = bitcast i32* %2717 to <2 x i32>*
  store <2 x i32> %2715, <2 x i32>* %2718, align 4
  %2719 = load <2 x i64>, <2 x i64>* %2636, align 16
  %2720 = trunc <2 x i64> %2719 to <2 x i32>
  %2721 = or i64 %2689, 12
  %2722 = getelementptr inbounds i32, i32* %1, i64 %2721
  %2723 = bitcast i32* %2722 to <2 x i32>*
  store <2 x i32> %2720, <2 x i32>* %2723, align 4
  %2724 = load <2 x i64>, <2 x i64>* %2638, align 16
  %2725 = trunc <2 x i64> %2724 to <2 x i32>
  %2726 = or i64 %2689, 14
  %2727 = getelementptr inbounds i32, i32* %1, i64 %2726
  %2728 = bitcast i32* %2727 to <2 x i32>*
  store <2 x i32> %2725, <2 x i32>* %2728, align 4
  %2729 = load <2 x i64>, <2 x i64>* %2640, align 16
  %2730 = trunc <2 x i64> %2729 to <2 x i32>
  %2731 = or i64 %2689, 16
  %2732 = getelementptr inbounds i32, i32* %1, i64 %2731
  %2733 = bitcast i32* %2732 to <2 x i32>*
  store <2 x i32> %2730, <2 x i32>* %2733, align 4
  %2734 = load <2 x i64>, <2 x i64>* %2642, align 16
  %2735 = trunc <2 x i64> %2734 to <2 x i32>
  %2736 = or i64 %2689, 18
  %2737 = getelementptr inbounds i32, i32* %1, i64 %2736
  %2738 = bitcast i32* %2737 to <2 x i32>*
  store <2 x i32> %2735, <2 x i32>* %2738, align 4
  %2739 = load <2 x i64>, <2 x i64>* %2644, align 16
  %2740 = trunc <2 x i64> %2739 to <2 x i32>
  %2741 = or i64 %2689, 20
  %2742 = getelementptr inbounds i32, i32* %1, i64 %2741
  %2743 = bitcast i32* %2742 to <2 x i32>*
  store <2 x i32> %2740, <2 x i32>* %2743, align 4
  %2744 = load <2 x i64>, <2 x i64>* %2646, align 16
  %2745 = trunc <2 x i64> %2744 to <2 x i32>
  %2746 = or i64 %2689, 22
  %2747 = getelementptr inbounds i32, i32* %1, i64 %2746
  %2748 = bitcast i32* %2747 to <2 x i32>*
  store <2 x i32> %2745, <2 x i32>* %2748, align 4
  %2749 = load <2 x i64>, <2 x i64>* %2648, align 16
  %2750 = trunc <2 x i64> %2749 to <2 x i32>
  %2751 = or i64 %2689, 24
  %2752 = getelementptr inbounds i32, i32* %1, i64 %2751
  %2753 = bitcast i32* %2752 to <2 x i32>*
  store <2 x i32> %2750, <2 x i32>* %2753, align 4
  %2754 = load <2 x i64>, <2 x i64>* %2650, align 16
  %2755 = trunc <2 x i64> %2754 to <2 x i32>
  %2756 = or i64 %2689, 26
  %2757 = getelementptr inbounds i32, i32* %1, i64 %2756
  %2758 = bitcast i32* %2757 to <2 x i32>*
  store <2 x i32> %2755, <2 x i32>* %2758, align 4
  %2759 = load <2 x i64>, <2 x i64>* %2652, align 16
  %2760 = trunc <2 x i64> %2759 to <2 x i32>
  %2761 = or i64 %2689, 28
  %2762 = getelementptr inbounds i32, i32* %1, i64 %2761
  %2763 = bitcast i32* %2762 to <2 x i32>*
  store <2 x i32> %2760, <2 x i32>* %2763, align 4
  %2764 = load <2 x i64>, <2 x i64>* %2654, align 16
  %2765 = trunc <2 x i64> %2764 to <2 x i32>
  %2766 = or i64 %2689, 30
  %2767 = getelementptr inbounds i32, i32* %1, i64 %2766
  %2768 = bitcast i32* %2767 to <2 x i32>*
  store <2 x i32> %2765, <2 x i32>* %2768, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %2621) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %2620) #6
  %2769 = add nuw nsw i64 %2656, 1
  %2770 = icmp eq i64 %2769, 32
  br i1 %2770, label %6136, label %2655

2771:                                             ; preds = %2612
  %2772 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2131, <8 x i16> %2128) #6
  %2773 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2130, <8 x i16> %2129) #6
  %2774 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2129, <8 x i16> %2130) #6
  %2775 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2128, <8 x i16> %2131) #6
  %2776 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2354, <8 x i16> %2119) #6
  %2777 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2353, <8 x i16> %2118) #6
  %2778 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2118, <8 x i16> %2353) #6
  %2779 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2119, <8 x i16> %2354) #6
  %2780 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2112, <8 x i16> %2355) #6
  %2781 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2113, <8 x i16> %2356) #6
  %2782 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2356, <8 x i16> %2113) #6
  %2783 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2355, <8 x i16> %2112) #6
  %2784 = add <8 x i16> %2772, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2785 = icmp ult <8 x i16> %2784, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2786 = add <8 x i16> %2773, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2787 = icmp ult <8 x i16> %2786, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2788 = add <8 x i16> %2774, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2789 = icmp ult <8 x i16> %2788, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2790 = add <8 x i16> %2775, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2791 = icmp ult <8 x i16> %2790, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2792 = or <8 x i1> %2789, %2787
  %2793 = or <8 x i1> %2792, %2785
  %2794 = or <8 x i1> %2793, %2791
  %2795 = sext <8 x i1> %2794 to <8 x i16>
  %2796 = bitcast <8 x i16> %2795 to <16 x i8>
  %2797 = icmp slt <16 x i8> %2796, zeroinitializer
  %2798 = bitcast <16 x i1> %2797 to i16
  %2799 = zext i16 %2798 to i32
  %2800 = add <8 x i16> %1166, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2801 = icmp ult <8 x i16> %2800, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2802 = add <8 x i16> %1165, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2803 = icmp ult <8 x i16> %2802, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2804 = add <8 x i16> %1164, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2805 = icmp ult <8 x i16> %2804, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2806 = add <8 x i16> %1163, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2807 = icmp ult <8 x i16> %2806, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2808 = or <8 x i1> %2805, %2807
  %2809 = or <8 x i1> %2808, %2803
  %2810 = or <8 x i1> %2809, %2801
  %2811 = sext <8 x i1> %2810 to <8 x i16>
  %2812 = bitcast <8 x i16> %2811 to <16 x i8>
  %2813 = icmp slt <16 x i8> %2812, zeroinitializer
  %2814 = bitcast <16 x i1> %2813 to i16
  %2815 = zext i16 %2814 to i32
  %2816 = icmp eq i16 %2798, 0
  br i1 %2816, label %2817, label %2852

2817:                                             ; preds = %2771
  %2818 = add <8 x i16> %2776, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2819 = icmp ult <8 x i16> %2818, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2820 = add <8 x i16> %2777, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2821 = icmp ult <8 x i16> %2820, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2822 = add <8 x i16> %2778, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2823 = icmp ult <8 x i16> %2822, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2824 = add <8 x i16> %2779, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2825 = icmp ult <8 x i16> %2824, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2826 = or <8 x i1> %2823, %2821
  %2827 = or <8 x i1> %2826, %2819
  %2828 = or <8 x i1> %2827, %2825
  %2829 = sext <8 x i1> %2828 to <8 x i16>
  %2830 = bitcast <8 x i16> %2829 to <16 x i8>
  %2831 = icmp slt <16 x i8> %2830, zeroinitializer
  %2832 = bitcast <16 x i1> %2831 to i16
  %2833 = zext i16 %2832 to i32
  %2834 = icmp eq i16 %2814, 0
  br i1 %2834, label %2835, label %2852

2835:                                             ; preds = %2817
  %2836 = add <8 x i16> %2780, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2837 = icmp ult <8 x i16> %2836, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2838 = add <8 x i16> %2781, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2839 = icmp ult <8 x i16> %2838, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2840 = add <8 x i16> %2782, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2841 = icmp ult <8 x i16> %2840, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2842 = add <8 x i16> %2783, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2843 = icmp ult <8 x i16> %2842, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2844 = or <8 x i1> %2843, %2837
  %2845 = or <8 x i1> %2844, %2839
  %2846 = or <8 x i1> %2845, %2841
  %2847 = sext <8 x i1> %2846 to <8 x i16>
  %2848 = bitcast <8 x i16> %2847 to <16 x i8>
  %2849 = icmp slt <16 x i8> %2848, zeroinitializer
  %2850 = bitcast <16 x i1> %2849 to i16
  %2851 = zext i16 %2850 to i32
  br label %2852

2852:                                             ; preds = %2771, %2817, %2835
  %2853 = phi i32 [ %2799, %2771 ], [ %2833, %2817 ], [ %2833, %2835 ]
  %2854 = phi i32 [ %2815, %2771 ], [ %2815, %2817 ], [ %2851, %2835 ]
  %2855 = sub nsw i32 0, %2853
  %2856 = icmp eq i32 %2854, %2855
  br i1 %2856, label %3011, label %2857

2857:                                             ; preds = %2852
  br i1 %97, label %2858, label %2859

2858:                                             ; preds = %2857
  tail call void @vpx_highbd_fdct32x32_rd_c(i16* %0, i32* %1, i32 %2) #6
  br label %6136

2859:                                             ; preds = %2857
  %2860 = bitcast [32 x i64]* %4 to i8*
  %2861 = bitcast [32 x i64]* %5 to i8*
  %2862 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %2863 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %2864 = bitcast [32 x i64]* %5 to <2 x i64>*
  %2865 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %2866 = bitcast i64* %2865 to <2 x i64>*
  %2867 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %2868 = bitcast i64* %2867 to <2 x i64>*
  %2869 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %2870 = bitcast i64* %2869 to <2 x i64>*
  %2871 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %2872 = bitcast i64* %2871 to <2 x i64>*
  %2873 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %2874 = bitcast i64* %2873 to <2 x i64>*
  %2875 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %2876 = bitcast i64* %2875 to <2 x i64>*
  %2877 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %2878 = bitcast i64* %2877 to <2 x i64>*
  %2879 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %2880 = bitcast i64* %2879 to <2 x i64>*
  %2881 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %2882 = bitcast i64* %2881 to <2 x i64>*
  %2883 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %2884 = bitcast i64* %2883 to <2 x i64>*
  %2885 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %2886 = bitcast i64* %2885 to <2 x i64>*
  %2887 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %2888 = bitcast i64* %2887 to <2 x i64>*
  %2889 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %2890 = bitcast i64* %2889 to <2 x i64>*
  %2891 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %2892 = bitcast i64* %2891 to <2 x i64>*
  %2893 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %2894 = bitcast i64* %2893 to <2 x i64>*
  br label %2895

2895:                                             ; preds = %2928, %2859
  %2896 = phi i64 [ 0, %2859 ], [ %3009, %2928 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %2860) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %2860, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %2861) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %2861, i8 -86, i64 256, i1 false) #6
  br label %2897

2897:                                             ; preds = %2897, %2895
  %2898 = phi i64 [ 0, %2895 ], [ %2926, %2897 ]
  %2899 = shl i64 %2898, 5
  %2900 = add nuw nsw i64 %2899, %2896
  %2901 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %2900
  %2902 = load i16, i16* %2901, align 2
  %2903 = sext i16 %2902 to i64
  %2904 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %2898
  store i64 %2903, i64* %2904, align 16
  %2905 = or i64 %2898, 1
  %2906 = shl i64 %2905, 5
  %2907 = add nuw nsw i64 %2906, %2896
  %2908 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %2907
  %2909 = load i16, i16* %2908, align 2
  %2910 = sext i16 %2909 to i64
  %2911 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %2905
  store i64 %2910, i64* %2911, align 8
  %2912 = or i64 %2898, 2
  %2913 = shl i64 %2912, 5
  %2914 = add nuw nsw i64 %2913, %2896
  %2915 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %2914
  %2916 = load i16, i16* %2915, align 2
  %2917 = sext i16 %2916 to i64
  %2918 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %2912
  store i64 %2917, i64* %2918, align 16
  %2919 = or i64 %2898, 3
  %2920 = shl i64 %2919, 5
  %2921 = add nuw nsw i64 %2920, %2896
  %2922 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %2921
  %2923 = load i16, i16* %2922, align 2
  %2924 = sext i16 %2923 to i64
  %2925 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %2919
  store i64 %2924, i64* %2925, align 8
  %2926 = add nuw nsw i64 %2898, 4
  %2927 = icmp eq i64 %2926, 32
  br i1 %2927, label %2928, label %2897

2928:                                             ; preds = %2897
  call void @vpx_fdct32(i64* nonnull %2862, i64* nonnull %2863, i32 1) #6
  %2929 = shl i64 %2896, 5
  %2930 = load <2 x i64>, <2 x i64>* %2864, align 16
  %2931 = trunc <2 x i64> %2930 to <2 x i32>
  %2932 = getelementptr inbounds i32, i32* %1, i64 %2929
  %2933 = bitcast i32* %2932 to <2 x i32>*
  store <2 x i32> %2931, <2 x i32>* %2933, align 4
  %2934 = load <2 x i64>, <2 x i64>* %2866, align 16
  %2935 = trunc <2 x i64> %2934 to <2 x i32>
  %2936 = or i64 %2929, 2
  %2937 = getelementptr inbounds i32, i32* %1, i64 %2936
  %2938 = bitcast i32* %2937 to <2 x i32>*
  store <2 x i32> %2935, <2 x i32>* %2938, align 4
  %2939 = load <2 x i64>, <2 x i64>* %2868, align 16
  %2940 = trunc <2 x i64> %2939 to <2 x i32>
  %2941 = or i64 %2929, 4
  %2942 = getelementptr inbounds i32, i32* %1, i64 %2941
  %2943 = bitcast i32* %2942 to <2 x i32>*
  store <2 x i32> %2940, <2 x i32>* %2943, align 4
  %2944 = load <2 x i64>, <2 x i64>* %2870, align 16
  %2945 = trunc <2 x i64> %2944 to <2 x i32>
  %2946 = or i64 %2929, 6
  %2947 = getelementptr inbounds i32, i32* %1, i64 %2946
  %2948 = bitcast i32* %2947 to <2 x i32>*
  store <2 x i32> %2945, <2 x i32>* %2948, align 4
  %2949 = load <2 x i64>, <2 x i64>* %2872, align 16
  %2950 = trunc <2 x i64> %2949 to <2 x i32>
  %2951 = or i64 %2929, 8
  %2952 = getelementptr inbounds i32, i32* %1, i64 %2951
  %2953 = bitcast i32* %2952 to <2 x i32>*
  store <2 x i32> %2950, <2 x i32>* %2953, align 4
  %2954 = load <2 x i64>, <2 x i64>* %2874, align 16
  %2955 = trunc <2 x i64> %2954 to <2 x i32>
  %2956 = or i64 %2929, 10
  %2957 = getelementptr inbounds i32, i32* %1, i64 %2956
  %2958 = bitcast i32* %2957 to <2 x i32>*
  store <2 x i32> %2955, <2 x i32>* %2958, align 4
  %2959 = load <2 x i64>, <2 x i64>* %2876, align 16
  %2960 = trunc <2 x i64> %2959 to <2 x i32>
  %2961 = or i64 %2929, 12
  %2962 = getelementptr inbounds i32, i32* %1, i64 %2961
  %2963 = bitcast i32* %2962 to <2 x i32>*
  store <2 x i32> %2960, <2 x i32>* %2963, align 4
  %2964 = load <2 x i64>, <2 x i64>* %2878, align 16
  %2965 = trunc <2 x i64> %2964 to <2 x i32>
  %2966 = or i64 %2929, 14
  %2967 = getelementptr inbounds i32, i32* %1, i64 %2966
  %2968 = bitcast i32* %2967 to <2 x i32>*
  store <2 x i32> %2965, <2 x i32>* %2968, align 4
  %2969 = load <2 x i64>, <2 x i64>* %2880, align 16
  %2970 = trunc <2 x i64> %2969 to <2 x i32>
  %2971 = or i64 %2929, 16
  %2972 = getelementptr inbounds i32, i32* %1, i64 %2971
  %2973 = bitcast i32* %2972 to <2 x i32>*
  store <2 x i32> %2970, <2 x i32>* %2973, align 4
  %2974 = load <2 x i64>, <2 x i64>* %2882, align 16
  %2975 = trunc <2 x i64> %2974 to <2 x i32>
  %2976 = or i64 %2929, 18
  %2977 = getelementptr inbounds i32, i32* %1, i64 %2976
  %2978 = bitcast i32* %2977 to <2 x i32>*
  store <2 x i32> %2975, <2 x i32>* %2978, align 4
  %2979 = load <2 x i64>, <2 x i64>* %2884, align 16
  %2980 = trunc <2 x i64> %2979 to <2 x i32>
  %2981 = or i64 %2929, 20
  %2982 = getelementptr inbounds i32, i32* %1, i64 %2981
  %2983 = bitcast i32* %2982 to <2 x i32>*
  store <2 x i32> %2980, <2 x i32>* %2983, align 4
  %2984 = load <2 x i64>, <2 x i64>* %2886, align 16
  %2985 = trunc <2 x i64> %2984 to <2 x i32>
  %2986 = or i64 %2929, 22
  %2987 = getelementptr inbounds i32, i32* %1, i64 %2986
  %2988 = bitcast i32* %2987 to <2 x i32>*
  store <2 x i32> %2985, <2 x i32>* %2988, align 4
  %2989 = load <2 x i64>, <2 x i64>* %2888, align 16
  %2990 = trunc <2 x i64> %2989 to <2 x i32>
  %2991 = or i64 %2929, 24
  %2992 = getelementptr inbounds i32, i32* %1, i64 %2991
  %2993 = bitcast i32* %2992 to <2 x i32>*
  store <2 x i32> %2990, <2 x i32>* %2993, align 4
  %2994 = load <2 x i64>, <2 x i64>* %2890, align 16
  %2995 = trunc <2 x i64> %2994 to <2 x i32>
  %2996 = or i64 %2929, 26
  %2997 = getelementptr inbounds i32, i32* %1, i64 %2996
  %2998 = bitcast i32* %2997 to <2 x i32>*
  store <2 x i32> %2995, <2 x i32>* %2998, align 4
  %2999 = load <2 x i64>, <2 x i64>* %2892, align 16
  %3000 = trunc <2 x i64> %2999 to <2 x i32>
  %3001 = or i64 %2929, 28
  %3002 = getelementptr inbounds i32, i32* %1, i64 %3001
  %3003 = bitcast i32* %3002 to <2 x i32>*
  store <2 x i32> %3000, <2 x i32>* %3003, align 4
  %3004 = load <2 x i64>, <2 x i64>* %2894, align 16
  %3005 = trunc <2 x i64> %3004 to <2 x i32>
  %3006 = or i64 %2929, 30
  %3007 = getelementptr inbounds i32, i32* %1, i64 %3006
  %3008 = bitcast i32* %3007 to <2 x i32>*
  store <2 x i32> %3005, <2 x i32>* %3008, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %2861) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %2860) #6
  %3009 = add nuw nsw i64 %2896, 1
  %3010 = icmp eq i64 %3009, 32
  br i1 %3010, label %6136, label %2895

3011:                                             ; preds = %2852
  %3012 = shufflevector <8 x i16> %2134, <8 x i16> %2133, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3013 = shufflevector <8 x i16> %2134, <8 x i16> %2133, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3014 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3012, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %3015 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3013, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %3016 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3012, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %3017 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3013, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %3018 = add <4 x i32> %3014, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3019 = add <4 x i32> %3015, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3020 = add <4 x i32> %3016, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3021 = add <4 x i32> %3017, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3022 = ashr <4 x i32> %3018, <i32 14, i32 14, i32 14, i32 14>
  %3023 = ashr <4 x i32> %3019, <i32 14, i32 14, i32 14, i32 14>
  %3024 = ashr <4 x i32> %3020, <i32 14, i32 14, i32 14, i32 14>
  %3025 = ashr <4 x i32> %3021, <i32 14, i32 14, i32 14, i32 14>
  %3026 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3022, <4 x i32> %3023) #6
  %3027 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3024, <4 x i32> %3025) #6
  %3028 = add <8 x i16> %3026, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3029 = icmp ult <8 x i16> %3028, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3030 = add <8 x i16> %3027, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3031 = icmp ult <8 x i16> %3030, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3032 = or <8 x i1> %3031, %3029
  %3033 = sext <8 x i1> %3032 to <8 x i16>
  %3034 = bitcast <8 x i16> %3033 to <16 x i8>
  %3035 = icmp slt <16 x i8> %3034, zeroinitializer
  %3036 = bitcast <16 x i1> %3035 to i16
  %3037 = icmp eq i16 %3036, 0
  br i1 %3037, label %3192, label %3038

3038:                                             ; preds = %3011
  br i1 %97, label %3039, label %3040

3039:                                             ; preds = %3038
  tail call void @vpx_highbd_fdct32x32_rd_c(i16* %0, i32* %1, i32 %2) #6
  br label %6136

3040:                                             ; preds = %3038
  %3041 = bitcast [32 x i64]* %4 to i8*
  %3042 = bitcast [32 x i64]* %5 to i8*
  %3043 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %3044 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %3045 = bitcast [32 x i64]* %5 to <2 x i64>*
  %3046 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %3047 = bitcast i64* %3046 to <2 x i64>*
  %3048 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %3049 = bitcast i64* %3048 to <2 x i64>*
  %3050 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %3051 = bitcast i64* %3050 to <2 x i64>*
  %3052 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %3053 = bitcast i64* %3052 to <2 x i64>*
  %3054 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %3055 = bitcast i64* %3054 to <2 x i64>*
  %3056 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %3057 = bitcast i64* %3056 to <2 x i64>*
  %3058 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %3059 = bitcast i64* %3058 to <2 x i64>*
  %3060 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %3061 = bitcast i64* %3060 to <2 x i64>*
  %3062 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %3063 = bitcast i64* %3062 to <2 x i64>*
  %3064 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %3065 = bitcast i64* %3064 to <2 x i64>*
  %3066 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %3067 = bitcast i64* %3066 to <2 x i64>*
  %3068 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %3069 = bitcast i64* %3068 to <2 x i64>*
  %3070 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %3071 = bitcast i64* %3070 to <2 x i64>*
  %3072 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %3073 = bitcast i64* %3072 to <2 x i64>*
  %3074 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %3075 = bitcast i64* %3074 to <2 x i64>*
  br label %3076

3076:                                             ; preds = %3109, %3040
  %3077 = phi i64 [ 0, %3040 ], [ %3190, %3109 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %3041) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %3041, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %3042) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %3042, i8 -86, i64 256, i1 false) #6
  br label %3078

3078:                                             ; preds = %3078, %3076
  %3079 = phi i64 [ 0, %3076 ], [ %3107, %3078 ]
  %3080 = shl i64 %3079, 5
  %3081 = add nuw nsw i64 %3080, %3077
  %3082 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %3081
  %3083 = load i16, i16* %3082, align 2
  %3084 = sext i16 %3083 to i64
  %3085 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %3079
  store i64 %3084, i64* %3085, align 16
  %3086 = or i64 %3079, 1
  %3087 = shl i64 %3086, 5
  %3088 = add nuw nsw i64 %3087, %3077
  %3089 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %3088
  %3090 = load i16, i16* %3089, align 2
  %3091 = sext i16 %3090 to i64
  %3092 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %3086
  store i64 %3091, i64* %3092, align 8
  %3093 = or i64 %3079, 2
  %3094 = shl i64 %3093, 5
  %3095 = add nuw nsw i64 %3094, %3077
  %3096 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %3095
  %3097 = load i16, i16* %3096, align 2
  %3098 = sext i16 %3097 to i64
  %3099 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %3093
  store i64 %3098, i64* %3099, align 16
  %3100 = or i64 %3079, 3
  %3101 = shl i64 %3100, 5
  %3102 = add nuw nsw i64 %3101, %3077
  %3103 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %3102
  %3104 = load i16, i16* %3103, align 2
  %3105 = sext i16 %3104 to i64
  %3106 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %3100
  store i64 %3105, i64* %3106, align 8
  %3107 = add nuw nsw i64 %3079, 4
  %3108 = icmp eq i64 %3107, 32
  br i1 %3108, label %3109, label %3078

3109:                                             ; preds = %3078
  call void @vpx_fdct32(i64* nonnull %3043, i64* nonnull %3044, i32 1) #6
  %3110 = shl i64 %3077, 5
  %3111 = load <2 x i64>, <2 x i64>* %3045, align 16
  %3112 = trunc <2 x i64> %3111 to <2 x i32>
  %3113 = getelementptr inbounds i32, i32* %1, i64 %3110
  %3114 = bitcast i32* %3113 to <2 x i32>*
  store <2 x i32> %3112, <2 x i32>* %3114, align 4
  %3115 = load <2 x i64>, <2 x i64>* %3047, align 16
  %3116 = trunc <2 x i64> %3115 to <2 x i32>
  %3117 = or i64 %3110, 2
  %3118 = getelementptr inbounds i32, i32* %1, i64 %3117
  %3119 = bitcast i32* %3118 to <2 x i32>*
  store <2 x i32> %3116, <2 x i32>* %3119, align 4
  %3120 = load <2 x i64>, <2 x i64>* %3049, align 16
  %3121 = trunc <2 x i64> %3120 to <2 x i32>
  %3122 = or i64 %3110, 4
  %3123 = getelementptr inbounds i32, i32* %1, i64 %3122
  %3124 = bitcast i32* %3123 to <2 x i32>*
  store <2 x i32> %3121, <2 x i32>* %3124, align 4
  %3125 = load <2 x i64>, <2 x i64>* %3051, align 16
  %3126 = trunc <2 x i64> %3125 to <2 x i32>
  %3127 = or i64 %3110, 6
  %3128 = getelementptr inbounds i32, i32* %1, i64 %3127
  %3129 = bitcast i32* %3128 to <2 x i32>*
  store <2 x i32> %3126, <2 x i32>* %3129, align 4
  %3130 = load <2 x i64>, <2 x i64>* %3053, align 16
  %3131 = trunc <2 x i64> %3130 to <2 x i32>
  %3132 = or i64 %3110, 8
  %3133 = getelementptr inbounds i32, i32* %1, i64 %3132
  %3134 = bitcast i32* %3133 to <2 x i32>*
  store <2 x i32> %3131, <2 x i32>* %3134, align 4
  %3135 = load <2 x i64>, <2 x i64>* %3055, align 16
  %3136 = trunc <2 x i64> %3135 to <2 x i32>
  %3137 = or i64 %3110, 10
  %3138 = getelementptr inbounds i32, i32* %1, i64 %3137
  %3139 = bitcast i32* %3138 to <2 x i32>*
  store <2 x i32> %3136, <2 x i32>* %3139, align 4
  %3140 = load <2 x i64>, <2 x i64>* %3057, align 16
  %3141 = trunc <2 x i64> %3140 to <2 x i32>
  %3142 = or i64 %3110, 12
  %3143 = getelementptr inbounds i32, i32* %1, i64 %3142
  %3144 = bitcast i32* %3143 to <2 x i32>*
  store <2 x i32> %3141, <2 x i32>* %3144, align 4
  %3145 = load <2 x i64>, <2 x i64>* %3059, align 16
  %3146 = trunc <2 x i64> %3145 to <2 x i32>
  %3147 = or i64 %3110, 14
  %3148 = getelementptr inbounds i32, i32* %1, i64 %3147
  %3149 = bitcast i32* %3148 to <2 x i32>*
  store <2 x i32> %3146, <2 x i32>* %3149, align 4
  %3150 = load <2 x i64>, <2 x i64>* %3061, align 16
  %3151 = trunc <2 x i64> %3150 to <2 x i32>
  %3152 = or i64 %3110, 16
  %3153 = getelementptr inbounds i32, i32* %1, i64 %3152
  %3154 = bitcast i32* %3153 to <2 x i32>*
  store <2 x i32> %3151, <2 x i32>* %3154, align 4
  %3155 = load <2 x i64>, <2 x i64>* %3063, align 16
  %3156 = trunc <2 x i64> %3155 to <2 x i32>
  %3157 = or i64 %3110, 18
  %3158 = getelementptr inbounds i32, i32* %1, i64 %3157
  %3159 = bitcast i32* %3158 to <2 x i32>*
  store <2 x i32> %3156, <2 x i32>* %3159, align 4
  %3160 = load <2 x i64>, <2 x i64>* %3065, align 16
  %3161 = trunc <2 x i64> %3160 to <2 x i32>
  %3162 = or i64 %3110, 20
  %3163 = getelementptr inbounds i32, i32* %1, i64 %3162
  %3164 = bitcast i32* %3163 to <2 x i32>*
  store <2 x i32> %3161, <2 x i32>* %3164, align 4
  %3165 = load <2 x i64>, <2 x i64>* %3067, align 16
  %3166 = trunc <2 x i64> %3165 to <2 x i32>
  %3167 = or i64 %3110, 22
  %3168 = getelementptr inbounds i32, i32* %1, i64 %3167
  %3169 = bitcast i32* %3168 to <2 x i32>*
  store <2 x i32> %3166, <2 x i32>* %3169, align 4
  %3170 = load <2 x i64>, <2 x i64>* %3069, align 16
  %3171 = trunc <2 x i64> %3170 to <2 x i32>
  %3172 = or i64 %3110, 24
  %3173 = getelementptr inbounds i32, i32* %1, i64 %3172
  %3174 = bitcast i32* %3173 to <2 x i32>*
  store <2 x i32> %3171, <2 x i32>* %3174, align 4
  %3175 = load <2 x i64>, <2 x i64>* %3071, align 16
  %3176 = trunc <2 x i64> %3175 to <2 x i32>
  %3177 = or i64 %3110, 26
  %3178 = getelementptr inbounds i32, i32* %1, i64 %3177
  %3179 = bitcast i32* %3178 to <2 x i32>*
  store <2 x i32> %3176, <2 x i32>* %3179, align 4
  %3180 = load <2 x i64>, <2 x i64>* %3073, align 16
  %3181 = trunc <2 x i64> %3180 to <2 x i32>
  %3182 = or i64 %3110, 28
  %3183 = getelementptr inbounds i32, i32* %1, i64 %3182
  %3184 = bitcast i32* %3183 to <2 x i32>*
  store <2 x i32> %3181, <2 x i32>* %3184, align 4
  %3185 = load <2 x i64>, <2 x i64>* %3075, align 16
  %3186 = trunc <2 x i64> %3185 to <2 x i32>
  %3187 = or i64 %3110, 30
  %3188 = getelementptr inbounds i32, i32* %1, i64 %3187
  %3189 = bitcast i32* %3188 to <2 x i32>*
  store <2 x i32> %3186, <2 x i32>* %3189, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %3042) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %3041) #6
  %3190 = add nuw nsw i64 %3077, 1
  %3191 = icmp eq i64 %3190, 32
  br i1 %3191, label %6136, label %3076

3192:                                             ; preds = %3011
  %3193 = shufflevector <8 x i16> %2530, <8 x i16> %2541, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3194 = shufflevector <8 x i16> %2530, <8 x i16> %2541, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3195 = shufflevector <8 x i16> %2531, <8 x i16> %2540, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3196 = shufflevector <8 x i16> %2531, <8 x i16> %2540, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3197 = shufflevector <8 x i16> %2532, <8 x i16> %2539, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3198 = shufflevector <8 x i16> %2532, <8 x i16> %2539, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3199 = shufflevector <8 x i16> %2533, <8 x i16> %2538, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3200 = shufflevector <8 x i16> %2533, <8 x i16> %2538, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3201 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3193, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %3202 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3194, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %3203 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3195, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %3204 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3196, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %3205 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3197, <8 x i16> <i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137>) #6
  %3206 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3198, <8 x i16> <i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137>) #6
  %3207 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3199, <8 x i16> <i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137>) #6
  %3208 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3200, <8 x i16> <i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137>) #6
  %3209 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3199, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %3210 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3200, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %3211 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3197, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %3212 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3198, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %3213 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3195, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %3214 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3196, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %3215 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3193, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %3216 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3194, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %3217 = add <4 x i32> %3201, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3218 = add <4 x i32> %3202, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3219 = add <4 x i32> %3203, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3220 = add <4 x i32> %3204, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3221 = add <4 x i32> %3205, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3222 = add <4 x i32> %3206, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3223 = add <4 x i32> %3207, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3224 = add <4 x i32> %3208, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3225 = add <4 x i32> %3209, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3226 = add <4 x i32> %3210, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3227 = add <4 x i32> %3211, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3228 = add <4 x i32> %3212, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3229 = add <4 x i32> %3213, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3230 = add <4 x i32> %3214, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3231 = add <4 x i32> %3215, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3232 = add <4 x i32> %3216, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3233 = ashr <4 x i32> %3217, <i32 14, i32 14, i32 14, i32 14>
  %3234 = ashr <4 x i32> %3218, <i32 14, i32 14, i32 14, i32 14>
  %3235 = ashr <4 x i32> %3219, <i32 14, i32 14, i32 14, i32 14>
  %3236 = ashr <4 x i32> %3220, <i32 14, i32 14, i32 14, i32 14>
  %3237 = ashr <4 x i32> %3221, <i32 14, i32 14, i32 14, i32 14>
  %3238 = ashr <4 x i32> %3222, <i32 14, i32 14, i32 14, i32 14>
  %3239 = ashr <4 x i32> %3223, <i32 14, i32 14, i32 14, i32 14>
  %3240 = ashr <4 x i32> %3224, <i32 14, i32 14, i32 14, i32 14>
  %3241 = ashr <4 x i32> %3225, <i32 14, i32 14, i32 14, i32 14>
  %3242 = ashr <4 x i32> %3226, <i32 14, i32 14, i32 14, i32 14>
  %3243 = ashr <4 x i32> %3227, <i32 14, i32 14, i32 14, i32 14>
  %3244 = ashr <4 x i32> %3228, <i32 14, i32 14, i32 14, i32 14>
  %3245 = ashr <4 x i32> %3229, <i32 14, i32 14, i32 14, i32 14>
  %3246 = ashr <4 x i32> %3230, <i32 14, i32 14, i32 14, i32 14>
  %3247 = ashr <4 x i32> %3231, <i32 14, i32 14, i32 14, i32 14>
  %3248 = ashr <4 x i32> %3232, <i32 14, i32 14, i32 14, i32 14>
  %3249 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3233, <4 x i32> %3234) #6
  %3250 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3235, <4 x i32> %3236) #6
  %3251 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3237, <4 x i32> %3238) #6
  %3252 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3239, <4 x i32> %3240) #6
  %3253 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3241, <4 x i32> %3242) #6
  %3254 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3243, <4 x i32> %3244) #6
  %3255 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3245, <4 x i32> %3246) #6
  %3256 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3247, <4 x i32> %3248) #6
  %3257 = add <8 x i16> %3249, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3258 = icmp ult <8 x i16> %3257, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3259 = add <8 x i16> %3250, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3260 = icmp ult <8 x i16> %3259, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3261 = add <8 x i16> %3251, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3262 = icmp ult <8 x i16> %3261, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3263 = add <8 x i16> %3252, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3264 = icmp ult <8 x i16> %3263, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3265 = or <8 x i1> %3260, %3258
  %3266 = or <8 x i1> %3265, %3262
  %3267 = or <8 x i1> %3266, %3264
  %3268 = sext <8 x i1> %3267 to <8 x i16>
  %3269 = bitcast <8 x i16> %3268 to <16 x i8>
  %3270 = icmp slt <16 x i8> %3269, zeroinitializer
  %3271 = bitcast <16 x i1> %3270 to i16
  %3272 = zext i16 %3271 to i32
  %3273 = add <8 x i16> %3253, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3274 = icmp ult <8 x i16> %3273, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3275 = add <8 x i16> %3254, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3276 = icmp ult <8 x i16> %3275, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3277 = add <8 x i16> %3255, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3278 = icmp ult <8 x i16> %3277, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3279 = add <8 x i16> %3256, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3280 = icmp ult <8 x i16> %3279, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3281 = or <8 x i1> %3276, %3274
  %3282 = or <8 x i1> %3281, %3278
  %3283 = or <8 x i1> %3282, %3280
  %3284 = sext <8 x i1> %3283 to <8 x i16>
  %3285 = bitcast <8 x i16> %3284 to <16 x i8>
  %3286 = icmp slt <16 x i8> %3285, zeroinitializer
  %3287 = bitcast <16 x i1> %3286 to i16
  %3288 = zext i16 %3287 to i32
  %3289 = sub nsw i32 0, %3272
  %3290 = icmp eq i32 %3288, %3289
  br i1 %3290, label %3445, label %3291

3291:                                             ; preds = %3192
  br i1 %97, label %3292, label %3293

3292:                                             ; preds = %3291
  tail call void @vpx_highbd_fdct32x32_rd_c(i16* %0, i32* %1, i32 %2) #6
  br label %6136

3293:                                             ; preds = %3291
  %3294 = bitcast [32 x i64]* %4 to i8*
  %3295 = bitcast [32 x i64]* %5 to i8*
  %3296 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %3297 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %3298 = bitcast [32 x i64]* %5 to <2 x i64>*
  %3299 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %3300 = bitcast i64* %3299 to <2 x i64>*
  %3301 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %3302 = bitcast i64* %3301 to <2 x i64>*
  %3303 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %3304 = bitcast i64* %3303 to <2 x i64>*
  %3305 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %3306 = bitcast i64* %3305 to <2 x i64>*
  %3307 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %3308 = bitcast i64* %3307 to <2 x i64>*
  %3309 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %3310 = bitcast i64* %3309 to <2 x i64>*
  %3311 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %3312 = bitcast i64* %3311 to <2 x i64>*
  %3313 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %3314 = bitcast i64* %3313 to <2 x i64>*
  %3315 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %3316 = bitcast i64* %3315 to <2 x i64>*
  %3317 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %3318 = bitcast i64* %3317 to <2 x i64>*
  %3319 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %3320 = bitcast i64* %3319 to <2 x i64>*
  %3321 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %3322 = bitcast i64* %3321 to <2 x i64>*
  %3323 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %3324 = bitcast i64* %3323 to <2 x i64>*
  %3325 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %3326 = bitcast i64* %3325 to <2 x i64>*
  %3327 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %3328 = bitcast i64* %3327 to <2 x i64>*
  br label %3329

3329:                                             ; preds = %3362, %3293
  %3330 = phi i64 [ 0, %3293 ], [ %3443, %3362 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %3294) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %3294, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %3295) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %3295, i8 -86, i64 256, i1 false) #6
  br label %3331

3331:                                             ; preds = %3331, %3329
  %3332 = phi i64 [ 0, %3329 ], [ %3360, %3331 ]
  %3333 = shl i64 %3332, 5
  %3334 = add nuw nsw i64 %3333, %3330
  %3335 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %3334
  %3336 = load i16, i16* %3335, align 2
  %3337 = sext i16 %3336 to i64
  %3338 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %3332
  store i64 %3337, i64* %3338, align 16
  %3339 = or i64 %3332, 1
  %3340 = shl i64 %3339, 5
  %3341 = add nuw nsw i64 %3340, %3330
  %3342 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %3341
  %3343 = load i16, i16* %3342, align 2
  %3344 = sext i16 %3343 to i64
  %3345 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %3339
  store i64 %3344, i64* %3345, align 8
  %3346 = or i64 %3332, 2
  %3347 = shl i64 %3346, 5
  %3348 = add nuw nsw i64 %3347, %3330
  %3349 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %3348
  %3350 = load i16, i16* %3349, align 2
  %3351 = sext i16 %3350 to i64
  %3352 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %3346
  store i64 %3351, i64* %3352, align 16
  %3353 = or i64 %3332, 3
  %3354 = shl i64 %3353, 5
  %3355 = add nuw nsw i64 %3354, %3330
  %3356 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %3355
  %3357 = load i16, i16* %3356, align 2
  %3358 = sext i16 %3357 to i64
  %3359 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %3353
  store i64 %3358, i64* %3359, align 8
  %3360 = add nuw nsw i64 %3332, 4
  %3361 = icmp eq i64 %3360, 32
  br i1 %3361, label %3362, label %3331

3362:                                             ; preds = %3331
  call void @vpx_fdct32(i64* nonnull %3296, i64* nonnull %3297, i32 1) #6
  %3363 = shl i64 %3330, 5
  %3364 = load <2 x i64>, <2 x i64>* %3298, align 16
  %3365 = trunc <2 x i64> %3364 to <2 x i32>
  %3366 = getelementptr inbounds i32, i32* %1, i64 %3363
  %3367 = bitcast i32* %3366 to <2 x i32>*
  store <2 x i32> %3365, <2 x i32>* %3367, align 4
  %3368 = load <2 x i64>, <2 x i64>* %3300, align 16
  %3369 = trunc <2 x i64> %3368 to <2 x i32>
  %3370 = or i64 %3363, 2
  %3371 = getelementptr inbounds i32, i32* %1, i64 %3370
  %3372 = bitcast i32* %3371 to <2 x i32>*
  store <2 x i32> %3369, <2 x i32>* %3372, align 4
  %3373 = load <2 x i64>, <2 x i64>* %3302, align 16
  %3374 = trunc <2 x i64> %3373 to <2 x i32>
  %3375 = or i64 %3363, 4
  %3376 = getelementptr inbounds i32, i32* %1, i64 %3375
  %3377 = bitcast i32* %3376 to <2 x i32>*
  store <2 x i32> %3374, <2 x i32>* %3377, align 4
  %3378 = load <2 x i64>, <2 x i64>* %3304, align 16
  %3379 = trunc <2 x i64> %3378 to <2 x i32>
  %3380 = or i64 %3363, 6
  %3381 = getelementptr inbounds i32, i32* %1, i64 %3380
  %3382 = bitcast i32* %3381 to <2 x i32>*
  store <2 x i32> %3379, <2 x i32>* %3382, align 4
  %3383 = load <2 x i64>, <2 x i64>* %3306, align 16
  %3384 = trunc <2 x i64> %3383 to <2 x i32>
  %3385 = or i64 %3363, 8
  %3386 = getelementptr inbounds i32, i32* %1, i64 %3385
  %3387 = bitcast i32* %3386 to <2 x i32>*
  store <2 x i32> %3384, <2 x i32>* %3387, align 4
  %3388 = load <2 x i64>, <2 x i64>* %3308, align 16
  %3389 = trunc <2 x i64> %3388 to <2 x i32>
  %3390 = or i64 %3363, 10
  %3391 = getelementptr inbounds i32, i32* %1, i64 %3390
  %3392 = bitcast i32* %3391 to <2 x i32>*
  store <2 x i32> %3389, <2 x i32>* %3392, align 4
  %3393 = load <2 x i64>, <2 x i64>* %3310, align 16
  %3394 = trunc <2 x i64> %3393 to <2 x i32>
  %3395 = or i64 %3363, 12
  %3396 = getelementptr inbounds i32, i32* %1, i64 %3395
  %3397 = bitcast i32* %3396 to <2 x i32>*
  store <2 x i32> %3394, <2 x i32>* %3397, align 4
  %3398 = load <2 x i64>, <2 x i64>* %3312, align 16
  %3399 = trunc <2 x i64> %3398 to <2 x i32>
  %3400 = or i64 %3363, 14
  %3401 = getelementptr inbounds i32, i32* %1, i64 %3400
  %3402 = bitcast i32* %3401 to <2 x i32>*
  store <2 x i32> %3399, <2 x i32>* %3402, align 4
  %3403 = load <2 x i64>, <2 x i64>* %3314, align 16
  %3404 = trunc <2 x i64> %3403 to <2 x i32>
  %3405 = or i64 %3363, 16
  %3406 = getelementptr inbounds i32, i32* %1, i64 %3405
  %3407 = bitcast i32* %3406 to <2 x i32>*
  store <2 x i32> %3404, <2 x i32>* %3407, align 4
  %3408 = load <2 x i64>, <2 x i64>* %3316, align 16
  %3409 = trunc <2 x i64> %3408 to <2 x i32>
  %3410 = or i64 %3363, 18
  %3411 = getelementptr inbounds i32, i32* %1, i64 %3410
  %3412 = bitcast i32* %3411 to <2 x i32>*
  store <2 x i32> %3409, <2 x i32>* %3412, align 4
  %3413 = load <2 x i64>, <2 x i64>* %3318, align 16
  %3414 = trunc <2 x i64> %3413 to <2 x i32>
  %3415 = or i64 %3363, 20
  %3416 = getelementptr inbounds i32, i32* %1, i64 %3415
  %3417 = bitcast i32* %3416 to <2 x i32>*
  store <2 x i32> %3414, <2 x i32>* %3417, align 4
  %3418 = load <2 x i64>, <2 x i64>* %3320, align 16
  %3419 = trunc <2 x i64> %3418 to <2 x i32>
  %3420 = or i64 %3363, 22
  %3421 = getelementptr inbounds i32, i32* %1, i64 %3420
  %3422 = bitcast i32* %3421 to <2 x i32>*
  store <2 x i32> %3419, <2 x i32>* %3422, align 4
  %3423 = load <2 x i64>, <2 x i64>* %3322, align 16
  %3424 = trunc <2 x i64> %3423 to <2 x i32>
  %3425 = or i64 %3363, 24
  %3426 = getelementptr inbounds i32, i32* %1, i64 %3425
  %3427 = bitcast i32* %3426 to <2 x i32>*
  store <2 x i32> %3424, <2 x i32>* %3427, align 4
  %3428 = load <2 x i64>, <2 x i64>* %3324, align 16
  %3429 = trunc <2 x i64> %3428 to <2 x i32>
  %3430 = or i64 %3363, 26
  %3431 = getelementptr inbounds i32, i32* %1, i64 %3430
  %3432 = bitcast i32* %3431 to <2 x i32>*
  store <2 x i32> %3429, <2 x i32>* %3432, align 4
  %3433 = load <2 x i64>, <2 x i64>* %3326, align 16
  %3434 = trunc <2 x i64> %3433 to <2 x i32>
  %3435 = or i64 %3363, 28
  %3436 = getelementptr inbounds i32, i32* %1, i64 %3435
  %3437 = bitcast i32* %3436 to <2 x i32>*
  store <2 x i32> %3434, <2 x i32>* %3437, align 4
  %3438 = load <2 x i64>, <2 x i64>* %3328, align 16
  %3439 = trunc <2 x i64> %3438 to <2 x i32>
  %3440 = or i64 %3363, 30
  %3441 = getelementptr inbounds i32, i32* %1, i64 %3440
  %3442 = bitcast i32* %3441 to <2 x i32>*
  store <2 x i32> %3439, <2 x i32>* %3442, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %3295) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %3294) #6
  %3443 = add nuw nsw i64 %3330, 1
  %3444 = icmp eq i64 %3443, 32
  br i1 %3444, label %6136, label %3329

3445:                                             ; preds = %3192
  %3446 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %3026, <8 x i16> %2132) #6
  %3447 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2132, <8 x i16> %3026) #6
  %3448 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2135, <8 x i16> %3027) #6
  %3449 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %3027, <8 x i16> %2135) #6
  %3450 = add <8 x i16> %3446, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3451 = icmp ult <8 x i16> %3450, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3452 = add <8 x i16> %3447, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3453 = icmp ult <8 x i16> %3452, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3454 = add <8 x i16> %3448, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3455 = icmp ult <8 x i16> %3454, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3456 = add <8 x i16> %3449, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3457 = icmp ult <8 x i16> %3456, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3458 = or <8 x i1> %3453, %3451
  %3459 = or <8 x i1> %3458, %3455
  %3460 = or <8 x i1> %3459, %3457
  %3461 = sext <8 x i1> %3460 to <8 x i16>
  %3462 = bitcast <8 x i16> %3461 to <16 x i8>
  %3463 = icmp slt <16 x i8> %3462, zeroinitializer
  %3464 = bitcast <16 x i1> %3463 to i16
  %3465 = icmp eq i16 %3464, 0
  br i1 %3465, label %3620, label %3466

3466:                                             ; preds = %3445
  br i1 %97, label %3467, label %3468

3467:                                             ; preds = %3466
  tail call void @vpx_highbd_fdct32x32_rd_c(i16* %0, i32* %1, i32 %2) #6
  br label %6136

3468:                                             ; preds = %3466
  %3469 = bitcast [32 x i64]* %4 to i8*
  %3470 = bitcast [32 x i64]* %5 to i8*
  %3471 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %3472 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %3473 = bitcast [32 x i64]* %5 to <2 x i64>*
  %3474 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %3475 = bitcast i64* %3474 to <2 x i64>*
  %3476 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %3477 = bitcast i64* %3476 to <2 x i64>*
  %3478 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %3479 = bitcast i64* %3478 to <2 x i64>*
  %3480 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %3481 = bitcast i64* %3480 to <2 x i64>*
  %3482 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %3483 = bitcast i64* %3482 to <2 x i64>*
  %3484 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %3485 = bitcast i64* %3484 to <2 x i64>*
  %3486 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %3487 = bitcast i64* %3486 to <2 x i64>*
  %3488 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %3489 = bitcast i64* %3488 to <2 x i64>*
  %3490 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %3491 = bitcast i64* %3490 to <2 x i64>*
  %3492 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %3493 = bitcast i64* %3492 to <2 x i64>*
  %3494 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %3495 = bitcast i64* %3494 to <2 x i64>*
  %3496 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %3497 = bitcast i64* %3496 to <2 x i64>*
  %3498 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %3499 = bitcast i64* %3498 to <2 x i64>*
  %3500 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %3501 = bitcast i64* %3500 to <2 x i64>*
  %3502 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %3503 = bitcast i64* %3502 to <2 x i64>*
  br label %3504

3504:                                             ; preds = %3537, %3468
  %3505 = phi i64 [ 0, %3468 ], [ %3618, %3537 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %3469) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %3469, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %3470) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %3470, i8 -86, i64 256, i1 false) #6
  br label %3506

3506:                                             ; preds = %3506, %3504
  %3507 = phi i64 [ 0, %3504 ], [ %3535, %3506 ]
  %3508 = shl i64 %3507, 5
  %3509 = add nuw nsw i64 %3508, %3505
  %3510 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %3509
  %3511 = load i16, i16* %3510, align 2
  %3512 = sext i16 %3511 to i64
  %3513 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %3507
  store i64 %3512, i64* %3513, align 16
  %3514 = or i64 %3507, 1
  %3515 = shl i64 %3514, 5
  %3516 = add nuw nsw i64 %3515, %3505
  %3517 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %3516
  %3518 = load i16, i16* %3517, align 2
  %3519 = sext i16 %3518 to i64
  %3520 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %3514
  store i64 %3519, i64* %3520, align 8
  %3521 = or i64 %3507, 2
  %3522 = shl i64 %3521, 5
  %3523 = add nuw nsw i64 %3522, %3505
  %3524 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %3523
  %3525 = load i16, i16* %3524, align 2
  %3526 = sext i16 %3525 to i64
  %3527 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %3521
  store i64 %3526, i64* %3527, align 16
  %3528 = or i64 %3507, 3
  %3529 = shl i64 %3528, 5
  %3530 = add nuw nsw i64 %3529, %3505
  %3531 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %3530
  %3532 = load i16, i16* %3531, align 2
  %3533 = sext i16 %3532 to i64
  %3534 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %3528
  store i64 %3533, i64* %3534, align 8
  %3535 = add nuw nsw i64 %3507, 4
  %3536 = icmp eq i64 %3535, 32
  br i1 %3536, label %3537, label %3506

3537:                                             ; preds = %3506
  call void @vpx_fdct32(i64* nonnull %3471, i64* nonnull %3472, i32 1) #6
  %3538 = shl i64 %3505, 5
  %3539 = load <2 x i64>, <2 x i64>* %3473, align 16
  %3540 = trunc <2 x i64> %3539 to <2 x i32>
  %3541 = getelementptr inbounds i32, i32* %1, i64 %3538
  %3542 = bitcast i32* %3541 to <2 x i32>*
  store <2 x i32> %3540, <2 x i32>* %3542, align 4
  %3543 = load <2 x i64>, <2 x i64>* %3475, align 16
  %3544 = trunc <2 x i64> %3543 to <2 x i32>
  %3545 = or i64 %3538, 2
  %3546 = getelementptr inbounds i32, i32* %1, i64 %3545
  %3547 = bitcast i32* %3546 to <2 x i32>*
  store <2 x i32> %3544, <2 x i32>* %3547, align 4
  %3548 = load <2 x i64>, <2 x i64>* %3477, align 16
  %3549 = trunc <2 x i64> %3548 to <2 x i32>
  %3550 = or i64 %3538, 4
  %3551 = getelementptr inbounds i32, i32* %1, i64 %3550
  %3552 = bitcast i32* %3551 to <2 x i32>*
  store <2 x i32> %3549, <2 x i32>* %3552, align 4
  %3553 = load <2 x i64>, <2 x i64>* %3479, align 16
  %3554 = trunc <2 x i64> %3553 to <2 x i32>
  %3555 = or i64 %3538, 6
  %3556 = getelementptr inbounds i32, i32* %1, i64 %3555
  %3557 = bitcast i32* %3556 to <2 x i32>*
  store <2 x i32> %3554, <2 x i32>* %3557, align 4
  %3558 = load <2 x i64>, <2 x i64>* %3481, align 16
  %3559 = trunc <2 x i64> %3558 to <2 x i32>
  %3560 = or i64 %3538, 8
  %3561 = getelementptr inbounds i32, i32* %1, i64 %3560
  %3562 = bitcast i32* %3561 to <2 x i32>*
  store <2 x i32> %3559, <2 x i32>* %3562, align 4
  %3563 = load <2 x i64>, <2 x i64>* %3483, align 16
  %3564 = trunc <2 x i64> %3563 to <2 x i32>
  %3565 = or i64 %3538, 10
  %3566 = getelementptr inbounds i32, i32* %1, i64 %3565
  %3567 = bitcast i32* %3566 to <2 x i32>*
  store <2 x i32> %3564, <2 x i32>* %3567, align 4
  %3568 = load <2 x i64>, <2 x i64>* %3485, align 16
  %3569 = trunc <2 x i64> %3568 to <2 x i32>
  %3570 = or i64 %3538, 12
  %3571 = getelementptr inbounds i32, i32* %1, i64 %3570
  %3572 = bitcast i32* %3571 to <2 x i32>*
  store <2 x i32> %3569, <2 x i32>* %3572, align 4
  %3573 = load <2 x i64>, <2 x i64>* %3487, align 16
  %3574 = trunc <2 x i64> %3573 to <2 x i32>
  %3575 = or i64 %3538, 14
  %3576 = getelementptr inbounds i32, i32* %1, i64 %3575
  %3577 = bitcast i32* %3576 to <2 x i32>*
  store <2 x i32> %3574, <2 x i32>* %3577, align 4
  %3578 = load <2 x i64>, <2 x i64>* %3489, align 16
  %3579 = trunc <2 x i64> %3578 to <2 x i32>
  %3580 = or i64 %3538, 16
  %3581 = getelementptr inbounds i32, i32* %1, i64 %3580
  %3582 = bitcast i32* %3581 to <2 x i32>*
  store <2 x i32> %3579, <2 x i32>* %3582, align 4
  %3583 = load <2 x i64>, <2 x i64>* %3491, align 16
  %3584 = trunc <2 x i64> %3583 to <2 x i32>
  %3585 = or i64 %3538, 18
  %3586 = getelementptr inbounds i32, i32* %1, i64 %3585
  %3587 = bitcast i32* %3586 to <2 x i32>*
  store <2 x i32> %3584, <2 x i32>* %3587, align 4
  %3588 = load <2 x i64>, <2 x i64>* %3493, align 16
  %3589 = trunc <2 x i64> %3588 to <2 x i32>
  %3590 = or i64 %3538, 20
  %3591 = getelementptr inbounds i32, i32* %1, i64 %3590
  %3592 = bitcast i32* %3591 to <2 x i32>*
  store <2 x i32> %3589, <2 x i32>* %3592, align 4
  %3593 = load <2 x i64>, <2 x i64>* %3495, align 16
  %3594 = trunc <2 x i64> %3593 to <2 x i32>
  %3595 = or i64 %3538, 22
  %3596 = getelementptr inbounds i32, i32* %1, i64 %3595
  %3597 = bitcast i32* %3596 to <2 x i32>*
  store <2 x i32> %3594, <2 x i32>* %3597, align 4
  %3598 = load <2 x i64>, <2 x i64>* %3497, align 16
  %3599 = trunc <2 x i64> %3598 to <2 x i32>
  %3600 = or i64 %3538, 24
  %3601 = getelementptr inbounds i32, i32* %1, i64 %3600
  %3602 = bitcast i32* %3601 to <2 x i32>*
  store <2 x i32> %3599, <2 x i32>* %3602, align 4
  %3603 = load <2 x i64>, <2 x i64>* %3499, align 16
  %3604 = trunc <2 x i64> %3603 to <2 x i32>
  %3605 = or i64 %3538, 26
  %3606 = getelementptr inbounds i32, i32* %1, i64 %3605
  %3607 = bitcast i32* %3606 to <2 x i32>*
  store <2 x i32> %3604, <2 x i32>* %3607, align 4
  %3608 = load <2 x i64>, <2 x i64>* %3501, align 16
  %3609 = trunc <2 x i64> %3608 to <2 x i32>
  %3610 = or i64 %3538, 28
  %3611 = getelementptr inbounds i32, i32* %1, i64 %3610
  %3612 = bitcast i32* %3611 to <2 x i32>*
  store <2 x i32> %3609, <2 x i32>* %3612, align 4
  %3613 = load <2 x i64>, <2 x i64>* %3503, align 16
  %3614 = trunc <2 x i64> %3613 to <2 x i32>
  %3615 = or i64 %3538, 30
  %3616 = getelementptr inbounds i32, i32* %1, i64 %3615
  %3617 = bitcast i32* %3616 to <2 x i32>*
  store <2 x i32> %3614, <2 x i32>* %3617, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %3470) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %3469) #6
  %3618 = add nuw nsw i64 %3505, 1
  %3619 = icmp eq i64 %3618, 32
  br i1 %3619, label %6136, label %3504

3620:                                             ; preds = %3445
  %3621 = shufflevector <8 x i16> %2772, <8 x i16> %2773, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3622 = shufflevector <8 x i16> %2772, <8 x i16> %2773, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3623 = shufflevector <8 x i16> %2774, <8 x i16> %2775, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3624 = shufflevector <8 x i16> %2774, <8 x i16> %2775, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3625 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3621, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %3626 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3622, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %3627 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3621, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %3628 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3622, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %3629 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3623, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %3630 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3624, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %3631 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3623, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %3632 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3624, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %3633 = add <4 x i32> %3625, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3634 = add <4 x i32> %3626, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3635 = add <4 x i32> %3627, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3636 = add <4 x i32> %3628, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3637 = add <4 x i32> %3629, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3638 = add <4 x i32> %3630, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3639 = add <4 x i32> %3631, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3640 = add <4 x i32> %3632, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3641 = ashr <4 x i32> %3633, <i32 14, i32 14, i32 14, i32 14>
  %3642 = ashr <4 x i32> %3634, <i32 14, i32 14, i32 14, i32 14>
  %3643 = ashr <4 x i32> %3635, <i32 14, i32 14, i32 14, i32 14>
  %3644 = ashr <4 x i32> %3636, <i32 14, i32 14, i32 14, i32 14>
  %3645 = ashr <4 x i32> %3637, <i32 14, i32 14, i32 14, i32 14>
  %3646 = ashr <4 x i32> %3638, <i32 14, i32 14, i32 14, i32 14>
  %3647 = ashr <4 x i32> %3639, <i32 14, i32 14, i32 14, i32 14>
  %3648 = ashr <4 x i32> %3640, <i32 14, i32 14, i32 14, i32 14>
  %3649 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3641, <4 x i32> %3642) #6
  store <8 x i16> %3649, <8 x i16>* %32, align 16
  %3650 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3643, <4 x i32> %3644) #6
  store <8 x i16> %3650, <8 x i16>* %34, align 16
  %3651 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3645, <4 x i32> %3646) #6
  store <8 x i16> %3651, <8 x i16>* %36, align 16
  %3652 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3647, <4 x i32> %3648) #6
  store <8 x i16> %3652, <8 x i16>* %38, align 16
  %3653 = add <8 x i16> %3649, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3654 = icmp ult <8 x i16> %3653, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3655 = add <8 x i16> %3650, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3656 = icmp ult <8 x i16> %3655, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3657 = add <8 x i16> %3651, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3658 = icmp ult <8 x i16> %3657, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3659 = add <8 x i16> %3652, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3660 = icmp ult <8 x i16> %3659, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3661 = or <8 x i1> %3656, %3654
  %3662 = or <8 x i1> %3661, %3658
  %3663 = or <8 x i1> %3662, %3660
  %3664 = sext <8 x i1> %3663 to <8 x i16>
  %3665 = bitcast <8 x i16> %3664 to <16 x i8>
  %3666 = icmp slt <16 x i8> %3665, zeroinitializer
  %3667 = bitcast <16 x i1> %3666 to i16
  %3668 = icmp eq i16 %3667, 0
  br i1 %3668, label %3823, label %3669

3669:                                             ; preds = %3620
  br i1 %97, label %3670, label %3671

3670:                                             ; preds = %3669
  tail call void @vpx_highbd_fdct32x32_rd_c(i16* %0, i32* %1, i32 %2) #6
  br label %6136

3671:                                             ; preds = %3669
  %3672 = bitcast [32 x i64]* %4 to i8*
  %3673 = bitcast [32 x i64]* %5 to i8*
  %3674 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %3675 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %3676 = bitcast [32 x i64]* %5 to <2 x i64>*
  %3677 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %3678 = bitcast i64* %3677 to <2 x i64>*
  %3679 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %3680 = bitcast i64* %3679 to <2 x i64>*
  %3681 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %3682 = bitcast i64* %3681 to <2 x i64>*
  %3683 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %3684 = bitcast i64* %3683 to <2 x i64>*
  %3685 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %3686 = bitcast i64* %3685 to <2 x i64>*
  %3687 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %3688 = bitcast i64* %3687 to <2 x i64>*
  %3689 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %3690 = bitcast i64* %3689 to <2 x i64>*
  %3691 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %3692 = bitcast i64* %3691 to <2 x i64>*
  %3693 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %3694 = bitcast i64* %3693 to <2 x i64>*
  %3695 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %3696 = bitcast i64* %3695 to <2 x i64>*
  %3697 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %3698 = bitcast i64* %3697 to <2 x i64>*
  %3699 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %3700 = bitcast i64* %3699 to <2 x i64>*
  %3701 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %3702 = bitcast i64* %3701 to <2 x i64>*
  %3703 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %3704 = bitcast i64* %3703 to <2 x i64>*
  %3705 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %3706 = bitcast i64* %3705 to <2 x i64>*
  br label %3707

3707:                                             ; preds = %3740, %3671
  %3708 = phi i64 [ 0, %3671 ], [ %3821, %3740 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %3672) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %3672, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %3673) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %3673, i8 -86, i64 256, i1 false) #6
  br label %3709

3709:                                             ; preds = %3709, %3707
  %3710 = phi i64 [ 0, %3707 ], [ %3738, %3709 ]
  %3711 = shl i64 %3710, 5
  %3712 = add nuw nsw i64 %3711, %3708
  %3713 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %3712
  %3714 = load i16, i16* %3713, align 2
  %3715 = sext i16 %3714 to i64
  %3716 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %3710
  store i64 %3715, i64* %3716, align 16
  %3717 = or i64 %3710, 1
  %3718 = shl i64 %3717, 5
  %3719 = add nuw nsw i64 %3718, %3708
  %3720 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %3719
  %3721 = load i16, i16* %3720, align 2
  %3722 = sext i16 %3721 to i64
  %3723 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %3717
  store i64 %3722, i64* %3723, align 8
  %3724 = or i64 %3710, 2
  %3725 = shl i64 %3724, 5
  %3726 = add nuw nsw i64 %3725, %3708
  %3727 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %3726
  %3728 = load i16, i16* %3727, align 2
  %3729 = sext i16 %3728 to i64
  %3730 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %3724
  store i64 %3729, i64* %3730, align 16
  %3731 = or i64 %3710, 3
  %3732 = shl i64 %3731, 5
  %3733 = add nuw nsw i64 %3732, %3708
  %3734 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %3733
  %3735 = load i16, i16* %3734, align 2
  %3736 = sext i16 %3735 to i64
  %3737 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %3731
  store i64 %3736, i64* %3737, align 8
  %3738 = add nuw nsw i64 %3710, 4
  %3739 = icmp eq i64 %3738, 32
  br i1 %3739, label %3740, label %3709

3740:                                             ; preds = %3709
  call void @vpx_fdct32(i64* nonnull %3674, i64* nonnull %3675, i32 1) #6
  %3741 = shl i64 %3708, 5
  %3742 = load <2 x i64>, <2 x i64>* %3676, align 16
  %3743 = trunc <2 x i64> %3742 to <2 x i32>
  %3744 = getelementptr inbounds i32, i32* %1, i64 %3741
  %3745 = bitcast i32* %3744 to <2 x i32>*
  store <2 x i32> %3743, <2 x i32>* %3745, align 4
  %3746 = load <2 x i64>, <2 x i64>* %3678, align 16
  %3747 = trunc <2 x i64> %3746 to <2 x i32>
  %3748 = or i64 %3741, 2
  %3749 = getelementptr inbounds i32, i32* %1, i64 %3748
  %3750 = bitcast i32* %3749 to <2 x i32>*
  store <2 x i32> %3747, <2 x i32>* %3750, align 4
  %3751 = load <2 x i64>, <2 x i64>* %3680, align 16
  %3752 = trunc <2 x i64> %3751 to <2 x i32>
  %3753 = or i64 %3741, 4
  %3754 = getelementptr inbounds i32, i32* %1, i64 %3753
  %3755 = bitcast i32* %3754 to <2 x i32>*
  store <2 x i32> %3752, <2 x i32>* %3755, align 4
  %3756 = load <2 x i64>, <2 x i64>* %3682, align 16
  %3757 = trunc <2 x i64> %3756 to <2 x i32>
  %3758 = or i64 %3741, 6
  %3759 = getelementptr inbounds i32, i32* %1, i64 %3758
  %3760 = bitcast i32* %3759 to <2 x i32>*
  store <2 x i32> %3757, <2 x i32>* %3760, align 4
  %3761 = load <2 x i64>, <2 x i64>* %3684, align 16
  %3762 = trunc <2 x i64> %3761 to <2 x i32>
  %3763 = or i64 %3741, 8
  %3764 = getelementptr inbounds i32, i32* %1, i64 %3763
  %3765 = bitcast i32* %3764 to <2 x i32>*
  store <2 x i32> %3762, <2 x i32>* %3765, align 4
  %3766 = load <2 x i64>, <2 x i64>* %3686, align 16
  %3767 = trunc <2 x i64> %3766 to <2 x i32>
  %3768 = or i64 %3741, 10
  %3769 = getelementptr inbounds i32, i32* %1, i64 %3768
  %3770 = bitcast i32* %3769 to <2 x i32>*
  store <2 x i32> %3767, <2 x i32>* %3770, align 4
  %3771 = load <2 x i64>, <2 x i64>* %3688, align 16
  %3772 = trunc <2 x i64> %3771 to <2 x i32>
  %3773 = or i64 %3741, 12
  %3774 = getelementptr inbounds i32, i32* %1, i64 %3773
  %3775 = bitcast i32* %3774 to <2 x i32>*
  store <2 x i32> %3772, <2 x i32>* %3775, align 4
  %3776 = load <2 x i64>, <2 x i64>* %3690, align 16
  %3777 = trunc <2 x i64> %3776 to <2 x i32>
  %3778 = or i64 %3741, 14
  %3779 = getelementptr inbounds i32, i32* %1, i64 %3778
  %3780 = bitcast i32* %3779 to <2 x i32>*
  store <2 x i32> %3777, <2 x i32>* %3780, align 4
  %3781 = load <2 x i64>, <2 x i64>* %3692, align 16
  %3782 = trunc <2 x i64> %3781 to <2 x i32>
  %3783 = or i64 %3741, 16
  %3784 = getelementptr inbounds i32, i32* %1, i64 %3783
  %3785 = bitcast i32* %3784 to <2 x i32>*
  store <2 x i32> %3782, <2 x i32>* %3785, align 4
  %3786 = load <2 x i64>, <2 x i64>* %3694, align 16
  %3787 = trunc <2 x i64> %3786 to <2 x i32>
  %3788 = or i64 %3741, 18
  %3789 = getelementptr inbounds i32, i32* %1, i64 %3788
  %3790 = bitcast i32* %3789 to <2 x i32>*
  store <2 x i32> %3787, <2 x i32>* %3790, align 4
  %3791 = load <2 x i64>, <2 x i64>* %3696, align 16
  %3792 = trunc <2 x i64> %3791 to <2 x i32>
  %3793 = or i64 %3741, 20
  %3794 = getelementptr inbounds i32, i32* %1, i64 %3793
  %3795 = bitcast i32* %3794 to <2 x i32>*
  store <2 x i32> %3792, <2 x i32>* %3795, align 4
  %3796 = load <2 x i64>, <2 x i64>* %3698, align 16
  %3797 = trunc <2 x i64> %3796 to <2 x i32>
  %3798 = or i64 %3741, 22
  %3799 = getelementptr inbounds i32, i32* %1, i64 %3798
  %3800 = bitcast i32* %3799 to <2 x i32>*
  store <2 x i32> %3797, <2 x i32>* %3800, align 4
  %3801 = load <2 x i64>, <2 x i64>* %3700, align 16
  %3802 = trunc <2 x i64> %3801 to <2 x i32>
  %3803 = or i64 %3741, 24
  %3804 = getelementptr inbounds i32, i32* %1, i64 %3803
  %3805 = bitcast i32* %3804 to <2 x i32>*
  store <2 x i32> %3802, <2 x i32>* %3805, align 4
  %3806 = load <2 x i64>, <2 x i64>* %3702, align 16
  %3807 = trunc <2 x i64> %3806 to <2 x i32>
  %3808 = or i64 %3741, 26
  %3809 = getelementptr inbounds i32, i32* %1, i64 %3808
  %3810 = bitcast i32* %3809 to <2 x i32>*
  store <2 x i32> %3807, <2 x i32>* %3810, align 4
  %3811 = load <2 x i64>, <2 x i64>* %3704, align 16
  %3812 = trunc <2 x i64> %3811 to <2 x i32>
  %3813 = or i64 %3741, 28
  %3814 = getelementptr inbounds i32, i32* %1, i64 %3813
  %3815 = bitcast i32* %3814 to <2 x i32>*
  store <2 x i32> %3812, <2 x i32>* %3815, align 4
  %3816 = load <2 x i64>, <2 x i64>* %3706, align 16
  %3817 = trunc <2 x i64> %3816 to <2 x i32>
  %3818 = or i64 %3741, 30
  %3819 = getelementptr inbounds i32, i32* %1, i64 %3818
  %3820 = bitcast i32* %3819 to <2 x i32>*
  store <2 x i32> %3817, <2 x i32>* %3820, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %3673) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %3672) #6
  %3821 = add nuw nsw i64 %3708, 1
  %3822 = icmp eq i64 %3821, 32
  br i1 %3822, label %6136, label %3707

3823:                                             ; preds = %3620
  %3824 = shufflevector <8 x i16> %2777, <8 x i16> %2782, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3825 = shufflevector <8 x i16> %2777, <8 x i16> %2782, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3826 = shufflevector <8 x i16> %2778, <8 x i16> %2781, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3827 = shufflevector <8 x i16> %2778, <8 x i16> %2781, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3828 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3824, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %3829 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3825, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %3830 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3826, <8 x i16> <i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137>) #6
  %3831 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3827, <8 x i16> <i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137>) #6
  %3832 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3826, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %3833 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3827, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %3834 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3824, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %3835 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3825, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %3836 = add <4 x i32> %3828, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3837 = add <4 x i32> %3829, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3838 = add <4 x i32> %3830, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3839 = add <4 x i32> %3831, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3840 = add <4 x i32> %3832, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3841 = add <4 x i32> %3833, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3842 = add <4 x i32> %3834, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3843 = add <4 x i32> %3835, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3844 = ashr <4 x i32> %3836, <i32 14, i32 14, i32 14, i32 14>
  %3845 = ashr <4 x i32> %3837, <i32 14, i32 14, i32 14, i32 14>
  %3846 = ashr <4 x i32> %3838, <i32 14, i32 14, i32 14, i32 14>
  %3847 = ashr <4 x i32> %3839, <i32 14, i32 14, i32 14, i32 14>
  %3848 = ashr <4 x i32> %3840, <i32 14, i32 14, i32 14, i32 14>
  %3849 = ashr <4 x i32> %3841, <i32 14, i32 14, i32 14, i32 14>
  %3850 = ashr <4 x i32> %3842, <i32 14, i32 14, i32 14, i32 14>
  %3851 = ashr <4 x i32> %3843, <i32 14, i32 14, i32 14, i32 14>
  %3852 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3844, <4 x i32> %3845) #6
  %3853 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3846, <4 x i32> %3847) #6
  %3854 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3848, <4 x i32> %3849) #6
  %3855 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3850, <4 x i32> %3851) #6
  %3856 = add <8 x i16> %3852, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3857 = icmp ult <8 x i16> %3856, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3858 = add <8 x i16> %3853, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3859 = icmp ult <8 x i16> %3858, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3860 = add <8 x i16> %3854, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3861 = icmp ult <8 x i16> %3860, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3862 = add <8 x i16> %3855, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3863 = icmp ult <8 x i16> %3862, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3864 = or <8 x i1> %3859, %3857
  %3865 = or <8 x i1> %3864, %3861
  %3866 = or <8 x i1> %3865, %3863
  %3867 = sext <8 x i1> %3866 to <8 x i16>
  %3868 = bitcast <8 x i16> %3867 to <16 x i8>
  %3869 = icmp slt <16 x i8> %3868, zeroinitializer
  %3870 = bitcast <16 x i1> %3869 to i16
  %3871 = icmp eq i16 %3870, 0
  br i1 %3871, label %4026, label %3872

3872:                                             ; preds = %3823
  br i1 %97, label %3873, label %3874

3873:                                             ; preds = %3872
  tail call void @vpx_highbd_fdct32x32_rd_c(i16* %0, i32* %1, i32 %2) #6
  br label %6136

3874:                                             ; preds = %3872
  %3875 = bitcast [32 x i64]* %4 to i8*
  %3876 = bitcast [32 x i64]* %5 to i8*
  %3877 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %3878 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %3879 = bitcast [32 x i64]* %5 to <2 x i64>*
  %3880 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %3881 = bitcast i64* %3880 to <2 x i64>*
  %3882 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %3883 = bitcast i64* %3882 to <2 x i64>*
  %3884 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %3885 = bitcast i64* %3884 to <2 x i64>*
  %3886 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %3887 = bitcast i64* %3886 to <2 x i64>*
  %3888 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %3889 = bitcast i64* %3888 to <2 x i64>*
  %3890 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %3891 = bitcast i64* %3890 to <2 x i64>*
  %3892 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %3893 = bitcast i64* %3892 to <2 x i64>*
  %3894 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %3895 = bitcast i64* %3894 to <2 x i64>*
  %3896 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %3897 = bitcast i64* %3896 to <2 x i64>*
  %3898 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %3899 = bitcast i64* %3898 to <2 x i64>*
  %3900 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %3901 = bitcast i64* %3900 to <2 x i64>*
  %3902 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %3903 = bitcast i64* %3902 to <2 x i64>*
  %3904 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %3905 = bitcast i64* %3904 to <2 x i64>*
  %3906 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %3907 = bitcast i64* %3906 to <2 x i64>*
  %3908 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %3909 = bitcast i64* %3908 to <2 x i64>*
  br label %3910

3910:                                             ; preds = %3943, %3874
  %3911 = phi i64 [ 0, %3874 ], [ %4024, %3943 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %3875) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %3875, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %3876) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %3876, i8 -86, i64 256, i1 false) #6
  br label %3912

3912:                                             ; preds = %3912, %3910
  %3913 = phi i64 [ 0, %3910 ], [ %3941, %3912 ]
  %3914 = shl i64 %3913, 5
  %3915 = add nuw nsw i64 %3914, %3911
  %3916 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %3915
  %3917 = load i16, i16* %3916, align 2
  %3918 = sext i16 %3917 to i64
  %3919 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %3913
  store i64 %3918, i64* %3919, align 16
  %3920 = or i64 %3913, 1
  %3921 = shl i64 %3920, 5
  %3922 = add nuw nsw i64 %3921, %3911
  %3923 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %3922
  %3924 = load i16, i16* %3923, align 2
  %3925 = sext i16 %3924 to i64
  %3926 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %3920
  store i64 %3925, i64* %3926, align 8
  %3927 = or i64 %3913, 2
  %3928 = shl i64 %3927, 5
  %3929 = add nuw nsw i64 %3928, %3911
  %3930 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %3929
  %3931 = load i16, i16* %3930, align 2
  %3932 = sext i16 %3931 to i64
  %3933 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %3927
  store i64 %3932, i64* %3933, align 16
  %3934 = or i64 %3913, 3
  %3935 = shl i64 %3934, 5
  %3936 = add nuw nsw i64 %3935, %3911
  %3937 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %3936
  %3938 = load i16, i16* %3937, align 2
  %3939 = sext i16 %3938 to i64
  %3940 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %3934
  store i64 %3939, i64* %3940, align 8
  %3941 = add nuw nsw i64 %3913, 4
  %3942 = icmp eq i64 %3941, 32
  br i1 %3942, label %3943, label %3912

3943:                                             ; preds = %3912
  call void @vpx_fdct32(i64* nonnull %3877, i64* nonnull %3878, i32 1) #6
  %3944 = shl i64 %3911, 5
  %3945 = load <2 x i64>, <2 x i64>* %3879, align 16
  %3946 = trunc <2 x i64> %3945 to <2 x i32>
  %3947 = getelementptr inbounds i32, i32* %1, i64 %3944
  %3948 = bitcast i32* %3947 to <2 x i32>*
  store <2 x i32> %3946, <2 x i32>* %3948, align 4
  %3949 = load <2 x i64>, <2 x i64>* %3881, align 16
  %3950 = trunc <2 x i64> %3949 to <2 x i32>
  %3951 = or i64 %3944, 2
  %3952 = getelementptr inbounds i32, i32* %1, i64 %3951
  %3953 = bitcast i32* %3952 to <2 x i32>*
  store <2 x i32> %3950, <2 x i32>* %3953, align 4
  %3954 = load <2 x i64>, <2 x i64>* %3883, align 16
  %3955 = trunc <2 x i64> %3954 to <2 x i32>
  %3956 = or i64 %3944, 4
  %3957 = getelementptr inbounds i32, i32* %1, i64 %3956
  %3958 = bitcast i32* %3957 to <2 x i32>*
  store <2 x i32> %3955, <2 x i32>* %3958, align 4
  %3959 = load <2 x i64>, <2 x i64>* %3885, align 16
  %3960 = trunc <2 x i64> %3959 to <2 x i32>
  %3961 = or i64 %3944, 6
  %3962 = getelementptr inbounds i32, i32* %1, i64 %3961
  %3963 = bitcast i32* %3962 to <2 x i32>*
  store <2 x i32> %3960, <2 x i32>* %3963, align 4
  %3964 = load <2 x i64>, <2 x i64>* %3887, align 16
  %3965 = trunc <2 x i64> %3964 to <2 x i32>
  %3966 = or i64 %3944, 8
  %3967 = getelementptr inbounds i32, i32* %1, i64 %3966
  %3968 = bitcast i32* %3967 to <2 x i32>*
  store <2 x i32> %3965, <2 x i32>* %3968, align 4
  %3969 = load <2 x i64>, <2 x i64>* %3889, align 16
  %3970 = trunc <2 x i64> %3969 to <2 x i32>
  %3971 = or i64 %3944, 10
  %3972 = getelementptr inbounds i32, i32* %1, i64 %3971
  %3973 = bitcast i32* %3972 to <2 x i32>*
  store <2 x i32> %3970, <2 x i32>* %3973, align 4
  %3974 = load <2 x i64>, <2 x i64>* %3891, align 16
  %3975 = trunc <2 x i64> %3974 to <2 x i32>
  %3976 = or i64 %3944, 12
  %3977 = getelementptr inbounds i32, i32* %1, i64 %3976
  %3978 = bitcast i32* %3977 to <2 x i32>*
  store <2 x i32> %3975, <2 x i32>* %3978, align 4
  %3979 = load <2 x i64>, <2 x i64>* %3893, align 16
  %3980 = trunc <2 x i64> %3979 to <2 x i32>
  %3981 = or i64 %3944, 14
  %3982 = getelementptr inbounds i32, i32* %1, i64 %3981
  %3983 = bitcast i32* %3982 to <2 x i32>*
  store <2 x i32> %3980, <2 x i32>* %3983, align 4
  %3984 = load <2 x i64>, <2 x i64>* %3895, align 16
  %3985 = trunc <2 x i64> %3984 to <2 x i32>
  %3986 = or i64 %3944, 16
  %3987 = getelementptr inbounds i32, i32* %1, i64 %3986
  %3988 = bitcast i32* %3987 to <2 x i32>*
  store <2 x i32> %3985, <2 x i32>* %3988, align 4
  %3989 = load <2 x i64>, <2 x i64>* %3897, align 16
  %3990 = trunc <2 x i64> %3989 to <2 x i32>
  %3991 = or i64 %3944, 18
  %3992 = getelementptr inbounds i32, i32* %1, i64 %3991
  %3993 = bitcast i32* %3992 to <2 x i32>*
  store <2 x i32> %3990, <2 x i32>* %3993, align 4
  %3994 = load <2 x i64>, <2 x i64>* %3899, align 16
  %3995 = trunc <2 x i64> %3994 to <2 x i32>
  %3996 = or i64 %3944, 20
  %3997 = getelementptr inbounds i32, i32* %1, i64 %3996
  %3998 = bitcast i32* %3997 to <2 x i32>*
  store <2 x i32> %3995, <2 x i32>* %3998, align 4
  %3999 = load <2 x i64>, <2 x i64>* %3901, align 16
  %4000 = trunc <2 x i64> %3999 to <2 x i32>
  %4001 = or i64 %3944, 22
  %4002 = getelementptr inbounds i32, i32* %1, i64 %4001
  %4003 = bitcast i32* %4002 to <2 x i32>*
  store <2 x i32> %4000, <2 x i32>* %4003, align 4
  %4004 = load <2 x i64>, <2 x i64>* %3903, align 16
  %4005 = trunc <2 x i64> %4004 to <2 x i32>
  %4006 = or i64 %3944, 24
  %4007 = getelementptr inbounds i32, i32* %1, i64 %4006
  %4008 = bitcast i32* %4007 to <2 x i32>*
  store <2 x i32> %4005, <2 x i32>* %4008, align 4
  %4009 = load <2 x i64>, <2 x i64>* %3905, align 16
  %4010 = trunc <2 x i64> %4009 to <2 x i32>
  %4011 = or i64 %3944, 26
  %4012 = getelementptr inbounds i32, i32* %1, i64 %4011
  %4013 = bitcast i32* %4012 to <2 x i32>*
  store <2 x i32> %4010, <2 x i32>* %4013, align 4
  %4014 = load <2 x i64>, <2 x i64>* %3907, align 16
  %4015 = trunc <2 x i64> %4014 to <2 x i32>
  %4016 = or i64 %3944, 28
  %4017 = getelementptr inbounds i32, i32* %1, i64 %4016
  %4018 = bitcast i32* %4017 to <2 x i32>*
  store <2 x i32> %4015, <2 x i32>* %4018, align 4
  %4019 = load <2 x i64>, <2 x i64>* %3909, align 16
  %4020 = trunc <2 x i64> %4019 to <2 x i32>
  %4021 = or i64 %3944, 30
  %4022 = getelementptr inbounds i32, i32* %1, i64 %4021
  %4023 = bitcast i32* %4022 to <2 x i32>*
  store <2 x i32> %4020, <2 x i32>* %4023, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %3876) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %3875) #6
  %4024 = add nuw nsw i64 %3911, 1
  %4025 = icmp eq i64 %4024, 32
  br i1 %4025, label %6136, label %3910

4026:                                             ; preds = %3823
  %4027 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %3250, <8 x i16> %2528) #6
  %4028 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %3249, <8 x i16> %2529) #6
  %4029 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2529, <8 x i16> %3249) #6
  %4030 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2528, <8 x i16> %3250) #6
  %4031 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2535, <8 x i16> %3251) #6
  %4032 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2534, <8 x i16> %3252) #6
  %4033 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %3252, <8 x i16> %2534) #6
  %4034 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %3251, <8 x i16> %2535) #6
  %4035 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %3254, <8 x i16> %2536) #6
  %4036 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %3253, <8 x i16> %2537) #6
  %4037 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2537, <8 x i16> %3253) #6
  %4038 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2536, <8 x i16> %3254) #6
  %4039 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2543, <8 x i16> %3255) #6
  %4040 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2542, <8 x i16> %3256) #6
  %4041 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %3256, <8 x i16> %2542) #6
  %4042 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %3255, <8 x i16> %2543) #6
  %4043 = add <8 x i16> %4027, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4044 = icmp ult <8 x i16> %4043, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4045 = add <8 x i16> %4028, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4046 = icmp ult <8 x i16> %4045, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4047 = add <8 x i16> %4029, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4048 = icmp ult <8 x i16> %4047, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4049 = add <8 x i16> %4030, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4050 = icmp ult <8 x i16> %4049, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4051 = or <8 x i1> %4048, %4046
  %4052 = or <8 x i1> %4051, %4044
  %4053 = or <8 x i1> %4052, %4050
  %4054 = sext <8 x i1> %4053 to <8 x i16>
  %4055 = bitcast <8 x i16> %4054 to <16 x i8>
  %4056 = icmp slt <16 x i8> %4055, zeroinitializer
  %4057 = bitcast <16 x i1> %4056 to i16
  %4058 = zext i16 %4057 to i32
  %4059 = add <8 x i16> %4031, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4060 = icmp ult <8 x i16> %4059, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4061 = add <8 x i16> %4032, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4062 = icmp ult <8 x i16> %4061, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4063 = add <8 x i16> %4033, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4064 = icmp ult <8 x i16> %4063, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4065 = add <8 x i16> %4034, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4066 = icmp ult <8 x i16> %4065, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4067 = or <8 x i1> %4066, %4060
  %4068 = or <8 x i1> %4067, %4062
  %4069 = or <8 x i1> %4068, %4064
  %4070 = sext <8 x i1> %4069 to <8 x i16>
  %4071 = bitcast <8 x i16> %4070 to <16 x i8>
  %4072 = icmp slt <16 x i8> %4071, zeroinitializer
  %4073 = bitcast <16 x i1> %4072 to i16
  %4074 = zext i16 %4073 to i32
  %4075 = icmp eq i16 %4057, 0
  br i1 %4075, label %4076, label %4111

4076:                                             ; preds = %4026
  %4077 = add <8 x i16> %4035, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4078 = icmp ult <8 x i16> %4077, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4079 = add <8 x i16> %4036, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4080 = icmp ult <8 x i16> %4079, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4081 = add <8 x i16> %4037, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4082 = icmp ult <8 x i16> %4081, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4083 = add <8 x i16> %4038, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4084 = icmp ult <8 x i16> %4083, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4085 = or <8 x i1> %4082, %4080
  %4086 = or <8 x i1> %4085, %4078
  %4087 = or <8 x i1> %4086, %4084
  %4088 = sext <8 x i1> %4087 to <8 x i16>
  %4089 = bitcast <8 x i16> %4088 to <16 x i8>
  %4090 = icmp slt <16 x i8> %4089, zeroinitializer
  %4091 = bitcast <16 x i1> %4090 to i16
  %4092 = zext i16 %4091 to i32
  %4093 = icmp eq i16 %4073, 0
  br i1 %4093, label %4094, label %4111

4094:                                             ; preds = %4076
  %4095 = add <8 x i16> %4039, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4096 = icmp ult <8 x i16> %4095, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4097 = add <8 x i16> %4040, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4098 = icmp ult <8 x i16> %4097, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4099 = add <8 x i16> %4041, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4100 = icmp ult <8 x i16> %4099, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4101 = add <8 x i16> %4042, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4102 = icmp ult <8 x i16> %4101, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4103 = or <8 x i1> %4102, %4096
  %4104 = or <8 x i1> %4103, %4098
  %4105 = or <8 x i1> %4104, %4100
  %4106 = sext <8 x i1> %4105 to <8 x i16>
  %4107 = bitcast <8 x i16> %4106 to <16 x i8>
  %4108 = icmp slt <16 x i8> %4107, zeroinitializer
  %4109 = bitcast <16 x i1> %4108 to i16
  %4110 = zext i16 %4109 to i32
  br label %4111

4111:                                             ; preds = %4026, %4076, %4094
  %4112 = phi i32 [ %4058, %4026 ], [ %4092, %4076 ], [ %4092, %4094 ]
  %4113 = phi i32 [ %4074, %4026 ], [ %4074, %4076 ], [ %4110, %4094 ]
  %4114 = sub nsw i32 0, %4112
  %4115 = icmp eq i32 %4113, %4114
  br i1 %4115, label %4270, label %4116

4116:                                             ; preds = %4111
  br i1 %97, label %4117, label %4118

4117:                                             ; preds = %4116
  tail call void @vpx_highbd_fdct32x32_rd_c(i16* %0, i32* %1, i32 %2) #6
  br label %6136

4118:                                             ; preds = %4116
  %4119 = bitcast [32 x i64]* %4 to i8*
  %4120 = bitcast [32 x i64]* %5 to i8*
  %4121 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %4122 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %4123 = bitcast [32 x i64]* %5 to <2 x i64>*
  %4124 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %4125 = bitcast i64* %4124 to <2 x i64>*
  %4126 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %4127 = bitcast i64* %4126 to <2 x i64>*
  %4128 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %4129 = bitcast i64* %4128 to <2 x i64>*
  %4130 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %4131 = bitcast i64* %4130 to <2 x i64>*
  %4132 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %4133 = bitcast i64* %4132 to <2 x i64>*
  %4134 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %4135 = bitcast i64* %4134 to <2 x i64>*
  %4136 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %4137 = bitcast i64* %4136 to <2 x i64>*
  %4138 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %4139 = bitcast i64* %4138 to <2 x i64>*
  %4140 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %4141 = bitcast i64* %4140 to <2 x i64>*
  %4142 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %4143 = bitcast i64* %4142 to <2 x i64>*
  %4144 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %4145 = bitcast i64* %4144 to <2 x i64>*
  %4146 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %4147 = bitcast i64* %4146 to <2 x i64>*
  %4148 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %4149 = bitcast i64* %4148 to <2 x i64>*
  %4150 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %4151 = bitcast i64* %4150 to <2 x i64>*
  %4152 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %4153 = bitcast i64* %4152 to <2 x i64>*
  br label %4154

4154:                                             ; preds = %4187, %4118
  %4155 = phi i64 [ 0, %4118 ], [ %4268, %4187 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %4119) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %4119, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %4120) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %4120, i8 -86, i64 256, i1 false) #6
  br label %4156

4156:                                             ; preds = %4156, %4154
  %4157 = phi i64 [ 0, %4154 ], [ %4185, %4156 ]
  %4158 = shl i64 %4157, 5
  %4159 = add nuw nsw i64 %4158, %4155
  %4160 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %4159
  %4161 = load i16, i16* %4160, align 2
  %4162 = sext i16 %4161 to i64
  %4163 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %4157
  store i64 %4162, i64* %4163, align 16
  %4164 = or i64 %4157, 1
  %4165 = shl i64 %4164, 5
  %4166 = add nuw nsw i64 %4165, %4155
  %4167 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %4166
  %4168 = load i16, i16* %4167, align 2
  %4169 = sext i16 %4168 to i64
  %4170 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %4164
  store i64 %4169, i64* %4170, align 8
  %4171 = or i64 %4157, 2
  %4172 = shl i64 %4171, 5
  %4173 = add nuw nsw i64 %4172, %4155
  %4174 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %4173
  %4175 = load i16, i16* %4174, align 2
  %4176 = sext i16 %4175 to i64
  %4177 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %4171
  store i64 %4176, i64* %4177, align 16
  %4178 = or i64 %4157, 3
  %4179 = shl i64 %4178, 5
  %4180 = add nuw nsw i64 %4179, %4155
  %4181 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %4180
  %4182 = load i16, i16* %4181, align 2
  %4183 = sext i16 %4182 to i64
  %4184 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %4178
  store i64 %4183, i64* %4184, align 8
  %4185 = add nuw nsw i64 %4157, 4
  %4186 = icmp eq i64 %4185, 32
  br i1 %4186, label %4187, label %4156

4187:                                             ; preds = %4156
  call void @vpx_fdct32(i64* nonnull %4121, i64* nonnull %4122, i32 1) #6
  %4188 = shl i64 %4155, 5
  %4189 = load <2 x i64>, <2 x i64>* %4123, align 16
  %4190 = trunc <2 x i64> %4189 to <2 x i32>
  %4191 = getelementptr inbounds i32, i32* %1, i64 %4188
  %4192 = bitcast i32* %4191 to <2 x i32>*
  store <2 x i32> %4190, <2 x i32>* %4192, align 4
  %4193 = load <2 x i64>, <2 x i64>* %4125, align 16
  %4194 = trunc <2 x i64> %4193 to <2 x i32>
  %4195 = or i64 %4188, 2
  %4196 = getelementptr inbounds i32, i32* %1, i64 %4195
  %4197 = bitcast i32* %4196 to <2 x i32>*
  store <2 x i32> %4194, <2 x i32>* %4197, align 4
  %4198 = load <2 x i64>, <2 x i64>* %4127, align 16
  %4199 = trunc <2 x i64> %4198 to <2 x i32>
  %4200 = or i64 %4188, 4
  %4201 = getelementptr inbounds i32, i32* %1, i64 %4200
  %4202 = bitcast i32* %4201 to <2 x i32>*
  store <2 x i32> %4199, <2 x i32>* %4202, align 4
  %4203 = load <2 x i64>, <2 x i64>* %4129, align 16
  %4204 = trunc <2 x i64> %4203 to <2 x i32>
  %4205 = or i64 %4188, 6
  %4206 = getelementptr inbounds i32, i32* %1, i64 %4205
  %4207 = bitcast i32* %4206 to <2 x i32>*
  store <2 x i32> %4204, <2 x i32>* %4207, align 4
  %4208 = load <2 x i64>, <2 x i64>* %4131, align 16
  %4209 = trunc <2 x i64> %4208 to <2 x i32>
  %4210 = or i64 %4188, 8
  %4211 = getelementptr inbounds i32, i32* %1, i64 %4210
  %4212 = bitcast i32* %4211 to <2 x i32>*
  store <2 x i32> %4209, <2 x i32>* %4212, align 4
  %4213 = load <2 x i64>, <2 x i64>* %4133, align 16
  %4214 = trunc <2 x i64> %4213 to <2 x i32>
  %4215 = or i64 %4188, 10
  %4216 = getelementptr inbounds i32, i32* %1, i64 %4215
  %4217 = bitcast i32* %4216 to <2 x i32>*
  store <2 x i32> %4214, <2 x i32>* %4217, align 4
  %4218 = load <2 x i64>, <2 x i64>* %4135, align 16
  %4219 = trunc <2 x i64> %4218 to <2 x i32>
  %4220 = or i64 %4188, 12
  %4221 = getelementptr inbounds i32, i32* %1, i64 %4220
  %4222 = bitcast i32* %4221 to <2 x i32>*
  store <2 x i32> %4219, <2 x i32>* %4222, align 4
  %4223 = load <2 x i64>, <2 x i64>* %4137, align 16
  %4224 = trunc <2 x i64> %4223 to <2 x i32>
  %4225 = or i64 %4188, 14
  %4226 = getelementptr inbounds i32, i32* %1, i64 %4225
  %4227 = bitcast i32* %4226 to <2 x i32>*
  store <2 x i32> %4224, <2 x i32>* %4227, align 4
  %4228 = load <2 x i64>, <2 x i64>* %4139, align 16
  %4229 = trunc <2 x i64> %4228 to <2 x i32>
  %4230 = or i64 %4188, 16
  %4231 = getelementptr inbounds i32, i32* %1, i64 %4230
  %4232 = bitcast i32* %4231 to <2 x i32>*
  store <2 x i32> %4229, <2 x i32>* %4232, align 4
  %4233 = load <2 x i64>, <2 x i64>* %4141, align 16
  %4234 = trunc <2 x i64> %4233 to <2 x i32>
  %4235 = or i64 %4188, 18
  %4236 = getelementptr inbounds i32, i32* %1, i64 %4235
  %4237 = bitcast i32* %4236 to <2 x i32>*
  store <2 x i32> %4234, <2 x i32>* %4237, align 4
  %4238 = load <2 x i64>, <2 x i64>* %4143, align 16
  %4239 = trunc <2 x i64> %4238 to <2 x i32>
  %4240 = or i64 %4188, 20
  %4241 = getelementptr inbounds i32, i32* %1, i64 %4240
  %4242 = bitcast i32* %4241 to <2 x i32>*
  store <2 x i32> %4239, <2 x i32>* %4242, align 4
  %4243 = load <2 x i64>, <2 x i64>* %4145, align 16
  %4244 = trunc <2 x i64> %4243 to <2 x i32>
  %4245 = or i64 %4188, 22
  %4246 = getelementptr inbounds i32, i32* %1, i64 %4245
  %4247 = bitcast i32* %4246 to <2 x i32>*
  store <2 x i32> %4244, <2 x i32>* %4247, align 4
  %4248 = load <2 x i64>, <2 x i64>* %4147, align 16
  %4249 = trunc <2 x i64> %4248 to <2 x i32>
  %4250 = or i64 %4188, 24
  %4251 = getelementptr inbounds i32, i32* %1, i64 %4250
  %4252 = bitcast i32* %4251 to <2 x i32>*
  store <2 x i32> %4249, <2 x i32>* %4252, align 4
  %4253 = load <2 x i64>, <2 x i64>* %4149, align 16
  %4254 = trunc <2 x i64> %4253 to <2 x i32>
  %4255 = or i64 %4188, 26
  %4256 = getelementptr inbounds i32, i32* %1, i64 %4255
  %4257 = bitcast i32* %4256 to <2 x i32>*
  store <2 x i32> %4254, <2 x i32>* %4257, align 4
  %4258 = load <2 x i64>, <2 x i64>* %4151, align 16
  %4259 = trunc <2 x i64> %4258 to <2 x i32>
  %4260 = or i64 %4188, 28
  %4261 = getelementptr inbounds i32, i32* %1, i64 %4260
  %4262 = bitcast i32* %4261 to <2 x i32>*
  store <2 x i32> %4259, <2 x i32>* %4262, align 4
  %4263 = load <2 x i64>, <2 x i64>* %4153, align 16
  %4264 = trunc <2 x i64> %4263 to <2 x i32>
  %4265 = or i64 %4188, 30
  %4266 = getelementptr inbounds i32, i32* %1, i64 %4265
  %4267 = bitcast i32* %4266 to <2 x i32>*
  store <2 x i32> %4264, <2 x i32>* %4267, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %4120) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %4119) #6
  %4268 = add nuw nsw i64 %4155, 1
  %4269 = icmp eq i64 %4268, 32
  br i1 %4269, label %6136, label %4154

4270:                                             ; preds = %4111
  %4271 = shufflevector <8 x i16> %3446, <8 x i16> %3449, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %4272 = shufflevector <8 x i16> %3446, <8 x i16> %3449, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %4273 = shufflevector <8 x i16> %3447, <8 x i16> %3448, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %4274 = shufflevector <8 x i16> %3447, <8 x i16> %3448, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %4275 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4271, <8 x i16> <i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069>) #6
  %4276 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4272, <8 x i16> <i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069>) #6
  %4277 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4273, <8 x i16> <i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102>) #6
  %4278 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4274, <8 x i16> <i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102>) #6
  %4279 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4273, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %4280 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4274, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %4281 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4271, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %4282 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4272, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %4283 = add <4 x i32> %4275, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4284 = add <4 x i32> %4276, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4285 = add <4 x i32> %4277, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4286 = add <4 x i32> %4278, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4287 = add <4 x i32> %4279, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4288 = add <4 x i32> %4280, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4289 = add <4 x i32> %4281, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4290 = add <4 x i32> %4282, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4291 = ashr <4 x i32> %4283, <i32 14, i32 14, i32 14, i32 14>
  %4292 = ashr <4 x i32> %4284, <i32 14, i32 14, i32 14, i32 14>
  %4293 = ashr <4 x i32> %4285, <i32 14, i32 14, i32 14, i32 14>
  %4294 = ashr <4 x i32> %4286, <i32 14, i32 14, i32 14, i32 14>
  %4295 = ashr <4 x i32> %4287, <i32 14, i32 14, i32 14, i32 14>
  %4296 = ashr <4 x i32> %4288, <i32 14, i32 14, i32 14, i32 14>
  %4297 = ashr <4 x i32> %4289, <i32 14, i32 14, i32 14, i32 14>
  %4298 = ashr <4 x i32> %4290, <i32 14, i32 14, i32 14, i32 14>
  %4299 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %4291, <4 x i32> %4292) #6
  store <8 x i16> %4299, <8 x i16>* %40, align 16
  %4300 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %4293, <4 x i32> %4294) #6
  store <8 x i16> %4300, <8 x i16>* %42, align 16
  %4301 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %4295, <4 x i32> %4296) #6
  store <8 x i16> %4301, <8 x i16>* %44, align 16
  %4302 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %4297, <4 x i32> %4298) #6
  store <8 x i16> %4302, <8 x i16>* %46, align 16
  %4303 = add <8 x i16> %4299, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4304 = icmp ult <8 x i16> %4303, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4305 = add <8 x i16> %4300, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4306 = icmp ult <8 x i16> %4305, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4307 = add <8 x i16> %4301, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4308 = icmp ult <8 x i16> %4307, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4309 = add <8 x i16> %4302, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4310 = icmp ult <8 x i16> %4309, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4311 = or <8 x i1> %4306, %4304
  %4312 = or <8 x i1> %4311, %4308
  %4313 = or <8 x i1> %4312, %4310
  %4314 = sext <8 x i1> %4313 to <8 x i16>
  %4315 = bitcast <8 x i16> %4314 to <16 x i8>
  %4316 = icmp slt <16 x i8> %4315, zeroinitializer
  %4317 = bitcast <16 x i1> %4316 to i16
  %4318 = icmp eq i16 %4317, 0
  br i1 %4318, label %4473, label %4319

4319:                                             ; preds = %4270
  br i1 %97, label %4320, label %4321

4320:                                             ; preds = %4319
  tail call void @vpx_highbd_fdct32x32_rd_c(i16* %0, i32* %1, i32 %2) #6
  br label %6136

4321:                                             ; preds = %4319
  %4322 = bitcast [32 x i64]* %4 to i8*
  %4323 = bitcast [32 x i64]* %5 to i8*
  %4324 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %4325 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %4326 = bitcast [32 x i64]* %5 to <2 x i64>*
  %4327 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %4328 = bitcast i64* %4327 to <2 x i64>*
  %4329 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %4330 = bitcast i64* %4329 to <2 x i64>*
  %4331 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %4332 = bitcast i64* %4331 to <2 x i64>*
  %4333 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %4334 = bitcast i64* %4333 to <2 x i64>*
  %4335 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %4336 = bitcast i64* %4335 to <2 x i64>*
  %4337 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %4338 = bitcast i64* %4337 to <2 x i64>*
  %4339 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %4340 = bitcast i64* %4339 to <2 x i64>*
  %4341 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %4342 = bitcast i64* %4341 to <2 x i64>*
  %4343 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %4344 = bitcast i64* %4343 to <2 x i64>*
  %4345 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %4346 = bitcast i64* %4345 to <2 x i64>*
  %4347 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %4348 = bitcast i64* %4347 to <2 x i64>*
  %4349 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %4350 = bitcast i64* %4349 to <2 x i64>*
  %4351 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %4352 = bitcast i64* %4351 to <2 x i64>*
  %4353 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %4354 = bitcast i64* %4353 to <2 x i64>*
  %4355 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %4356 = bitcast i64* %4355 to <2 x i64>*
  br label %4357

4357:                                             ; preds = %4390, %4321
  %4358 = phi i64 [ 0, %4321 ], [ %4471, %4390 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %4322) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %4322, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %4323) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %4323, i8 -86, i64 256, i1 false) #6
  br label %4359

4359:                                             ; preds = %4359, %4357
  %4360 = phi i64 [ 0, %4357 ], [ %4388, %4359 ]
  %4361 = shl i64 %4360, 5
  %4362 = add nuw nsw i64 %4361, %4358
  %4363 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %4362
  %4364 = load i16, i16* %4363, align 2
  %4365 = sext i16 %4364 to i64
  %4366 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %4360
  store i64 %4365, i64* %4366, align 16
  %4367 = or i64 %4360, 1
  %4368 = shl i64 %4367, 5
  %4369 = add nuw nsw i64 %4368, %4358
  %4370 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %4369
  %4371 = load i16, i16* %4370, align 2
  %4372 = sext i16 %4371 to i64
  %4373 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %4367
  store i64 %4372, i64* %4373, align 8
  %4374 = or i64 %4360, 2
  %4375 = shl i64 %4374, 5
  %4376 = add nuw nsw i64 %4375, %4358
  %4377 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %4376
  %4378 = load i16, i16* %4377, align 2
  %4379 = sext i16 %4378 to i64
  %4380 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %4374
  store i64 %4379, i64* %4380, align 16
  %4381 = or i64 %4360, 3
  %4382 = shl i64 %4381, 5
  %4383 = add nuw nsw i64 %4382, %4358
  %4384 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %4383
  %4385 = load i16, i16* %4384, align 2
  %4386 = sext i16 %4385 to i64
  %4387 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %4381
  store i64 %4386, i64* %4387, align 8
  %4388 = add nuw nsw i64 %4360, 4
  %4389 = icmp eq i64 %4388, 32
  br i1 %4389, label %4390, label %4359

4390:                                             ; preds = %4359
  call void @vpx_fdct32(i64* nonnull %4324, i64* nonnull %4325, i32 1) #6
  %4391 = shl i64 %4358, 5
  %4392 = load <2 x i64>, <2 x i64>* %4326, align 16
  %4393 = trunc <2 x i64> %4392 to <2 x i32>
  %4394 = getelementptr inbounds i32, i32* %1, i64 %4391
  %4395 = bitcast i32* %4394 to <2 x i32>*
  store <2 x i32> %4393, <2 x i32>* %4395, align 4
  %4396 = load <2 x i64>, <2 x i64>* %4328, align 16
  %4397 = trunc <2 x i64> %4396 to <2 x i32>
  %4398 = or i64 %4391, 2
  %4399 = getelementptr inbounds i32, i32* %1, i64 %4398
  %4400 = bitcast i32* %4399 to <2 x i32>*
  store <2 x i32> %4397, <2 x i32>* %4400, align 4
  %4401 = load <2 x i64>, <2 x i64>* %4330, align 16
  %4402 = trunc <2 x i64> %4401 to <2 x i32>
  %4403 = or i64 %4391, 4
  %4404 = getelementptr inbounds i32, i32* %1, i64 %4403
  %4405 = bitcast i32* %4404 to <2 x i32>*
  store <2 x i32> %4402, <2 x i32>* %4405, align 4
  %4406 = load <2 x i64>, <2 x i64>* %4332, align 16
  %4407 = trunc <2 x i64> %4406 to <2 x i32>
  %4408 = or i64 %4391, 6
  %4409 = getelementptr inbounds i32, i32* %1, i64 %4408
  %4410 = bitcast i32* %4409 to <2 x i32>*
  store <2 x i32> %4407, <2 x i32>* %4410, align 4
  %4411 = load <2 x i64>, <2 x i64>* %4334, align 16
  %4412 = trunc <2 x i64> %4411 to <2 x i32>
  %4413 = or i64 %4391, 8
  %4414 = getelementptr inbounds i32, i32* %1, i64 %4413
  %4415 = bitcast i32* %4414 to <2 x i32>*
  store <2 x i32> %4412, <2 x i32>* %4415, align 4
  %4416 = load <2 x i64>, <2 x i64>* %4336, align 16
  %4417 = trunc <2 x i64> %4416 to <2 x i32>
  %4418 = or i64 %4391, 10
  %4419 = getelementptr inbounds i32, i32* %1, i64 %4418
  %4420 = bitcast i32* %4419 to <2 x i32>*
  store <2 x i32> %4417, <2 x i32>* %4420, align 4
  %4421 = load <2 x i64>, <2 x i64>* %4338, align 16
  %4422 = trunc <2 x i64> %4421 to <2 x i32>
  %4423 = or i64 %4391, 12
  %4424 = getelementptr inbounds i32, i32* %1, i64 %4423
  %4425 = bitcast i32* %4424 to <2 x i32>*
  store <2 x i32> %4422, <2 x i32>* %4425, align 4
  %4426 = load <2 x i64>, <2 x i64>* %4340, align 16
  %4427 = trunc <2 x i64> %4426 to <2 x i32>
  %4428 = or i64 %4391, 14
  %4429 = getelementptr inbounds i32, i32* %1, i64 %4428
  %4430 = bitcast i32* %4429 to <2 x i32>*
  store <2 x i32> %4427, <2 x i32>* %4430, align 4
  %4431 = load <2 x i64>, <2 x i64>* %4342, align 16
  %4432 = trunc <2 x i64> %4431 to <2 x i32>
  %4433 = or i64 %4391, 16
  %4434 = getelementptr inbounds i32, i32* %1, i64 %4433
  %4435 = bitcast i32* %4434 to <2 x i32>*
  store <2 x i32> %4432, <2 x i32>* %4435, align 4
  %4436 = load <2 x i64>, <2 x i64>* %4344, align 16
  %4437 = trunc <2 x i64> %4436 to <2 x i32>
  %4438 = or i64 %4391, 18
  %4439 = getelementptr inbounds i32, i32* %1, i64 %4438
  %4440 = bitcast i32* %4439 to <2 x i32>*
  store <2 x i32> %4437, <2 x i32>* %4440, align 4
  %4441 = load <2 x i64>, <2 x i64>* %4346, align 16
  %4442 = trunc <2 x i64> %4441 to <2 x i32>
  %4443 = or i64 %4391, 20
  %4444 = getelementptr inbounds i32, i32* %1, i64 %4443
  %4445 = bitcast i32* %4444 to <2 x i32>*
  store <2 x i32> %4442, <2 x i32>* %4445, align 4
  %4446 = load <2 x i64>, <2 x i64>* %4348, align 16
  %4447 = trunc <2 x i64> %4446 to <2 x i32>
  %4448 = or i64 %4391, 22
  %4449 = getelementptr inbounds i32, i32* %1, i64 %4448
  %4450 = bitcast i32* %4449 to <2 x i32>*
  store <2 x i32> %4447, <2 x i32>* %4450, align 4
  %4451 = load <2 x i64>, <2 x i64>* %4350, align 16
  %4452 = trunc <2 x i64> %4451 to <2 x i32>
  %4453 = or i64 %4391, 24
  %4454 = getelementptr inbounds i32, i32* %1, i64 %4453
  %4455 = bitcast i32* %4454 to <2 x i32>*
  store <2 x i32> %4452, <2 x i32>* %4455, align 4
  %4456 = load <2 x i64>, <2 x i64>* %4352, align 16
  %4457 = trunc <2 x i64> %4456 to <2 x i32>
  %4458 = or i64 %4391, 26
  %4459 = getelementptr inbounds i32, i32* %1, i64 %4458
  %4460 = bitcast i32* %4459 to <2 x i32>*
  store <2 x i32> %4457, <2 x i32>* %4460, align 4
  %4461 = load <2 x i64>, <2 x i64>* %4354, align 16
  %4462 = trunc <2 x i64> %4461 to <2 x i32>
  %4463 = or i64 %4391, 28
  %4464 = getelementptr inbounds i32, i32* %1, i64 %4463
  %4465 = bitcast i32* %4464 to <2 x i32>*
  store <2 x i32> %4462, <2 x i32>* %4465, align 4
  %4466 = load <2 x i64>, <2 x i64>* %4356, align 16
  %4467 = trunc <2 x i64> %4466 to <2 x i32>
  %4468 = or i64 %4391, 30
  %4469 = getelementptr inbounds i32, i32* %1, i64 %4468
  %4470 = bitcast i32* %4469 to <2 x i32>*
  store <2 x i32> %4467, <2 x i32>* %4470, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %4323) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %4322) #6
  %4471 = add nuw nsw i64 %4358, 1
  %4472 = icmp eq i64 %4471, 32
  br i1 %4472, label %6136, label %4357

4473:                                             ; preds = %4270
  %4474 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %3852, <8 x i16> %2776) #6
  %4475 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2776, <8 x i16> %3852) #6
  %4476 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2779, <8 x i16> %3853) #6
  %4477 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %3853, <8 x i16> %2779) #6
  %4478 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %3854, <8 x i16> %2780) #6
  %4479 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2780, <8 x i16> %3854) #6
  %4480 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2783, <8 x i16> %3855) #6
  %4481 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %3855, <8 x i16> %2783) #6
  %4482 = add <8 x i16> %4474, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4483 = icmp ult <8 x i16> %4482, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4484 = add <8 x i16> %4475, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4485 = icmp ult <8 x i16> %4484, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4486 = add <8 x i16> %4476, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4487 = icmp ult <8 x i16> %4486, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4488 = add <8 x i16> %4477, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4489 = icmp ult <8 x i16> %4488, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4490 = or <8 x i1> %4485, %4483
  %4491 = or <8 x i1> %4490, %4487
  %4492 = or <8 x i1> %4491, %4489
  %4493 = sext <8 x i1> %4492 to <8 x i16>
  %4494 = bitcast <8 x i16> %4493 to <16 x i8>
  %4495 = icmp slt <16 x i8> %4494, zeroinitializer
  %4496 = bitcast <16 x i1> %4495 to i16
  %4497 = zext i16 %4496 to i32
  %4498 = add <8 x i16> %4478, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4499 = icmp ult <8 x i16> %4498, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4500 = add <8 x i16> %4479, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4501 = icmp ult <8 x i16> %4500, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4502 = add <8 x i16> %4480, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4503 = icmp ult <8 x i16> %4502, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4504 = add <8 x i16> %4481, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4505 = icmp ult <8 x i16> %4504, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4506 = or <8 x i1> %4501, %4499
  %4507 = or <8 x i1> %4506, %4503
  %4508 = or <8 x i1> %4507, %4505
  %4509 = sext <8 x i1> %4508 to <8 x i16>
  %4510 = bitcast <8 x i16> %4509 to <16 x i8>
  %4511 = icmp slt <16 x i8> %4510, zeroinitializer
  %4512 = bitcast <16 x i1> %4511 to i16
  %4513 = zext i16 %4512 to i32
  %4514 = sub nsw i32 0, %4497
  %4515 = icmp eq i32 %4513, %4514
  br i1 %4515, label %4670, label %4516

4516:                                             ; preds = %4473
  br i1 %97, label %4517, label %4518

4517:                                             ; preds = %4516
  tail call void @vpx_highbd_fdct32x32_rd_c(i16* %0, i32* %1, i32 %2) #6
  br label %6136

4518:                                             ; preds = %4516
  %4519 = bitcast [32 x i64]* %4 to i8*
  %4520 = bitcast [32 x i64]* %5 to i8*
  %4521 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %4522 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %4523 = bitcast [32 x i64]* %5 to <2 x i64>*
  %4524 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %4525 = bitcast i64* %4524 to <2 x i64>*
  %4526 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %4527 = bitcast i64* %4526 to <2 x i64>*
  %4528 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %4529 = bitcast i64* %4528 to <2 x i64>*
  %4530 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %4531 = bitcast i64* %4530 to <2 x i64>*
  %4532 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %4533 = bitcast i64* %4532 to <2 x i64>*
  %4534 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %4535 = bitcast i64* %4534 to <2 x i64>*
  %4536 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %4537 = bitcast i64* %4536 to <2 x i64>*
  %4538 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %4539 = bitcast i64* %4538 to <2 x i64>*
  %4540 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %4541 = bitcast i64* %4540 to <2 x i64>*
  %4542 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %4543 = bitcast i64* %4542 to <2 x i64>*
  %4544 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %4545 = bitcast i64* %4544 to <2 x i64>*
  %4546 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %4547 = bitcast i64* %4546 to <2 x i64>*
  %4548 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %4549 = bitcast i64* %4548 to <2 x i64>*
  %4550 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %4551 = bitcast i64* %4550 to <2 x i64>*
  %4552 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %4553 = bitcast i64* %4552 to <2 x i64>*
  br label %4554

4554:                                             ; preds = %4587, %4518
  %4555 = phi i64 [ 0, %4518 ], [ %4668, %4587 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %4519) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %4519, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %4520) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %4520, i8 -86, i64 256, i1 false) #6
  br label %4556

4556:                                             ; preds = %4556, %4554
  %4557 = phi i64 [ 0, %4554 ], [ %4585, %4556 ]
  %4558 = shl i64 %4557, 5
  %4559 = add nuw nsw i64 %4558, %4555
  %4560 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %4559
  %4561 = load i16, i16* %4560, align 2
  %4562 = sext i16 %4561 to i64
  %4563 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %4557
  store i64 %4562, i64* %4563, align 16
  %4564 = or i64 %4557, 1
  %4565 = shl i64 %4564, 5
  %4566 = add nuw nsw i64 %4565, %4555
  %4567 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %4566
  %4568 = load i16, i16* %4567, align 2
  %4569 = sext i16 %4568 to i64
  %4570 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %4564
  store i64 %4569, i64* %4570, align 8
  %4571 = or i64 %4557, 2
  %4572 = shl i64 %4571, 5
  %4573 = add nuw nsw i64 %4572, %4555
  %4574 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %4573
  %4575 = load i16, i16* %4574, align 2
  %4576 = sext i16 %4575 to i64
  %4577 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %4571
  store i64 %4576, i64* %4577, align 16
  %4578 = or i64 %4557, 3
  %4579 = shl i64 %4578, 5
  %4580 = add nuw nsw i64 %4579, %4555
  %4581 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %4580
  %4582 = load i16, i16* %4581, align 2
  %4583 = sext i16 %4582 to i64
  %4584 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %4578
  store i64 %4583, i64* %4584, align 8
  %4585 = add nuw nsw i64 %4557, 4
  %4586 = icmp eq i64 %4585, 32
  br i1 %4586, label %4587, label %4556

4587:                                             ; preds = %4556
  call void @vpx_fdct32(i64* nonnull %4521, i64* nonnull %4522, i32 1) #6
  %4588 = shl i64 %4555, 5
  %4589 = load <2 x i64>, <2 x i64>* %4523, align 16
  %4590 = trunc <2 x i64> %4589 to <2 x i32>
  %4591 = getelementptr inbounds i32, i32* %1, i64 %4588
  %4592 = bitcast i32* %4591 to <2 x i32>*
  store <2 x i32> %4590, <2 x i32>* %4592, align 4
  %4593 = load <2 x i64>, <2 x i64>* %4525, align 16
  %4594 = trunc <2 x i64> %4593 to <2 x i32>
  %4595 = or i64 %4588, 2
  %4596 = getelementptr inbounds i32, i32* %1, i64 %4595
  %4597 = bitcast i32* %4596 to <2 x i32>*
  store <2 x i32> %4594, <2 x i32>* %4597, align 4
  %4598 = load <2 x i64>, <2 x i64>* %4527, align 16
  %4599 = trunc <2 x i64> %4598 to <2 x i32>
  %4600 = or i64 %4588, 4
  %4601 = getelementptr inbounds i32, i32* %1, i64 %4600
  %4602 = bitcast i32* %4601 to <2 x i32>*
  store <2 x i32> %4599, <2 x i32>* %4602, align 4
  %4603 = load <2 x i64>, <2 x i64>* %4529, align 16
  %4604 = trunc <2 x i64> %4603 to <2 x i32>
  %4605 = or i64 %4588, 6
  %4606 = getelementptr inbounds i32, i32* %1, i64 %4605
  %4607 = bitcast i32* %4606 to <2 x i32>*
  store <2 x i32> %4604, <2 x i32>* %4607, align 4
  %4608 = load <2 x i64>, <2 x i64>* %4531, align 16
  %4609 = trunc <2 x i64> %4608 to <2 x i32>
  %4610 = or i64 %4588, 8
  %4611 = getelementptr inbounds i32, i32* %1, i64 %4610
  %4612 = bitcast i32* %4611 to <2 x i32>*
  store <2 x i32> %4609, <2 x i32>* %4612, align 4
  %4613 = load <2 x i64>, <2 x i64>* %4533, align 16
  %4614 = trunc <2 x i64> %4613 to <2 x i32>
  %4615 = or i64 %4588, 10
  %4616 = getelementptr inbounds i32, i32* %1, i64 %4615
  %4617 = bitcast i32* %4616 to <2 x i32>*
  store <2 x i32> %4614, <2 x i32>* %4617, align 4
  %4618 = load <2 x i64>, <2 x i64>* %4535, align 16
  %4619 = trunc <2 x i64> %4618 to <2 x i32>
  %4620 = or i64 %4588, 12
  %4621 = getelementptr inbounds i32, i32* %1, i64 %4620
  %4622 = bitcast i32* %4621 to <2 x i32>*
  store <2 x i32> %4619, <2 x i32>* %4622, align 4
  %4623 = load <2 x i64>, <2 x i64>* %4537, align 16
  %4624 = trunc <2 x i64> %4623 to <2 x i32>
  %4625 = or i64 %4588, 14
  %4626 = getelementptr inbounds i32, i32* %1, i64 %4625
  %4627 = bitcast i32* %4626 to <2 x i32>*
  store <2 x i32> %4624, <2 x i32>* %4627, align 4
  %4628 = load <2 x i64>, <2 x i64>* %4539, align 16
  %4629 = trunc <2 x i64> %4628 to <2 x i32>
  %4630 = or i64 %4588, 16
  %4631 = getelementptr inbounds i32, i32* %1, i64 %4630
  %4632 = bitcast i32* %4631 to <2 x i32>*
  store <2 x i32> %4629, <2 x i32>* %4632, align 4
  %4633 = load <2 x i64>, <2 x i64>* %4541, align 16
  %4634 = trunc <2 x i64> %4633 to <2 x i32>
  %4635 = or i64 %4588, 18
  %4636 = getelementptr inbounds i32, i32* %1, i64 %4635
  %4637 = bitcast i32* %4636 to <2 x i32>*
  store <2 x i32> %4634, <2 x i32>* %4637, align 4
  %4638 = load <2 x i64>, <2 x i64>* %4543, align 16
  %4639 = trunc <2 x i64> %4638 to <2 x i32>
  %4640 = or i64 %4588, 20
  %4641 = getelementptr inbounds i32, i32* %1, i64 %4640
  %4642 = bitcast i32* %4641 to <2 x i32>*
  store <2 x i32> %4639, <2 x i32>* %4642, align 4
  %4643 = load <2 x i64>, <2 x i64>* %4545, align 16
  %4644 = trunc <2 x i64> %4643 to <2 x i32>
  %4645 = or i64 %4588, 22
  %4646 = getelementptr inbounds i32, i32* %1, i64 %4645
  %4647 = bitcast i32* %4646 to <2 x i32>*
  store <2 x i32> %4644, <2 x i32>* %4647, align 4
  %4648 = load <2 x i64>, <2 x i64>* %4547, align 16
  %4649 = trunc <2 x i64> %4648 to <2 x i32>
  %4650 = or i64 %4588, 24
  %4651 = getelementptr inbounds i32, i32* %1, i64 %4650
  %4652 = bitcast i32* %4651 to <2 x i32>*
  store <2 x i32> %4649, <2 x i32>* %4652, align 4
  %4653 = load <2 x i64>, <2 x i64>* %4549, align 16
  %4654 = trunc <2 x i64> %4653 to <2 x i32>
  %4655 = or i64 %4588, 26
  %4656 = getelementptr inbounds i32, i32* %1, i64 %4655
  %4657 = bitcast i32* %4656 to <2 x i32>*
  store <2 x i32> %4654, <2 x i32>* %4657, align 4
  %4658 = load <2 x i64>, <2 x i64>* %4551, align 16
  %4659 = trunc <2 x i64> %4658 to <2 x i32>
  %4660 = or i64 %4588, 28
  %4661 = getelementptr inbounds i32, i32* %1, i64 %4660
  %4662 = bitcast i32* %4661 to <2 x i32>*
  store <2 x i32> %4659, <2 x i32>* %4662, align 4
  %4663 = load <2 x i64>, <2 x i64>* %4553, align 16
  %4664 = trunc <2 x i64> %4663 to <2 x i32>
  %4665 = or i64 %4588, 30
  %4666 = getelementptr inbounds i32, i32* %1, i64 %4665
  %4667 = bitcast i32* %4666 to <2 x i32>*
  store <2 x i32> %4664, <2 x i32>* %4667, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %4520) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %4519) #6
  %4668 = add nuw nsw i64 %4555, 1
  %4669 = icmp eq i64 %4668, 32
  br i1 %4669, label %6136, label %4554

4670:                                             ; preds = %4473
  %4671 = shufflevector <8 x i16> %4028, <8 x i16> %4041, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %4672 = shufflevector <8 x i16> %4028, <8 x i16> %4041, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %4673 = shufflevector <8 x i16> %4029, <8 x i16> %4040, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %4674 = shufflevector <8 x i16> %4029, <8 x i16> %4040, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %4675 = shufflevector <8 x i16> %4032, <8 x i16> %4037, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %4676 = shufflevector <8 x i16> %4032, <8 x i16> %4037, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %4677 = shufflevector <8 x i16> %4033, <8 x i16> %4036, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %4678 = shufflevector <8 x i16> %4033, <8 x i16> %4036, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %4679 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4671, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %4680 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4672, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %4681 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4673, <8 x i16> <i16 -3196, i16 -16069, i16 -3196, i16 -16069, i16 -3196, i16 -16069, i16 -3196, i16 -16069>) #6
  %4682 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4674, <8 x i16> <i16 -3196, i16 -16069, i16 -3196, i16 -16069, i16 -3196, i16 -16069, i16 -3196, i16 -16069>) #6
  %4683 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4675, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %4684 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4676, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %4685 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4677, <8 x i16> <i16 -13623, i16 -9102, i16 -13623, i16 -9102, i16 -13623, i16 -9102, i16 -13623, i16 -9102>) #6
  %4686 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4678, <8 x i16> <i16 -13623, i16 -9102, i16 -13623, i16 -9102, i16 -13623, i16 -9102, i16 -13623, i16 -9102>) #6
  %4687 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4677, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %4688 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4678, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %4689 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4675, <8 x i16> <i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102>) #6
  %4690 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4676, <8 x i16> <i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102>) #6
  %4691 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4673, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %4692 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4674, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %4693 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4671, <8 x i16> <i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069>) #6
  %4694 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4672, <8 x i16> <i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069>) #6
  %4695 = add <4 x i32> %4679, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4696 = add <4 x i32> %4680, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4697 = add <4 x i32> %4681, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4698 = add <4 x i32> %4682, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4699 = add <4 x i32> %4683, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4700 = add <4 x i32> %4684, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4701 = add <4 x i32> %4685, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4702 = add <4 x i32> %4686, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4703 = ashr <4 x i32> %4695, <i32 14, i32 14, i32 14, i32 14>
  %4704 = ashr <4 x i32> %4696, <i32 14, i32 14, i32 14, i32 14>
  %4705 = ashr <4 x i32> %4697, <i32 14, i32 14, i32 14, i32 14>
  %4706 = ashr <4 x i32> %4698, <i32 14, i32 14, i32 14, i32 14>
  %4707 = ashr <4 x i32> %4699, <i32 14, i32 14, i32 14, i32 14>
  %4708 = ashr <4 x i32> %4700, <i32 14, i32 14, i32 14, i32 14>
  %4709 = ashr <4 x i32> %4701, <i32 14, i32 14, i32 14, i32 14>
  %4710 = ashr <4 x i32> %4702, <i32 14, i32 14, i32 14, i32 14>
  %4711 = add <4 x i32> %4687, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4712 = add <4 x i32> %4688, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4713 = add <4 x i32> %4689, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4714 = add <4 x i32> %4690, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4715 = add <4 x i32> %4691, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4716 = add <4 x i32> %4692, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4717 = add <4 x i32> %4693, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4718 = add <4 x i32> %4694, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4719 = ashr <4 x i32> %4711, <i32 14, i32 14, i32 14, i32 14>
  %4720 = ashr <4 x i32> %4712, <i32 14, i32 14, i32 14, i32 14>
  %4721 = ashr <4 x i32> %4713, <i32 14, i32 14, i32 14, i32 14>
  %4722 = ashr <4 x i32> %4714, <i32 14, i32 14, i32 14, i32 14>
  %4723 = ashr <4 x i32> %4715, <i32 14, i32 14, i32 14, i32 14>
  %4724 = ashr <4 x i32> %4716, <i32 14, i32 14, i32 14, i32 14>
  %4725 = ashr <4 x i32> %4717, <i32 14, i32 14, i32 14, i32 14>
  %4726 = ashr <4 x i32> %4718, <i32 14, i32 14, i32 14, i32 14>
  %4727 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %4703, <4 x i32> %4704) #6
  %4728 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %4705, <4 x i32> %4706) #6
  %4729 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %4707, <4 x i32> %4708) #6
  %4730 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %4709, <4 x i32> %4710) #6
  %4731 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %4719, <4 x i32> %4720) #6
  %4732 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %4721, <4 x i32> %4722) #6
  %4733 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %4723, <4 x i32> %4724) #6
  %4734 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %4725, <4 x i32> %4726) #6
  %4735 = add <8 x i16> %4727, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4736 = icmp ult <8 x i16> %4735, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4737 = add <8 x i16> %4728, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4738 = icmp ult <8 x i16> %4737, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4739 = add <8 x i16> %4729, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4740 = icmp ult <8 x i16> %4739, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4741 = add <8 x i16> %4730, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4742 = icmp ult <8 x i16> %4741, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4743 = or <8 x i1> %4738, %4736
  %4744 = or <8 x i1> %4743, %4740
  %4745 = or <8 x i1> %4744, %4742
  %4746 = sext <8 x i1> %4745 to <8 x i16>
  %4747 = bitcast <8 x i16> %4746 to <16 x i8>
  %4748 = icmp slt <16 x i8> %4747, zeroinitializer
  %4749 = bitcast <16 x i1> %4748 to i16
  %4750 = zext i16 %4749 to i32
  %4751 = add <8 x i16> %4731, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4752 = icmp ult <8 x i16> %4751, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4753 = add <8 x i16> %4732, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4754 = icmp ult <8 x i16> %4753, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4755 = add <8 x i16> %4733, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4756 = icmp ult <8 x i16> %4755, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4757 = add <8 x i16> %4734, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4758 = icmp ult <8 x i16> %4757, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4759 = or <8 x i1> %4754, %4752
  %4760 = or <8 x i1> %4759, %4756
  %4761 = or <8 x i1> %4760, %4758
  %4762 = sext <8 x i1> %4761 to <8 x i16>
  %4763 = bitcast <8 x i16> %4762 to <16 x i8>
  %4764 = icmp slt <16 x i8> %4763, zeroinitializer
  %4765 = bitcast <16 x i1> %4764 to i16
  %4766 = zext i16 %4765 to i32
  %4767 = sub nsw i32 0, %4750
  %4768 = icmp eq i32 %4766, %4767
  br i1 %4768, label %4923, label %4769

4769:                                             ; preds = %4670
  br i1 %97, label %4770, label %4771

4770:                                             ; preds = %4769
  tail call void @vpx_highbd_fdct32x32_rd_c(i16* %0, i32* %1, i32 %2) #6
  br label %6136

4771:                                             ; preds = %4769
  %4772 = bitcast [32 x i64]* %4 to i8*
  %4773 = bitcast [32 x i64]* %5 to i8*
  %4774 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %4775 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %4776 = bitcast [32 x i64]* %5 to <2 x i64>*
  %4777 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %4778 = bitcast i64* %4777 to <2 x i64>*
  %4779 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %4780 = bitcast i64* %4779 to <2 x i64>*
  %4781 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %4782 = bitcast i64* %4781 to <2 x i64>*
  %4783 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %4784 = bitcast i64* %4783 to <2 x i64>*
  %4785 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %4786 = bitcast i64* %4785 to <2 x i64>*
  %4787 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %4788 = bitcast i64* %4787 to <2 x i64>*
  %4789 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %4790 = bitcast i64* %4789 to <2 x i64>*
  %4791 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %4792 = bitcast i64* %4791 to <2 x i64>*
  %4793 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %4794 = bitcast i64* %4793 to <2 x i64>*
  %4795 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %4796 = bitcast i64* %4795 to <2 x i64>*
  %4797 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %4798 = bitcast i64* %4797 to <2 x i64>*
  %4799 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %4800 = bitcast i64* %4799 to <2 x i64>*
  %4801 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %4802 = bitcast i64* %4801 to <2 x i64>*
  %4803 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %4804 = bitcast i64* %4803 to <2 x i64>*
  %4805 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %4806 = bitcast i64* %4805 to <2 x i64>*
  br label %4807

4807:                                             ; preds = %4840, %4771
  %4808 = phi i64 [ 0, %4771 ], [ %4921, %4840 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %4772) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %4772, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %4773) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %4773, i8 -86, i64 256, i1 false) #6
  br label %4809

4809:                                             ; preds = %4809, %4807
  %4810 = phi i64 [ 0, %4807 ], [ %4838, %4809 ]
  %4811 = shl i64 %4810, 5
  %4812 = add nuw nsw i64 %4811, %4808
  %4813 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %4812
  %4814 = load i16, i16* %4813, align 2
  %4815 = sext i16 %4814 to i64
  %4816 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %4810
  store i64 %4815, i64* %4816, align 16
  %4817 = or i64 %4810, 1
  %4818 = shl i64 %4817, 5
  %4819 = add nuw nsw i64 %4818, %4808
  %4820 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %4819
  %4821 = load i16, i16* %4820, align 2
  %4822 = sext i16 %4821 to i64
  %4823 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %4817
  store i64 %4822, i64* %4823, align 8
  %4824 = or i64 %4810, 2
  %4825 = shl i64 %4824, 5
  %4826 = add nuw nsw i64 %4825, %4808
  %4827 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %4826
  %4828 = load i16, i16* %4827, align 2
  %4829 = sext i16 %4828 to i64
  %4830 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %4824
  store i64 %4829, i64* %4830, align 16
  %4831 = or i64 %4810, 3
  %4832 = shl i64 %4831, 5
  %4833 = add nuw nsw i64 %4832, %4808
  %4834 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %4833
  %4835 = load i16, i16* %4834, align 2
  %4836 = sext i16 %4835 to i64
  %4837 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %4831
  store i64 %4836, i64* %4837, align 8
  %4838 = add nuw nsw i64 %4810, 4
  %4839 = icmp eq i64 %4838, 32
  br i1 %4839, label %4840, label %4809

4840:                                             ; preds = %4809
  call void @vpx_fdct32(i64* nonnull %4774, i64* nonnull %4775, i32 1) #6
  %4841 = shl i64 %4808, 5
  %4842 = load <2 x i64>, <2 x i64>* %4776, align 16
  %4843 = trunc <2 x i64> %4842 to <2 x i32>
  %4844 = getelementptr inbounds i32, i32* %1, i64 %4841
  %4845 = bitcast i32* %4844 to <2 x i32>*
  store <2 x i32> %4843, <2 x i32>* %4845, align 4
  %4846 = load <2 x i64>, <2 x i64>* %4778, align 16
  %4847 = trunc <2 x i64> %4846 to <2 x i32>
  %4848 = or i64 %4841, 2
  %4849 = getelementptr inbounds i32, i32* %1, i64 %4848
  %4850 = bitcast i32* %4849 to <2 x i32>*
  store <2 x i32> %4847, <2 x i32>* %4850, align 4
  %4851 = load <2 x i64>, <2 x i64>* %4780, align 16
  %4852 = trunc <2 x i64> %4851 to <2 x i32>
  %4853 = or i64 %4841, 4
  %4854 = getelementptr inbounds i32, i32* %1, i64 %4853
  %4855 = bitcast i32* %4854 to <2 x i32>*
  store <2 x i32> %4852, <2 x i32>* %4855, align 4
  %4856 = load <2 x i64>, <2 x i64>* %4782, align 16
  %4857 = trunc <2 x i64> %4856 to <2 x i32>
  %4858 = or i64 %4841, 6
  %4859 = getelementptr inbounds i32, i32* %1, i64 %4858
  %4860 = bitcast i32* %4859 to <2 x i32>*
  store <2 x i32> %4857, <2 x i32>* %4860, align 4
  %4861 = load <2 x i64>, <2 x i64>* %4784, align 16
  %4862 = trunc <2 x i64> %4861 to <2 x i32>
  %4863 = or i64 %4841, 8
  %4864 = getelementptr inbounds i32, i32* %1, i64 %4863
  %4865 = bitcast i32* %4864 to <2 x i32>*
  store <2 x i32> %4862, <2 x i32>* %4865, align 4
  %4866 = load <2 x i64>, <2 x i64>* %4786, align 16
  %4867 = trunc <2 x i64> %4866 to <2 x i32>
  %4868 = or i64 %4841, 10
  %4869 = getelementptr inbounds i32, i32* %1, i64 %4868
  %4870 = bitcast i32* %4869 to <2 x i32>*
  store <2 x i32> %4867, <2 x i32>* %4870, align 4
  %4871 = load <2 x i64>, <2 x i64>* %4788, align 16
  %4872 = trunc <2 x i64> %4871 to <2 x i32>
  %4873 = or i64 %4841, 12
  %4874 = getelementptr inbounds i32, i32* %1, i64 %4873
  %4875 = bitcast i32* %4874 to <2 x i32>*
  store <2 x i32> %4872, <2 x i32>* %4875, align 4
  %4876 = load <2 x i64>, <2 x i64>* %4790, align 16
  %4877 = trunc <2 x i64> %4876 to <2 x i32>
  %4878 = or i64 %4841, 14
  %4879 = getelementptr inbounds i32, i32* %1, i64 %4878
  %4880 = bitcast i32* %4879 to <2 x i32>*
  store <2 x i32> %4877, <2 x i32>* %4880, align 4
  %4881 = load <2 x i64>, <2 x i64>* %4792, align 16
  %4882 = trunc <2 x i64> %4881 to <2 x i32>
  %4883 = or i64 %4841, 16
  %4884 = getelementptr inbounds i32, i32* %1, i64 %4883
  %4885 = bitcast i32* %4884 to <2 x i32>*
  store <2 x i32> %4882, <2 x i32>* %4885, align 4
  %4886 = load <2 x i64>, <2 x i64>* %4794, align 16
  %4887 = trunc <2 x i64> %4886 to <2 x i32>
  %4888 = or i64 %4841, 18
  %4889 = getelementptr inbounds i32, i32* %1, i64 %4888
  %4890 = bitcast i32* %4889 to <2 x i32>*
  store <2 x i32> %4887, <2 x i32>* %4890, align 4
  %4891 = load <2 x i64>, <2 x i64>* %4796, align 16
  %4892 = trunc <2 x i64> %4891 to <2 x i32>
  %4893 = or i64 %4841, 20
  %4894 = getelementptr inbounds i32, i32* %1, i64 %4893
  %4895 = bitcast i32* %4894 to <2 x i32>*
  store <2 x i32> %4892, <2 x i32>* %4895, align 4
  %4896 = load <2 x i64>, <2 x i64>* %4798, align 16
  %4897 = trunc <2 x i64> %4896 to <2 x i32>
  %4898 = or i64 %4841, 22
  %4899 = getelementptr inbounds i32, i32* %1, i64 %4898
  %4900 = bitcast i32* %4899 to <2 x i32>*
  store <2 x i32> %4897, <2 x i32>* %4900, align 4
  %4901 = load <2 x i64>, <2 x i64>* %4800, align 16
  %4902 = trunc <2 x i64> %4901 to <2 x i32>
  %4903 = or i64 %4841, 24
  %4904 = getelementptr inbounds i32, i32* %1, i64 %4903
  %4905 = bitcast i32* %4904 to <2 x i32>*
  store <2 x i32> %4902, <2 x i32>* %4905, align 4
  %4906 = load <2 x i64>, <2 x i64>* %4802, align 16
  %4907 = trunc <2 x i64> %4906 to <2 x i32>
  %4908 = or i64 %4841, 26
  %4909 = getelementptr inbounds i32, i32* %1, i64 %4908
  %4910 = bitcast i32* %4909 to <2 x i32>*
  store <2 x i32> %4907, <2 x i32>* %4910, align 4
  %4911 = load <2 x i64>, <2 x i64>* %4804, align 16
  %4912 = trunc <2 x i64> %4911 to <2 x i32>
  %4913 = or i64 %4841, 28
  %4914 = getelementptr inbounds i32, i32* %1, i64 %4913
  %4915 = bitcast i32* %4914 to <2 x i32>*
  store <2 x i32> %4912, <2 x i32>* %4915, align 4
  %4916 = load <2 x i64>, <2 x i64>* %4806, align 16
  %4917 = trunc <2 x i64> %4916 to <2 x i32>
  %4918 = or i64 %4841, 30
  %4919 = getelementptr inbounds i32, i32* %1, i64 %4918
  %4920 = bitcast i32* %4919 to <2 x i32>*
  store <2 x i32> %4917, <2 x i32>* %4920, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %4773) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %4772) #6
  %4921 = add nuw nsw i64 %4808, 1
  %4922 = icmp eq i64 %4921, 32
  br i1 %4922, label %6136, label %4807

4923:                                             ; preds = %4670
  %4924 = shufflevector <8 x i16> %4474, <8 x i16> %4481, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %4925 = shufflevector <8 x i16> %4474, <8 x i16> %4481, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %4926 = shufflevector <8 x i16> %4475, <8 x i16> %4480, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %4927 = shufflevector <8 x i16> %4475, <8 x i16> %4480, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %4928 = shufflevector <8 x i16> %4476, <8 x i16> %4479, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %4929 = shufflevector <8 x i16> %4476, <8 x i16> %4479, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %4930 = shufflevector <8 x i16> %4477, <8 x i16> %4478, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %4931 = shufflevector <8 x i16> %4477, <8 x i16> %4478, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %4932 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4924, <8 x i16> <i16 1606, i16 16305, i16 1606, i16 16305, i16 1606, i16 16305, i16 1606, i16 16305>) #6
  %4933 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4925, <8 x i16> <i16 1606, i16 16305, i16 1606, i16 16305, i16 1606, i16 16305, i16 1606, i16 16305>) #6
  %4934 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4926, <8 x i16> <i16 12665, i16 10394, i16 12665, i16 10394, i16 12665, i16 10394, i16 12665, i16 10394>) #6
  %4935 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4927, <8 x i16> <i16 12665, i16 10394, i16 12665, i16 10394, i16 12665, i16 10394, i16 12665, i16 10394>) #6
  %4936 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4928, <8 x i16> <i16 7723, i16 14449, i16 7723, i16 14449, i16 7723, i16 14449, i16 7723, i16 14449>) #6
  %4937 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4929, <8 x i16> <i16 7723, i16 14449, i16 7723, i16 14449, i16 7723, i16 14449, i16 7723, i16 14449>) #6
  %4938 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4930, <8 x i16> <i16 15679, i16 4756, i16 15679, i16 4756, i16 15679, i16 4756, i16 15679, i16 4756>) #6
  %4939 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4931, <8 x i16> <i16 15679, i16 4756, i16 15679, i16 4756, i16 15679, i16 4756, i16 15679, i16 4756>) #6
  %4940 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4930, <8 x i16> <i16 -4756, i16 15679, i16 -4756, i16 15679, i16 -4756, i16 15679, i16 -4756, i16 15679>) #6
  %4941 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4931, <8 x i16> <i16 -4756, i16 15679, i16 -4756, i16 15679, i16 -4756, i16 15679, i16 -4756, i16 15679>) #6
  %4942 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4928, <8 x i16> <i16 -14449, i16 7723, i16 -14449, i16 7723, i16 -14449, i16 7723, i16 -14449, i16 7723>) #6
  %4943 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4929, <8 x i16> <i16 -14449, i16 7723, i16 -14449, i16 7723, i16 -14449, i16 7723, i16 -14449, i16 7723>) #6
  %4944 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4926, <8 x i16> <i16 -10394, i16 12665, i16 -10394, i16 12665, i16 -10394, i16 12665, i16 -10394, i16 12665>) #6
  %4945 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4927, <8 x i16> <i16 -10394, i16 12665, i16 -10394, i16 12665, i16 -10394, i16 12665, i16 -10394, i16 12665>) #6
  %4946 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4924, <8 x i16> <i16 -16305, i16 1606, i16 -16305, i16 1606, i16 -16305, i16 1606, i16 -16305, i16 1606>) #6
  %4947 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %4925, <8 x i16> <i16 -16305, i16 1606, i16 -16305, i16 1606, i16 -16305, i16 1606, i16 -16305, i16 1606>) #6
  %4948 = add <4 x i32> %4932, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4949 = add <4 x i32> %4933, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4950 = add <4 x i32> %4934, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4951 = add <4 x i32> %4935, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4952 = add <4 x i32> %4936, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4953 = add <4 x i32> %4937, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4954 = add <4 x i32> %4938, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4955 = add <4 x i32> %4939, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4956 = add <4 x i32> %4940, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4957 = add <4 x i32> %4941, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4958 = add <4 x i32> %4942, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4959 = add <4 x i32> %4943, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4960 = add <4 x i32> %4944, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4961 = add <4 x i32> %4945, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4962 = add <4 x i32> %4946, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4963 = add <4 x i32> %4947, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4964 = ashr <4 x i32> %4948, <i32 14, i32 14, i32 14, i32 14>
  %4965 = ashr <4 x i32> %4949, <i32 14, i32 14, i32 14, i32 14>
  %4966 = ashr <4 x i32> %4950, <i32 14, i32 14, i32 14, i32 14>
  %4967 = ashr <4 x i32> %4951, <i32 14, i32 14, i32 14, i32 14>
  %4968 = ashr <4 x i32> %4952, <i32 14, i32 14, i32 14, i32 14>
  %4969 = ashr <4 x i32> %4953, <i32 14, i32 14, i32 14, i32 14>
  %4970 = ashr <4 x i32> %4954, <i32 14, i32 14, i32 14, i32 14>
  %4971 = ashr <4 x i32> %4955, <i32 14, i32 14, i32 14, i32 14>
  %4972 = ashr <4 x i32> %4956, <i32 14, i32 14, i32 14, i32 14>
  %4973 = ashr <4 x i32> %4957, <i32 14, i32 14, i32 14, i32 14>
  %4974 = ashr <4 x i32> %4958, <i32 14, i32 14, i32 14, i32 14>
  %4975 = ashr <4 x i32> %4959, <i32 14, i32 14, i32 14, i32 14>
  %4976 = ashr <4 x i32> %4960, <i32 14, i32 14, i32 14, i32 14>
  %4977 = ashr <4 x i32> %4961, <i32 14, i32 14, i32 14, i32 14>
  %4978 = ashr <4 x i32> %4962, <i32 14, i32 14, i32 14, i32 14>
  %4979 = ashr <4 x i32> %4963, <i32 14, i32 14, i32 14, i32 14>
  %4980 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %4964, <4 x i32> %4965) #6
  store <8 x i16> %4980, <8 x i16>* %48, align 16
  %4981 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %4966, <4 x i32> %4967) #6
  store <8 x i16> %4981, <8 x i16>* %50, align 16
  %4982 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %4968, <4 x i32> %4969) #6
  store <8 x i16> %4982, <8 x i16>* %52, align 16
  %4983 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %4970, <4 x i32> %4971) #6
  store <8 x i16> %4983, <8 x i16>* %54, align 16
  %4984 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %4972, <4 x i32> %4973) #6
  store <8 x i16> %4984, <8 x i16>* %56, align 16
  %4985 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %4974, <4 x i32> %4975) #6
  store <8 x i16> %4985, <8 x i16>* %58, align 16
  %4986 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %4976, <4 x i32> %4977) #6
  store <8 x i16> %4986, <8 x i16>* %60, align 16
  %4987 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %4978, <4 x i32> %4979) #6
  store <8 x i16> %4987, <8 x i16>* %62, align 16
  %4988 = add <8 x i16> %4980, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4989 = icmp ult <8 x i16> %4988, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4990 = add <8 x i16> %4981, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4991 = icmp ult <8 x i16> %4990, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4992 = add <8 x i16> %4982, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4993 = icmp ult <8 x i16> %4992, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4994 = add <8 x i16> %4983, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %4995 = icmp ult <8 x i16> %4994, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %4996 = or <8 x i1> %4991, %4989
  %4997 = or <8 x i1> %4996, %4993
  %4998 = or <8 x i1> %4997, %4995
  %4999 = sext <8 x i1> %4998 to <8 x i16>
  %5000 = bitcast <8 x i16> %4999 to <16 x i8>
  %5001 = icmp slt <16 x i8> %5000, zeroinitializer
  %5002 = bitcast <16 x i1> %5001 to i16
  %5003 = zext i16 %5002 to i32
  %5004 = add <8 x i16> %4984, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5005 = icmp ult <8 x i16> %5004, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5006 = add <8 x i16> %4985, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5007 = icmp ult <8 x i16> %5006, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5008 = add <8 x i16> %4986, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5009 = icmp ult <8 x i16> %5008, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5010 = add <8 x i16> %4987, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5011 = icmp ult <8 x i16> %5010, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5012 = or <8 x i1> %5007, %5005
  %5013 = or <8 x i1> %5012, %5009
  %5014 = or <8 x i1> %5013, %5011
  %5015 = sext <8 x i1> %5014 to <8 x i16>
  %5016 = bitcast <8 x i16> %5015 to <16 x i8>
  %5017 = icmp slt <16 x i8> %5016, zeroinitializer
  %5018 = bitcast <16 x i1> %5017 to i16
  %5019 = zext i16 %5018 to i32
  %5020 = sub nsw i32 0, %5003
  %5021 = icmp eq i32 %5019, %5020
  br i1 %5021, label %5176, label %5022

5022:                                             ; preds = %4923
  br i1 %97, label %5023, label %5024

5023:                                             ; preds = %5022
  tail call void @vpx_highbd_fdct32x32_rd_c(i16* %0, i32* %1, i32 %2) #6
  br label %6136

5024:                                             ; preds = %5022
  %5025 = bitcast [32 x i64]* %4 to i8*
  %5026 = bitcast [32 x i64]* %5 to i8*
  %5027 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %5028 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %5029 = bitcast [32 x i64]* %5 to <2 x i64>*
  %5030 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %5031 = bitcast i64* %5030 to <2 x i64>*
  %5032 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %5033 = bitcast i64* %5032 to <2 x i64>*
  %5034 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %5035 = bitcast i64* %5034 to <2 x i64>*
  %5036 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %5037 = bitcast i64* %5036 to <2 x i64>*
  %5038 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %5039 = bitcast i64* %5038 to <2 x i64>*
  %5040 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %5041 = bitcast i64* %5040 to <2 x i64>*
  %5042 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %5043 = bitcast i64* %5042 to <2 x i64>*
  %5044 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %5045 = bitcast i64* %5044 to <2 x i64>*
  %5046 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %5047 = bitcast i64* %5046 to <2 x i64>*
  %5048 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %5049 = bitcast i64* %5048 to <2 x i64>*
  %5050 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %5051 = bitcast i64* %5050 to <2 x i64>*
  %5052 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %5053 = bitcast i64* %5052 to <2 x i64>*
  %5054 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %5055 = bitcast i64* %5054 to <2 x i64>*
  %5056 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %5057 = bitcast i64* %5056 to <2 x i64>*
  %5058 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %5059 = bitcast i64* %5058 to <2 x i64>*
  br label %5060

5060:                                             ; preds = %5093, %5024
  %5061 = phi i64 [ 0, %5024 ], [ %5174, %5093 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %5025) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %5025, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %5026) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %5026, i8 -86, i64 256, i1 false) #6
  br label %5062

5062:                                             ; preds = %5062, %5060
  %5063 = phi i64 [ 0, %5060 ], [ %5091, %5062 ]
  %5064 = shl i64 %5063, 5
  %5065 = add nuw nsw i64 %5064, %5061
  %5066 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %5065
  %5067 = load i16, i16* %5066, align 2
  %5068 = sext i16 %5067 to i64
  %5069 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %5063
  store i64 %5068, i64* %5069, align 16
  %5070 = or i64 %5063, 1
  %5071 = shl i64 %5070, 5
  %5072 = add nuw nsw i64 %5071, %5061
  %5073 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %5072
  %5074 = load i16, i16* %5073, align 2
  %5075 = sext i16 %5074 to i64
  %5076 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %5070
  store i64 %5075, i64* %5076, align 8
  %5077 = or i64 %5063, 2
  %5078 = shl i64 %5077, 5
  %5079 = add nuw nsw i64 %5078, %5061
  %5080 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %5079
  %5081 = load i16, i16* %5080, align 2
  %5082 = sext i16 %5081 to i64
  %5083 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %5077
  store i64 %5082, i64* %5083, align 16
  %5084 = or i64 %5063, 3
  %5085 = shl i64 %5084, 5
  %5086 = add nuw nsw i64 %5085, %5061
  %5087 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %5086
  %5088 = load i16, i16* %5087, align 2
  %5089 = sext i16 %5088 to i64
  %5090 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %5084
  store i64 %5089, i64* %5090, align 8
  %5091 = add nuw nsw i64 %5063, 4
  %5092 = icmp eq i64 %5091, 32
  br i1 %5092, label %5093, label %5062

5093:                                             ; preds = %5062
  call void @vpx_fdct32(i64* nonnull %5027, i64* nonnull %5028, i32 1) #6
  %5094 = shl i64 %5061, 5
  %5095 = load <2 x i64>, <2 x i64>* %5029, align 16
  %5096 = trunc <2 x i64> %5095 to <2 x i32>
  %5097 = getelementptr inbounds i32, i32* %1, i64 %5094
  %5098 = bitcast i32* %5097 to <2 x i32>*
  store <2 x i32> %5096, <2 x i32>* %5098, align 4
  %5099 = load <2 x i64>, <2 x i64>* %5031, align 16
  %5100 = trunc <2 x i64> %5099 to <2 x i32>
  %5101 = or i64 %5094, 2
  %5102 = getelementptr inbounds i32, i32* %1, i64 %5101
  %5103 = bitcast i32* %5102 to <2 x i32>*
  store <2 x i32> %5100, <2 x i32>* %5103, align 4
  %5104 = load <2 x i64>, <2 x i64>* %5033, align 16
  %5105 = trunc <2 x i64> %5104 to <2 x i32>
  %5106 = or i64 %5094, 4
  %5107 = getelementptr inbounds i32, i32* %1, i64 %5106
  %5108 = bitcast i32* %5107 to <2 x i32>*
  store <2 x i32> %5105, <2 x i32>* %5108, align 4
  %5109 = load <2 x i64>, <2 x i64>* %5035, align 16
  %5110 = trunc <2 x i64> %5109 to <2 x i32>
  %5111 = or i64 %5094, 6
  %5112 = getelementptr inbounds i32, i32* %1, i64 %5111
  %5113 = bitcast i32* %5112 to <2 x i32>*
  store <2 x i32> %5110, <2 x i32>* %5113, align 4
  %5114 = load <2 x i64>, <2 x i64>* %5037, align 16
  %5115 = trunc <2 x i64> %5114 to <2 x i32>
  %5116 = or i64 %5094, 8
  %5117 = getelementptr inbounds i32, i32* %1, i64 %5116
  %5118 = bitcast i32* %5117 to <2 x i32>*
  store <2 x i32> %5115, <2 x i32>* %5118, align 4
  %5119 = load <2 x i64>, <2 x i64>* %5039, align 16
  %5120 = trunc <2 x i64> %5119 to <2 x i32>
  %5121 = or i64 %5094, 10
  %5122 = getelementptr inbounds i32, i32* %1, i64 %5121
  %5123 = bitcast i32* %5122 to <2 x i32>*
  store <2 x i32> %5120, <2 x i32>* %5123, align 4
  %5124 = load <2 x i64>, <2 x i64>* %5041, align 16
  %5125 = trunc <2 x i64> %5124 to <2 x i32>
  %5126 = or i64 %5094, 12
  %5127 = getelementptr inbounds i32, i32* %1, i64 %5126
  %5128 = bitcast i32* %5127 to <2 x i32>*
  store <2 x i32> %5125, <2 x i32>* %5128, align 4
  %5129 = load <2 x i64>, <2 x i64>* %5043, align 16
  %5130 = trunc <2 x i64> %5129 to <2 x i32>
  %5131 = or i64 %5094, 14
  %5132 = getelementptr inbounds i32, i32* %1, i64 %5131
  %5133 = bitcast i32* %5132 to <2 x i32>*
  store <2 x i32> %5130, <2 x i32>* %5133, align 4
  %5134 = load <2 x i64>, <2 x i64>* %5045, align 16
  %5135 = trunc <2 x i64> %5134 to <2 x i32>
  %5136 = or i64 %5094, 16
  %5137 = getelementptr inbounds i32, i32* %1, i64 %5136
  %5138 = bitcast i32* %5137 to <2 x i32>*
  store <2 x i32> %5135, <2 x i32>* %5138, align 4
  %5139 = load <2 x i64>, <2 x i64>* %5047, align 16
  %5140 = trunc <2 x i64> %5139 to <2 x i32>
  %5141 = or i64 %5094, 18
  %5142 = getelementptr inbounds i32, i32* %1, i64 %5141
  %5143 = bitcast i32* %5142 to <2 x i32>*
  store <2 x i32> %5140, <2 x i32>* %5143, align 4
  %5144 = load <2 x i64>, <2 x i64>* %5049, align 16
  %5145 = trunc <2 x i64> %5144 to <2 x i32>
  %5146 = or i64 %5094, 20
  %5147 = getelementptr inbounds i32, i32* %1, i64 %5146
  %5148 = bitcast i32* %5147 to <2 x i32>*
  store <2 x i32> %5145, <2 x i32>* %5148, align 4
  %5149 = load <2 x i64>, <2 x i64>* %5051, align 16
  %5150 = trunc <2 x i64> %5149 to <2 x i32>
  %5151 = or i64 %5094, 22
  %5152 = getelementptr inbounds i32, i32* %1, i64 %5151
  %5153 = bitcast i32* %5152 to <2 x i32>*
  store <2 x i32> %5150, <2 x i32>* %5153, align 4
  %5154 = load <2 x i64>, <2 x i64>* %5053, align 16
  %5155 = trunc <2 x i64> %5154 to <2 x i32>
  %5156 = or i64 %5094, 24
  %5157 = getelementptr inbounds i32, i32* %1, i64 %5156
  %5158 = bitcast i32* %5157 to <2 x i32>*
  store <2 x i32> %5155, <2 x i32>* %5158, align 4
  %5159 = load <2 x i64>, <2 x i64>* %5055, align 16
  %5160 = trunc <2 x i64> %5159 to <2 x i32>
  %5161 = or i64 %5094, 26
  %5162 = getelementptr inbounds i32, i32* %1, i64 %5161
  %5163 = bitcast i32* %5162 to <2 x i32>*
  store <2 x i32> %5160, <2 x i32>* %5163, align 4
  %5164 = load <2 x i64>, <2 x i64>* %5057, align 16
  %5165 = trunc <2 x i64> %5164 to <2 x i32>
  %5166 = or i64 %5094, 28
  %5167 = getelementptr inbounds i32, i32* %1, i64 %5166
  %5168 = bitcast i32* %5167 to <2 x i32>*
  store <2 x i32> %5165, <2 x i32>* %5168, align 4
  %5169 = load <2 x i64>, <2 x i64>* %5059, align 16
  %5170 = trunc <2 x i64> %5169 to <2 x i32>
  %5171 = or i64 %5094, 30
  %5172 = getelementptr inbounds i32, i32* %1, i64 %5171
  %5173 = bitcast i32* %5172 to <2 x i32>*
  store <2 x i32> %5170, <2 x i32>* %5173, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %5026) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %5025) #6
  %5174 = add nuw nsw i64 %5061, 1
  %5175 = icmp eq i64 %5174, 32
  br i1 %5175, label %6136, label %5060

5176:                                             ; preds = %4923
  %5177 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %4727, <8 x i16> %4027) #6
  %5178 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %4027, <8 x i16> %4727) #6
  %5179 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %4030, <8 x i16> %4728) #6
  %5180 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %4728, <8 x i16> %4030) #6
  %5181 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %4729, <8 x i16> %4031) #6
  %5182 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %4031, <8 x i16> %4729) #6
  %5183 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %4034, <8 x i16> %4730) #6
  %5184 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %4730, <8 x i16> %4034) #6
  %5185 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %4731, <8 x i16> %4035) #6
  %5186 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %4035, <8 x i16> %4731) #6
  %5187 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %4038, <8 x i16> %4732) #6
  %5188 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %4732, <8 x i16> %4038) #6
  %5189 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %4733, <8 x i16> %4039) #6
  %5190 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %4039, <8 x i16> %4733) #6
  %5191 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %4042, <8 x i16> %4734) #6
  %5192 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %4734, <8 x i16> %4042) #6
  %5193 = add <8 x i16> %5177, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5194 = icmp ult <8 x i16> %5193, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5195 = add <8 x i16> %5178, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5196 = icmp ult <8 x i16> %5195, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5197 = add <8 x i16> %5179, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5198 = icmp ult <8 x i16> %5197, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5199 = add <8 x i16> %5180, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5200 = icmp ult <8 x i16> %5199, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5201 = or <8 x i1> %5196, %5194
  %5202 = or <8 x i1> %5201, %5198
  %5203 = or <8 x i1> %5202, %5200
  %5204 = sext <8 x i1> %5203 to <8 x i16>
  %5205 = bitcast <8 x i16> %5204 to <16 x i8>
  %5206 = icmp slt <16 x i8> %5205, zeroinitializer
  %5207 = bitcast <16 x i1> %5206 to i16
  %5208 = zext i16 %5207 to i32
  %5209 = add <8 x i16> %5181, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5210 = icmp ult <8 x i16> %5209, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5211 = add <8 x i16> %5182, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5212 = icmp ult <8 x i16> %5211, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5213 = add <8 x i16> %5183, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5214 = icmp ult <8 x i16> %5213, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5215 = add <8 x i16> %5184, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5216 = icmp ult <8 x i16> %5215, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5217 = or <8 x i1> %5212, %5210
  %5218 = or <8 x i1> %5217, %5214
  %5219 = or <8 x i1> %5218, %5216
  %5220 = sext <8 x i1> %5219 to <8 x i16>
  %5221 = bitcast <8 x i16> %5220 to <16 x i8>
  %5222 = icmp slt <16 x i8> %5221, zeroinitializer
  %5223 = bitcast <16 x i1> %5222 to i16
  %5224 = zext i16 %5223 to i32
  %5225 = icmp eq i16 %5207, 0
  br i1 %5225, label %5226, label %5261

5226:                                             ; preds = %5176
  %5227 = add <8 x i16> %5185, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5228 = icmp ult <8 x i16> %5227, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5229 = add <8 x i16> %5186, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5230 = icmp ult <8 x i16> %5229, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5231 = add <8 x i16> %5187, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5232 = icmp ult <8 x i16> %5231, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5233 = add <8 x i16> %5188, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5234 = icmp ult <8 x i16> %5233, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5235 = or <8 x i1> %5230, %5228
  %5236 = or <8 x i1> %5235, %5232
  %5237 = or <8 x i1> %5236, %5234
  %5238 = sext <8 x i1> %5237 to <8 x i16>
  %5239 = bitcast <8 x i16> %5238 to <16 x i8>
  %5240 = icmp slt <16 x i8> %5239, zeroinitializer
  %5241 = bitcast <16 x i1> %5240 to i16
  %5242 = zext i16 %5241 to i32
  %5243 = icmp eq i16 %5223, 0
  br i1 %5243, label %5244, label %5261

5244:                                             ; preds = %5226
  %5245 = add <8 x i16> %5189, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5246 = icmp ult <8 x i16> %5245, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5247 = add <8 x i16> %5190, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5248 = icmp ult <8 x i16> %5247, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5249 = add <8 x i16> %5191, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5250 = icmp ult <8 x i16> %5249, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5251 = add <8 x i16> %5192, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5252 = icmp ult <8 x i16> %5251, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5253 = or <8 x i1> %5248, %5246
  %5254 = or <8 x i1> %5253, %5250
  %5255 = or <8 x i1> %5254, %5252
  %5256 = sext <8 x i1> %5255 to <8 x i16>
  %5257 = bitcast <8 x i16> %5256 to <16 x i8>
  %5258 = icmp slt <16 x i8> %5257, zeroinitializer
  %5259 = bitcast <16 x i1> %5258 to i16
  %5260 = zext i16 %5259 to i32
  br label %5261

5261:                                             ; preds = %5176, %5226, %5244
  %5262 = phi i32 [ %5208, %5176 ], [ %5242, %5226 ], [ %5242, %5244 ]
  %5263 = phi i32 [ %5224, %5176 ], [ %5224, %5226 ], [ %5260, %5244 ]
  %5264 = sub nsw i32 0, %5262
  %5265 = icmp eq i32 %5263, %5264
  br i1 %5265, label %5420, label %5266

5266:                                             ; preds = %5261
  br i1 %97, label %5267, label %5268

5267:                                             ; preds = %5266
  tail call void @vpx_highbd_fdct32x32_rd_c(i16* %0, i32* %1, i32 %2) #6
  br label %6136

5268:                                             ; preds = %5266
  %5269 = bitcast [32 x i64]* %4 to i8*
  %5270 = bitcast [32 x i64]* %5 to i8*
  %5271 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %5272 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %5273 = bitcast [32 x i64]* %5 to <2 x i64>*
  %5274 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %5275 = bitcast i64* %5274 to <2 x i64>*
  %5276 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %5277 = bitcast i64* %5276 to <2 x i64>*
  %5278 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %5279 = bitcast i64* %5278 to <2 x i64>*
  %5280 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %5281 = bitcast i64* %5280 to <2 x i64>*
  %5282 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %5283 = bitcast i64* %5282 to <2 x i64>*
  %5284 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %5285 = bitcast i64* %5284 to <2 x i64>*
  %5286 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %5287 = bitcast i64* %5286 to <2 x i64>*
  %5288 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %5289 = bitcast i64* %5288 to <2 x i64>*
  %5290 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %5291 = bitcast i64* %5290 to <2 x i64>*
  %5292 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %5293 = bitcast i64* %5292 to <2 x i64>*
  %5294 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %5295 = bitcast i64* %5294 to <2 x i64>*
  %5296 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %5297 = bitcast i64* %5296 to <2 x i64>*
  %5298 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %5299 = bitcast i64* %5298 to <2 x i64>*
  %5300 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %5301 = bitcast i64* %5300 to <2 x i64>*
  %5302 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %5303 = bitcast i64* %5302 to <2 x i64>*
  br label %5304

5304:                                             ; preds = %5337, %5268
  %5305 = phi i64 [ 0, %5268 ], [ %5418, %5337 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %5269) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %5269, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %5270) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %5270, i8 -86, i64 256, i1 false) #6
  br label %5306

5306:                                             ; preds = %5306, %5304
  %5307 = phi i64 [ 0, %5304 ], [ %5335, %5306 ]
  %5308 = shl i64 %5307, 5
  %5309 = add nuw nsw i64 %5308, %5305
  %5310 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %5309
  %5311 = load i16, i16* %5310, align 2
  %5312 = sext i16 %5311 to i64
  %5313 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %5307
  store i64 %5312, i64* %5313, align 16
  %5314 = or i64 %5307, 1
  %5315 = shl i64 %5314, 5
  %5316 = add nuw nsw i64 %5315, %5305
  %5317 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %5316
  %5318 = load i16, i16* %5317, align 2
  %5319 = sext i16 %5318 to i64
  %5320 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %5314
  store i64 %5319, i64* %5320, align 8
  %5321 = or i64 %5307, 2
  %5322 = shl i64 %5321, 5
  %5323 = add nuw nsw i64 %5322, %5305
  %5324 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %5323
  %5325 = load i16, i16* %5324, align 2
  %5326 = sext i16 %5325 to i64
  %5327 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %5321
  store i64 %5326, i64* %5327, align 16
  %5328 = or i64 %5307, 3
  %5329 = shl i64 %5328, 5
  %5330 = add nuw nsw i64 %5329, %5305
  %5331 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %5330
  %5332 = load i16, i16* %5331, align 2
  %5333 = sext i16 %5332 to i64
  %5334 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %5328
  store i64 %5333, i64* %5334, align 8
  %5335 = add nuw nsw i64 %5307, 4
  %5336 = icmp eq i64 %5335, 32
  br i1 %5336, label %5337, label %5306

5337:                                             ; preds = %5306
  call void @vpx_fdct32(i64* nonnull %5271, i64* nonnull %5272, i32 1) #6
  %5338 = shl i64 %5305, 5
  %5339 = load <2 x i64>, <2 x i64>* %5273, align 16
  %5340 = trunc <2 x i64> %5339 to <2 x i32>
  %5341 = getelementptr inbounds i32, i32* %1, i64 %5338
  %5342 = bitcast i32* %5341 to <2 x i32>*
  store <2 x i32> %5340, <2 x i32>* %5342, align 4
  %5343 = load <2 x i64>, <2 x i64>* %5275, align 16
  %5344 = trunc <2 x i64> %5343 to <2 x i32>
  %5345 = or i64 %5338, 2
  %5346 = getelementptr inbounds i32, i32* %1, i64 %5345
  %5347 = bitcast i32* %5346 to <2 x i32>*
  store <2 x i32> %5344, <2 x i32>* %5347, align 4
  %5348 = load <2 x i64>, <2 x i64>* %5277, align 16
  %5349 = trunc <2 x i64> %5348 to <2 x i32>
  %5350 = or i64 %5338, 4
  %5351 = getelementptr inbounds i32, i32* %1, i64 %5350
  %5352 = bitcast i32* %5351 to <2 x i32>*
  store <2 x i32> %5349, <2 x i32>* %5352, align 4
  %5353 = load <2 x i64>, <2 x i64>* %5279, align 16
  %5354 = trunc <2 x i64> %5353 to <2 x i32>
  %5355 = or i64 %5338, 6
  %5356 = getelementptr inbounds i32, i32* %1, i64 %5355
  %5357 = bitcast i32* %5356 to <2 x i32>*
  store <2 x i32> %5354, <2 x i32>* %5357, align 4
  %5358 = load <2 x i64>, <2 x i64>* %5281, align 16
  %5359 = trunc <2 x i64> %5358 to <2 x i32>
  %5360 = or i64 %5338, 8
  %5361 = getelementptr inbounds i32, i32* %1, i64 %5360
  %5362 = bitcast i32* %5361 to <2 x i32>*
  store <2 x i32> %5359, <2 x i32>* %5362, align 4
  %5363 = load <2 x i64>, <2 x i64>* %5283, align 16
  %5364 = trunc <2 x i64> %5363 to <2 x i32>
  %5365 = or i64 %5338, 10
  %5366 = getelementptr inbounds i32, i32* %1, i64 %5365
  %5367 = bitcast i32* %5366 to <2 x i32>*
  store <2 x i32> %5364, <2 x i32>* %5367, align 4
  %5368 = load <2 x i64>, <2 x i64>* %5285, align 16
  %5369 = trunc <2 x i64> %5368 to <2 x i32>
  %5370 = or i64 %5338, 12
  %5371 = getelementptr inbounds i32, i32* %1, i64 %5370
  %5372 = bitcast i32* %5371 to <2 x i32>*
  store <2 x i32> %5369, <2 x i32>* %5372, align 4
  %5373 = load <2 x i64>, <2 x i64>* %5287, align 16
  %5374 = trunc <2 x i64> %5373 to <2 x i32>
  %5375 = or i64 %5338, 14
  %5376 = getelementptr inbounds i32, i32* %1, i64 %5375
  %5377 = bitcast i32* %5376 to <2 x i32>*
  store <2 x i32> %5374, <2 x i32>* %5377, align 4
  %5378 = load <2 x i64>, <2 x i64>* %5289, align 16
  %5379 = trunc <2 x i64> %5378 to <2 x i32>
  %5380 = or i64 %5338, 16
  %5381 = getelementptr inbounds i32, i32* %1, i64 %5380
  %5382 = bitcast i32* %5381 to <2 x i32>*
  store <2 x i32> %5379, <2 x i32>* %5382, align 4
  %5383 = load <2 x i64>, <2 x i64>* %5291, align 16
  %5384 = trunc <2 x i64> %5383 to <2 x i32>
  %5385 = or i64 %5338, 18
  %5386 = getelementptr inbounds i32, i32* %1, i64 %5385
  %5387 = bitcast i32* %5386 to <2 x i32>*
  store <2 x i32> %5384, <2 x i32>* %5387, align 4
  %5388 = load <2 x i64>, <2 x i64>* %5293, align 16
  %5389 = trunc <2 x i64> %5388 to <2 x i32>
  %5390 = or i64 %5338, 20
  %5391 = getelementptr inbounds i32, i32* %1, i64 %5390
  %5392 = bitcast i32* %5391 to <2 x i32>*
  store <2 x i32> %5389, <2 x i32>* %5392, align 4
  %5393 = load <2 x i64>, <2 x i64>* %5295, align 16
  %5394 = trunc <2 x i64> %5393 to <2 x i32>
  %5395 = or i64 %5338, 22
  %5396 = getelementptr inbounds i32, i32* %1, i64 %5395
  %5397 = bitcast i32* %5396 to <2 x i32>*
  store <2 x i32> %5394, <2 x i32>* %5397, align 4
  %5398 = load <2 x i64>, <2 x i64>* %5297, align 16
  %5399 = trunc <2 x i64> %5398 to <2 x i32>
  %5400 = or i64 %5338, 24
  %5401 = getelementptr inbounds i32, i32* %1, i64 %5400
  %5402 = bitcast i32* %5401 to <2 x i32>*
  store <2 x i32> %5399, <2 x i32>* %5402, align 4
  %5403 = load <2 x i64>, <2 x i64>* %5299, align 16
  %5404 = trunc <2 x i64> %5403 to <2 x i32>
  %5405 = or i64 %5338, 26
  %5406 = getelementptr inbounds i32, i32* %1, i64 %5405
  %5407 = bitcast i32* %5406 to <2 x i32>*
  store <2 x i32> %5404, <2 x i32>* %5407, align 4
  %5408 = load <2 x i64>, <2 x i64>* %5301, align 16
  %5409 = trunc <2 x i64> %5408 to <2 x i32>
  %5410 = or i64 %5338, 28
  %5411 = getelementptr inbounds i32, i32* %1, i64 %5410
  %5412 = bitcast i32* %5411 to <2 x i32>*
  store <2 x i32> %5409, <2 x i32>* %5412, align 4
  %5413 = load <2 x i64>, <2 x i64>* %5303, align 16
  %5414 = trunc <2 x i64> %5413 to <2 x i32>
  %5415 = or i64 %5338, 30
  %5416 = getelementptr inbounds i32, i32* %1, i64 %5415
  %5417 = bitcast i32* %5416 to <2 x i32>*
  store <2 x i32> %5414, <2 x i32>* %5417, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %5270) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %5269) #6
  %5418 = add nuw nsw i64 %5305, 1
  %5419 = icmp eq i64 %5418, 32
  br i1 %5419, label %6136, label %5304

5420:                                             ; preds = %5261
  %5421 = shufflevector <8 x i16> %5177, <8 x i16> %5192, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %5422 = shufflevector <8 x i16> %5177, <8 x i16> %5192, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %5423 = shufflevector <8 x i16> %5178, <8 x i16> %5191, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %5424 = shufflevector <8 x i16> %5178, <8 x i16> %5191, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %5425 = shufflevector <8 x i16> %5179, <8 x i16> %5190, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %5426 = shufflevector <8 x i16> %5179, <8 x i16> %5190, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %5427 = shufflevector <8 x i16> %5180, <8 x i16> %5189, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %5428 = shufflevector <8 x i16> %5180, <8 x i16> %5189, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %5429 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %5421, <8 x i16> <i16 804, i16 16364, i16 804, i16 16364, i16 804, i16 16364, i16 804, i16 16364>) #6
  %5430 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %5422, <8 x i16> <i16 804, i16 16364, i16 804, i16 16364, i16 804, i16 16364, i16 804, i16 16364>) #6
  %5431 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %5423, <8 x i16> <i16 12140, i16 11003, i16 12140, i16 11003, i16 12140, i16 11003, i16 12140, i16 11003>) #6
  %5432 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %5424, <8 x i16> <i16 12140, i16 11003, i16 12140, i16 11003, i16 12140, i16 11003, i16 12140, i16 11003>) #6
  %5433 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %5425, <8 x i16> <i16 7005, i16 14811, i16 7005, i16 14811, i16 7005, i16 14811, i16 7005, i16 14811>) #6
  %5434 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %5426, <8 x i16> <i16 7005, i16 14811, i16 7005, i16 14811, i16 7005, i16 14811, i16 7005, i16 14811>) #6
  %5435 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %5427, <8 x i16> <i16 15426, i16 5520, i16 15426, i16 5520, i16 15426, i16 5520, i16 15426, i16 5520>) #6
  %5436 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %5428, <8 x i16> <i16 15426, i16 5520, i16 15426, i16 5520, i16 15426, i16 5520, i16 15426, i16 5520>) #6
  %5437 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %5427, <8 x i16> <i16 -5520, i16 15426, i16 -5520, i16 15426, i16 -5520, i16 15426, i16 -5520, i16 15426>) #6
  %5438 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %5428, <8 x i16> <i16 -5520, i16 15426, i16 -5520, i16 15426, i16 -5520, i16 15426, i16 -5520, i16 15426>) #6
  %5439 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %5425, <8 x i16> <i16 -14811, i16 7005, i16 -14811, i16 7005, i16 -14811, i16 7005, i16 -14811, i16 7005>) #6
  %5440 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %5426, <8 x i16> <i16 -14811, i16 7005, i16 -14811, i16 7005, i16 -14811, i16 7005, i16 -14811, i16 7005>) #6
  %5441 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %5423, <8 x i16> <i16 -11003, i16 12140, i16 -11003, i16 12140, i16 -11003, i16 12140, i16 -11003, i16 12140>) #6
  %5442 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %5424, <8 x i16> <i16 -11003, i16 12140, i16 -11003, i16 12140, i16 -11003, i16 12140, i16 -11003, i16 12140>) #6
  %5443 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %5421, <8 x i16> <i16 -16364, i16 804, i16 -16364, i16 804, i16 -16364, i16 804, i16 -16364, i16 804>) #6
  %5444 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %5422, <8 x i16> <i16 -16364, i16 804, i16 -16364, i16 804, i16 -16364, i16 804, i16 -16364, i16 804>) #6
  %5445 = add <4 x i32> %5429, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5446 = add <4 x i32> %5430, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5447 = add <4 x i32> %5431, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5448 = add <4 x i32> %5432, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5449 = add <4 x i32> %5433, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5450 = add <4 x i32> %5434, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5451 = add <4 x i32> %5435, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5452 = add <4 x i32> %5436, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5453 = add <4 x i32> %5437, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5454 = add <4 x i32> %5438, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5455 = add <4 x i32> %5439, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5456 = add <4 x i32> %5440, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5457 = add <4 x i32> %5441, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5458 = add <4 x i32> %5442, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5459 = add <4 x i32> %5443, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5460 = add <4 x i32> %5444, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5461 = ashr <4 x i32> %5445, <i32 14, i32 14, i32 14, i32 14>
  %5462 = ashr <4 x i32> %5446, <i32 14, i32 14, i32 14, i32 14>
  %5463 = ashr <4 x i32> %5447, <i32 14, i32 14, i32 14, i32 14>
  %5464 = ashr <4 x i32> %5448, <i32 14, i32 14, i32 14, i32 14>
  %5465 = ashr <4 x i32> %5449, <i32 14, i32 14, i32 14, i32 14>
  %5466 = ashr <4 x i32> %5450, <i32 14, i32 14, i32 14, i32 14>
  %5467 = ashr <4 x i32> %5451, <i32 14, i32 14, i32 14, i32 14>
  %5468 = ashr <4 x i32> %5452, <i32 14, i32 14, i32 14, i32 14>
  %5469 = ashr <4 x i32> %5453, <i32 14, i32 14, i32 14, i32 14>
  %5470 = ashr <4 x i32> %5454, <i32 14, i32 14, i32 14, i32 14>
  %5471 = ashr <4 x i32> %5455, <i32 14, i32 14, i32 14, i32 14>
  %5472 = ashr <4 x i32> %5456, <i32 14, i32 14, i32 14, i32 14>
  %5473 = ashr <4 x i32> %5457, <i32 14, i32 14, i32 14, i32 14>
  %5474 = ashr <4 x i32> %5458, <i32 14, i32 14, i32 14, i32 14>
  %5475 = ashr <4 x i32> %5459, <i32 14, i32 14, i32 14, i32 14>
  %5476 = ashr <4 x i32> %5460, <i32 14, i32 14, i32 14, i32 14>
  %5477 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %5461, <4 x i32> %5462) #6
  store <8 x i16> %5477, <8 x i16>* %64, align 16
  %5478 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %5463, <4 x i32> %5464) #6
  store <8 x i16> %5478, <8 x i16>* %66, align 16
  %5479 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %5465, <4 x i32> %5466) #6
  store <8 x i16> %5479, <8 x i16>* %68, align 16
  %5480 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %5467, <4 x i32> %5468) #6
  store <8 x i16> %5480, <8 x i16>* %70, align 16
  %5481 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %5469, <4 x i32> %5470) #6
  store <8 x i16> %5481, <8 x i16>* %72, align 16
  %5482 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %5471, <4 x i32> %5472) #6
  store <8 x i16> %5482, <8 x i16>* %74, align 16
  %5483 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %5473, <4 x i32> %5474) #6
  store <8 x i16> %5483, <8 x i16>* %76, align 16
  %5484 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %5475, <4 x i32> %5476) #6
  store <8 x i16> %5484, <8 x i16>* %78, align 16
  %5485 = add <8 x i16> %5477, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5486 = icmp ult <8 x i16> %5485, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5487 = add <8 x i16> %5478, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5488 = icmp ult <8 x i16> %5487, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5489 = add <8 x i16> %5479, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5490 = icmp ult <8 x i16> %5489, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5491 = add <8 x i16> %5480, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5492 = icmp ult <8 x i16> %5491, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5493 = or <8 x i1> %5488, %5486
  %5494 = or <8 x i1> %5493, %5490
  %5495 = or <8 x i1> %5494, %5492
  %5496 = sext <8 x i1> %5495 to <8 x i16>
  %5497 = bitcast <8 x i16> %5496 to <16 x i8>
  %5498 = icmp slt <16 x i8> %5497, zeroinitializer
  %5499 = bitcast <16 x i1> %5498 to i16
  %5500 = zext i16 %5499 to i32
  %5501 = add <8 x i16> %5481, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5502 = icmp ult <8 x i16> %5501, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5503 = add <8 x i16> %5482, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5504 = icmp ult <8 x i16> %5503, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5505 = add <8 x i16> %5483, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5506 = icmp ult <8 x i16> %5505, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5507 = add <8 x i16> %5484, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5508 = icmp ult <8 x i16> %5507, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5509 = or <8 x i1> %5504, %5502
  %5510 = or <8 x i1> %5509, %5506
  %5511 = or <8 x i1> %5510, %5508
  %5512 = sext <8 x i1> %5511 to <8 x i16>
  %5513 = bitcast <8 x i16> %5512 to <16 x i8>
  %5514 = icmp slt <16 x i8> %5513, zeroinitializer
  %5515 = bitcast <16 x i1> %5514 to i16
  %5516 = zext i16 %5515 to i32
  %5517 = sub nsw i32 0, %5500
  %5518 = icmp eq i32 %5516, %5517
  br i1 %5518, label %5673, label %5519

5519:                                             ; preds = %5420
  br i1 %97, label %5520, label %5521

5520:                                             ; preds = %5519
  tail call void @vpx_highbd_fdct32x32_rd_c(i16* %0, i32* %1, i32 %2) #6
  br label %6136

5521:                                             ; preds = %5519
  %5522 = bitcast [32 x i64]* %4 to i8*
  %5523 = bitcast [32 x i64]* %5 to i8*
  %5524 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %5525 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %5526 = bitcast [32 x i64]* %5 to <2 x i64>*
  %5527 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %5528 = bitcast i64* %5527 to <2 x i64>*
  %5529 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %5530 = bitcast i64* %5529 to <2 x i64>*
  %5531 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %5532 = bitcast i64* %5531 to <2 x i64>*
  %5533 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %5534 = bitcast i64* %5533 to <2 x i64>*
  %5535 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %5536 = bitcast i64* %5535 to <2 x i64>*
  %5537 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %5538 = bitcast i64* %5537 to <2 x i64>*
  %5539 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %5540 = bitcast i64* %5539 to <2 x i64>*
  %5541 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %5542 = bitcast i64* %5541 to <2 x i64>*
  %5543 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %5544 = bitcast i64* %5543 to <2 x i64>*
  %5545 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %5546 = bitcast i64* %5545 to <2 x i64>*
  %5547 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %5548 = bitcast i64* %5547 to <2 x i64>*
  %5549 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %5550 = bitcast i64* %5549 to <2 x i64>*
  %5551 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %5552 = bitcast i64* %5551 to <2 x i64>*
  %5553 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %5554 = bitcast i64* %5553 to <2 x i64>*
  %5555 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %5556 = bitcast i64* %5555 to <2 x i64>*
  br label %5557

5557:                                             ; preds = %5590, %5521
  %5558 = phi i64 [ 0, %5521 ], [ %5671, %5590 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %5522) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %5522, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %5523) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %5523, i8 -86, i64 256, i1 false) #6
  br label %5559

5559:                                             ; preds = %5559, %5557
  %5560 = phi i64 [ 0, %5557 ], [ %5588, %5559 ]
  %5561 = shl i64 %5560, 5
  %5562 = add nuw nsw i64 %5561, %5558
  %5563 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %5562
  %5564 = load i16, i16* %5563, align 2
  %5565 = sext i16 %5564 to i64
  %5566 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %5560
  store i64 %5565, i64* %5566, align 16
  %5567 = or i64 %5560, 1
  %5568 = shl i64 %5567, 5
  %5569 = add nuw nsw i64 %5568, %5558
  %5570 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %5569
  %5571 = load i16, i16* %5570, align 2
  %5572 = sext i16 %5571 to i64
  %5573 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %5567
  store i64 %5572, i64* %5573, align 8
  %5574 = or i64 %5560, 2
  %5575 = shl i64 %5574, 5
  %5576 = add nuw nsw i64 %5575, %5558
  %5577 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %5576
  %5578 = load i16, i16* %5577, align 2
  %5579 = sext i16 %5578 to i64
  %5580 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %5574
  store i64 %5579, i64* %5580, align 16
  %5581 = or i64 %5560, 3
  %5582 = shl i64 %5581, 5
  %5583 = add nuw nsw i64 %5582, %5558
  %5584 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %5583
  %5585 = load i16, i16* %5584, align 2
  %5586 = sext i16 %5585 to i64
  %5587 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %5581
  store i64 %5586, i64* %5587, align 8
  %5588 = add nuw nsw i64 %5560, 4
  %5589 = icmp eq i64 %5588, 32
  br i1 %5589, label %5590, label %5559

5590:                                             ; preds = %5559
  call void @vpx_fdct32(i64* nonnull %5524, i64* nonnull %5525, i32 1) #6
  %5591 = shl i64 %5558, 5
  %5592 = load <2 x i64>, <2 x i64>* %5526, align 16
  %5593 = trunc <2 x i64> %5592 to <2 x i32>
  %5594 = getelementptr inbounds i32, i32* %1, i64 %5591
  %5595 = bitcast i32* %5594 to <2 x i32>*
  store <2 x i32> %5593, <2 x i32>* %5595, align 4
  %5596 = load <2 x i64>, <2 x i64>* %5528, align 16
  %5597 = trunc <2 x i64> %5596 to <2 x i32>
  %5598 = or i64 %5591, 2
  %5599 = getelementptr inbounds i32, i32* %1, i64 %5598
  %5600 = bitcast i32* %5599 to <2 x i32>*
  store <2 x i32> %5597, <2 x i32>* %5600, align 4
  %5601 = load <2 x i64>, <2 x i64>* %5530, align 16
  %5602 = trunc <2 x i64> %5601 to <2 x i32>
  %5603 = or i64 %5591, 4
  %5604 = getelementptr inbounds i32, i32* %1, i64 %5603
  %5605 = bitcast i32* %5604 to <2 x i32>*
  store <2 x i32> %5602, <2 x i32>* %5605, align 4
  %5606 = load <2 x i64>, <2 x i64>* %5532, align 16
  %5607 = trunc <2 x i64> %5606 to <2 x i32>
  %5608 = or i64 %5591, 6
  %5609 = getelementptr inbounds i32, i32* %1, i64 %5608
  %5610 = bitcast i32* %5609 to <2 x i32>*
  store <2 x i32> %5607, <2 x i32>* %5610, align 4
  %5611 = load <2 x i64>, <2 x i64>* %5534, align 16
  %5612 = trunc <2 x i64> %5611 to <2 x i32>
  %5613 = or i64 %5591, 8
  %5614 = getelementptr inbounds i32, i32* %1, i64 %5613
  %5615 = bitcast i32* %5614 to <2 x i32>*
  store <2 x i32> %5612, <2 x i32>* %5615, align 4
  %5616 = load <2 x i64>, <2 x i64>* %5536, align 16
  %5617 = trunc <2 x i64> %5616 to <2 x i32>
  %5618 = or i64 %5591, 10
  %5619 = getelementptr inbounds i32, i32* %1, i64 %5618
  %5620 = bitcast i32* %5619 to <2 x i32>*
  store <2 x i32> %5617, <2 x i32>* %5620, align 4
  %5621 = load <2 x i64>, <2 x i64>* %5538, align 16
  %5622 = trunc <2 x i64> %5621 to <2 x i32>
  %5623 = or i64 %5591, 12
  %5624 = getelementptr inbounds i32, i32* %1, i64 %5623
  %5625 = bitcast i32* %5624 to <2 x i32>*
  store <2 x i32> %5622, <2 x i32>* %5625, align 4
  %5626 = load <2 x i64>, <2 x i64>* %5540, align 16
  %5627 = trunc <2 x i64> %5626 to <2 x i32>
  %5628 = or i64 %5591, 14
  %5629 = getelementptr inbounds i32, i32* %1, i64 %5628
  %5630 = bitcast i32* %5629 to <2 x i32>*
  store <2 x i32> %5627, <2 x i32>* %5630, align 4
  %5631 = load <2 x i64>, <2 x i64>* %5542, align 16
  %5632 = trunc <2 x i64> %5631 to <2 x i32>
  %5633 = or i64 %5591, 16
  %5634 = getelementptr inbounds i32, i32* %1, i64 %5633
  %5635 = bitcast i32* %5634 to <2 x i32>*
  store <2 x i32> %5632, <2 x i32>* %5635, align 4
  %5636 = load <2 x i64>, <2 x i64>* %5544, align 16
  %5637 = trunc <2 x i64> %5636 to <2 x i32>
  %5638 = or i64 %5591, 18
  %5639 = getelementptr inbounds i32, i32* %1, i64 %5638
  %5640 = bitcast i32* %5639 to <2 x i32>*
  store <2 x i32> %5637, <2 x i32>* %5640, align 4
  %5641 = load <2 x i64>, <2 x i64>* %5546, align 16
  %5642 = trunc <2 x i64> %5641 to <2 x i32>
  %5643 = or i64 %5591, 20
  %5644 = getelementptr inbounds i32, i32* %1, i64 %5643
  %5645 = bitcast i32* %5644 to <2 x i32>*
  store <2 x i32> %5642, <2 x i32>* %5645, align 4
  %5646 = load <2 x i64>, <2 x i64>* %5548, align 16
  %5647 = trunc <2 x i64> %5646 to <2 x i32>
  %5648 = or i64 %5591, 22
  %5649 = getelementptr inbounds i32, i32* %1, i64 %5648
  %5650 = bitcast i32* %5649 to <2 x i32>*
  store <2 x i32> %5647, <2 x i32>* %5650, align 4
  %5651 = load <2 x i64>, <2 x i64>* %5550, align 16
  %5652 = trunc <2 x i64> %5651 to <2 x i32>
  %5653 = or i64 %5591, 24
  %5654 = getelementptr inbounds i32, i32* %1, i64 %5653
  %5655 = bitcast i32* %5654 to <2 x i32>*
  store <2 x i32> %5652, <2 x i32>* %5655, align 4
  %5656 = load <2 x i64>, <2 x i64>* %5552, align 16
  %5657 = trunc <2 x i64> %5656 to <2 x i32>
  %5658 = or i64 %5591, 26
  %5659 = getelementptr inbounds i32, i32* %1, i64 %5658
  %5660 = bitcast i32* %5659 to <2 x i32>*
  store <2 x i32> %5657, <2 x i32>* %5660, align 4
  %5661 = load <2 x i64>, <2 x i64>* %5554, align 16
  %5662 = trunc <2 x i64> %5661 to <2 x i32>
  %5663 = or i64 %5591, 28
  %5664 = getelementptr inbounds i32, i32* %1, i64 %5663
  %5665 = bitcast i32* %5664 to <2 x i32>*
  store <2 x i32> %5662, <2 x i32>* %5665, align 4
  %5666 = load <2 x i64>, <2 x i64>* %5556, align 16
  %5667 = trunc <2 x i64> %5666 to <2 x i32>
  %5668 = or i64 %5591, 30
  %5669 = getelementptr inbounds i32, i32* %1, i64 %5668
  %5670 = bitcast i32* %5669 to <2 x i32>*
  store <2 x i32> %5667, <2 x i32>* %5670, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %5523) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %5522) #6
  %5671 = add nuw nsw i64 %5558, 1
  %5672 = icmp eq i64 %5671, 32
  br i1 %5672, label %6136, label %5557

5673:                                             ; preds = %5420
  %5674 = shufflevector <8 x i16> %5181, <8 x i16> %5188, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %5675 = shufflevector <8 x i16> %5181, <8 x i16> %5188, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %5676 = shufflevector <8 x i16> %5182, <8 x i16> %5187, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %5677 = shufflevector <8 x i16> %5182, <8 x i16> %5187, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %5678 = shufflevector <8 x i16> %5183, <8 x i16> %5186, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %5679 = shufflevector <8 x i16> %5183, <8 x i16> %5186, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %5680 = shufflevector <8 x i16> %5184, <8 x i16> %5185, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %5681 = shufflevector <8 x i16> %5184, <8 x i16> %5185, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %5682 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %5674, <8 x i16> <i16 3981, i16 15893, i16 3981, i16 15893, i16 3981, i16 15893, i16 3981, i16 15893>) #6
  %5683 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %5675, <8 x i16> <i16 3981, i16 15893, i16 3981, i16 15893, i16 3981, i16 15893, i16 3981, i16 15893>) #6
  %5684 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %5676, <8 x i16> <i16 14053, i16 8423, i16 14053, i16 8423, i16 14053, i16 8423, i16 14053, i16 8423>) #6
  %5685 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %5677, <8 x i16> <i16 14053, i16 8423, i16 14053, i16 8423, i16 14053, i16 8423, i16 14053, i16 8423>) #6
  %5686 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %5678, <8 x i16> <i16 9760, i16 13160, i16 9760, i16 13160, i16 9760, i16 13160, i16 9760, i16 13160>) #6
  %5687 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %5679, <8 x i16> <i16 9760, i16 13160, i16 9760, i16 13160, i16 9760, i16 13160, i16 9760, i16 13160>) #6
  %5688 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %5680, <8 x i16> <i16 16207, i16 2404, i16 16207, i16 2404, i16 16207, i16 2404, i16 16207, i16 2404>) #6
  %5689 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %5681, <8 x i16> <i16 16207, i16 2404, i16 16207, i16 2404, i16 16207, i16 2404, i16 16207, i16 2404>) #6
  %5690 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %5680, <8 x i16> <i16 -2404, i16 16207, i16 -2404, i16 16207, i16 -2404, i16 16207, i16 -2404, i16 16207>) #6
  %5691 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %5681, <8 x i16> <i16 -2404, i16 16207, i16 -2404, i16 16207, i16 -2404, i16 16207, i16 -2404, i16 16207>) #6
  %5692 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %5678, <8 x i16> <i16 -13160, i16 9760, i16 -13160, i16 9760, i16 -13160, i16 9760, i16 -13160, i16 9760>) #6
  %5693 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %5679, <8 x i16> <i16 -13160, i16 9760, i16 -13160, i16 9760, i16 -13160, i16 9760, i16 -13160, i16 9760>) #6
  %5694 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %5676, <8 x i16> <i16 -8423, i16 14053, i16 -8423, i16 14053, i16 -8423, i16 14053, i16 -8423, i16 14053>) #6
  %5695 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %5677, <8 x i16> <i16 -8423, i16 14053, i16 -8423, i16 14053, i16 -8423, i16 14053, i16 -8423, i16 14053>) #6
  %5696 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %5674, <8 x i16> <i16 -15893, i16 3981, i16 -15893, i16 3981, i16 -15893, i16 3981, i16 -15893, i16 3981>) #6
  %5697 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %5675, <8 x i16> <i16 -15893, i16 3981, i16 -15893, i16 3981, i16 -15893, i16 3981, i16 -15893, i16 3981>) #6
  %5698 = add <4 x i32> %5682, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5699 = add <4 x i32> %5683, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5700 = add <4 x i32> %5684, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5701 = add <4 x i32> %5685, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5702 = add <4 x i32> %5686, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5703 = add <4 x i32> %5687, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5704 = add <4 x i32> %5688, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5705 = add <4 x i32> %5689, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5706 = add <4 x i32> %5690, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5707 = add <4 x i32> %5691, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5708 = add <4 x i32> %5692, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5709 = add <4 x i32> %5693, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5710 = add <4 x i32> %5694, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5711 = add <4 x i32> %5695, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5712 = add <4 x i32> %5696, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5713 = add <4 x i32> %5697, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5714 = ashr <4 x i32> %5698, <i32 14, i32 14, i32 14, i32 14>
  %5715 = ashr <4 x i32> %5699, <i32 14, i32 14, i32 14, i32 14>
  %5716 = ashr <4 x i32> %5700, <i32 14, i32 14, i32 14, i32 14>
  %5717 = ashr <4 x i32> %5701, <i32 14, i32 14, i32 14, i32 14>
  %5718 = ashr <4 x i32> %5702, <i32 14, i32 14, i32 14, i32 14>
  %5719 = ashr <4 x i32> %5703, <i32 14, i32 14, i32 14, i32 14>
  %5720 = ashr <4 x i32> %5704, <i32 14, i32 14, i32 14, i32 14>
  %5721 = ashr <4 x i32> %5705, <i32 14, i32 14, i32 14, i32 14>
  %5722 = ashr <4 x i32> %5706, <i32 14, i32 14, i32 14, i32 14>
  %5723 = ashr <4 x i32> %5707, <i32 14, i32 14, i32 14, i32 14>
  %5724 = ashr <4 x i32> %5708, <i32 14, i32 14, i32 14, i32 14>
  %5725 = ashr <4 x i32> %5709, <i32 14, i32 14, i32 14, i32 14>
  %5726 = ashr <4 x i32> %5710, <i32 14, i32 14, i32 14, i32 14>
  %5727 = ashr <4 x i32> %5711, <i32 14, i32 14, i32 14, i32 14>
  %5728 = ashr <4 x i32> %5712, <i32 14, i32 14, i32 14, i32 14>
  %5729 = ashr <4 x i32> %5713, <i32 14, i32 14, i32 14, i32 14>
  %5730 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %5714, <4 x i32> %5715) #6
  store <8 x i16> %5730, <8 x i16>* %80, align 16
  %5731 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %5716, <4 x i32> %5717) #6
  store <8 x i16> %5731, <8 x i16>* %82, align 16
  %5732 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %5718, <4 x i32> %5719) #6
  store <8 x i16> %5732, <8 x i16>* %84, align 16
  %5733 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %5720, <4 x i32> %5721) #6
  store <8 x i16> %5733, <8 x i16>* %86, align 16
  %5734 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %5722, <4 x i32> %5723) #6
  store <8 x i16> %5734, <8 x i16>* %88, align 16
  %5735 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %5724, <4 x i32> %5725) #6
  store <8 x i16> %5735, <8 x i16>* %90, align 16
  %5736 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %5726, <4 x i32> %5727) #6
  store <8 x i16> %5736, <8 x i16>* %92, align 16
  %5737 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %5728, <4 x i32> %5729) #6
  store <8 x i16> %5737, <8 x i16>* %94, align 16
  %5738 = add <8 x i16> %5730, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5739 = icmp ult <8 x i16> %5738, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5740 = add <8 x i16> %5731, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5741 = icmp ult <8 x i16> %5740, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5742 = add <8 x i16> %5732, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5743 = icmp ult <8 x i16> %5742, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5744 = add <8 x i16> %5733, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5745 = icmp ult <8 x i16> %5744, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5746 = or <8 x i1> %5741, %5739
  %5747 = or <8 x i1> %5746, %5743
  %5748 = or <8 x i1> %5747, %5745
  %5749 = sext <8 x i1> %5748 to <8 x i16>
  %5750 = bitcast <8 x i16> %5749 to <16 x i8>
  %5751 = icmp slt <16 x i8> %5750, zeroinitializer
  %5752 = bitcast <16 x i1> %5751 to i16
  %5753 = zext i16 %5752 to i32
  %5754 = add <8 x i16> %5734, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5755 = icmp ult <8 x i16> %5754, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5756 = add <8 x i16> %5735, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5757 = icmp ult <8 x i16> %5756, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5758 = add <8 x i16> %5736, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5759 = icmp ult <8 x i16> %5758, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5760 = add <8 x i16> %5737, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5761 = icmp ult <8 x i16> %5760, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5762 = or <8 x i1> %5757, %5755
  %5763 = or <8 x i1> %5762, %5759
  %5764 = or <8 x i1> %5763, %5761
  %5765 = sext <8 x i1> %5764 to <8 x i16>
  %5766 = bitcast <8 x i16> %5765 to <16 x i8>
  %5767 = icmp slt <16 x i8> %5766, zeroinitializer
  %5768 = bitcast <16 x i1> %5767 to i16
  %5769 = zext i16 %5768 to i32
  %5770 = sub nsw i32 0, %5753
  %5771 = icmp eq i32 %5769, %5770
  br i1 %5771, label %5926, label %5772

5772:                                             ; preds = %5673
  br i1 %97, label %5773, label %5774

5773:                                             ; preds = %5772
  tail call void @vpx_highbd_fdct32x32_rd_c(i16* %0, i32* %1, i32 %2) #6
  br label %6136

5774:                                             ; preds = %5772
  %5775 = bitcast [32 x i64]* %4 to i8*
  %5776 = bitcast [32 x i64]* %5 to i8*
  %5777 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %5778 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %5779 = bitcast [32 x i64]* %5 to <2 x i64>*
  %5780 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %5781 = bitcast i64* %5780 to <2 x i64>*
  %5782 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %5783 = bitcast i64* %5782 to <2 x i64>*
  %5784 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %5785 = bitcast i64* %5784 to <2 x i64>*
  %5786 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %5787 = bitcast i64* %5786 to <2 x i64>*
  %5788 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %5789 = bitcast i64* %5788 to <2 x i64>*
  %5790 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %5791 = bitcast i64* %5790 to <2 x i64>*
  %5792 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %5793 = bitcast i64* %5792 to <2 x i64>*
  %5794 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %5795 = bitcast i64* %5794 to <2 x i64>*
  %5796 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %5797 = bitcast i64* %5796 to <2 x i64>*
  %5798 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %5799 = bitcast i64* %5798 to <2 x i64>*
  %5800 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %5801 = bitcast i64* %5800 to <2 x i64>*
  %5802 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %5803 = bitcast i64* %5802 to <2 x i64>*
  %5804 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %5805 = bitcast i64* %5804 to <2 x i64>*
  %5806 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %5807 = bitcast i64* %5806 to <2 x i64>*
  %5808 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %5809 = bitcast i64* %5808 to <2 x i64>*
  br label %5810

5810:                                             ; preds = %5843, %5774
  %5811 = phi i64 [ 0, %5774 ], [ %5924, %5843 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %5775) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %5775, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %5776) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %5776, i8 -86, i64 256, i1 false) #6
  br label %5812

5812:                                             ; preds = %5812, %5810
  %5813 = phi i64 [ 0, %5810 ], [ %5841, %5812 ]
  %5814 = shl i64 %5813, 5
  %5815 = add nuw nsw i64 %5814, %5811
  %5816 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %5815
  %5817 = load i16, i16* %5816, align 2
  %5818 = sext i16 %5817 to i64
  %5819 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %5813
  store i64 %5818, i64* %5819, align 16
  %5820 = or i64 %5813, 1
  %5821 = shl i64 %5820, 5
  %5822 = add nuw nsw i64 %5821, %5811
  %5823 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %5822
  %5824 = load i16, i16* %5823, align 2
  %5825 = sext i16 %5824 to i64
  %5826 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %5820
  store i64 %5825, i64* %5826, align 8
  %5827 = or i64 %5813, 2
  %5828 = shl i64 %5827, 5
  %5829 = add nuw nsw i64 %5828, %5811
  %5830 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %5829
  %5831 = load i16, i16* %5830, align 2
  %5832 = sext i16 %5831 to i64
  %5833 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %5827
  store i64 %5832, i64* %5833, align 16
  %5834 = or i64 %5813, 3
  %5835 = shl i64 %5834, 5
  %5836 = add nuw nsw i64 %5835, %5811
  %5837 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %5836
  %5838 = load i16, i16* %5837, align 2
  %5839 = sext i16 %5838 to i64
  %5840 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %5834
  store i64 %5839, i64* %5840, align 8
  %5841 = add nuw nsw i64 %5813, 4
  %5842 = icmp eq i64 %5841, 32
  br i1 %5842, label %5843, label %5812

5843:                                             ; preds = %5812
  call void @vpx_fdct32(i64* nonnull %5777, i64* nonnull %5778, i32 1) #6
  %5844 = shl i64 %5811, 5
  %5845 = load <2 x i64>, <2 x i64>* %5779, align 16
  %5846 = trunc <2 x i64> %5845 to <2 x i32>
  %5847 = getelementptr inbounds i32, i32* %1, i64 %5844
  %5848 = bitcast i32* %5847 to <2 x i32>*
  store <2 x i32> %5846, <2 x i32>* %5848, align 4
  %5849 = load <2 x i64>, <2 x i64>* %5781, align 16
  %5850 = trunc <2 x i64> %5849 to <2 x i32>
  %5851 = or i64 %5844, 2
  %5852 = getelementptr inbounds i32, i32* %1, i64 %5851
  %5853 = bitcast i32* %5852 to <2 x i32>*
  store <2 x i32> %5850, <2 x i32>* %5853, align 4
  %5854 = load <2 x i64>, <2 x i64>* %5783, align 16
  %5855 = trunc <2 x i64> %5854 to <2 x i32>
  %5856 = or i64 %5844, 4
  %5857 = getelementptr inbounds i32, i32* %1, i64 %5856
  %5858 = bitcast i32* %5857 to <2 x i32>*
  store <2 x i32> %5855, <2 x i32>* %5858, align 4
  %5859 = load <2 x i64>, <2 x i64>* %5785, align 16
  %5860 = trunc <2 x i64> %5859 to <2 x i32>
  %5861 = or i64 %5844, 6
  %5862 = getelementptr inbounds i32, i32* %1, i64 %5861
  %5863 = bitcast i32* %5862 to <2 x i32>*
  store <2 x i32> %5860, <2 x i32>* %5863, align 4
  %5864 = load <2 x i64>, <2 x i64>* %5787, align 16
  %5865 = trunc <2 x i64> %5864 to <2 x i32>
  %5866 = or i64 %5844, 8
  %5867 = getelementptr inbounds i32, i32* %1, i64 %5866
  %5868 = bitcast i32* %5867 to <2 x i32>*
  store <2 x i32> %5865, <2 x i32>* %5868, align 4
  %5869 = load <2 x i64>, <2 x i64>* %5789, align 16
  %5870 = trunc <2 x i64> %5869 to <2 x i32>
  %5871 = or i64 %5844, 10
  %5872 = getelementptr inbounds i32, i32* %1, i64 %5871
  %5873 = bitcast i32* %5872 to <2 x i32>*
  store <2 x i32> %5870, <2 x i32>* %5873, align 4
  %5874 = load <2 x i64>, <2 x i64>* %5791, align 16
  %5875 = trunc <2 x i64> %5874 to <2 x i32>
  %5876 = or i64 %5844, 12
  %5877 = getelementptr inbounds i32, i32* %1, i64 %5876
  %5878 = bitcast i32* %5877 to <2 x i32>*
  store <2 x i32> %5875, <2 x i32>* %5878, align 4
  %5879 = load <2 x i64>, <2 x i64>* %5793, align 16
  %5880 = trunc <2 x i64> %5879 to <2 x i32>
  %5881 = or i64 %5844, 14
  %5882 = getelementptr inbounds i32, i32* %1, i64 %5881
  %5883 = bitcast i32* %5882 to <2 x i32>*
  store <2 x i32> %5880, <2 x i32>* %5883, align 4
  %5884 = load <2 x i64>, <2 x i64>* %5795, align 16
  %5885 = trunc <2 x i64> %5884 to <2 x i32>
  %5886 = or i64 %5844, 16
  %5887 = getelementptr inbounds i32, i32* %1, i64 %5886
  %5888 = bitcast i32* %5887 to <2 x i32>*
  store <2 x i32> %5885, <2 x i32>* %5888, align 4
  %5889 = load <2 x i64>, <2 x i64>* %5797, align 16
  %5890 = trunc <2 x i64> %5889 to <2 x i32>
  %5891 = or i64 %5844, 18
  %5892 = getelementptr inbounds i32, i32* %1, i64 %5891
  %5893 = bitcast i32* %5892 to <2 x i32>*
  store <2 x i32> %5890, <2 x i32>* %5893, align 4
  %5894 = load <2 x i64>, <2 x i64>* %5799, align 16
  %5895 = trunc <2 x i64> %5894 to <2 x i32>
  %5896 = or i64 %5844, 20
  %5897 = getelementptr inbounds i32, i32* %1, i64 %5896
  %5898 = bitcast i32* %5897 to <2 x i32>*
  store <2 x i32> %5895, <2 x i32>* %5898, align 4
  %5899 = load <2 x i64>, <2 x i64>* %5801, align 16
  %5900 = trunc <2 x i64> %5899 to <2 x i32>
  %5901 = or i64 %5844, 22
  %5902 = getelementptr inbounds i32, i32* %1, i64 %5901
  %5903 = bitcast i32* %5902 to <2 x i32>*
  store <2 x i32> %5900, <2 x i32>* %5903, align 4
  %5904 = load <2 x i64>, <2 x i64>* %5803, align 16
  %5905 = trunc <2 x i64> %5904 to <2 x i32>
  %5906 = or i64 %5844, 24
  %5907 = getelementptr inbounds i32, i32* %1, i64 %5906
  %5908 = bitcast i32* %5907 to <2 x i32>*
  store <2 x i32> %5905, <2 x i32>* %5908, align 4
  %5909 = load <2 x i64>, <2 x i64>* %5805, align 16
  %5910 = trunc <2 x i64> %5909 to <2 x i32>
  %5911 = or i64 %5844, 26
  %5912 = getelementptr inbounds i32, i32* %1, i64 %5911
  %5913 = bitcast i32* %5912 to <2 x i32>*
  store <2 x i32> %5910, <2 x i32>* %5913, align 4
  %5914 = load <2 x i64>, <2 x i64>* %5807, align 16
  %5915 = trunc <2 x i64> %5914 to <2 x i32>
  %5916 = or i64 %5844, 28
  %5917 = getelementptr inbounds i32, i32* %1, i64 %5916
  %5918 = bitcast i32* %5917 to <2 x i32>*
  store <2 x i32> %5915, <2 x i32>* %5918, align 4
  %5919 = load <2 x i64>, <2 x i64>* %5809, align 16
  %5920 = trunc <2 x i64> %5919 to <2 x i32>
  %5921 = or i64 %5844, 30
  %5922 = getelementptr inbounds i32, i32* %1, i64 %5921
  %5923 = bitcast i32* %5922 to <2 x i32>*
  store <2 x i32> %5920, <2 x i32>* %5923, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %5776) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %5775) #6
  %5924 = add nuw nsw i64 %5811, 1
  %5925 = icmp eq i64 %5924, 32
  br i1 %5925, label %6136, label %5810

5926:                                             ; preds = %5673
  %5927 = shl nsw i64 %100, 5
  %5928 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %5927
  %5929 = getelementptr inbounds i32, i32* %1, i64 %5927
  br label %5930

5930:                                             ; preds = %6128, %5926
  %5931 = phi i64 [ 0, %5926 ], [ %6131, %6128 ]
  %5932 = phi i32* [ %5929, %5926 ], [ %6130, %6128 ]
  %5933 = phi i16* [ %5928, %5926 ], [ %6129, %6128 ]
  %5934 = shl nsw i64 %5931, 3
  %5935 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 %5934
  %5936 = bitcast <2 x i64>* %5935 to <8 x i16>*
  %5937 = load <8 x i16>, <8 x i16>* %5936, align 16
  %5938 = getelementptr inbounds <2 x i64>, <2 x i64>* %5935, i64 1
  %5939 = bitcast <2 x i64>* %5938 to <8 x i16>*
  %5940 = load <8 x i16>, <8 x i16>* %5939, align 16
  %5941 = shufflevector <8 x i16> %5937, <8 x i16> %5940, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %5942 = getelementptr inbounds <2 x i64>, <2 x i64>* %5935, i64 2
  %5943 = bitcast <2 x i64>* %5942 to <8 x i16>*
  %5944 = load <8 x i16>, <8 x i16>* %5943, align 16
  %5945 = getelementptr inbounds <2 x i64>, <2 x i64>* %5935, i64 3
  %5946 = bitcast <2 x i64>* %5945 to <8 x i16>*
  %5947 = load <8 x i16>, <8 x i16>* %5946, align 16
  %5948 = shufflevector <8 x i16> %5944, <8 x i16> %5947, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %5949 = shufflevector <8 x i16> %5937, <8 x i16> %5940, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %5950 = shufflevector <8 x i16> %5944, <8 x i16> %5947, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %5951 = getelementptr inbounds <2 x i64>, <2 x i64>* %5935, i64 4
  %5952 = bitcast <2 x i64>* %5951 to <8 x i16>*
  %5953 = load <8 x i16>, <8 x i16>* %5952, align 16
  %5954 = getelementptr inbounds <2 x i64>, <2 x i64>* %5935, i64 5
  %5955 = bitcast <2 x i64>* %5954 to <8 x i16>*
  %5956 = load <8 x i16>, <8 x i16>* %5955, align 16
  %5957 = shufflevector <8 x i16> %5953, <8 x i16> %5956, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %5958 = getelementptr inbounds <2 x i64>, <2 x i64>* %5935, i64 6
  %5959 = bitcast <2 x i64>* %5958 to <8 x i16>*
  %5960 = load <8 x i16>, <8 x i16>* %5959, align 16
  %5961 = getelementptr inbounds <2 x i64>, <2 x i64>* %5935, i64 7
  %5962 = bitcast <2 x i64>* %5961 to <8 x i16>*
  %5963 = load <8 x i16>, <8 x i16>* %5962, align 16
  %5964 = shufflevector <8 x i16> %5960, <8 x i16> %5963, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %5965 = shufflevector <8 x i16> %5953, <8 x i16> %5956, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %5966 = shufflevector <8 x i16> %5960, <8 x i16> %5963, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %5967 = bitcast <8 x i16> %5941 to <4 x i32>
  %5968 = bitcast <8 x i16> %5948 to <4 x i32>
  %5969 = shufflevector <4 x i32> %5967, <4 x i32> %5968, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %5970 = bitcast <4 x i32> %5969 to <2 x i64>
  %5971 = bitcast <8 x i16> %5949 to <4 x i32>
  %5972 = bitcast <8 x i16> %5950 to <4 x i32>
  %5973 = shufflevector <4 x i32> %5971, <4 x i32> %5972, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %5974 = bitcast <4 x i32> %5973 to <2 x i64>
  %5975 = shufflevector <4 x i32> %5967, <4 x i32> %5968, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %5976 = bitcast <4 x i32> %5975 to <2 x i64>
  %5977 = shufflevector <4 x i32> %5971, <4 x i32> %5972, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %5978 = bitcast <4 x i32> %5977 to <2 x i64>
  %5979 = bitcast <8 x i16> %5957 to <4 x i32>
  %5980 = bitcast <8 x i16> %5964 to <4 x i32>
  %5981 = shufflevector <4 x i32> %5979, <4 x i32> %5980, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %5982 = bitcast <4 x i32> %5981 to <2 x i64>
  %5983 = bitcast <8 x i16> %5965 to <4 x i32>
  %5984 = bitcast <8 x i16> %5966 to <4 x i32>
  %5985 = shufflevector <4 x i32> %5983, <4 x i32> %5984, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %5986 = bitcast <4 x i32> %5985 to <2 x i64>
  %5987 = shufflevector <4 x i32> %5979, <4 x i32> %5980, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %5988 = bitcast <4 x i32> %5987 to <2 x i64>
  %5989 = shufflevector <4 x i32> %5983, <4 x i32> %5984, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %5990 = bitcast <4 x i32> %5989 to <2 x i64>
  %5991 = shufflevector <2 x i64> %5970, <2 x i64> %5982, <2 x i32> <i32 0, i32 2>
  %5992 = shufflevector <2 x i64> %5970, <2 x i64> %5982, <2 x i32> <i32 1, i32 3>
  %5993 = shufflevector <2 x i64> %5976, <2 x i64> %5988, <2 x i32> <i32 0, i32 2>
  %5994 = shufflevector <2 x i64> %5976, <2 x i64> %5988, <2 x i32> <i32 1, i32 3>
  %5995 = shufflevector <2 x i64> %5974, <2 x i64> %5986, <2 x i32> <i32 0, i32 2>
  %5996 = shufflevector <2 x i64> %5974, <2 x i64> %5986, <2 x i32> <i32 1, i32 3>
  %5997 = shufflevector <2 x i64> %5978, <2 x i64> %5990, <2 x i32> <i32 0, i32 2>
  %5998 = shufflevector <2 x i64> %5978, <2 x i64> %5990, <2 x i32> <i32 1, i32 3>
  %5999 = bitcast <2 x i64> %5991 to <8 x i16>
  br i1 %97, label %6000, label %6064

6000:                                             ; preds = %5930
  %6001 = icmp sgt <8 x i16> %5999, zeroinitializer
  %6002 = bitcast <2 x i64> %5992 to <8 x i16>
  %6003 = icmp sgt <8 x i16> %6002, zeroinitializer
  %6004 = bitcast <2 x i64> %5993 to <8 x i16>
  %6005 = icmp sgt <8 x i16> %6004, zeroinitializer
  %6006 = bitcast <2 x i64> %5994 to <8 x i16>
  %6007 = icmp sgt <8 x i16> %6006, zeroinitializer
  %6008 = bitcast <2 x i64> %5995 to <8 x i16>
  %6009 = icmp sgt <8 x i16> %6008, zeroinitializer
  %6010 = bitcast <2 x i64> %5996 to <8 x i16>
  %6011 = icmp sgt <8 x i16> %6010, zeroinitializer
  %6012 = bitcast <2 x i64> %5997 to <8 x i16>
  %6013 = icmp sgt <8 x i16> %6012, zeroinitializer
  %6014 = bitcast <2 x i64> %5998 to <8 x i16>
  %6015 = icmp sgt <8 x i16> %6014, zeroinitializer
  %6016 = zext <8 x i1> %6001 to <8 x i16>
  %6017 = zext <8 x i1> %6003 to <8 x i16>
  %6018 = zext <8 x i1> %6005 to <8 x i16>
  %6019 = zext <8 x i1> %6007 to <8 x i16>
  %6020 = zext <8 x i1> %6009 to <8 x i16>
  %6021 = zext <8 x i1> %6011 to <8 x i16>
  %6022 = zext <8 x i1> %6013 to <8 x i16>
  %6023 = zext <8 x i1> %6015 to <8 x i16>
  %6024 = add <8 x i16> %5999, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %6025 = add <8 x i16> %6024, %6016
  %6026 = add <8 x i16> %6002, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %6027 = add <8 x i16> %6026, %6017
  %6028 = add <8 x i16> %6004, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %6029 = add <8 x i16> %6028, %6018
  %6030 = add <8 x i16> %6006, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %6031 = add <8 x i16> %6030, %6019
  %6032 = add <8 x i16> %6008, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %6033 = add <8 x i16> %6032, %6020
  %6034 = add <8 x i16> %6010, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %6035 = add <8 x i16> %6034, %6021
  %6036 = add <8 x i16> %6012, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %6037 = add <8 x i16> %6036, %6022
  %6038 = add <8 x i16> %6014, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %6039 = add <8 x i16> %6038, %6023
  %6040 = ashr <8 x i16> %6025, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %6041 = ashr <8 x i16> %6027, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %6042 = ashr <8 x i16> %6029, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %6043 = ashr <8 x i16> %6031, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %6044 = ashr <8 x i16> %6033, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %6045 = ashr <8 x i16> %6035, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %6046 = ashr <8 x i16> %6037, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %6047 = ashr <8 x i16> %6039, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %6048 = bitcast i16* %5933 to <8 x i16>*
  store <8 x i16> %6040, <8 x i16>* %6048, align 1
  %6049 = getelementptr inbounds i16, i16* %5933, i64 32
  %6050 = bitcast i16* %6049 to <8 x i16>*
  store <8 x i16> %6041, <8 x i16>* %6050, align 1
  %6051 = getelementptr inbounds i16, i16* %5933, i64 64
  %6052 = bitcast i16* %6051 to <8 x i16>*
  store <8 x i16> %6042, <8 x i16>* %6052, align 1
  %6053 = getelementptr inbounds i16, i16* %5933, i64 96
  %6054 = bitcast i16* %6053 to <8 x i16>*
  store <8 x i16> %6043, <8 x i16>* %6054, align 1
  %6055 = getelementptr inbounds i16, i16* %5933, i64 128
  %6056 = bitcast i16* %6055 to <8 x i16>*
  store <8 x i16> %6044, <8 x i16>* %6056, align 1
  %6057 = getelementptr inbounds i16, i16* %5933, i64 160
  %6058 = bitcast i16* %6057 to <8 x i16>*
  store <8 x i16> %6045, <8 x i16>* %6058, align 1
  %6059 = getelementptr inbounds i16, i16* %5933, i64 192
  %6060 = bitcast i16* %6059 to <8 x i16>*
  store <8 x i16> %6046, <8 x i16>* %6060, align 1
  %6061 = getelementptr inbounds i16, i16* %5933, i64 224
  %6062 = bitcast i16* %6061 to <8 x i16>*
  store <8 x i16> %6047, <8 x i16>* %6062, align 1
  %6063 = getelementptr inbounds i16, i16* %5933, i64 8
  br label %6128

6064:                                             ; preds = %5930
  %6065 = ashr <8 x i16> %5999, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %6066 = shufflevector <8 x i16> %5999, <8 x i16> %6065, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %6067 = shufflevector <8 x i16> %5999, <8 x i16> %6065, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %6068 = bitcast i32* %5932 to <8 x i16>*
  store <8 x i16> %6066, <8 x i16>* %6068, align 1
  %6069 = getelementptr inbounds i32, i32* %5932, i64 4
  %6070 = bitcast i32* %6069 to <8 x i16>*
  store <8 x i16> %6067, <8 x i16>* %6070, align 1
  %6071 = getelementptr inbounds i32, i32* %5932, i64 32
  %6072 = bitcast <2 x i64> %5992 to <8 x i16>
  %6073 = ashr <8 x i16> %6072, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %6074 = shufflevector <8 x i16> %6072, <8 x i16> %6073, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %6075 = shufflevector <8 x i16> %6072, <8 x i16> %6073, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %6076 = bitcast i32* %6071 to <8 x i16>*
  store <8 x i16> %6074, <8 x i16>* %6076, align 1
  %6077 = getelementptr inbounds i32, i32* %5932, i64 36
  %6078 = bitcast i32* %6077 to <8 x i16>*
  store <8 x i16> %6075, <8 x i16>* %6078, align 1
  %6079 = getelementptr inbounds i32, i32* %5932, i64 64
  %6080 = bitcast <2 x i64> %5993 to <8 x i16>
  %6081 = ashr <8 x i16> %6080, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %6082 = shufflevector <8 x i16> %6080, <8 x i16> %6081, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %6083 = shufflevector <8 x i16> %6080, <8 x i16> %6081, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %6084 = bitcast i32* %6079 to <8 x i16>*
  store <8 x i16> %6082, <8 x i16>* %6084, align 1
  %6085 = getelementptr inbounds i32, i32* %5932, i64 68
  %6086 = bitcast i32* %6085 to <8 x i16>*
  store <8 x i16> %6083, <8 x i16>* %6086, align 1
  %6087 = getelementptr inbounds i32, i32* %5932, i64 96
  %6088 = bitcast <2 x i64> %5994 to <8 x i16>
  %6089 = ashr <8 x i16> %6088, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %6090 = shufflevector <8 x i16> %6088, <8 x i16> %6089, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %6091 = shufflevector <8 x i16> %6088, <8 x i16> %6089, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %6092 = bitcast i32* %6087 to <8 x i16>*
  store <8 x i16> %6090, <8 x i16>* %6092, align 1
  %6093 = getelementptr inbounds i32, i32* %5932, i64 100
  %6094 = bitcast i32* %6093 to <8 x i16>*
  store <8 x i16> %6091, <8 x i16>* %6094, align 1
  %6095 = getelementptr inbounds i32, i32* %5932, i64 128
  %6096 = bitcast <2 x i64> %5995 to <8 x i16>
  %6097 = ashr <8 x i16> %6096, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %6098 = shufflevector <8 x i16> %6096, <8 x i16> %6097, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %6099 = shufflevector <8 x i16> %6096, <8 x i16> %6097, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %6100 = bitcast i32* %6095 to <8 x i16>*
  store <8 x i16> %6098, <8 x i16>* %6100, align 1
  %6101 = getelementptr inbounds i32, i32* %5932, i64 132
  %6102 = bitcast i32* %6101 to <8 x i16>*
  store <8 x i16> %6099, <8 x i16>* %6102, align 1
  %6103 = getelementptr inbounds i32, i32* %5932, i64 160
  %6104 = bitcast <2 x i64> %5996 to <8 x i16>
  %6105 = ashr <8 x i16> %6104, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %6106 = shufflevector <8 x i16> %6104, <8 x i16> %6105, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %6107 = shufflevector <8 x i16> %6104, <8 x i16> %6105, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %6108 = bitcast i32* %6103 to <8 x i16>*
  store <8 x i16> %6106, <8 x i16>* %6108, align 1
  %6109 = getelementptr inbounds i32, i32* %5932, i64 164
  %6110 = bitcast i32* %6109 to <8 x i16>*
  store <8 x i16> %6107, <8 x i16>* %6110, align 1
  %6111 = getelementptr inbounds i32, i32* %5932, i64 192
  %6112 = bitcast <2 x i64> %5997 to <8 x i16>
  %6113 = ashr <8 x i16> %6112, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %6114 = shufflevector <8 x i16> %6112, <8 x i16> %6113, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %6115 = shufflevector <8 x i16> %6112, <8 x i16> %6113, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %6116 = bitcast i32* %6111 to <8 x i16>*
  store <8 x i16> %6114, <8 x i16>* %6116, align 1
  %6117 = getelementptr inbounds i32, i32* %5932, i64 196
  %6118 = bitcast i32* %6117 to <8 x i16>*
  store <8 x i16> %6115, <8 x i16>* %6118, align 1
  %6119 = getelementptr inbounds i32, i32* %5932, i64 224
  %6120 = bitcast <2 x i64> %5998 to <8 x i16>
  %6121 = ashr <8 x i16> %6120, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %6122 = shufflevector <8 x i16> %6120, <8 x i16> %6121, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %6123 = shufflevector <8 x i16> %6120, <8 x i16> %6121, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %6124 = bitcast i32* %6119 to <8 x i16>*
  store <8 x i16> %6122, <8 x i16>* %6124, align 1
  %6125 = getelementptr inbounds i32, i32* %5932, i64 228
  %6126 = bitcast i32* %6125 to <8 x i16>*
  store <8 x i16> %6123, <8 x i16>* %6126, align 1
  %6127 = getelementptr inbounds i32, i32* %5932, i64 8
  br label %6128

6128:                                             ; preds = %6064, %6000
  %6129 = phi i16* [ %6063, %6000 ], [ %5933, %6064 ]
  %6130 = phi i32* [ %5932, %6000 ], [ %6127, %6064 ]
  %6131 = add nuw nsw i64 %5931, 1
  %6132 = icmp eq i64 %6131, 4
  br i1 %6132, label %6133, label %5930

6133:                                             ; preds = %6128
  call void @llvm.lifetime.end.p0i8(i64 512, i8* nonnull %11) #6
  %6134 = add nuw nsw i64 %100, 8
  %6135 = icmp ult i64 %6134, 32
  br i1 %6135, label %99, label %6137

6136:                                             ; preds = %398, %617, %836, %1055, %1331, %1584, %1947, %2241, %2444, %2688, %2928, %3109, %3362, %3537, %3740, %3943, %4187, %4390, %4587, %4840, %5093, %5337, %5590, %5843, %5773, %5520, %5023, %4770, %4320, %3873, %3670, %3292, %3039, %2374, %1514, %1261, %2171, %2618, %2858, %3467, %4117, %4517, %5267
  call void @llvm.lifetime.end.p0i8(i64 512, i8* nonnull %11) #6
  br label %6140

6137:                                             ; preds = %6133
  %6138 = add nuw nsw i32 %96, 1
  %6139 = icmp eq i32 %6138, 2
  br i1 %6139, label %6140, label %95

6140:                                             ; preds = %6137, %6136
  call void @llvm.lifetime.end.p0i8(i64 2048, i8* nonnull %10) #6
  ret void
}

declare void @vpx_highbd_fdct32x32_rd_c(i16*, i32*, i32) local_unnamed_addr #3

; Function Attrs: nounwind ssp uwtable
define hidden void @vpx_highbd_fdct32x32_sse2(i16*, i32*, i32) local_unnamed_addr #2 {
  %4 = alloca [32 x i64], align 16
  %5 = alloca [32 x i64], align 16
  %6 = alloca [1024 x i16], align 16
  %7 = alloca [32 x <2 x i64>], align 16
  %8 = shl nsw i32 %2, 1
  %9 = mul nsw i32 %2, 3
  %10 = bitcast [1024 x i16]* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 2048, i8* nonnull %10) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %10, i8 -86, i64 2048, i1 false)
  %11 = bitcast [32 x <2 x i64>]* %7 to i8*
  %12 = mul nsw i32 %2, 31
  %13 = sext i32 %12 to i64
  %14 = sext i32 %2 to i64
  %15 = sext i32 %8 to i64
  %16 = sext i32 %9 to i64
  %17 = sub nsw i64 0, %16
  %18 = sub nsw i64 0, %15
  %19 = sub nsw i64 0, %14
  %20 = shl nsw i32 %2, 2
  %21 = sext i32 %20 to i64
  %22 = mul nsw i32 %2, 27
  %23 = sext i32 %22 to i64
  %24 = shl nsw i32 %2, 3
  %25 = sext i32 %24 to i64
  %26 = mul nsw i32 %2, 23
  %27 = sext i32 %26 to i64
  %28 = mul nsw i32 %2, 12
  %29 = sext i32 %28 to i64
  %30 = mul nsw i32 %2, 19
  %31 = sext i32 %30 to i64
  %32 = bitcast [32 x <2 x i64>]* %7 to <8 x i16>*
  %33 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 16
  %34 = bitcast <2 x i64>* %33 to <8 x i16>*
  %35 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 8
  %36 = bitcast <2 x i64>* %35 to <8 x i16>*
  %37 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 24
  %38 = bitcast <2 x i64>* %37 to <8 x i16>*
  %39 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 4
  %40 = bitcast <2 x i64>* %39 to <8 x i16>*
  %41 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 20
  %42 = bitcast <2 x i64>* %41 to <8 x i16>*
  %43 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 12
  %44 = bitcast <2 x i64>* %43 to <8 x i16>*
  %45 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 28
  %46 = bitcast <2 x i64>* %45 to <8 x i16>*
  %47 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 2
  %48 = bitcast <2 x i64>* %47 to <8 x i16>*
  %49 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 18
  %50 = bitcast <2 x i64>* %49 to <8 x i16>*
  %51 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 10
  %52 = bitcast <2 x i64>* %51 to <8 x i16>*
  %53 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 26
  %54 = bitcast <2 x i64>* %53 to <8 x i16>*
  %55 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 6
  %56 = bitcast <2 x i64>* %55 to <8 x i16>*
  %57 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 22
  %58 = bitcast <2 x i64>* %57 to <8 x i16>*
  %59 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 14
  %60 = bitcast <2 x i64>* %59 to <8 x i16>*
  %61 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 30
  %62 = bitcast <2 x i64>* %61 to <8 x i16>*
  %63 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 1
  %64 = bitcast <2 x i64>* %63 to <8 x i16>*
  %65 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 17
  %66 = bitcast <2 x i64>* %65 to <8 x i16>*
  %67 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 9
  %68 = bitcast <2 x i64>* %67 to <8 x i16>*
  %69 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 25
  %70 = bitcast <2 x i64>* %69 to <8 x i16>*
  %71 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 7
  %72 = bitcast <2 x i64>* %71 to <8 x i16>*
  %73 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 23
  %74 = bitcast <2 x i64>* %73 to <8 x i16>*
  %75 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 15
  %76 = bitcast <2 x i64>* %75 to <8 x i16>*
  %77 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 31
  %78 = bitcast <2 x i64>* %77 to <8 x i16>*
  %79 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 5
  %80 = bitcast <2 x i64>* %79 to <8 x i16>*
  %81 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 21
  %82 = bitcast <2 x i64>* %81 to <8 x i16>*
  %83 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 13
  %84 = bitcast <2 x i64>* %83 to <8 x i16>*
  %85 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 29
  %86 = bitcast <2 x i64>* %85 to <8 x i16>*
  %87 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 3
  %88 = bitcast <2 x i64>* %87 to <8 x i16>*
  %89 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 19
  %90 = bitcast <2 x i64>* %89 to <8 x i16>*
  %91 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 11
  %92 = bitcast <2 x i64>* %91 to <8 x i16>*
  %93 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 27
  %94 = bitcast <2 x i64>* %93 to <8 x i16>*
  br label %95

95:                                               ; preds = %11293, %3
  %96 = phi i32 [ 0, %3 ], [ %11294, %11293 ]
  %97 = icmp eq i32 %96, 0
  br label %98

98:                                               ; preds = %95, %11289
  %99 = phi i64 [ 0, %95 ], [ %11290, %11289 ]
  call void @llvm.lifetime.start.p0i8(i64 512, i8* nonnull %11) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %11, i8 -86, i64 512, i1 false)
  br i1 %97, label %100, label %261

100:                                              ; preds = %98
  %101 = getelementptr inbounds i16, i16* %0, i64 %99
  %102 = getelementptr inbounds i16, i16* %101, i64 %13
  %103 = bitcast i16* %101 to <8 x i16>*
  %104 = load <8 x i16>, <8 x i16>* %103, align 1
  %105 = getelementptr inbounds i16, i16* %101, i64 %14
  %106 = bitcast i16* %105 to <8 x i16>*
  %107 = load <8 x i16>, <8 x i16>* %106, align 1
  %108 = getelementptr inbounds i16, i16* %101, i64 %15
  %109 = bitcast i16* %108 to <8 x i16>*
  %110 = load <8 x i16>, <8 x i16>* %109, align 1
  %111 = getelementptr inbounds i16, i16* %101, i64 %16
  %112 = bitcast i16* %111 to <8 x i16>*
  %113 = load <8 x i16>, <8 x i16>* %112, align 1
  %114 = getelementptr inbounds i16, i16* %102, i64 %17
  %115 = bitcast i16* %114 to <8 x i16>*
  %116 = load <8 x i16>, <8 x i16>* %115, align 1
  %117 = getelementptr inbounds i16, i16* %102, i64 %18
  %118 = bitcast i16* %117 to <8 x i16>*
  %119 = load <8 x i16>, <8 x i16>* %118, align 1
  %120 = getelementptr inbounds i16, i16* %102, i64 %19
  %121 = bitcast i16* %120 to <8 x i16>*
  %122 = load <8 x i16>, <8 x i16>* %121, align 1
  %123 = bitcast i16* %102 to <8 x i16>*
  %124 = load <8 x i16>, <8 x i16>* %123, align 1
  %125 = add <8 x i16> %124, %104
  %126 = add <8 x i16> %122, %107
  %127 = add <8 x i16> %119, %110
  %128 = add <8 x i16> %116, %113
  %129 = sub <8 x i16> %113, %116
  %130 = sub <8 x i16> %110, %119
  %131 = sub <8 x i16> %107, %122
  %132 = sub <8 x i16> %104, %124
  %133 = shl <8 x i16> %125, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %134 = shl <8 x i16> %126, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %135 = shl <8 x i16> %127, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %136 = shl <8 x i16> %128, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %137 = shl <8 x i16> %129, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %138 = shl <8 x i16> %130, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %139 = shl <8 x i16> %131, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %140 = shl <8 x i16> %132, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %141 = getelementptr inbounds i16, i16* %101, i64 %21
  %142 = getelementptr inbounds i16, i16* %101, i64 %23
  %143 = bitcast i16* %141 to <8 x i16>*
  %144 = load <8 x i16>, <8 x i16>* %143, align 1
  %145 = getelementptr inbounds i16, i16* %141, i64 %14
  %146 = bitcast i16* %145 to <8 x i16>*
  %147 = load <8 x i16>, <8 x i16>* %146, align 1
  %148 = getelementptr inbounds i16, i16* %141, i64 %15
  %149 = bitcast i16* %148 to <8 x i16>*
  %150 = load <8 x i16>, <8 x i16>* %149, align 1
  %151 = getelementptr inbounds i16, i16* %141, i64 %16
  %152 = bitcast i16* %151 to <8 x i16>*
  %153 = load <8 x i16>, <8 x i16>* %152, align 1
  %154 = getelementptr inbounds i16, i16* %142, i64 %17
  %155 = bitcast i16* %154 to <8 x i16>*
  %156 = load <8 x i16>, <8 x i16>* %155, align 1
  %157 = getelementptr inbounds i16, i16* %142, i64 %18
  %158 = bitcast i16* %157 to <8 x i16>*
  %159 = load <8 x i16>, <8 x i16>* %158, align 1
  %160 = getelementptr inbounds i16, i16* %142, i64 %19
  %161 = bitcast i16* %160 to <8 x i16>*
  %162 = load <8 x i16>, <8 x i16>* %161, align 1
  %163 = bitcast i16* %142 to <8 x i16>*
  %164 = load <8 x i16>, <8 x i16>* %163, align 1
  %165 = add <8 x i16> %164, %144
  %166 = add <8 x i16> %162, %147
  %167 = add <8 x i16> %159, %150
  %168 = add <8 x i16> %156, %153
  %169 = sub <8 x i16> %153, %156
  %170 = sub <8 x i16> %150, %159
  %171 = sub <8 x i16> %147, %162
  %172 = sub <8 x i16> %144, %164
  %173 = shl <8 x i16> %165, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %174 = shl <8 x i16> %166, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %175 = shl <8 x i16> %167, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %176 = shl <8 x i16> %168, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %177 = shl <8 x i16> %169, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %178 = shl <8 x i16> %170, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %179 = shl <8 x i16> %171, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %180 = shl <8 x i16> %172, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %181 = getelementptr inbounds i16, i16* %101, i64 %25
  %182 = getelementptr inbounds i16, i16* %101, i64 %27
  %183 = bitcast i16* %181 to <8 x i16>*
  %184 = load <8 x i16>, <8 x i16>* %183, align 1
  %185 = getelementptr inbounds i16, i16* %181, i64 %14
  %186 = bitcast i16* %185 to <8 x i16>*
  %187 = load <8 x i16>, <8 x i16>* %186, align 1
  %188 = getelementptr inbounds i16, i16* %181, i64 %15
  %189 = bitcast i16* %188 to <8 x i16>*
  %190 = load <8 x i16>, <8 x i16>* %189, align 1
  %191 = getelementptr inbounds i16, i16* %181, i64 %16
  %192 = bitcast i16* %191 to <8 x i16>*
  %193 = load <8 x i16>, <8 x i16>* %192, align 1
  %194 = getelementptr inbounds i16, i16* %182, i64 %17
  %195 = bitcast i16* %194 to <8 x i16>*
  %196 = load <8 x i16>, <8 x i16>* %195, align 1
  %197 = getelementptr inbounds i16, i16* %182, i64 %18
  %198 = bitcast i16* %197 to <8 x i16>*
  %199 = load <8 x i16>, <8 x i16>* %198, align 1
  %200 = getelementptr inbounds i16, i16* %182, i64 %19
  %201 = bitcast i16* %200 to <8 x i16>*
  %202 = load <8 x i16>, <8 x i16>* %201, align 1
  %203 = bitcast i16* %182 to <8 x i16>*
  %204 = load <8 x i16>, <8 x i16>* %203, align 1
  %205 = add <8 x i16> %204, %184
  %206 = add <8 x i16> %202, %187
  %207 = add <8 x i16> %199, %190
  %208 = add <8 x i16> %196, %193
  %209 = sub <8 x i16> %193, %196
  %210 = sub <8 x i16> %190, %199
  %211 = sub <8 x i16> %187, %202
  %212 = sub <8 x i16> %184, %204
  %213 = shl <8 x i16> %205, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %214 = shl <8 x i16> %206, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %215 = shl <8 x i16> %207, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %216 = shl <8 x i16> %208, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %217 = shl <8 x i16> %209, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %218 = shl <8 x i16> %210, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %219 = shl <8 x i16> %211, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %220 = shl <8 x i16> %212, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %221 = getelementptr inbounds i16, i16* %101, i64 %29
  %222 = getelementptr inbounds i16, i16* %101, i64 %31
  %223 = bitcast i16* %221 to <8 x i16>*
  %224 = load <8 x i16>, <8 x i16>* %223, align 1
  %225 = getelementptr inbounds i16, i16* %221, i64 %14
  %226 = bitcast i16* %225 to <8 x i16>*
  %227 = load <8 x i16>, <8 x i16>* %226, align 1
  %228 = getelementptr inbounds i16, i16* %221, i64 %15
  %229 = bitcast i16* %228 to <8 x i16>*
  %230 = load <8 x i16>, <8 x i16>* %229, align 1
  %231 = getelementptr inbounds i16, i16* %221, i64 %16
  %232 = bitcast i16* %231 to <8 x i16>*
  %233 = load <8 x i16>, <8 x i16>* %232, align 1
  %234 = getelementptr inbounds i16, i16* %222, i64 %17
  %235 = bitcast i16* %234 to <8 x i16>*
  %236 = load <8 x i16>, <8 x i16>* %235, align 1
  %237 = getelementptr inbounds i16, i16* %222, i64 %18
  %238 = bitcast i16* %237 to <8 x i16>*
  %239 = load <8 x i16>, <8 x i16>* %238, align 1
  %240 = getelementptr inbounds i16, i16* %222, i64 %19
  %241 = bitcast i16* %240 to <8 x i16>*
  %242 = load <8 x i16>, <8 x i16>* %241, align 1
  %243 = bitcast i16* %222 to <8 x i16>*
  %244 = load <8 x i16>, <8 x i16>* %243, align 1
  %245 = add <8 x i16> %244, %224
  %246 = add <8 x i16> %242, %227
  %247 = add <8 x i16> %239, %230
  %248 = add <8 x i16> %236, %233
  %249 = sub <8 x i16> %233, %236
  %250 = sub <8 x i16> %230, %239
  %251 = sub <8 x i16> %227, %242
  %252 = sub <8 x i16> %224, %244
  %253 = shl <8 x i16> %245, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %254 = shl <8 x i16> %246, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %255 = shl <8 x i16> %247, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %256 = shl <8 x i16> %248, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %257 = shl <8 x i16> %249, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %258 = shl <8 x i16> %250, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %259 = shl <8 x i16> %251, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %260 = shl <8 x i16> %252, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  br label %1393

261:                                              ; preds = %98
  %262 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %99
  %263 = bitcast i16* %262 to <8 x i16>*
  %264 = load <8 x i16>, <8 x i16>* %263, align 16
  %265 = getelementptr inbounds i16, i16* %262, i64 32
  %266 = bitcast i16* %265 to <8 x i16>*
  %267 = load <8 x i16>, <8 x i16>* %266, align 16
  %268 = getelementptr inbounds i16, i16* %262, i64 64
  %269 = bitcast i16* %268 to <8 x i16>*
  %270 = load <8 x i16>, <8 x i16>* %269, align 16
  %271 = getelementptr inbounds i16, i16* %262, i64 96
  %272 = bitcast i16* %271 to <8 x i16>*
  %273 = load <8 x i16>, <8 x i16>* %272, align 16
  %274 = getelementptr inbounds i16, i16* %262, i64 896
  %275 = bitcast i16* %274 to <8 x i16>*
  %276 = load <8 x i16>, <8 x i16>* %275, align 16
  %277 = getelementptr inbounds i16, i16* %262, i64 928
  %278 = bitcast i16* %277 to <8 x i16>*
  %279 = load <8 x i16>, <8 x i16>* %278, align 16
  %280 = getelementptr inbounds i16, i16* %262, i64 960
  %281 = bitcast i16* %280 to <8 x i16>*
  %282 = load <8 x i16>, <8 x i16>* %281, align 16
  %283 = getelementptr inbounds i16, i16* %262, i64 992
  %284 = bitcast i16* %283 to <8 x i16>*
  %285 = load <8 x i16>, <8 x i16>* %284, align 16
  %286 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %264, <8 x i16> %285) #6
  %287 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %267, <8 x i16> %282) #6
  %288 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %270, <8 x i16> %279) #6
  %289 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %273, <8 x i16> %276) #6
  %290 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %273, <8 x i16> %276) #6
  %291 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %270, <8 x i16> %279) #6
  %292 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %267, <8 x i16> %282) #6
  %293 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %264, <8 x i16> %285) #6
  %294 = add <8 x i16> %286, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %295 = icmp ult <8 x i16> %294, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %296 = add <8 x i16> %287, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %297 = icmp ult <8 x i16> %296, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %298 = add <8 x i16> %288, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %299 = icmp ult <8 x i16> %298, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %300 = add <8 x i16> %289, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %301 = icmp ult <8 x i16> %300, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %302 = or <8 x i1> %299, %301
  %303 = or <8 x i1> %302, %297
  %304 = or <8 x i1> %303, %295
  %305 = sext <8 x i1> %304 to <8 x i16>
  %306 = bitcast <8 x i16> %305 to <16 x i8>
  %307 = icmp slt <16 x i8> %306, zeroinitializer
  %308 = bitcast <16 x i1> %307 to i16
  %309 = zext i16 %308 to i32
  %310 = add <8 x i16> %290, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %311 = icmp ult <8 x i16> %310, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %312 = add <8 x i16> %291, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %313 = icmp ult <8 x i16> %312, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %314 = add <8 x i16> %292, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %315 = icmp ult <8 x i16> %314, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %316 = add <8 x i16> %293, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %317 = icmp ult <8 x i16> %316, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %318 = or <8 x i1> %313, %311
  %319 = or <8 x i1> %318, %315
  %320 = or <8 x i1> %319, %317
  %321 = sext <8 x i1> %320 to <8 x i16>
  %322 = bitcast <8 x i16> %321 to <16 x i8>
  %323 = icmp slt <16 x i8> %322, zeroinitializer
  %324 = bitcast <16 x i1> %323 to i16
  %325 = zext i16 %324 to i32
  %326 = sub nsw i32 0, %309
  %327 = icmp eq i32 %325, %326
  br i1 %327, label %544, label %328

328:                                              ; preds = %261
  %329 = bitcast [32 x i64]* %4 to i8*
  %330 = bitcast [32 x i64]* %5 to i8*
  %331 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %332 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %333 = bitcast [32 x i64]* %5 to <2 x i64>*
  %334 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %335 = bitcast i64* %334 to <2 x i64>*
  %336 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %337 = bitcast i64* %336 to <2 x i64>*
  %338 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %339 = bitcast i64* %338 to <2 x i64>*
  %340 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %341 = bitcast i64* %340 to <2 x i64>*
  %342 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %343 = bitcast i64* %342 to <2 x i64>*
  %344 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %345 = bitcast i64* %344 to <2 x i64>*
  %346 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %347 = bitcast i64* %346 to <2 x i64>*
  %348 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %349 = bitcast i64* %348 to <2 x i64>*
  %350 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %351 = bitcast i64* %350 to <2 x i64>*
  %352 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %353 = bitcast i64* %352 to <2 x i64>*
  %354 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %355 = bitcast i64* %354 to <2 x i64>*
  %356 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %357 = bitcast i64* %356 to <2 x i64>*
  %358 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %359 = bitcast i64* %358 to <2 x i64>*
  %360 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %361 = bitcast i64* %360 to <2 x i64>*
  %362 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %363 = bitcast i64* %362 to <2 x i64>*
  br label %364

364:                                              ; preds = %397, %328
  %365 = phi i64 [ 0, %328 ], [ %542, %397 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %329) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %329, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %330) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %330, i8 -86, i64 256, i1 false) #6
  br label %366

366:                                              ; preds = %366, %364
  %367 = phi i64 [ 0, %364 ], [ %395, %366 ]
  %368 = shl i64 %367, 5
  %369 = add nuw nsw i64 %368, %365
  %370 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %369
  %371 = load i16, i16* %370, align 2
  %372 = sext i16 %371 to i64
  %373 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %367
  store i64 %372, i64* %373, align 16
  %374 = or i64 %367, 1
  %375 = shl i64 %374, 5
  %376 = add nuw nsw i64 %375, %365
  %377 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %376
  %378 = load i16, i16* %377, align 2
  %379 = sext i16 %378 to i64
  %380 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %374
  store i64 %379, i64* %380, align 8
  %381 = or i64 %367, 2
  %382 = shl i64 %381, 5
  %383 = add nuw nsw i64 %382, %365
  %384 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %383
  %385 = load i16, i16* %384, align 2
  %386 = sext i16 %385 to i64
  %387 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %381
  store i64 %386, i64* %387, align 16
  %388 = or i64 %367, 3
  %389 = shl i64 %388, 5
  %390 = add nuw nsw i64 %389, %365
  %391 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %390
  %392 = load i16, i16* %391, align 2
  %393 = sext i16 %392 to i64
  %394 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %388
  store i64 %393, i64* %394, align 8
  %395 = add nuw nsw i64 %367, 4
  %396 = icmp eq i64 %395, 32
  br i1 %396, label %397, label %366

397:                                              ; preds = %366
  call void @vpx_fdct32(i64* nonnull %331, i64* nonnull %332, i32 0) #6
  %398 = shl i64 %365, 5
  %399 = load <2 x i64>, <2 x i64>* %333, align 16
  %400 = add nsw <2 x i64> %399, <i64 1, i64 1>
  %401 = lshr <2 x i64> %399, <i64 63, i64 63>
  %402 = add nsw <2 x i64> %400, %401
  %403 = lshr <2 x i64> %402, <i64 2, i64 2>
  %404 = trunc <2 x i64> %403 to <2 x i32>
  %405 = getelementptr inbounds i32, i32* %1, i64 %398
  %406 = bitcast i32* %405 to <2 x i32>*
  store <2 x i32> %404, <2 x i32>* %406, align 4
  %407 = load <2 x i64>, <2 x i64>* %335, align 16
  %408 = add nsw <2 x i64> %407, <i64 1, i64 1>
  %409 = lshr <2 x i64> %407, <i64 63, i64 63>
  %410 = add nsw <2 x i64> %408, %409
  %411 = lshr <2 x i64> %410, <i64 2, i64 2>
  %412 = trunc <2 x i64> %411 to <2 x i32>
  %413 = or i64 %398, 2
  %414 = getelementptr inbounds i32, i32* %1, i64 %413
  %415 = bitcast i32* %414 to <2 x i32>*
  store <2 x i32> %412, <2 x i32>* %415, align 4
  %416 = load <2 x i64>, <2 x i64>* %337, align 16
  %417 = add nsw <2 x i64> %416, <i64 1, i64 1>
  %418 = lshr <2 x i64> %416, <i64 63, i64 63>
  %419 = add nsw <2 x i64> %417, %418
  %420 = lshr <2 x i64> %419, <i64 2, i64 2>
  %421 = trunc <2 x i64> %420 to <2 x i32>
  %422 = or i64 %398, 4
  %423 = getelementptr inbounds i32, i32* %1, i64 %422
  %424 = bitcast i32* %423 to <2 x i32>*
  store <2 x i32> %421, <2 x i32>* %424, align 4
  %425 = load <2 x i64>, <2 x i64>* %339, align 16
  %426 = add nsw <2 x i64> %425, <i64 1, i64 1>
  %427 = lshr <2 x i64> %425, <i64 63, i64 63>
  %428 = add nsw <2 x i64> %426, %427
  %429 = lshr <2 x i64> %428, <i64 2, i64 2>
  %430 = trunc <2 x i64> %429 to <2 x i32>
  %431 = or i64 %398, 6
  %432 = getelementptr inbounds i32, i32* %1, i64 %431
  %433 = bitcast i32* %432 to <2 x i32>*
  store <2 x i32> %430, <2 x i32>* %433, align 4
  %434 = load <2 x i64>, <2 x i64>* %341, align 16
  %435 = add nsw <2 x i64> %434, <i64 1, i64 1>
  %436 = lshr <2 x i64> %434, <i64 63, i64 63>
  %437 = add nsw <2 x i64> %435, %436
  %438 = lshr <2 x i64> %437, <i64 2, i64 2>
  %439 = trunc <2 x i64> %438 to <2 x i32>
  %440 = or i64 %398, 8
  %441 = getelementptr inbounds i32, i32* %1, i64 %440
  %442 = bitcast i32* %441 to <2 x i32>*
  store <2 x i32> %439, <2 x i32>* %442, align 4
  %443 = load <2 x i64>, <2 x i64>* %343, align 16
  %444 = add nsw <2 x i64> %443, <i64 1, i64 1>
  %445 = lshr <2 x i64> %443, <i64 63, i64 63>
  %446 = add nsw <2 x i64> %444, %445
  %447 = lshr <2 x i64> %446, <i64 2, i64 2>
  %448 = trunc <2 x i64> %447 to <2 x i32>
  %449 = or i64 %398, 10
  %450 = getelementptr inbounds i32, i32* %1, i64 %449
  %451 = bitcast i32* %450 to <2 x i32>*
  store <2 x i32> %448, <2 x i32>* %451, align 4
  %452 = load <2 x i64>, <2 x i64>* %345, align 16
  %453 = add nsw <2 x i64> %452, <i64 1, i64 1>
  %454 = lshr <2 x i64> %452, <i64 63, i64 63>
  %455 = add nsw <2 x i64> %453, %454
  %456 = lshr <2 x i64> %455, <i64 2, i64 2>
  %457 = trunc <2 x i64> %456 to <2 x i32>
  %458 = or i64 %398, 12
  %459 = getelementptr inbounds i32, i32* %1, i64 %458
  %460 = bitcast i32* %459 to <2 x i32>*
  store <2 x i32> %457, <2 x i32>* %460, align 4
  %461 = load <2 x i64>, <2 x i64>* %347, align 16
  %462 = add nsw <2 x i64> %461, <i64 1, i64 1>
  %463 = lshr <2 x i64> %461, <i64 63, i64 63>
  %464 = add nsw <2 x i64> %462, %463
  %465 = lshr <2 x i64> %464, <i64 2, i64 2>
  %466 = trunc <2 x i64> %465 to <2 x i32>
  %467 = or i64 %398, 14
  %468 = getelementptr inbounds i32, i32* %1, i64 %467
  %469 = bitcast i32* %468 to <2 x i32>*
  store <2 x i32> %466, <2 x i32>* %469, align 4
  %470 = load <2 x i64>, <2 x i64>* %349, align 16
  %471 = add nsw <2 x i64> %470, <i64 1, i64 1>
  %472 = lshr <2 x i64> %470, <i64 63, i64 63>
  %473 = add nsw <2 x i64> %471, %472
  %474 = lshr <2 x i64> %473, <i64 2, i64 2>
  %475 = trunc <2 x i64> %474 to <2 x i32>
  %476 = or i64 %398, 16
  %477 = getelementptr inbounds i32, i32* %1, i64 %476
  %478 = bitcast i32* %477 to <2 x i32>*
  store <2 x i32> %475, <2 x i32>* %478, align 4
  %479 = load <2 x i64>, <2 x i64>* %351, align 16
  %480 = add nsw <2 x i64> %479, <i64 1, i64 1>
  %481 = lshr <2 x i64> %479, <i64 63, i64 63>
  %482 = add nsw <2 x i64> %480, %481
  %483 = lshr <2 x i64> %482, <i64 2, i64 2>
  %484 = trunc <2 x i64> %483 to <2 x i32>
  %485 = or i64 %398, 18
  %486 = getelementptr inbounds i32, i32* %1, i64 %485
  %487 = bitcast i32* %486 to <2 x i32>*
  store <2 x i32> %484, <2 x i32>* %487, align 4
  %488 = load <2 x i64>, <2 x i64>* %353, align 16
  %489 = add nsw <2 x i64> %488, <i64 1, i64 1>
  %490 = lshr <2 x i64> %488, <i64 63, i64 63>
  %491 = add nsw <2 x i64> %489, %490
  %492 = lshr <2 x i64> %491, <i64 2, i64 2>
  %493 = trunc <2 x i64> %492 to <2 x i32>
  %494 = or i64 %398, 20
  %495 = getelementptr inbounds i32, i32* %1, i64 %494
  %496 = bitcast i32* %495 to <2 x i32>*
  store <2 x i32> %493, <2 x i32>* %496, align 4
  %497 = load <2 x i64>, <2 x i64>* %355, align 16
  %498 = add nsw <2 x i64> %497, <i64 1, i64 1>
  %499 = lshr <2 x i64> %497, <i64 63, i64 63>
  %500 = add nsw <2 x i64> %498, %499
  %501 = lshr <2 x i64> %500, <i64 2, i64 2>
  %502 = trunc <2 x i64> %501 to <2 x i32>
  %503 = or i64 %398, 22
  %504 = getelementptr inbounds i32, i32* %1, i64 %503
  %505 = bitcast i32* %504 to <2 x i32>*
  store <2 x i32> %502, <2 x i32>* %505, align 4
  %506 = load <2 x i64>, <2 x i64>* %357, align 16
  %507 = add nsw <2 x i64> %506, <i64 1, i64 1>
  %508 = lshr <2 x i64> %506, <i64 63, i64 63>
  %509 = add nsw <2 x i64> %507, %508
  %510 = lshr <2 x i64> %509, <i64 2, i64 2>
  %511 = trunc <2 x i64> %510 to <2 x i32>
  %512 = or i64 %398, 24
  %513 = getelementptr inbounds i32, i32* %1, i64 %512
  %514 = bitcast i32* %513 to <2 x i32>*
  store <2 x i32> %511, <2 x i32>* %514, align 4
  %515 = load <2 x i64>, <2 x i64>* %359, align 16
  %516 = add nsw <2 x i64> %515, <i64 1, i64 1>
  %517 = lshr <2 x i64> %515, <i64 63, i64 63>
  %518 = add nsw <2 x i64> %516, %517
  %519 = lshr <2 x i64> %518, <i64 2, i64 2>
  %520 = trunc <2 x i64> %519 to <2 x i32>
  %521 = or i64 %398, 26
  %522 = getelementptr inbounds i32, i32* %1, i64 %521
  %523 = bitcast i32* %522 to <2 x i32>*
  store <2 x i32> %520, <2 x i32>* %523, align 4
  %524 = load <2 x i64>, <2 x i64>* %361, align 16
  %525 = add nsw <2 x i64> %524, <i64 1, i64 1>
  %526 = lshr <2 x i64> %524, <i64 63, i64 63>
  %527 = add nsw <2 x i64> %525, %526
  %528 = lshr <2 x i64> %527, <i64 2, i64 2>
  %529 = trunc <2 x i64> %528 to <2 x i32>
  %530 = or i64 %398, 28
  %531 = getelementptr inbounds i32, i32* %1, i64 %530
  %532 = bitcast i32* %531 to <2 x i32>*
  store <2 x i32> %529, <2 x i32>* %532, align 4
  %533 = load <2 x i64>, <2 x i64>* %363, align 16
  %534 = add nsw <2 x i64> %533, <i64 1, i64 1>
  %535 = lshr <2 x i64> %533, <i64 63, i64 63>
  %536 = add nsw <2 x i64> %534, %535
  %537 = lshr <2 x i64> %536, <i64 2, i64 2>
  %538 = trunc <2 x i64> %537 to <2 x i32>
  %539 = or i64 %398, 30
  %540 = getelementptr inbounds i32, i32* %1, i64 %539
  %541 = bitcast i32* %540 to <2 x i32>*
  store <2 x i32> %538, <2 x i32>* %541, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %330) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %329) #6
  %542 = add nuw nsw i64 %365, 1
  %543 = icmp eq i64 %542, 32
  br i1 %543, label %11292, label %364

544:                                              ; preds = %261
  %545 = getelementptr inbounds i16, i16* %262, i64 128
  %546 = bitcast i16* %545 to <8 x i16>*
  %547 = load <8 x i16>, <8 x i16>* %546, align 16
  %548 = getelementptr inbounds i16, i16* %262, i64 160
  %549 = bitcast i16* %548 to <8 x i16>*
  %550 = load <8 x i16>, <8 x i16>* %549, align 16
  %551 = getelementptr inbounds i16, i16* %262, i64 192
  %552 = bitcast i16* %551 to <8 x i16>*
  %553 = load <8 x i16>, <8 x i16>* %552, align 16
  %554 = getelementptr inbounds i16, i16* %262, i64 224
  %555 = bitcast i16* %554 to <8 x i16>*
  %556 = load <8 x i16>, <8 x i16>* %555, align 16
  %557 = getelementptr inbounds i16, i16* %262, i64 768
  %558 = bitcast i16* %557 to <8 x i16>*
  %559 = load <8 x i16>, <8 x i16>* %558, align 16
  %560 = getelementptr inbounds i16, i16* %262, i64 800
  %561 = bitcast i16* %560 to <8 x i16>*
  %562 = load <8 x i16>, <8 x i16>* %561, align 16
  %563 = getelementptr inbounds i16, i16* %262, i64 832
  %564 = bitcast i16* %563 to <8 x i16>*
  %565 = load <8 x i16>, <8 x i16>* %564, align 16
  %566 = getelementptr inbounds i16, i16* %262, i64 864
  %567 = bitcast i16* %566 to <8 x i16>*
  %568 = load <8 x i16>, <8 x i16>* %567, align 16
  %569 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %547, <8 x i16> %568) #6
  %570 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %550, <8 x i16> %565) #6
  %571 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %553, <8 x i16> %562) #6
  %572 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %556, <8 x i16> %559) #6
  %573 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %556, <8 x i16> %559) #6
  %574 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %553, <8 x i16> %562) #6
  %575 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %550, <8 x i16> %565) #6
  %576 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %547, <8 x i16> %568) #6
  %577 = add <8 x i16> %569, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %578 = icmp ult <8 x i16> %577, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %579 = add <8 x i16> %570, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %580 = icmp ult <8 x i16> %579, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %581 = add <8 x i16> %571, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %582 = icmp ult <8 x i16> %581, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %583 = add <8 x i16> %572, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %584 = icmp ult <8 x i16> %583, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %585 = or <8 x i1> %582, %584
  %586 = or <8 x i1> %585, %580
  %587 = or <8 x i1> %586, %578
  %588 = sext <8 x i1> %587 to <8 x i16>
  %589 = bitcast <8 x i16> %588 to <16 x i8>
  %590 = icmp slt <16 x i8> %589, zeroinitializer
  %591 = bitcast <16 x i1> %590 to i16
  %592 = zext i16 %591 to i32
  %593 = add <8 x i16> %573, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %594 = icmp ult <8 x i16> %593, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %595 = add <8 x i16> %574, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %596 = icmp ult <8 x i16> %595, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %597 = add <8 x i16> %575, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %598 = icmp ult <8 x i16> %597, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %599 = add <8 x i16> %576, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %600 = icmp ult <8 x i16> %599, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %601 = or <8 x i1> %596, %594
  %602 = or <8 x i1> %601, %598
  %603 = or <8 x i1> %602, %600
  %604 = sext <8 x i1> %603 to <8 x i16>
  %605 = bitcast <8 x i16> %604 to <16 x i8>
  %606 = icmp slt <16 x i8> %605, zeroinitializer
  %607 = bitcast <16 x i1> %606 to i16
  %608 = zext i16 %607 to i32
  %609 = sub nsw i32 0, %592
  %610 = icmp eq i32 %608, %609
  br i1 %610, label %827, label %611

611:                                              ; preds = %544
  %612 = bitcast [32 x i64]* %4 to i8*
  %613 = bitcast [32 x i64]* %5 to i8*
  %614 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %615 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %616 = bitcast [32 x i64]* %5 to <2 x i64>*
  %617 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %618 = bitcast i64* %617 to <2 x i64>*
  %619 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %620 = bitcast i64* %619 to <2 x i64>*
  %621 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %622 = bitcast i64* %621 to <2 x i64>*
  %623 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %624 = bitcast i64* %623 to <2 x i64>*
  %625 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %626 = bitcast i64* %625 to <2 x i64>*
  %627 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %628 = bitcast i64* %627 to <2 x i64>*
  %629 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %630 = bitcast i64* %629 to <2 x i64>*
  %631 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %632 = bitcast i64* %631 to <2 x i64>*
  %633 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %634 = bitcast i64* %633 to <2 x i64>*
  %635 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %636 = bitcast i64* %635 to <2 x i64>*
  %637 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %638 = bitcast i64* %637 to <2 x i64>*
  %639 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %640 = bitcast i64* %639 to <2 x i64>*
  %641 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %642 = bitcast i64* %641 to <2 x i64>*
  %643 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %644 = bitcast i64* %643 to <2 x i64>*
  %645 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %646 = bitcast i64* %645 to <2 x i64>*
  br label %647

647:                                              ; preds = %680, %611
  %648 = phi i64 [ 0, %611 ], [ %825, %680 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %612) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %612, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %613) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %613, i8 -86, i64 256, i1 false) #6
  br label %649

649:                                              ; preds = %649, %647
  %650 = phi i64 [ 0, %647 ], [ %678, %649 ]
  %651 = shl i64 %650, 5
  %652 = add nuw nsw i64 %651, %648
  %653 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %652
  %654 = load i16, i16* %653, align 2
  %655 = sext i16 %654 to i64
  %656 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %650
  store i64 %655, i64* %656, align 16
  %657 = or i64 %650, 1
  %658 = shl i64 %657, 5
  %659 = add nuw nsw i64 %658, %648
  %660 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %659
  %661 = load i16, i16* %660, align 2
  %662 = sext i16 %661 to i64
  %663 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %657
  store i64 %662, i64* %663, align 8
  %664 = or i64 %650, 2
  %665 = shl i64 %664, 5
  %666 = add nuw nsw i64 %665, %648
  %667 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %666
  %668 = load i16, i16* %667, align 2
  %669 = sext i16 %668 to i64
  %670 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %664
  store i64 %669, i64* %670, align 16
  %671 = or i64 %650, 3
  %672 = shl i64 %671, 5
  %673 = add nuw nsw i64 %672, %648
  %674 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %673
  %675 = load i16, i16* %674, align 2
  %676 = sext i16 %675 to i64
  %677 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %671
  store i64 %676, i64* %677, align 8
  %678 = add nuw nsw i64 %650, 4
  %679 = icmp eq i64 %678, 32
  br i1 %679, label %680, label %649

680:                                              ; preds = %649
  call void @vpx_fdct32(i64* nonnull %614, i64* nonnull %615, i32 0) #6
  %681 = shl i64 %648, 5
  %682 = load <2 x i64>, <2 x i64>* %616, align 16
  %683 = add nsw <2 x i64> %682, <i64 1, i64 1>
  %684 = lshr <2 x i64> %682, <i64 63, i64 63>
  %685 = add nsw <2 x i64> %683, %684
  %686 = lshr <2 x i64> %685, <i64 2, i64 2>
  %687 = trunc <2 x i64> %686 to <2 x i32>
  %688 = getelementptr inbounds i32, i32* %1, i64 %681
  %689 = bitcast i32* %688 to <2 x i32>*
  store <2 x i32> %687, <2 x i32>* %689, align 4
  %690 = load <2 x i64>, <2 x i64>* %618, align 16
  %691 = add nsw <2 x i64> %690, <i64 1, i64 1>
  %692 = lshr <2 x i64> %690, <i64 63, i64 63>
  %693 = add nsw <2 x i64> %691, %692
  %694 = lshr <2 x i64> %693, <i64 2, i64 2>
  %695 = trunc <2 x i64> %694 to <2 x i32>
  %696 = or i64 %681, 2
  %697 = getelementptr inbounds i32, i32* %1, i64 %696
  %698 = bitcast i32* %697 to <2 x i32>*
  store <2 x i32> %695, <2 x i32>* %698, align 4
  %699 = load <2 x i64>, <2 x i64>* %620, align 16
  %700 = add nsw <2 x i64> %699, <i64 1, i64 1>
  %701 = lshr <2 x i64> %699, <i64 63, i64 63>
  %702 = add nsw <2 x i64> %700, %701
  %703 = lshr <2 x i64> %702, <i64 2, i64 2>
  %704 = trunc <2 x i64> %703 to <2 x i32>
  %705 = or i64 %681, 4
  %706 = getelementptr inbounds i32, i32* %1, i64 %705
  %707 = bitcast i32* %706 to <2 x i32>*
  store <2 x i32> %704, <2 x i32>* %707, align 4
  %708 = load <2 x i64>, <2 x i64>* %622, align 16
  %709 = add nsw <2 x i64> %708, <i64 1, i64 1>
  %710 = lshr <2 x i64> %708, <i64 63, i64 63>
  %711 = add nsw <2 x i64> %709, %710
  %712 = lshr <2 x i64> %711, <i64 2, i64 2>
  %713 = trunc <2 x i64> %712 to <2 x i32>
  %714 = or i64 %681, 6
  %715 = getelementptr inbounds i32, i32* %1, i64 %714
  %716 = bitcast i32* %715 to <2 x i32>*
  store <2 x i32> %713, <2 x i32>* %716, align 4
  %717 = load <2 x i64>, <2 x i64>* %624, align 16
  %718 = add nsw <2 x i64> %717, <i64 1, i64 1>
  %719 = lshr <2 x i64> %717, <i64 63, i64 63>
  %720 = add nsw <2 x i64> %718, %719
  %721 = lshr <2 x i64> %720, <i64 2, i64 2>
  %722 = trunc <2 x i64> %721 to <2 x i32>
  %723 = or i64 %681, 8
  %724 = getelementptr inbounds i32, i32* %1, i64 %723
  %725 = bitcast i32* %724 to <2 x i32>*
  store <2 x i32> %722, <2 x i32>* %725, align 4
  %726 = load <2 x i64>, <2 x i64>* %626, align 16
  %727 = add nsw <2 x i64> %726, <i64 1, i64 1>
  %728 = lshr <2 x i64> %726, <i64 63, i64 63>
  %729 = add nsw <2 x i64> %727, %728
  %730 = lshr <2 x i64> %729, <i64 2, i64 2>
  %731 = trunc <2 x i64> %730 to <2 x i32>
  %732 = or i64 %681, 10
  %733 = getelementptr inbounds i32, i32* %1, i64 %732
  %734 = bitcast i32* %733 to <2 x i32>*
  store <2 x i32> %731, <2 x i32>* %734, align 4
  %735 = load <2 x i64>, <2 x i64>* %628, align 16
  %736 = add nsw <2 x i64> %735, <i64 1, i64 1>
  %737 = lshr <2 x i64> %735, <i64 63, i64 63>
  %738 = add nsw <2 x i64> %736, %737
  %739 = lshr <2 x i64> %738, <i64 2, i64 2>
  %740 = trunc <2 x i64> %739 to <2 x i32>
  %741 = or i64 %681, 12
  %742 = getelementptr inbounds i32, i32* %1, i64 %741
  %743 = bitcast i32* %742 to <2 x i32>*
  store <2 x i32> %740, <2 x i32>* %743, align 4
  %744 = load <2 x i64>, <2 x i64>* %630, align 16
  %745 = add nsw <2 x i64> %744, <i64 1, i64 1>
  %746 = lshr <2 x i64> %744, <i64 63, i64 63>
  %747 = add nsw <2 x i64> %745, %746
  %748 = lshr <2 x i64> %747, <i64 2, i64 2>
  %749 = trunc <2 x i64> %748 to <2 x i32>
  %750 = or i64 %681, 14
  %751 = getelementptr inbounds i32, i32* %1, i64 %750
  %752 = bitcast i32* %751 to <2 x i32>*
  store <2 x i32> %749, <2 x i32>* %752, align 4
  %753 = load <2 x i64>, <2 x i64>* %632, align 16
  %754 = add nsw <2 x i64> %753, <i64 1, i64 1>
  %755 = lshr <2 x i64> %753, <i64 63, i64 63>
  %756 = add nsw <2 x i64> %754, %755
  %757 = lshr <2 x i64> %756, <i64 2, i64 2>
  %758 = trunc <2 x i64> %757 to <2 x i32>
  %759 = or i64 %681, 16
  %760 = getelementptr inbounds i32, i32* %1, i64 %759
  %761 = bitcast i32* %760 to <2 x i32>*
  store <2 x i32> %758, <2 x i32>* %761, align 4
  %762 = load <2 x i64>, <2 x i64>* %634, align 16
  %763 = add nsw <2 x i64> %762, <i64 1, i64 1>
  %764 = lshr <2 x i64> %762, <i64 63, i64 63>
  %765 = add nsw <2 x i64> %763, %764
  %766 = lshr <2 x i64> %765, <i64 2, i64 2>
  %767 = trunc <2 x i64> %766 to <2 x i32>
  %768 = or i64 %681, 18
  %769 = getelementptr inbounds i32, i32* %1, i64 %768
  %770 = bitcast i32* %769 to <2 x i32>*
  store <2 x i32> %767, <2 x i32>* %770, align 4
  %771 = load <2 x i64>, <2 x i64>* %636, align 16
  %772 = add nsw <2 x i64> %771, <i64 1, i64 1>
  %773 = lshr <2 x i64> %771, <i64 63, i64 63>
  %774 = add nsw <2 x i64> %772, %773
  %775 = lshr <2 x i64> %774, <i64 2, i64 2>
  %776 = trunc <2 x i64> %775 to <2 x i32>
  %777 = or i64 %681, 20
  %778 = getelementptr inbounds i32, i32* %1, i64 %777
  %779 = bitcast i32* %778 to <2 x i32>*
  store <2 x i32> %776, <2 x i32>* %779, align 4
  %780 = load <2 x i64>, <2 x i64>* %638, align 16
  %781 = add nsw <2 x i64> %780, <i64 1, i64 1>
  %782 = lshr <2 x i64> %780, <i64 63, i64 63>
  %783 = add nsw <2 x i64> %781, %782
  %784 = lshr <2 x i64> %783, <i64 2, i64 2>
  %785 = trunc <2 x i64> %784 to <2 x i32>
  %786 = or i64 %681, 22
  %787 = getelementptr inbounds i32, i32* %1, i64 %786
  %788 = bitcast i32* %787 to <2 x i32>*
  store <2 x i32> %785, <2 x i32>* %788, align 4
  %789 = load <2 x i64>, <2 x i64>* %640, align 16
  %790 = add nsw <2 x i64> %789, <i64 1, i64 1>
  %791 = lshr <2 x i64> %789, <i64 63, i64 63>
  %792 = add nsw <2 x i64> %790, %791
  %793 = lshr <2 x i64> %792, <i64 2, i64 2>
  %794 = trunc <2 x i64> %793 to <2 x i32>
  %795 = or i64 %681, 24
  %796 = getelementptr inbounds i32, i32* %1, i64 %795
  %797 = bitcast i32* %796 to <2 x i32>*
  store <2 x i32> %794, <2 x i32>* %797, align 4
  %798 = load <2 x i64>, <2 x i64>* %642, align 16
  %799 = add nsw <2 x i64> %798, <i64 1, i64 1>
  %800 = lshr <2 x i64> %798, <i64 63, i64 63>
  %801 = add nsw <2 x i64> %799, %800
  %802 = lshr <2 x i64> %801, <i64 2, i64 2>
  %803 = trunc <2 x i64> %802 to <2 x i32>
  %804 = or i64 %681, 26
  %805 = getelementptr inbounds i32, i32* %1, i64 %804
  %806 = bitcast i32* %805 to <2 x i32>*
  store <2 x i32> %803, <2 x i32>* %806, align 4
  %807 = load <2 x i64>, <2 x i64>* %644, align 16
  %808 = add nsw <2 x i64> %807, <i64 1, i64 1>
  %809 = lshr <2 x i64> %807, <i64 63, i64 63>
  %810 = add nsw <2 x i64> %808, %809
  %811 = lshr <2 x i64> %810, <i64 2, i64 2>
  %812 = trunc <2 x i64> %811 to <2 x i32>
  %813 = or i64 %681, 28
  %814 = getelementptr inbounds i32, i32* %1, i64 %813
  %815 = bitcast i32* %814 to <2 x i32>*
  store <2 x i32> %812, <2 x i32>* %815, align 4
  %816 = load <2 x i64>, <2 x i64>* %646, align 16
  %817 = add nsw <2 x i64> %816, <i64 1, i64 1>
  %818 = lshr <2 x i64> %816, <i64 63, i64 63>
  %819 = add nsw <2 x i64> %817, %818
  %820 = lshr <2 x i64> %819, <i64 2, i64 2>
  %821 = trunc <2 x i64> %820 to <2 x i32>
  %822 = or i64 %681, 30
  %823 = getelementptr inbounds i32, i32* %1, i64 %822
  %824 = bitcast i32* %823 to <2 x i32>*
  store <2 x i32> %821, <2 x i32>* %824, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %613) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %612) #6
  %825 = add nuw nsw i64 %648, 1
  %826 = icmp eq i64 %825, 32
  br i1 %826, label %11292, label %647

827:                                              ; preds = %544
  %828 = getelementptr inbounds i16, i16* %262, i64 256
  %829 = bitcast i16* %828 to <8 x i16>*
  %830 = load <8 x i16>, <8 x i16>* %829, align 16
  %831 = getelementptr inbounds i16, i16* %262, i64 288
  %832 = bitcast i16* %831 to <8 x i16>*
  %833 = load <8 x i16>, <8 x i16>* %832, align 16
  %834 = getelementptr inbounds i16, i16* %262, i64 320
  %835 = bitcast i16* %834 to <8 x i16>*
  %836 = load <8 x i16>, <8 x i16>* %835, align 16
  %837 = getelementptr inbounds i16, i16* %262, i64 352
  %838 = bitcast i16* %837 to <8 x i16>*
  %839 = load <8 x i16>, <8 x i16>* %838, align 16
  %840 = getelementptr inbounds i16, i16* %262, i64 640
  %841 = bitcast i16* %840 to <8 x i16>*
  %842 = load <8 x i16>, <8 x i16>* %841, align 16
  %843 = getelementptr inbounds i16, i16* %262, i64 672
  %844 = bitcast i16* %843 to <8 x i16>*
  %845 = load <8 x i16>, <8 x i16>* %844, align 16
  %846 = getelementptr inbounds i16, i16* %262, i64 704
  %847 = bitcast i16* %846 to <8 x i16>*
  %848 = load <8 x i16>, <8 x i16>* %847, align 16
  %849 = getelementptr inbounds i16, i16* %262, i64 736
  %850 = bitcast i16* %849 to <8 x i16>*
  %851 = load <8 x i16>, <8 x i16>* %850, align 16
  %852 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %830, <8 x i16> %851) #6
  %853 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %833, <8 x i16> %848) #6
  %854 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %836, <8 x i16> %845) #6
  %855 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %839, <8 x i16> %842) #6
  %856 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %839, <8 x i16> %842) #6
  %857 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %836, <8 x i16> %845) #6
  %858 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %833, <8 x i16> %848) #6
  %859 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %830, <8 x i16> %851) #6
  %860 = add <8 x i16> %852, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %861 = icmp ult <8 x i16> %860, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %862 = add <8 x i16> %853, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %863 = icmp ult <8 x i16> %862, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %864 = add <8 x i16> %854, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %865 = icmp ult <8 x i16> %864, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %866 = add <8 x i16> %855, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %867 = icmp ult <8 x i16> %866, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %868 = or <8 x i1> %865, %867
  %869 = or <8 x i1> %868, %863
  %870 = or <8 x i1> %869, %861
  %871 = sext <8 x i1> %870 to <8 x i16>
  %872 = bitcast <8 x i16> %871 to <16 x i8>
  %873 = icmp slt <16 x i8> %872, zeroinitializer
  %874 = bitcast <16 x i1> %873 to i16
  %875 = zext i16 %874 to i32
  %876 = add <8 x i16> %856, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %877 = icmp ult <8 x i16> %876, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %878 = add <8 x i16> %857, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %879 = icmp ult <8 x i16> %878, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %880 = add <8 x i16> %858, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %881 = icmp ult <8 x i16> %880, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %882 = add <8 x i16> %859, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %883 = icmp ult <8 x i16> %882, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %884 = or <8 x i1> %879, %877
  %885 = or <8 x i1> %884, %881
  %886 = or <8 x i1> %885, %883
  %887 = sext <8 x i1> %886 to <8 x i16>
  %888 = bitcast <8 x i16> %887 to <16 x i8>
  %889 = icmp slt <16 x i8> %888, zeroinitializer
  %890 = bitcast <16 x i1> %889 to i16
  %891 = zext i16 %890 to i32
  %892 = sub nsw i32 0, %875
  %893 = icmp eq i32 %891, %892
  br i1 %893, label %1110, label %894

894:                                              ; preds = %827
  %895 = bitcast [32 x i64]* %4 to i8*
  %896 = bitcast [32 x i64]* %5 to i8*
  %897 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %898 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %899 = bitcast [32 x i64]* %5 to <2 x i64>*
  %900 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %901 = bitcast i64* %900 to <2 x i64>*
  %902 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %903 = bitcast i64* %902 to <2 x i64>*
  %904 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %905 = bitcast i64* %904 to <2 x i64>*
  %906 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %907 = bitcast i64* %906 to <2 x i64>*
  %908 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %909 = bitcast i64* %908 to <2 x i64>*
  %910 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %911 = bitcast i64* %910 to <2 x i64>*
  %912 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %913 = bitcast i64* %912 to <2 x i64>*
  %914 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %915 = bitcast i64* %914 to <2 x i64>*
  %916 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %917 = bitcast i64* %916 to <2 x i64>*
  %918 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %919 = bitcast i64* %918 to <2 x i64>*
  %920 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %921 = bitcast i64* %920 to <2 x i64>*
  %922 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %923 = bitcast i64* %922 to <2 x i64>*
  %924 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %925 = bitcast i64* %924 to <2 x i64>*
  %926 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %927 = bitcast i64* %926 to <2 x i64>*
  %928 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %929 = bitcast i64* %928 to <2 x i64>*
  br label %930

930:                                              ; preds = %963, %894
  %931 = phi i64 [ 0, %894 ], [ %1108, %963 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %895) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %895, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %896) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %896, i8 -86, i64 256, i1 false) #6
  br label %932

932:                                              ; preds = %932, %930
  %933 = phi i64 [ 0, %930 ], [ %961, %932 ]
  %934 = shl i64 %933, 5
  %935 = add nuw nsw i64 %934, %931
  %936 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %935
  %937 = load i16, i16* %936, align 2
  %938 = sext i16 %937 to i64
  %939 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %933
  store i64 %938, i64* %939, align 16
  %940 = or i64 %933, 1
  %941 = shl i64 %940, 5
  %942 = add nuw nsw i64 %941, %931
  %943 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %942
  %944 = load i16, i16* %943, align 2
  %945 = sext i16 %944 to i64
  %946 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %940
  store i64 %945, i64* %946, align 8
  %947 = or i64 %933, 2
  %948 = shl i64 %947, 5
  %949 = add nuw nsw i64 %948, %931
  %950 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %949
  %951 = load i16, i16* %950, align 2
  %952 = sext i16 %951 to i64
  %953 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %947
  store i64 %952, i64* %953, align 16
  %954 = or i64 %933, 3
  %955 = shl i64 %954, 5
  %956 = add nuw nsw i64 %955, %931
  %957 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %956
  %958 = load i16, i16* %957, align 2
  %959 = sext i16 %958 to i64
  %960 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %954
  store i64 %959, i64* %960, align 8
  %961 = add nuw nsw i64 %933, 4
  %962 = icmp eq i64 %961, 32
  br i1 %962, label %963, label %932

963:                                              ; preds = %932
  call void @vpx_fdct32(i64* nonnull %897, i64* nonnull %898, i32 0) #6
  %964 = shl i64 %931, 5
  %965 = load <2 x i64>, <2 x i64>* %899, align 16
  %966 = add nsw <2 x i64> %965, <i64 1, i64 1>
  %967 = lshr <2 x i64> %965, <i64 63, i64 63>
  %968 = add nsw <2 x i64> %966, %967
  %969 = lshr <2 x i64> %968, <i64 2, i64 2>
  %970 = trunc <2 x i64> %969 to <2 x i32>
  %971 = getelementptr inbounds i32, i32* %1, i64 %964
  %972 = bitcast i32* %971 to <2 x i32>*
  store <2 x i32> %970, <2 x i32>* %972, align 4
  %973 = load <2 x i64>, <2 x i64>* %901, align 16
  %974 = add nsw <2 x i64> %973, <i64 1, i64 1>
  %975 = lshr <2 x i64> %973, <i64 63, i64 63>
  %976 = add nsw <2 x i64> %974, %975
  %977 = lshr <2 x i64> %976, <i64 2, i64 2>
  %978 = trunc <2 x i64> %977 to <2 x i32>
  %979 = or i64 %964, 2
  %980 = getelementptr inbounds i32, i32* %1, i64 %979
  %981 = bitcast i32* %980 to <2 x i32>*
  store <2 x i32> %978, <2 x i32>* %981, align 4
  %982 = load <2 x i64>, <2 x i64>* %903, align 16
  %983 = add nsw <2 x i64> %982, <i64 1, i64 1>
  %984 = lshr <2 x i64> %982, <i64 63, i64 63>
  %985 = add nsw <2 x i64> %983, %984
  %986 = lshr <2 x i64> %985, <i64 2, i64 2>
  %987 = trunc <2 x i64> %986 to <2 x i32>
  %988 = or i64 %964, 4
  %989 = getelementptr inbounds i32, i32* %1, i64 %988
  %990 = bitcast i32* %989 to <2 x i32>*
  store <2 x i32> %987, <2 x i32>* %990, align 4
  %991 = load <2 x i64>, <2 x i64>* %905, align 16
  %992 = add nsw <2 x i64> %991, <i64 1, i64 1>
  %993 = lshr <2 x i64> %991, <i64 63, i64 63>
  %994 = add nsw <2 x i64> %992, %993
  %995 = lshr <2 x i64> %994, <i64 2, i64 2>
  %996 = trunc <2 x i64> %995 to <2 x i32>
  %997 = or i64 %964, 6
  %998 = getelementptr inbounds i32, i32* %1, i64 %997
  %999 = bitcast i32* %998 to <2 x i32>*
  store <2 x i32> %996, <2 x i32>* %999, align 4
  %1000 = load <2 x i64>, <2 x i64>* %907, align 16
  %1001 = add nsw <2 x i64> %1000, <i64 1, i64 1>
  %1002 = lshr <2 x i64> %1000, <i64 63, i64 63>
  %1003 = add nsw <2 x i64> %1001, %1002
  %1004 = lshr <2 x i64> %1003, <i64 2, i64 2>
  %1005 = trunc <2 x i64> %1004 to <2 x i32>
  %1006 = or i64 %964, 8
  %1007 = getelementptr inbounds i32, i32* %1, i64 %1006
  %1008 = bitcast i32* %1007 to <2 x i32>*
  store <2 x i32> %1005, <2 x i32>* %1008, align 4
  %1009 = load <2 x i64>, <2 x i64>* %909, align 16
  %1010 = add nsw <2 x i64> %1009, <i64 1, i64 1>
  %1011 = lshr <2 x i64> %1009, <i64 63, i64 63>
  %1012 = add nsw <2 x i64> %1010, %1011
  %1013 = lshr <2 x i64> %1012, <i64 2, i64 2>
  %1014 = trunc <2 x i64> %1013 to <2 x i32>
  %1015 = or i64 %964, 10
  %1016 = getelementptr inbounds i32, i32* %1, i64 %1015
  %1017 = bitcast i32* %1016 to <2 x i32>*
  store <2 x i32> %1014, <2 x i32>* %1017, align 4
  %1018 = load <2 x i64>, <2 x i64>* %911, align 16
  %1019 = add nsw <2 x i64> %1018, <i64 1, i64 1>
  %1020 = lshr <2 x i64> %1018, <i64 63, i64 63>
  %1021 = add nsw <2 x i64> %1019, %1020
  %1022 = lshr <2 x i64> %1021, <i64 2, i64 2>
  %1023 = trunc <2 x i64> %1022 to <2 x i32>
  %1024 = or i64 %964, 12
  %1025 = getelementptr inbounds i32, i32* %1, i64 %1024
  %1026 = bitcast i32* %1025 to <2 x i32>*
  store <2 x i32> %1023, <2 x i32>* %1026, align 4
  %1027 = load <2 x i64>, <2 x i64>* %913, align 16
  %1028 = add nsw <2 x i64> %1027, <i64 1, i64 1>
  %1029 = lshr <2 x i64> %1027, <i64 63, i64 63>
  %1030 = add nsw <2 x i64> %1028, %1029
  %1031 = lshr <2 x i64> %1030, <i64 2, i64 2>
  %1032 = trunc <2 x i64> %1031 to <2 x i32>
  %1033 = or i64 %964, 14
  %1034 = getelementptr inbounds i32, i32* %1, i64 %1033
  %1035 = bitcast i32* %1034 to <2 x i32>*
  store <2 x i32> %1032, <2 x i32>* %1035, align 4
  %1036 = load <2 x i64>, <2 x i64>* %915, align 16
  %1037 = add nsw <2 x i64> %1036, <i64 1, i64 1>
  %1038 = lshr <2 x i64> %1036, <i64 63, i64 63>
  %1039 = add nsw <2 x i64> %1037, %1038
  %1040 = lshr <2 x i64> %1039, <i64 2, i64 2>
  %1041 = trunc <2 x i64> %1040 to <2 x i32>
  %1042 = or i64 %964, 16
  %1043 = getelementptr inbounds i32, i32* %1, i64 %1042
  %1044 = bitcast i32* %1043 to <2 x i32>*
  store <2 x i32> %1041, <2 x i32>* %1044, align 4
  %1045 = load <2 x i64>, <2 x i64>* %917, align 16
  %1046 = add nsw <2 x i64> %1045, <i64 1, i64 1>
  %1047 = lshr <2 x i64> %1045, <i64 63, i64 63>
  %1048 = add nsw <2 x i64> %1046, %1047
  %1049 = lshr <2 x i64> %1048, <i64 2, i64 2>
  %1050 = trunc <2 x i64> %1049 to <2 x i32>
  %1051 = or i64 %964, 18
  %1052 = getelementptr inbounds i32, i32* %1, i64 %1051
  %1053 = bitcast i32* %1052 to <2 x i32>*
  store <2 x i32> %1050, <2 x i32>* %1053, align 4
  %1054 = load <2 x i64>, <2 x i64>* %919, align 16
  %1055 = add nsw <2 x i64> %1054, <i64 1, i64 1>
  %1056 = lshr <2 x i64> %1054, <i64 63, i64 63>
  %1057 = add nsw <2 x i64> %1055, %1056
  %1058 = lshr <2 x i64> %1057, <i64 2, i64 2>
  %1059 = trunc <2 x i64> %1058 to <2 x i32>
  %1060 = or i64 %964, 20
  %1061 = getelementptr inbounds i32, i32* %1, i64 %1060
  %1062 = bitcast i32* %1061 to <2 x i32>*
  store <2 x i32> %1059, <2 x i32>* %1062, align 4
  %1063 = load <2 x i64>, <2 x i64>* %921, align 16
  %1064 = add nsw <2 x i64> %1063, <i64 1, i64 1>
  %1065 = lshr <2 x i64> %1063, <i64 63, i64 63>
  %1066 = add nsw <2 x i64> %1064, %1065
  %1067 = lshr <2 x i64> %1066, <i64 2, i64 2>
  %1068 = trunc <2 x i64> %1067 to <2 x i32>
  %1069 = or i64 %964, 22
  %1070 = getelementptr inbounds i32, i32* %1, i64 %1069
  %1071 = bitcast i32* %1070 to <2 x i32>*
  store <2 x i32> %1068, <2 x i32>* %1071, align 4
  %1072 = load <2 x i64>, <2 x i64>* %923, align 16
  %1073 = add nsw <2 x i64> %1072, <i64 1, i64 1>
  %1074 = lshr <2 x i64> %1072, <i64 63, i64 63>
  %1075 = add nsw <2 x i64> %1073, %1074
  %1076 = lshr <2 x i64> %1075, <i64 2, i64 2>
  %1077 = trunc <2 x i64> %1076 to <2 x i32>
  %1078 = or i64 %964, 24
  %1079 = getelementptr inbounds i32, i32* %1, i64 %1078
  %1080 = bitcast i32* %1079 to <2 x i32>*
  store <2 x i32> %1077, <2 x i32>* %1080, align 4
  %1081 = load <2 x i64>, <2 x i64>* %925, align 16
  %1082 = add nsw <2 x i64> %1081, <i64 1, i64 1>
  %1083 = lshr <2 x i64> %1081, <i64 63, i64 63>
  %1084 = add nsw <2 x i64> %1082, %1083
  %1085 = lshr <2 x i64> %1084, <i64 2, i64 2>
  %1086 = trunc <2 x i64> %1085 to <2 x i32>
  %1087 = or i64 %964, 26
  %1088 = getelementptr inbounds i32, i32* %1, i64 %1087
  %1089 = bitcast i32* %1088 to <2 x i32>*
  store <2 x i32> %1086, <2 x i32>* %1089, align 4
  %1090 = load <2 x i64>, <2 x i64>* %927, align 16
  %1091 = add nsw <2 x i64> %1090, <i64 1, i64 1>
  %1092 = lshr <2 x i64> %1090, <i64 63, i64 63>
  %1093 = add nsw <2 x i64> %1091, %1092
  %1094 = lshr <2 x i64> %1093, <i64 2, i64 2>
  %1095 = trunc <2 x i64> %1094 to <2 x i32>
  %1096 = or i64 %964, 28
  %1097 = getelementptr inbounds i32, i32* %1, i64 %1096
  %1098 = bitcast i32* %1097 to <2 x i32>*
  store <2 x i32> %1095, <2 x i32>* %1098, align 4
  %1099 = load <2 x i64>, <2 x i64>* %929, align 16
  %1100 = add nsw <2 x i64> %1099, <i64 1, i64 1>
  %1101 = lshr <2 x i64> %1099, <i64 63, i64 63>
  %1102 = add nsw <2 x i64> %1100, %1101
  %1103 = lshr <2 x i64> %1102, <i64 2, i64 2>
  %1104 = trunc <2 x i64> %1103 to <2 x i32>
  %1105 = or i64 %964, 30
  %1106 = getelementptr inbounds i32, i32* %1, i64 %1105
  %1107 = bitcast i32* %1106 to <2 x i32>*
  store <2 x i32> %1104, <2 x i32>* %1107, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %896) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %895) #6
  %1108 = add nuw nsw i64 %931, 1
  %1109 = icmp eq i64 %1108, 32
  br i1 %1109, label %11292, label %930

1110:                                             ; preds = %827
  %1111 = getelementptr inbounds i16, i16* %262, i64 384
  %1112 = bitcast i16* %1111 to <8 x i16>*
  %1113 = load <8 x i16>, <8 x i16>* %1112, align 16
  %1114 = getelementptr inbounds i16, i16* %262, i64 416
  %1115 = bitcast i16* %1114 to <8 x i16>*
  %1116 = load <8 x i16>, <8 x i16>* %1115, align 16
  %1117 = getelementptr inbounds i16, i16* %262, i64 448
  %1118 = bitcast i16* %1117 to <8 x i16>*
  %1119 = load <8 x i16>, <8 x i16>* %1118, align 16
  %1120 = getelementptr inbounds i16, i16* %262, i64 480
  %1121 = bitcast i16* %1120 to <8 x i16>*
  %1122 = load <8 x i16>, <8 x i16>* %1121, align 16
  %1123 = getelementptr inbounds i16, i16* %262, i64 512
  %1124 = bitcast i16* %1123 to <8 x i16>*
  %1125 = load <8 x i16>, <8 x i16>* %1124, align 16
  %1126 = getelementptr inbounds i16, i16* %262, i64 544
  %1127 = bitcast i16* %1126 to <8 x i16>*
  %1128 = load <8 x i16>, <8 x i16>* %1127, align 16
  %1129 = getelementptr inbounds i16, i16* %262, i64 576
  %1130 = bitcast i16* %1129 to <8 x i16>*
  %1131 = load <8 x i16>, <8 x i16>* %1130, align 16
  %1132 = getelementptr inbounds i16, i16* %262, i64 608
  %1133 = bitcast i16* %1132 to <8 x i16>*
  %1134 = load <8 x i16>, <8 x i16>* %1133, align 16
  %1135 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1113, <8 x i16> %1134) #6
  %1136 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1116, <8 x i16> %1131) #6
  %1137 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1119, <8 x i16> %1128) #6
  %1138 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1122, <8 x i16> %1125) #6
  %1139 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1122, <8 x i16> %1125) #6
  %1140 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1119, <8 x i16> %1128) #6
  %1141 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1116, <8 x i16> %1131) #6
  %1142 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1113, <8 x i16> %1134) #6
  %1143 = add <8 x i16> %1135, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1144 = icmp ult <8 x i16> %1143, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1145 = add <8 x i16> %1136, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1146 = icmp ult <8 x i16> %1145, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1147 = add <8 x i16> %1137, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1148 = icmp ult <8 x i16> %1147, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1149 = add <8 x i16> %1138, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1150 = icmp ult <8 x i16> %1149, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1151 = or <8 x i1> %1148, %1150
  %1152 = or <8 x i1> %1151, %1146
  %1153 = or <8 x i1> %1152, %1144
  %1154 = sext <8 x i1> %1153 to <8 x i16>
  %1155 = bitcast <8 x i16> %1154 to <16 x i8>
  %1156 = icmp slt <16 x i8> %1155, zeroinitializer
  %1157 = bitcast <16 x i1> %1156 to i16
  %1158 = zext i16 %1157 to i32
  %1159 = add <8 x i16> %1139, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1160 = icmp ult <8 x i16> %1159, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1161 = add <8 x i16> %1140, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1162 = icmp ult <8 x i16> %1161, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1163 = add <8 x i16> %1141, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1164 = icmp ult <8 x i16> %1163, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1165 = add <8 x i16> %1142, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1166 = icmp ult <8 x i16> %1165, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1167 = or <8 x i1> %1162, %1160
  %1168 = or <8 x i1> %1167, %1164
  %1169 = or <8 x i1> %1168, %1166
  %1170 = sext <8 x i1> %1169 to <8 x i16>
  %1171 = bitcast <8 x i16> %1170 to <16 x i8>
  %1172 = icmp slt <16 x i8> %1171, zeroinitializer
  %1173 = bitcast <16 x i1> %1172 to i16
  %1174 = zext i16 %1173 to i32
  %1175 = sub nsw i32 0, %1158
  %1176 = icmp eq i32 %1174, %1175
  br i1 %1176, label %1393, label %1177

1177:                                             ; preds = %1110
  %1178 = bitcast [32 x i64]* %4 to i8*
  %1179 = bitcast [32 x i64]* %5 to i8*
  %1180 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %1181 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %1182 = bitcast [32 x i64]* %5 to <2 x i64>*
  %1183 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %1184 = bitcast i64* %1183 to <2 x i64>*
  %1185 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %1186 = bitcast i64* %1185 to <2 x i64>*
  %1187 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %1188 = bitcast i64* %1187 to <2 x i64>*
  %1189 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %1190 = bitcast i64* %1189 to <2 x i64>*
  %1191 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %1192 = bitcast i64* %1191 to <2 x i64>*
  %1193 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %1194 = bitcast i64* %1193 to <2 x i64>*
  %1195 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %1196 = bitcast i64* %1195 to <2 x i64>*
  %1197 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %1198 = bitcast i64* %1197 to <2 x i64>*
  %1199 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %1200 = bitcast i64* %1199 to <2 x i64>*
  %1201 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %1202 = bitcast i64* %1201 to <2 x i64>*
  %1203 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %1204 = bitcast i64* %1203 to <2 x i64>*
  %1205 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %1206 = bitcast i64* %1205 to <2 x i64>*
  %1207 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %1208 = bitcast i64* %1207 to <2 x i64>*
  %1209 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %1210 = bitcast i64* %1209 to <2 x i64>*
  %1211 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %1212 = bitcast i64* %1211 to <2 x i64>*
  br label %1213

1213:                                             ; preds = %1246, %1177
  %1214 = phi i64 [ 0, %1177 ], [ %1391, %1246 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %1178) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %1178, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %1179) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %1179, i8 -86, i64 256, i1 false) #6
  br label %1215

1215:                                             ; preds = %1215, %1213
  %1216 = phi i64 [ 0, %1213 ], [ %1244, %1215 ]
  %1217 = shl i64 %1216, 5
  %1218 = add nuw nsw i64 %1217, %1214
  %1219 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %1218
  %1220 = load i16, i16* %1219, align 2
  %1221 = sext i16 %1220 to i64
  %1222 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %1216
  store i64 %1221, i64* %1222, align 16
  %1223 = or i64 %1216, 1
  %1224 = shl i64 %1223, 5
  %1225 = add nuw nsw i64 %1224, %1214
  %1226 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %1225
  %1227 = load i16, i16* %1226, align 2
  %1228 = sext i16 %1227 to i64
  %1229 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %1223
  store i64 %1228, i64* %1229, align 8
  %1230 = or i64 %1216, 2
  %1231 = shl i64 %1230, 5
  %1232 = add nuw nsw i64 %1231, %1214
  %1233 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %1232
  %1234 = load i16, i16* %1233, align 2
  %1235 = sext i16 %1234 to i64
  %1236 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %1230
  store i64 %1235, i64* %1236, align 16
  %1237 = or i64 %1216, 3
  %1238 = shl i64 %1237, 5
  %1239 = add nuw nsw i64 %1238, %1214
  %1240 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %1239
  %1241 = load i16, i16* %1240, align 2
  %1242 = sext i16 %1241 to i64
  %1243 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %1237
  store i64 %1242, i64* %1243, align 8
  %1244 = add nuw nsw i64 %1216, 4
  %1245 = icmp eq i64 %1244, 32
  br i1 %1245, label %1246, label %1215

1246:                                             ; preds = %1215
  call void @vpx_fdct32(i64* nonnull %1180, i64* nonnull %1181, i32 0) #6
  %1247 = shl i64 %1214, 5
  %1248 = load <2 x i64>, <2 x i64>* %1182, align 16
  %1249 = add nsw <2 x i64> %1248, <i64 1, i64 1>
  %1250 = lshr <2 x i64> %1248, <i64 63, i64 63>
  %1251 = add nsw <2 x i64> %1249, %1250
  %1252 = lshr <2 x i64> %1251, <i64 2, i64 2>
  %1253 = trunc <2 x i64> %1252 to <2 x i32>
  %1254 = getelementptr inbounds i32, i32* %1, i64 %1247
  %1255 = bitcast i32* %1254 to <2 x i32>*
  store <2 x i32> %1253, <2 x i32>* %1255, align 4
  %1256 = load <2 x i64>, <2 x i64>* %1184, align 16
  %1257 = add nsw <2 x i64> %1256, <i64 1, i64 1>
  %1258 = lshr <2 x i64> %1256, <i64 63, i64 63>
  %1259 = add nsw <2 x i64> %1257, %1258
  %1260 = lshr <2 x i64> %1259, <i64 2, i64 2>
  %1261 = trunc <2 x i64> %1260 to <2 x i32>
  %1262 = or i64 %1247, 2
  %1263 = getelementptr inbounds i32, i32* %1, i64 %1262
  %1264 = bitcast i32* %1263 to <2 x i32>*
  store <2 x i32> %1261, <2 x i32>* %1264, align 4
  %1265 = load <2 x i64>, <2 x i64>* %1186, align 16
  %1266 = add nsw <2 x i64> %1265, <i64 1, i64 1>
  %1267 = lshr <2 x i64> %1265, <i64 63, i64 63>
  %1268 = add nsw <2 x i64> %1266, %1267
  %1269 = lshr <2 x i64> %1268, <i64 2, i64 2>
  %1270 = trunc <2 x i64> %1269 to <2 x i32>
  %1271 = or i64 %1247, 4
  %1272 = getelementptr inbounds i32, i32* %1, i64 %1271
  %1273 = bitcast i32* %1272 to <2 x i32>*
  store <2 x i32> %1270, <2 x i32>* %1273, align 4
  %1274 = load <2 x i64>, <2 x i64>* %1188, align 16
  %1275 = add nsw <2 x i64> %1274, <i64 1, i64 1>
  %1276 = lshr <2 x i64> %1274, <i64 63, i64 63>
  %1277 = add nsw <2 x i64> %1275, %1276
  %1278 = lshr <2 x i64> %1277, <i64 2, i64 2>
  %1279 = trunc <2 x i64> %1278 to <2 x i32>
  %1280 = or i64 %1247, 6
  %1281 = getelementptr inbounds i32, i32* %1, i64 %1280
  %1282 = bitcast i32* %1281 to <2 x i32>*
  store <2 x i32> %1279, <2 x i32>* %1282, align 4
  %1283 = load <2 x i64>, <2 x i64>* %1190, align 16
  %1284 = add nsw <2 x i64> %1283, <i64 1, i64 1>
  %1285 = lshr <2 x i64> %1283, <i64 63, i64 63>
  %1286 = add nsw <2 x i64> %1284, %1285
  %1287 = lshr <2 x i64> %1286, <i64 2, i64 2>
  %1288 = trunc <2 x i64> %1287 to <2 x i32>
  %1289 = or i64 %1247, 8
  %1290 = getelementptr inbounds i32, i32* %1, i64 %1289
  %1291 = bitcast i32* %1290 to <2 x i32>*
  store <2 x i32> %1288, <2 x i32>* %1291, align 4
  %1292 = load <2 x i64>, <2 x i64>* %1192, align 16
  %1293 = add nsw <2 x i64> %1292, <i64 1, i64 1>
  %1294 = lshr <2 x i64> %1292, <i64 63, i64 63>
  %1295 = add nsw <2 x i64> %1293, %1294
  %1296 = lshr <2 x i64> %1295, <i64 2, i64 2>
  %1297 = trunc <2 x i64> %1296 to <2 x i32>
  %1298 = or i64 %1247, 10
  %1299 = getelementptr inbounds i32, i32* %1, i64 %1298
  %1300 = bitcast i32* %1299 to <2 x i32>*
  store <2 x i32> %1297, <2 x i32>* %1300, align 4
  %1301 = load <2 x i64>, <2 x i64>* %1194, align 16
  %1302 = add nsw <2 x i64> %1301, <i64 1, i64 1>
  %1303 = lshr <2 x i64> %1301, <i64 63, i64 63>
  %1304 = add nsw <2 x i64> %1302, %1303
  %1305 = lshr <2 x i64> %1304, <i64 2, i64 2>
  %1306 = trunc <2 x i64> %1305 to <2 x i32>
  %1307 = or i64 %1247, 12
  %1308 = getelementptr inbounds i32, i32* %1, i64 %1307
  %1309 = bitcast i32* %1308 to <2 x i32>*
  store <2 x i32> %1306, <2 x i32>* %1309, align 4
  %1310 = load <2 x i64>, <2 x i64>* %1196, align 16
  %1311 = add nsw <2 x i64> %1310, <i64 1, i64 1>
  %1312 = lshr <2 x i64> %1310, <i64 63, i64 63>
  %1313 = add nsw <2 x i64> %1311, %1312
  %1314 = lshr <2 x i64> %1313, <i64 2, i64 2>
  %1315 = trunc <2 x i64> %1314 to <2 x i32>
  %1316 = or i64 %1247, 14
  %1317 = getelementptr inbounds i32, i32* %1, i64 %1316
  %1318 = bitcast i32* %1317 to <2 x i32>*
  store <2 x i32> %1315, <2 x i32>* %1318, align 4
  %1319 = load <2 x i64>, <2 x i64>* %1198, align 16
  %1320 = add nsw <2 x i64> %1319, <i64 1, i64 1>
  %1321 = lshr <2 x i64> %1319, <i64 63, i64 63>
  %1322 = add nsw <2 x i64> %1320, %1321
  %1323 = lshr <2 x i64> %1322, <i64 2, i64 2>
  %1324 = trunc <2 x i64> %1323 to <2 x i32>
  %1325 = or i64 %1247, 16
  %1326 = getelementptr inbounds i32, i32* %1, i64 %1325
  %1327 = bitcast i32* %1326 to <2 x i32>*
  store <2 x i32> %1324, <2 x i32>* %1327, align 4
  %1328 = load <2 x i64>, <2 x i64>* %1200, align 16
  %1329 = add nsw <2 x i64> %1328, <i64 1, i64 1>
  %1330 = lshr <2 x i64> %1328, <i64 63, i64 63>
  %1331 = add nsw <2 x i64> %1329, %1330
  %1332 = lshr <2 x i64> %1331, <i64 2, i64 2>
  %1333 = trunc <2 x i64> %1332 to <2 x i32>
  %1334 = or i64 %1247, 18
  %1335 = getelementptr inbounds i32, i32* %1, i64 %1334
  %1336 = bitcast i32* %1335 to <2 x i32>*
  store <2 x i32> %1333, <2 x i32>* %1336, align 4
  %1337 = load <2 x i64>, <2 x i64>* %1202, align 16
  %1338 = add nsw <2 x i64> %1337, <i64 1, i64 1>
  %1339 = lshr <2 x i64> %1337, <i64 63, i64 63>
  %1340 = add nsw <2 x i64> %1338, %1339
  %1341 = lshr <2 x i64> %1340, <i64 2, i64 2>
  %1342 = trunc <2 x i64> %1341 to <2 x i32>
  %1343 = or i64 %1247, 20
  %1344 = getelementptr inbounds i32, i32* %1, i64 %1343
  %1345 = bitcast i32* %1344 to <2 x i32>*
  store <2 x i32> %1342, <2 x i32>* %1345, align 4
  %1346 = load <2 x i64>, <2 x i64>* %1204, align 16
  %1347 = add nsw <2 x i64> %1346, <i64 1, i64 1>
  %1348 = lshr <2 x i64> %1346, <i64 63, i64 63>
  %1349 = add nsw <2 x i64> %1347, %1348
  %1350 = lshr <2 x i64> %1349, <i64 2, i64 2>
  %1351 = trunc <2 x i64> %1350 to <2 x i32>
  %1352 = or i64 %1247, 22
  %1353 = getelementptr inbounds i32, i32* %1, i64 %1352
  %1354 = bitcast i32* %1353 to <2 x i32>*
  store <2 x i32> %1351, <2 x i32>* %1354, align 4
  %1355 = load <2 x i64>, <2 x i64>* %1206, align 16
  %1356 = add nsw <2 x i64> %1355, <i64 1, i64 1>
  %1357 = lshr <2 x i64> %1355, <i64 63, i64 63>
  %1358 = add nsw <2 x i64> %1356, %1357
  %1359 = lshr <2 x i64> %1358, <i64 2, i64 2>
  %1360 = trunc <2 x i64> %1359 to <2 x i32>
  %1361 = or i64 %1247, 24
  %1362 = getelementptr inbounds i32, i32* %1, i64 %1361
  %1363 = bitcast i32* %1362 to <2 x i32>*
  store <2 x i32> %1360, <2 x i32>* %1363, align 4
  %1364 = load <2 x i64>, <2 x i64>* %1208, align 16
  %1365 = add nsw <2 x i64> %1364, <i64 1, i64 1>
  %1366 = lshr <2 x i64> %1364, <i64 63, i64 63>
  %1367 = add nsw <2 x i64> %1365, %1366
  %1368 = lshr <2 x i64> %1367, <i64 2, i64 2>
  %1369 = trunc <2 x i64> %1368 to <2 x i32>
  %1370 = or i64 %1247, 26
  %1371 = getelementptr inbounds i32, i32* %1, i64 %1370
  %1372 = bitcast i32* %1371 to <2 x i32>*
  store <2 x i32> %1369, <2 x i32>* %1372, align 4
  %1373 = load <2 x i64>, <2 x i64>* %1210, align 16
  %1374 = add nsw <2 x i64> %1373, <i64 1, i64 1>
  %1375 = lshr <2 x i64> %1373, <i64 63, i64 63>
  %1376 = add nsw <2 x i64> %1374, %1375
  %1377 = lshr <2 x i64> %1376, <i64 2, i64 2>
  %1378 = trunc <2 x i64> %1377 to <2 x i32>
  %1379 = or i64 %1247, 28
  %1380 = getelementptr inbounds i32, i32* %1, i64 %1379
  %1381 = bitcast i32* %1380 to <2 x i32>*
  store <2 x i32> %1378, <2 x i32>* %1381, align 4
  %1382 = load <2 x i64>, <2 x i64>* %1212, align 16
  %1383 = add nsw <2 x i64> %1382, <i64 1, i64 1>
  %1384 = lshr <2 x i64> %1382, <i64 63, i64 63>
  %1385 = add nsw <2 x i64> %1383, %1384
  %1386 = lshr <2 x i64> %1385, <i64 2, i64 2>
  %1387 = trunc <2 x i64> %1386 to <2 x i32>
  %1388 = or i64 %1247, 30
  %1389 = getelementptr inbounds i32, i32* %1, i64 %1388
  %1390 = bitcast i32* %1389 to <2 x i32>*
  store <2 x i32> %1387, <2 x i32>* %1390, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %1179) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %1178) #6
  %1391 = add nuw nsw i64 %1214, 1
  %1392 = icmp eq i64 %1391, 32
  br i1 %1392, label %11292, label %1213

1393:                                             ; preds = %1110, %100
  %1394 = phi <8 x i16> [ %140, %100 ], [ %293, %1110 ]
  %1395 = phi <8 x i16> [ %139, %100 ], [ %292, %1110 ]
  %1396 = phi <8 x i16> [ %138, %100 ], [ %291, %1110 ]
  %1397 = phi <8 x i16> [ %137, %100 ], [ %290, %1110 ]
  %1398 = phi <8 x i16> [ %180, %100 ], [ %576, %1110 ]
  %1399 = phi <8 x i16> [ %179, %100 ], [ %575, %1110 ]
  %1400 = phi <8 x i16> [ %178, %100 ], [ %574, %1110 ]
  %1401 = phi <8 x i16> [ %177, %100 ], [ %573, %1110 ]
  %1402 = phi <8 x i16> [ %220, %100 ], [ %859, %1110 ]
  %1403 = phi <8 x i16> [ %219, %100 ], [ %858, %1110 ]
  %1404 = phi <8 x i16> [ %218, %100 ], [ %857, %1110 ]
  %1405 = phi <8 x i16> [ %217, %100 ], [ %856, %1110 ]
  %1406 = phi <8 x i16> [ %260, %100 ], [ %1142, %1110 ]
  %1407 = phi <8 x i16> [ %259, %100 ], [ %1141, %1110 ]
  %1408 = phi <8 x i16> [ %258, %100 ], [ %1140, %1110 ]
  %1409 = phi <8 x i16> [ %257, %100 ], [ %1139, %1110 ]
  %1410 = phi <8 x i16> [ %256, %100 ], [ %1138, %1110 ]
  %1411 = phi <8 x i16> [ %255, %100 ], [ %1137, %1110 ]
  %1412 = phi <8 x i16> [ %254, %100 ], [ %1136, %1110 ]
  %1413 = phi <8 x i16> [ %253, %100 ], [ %1135, %1110 ]
  %1414 = phi <8 x i16> [ %216, %100 ], [ %855, %1110 ]
  %1415 = phi <8 x i16> [ %215, %100 ], [ %854, %1110 ]
  %1416 = phi <8 x i16> [ %214, %100 ], [ %853, %1110 ]
  %1417 = phi <8 x i16> [ %213, %100 ], [ %852, %1110 ]
  %1418 = phi <8 x i16> [ %176, %100 ], [ %572, %1110 ]
  %1419 = phi <8 x i16> [ %175, %100 ], [ %571, %1110 ]
  %1420 = phi <8 x i16> [ %174, %100 ], [ %570, %1110 ]
  %1421 = phi <8 x i16> [ %173, %100 ], [ %569, %1110 ]
  %1422 = phi <8 x i16> [ %136, %100 ], [ %289, %1110 ]
  %1423 = phi <8 x i16> [ %135, %100 ], [ %288, %1110 ]
  %1424 = phi <8 x i16> [ %134, %100 ], [ %287, %1110 ]
  %1425 = phi <8 x i16> [ %133, %100 ], [ %286, %1110 ]
  %1426 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1425, <8 x i16> %1410) #6
  %1427 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1424, <8 x i16> %1411) #6
  %1428 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1423, <8 x i16> %1412) #6
  %1429 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1422, <8 x i16> %1413) #6
  %1430 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1421, <8 x i16> %1414) #6
  %1431 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1420, <8 x i16> %1415) #6
  %1432 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1419, <8 x i16> %1416) #6
  %1433 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1418, <8 x i16> %1417) #6
  %1434 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1418, <8 x i16> %1417) #6
  %1435 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1419, <8 x i16> %1416) #6
  %1436 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1420, <8 x i16> %1415) #6
  %1437 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1421, <8 x i16> %1414) #6
  %1438 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1422, <8 x i16> %1413) #6
  %1439 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1423, <8 x i16> %1412) #6
  %1440 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1424, <8 x i16> %1411) #6
  %1441 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1425, <8 x i16> %1410) #6
  %1442 = add <8 x i16> %1426, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1443 = icmp ult <8 x i16> %1442, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1444 = add <8 x i16> %1427, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1445 = icmp ult <8 x i16> %1444, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1446 = add <8 x i16> %1428, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1447 = icmp ult <8 x i16> %1446, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1448 = add <8 x i16> %1429, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1449 = icmp ult <8 x i16> %1448, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1450 = or <8 x i1> %1447, %1449
  %1451 = or <8 x i1> %1450, %1445
  %1452 = or <8 x i1> %1451, %1443
  %1453 = sext <8 x i1> %1452 to <8 x i16>
  %1454 = bitcast <8 x i16> %1453 to <16 x i8>
  %1455 = icmp slt <16 x i8> %1454, zeroinitializer
  %1456 = bitcast <16 x i1> %1455 to i16
  %1457 = zext i16 %1456 to i32
  %1458 = add <8 x i16> %1430, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1459 = icmp ult <8 x i16> %1458, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1460 = add <8 x i16> %1431, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1461 = icmp ult <8 x i16> %1460, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1462 = add <8 x i16> %1432, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1463 = icmp ult <8 x i16> %1462, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1464 = add <8 x i16> %1433, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1465 = icmp ult <8 x i16> %1464, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1466 = or <8 x i1> %1463, %1465
  %1467 = or <8 x i1> %1466, %1461
  %1468 = or <8 x i1> %1467, %1459
  %1469 = sext <8 x i1> %1468 to <8 x i16>
  %1470 = bitcast <8 x i16> %1469 to <16 x i8>
  %1471 = icmp slt <16 x i8> %1470, zeroinitializer
  %1472 = bitcast <16 x i1> %1471 to i16
  %1473 = zext i16 %1472 to i32
  %1474 = icmp eq i16 %1456, 0
  br i1 %1474, label %1475, label %1510

1475:                                             ; preds = %1393
  %1476 = add <8 x i16> %1434, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1477 = icmp ult <8 x i16> %1476, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1478 = add <8 x i16> %1435, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1479 = icmp ult <8 x i16> %1478, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1480 = add <8 x i16> %1436, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1481 = icmp ult <8 x i16> %1480, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1482 = add <8 x i16> %1437, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1483 = icmp ult <8 x i16> %1482, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1484 = or <8 x i1> %1479, %1477
  %1485 = or <8 x i1> %1484, %1481
  %1486 = or <8 x i1> %1485, %1483
  %1487 = sext <8 x i1> %1486 to <8 x i16>
  %1488 = bitcast <8 x i16> %1487 to <16 x i8>
  %1489 = icmp slt <16 x i8> %1488, zeroinitializer
  %1490 = bitcast <16 x i1> %1489 to i16
  %1491 = zext i16 %1490 to i32
  %1492 = icmp eq i16 %1472, 0
  br i1 %1492, label %1493, label %1510

1493:                                             ; preds = %1475
  %1494 = add <8 x i16> %1438, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1495 = icmp ult <8 x i16> %1494, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1496 = add <8 x i16> %1439, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1497 = icmp ult <8 x i16> %1496, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1498 = add <8 x i16> %1440, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1499 = icmp ult <8 x i16> %1498, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1500 = add <8 x i16> %1441, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1501 = icmp ult <8 x i16> %1500, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1502 = or <8 x i1> %1497, %1495
  %1503 = or <8 x i1> %1502, %1499
  %1504 = or <8 x i1> %1503, %1501
  %1505 = sext <8 x i1> %1504 to <8 x i16>
  %1506 = bitcast <8 x i16> %1505 to <16 x i8>
  %1507 = icmp slt <16 x i8> %1506, zeroinitializer
  %1508 = bitcast <16 x i1> %1507 to i16
  %1509 = zext i16 %1508 to i32
  br label %1510

1510:                                             ; preds = %1393, %1475, %1493
  %1511 = phi i32 [ %1457, %1393 ], [ %1491, %1475 ], [ %1491, %1493 ]
  %1512 = phi i32 [ %1473, %1393 ], [ %1473, %1475 ], [ %1509, %1493 ]
  %1513 = sub nsw i32 0, %1511
  %1514 = icmp eq i32 %1512, %1513
  br i1 %1514, label %1733, label %1515

1515:                                             ; preds = %1510
  br i1 %97, label %1516, label %1517

1516:                                             ; preds = %1515
  tail call void @vpx_highbd_fdct32x32_c(i16* %0, i32* %1, i32 %2) #6
  br label %11292

1517:                                             ; preds = %1515
  %1518 = bitcast [32 x i64]* %4 to i8*
  %1519 = bitcast [32 x i64]* %5 to i8*
  %1520 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %1521 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %1522 = bitcast [32 x i64]* %5 to <2 x i64>*
  %1523 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %1524 = bitcast i64* %1523 to <2 x i64>*
  %1525 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %1526 = bitcast i64* %1525 to <2 x i64>*
  %1527 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %1528 = bitcast i64* %1527 to <2 x i64>*
  %1529 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %1530 = bitcast i64* %1529 to <2 x i64>*
  %1531 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %1532 = bitcast i64* %1531 to <2 x i64>*
  %1533 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %1534 = bitcast i64* %1533 to <2 x i64>*
  %1535 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %1536 = bitcast i64* %1535 to <2 x i64>*
  %1537 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %1538 = bitcast i64* %1537 to <2 x i64>*
  %1539 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %1540 = bitcast i64* %1539 to <2 x i64>*
  %1541 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %1542 = bitcast i64* %1541 to <2 x i64>*
  %1543 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %1544 = bitcast i64* %1543 to <2 x i64>*
  %1545 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %1546 = bitcast i64* %1545 to <2 x i64>*
  %1547 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %1548 = bitcast i64* %1547 to <2 x i64>*
  %1549 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %1550 = bitcast i64* %1549 to <2 x i64>*
  %1551 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %1552 = bitcast i64* %1551 to <2 x i64>*
  br label %1553

1553:                                             ; preds = %1586, %1517
  %1554 = phi i64 [ 0, %1517 ], [ %1731, %1586 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %1518) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %1518, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %1519) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %1519, i8 -86, i64 256, i1 false) #6
  br label %1555

1555:                                             ; preds = %1555, %1553
  %1556 = phi i64 [ 0, %1553 ], [ %1584, %1555 ]
  %1557 = shl i64 %1556, 5
  %1558 = add nuw nsw i64 %1557, %1554
  %1559 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %1558
  %1560 = load i16, i16* %1559, align 2
  %1561 = sext i16 %1560 to i64
  %1562 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %1556
  store i64 %1561, i64* %1562, align 16
  %1563 = or i64 %1556, 1
  %1564 = shl i64 %1563, 5
  %1565 = add nuw nsw i64 %1564, %1554
  %1566 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %1565
  %1567 = load i16, i16* %1566, align 2
  %1568 = sext i16 %1567 to i64
  %1569 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %1563
  store i64 %1568, i64* %1569, align 8
  %1570 = or i64 %1556, 2
  %1571 = shl i64 %1570, 5
  %1572 = add nuw nsw i64 %1571, %1554
  %1573 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %1572
  %1574 = load i16, i16* %1573, align 2
  %1575 = sext i16 %1574 to i64
  %1576 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %1570
  store i64 %1575, i64* %1576, align 16
  %1577 = or i64 %1556, 3
  %1578 = shl i64 %1577, 5
  %1579 = add nuw nsw i64 %1578, %1554
  %1580 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %1579
  %1581 = load i16, i16* %1580, align 2
  %1582 = sext i16 %1581 to i64
  %1583 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %1577
  store i64 %1582, i64* %1583, align 8
  %1584 = add nuw nsw i64 %1556, 4
  %1585 = icmp eq i64 %1584, 32
  br i1 %1585, label %1586, label %1555

1586:                                             ; preds = %1555
  call void @vpx_fdct32(i64* nonnull %1520, i64* nonnull %1521, i32 0) #6
  %1587 = shl i64 %1554, 5
  %1588 = load <2 x i64>, <2 x i64>* %1522, align 16
  %1589 = add nsw <2 x i64> %1588, <i64 1, i64 1>
  %1590 = lshr <2 x i64> %1588, <i64 63, i64 63>
  %1591 = add nsw <2 x i64> %1589, %1590
  %1592 = lshr <2 x i64> %1591, <i64 2, i64 2>
  %1593 = trunc <2 x i64> %1592 to <2 x i32>
  %1594 = getelementptr inbounds i32, i32* %1, i64 %1587
  %1595 = bitcast i32* %1594 to <2 x i32>*
  store <2 x i32> %1593, <2 x i32>* %1595, align 4
  %1596 = load <2 x i64>, <2 x i64>* %1524, align 16
  %1597 = add nsw <2 x i64> %1596, <i64 1, i64 1>
  %1598 = lshr <2 x i64> %1596, <i64 63, i64 63>
  %1599 = add nsw <2 x i64> %1597, %1598
  %1600 = lshr <2 x i64> %1599, <i64 2, i64 2>
  %1601 = trunc <2 x i64> %1600 to <2 x i32>
  %1602 = or i64 %1587, 2
  %1603 = getelementptr inbounds i32, i32* %1, i64 %1602
  %1604 = bitcast i32* %1603 to <2 x i32>*
  store <2 x i32> %1601, <2 x i32>* %1604, align 4
  %1605 = load <2 x i64>, <2 x i64>* %1526, align 16
  %1606 = add nsw <2 x i64> %1605, <i64 1, i64 1>
  %1607 = lshr <2 x i64> %1605, <i64 63, i64 63>
  %1608 = add nsw <2 x i64> %1606, %1607
  %1609 = lshr <2 x i64> %1608, <i64 2, i64 2>
  %1610 = trunc <2 x i64> %1609 to <2 x i32>
  %1611 = or i64 %1587, 4
  %1612 = getelementptr inbounds i32, i32* %1, i64 %1611
  %1613 = bitcast i32* %1612 to <2 x i32>*
  store <2 x i32> %1610, <2 x i32>* %1613, align 4
  %1614 = load <2 x i64>, <2 x i64>* %1528, align 16
  %1615 = add nsw <2 x i64> %1614, <i64 1, i64 1>
  %1616 = lshr <2 x i64> %1614, <i64 63, i64 63>
  %1617 = add nsw <2 x i64> %1615, %1616
  %1618 = lshr <2 x i64> %1617, <i64 2, i64 2>
  %1619 = trunc <2 x i64> %1618 to <2 x i32>
  %1620 = or i64 %1587, 6
  %1621 = getelementptr inbounds i32, i32* %1, i64 %1620
  %1622 = bitcast i32* %1621 to <2 x i32>*
  store <2 x i32> %1619, <2 x i32>* %1622, align 4
  %1623 = load <2 x i64>, <2 x i64>* %1530, align 16
  %1624 = add nsw <2 x i64> %1623, <i64 1, i64 1>
  %1625 = lshr <2 x i64> %1623, <i64 63, i64 63>
  %1626 = add nsw <2 x i64> %1624, %1625
  %1627 = lshr <2 x i64> %1626, <i64 2, i64 2>
  %1628 = trunc <2 x i64> %1627 to <2 x i32>
  %1629 = or i64 %1587, 8
  %1630 = getelementptr inbounds i32, i32* %1, i64 %1629
  %1631 = bitcast i32* %1630 to <2 x i32>*
  store <2 x i32> %1628, <2 x i32>* %1631, align 4
  %1632 = load <2 x i64>, <2 x i64>* %1532, align 16
  %1633 = add nsw <2 x i64> %1632, <i64 1, i64 1>
  %1634 = lshr <2 x i64> %1632, <i64 63, i64 63>
  %1635 = add nsw <2 x i64> %1633, %1634
  %1636 = lshr <2 x i64> %1635, <i64 2, i64 2>
  %1637 = trunc <2 x i64> %1636 to <2 x i32>
  %1638 = or i64 %1587, 10
  %1639 = getelementptr inbounds i32, i32* %1, i64 %1638
  %1640 = bitcast i32* %1639 to <2 x i32>*
  store <2 x i32> %1637, <2 x i32>* %1640, align 4
  %1641 = load <2 x i64>, <2 x i64>* %1534, align 16
  %1642 = add nsw <2 x i64> %1641, <i64 1, i64 1>
  %1643 = lshr <2 x i64> %1641, <i64 63, i64 63>
  %1644 = add nsw <2 x i64> %1642, %1643
  %1645 = lshr <2 x i64> %1644, <i64 2, i64 2>
  %1646 = trunc <2 x i64> %1645 to <2 x i32>
  %1647 = or i64 %1587, 12
  %1648 = getelementptr inbounds i32, i32* %1, i64 %1647
  %1649 = bitcast i32* %1648 to <2 x i32>*
  store <2 x i32> %1646, <2 x i32>* %1649, align 4
  %1650 = load <2 x i64>, <2 x i64>* %1536, align 16
  %1651 = add nsw <2 x i64> %1650, <i64 1, i64 1>
  %1652 = lshr <2 x i64> %1650, <i64 63, i64 63>
  %1653 = add nsw <2 x i64> %1651, %1652
  %1654 = lshr <2 x i64> %1653, <i64 2, i64 2>
  %1655 = trunc <2 x i64> %1654 to <2 x i32>
  %1656 = or i64 %1587, 14
  %1657 = getelementptr inbounds i32, i32* %1, i64 %1656
  %1658 = bitcast i32* %1657 to <2 x i32>*
  store <2 x i32> %1655, <2 x i32>* %1658, align 4
  %1659 = load <2 x i64>, <2 x i64>* %1538, align 16
  %1660 = add nsw <2 x i64> %1659, <i64 1, i64 1>
  %1661 = lshr <2 x i64> %1659, <i64 63, i64 63>
  %1662 = add nsw <2 x i64> %1660, %1661
  %1663 = lshr <2 x i64> %1662, <i64 2, i64 2>
  %1664 = trunc <2 x i64> %1663 to <2 x i32>
  %1665 = or i64 %1587, 16
  %1666 = getelementptr inbounds i32, i32* %1, i64 %1665
  %1667 = bitcast i32* %1666 to <2 x i32>*
  store <2 x i32> %1664, <2 x i32>* %1667, align 4
  %1668 = load <2 x i64>, <2 x i64>* %1540, align 16
  %1669 = add nsw <2 x i64> %1668, <i64 1, i64 1>
  %1670 = lshr <2 x i64> %1668, <i64 63, i64 63>
  %1671 = add nsw <2 x i64> %1669, %1670
  %1672 = lshr <2 x i64> %1671, <i64 2, i64 2>
  %1673 = trunc <2 x i64> %1672 to <2 x i32>
  %1674 = or i64 %1587, 18
  %1675 = getelementptr inbounds i32, i32* %1, i64 %1674
  %1676 = bitcast i32* %1675 to <2 x i32>*
  store <2 x i32> %1673, <2 x i32>* %1676, align 4
  %1677 = load <2 x i64>, <2 x i64>* %1542, align 16
  %1678 = add nsw <2 x i64> %1677, <i64 1, i64 1>
  %1679 = lshr <2 x i64> %1677, <i64 63, i64 63>
  %1680 = add nsw <2 x i64> %1678, %1679
  %1681 = lshr <2 x i64> %1680, <i64 2, i64 2>
  %1682 = trunc <2 x i64> %1681 to <2 x i32>
  %1683 = or i64 %1587, 20
  %1684 = getelementptr inbounds i32, i32* %1, i64 %1683
  %1685 = bitcast i32* %1684 to <2 x i32>*
  store <2 x i32> %1682, <2 x i32>* %1685, align 4
  %1686 = load <2 x i64>, <2 x i64>* %1544, align 16
  %1687 = add nsw <2 x i64> %1686, <i64 1, i64 1>
  %1688 = lshr <2 x i64> %1686, <i64 63, i64 63>
  %1689 = add nsw <2 x i64> %1687, %1688
  %1690 = lshr <2 x i64> %1689, <i64 2, i64 2>
  %1691 = trunc <2 x i64> %1690 to <2 x i32>
  %1692 = or i64 %1587, 22
  %1693 = getelementptr inbounds i32, i32* %1, i64 %1692
  %1694 = bitcast i32* %1693 to <2 x i32>*
  store <2 x i32> %1691, <2 x i32>* %1694, align 4
  %1695 = load <2 x i64>, <2 x i64>* %1546, align 16
  %1696 = add nsw <2 x i64> %1695, <i64 1, i64 1>
  %1697 = lshr <2 x i64> %1695, <i64 63, i64 63>
  %1698 = add nsw <2 x i64> %1696, %1697
  %1699 = lshr <2 x i64> %1698, <i64 2, i64 2>
  %1700 = trunc <2 x i64> %1699 to <2 x i32>
  %1701 = or i64 %1587, 24
  %1702 = getelementptr inbounds i32, i32* %1, i64 %1701
  %1703 = bitcast i32* %1702 to <2 x i32>*
  store <2 x i32> %1700, <2 x i32>* %1703, align 4
  %1704 = load <2 x i64>, <2 x i64>* %1548, align 16
  %1705 = add nsw <2 x i64> %1704, <i64 1, i64 1>
  %1706 = lshr <2 x i64> %1704, <i64 63, i64 63>
  %1707 = add nsw <2 x i64> %1705, %1706
  %1708 = lshr <2 x i64> %1707, <i64 2, i64 2>
  %1709 = trunc <2 x i64> %1708 to <2 x i32>
  %1710 = or i64 %1587, 26
  %1711 = getelementptr inbounds i32, i32* %1, i64 %1710
  %1712 = bitcast i32* %1711 to <2 x i32>*
  store <2 x i32> %1709, <2 x i32>* %1712, align 4
  %1713 = load <2 x i64>, <2 x i64>* %1550, align 16
  %1714 = add nsw <2 x i64> %1713, <i64 1, i64 1>
  %1715 = lshr <2 x i64> %1713, <i64 63, i64 63>
  %1716 = add nsw <2 x i64> %1714, %1715
  %1717 = lshr <2 x i64> %1716, <i64 2, i64 2>
  %1718 = trunc <2 x i64> %1717 to <2 x i32>
  %1719 = or i64 %1587, 28
  %1720 = getelementptr inbounds i32, i32* %1, i64 %1719
  %1721 = bitcast i32* %1720 to <2 x i32>*
  store <2 x i32> %1718, <2 x i32>* %1721, align 4
  %1722 = load <2 x i64>, <2 x i64>* %1552, align 16
  %1723 = add nsw <2 x i64> %1722, <i64 1, i64 1>
  %1724 = lshr <2 x i64> %1722, <i64 63, i64 63>
  %1725 = add nsw <2 x i64> %1723, %1724
  %1726 = lshr <2 x i64> %1725, <i64 2, i64 2>
  %1727 = trunc <2 x i64> %1726 to <2 x i32>
  %1728 = or i64 %1587, 30
  %1729 = getelementptr inbounds i32, i32* %1, i64 %1728
  %1730 = bitcast i32* %1729 to <2 x i32>*
  store <2 x i32> %1727, <2 x i32>* %1730, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %1519) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %1518) #6
  %1731 = add nuw nsw i64 %1554, 1
  %1732 = icmp eq i64 %1731, 32
  br i1 %1732, label %11292, label %1553

1733:                                             ; preds = %1510
  %1734 = shufflevector <8 x i16> %1398, <8 x i16> %1405, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1735 = shufflevector <8 x i16> %1398, <8 x i16> %1405, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1736 = shufflevector <8 x i16> %1399, <8 x i16> %1404, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1737 = shufflevector <8 x i16> %1399, <8 x i16> %1404, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1738 = shufflevector <8 x i16> %1400, <8 x i16> %1403, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1739 = shufflevector <8 x i16> %1400, <8 x i16> %1403, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1740 = shufflevector <8 x i16> %1401, <8 x i16> %1402, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1741 = shufflevector <8 x i16> %1401, <8 x i16> %1402, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1742 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1734, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %1743 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1735, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %1744 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1736, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %1745 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1737, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %1746 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1738, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %1747 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1739, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %1748 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1740, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %1749 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1741, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %1750 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1740, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %1751 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1741, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %1752 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1738, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %1753 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1739, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %1754 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1736, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %1755 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1737, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %1756 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1734, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %1757 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %1735, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %1758 = add <4 x i32> %1742, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1759 = add <4 x i32> %1743, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1760 = add <4 x i32> %1744, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1761 = add <4 x i32> %1745, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1762 = add <4 x i32> %1746, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1763 = add <4 x i32> %1747, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1764 = add <4 x i32> %1748, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1765 = add <4 x i32> %1749, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1766 = add <4 x i32> %1750, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1767 = add <4 x i32> %1751, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1768 = add <4 x i32> %1752, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1769 = add <4 x i32> %1753, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1770 = add <4 x i32> %1754, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1771 = add <4 x i32> %1755, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1772 = add <4 x i32> %1756, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1773 = add <4 x i32> %1757, <i32 8192, i32 8192, i32 8192, i32 8192>
  %1774 = ashr <4 x i32> %1758, <i32 14, i32 14, i32 14, i32 14>
  %1775 = ashr <4 x i32> %1759, <i32 14, i32 14, i32 14, i32 14>
  %1776 = ashr <4 x i32> %1760, <i32 14, i32 14, i32 14, i32 14>
  %1777 = ashr <4 x i32> %1761, <i32 14, i32 14, i32 14, i32 14>
  %1778 = ashr <4 x i32> %1762, <i32 14, i32 14, i32 14, i32 14>
  %1779 = ashr <4 x i32> %1763, <i32 14, i32 14, i32 14, i32 14>
  %1780 = ashr <4 x i32> %1764, <i32 14, i32 14, i32 14, i32 14>
  %1781 = ashr <4 x i32> %1765, <i32 14, i32 14, i32 14, i32 14>
  %1782 = ashr <4 x i32> %1766, <i32 14, i32 14, i32 14, i32 14>
  %1783 = ashr <4 x i32> %1767, <i32 14, i32 14, i32 14, i32 14>
  %1784 = ashr <4 x i32> %1768, <i32 14, i32 14, i32 14, i32 14>
  %1785 = ashr <4 x i32> %1769, <i32 14, i32 14, i32 14, i32 14>
  %1786 = ashr <4 x i32> %1770, <i32 14, i32 14, i32 14, i32 14>
  %1787 = ashr <4 x i32> %1771, <i32 14, i32 14, i32 14, i32 14>
  %1788 = ashr <4 x i32> %1772, <i32 14, i32 14, i32 14, i32 14>
  %1789 = ashr <4 x i32> %1773, <i32 14, i32 14, i32 14, i32 14>
  %1790 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1774, <4 x i32> %1775) #6
  %1791 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1776, <4 x i32> %1777) #6
  %1792 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1778, <4 x i32> %1779) #6
  %1793 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1780, <4 x i32> %1781) #6
  %1794 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1782, <4 x i32> %1783) #6
  %1795 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1784, <4 x i32> %1785) #6
  %1796 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1786, <4 x i32> %1787) #6
  %1797 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %1788, <4 x i32> %1789) #6
  %1798 = add <8 x i16> %1790, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1799 = icmp ult <8 x i16> %1798, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1800 = add <8 x i16> %1791, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1801 = icmp ult <8 x i16> %1800, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1802 = add <8 x i16> %1792, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1803 = icmp ult <8 x i16> %1802, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1804 = add <8 x i16> %1793, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1805 = icmp ult <8 x i16> %1804, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1806 = or <8 x i1> %1801, %1799
  %1807 = or <8 x i1> %1806, %1803
  %1808 = or <8 x i1> %1807, %1805
  %1809 = sext <8 x i1> %1808 to <8 x i16>
  %1810 = bitcast <8 x i16> %1809 to <16 x i8>
  %1811 = icmp slt <16 x i8> %1810, zeroinitializer
  %1812 = bitcast <16 x i1> %1811 to i16
  %1813 = zext i16 %1812 to i32
  %1814 = add <8 x i16> %1794, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1815 = icmp ult <8 x i16> %1814, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1816 = add <8 x i16> %1795, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1817 = icmp ult <8 x i16> %1816, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1818 = add <8 x i16> %1796, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1819 = icmp ult <8 x i16> %1818, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1820 = add <8 x i16> %1797, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %1821 = icmp ult <8 x i16> %1820, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1822 = or <8 x i1> %1817, %1815
  %1823 = or <8 x i1> %1822, %1819
  %1824 = or <8 x i1> %1823, %1821
  %1825 = sext <8 x i1> %1824 to <8 x i16>
  %1826 = bitcast <8 x i16> %1825 to <16 x i8>
  %1827 = icmp slt <16 x i8> %1826, zeroinitializer
  %1828 = bitcast <16 x i1> %1827 to i16
  %1829 = zext i16 %1828 to i32
  %1830 = sub nsw i32 0, %1813
  %1831 = icmp eq i32 %1829, %1830
  br i1 %1831, label %2050, label %1832

1832:                                             ; preds = %1733
  br i1 %97, label %1833, label %1834

1833:                                             ; preds = %1832
  tail call void @vpx_highbd_fdct32x32_c(i16* %0, i32* %1, i32 %2) #6
  br label %11292

1834:                                             ; preds = %1832
  %1835 = bitcast [32 x i64]* %4 to i8*
  %1836 = bitcast [32 x i64]* %5 to i8*
  %1837 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %1838 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %1839 = bitcast [32 x i64]* %5 to <2 x i64>*
  %1840 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %1841 = bitcast i64* %1840 to <2 x i64>*
  %1842 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %1843 = bitcast i64* %1842 to <2 x i64>*
  %1844 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %1845 = bitcast i64* %1844 to <2 x i64>*
  %1846 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %1847 = bitcast i64* %1846 to <2 x i64>*
  %1848 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %1849 = bitcast i64* %1848 to <2 x i64>*
  %1850 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %1851 = bitcast i64* %1850 to <2 x i64>*
  %1852 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %1853 = bitcast i64* %1852 to <2 x i64>*
  %1854 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %1855 = bitcast i64* %1854 to <2 x i64>*
  %1856 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %1857 = bitcast i64* %1856 to <2 x i64>*
  %1858 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %1859 = bitcast i64* %1858 to <2 x i64>*
  %1860 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %1861 = bitcast i64* %1860 to <2 x i64>*
  %1862 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %1863 = bitcast i64* %1862 to <2 x i64>*
  %1864 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %1865 = bitcast i64* %1864 to <2 x i64>*
  %1866 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %1867 = bitcast i64* %1866 to <2 x i64>*
  %1868 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %1869 = bitcast i64* %1868 to <2 x i64>*
  br label %1870

1870:                                             ; preds = %1903, %1834
  %1871 = phi i64 [ 0, %1834 ], [ %2048, %1903 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %1835) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %1835, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %1836) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %1836, i8 -86, i64 256, i1 false) #6
  br label %1872

1872:                                             ; preds = %1872, %1870
  %1873 = phi i64 [ 0, %1870 ], [ %1901, %1872 ]
  %1874 = shl i64 %1873, 5
  %1875 = add nuw nsw i64 %1874, %1871
  %1876 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %1875
  %1877 = load i16, i16* %1876, align 2
  %1878 = sext i16 %1877 to i64
  %1879 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %1873
  store i64 %1878, i64* %1879, align 16
  %1880 = or i64 %1873, 1
  %1881 = shl i64 %1880, 5
  %1882 = add nuw nsw i64 %1881, %1871
  %1883 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %1882
  %1884 = load i16, i16* %1883, align 2
  %1885 = sext i16 %1884 to i64
  %1886 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %1880
  store i64 %1885, i64* %1886, align 8
  %1887 = or i64 %1873, 2
  %1888 = shl i64 %1887, 5
  %1889 = add nuw nsw i64 %1888, %1871
  %1890 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %1889
  %1891 = load i16, i16* %1890, align 2
  %1892 = sext i16 %1891 to i64
  %1893 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %1887
  store i64 %1892, i64* %1893, align 16
  %1894 = or i64 %1873, 3
  %1895 = shl i64 %1894, 5
  %1896 = add nuw nsw i64 %1895, %1871
  %1897 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %1896
  %1898 = load i16, i16* %1897, align 2
  %1899 = sext i16 %1898 to i64
  %1900 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %1894
  store i64 %1899, i64* %1900, align 8
  %1901 = add nuw nsw i64 %1873, 4
  %1902 = icmp eq i64 %1901, 32
  br i1 %1902, label %1903, label %1872

1903:                                             ; preds = %1872
  call void @vpx_fdct32(i64* nonnull %1837, i64* nonnull %1838, i32 0) #6
  %1904 = shl i64 %1871, 5
  %1905 = load <2 x i64>, <2 x i64>* %1839, align 16
  %1906 = add nsw <2 x i64> %1905, <i64 1, i64 1>
  %1907 = lshr <2 x i64> %1905, <i64 63, i64 63>
  %1908 = add nsw <2 x i64> %1906, %1907
  %1909 = lshr <2 x i64> %1908, <i64 2, i64 2>
  %1910 = trunc <2 x i64> %1909 to <2 x i32>
  %1911 = getelementptr inbounds i32, i32* %1, i64 %1904
  %1912 = bitcast i32* %1911 to <2 x i32>*
  store <2 x i32> %1910, <2 x i32>* %1912, align 4
  %1913 = load <2 x i64>, <2 x i64>* %1841, align 16
  %1914 = add nsw <2 x i64> %1913, <i64 1, i64 1>
  %1915 = lshr <2 x i64> %1913, <i64 63, i64 63>
  %1916 = add nsw <2 x i64> %1914, %1915
  %1917 = lshr <2 x i64> %1916, <i64 2, i64 2>
  %1918 = trunc <2 x i64> %1917 to <2 x i32>
  %1919 = or i64 %1904, 2
  %1920 = getelementptr inbounds i32, i32* %1, i64 %1919
  %1921 = bitcast i32* %1920 to <2 x i32>*
  store <2 x i32> %1918, <2 x i32>* %1921, align 4
  %1922 = load <2 x i64>, <2 x i64>* %1843, align 16
  %1923 = add nsw <2 x i64> %1922, <i64 1, i64 1>
  %1924 = lshr <2 x i64> %1922, <i64 63, i64 63>
  %1925 = add nsw <2 x i64> %1923, %1924
  %1926 = lshr <2 x i64> %1925, <i64 2, i64 2>
  %1927 = trunc <2 x i64> %1926 to <2 x i32>
  %1928 = or i64 %1904, 4
  %1929 = getelementptr inbounds i32, i32* %1, i64 %1928
  %1930 = bitcast i32* %1929 to <2 x i32>*
  store <2 x i32> %1927, <2 x i32>* %1930, align 4
  %1931 = load <2 x i64>, <2 x i64>* %1845, align 16
  %1932 = add nsw <2 x i64> %1931, <i64 1, i64 1>
  %1933 = lshr <2 x i64> %1931, <i64 63, i64 63>
  %1934 = add nsw <2 x i64> %1932, %1933
  %1935 = lshr <2 x i64> %1934, <i64 2, i64 2>
  %1936 = trunc <2 x i64> %1935 to <2 x i32>
  %1937 = or i64 %1904, 6
  %1938 = getelementptr inbounds i32, i32* %1, i64 %1937
  %1939 = bitcast i32* %1938 to <2 x i32>*
  store <2 x i32> %1936, <2 x i32>* %1939, align 4
  %1940 = load <2 x i64>, <2 x i64>* %1847, align 16
  %1941 = add nsw <2 x i64> %1940, <i64 1, i64 1>
  %1942 = lshr <2 x i64> %1940, <i64 63, i64 63>
  %1943 = add nsw <2 x i64> %1941, %1942
  %1944 = lshr <2 x i64> %1943, <i64 2, i64 2>
  %1945 = trunc <2 x i64> %1944 to <2 x i32>
  %1946 = or i64 %1904, 8
  %1947 = getelementptr inbounds i32, i32* %1, i64 %1946
  %1948 = bitcast i32* %1947 to <2 x i32>*
  store <2 x i32> %1945, <2 x i32>* %1948, align 4
  %1949 = load <2 x i64>, <2 x i64>* %1849, align 16
  %1950 = add nsw <2 x i64> %1949, <i64 1, i64 1>
  %1951 = lshr <2 x i64> %1949, <i64 63, i64 63>
  %1952 = add nsw <2 x i64> %1950, %1951
  %1953 = lshr <2 x i64> %1952, <i64 2, i64 2>
  %1954 = trunc <2 x i64> %1953 to <2 x i32>
  %1955 = or i64 %1904, 10
  %1956 = getelementptr inbounds i32, i32* %1, i64 %1955
  %1957 = bitcast i32* %1956 to <2 x i32>*
  store <2 x i32> %1954, <2 x i32>* %1957, align 4
  %1958 = load <2 x i64>, <2 x i64>* %1851, align 16
  %1959 = add nsw <2 x i64> %1958, <i64 1, i64 1>
  %1960 = lshr <2 x i64> %1958, <i64 63, i64 63>
  %1961 = add nsw <2 x i64> %1959, %1960
  %1962 = lshr <2 x i64> %1961, <i64 2, i64 2>
  %1963 = trunc <2 x i64> %1962 to <2 x i32>
  %1964 = or i64 %1904, 12
  %1965 = getelementptr inbounds i32, i32* %1, i64 %1964
  %1966 = bitcast i32* %1965 to <2 x i32>*
  store <2 x i32> %1963, <2 x i32>* %1966, align 4
  %1967 = load <2 x i64>, <2 x i64>* %1853, align 16
  %1968 = add nsw <2 x i64> %1967, <i64 1, i64 1>
  %1969 = lshr <2 x i64> %1967, <i64 63, i64 63>
  %1970 = add nsw <2 x i64> %1968, %1969
  %1971 = lshr <2 x i64> %1970, <i64 2, i64 2>
  %1972 = trunc <2 x i64> %1971 to <2 x i32>
  %1973 = or i64 %1904, 14
  %1974 = getelementptr inbounds i32, i32* %1, i64 %1973
  %1975 = bitcast i32* %1974 to <2 x i32>*
  store <2 x i32> %1972, <2 x i32>* %1975, align 4
  %1976 = load <2 x i64>, <2 x i64>* %1855, align 16
  %1977 = add nsw <2 x i64> %1976, <i64 1, i64 1>
  %1978 = lshr <2 x i64> %1976, <i64 63, i64 63>
  %1979 = add nsw <2 x i64> %1977, %1978
  %1980 = lshr <2 x i64> %1979, <i64 2, i64 2>
  %1981 = trunc <2 x i64> %1980 to <2 x i32>
  %1982 = or i64 %1904, 16
  %1983 = getelementptr inbounds i32, i32* %1, i64 %1982
  %1984 = bitcast i32* %1983 to <2 x i32>*
  store <2 x i32> %1981, <2 x i32>* %1984, align 4
  %1985 = load <2 x i64>, <2 x i64>* %1857, align 16
  %1986 = add nsw <2 x i64> %1985, <i64 1, i64 1>
  %1987 = lshr <2 x i64> %1985, <i64 63, i64 63>
  %1988 = add nsw <2 x i64> %1986, %1987
  %1989 = lshr <2 x i64> %1988, <i64 2, i64 2>
  %1990 = trunc <2 x i64> %1989 to <2 x i32>
  %1991 = or i64 %1904, 18
  %1992 = getelementptr inbounds i32, i32* %1, i64 %1991
  %1993 = bitcast i32* %1992 to <2 x i32>*
  store <2 x i32> %1990, <2 x i32>* %1993, align 4
  %1994 = load <2 x i64>, <2 x i64>* %1859, align 16
  %1995 = add nsw <2 x i64> %1994, <i64 1, i64 1>
  %1996 = lshr <2 x i64> %1994, <i64 63, i64 63>
  %1997 = add nsw <2 x i64> %1995, %1996
  %1998 = lshr <2 x i64> %1997, <i64 2, i64 2>
  %1999 = trunc <2 x i64> %1998 to <2 x i32>
  %2000 = or i64 %1904, 20
  %2001 = getelementptr inbounds i32, i32* %1, i64 %2000
  %2002 = bitcast i32* %2001 to <2 x i32>*
  store <2 x i32> %1999, <2 x i32>* %2002, align 4
  %2003 = load <2 x i64>, <2 x i64>* %1861, align 16
  %2004 = add nsw <2 x i64> %2003, <i64 1, i64 1>
  %2005 = lshr <2 x i64> %2003, <i64 63, i64 63>
  %2006 = add nsw <2 x i64> %2004, %2005
  %2007 = lshr <2 x i64> %2006, <i64 2, i64 2>
  %2008 = trunc <2 x i64> %2007 to <2 x i32>
  %2009 = or i64 %1904, 22
  %2010 = getelementptr inbounds i32, i32* %1, i64 %2009
  %2011 = bitcast i32* %2010 to <2 x i32>*
  store <2 x i32> %2008, <2 x i32>* %2011, align 4
  %2012 = load <2 x i64>, <2 x i64>* %1863, align 16
  %2013 = add nsw <2 x i64> %2012, <i64 1, i64 1>
  %2014 = lshr <2 x i64> %2012, <i64 63, i64 63>
  %2015 = add nsw <2 x i64> %2013, %2014
  %2016 = lshr <2 x i64> %2015, <i64 2, i64 2>
  %2017 = trunc <2 x i64> %2016 to <2 x i32>
  %2018 = or i64 %1904, 24
  %2019 = getelementptr inbounds i32, i32* %1, i64 %2018
  %2020 = bitcast i32* %2019 to <2 x i32>*
  store <2 x i32> %2017, <2 x i32>* %2020, align 4
  %2021 = load <2 x i64>, <2 x i64>* %1865, align 16
  %2022 = add nsw <2 x i64> %2021, <i64 1, i64 1>
  %2023 = lshr <2 x i64> %2021, <i64 63, i64 63>
  %2024 = add nsw <2 x i64> %2022, %2023
  %2025 = lshr <2 x i64> %2024, <i64 2, i64 2>
  %2026 = trunc <2 x i64> %2025 to <2 x i32>
  %2027 = or i64 %1904, 26
  %2028 = getelementptr inbounds i32, i32* %1, i64 %2027
  %2029 = bitcast i32* %2028 to <2 x i32>*
  store <2 x i32> %2026, <2 x i32>* %2029, align 4
  %2030 = load <2 x i64>, <2 x i64>* %1867, align 16
  %2031 = add nsw <2 x i64> %2030, <i64 1, i64 1>
  %2032 = lshr <2 x i64> %2030, <i64 63, i64 63>
  %2033 = add nsw <2 x i64> %2031, %2032
  %2034 = lshr <2 x i64> %2033, <i64 2, i64 2>
  %2035 = trunc <2 x i64> %2034 to <2 x i32>
  %2036 = or i64 %1904, 28
  %2037 = getelementptr inbounds i32, i32* %1, i64 %2036
  %2038 = bitcast i32* %2037 to <2 x i32>*
  store <2 x i32> %2035, <2 x i32>* %2038, align 4
  %2039 = load <2 x i64>, <2 x i64>* %1869, align 16
  %2040 = add nsw <2 x i64> %2039, <i64 1, i64 1>
  %2041 = lshr <2 x i64> %2039, <i64 63, i64 63>
  %2042 = add nsw <2 x i64> %2040, %2041
  %2043 = lshr <2 x i64> %2042, <i64 2, i64 2>
  %2044 = trunc <2 x i64> %2043 to <2 x i32>
  %2045 = or i64 %1904, 30
  %2046 = getelementptr inbounds i32, i32* %1, i64 %2045
  %2047 = bitcast i32* %2046 to <2 x i32>*
  store <2 x i32> %2044, <2 x i32>* %2047, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %1836) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %1835) #6
  %2048 = add nuw nsw i64 %1871, 1
  %2049 = icmp eq i64 %2048, 32
  br i1 %2049, label %11292, label %1870

2050:                                             ; preds = %1733
  br i1 %97, label %2051, label %3249

2051:                                             ; preds = %2050
  %2052 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1433, <8 x i16> %1426) #6
  %2053 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1432, <8 x i16> %1427) #6
  %2054 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1431, <8 x i16> %1428) #6
  %2055 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1430, <8 x i16> %1429) #6
  %2056 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1429, <8 x i16> %1430) #6
  %2057 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1428, <8 x i16> %1431) #6
  %2058 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1427, <8 x i16> %1432) #6
  %2059 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1426, <8 x i16> %1433) #6
  %2060 = add <8 x i16> %2052, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2061 = icmp ult <8 x i16> %2060, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2062 = add <8 x i16> %2053, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2063 = icmp ult <8 x i16> %2062, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2064 = add <8 x i16> %2054, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2065 = icmp ult <8 x i16> %2064, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2066 = add <8 x i16> %2055, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2067 = icmp ult <8 x i16> %2066, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2068 = or <8 x i1> %2065, %2067
  %2069 = or <8 x i1> %2068, %2063
  %2070 = or <8 x i1> %2069, %2061
  %2071 = sext <8 x i1> %2070 to <8 x i16>
  %2072 = bitcast <8 x i16> %2071 to <16 x i8>
  %2073 = icmp slt <16 x i8> %2072, zeroinitializer
  %2074 = bitcast <16 x i1> %2073 to i16
  %2075 = zext i16 %2074 to i32
  %2076 = add <8 x i16> %2056, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2077 = icmp ult <8 x i16> %2076, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2078 = add <8 x i16> %2057, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2079 = icmp ult <8 x i16> %2078, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2080 = add <8 x i16> %2058, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2081 = icmp ult <8 x i16> %2080, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2082 = add <8 x i16> %2059, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2083 = icmp ult <8 x i16> %2082, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2084 = or <8 x i1> %2079, %2077
  %2085 = or <8 x i1> %2084, %2081
  %2086 = or <8 x i1> %2085, %2083
  %2087 = sext <8 x i1> %2086 to <8 x i16>
  %2088 = bitcast <8 x i16> %2087 to <16 x i8>
  %2089 = icmp slt <16 x i8> %2088, zeroinitializer
  %2090 = bitcast <16 x i1> %2089 to i16
  %2091 = zext i16 %2090 to i32
  %2092 = sub nsw i32 0, %2075
  %2093 = icmp eq i32 %2091, %2092
  br i1 %2093, label %2095, label %2094

2094:                                             ; preds = %2051
  tail call void @vpx_highbd_fdct32x32_c(i16* %0, i32* %1, i32 %2) #6
  br label %11292

2095:                                             ; preds = %2051
  %2096 = shufflevector <8 x i16> %1439, <8 x i16> %1436, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2097 = shufflevector <8 x i16> %1439, <8 x i16> %1436, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2098 = shufflevector <8 x i16> %1438, <8 x i16> %1437, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2099 = shufflevector <8 x i16> %1438, <8 x i16> %1437, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2100 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2096, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %2101 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2097, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %2102 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2098, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %2103 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2099, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %2104 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2098, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %2105 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2099, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %2106 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2096, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %2107 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2097, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %2108 = add <4 x i32> %2100, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2109 = add <4 x i32> %2101, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2110 = add <4 x i32> %2102, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2111 = add <4 x i32> %2103, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2112 = add <4 x i32> %2104, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2113 = add <4 x i32> %2105, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2114 = add <4 x i32> %2106, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2115 = add <4 x i32> %2107, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2116 = ashr <4 x i32> %2108, <i32 14, i32 14, i32 14, i32 14>
  %2117 = ashr <4 x i32> %2109, <i32 14, i32 14, i32 14, i32 14>
  %2118 = ashr <4 x i32> %2110, <i32 14, i32 14, i32 14, i32 14>
  %2119 = ashr <4 x i32> %2111, <i32 14, i32 14, i32 14, i32 14>
  %2120 = ashr <4 x i32> %2112, <i32 14, i32 14, i32 14, i32 14>
  %2121 = ashr <4 x i32> %2113, <i32 14, i32 14, i32 14, i32 14>
  %2122 = ashr <4 x i32> %2114, <i32 14, i32 14, i32 14, i32 14>
  %2123 = ashr <4 x i32> %2115, <i32 14, i32 14, i32 14, i32 14>
  %2124 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2116, <4 x i32> %2117) #6
  %2125 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2118, <4 x i32> %2119) #6
  %2126 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2120, <4 x i32> %2121) #6
  %2127 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2122, <4 x i32> %2123) #6
  %2128 = add <8 x i16> %2124, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2129 = icmp ult <8 x i16> %2128, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2130 = add <8 x i16> %2125, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2131 = icmp ult <8 x i16> %2130, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2132 = add <8 x i16> %2126, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2133 = icmp ult <8 x i16> %2132, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2134 = add <8 x i16> %2127, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2135 = icmp ult <8 x i16> %2134, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2136 = or <8 x i1> %2131, %2129
  %2137 = or <8 x i1> %2136, %2133
  %2138 = or <8 x i1> %2137, %2135
  %2139 = sext <8 x i1> %2138 to <8 x i16>
  %2140 = bitcast <8 x i16> %2139 to <16 x i8>
  %2141 = icmp slt <16 x i8> %2140, zeroinitializer
  %2142 = bitcast <16 x i1> %2141 to i16
  %2143 = icmp eq i16 %2142, 0
  br i1 %2143, label %2145, label %2144

2144:                                             ; preds = %2095
  tail call void @vpx_highbd_fdct32x32_c(i16* %0, i32* %1, i32 %2) #6
  br label %11292

2145:                                             ; preds = %2095
  %2146 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1793, <8 x i16> %1409) #6
  %2147 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1792, <8 x i16> %1408) #6
  %2148 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1791, <8 x i16> %1407) #6
  %2149 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1790, <8 x i16> %1406) #6
  %2150 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1406, <8 x i16> %1790) #6
  %2151 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1407, <8 x i16> %1791) #6
  %2152 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1408, <8 x i16> %1792) #6
  %2153 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1409, <8 x i16> %1793) #6
  %2154 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1394, <8 x i16> %1794) #6
  %2155 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1395, <8 x i16> %1795) #6
  %2156 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1396, <8 x i16> %1796) #6
  %2157 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1397, <8 x i16> %1797) #6
  %2158 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1797, <8 x i16> %1397) #6
  %2159 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1796, <8 x i16> %1396) #6
  %2160 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1795, <8 x i16> %1395) #6
  %2161 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %1794, <8 x i16> %1394) #6
  %2162 = add <8 x i16> %2146, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2163 = icmp ult <8 x i16> %2162, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2164 = add <8 x i16> %2147, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2165 = icmp ult <8 x i16> %2164, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2166 = add <8 x i16> %2148, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2167 = icmp ult <8 x i16> %2166, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2168 = add <8 x i16> %2149, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2169 = icmp ult <8 x i16> %2168, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2170 = or <8 x i1> %2167, %2169
  %2171 = or <8 x i1> %2170, %2165
  %2172 = or <8 x i1> %2171, %2163
  %2173 = sext <8 x i1> %2172 to <8 x i16>
  %2174 = bitcast <8 x i16> %2173 to <16 x i8>
  %2175 = icmp slt <16 x i8> %2174, zeroinitializer
  %2176 = bitcast <16 x i1> %2175 to i16
  %2177 = zext i16 %2176 to i32
  %2178 = add <8 x i16> %2150, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2179 = icmp ult <8 x i16> %2178, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2180 = add <8 x i16> %2151, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2181 = icmp ult <8 x i16> %2180, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2182 = add <8 x i16> %2152, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2183 = icmp ult <8 x i16> %2182, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2184 = add <8 x i16> %2153, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2185 = icmp ult <8 x i16> %2184, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2186 = or <8 x i1> %2181, %2179
  %2187 = or <8 x i1> %2186, %2183
  %2188 = or <8 x i1> %2187, %2185
  %2189 = sext <8 x i1> %2188 to <8 x i16>
  %2190 = bitcast <8 x i16> %2189 to <16 x i8>
  %2191 = icmp slt <16 x i8> %2190, zeroinitializer
  %2192 = bitcast <16 x i1> %2191 to i16
  %2193 = zext i16 %2192 to i32
  %2194 = icmp eq i16 %2176, 0
  br i1 %2194, label %2195, label %2230

2195:                                             ; preds = %2145
  %2196 = add <8 x i16> %2154, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2197 = icmp ult <8 x i16> %2196, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2198 = add <8 x i16> %2155, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2199 = icmp ult <8 x i16> %2198, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2200 = add <8 x i16> %2156, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2201 = icmp ult <8 x i16> %2200, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2202 = add <8 x i16> %2157, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2203 = icmp ult <8 x i16> %2202, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2204 = or <8 x i1> %2199, %2197
  %2205 = or <8 x i1> %2204, %2201
  %2206 = or <8 x i1> %2205, %2203
  %2207 = sext <8 x i1> %2206 to <8 x i16>
  %2208 = bitcast <8 x i16> %2207 to <16 x i8>
  %2209 = icmp slt <16 x i8> %2208, zeroinitializer
  %2210 = bitcast <16 x i1> %2209 to i16
  %2211 = zext i16 %2210 to i32
  %2212 = icmp eq i16 %2192, 0
  br i1 %2212, label %2213, label %2230

2213:                                             ; preds = %2195
  %2214 = add <8 x i16> %2158, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2215 = icmp ult <8 x i16> %2214, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2216 = add <8 x i16> %2159, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2217 = icmp ult <8 x i16> %2216, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2218 = add <8 x i16> %2160, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2219 = icmp ult <8 x i16> %2218, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2220 = add <8 x i16> %2161, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2221 = icmp ult <8 x i16> %2220, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2222 = or <8 x i1> %2219, %2221
  %2223 = or <8 x i1> %2222, %2217
  %2224 = or <8 x i1> %2223, %2215
  %2225 = sext <8 x i1> %2224 to <8 x i16>
  %2226 = bitcast <8 x i16> %2225 to <16 x i8>
  %2227 = icmp slt <16 x i8> %2226, zeroinitializer
  %2228 = bitcast <16 x i1> %2227 to i16
  %2229 = zext i16 %2228 to i32
  br label %2230

2230:                                             ; preds = %2145, %2195, %2213
  %2231 = phi i32 [ %2177, %2145 ], [ %2211, %2195 ], [ %2211, %2213 ]
  %2232 = phi i32 [ %2193, %2145 ], [ %2193, %2195 ], [ %2229, %2213 ]
  %2233 = sub nsw i32 0, %2231
  %2234 = icmp eq i32 %2232, %2233
  br i1 %2234, label %2236, label %2235

2235:                                             ; preds = %2230
  tail call void @vpx_highbd_fdct32x32_c(i16* %0, i32* %1, i32 %2) #6
  br label %11292

2236:                                             ; preds = %2230
  %2237 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2055, <8 x i16> %2052) #6
  %2238 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2054, <8 x i16> %2053) #6
  %2239 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2053, <8 x i16> %2054) #6
  %2240 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2052, <8 x i16> %2055) #6
  %2241 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2125, <8 x i16> %1434) #6
  %2242 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2124, <8 x i16> %1435) #6
  %2243 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1435, <8 x i16> %2124) #6
  %2244 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1434, <8 x i16> %2125) #6
  %2245 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1441, <8 x i16> %2126) #6
  %2246 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %1440, <8 x i16> %2127) #6
  %2247 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2127, <8 x i16> %1440) #6
  %2248 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2126, <8 x i16> %1441) #6
  %2249 = add <8 x i16> %2237, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2250 = icmp ult <8 x i16> %2249, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2251 = add <8 x i16> %2238, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2252 = icmp ult <8 x i16> %2251, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2253 = add <8 x i16> %2239, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2254 = icmp ult <8 x i16> %2253, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2255 = add <8 x i16> %2240, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2256 = icmp ult <8 x i16> %2255, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2257 = or <8 x i1> %2254, %2252
  %2258 = or <8 x i1> %2257, %2250
  %2259 = or <8 x i1> %2258, %2256
  %2260 = sext <8 x i1> %2259 to <8 x i16>
  %2261 = bitcast <8 x i16> %2260 to <16 x i8>
  %2262 = icmp slt <16 x i8> %2261, zeroinitializer
  %2263 = bitcast <16 x i1> %2262 to i16
  %2264 = zext i16 %2263 to i32
  %2265 = add <8 x i16> %1421, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2266 = icmp ult <8 x i16> %2265, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2267 = add <8 x i16> %1420, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2268 = icmp ult <8 x i16> %2267, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2269 = add <8 x i16> %1419, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2270 = icmp ult <8 x i16> %2269, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2271 = add <8 x i16> %1418, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2272 = icmp ult <8 x i16> %2271, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2273 = or <8 x i1> %2270, %2272
  %2274 = or <8 x i1> %2273, %2268
  %2275 = or <8 x i1> %2274, %2266
  %2276 = sext <8 x i1> %2275 to <8 x i16>
  %2277 = bitcast <8 x i16> %2276 to <16 x i8>
  %2278 = icmp slt <16 x i8> %2277, zeroinitializer
  %2279 = bitcast <16 x i1> %2278 to i16
  %2280 = zext i16 %2279 to i32
  %2281 = icmp eq i16 %2263, 0
  br i1 %2281, label %2282, label %2317

2282:                                             ; preds = %2236
  %2283 = add <8 x i16> %2241, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2284 = icmp ult <8 x i16> %2283, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2285 = add <8 x i16> %2242, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2286 = icmp ult <8 x i16> %2285, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2287 = add <8 x i16> %2243, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2288 = icmp ult <8 x i16> %2287, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2289 = add <8 x i16> %2244, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2290 = icmp ult <8 x i16> %2289, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2291 = or <8 x i1> %2288, %2286
  %2292 = or <8 x i1> %2291, %2284
  %2293 = or <8 x i1> %2292, %2290
  %2294 = sext <8 x i1> %2293 to <8 x i16>
  %2295 = bitcast <8 x i16> %2294 to <16 x i8>
  %2296 = icmp slt <16 x i8> %2295, zeroinitializer
  %2297 = bitcast <16 x i1> %2296 to i16
  %2298 = zext i16 %2297 to i32
  %2299 = icmp eq i16 %2279, 0
  br i1 %2299, label %2300, label %2317

2300:                                             ; preds = %2282
  %2301 = add <8 x i16> %2245, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2302 = icmp ult <8 x i16> %2301, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2303 = add <8 x i16> %2246, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2304 = icmp ult <8 x i16> %2303, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2305 = add <8 x i16> %2247, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2306 = icmp ult <8 x i16> %2305, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2307 = add <8 x i16> %2248, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2308 = icmp ult <8 x i16> %2307, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2309 = or <8 x i1> %2308, %2302
  %2310 = or <8 x i1> %2309, %2304
  %2311 = or <8 x i1> %2310, %2306
  %2312 = sext <8 x i1> %2311 to <8 x i16>
  %2313 = bitcast <8 x i16> %2312 to <16 x i8>
  %2314 = icmp slt <16 x i8> %2313, zeroinitializer
  %2315 = bitcast <16 x i1> %2314 to i16
  %2316 = zext i16 %2315 to i32
  br label %2317

2317:                                             ; preds = %2236, %2282, %2300
  %2318 = phi i32 [ %2264, %2236 ], [ %2298, %2282 ], [ %2298, %2300 ]
  %2319 = phi i32 [ %2280, %2236 ], [ %2280, %2282 ], [ %2316, %2300 ]
  %2320 = sub nsw i32 0, %2318
  %2321 = icmp eq i32 %2319, %2320
  br i1 %2321, label %2323, label %2322

2322:                                             ; preds = %2317
  tail call void @vpx_highbd_fdct32x32_c(i16* %0, i32* %1, i32 %2) #6
  br label %11292

2323:                                             ; preds = %2317
  %2324 = shufflevector <8 x i16> %2058, <8 x i16> %2057, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2325 = shufflevector <8 x i16> %2058, <8 x i16> %2057, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2326 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2324, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %2327 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2325, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %2328 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2324, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %2329 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2325, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %2330 = add <4 x i32> %2326, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2331 = add <4 x i32> %2327, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2332 = add <4 x i32> %2328, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2333 = add <4 x i32> %2329, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2334 = ashr <4 x i32> %2330, <i32 14, i32 14, i32 14, i32 14>
  %2335 = ashr <4 x i32> %2331, <i32 14, i32 14, i32 14, i32 14>
  %2336 = ashr <4 x i32> %2332, <i32 14, i32 14, i32 14, i32 14>
  %2337 = ashr <4 x i32> %2333, <i32 14, i32 14, i32 14, i32 14>
  %2338 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2334, <4 x i32> %2335) #6
  %2339 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2336, <4 x i32> %2337) #6
  %2340 = add <8 x i16> %2338, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2341 = icmp ult <8 x i16> %2340, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2342 = add <8 x i16> %2339, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2343 = icmp ult <8 x i16> %2342, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2344 = or <8 x i1> %2343, %2341
  %2345 = sext <8 x i1> %2344 to <8 x i16>
  %2346 = bitcast <8 x i16> %2345 to <16 x i8>
  %2347 = icmp slt <16 x i8> %2346, zeroinitializer
  %2348 = bitcast <16 x i1> %2347 to i16
  %2349 = icmp eq i16 %2348, 0
  br i1 %2349, label %2351, label %2350

2350:                                             ; preds = %2323
  tail call void @vpx_highbd_fdct32x32_c(i16* %0, i32* %1, i32 %2) #6
  br label %11292

2351:                                             ; preds = %2323
  %2352 = shufflevector <8 x i16> %2148, <8 x i16> %2159, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2353 = shufflevector <8 x i16> %2148, <8 x i16> %2159, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2354 = shufflevector <8 x i16> %2149, <8 x i16> %2158, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2355 = shufflevector <8 x i16> %2149, <8 x i16> %2158, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2356 = shufflevector <8 x i16> %2150, <8 x i16> %2157, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2357 = shufflevector <8 x i16> %2150, <8 x i16> %2157, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2358 = shufflevector <8 x i16> %2151, <8 x i16> %2156, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2359 = shufflevector <8 x i16> %2151, <8 x i16> %2156, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2360 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2352, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %2361 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2353, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %2362 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2354, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %2363 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2355, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %2364 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2356, <8 x i16> <i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137>) #6
  %2365 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2357, <8 x i16> <i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137>) #6
  %2366 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2358, <8 x i16> <i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137>) #6
  %2367 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2359, <8 x i16> <i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137>) #6
  %2368 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2358, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %2369 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2359, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %2370 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2356, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %2371 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2357, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %2372 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2354, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %2373 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2355, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %2374 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2352, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %2375 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2353, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %2376 = add <4 x i32> %2360, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2377 = add <4 x i32> %2361, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2378 = add <4 x i32> %2362, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2379 = add <4 x i32> %2363, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2380 = add <4 x i32> %2364, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2381 = add <4 x i32> %2365, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2382 = add <4 x i32> %2366, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2383 = add <4 x i32> %2367, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2384 = add <4 x i32> %2368, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2385 = add <4 x i32> %2369, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2386 = add <4 x i32> %2370, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2387 = add <4 x i32> %2371, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2388 = add <4 x i32> %2372, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2389 = add <4 x i32> %2373, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2390 = add <4 x i32> %2374, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2391 = add <4 x i32> %2375, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2392 = ashr <4 x i32> %2376, <i32 14, i32 14, i32 14, i32 14>
  %2393 = ashr <4 x i32> %2377, <i32 14, i32 14, i32 14, i32 14>
  %2394 = ashr <4 x i32> %2378, <i32 14, i32 14, i32 14, i32 14>
  %2395 = ashr <4 x i32> %2379, <i32 14, i32 14, i32 14, i32 14>
  %2396 = ashr <4 x i32> %2380, <i32 14, i32 14, i32 14, i32 14>
  %2397 = ashr <4 x i32> %2381, <i32 14, i32 14, i32 14, i32 14>
  %2398 = ashr <4 x i32> %2382, <i32 14, i32 14, i32 14, i32 14>
  %2399 = ashr <4 x i32> %2383, <i32 14, i32 14, i32 14, i32 14>
  %2400 = ashr <4 x i32> %2384, <i32 14, i32 14, i32 14, i32 14>
  %2401 = ashr <4 x i32> %2385, <i32 14, i32 14, i32 14, i32 14>
  %2402 = ashr <4 x i32> %2386, <i32 14, i32 14, i32 14, i32 14>
  %2403 = ashr <4 x i32> %2387, <i32 14, i32 14, i32 14, i32 14>
  %2404 = ashr <4 x i32> %2388, <i32 14, i32 14, i32 14, i32 14>
  %2405 = ashr <4 x i32> %2389, <i32 14, i32 14, i32 14, i32 14>
  %2406 = ashr <4 x i32> %2390, <i32 14, i32 14, i32 14, i32 14>
  %2407 = ashr <4 x i32> %2391, <i32 14, i32 14, i32 14, i32 14>
  %2408 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2392, <4 x i32> %2393) #6
  %2409 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2394, <4 x i32> %2395) #6
  %2410 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2396, <4 x i32> %2397) #6
  %2411 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2398, <4 x i32> %2399) #6
  %2412 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2400, <4 x i32> %2401) #6
  %2413 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2402, <4 x i32> %2403) #6
  %2414 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2404, <4 x i32> %2405) #6
  %2415 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2406, <4 x i32> %2407) #6
  %2416 = add <8 x i16> %2408, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2417 = icmp ult <8 x i16> %2416, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2418 = add <8 x i16> %2409, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2419 = icmp ult <8 x i16> %2418, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2420 = add <8 x i16> %2410, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2421 = icmp ult <8 x i16> %2420, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2422 = add <8 x i16> %2411, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2423 = icmp ult <8 x i16> %2422, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2424 = or <8 x i1> %2419, %2417
  %2425 = or <8 x i1> %2424, %2421
  %2426 = or <8 x i1> %2425, %2423
  %2427 = sext <8 x i1> %2426 to <8 x i16>
  %2428 = bitcast <8 x i16> %2427 to <16 x i8>
  %2429 = icmp slt <16 x i8> %2428, zeroinitializer
  %2430 = bitcast <16 x i1> %2429 to i16
  %2431 = zext i16 %2430 to i32
  %2432 = add <8 x i16> %2412, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2433 = icmp ult <8 x i16> %2432, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2434 = add <8 x i16> %2413, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2435 = icmp ult <8 x i16> %2434, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2436 = add <8 x i16> %2414, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2437 = icmp ult <8 x i16> %2436, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2438 = add <8 x i16> %2415, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2439 = icmp ult <8 x i16> %2438, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2440 = or <8 x i1> %2435, %2433
  %2441 = or <8 x i1> %2440, %2437
  %2442 = or <8 x i1> %2441, %2439
  %2443 = sext <8 x i1> %2442 to <8 x i16>
  %2444 = bitcast <8 x i16> %2443 to <16 x i8>
  %2445 = icmp slt <16 x i8> %2444, zeroinitializer
  %2446 = bitcast <16 x i1> %2445 to i16
  %2447 = zext i16 %2446 to i32
  %2448 = sub nsw i32 0, %2431
  %2449 = icmp eq i32 %2447, %2448
  br i1 %2449, label %2451, label %2450

2450:                                             ; preds = %2351
  tail call void @vpx_highbd_fdct32x32_c(i16* %0, i32* %1, i32 %2) #6
  br label %11292

2451:                                             ; preds = %2351
  %2452 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2338, <8 x i16> %2056) #6
  %2453 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2056, <8 x i16> %2338) #6
  %2454 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2059, <8 x i16> %2339) #6
  %2455 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2339, <8 x i16> %2059) #6
  %2456 = add <8 x i16> %2452, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2457 = icmp ult <8 x i16> %2456, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2458 = add <8 x i16> %2453, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2459 = icmp ult <8 x i16> %2458, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2460 = add <8 x i16> %2454, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2461 = icmp ult <8 x i16> %2460, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2462 = add <8 x i16> %2455, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2463 = icmp ult <8 x i16> %2462, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2464 = or <8 x i1> %2459, %2457
  %2465 = or <8 x i1> %2464, %2461
  %2466 = or <8 x i1> %2465, %2463
  %2467 = sext <8 x i1> %2466 to <8 x i16>
  %2468 = bitcast <8 x i16> %2467 to <16 x i8>
  %2469 = icmp slt <16 x i8> %2468, zeroinitializer
  %2470 = bitcast <16 x i1> %2469 to i16
  %2471 = icmp eq i16 %2470, 0
  br i1 %2471, label %2473, label %2472

2472:                                             ; preds = %2451
  tail call void @vpx_highbd_fdct32x32_c(i16* %0, i32* %1, i32 %2) #6
  br label %11292

2473:                                             ; preds = %2451
  %2474 = shufflevector <8 x i16> %2237, <8 x i16> %2238, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2475 = shufflevector <8 x i16> %2237, <8 x i16> %2238, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2476 = shufflevector <8 x i16> %2239, <8 x i16> %2240, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2477 = shufflevector <8 x i16> %2239, <8 x i16> %2240, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2478 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2474, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %2479 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2475, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %2480 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2474, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %2481 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2475, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %2482 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2476, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %2483 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2477, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %2484 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2476, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %2485 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2477, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %2486 = add <4 x i32> %2478, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2487 = add <4 x i32> %2479, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2488 = add <4 x i32> %2480, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2489 = add <4 x i32> %2481, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2490 = add <4 x i32> %2482, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2491 = add <4 x i32> %2483, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2492 = add <4 x i32> %2484, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2493 = add <4 x i32> %2485, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2494 = ashr <4 x i32> %2486, <i32 14, i32 14, i32 14, i32 14>
  %2495 = ashr <4 x i32> %2487, <i32 14, i32 14, i32 14, i32 14>
  %2496 = ashr <4 x i32> %2488, <i32 14, i32 14, i32 14, i32 14>
  %2497 = ashr <4 x i32> %2489, <i32 14, i32 14, i32 14, i32 14>
  %2498 = ashr <4 x i32> %2490, <i32 14, i32 14, i32 14, i32 14>
  %2499 = ashr <4 x i32> %2491, <i32 14, i32 14, i32 14, i32 14>
  %2500 = ashr <4 x i32> %2492, <i32 14, i32 14, i32 14, i32 14>
  %2501 = ashr <4 x i32> %2493, <i32 14, i32 14, i32 14, i32 14>
  %2502 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2494, <4 x i32> %2495) #6
  store <8 x i16> %2502, <8 x i16>* %32, align 16
  %2503 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2496, <4 x i32> %2497) #6
  store <8 x i16> %2503, <8 x i16>* %34, align 16
  %2504 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2498, <4 x i32> %2499) #6
  store <8 x i16> %2504, <8 x i16>* %36, align 16
  %2505 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2500, <4 x i32> %2501) #6
  store <8 x i16> %2505, <8 x i16>* %38, align 16
  %2506 = add <8 x i16> %2502, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2507 = icmp ult <8 x i16> %2506, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2508 = add <8 x i16> %2503, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2509 = icmp ult <8 x i16> %2508, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2510 = add <8 x i16> %2504, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2511 = icmp ult <8 x i16> %2510, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2512 = add <8 x i16> %2505, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2513 = icmp ult <8 x i16> %2512, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2514 = or <8 x i1> %2509, %2507
  %2515 = or <8 x i1> %2514, %2511
  %2516 = or <8 x i1> %2515, %2513
  %2517 = sext <8 x i1> %2516 to <8 x i16>
  %2518 = bitcast <8 x i16> %2517 to <16 x i8>
  %2519 = icmp slt <16 x i8> %2518, zeroinitializer
  %2520 = bitcast <16 x i1> %2519 to i16
  %2521 = icmp eq i16 %2520, 0
  br i1 %2521, label %2523, label %2522

2522:                                             ; preds = %2473
  tail call void @vpx_highbd_fdct32x32_c(i16* %0, i32* %1, i32 %2) #6
  br label %11292

2523:                                             ; preds = %2473
  %2524 = shufflevector <8 x i16> %2242, <8 x i16> %2247, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2525 = shufflevector <8 x i16> %2242, <8 x i16> %2247, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2526 = shufflevector <8 x i16> %2243, <8 x i16> %2246, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2527 = shufflevector <8 x i16> %2243, <8 x i16> %2246, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2528 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2524, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %2529 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2525, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %2530 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2526, <8 x i16> <i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137>) #6
  %2531 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2527, <8 x i16> <i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137, i16 -6270, i16 -15137>) #6
  %2532 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2526, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %2533 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2527, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %2534 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2524, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %2535 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2525, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %2536 = add <4 x i32> %2528, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2537 = add <4 x i32> %2529, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2538 = add <4 x i32> %2530, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2539 = add <4 x i32> %2531, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2540 = add <4 x i32> %2532, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2541 = add <4 x i32> %2533, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2542 = add <4 x i32> %2534, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2543 = add <4 x i32> %2535, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2544 = ashr <4 x i32> %2536, <i32 14, i32 14, i32 14, i32 14>
  %2545 = ashr <4 x i32> %2537, <i32 14, i32 14, i32 14, i32 14>
  %2546 = ashr <4 x i32> %2538, <i32 14, i32 14, i32 14, i32 14>
  %2547 = ashr <4 x i32> %2539, <i32 14, i32 14, i32 14, i32 14>
  %2548 = ashr <4 x i32> %2540, <i32 14, i32 14, i32 14, i32 14>
  %2549 = ashr <4 x i32> %2541, <i32 14, i32 14, i32 14, i32 14>
  %2550 = ashr <4 x i32> %2542, <i32 14, i32 14, i32 14, i32 14>
  %2551 = ashr <4 x i32> %2543, <i32 14, i32 14, i32 14, i32 14>
  %2552 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2544, <4 x i32> %2545) #6
  %2553 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2546, <4 x i32> %2547) #6
  %2554 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2548, <4 x i32> %2549) #6
  %2555 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2550, <4 x i32> %2551) #6
  %2556 = add <8 x i16> %2552, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2557 = icmp ult <8 x i16> %2556, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2558 = add <8 x i16> %2553, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2559 = icmp ult <8 x i16> %2558, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2560 = add <8 x i16> %2554, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2561 = icmp ult <8 x i16> %2560, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2562 = add <8 x i16> %2555, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2563 = icmp ult <8 x i16> %2562, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2564 = or <8 x i1> %2559, %2557
  %2565 = or <8 x i1> %2564, %2561
  %2566 = or <8 x i1> %2565, %2563
  %2567 = sext <8 x i1> %2566 to <8 x i16>
  %2568 = bitcast <8 x i16> %2567 to <16 x i8>
  %2569 = icmp slt <16 x i8> %2568, zeroinitializer
  %2570 = bitcast <16 x i1> %2569 to i16
  %2571 = icmp eq i16 %2570, 0
  br i1 %2571, label %2573, label %2572

2572:                                             ; preds = %2523
  tail call void @vpx_highbd_fdct32x32_c(i16* %0, i32* %1, i32 %2) #6
  br label %11292

2573:                                             ; preds = %2523
  %2574 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2409, <8 x i16> %2146) #6
  %2575 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2408, <8 x i16> %2147) #6
  %2576 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2147, <8 x i16> %2408) #6
  %2577 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2146, <8 x i16> %2409) #6
  %2578 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2153, <8 x i16> %2410) #6
  %2579 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2152, <8 x i16> %2411) #6
  %2580 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2411, <8 x i16> %2152) #6
  %2581 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2410, <8 x i16> %2153) #6
  %2582 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2413, <8 x i16> %2154) #6
  %2583 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2412, <8 x i16> %2155) #6
  %2584 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2155, <8 x i16> %2412) #6
  %2585 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2154, <8 x i16> %2413) #6
  %2586 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2161, <8 x i16> %2414) #6
  %2587 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2160, <8 x i16> %2415) #6
  %2588 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2415, <8 x i16> %2160) #6
  %2589 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2414, <8 x i16> %2161) #6
  %2590 = add <8 x i16> %2574, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2591 = icmp ult <8 x i16> %2590, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2592 = add <8 x i16> %2575, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2593 = icmp ult <8 x i16> %2592, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2594 = add <8 x i16> %2576, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2595 = icmp ult <8 x i16> %2594, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2596 = add <8 x i16> %2577, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2597 = icmp ult <8 x i16> %2596, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2598 = or <8 x i1> %2595, %2593
  %2599 = or <8 x i1> %2598, %2591
  %2600 = or <8 x i1> %2599, %2597
  %2601 = sext <8 x i1> %2600 to <8 x i16>
  %2602 = bitcast <8 x i16> %2601 to <16 x i8>
  %2603 = icmp slt <16 x i8> %2602, zeroinitializer
  %2604 = bitcast <16 x i1> %2603 to i16
  %2605 = zext i16 %2604 to i32
  %2606 = add <8 x i16> %2578, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2607 = icmp ult <8 x i16> %2606, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2608 = add <8 x i16> %2579, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2609 = icmp ult <8 x i16> %2608, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2610 = add <8 x i16> %2580, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2611 = icmp ult <8 x i16> %2610, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2612 = add <8 x i16> %2581, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2613 = icmp ult <8 x i16> %2612, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2614 = or <8 x i1> %2613, %2607
  %2615 = or <8 x i1> %2614, %2609
  %2616 = or <8 x i1> %2615, %2611
  %2617 = sext <8 x i1> %2616 to <8 x i16>
  %2618 = bitcast <8 x i16> %2617 to <16 x i8>
  %2619 = icmp slt <16 x i8> %2618, zeroinitializer
  %2620 = bitcast <16 x i1> %2619 to i16
  %2621 = zext i16 %2620 to i32
  %2622 = icmp eq i16 %2604, 0
  br i1 %2622, label %2623, label %2658

2623:                                             ; preds = %2573
  %2624 = add <8 x i16> %2582, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2625 = icmp ult <8 x i16> %2624, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2626 = add <8 x i16> %2583, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2627 = icmp ult <8 x i16> %2626, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2628 = add <8 x i16> %2584, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2629 = icmp ult <8 x i16> %2628, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2630 = add <8 x i16> %2585, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2631 = icmp ult <8 x i16> %2630, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2632 = or <8 x i1> %2629, %2627
  %2633 = or <8 x i1> %2632, %2625
  %2634 = or <8 x i1> %2633, %2631
  %2635 = sext <8 x i1> %2634 to <8 x i16>
  %2636 = bitcast <8 x i16> %2635 to <16 x i8>
  %2637 = icmp slt <16 x i8> %2636, zeroinitializer
  %2638 = bitcast <16 x i1> %2637 to i16
  %2639 = zext i16 %2638 to i32
  %2640 = icmp eq i16 %2620, 0
  br i1 %2640, label %2641, label %2658

2641:                                             ; preds = %2623
  %2642 = add <8 x i16> %2586, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2643 = icmp ult <8 x i16> %2642, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2644 = add <8 x i16> %2587, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2645 = icmp ult <8 x i16> %2644, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2646 = add <8 x i16> %2588, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2647 = icmp ult <8 x i16> %2646, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2648 = add <8 x i16> %2589, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2649 = icmp ult <8 x i16> %2648, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2650 = or <8 x i1> %2649, %2643
  %2651 = or <8 x i1> %2650, %2645
  %2652 = or <8 x i1> %2651, %2647
  %2653 = sext <8 x i1> %2652 to <8 x i16>
  %2654 = bitcast <8 x i16> %2653 to <16 x i8>
  %2655 = icmp slt <16 x i8> %2654, zeroinitializer
  %2656 = bitcast <16 x i1> %2655 to i16
  %2657 = zext i16 %2656 to i32
  br label %2658

2658:                                             ; preds = %2573, %2623, %2641
  %2659 = phi i32 [ %2605, %2573 ], [ %2639, %2623 ], [ %2639, %2641 ]
  %2660 = phi i32 [ %2621, %2573 ], [ %2621, %2623 ], [ %2657, %2641 ]
  %2661 = sub nsw i32 0, %2659
  %2662 = icmp eq i32 %2660, %2661
  br i1 %2662, label %2664, label %2663

2663:                                             ; preds = %2658
  tail call void @vpx_highbd_fdct32x32_c(i16* %0, i32* %1, i32 %2) #6
  br label %11292

2664:                                             ; preds = %2658
  %2665 = shufflevector <8 x i16> %2452, <8 x i16> %2455, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2666 = shufflevector <8 x i16> %2452, <8 x i16> %2455, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2667 = shufflevector <8 x i16> %2453, <8 x i16> %2454, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2668 = shufflevector <8 x i16> %2453, <8 x i16> %2454, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2669 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2665, <8 x i16> <i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069>) #6
  %2670 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2666, <8 x i16> <i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069>) #6
  %2671 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2667, <8 x i16> <i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102>) #6
  %2672 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2668, <8 x i16> <i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102>) #6
  %2673 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2667, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %2674 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2668, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %2675 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2665, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %2676 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2666, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %2677 = add <4 x i32> %2669, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2678 = add <4 x i32> %2670, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2679 = add <4 x i32> %2671, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2680 = add <4 x i32> %2672, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2681 = add <4 x i32> %2673, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2682 = add <4 x i32> %2674, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2683 = add <4 x i32> %2675, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2684 = add <4 x i32> %2676, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2685 = ashr <4 x i32> %2677, <i32 14, i32 14, i32 14, i32 14>
  %2686 = ashr <4 x i32> %2678, <i32 14, i32 14, i32 14, i32 14>
  %2687 = ashr <4 x i32> %2679, <i32 14, i32 14, i32 14, i32 14>
  %2688 = ashr <4 x i32> %2680, <i32 14, i32 14, i32 14, i32 14>
  %2689 = ashr <4 x i32> %2681, <i32 14, i32 14, i32 14, i32 14>
  %2690 = ashr <4 x i32> %2682, <i32 14, i32 14, i32 14, i32 14>
  %2691 = ashr <4 x i32> %2683, <i32 14, i32 14, i32 14, i32 14>
  %2692 = ashr <4 x i32> %2684, <i32 14, i32 14, i32 14, i32 14>
  %2693 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2685, <4 x i32> %2686) #6
  store <8 x i16> %2693, <8 x i16>* %40, align 16
  %2694 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2687, <4 x i32> %2688) #6
  store <8 x i16> %2694, <8 x i16>* %42, align 16
  %2695 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2689, <4 x i32> %2690) #6
  store <8 x i16> %2695, <8 x i16>* %44, align 16
  %2696 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2691, <4 x i32> %2692) #6
  store <8 x i16> %2696, <8 x i16>* %46, align 16
  %2697 = add <8 x i16> %2693, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2698 = icmp ult <8 x i16> %2697, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2699 = add <8 x i16> %2694, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2700 = icmp ult <8 x i16> %2699, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2701 = add <8 x i16> %2695, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2702 = icmp ult <8 x i16> %2701, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2703 = add <8 x i16> %2696, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2704 = icmp ult <8 x i16> %2703, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2705 = or <8 x i1> %2700, %2698
  %2706 = or <8 x i1> %2705, %2702
  %2707 = or <8 x i1> %2706, %2704
  %2708 = sext <8 x i1> %2707 to <8 x i16>
  %2709 = bitcast <8 x i16> %2708 to <16 x i8>
  %2710 = icmp slt <16 x i8> %2709, zeroinitializer
  %2711 = bitcast <16 x i1> %2710 to i16
  %2712 = icmp eq i16 %2711, 0
  br i1 %2712, label %2714, label %2713

2713:                                             ; preds = %2664
  tail call void @vpx_highbd_fdct32x32_c(i16* %0, i32* %1, i32 %2) #6
  br label %11292

2714:                                             ; preds = %2664
  %2715 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2552, <8 x i16> %2241) #6
  %2716 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2241, <8 x i16> %2552) #6
  %2717 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2244, <8 x i16> %2553) #6
  %2718 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2553, <8 x i16> %2244) #6
  %2719 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2554, <8 x i16> %2245) #6
  %2720 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2245, <8 x i16> %2554) #6
  %2721 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2248, <8 x i16> %2555) #6
  %2722 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2555, <8 x i16> %2248) #6
  %2723 = add <8 x i16> %2715, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2724 = icmp ult <8 x i16> %2723, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2725 = add <8 x i16> %2716, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2726 = icmp ult <8 x i16> %2725, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2727 = add <8 x i16> %2717, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2728 = icmp ult <8 x i16> %2727, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2729 = add <8 x i16> %2718, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2730 = icmp ult <8 x i16> %2729, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2731 = or <8 x i1> %2726, %2724
  %2732 = or <8 x i1> %2731, %2728
  %2733 = or <8 x i1> %2732, %2730
  %2734 = sext <8 x i1> %2733 to <8 x i16>
  %2735 = bitcast <8 x i16> %2734 to <16 x i8>
  %2736 = icmp slt <16 x i8> %2735, zeroinitializer
  %2737 = bitcast <16 x i1> %2736 to i16
  %2738 = zext i16 %2737 to i32
  %2739 = add <8 x i16> %2719, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2740 = icmp ult <8 x i16> %2739, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2741 = add <8 x i16> %2720, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2742 = icmp ult <8 x i16> %2741, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2743 = add <8 x i16> %2721, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2744 = icmp ult <8 x i16> %2743, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2745 = add <8 x i16> %2722, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2746 = icmp ult <8 x i16> %2745, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2747 = or <8 x i1> %2742, %2740
  %2748 = or <8 x i1> %2747, %2744
  %2749 = or <8 x i1> %2748, %2746
  %2750 = sext <8 x i1> %2749 to <8 x i16>
  %2751 = bitcast <8 x i16> %2750 to <16 x i8>
  %2752 = icmp slt <16 x i8> %2751, zeroinitializer
  %2753 = bitcast <16 x i1> %2752 to i16
  %2754 = zext i16 %2753 to i32
  %2755 = sub nsw i32 0, %2738
  %2756 = icmp eq i32 %2754, %2755
  br i1 %2756, label %2758, label %2757

2757:                                             ; preds = %2714
  tail call void @vpx_highbd_fdct32x32_c(i16* %0, i32* %1, i32 %2) #6
  br label %11292

2758:                                             ; preds = %2714
  %2759 = shufflevector <8 x i16> %2575, <8 x i16> %2588, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2760 = shufflevector <8 x i16> %2575, <8 x i16> %2588, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2761 = shufflevector <8 x i16> %2576, <8 x i16> %2587, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2762 = shufflevector <8 x i16> %2576, <8 x i16> %2587, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2763 = shufflevector <8 x i16> %2579, <8 x i16> %2584, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2764 = shufflevector <8 x i16> %2579, <8 x i16> %2584, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2765 = shufflevector <8 x i16> %2580, <8 x i16> %2583, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2766 = shufflevector <8 x i16> %2580, <8 x i16> %2583, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2767 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2759, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %2768 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2760, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %2769 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2761, <8 x i16> <i16 -3196, i16 -16069, i16 -3196, i16 -16069, i16 -3196, i16 -16069, i16 -3196, i16 -16069>) #6
  %2770 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2762, <8 x i16> <i16 -3196, i16 -16069, i16 -3196, i16 -16069, i16 -3196, i16 -16069, i16 -3196, i16 -16069>) #6
  %2771 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2763, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %2772 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2764, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %2773 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2765, <8 x i16> <i16 -13623, i16 -9102, i16 -13623, i16 -9102, i16 -13623, i16 -9102, i16 -13623, i16 -9102>) #6
  %2774 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2766, <8 x i16> <i16 -13623, i16 -9102, i16 -13623, i16 -9102, i16 -13623, i16 -9102, i16 -13623, i16 -9102>) #6
  %2775 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2765, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %2776 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2766, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %2777 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2763, <8 x i16> <i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102>) #6
  %2778 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2764, <8 x i16> <i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102>) #6
  %2779 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2761, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %2780 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2762, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %2781 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2759, <8 x i16> <i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069>) #6
  %2782 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2760, <8 x i16> <i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069>) #6
  %2783 = add <4 x i32> %2767, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2784 = add <4 x i32> %2768, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2785 = add <4 x i32> %2769, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2786 = add <4 x i32> %2770, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2787 = add <4 x i32> %2771, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2788 = add <4 x i32> %2772, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2789 = add <4 x i32> %2773, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2790 = add <4 x i32> %2774, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2791 = ashr <4 x i32> %2783, <i32 14, i32 14, i32 14, i32 14>
  %2792 = ashr <4 x i32> %2784, <i32 14, i32 14, i32 14, i32 14>
  %2793 = ashr <4 x i32> %2785, <i32 14, i32 14, i32 14, i32 14>
  %2794 = ashr <4 x i32> %2786, <i32 14, i32 14, i32 14, i32 14>
  %2795 = ashr <4 x i32> %2787, <i32 14, i32 14, i32 14, i32 14>
  %2796 = ashr <4 x i32> %2788, <i32 14, i32 14, i32 14, i32 14>
  %2797 = ashr <4 x i32> %2789, <i32 14, i32 14, i32 14, i32 14>
  %2798 = ashr <4 x i32> %2790, <i32 14, i32 14, i32 14, i32 14>
  %2799 = add <4 x i32> %2775, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2800 = add <4 x i32> %2776, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2801 = add <4 x i32> %2777, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2802 = add <4 x i32> %2778, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2803 = add <4 x i32> %2779, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2804 = add <4 x i32> %2780, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2805 = add <4 x i32> %2781, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2806 = add <4 x i32> %2782, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2807 = ashr <4 x i32> %2799, <i32 14, i32 14, i32 14, i32 14>
  %2808 = ashr <4 x i32> %2800, <i32 14, i32 14, i32 14, i32 14>
  %2809 = ashr <4 x i32> %2801, <i32 14, i32 14, i32 14, i32 14>
  %2810 = ashr <4 x i32> %2802, <i32 14, i32 14, i32 14, i32 14>
  %2811 = ashr <4 x i32> %2803, <i32 14, i32 14, i32 14, i32 14>
  %2812 = ashr <4 x i32> %2804, <i32 14, i32 14, i32 14, i32 14>
  %2813 = ashr <4 x i32> %2805, <i32 14, i32 14, i32 14, i32 14>
  %2814 = ashr <4 x i32> %2806, <i32 14, i32 14, i32 14, i32 14>
  %2815 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2791, <4 x i32> %2792) #6
  %2816 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2793, <4 x i32> %2794) #6
  %2817 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2795, <4 x i32> %2796) #6
  %2818 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2797, <4 x i32> %2798) #6
  %2819 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2807, <4 x i32> %2808) #6
  %2820 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2809, <4 x i32> %2810) #6
  %2821 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2811, <4 x i32> %2812) #6
  %2822 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2813, <4 x i32> %2814) #6
  %2823 = add <8 x i16> %2815, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2824 = icmp ult <8 x i16> %2823, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2825 = add <8 x i16> %2816, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2826 = icmp ult <8 x i16> %2825, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2827 = add <8 x i16> %2817, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2828 = icmp ult <8 x i16> %2827, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2829 = add <8 x i16> %2818, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2830 = icmp ult <8 x i16> %2829, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2831 = or <8 x i1> %2826, %2824
  %2832 = or <8 x i1> %2831, %2828
  %2833 = or <8 x i1> %2832, %2830
  %2834 = sext <8 x i1> %2833 to <8 x i16>
  %2835 = bitcast <8 x i16> %2834 to <16 x i8>
  %2836 = icmp slt <16 x i8> %2835, zeroinitializer
  %2837 = bitcast <16 x i1> %2836 to i16
  %2838 = zext i16 %2837 to i32
  %2839 = add <8 x i16> %2819, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2840 = icmp ult <8 x i16> %2839, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2841 = add <8 x i16> %2820, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2842 = icmp ult <8 x i16> %2841, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2843 = add <8 x i16> %2821, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2844 = icmp ult <8 x i16> %2843, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2845 = add <8 x i16> %2822, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2846 = icmp ult <8 x i16> %2845, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2847 = or <8 x i1> %2842, %2840
  %2848 = or <8 x i1> %2847, %2844
  %2849 = or <8 x i1> %2848, %2846
  %2850 = sext <8 x i1> %2849 to <8 x i16>
  %2851 = bitcast <8 x i16> %2850 to <16 x i8>
  %2852 = icmp slt <16 x i8> %2851, zeroinitializer
  %2853 = bitcast <16 x i1> %2852 to i16
  %2854 = zext i16 %2853 to i32
  %2855 = sub nsw i32 0, %2838
  %2856 = icmp eq i32 %2854, %2855
  br i1 %2856, label %2858, label %2857

2857:                                             ; preds = %2758
  tail call void @vpx_highbd_fdct32x32_c(i16* %0, i32* %1, i32 %2) #6
  br label %11292

2858:                                             ; preds = %2758
  %2859 = shufflevector <8 x i16> %2715, <8 x i16> %2722, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2860 = shufflevector <8 x i16> %2715, <8 x i16> %2722, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2861 = shufflevector <8 x i16> %2716, <8 x i16> %2721, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2862 = shufflevector <8 x i16> %2716, <8 x i16> %2721, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2863 = shufflevector <8 x i16> %2717, <8 x i16> %2720, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2864 = shufflevector <8 x i16> %2717, <8 x i16> %2720, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2865 = shufflevector <8 x i16> %2718, <8 x i16> %2719, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2866 = shufflevector <8 x i16> %2718, <8 x i16> %2719, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2867 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2859, <8 x i16> <i16 1606, i16 16305, i16 1606, i16 16305, i16 1606, i16 16305, i16 1606, i16 16305>) #6
  %2868 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2860, <8 x i16> <i16 1606, i16 16305, i16 1606, i16 16305, i16 1606, i16 16305, i16 1606, i16 16305>) #6
  %2869 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2861, <8 x i16> <i16 12665, i16 10394, i16 12665, i16 10394, i16 12665, i16 10394, i16 12665, i16 10394>) #6
  %2870 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2862, <8 x i16> <i16 12665, i16 10394, i16 12665, i16 10394, i16 12665, i16 10394, i16 12665, i16 10394>) #6
  %2871 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2863, <8 x i16> <i16 7723, i16 14449, i16 7723, i16 14449, i16 7723, i16 14449, i16 7723, i16 14449>) #6
  %2872 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2864, <8 x i16> <i16 7723, i16 14449, i16 7723, i16 14449, i16 7723, i16 14449, i16 7723, i16 14449>) #6
  %2873 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2865, <8 x i16> <i16 15679, i16 4756, i16 15679, i16 4756, i16 15679, i16 4756, i16 15679, i16 4756>) #6
  %2874 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2866, <8 x i16> <i16 15679, i16 4756, i16 15679, i16 4756, i16 15679, i16 4756, i16 15679, i16 4756>) #6
  %2875 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2865, <8 x i16> <i16 -4756, i16 15679, i16 -4756, i16 15679, i16 -4756, i16 15679, i16 -4756, i16 15679>) #6
  %2876 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2866, <8 x i16> <i16 -4756, i16 15679, i16 -4756, i16 15679, i16 -4756, i16 15679, i16 -4756, i16 15679>) #6
  %2877 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2863, <8 x i16> <i16 -14449, i16 7723, i16 -14449, i16 7723, i16 -14449, i16 7723, i16 -14449, i16 7723>) #6
  %2878 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2864, <8 x i16> <i16 -14449, i16 7723, i16 -14449, i16 7723, i16 -14449, i16 7723, i16 -14449, i16 7723>) #6
  %2879 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2861, <8 x i16> <i16 -10394, i16 12665, i16 -10394, i16 12665, i16 -10394, i16 12665, i16 -10394, i16 12665>) #6
  %2880 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2862, <8 x i16> <i16 -10394, i16 12665, i16 -10394, i16 12665, i16 -10394, i16 12665, i16 -10394, i16 12665>) #6
  %2881 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2859, <8 x i16> <i16 -16305, i16 1606, i16 -16305, i16 1606, i16 -16305, i16 1606, i16 -16305, i16 1606>) #6
  %2882 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %2860, <8 x i16> <i16 -16305, i16 1606, i16 -16305, i16 1606, i16 -16305, i16 1606, i16 -16305, i16 1606>) #6
  %2883 = add <4 x i32> %2867, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2884 = add <4 x i32> %2868, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2885 = add <4 x i32> %2869, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2886 = add <4 x i32> %2870, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2887 = add <4 x i32> %2871, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2888 = add <4 x i32> %2872, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2889 = add <4 x i32> %2873, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2890 = add <4 x i32> %2874, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2891 = add <4 x i32> %2875, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2892 = add <4 x i32> %2876, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2893 = add <4 x i32> %2877, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2894 = add <4 x i32> %2878, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2895 = add <4 x i32> %2879, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2896 = add <4 x i32> %2880, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2897 = add <4 x i32> %2881, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2898 = add <4 x i32> %2882, <i32 8192, i32 8192, i32 8192, i32 8192>
  %2899 = ashr <4 x i32> %2883, <i32 14, i32 14, i32 14, i32 14>
  %2900 = ashr <4 x i32> %2884, <i32 14, i32 14, i32 14, i32 14>
  %2901 = ashr <4 x i32> %2885, <i32 14, i32 14, i32 14, i32 14>
  %2902 = ashr <4 x i32> %2886, <i32 14, i32 14, i32 14, i32 14>
  %2903 = ashr <4 x i32> %2887, <i32 14, i32 14, i32 14, i32 14>
  %2904 = ashr <4 x i32> %2888, <i32 14, i32 14, i32 14, i32 14>
  %2905 = ashr <4 x i32> %2889, <i32 14, i32 14, i32 14, i32 14>
  %2906 = ashr <4 x i32> %2890, <i32 14, i32 14, i32 14, i32 14>
  %2907 = ashr <4 x i32> %2891, <i32 14, i32 14, i32 14, i32 14>
  %2908 = ashr <4 x i32> %2892, <i32 14, i32 14, i32 14, i32 14>
  %2909 = ashr <4 x i32> %2893, <i32 14, i32 14, i32 14, i32 14>
  %2910 = ashr <4 x i32> %2894, <i32 14, i32 14, i32 14, i32 14>
  %2911 = ashr <4 x i32> %2895, <i32 14, i32 14, i32 14, i32 14>
  %2912 = ashr <4 x i32> %2896, <i32 14, i32 14, i32 14, i32 14>
  %2913 = ashr <4 x i32> %2897, <i32 14, i32 14, i32 14, i32 14>
  %2914 = ashr <4 x i32> %2898, <i32 14, i32 14, i32 14, i32 14>
  %2915 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2899, <4 x i32> %2900) #6
  store <8 x i16> %2915, <8 x i16>* %48, align 16
  %2916 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2901, <4 x i32> %2902) #6
  store <8 x i16> %2916, <8 x i16>* %50, align 16
  %2917 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2903, <4 x i32> %2904) #6
  store <8 x i16> %2917, <8 x i16>* %52, align 16
  %2918 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2905, <4 x i32> %2906) #6
  store <8 x i16> %2918, <8 x i16>* %54, align 16
  %2919 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2907, <4 x i32> %2908) #6
  store <8 x i16> %2919, <8 x i16>* %56, align 16
  %2920 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2909, <4 x i32> %2910) #6
  store <8 x i16> %2920, <8 x i16>* %58, align 16
  %2921 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2911, <4 x i32> %2912) #6
  store <8 x i16> %2921, <8 x i16>* %60, align 16
  %2922 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %2913, <4 x i32> %2914) #6
  store <8 x i16> %2922, <8 x i16>* %62, align 16
  %2923 = add <8 x i16> %2915, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2924 = icmp ult <8 x i16> %2923, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2925 = add <8 x i16> %2916, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2926 = icmp ult <8 x i16> %2925, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2927 = add <8 x i16> %2917, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2928 = icmp ult <8 x i16> %2927, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2929 = add <8 x i16> %2918, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2930 = icmp ult <8 x i16> %2929, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2931 = or <8 x i1> %2926, %2924
  %2932 = or <8 x i1> %2931, %2928
  %2933 = or <8 x i1> %2932, %2930
  %2934 = sext <8 x i1> %2933 to <8 x i16>
  %2935 = bitcast <8 x i16> %2934 to <16 x i8>
  %2936 = icmp slt <16 x i8> %2935, zeroinitializer
  %2937 = bitcast <16 x i1> %2936 to i16
  %2938 = zext i16 %2937 to i32
  %2939 = add <8 x i16> %2919, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2940 = icmp ult <8 x i16> %2939, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2941 = add <8 x i16> %2920, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2942 = icmp ult <8 x i16> %2941, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2943 = add <8 x i16> %2921, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2944 = icmp ult <8 x i16> %2943, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2945 = add <8 x i16> %2922, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2946 = icmp ult <8 x i16> %2945, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2947 = or <8 x i1> %2942, %2940
  %2948 = or <8 x i1> %2947, %2944
  %2949 = or <8 x i1> %2948, %2946
  %2950 = sext <8 x i1> %2949 to <8 x i16>
  %2951 = bitcast <8 x i16> %2950 to <16 x i8>
  %2952 = icmp slt <16 x i8> %2951, zeroinitializer
  %2953 = bitcast <16 x i1> %2952 to i16
  %2954 = zext i16 %2953 to i32
  %2955 = sub nsw i32 0, %2938
  %2956 = icmp eq i32 %2954, %2955
  br i1 %2956, label %2958, label %2957

2957:                                             ; preds = %2858
  tail call void @vpx_highbd_fdct32x32_c(i16* %0, i32* %1, i32 %2) #6
  br label %11292

2958:                                             ; preds = %2858
  %2959 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2815, <8 x i16> %2574) #6
  %2960 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2574, <8 x i16> %2815) #6
  %2961 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2577, <8 x i16> %2816) #6
  %2962 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2816, <8 x i16> %2577) #6
  %2963 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2817, <8 x i16> %2578) #6
  %2964 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2578, <8 x i16> %2817) #6
  %2965 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2581, <8 x i16> %2818) #6
  %2966 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2818, <8 x i16> %2581) #6
  %2967 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2819, <8 x i16> %2582) #6
  %2968 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2582, <8 x i16> %2819) #6
  %2969 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2585, <8 x i16> %2820) #6
  %2970 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2820, <8 x i16> %2585) #6
  %2971 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2821, <8 x i16> %2586) #6
  %2972 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2586, <8 x i16> %2821) #6
  %2973 = tail call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %2589, <8 x i16> %2822) #6
  %2974 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %2822, <8 x i16> %2589) #6
  %2975 = add <8 x i16> %2959, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2976 = icmp ult <8 x i16> %2975, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2977 = add <8 x i16> %2960, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2978 = icmp ult <8 x i16> %2977, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2979 = add <8 x i16> %2961, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2980 = icmp ult <8 x i16> %2979, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2981 = add <8 x i16> %2962, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2982 = icmp ult <8 x i16> %2981, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2983 = or <8 x i1> %2978, %2976
  %2984 = or <8 x i1> %2983, %2980
  %2985 = or <8 x i1> %2984, %2982
  %2986 = sext <8 x i1> %2985 to <8 x i16>
  %2987 = bitcast <8 x i16> %2986 to <16 x i8>
  %2988 = icmp slt <16 x i8> %2987, zeroinitializer
  %2989 = bitcast <16 x i1> %2988 to i16
  %2990 = zext i16 %2989 to i32
  %2991 = add <8 x i16> %2963, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2992 = icmp ult <8 x i16> %2991, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2993 = add <8 x i16> %2964, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2994 = icmp ult <8 x i16> %2993, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2995 = add <8 x i16> %2965, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2996 = icmp ult <8 x i16> %2995, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2997 = add <8 x i16> %2966, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %2998 = icmp ult <8 x i16> %2997, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %2999 = or <8 x i1> %2994, %2992
  %3000 = or <8 x i1> %2999, %2996
  %3001 = or <8 x i1> %3000, %2998
  %3002 = sext <8 x i1> %3001 to <8 x i16>
  %3003 = bitcast <8 x i16> %3002 to <16 x i8>
  %3004 = icmp slt <16 x i8> %3003, zeroinitializer
  %3005 = bitcast <16 x i1> %3004 to i16
  %3006 = zext i16 %3005 to i32
  %3007 = icmp eq i16 %2989, 0
  br i1 %3007, label %3008, label %3043

3008:                                             ; preds = %2958
  %3009 = add <8 x i16> %2967, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3010 = icmp ult <8 x i16> %3009, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3011 = add <8 x i16> %2968, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3012 = icmp ult <8 x i16> %3011, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3013 = add <8 x i16> %2969, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3014 = icmp ult <8 x i16> %3013, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3015 = add <8 x i16> %2970, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3016 = icmp ult <8 x i16> %3015, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3017 = or <8 x i1> %3012, %3010
  %3018 = or <8 x i1> %3017, %3014
  %3019 = or <8 x i1> %3018, %3016
  %3020 = sext <8 x i1> %3019 to <8 x i16>
  %3021 = bitcast <8 x i16> %3020 to <16 x i8>
  %3022 = icmp slt <16 x i8> %3021, zeroinitializer
  %3023 = bitcast <16 x i1> %3022 to i16
  %3024 = zext i16 %3023 to i32
  %3025 = icmp eq i16 %3005, 0
  br i1 %3025, label %3026, label %3043

3026:                                             ; preds = %3008
  %3027 = add <8 x i16> %2971, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3028 = icmp ult <8 x i16> %3027, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3029 = add <8 x i16> %2972, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3030 = icmp ult <8 x i16> %3029, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3031 = add <8 x i16> %2973, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3032 = icmp ult <8 x i16> %3031, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3033 = add <8 x i16> %2974, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3034 = icmp ult <8 x i16> %3033, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3035 = or <8 x i1> %3030, %3028
  %3036 = or <8 x i1> %3035, %3032
  %3037 = or <8 x i1> %3036, %3034
  %3038 = sext <8 x i1> %3037 to <8 x i16>
  %3039 = bitcast <8 x i16> %3038 to <16 x i8>
  %3040 = icmp slt <16 x i8> %3039, zeroinitializer
  %3041 = bitcast <16 x i1> %3040 to i16
  %3042 = zext i16 %3041 to i32
  br label %3043

3043:                                             ; preds = %2958, %3008, %3026
  %3044 = phi i32 [ %2990, %2958 ], [ %3024, %3008 ], [ %3024, %3026 ]
  %3045 = phi i32 [ %3006, %2958 ], [ %3006, %3008 ], [ %3042, %3026 ]
  %3046 = sub nsw i32 0, %3044
  %3047 = icmp eq i32 %3045, %3046
  br i1 %3047, label %3049, label %3048

3048:                                             ; preds = %3043
  tail call void @vpx_highbd_fdct32x32_c(i16* %0, i32* %1, i32 %2) #6
  br label %11292

3049:                                             ; preds = %3043
  %3050 = shufflevector <8 x i16> %2959, <8 x i16> %2974, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3051 = shufflevector <8 x i16> %2959, <8 x i16> %2974, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3052 = shufflevector <8 x i16> %2960, <8 x i16> %2973, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3053 = shufflevector <8 x i16> %2960, <8 x i16> %2973, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3054 = shufflevector <8 x i16> %2961, <8 x i16> %2972, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3055 = shufflevector <8 x i16> %2961, <8 x i16> %2972, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3056 = shufflevector <8 x i16> %2962, <8 x i16> %2971, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3057 = shufflevector <8 x i16> %2962, <8 x i16> %2971, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3058 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3050, <8 x i16> <i16 804, i16 16364, i16 804, i16 16364, i16 804, i16 16364, i16 804, i16 16364>) #6
  %3059 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3051, <8 x i16> <i16 804, i16 16364, i16 804, i16 16364, i16 804, i16 16364, i16 804, i16 16364>) #6
  %3060 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3052, <8 x i16> <i16 12140, i16 11003, i16 12140, i16 11003, i16 12140, i16 11003, i16 12140, i16 11003>) #6
  %3061 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3053, <8 x i16> <i16 12140, i16 11003, i16 12140, i16 11003, i16 12140, i16 11003, i16 12140, i16 11003>) #6
  %3062 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3054, <8 x i16> <i16 7005, i16 14811, i16 7005, i16 14811, i16 7005, i16 14811, i16 7005, i16 14811>) #6
  %3063 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3055, <8 x i16> <i16 7005, i16 14811, i16 7005, i16 14811, i16 7005, i16 14811, i16 7005, i16 14811>) #6
  %3064 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3056, <8 x i16> <i16 15426, i16 5520, i16 15426, i16 5520, i16 15426, i16 5520, i16 15426, i16 5520>) #6
  %3065 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3057, <8 x i16> <i16 15426, i16 5520, i16 15426, i16 5520, i16 15426, i16 5520, i16 15426, i16 5520>) #6
  %3066 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3056, <8 x i16> <i16 -5520, i16 15426, i16 -5520, i16 15426, i16 -5520, i16 15426, i16 -5520, i16 15426>) #6
  %3067 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3057, <8 x i16> <i16 -5520, i16 15426, i16 -5520, i16 15426, i16 -5520, i16 15426, i16 -5520, i16 15426>) #6
  %3068 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3054, <8 x i16> <i16 -14811, i16 7005, i16 -14811, i16 7005, i16 -14811, i16 7005, i16 -14811, i16 7005>) #6
  %3069 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3055, <8 x i16> <i16 -14811, i16 7005, i16 -14811, i16 7005, i16 -14811, i16 7005, i16 -14811, i16 7005>) #6
  %3070 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3052, <8 x i16> <i16 -11003, i16 12140, i16 -11003, i16 12140, i16 -11003, i16 12140, i16 -11003, i16 12140>) #6
  %3071 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3053, <8 x i16> <i16 -11003, i16 12140, i16 -11003, i16 12140, i16 -11003, i16 12140, i16 -11003, i16 12140>) #6
  %3072 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3050, <8 x i16> <i16 -16364, i16 804, i16 -16364, i16 804, i16 -16364, i16 804, i16 -16364, i16 804>) #6
  %3073 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3051, <8 x i16> <i16 -16364, i16 804, i16 -16364, i16 804, i16 -16364, i16 804, i16 -16364, i16 804>) #6
  %3074 = add <4 x i32> %3058, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3075 = add <4 x i32> %3059, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3076 = add <4 x i32> %3060, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3077 = add <4 x i32> %3061, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3078 = add <4 x i32> %3062, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3079 = add <4 x i32> %3063, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3080 = add <4 x i32> %3064, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3081 = add <4 x i32> %3065, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3082 = add <4 x i32> %3066, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3083 = add <4 x i32> %3067, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3084 = add <4 x i32> %3068, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3085 = add <4 x i32> %3069, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3086 = add <4 x i32> %3070, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3087 = add <4 x i32> %3071, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3088 = add <4 x i32> %3072, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3089 = add <4 x i32> %3073, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3090 = ashr <4 x i32> %3074, <i32 14, i32 14, i32 14, i32 14>
  %3091 = ashr <4 x i32> %3075, <i32 14, i32 14, i32 14, i32 14>
  %3092 = ashr <4 x i32> %3076, <i32 14, i32 14, i32 14, i32 14>
  %3093 = ashr <4 x i32> %3077, <i32 14, i32 14, i32 14, i32 14>
  %3094 = ashr <4 x i32> %3078, <i32 14, i32 14, i32 14, i32 14>
  %3095 = ashr <4 x i32> %3079, <i32 14, i32 14, i32 14, i32 14>
  %3096 = ashr <4 x i32> %3080, <i32 14, i32 14, i32 14, i32 14>
  %3097 = ashr <4 x i32> %3081, <i32 14, i32 14, i32 14, i32 14>
  %3098 = ashr <4 x i32> %3082, <i32 14, i32 14, i32 14, i32 14>
  %3099 = ashr <4 x i32> %3083, <i32 14, i32 14, i32 14, i32 14>
  %3100 = ashr <4 x i32> %3084, <i32 14, i32 14, i32 14, i32 14>
  %3101 = ashr <4 x i32> %3085, <i32 14, i32 14, i32 14, i32 14>
  %3102 = ashr <4 x i32> %3086, <i32 14, i32 14, i32 14, i32 14>
  %3103 = ashr <4 x i32> %3087, <i32 14, i32 14, i32 14, i32 14>
  %3104 = ashr <4 x i32> %3088, <i32 14, i32 14, i32 14, i32 14>
  %3105 = ashr <4 x i32> %3089, <i32 14, i32 14, i32 14, i32 14>
  %3106 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3090, <4 x i32> %3091) #6
  store <8 x i16> %3106, <8 x i16>* %64, align 16
  %3107 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3092, <4 x i32> %3093) #6
  store <8 x i16> %3107, <8 x i16>* %66, align 16
  %3108 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3094, <4 x i32> %3095) #6
  store <8 x i16> %3108, <8 x i16>* %68, align 16
  %3109 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3096, <4 x i32> %3097) #6
  store <8 x i16> %3109, <8 x i16>* %70, align 16
  %3110 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3098, <4 x i32> %3099) #6
  store <8 x i16> %3110, <8 x i16>* %72, align 16
  %3111 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3100, <4 x i32> %3101) #6
  store <8 x i16> %3111, <8 x i16>* %74, align 16
  %3112 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3102, <4 x i32> %3103) #6
  store <8 x i16> %3112, <8 x i16>* %76, align 16
  %3113 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3104, <4 x i32> %3105) #6
  store <8 x i16> %3113, <8 x i16>* %78, align 16
  %3114 = add <8 x i16> %3106, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3115 = icmp ult <8 x i16> %3114, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3116 = add <8 x i16> %3107, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3117 = icmp ult <8 x i16> %3116, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3118 = add <8 x i16> %3108, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3119 = icmp ult <8 x i16> %3118, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3120 = add <8 x i16> %3109, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3121 = icmp ult <8 x i16> %3120, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3122 = or <8 x i1> %3117, %3115
  %3123 = or <8 x i1> %3122, %3119
  %3124 = or <8 x i1> %3123, %3121
  %3125 = sext <8 x i1> %3124 to <8 x i16>
  %3126 = bitcast <8 x i16> %3125 to <16 x i8>
  %3127 = icmp slt <16 x i8> %3126, zeroinitializer
  %3128 = bitcast <16 x i1> %3127 to i16
  %3129 = zext i16 %3128 to i32
  %3130 = add <8 x i16> %3110, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3131 = icmp ult <8 x i16> %3130, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3132 = add <8 x i16> %3111, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3133 = icmp ult <8 x i16> %3132, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3134 = add <8 x i16> %3112, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3135 = icmp ult <8 x i16> %3134, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3136 = add <8 x i16> %3113, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3137 = icmp ult <8 x i16> %3136, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3138 = or <8 x i1> %3133, %3131
  %3139 = or <8 x i1> %3138, %3135
  %3140 = or <8 x i1> %3139, %3137
  %3141 = sext <8 x i1> %3140 to <8 x i16>
  %3142 = bitcast <8 x i16> %3141 to <16 x i8>
  %3143 = icmp slt <16 x i8> %3142, zeroinitializer
  %3144 = bitcast <16 x i1> %3143 to i16
  %3145 = zext i16 %3144 to i32
  %3146 = sub nsw i32 0, %3129
  %3147 = icmp eq i32 %3145, %3146
  br i1 %3147, label %3149, label %3148

3148:                                             ; preds = %3049
  tail call void @vpx_highbd_fdct32x32_c(i16* %0, i32* %1, i32 %2) #6
  br label %11292

3149:                                             ; preds = %3049
  %3150 = shufflevector <8 x i16> %2963, <8 x i16> %2970, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3151 = shufflevector <8 x i16> %2963, <8 x i16> %2970, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3152 = shufflevector <8 x i16> %2964, <8 x i16> %2969, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3153 = shufflevector <8 x i16> %2964, <8 x i16> %2969, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3154 = shufflevector <8 x i16> %2965, <8 x i16> %2968, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3155 = shufflevector <8 x i16> %2965, <8 x i16> %2968, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3156 = shufflevector <8 x i16> %2966, <8 x i16> %2967, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3157 = shufflevector <8 x i16> %2966, <8 x i16> %2967, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3158 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3150, <8 x i16> <i16 3981, i16 15893, i16 3981, i16 15893, i16 3981, i16 15893, i16 3981, i16 15893>) #6
  %3159 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3151, <8 x i16> <i16 3981, i16 15893, i16 3981, i16 15893, i16 3981, i16 15893, i16 3981, i16 15893>) #6
  %3160 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3152, <8 x i16> <i16 14053, i16 8423, i16 14053, i16 8423, i16 14053, i16 8423, i16 14053, i16 8423>) #6
  %3161 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3153, <8 x i16> <i16 14053, i16 8423, i16 14053, i16 8423, i16 14053, i16 8423, i16 14053, i16 8423>) #6
  %3162 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3154, <8 x i16> <i16 9760, i16 13160, i16 9760, i16 13160, i16 9760, i16 13160, i16 9760, i16 13160>) #6
  %3163 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3155, <8 x i16> <i16 9760, i16 13160, i16 9760, i16 13160, i16 9760, i16 13160, i16 9760, i16 13160>) #6
  %3164 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3156, <8 x i16> <i16 16207, i16 2404, i16 16207, i16 2404, i16 16207, i16 2404, i16 16207, i16 2404>) #6
  %3165 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3157, <8 x i16> <i16 16207, i16 2404, i16 16207, i16 2404, i16 16207, i16 2404, i16 16207, i16 2404>) #6
  %3166 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3156, <8 x i16> <i16 -2404, i16 16207, i16 -2404, i16 16207, i16 -2404, i16 16207, i16 -2404, i16 16207>) #6
  %3167 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3157, <8 x i16> <i16 -2404, i16 16207, i16 -2404, i16 16207, i16 -2404, i16 16207, i16 -2404, i16 16207>) #6
  %3168 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3154, <8 x i16> <i16 -13160, i16 9760, i16 -13160, i16 9760, i16 -13160, i16 9760, i16 -13160, i16 9760>) #6
  %3169 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3155, <8 x i16> <i16 -13160, i16 9760, i16 -13160, i16 9760, i16 -13160, i16 9760, i16 -13160, i16 9760>) #6
  %3170 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3152, <8 x i16> <i16 -8423, i16 14053, i16 -8423, i16 14053, i16 -8423, i16 14053, i16 -8423, i16 14053>) #6
  %3171 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3153, <8 x i16> <i16 -8423, i16 14053, i16 -8423, i16 14053, i16 -8423, i16 14053, i16 -8423, i16 14053>) #6
  %3172 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3150, <8 x i16> <i16 -15893, i16 3981, i16 -15893, i16 3981, i16 -15893, i16 3981, i16 -15893, i16 3981>) #6
  %3173 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3151, <8 x i16> <i16 -15893, i16 3981, i16 -15893, i16 3981, i16 -15893, i16 3981, i16 -15893, i16 3981>) #6
  %3174 = add <4 x i32> %3158, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3175 = add <4 x i32> %3159, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3176 = add <4 x i32> %3160, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3177 = add <4 x i32> %3161, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3178 = add <4 x i32> %3162, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3179 = add <4 x i32> %3163, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3180 = add <4 x i32> %3164, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3181 = add <4 x i32> %3165, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3182 = add <4 x i32> %3166, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3183 = add <4 x i32> %3167, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3184 = add <4 x i32> %3168, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3185 = add <4 x i32> %3169, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3186 = add <4 x i32> %3170, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3187 = add <4 x i32> %3171, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3188 = add <4 x i32> %3172, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3189 = add <4 x i32> %3173, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3190 = ashr <4 x i32> %3174, <i32 14, i32 14, i32 14, i32 14>
  %3191 = ashr <4 x i32> %3175, <i32 14, i32 14, i32 14, i32 14>
  %3192 = ashr <4 x i32> %3176, <i32 14, i32 14, i32 14, i32 14>
  %3193 = ashr <4 x i32> %3177, <i32 14, i32 14, i32 14, i32 14>
  %3194 = ashr <4 x i32> %3178, <i32 14, i32 14, i32 14, i32 14>
  %3195 = ashr <4 x i32> %3179, <i32 14, i32 14, i32 14, i32 14>
  %3196 = ashr <4 x i32> %3180, <i32 14, i32 14, i32 14, i32 14>
  %3197 = ashr <4 x i32> %3181, <i32 14, i32 14, i32 14, i32 14>
  %3198 = ashr <4 x i32> %3182, <i32 14, i32 14, i32 14, i32 14>
  %3199 = ashr <4 x i32> %3183, <i32 14, i32 14, i32 14, i32 14>
  %3200 = ashr <4 x i32> %3184, <i32 14, i32 14, i32 14, i32 14>
  %3201 = ashr <4 x i32> %3185, <i32 14, i32 14, i32 14, i32 14>
  %3202 = ashr <4 x i32> %3186, <i32 14, i32 14, i32 14, i32 14>
  %3203 = ashr <4 x i32> %3187, <i32 14, i32 14, i32 14, i32 14>
  %3204 = ashr <4 x i32> %3188, <i32 14, i32 14, i32 14, i32 14>
  %3205 = ashr <4 x i32> %3189, <i32 14, i32 14, i32 14, i32 14>
  %3206 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3190, <4 x i32> %3191) #6
  store <8 x i16> %3206, <8 x i16>* %80, align 16
  %3207 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3192, <4 x i32> %3193) #6
  store <8 x i16> %3207, <8 x i16>* %82, align 16
  %3208 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3194, <4 x i32> %3195) #6
  store <8 x i16> %3208, <8 x i16>* %84, align 16
  %3209 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3196, <4 x i32> %3197) #6
  store <8 x i16> %3209, <8 x i16>* %86, align 16
  %3210 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3198, <4 x i32> %3199) #6
  store <8 x i16> %3210, <8 x i16>* %88, align 16
  %3211 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3200, <4 x i32> %3201) #6
  store <8 x i16> %3211, <8 x i16>* %90, align 16
  %3212 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3202, <4 x i32> %3203) #6
  store <8 x i16> %3212, <8 x i16>* %92, align 16
  %3213 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %3204, <4 x i32> %3205) #6
  store <8 x i16> %3213, <8 x i16>* %94, align 16
  %3214 = add <8 x i16> %3206, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3215 = icmp ult <8 x i16> %3214, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3216 = add <8 x i16> %3207, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3217 = icmp ult <8 x i16> %3216, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3218 = add <8 x i16> %3208, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3219 = icmp ult <8 x i16> %3218, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3220 = add <8 x i16> %3209, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3221 = icmp ult <8 x i16> %3220, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3222 = or <8 x i1> %3217, %3215
  %3223 = or <8 x i1> %3222, %3219
  %3224 = or <8 x i1> %3223, %3221
  %3225 = sext <8 x i1> %3224 to <8 x i16>
  %3226 = bitcast <8 x i16> %3225 to <16 x i8>
  %3227 = icmp slt <16 x i8> %3226, zeroinitializer
  %3228 = bitcast <16 x i1> %3227 to i16
  %3229 = zext i16 %3228 to i32
  %3230 = add <8 x i16> %3210, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3231 = icmp ult <8 x i16> %3230, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3232 = add <8 x i16> %3211, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3233 = icmp ult <8 x i16> %3232, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3234 = add <8 x i16> %3212, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3235 = icmp ult <8 x i16> %3234, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3236 = add <8 x i16> %3213, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %3237 = icmp ult <8 x i16> %3236, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %3238 = or <8 x i1> %3233, %3231
  %3239 = or <8 x i1> %3238, %3235
  %3240 = or <8 x i1> %3239, %3237
  %3241 = sext <8 x i1> %3240 to <8 x i16>
  %3242 = bitcast <8 x i16> %3241 to <16 x i8>
  %3243 = icmp slt <16 x i8> %3242, zeroinitializer
  %3244 = bitcast <16 x i1> %3243 to i16
  %3245 = zext i16 %3244 to i32
  %3246 = sub nsw i32 0, %3229
  %3247 = icmp eq i32 %3245, %3246
  br i1 %3247, label %11082, label %3248

3248:                                             ; preds = %3149
  tail call void @vpx_highbd_fdct32x32_c(i16* %0, i32* %1, i32 %2) #6
  br label %11292

3249:                                             ; preds = %2050
  %3250 = shufflevector <8 x i16> %1426, <8 x i16> %1433, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3251 = shufflevector <8 x i16> %1426, <8 x i16> %1433, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3252 = shufflevector <8 x i16> %1427, <8 x i16> %1432, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3253 = shufflevector <8 x i16> %1427, <8 x i16> %1432, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3254 = shufflevector <8 x i16> %1428, <8 x i16> %1431, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3255 = shufflevector <8 x i16> %1428, <8 x i16> %1431, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3256 = shufflevector <8 x i16> %1429, <8 x i16> %1430, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3257 = shufflevector <8 x i16> %1429, <8 x i16> %1430, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3258 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3250, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %3259 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3251, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %3260 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3252, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %3261 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3253, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %3262 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3254, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %3263 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3255, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %3264 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3256, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %3265 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3257, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %3266 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3256, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %3267 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3257, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %3268 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3254, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %3269 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3255, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %3270 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3252, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %3271 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3253, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %3272 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3250, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %3273 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3251, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %3274 = shufflevector <8 x i16> %1439, <8 x i16> %1436, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3275 = shufflevector <8 x i16> %1439, <8 x i16> %1436, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3276 = shufflevector <8 x i16> %1438, <8 x i16> %1437, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3277 = shufflevector <8 x i16> %1438, <8 x i16> %1437, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3278 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3274, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %3279 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3275, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %3280 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3276, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %3281 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3277, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %3282 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3276, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %3283 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3277, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %3284 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3274, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %3285 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3275, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %3286 = add <4 x i32> %3278, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3287 = add <4 x i32> %3279, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3288 = add <4 x i32> %3280, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3289 = add <4 x i32> %3281, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3290 = add <4 x i32> %3282, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3291 = add <4 x i32> %3283, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3292 = add <4 x i32> %3284, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3293 = add <4 x i32> %3285, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3294 = ashr <4 x i32> %3286, <i32 14, i32 14, i32 14, i32 14>
  %3295 = ashr <4 x i32> %3287, <i32 14, i32 14, i32 14, i32 14>
  %3296 = ashr <4 x i32> %3288, <i32 14, i32 14, i32 14, i32 14>
  %3297 = ashr <4 x i32> %3289, <i32 14, i32 14, i32 14, i32 14>
  %3298 = ashr <4 x i32> %3290, <i32 14, i32 14, i32 14, i32 14>
  %3299 = ashr <4 x i32> %3291, <i32 14, i32 14, i32 14, i32 14>
  %3300 = ashr <4 x i32> %3292, <i32 14, i32 14, i32 14, i32 14>
  %3301 = ashr <4 x i32> %3293, <i32 14, i32 14, i32 14, i32 14>
  %3302 = shufflevector <8 x i16> %1409, <8 x i16> %1793, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3303 = shufflevector <8 x i16> %1409, <8 x i16> %1793, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3304 = shufflevector <8 x i16> %1408, <8 x i16> %1792, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3305 = shufflevector <8 x i16> %1408, <8 x i16> %1792, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3306 = shufflevector <8 x i16> %1407, <8 x i16> %1791, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3307 = shufflevector <8 x i16> %1407, <8 x i16> %1791, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3308 = shufflevector <8 x i16> %1406, <8 x i16> %1790, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3309 = shufflevector <8 x i16> %1406, <8 x i16> %1790, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3310 = shufflevector <8 x i16> %1397, <8 x i16> %1797, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3311 = shufflevector <8 x i16> %1397, <8 x i16> %1797, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3312 = shufflevector <8 x i16> %1396, <8 x i16> %1796, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3313 = shufflevector <8 x i16> %1396, <8 x i16> %1796, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3314 = shufflevector <8 x i16> %1395, <8 x i16> %1795, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3315 = shufflevector <8 x i16> %1395, <8 x i16> %1795, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3316 = shufflevector <8 x i16> %1394, <8 x i16> %1794, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3317 = shufflevector <8 x i16> %1394, <8 x i16> %1794, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3318 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3302, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %3319 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3303, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %3320 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3304, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %3321 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3305, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %3322 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3306, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %3323 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3307, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %3324 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3308, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %3325 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3309, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %3326 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3308, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %3327 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3309, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %3328 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3306, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %3329 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3307, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %3330 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3304, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %3331 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3305, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %3332 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3302, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %3333 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3303, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %3334 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3316, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %3335 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3317, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %3336 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3314, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %3337 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3315, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %3338 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3312, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %3339 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3313, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %3340 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3310, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %3341 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3311, <8 x i16> <i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1, i16 1, i16 -1>) #6
  %3342 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3310, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %3343 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3311, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %3344 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3312, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %3345 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3313, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %3346 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3314, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %3347 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3315, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %3348 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3316, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %3349 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %3317, <8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>) #6
  %3350 = ashr <8 x i16> %1434, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %3351 = ashr <8 x i16> %1435, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %3352 = ashr <8 x i16> %1440, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %3353 = ashr <8 x i16> %1441, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %3354 = shufflevector <8 x i16> %1434, <8 x i16> %3350, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3355 = shufflevector <8 x i16> %1434, <8 x i16> %3350, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3356 = shufflevector <8 x i16> %1435, <8 x i16> %3351, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3357 = shufflevector <8 x i16> %1435, <8 x i16> %3351, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3358 = shufflevector <8 x i16> %1440, <8 x i16> %3352, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3359 = shufflevector <8 x i16> %1440, <8 x i16> %3352, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3360 = shufflevector <8 x i16> %1441, <8 x i16> %3353, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %3361 = shufflevector <8 x i16> %1441, <8 x i16> %3353, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %3362 = add <4 x i32> %3264, %3258
  %3363 = add <4 x i32> %3265, %3259
  %3364 = add <4 x i32> %3262, %3260
  %3365 = add <4 x i32> %3263, %3261
  %3366 = sub <4 x i32> %3260, %3262
  %3367 = sub <4 x i32> %3261, %3263
  %3368 = sub <4 x i32> %3258, %3264
  %3369 = sub <4 x i32> %3259, %3265
  %3370 = bitcast <8 x i16> %3354 to <4 x i32>
  %3371 = add <4 x i32> %3296, %3370
  %3372 = bitcast <8 x i16> %3355 to <4 x i32>
  %3373 = add <4 x i32> %3297, %3372
  %3374 = bitcast <8 x i16> %3356 to <4 x i32>
  %3375 = add <4 x i32> %3294, %3374
  %3376 = bitcast <8 x i16> %3357 to <4 x i32>
  %3377 = add <4 x i32> %3295, %3376
  %3378 = sub <4 x i32> %3374, %3294
  %3379 = sub <4 x i32> %3376, %3295
  %3380 = sub <4 x i32> %3370, %3296
  %3381 = sub <4 x i32> %3372, %3297
  %3382 = bitcast <8 x i16> %3360 to <4 x i32>
  %3383 = sub <4 x i32> %3382, %3298
  %3384 = bitcast <8 x i16> %3361 to <4 x i32>
  %3385 = sub <4 x i32> %3384, %3299
  %3386 = bitcast <8 x i16> %3358 to <4 x i32>
  %3387 = sub <4 x i32> %3386, %3300
  %3388 = bitcast <8 x i16> %3359 to <4 x i32>
  %3389 = sub <4 x i32> %3388, %3301
  %3390 = add <4 x i32> %3300, %3386
  %3391 = add <4 x i32> %3301, %3388
  %3392 = add <4 x i32> %3298, %3382
  %3393 = add <4 x i32> %3299, %3384
  %3394 = shufflevector <4 x i32> %3270, <4 x i32> %3268, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %3395 = bitcast <4 x i32> %3394 to <2 x i64>
  %3396 = shufflevector <4 x i32> %3270, <4 x i32> %3268, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %3397 = bitcast <4 x i32> %3396 to <2 x i64>
  %3398 = shufflevector <4 x i32> %3271, <4 x i32> %3269, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %3399 = bitcast <4 x i32> %3398 to <2 x i64>
  %3400 = shufflevector <4 x i32> %3271, <4 x i32> %3269, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %3401 = bitcast <4 x i32> %3400 to <2 x i64>
  %3402 = and <2 x i64> %3395, <i64 4294967295, i64 4294967295>
  %3403 = mul nuw nsw <2 x i64> %3402, <i64 11585, i64 11585>
  %3404 = lshr <2 x i64> %3395, <i64 32, i64 32>
  %3405 = mul nuw <2 x i64> %3404, <i64 4294955711, i64 4294955711>
  %3406 = add <2 x i64> %3405, %3403
  %3407 = and <2 x i64> %3397, <i64 4294967295, i64 4294967295>
  %3408 = mul nuw nsw <2 x i64> %3407, <i64 11585, i64 11585>
  %3409 = lshr <2 x i64> %3397, <i64 32, i64 32>
  %3410 = mul nuw <2 x i64> %3409, <i64 4294955711, i64 4294955711>
  %3411 = add <2 x i64> %3410, %3408
  %3412 = and <2 x i64> %3399, <i64 4294967295, i64 4294967295>
  %3413 = mul nuw nsw <2 x i64> %3412, <i64 11585, i64 11585>
  %3414 = lshr <2 x i64> %3399, <i64 32, i64 32>
  %3415 = mul nuw <2 x i64> %3414, <i64 4294955711, i64 4294955711>
  %3416 = add <2 x i64> %3415, %3413
  %3417 = and <2 x i64> %3401, <i64 4294967295, i64 4294967295>
  %3418 = mul nuw nsw <2 x i64> %3417, <i64 11585, i64 11585>
  %3419 = lshr <2 x i64> %3401, <i64 32, i64 32>
  %3420 = mul nuw <2 x i64> %3419, <i64 4294955711, i64 4294955711>
  %3421 = add <2 x i64> %3420, %3418
  %3422 = mul nuw nsw <2 x i64> %3404, <i64 11585, i64 11585>
  %3423 = add nuw nsw <2 x i64> %3422, %3403
  %3424 = mul nuw nsw <2 x i64> %3409, <i64 11585, i64 11585>
  %3425 = add nuw nsw <2 x i64> %3424, %3408
  %3426 = mul nuw nsw <2 x i64> %3414, <i64 11585, i64 11585>
  %3427 = add nuw nsw <2 x i64> %3426, %3413
  %3428 = mul nuw nsw <2 x i64> %3419, <i64 11585, i64 11585>
  %3429 = add nuw nsw <2 x i64> %3428, %3418
  %3430 = shl <2 x i64> %3406, <i64 1, i64 1>
  %3431 = shl <2 x i64> %3411, <i64 1, i64 1>
  %3432 = shl <2 x i64> %3416, <i64 1, i64 1>
  %3433 = shl <2 x i64> %3421, <i64 1, i64 1>
  %3434 = bitcast <2 x i64> %3430 to <4 x i32>
  %3435 = shufflevector <4 x i32> %3434, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %3436 = bitcast <4 x i32> %3435 to <2 x i64>
  %3437 = bitcast <2 x i64> %3431 to <4 x i32>
  %3438 = shufflevector <4 x i32> %3437, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %3439 = bitcast <4 x i32> %3438 to <2 x i64>
  %3440 = bitcast <2 x i64> %3432 to <4 x i32>
  %3441 = shufflevector <4 x i32> %3440, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %3442 = bitcast <4 x i32> %3441 to <2 x i64>
  %3443 = bitcast <2 x i64> %3433 to <4 x i32>
  %3444 = shufflevector <4 x i32> %3443, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %3445 = bitcast <4 x i32> %3444 to <2 x i64>
  %3446 = shufflevector <2 x i64> %3436, <2 x i64> %3439, <2 x i32> <i32 0, i32 2>
  %3447 = shufflevector <2 x i64> %3442, <2 x i64> %3445, <2 x i32> <i32 0, i32 2>
  %3448 = bitcast <2 x i64> %3446 to <4 x i32>
  %3449 = bitcast <2 x i64> %3447 to <4 x i32>
  %3450 = add <4 x i32> %3448, <i32 1, i32 1, i32 1, i32 1>
  %3451 = icmp ugt <4 x i32> %3450, <i32 1, i32 1, i32 1, i32 1>
  %3452 = sext <4 x i1> %3451 to <4 x i32>
  %3453 = bitcast <4 x i32> %3452 to <16 x i8>
  %3454 = icmp slt <16 x i8> %3453, zeroinitializer
  %3455 = bitcast <16 x i1> %3454 to i16
  %3456 = zext i16 %3455 to i32
  %3457 = add <4 x i32> %3449, <i32 1, i32 1, i32 1, i32 1>
  %3458 = icmp ugt <4 x i32> %3457, <i32 1, i32 1, i32 1, i32 1>
  %3459 = sext <4 x i1> %3458 to <4 x i32>
  %3460 = bitcast <4 x i32> %3459 to <16 x i8>
  %3461 = icmp slt <16 x i8> %3460, zeroinitializer
  %3462 = bitcast <16 x i1> %3461 to i16
  %3463 = zext i16 %3462 to i32
  %3464 = sub nsw i32 0, %3456
  %3465 = icmp eq i32 %3463, %3464
  br i1 %3465, label %3466, label %3503

3466:                                             ; preds = %3249
  %3467 = shl nuw nsw <2 x i64> %3423, <i64 1, i64 1>
  %3468 = shl nuw nsw <2 x i64> %3425, <i64 1, i64 1>
  %3469 = shl nuw nsw <2 x i64> %3427, <i64 1, i64 1>
  %3470 = shl nuw nsw <2 x i64> %3429, <i64 1, i64 1>
  %3471 = bitcast <2 x i64> %3467 to <4 x i32>
  %3472 = shufflevector <4 x i32> %3471, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %3473 = bitcast <4 x i32> %3472 to <2 x i64>
  %3474 = bitcast <2 x i64> %3468 to <4 x i32>
  %3475 = shufflevector <4 x i32> %3474, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %3476 = bitcast <4 x i32> %3475 to <2 x i64>
  %3477 = bitcast <2 x i64> %3469 to <4 x i32>
  %3478 = shufflevector <4 x i32> %3477, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %3479 = bitcast <4 x i32> %3478 to <2 x i64>
  %3480 = bitcast <2 x i64> %3470 to <4 x i32>
  %3481 = shufflevector <4 x i32> %3480, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %3482 = bitcast <4 x i32> %3481 to <2 x i64>
  %3483 = shufflevector <2 x i64> %3473, <2 x i64> %3476, <2 x i32> <i32 0, i32 2>
  %3484 = shufflevector <2 x i64> %3479, <2 x i64> %3482, <2 x i32> <i32 0, i32 2>
  %3485 = bitcast <2 x i64> %3483 to <4 x i32>
  %3486 = bitcast <2 x i64> %3484 to <4 x i32>
  %3487 = add <4 x i32> %3485, <i32 1, i32 1, i32 1, i32 1>
  %3488 = icmp ugt <4 x i32> %3487, <i32 1, i32 1, i32 1, i32 1>
  %3489 = sext <4 x i1> %3488 to <4 x i32>
  %3490 = bitcast <4 x i32> %3489 to <16 x i8>
  %3491 = icmp slt <16 x i8> %3490, zeroinitializer
  %3492 = bitcast <16 x i1> %3491 to i16
  %3493 = zext i16 %3492 to i32
  %3494 = add <4 x i32> %3486, <i32 1, i32 1, i32 1, i32 1>
  %3495 = icmp ugt <4 x i32> %3494, <i32 1, i32 1, i32 1, i32 1>
  %3496 = sext <4 x i1> %3495 to <4 x i32>
  %3497 = bitcast <4 x i32> %3496 to <16 x i8>
  %3498 = icmp slt <16 x i8> %3497, zeroinitializer
  %3499 = bitcast <16 x i1> %3498 to i16
  %3500 = zext i16 %3499 to i32
  %3501 = sub nsw i32 0, %3493
  %3502 = icmp eq i32 %3500, %3501
  br i1 %3502, label %3719, label %3503

3503:                                             ; preds = %3249, %3466
  %3504 = bitcast [32 x i64]* %4 to i8*
  %3505 = bitcast [32 x i64]* %5 to i8*
  %3506 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %3507 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %3508 = bitcast [32 x i64]* %5 to <2 x i64>*
  %3509 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %3510 = bitcast i64* %3509 to <2 x i64>*
  %3511 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %3512 = bitcast i64* %3511 to <2 x i64>*
  %3513 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %3514 = bitcast i64* %3513 to <2 x i64>*
  %3515 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %3516 = bitcast i64* %3515 to <2 x i64>*
  %3517 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %3518 = bitcast i64* %3517 to <2 x i64>*
  %3519 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %3520 = bitcast i64* %3519 to <2 x i64>*
  %3521 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %3522 = bitcast i64* %3521 to <2 x i64>*
  %3523 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %3524 = bitcast i64* %3523 to <2 x i64>*
  %3525 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %3526 = bitcast i64* %3525 to <2 x i64>*
  %3527 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %3528 = bitcast i64* %3527 to <2 x i64>*
  %3529 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %3530 = bitcast i64* %3529 to <2 x i64>*
  %3531 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %3532 = bitcast i64* %3531 to <2 x i64>*
  %3533 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %3534 = bitcast i64* %3533 to <2 x i64>*
  %3535 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %3536 = bitcast i64* %3535 to <2 x i64>*
  %3537 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %3538 = bitcast i64* %3537 to <2 x i64>*
  br label %3539

3539:                                             ; preds = %3572, %3503
  %3540 = phi i64 [ 0, %3503 ], [ %3717, %3572 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %3504) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %3504, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %3505) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %3505, i8 -86, i64 256, i1 false) #6
  br label %3541

3541:                                             ; preds = %3541, %3539
  %3542 = phi i64 [ 0, %3539 ], [ %3570, %3541 ]
  %3543 = shl i64 %3542, 5
  %3544 = add nuw nsw i64 %3543, %3540
  %3545 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %3544
  %3546 = load i16, i16* %3545, align 2
  %3547 = sext i16 %3546 to i64
  %3548 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %3542
  store i64 %3547, i64* %3548, align 16
  %3549 = or i64 %3542, 1
  %3550 = shl i64 %3549, 5
  %3551 = add nuw nsw i64 %3550, %3540
  %3552 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %3551
  %3553 = load i16, i16* %3552, align 2
  %3554 = sext i16 %3553 to i64
  %3555 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %3549
  store i64 %3554, i64* %3555, align 8
  %3556 = or i64 %3542, 2
  %3557 = shl i64 %3556, 5
  %3558 = add nuw nsw i64 %3557, %3540
  %3559 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %3558
  %3560 = load i16, i16* %3559, align 2
  %3561 = sext i16 %3560 to i64
  %3562 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %3556
  store i64 %3561, i64* %3562, align 16
  %3563 = or i64 %3542, 3
  %3564 = shl i64 %3563, 5
  %3565 = add nuw nsw i64 %3564, %3540
  %3566 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %3565
  %3567 = load i16, i16* %3566, align 2
  %3568 = sext i16 %3567 to i64
  %3569 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %3563
  store i64 %3568, i64* %3569, align 8
  %3570 = add nuw nsw i64 %3542, 4
  %3571 = icmp eq i64 %3570, 32
  br i1 %3571, label %3572, label %3541

3572:                                             ; preds = %3541
  call void @vpx_fdct32(i64* nonnull %3506, i64* nonnull %3507, i32 0) #6
  %3573 = shl i64 %3540, 5
  %3574 = load <2 x i64>, <2 x i64>* %3508, align 16
  %3575 = add nsw <2 x i64> %3574, <i64 1, i64 1>
  %3576 = lshr <2 x i64> %3574, <i64 63, i64 63>
  %3577 = add nsw <2 x i64> %3575, %3576
  %3578 = lshr <2 x i64> %3577, <i64 2, i64 2>
  %3579 = trunc <2 x i64> %3578 to <2 x i32>
  %3580 = getelementptr inbounds i32, i32* %1, i64 %3573
  %3581 = bitcast i32* %3580 to <2 x i32>*
  store <2 x i32> %3579, <2 x i32>* %3581, align 4
  %3582 = load <2 x i64>, <2 x i64>* %3510, align 16
  %3583 = add nsw <2 x i64> %3582, <i64 1, i64 1>
  %3584 = lshr <2 x i64> %3582, <i64 63, i64 63>
  %3585 = add nsw <2 x i64> %3583, %3584
  %3586 = lshr <2 x i64> %3585, <i64 2, i64 2>
  %3587 = trunc <2 x i64> %3586 to <2 x i32>
  %3588 = or i64 %3573, 2
  %3589 = getelementptr inbounds i32, i32* %1, i64 %3588
  %3590 = bitcast i32* %3589 to <2 x i32>*
  store <2 x i32> %3587, <2 x i32>* %3590, align 4
  %3591 = load <2 x i64>, <2 x i64>* %3512, align 16
  %3592 = add nsw <2 x i64> %3591, <i64 1, i64 1>
  %3593 = lshr <2 x i64> %3591, <i64 63, i64 63>
  %3594 = add nsw <2 x i64> %3592, %3593
  %3595 = lshr <2 x i64> %3594, <i64 2, i64 2>
  %3596 = trunc <2 x i64> %3595 to <2 x i32>
  %3597 = or i64 %3573, 4
  %3598 = getelementptr inbounds i32, i32* %1, i64 %3597
  %3599 = bitcast i32* %3598 to <2 x i32>*
  store <2 x i32> %3596, <2 x i32>* %3599, align 4
  %3600 = load <2 x i64>, <2 x i64>* %3514, align 16
  %3601 = add nsw <2 x i64> %3600, <i64 1, i64 1>
  %3602 = lshr <2 x i64> %3600, <i64 63, i64 63>
  %3603 = add nsw <2 x i64> %3601, %3602
  %3604 = lshr <2 x i64> %3603, <i64 2, i64 2>
  %3605 = trunc <2 x i64> %3604 to <2 x i32>
  %3606 = or i64 %3573, 6
  %3607 = getelementptr inbounds i32, i32* %1, i64 %3606
  %3608 = bitcast i32* %3607 to <2 x i32>*
  store <2 x i32> %3605, <2 x i32>* %3608, align 4
  %3609 = load <2 x i64>, <2 x i64>* %3516, align 16
  %3610 = add nsw <2 x i64> %3609, <i64 1, i64 1>
  %3611 = lshr <2 x i64> %3609, <i64 63, i64 63>
  %3612 = add nsw <2 x i64> %3610, %3611
  %3613 = lshr <2 x i64> %3612, <i64 2, i64 2>
  %3614 = trunc <2 x i64> %3613 to <2 x i32>
  %3615 = or i64 %3573, 8
  %3616 = getelementptr inbounds i32, i32* %1, i64 %3615
  %3617 = bitcast i32* %3616 to <2 x i32>*
  store <2 x i32> %3614, <2 x i32>* %3617, align 4
  %3618 = load <2 x i64>, <2 x i64>* %3518, align 16
  %3619 = add nsw <2 x i64> %3618, <i64 1, i64 1>
  %3620 = lshr <2 x i64> %3618, <i64 63, i64 63>
  %3621 = add nsw <2 x i64> %3619, %3620
  %3622 = lshr <2 x i64> %3621, <i64 2, i64 2>
  %3623 = trunc <2 x i64> %3622 to <2 x i32>
  %3624 = or i64 %3573, 10
  %3625 = getelementptr inbounds i32, i32* %1, i64 %3624
  %3626 = bitcast i32* %3625 to <2 x i32>*
  store <2 x i32> %3623, <2 x i32>* %3626, align 4
  %3627 = load <2 x i64>, <2 x i64>* %3520, align 16
  %3628 = add nsw <2 x i64> %3627, <i64 1, i64 1>
  %3629 = lshr <2 x i64> %3627, <i64 63, i64 63>
  %3630 = add nsw <2 x i64> %3628, %3629
  %3631 = lshr <2 x i64> %3630, <i64 2, i64 2>
  %3632 = trunc <2 x i64> %3631 to <2 x i32>
  %3633 = or i64 %3573, 12
  %3634 = getelementptr inbounds i32, i32* %1, i64 %3633
  %3635 = bitcast i32* %3634 to <2 x i32>*
  store <2 x i32> %3632, <2 x i32>* %3635, align 4
  %3636 = load <2 x i64>, <2 x i64>* %3522, align 16
  %3637 = add nsw <2 x i64> %3636, <i64 1, i64 1>
  %3638 = lshr <2 x i64> %3636, <i64 63, i64 63>
  %3639 = add nsw <2 x i64> %3637, %3638
  %3640 = lshr <2 x i64> %3639, <i64 2, i64 2>
  %3641 = trunc <2 x i64> %3640 to <2 x i32>
  %3642 = or i64 %3573, 14
  %3643 = getelementptr inbounds i32, i32* %1, i64 %3642
  %3644 = bitcast i32* %3643 to <2 x i32>*
  store <2 x i32> %3641, <2 x i32>* %3644, align 4
  %3645 = load <2 x i64>, <2 x i64>* %3524, align 16
  %3646 = add nsw <2 x i64> %3645, <i64 1, i64 1>
  %3647 = lshr <2 x i64> %3645, <i64 63, i64 63>
  %3648 = add nsw <2 x i64> %3646, %3647
  %3649 = lshr <2 x i64> %3648, <i64 2, i64 2>
  %3650 = trunc <2 x i64> %3649 to <2 x i32>
  %3651 = or i64 %3573, 16
  %3652 = getelementptr inbounds i32, i32* %1, i64 %3651
  %3653 = bitcast i32* %3652 to <2 x i32>*
  store <2 x i32> %3650, <2 x i32>* %3653, align 4
  %3654 = load <2 x i64>, <2 x i64>* %3526, align 16
  %3655 = add nsw <2 x i64> %3654, <i64 1, i64 1>
  %3656 = lshr <2 x i64> %3654, <i64 63, i64 63>
  %3657 = add nsw <2 x i64> %3655, %3656
  %3658 = lshr <2 x i64> %3657, <i64 2, i64 2>
  %3659 = trunc <2 x i64> %3658 to <2 x i32>
  %3660 = or i64 %3573, 18
  %3661 = getelementptr inbounds i32, i32* %1, i64 %3660
  %3662 = bitcast i32* %3661 to <2 x i32>*
  store <2 x i32> %3659, <2 x i32>* %3662, align 4
  %3663 = load <2 x i64>, <2 x i64>* %3528, align 16
  %3664 = add nsw <2 x i64> %3663, <i64 1, i64 1>
  %3665 = lshr <2 x i64> %3663, <i64 63, i64 63>
  %3666 = add nsw <2 x i64> %3664, %3665
  %3667 = lshr <2 x i64> %3666, <i64 2, i64 2>
  %3668 = trunc <2 x i64> %3667 to <2 x i32>
  %3669 = or i64 %3573, 20
  %3670 = getelementptr inbounds i32, i32* %1, i64 %3669
  %3671 = bitcast i32* %3670 to <2 x i32>*
  store <2 x i32> %3668, <2 x i32>* %3671, align 4
  %3672 = load <2 x i64>, <2 x i64>* %3530, align 16
  %3673 = add nsw <2 x i64> %3672, <i64 1, i64 1>
  %3674 = lshr <2 x i64> %3672, <i64 63, i64 63>
  %3675 = add nsw <2 x i64> %3673, %3674
  %3676 = lshr <2 x i64> %3675, <i64 2, i64 2>
  %3677 = trunc <2 x i64> %3676 to <2 x i32>
  %3678 = or i64 %3573, 22
  %3679 = getelementptr inbounds i32, i32* %1, i64 %3678
  %3680 = bitcast i32* %3679 to <2 x i32>*
  store <2 x i32> %3677, <2 x i32>* %3680, align 4
  %3681 = load <2 x i64>, <2 x i64>* %3532, align 16
  %3682 = add nsw <2 x i64> %3681, <i64 1, i64 1>
  %3683 = lshr <2 x i64> %3681, <i64 63, i64 63>
  %3684 = add nsw <2 x i64> %3682, %3683
  %3685 = lshr <2 x i64> %3684, <i64 2, i64 2>
  %3686 = trunc <2 x i64> %3685 to <2 x i32>
  %3687 = or i64 %3573, 24
  %3688 = getelementptr inbounds i32, i32* %1, i64 %3687
  %3689 = bitcast i32* %3688 to <2 x i32>*
  store <2 x i32> %3686, <2 x i32>* %3689, align 4
  %3690 = load <2 x i64>, <2 x i64>* %3534, align 16
  %3691 = add nsw <2 x i64> %3690, <i64 1, i64 1>
  %3692 = lshr <2 x i64> %3690, <i64 63, i64 63>
  %3693 = add nsw <2 x i64> %3691, %3692
  %3694 = lshr <2 x i64> %3693, <i64 2, i64 2>
  %3695 = trunc <2 x i64> %3694 to <2 x i32>
  %3696 = or i64 %3573, 26
  %3697 = getelementptr inbounds i32, i32* %1, i64 %3696
  %3698 = bitcast i32* %3697 to <2 x i32>*
  store <2 x i32> %3695, <2 x i32>* %3698, align 4
  %3699 = load <2 x i64>, <2 x i64>* %3536, align 16
  %3700 = add nsw <2 x i64> %3699, <i64 1, i64 1>
  %3701 = lshr <2 x i64> %3699, <i64 63, i64 63>
  %3702 = add nsw <2 x i64> %3700, %3701
  %3703 = lshr <2 x i64> %3702, <i64 2, i64 2>
  %3704 = trunc <2 x i64> %3703 to <2 x i32>
  %3705 = or i64 %3573, 28
  %3706 = getelementptr inbounds i32, i32* %1, i64 %3705
  %3707 = bitcast i32* %3706 to <2 x i32>*
  store <2 x i32> %3704, <2 x i32>* %3707, align 4
  %3708 = load <2 x i64>, <2 x i64>* %3538, align 16
  %3709 = add nsw <2 x i64> %3708, <i64 1, i64 1>
  %3710 = lshr <2 x i64> %3708, <i64 63, i64 63>
  %3711 = add nsw <2 x i64> %3709, %3710
  %3712 = lshr <2 x i64> %3711, <i64 2, i64 2>
  %3713 = trunc <2 x i64> %3712 to <2 x i32>
  %3714 = or i64 %3573, 30
  %3715 = getelementptr inbounds i32, i32* %1, i64 %3714
  %3716 = bitcast i32* %3715 to <2 x i32>*
  store <2 x i32> %3713, <2 x i32>* %3716, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %3505) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %3504) #6
  %3717 = add nuw nsw i64 %3540, 1
  %3718 = icmp eq i64 %3717, 32
  br i1 %3718, label %11292, label %3539

3719:                                             ; preds = %3466
  %3720 = bitcast <2 x i64> %3406 to <4 x i32>
  %3721 = shufflevector <4 x i32> %3720, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3722 = bitcast <4 x i32> %3721 to <2 x i64>
  %3723 = bitcast <2 x i64> %3411 to <4 x i32>
  %3724 = shufflevector <4 x i32> %3723, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3725 = bitcast <4 x i32> %3724 to <2 x i64>
  %3726 = shufflevector <2 x i64> %3722, <2 x i64> %3725, <2 x i32> <i32 0, i32 2>
  %3727 = bitcast <2 x i64> %3416 to <4 x i32>
  %3728 = shufflevector <4 x i32> %3727, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3729 = bitcast <4 x i32> %3728 to <2 x i64>
  %3730 = bitcast <2 x i64> %3421 to <4 x i32>
  %3731 = shufflevector <4 x i32> %3730, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3732 = bitcast <4 x i32> %3731 to <2 x i64>
  %3733 = shufflevector <2 x i64> %3729, <2 x i64> %3732, <2 x i32> <i32 0, i32 2>
  %3734 = bitcast <2 x i64> %3423 to <4 x i32>
  %3735 = shufflevector <4 x i32> %3734, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3736 = bitcast <4 x i32> %3735 to <2 x i64>
  %3737 = bitcast <2 x i64> %3425 to <4 x i32>
  %3738 = shufflevector <4 x i32> %3737, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3739 = bitcast <4 x i32> %3738 to <2 x i64>
  %3740 = shufflevector <2 x i64> %3736, <2 x i64> %3739, <2 x i32> <i32 0, i32 2>
  %3741 = bitcast <2 x i64> %3427 to <4 x i32>
  %3742 = shufflevector <4 x i32> %3741, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3743 = bitcast <4 x i32> %3742 to <2 x i64>
  %3744 = bitcast <2 x i64> %3429 to <4 x i32>
  %3745 = shufflevector <4 x i32> %3744, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %3746 = bitcast <4 x i32> %3745 to <2 x i64>
  %3747 = shufflevector <2 x i64> %3743, <2 x i64> %3746, <2 x i32> <i32 0, i32 2>
  %3748 = bitcast <2 x i64> %3726 to <4 x i32>
  %3749 = add <4 x i32> %3748, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3750 = bitcast <2 x i64> %3733 to <4 x i32>
  %3751 = add <4 x i32> %3750, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3752 = bitcast <2 x i64> %3740 to <4 x i32>
  %3753 = add <4 x i32> %3752, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3754 = bitcast <2 x i64> %3747 to <4 x i32>
  %3755 = add <4 x i32> %3754, <i32 8192, i32 8192, i32 8192, i32 8192>
  %3756 = ashr <4 x i32> %3749, <i32 14, i32 14, i32 14, i32 14>
  %3757 = ashr <4 x i32> %3751, <i32 14, i32 14, i32 14, i32 14>
  %3758 = ashr <4 x i32> %3753, <i32 14, i32 14, i32 14, i32 14>
  %3759 = ashr <4 x i32> %3755, <i32 14, i32 14, i32 14, i32 14>
  %3760 = shufflevector <4 x i32> %3322, <4 x i32> %3344, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %3761 = bitcast <4 x i32> %3760 to <2 x i64>
  %3762 = shufflevector <4 x i32> %3322, <4 x i32> %3344, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %3763 = bitcast <4 x i32> %3762 to <2 x i64>
  %3764 = shufflevector <4 x i32> %3323, <4 x i32> %3345, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %3765 = bitcast <4 x i32> %3764 to <2 x i64>
  %3766 = shufflevector <4 x i32> %3323, <4 x i32> %3345, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %3767 = bitcast <4 x i32> %3766 to <2 x i64>
  %3768 = shufflevector <4 x i32> %3324, <4 x i32> %3342, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %3769 = bitcast <4 x i32> %3768 to <2 x i64>
  %3770 = shufflevector <4 x i32> %3324, <4 x i32> %3342, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %3771 = bitcast <4 x i32> %3770 to <2 x i64>
  %3772 = shufflevector <4 x i32> %3325, <4 x i32> %3343, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %3773 = bitcast <4 x i32> %3772 to <2 x i64>
  %3774 = shufflevector <4 x i32> %3325, <4 x i32> %3343, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %3775 = bitcast <4 x i32> %3774 to <2 x i64>
  %3776 = shufflevector <4 x i32> %3326, <4 x i32> %3340, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %3777 = bitcast <4 x i32> %3776 to <2 x i64>
  %3778 = shufflevector <4 x i32> %3326, <4 x i32> %3340, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %3779 = bitcast <4 x i32> %3778 to <2 x i64>
  %3780 = shufflevector <4 x i32> %3327, <4 x i32> %3341, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %3781 = bitcast <4 x i32> %3780 to <2 x i64>
  %3782 = shufflevector <4 x i32> %3327, <4 x i32> %3341, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %3783 = bitcast <4 x i32> %3782 to <2 x i64>
  %3784 = shufflevector <4 x i32> %3328, <4 x i32> %3338, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %3785 = bitcast <4 x i32> %3784 to <2 x i64>
  %3786 = shufflevector <4 x i32> %3328, <4 x i32> %3338, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %3787 = bitcast <4 x i32> %3786 to <2 x i64>
  %3788 = shufflevector <4 x i32> %3329, <4 x i32> %3339, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %3789 = bitcast <4 x i32> %3788 to <2 x i64>
  %3790 = shufflevector <4 x i32> %3329, <4 x i32> %3339, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %3791 = bitcast <4 x i32> %3790 to <2 x i64>
  %3792 = and <2 x i64> %3761, <i64 4294967295, i64 4294967295>
  %3793 = mul nuw <2 x i64> %3792, <i64 4294952159, i64 4294952159>
  %3794 = lshr <2 x i64> %3761, <i64 32, i64 32>
  %3795 = mul nuw nsw <2 x i64> %3794, <i64 6270, i64 6270>
  %3796 = add <2 x i64> %3795, %3793
  %3797 = and <2 x i64> %3763, <i64 4294967295, i64 4294967295>
  %3798 = mul nuw <2 x i64> %3797, <i64 4294952159, i64 4294952159>
  %3799 = lshr <2 x i64> %3763, <i64 32, i64 32>
  %3800 = mul nuw nsw <2 x i64> %3799, <i64 6270, i64 6270>
  %3801 = add <2 x i64> %3800, %3798
  %3802 = and <2 x i64> %3765, <i64 4294967295, i64 4294967295>
  %3803 = mul nuw <2 x i64> %3802, <i64 4294952159, i64 4294952159>
  %3804 = lshr <2 x i64> %3765, <i64 32, i64 32>
  %3805 = mul nuw nsw <2 x i64> %3804, <i64 6270, i64 6270>
  %3806 = add <2 x i64> %3805, %3803
  %3807 = and <2 x i64> %3767, <i64 4294967295, i64 4294967295>
  %3808 = mul nuw <2 x i64> %3807, <i64 4294952159, i64 4294952159>
  %3809 = lshr <2 x i64> %3767, <i64 32, i64 32>
  %3810 = mul nuw nsw <2 x i64> %3809, <i64 6270, i64 6270>
  %3811 = add <2 x i64> %3810, %3808
  %3812 = and <2 x i64> %3769, <i64 4294967295, i64 4294967295>
  %3813 = mul nuw <2 x i64> %3812, <i64 4294952159, i64 4294952159>
  %3814 = lshr <2 x i64> %3769, <i64 32, i64 32>
  %3815 = mul nuw nsw <2 x i64> %3814, <i64 6270, i64 6270>
  %3816 = add <2 x i64> %3815, %3813
  %3817 = and <2 x i64> %3771, <i64 4294967295, i64 4294967295>
  %3818 = mul nuw <2 x i64> %3817, <i64 4294952159, i64 4294952159>
  %3819 = lshr <2 x i64> %3771, <i64 32, i64 32>
  %3820 = mul nuw nsw <2 x i64> %3819, <i64 6270, i64 6270>
  %3821 = add <2 x i64> %3820, %3818
  %3822 = and <2 x i64> %3773, <i64 4294967295, i64 4294967295>
  %3823 = mul nuw <2 x i64> %3822, <i64 4294952159, i64 4294952159>
  %3824 = lshr <2 x i64> %3773, <i64 32, i64 32>
  %3825 = mul nuw nsw <2 x i64> %3824, <i64 6270, i64 6270>
  %3826 = add <2 x i64> %3825, %3823
  %3827 = and <2 x i64> %3775, <i64 4294967295, i64 4294967295>
  %3828 = mul nuw <2 x i64> %3827, <i64 4294952159, i64 4294952159>
  %3829 = lshr <2 x i64> %3775, <i64 32, i64 32>
  %3830 = mul nuw nsw <2 x i64> %3829, <i64 6270, i64 6270>
  %3831 = add <2 x i64> %3830, %3828
  %3832 = and <2 x i64> %3777, <i64 4294967295, i64 4294967295>
  %3833 = mul nuw <2 x i64> %3832, <i64 4294961026, i64 4294961026>
  %3834 = lshr <2 x i64> %3777, <i64 32, i64 32>
  %3835 = mul nuw <2 x i64> %3834, <i64 4294952159, i64 4294952159>
  %3836 = add <2 x i64> %3835, %3833
  %3837 = and <2 x i64> %3779, <i64 4294967295, i64 4294967295>
  %3838 = mul nuw <2 x i64> %3837, <i64 4294961026, i64 4294961026>
  %3839 = lshr <2 x i64> %3779, <i64 32, i64 32>
  %3840 = mul nuw <2 x i64> %3839, <i64 4294952159, i64 4294952159>
  %3841 = add <2 x i64> %3840, %3838
  %3842 = and <2 x i64> %3781, <i64 4294967295, i64 4294967295>
  %3843 = mul nuw <2 x i64> %3842, <i64 4294961026, i64 4294961026>
  %3844 = lshr <2 x i64> %3781, <i64 32, i64 32>
  %3845 = mul nuw <2 x i64> %3844, <i64 4294952159, i64 4294952159>
  %3846 = add <2 x i64> %3845, %3843
  %3847 = and <2 x i64> %3783, <i64 4294967295, i64 4294967295>
  %3848 = mul nuw <2 x i64> %3847, <i64 4294961026, i64 4294961026>
  %3849 = lshr <2 x i64> %3783, <i64 32, i64 32>
  %3850 = mul nuw <2 x i64> %3849, <i64 4294952159, i64 4294952159>
  %3851 = add <2 x i64> %3850, %3848
  %3852 = and <2 x i64> %3785, <i64 4294967295, i64 4294967295>
  %3853 = mul nuw <2 x i64> %3852, <i64 4294961026, i64 4294961026>
  %3854 = lshr <2 x i64> %3785, <i64 32, i64 32>
  %3855 = mul nuw <2 x i64> %3854, <i64 4294952159, i64 4294952159>
  %3856 = add <2 x i64> %3855, %3853
  %3857 = and <2 x i64> %3787, <i64 4294967295, i64 4294967295>
  %3858 = mul nuw <2 x i64> %3857, <i64 4294961026, i64 4294961026>
  %3859 = lshr <2 x i64> %3787, <i64 32, i64 32>
  %3860 = mul nuw <2 x i64> %3859, <i64 4294952159, i64 4294952159>
  %3861 = add <2 x i64> %3860, %3858
  %3862 = and <2 x i64> %3789, <i64 4294967295, i64 4294967295>
  %3863 = mul nuw <2 x i64> %3862, <i64 4294961026, i64 4294961026>
  %3864 = lshr <2 x i64> %3789, <i64 32, i64 32>
  %3865 = mul nuw <2 x i64> %3864, <i64 4294952159, i64 4294952159>
  %3866 = add <2 x i64> %3865, %3863
  %3867 = and <2 x i64> %3791, <i64 4294967295, i64 4294967295>
  %3868 = mul nuw <2 x i64> %3867, <i64 4294961026, i64 4294961026>
  %3869 = lshr <2 x i64> %3791, <i64 32, i64 32>
  %3870 = mul nuw <2 x i64> %3869, <i64 4294952159, i64 4294952159>
  %3871 = add <2 x i64> %3870, %3868
  %3872 = mul nuw <2 x i64> %3852, <i64 4294952159, i64 4294952159>
  %3873 = mul nuw nsw <2 x i64> %3854, <i64 6270, i64 6270>
  %3874 = add <2 x i64> %3873, %3872
  %3875 = mul nuw <2 x i64> %3857, <i64 4294952159, i64 4294952159>
  %3876 = mul nuw nsw <2 x i64> %3859, <i64 6270, i64 6270>
  %3877 = add <2 x i64> %3876, %3875
  %3878 = mul nuw <2 x i64> %3862, <i64 4294952159, i64 4294952159>
  %3879 = mul nuw nsw <2 x i64> %3864, <i64 6270, i64 6270>
  %3880 = add <2 x i64> %3879, %3878
  %3881 = mul nuw <2 x i64> %3867, <i64 4294952159, i64 4294952159>
  %3882 = mul nuw nsw <2 x i64> %3869, <i64 6270, i64 6270>
  %3883 = add <2 x i64> %3882, %3881
  %3884 = mul nuw <2 x i64> %3832, <i64 4294952159, i64 4294952159>
  %3885 = mul nuw nsw <2 x i64> %3834, <i64 6270, i64 6270>
  %3886 = add <2 x i64> %3885, %3884
  %3887 = mul nuw <2 x i64> %3837, <i64 4294952159, i64 4294952159>
  %3888 = mul nuw nsw <2 x i64> %3839, <i64 6270, i64 6270>
  %3889 = add <2 x i64> %3888, %3887
  %3890 = mul nuw <2 x i64> %3842, <i64 4294952159, i64 4294952159>
  %3891 = mul nuw nsw <2 x i64> %3844, <i64 6270, i64 6270>
  %3892 = add <2 x i64> %3891, %3890
  %3893 = mul nuw <2 x i64> %3847, <i64 4294952159, i64 4294952159>
  %3894 = mul nuw nsw <2 x i64> %3849, <i64 6270, i64 6270>
  %3895 = add <2 x i64> %3894, %3893
  %3896 = mul nuw nsw <2 x i64> %3812, <i64 6270, i64 6270>
  %3897 = mul nuw nsw <2 x i64> %3814, <i64 15137, i64 15137>
  %3898 = add nuw nsw <2 x i64> %3897, %3896
  %3899 = mul nuw nsw <2 x i64> %3817, <i64 6270, i64 6270>
  %3900 = mul nuw nsw <2 x i64> %3819, <i64 15137, i64 15137>
  %3901 = add nuw nsw <2 x i64> %3900, %3899
  %3902 = mul nuw nsw <2 x i64> %3822, <i64 6270, i64 6270>
  %3903 = mul nuw nsw <2 x i64> %3824, <i64 15137, i64 15137>
  %3904 = add nuw nsw <2 x i64> %3903, %3902
  %3905 = mul nuw nsw <2 x i64> %3827, <i64 6270, i64 6270>
  %3906 = mul nuw nsw <2 x i64> %3829, <i64 15137, i64 15137>
  %3907 = add nuw nsw <2 x i64> %3906, %3905
  %3908 = mul nuw nsw <2 x i64> %3792, <i64 6270, i64 6270>
  %3909 = mul nuw nsw <2 x i64> %3794, <i64 15137, i64 15137>
  %3910 = add nuw nsw <2 x i64> %3909, %3908
  %3911 = mul nuw nsw <2 x i64> %3797, <i64 6270, i64 6270>
  %3912 = mul nuw nsw <2 x i64> %3799, <i64 15137, i64 15137>
  %3913 = add nuw nsw <2 x i64> %3912, %3911
  %3914 = mul nuw nsw <2 x i64> %3802, <i64 6270, i64 6270>
  %3915 = mul nuw nsw <2 x i64> %3804, <i64 15137, i64 15137>
  %3916 = add nuw nsw <2 x i64> %3915, %3914
  %3917 = mul nuw nsw <2 x i64> %3807, <i64 6270, i64 6270>
  %3918 = mul nuw nsw <2 x i64> %3809, <i64 15137, i64 15137>
  %3919 = add nuw nsw <2 x i64> %3918, %3917
  %3920 = shl <2 x i64> %3796, <i64 1, i64 1>
  %3921 = shl <2 x i64> %3801, <i64 1, i64 1>
  %3922 = shl <2 x i64> %3806, <i64 1, i64 1>
  %3923 = shl <2 x i64> %3811, <i64 1, i64 1>
  %3924 = bitcast <2 x i64> %3920 to <4 x i32>
  %3925 = shufflevector <4 x i32> %3924, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %3926 = bitcast <4 x i32> %3925 to <2 x i64>
  %3927 = bitcast <2 x i64> %3921 to <4 x i32>
  %3928 = shufflevector <4 x i32> %3927, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %3929 = bitcast <4 x i32> %3928 to <2 x i64>
  %3930 = bitcast <2 x i64> %3922 to <4 x i32>
  %3931 = shufflevector <4 x i32> %3930, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %3932 = bitcast <4 x i32> %3931 to <2 x i64>
  %3933 = bitcast <2 x i64> %3923 to <4 x i32>
  %3934 = shufflevector <4 x i32> %3933, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %3935 = bitcast <4 x i32> %3934 to <2 x i64>
  %3936 = shufflevector <2 x i64> %3926, <2 x i64> %3929, <2 x i32> <i32 0, i32 2>
  %3937 = shufflevector <2 x i64> %3932, <2 x i64> %3935, <2 x i32> <i32 0, i32 2>
  %3938 = bitcast <2 x i64> %3936 to <4 x i32>
  %3939 = bitcast <2 x i64> %3937 to <4 x i32>
  %3940 = add <4 x i32> %3938, <i32 1, i32 1, i32 1, i32 1>
  %3941 = icmp ugt <4 x i32> %3940, <i32 1, i32 1, i32 1, i32 1>
  %3942 = sext <4 x i1> %3941 to <4 x i32>
  %3943 = bitcast <4 x i32> %3942 to <16 x i8>
  %3944 = icmp slt <16 x i8> %3943, zeroinitializer
  %3945 = bitcast <16 x i1> %3944 to i16
  %3946 = zext i16 %3945 to i32
  %3947 = add <4 x i32> %3939, <i32 1, i32 1, i32 1, i32 1>
  %3948 = icmp ugt <4 x i32> %3947, <i32 1, i32 1, i32 1, i32 1>
  %3949 = sext <4 x i1> %3948 to <4 x i32>
  %3950 = bitcast <4 x i32> %3949 to <16 x i8>
  %3951 = icmp slt <16 x i8> %3950, zeroinitializer
  %3952 = bitcast <16 x i1> %3951 to i16
  %3953 = zext i16 %3952 to i32
  %3954 = sub nsw i32 0, %3946
  %3955 = icmp eq i32 %3953, %3954
  br i1 %3955, label %3956, label %4215

3956:                                             ; preds = %3719
  %3957 = shl <2 x i64> %3816, <i64 1, i64 1>
  %3958 = shl <2 x i64> %3821, <i64 1, i64 1>
  %3959 = shl <2 x i64> %3826, <i64 1, i64 1>
  %3960 = shl <2 x i64> %3831, <i64 1, i64 1>
  %3961 = bitcast <2 x i64> %3957 to <4 x i32>
  %3962 = shufflevector <4 x i32> %3961, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %3963 = bitcast <4 x i32> %3962 to <2 x i64>
  %3964 = bitcast <2 x i64> %3958 to <4 x i32>
  %3965 = shufflevector <4 x i32> %3964, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %3966 = bitcast <4 x i32> %3965 to <2 x i64>
  %3967 = bitcast <2 x i64> %3959 to <4 x i32>
  %3968 = shufflevector <4 x i32> %3967, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %3969 = bitcast <4 x i32> %3968 to <2 x i64>
  %3970 = bitcast <2 x i64> %3960 to <4 x i32>
  %3971 = shufflevector <4 x i32> %3970, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %3972 = bitcast <4 x i32> %3971 to <2 x i64>
  %3973 = shufflevector <2 x i64> %3963, <2 x i64> %3966, <2 x i32> <i32 0, i32 2>
  %3974 = shufflevector <2 x i64> %3969, <2 x i64> %3972, <2 x i32> <i32 0, i32 2>
  %3975 = bitcast <2 x i64> %3973 to <4 x i32>
  %3976 = bitcast <2 x i64> %3974 to <4 x i32>
  %3977 = add <4 x i32> %3975, <i32 1, i32 1, i32 1, i32 1>
  %3978 = icmp ugt <4 x i32> %3977, <i32 1, i32 1, i32 1, i32 1>
  %3979 = sext <4 x i1> %3978 to <4 x i32>
  %3980 = bitcast <4 x i32> %3979 to <16 x i8>
  %3981 = icmp slt <16 x i8> %3980, zeroinitializer
  %3982 = bitcast <16 x i1> %3981 to i16
  %3983 = zext i16 %3982 to i32
  %3984 = add <4 x i32> %3976, <i32 1, i32 1, i32 1, i32 1>
  %3985 = icmp ugt <4 x i32> %3984, <i32 1, i32 1, i32 1, i32 1>
  %3986 = sext <4 x i1> %3985 to <4 x i32>
  %3987 = bitcast <4 x i32> %3986 to <16 x i8>
  %3988 = icmp slt <16 x i8> %3987, zeroinitializer
  %3989 = bitcast <16 x i1> %3988 to i16
  %3990 = zext i16 %3989 to i32
  %3991 = sub nsw i32 0, %3983
  %3992 = icmp eq i32 %3990, %3991
  br i1 %3992, label %3993, label %4215

3993:                                             ; preds = %3956
  %3994 = shl <2 x i64> %3836, <i64 1, i64 1>
  %3995 = shl <2 x i64> %3841, <i64 1, i64 1>
  %3996 = shl <2 x i64> %3846, <i64 1, i64 1>
  %3997 = shl <2 x i64> %3851, <i64 1, i64 1>
  %3998 = bitcast <2 x i64> %3994 to <4 x i32>
  %3999 = shufflevector <4 x i32> %3998, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4000 = bitcast <4 x i32> %3999 to <2 x i64>
  %4001 = bitcast <2 x i64> %3995 to <4 x i32>
  %4002 = shufflevector <4 x i32> %4001, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4003 = bitcast <4 x i32> %4002 to <2 x i64>
  %4004 = bitcast <2 x i64> %3996 to <4 x i32>
  %4005 = shufflevector <4 x i32> %4004, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4006 = bitcast <4 x i32> %4005 to <2 x i64>
  %4007 = bitcast <2 x i64> %3997 to <4 x i32>
  %4008 = shufflevector <4 x i32> %4007, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4009 = bitcast <4 x i32> %4008 to <2 x i64>
  %4010 = shufflevector <2 x i64> %4000, <2 x i64> %4003, <2 x i32> <i32 0, i32 2>
  %4011 = shufflevector <2 x i64> %4006, <2 x i64> %4009, <2 x i32> <i32 0, i32 2>
  %4012 = bitcast <2 x i64> %4010 to <4 x i32>
  %4013 = bitcast <2 x i64> %4011 to <4 x i32>
  %4014 = add <4 x i32> %4012, <i32 1, i32 1, i32 1, i32 1>
  %4015 = icmp ugt <4 x i32> %4014, <i32 1, i32 1, i32 1, i32 1>
  %4016 = sext <4 x i1> %4015 to <4 x i32>
  %4017 = bitcast <4 x i32> %4016 to <16 x i8>
  %4018 = icmp slt <16 x i8> %4017, zeroinitializer
  %4019 = bitcast <16 x i1> %4018 to i16
  %4020 = zext i16 %4019 to i32
  %4021 = add <4 x i32> %4013, <i32 1, i32 1, i32 1, i32 1>
  %4022 = icmp ugt <4 x i32> %4021, <i32 1, i32 1, i32 1, i32 1>
  %4023 = sext <4 x i1> %4022 to <4 x i32>
  %4024 = bitcast <4 x i32> %4023 to <16 x i8>
  %4025 = icmp slt <16 x i8> %4024, zeroinitializer
  %4026 = bitcast <16 x i1> %4025 to i16
  %4027 = zext i16 %4026 to i32
  %4028 = sub nsw i32 0, %4020
  %4029 = icmp eq i32 %4027, %4028
  br i1 %4029, label %4030, label %4215

4030:                                             ; preds = %3993
  %4031 = shl <2 x i64> %3856, <i64 1, i64 1>
  %4032 = shl <2 x i64> %3861, <i64 1, i64 1>
  %4033 = shl <2 x i64> %3866, <i64 1, i64 1>
  %4034 = shl <2 x i64> %3871, <i64 1, i64 1>
  %4035 = bitcast <2 x i64> %4031 to <4 x i32>
  %4036 = shufflevector <4 x i32> %4035, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4037 = bitcast <4 x i32> %4036 to <2 x i64>
  %4038 = bitcast <2 x i64> %4032 to <4 x i32>
  %4039 = shufflevector <4 x i32> %4038, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4040 = bitcast <4 x i32> %4039 to <2 x i64>
  %4041 = bitcast <2 x i64> %4033 to <4 x i32>
  %4042 = shufflevector <4 x i32> %4041, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4043 = bitcast <4 x i32> %4042 to <2 x i64>
  %4044 = bitcast <2 x i64> %4034 to <4 x i32>
  %4045 = shufflevector <4 x i32> %4044, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4046 = bitcast <4 x i32> %4045 to <2 x i64>
  %4047 = shufflevector <2 x i64> %4037, <2 x i64> %4040, <2 x i32> <i32 0, i32 2>
  %4048 = shufflevector <2 x i64> %4043, <2 x i64> %4046, <2 x i32> <i32 0, i32 2>
  %4049 = bitcast <2 x i64> %4047 to <4 x i32>
  %4050 = bitcast <2 x i64> %4048 to <4 x i32>
  %4051 = add <4 x i32> %4049, <i32 1, i32 1, i32 1, i32 1>
  %4052 = icmp ugt <4 x i32> %4051, <i32 1, i32 1, i32 1, i32 1>
  %4053 = sext <4 x i1> %4052 to <4 x i32>
  %4054 = bitcast <4 x i32> %4053 to <16 x i8>
  %4055 = icmp slt <16 x i8> %4054, zeroinitializer
  %4056 = bitcast <16 x i1> %4055 to i16
  %4057 = zext i16 %4056 to i32
  %4058 = add <4 x i32> %4050, <i32 1, i32 1, i32 1, i32 1>
  %4059 = icmp ugt <4 x i32> %4058, <i32 1, i32 1, i32 1, i32 1>
  %4060 = sext <4 x i1> %4059 to <4 x i32>
  %4061 = bitcast <4 x i32> %4060 to <16 x i8>
  %4062 = icmp slt <16 x i8> %4061, zeroinitializer
  %4063 = bitcast <16 x i1> %4062 to i16
  %4064 = zext i16 %4063 to i32
  %4065 = sub nsw i32 0, %4057
  %4066 = icmp eq i32 %4064, %4065
  br i1 %4066, label %4067, label %4215

4067:                                             ; preds = %4030
  %4068 = shl <2 x i64> %3874, <i64 1, i64 1>
  %4069 = shl <2 x i64> %3877, <i64 1, i64 1>
  %4070 = shl <2 x i64> %3880, <i64 1, i64 1>
  %4071 = shl <2 x i64> %3883, <i64 1, i64 1>
  %4072 = bitcast <2 x i64> %4068 to <4 x i32>
  %4073 = shufflevector <4 x i32> %4072, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4074 = bitcast <4 x i32> %4073 to <2 x i64>
  %4075 = bitcast <2 x i64> %4069 to <4 x i32>
  %4076 = shufflevector <4 x i32> %4075, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4077 = bitcast <4 x i32> %4076 to <2 x i64>
  %4078 = bitcast <2 x i64> %4070 to <4 x i32>
  %4079 = shufflevector <4 x i32> %4078, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4080 = bitcast <4 x i32> %4079 to <2 x i64>
  %4081 = bitcast <2 x i64> %4071 to <4 x i32>
  %4082 = shufflevector <4 x i32> %4081, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4083 = bitcast <4 x i32> %4082 to <2 x i64>
  %4084 = shufflevector <2 x i64> %4074, <2 x i64> %4077, <2 x i32> <i32 0, i32 2>
  %4085 = shufflevector <2 x i64> %4080, <2 x i64> %4083, <2 x i32> <i32 0, i32 2>
  %4086 = bitcast <2 x i64> %4084 to <4 x i32>
  %4087 = bitcast <2 x i64> %4085 to <4 x i32>
  %4088 = add <4 x i32> %4086, <i32 1, i32 1, i32 1, i32 1>
  %4089 = icmp ugt <4 x i32> %4088, <i32 1, i32 1, i32 1, i32 1>
  %4090 = sext <4 x i1> %4089 to <4 x i32>
  %4091 = bitcast <4 x i32> %4090 to <16 x i8>
  %4092 = icmp slt <16 x i8> %4091, zeroinitializer
  %4093 = bitcast <16 x i1> %4092 to i16
  %4094 = zext i16 %4093 to i32
  %4095 = add <4 x i32> %4087, <i32 1, i32 1, i32 1, i32 1>
  %4096 = icmp ugt <4 x i32> %4095, <i32 1, i32 1, i32 1, i32 1>
  %4097 = sext <4 x i1> %4096 to <4 x i32>
  %4098 = bitcast <4 x i32> %4097 to <16 x i8>
  %4099 = icmp slt <16 x i8> %4098, zeroinitializer
  %4100 = bitcast <16 x i1> %4099 to i16
  %4101 = zext i16 %4100 to i32
  %4102 = sub nsw i32 0, %4094
  %4103 = icmp eq i32 %4101, %4102
  br i1 %4103, label %4104, label %4215

4104:                                             ; preds = %4067
  %4105 = shl <2 x i64> %3886, <i64 1, i64 1>
  %4106 = shl <2 x i64> %3889, <i64 1, i64 1>
  %4107 = shl <2 x i64> %3892, <i64 1, i64 1>
  %4108 = shl <2 x i64> %3895, <i64 1, i64 1>
  %4109 = bitcast <2 x i64> %4105 to <4 x i32>
  %4110 = shufflevector <4 x i32> %4109, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4111 = bitcast <4 x i32> %4110 to <2 x i64>
  %4112 = bitcast <2 x i64> %4106 to <4 x i32>
  %4113 = shufflevector <4 x i32> %4112, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4114 = bitcast <4 x i32> %4113 to <2 x i64>
  %4115 = bitcast <2 x i64> %4107 to <4 x i32>
  %4116 = shufflevector <4 x i32> %4115, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4117 = bitcast <4 x i32> %4116 to <2 x i64>
  %4118 = bitcast <2 x i64> %4108 to <4 x i32>
  %4119 = shufflevector <4 x i32> %4118, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4120 = bitcast <4 x i32> %4119 to <2 x i64>
  %4121 = shufflevector <2 x i64> %4111, <2 x i64> %4114, <2 x i32> <i32 0, i32 2>
  %4122 = shufflevector <2 x i64> %4117, <2 x i64> %4120, <2 x i32> <i32 0, i32 2>
  %4123 = bitcast <2 x i64> %4121 to <4 x i32>
  %4124 = bitcast <2 x i64> %4122 to <4 x i32>
  %4125 = add <4 x i32> %4123, <i32 1, i32 1, i32 1, i32 1>
  %4126 = icmp ugt <4 x i32> %4125, <i32 1, i32 1, i32 1, i32 1>
  %4127 = sext <4 x i1> %4126 to <4 x i32>
  %4128 = bitcast <4 x i32> %4127 to <16 x i8>
  %4129 = icmp slt <16 x i8> %4128, zeroinitializer
  %4130 = bitcast <16 x i1> %4129 to i16
  %4131 = zext i16 %4130 to i32
  %4132 = add <4 x i32> %4124, <i32 1, i32 1, i32 1, i32 1>
  %4133 = icmp ugt <4 x i32> %4132, <i32 1, i32 1, i32 1, i32 1>
  %4134 = sext <4 x i1> %4133 to <4 x i32>
  %4135 = bitcast <4 x i32> %4134 to <16 x i8>
  %4136 = icmp slt <16 x i8> %4135, zeroinitializer
  %4137 = bitcast <16 x i1> %4136 to i16
  %4138 = zext i16 %4137 to i32
  %4139 = sub nsw i32 0, %4131
  %4140 = icmp eq i32 %4138, %4139
  br i1 %4140, label %4141, label %4215

4141:                                             ; preds = %4104
  %4142 = shl nuw nsw <2 x i64> %3898, <i64 1, i64 1>
  %4143 = shl nuw nsw <2 x i64> %3901, <i64 1, i64 1>
  %4144 = shl nuw nsw <2 x i64> %3904, <i64 1, i64 1>
  %4145 = shl nuw nsw <2 x i64> %3907, <i64 1, i64 1>
  %4146 = bitcast <2 x i64> %4142 to <4 x i32>
  %4147 = shufflevector <4 x i32> %4146, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4148 = bitcast <4 x i32> %4147 to <2 x i64>
  %4149 = bitcast <2 x i64> %4143 to <4 x i32>
  %4150 = shufflevector <4 x i32> %4149, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4151 = bitcast <4 x i32> %4150 to <2 x i64>
  %4152 = bitcast <2 x i64> %4144 to <4 x i32>
  %4153 = shufflevector <4 x i32> %4152, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4154 = bitcast <4 x i32> %4153 to <2 x i64>
  %4155 = bitcast <2 x i64> %4145 to <4 x i32>
  %4156 = shufflevector <4 x i32> %4155, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4157 = bitcast <4 x i32> %4156 to <2 x i64>
  %4158 = shufflevector <2 x i64> %4148, <2 x i64> %4151, <2 x i32> <i32 0, i32 2>
  %4159 = shufflevector <2 x i64> %4154, <2 x i64> %4157, <2 x i32> <i32 0, i32 2>
  %4160 = bitcast <2 x i64> %4158 to <4 x i32>
  %4161 = bitcast <2 x i64> %4159 to <4 x i32>
  %4162 = add <4 x i32> %4160, <i32 1, i32 1, i32 1, i32 1>
  %4163 = icmp ugt <4 x i32> %4162, <i32 1, i32 1, i32 1, i32 1>
  %4164 = sext <4 x i1> %4163 to <4 x i32>
  %4165 = bitcast <4 x i32> %4164 to <16 x i8>
  %4166 = icmp slt <16 x i8> %4165, zeroinitializer
  %4167 = bitcast <16 x i1> %4166 to i16
  %4168 = zext i16 %4167 to i32
  %4169 = add <4 x i32> %4161, <i32 1, i32 1, i32 1, i32 1>
  %4170 = icmp ugt <4 x i32> %4169, <i32 1, i32 1, i32 1, i32 1>
  %4171 = sext <4 x i1> %4170 to <4 x i32>
  %4172 = bitcast <4 x i32> %4171 to <16 x i8>
  %4173 = icmp slt <16 x i8> %4172, zeroinitializer
  %4174 = bitcast <16 x i1> %4173 to i16
  %4175 = zext i16 %4174 to i32
  %4176 = sub nsw i32 0, %4168
  %4177 = icmp eq i32 %4175, %4176
  br i1 %4177, label %4178, label %4215

4178:                                             ; preds = %4141
  %4179 = shl nuw nsw <2 x i64> %3910, <i64 1, i64 1>
  %4180 = shl nuw nsw <2 x i64> %3913, <i64 1, i64 1>
  %4181 = shl nuw nsw <2 x i64> %3916, <i64 1, i64 1>
  %4182 = shl nuw nsw <2 x i64> %3919, <i64 1, i64 1>
  %4183 = bitcast <2 x i64> %4179 to <4 x i32>
  %4184 = shufflevector <4 x i32> %4183, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4185 = bitcast <4 x i32> %4184 to <2 x i64>
  %4186 = bitcast <2 x i64> %4180 to <4 x i32>
  %4187 = shufflevector <4 x i32> %4186, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4188 = bitcast <4 x i32> %4187 to <2 x i64>
  %4189 = bitcast <2 x i64> %4181 to <4 x i32>
  %4190 = shufflevector <4 x i32> %4189, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4191 = bitcast <4 x i32> %4190 to <2 x i64>
  %4192 = bitcast <2 x i64> %4182 to <4 x i32>
  %4193 = shufflevector <4 x i32> %4192, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4194 = bitcast <4 x i32> %4193 to <2 x i64>
  %4195 = shufflevector <2 x i64> %4185, <2 x i64> %4188, <2 x i32> <i32 0, i32 2>
  %4196 = shufflevector <2 x i64> %4191, <2 x i64> %4194, <2 x i32> <i32 0, i32 2>
  %4197 = bitcast <2 x i64> %4195 to <4 x i32>
  %4198 = bitcast <2 x i64> %4196 to <4 x i32>
  %4199 = add <4 x i32> %4197, <i32 1, i32 1, i32 1, i32 1>
  %4200 = icmp ugt <4 x i32> %4199, <i32 1, i32 1, i32 1, i32 1>
  %4201 = sext <4 x i1> %4200 to <4 x i32>
  %4202 = bitcast <4 x i32> %4201 to <16 x i8>
  %4203 = icmp slt <16 x i8> %4202, zeroinitializer
  %4204 = bitcast <16 x i1> %4203 to i16
  %4205 = zext i16 %4204 to i32
  %4206 = add <4 x i32> %4198, <i32 1, i32 1, i32 1, i32 1>
  %4207 = icmp ugt <4 x i32> %4206, <i32 1, i32 1, i32 1, i32 1>
  %4208 = sext <4 x i1> %4207 to <4 x i32>
  %4209 = bitcast <4 x i32> %4208 to <16 x i8>
  %4210 = icmp slt <16 x i8> %4209, zeroinitializer
  %4211 = bitcast <16 x i1> %4210 to i16
  %4212 = zext i16 %4211 to i32
  %4213 = sub nsw i32 0, %4205
  %4214 = icmp eq i32 %4212, %4213
  br i1 %4214, label %4431, label %4215

4215:                                             ; preds = %4141, %4104, %4067, %4030, %3993, %3956, %3719, %4178
  %4216 = bitcast [32 x i64]* %4 to i8*
  %4217 = bitcast [32 x i64]* %5 to i8*
  %4218 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %4219 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %4220 = bitcast [32 x i64]* %5 to <2 x i64>*
  %4221 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %4222 = bitcast i64* %4221 to <2 x i64>*
  %4223 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %4224 = bitcast i64* %4223 to <2 x i64>*
  %4225 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %4226 = bitcast i64* %4225 to <2 x i64>*
  %4227 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %4228 = bitcast i64* %4227 to <2 x i64>*
  %4229 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %4230 = bitcast i64* %4229 to <2 x i64>*
  %4231 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %4232 = bitcast i64* %4231 to <2 x i64>*
  %4233 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %4234 = bitcast i64* %4233 to <2 x i64>*
  %4235 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %4236 = bitcast i64* %4235 to <2 x i64>*
  %4237 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %4238 = bitcast i64* %4237 to <2 x i64>*
  %4239 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %4240 = bitcast i64* %4239 to <2 x i64>*
  %4241 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %4242 = bitcast i64* %4241 to <2 x i64>*
  %4243 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %4244 = bitcast i64* %4243 to <2 x i64>*
  %4245 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %4246 = bitcast i64* %4245 to <2 x i64>*
  %4247 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %4248 = bitcast i64* %4247 to <2 x i64>*
  %4249 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %4250 = bitcast i64* %4249 to <2 x i64>*
  br label %4251

4251:                                             ; preds = %4284, %4215
  %4252 = phi i64 [ 0, %4215 ], [ %4429, %4284 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %4216) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %4216, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %4217) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %4217, i8 -86, i64 256, i1 false) #6
  br label %4253

4253:                                             ; preds = %4253, %4251
  %4254 = phi i64 [ 0, %4251 ], [ %4282, %4253 ]
  %4255 = shl i64 %4254, 5
  %4256 = add nuw nsw i64 %4255, %4252
  %4257 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %4256
  %4258 = load i16, i16* %4257, align 2
  %4259 = sext i16 %4258 to i64
  %4260 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %4254
  store i64 %4259, i64* %4260, align 16
  %4261 = or i64 %4254, 1
  %4262 = shl i64 %4261, 5
  %4263 = add nuw nsw i64 %4262, %4252
  %4264 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %4263
  %4265 = load i16, i16* %4264, align 2
  %4266 = sext i16 %4265 to i64
  %4267 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %4261
  store i64 %4266, i64* %4267, align 8
  %4268 = or i64 %4254, 2
  %4269 = shl i64 %4268, 5
  %4270 = add nuw nsw i64 %4269, %4252
  %4271 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %4270
  %4272 = load i16, i16* %4271, align 2
  %4273 = sext i16 %4272 to i64
  %4274 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %4268
  store i64 %4273, i64* %4274, align 16
  %4275 = or i64 %4254, 3
  %4276 = shl i64 %4275, 5
  %4277 = add nuw nsw i64 %4276, %4252
  %4278 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %4277
  %4279 = load i16, i16* %4278, align 2
  %4280 = sext i16 %4279 to i64
  %4281 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %4275
  store i64 %4280, i64* %4281, align 8
  %4282 = add nuw nsw i64 %4254, 4
  %4283 = icmp eq i64 %4282, 32
  br i1 %4283, label %4284, label %4253

4284:                                             ; preds = %4253
  call void @vpx_fdct32(i64* nonnull %4218, i64* nonnull %4219, i32 0) #6
  %4285 = shl i64 %4252, 5
  %4286 = load <2 x i64>, <2 x i64>* %4220, align 16
  %4287 = add nsw <2 x i64> %4286, <i64 1, i64 1>
  %4288 = lshr <2 x i64> %4286, <i64 63, i64 63>
  %4289 = add nsw <2 x i64> %4287, %4288
  %4290 = lshr <2 x i64> %4289, <i64 2, i64 2>
  %4291 = trunc <2 x i64> %4290 to <2 x i32>
  %4292 = getelementptr inbounds i32, i32* %1, i64 %4285
  %4293 = bitcast i32* %4292 to <2 x i32>*
  store <2 x i32> %4291, <2 x i32>* %4293, align 4
  %4294 = load <2 x i64>, <2 x i64>* %4222, align 16
  %4295 = add nsw <2 x i64> %4294, <i64 1, i64 1>
  %4296 = lshr <2 x i64> %4294, <i64 63, i64 63>
  %4297 = add nsw <2 x i64> %4295, %4296
  %4298 = lshr <2 x i64> %4297, <i64 2, i64 2>
  %4299 = trunc <2 x i64> %4298 to <2 x i32>
  %4300 = or i64 %4285, 2
  %4301 = getelementptr inbounds i32, i32* %1, i64 %4300
  %4302 = bitcast i32* %4301 to <2 x i32>*
  store <2 x i32> %4299, <2 x i32>* %4302, align 4
  %4303 = load <2 x i64>, <2 x i64>* %4224, align 16
  %4304 = add nsw <2 x i64> %4303, <i64 1, i64 1>
  %4305 = lshr <2 x i64> %4303, <i64 63, i64 63>
  %4306 = add nsw <2 x i64> %4304, %4305
  %4307 = lshr <2 x i64> %4306, <i64 2, i64 2>
  %4308 = trunc <2 x i64> %4307 to <2 x i32>
  %4309 = or i64 %4285, 4
  %4310 = getelementptr inbounds i32, i32* %1, i64 %4309
  %4311 = bitcast i32* %4310 to <2 x i32>*
  store <2 x i32> %4308, <2 x i32>* %4311, align 4
  %4312 = load <2 x i64>, <2 x i64>* %4226, align 16
  %4313 = add nsw <2 x i64> %4312, <i64 1, i64 1>
  %4314 = lshr <2 x i64> %4312, <i64 63, i64 63>
  %4315 = add nsw <2 x i64> %4313, %4314
  %4316 = lshr <2 x i64> %4315, <i64 2, i64 2>
  %4317 = trunc <2 x i64> %4316 to <2 x i32>
  %4318 = or i64 %4285, 6
  %4319 = getelementptr inbounds i32, i32* %1, i64 %4318
  %4320 = bitcast i32* %4319 to <2 x i32>*
  store <2 x i32> %4317, <2 x i32>* %4320, align 4
  %4321 = load <2 x i64>, <2 x i64>* %4228, align 16
  %4322 = add nsw <2 x i64> %4321, <i64 1, i64 1>
  %4323 = lshr <2 x i64> %4321, <i64 63, i64 63>
  %4324 = add nsw <2 x i64> %4322, %4323
  %4325 = lshr <2 x i64> %4324, <i64 2, i64 2>
  %4326 = trunc <2 x i64> %4325 to <2 x i32>
  %4327 = or i64 %4285, 8
  %4328 = getelementptr inbounds i32, i32* %1, i64 %4327
  %4329 = bitcast i32* %4328 to <2 x i32>*
  store <2 x i32> %4326, <2 x i32>* %4329, align 4
  %4330 = load <2 x i64>, <2 x i64>* %4230, align 16
  %4331 = add nsw <2 x i64> %4330, <i64 1, i64 1>
  %4332 = lshr <2 x i64> %4330, <i64 63, i64 63>
  %4333 = add nsw <2 x i64> %4331, %4332
  %4334 = lshr <2 x i64> %4333, <i64 2, i64 2>
  %4335 = trunc <2 x i64> %4334 to <2 x i32>
  %4336 = or i64 %4285, 10
  %4337 = getelementptr inbounds i32, i32* %1, i64 %4336
  %4338 = bitcast i32* %4337 to <2 x i32>*
  store <2 x i32> %4335, <2 x i32>* %4338, align 4
  %4339 = load <2 x i64>, <2 x i64>* %4232, align 16
  %4340 = add nsw <2 x i64> %4339, <i64 1, i64 1>
  %4341 = lshr <2 x i64> %4339, <i64 63, i64 63>
  %4342 = add nsw <2 x i64> %4340, %4341
  %4343 = lshr <2 x i64> %4342, <i64 2, i64 2>
  %4344 = trunc <2 x i64> %4343 to <2 x i32>
  %4345 = or i64 %4285, 12
  %4346 = getelementptr inbounds i32, i32* %1, i64 %4345
  %4347 = bitcast i32* %4346 to <2 x i32>*
  store <2 x i32> %4344, <2 x i32>* %4347, align 4
  %4348 = load <2 x i64>, <2 x i64>* %4234, align 16
  %4349 = add nsw <2 x i64> %4348, <i64 1, i64 1>
  %4350 = lshr <2 x i64> %4348, <i64 63, i64 63>
  %4351 = add nsw <2 x i64> %4349, %4350
  %4352 = lshr <2 x i64> %4351, <i64 2, i64 2>
  %4353 = trunc <2 x i64> %4352 to <2 x i32>
  %4354 = or i64 %4285, 14
  %4355 = getelementptr inbounds i32, i32* %1, i64 %4354
  %4356 = bitcast i32* %4355 to <2 x i32>*
  store <2 x i32> %4353, <2 x i32>* %4356, align 4
  %4357 = load <2 x i64>, <2 x i64>* %4236, align 16
  %4358 = add nsw <2 x i64> %4357, <i64 1, i64 1>
  %4359 = lshr <2 x i64> %4357, <i64 63, i64 63>
  %4360 = add nsw <2 x i64> %4358, %4359
  %4361 = lshr <2 x i64> %4360, <i64 2, i64 2>
  %4362 = trunc <2 x i64> %4361 to <2 x i32>
  %4363 = or i64 %4285, 16
  %4364 = getelementptr inbounds i32, i32* %1, i64 %4363
  %4365 = bitcast i32* %4364 to <2 x i32>*
  store <2 x i32> %4362, <2 x i32>* %4365, align 4
  %4366 = load <2 x i64>, <2 x i64>* %4238, align 16
  %4367 = add nsw <2 x i64> %4366, <i64 1, i64 1>
  %4368 = lshr <2 x i64> %4366, <i64 63, i64 63>
  %4369 = add nsw <2 x i64> %4367, %4368
  %4370 = lshr <2 x i64> %4369, <i64 2, i64 2>
  %4371 = trunc <2 x i64> %4370 to <2 x i32>
  %4372 = or i64 %4285, 18
  %4373 = getelementptr inbounds i32, i32* %1, i64 %4372
  %4374 = bitcast i32* %4373 to <2 x i32>*
  store <2 x i32> %4371, <2 x i32>* %4374, align 4
  %4375 = load <2 x i64>, <2 x i64>* %4240, align 16
  %4376 = add nsw <2 x i64> %4375, <i64 1, i64 1>
  %4377 = lshr <2 x i64> %4375, <i64 63, i64 63>
  %4378 = add nsw <2 x i64> %4376, %4377
  %4379 = lshr <2 x i64> %4378, <i64 2, i64 2>
  %4380 = trunc <2 x i64> %4379 to <2 x i32>
  %4381 = or i64 %4285, 20
  %4382 = getelementptr inbounds i32, i32* %1, i64 %4381
  %4383 = bitcast i32* %4382 to <2 x i32>*
  store <2 x i32> %4380, <2 x i32>* %4383, align 4
  %4384 = load <2 x i64>, <2 x i64>* %4242, align 16
  %4385 = add nsw <2 x i64> %4384, <i64 1, i64 1>
  %4386 = lshr <2 x i64> %4384, <i64 63, i64 63>
  %4387 = add nsw <2 x i64> %4385, %4386
  %4388 = lshr <2 x i64> %4387, <i64 2, i64 2>
  %4389 = trunc <2 x i64> %4388 to <2 x i32>
  %4390 = or i64 %4285, 22
  %4391 = getelementptr inbounds i32, i32* %1, i64 %4390
  %4392 = bitcast i32* %4391 to <2 x i32>*
  store <2 x i32> %4389, <2 x i32>* %4392, align 4
  %4393 = load <2 x i64>, <2 x i64>* %4244, align 16
  %4394 = add nsw <2 x i64> %4393, <i64 1, i64 1>
  %4395 = lshr <2 x i64> %4393, <i64 63, i64 63>
  %4396 = add nsw <2 x i64> %4394, %4395
  %4397 = lshr <2 x i64> %4396, <i64 2, i64 2>
  %4398 = trunc <2 x i64> %4397 to <2 x i32>
  %4399 = or i64 %4285, 24
  %4400 = getelementptr inbounds i32, i32* %1, i64 %4399
  %4401 = bitcast i32* %4400 to <2 x i32>*
  store <2 x i32> %4398, <2 x i32>* %4401, align 4
  %4402 = load <2 x i64>, <2 x i64>* %4246, align 16
  %4403 = add nsw <2 x i64> %4402, <i64 1, i64 1>
  %4404 = lshr <2 x i64> %4402, <i64 63, i64 63>
  %4405 = add nsw <2 x i64> %4403, %4404
  %4406 = lshr <2 x i64> %4405, <i64 2, i64 2>
  %4407 = trunc <2 x i64> %4406 to <2 x i32>
  %4408 = or i64 %4285, 26
  %4409 = getelementptr inbounds i32, i32* %1, i64 %4408
  %4410 = bitcast i32* %4409 to <2 x i32>*
  store <2 x i32> %4407, <2 x i32>* %4410, align 4
  %4411 = load <2 x i64>, <2 x i64>* %4248, align 16
  %4412 = add nsw <2 x i64> %4411, <i64 1, i64 1>
  %4413 = lshr <2 x i64> %4411, <i64 63, i64 63>
  %4414 = add nsw <2 x i64> %4412, %4413
  %4415 = lshr <2 x i64> %4414, <i64 2, i64 2>
  %4416 = trunc <2 x i64> %4415 to <2 x i32>
  %4417 = or i64 %4285, 28
  %4418 = getelementptr inbounds i32, i32* %1, i64 %4417
  %4419 = bitcast i32* %4418 to <2 x i32>*
  store <2 x i32> %4416, <2 x i32>* %4419, align 4
  %4420 = load <2 x i64>, <2 x i64>* %4250, align 16
  %4421 = add nsw <2 x i64> %4420, <i64 1, i64 1>
  %4422 = lshr <2 x i64> %4420, <i64 63, i64 63>
  %4423 = add nsw <2 x i64> %4421, %4422
  %4424 = lshr <2 x i64> %4423, <i64 2, i64 2>
  %4425 = trunc <2 x i64> %4424 to <2 x i32>
  %4426 = or i64 %4285, 30
  %4427 = getelementptr inbounds i32, i32* %1, i64 %4426
  %4428 = bitcast i32* %4427 to <2 x i32>*
  store <2 x i32> %4425, <2 x i32>* %4428, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %4217) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %4216) #6
  %4429 = add nuw nsw i64 %4252, 1
  %4430 = icmp eq i64 %4429, 32
  br i1 %4430, label %11292, label %4251

4431:                                             ; preds = %4178
  %4432 = bitcast <2 x i64> %3796 to <4 x i32>
  %4433 = shufflevector <4 x i32> %4432, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %4434 = bitcast <4 x i32> %4433 to <2 x i64>
  %4435 = bitcast <2 x i64> %3801 to <4 x i32>
  %4436 = shufflevector <4 x i32> %4435, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %4437 = bitcast <4 x i32> %4436 to <2 x i64>
  %4438 = shufflevector <2 x i64> %4434, <2 x i64> %4437, <2 x i32> <i32 0, i32 2>
  %4439 = bitcast <2 x i64> %3806 to <4 x i32>
  %4440 = shufflevector <4 x i32> %4439, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %4441 = bitcast <4 x i32> %4440 to <2 x i64>
  %4442 = bitcast <2 x i64> %3811 to <4 x i32>
  %4443 = shufflevector <4 x i32> %4442, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %4444 = bitcast <4 x i32> %4443 to <2 x i64>
  %4445 = shufflevector <2 x i64> %4441, <2 x i64> %4444, <2 x i32> <i32 0, i32 2>
  %4446 = bitcast <2 x i64> %3816 to <4 x i32>
  %4447 = shufflevector <4 x i32> %4446, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %4448 = bitcast <4 x i32> %4447 to <2 x i64>
  %4449 = bitcast <2 x i64> %3821 to <4 x i32>
  %4450 = shufflevector <4 x i32> %4449, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %4451 = bitcast <4 x i32> %4450 to <2 x i64>
  %4452 = shufflevector <2 x i64> %4448, <2 x i64> %4451, <2 x i32> <i32 0, i32 2>
  %4453 = bitcast <2 x i64> %3826 to <4 x i32>
  %4454 = shufflevector <4 x i32> %4453, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %4455 = bitcast <4 x i32> %4454 to <2 x i64>
  %4456 = bitcast <2 x i64> %3831 to <4 x i32>
  %4457 = shufflevector <4 x i32> %4456, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %4458 = bitcast <4 x i32> %4457 to <2 x i64>
  %4459 = shufflevector <2 x i64> %4455, <2 x i64> %4458, <2 x i32> <i32 0, i32 2>
  %4460 = bitcast <2 x i64> %3836 to <4 x i32>
  %4461 = shufflevector <4 x i32> %4460, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %4462 = bitcast <4 x i32> %4461 to <2 x i64>
  %4463 = bitcast <2 x i64> %3841 to <4 x i32>
  %4464 = shufflevector <4 x i32> %4463, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %4465 = bitcast <4 x i32> %4464 to <2 x i64>
  %4466 = shufflevector <2 x i64> %4462, <2 x i64> %4465, <2 x i32> <i32 0, i32 2>
  %4467 = bitcast <2 x i64> %3846 to <4 x i32>
  %4468 = shufflevector <4 x i32> %4467, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %4469 = bitcast <4 x i32> %4468 to <2 x i64>
  %4470 = bitcast <2 x i64> %3851 to <4 x i32>
  %4471 = shufflevector <4 x i32> %4470, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %4472 = bitcast <4 x i32> %4471 to <2 x i64>
  %4473 = shufflevector <2 x i64> %4469, <2 x i64> %4472, <2 x i32> <i32 0, i32 2>
  %4474 = bitcast <2 x i64> %3856 to <4 x i32>
  %4475 = shufflevector <4 x i32> %4474, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %4476 = bitcast <4 x i32> %4475 to <2 x i64>
  %4477 = bitcast <2 x i64> %3861 to <4 x i32>
  %4478 = shufflevector <4 x i32> %4477, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %4479 = bitcast <4 x i32> %4478 to <2 x i64>
  %4480 = shufflevector <2 x i64> %4476, <2 x i64> %4479, <2 x i32> <i32 0, i32 2>
  %4481 = bitcast <2 x i64> %3866 to <4 x i32>
  %4482 = shufflevector <4 x i32> %4481, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %4483 = bitcast <4 x i32> %4482 to <2 x i64>
  %4484 = bitcast <2 x i64> %3871 to <4 x i32>
  %4485 = shufflevector <4 x i32> %4484, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %4486 = bitcast <4 x i32> %4485 to <2 x i64>
  %4487 = shufflevector <2 x i64> %4483, <2 x i64> %4486, <2 x i32> <i32 0, i32 2>
  %4488 = bitcast <2 x i64> %3874 to <4 x i32>
  %4489 = shufflevector <4 x i32> %4488, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %4490 = bitcast <4 x i32> %4489 to <2 x i64>
  %4491 = bitcast <2 x i64> %3877 to <4 x i32>
  %4492 = shufflevector <4 x i32> %4491, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %4493 = bitcast <4 x i32> %4492 to <2 x i64>
  %4494 = shufflevector <2 x i64> %4490, <2 x i64> %4493, <2 x i32> <i32 0, i32 2>
  %4495 = bitcast <2 x i64> %3880 to <4 x i32>
  %4496 = shufflevector <4 x i32> %4495, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %4497 = bitcast <4 x i32> %4496 to <2 x i64>
  %4498 = bitcast <2 x i64> %3883 to <4 x i32>
  %4499 = shufflevector <4 x i32> %4498, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %4500 = bitcast <4 x i32> %4499 to <2 x i64>
  %4501 = shufflevector <2 x i64> %4497, <2 x i64> %4500, <2 x i32> <i32 0, i32 2>
  %4502 = bitcast <2 x i64> %3886 to <4 x i32>
  %4503 = shufflevector <4 x i32> %4502, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %4504 = bitcast <4 x i32> %4503 to <2 x i64>
  %4505 = bitcast <2 x i64> %3889 to <4 x i32>
  %4506 = shufflevector <4 x i32> %4505, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %4507 = bitcast <4 x i32> %4506 to <2 x i64>
  %4508 = shufflevector <2 x i64> %4504, <2 x i64> %4507, <2 x i32> <i32 0, i32 2>
  %4509 = bitcast <2 x i64> %3892 to <4 x i32>
  %4510 = shufflevector <4 x i32> %4509, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %4511 = bitcast <4 x i32> %4510 to <2 x i64>
  %4512 = bitcast <2 x i64> %3895 to <4 x i32>
  %4513 = shufflevector <4 x i32> %4512, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %4514 = bitcast <4 x i32> %4513 to <2 x i64>
  %4515 = shufflevector <2 x i64> %4511, <2 x i64> %4514, <2 x i32> <i32 0, i32 2>
  %4516 = bitcast <2 x i64> %3898 to <4 x i32>
  %4517 = shufflevector <4 x i32> %4516, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %4518 = bitcast <4 x i32> %4517 to <2 x i64>
  %4519 = bitcast <2 x i64> %3901 to <4 x i32>
  %4520 = shufflevector <4 x i32> %4519, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %4521 = bitcast <4 x i32> %4520 to <2 x i64>
  %4522 = shufflevector <2 x i64> %4518, <2 x i64> %4521, <2 x i32> <i32 0, i32 2>
  %4523 = bitcast <2 x i64> %3904 to <4 x i32>
  %4524 = shufflevector <4 x i32> %4523, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %4525 = bitcast <4 x i32> %4524 to <2 x i64>
  %4526 = bitcast <2 x i64> %3907 to <4 x i32>
  %4527 = shufflevector <4 x i32> %4526, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %4528 = bitcast <4 x i32> %4527 to <2 x i64>
  %4529 = shufflevector <2 x i64> %4525, <2 x i64> %4528, <2 x i32> <i32 0, i32 2>
  %4530 = bitcast <2 x i64> %3910 to <4 x i32>
  %4531 = shufflevector <4 x i32> %4530, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %4532 = bitcast <4 x i32> %4531 to <2 x i64>
  %4533 = bitcast <2 x i64> %3913 to <4 x i32>
  %4534 = shufflevector <4 x i32> %4533, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %4535 = bitcast <4 x i32> %4534 to <2 x i64>
  %4536 = shufflevector <2 x i64> %4532, <2 x i64> %4535, <2 x i32> <i32 0, i32 2>
  %4537 = bitcast <2 x i64> %3916 to <4 x i32>
  %4538 = shufflevector <4 x i32> %4537, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %4539 = bitcast <4 x i32> %4538 to <2 x i64>
  %4540 = bitcast <2 x i64> %3919 to <4 x i32>
  %4541 = shufflevector <4 x i32> %4540, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %4542 = bitcast <4 x i32> %4541 to <2 x i64>
  %4543 = shufflevector <2 x i64> %4539, <2 x i64> %4542, <2 x i32> <i32 0, i32 2>
  %4544 = bitcast <2 x i64> %4438 to <4 x i32>
  %4545 = add <4 x i32> %4544, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4546 = bitcast <2 x i64> %4445 to <4 x i32>
  %4547 = add <4 x i32> %4546, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4548 = bitcast <2 x i64> %4452 to <4 x i32>
  %4549 = add <4 x i32> %4548, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4550 = bitcast <2 x i64> %4459 to <4 x i32>
  %4551 = add <4 x i32> %4550, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4552 = bitcast <2 x i64> %4466 to <4 x i32>
  %4553 = add <4 x i32> %4552, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4554 = bitcast <2 x i64> %4473 to <4 x i32>
  %4555 = add <4 x i32> %4554, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4556 = bitcast <2 x i64> %4480 to <4 x i32>
  %4557 = add <4 x i32> %4556, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4558 = bitcast <2 x i64> %4487 to <4 x i32>
  %4559 = add <4 x i32> %4558, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4560 = bitcast <2 x i64> %4494 to <4 x i32>
  %4561 = add <4 x i32> %4560, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4562 = bitcast <2 x i64> %4501 to <4 x i32>
  %4563 = add <4 x i32> %4562, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4564 = bitcast <2 x i64> %4508 to <4 x i32>
  %4565 = add <4 x i32> %4564, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4566 = bitcast <2 x i64> %4515 to <4 x i32>
  %4567 = add <4 x i32> %4566, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4568 = bitcast <2 x i64> %4522 to <4 x i32>
  %4569 = add <4 x i32> %4568, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4570 = bitcast <2 x i64> %4529 to <4 x i32>
  %4571 = add <4 x i32> %4570, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4572 = bitcast <2 x i64> %4536 to <4 x i32>
  %4573 = add <4 x i32> %4572, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4574 = bitcast <2 x i64> %4543 to <4 x i32>
  %4575 = add <4 x i32> %4574, <i32 8192, i32 8192, i32 8192, i32 8192>
  %4576 = ashr <4 x i32> %4545, <i32 14, i32 14, i32 14, i32 14>
  %4577 = ashr <4 x i32> %4547, <i32 14, i32 14, i32 14, i32 14>
  %4578 = ashr <4 x i32> %4549, <i32 14, i32 14, i32 14, i32 14>
  %4579 = ashr <4 x i32> %4551, <i32 14, i32 14, i32 14, i32 14>
  %4580 = ashr <4 x i32> %4553, <i32 14, i32 14, i32 14, i32 14>
  %4581 = ashr <4 x i32> %4555, <i32 14, i32 14, i32 14, i32 14>
  %4582 = ashr <4 x i32> %4557, <i32 14, i32 14, i32 14, i32 14>
  %4583 = ashr <4 x i32> %4559, <i32 14, i32 14, i32 14, i32 14>
  %4584 = ashr <4 x i32> %4561, <i32 14, i32 14, i32 14, i32 14>
  %4585 = ashr <4 x i32> %4563, <i32 14, i32 14, i32 14, i32 14>
  %4586 = ashr <4 x i32> %4565, <i32 14, i32 14, i32 14, i32 14>
  %4587 = ashr <4 x i32> %4567, <i32 14, i32 14, i32 14, i32 14>
  %4588 = ashr <4 x i32> %4569, <i32 14, i32 14, i32 14, i32 14>
  %4589 = ashr <4 x i32> %4571, <i32 14, i32 14, i32 14, i32 14>
  %4590 = ashr <4 x i32> %4573, <i32 14, i32 14, i32 14, i32 14>
  %4591 = ashr <4 x i32> %4575, <i32 14, i32 14, i32 14, i32 14>
  %4592 = add <4 x i32> %3756, %3266
  %4593 = add <4 x i32> %3757, %3267
  %4594 = sub <4 x i32> %3266, %3756
  %4595 = sub <4 x i32> %3267, %3757
  %4596 = sub <4 x i32> %3272, %3758
  %4597 = sub <4 x i32> %3273, %3759
  %4598 = add <4 x i32> %3758, %3272
  %4599 = add <4 x i32> %3759, %3273
  %4600 = shufflevector <4 x i32> %3362, <4 x i32> %3364, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %4601 = bitcast <4 x i32> %4600 to <2 x i64>
  %4602 = shufflevector <4 x i32> %3362, <4 x i32> %3364, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %4603 = bitcast <4 x i32> %4602 to <2 x i64>
  %4604 = shufflevector <4 x i32> %3363, <4 x i32> %3365, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %4605 = bitcast <4 x i32> %4604 to <2 x i64>
  %4606 = shufflevector <4 x i32> %3363, <4 x i32> %3365, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %4607 = bitcast <4 x i32> %4606 to <2 x i64>
  %4608 = shufflevector <4 x i32> %3366, <4 x i32> %3368, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %4609 = bitcast <4 x i32> %4608 to <2 x i64>
  %4610 = shufflevector <4 x i32> %3366, <4 x i32> %3368, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %4611 = bitcast <4 x i32> %4610 to <2 x i64>
  %4612 = shufflevector <4 x i32> %3367, <4 x i32> %3369, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %4613 = bitcast <4 x i32> %4612 to <2 x i64>
  %4614 = shufflevector <4 x i32> %3367, <4 x i32> %3369, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %4615 = bitcast <4 x i32> %4614 to <2 x i64>
  %4616 = and <2 x i64> %4601, <i64 4294967295, i64 4294967295>
  %4617 = mul nuw nsw <2 x i64> %4616, <i64 11585, i64 11585>
  %4618 = lshr <2 x i64> %4601, <i64 32, i64 32>
  %4619 = mul nuw nsw <2 x i64> %4618, <i64 11585, i64 11585>
  %4620 = add nuw nsw <2 x i64> %4619, %4617
  %4621 = and <2 x i64> %4603, <i64 4294967295, i64 4294967295>
  %4622 = mul nuw nsw <2 x i64> %4621, <i64 11585, i64 11585>
  %4623 = lshr <2 x i64> %4603, <i64 32, i64 32>
  %4624 = mul nuw nsw <2 x i64> %4623, <i64 11585, i64 11585>
  %4625 = add nuw nsw <2 x i64> %4624, %4622
  %4626 = and <2 x i64> %4605, <i64 4294967295, i64 4294967295>
  %4627 = mul nuw nsw <2 x i64> %4626, <i64 11585, i64 11585>
  %4628 = lshr <2 x i64> %4605, <i64 32, i64 32>
  %4629 = mul nuw nsw <2 x i64> %4628, <i64 11585, i64 11585>
  %4630 = add nuw nsw <2 x i64> %4629, %4627
  %4631 = and <2 x i64> %4607, <i64 4294967295, i64 4294967295>
  %4632 = mul nuw nsw <2 x i64> %4631, <i64 11585, i64 11585>
  %4633 = lshr <2 x i64> %4607, <i64 32, i64 32>
  %4634 = mul nuw nsw <2 x i64> %4633, <i64 11585, i64 11585>
  %4635 = add nuw nsw <2 x i64> %4634, %4632
  %4636 = mul nuw <2 x i64> %4618, <i64 4294955711, i64 4294955711>
  %4637 = add <2 x i64> %4636, %4617
  %4638 = mul nuw <2 x i64> %4623, <i64 4294955711, i64 4294955711>
  %4639 = add <2 x i64> %4638, %4622
  %4640 = mul nuw <2 x i64> %4628, <i64 4294955711, i64 4294955711>
  %4641 = add <2 x i64> %4640, %4627
  %4642 = mul nuw <2 x i64> %4633, <i64 4294955711, i64 4294955711>
  %4643 = add <2 x i64> %4642, %4632
  %4644 = and <2 x i64> %4609, <i64 4294967295, i64 4294967295>
  %4645 = mul nuw nsw <2 x i64> %4644, <i64 6270, i64 6270>
  %4646 = lshr <2 x i64> %4609, <i64 32, i64 32>
  %4647 = mul nuw nsw <2 x i64> %4646, <i64 15137, i64 15137>
  %4648 = add nuw nsw <2 x i64> %4647, %4645
  %4649 = and <2 x i64> %4611, <i64 4294967295, i64 4294967295>
  %4650 = mul nuw nsw <2 x i64> %4649, <i64 6270, i64 6270>
  %4651 = lshr <2 x i64> %4611, <i64 32, i64 32>
  %4652 = mul nuw nsw <2 x i64> %4651, <i64 15137, i64 15137>
  %4653 = add nuw nsw <2 x i64> %4652, %4650
  %4654 = and <2 x i64> %4613, <i64 4294967295, i64 4294967295>
  %4655 = mul nuw nsw <2 x i64> %4654, <i64 6270, i64 6270>
  %4656 = lshr <2 x i64> %4613, <i64 32, i64 32>
  %4657 = mul nuw nsw <2 x i64> %4656, <i64 15137, i64 15137>
  %4658 = add nuw nsw <2 x i64> %4657, %4655
  %4659 = and <2 x i64> %4615, <i64 4294967295, i64 4294967295>
  %4660 = mul nuw nsw <2 x i64> %4659, <i64 6270, i64 6270>
  %4661 = lshr <2 x i64> %4615, <i64 32, i64 32>
  %4662 = mul nuw nsw <2 x i64> %4661, <i64 15137, i64 15137>
  %4663 = add nuw nsw <2 x i64> %4662, %4660
  %4664 = mul nuw <2 x i64> %4644, <i64 4294952159, i64 4294952159>
  %4665 = mul nuw nsw <2 x i64> %4646, <i64 6270, i64 6270>
  %4666 = add <2 x i64> %4665, %4664
  %4667 = mul nuw <2 x i64> %4649, <i64 4294952159, i64 4294952159>
  %4668 = mul nuw nsw <2 x i64> %4651, <i64 6270, i64 6270>
  %4669 = add <2 x i64> %4668, %4667
  %4670 = mul nuw <2 x i64> %4654, <i64 4294952159, i64 4294952159>
  %4671 = mul nuw nsw <2 x i64> %4656, <i64 6270, i64 6270>
  %4672 = add <2 x i64> %4671, %4670
  %4673 = mul nuw <2 x i64> %4659, <i64 4294952159, i64 4294952159>
  %4674 = mul nuw nsw <2 x i64> %4661, <i64 6270, i64 6270>
  %4675 = add <2 x i64> %4674, %4673
  %4676 = shl nuw nsw <2 x i64> %4620, <i64 1, i64 1>
  %4677 = shl nuw nsw <2 x i64> %4625, <i64 1, i64 1>
  %4678 = shl nuw nsw <2 x i64> %4630, <i64 1, i64 1>
  %4679 = shl nuw nsw <2 x i64> %4635, <i64 1, i64 1>
  %4680 = bitcast <2 x i64> %4676 to <4 x i32>
  %4681 = shufflevector <4 x i32> %4680, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4682 = bitcast <4 x i32> %4681 to <2 x i64>
  %4683 = bitcast <2 x i64> %4677 to <4 x i32>
  %4684 = shufflevector <4 x i32> %4683, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4685 = bitcast <4 x i32> %4684 to <2 x i64>
  %4686 = bitcast <2 x i64> %4678 to <4 x i32>
  %4687 = shufflevector <4 x i32> %4686, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4688 = bitcast <4 x i32> %4687 to <2 x i64>
  %4689 = bitcast <2 x i64> %4679 to <4 x i32>
  %4690 = shufflevector <4 x i32> %4689, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4691 = bitcast <4 x i32> %4690 to <2 x i64>
  %4692 = shufflevector <2 x i64> %4682, <2 x i64> %4685, <2 x i32> <i32 0, i32 2>
  %4693 = shufflevector <2 x i64> %4688, <2 x i64> %4691, <2 x i32> <i32 0, i32 2>
  %4694 = bitcast <2 x i64> %4692 to <4 x i32>
  %4695 = bitcast <2 x i64> %4693 to <4 x i32>
  %4696 = add <4 x i32> %4694, <i32 1, i32 1, i32 1, i32 1>
  %4697 = icmp ugt <4 x i32> %4696, <i32 1, i32 1, i32 1, i32 1>
  %4698 = sext <4 x i1> %4697 to <4 x i32>
  %4699 = bitcast <4 x i32> %4698 to <16 x i8>
  %4700 = icmp slt <16 x i8> %4699, zeroinitializer
  %4701 = bitcast <16 x i1> %4700 to i16
  %4702 = zext i16 %4701 to i32
  %4703 = add <4 x i32> %4695, <i32 1, i32 1, i32 1, i32 1>
  %4704 = icmp ugt <4 x i32> %4703, <i32 1, i32 1, i32 1, i32 1>
  %4705 = sext <4 x i1> %4704 to <4 x i32>
  %4706 = bitcast <4 x i32> %4705 to <16 x i8>
  %4707 = icmp slt <16 x i8> %4706, zeroinitializer
  %4708 = bitcast <16 x i1> %4707 to i16
  %4709 = zext i16 %4708 to i32
  %4710 = sub nsw i32 0, %4702
  %4711 = icmp eq i32 %4709, %4710
  br i1 %4711, label %4712, label %4823

4712:                                             ; preds = %4431
  %4713 = shl <2 x i64> %4637, <i64 1, i64 1>
  %4714 = shl <2 x i64> %4639, <i64 1, i64 1>
  %4715 = shl <2 x i64> %4641, <i64 1, i64 1>
  %4716 = shl <2 x i64> %4643, <i64 1, i64 1>
  %4717 = bitcast <2 x i64> %4713 to <4 x i32>
  %4718 = shufflevector <4 x i32> %4717, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4719 = bitcast <4 x i32> %4718 to <2 x i64>
  %4720 = bitcast <2 x i64> %4714 to <4 x i32>
  %4721 = shufflevector <4 x i32> %4720, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4722 = bitcast <4 x i32> %4721 to <2 x i64>
  %4723 = bitcast <2 x i64> %4715 to <4 x i32>
  %4724 = shufflevector <4 x i32> %4723, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4725 = bitcast <4 x i32> %4724 to <2 x i64>
  %4726 = bitcast <2 x i64> %4716 to <4 x i32>
  %4727 = shufflevector <4 x i32> %4726, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4728 = bitcast <4 x i32> %4727 to <2 x i64>
  %4729 = shufflevector <2 x i64> %4719, <2 x i64> %4722, <2 x i32> <i32 0, i32 2>
  %4730 = shufflevector <2 x i64> %4725, <2 x i64> %4728, <2 x i32> <i32 0, i32 2>
  %4731 = bitcast <2 x i64> %4729 to <4 x i32>
  %4732 = bitcast <2 x i64> %4730 to <4 x i32>
  %4733 = add <4 x i32> %4731, <i32 1, i32 1, i32 1, i32 1>
  %4734 = icmp ugt <4 x i32> %4733, <i32 1, i32 1, i32 1, i32 1>
  %4735 = sext <4 x i1> %4734 to <4 x i32>
  %4736 = bitcast <4 x i32> %4735 to <16 x i8>
  %4737 = icmp slt <16 x i8> %4736, zeroinitializer
  %4738 = bitcast <16 x i1> %4737 to i16
  %4739 = zext i16 %4738 to i32
  %4740 = add <4 x i32> %4732, <i32 1, i32 1, i32 1, i32 1>
  %4741 = icmp ugt <4 x i32> %4740, <i32 1, i32 1, i32 1, i32 1>
  %4742 = sext <4 x i1> %4741 to <4 x i32>
  %4743 = bitcast <4 x i32> %4742 to <16 x i8>
  %4744 = icmp slt <16 x i8> %4743, zeroinitializer
  %4745 = bitcast <16 x i1> %4744 to i16
  %4746 = zext i16 %4745 to i32
  %4747 = sub nsw i32 0, %4739
  %4748 = icmp eq i32 %4746, %4747
  br i1 %4748, label %4749, label %4823

4749:                                             ; preds = %4712
  %4750 = shl nuw nsw <2 x i64> %4648, <i64 1, i64 1>
  %4751 = shl nuw nsw <2 x i64> %4653, <i64 1, i64 1>
  %4752 = shl nuw nsw <2 x i64> %4658, <i64 1, i64 1>
  %4753 = shl nuw nsw <2 x i64> %4663, <i64 1, i64 1>
  %4754 = bitcast <2 x i64> %4750 to <4 x i32>
  %4755 = shufflevector <4 x i32> %4754, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4756 = bitcast <4 x i32> %4755 to <2 x i64>
  %4757 = bitcast <2 x i64> %4751 to <4 x i32>
  %4758 = shufflevector <4 x i32> %4757, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4759 = bitcast <4 x i32> %4758 to <2 x i64>
  %4760 = bitcast <2 x i64> %4752 to <4 x i32>
  %4761 = shufflevector <4 x i32> %4760, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4762 = bitcast <4 x i32> %4761 to <2 x i64>
  %4763 = bitcast <2 x i64> %4753 to <4 x i32>
  %4764 = shufflevector <4 x i32> %4763, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4765 = bitcast <4 x i32> %4764 to <2 x i64>
  %4766 = shufflevector <2 x i64> %4756, <2 x i64> %4759, <2 x i32> <i32 0, i32 2>
  %4767 = shufflevector <2 x i64> %4762, <2 x i64> %4765, <2 x i32> <i32 0, i32 2>
  %4768 = bitcast <2 x i64> %4766 to <4 x i32>
  %4769 = bitcast <2 x i64> %4767 to <4 x i32>
  %4770 = add <4 x i32> %4768, <i32 1, i32 1, i32 1, i32 1>
  %4771 = icmp ugt <4 x i32> %4770, <i32 1, i32 1, i32 1, i32 1>
  %4772 = sext <4 x i1> %4771 to <4 x i32>
  %4773 = bitcast <4 x i32> %4772 to <16 x i8>
  %4774 = icmp slt <16 x i8> %4773, zeroinitializer
  %4775 = bitcast <16 x i1> %4774 to i16
  %4776 = zext i16 %4775 to i32
  %4777 = add <4 x i32> %4769, <i32 1, i32 1, i32 1, i32 1>
  %4778 = icmp ugt <4 x i32> %4777, <i32 1, i32 1, i32 1, i32 1>
  %4779 = sext <4 x i1> %4778 to <4 x i32>
  %4780 = bitcast <4 x i32> %4779 to <16 x i8>
  %4781 = icmp slt <16 x i8> %4780, zeroinitializer
  %4782 = bitcast <16 x i1> %4781 to i16
  %4783 = zext i16 %4782 to i32
  %4784 = sub nsw i32 0, %4776
  %4785 = icmp eq i32 %4783, %4784
  br i1 %4785, label %4786, label %4823

4786:                                             ; preds = %4749
  %4787 = shl <2 x i64> %4666, <i64 1, i64 1>
  %4788 = shl <2 x i64> %4669, <i64 1, i64 1>
  %4789 = shl <2 x i64> %4672, <i64 1, i64 1>
  %4790 = shl <2 x i64> %4675, <i64 1, i64 1>
  %4791 = bitcast <2 x i64> %4787 to <4 x i32>
  %4792 = shufflevector <4 x i32> %4791, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4793 = bitcast <4 x i32> %4792 to <2 x i64>
  %4794 = bitcast <2 x i64> %4788 to <4 x i32>
  %4795 = shufflevector <4 x i32> %4794, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4796 = bitcast <4 x i32> %4795 to <2 x i64>
  %4797 = bitcast <2 x i64> %4789 to <4 x i32>
  %4798 = shufflevector <4 x i32> %4797, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4799 = bitcast <4 x i32> %4798 to <2 x i64>
  %4800 = bitcast <2 x i64> %4790 to <4 x i32>
  %4801 = shufflevector <4 x i32> %4800, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %4802 = bitcast <4 x i32> %4801 to <2 x i64>
  %4803 = shufflevector <2 x i64> %4793, <2 x i64> %4796, <2 x i32> <i32 0, i32 2>
  %4804 = shufflevector <2 x i64> %4799, <2 x i64> %4802, <2 x i32> <i32 0, i32 2>
  %4805 = bitcast <2 x i64> %4803 to <4 x i32>
  %4806 = bitcast <2 x i64> %4804 to <4 x i32>
  %4807 = add <4 x i32> %4805, <i32 1, i32 1, i32 1, i32 1>
  %4808 = icmp ugt <4 x i32> %4807, <i32 1, i32 1, i32 1, i32 1>
  %4809 = sext <4 x i1> %4808 to <4 x i32>
  %4810 = bitcast <4 x i32> %4809 to <16 x i8>
  %4811 = icmp slt <16 x i8> %4810, zeroinitializer
  %4812 = bitcast <16 x i1> %4811 to i16
  %4813 = zext i16 %4812 to i32
  %4814 = add <4 x i32> %4806, <i32 1, i32 1, i32 1, i32 1>
  %4815 = icmp ugt <4 x i32> %4814, <i32 1, i32 1, i32 1, i32 1>
  %4816 = sext <4 x i1> %4815 to <4 x i32>
  %4817 = bitcast <4 x i32> %4816 to <16 x i8>
  %4818 = icmp slt <16 x i8> %4817, zeroinitializer
  %4819 = bitcast <16 x i1> %4818 to i16
  %4820 = zext i16 %4819 to i32
  %4821 = sub nsw i32 0, %4813
  %4822 = icmp eq i32 %4820, %4821
  br i1 %4822, label %5039, label %4823

4823:                                             ; preds = %4749, %4712, %4431, %4786
  %4824 = bitcast [32 x i64]* %4 to i8*
  %4825 = bitcast [32 x i64]* %5 to i8*
  %4826 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %4827 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %4828 = bitcast [32 x i64]* %5 to <2 x i64>*
  %4829 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %4830 = bitcast i64* %4829 to <2 x i64>*
  %4831 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %4832 = bitcast i64* %4831 to <2 x i64>*
  %4833 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %4834 = bitcast i64* %4833 to <2 x i64>*
  %4835 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %4836 = bitcast i64* %4835 to <2 x i64>*
  %4837 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %4838 = bitcast i64* %4837 to <2 x i64>*
  %4839 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %4840 = bitcast i64* %4839 to <2 x i64>*
  %4841 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %4842 = bitcast i64* %4841 to <2 x i64>*
  %4843 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %4844 = bitcast i64* %4843 to <2 x i64>*
  %4845 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %4846 = bitcast i64* %4845 to <2 x i64>*
  %4847 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %4848 = bitcast i64* %4847 to <2 x i64>*
  %4849 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %4850 = bitcast i64* %4849 to <2 x i64>*
  %4851 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %4852 = bitcast i64* %4851 to <2 x i64>*
  %4853 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %4854 = bitcast i64* %4853 to <2 x i64>*
  %4855 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %4856 = bitcast i64* %4855 to <2 x i64>*
  %4857 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %4858 = bitcast i64* %4857 to <2 x i64>*
  br label %4859

4859:                                             ; preds = %4892, %4823
  %4860 = phi i64 [ 0, %4823 ], [ %5037, %4892 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %4824) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %4824, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %4825) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %4825, i8 -86, i64 256, i1 false) #6
  br label %4861

4861:                                             ; preds = %4861, %4859
  %4862 = phi i64 [ 0, %4859 ], [ %4890, %4861 ]
  %4863 = shl i64 %4862, 5
  %4864 = add nuw nsw i64 %4863, %4860
  %4865 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %4864
  %4866 = load i16, i16* %4865, align 2
  %4867 = sext i16 %4866 to i64
  %4868 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %4862
  store i64 %4867, i64* %4868, align 16
  %4869 = or i64 %4862, 1
  %4870 = shl i64 %4869, 5
  %4871 = add nuw nsw i64 %4870, %4860
  %4872 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %4871
  %4873 = load i16, i16* %4872, align 2
  %4874 = sext i16 %4873 to i64
  %4875 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %4869
  store i64 %4874, i64* %4875, align 8
  %4876 = or i64 %4862, 2
  %4877 = shl i64 %4876, 5
  %4878 = add nuw nsw i64 %4877, %4860
  %4879 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %4878
  %4880 = load i16, i16* %4879, align 2
  %4881 = sext i16 %4880 to i64
  %4882 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %4876
  store i64 %4881, i64* %4882, align 16
  %4883 = or i64 %4862, 3
  %4884 = shl i64 %4883, 5
  %4885 = add nuw nsw i64 %4884, %4860
  %4886 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %4885
  %4887 = load i16, i16* %4886, align 2
  %4888 = sext i16 %4887 to i64
  %4889 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %4883
  store i64 %4888, i64* %4889, align 8
  %4890 = add nuw nsw i64 %4862, 4
  %4891 = icmp eq i64 %4890, 32
  br i1 %4891, label %4892, label %4861

4892:                                             ; preds = %4861
  call void @vpx_fdct32(i64* nonnull %4826, i64* nonnull %4827, i32 0) #6
  %4893 = shl i64 %4860, 5
  %4894 = load <2 x i64>, <2 x i64>* %4828, align 16
  %4895 = add nsw <2 x i64> %4894, <i64 1, i64 1>
  %4896 = lshr <2 x i64> %4894, <i64 63, i64 63>
  %4897 = add nsw <2 x i64> %4895, %4896
  %4898 = lshr <2 x i64> %4897, <i64 2, i64 2>
  %4899 = trunc <2 x i64> %4898 to <2 x i32>
  %4900 = getelementptr inbounds i32, i32* %1, i64 %4893
  %4901 = bitcast i32* %4900 to <2 x i32>*
  store <2 x i32> %4899, <2 x i32>* %4901, align 4
  %4902 = load <2 x i64>, <2 x i64>* %4830, align 16
  %4903 = add nsw <2 x i64> %4902, <i64 1, i64 1>
  %4904 = lshr <2 x i64> %4902, <i64 63, i64 63>
  %4905 = add nsw <2 x i64> %4903, %4904
  %4906 = lshr <2 x i64> %4905, <i64 2, i64 2>
  %4907 = trunc <2 x i64> %4906 to <2 x i32>
  %4908 = or i64 %4893, 2
  %4909 = getelementptr inbounds i32, i32* %1, i64 %4908
  %4910 = bitcast i32* %4909 to <2 x i32>*
  store <2 x i32> %4907, <2 x i32>* %4910, align 4
  %4911 = load <2 x i64>, <2 x i64>* %4832, align 16
  %4912 = add nsw <2 x i64> %4911, <i64 1, i64 1>
  %4913 = lshr <2 x i64> %4911, <i64 63, i64 63>
  %4914 = add nsw <2 x i64> %4912, %4913
  %4915 = lshr <2 x i64> %4914, <i64 2, i64 2>
  %4916 = trunc <2 x i64> %4915 to <2 x i32>
  %4917 = or i64 %4893, 4
  %4918 = getelementptr inbounds i32, i32* %1, i64 %4917
  %4919 = bitcast i32* %4918 to <2 x i32>*
  store <2 x i32> %4916, <2 x i32>* %4919, align 4
  %4920 = load <2 x i64>, <2 x i64>* %4834, align 16
  %4921 = add nsw <2 x i64> %4920, <i64 1, i64 1>
  %4922 = lshr <2 x i64> %4920, <i64 63, i64 63>
  %4923 = add nsw <2 x i64> %4921, %4922
  %4924 = lshr <2 x i64> %4923, <i64 2, i64 2>
  %4925 = trunc <2 x i64> %4924 to <2 x i32>
  %4926 = or i64 %4893, 6
  %4927 = getelementptr inbounds i32, i32* %1, i64 %4926
  %4928 = bitcast i32* %4927 to <2 x i32>*
  store <2 x i32> %4925, <2 x i32>* %4928, align 4
  %4929 = load <2 x i64>, <2 x i64>* %4836, align 16
  %4930 = add nsw <2 x i64> %4929, <i64 1, i64 1>
  %4931 = lshr <2 x i64> %4929, <i64 63, i64 63>
  %4932 = add nsw <2 x i64> %4930, %4931
  %4933 = lshr <2 x i64> %4932, <i64 2, i64 2>
  %4934 = trunc <2 x i64> %4933 to <2 x i32>
  %4935 = or i64 %4893, 8
  %4936 = getelementptr inbounds i32, i32* %1, i64 %4935
  %4937 = bitcast i32* %4936 to <2 x i32>*
  store <2 x i32> %4934, <2 x i32>* %4937, align 4
  %4938 = load <2 x i64>, <2 x i64>* %4838, align 16
  %4939 = add nsw <2 x i64> %4938, <i64 1, i64 1>
  %4940 = lshr <2 x i64> %4938, <i64 63, i64 63>
  %4941 = add nsw <2 x i64> %4939, %4940
  %4942 = lshr <2 x i64> %4941, <i64 2, i64 2>
  %4943 = trunc <2 x i64> %4942 to <2 x i32>
  %4944 = or i64 %4893, 10
  %4945 = getelementptr inbounds i32, i32* %1, i64 %4944
  %4946 = bitcast i32* %4945 to <2 x i32>*
  store <2 x i32> %4943, <2 x i32>* %4946, align 4
  %4947 = load <2 x i64>, <2 x i64>* %4840, align 16
  %4948 = add nsw <2 x i64> %4947, <i64 1, i64 1>
  %4949 = lshr <2 x i64> %4947, <i64 63, i64 63>
  %4950 = add nsw <2 x i64> %4948, %4949
  %4951 = lshr <2 x i64> %4950, <i64 2, i64 2>
  %4952 = trunc <2 x i64> %4951 to <2 x i32>
  %4953 = or i64 %4893, 12
  %4954 = getelementptr inbounds i32, i32* %1, i64 %4953
  %4955 = bitcast i32* %4954 to <2 x i32>*
  store <2 x i32> %4952, <2 x i32>* %4955, align 4
  %4956 = load <2 x i64>, <2 x i64>* %4842, align 16
  %4957 = add nsw <2 x i64> %4956, <i64 1, i64 1>
  %4958 = lshr <2 x i64> %4956, <i64 63, i64 63>
  %4959 = add nsw <2 x i64> %4957, %4958
  %4960 = lshr <2 x i64> %4959, <i64 2, i64 2>
  %4961 = trunc <2 x i64> %4960 to <2 x i32>
  %4962 = or i64 %4893, 14
  %4963 = getelementptr inbounds i32, i32* %1, i64 %4962
  %4964 = bitcast i32* %4963 to <2 x i32>*
  store <2 x i32> %4961, <2 x i32>* %4964, align 4
  %4965 = load <2 x i64>, <2 x i64>* %4844, align 16
  %4966 = add nsw <2 x i64> %4965, <i64 1, i64 1>
  %4967 = lshr <2 x i64> %4965, <i64 63, i64 63>
  %4968 = add nsw <2 x i64> %4966, %4967
  %4969 = lshr <2 x i64> %4968, <i64 2, i64 2>
  %4970 = trunc <2 x i64> %4969 to <2 x i32>
  %4971 = or i64 %4893, 16
  %4972 = getelementptr inbounds i32, i32* %1, i64 %4971
  %4973 = bitcast i32* %4972 to <2 x i32>*
  store <2 x i32> %4970, <2 x i32>* %4973, align 4
  %4974 = load <2 x i64>, <2 x i64>* %4846, align 16
  %4975 = add nsw <2 x i64> %4974, <i64 1, i64 1>
  %4976 = lshr <2 x i64> %4974, <i64 63, i64 63>
  %4977 = add nsw <2 x i64> %4975, %4976
  %4978 = lshr <2 x i64> %4977, <i64 2, i64 2>
  %4979 = trunc <2 x i64> %4978 to <2 x i32>
  %4980 = or i64 %4893, 18
  %4981 = getelementptr inbounds i32, i32* %1, i64 %4980
  %4982 = bitcast i32* %4981 to <2 x i32>*
  store <2 x i32> %4979, <2 x i32>* %4982, align 4
  %4983 = load <2 x i64>, <2 x i64>* %4848, align 16
  %4984 = add nsw <2 x i64> %4983, <i64 1, i64 1>
  %4985 = lshr <2 x i64> %4983, <i64 63, i64 63>
  %4986 = add nsw <2 x i64> %4984, %4985
  %4987 = lshr <2 x i64> %4986, <i64 2, i64 2>
  %4988 = trunc <2 x i64> %4987 to <2 x i32>
  %4989 = or i64 %4893, 20
  %4990 = getelementptr inbounds i32, i32* %1, i64 %4989
  %4991 = bitcast i32* %4990 to <2 x i32>*
  store <2 x i32> %4988, <2 x i32>* %4991, align 4
  %4992 = load <2 x i64>, <2 x i64>* %4850, align 16
  %4993 = add nsw <2 x i64> %4992, <i64 1, i64 1>
  %4994 = lshr <2 x i64> %4992, <i64 63, i64 63>
  %4995 = add nsw <2 x i64> %4993, %4994
  %4996 = lshr <2 x i64> %4995, <i64 2, i64 2>
  %4997 = trunc <2 x i64> %4996 to <2 x i32>
  %4998 = or i64 %4893, 22
  %4999 = getelementptr inbounds i32, i32* %1, i64 %4998
  %5000 = bitcast i32* %4999 to <2 x i32>*
  store <2 x i32> %4997, <2 x i32>* %5000, align 4
  %5001 = load <2 x i64>, <2 x i64>* %4852, align 16
  %5002 = add nsw <2 x i64> %5001, <i64 1, i64 1>
  %5003 = lshr <2 x i64> %5001, <i64 63, i64 63>
  %5004 = add nsw <2 x i64> %5002, %5003
  %5005 = lshr <2 x i64> %5004, <i64 2, i64 2>
  %5006 = trunc <2 x i64> %5005 to <2 x i32>
  %5007 = or i64 %4893, 24
  %5008 = getelementptr inbounds i32, i32* %1, i64 %5007
  %5009 = bitcast i32* %5008 to <2 x i32>*
  store <2 x i32> %5006, <2 x i32>* %5009, align 4
  %5010 = load <2 x i64>, <2 x i64>* %4854, align 16
  %5011 = add nsw <2 x i64> %5010, <i64 1, i64 1>
  %5012 = lshr <2 x i64> %5010, <i64 63, i64 63>
  %5013 = add nsw <2 x i64> %5011, %5012
  %5014 = lshr <2 x i64> %5013, <i64 2, i64 2>
  %5015 = trunc <2 x i64> %5014 to <2 x i32>
  %5016 = or i64 %4893, 26
  %5017 = getelementptr inbounds i32, i32* %1, i64 %5016
  %5018 = bitcast i32* %5017 to <2 x i32>*
  store <2 x i32> %5015, <2 x i32>* %5018, align 4
  %5019 = load <2 x i64>, <2 x i64>* %4856, align 16
  %5020 = add nsw <2 x i64> %5019, <i64 1, i64 1>
  %5021 = lshr <2 x i64> %5019, <i64 63, i64 63>
  %5022 = add nsw <2 x i64> %5020, %5021
  %5023 = lshr <2 x i64> %5022, <i64 2, i64 2>
  %5024 = trunc <2 x i64> %5023 to <2 x i32>
  %5025 = or i64 %4893, 28
  %5026 = getelementptr inbounds i32, i32* %1, i64 %5025
  %5027 = bitcast i32* %5026 to <2 x i32>*
  store <2 x i32> %5024, <2 x i32>* %5027, align 4
  %5028 = load <2 x i64>, <2 x i64>* %4858, align 16
  %5029 = add nsw <2 x i64> %5028, <i64 1, i64 1>
  %5030 = lshr <2 x i64> %5028, <i64 63, i64 63>
  %5031 = add nsw <2 x i64> %5029, %5030
  %5032 = lshr <2 x i64> %5031, <i64 2, i64 2>
  %5033 = trunc <2 x i64> %5032 to <2 x i32>
  %5034 = or i64 %4893, 30
  %5035 = getelementptr inbounds i32, i32* %1, i64 %5034
  %5036 = bitcast i32* %5035 to <2 x i32>*
  store <2 x i32> %5033, <2 x i32>* %5036, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %4825) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %4824) #6
  %5037 = add nuw nsw i64 %4860, 1
  %5038 = icmp eq i64 %5037, 32
  br i1 %5038, label %11292, label %4859

5039:                                             ; preds = %4786
  %5040 = bitcast <2 x i64> %4620 to <4 x i32>
  %5041 = shufflevector <4 x i32> %5040, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %5042 = bitcast <4 x i32> %5041 to <2 x i64>
  %5043 = bitcast <2 x i64> %4625 to <4 x i32>
  %5044 = shufflevector <4 x i32> %5043, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %5045 = bitcast <4 x i32> %5044 to <2 x i64>
  %5046 = shufflevector <2 x i64> %5042, <2 x i64> %5045, <2 x i32> <i32 0, i32 2>
  %5047 = bitcast <2 x i64> %4630 to <4 x i32>
  %5048 = shufflevector <4 x i32> %5047, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %5049 = bitcast <4 x i32> %5048 to <2 x i64>
  %5050 = bitcast <2 x i64> %4635 to <4 x i32>
  %5051 = shufflevector <4 x i32> %5050, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %5052 = bitcast <4 x i32> %5051 to <2 x i64>
  %5053 = shufflevector <2 x i64> %5049, <2 x i64> %5052, <2 x i32> <i32 0, i32 2>
  %5054 = bitcast <2 x i64> %4637 to <4 x i32>
  %5055 = shufflevector <4 x i32> %5054, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %5056 = bitcast <4 x i32> %5055 to <2 x i64>
  %5057 = bitcast <2 x i64> %4639 to <4 x i32>
  %5058 = shufflevector <4 x i32> %5057, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %5059 = bitcast <4 x i32> %5058 to <2 x i64>
  %5060 = shufflevector <2 x i64> %5056, <2 x i64> %5059, <2 x i32> <i32 0, i32 2>
  %5061 = bitcast <2 x i64> %4641 to <4 x i32>
  %5062 = shufflevector <4 x i32> %5061, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %5063 = bitcast <4 x i32> %5062 to <2 x i64>
  %5064 = bitcast <2 x i64> %4643 to <4 x i32>
  %5065 = shufflevector <4 x i32> %5064, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %5066 = bitcast <4 x i32> %5065 to <2 x i64>
  %5067 = shufflevector <2 x i64> %5063, <2 x i64> %5066, <2 x i32> <i32 0, i32 2>
  %5068 = bitcast <2 x i64> %4648 to <4 x i32>
  %5069 = shufflevector <4 x i32> %5068, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %5070 = bitcast <4 x i32> %5069 to <2 x i64>
  %5071 = bitcast <2 x i64> %4653 to <4 x i32>
  %5072 = shufflevector <4 x i32> %5071, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %5073 = bitcast <4 x i32> %5072 to <2 x i64>
  %5074 = shufflevector <2 x i64> %5070, <2 x i64> %5073, <2 x i32> <i32 0, i32 2>
  %5075 = bitcast <2 x i64> %4658 to <4 x i32>
  %5076 = shufflevector <4 x i32> %5075, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %5077 = bitcast <4 x i32> %5076 to <2 x i64>
  %5078 = bitcast <2 x i64> %4663 to <4 x i32>
  %5079 = shufflevector <4 x i32> %5078, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %5080 = bitcast <4 x i32> %5079 to <2 x i64>
  %5081 = shufflevector <2 x i64> %5077, <2 x i64> %5080, <2 x i32> <i32 0, i32 2>
  %5082 = bitcast <2 x i64> %4666 to <4 x i32>
  %5083 = shufflevector <4 x i32> %5082, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %5084 = bitcast <4 x i32> %5083 to <2 x i64>
  %5085 = bitcast <2 x i64> %4669 to <4 x i32>
  %5086 = shufflevector <4 x i32> %5085, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %5087 = bitcast <4 x i32> %5086 to <2 x i64>
  %5088 = shufflevector <2 x i64> %5084, <2 x i64> %5087, <2 x i32> <i32 0, i32 2>
  %5089 = bitcast <2 x i64> %4672 to <4 x i32>
  %5090 = shufflevector <4 x i32> %5089, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %5091 = bitcast <4 x i32> %5090 to <2 x i64>
  %5092 = bitcast <2 x i64> %4675 to <4 x i32>
  %5093 = shufflevector <4 x i32> %5092, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %5094 = bitcast <4 x i32> %5093 to <2 x i64>
  %5095 = shufflevector <2 x i64> %5091, <2 x i64> %5094, <2 x i32> <i32 0, i32 2>
  %5096 = bitcast <2 x i64> %5046 to <4 x i32>
  %5097 = add <4 x i32> %5096, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5098 = bitcast <2 x i64> %5053 to <4 x i32>
  %5099 = add <4 x i32> %5098, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5100 = bitcast <2 x i64> %5060 to <4 x i32>
  %5101 = add <4 x i32> %5100, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5102 = bitcast <2 x i64> %5067 to <4 x i32>
  %5103 = add <4 x i32> %5102, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5104 = bitcast <2 x i64> %5074 to <4 x i32>
  %5105 = add <4 x i32> %5104, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5106 = bitcast <2 x i64> %5081 to <4 x i32>
  %5107 = add <4 x i32> %5106, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5108 = bitcast <2 x i64> %5088 to <4 x i32>
  %5109 = add <4 x i32> %5108, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5110 = bitcast <2 x i64> %5095 to <4 x i32>
  %5111 = add <4 x i32> %5110, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5112 = ashr <4 x i32> %5097, <i32 14, i32 14, i32 14, i32 14>
  %5113 = ashr <4 x i32> %5099, <i32 14, i32 14, i32 14, i32 14>
  %5114 = ashr <4 x i32> %5101, <i32 14, i32 14, i32 14, i32 14>
  %5115 = ashr <4 x i32> %5103, <i32 14, i32 14, i32 14, i32 14>
  %5116 = ashr <4 x i32> %5105, <i32 14, i32 14, i32 14, i32 14>
  %5117 = ashr <4 x i32> %5107, <i32 14, i32 14, i32 14, i32 14>
  %5118 = ashr <4 x i32> %5109, <i32 14, i32 14, i32 14, i32 14>
  %5119 = ashr <4 x i32> %5111, <i32 14, i32 14, i32 14, i32 14>
  %5120 = lshr <4 x i32> %5097, <i32 31, i32 31, i32 31, i32 31>
  %5121 = lshr <4 x i32> %5099, <i32 31, i32 31, i32 31, i32 31>
  %5122 = lshr <4 x i32> %5101, <i32 31, i32 31, i32 31, i32 31>
  %5123 = lshr <4 x i32> %5103, <i32 31, i32 31, i32 31, i32 31>
  %5124 = lshr <4 x i32> %5105, <i32 31, i32 31, i32 31, i32 31>
  %5125 = lshr <4 x i32> %5107, <i32 31, i32 31, i32 31, i32 31>
  %5126 = lshr <4 x i32> %5109, <i32 31, i32 31, i32 31, i32 31>
  %5127 = lshr <4 x i32> %5111, <i32 31, i32 31, i32 31, i32 31>
  %5128 = add nuw nsw <4 x i32> %5120, <i32 1, i32 1, i32 1, i32 1>
  %5129 = add nsw <4 x i32> %5128, %5112
  %5130 = add nuw nsw <4 x i32> %5121, <i32 1, i32 1, i32 1, i32 1>
  %5131 = add nsw <4 x i32> %5130, %5113
  %5132 = add nuw nsw <4 x i32> %5122, <i32 1, i32 1, i32 1, i32 1>
  %5133 = add nsw <4 x i32> %5132, %5114
  %5134 = add nuw nsw <4 x i32> %5123, <i32 1, i32 1, i32 1, i32 1>
  %5135 = add nsw <4 x i32> %5134, %5115
  %5136 = add nuw nsw <4 x i32> %5124, <i32 1, i32 1, i32 1, i32 1>
  %5137 = add nsw <4 x i32> %5136, %5116
  %5138 = add nuw nsw <4 x i32> %5125, <i32 1, i32 1, i32 1, i32 1>
  %5139 = add nsw <4 x i32> %5138, %5117
  %5140 = add nuw nsw <4 x i32> %5126, <i32 1, i32 1, i32 1, i32 1>
  %5141 = add nsw <4 x i32> %5140, %5118
  %5142 = add nuw nsw <4 x i32> %5127, <i32 1, i32 1, i32 1, i32 1>
  %5143 = add nsw <4 x i32> %5142, %5119
  %5144 = ashr <4 x i32> %5129, <i32 2, i32 2, i32 2, i32 2>
  %5145 = ashr <4 x i32> %5131, <i32 2, i32 2, i32 2, i32 2>
  %5146 = ashr <4 x i32> %5133, <i32 2, i32 2, i32 2, i32 2>
  %5147 = ashr <4 x i32> %5135, <i32 2, i32 2, i32 2, i32 2>
  %5148 = ashr <4 x i32> %5137, <i32 2, i32 2, i32 2, i32 2>
  %5149 = ashr <4 x i32> %5139, <i32 2, i32 2, i32 2, i32 2>
  %5150 = ashr <4 x i32> %5141, <i32 2, i32 2, i32 2, i32 2>
  %5151 = ashr <4 x i32> %5143, <i32 2, i32 2, i32 2, i32 2>
  %5152 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %5144, <4 x i32> %5145) #6
  store <8 x i16> %5152, <8 x i16>* %32, align 16
  %5153 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %5146, <4 x i32> %5147) #6
  store <8 x i16> %5153, <8 x i16>* %34, align 16
  %5154 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %5148, <4 x i32> %5149) #6
  store <8 x i16> %5154, <8 x i16>* %36, align 16
  %5155 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %5150, <4 x i32> %5151) #6
  store <8 x i16> %5155, <8 x i16>* %38, align 16
  %5156 = add <8 x i16> %5152, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5157 = icmp ult <8 x i16> %5156, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5158 = add <8 x i16> %5153, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5159 = icmp ult <8 x i16> %5158, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5160 = add <8 x i16> %5154, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5161 = icmp ult <8 x i16> %5160, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5162 = add <8 x i16> %5155, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %5163 = icmp ult <8 x i16> %5162, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %5164 = or <8 x i1> %5159, %5157
  %5165 = or <8 x i1> %5164, %5161
  %5166 = or <8 x i1> %5165, %5163
  %5167 = sext <8 x i1> %5166 to <8 x i16>
  %5168 = bitcast <8 x i16> %5167 to <16 x i8>
  %5169 = icmp slt <16 x i8> %5168, zeroinitializer
  %5170 = bitcast <16 x i1> %5169 to i16
  %5171 = icmp eq i16 %5170, 0
  br i1 %5171, label %5388, label %5172

5172:                                             ; preds = %5039
  %5173 = bitcast [32 x i64]* %4 to i8*
  %5174 = bitcast [32 x i64]* %5 to i8*
  %5175 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %5176 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %5177 = bitcast [32 x i64]* %5 to <2 x i64>*
  %5178 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %5179 = bitcast i64* %5178 to <2 x i64>*
  %5180 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %5181 = bitcast i64* %5180 to <2 x i64>*
  %5182 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %5183 = bitcast i64* %5182 to <2 x i64>*
  %5184 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %5185 = bitcast i64* %5184 to <2 x i64>*
  %5186 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %5187 = bitcast i64* %5186 to <2 x i64>*
  %5188 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %5189 = bitcast i64* %5188 to <2 x i64>*
  %5190 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %5191 = bitcast i64* %5190 to <2 x i64>*
  %5192 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %5193 = bitcast i64* %5192 to <2 x i64>*
  %5194 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %5195 = bitcast i64* %5194 to <2 x i64>*
  %5196 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %5197 = bitcast i64* %5196 to <2 x i64>*
  %5198 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %5199 = bitcast i64* %5198 to <2 x i64>*
  %5200 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %5201 = bitcast i64* %5200 to <2 x i64>*
  %5202 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %5203 = bitcast i64* %5202 to <2 x i64>*
  %5204 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %5205 = bitcast i64* %5204 to <2 x i64>*
  %5206 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %5207 = bitcast i64* %5206 to <2 x i64>*
  br label %5208

5208:                                             ; preds = %5241, %5172
  %5209 = phi i64 [ 0, %5172 ], [ %5386, %5241 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %5173) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %5173, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %5174) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %5174, i8 -86, i64 256, i1 false) #6
  br label %5210

5210:                                             ; preds = %5210, %5208
  %5211 = phi i64 [ 0, %5208 ], [ %5239, %5210 ]
  %5212 = shl i64 %5211, 5
  %5213 = add nuw nsw i64 %5212, %5209
  %5214 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %5213
  %5215 = load i16, i16* %5214, align 2
  %5216 = sext i16 %5215 to i64
  %5217 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %5211
  store i64 %5216, i64* %5217, align 16
  %5218 = or i64 %5211, 1
  %5219 = shl i64 %5218, 5
  %5220 = add nuw nsw i64 %5219, %5209
  %5221 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %5220
  %5222 = load i16, i16* %5221, align 2
  %5223 = sext i16 %5222 to i64
  %5224 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %5218
  store i64 %5223, i64* %5224, align 8
  %5225 = or i64 %5211, 2
  %5226 = shl i64 %5225, 5
  %5227 = add nuw nsw i64 %5226, %5209
  %5228 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %5227
  %5229 = load i16, i16* %5228, align 2
  %5230 = sext i16 %5229 to i64
  %5231 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %5225
  store i64 %5230, i64* %5231, align 16
  %5232 = or i64 %5211, 3
  %5233 = shl i64 %5232, 5
  %5234 = add nuw nsw i64 %5233, %5209
  %5235 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %5234
  %5236 = load i16, i16* %5235, align 2
  %5237 = sext i16 %5236 to i64
  %5238 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %5232
  store i64 %5237, i64* %5238, align 8
  %5239 = add nuw nsw i64 %5211, 4
  %5240 = icmp eq i64 %5239, 32
  br i1 %5240, label %5241, label %5210

5241:                                             ; preds = %5210
  call void @vpx_fdct32(i64* nonnull %5175, i64* nonnull %5176, i32 0) #6
  %5242 = shl i64 %5209, 5
  %5243 = load <2 x i64>, <2 x i64>* %5177, align 16
  %5244 = add nsw <2 x i64> %5243, <i64 1, i64 1>
  %5245 = lshr <2 x i64> %5243, <i64 63, i64 63>
  %5246 = add nsw <2 x i64> %5244, %5245
  %5247 = lshr <2 x i64> %5246, <i64 2, i64 2>
  %5248 = trunc <2 x i64> %5247 to <2 x i32>
  %5249 = getelementptr inbounds i32, i32* %1, i64 %5242
  %5250 = bitcast i32* %5249 to <2 x i32>*
  store <2 x i32> %5248, <2 x i32>* %5250, align 4
  %5251 = load <2 x i64>, <2 x i64>* %5179, align 16
  %5252 = add nsw <2 x i64> %5251, <i64 1, i64 1>
  %5253 = lshr <2 x i64> %5251, <i64 63, i64 63>
  %5254 = add nsw <2 x i64> %5252, %5253
  %5255 = lshr <2 x i64> %5254, <i64 2, i64 2>
  %5256 = trunc <2 x i64> %5255 to <2 x i32>
  %5257 = or i64 %5242, 2
  %5258 = getelementptr inbounds i32, i32* %1, i64 %5257
  %5259 = bitcast i32* %5258 to <2 x i32>*
  store <2 x i32> %5256, <2 x i32>* %5259, align 4
  %5260 = load <2 x i64>, <2 x i64>* %5181, align 16
  %5261 = add nsw <2 x i64> %5260, <i64 1, i64 1>
  %5262 = lshr <2 x i64> %5260, <i64 63, i64 63>
  %5263 = add nsw <2 x i64> %5261, %5262
  %5264 = lshr <2 x i64> %5263, <i64 2, i64 2>
  %5265 = trunc <2 x i64> %5264 to <2 x i32>
  %5266 = or i64 %5242, 4
  %5267 = getelementptr inbounds i32, i32* %1, i64 %5266
  %5268 = bitcast i32* %5267 to <2 x i32>*
  store <2 x i32> %5265, <2 x i32>* %5268, align 4
  %5269 = load <2 x i64>, <2 x i64>* %5183, align 16
  %5270 = add nsw <2 x i64> %5269, <i64 1, i64 1>
  %5271 = lshr <2 x i64> %5269, <i64 63, i64 63>
  %5272 = add nsw <2 x i64> %5270, %5271
  %5273 = lshr <2 x i64> %5272, <i64 2, i64 2>
  %5274 = trunc <2 x i64> %5273 to <2 x i32>
  %5275 = or i64 %5242, 6
  %5276 = getelementptr inbounds i32, i32* %1, i64 %5275
  %5277 = bitcast i32* %5276 to <2 x i32>*
  store <2 x i32> %5274, <2 x i32>* %5277, align 4
  %5278 = load <2 x i64>, <2 x i64>* %5185, align 16
  %5279 = add nsw <2 x i64> %5278, <i64 1, i64 1>
  %5280 = lshr <2 x i64> %5278, <i64 63, i64 63>
  %5281 = add nsw <2 x i64> %5279, %5280
  %5282 = lshr <2 x i64> %5281, <i64 2, i64 2>
  %5283 = trunc <2 x i64> %5282 to <2 x i32>
  %5284 = or i64 %5242, 8
  %5285 = getelementptr inbounds i32, i32* %1, i64 %5284
  %5286 = bitcast i32* %5285 to <2 x i32>*
  store <2 x i32> %5283, <2 x i32>* %5286, align 4
  %5287 = load <2 x i64>, <2 x i64>* %5187, align 16
  %5288 = add nsw <2 x i64> %5287, <i64 1, i64 1>
  %5289 = lshr <2 x i64> %5287, <i64 63, i64 63>
  %5290 = add nsw <2 x i64> %5288, %5289
  %5291 = lshr <2 x i64> %5290, <i64 2, i64 2>
  %5292 = trunc <2 x i64> %5291 to <2 x i32>
  %5293 = or i64 %5242, 10
  %5294 = getelementptr inbounds i32, i32* %1, i64 %5293
  %5295 = bitcast i32* %5294 to <2 x i32>*
  store <2 x i32> %5292, <2 x i32>* %5295, align 4
  %5296 = load <2 x i64>, <2 x i64>* %5189, align 16
  %5297 = add nsw <2 x i64> %5296, <i64 1, i64 1>
  %5298 = lshr <2 x i64> %5296, <i64 63, i64 63>
  %5299 = add nsw <2 x i64> %5297, %5298
  %5300 = lshr <2 x i64> %5299, <i64 2, i64 2>
  %5301 = trunc <2 x i64> %5300 to <2 x i32>
  %5302 = or i64 %5242, 12
  %5303 = getelementptr inbounds i32, i32* %1, i64 %5302
  %5304 = bitcast i32* %5303 to <2 x i32>*
  store <2 x i32> %5301, <2 x i32>* %5304, align 4
  %5305 = load <2 x i64>, <2 x i64>* %5191, align 16
  %5306 = add nsw <2 x i64> %5305, <i64 1, i64 1>
  %5307 = lshr <2 x i64> %5305, <i64 63, i64 63>
  %5308 = add nsw <2 x i64> %5306, %5307
  %5309 = lshr <2 x i64> %5308, <i64 2, i64 2>
  %5310 = trunc <2 x i64> %5309 to <2 x i32>
  %5311 = or i64 %5242, 14
  %5312 = getelementptr inbounds i32, i32* %1, i64 %5311
  %5313 = bitcast i32* %5312 to <2 x i32>*
  store <2 x i32> %5310, <2 x i32>* %5313, align 4
  %5314 = load <2 x i64>, <2 x i64>* %5193, align 16
  %5315 = add nsw <2 x i64> %5314, <i64 1, i64 1>
  %5316 = lshr <2 x i64> %5314, <i64 63, i64 63>
  %5317 = add nsw <2 x i64> %5315, %5316
  %5318 = lshr <2 x i64> %5317, <i64 2, i64 2>
  %5319 = trunc <2 x i64> %5318 to <2 x i32>
  %5320 = or i64 %5242, 16
  %5321 = getelementptr inbounds i32, i32* %1, i64 %5320
  %5322 = bitcast i32* %5321 to <2 x i32>*
  store <2 x i32> %5319, <2 x i32>* %5322, align 4
  %5323 = load <2 x i64>, <2 x i64>* %5195, align 16
  %5324 = add nsw <2 x i64> %5323, <i64 1, i64 1>
  %5325 = lshr <2 x i64> %5323, <i64 63, i64 63>
  %5326 = add nsw <2 x i64> %5324, %5325
  %5327 = lshr <2 x i64> %5326, <i64 2, i64 2>
  %5328 = trunc <2 x i64> %5327 to <2 x i32>
  %5329 = or i64 %5242, 18
  %5330 = getelementptr inbounds i32, i32* %1, i64 %5329
  %5331 = bitcast i32* %5330 to <2 x i32>*
  store <2 x i32> %5328, <2 x i32>* %5331, align 4
  %5332 = load <2 x i64>, <2 x i64>* %5197, align 16
  %5333 = add nsw <2 x i64> %5332, <i64 1, i64 1>
  %5334 = lshr <2 x i64> %5332, <i64 63, i64 63>
  %5335 = add nsw <2 x i64> %5333, %5334
  %5336 = lshr <2 x i64> %5335, <i64 2, i64 2>
  %5337 = trunc <2 x i64> %5336 to <2 x i32>
  %5338 = or i64 %5242, 20
  %5339 = getelementptr inbounds i32, i32* %1, i64 %5338
  %5340 = bitcast i32* %5339 to <2 x i32>*
  store <2 x i32> %5337, <2 x i32>* %5340, align 4
  %5341 = load <2 x i64>, <2 x i64>* %5199, align 16
  %5342 = add nsw <2 x i64> %5341, <i64 1, i64 1>
  %5343 = lshr <2 x i64> %5341, <i64 63, i64 63>
  %5344 = add nsw <2 x i64> %5342, %5343
  %5345 = lshr <2 x i64> %5344, <i64 2, i64 2>
  %5346 = trunc <2 x i64> %5345 to <2 x i32>
  %5347 = or i64 %5242, 22
  %5348 = getelementptr inbounds i32, i32* %1, i64 %5347
  %5349 = bitcast i32* %5348 to <2 x i32>*
  store <2 x i32> %5346, <2 x i32>* %5349, align 4
  %5350 = load <2 x i64>, <2 x i64>* %5201, align 16
  %5351 = add nsw <2 x i64> %5350, <i64 1, i64 1>
  %5352 = lshr <2 x i64> %5350, <i64 63, i64 63>
  %5353 = add nsw <2 x i64> %5351, %5352
  %5354 = lshr <2 x i64> %5353, <i64 2, i64 2>
  %5355 = trunc <2 x i64> %5354 to <2 x i32>
  %5356 = or i64 %5242, 24
  %5357 = getelementptr inbounds i32, i32* %1, i64 %5356
  %5358 = bitcast i32* %5357 to <2 x i32>*
  store <2 x i32> %5355, <2 x i32>* %5358, align 4
  %5359 = load <2 x i64>, <2 x i64>* %5203, align 16
  %5360 = add nsw <2 x i64> %5359, <i64 1, i64 1>
  %5361 = lshr <2 x i64> %5359, <i64 63, i64 63>
  %5362 = add nsw <2 x i64> %5360, %5361
  %5363 = lshr <2 x i64> %5362, <i64 2, i64 2>
  %5364 = trunc <2 x i64> %5363 to <2 x i32>
  %5365 = or i64 %5242, 26
  %5366 = getelementptr inbounds i32, i32* %1, i64 %5365
  %5367 = bitcast i32* %5366 to <2 x i32>*
  store <2 x i32> %5364, <2 x i32>* %5367, align 4
  %5368 = load <2 x i64>, <2 x i64>* %5205, align 16
  %5369 = add nsw <2 x i64> %5368, <i64 1, i64 1>
  %5370 = lshr <2 x i64> %5368, <i64 63, i64 63>
  %5371 = add nsw <2 x i64> %5369, %5370
  %5372 = lshr <2 x i64> %5371, <i64 2, i64 2>
  %5373 = trunc <2 x i64> %5372 to <2 x i32>
  %5374 = or i64 %5242, 28
  %5375 = getelementptr inbounds i32, i32* %1, i64 %5374
  %5376 = bitcast i32* %5375 to <2 x i32>*
  store <2 x i32> %5373, <2 x i32>* %5376, align 4
  %5377 = load <2 x i64>, <2 x i64>* %5207, align 16
  %5378 = add nsw <2 x i64> %5377, <i64 1, i64 1>
  %5379 = lshr <2 x i64> %5377, <i64 63, i64 63>
  %5380 = add nsw <2 x i64> %5378, %5379
  %5381 = lshr <2 x i64> %5380, <i64 2, i64 2>
  %5382 = trunc <2 x i64> %5381 to <2 x i32>
  %5383 = or i64 %5242, 30
  %5384 = getelementptr inbounds i32, i32* %1, i64 %5383
  %5385 = bitcast i32* %5384 to <2 x i32>*
  store <2 x i32> %5382, <2 x i32>* %5385, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %5174) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %5173) #6
  %5386 = add nuw nsw i64 %5209, 1
  %5387 = icmp eq i64 %5386, 32
  br i1 %5387, label %11292, label %5208

5388:                                             ; preds = %5039
  %5389 = shufflevector <4 x i32> %3375, <4 x i32> %3390, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %5390 = bitcast <4 x i32> %5389 to <2 x i64>
  %5391 = shufflevector <4 x i32> %3375, <4 x i32> %3390, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %5392 = bitcast <4 x i32> %5391 to <2 x i64>
  %5393 = shufflevector <4 x i32> %3377, <4 x i32> %3391, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %5394 = bitcast <4 x i32> %5393 to <2 x i64>
  %5395 = shufflevector <4 x i32> %3377, <4 x i32> %3391, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %5396 = bitcast <4 x i32> %5395 to <2 x i64>
  %5397 = shufflevector <4 x i32> %3378, <4 x i32> %3387, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %5398 = bitcast <4 x i32> %5397 to <2 x i64>
  %5399 = shufflevector <4 x i32> %3378, <4 x i32> %3387, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %5400 = bitcast <4 x i32> %5399 to <2 x i64>
  %5401 = shufflevector <4 x i32> %3379, <4 x i32> %3389, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %5402 = bitcast <4 x i32> %5401 to <2 x i64>
  %5403 = shufflevector <4 x i32> %3379, <4 x i32> %3389, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %5404 = bitcast <4 x i32> %5403 to <2 x i64>
  %5405 = and <2 x i64> %5390, <i64 4294967295, i64 4294967295>
  %5406 = mul nuw <2 x i64> %5405, <i64 4294952159, i64 4294952159>
  %5407 = lshr <2 x i64> %5390, <i64 32, i64 32>
  %5408 = mul nuw nsw <2 x i64> %5407, <i64 6270, i64 6270>
  %5409 = add <2 x i64> %5408, %5406
  %5410 = and <2 x i64> %5392, <i64 4294967295, i64 4294967295>
  %5411 = mul nuw <2 x i64> %5410, <i64 4294952159, i64 4294952159>
  %5412 = lshr <2 x i64> %5392, <i64 32, i64 32>
  %5413 = mul nuw nsw <2 x i64> %5412, <i64 6270, i64 6270>
  %5414 = add <2 x i64> %5413, %5411
  %5415 = and <2 x i64> %5394, <i64 4294967295, i64 4294967295>
  %5416 = mul nuw <2 x i64> %5415, <i64 4294952159, i64 4294952159>
  %5417 = lshr <2 x i64> %5394, <i64 32, i64 32>
  %5418 = mul nuw nsw <2 x i64> %5417, <i64 6270, i64 6270>
  %5419 = add <2 x i64> %5418, %5416
  %5420 = and <2 x i64> %5396, <i64 4294967295, i64 4294967295>
  %5421 = mul nuw <2 x i64> %5420, <i64 4294952159, i64 4294952159>
  %5422 = lshr <2 x i64> %5396, <i64 32, i64 32>
  %5423 = mul nuw nsw <2 x i64> %5422, <i64 6270, i64 6270>
  %5424 = add <2 x i64> %5423, %5421
  %5425 = and <2 x i64> %5398, <i64 4294967295, i64 4294967295>
  %5426 = mul nuw <2 x i64> %5425, <i64 4294961026, i64 4294961026>
  %5427 = lshr <2 x i64> %5398, <i64 32, i64 32>
  %5428 = mul nuw <2 x i64> %5427, <i64 4294952159, i64 4294952159>
  %5429 = add <2 x i64> %5428, %5426
  %5430 = and <2 x i64> %5400, <i64 4294967295, i64 4294967295>
  %5431 = mul nuw <2 x i64> %5430, <i64 4294961026, i64 4294961026>
  %5432 = lshr <2 x i64> %5400, <i64 32, i64 32>
  %5433 = mul nuw <2 x i64> %5432, <i64 4294952159, i64 4294952159>
  %5434 = add <2 x i64> %5433, %5431
  %5435 = and <2 x i64> %5402, <i64 4294967295, i64 4294967295>
  %5436 = mul nuw <2 x i64> %5435, <i64 4294961026, i64 4294961026>
  %5437 = lshr <2 x i64> %5402, <i64 32, i64 32>
  %5438 = mul nuw <2 x i64> %5437, <i64 4294952159, i64 4294952159>
  %5439 = add <2 x i64> %5438, %5436
  %5440 = and <2 x i64> %5404, <i64 4294967295, i64 4294967295>
  %5441 = mul nuw <2 x i64> %5440, <i64 4294961026, i64 4294961026>
  %5442 = lshr <2 x i64> %5404, <i64 32, i64 32>
  %5443 = mul nuw <2 x i64> %5442, <i64 4294952159, i64 4294952159>
  %5444 = add <2 x i64> %5443, %5441
  %5445 = mul nuw <2 x i64> %5425, <i64 4294952159, i64 4294952159>
  %5446 = mul nuw nsw <2 x i64> %5427, <i64 6270, i64 6270>
  %5447 = add <2 x i64> %5446, %5445
  %5448 = mul nuw <2 x i64> %5430, <i64 4294952159, i64 4294952159>
  %5449 = mul nuw nsw <2 x i64> %5432, <i64 6270, i64 6270>
  %5450 = add <2 x i64> %5449, %5448
  %5451 = mul nuw <2 x i64> %5435, <i64 4294952159, i64 4294952159>
  %5452 = mul nuw nsw <2 x i64> %5437, <i64 6270, i64 6270>
  %5453 = add <2 x i64> %5452, %5451
  %5454 = mul nuw <2 x i64> %5440, <i64 4294952159, i64 4294952159>
  %5455 = mul nuw nsw <2 x i64> %5442, <i64 6270, i64 6270>
  %5456 = add <2 x i64> %5455, %5454
  %5457 = mul nuw nsw <2 x i64> %5405, <i64 6270, i64 6270>
  %5458 = mul nuw nsw <2 x i64> %5407, <i64 15137, i64 15137>
  %5459 = add nuw nsw <2 x i64> %5458, %5457
  %5460 = mul nuw nsw <2 x i64> %5410, <i64 6270, i64 6270>
  %5461 = mul nuw nsw <2 x i64> %5412, <i64 15137, i64 15137>
  %5462 = add nuw nsw <2 x i64> %5461, %5460
  %5463 = mul nuw nsw <2 x i64> %5415, <i64 6270, i64 6270>
  %5464 = mul nuw nsw <2 x i64> %5417, <i64 15137, i64 15137>
  %5465 = add nuw nsw <2 x i64> %5464, %5463
  %5466 = mul nuw nsw <2 x i64> %5420, <i64 6270, i64 6270>
  %5467 = mul nuw nsw <2 x i64> %5422, <i64 15137, i64 15137>
  %5468 = add nuw nsw <2 x i64> %5467, %5466
  %5469 = shl <2 x i64> %5409, <i64 1, i64 1>
  %5470 = shl <2 x i64> %5414, <i64 1, i64 1>
  %5471 = shl <2 x i64> %5419, <i64 1, i64 1>
  %5472 = shl <2 x i64> %5424, <i64 1, i64 1>
  %5473 = bitcast <2 x i64> %5469 to <4 x i32>
  %5474 = shufflevector <4 x i32> %5473, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %5475 = bitcast <4 x i32> %5474 to <2 x i64>
  %5476 = bitcast <2 x i64> %5470 to <4 x i32>
  %5477 = shufflevector <4 x i32> %5476, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %5478 = bitcast <4 x i32> %5477 to <2 x i64>
  %5479 = bitcast <2 x i64> %5471 to <4 x i32>
  %5480 = shufflevector <4 x i32> %5479, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %5481 = bitcast <4 x i32> %5480 to <2 x i64>
  %5482 = bitcast <2 x i64> %5472 to <4 x i32>
  %5483 = shufflevector <4 x i32> %5482, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %5484 = bitcast <4 x i32> %5483 to <2 x i64>
  %5485 = shufflevector <2 x i64> %5475, <2 x i64> %5478, <2 x i32> <i32 0, i32 2>
  %5486 = shufflevector <2 x i64> %5481, <2 x i64> %5484, <2 x i32> <i32 0, i32 2>
  %5487 = bitcast <2 x i64> %5485 to <4 x i32>
  %5488 = bitcast <2 x i64> %5486 to <4 x i32>
  %5489 = add <4 x i32> %5487, <i32 1, i32 1, i32 1, i32 1>
  %5490 = icmp ugt <4 x i32> %5489, <i32 1, i32 1, i32 1, i32 1>
  %5491 = sext <4 x i1> %5490 to <4 x i32>
  %5492 = bitcast <4 x i32> %5491 to <16 x i8>
  %5493 = icmp slt <16 x i8> %5492, zeroinitializer
  %5494 = bitcast <16 x i1> %5493 to i16
  %5495 = zext i16 %5494 to i32
  %5496 = add <4 x i32> %5488, <i32 1, i32 1, i32 1, i32 1>
  %5497 = icmp ugt <4 x i32> %5496, <i32 1, i32 1, i32 1, i32 1>
  %5498 = sext <4 x i1> %5497 to <4 x i32>
  %5499 = bitcast <4 x i32> %5498 to <16 x i8>
  %5500 = icmp slt <16 x i8> %5499, zeroinitializer
  %5501 = bitcast <16 x i1> %5500 to i16
  %5502 = zext i16 %5501 to i32
  %5503 = sub nsw i32 0, %5495
  %5504 = icmp eq i32 %5502, %5503
  br i1 %5504, label %5505, label %5616

5505:                                             ; preds = %5388
  %5506 = shl <2 x i64> %5429, <i64 1, i64 1>
  %5507 = shl <2 x i64> %5434, <i64 1, i64 1>
  %5508 = shl <2 x i64> %5439, <i64 1, i64 1>
  %5509 = shl <2 x i64> %5444, <i64 1, i64 1>
  %5510 = bitcast <2 x i64> %5506 to <4 x i32>
  %5511 = shufflevector <4 x i32> %5510, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %5512 = bitcast <4 x i32> %5511 to <2 x i64>
  %5513 = bitcast <2 x i64> %5507 to <4 x i32>
  %5514 = shufflevector <4 x i32> %5513, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %5515 = bitcast <4 x i32> %5514 to <2 x i64>
  %5516 = bitcast <2 x i64> %5508 to <4 x i32>
  %5517 = shufflevector <4 x i32> %5516, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %5518 = bitcast <4 x i32> %5517 to <2 x i64>
  %5519 = bitcast <2 x i64> %5509 to <4 x i32>
  %5520 = shufflevector <4 x i32> %5519, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %5521 = bitcast <4 x i32> %5520 to <2 x i64>
  %5522 = shufflevector <2 x i64> %5512, <2 x i64> %5515, <2 x i32> <i32 0, i32 2>
  %5523 = shufflevector <2 x i64> %5518, <2 x i64> %5521, <2 x i32> <i32 0, i32 2>
  %5524 = bitcast <2 x i64> %5522 to <4 x i32>
  %5525 = bitcast <2 x i64> %5523 to <4 x i32>
  %5526 = add <4 x i32> %5524, <i32 1, i32 1, i32 1, i32 1>
  %5527 = icmp ugt <4 x i32> %5526, <i32 1, i32 1, i32 1, i32 1>
  %5528 = sext <4 x i1> %5527 to <4 x i32>
  %5529 = bitcast <4 x i32> %5528 to <16 x i8>
  %5530 = icmp slt <16 x i8> %5529, zeroinitializer
  %5531 = bitcast <16 x i1> %5530 to i16
  %5532 = zext i16 %5531 to i32
  %5533 = add <4 x i32> %5525, <i32 1, i32 1, i32 1, i32 1>
  %5534 = icmp ugt <4 x i32> %5533, <i32 1, i32 1, i32 1, i32 1>
  %5535 = sext <4 x i1> %5534 to <4 x i32>
  %5536 = bitcast <4 x i32> %5535 to <16 x i8>
  %5537 = icmp slt <16 x i8> %5536, zeroinitializer
  %5538 = bitcast <16 x i1> %5537 to i16
  %5539 = zext i16 %5538 to i32
  %5540 = sub nsw i32 0, %5532
  %5541 = icmp eq i32 %5539, %5540
  br i1 %5541, label %5542, label %5616

5542:                                             ; preds = %5505
  %5543 = shl <2 x i64> %5447, <i64 1, i64 1>
  %5544 = shl <2 x i64> %5450, <i64 1, i64 1>
  %5545 = shl <2 x i64> %5453, <i64 1, i64 1>
  %5546 = shl <2 x i64> %5456, <i64 1, i64 1>
  %5547 = bitcast <2 x i64> %5543 to <4 x i32>
  %5548 = shufflevector <4 x i32> %5547, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %5549 = bitcast <4 x i32> %5548 to <2 x i64>
  %5550 = bitcast <2 x i64> %5544 to <4 x i32>
  %5551 = shufflevector <4 x i32> %5550, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %5552 = bitcast <4 x i32> %5551 to <2 x i64>
  %5553 = bitcast <2 x i64> %5545 to <4 x i32>
  %5554 = shufflevector <4 x i32> %5553, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %5555 = bitcast <4 x i32> %5554 to <2 x i64>
  %5556 = bitcast <2 x i64> %5546 to <4 x i32>
  %5557 = shufflevector <4 x i32> %5556, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %5558 = bitcast <4 x i32> %5557 to <2 x i64>
  %5559 = shufflevector <2 x i64> %5549, <2 x i64> %5552, <2 x i32> <i32 0, i32 2>
  %5560 = shufflevector <2 x i64> %5555, <2 x i64> %5558, <2 x i32> <i32 0, i32 2>
  %5561 = bitcast <2 x i64> %5559 to <4 x i32>
  %5562 = bitcast <2 x i64> %5560 to <4 x i32>
  %5563 = add <4 x i32> %5561, <i32 1, i32 1, i32 1, i32 1>
  %5564 = icmp ugt <4 x i32> %5563, <i32 1, i32 1, i32 1, i32 1>
  %5565 = sext <4 x i1> %5564 to <4 x i32>
  %5566 = bitcast <4 x i32> %5565 to <16 x i8>
  %5567 = icmp slt <16 x i8> %5566, zeroinitializer
  %5568 = bitcast <16 x i1> %5567 to i16
  %5569 = zext i16 %5568 to i32
  %5570 = add <4 x i32> %5562, <i32 1, i32 1, i32 1, i32 1>
  %5571 = icmp ugt <4 x i32> %5570, <i32 1, i32 1, i32 1, i32 1>
  %5572 = sext <4 x i1> %5571 to <4 x i32>
  %5573 = bitcast <4 x i32> %5572 to <16 x i8>
  %5574 = icmp slt <16 x i8> %5573, zeroinitializer
  %5575 = bitcast <16 x i1> %5574 to i16
  %5576 = zext i16 %5575 to i32
  %5577 = sub nsw i32 0, %5569
  %5578 = icmp eq i32 %5576, %5577
  br i1 %5578, label %5579, label %5616

5579:                                             ; preds = %5542
  %5580 = shl nuw nsw <2 x i64> %5459, <i64 1, i64 1>
  %5581 = shl nuw nsw <2 x i64> %5462, <i64 1, i64 1>
  %5582 = shl nuw nsw <2 x i64> %5465, <i64 1, i64 1>
  %5583 = shl nuw nsw <2 x i64> %5468, <i64 1, i64 1>
  %5584 = bitcast <2 x i64> %5580 to <4 x i32>
  %5585 = shufflevector <4 x i32> %5584, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %5586 = bitcast <4 x i32> %5585 to <2 x i64>
  %5587 = bitcast <2 x i64> %5581 to <4 x i32>
  %5588 = shufflevector <4 x i32> %5587, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %5589 = bitcast <4 x i32> %5588 to <2 x i64>
  %5590 = bitcast <2 x i64> %5582 to <4 x i32>
  %5591 = shufflevector <4 x i32> %5590, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %5592 = bitcast <4 x i32> %5591 to <2 x i64>
  %5593 = bitcast <2 x i64> %5583 to <4 x i32>
  %5594 = shufflevector <4 x i32> %5593, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %5595 = bitcast <4 x i32> %5594 to <2 x i64>
  %5596 = shufflevector <2 x i64> %5586, <2 x i64> %5589, <2 x i32> <i32 0, i32 2>
  %5597 = shufflevector <2 x i64> %5592, <2 x i64> %5595, <2 x i32> <i32 0, i32 2>
  %5598 = bitcast <2 x i64> %5596 to <4 x i32>
  %5599 = bitcast <2 x i64> %5597 to <4 x i32>
  %5600 = add <4 x i32> %5598, <i32 1, i32 1, i32 1, i32 1>
  %5601 = icmp ugt <4 x i32> %5600, <i32 1, i32 1, i32 1, i32 1>
  %5602 = sext <4 x i1> %5601 to <4 x i32>
  %5603 = bitcast <4 x i32> %5602 to <16 x i8>
  %5604 = icmp slt <16 x i8> %5603, zeroinitializer
  %5605 = bitcast <16 x i1> %5604 to i16
  %5606 = zext i16 %5605 to i32
  %5607 = add <4 x i32> %5599, <i32 1, i32 1, i32 1, i32 1>
  %5608 = icmp ugt <4 x i32> %5607, <i32 1, i32 1, i32 1, i32 1>
  %5609 = sext <4 x i1> %5608 to <4 x i32>
  %5610 = bitcast <4 x i32> %5609 to <16 x i8>
  %5611 = icmp slt <16 x i8> %5610, zeroinitializer
  %5612 = bitcast <16 x i1> %5611 to i16
  %5613 = zext i16 %5612 to i32
  %5614 = sub nsw i32 0, %5606
  %5615 = icmp eq i32 %5613, %5614
  br i1 %5615, label %5832, label %5616

5616:                                             ; preds = %5542, %5505, %5388, %5579
  %5617 = bitcast [32 x i64]* %4 to i8*
  %5618 = bitcast [32 x i64]* %5 to i8*
  %5619 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %5620 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %5621 = bitcast [32 x i64]* %5 to <2 x i64>*
  %5622 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %5623 = bitcast i64* %5622 to <2 x i64>*
  %5624 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %5625 = bitcast i64* %5624 to <2 x i64>*
  %5626 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %5627 = bitcast i64* %5626 to <2 x i64>*
  %5628 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %5629 = bitcast i64* %5628 to <2 x i64>*
  %5630 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %5631 = bitcast i64* %5630 to <2 x i64>*
  %5632 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %5633 = bitcast i64* %5632 to <2 x i64>*
  %5634 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %5635 = bitcast i64* %5634 to <2 x i64>*
  %5636 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %5637 = bitcast i64* %5636 to <2 x i64>*
  %5638 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %5639 = bitcast i64* %5638 to <2 x i64>*
  %5640 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %5641 = bitcast i64* %5640 to <2 x i64>*
  %5642 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %5643 = bitcast i64* %5642 to <2 x i64>*
  %5644 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %5645 = bitcast i64* %5644 to <2 x i64>*
  %5646 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %5647 = bitcast i64* %5646 to <2 x i64>*
  %5648 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %5649 = bitcast i64* %5648 to <2 x i64>*
  %5650 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %5651 = bitcast i64* %5650 to <2 x i64>*
  br label %5652

5652:                                             ; preds = %5685, %5616
  %5653 = phi i64 [ 0, %5616 ], [ %5830, %5685 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %5617) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %5617, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %5618) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %5618, i8 -86, i64 256, i1 false) #6
  br label %5654

5654:                                             ; preds = %5654, %5652
  %5655 = phi i64 [ 0, %5652 ], [ %5683, %5654 ]
  %5656 = shl i64 %5655, 5
  %5657 = add nuw nsw i64 %5656, %5653
  %5658 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %5657
  %5659 = load i16, i16* %5658, align 2
  %5660 = sext i16 %5659 to i64
  %5661 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %5655
  store i64 %5660, i64* %5661, align 16
  %5662 = or i64 %5655, 1
  %5663 = shl i64 %5662, 5
  %5664 = add nuw nsw i64 %5663, %5653
  %5665 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %5664
  %5666 = load i16, i16* %5665, align 2
  %5667 = sext i16 %5666 to i64
  %5668 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %5662
  store i64 %5667, i64* %5668, align 8
  %5669 = or i64 %5655, 2
  %5670 = shl i64 %5669, 5
  %5671 = add nuw nsw i64 %5670, %5653
  %5672 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %5671
  %5673 = load i16, i16* %5672, align 2
  %5674 = sext i16 %5673 to i64
  %5675 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %5669
  store i64 %5674, i64* %5675, align 16
  %5676 = or i64 %5655, 3
  %5677 = shl i64 %5676, 5
  %5678 = add nuw nsw i64 %5677, %5653
  %5679 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %5678
  %5680 = load i16, i16* %5679, align 2
  %5681 = sext i16 %5680 to i64
  %5682 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %5676
  store i64 %5681, i64* %5682, align 8
  %5683 = add nuw nsw i64 %5655, 4
  %5684 = icmp eq i64 %5683, 32
  br i1 %5684, label %5685, label %5654

5685:                                             ; preds = %5654
  call void @vpx_fdct32(i64* nonnull %5619, i64* nonnull %5620, i32 0) #6
  %5686 = shl i64 %5653, 5
  %5687 = load <2 x i64>, <2 x i64>* %5621, align 16
  %5688 = add nsw <2 x i64> %5687, <i64 1, i64 1>
  %5689 = lshr <2 x i64> %5687, <i64 63, i64 63>
  %5690 = add nsw <2 x i64> %5688, %5689
  %5691 = lshr <2 x i64> %5690, <i64 2, i64 2>
  %5692 = trunc <2 x i64> %5691 to <2 x i32>
  %5693 = getelementptr inbounds i32, i32* %1, i64 %5686
  %5694 = bitcast i32* %5693 to <2 x i32>*
  store <2 x i32> %5692, <2 x i32>* %5694, align 4
  %5695 = load <2 x i64>, <2 x i64>* %5623, align 16
  %5696 = add nsw <2 x i64> %5695, <i64 1, i64 1>
  %5697 = lshr <2 x i64> %5695, <i64 63, i64 63>
  %5698 = add nsw <2 x i64> %5696, %5697
  %5699 = lshr <2 x i64> %5698, <i64 2, i64 2>
  %5700 = trunc <2 x i64> %5699 to <2 x i32>
  %5701 = or i64 %5686, 2
  %5702 = getelementptr inbounds i32, i32* %1, i64 %5701
  %5703 = bitcast i32* %5702 to <2 x i32>*
  store <2 x i32> %5700, <2 x i32>* %5703, align 4
  %5704 = load <2 x i64>, <2 x i64>* %5625, align 16
  %5705 = add nsw <2 x i64> %5704, <i64 1, i64 1>
  %5706 = lshr <2 x i64> %5704, <i64 63, i64 63>
  %5707 = add nsw <2 x i64> %5705, %5706
  %5708 = lshr <2 x i64> %5707, <i64 2, i64 2>
  %5709 = trunc <2 x i64> %5708 to <2 x i32>
  %5710 = or i64 %5686, 4
  %5711 = getelementptr inbounds i32, i32* %1, i64 %5710
  %5712 = bitcast i32* %5711 to <2 x i32>*
  store <2 x i32> %5709, <2 x i32>* %5712, align 4
  %5713 = load <2 x i64>, <2 x i64>* %5627, align 16
  %5714 = add nsw <2 x i64> %5713, <i64 1, i64 1>
  %5715 = lshr <2 x i64> %5713, <i64 63, i64 63>
  %5716 = add nsw <2 x i64> %5714, %5715
  %5717 = lshr <2 x i64> %5716, <i64 2, i64 2>
  %5718 = trunc <2 x i64> %5717 to <2 x i32>
  %5719 = or i64 %5686, 6
  %5720 = getelementptr inbounds i32, i32* %1, i64 %5719
  %5721 = bitcast i32* %5720 to <2 x i32>*
  store <2 x i32> %5718, <2 x i32>* %5721, align 4
  %5722 = load <2 x i64>, <2 x i64>* %5629, align 16
  %5723 = add nsw <2 x i64> %5722, <i64 1, i64 1>
  %5724 = lshr <2 x i64> %5722, <i64 63, i64 63>
  %5725 = add nsw <2 x i64> %5723, %5724
  %5726 = lshr <2 x i64> %5725, <i64 2, i64 2>
  %5727 = trunc <2 x i64> %5726 to <2 x i32>
  %5728 = or i64 %5686, 8
  %5729 = getelementptr inbounds i32, i32* %1, i64 %5728
  %5730 = bitcast i32* %5729 to <2 x i32>*
  store <2 x i32> %5727, <2 x i32>* %5730, align 4
  %5731 = load <2 x i64>, <2 x i64>* %5631, align 16
  %5732 = add nsw <2 x i64> %5731, <i64 1, i64 1>
  %5733 = lshr <2 x i64> %5731, <i64 63, i64 63>
  %5734 = add nsw <2 x i64> %5732, %5733
  %5735 = lshr <2 x i64> %5734, <i64 2, i64 2>
  %5736 = trunc <2 x i64> %5735 to <2 x i32>
  %5737 = or i64 %5686, 10
  %5738 = getelementptr inbounds i32, i32* %1, i64 %5737
  %5739 = bitcast i32* %5738 to <2 x i32>*
  store <2 x i32> %5736, <2 x i32>* %5739, align 4
  %5740 = load <2 x i64>, <2 x i64>* %5633, align 16
  %5741 = add nsw <2 x i64> %5740, <i64 1, i64 1>
  %5742 = lshr <2 x i64> %5740, <i64 63, i64 63>
  %5743 = add nsw <2 x i64> %5741, %5742
  %5744 = lshr <2 x i64> %5743, <i64 2, i64 2>
  %5745 = trunc <2 x i64> %5744 to <2 x i32>
  %5746 = or i64 %5686, 12
  %5747 = getelementptr inbounds i32, i32* %1, i64 %5746
  %5748 = bitcast i32* %5747 to <2 x i32>*
  store <2 x i32> %5745, <2 x i32>* %5748, align 4
  %5749 = load <2 x i64>, <2 x i64>* %5635, align 16
  %5750 = add nsw <2 x i64> %5749, <i64 1, i64 1>
  %5751 = lshr <2 x i64> %5749, <i64 63, i64 63>
  %5752 = add nsw <2 x i64> %5750, %5751
  %5753 = lshr <2 x i64> %5752, <i64 2, i64 2>
  %5754 = trunc <2 x i64> %5753 to <2 x i32>
  %5755 = or i64 %5686, 14
  %5756 = getelementptr inbounds i32, i32* %1, i64 %5755
  %5757 = bitcast i32* %5756 to <2 x i32>*
  store <2 x i32> %5754, <2 x i32>* %5757, align 4
  %5758 = load <2 x i64>, <2 x i64>* %5637, align 16
  %5759 = add nsw <2 x i64> %5758, <i64 1, i64 1>
  %5760 = lshr <2 x i64> %5758, <i64 63, i64 63>
  %5761 = add nsw <2 x i64> %5759, %5760
  %5762 = lshr <2 x i64> %5761, <i64 2, i64 2>
  %5763 = trunc <2 x i64> %5762 to <2 x i32>
  %5764 = or i64 %5686, 16
  %5765 = getelementptr inbounds i32, i32* %1, i64 %5764
  %5766 = bitcast i32* %5765 to <2 x i32>*
  store <2 x i32> %5763, <2 x i32>* %5766, align 4
  %5767 = load <2 x i64>, <2 x i64>* %5639, align 16
  %5768 = add nsw <2 x i64> %5767, <i64 1, i64 1>
  %5769 = lshr <2 x i64> %5767, <i64 63, i64 63>
  %5770 = add nsw <2 x i64> %5768, %5769
  %5771 = lshr <2 x i64> %5770, <i64 2, i64 2>
  %5772 = trunc <2 x i64> %5771 to <2 x i32>
  %5773 = or i64 %5686, 18
  %5774 = getelementptr inbounds i32, i32* %1, i64 %5773
  %5775 = bitcast i32* %5774 to <2 x i32>*
  store <2 x i32> %5772, <2 x i32>* %5775, align 4
  %5776 = load <2 x i64>, <2 x i64>* %5641, align 16
  %5777 = add nsw <2 x i64> %5776, <i64 1, i64 1>
  %5778 = lshr <2 x i64> %5776, <i64 63, i64 63>
  %5779 = add nsw <2 x i64> %5777, %5778
  %5780 = lshr <2 x i64> %5779, <i64 2, i64 2>
  %5781 = trunc <2 x i64> %5780 to <2 x i32>
  %5782 = or i64 %5686, 20
  %5783 = getelementptr inbounds i32, i32* %1, i64 %5782
  %5784 = bitcast i32* %5783 to <2 x i32>*
  store <2 x i32> %5781, <2 x i32>* %5784, align 4
  %5785 = load <2 x i64>, <2 x i64>* %5643, align 16
  %5786 = add nsw <2 x i64> %5785, <i64 1, i64 1>
  %5787 = lshr <2 x i64> %5785, <i64 63, i64 63>
  %5788 = add nsw <2 x i64> %5786, %5787
  %5789 = lshr <2 x i64> %5788, <i64 2, i64 2>
  %5790 = trunc <2 x i64> %5789 to <2 x i32>
  %5791 = or i64 %5686, 22
  %5792 = getelementptr inbounds i32, i32* %1, i64 %5791
  %5793 = bitcast i32* %5792 to <2 x i32>*
  store <2 x i32> %5790, <2 x i32>* %5793, align 4
  %5794 = load <2 x i64>, <2 x i64>* %5645, align 16
  %5795 = add nsw <2 x i64> %5794, <i64 1, i64 1>
  %5796 = lshr <2 x i64> %5794, <i64 63, i64 63>
  %5797 = add nsw <2 x i64> %5795, %5796
  %5798 = lshr <2 x i64> %5797, <i64 2, i64 2>
  %5799 = trunc <2 x i64> %5798 to <2 x i32>
  %5800 = or i64 %5686, 24
  %5801 = getelementptr inbounds i32, i32* %1, i64 %5800
  %5802 = bitcast i32* %5801 to <2 x i32>*
  store <2 x i32> %5799, <2 x i32>* %5802, align 4
  %5803 = load <2 x i64>, <2 x i64>* %5647, align 16
  %5804 = add nsw <2 x i64> %5803, <i64 1, i64 1>
  %5805 = lshr <2 x i64> %5803, <i64 63, i64 63>
  %5806 = add nsw <2 x i64> %5804, %5805
  %5807 = lshr <2 x i64> %5806, <i64 2, i64 2>
  %5808 = trunc <2 x i64> %5807 to <2 x i32>
  %5809 = or i64 %5686, 26
  %5810 = getelementptr inbounds i32, i32* %1, i64 %5809
  %5811 = bitcast i32* %5810 to <2 x i32>*
  store <2 x i32> %5808, <2 x i32>* %5811, align 4
  %5812 = load <2 x i64>, <2 x i64>* %5649, align 16
  %5813 = add nsw <2 x i64> %5812, <i64 1, i64 1>
  %5814 = lshr <2 x i64> %5812, <i64 63, i64 63>
  %5815 = add nsw <2 x i64> %5813, %5814
  %5816 = lshr <2 x i64> %5815, <i64 2, i64 2>
  %5817 = trunc <2 x i64> %5816 to <2 x i32>
  %5818 = or i64 %5686, 28
  %5819 = getelementptr inbounds i32, i32* %1, i64 %5818
  %5820 = bitcast i32* %5819 to <2 x i32>*
  store <2 x i32> %5817, <2 x i32>* %5820, align 4
  %5821 = load <2 x i64>, <2 x i64>* %5651, align 16
  %5822 = add nsw <2 x i64> %5821, <i64 1, i64 1>
  %5823 = lshr <2 x i64> %5821, <i64 63, i64 63>
  %5824 = add nsw <2 x i64> %5822, %5823
  %5825 = lshr <2 x i64> %5824, <i64 2, i64 2>
  %5826 = trunc <2 x i64> %5825 to <2 x i32>
  %5827 = or i64 %5686, 30
  %5828 = getelementptr inbounds i32, i32* %1, i64 %5827
  %5829 = bitcast i32* %5828 to <2 x i32>*
  store <2 x i32> %5826, <2 x i32>* %5829, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %5618) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %5617) #6
  %5830 = add nuw nsw i64 %5653, 1
  %5831 = icmp eq i64 %5830, 32
  br i1 %5831, label %11292, label %5652

5832:                                             ; preds = %5579
  %5833 = bitcast <2 x i64> %5409 to <4 x i32>
  %5834 = shufflevector <4 x i32> %5833, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %5835 = bitcast <4 x i32> %5834 to <2 x i64>
  %5836 = bitcast <2 x i64> %5414 to <4 x i32>
  %5837 = shufflevector <4 x i32> %5836, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %5838 = bitcast <4 x i32> %5837 to <2 x i64>
  %5839 = shufflevector <2 x i64> %5835, <2 x i64> %5838, <2 x i32> <i32 0, i32 2>
  %5840 = bitcast <2 x i64> %5419 to <4 x i32>
  %5841 = shufflevector <4 x i32> %5840, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %5842 = bitcast <4 x i32> %5841 to <2 x i64>
  %5843 = bitcast <2 x i64> %5424 to <4 x i32>
  %5844 = shufflevector <4 x i32> %5843, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %5845 = bitcast <4 x i32> %5844 to <2 x i64>
  %5846 = shufflevector <2 x i64> %5842, <2 x i64> %5845, <2 x i32> <i32 0, i32 2>
  %5847 = bitcast <2 x i64> %5429 to <4 x i32>
  %5848 = shufflevector <4 x i32> %5847, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %5849 = bitcast <4 x i32> %5848 to <2 x i64>
  %5850 = bitcast <2 x i64> %5434 to <4 x i32>
  %5851 = shufflevector <4 x i32> %5850, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %5852 = bitcast <4 x i32> %5851 to <2 x i64>
  %5853 = shufflevector <2 x i64> %5849, <2 x i64> %5852, <2 x i32> <i32 0, i32 2>
  %5854 = bitcast <2 x i64> %5439 to <4 x i32>
  %5855 = shufflevector <4 x i32> %5854, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %5856 = bitcast <4 x i32> %5855 to <2 x i64>
  %5857 = bitcast <2 x i64> %5444 to <4 x i32>
  %5858 = shufflevector <4 x i32> %5857, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %5859 = bitcast <4 x i32> %5858 to <2 x i64>
  %5860 = shufflevector <2 x i64> %5856, <2 x i64> %5859, <2 x i32> <i32 0, i32 2>
  %5861 = bitcast <2 x i64> %5447 to <4 x i32>
  %5862 = shufflevector <4 x i32> %5861, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %5863 = bitcast <4 x i32> %5862 to <2 x i64>
  %5864 = bitcast <2 x i64> %5450 to <4 x i32>
  %5865 = shufflevector <4 x i32> %5864, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %5866 = bitcast <4 x i32> %5865 to <2 x i64>
  %5867 = shufflevector <2 x i64> %5863, <2 x i64> %5866, <2 x i32> <i32 0, i32 2>
  %5868 = bitcast <2 x i64> %5453 to <4 x i32>
  %5869 = shufflevector <4 x i32> %5868, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %5870 = bitcast <4 x i32> %5869 to <2 x i64>
  %5871 = bitcast <2 x i64> %5456 to <4 x i32>
  %5872 = shufflevector <4 x i32> %5871, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %5873 = bitcast <4 x i32> %5872 to <2 x i64>
  %5874 = shufflevector <2 x i64> %5870, <2 x i64> %5873, <2 x i32> <i32 0, i32 2>
  %5875 = bitcast <2 x i64> %5459 to <4 x i32>
  %5876 = shufflevector <4 x i32> %5875, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %5877 = bitcast <4 x i32> %5876 to <2 x i64>
  %5878 = bitcast <2 x i64> %5462 to <4 x i32>
  %5879 = shufflevector <4 x i32> %5878, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %5880 = bitcast <4 x i32> %5879 to <2 x i64>
  %5881 = shufflevector <2 x i64> %5877, <2 x i64> %5880, <2 x i32> <i32 0, i32 2>
  %5882 = bitcast <2 x i64> %5465 to <4 x i32>
  %5883 = shufflevector <4 x i32> %5882, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %5884 = bitcast <4 x i32> %5883 to <2 x i64>
  %5885 = bitcast <2 x i64> %5468 to <4 x i32>
  %5886 = shufflevector <4 x i32> %5885, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %5887 = bitcast <4 x i32> %5886 to <2 x i64>
  %5888 = shufflevector <2 x i64> %5884, <2 x i64> %5887, <2 x i32> <i32 0, i32 2>
  %5889 = bitcast <2 x i64> %5839 to <4 x i32>
  %5890 = add <4 x i32> %5889, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5891 = bitcast <2 x i64> %5846 to <4 x i32>
  %5892 = add <4 x i32> %5891, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5893 = bitcast <2 x i64> %5853 to <4 x i32>
  %5894 = add <4 x i32> %5893, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5895 = bitcast <2 x i64> %5860 to <4 x i32>
  %5896 = add <4 x i32> %5895, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5897 = bitcast <2 x i64> %5867 to <4 x i32>
  %5898 = add <4 x i32> %5897, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5899 = bitcast <2 x i64> %5874 to <4 x i32>
  %5900 = add <4 x i32> %5899, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5901 = bitcast <2 x i64> %5881 to <4 x i32>
  %5902 = add <4 x i32> %5901, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5903 = bitcast <2 x i64> %5888 to <4 x i32>
  %5904 = add <4 x i32> %5903, <i32 8192, i32 8192, i32 8192, i32 8192>
  %5905 = ashr <4 x i32> %5890, <i32 14, i32 14, i32 14, i32 14>
  %5906 = ashr <4 x i32> %5892, <i32 14, i32 14, i32 14, i32 14>
  %5907 = ashr <4 x i32> %5894, <i32 14, i32 14, i32 14, i32 14>
  %5908 = ashr <4 x i32> %5896, <i32 14, i32 14, i32 14, i32 14>
  %5909 = ashr <4 x i32> %5898, <i32 14, i32 14, i32 14, i32 14>
  %5910 = ashr <4 x i32> %5900, <i32 14, i32 14, i32 14, i32 14>
  %5911 = ashr <4 x i32> %5902, <i32 14, i32 14, i32 14, i32 14>
  %5912 = ashr <4 x i32> %5904, <i32 14, i32 14, i32 14, i32 14>
  %5913 = add <4 x i32> %4578, %3318
  %5914 = add <4 x i32> %4579, %3319
  %5915 = add <4 x i32> %4576, %3320
  %5916 = add <4 x i32> %4577, %3321
  %5917 = sub <4 x i32> %3320, %4576
  %5918 = sub <4 x i32> %3321, %4577
  %5919 = sub <4 x i32> %3318, %4578
  %5920 = sub <4 x i32> %3319, %4579
  %5921 = sub <4 x i32> %3332, %4580
  %5922 = sub <4 x i32> %3333, %4581
  %5923 = sub <4 x i32> %3330, %4582
  %5924 = sub <4 x i32> %3331, %4583
  %5925 = add <4 x i32> %4582, %3330
  %5926 = add <4 x i32> %4583, %3331
  %5927 = add <4 x i32> %4580, %3332
  %5928 = add <4 x i32> %4581, %3333
  %5929 = add <4 x i32> %4586, %3334
  %5930 = add <4 x i32> %4587, %3335
  %5931 = add <4 x i32> %4584, %3336
  %5932 = add <4 x i32> %4585, %3337
  %5933 = sub <4 x i32> %3336, %4584
  %5934 = sub <4 x i32> %3337, %4585
  %5935 = sub <4 x i32> %3334, %4586
  %5936 = sub <4 x i32> %3335, %4587
  %5937 = sub <4 x i32> %3348, %4588
  %5938 = sub <4 x i32> %3349, %4589
  %5939 = sub <4 x i32> %3346, %4590
  %5940 = sub <4 x i32> %3347, %4591
  %5941 = add <4 x i32> %4590, %3346
  %5942 = add <4 x i32> %4591, %3347
  %5943 = add <4 x i32> %4588, %3348
  %5944 = add <4 x i32> %4589, %3349
  %5945 = shufflevector <4 x i32> %4592, <4 x i32> %4598, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %5946 = bitcast <4 x i32> %5945 to <2 x i64>
  %5947 = shufflevector <4 x i32> %4592, <4 x i32> %4598, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %5948 = bitcast <4 x i32> %5947 to <2 x i64>
  %5949 = shufflevector <4 x i32> %4593, <4 x i32> %4599, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %5950 = bitcast <4 x i32> %5949 to <2 x i64>
  %5951 = shufflevector <4 x i32> %4593, <4 x i32> %4599, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %5952 = bitcast <4 x i32> %5951 to <2 x i64>
  %5953 = shufflevector <4 x i32> %4594, <4 x i32> %4596, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %5954 = bitcast <4 x i32> %5953 to <2 x i64>
  %5955 = shufflevector <4 x i32> %4594, <4 x i32> %4596, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %5956 = bitcast <4 x i32> %5955 to <2 x i64>
  %5957 = shufflevector <4 x i32> %4595, <4 x i32> %4597, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %5958 = bitcast <4 x i32> %5957 to <2 x i64>
  %5959 = shufflevector <4 x i32> %4595, <4 x i32> %4597, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %5960 = bitcast <4 x i32> %5959 to <2 x i64>
  %5961 = and <2 x i64> %5946, <i64 4294967295, i64 4294967295>
  %5962 = mul nuw nsw <2 x i64> %5961, <i64 3196, i64 3196>
  %5963 = lshr <2 x i64> %5946, <i64 32, i64 32>
  %5964 = mul nuw nsw <2 x i64> %5963, <i64 16069, i64 16069>
  %5965 = add nuw nsw <2 x i64> %5964, %5962
  %5966 = and <2 x i64> %5948, <i64 4294967295, i64 4294967295>
  %5967 = mul nuw nsw <2 x i64> %5966, <i64 3196, i64 3196>
  %5968 = lshr <2 x i64> %5948, <i64 32, i64 32>
  %5969 = mul nuw nsw <2 x i64> %5968, <i64 16069, i64 16069>
  %5970 = add nuw nsw <2 x i64> %5969, %5967
  %5971 = and <2 x i64> %5950, <i64 4294967295, i64 4294967295>
  %5972 = mul nuw nsw <2 x i64> %5971, <i64 3196, i64 3196>
  %5973 = lshr <2 x i64> %5950, <i64 32, i64 32>
  %5974 = mul nuw nsw <2 x i64> %5973, <i64 16069, i64 16069>
  %5975 = add nuw nsw <2 x i64> %5974, %5972
  %5976 = and <2 x i64> %5952, <i64 4294967295, i64 4294967295>
  %5977 = mul nuw nsw <2 x i64> %5976, <i64 3196, i64 3196>
  %5978 = lshr <2 x i64> %5952, <i64 32, i64 32>
  %5979 = mul nuw nsw <2 x i64> %5978, <i64 16069, i64 16069>
  %5980 = add nuw nsw <2 x i64> %5979, %5977
  %5981 = and <2 x i64> %5954, <i64 4294967295, i64 4294967295>
  %5982 = mul nuw nsw <2 x i64> %5981, <i64 13623, i64 13623>
  %5983 = lshr <2 x i64> %5954, <i64 32, i64 32>
  %5984 = mul nuw nsw <2 x i64> %5983, <i64 9102, i64 9102>
  %5985 = add nuw nsw <2 x i64> %5984, %5982
  %5986 = and <2 x i64> %5956, <i64 4294967295, i64 4294967295>
  %5987 = mul nuw nsw <2 x i64> %5986, <i64 13623, i64 13623>
  %5988 = lshr <2 x i64> %5956, <i64 32, i64 32>
  %5989 = mul nuw nsw <2 x i64> %5988, <i64 9102, i64 9102>
  %5990 = add nuw nsw <2 x i64> %5989, %5987
  %5991 = and <2 x i64> %5958, <i64 4294967295, i64 4294967295>
  %5992 = mul nuw nsw <2 x i64> %5991, <i64 13623, i64 13623>
  %5993 = lshr <2 x i64> %5958, <i64 32, i64 32>
  %5994 = mul nuw nsw <2 x i64> %5993, <i64 9102, i64 9102>
  %5995 = add nuw nsw <2 x i64> %5994, %5992
  %5996 = and <2 x i64> %5960, <i64 4294967295, i64 4294967295>
  %5997 = mul nuw nsw <2 x i64> %5996, <i64 13623, i64 13623>
  %5998 = lshr <2 x i64> %5960, <i64 32, i64 32>
  %5999 = mul nuw nsw <2 x i64> %5998, <i64 9102, i64 9102>
  %6000 = add nuw nsw <2 x i64> %5999, %5997
  %6001 = mul nuw <2 x i64> %5981, <i64 4294958194, i64 4294958194>
  %6002 = mul nuw nsw <2 x i64> %5983, <i64 13623, i64 13623>
  %6003 = add <2 x i64> %6002, %6001
  %6004 = mul nuw <2 x i64> %5986, <i64 4294958194, i64 4294958194>
  %6005 = mul nuw nsw <2 x i64> %5988, <i64 13623, i64 13623>
  %6006 = add <2 x i64> %6005, %6004
  %6007 = mul nuw <2 x i64> %5991, <i64 4294958194, i64 4294958194>
  %6008 = mul nuw nsw <2 x i64> %5993, <i64 13623, i64 13623>
  %6009 = add <2 x i64> %6008, %6007
  %6010 = mul nuw <2 x i64> %5996, <i64 4294958194, i64 4294958194>
  %6011 = mul nuw nsw <2 x i64> %5998, <i64 13623, i64 13623>
  %6012 = add <2 x i64> %6011, %6010
  %6013 = mul nuw <2 x i64> %5961, <i64 4294951227, i64 4294951227>
  %6014 = mul nuw nsw <2 x i64> %5963, <i64 3196, i64 3196>
  %6015 = add <2 x i64> %6014, %6013
  %6016 = mul nuw <2 x i64> %5966, <i64 4294951227, i64 4294951227>
  %6017 = mul nuw nsw <2 x i64> %5968, <i64 3196, i64 3196>
  %6018 = add <2 x i64> %6017, %6016
  %6019 = mul nuw <2 x i64> %5971, <i64 4294951227, i64 4294951227>
  %6020 = mul nuw nsw <2 x i64> %5973, <i64 3196, i64 3196>
  %6021 = add <2 x i64> %6020, %6019
  %6022 = mul nuw <2 x i64> %5976, <i64 4294951227, i64 4294951227>
  %6023 = mul nuw nsw <2 x i64> %5978, <i64 3196, i64 3196>
  %6024 = add <2 x i64> %6023, %6022
  %6025 = shl nuw nsw <2 x i64> %5965, <i64 1, i64 1>
  %6026 = shl nuw nsw <2 x i64> %5970, <i64 1, i64 1>
  %6027 = shl nuw nsw <2 x i64> %5975, <i64 1, i64 1>
  %6028 = shl nuw nsw <2 x i64> %5980, <i64 1, i64 1>
  %6029 = bitcast <2 x i64> %6025 to <4 x i32>
  %6030 = shufflevector <4 x i32> %6029, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %6031 = bitcast <4 x i32> %6030 to <2 x i64>
  %6032 = bitcast <2 x i64> %6026 to <4 x i32>
  %6033 = shufflevector <4 x i32> %6032, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %6034 = bitcast <4 x i32> %6033 to <2 x i64>
  %6035 = bitcast <2 x i64> %6027 to <4 x i32>
  %6036 = shufflevector <4 x i32> %6035, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %6037 = bitcast <4 x i32> %6036 to <2 x i64>
  %6038 = bitcast <2 x i64> %6028 to <4 x i32>
  %6039 = shufflevector <4 x i32> %6038, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %6040 = bitcast <4 x i32> %6039 to <2 x i64>
  %6041 = shufflevector <2 x i64> %6031, <2 x i64> %6034, <2 x i32> <i32 0, i32 2>
  %6042 = shufflevector <2 x i64> %6037, <2 x i64> %6040, <2 x i32> <i32 0, i32 2>
  %6043 = bitcast <2 x i64> %6041 to <4 x i32>
  %6044 = bitcast <2 x i64> %6042 to <4 x i32>
  %6045 = add <4 x i32> %6043, <i32 1, i32 1, i32 1, i32 1>
  %6046 = icmp ugt <4 x i32> %6045, <i32 1, i32 1, i32 1, i32 1>
  %6047 = sext <4 x i1> %6046 to <4 x i32>
  %6048 = bitcast <4 x i32> %6047 to <16 x i8>
  %6049 = icmp slt <16 x i8> %6048, zeroinitializer
  %6050 = bitcast <16 x i1> %6049 to i16
  %6051 = zext i16 %6050 to i32
  %6052 = add <4 x i32> %6044, <i32 1, i32 1, i32 1, i32 1>
  %6053 = icmp ugt <4 x i32> %6052, <i32 1, i32 1, i32 1, i32 1>
  %6054 = sext <4 x i1> %6053 to <4 x i32>
  %6055 = bitcast <4 x i32> %6054 to <16 x i8>
  %6056 = icmp slt <16 x i8> %6055, zeroinitializer
  %6057 = bitcast <16 x i1> %6056 to i16
  %6058 = zext i16 %6057 to i32
  %6059 = sub nsw i32 0, %6051
  %6060 = icmp eq i32 %6058, %6059
  br i1 %6060, label %6061, label %6172

6061:                                             ; preds = %5832
  %6062 = shl nuw nsw <2 x i64> %5985, <i64 1, i64 1>
  %6063 = shl nuw nsw <2 x i64> %5990, <i64 1, i64 1>
  %6064 = shl nuw nsw <2 x i64> %5995, <i64 1, i64 1>
  %6065 = shl nuw nsw <2 x i64> %6000, <i64 1, i64 1>
  %6066 = bitcast <2 x i64> %6062 to <4 x i32>
  %6067 = shufflevector <4 x i32> %6066, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %6068 = bitcast <4 x i32> %6067 to <2 x i64>
  %6069 = bitcast <2 x i64> %6063 to <4 x i32>
  %6070 = shufflevector <4 x i32> %6069, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %6071 = bitcast <4 x i32> %6070 to <2 x i64>
  %6072 = bitcast <2 x i64> %6064 to <4 x i32>
  %6073 = shufflevector <4 x i32> %6072, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %6074 = bitcast <4 x i32> %6073 to <2 x i64>
  %6075 = bitcast <2 x i64> %6065 to <4 x i32>
  %6076 = shufflevector <4 x i32> %6075, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %6077 = bitcast <4 x i32> %6076 to <2 x i64>
  %6078 = shufflevector <2 x i64> %6068, <2 x i64> %6071, <2 x i32> <i32 0, i32 2>
  %6079 = shufflevector <2 x i64> %6074, <2 x i64> %6077, <2 x i32> <i32 0, i32 2>
  %6080 = bitcast <2 x i64> %6078 to <4 x i32>
  %6081 = bitcast <2 x i64> %6079 to <4 x i32>
  %6082 = add <4 x i32> %6080, <i32 1, i32 1, i32 1, i32 1>
  %6083 = icmp ugt <4 x i32> %6082, <i32 1, i32 1, i32 1, i32 1>
  %6084 = sext <4 x i1> %6083 to <4 x i32>
  %6085 = bitcast <4 x i32> %6084 to <16 x i8>
  %6086 = icmp slt <16 x i8> %6085, zeroinitializer
  %6087 = bitcast <16 x i1> %6086 to i16
  %6088 = zext i16 %6087 to i32
  %6089 = add <4 x i32> %6081, <i32 1, i32 1, i32 1, i32 1>
  %6090 = icmp ugt <4 x i32> %6089, <i32 1, i32 1, i32 1, i32 1>
  %6091 = sext <4 x i1> %6090 to <4 x i32>
  %6092 = bitcast <4 x i32> %6091 to <16 x i8>
  %6093 = icmp slt <16 x i8> %6092, zeroinitializer
  %6094 = bitcast <16 x i1> %6093 to i16
  %6095 = zext i16 %6094 to i32
  %6096 = sub nsw i32 0, %6088
  %6097 = icmp eq i32 %6095, %6096
  br i1 %6097, label %6098, label %6172

6098:                                             ; preds = %6061
  %6099 = shl <2 x i64> %6003, <i64 1, i64 1>
  %6100 = shl <2 x i64> %6006, <i64 1, i64 1>
  %6101 = shl <2 x i64> %6009, <i64 1, i64 1>
  %6102 = shl <2 x i64> %6012, <i64 1, i64 1>
  %6103 = bitcast <2 x i64> %6099 to <4 x i32>
  %6104 = shufflevector <4 x i32> %6103, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %6105 = bitcast <4 x i32> %6104 to <2 x i64>
  %6106 = bitcast <2 x i64> %6100 to <4 x i32>
  %6107 = shufflevector <4 x i32> %6106, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %6108 = bitcast <4 x i32> %6107 to <2 x i64>
  %6109 = bitcast <2 x i64> %6101 to <4 x i32>
  %6110 = shufflevector <4 x i32> %6109, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %6111 = bitcast <4 x i32> %6110 to <2 x i64>
  %6112 = bitcast <2 x i64> %6102 to <4 x i32>
  %6113 = shufflevector <4 x i32> %6112, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %6114 = bitcast <4 x i32> %6113 to <2 x i64>
  %6115 = shufflevector <2 x i64> %6105, <2 x i64> %6108, <2 x i32> <i32 0, i32 2>
  %6116 = shufflevector <2 x i64> %6111, <2 x i64> %6114, <2 x i32> <i32 0, i32 2>
  %6117 = bitcast <2 x i64> %6115 to <4 x i32>
  %6118 = bitcast <2 x i64> %6116 to <4 x i32>
  %6119 = add <4 x i32> %6117, <i32 1, i32 1, i32 1, i32 1>
  %6120 = icmp ugt <4 x i32> %6119, <i32 1, i32 1, i32 1, i32 1>
  %6121 = sext <4 x i1> %6120 to <4 x i32>
  %6122 = bitcast <4 x i32> %6121 to <16 x i8>
  %6123 = icmp slt <16 x i8> %6122, zeroinitializer
  %6124 = bitcast <16 x i1> %6123 to i16
  %6125 = zext i16 %6124 to i32
  %6126 = add <4 x i32> %6118, <i32 1, i32 1, i32 1, i32 1>
  %6127 = icmp ugt <4 x i32> %6126, <i32 1, i32 1, i32 1, i32 1>
  %6128 = sext <4 x i1> %6127 to <4 x i32>
  %6129 = bitcast <4 x i32> %6128 to <16 x i8>
  %6130 = icmp slt <16 x i8> %6129, zeroinitializer
  %6131 = bitcast <16 x i1> %6130 to i16
  %6132 = zext i16 %6131 to i32
  %6133 = sub nsw i32 0, %6125
  %6134 = icmp eq i32 %6132, %6133
  br i1 %6134, label %6135, label %6172

6135:                                             ; preds = %6098
  %6136 = shl <2 x i64> %6015, <i64 1, i64 1>
  %6137 = shl <2 x i64> %6018, <i64 1, i64 1>
  %6138 = shl <2 x i64> %6021, <i64 1, i64 1>
  %6139 = shl <2 x i64> %6024, <i64 1, i64 1>
  %6140 = bitcast <2 x i64> %6136 to <4 x i32>
  %6141 = shufflevector <4 x i32> %6140, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %6142 = bitcast <4 x i32> %6141 to <2 x i64>
  %6143 = bitcast <2 x i64> %6137 to <4 x i32>
  %6144 = shufflevector <4 x i32> %6143, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %6145 = bitcast <4 x i32> %6144 to <2 x i64>
  %6146 = bitcast <2 x i64> %6138 to <4 x i32>
  %6147 = shufflevector <4 x i32> %6146, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %6148 = bitcast <4 x i32> %6147 to <2 x i64>
  %6149 = bitcast <2 x i64> %6139 to <4 x i32>
  %6150 = shufflevector <4 x i32> %6149, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %6151 = bitcast <4 x i32> %6150 to <2 x i64>
  %6152 = shufflevector <2 x i64> %6142, <2 x i64> %6145, <2 x i32> <i32 0, i32 2>
  %6153 = shufflevector <2 x i64> %6148, <2 x i64> %6151, <2 x i32> <i32 0, i32 2>
  %6154 = bitcast <2 x i64> %6152 to <4 x i32>
  %6155 = bitcast <2 x i64> %6153 to <4 x i32>
  %6156 = add <4 x i32> %6154, <i32 1, i32 1, i32 1, i32 1>
  %6157 = icmp ugt <4 x i32> %6156, <i32 1, i32 1, i32 1, i32 1>
  %6158 = sext <4 x i1> %6157 to <4 x i32>
  %6159 = bitcast <4 x i32> %6158 to <16 x i8>
  %6160 = icmp slt <16 x i8> %6159, zeroinitializer
  %6161 = bitcast <16 x i1> %6160 to i16
  %6162 = zext i16 %6161 to i32
  %6163 = add <4 x i32> %6155, <i32 1, i32 1, i32 1, i32 1>
  %6164 = icmp ugt <4 x i32> %6163, <i32 1, i32 1, i32 1, i32 1>
  %6165 = sext <4 x i1> %6164 to <4 x i32>
  %6166 = bitcast <4 x i32> %6165 to <16 x i8>
  %6167 = icmp slt <16 x i8> %6166, zeroinitializer
  %6168 = bitcast <16 x i1> %6167 to i16
  %6169 = zext i16 %6168 to i32
  %6170 = sub nsw i32 0, %6162
  %6171 = icmp eq i32 %6169, %6170
  br i1 %6171, label %6388, label %6172

6172:                                             ; preds = %6098, %6061, %5832, %6135
  %6173 = bitcast [32 x i64]* %4 to i8*
  %6174 = bitcast [32 x i64]* %5 to i8*
  %6175 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %6176 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %6177 = bitcast [32 x i64]* %5 to <2 x i64>*
  %6178 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %6179 = bitcast i64* %6178 to <2 x i64>*
  %6180 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %6181 = bitcast i64* %6180 to <2 x i64>*
  %6182 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %6183 = bitcast i64* %6182 to <2 x i64>*
  %6184 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %6185 = bitcast i64* %6184 to <2 x i64>*
  %6186 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %6187 = bitcast i64* %6186 to <2 x i64>*
  %6188 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %6189 = bitcast i64* %6188 to <2 x i64>*
  %6190 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %6191 = bitcast i64* %6190 to <2 x i64>*
  %6192 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %6193 = bitcast i64* %6192 to <2 x i64>*
  %6194 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %6195 = bitcast i64* %6194 to <2 x i64>*
  %6196 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %6197 = bitcast i64* %6196 to <2 x i64>*
  %6198 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %6199 = bitcast i64* %6198 to <2 x i64>*
  %6200 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %6201 = bitcast i64* %6200 to <2 x i64>*
  %6202 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %6203 = bitcast i64* %6202 to <2 x i64>*
  %6204 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %6205 = bitcast i64* %6204 to <2 x i64>*
  %6206 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %6207 = bitcast i64* %6206 to <2 x i64>*
  br label %6208

6208:                                             ; preds = %6241, %6172
  %6209 = phi i64 [ 0, %6172 ], [ %6386, %6241 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %6173) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %6173, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %6174) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %6174, i8 -86, i64 256, i1 false) #6
  br label %6210

6210:                                             ; preds = %6210, %6208
  %6211 = phi i64 [ 0, %6208 ], [ %6239, %6210 ]
  %6212 = shl i64 %6211, 5
  %6213 = add nuw nsw i64 %6212, %6209
  %6214 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %6213
  %6215 = load i16, i16* %6214, align 2
  %6216 = sext i16 %6215 to i64
  %6217 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %6211
  store i64 %6216, i64* %6217, align 16
  %6218 = or i64 %6211, 1
  %6219 = shl i64 %6218, 5
  %6220 = add nuw nsw i64 %6219, %6209
  %6221 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %6220
  %6222 = load i16, i16* %6221, align 2
  %6223 = sext i16 %6222 to i64
  %6224 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %6218
  store i64 %6223, i64* %6224, align 8
  %6225 = or i64 %6211, 2
  %6226 = shl i64 %6225, 5
  %6227 = add nuw nsw i64 %6226, %6209
  %6228 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %6227
  %6229 = load i16, i16* %6228, align 2
  %6230 = sext i16 %6229 to i64
  %6231 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %6225
  store i64 %6230, i64* %6231, align 16
  %6232 = or i64 %6211, 3
  %6233 = shl i64 %6232, 5
  %6234 = add nuw nsw i64 %6233, %6209
  %6235 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %6234
  %6236 = load i16, i16* %6235, align 2
  %6237 = sext i16 %6236 to i64
  %6238 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %6232
  store i64 %6237, i64* %6238, align 8
  %6239 = add nuw nsw i64 %6211, 4
  %6240 = icmp eq i64 %6239, 32
  br i1 %6240, label %6241, label %6210

6241:                                             ; preds = %6210
  call void @vpx_fdct32(i64* nonnull %6175, i64* nonnull %6176, i32 0) #6
  %6242 = shl i64 %6209, 5
  %6243 = load <2 x i64>, <2 x i64>* %6177, align 16
  %6244 = add nsw <2 x i64> %6243, <i64 1, i64 1>
  %6245 = lshr <2 x i64> %6243, <i64 63, i64 63>
  %6246 = add nsw <2 x i64> %6244, %6245
  %6247 = lshr <2 x i64> %6246, <i64 2, i64 2>
  %6248 = trunc <2 x i64> %6247 to <2 x i32>
  %6249 = getelementptr inbounds i32, i32* %1, i64 %6242
  %6250 = bitcast i32* %6249 to <2 x i32>*
  store <2 x i32> %6248, <2 x i32>* %6250, align 4
  %6251 = load <2 x i64>, <2 x i64>* %6179, align 16
  %6252 = add nsw <2 x i64> %6251, <i64 1, i64 1>
  %6253 = lshr <2 x i64> %6251, <i64 63, i64 63>
  %6254 = add nsw <2 x i64> %6252, %6253
  %6255 = lshr <2 x i64> %6254, <i64 2, i64 2>
  %6256 = trunc <2 x i64> %6255 to <2 x i32>
  %6257 = or i64 %6242, 2
  %6258 = getelementptr inbounds i32, i32* %1, i64 %6257
  %6259 = bitcast i32* %6258 to <2 x i32>*
  store <2 x i32> %6256, <2 x i32>* %6259, align 4
  %6260 = load <2 x i64>, <2 x i64>* %6181, align 16
  %6261 = add nsw <2 x i64> %6260, <i64 1, i64 1>
  %6262 = lshr <2 x i64> %6260, <i64 63, i64 63>
  %6263 = add nsw <2 x i64> %6261, %6262
  %6264 = lshr <2 x i64> %6263, <i64 2, i64 2>
  %6265 = trunc <2 x i64> %6264 to <2 x i32>
  %6266 = or i64 %6242, 4
  %6267 = getelementptr inbounds i32, i32* %1, i64 %6266
  %6268 = bitcast i32* %6267 to <2 x i32>*
  store <2 x i32> %6265, <2 x i32>* %6268, align 4
  %6269 = load <2 x i64>, <2 x i64>* %6183, align 16
  %6270 = add nsw <2 x i64> %6269, <i64 1, i64 1>
  %6271 = lshr <2 x i64> %6269, <i64 63, i64 63>
  %6272 = add nsw <2 x i64> %6270, %6271
  %6273 = lshr <2 x i64> %6272, <i64 2, i64 2>
  %6274 = trunc <2 x i64> %6273 to <2 x i32>
  %6275 = or i64 %6242, 6
  %6276 = getelementptr inbounds i32, i32* %1, i64 %6275
  %6277 = bitcast i32* %6276 to <2 x i32>*
  store <2 x i32> %6274, <2 x i32>* %6277, align 4
  %6278 = load <2 x i64>, <2 x i64>* %6185, align 16
  %6279 = add nsw <2 x i64> %6278, <i64 1, i64 1>
  %6280 = lshr <2 x i64> %6278, <i64 63, i64 63>
  %6281 = add nsw <2 x i64> %6279, %6280
  %6282 = lshr <2 x i64> %6281, <i64 2, i64 2>
  %6283 = trunc <2 x i64> %6282 to <2 x i32>
  %6284 = or i64 %6242, 8
  %6285 = getelementptr inbounds i32, i32* %1, i64 %6284
  %6286 = bitcast i32* %6285 to <2 x i32>*
  store <2 x i32> %6283, <2 x i32>* %6286, align 4
  %6287 = load <2 x i64>, <2 x i64>* %6187, align 16
  %6288 = add nsw <2 x i64> %6287, <i64 1, i64 1>
  %6289 = lshr <2 x i64> %6287, <i64 63, i64 63>
  %6290 = add nsw <2 x i64> %6288, %6289
  %6291 = lshr <2 x i64> %6290, <i64 2, i64 2>
  %6292 = trunc <2 x i64> %6291 to <2 x i32>
  %6293 = or i64 %6242, 10
  %6294 = getelementptr inbounds i32, i32* %1, i64 %6293
  %6295 = bitcast i32* %6294 to <2 x i32>*
  store <2 x i32> %6292, <2 x i32>* %6295, align 4
  %6296 = load <2 x i64>, <2 x i64>* %6189, align 16
  %6297 = add nsw <2 x i64> %6296, <i64 1, i64 1>
  %6298 = lshr <2 x i64> %6296, <i64 63, i64 63>
  %6299 = add nsw <2 x i64> %6297, %6298
  %6300 = lshr <2 x i64> %6299, <i64 2, i64 2>
  %6301 = trunc <2 x i64> %6300 to <2 x i32>
  %6302 = or i64 %6242, 12
  %6303 = getelementptr inbounds i32, i32* %1, i64 %6302
  %6304 = bitcast i32* %6303 to <2 x i32>*
  store <2 x i32> %6301, <2 x i32>* %6304, align 4
  %6305 = load <2 x i64>, <2 x i64>* %6191, align 16
  %6306 = add nsw <2 x i64> %6305, <i64 1, i64 1>
  %6307 = lshr <2 x i64> %6305, <i64 63, i64 63>
  %6308 = add nsw <2 x i64> %6306, %6307
  %6309 = lshr <2 x i64> %6308, <i64 2, i64 2>
  %6310 = trunc <2 x i64> %6309 to <2 x i32>
  %6311 = or i64 %6242, 14
  %6312 = getelementptr inbounds i32, i32* %1, i64 %6311
  %6313 = bitcast i32* %6312 to <2 x i32>*
  store <2 x i32> %6310, <2 x i32>* %6313, align 4
  %6314 = load <2 x i64>, <2 x i64>* %6193, align 16
  %6315 = add nsw <2 x i64> %6314, <i64 1, i64 1>
  %6316 = lshr <2 x i64> %6314, <i64 63, i64 63>
  %6317 = add nsw <2 x i64> %6315, %6316
  %6318 = lshr <2 x i64> %6317, <i64 2, i64 2>
  %6319 = trunc <2 x i64> %6318 to <2 x i32>
  %6320 = or i64 %6242, 16
  %6321 = getelementptr inbounds i32, i32* %1, i64 %6320
  %6322 = bitcast i32* %6321 to <2 x i32>*
  store <2 x i32> %6319, <2 x i32>* %6322, align 4
  %6323 = load <2 x i64>, <2 x i64>* %6195, align 16
  %6324 = add nsw <2 x i64> %6323, <i64 1, i64 1>
  %6325 = lshr <2 x i64> %6323, <i64 63, i64 63>
  %6326 = add nsw <2 x i64> %6324, %6325
  %6327 = lshr <2 x i64> %6326, <i64 2, i64 2>
  %6328 = trunc <2 x i64> %6327 to <2 x i32>
  %6329 = or i64 %6242, 18
  %6330 = getelementptr inbounds i32, i32* %1, i64 %6329
  %6331 = bitcast i32* %6330 to <2 x i32>*
  store <2 x i32> %6328, <2 x i32>* %6331, align 4
  %6332 = load <2 x i64>, <2 x i64>* %6197, align 16
  %6333 = add nsw <2 x i64> %6332, <i64 1, i64 1>
  %6334 = lshr <2 x i64> %6332, <i64 63, i64 63>
  %6335 = add nsw <2 x i64> %6333, %6334
  %6336 = lshr <2 x i64> %6335, <i64 2, i64 2>
  %6337 = trunc <2 x i64> %6336 to <2 x i32>
  %6338 = or i64 %6242, 20
  %6339 = getelementptr inbounds i32, i32* %1, i64 %6338
  %6340 = bitcast i32* %6339 to <2 x i32>*
  store <2 x i32> %6337, <2 x i32>* %6340, align 4
  %6341 = load <2 x i64>, <2 x i64>* %6199, align 16
  %6342 = add nsw <2 x i64> %6341, <i64 1, i64 1>
  %6343 = lshr <2 x i64> %6341, <i64 63, i64 63>
  %6344 = add nsw <2 x i64> %6342, %6343
  %6345 = lshr <2 x i64> %6344, <i64 2, i64 2>
  %6346 = trunc <2 x i64> %6345 to <2 x i32>
  %6347 = or i64 %6242, 22
  %6348 = getelementptr inbounds i32, i32* %1, i64 %6347
  %6349 = bitcast i32* %6348 to <2 x i32>*
  store <2 x i32> %6346, <2 x i32>* %6349, align 4
  %6350 = load <2 x i64>, <2 x i64>* %6201, align 16
  %6351 = add nsw <2 x i64> %6350, <i64 1, i64 1>
  %6352 = lshr <2 x i64> %6350, <i64 63, i64 63>
  %6353 = add nsw <2 x i64> %6351, %6352
  %6354 = lshr <2 x i64> %6353, <i64 2, i64 2>
  %6355 = trunc <2 x i64> %6354 to <2 x i32>
  %6356 = or i64 %6242, 24
  %6357 = getelementptr inbounds i32, i32* %1, i64 %6356
  %6358 = bitcast i32* %6357 to <2 x i32>*
  store <2 x i32> %6355, <2 x i32>* %6358, align 4
  %6359 = load <2 x i64>, <2 x i64>* %6203, align 16
  %6360 = add nsw <2 x i64> %6359, <i64 1, i64 1>
  %6361 = lshr <2 x i64> %6359, <i64 63, i64 63>
  %6362 = add nsw <2 x i64> %6360, %6361
  %6363 = lshr <2 x i64> %6362, <i64 2, i64 2>
  %6364 = trunc <2 x i64> %6363 to <2 x i32>
  %6365 = or i64 %6242, 26
  %6366 = getelementptr inbounds i32, i32* %1, i64 %6365
  %6367 = bitcast i32* %6366 to <2 x i32>*
  store <2 x i32> %6364, <2 x i32>* %6367, align 4
  %6368 = load <2 x i64>, <2 x i64>* %6205, align 16
  %6369 = add nsw <2 x i64> %6368, <i64 1, i64 1>
  %6370 = lshr <2 x i64> %6368, <i64 63, i64 63>
  %6371 = add nsw <2 x i64> %6369, %6370
  %6372 = lshr <2 x i64> %6371, <i64 2, i64 2>
  %6373 = trunc <2 x i64> %6372 to <2 x i32>
  %6374 = or i64 %6242, 28
  %6375 = getelementptr inbounds i32, i32* %1, i64 %6374
  %6376 = bitcast i32* %6375 to <2 x i32>*
  store <2 x i32> %6373, <2 x i32>* %6376, align 4
  %6377 = load <2 x i64>, <2 x i64>* %6207, align 16
  %6378 = add nsw <2 x i64> %6377, <i64 1, i64 1>
  %6379 = lshr <2 x i64> %6377, <i64 63, i64 63>
  %6380 = add nsw <2 x i64> %6378, %6379
  %6381 = lshr <2 x i64> %6380, <i64 2, i64 2>
  %6382 = trunc <2 x i64> %6381 to <2 x i32>
  %6383 = or i64 %6242, 30
  %6384 = getelementptr inbounds i32, i32* %1, i64 %6383
  %6385 = bitcast i32* %6384 to <2 x i32>*
  store <2 x i32> %6382, <2 x i32>* %6385, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %6174) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %6173) #6
  %6386 = add nuw nsw i64 %6209, 1
  %6387 = icmp eq i64 %6386, 32
  br i1 %6387, label %11292, label %6208

6388:                                             ; preds = %6135
  %6389 = bitcast <2 x i64> %5965 to <4 x i32>
  %6390 = shufflevector <4 x i32> %6389, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %6391 = bitcast <4 x i32> %6390 to <2 x i64>
  %6392 = bitcast <2 x i64> %5970 to <4 x i32>
  %6393 = shufflevector <4 x i32> %6392, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %6394 = bitcast <4 x i32> %6393 to <2 x i64>
  %6395 = shufflevector <2 x i64> %6391, <2 x i64> %6394, <2 x i32> <i32 0, i32 2>
  %6396 = bitcast <2 x i64> %5975 to <4 x i32>
  %6397 = shufflevector <4 x i32> %6396, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %6398 = bitcast <4 x i32> %6397 to <2 x i64>
  %6399 = bitcast <2 x i64> %5980 to <4 x i32>
  %6400 = shufflevector <4 x i32> %6399, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %6401 = bitcast <4 x i32> %6400 to <2 x i64>
  %6402 = shufflevector <2 x i64> %6398, <2 x i64> %6401, <2 x i32> <i32 0, i32 2>
  %6403 = bitcast <2 x i64> %5985 to <4 x i32>
  %6404 = shufflevector <4 x i32> %6403, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %6405 = bitcast <4 x i32> %6404 to <2 x i64>
  %6406 = bitcast <2 x i64> %5990 to <4 x i32>
  %6407 = shufflevector <4 x i32> %6406, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %6408 = bitcast <4 x i32> %6407 to <2 x i64>
  %6409 = shufflevector <2 x i64> %6405, <2 x i64> %6408, <2 x i32> <i32 0, i32 2>
  %6410 = bitcast <2 x i64> %5995 to <4 x i32>
  %6411 = shufflevector <4 x i32> %6410, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %6412 = bitcast <4 x i32> %6411 to <2 x i64>
  %6413 = bitcast <2 x i64> %6000 to <4 x i32>
  %6414 = shufflevector <4 x i32> %6413, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %6415 = bitcast <4 x i32> %6414 to <2 x i64>
  %6416 = shufflevector <2 x i64> %6412, <2 x i64> %6415, <2 x i32> <i32 0, i32 2>
  %6417 = bitcast <2 x i64> %6003 to <4 x i32>
  %6418 = shufflevector <4 x i32> %6417, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %6419 = bitcast <4 x i32> %6418 to <2 x i64>
  %6420 = bitcast <2 x i64> %6006 to <4 x i32>
  %6421 = shufflevector <4 x i32> %6420, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %6422 = bitcast <4 x i32> %6421 to <2 x i64>
  %6423 = shufflevector <2 x i64> %6419, <2 x i64> %6422, <2 x i32> <i32 0, i32 2>
  %6424 = bitcast <2 x i64> %6009 to <4 x i32>
  %6425 = shufflevector <4 x i32> %6424, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %6426 = bitcast <4 x i32> %6425 to <2 x i64>
  %6427 = bitcast <2 x i64> %6012 to <4 x i32>
  %6428 = shufflevector <4 x i32> %6427, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %6429 = bitcast <4 x i32> %6428 to <2 x i64>
  %6430 = shufflevector <2 x i64> %6426, <2 x i64> %6429, <2 x i32> <i32 0, i32 2>
  %6431 = bitcast <2 x i64> %6015 to <4 x i32>
  %6432 = shufflevector <4 x i32> %6431, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %6433 = bitcast <4 x i32> %6432 to <2 x i64>
  %6434 = bitcast <2 x i64> %6018 to <4 x i32>
  %6435 = shufflevector <4 x i32> %6434, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %6436 = bitcast <4 x i32> %6435 to <2 x i64>
  %6437 = shufflevector <2 x i64> %6433, <2 x i64> %6436, <2 x i32> <i32 0, i32 2>
  %6438 = bitcast <2 x i64> %6021 to <4 x i32>
  %6439 = shufflevector <4 x i32> %6438, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %6440 = bitcast <4 x i32> %6439 to <2 x i64>
  %6441 = bitcast <2 x i64> %6024 to <4 x i32>
  %6442 = shufflevector <4 x i32> %6441, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %6443 = bitcast <4 x i32> %6442 to <2 x i64>
  %6444 = shufflevector <2 x i64> %6440, <2 x i64> %6443, <2 x i32> <i32 0, i32 2>
  %6445 = bitcast <2 x i64> %6395 to <4 x i32>
  %6446 = add <4 x i32> %6445, <i32 8192, i32 8192, i32 8192, i32 8192>
  %6447 = bitcast <2 x i64> %6402 to <4 x i32>
  %6448 = add <4 x i32> %6447, <i32 8192, i32 8192, i32 8192, i32 8192>
  %6449 = bitcast <2 x i64> %6409 to <4 x i32>
  %6450 = add <4 x i32> %6449, <i32 8192, i32 8192, i32 8192, i32 8192>
  %6451 = bitcast <2 x i64> %6416 to <4 x i32>
  %6452 = add <4 x i32> %6451, <i32 8192, i32 8192, i32 8192, i32 8192>
  %6453 = bitcast <2 x i64> %6423 to <4 x i32>
  %6454 = add <4 x i32> %6453, <i32 8192, i32 8192, i32 8192, i32 8192>
  %6455 = bitcast <2 x i64> %6430 to <4 x i32>
  %6456 = add <4 x i32> %6455, <i32 8192, i32 8192, i32 8192, i32 8192>
  %6457 = bitcast <2 x i64> %6437 to <4 x i32>
  %6458 = add <4 x i32> %6457, <i32 8192, i32 8192, i32 8192, i32 8192>
  %6459 = bitcast <2 x i64> %6444 to <4 x i32>
  %6460 = add <4 x i32> %6459, <i32 8192, i32 8192, i32 8192, i32 8192>
  %6461 = ashr <4 x i32> %6446, <i32 14, i32 14, i32 14, i32 14>
  %6462 = ashr <4 x i32> %6448, <i32 14, i32 14, i32 14, i32 14>
  %6463 = ashr <4 x i32> %6450, <i32 14, i32 14, i32 14, i32 14>
  %6464 = ashr <4 x i32> %6452, <i32 14, i32 14, i32 14, i32 14>
  %6465 = ashr <4 x i32> %6454, <i32 14, i32 14, i32 14, i32 14>
  %6466 = ashr <4 x i32> %6456, <i32 14, i32 14, i32 14, i32 14>
  %6467 = ashr <4 x i32> %6458, <i32 14, i32 14, i32 14, i32 14>
  %6468 = ashr <4 x i32> %6460, <i32 14, i32 14, i32 14, i32 14>
  %6469 = lshr <4 x i32> %6446, <i32 31, i32 31, i32 31, i32 31>
  %6470 = lshr <4 x i32> %6448, <i32 31, i32 31, i32 31, i32 31>
  %6471 = lshr <4 x i32> %6450, <i32 31, i32 31, i32 31, i32 31>
  %6472 = lshr <4 x i32> %6452, <i32 31, i32 31, i32 31, i32 31>
  %6473 = lshr <4 x i32> %6454, <i32 31, i32 31, i32 31, i32 31>
  %6474 = lshr <4 x i32> %6456, <i32 31, i32 31, i32 31, i32 31>
  %6475 = lshr <4 x i32> %6458, <i32 31, i32 31, i32 31, i32 31>
  %6476 = lshr <4 x i32> %6460, <i32 31, i32 31, i32 31, i32 31>
  %6477 = add nuw nsw <4 x i32> %6469, <i32 1, i32 1, i32 1, i32 1>
  %6478 = add nsw <4 x i32> %6477, %6461
  %6479 = add nuw nsw <4 x i32> %6470, <i32 1, i32 1, i32 1, i32 1>
  %6480 = add nsw <4 x i32> %6479, %6462
  %6481 = add nuw nsw <4 x i32> %6471, <i32 1, i32 1, i32 1, i32 1>
  %6482 = add nsw <4 x i32> %6481, %6463
  %6483 = add nuw nsw <4 x i32> %6472, <i32 1, i32 1, i32 1, i32 1>
  %6484 = add nsw <4 x i32> %6483, %6464
  %6485 = add nuw nsw <4 x i32> %6473, <i32 1, i32 1, i32 1, i32 1>
  %6486 = add nsw <4 x i32> %6485, %6465
  %6487 = add nuw nsw <4 x i32> %6474, <i32 1, i32 1, i32 1, i32 1>
  %6488 = add nsw <4 x i32> %6487, %6466
  %6489 = add nuw nsw <4 x i32> %6475, <i32 1, i32 1, i32 1, i32 1>
  %6490 = add nsw <4 x i32> %6489, %6467
  %6491 = add nuw nsw <4 x i32> %6476, <i32 1, i32 1, i32 1, i32 1>
  %6492 = add nsw <4 x i32> %6491, %6468
  %6493 = ashr <4 x i32> %6478, <i32 2, i32 2, i32 2, i32 2>
  %6494 = ashr <4 x i32> %6480, <i32 2, i32 2, i32 2, i32 2>
  %6495 = ashr <4 x i32> %6482, <i32 2, i32 2, i32 2, i32 2>
  %6496 = ashr <4 x i32> %6484, <i32 2, i32 2, i32 2, i32 2>
  %6497 = ashr <4 x i32> %6486, <i32 2, i32 2, i32 2, i32 2>
  %6498 = ashr <4 x i32> %6488, <i32 2, i32 2, i32 2, i32 2>
  %6499 = ashr <4 x i32> %6490, <i32 2, i32 2, i32 2, i32 2>
  %6500 = ashr <4 x i32> %6492, <i32 2, i32 2, i32 2, i32 2>
  %6501 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %6493, <4 x i32> %6494) #6
  store <8 x i16> %6501, <8 x i16>* %40, align 16
  %6502 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %6495, <4 x i32> %6496) #6
  store <8 x i16> %6502, <8 x i16>* %42, align 16
  %6503 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %6497, <4 x i32> %6498) #6
  store <8 x i16> %6503, <8 x i16>* %44, align 16
  %6504 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %6499, <4 x i32> %6500) #6
  store <8 x i16> %6504, <8 x i16>* %46, align 16
  %6505 = add <8 x i16> %6501, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %6506 = icmp ult <8 x i16> %6505, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %6507 = add <8 x i16> %6502, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %6508 = icmp ult <8 x i16> %6507, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %6509 = add <8 x i16> %6503, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %6510 = icmp ult <8 x i16> %6509, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %6511 = add <8 x i16> %6504, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %6512 = icmp ult <8 x i16> %6511, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %6513 = or <8 x i1> %6508, %6506
  %6514 = or <8 x i1> %6513, %6510
  %6515 = or <8 x i1> %6514, %6512
  %6516 = sext <8 x i1> %6515 to <8 x i16>
  %6517 = bitcast <8 x i16> %6516 to <16 x i8>
  %6518 = icmp slt <16 x i8> %6517, zeroinitializer
  %6519 = bitcast <16 x i1> %6518 to i16
  %6520 = icmp eq i16 %6519, 0
  br i1 %6520, label %6737, label %6521

6521:                                             ; preds = %6388
  %6522 = bitcast [32 x i64]* %4 to i8*
  %6523 = bitcast [32 x i64]* %5 to i8*
  %6524 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %6525 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %6526 = bitcast [32 x i64]* %5 to <2 x i64>*
  %6527 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %6528 = bitcast i64* %6527 to <2 x i64>*
  %6529 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %6530 = bitcast i64* %6529 to <2 x i64>*
  %6531 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %6532 = bitcast i64* %6531 to <2 x i64>*
  %6533 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %6534 = bitcast i64* %6533 to <2 x i64>*
  %6535 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %6536 = bitcast i64* %6535 to <2 x i64>*
  %6537 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %6538 = bitcast i64* %6537 to <2 x i64>*
  %6539 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %6540 = bitcast i64* %6539 to <2 x i64>*
  %6541 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %6542 = bitcast i64* %6541 to <2 x i64>*
  %6543 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %6544 = bitcast i64* %6543 to <2 x i64>*
  %6545 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %6546 = bitcast i64* %6545 to <2 x i64>*
  %6547 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %6548 = bitcast i64* %6547 to <2 x i64>*
  %6549 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %6550 = bitcast i64* %6549 to <2 x i64>*
  %6551 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %6552 = bitcast i64* %6551 to <2 x i64>*
  %6553 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %6554 = bitcast i64* %6553 to <2 x i64>*
  %6555 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %6556 = bitcast i64* %6555 to <2 x i64>*
  br label %6557

6557:                                             ; preds = %6590, %6521
  %6558 = phi i64 [ 0, %6521 ], [ %6735, %6590 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %6522) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %6522, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %6523) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %6523, i8 -86, i64 256, i1 false) #6
  br label %6559

6559:                                             ; preds = %6559, %6557
  %6560 = phi i64 [ 0, %6557 ], [ %6588, %6559 ]
  %6561 = shl i64 %6560, 5
  %6562 = add nuw nsw i64 %6561, %6558
  %6563 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %6562
  %6564 = load i16, i16* %6563, align 2
  %6565 = sext i16 %6564 to i64
  %6566 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %6560
  store i64 %6565, i64* %6566, align 16
  %6567 = or i64 %6560, 1
  %6568 = shl i64 %6567, 5
  %6569 = add nuw nsw i64 %6568, %6558
  %6570 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %6569
  %6571 = load i16, i16* %6570, align 2
  %6572 = sext i16 %6571 to i64
  %6573 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %6567
  store i64 %6572, i64* %6573, align 8
  %6574 = or i64 %6560, 2
  %6575 = shl i64 %6574, 5
  %6576 = add nuw nsw i64 %6575, %6558
  %6577 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %6576
  %6578 = load i16, i16* %6577, align 2
  %6579 = sext i16 %6578 to i64
  %6580 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %6574
  store i64 %6579, i64* %6580, align 16
  %6581 = or i64 %6560, 3
  %6582 = shl i64 %6581, 5
  %6583 = add nuw nsw i64 %6582, %6558
  %6584 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %6583
  %6585 = load i16, i16* %6584, align 2
  %6586 = sext i16 %6585 to i64
  %6587 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %6581
  store i64 %6586, i64* %6587, align 8
  %6588 = add nuw nsw i64 %6560, 4
  %6589 = icmp eq i64 %6588, 32
  br i1 %6589, label %6590, label %6559

6590:                                             ; preds = %6559
  call void @vpx_fdct32(i64* nonnull %6524, i64* nonnull %6525, i32 0) #6
  %6591 = shl i64 %6558, 5
  %6592 = load <2 x i64>, <2 x i64>* %6526, align 16
  %6593 = add nsw <2 x i64> %6592, <i64 1, i64 1>
  %6594 = lshr <2 x i64> %6592, <i64 63, i64 63>
  %6595 = add nsw <2 x i64> %6593, %6594
  %6596 = lshr <2 x i64> %6595, <i64 2, i64 2>
  %6597 = trunc <2 x i64> %6596 to <2 x i32>
  %6598 = getelementptr inbounds i32, i32* %1, i64 %6591
  %6599 = bitcast i32* %6598 to <2 x i32>*
  store <2 x i32> %6597, <2 x i32>* %6599, align 4
  %6600 = load <2 x i64>, <2 x i64>* %6528, align 16
  %6601 = add nsw <2 x i64> %6600, <i64 1, i64 1>
  %6602 = lshr <2 x i64> %6600, <i64 63, i64 63>
  %6603 = add nsw <2 x i64> %6601, %6602
  %6604 = lshr <2 x i64> %6603, <i64 2, i64 2>
  %6605 = trunc <2 x i64> %6604 to <2 x i32>
  %6606 = or i64 %6591, 2
  %6607 = getelementptr inbounds i32, i32* %1, i64 %6606
  %6608 = bitcast i32* %6607 to <2 x i32>*
  store <2 x i32> %6605, <2 x i32>* %6608, align 4
  %6609 = load <2 x i64>, <2 x i64>* %6530, align 16
  %6610 = add nsw <2 x i64> %6609, <i64 1, i64 1>
  %6611 = lshr <2 x i64> %6609, <i64 63, i64 63>
  %6612 = add nsw <2 x i64> %6610, %6611
  %6613 = lshr <2 x i64> %6612, <i64 2, i64 2>
  %6614 = trunc <2 x i64> %6613 to <2 x i32>
  %6615 = or i64 %6591, 4
  %6616 = getelementptr inbounds i32, i32* %1, i64 %6615
  %6617 = bitcast i32* %6616 to <2 x i32>*
  store <2 x i32> %6614, <2 x i32>* %6617, align 4
  %6618 = load <2 x i64>, <2 x i64>* %6532, align 16
  %6619 = add nsw <2 x i64> %6618, <i64 1, i64 1>
  %6620 = lshr <2 x i64> %6618, <i64 63, i64 63>
  %6621 = add nsw <2 x i64> %6619, %6620
  %6622 = lshr <2 x i64> %6621, <i64 2, i64 2>
  %6623 = trunc <2 x i64> %6622 to <2 x i32>
  %6624 = or i64 %6591, 6
  %6625 = getelementptr inbounds i32, i32* %1, i64 %6624
  %6626 = bitcast i32* %6625 to <2 x i32>*
  store <2 x i32> %6623, <2 x i32>* %6626, align 4
  %6627 = load <2 x i64>, <2 x i64>* %6534, align 16
  %6628 = add nsw <2 x i64> %6627, <i64 1, i64 1>
  %6629 = lshr <2 x i64> %6627, <i64 63, i64 63>
  %6630 = add nsw <2 x i64> %6628, %6629
  %6631 = lshr <2 x i64> %6630, <i64 2, i64 2>
  %6632 = trunc <2 x i64> %6631 to <2 x i32>
  %6633 = or i64 %6591, 8
  %6634 = getelementptr inbounds i32, i32* %1, i64 %6633
  %6635 = bitcast i32* %6634 to <2 x i32>*
  store <2 x i32> %6632, <2 x i32>* %6635, align 4
  %6636 = load <2 x i64>, <2 x i64>* %6536, align 16
  %6637 = add nsw <2 x i64> %6636, <i64 1, i64 1>
  %6638 = lshr <2 x i64> %6636, <i64 63, i64 63>
  %6639 = add nsw <2 x i64> %6637, %6638
  %6640 = lshr <2 x i64> %6639, <i64 2, i64 2>
  %6641 = trunc <2 x i64> %6640 to <2 x i32>
  %6642 = or i64 %6591, 10
  %6643 = getelementptr inbounds i32, i32* %1, i64 %6642
  %6644 = bitcast i32* %6643 to <2 x i32>*
  store <2 x i32> %6641, <2 x i32>* %6644, align 4
  %6645 = load <2 x i64>, <2 x i64>* %6538, align 16
  %6646 = add nsw <2 x i64> %6645, <i64 1, i64 1>
  %6647 = lshr <2 x i64> %6645, <i64 63, i64 63>
  %6648 = add nsw <2 x i64> %6646, %6647
  %6649 = lshr <2 x i64> %6648, <i64 2, i64 2>
  %6650 = trunc <2 x i64> %6649 to <2 x i32>
  %6651 = or i64 %6591, 12
  %6652 = getelementptr inbounds i32, i32* %1, i64 %6651
  %6653 = bitcast i32* %6652 to <2 x i32>*
  store <2 x i32> %6650, <2 x i32>* %6653, align 4
  %6654 = load <2 x i64>, <2 x i64>* %6540, align 16
  %6655 = add nsw <2 x i64> %6654, <i64 1, i64 1>
  %6656 = lshr <2 x i64> %6654, <i64 63, i64 63>
  %6657 = add nsw <2 x i64> %6655, %6656
  %6658 = lshr <2 x i64> %6657, <i64 2, i64 2>
  %6659 = trunc <2 x i64> %6658 to <2 x i32>
  %6660 = or i64 %6591, 14
  %6661 = getelementptr inbounds i32, i32* %1, i64 %6660
  %6662 = bitcast i32* %6661 to <2 x i32>*
  store <2 x i32> %6659, <2 x i32>* %6662, align 4
  %6663 = load <2 x i64>, <2 x i64>* %6542, align 16
  %6664 = add nsw <2 x i64> %6663, <i64 1, i64 1>
  %6665 = lshr <2 x i64> %6663, <i64 63, i64 63>
  %6666 = add nsw <2 x i64> %6664, %6665
  %6667 = lshr <2 x i64> %6666, <i64 2, i64 2>
  %6668 = trunc <2 x i64> %6667 to <2 x i32>
  %6669 = or i64 %6591, 16
  %6670 = getelementptr inbounds i32, i32* %1, i64 %6669
  %6671 = bitcast i32* %6670 to <2 x i32>*
  store <2 x i32> %6668, <2 x i32>* %6671, align 4
  %6672 = load <2 x i64>, <2 x i64>* %6544, align 16
  %6673 = add nsw <2 x i64> %6672, <i64 1, i64 1>
  %6674 = lshr <2 x i64> %6672, <i64 63, i64 63>
  %6675 = add nsw <2 x i64> %6673, %6674
  %6676 = lshr <2 x i64> %6675, <i64 2, i64 2>
  %6677 = trunc <2 x i64> %6676 to <2 x i32>
  %6678 = or i64 %6591, 18
  %6679 = getelementptr inbounds i32, i32* %1, i64 %6678
  %6680 = bitcast i32* %6679 to <2 x i32>*
  store <2 x i32> %6677, <2 x i32>* %6680, align 4
  %6681 = load <2 x i64>, <2 x i64>* %6546, align 16
  %6682 = add nsw <2 x i64> %6681, <i64 1, i64 1>
  %6683 = lshr <2 x i64> %6681, <i64 63, i64 63>
  %6684 = add nsw <2 x i64> %6682, %6683
  %6685 = lshr <2 x i64> %6684, <i64 2, i64 2>
  %6686 = trunc <2 x i64> %6685 to <2 x i32>
  %6687 = or i64 %6591, 20
  %6688 = getelementptr inbounds i32, i32* %1, i64 %6687
  %6689 = bitcast i32* %6688 to <2 x i32>*
  store <2 x i32> %6686, <2 x i32>* %6689, align 4
  %6690 = load <2 x i64>, <2 x i64>* %6548, align 16
  %6691 = add nsw <2 x i64> %6690, <i64 1, i64 1>
  %6692 = lshr <2 x i64> %6690, <i64 63, i64 63>
  %6693 = add nsw <2 x i64> %6691, %6692
  %6694 = lshr <2 x i64> %6693, <i64 2, i64 2>
  %6695 = trunc <2 x i64> %6694 to <2 x i32>
  %6696 = or i64 %6591, 22
  %6697 = getelementptr inbounds i32, i32* %1, i64 %6696
  %6698 = bitcast i32* %6697 to <2 x i32>*
  store <2 x i32> %6695, <2 x i32>* %6698, align 4
  %6699 = load <2 x i64>, <2 x i64>* %6550, align 16
  %6700 = add nsw <2 x i64> %6699, <i64 1, i64 1>
  %6701 = lshr <2 x i64> %6699, <i64 63, i64 63>
  %6702 = add nsw <2 x i64> %6700, %6701
  %6703 = lshr <2 x i64> %6702, <i64 2, i64 2>
  %6704 = trunc <2 x i64> %6703 to <2 x i32>
  %6705 = or i64 %6591, 24
  %6706 = getelementptr inbounds i32, i32* %1, i64 %6705
  %6707 = bitcast i32* %6706 to <2 x i32>*
  store <2 x i32> %6704, <2 x i32>* %6707, align 4
  %6708 = load <2 x i64>, <2 x i64>* %6552, align 16
  %6709 = add nsw <2 x i64> %6708, <i64 1, i64 1>
  %6710 = lshr <2 x i64> %6708, <i64 63, i64 63>
  %6711 = add nsw <2 x i64> %6709, %6710
  %6712 = lshr <2 x i64> %6711, <i64 2, i64 2>
  %6713 = trunc <2 x i64> %6712 to <2 x i32>
  %6714 = or i64 %6591, 26
  %6715 = getelementptr inbounds i32, i32* %1, i64 %6714
  %6716 = bitcast i32* %6715 to <2 x i32>*
  store <2 x i32> %6713, <2 x i32>* %6716, align 4
  %6717 = load <2 x i64>, <2 x i64>* %6554, align 16
  %6718 = add nsw <2 x i64> %6717, <i64 1, i64 1>
  %6719 = lshr <2 x i64> %6717, <i64 63, i64 63>
  %6720 = add nsw <2 x i64> %6718, %6719
  %6721 = lshr <2 x i64> %6720, <i64 2, i64 2>
  %6722 = trunc <2 x i64> %6721 to <2 x i32>
  %6723 = or i64 %6591, 28
  %6724 = getelementptr inbounds i32, i32* %1, i64 %6723
  %6725 = bitcast i32* %6724 to <2 x i32>*
  store <2 x i32> %6722, <2 x i32>* %6725, align 4
  %6726 = load <2 x i64>, <2 x i64>* %6556, align 16
  %6727 = add nsw <2 x i64> %6726, <i64 1, i64 1>
  %6728 = lshr <2 x i64> %6726, <i64 63, i64 63>
  %6729 = add nsw <2 x i64> %6727, %6728
  %6730 = lshr <2 x i64> %6729, <i64 2, i64 2>
  %6731 = trunc <2 x i64> %6730 to <2 x i32>
  %6732 = or i64 %6591, 30
  %6733 = getelementptr inbounds i32, i32* %1, i64 %6732
  %6734 = bitcast i32* %6733 to <2 x i32>*
  store <2 x i32> %6731, <2 x i32>* %6734, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %6523) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %6522) #6
  %6735 = add nuw nsw i64 %6558, 1
  %6736 = icmp eq i64 %6735, 32
  br i1 %6736, label %11292, label %6557

6737:                                             ; preds = %6388
  %6738 = add <4 x i32> %5905, %3371
  %6739 = add <4 x i32> %5906, %3373
  %6740 = sub <4 x i32> %3371, %5905
  %6741 = sub <4 x i32> %3373, %5906
  %6742 = sub <4 x i32> %3380, %5907
  %6743 = sub <4 x i32> %3381, %5908
  %6744 = add <4 x i32> %5907, %3380
  %6745 = add <4 x i32> %5908, %3381
  %6746 = add <4 x i32> %5909, %3383
  %6747 = add <4 x i32> %5910, %3385
  %6748 = sub <4 x i32> %3383, %5909
  %6749 = sub <4 x i32> %3385, %5910
  %6750 = sub <4 x i32> %3392, %5911
  %6751 = sub <4 x i32> %3393, %5912
  %6752 = add <4 x i32> %5911, %3392
  %6753 = add <4 x i32> %5912, %3393
  %6754 = shufflevector <4 x i32> %5915, <4 x i32> %5941, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %6755 = bitcast <4 x i32> %6754 to <2 x i64>
  %6756 = shufflevector <4 x i32> %5915, <4 x i32> %5941, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %6757 = bitcast <4 x i32> %6756 to <2 x i64>
  %6758 = shufflevector <4 x i32> %5916, <4 x i32> %5942, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %6759 = bitcast <4 x i32> %6758 to <2 x i64>
  %6760 = shufflevector <4 x i32> %5916, <4 x i32> %5942, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %6761 = bitcast <4 x i32> %6760 to <2 x i64>
  %6762 = shufflevector <4 x i32> %5917, <4 x i32> %5939, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %6763 = bitcast <4 x i32> %6762 to <2 x i64>
  %6764 = shufflevector <4 x i32> %5917, <4 x i32> %5939, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %6765 = bitcast <4 x i32> %6764 to <2 x i64>
  %6766 = shufflevector <4 x i32> %5918, <4 x i32> %5940, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %6767 = bitcast <4 x i32> %6766 to <2 x i64>
  %6768 = shufflevector <4 x i32> %5918, <4 x i32> %5940, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %6769 = bitcast <4 x i32> %6768 to <2 x i64>
  %6770 = shufflevector <4 x i32> %5923, <4 x i32> %5933, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %6771 = bitcast <4 x i32> %6770 to <2 x i64>
  %6772 = shufflevector <4 x i32> %5923, <4 x i32> %5933, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %6773 = bitcast <4 x i32> %6772 to <2 x i64>
  %6774 = shufflevector <4 x i32> %5924, <4 x i32> %5934, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %6775 = bitcast <4 x i32> %6774 to <2 x i64>
  %6776 = shufflevector <4 x i32> %5924, <4 x i32> %5934, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %6777 = bitcast <4 x i32> %6776 to <2 x i64>
  %6778 = shufflevector <4 x i32> %5925, <4 x i32> %5931, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %6779 = bitcast <4 x i32> %6778 to <2 x i64>
  %6780 = shufflevector <4 x i32> %5925, <4 x i32> %5931, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %6781 = bitcast <4 x i32> %6780 to <2 x i64>
  %6782 = shufflevector <4 x i32> %5926, <4 x i32> %5932, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %6783 = bitcast <4 x i32> %6782 to <2 x i64>
  %6784 = shufflevector <4 x i32> %5926, <4 x i32> %5932, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %6785 = bitcast <4 x i32> %6784 to <2 x i64>
  %6786 = and <2 x i64> %6755, <i64 4294967295, i64 4294967295>
  %6787 = mul nuw <2 x i64> %6786, <i64 4294951227, i64 4294951227>
  %6788 = lshr <2 x i64> %6755, <i64 32, i64 32>
  %6789 = mul nuw nsw <2 x i64> %6788, <i64 3196, i64 3196>
  %6790 = add <2 x i64> %6789, %6787
  %6791 = and <2 x i64> %6757, <i64 4294967295, i64 4294967295>
  %6792 = mul nuw <2 x i64> %6791, <i64 4294951227, i64 4294951227>
  %6793 = lshr <2 x i64> %6757, <i64 32, i64 32>
  %6794 = mul nuw nsw <2 x i64> %6793, <i64 3196, i64 3196>
  %6795 = add <2 x i64> %6794, %6792
  %6796 = and <2 x i64> %6759, <i64 4294967295, i64 4294967295>
  %6797 = mul nuw <2 x i64> %6796, <i64 4294951227, i64 4294951227>
  %6798 = lshr <2 x i64> %6759, <i64 32, i64 32>
  %6799 = mul nuw nsw <2 x i64> %6798, <i64 3196, i64 3196>
  %6800 = add <2 x i64> %6799, %6797
  %6801 = and <2 x i64> %6761, <i64 4294967295, i64 4294967295>
  %6802 = mul nuw <2 x i64> %6801, <i64 4294951227, i64 4294951227>
  %6803 = lshr <2 x i64> %6761, <i64 32, i64 32>
  %6804 = mul nuw nsw <2 x i64> %6803, <i64 3196, i64 3196>
  %6805 = add <2 x i64> %6804, %6802
  %6806 = and <2 x i64> %6763, <i64 4294967295, i64 4294967295>
  %6807 = mul nuw <2 x i64> %6806, <i64 4294964100, i64 4294964100>
  %6808 = lshr <2 x i64> %6763, <i64 32, i64 32>
  %6809 = mul nuw <2 x i64> %6808, <i64 4294951227, i64 4294951227>
  %6810 = add <2 x i64> %6809, %6807
  %6811 = and <2 x i64> %6765, <i64 4294967295, i64 4294967295>
  %6812 = mul nuw <2 x i64> %6811, <i64 4294964100, i64 4294964100>
  %6813 = lshr <2 x i64> %6765, <i64 32, i64 32>
  %6814 = mul nuw <2 x i64> %6813, <i64 4294951227, i64 4294951227>
  %6815 = add <2 x i64> %6814, %6812
  %6816 = and <2 x i64> %6767, <i64 4294967295, i64 4294967295>
  %6817 = mul nuw <2 x i64> %6816, <i64 4294964100, i64 4294964100>
  %6818 = lshr <2 x i64> %6767, <i64 32, i64 32>
  %6819 = mul nuw <2 x i64> %6818, <i64 4294951227, i64 4294951227>
  %6820 = add <2 x i64> %6819, %6817
  %6821 = and <2 x i64> %6769, <i64 4294967295, i64 4294967295>
  %6822 = mul nuw <2 x i64> %6821, <i64 4294964100, i64 4294964100>
  %6823 = lshr <2 x i64> %6769, <i64 32, i64 32>
  %6824 = mul nuw <2 x i64> %6823, <i64 4294951227, i64 4294951227>
  %6825 = add <2 x i64> %6824, %6822
  %6826 = and <2 x i64> %6771, <i64 4294967295, i64 4294967295>
  %6827 = mul nuw <2 x i64> %6826, <i64 4294958194, i64 4294958194>
  %6828 = lshr <2 x i64> %6771, <i64 32, i64 32>
  %6829 = mul nuw nsw <2 x i64> %6828, <i64 13623, i64 13623>
  %6830 = add <2 x i64> %6829, %6827
  %6831 = and <2 x i64> %6773, <i64 4294967295, i64 4294967295>
  %6832 = mul nuw <2 x i64> %6831, <i64 4294958194, i64 4294958194>
  %6833 = lshr <2 x i64> %6773, <i64 32, i64 32>
  %6834 = mul nuw nsw <2 x i64> %6833, <i64 13623, i64 13623>
  %6835 = add <2 x i64> %6834, %6832
  %6836 = and <2 x i64> %6775, <i64 4294967295, i64 4294967295>
  %6837 = mul nuw <2 x i64> %6836, <i64 4294958194, i64 4294958194>
  %6838 = lshr <2 x i64> %6775, <i64 32, i64 32>
  %6839 = mul nuw nsw <2 x i64> %6838, <i64 13623, i64 13623>
  %6840 = add <2 x i64> %6839, %6837
  %6841 = and <2 x i64> %6777, <i64 4294967295, i64 4294967295>
  %6842 = mul nuw <2 x i64> %6841, <i64 4294958194, i64 4294958194>
  %6843 = lshr <2 x i64> %6777, <i64 32, i64 32>
  %6844 = mul nuw nsw <2 x i64> %6843, <i64 13623, i64 13623>
  %6845 = add <2 x i64> %6844, %6842
  %6846 = and <2 x i64> %6779, <i64 4294967295, i64 4294967295>
  %6847 = mul nuw <2 x i64> %6846, <i64 4294953673, i64 4294953673>
  %6848 = lshr <2 x i64> %6779, <i64 32, i64 32>
  %6849 = mul nuw <2 x i64> %6848, <i64 4294958194, i64 4294958194>
  %6850 = add <2 x i64> %6849, %6847
  %6851 = and <2 x i64> %6781, <i64 4294967295, i64 4294967295>
  %6852 = mul nuw <2 x i64> %6851, <i64 4294953673, i64 4294953673>
  %6853 = lshr <2 x i64> %6781, <i64 32, i64 32>
  %6854 = mul nuw <2 x i64> %6853, <i64 4294958194, i64 4294958194>
  %6855 = add <2 x i64> %6854, %6852
  %6856 = and <2 x i64> %6783, <i64 4294967295, i64 4294967295>
  %6857 = mul nuw <2 x i64> %6856, <i64 4294953673, i64 4294953673>
  %6858 = lshr <2 x i64> %6783, <i64 32, i64 32>
  %6859 = mul nuw <2 x i64> %6858, <i64 4294958194, i64 4294958194>
  %6860 = add <2 x i64> %6859, %6857
  %6861 = and <2 x i64> %6785, <i64 4294967295, i64 4294967295>
  %6862 = mul nuw <2 x i64> %6861, <i64 4294953673, i64 4294953673>
  %6863 = lshr <2 x i64> %6785, <i64 32, i64 32>
  %6864 = mul nuw <2 x i64> %6863, <i64 4294958194, i64 4294958194>
  %6865 = add <2 x i64> %6864, %6862
  %6866 = mul nuw <2 x i64> %6846, <i64 4294958194, i64 4294958194>
  %6867 = mul nuw nsw <2 x i64> %6848, <i64 13623, i64 13623>
  %6868 = add <2 x i64> %6867, %6866
  %6869 = mul nuw <2 x i64> %6851, <i64 4294958194, i64 4294958194>
  %6870 = mul nuw nsw <2 x i64> %6853, <i64 13623, i64 13623>
  %6871 = add <2 x i64> %6870, %6869
  %6872 = mul nuw <2 x i64> %6856, <i64 4294958194, i64 4294958194>
  %6873 = mul nuw nsw <2 x i64> %6858, <i64 13623, i64 13623>
  %6874 = add <2 x i64> %6873, %6872
  %6875 = mul nuw <2 x i64> %6861, <i64 4294958194, i64 4294958194>
  %6876 = mul nuw nsw <2 x i64> %6863, <i64 13623, i64 13623>
  %6877 = add <2 x i64> %6876, %6875
  %6878 = mul nuw nsw <2 x i64> %6826, <i64 13623, i64 13623>
  %6879 = mul nuw nsw <2 x i64> %6828, <i64 9102, i64 9102>
  %6880 = add nuw nsw <2 x i64> %6879, %6878
  %6881 = mul nuw nsw <2 x i64> %6831, <i64 13623, i64 13623>
  %6882 = mul nuw nsw <2 x i64> %6833, <i64 9102, i64 9102>
  %6883 = add nuw nsw <2 x i64> %6882, %6881
  %6884 = mul nuw nsw <2 x i64> %6836, <i64 13623, i64 13623>
  %6885 = mul nuw nsw <2 x i64> %6838, <i64 9102, i64 9102>
  %6886 = add nuw nsw <2 x i64> %6885, %6884
  %6887 = mul nuw nsw <2 x i64> %6841, <i64 13623, i64 13623>
  %6888 = mul nuw nsw <2 x i64> %6843, <i64 9102, i64 9102>
  %6889 = add nuw nsw <2 x i64> %6888, %6887
  %6890 = mul nuw <2 x i64> %6806, <i64 4294951227, i64 4294951227>
  %6891 = mul nuw nsw <2 x i64> %6808, <i64 3196, i64 3196>
  %6892 = add <2 x i64> %6891, %6890
  %6893 = mul nuw <2 x i64> %6811, <i64 4294951227, i64 4294951227>
  %6894 = mul nuw nsw <2 x i64> %6813, <i64 3196, i64 3196>
  %6895 = add <2 x i64> %6894, %6893
  %6896 = mul nuw <2 x i64> %6816, <i64 4294951227, i64 4294951227>
  %6897 = mul nuw nsw <2 x i64> %6818, <i64 3196, i64 3196>
  %6898 = add <2 x i64> %6897, %6896
  %6899 = mul nuw <2 x i64> %6821, <i64 4294951227, i64 4294951227>
  %6900 = mul nuw nsw <2 x i64> %6823, <i64 3196, i64 3196>
  %6901 = add <2 x i64> %6900, %6899
  %6902 = mul nuw nsw <2 x i64> %6786, <i64 3196, i64 3196>
  %6903 = mul nuw nsw <2 x i64> %6788, <i64 16069, i64 16069>
  %6904 = add nuw nsw <2 x i64> %6903, %6902
  %6905 = mul nuw nsw <2 x i64> %6791, <i64 3196, i64 3196>
  %6906 = mul nuw nsw <2 x i64> %6793, <i64 16069, i64 16069>
  %6907 = add nuw nsw <2 x i64> %6906, %6905
  %6908 = mul nuw nsw <2 x i64> %6796, <i64 3196, i64 3196>
  %6909 = mul nuw nsw <2 x i64> %6798, <i64 16069, i64 16069>
  %6910 = add nuw nsw <2 x i64> %6909, %6908
  %6911 = mul nuw nsw <2 x i64> %6801, <i64 3196, i64 3196>
  %6912 = mul nuw nsw <2 x i64> %6803, <i64 16069, i64 16069>
  %6913 = add nuw nsw <2 x i64> %6912, %6911
  %6914 = shl <2 x i64> %6790, <i64 1, i64 1>
  %6915 = shl <2 x i64> %6795, <i64 1, i64 1>
  %6916 = shl <2 x i64> %6800, <i64 1, i64 1>
  %6917 = shl <2 x i64> %6805, <i64 1, i64 1>
  %6918 = bitcast <2 x i64> %6914 to <4 x i32>
  %6919 = shufflevector <4 x i32> %6918, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %6920 = bitcast <4 x i32> %6919 to <2 x i64>
  %6921 = bitcast <2 x i64> %6915 to <4 x i32>
  %6922 = shufflevector <4 x i32> %6921, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %6923 = bitcast <4 x i32> %6922 to <2 x i64>
  %6924 = bitcast <2 x i64> %6916 to <4 x i32>
  %6925 = shufflevector <4 x i32> %6924, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %6926 = bitcast <4 x i32> %6925 to <2 x i64>
  %6927 = bitcast <2 x i64> %6917 to <4 x i32>
  %6928 = shufflevector <4 x i32> %6927, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %6929 = bitcast <4 x i32> %6928 to <2 x i64>
  %6930 = shufflevector <2 x i64> %6920, <2 x i64> %6923, <2 x i32> <i32 0, i32 2>
  %6931 = shufflevector <2 x i64> %6926, <2 x i64> %6929, <2 x i32> <i32 0, i32 2>
  %6932 = bitcast <2 x i64> %6930 to <4 x i32>
  %6933 = bitcast <2 x i64> %6931 to <4 x i32>
  %6934 = add <4 x i32> %6932, <i32 1, i32 1, i32 1, i32 1>
  %6935 = icmp ugt <4 x i32> %6934, <i32 1, i32 1, i32 1, i32 1>
  %6936 = sext <4 x i1> %6935 to <4 x i32>
  %6937 = bitcast <4 x i32> %6936 to <16 x i8>
  %6938 = icmp slt <16 x i8> %6937, zeroinitializer
  %6939 = bitcast <16 x i1> %6938 to i16
  %6940 = zext i16 %6939 to i32
  %6941 = add <4 x i32> %6933, <i32 1, i32 1, i32 1, i32 1>
  %6942 = icmp ugt <4 x i32> %6941, <i32 1, i32 1, i32 1, i32 1>
  %6943 = sext <4 x i1> %6942 to <4 x i32>
  %6944 = bitcast <4 x i32> %6943 to <16 x i8>
  %6945 = icmp slt <16 x i8> %6944, zeroinitializer
  %6946 = bitcast <16 x i1> %6945 to i16
  %6947 = zext i16 %6946 to i32
  %6948 = sub nsw i32 0, %6940
  %6949 = icmp eq i32 %6947, %6948
  br i1 %6949, label %6950, label %7209

6950:                                             ; preds = %6737
  %6951 = shl <2 x i64> %6810, <i64 1, i64 1>
  %6952 = shl <2 x i64> %6815, <i64 1, i64 1>
  %6953 = shl <2 x i64> %6820, <i64 1, i64 1>
  %6954 = shl <2 x i64> %6825, <i64 1, i64 1>
  %6955 = bitcast <2 x i64> %6951 to <4 x i32>
  %6956 = shufflevector <4 x i32> %6955, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %6957 = bitcast <4 x i32> %6956 to <2 x i64>
  %6958 = bitcast <2 x i64> %6952 to <4 x i32>
  %6959 = shufflevector <4 x i32> %6958, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %6960 = bitcast <4 x i32> %6959 to <2 x i64>
  %6961 = bitcast <2 x i64> %6953 to <4 x i32>
  %6962 = shufflevector <4 x i32> %6961, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %6963 = bitcast <4 x i32> %6962 to <2 x i64>
  %6964 = bitcast <2 x i64> %6954 to <4 x i32>
  %6965 = shufflevector <4 x i32> %6964, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %6966 = bitcast <4 x i32> %6965 to <2 x i64>
  %6967 = shufflevector <2 x i64> %6957, <2 x i64> %6960, <2 x i32> <i32 0, i32 2>
  %6968 = shufflevector <2 x i64> %6963, <2 x i64> %6966, <2 x i32> <i32 0, i32 2>
  %6969 = bitcast <2 x i64> %6967 to <4 x i32>
  %6970 = bitcast <2 x i64> %6968 to <4 x i32>
  %6971 = add <4 x i32> %6969, <i32 1, i32 1, i32 1, i32 1>
  %6972 = icmp ugt <4 x i32> %6971, <i32 1, i32 1, i32 1, i32 1>
  %6973 = sext <4 x i1> %6972 to <4 x i32>
  %6974 = bitcast <4 x i32> %6973 to <16 x i8>
  %6975 = icmp slt <16 x i8> %6974, zeroinitializer
  %6976 = bitcast <16 x i1> %6975 to i16
  %6977 = zext i16 %6976 to i32
  %6978 = add <4 x i32> %6970, <i32 1, i32 1, i32 1, i32 1>
  %6979 = icmp ugt <4 x i32> %6978, <i32 1, i32 1, i32 1, i32 1>
  %6980 = sext <4 x i1> %6979 to <4 x i32>
  %6981 = bitcast <4 x i32> %6980 to <16 x i8>
  %6982 = icmp slt <16 x i8> %6981, zeroinitializer
  %6983 = bitcast <16 x i1> %6982 to i16
  %6984 = zext i16 %6983 to i32
  %6985 = sub nsw i32 0, %6977
  %6986 = icmp eq i32 %6984, %6985
  br i1 %6986, label %6987, label %7209

6987:                                             ; preds = %6950
  %6988 = shl <2 x i64> %6830, <i64 1, i64 1>
  %6989 = shl <2 x i64> %6835, <i64 1, i64 1>
  %6990 = shl <2 x i64> %6840, <i64 1, i64 1>
  %6991 = shl <2 x i64> %6845, <i64 1, i64 1>
  %6992 = bitcast <2 x i64> %6988 to <4 x i32>
  %6993 = shufflevector <4 x i32> %6992, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %6994 = bitcast <4 x i32> %6993 to <2 x i64>
  %6995 = bitcast <2 x i64> %6989 to <4 x i32>
  %6996 = shufflevector <4 x i32> %6995, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %6997 = bitcast <4 x i32> %6996 to <2 x i64>
  %6998 = bitcast <2 x i64> %6990 to <4 x i32>
  %6999 = shufflevector <4 x i32> %6998, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7000 = bitcast <4 x i32> %6999 to <2 x i64>
  %7001 = bitcast <2 x i64> %6991 to <4 x i32>
  %7002 = shufflevector <4 x i32> %7001, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7003 = bitcast <4 x i32> %7002 to <2 x i64>
  %7004 = shufflevector <2 x i64> %6994, <2 x i64> %6997, <2 x i32> <i32 0, i32 2>
  %7005 = shufflevector <2 x i64> %7000, <2 x i64> %7003, <2 x i32> <i32 0, i32 2>
  %7006 = bitcast <2 x i64> %7004 to <4 x i32>
  %7007 = bitcast <2 x i64> %7005 to <4 x i32>
  %7008 = add <4 x i32> %7006, <i32 1, i32 1, i32 1, i32 1>
  %7009 = icmp ugt <4 x i32> %7008, <i32 1, i32 1, i32 1, i32 1>
  %7010 = sext <4 x i1> %7009 to <4 x i32>
  %7011 = bitcast <4 x i32> %7010 to <16 x i8>
  %7012 = icmp slt <16 x i8> %7011, zeroinitializer
  %7013 = bitcast <16 x i1> %7012 to i16
  %7014 = zext i16 %7013 to i32
  %7015 = add <4 x i32> %7007, <i32 1, i32 1, i32 1, i32 1>
  %7016 = icmp ugt <4 x i32> %7015, <i32 1, i32 1, i32 1, i32 1>
  %7017 = sext <4 x i1> %7016 to <4 x i32>
  %7018 = bitcast <4 x i32> %7017 to <16 x i8>
  %7019 = icmp slt <16 x i8> %7018, zeroinitializer
  %7020 = bitcast <16 x i1> %7019 to i16
  %7021 = zext i16 %7020 to i32
  %7022 = sub nsw i32 0, %7014
  %7023 = icmp eq i32 %7021, %7022
  br i1 %7023, label %7024, label %7209

7024:                                             ; preds = %6987
  %7025 = shl <2 x i64> %6850, <i64 1, i64 1>
  %7026 = shl <2 x i64> %6855, <i64 1, i64 1>
  %7027 = shl <2 x i64> %6860, <i64 1, i64 1>
  %7028 = shl <2 x i64> %6865, <i64 1, i64 1>
  %7029 = bitcast <2 x i64> %7025 to <4 x i32>
  %7030 = shufflevector <4 x i32> %7029, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7031 = bitcast <4 x i32> %7030 to <2 x i64>
  %7032 = bitcast <2 x i64> %7026 to <4 x i32>
  %7033 = shufflevector <4 x i32> %7032, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7034 = bitcast <4 x i32> %7033 to <2 x i64>
  %7035 = bitcast <2 x i64> %7027 to <4 x i32>
  %7036 = shufflevector <4 x i32> %7035, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7037 = bitcast <4 x i32> %7036 to <2 x i64>
  %7038 = bitcast <2 x i64> %7028 to <4 x i32>
  %7039 = shufflevector <4 x i32> %7038, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7040 = bitcast <4 x i32> %7039 to <2 x i64>
  %7041 = shufflevector <2 x i64> %7031, <2 x i64> %7034, <2 x i32> <i32 0, i32 2>
  %7042 = shufflevector <2 x i64> %7037, <2 x i64> %7040, <2 x i32> <i32 0, i32 2>
  %7043 = bitcast <2 x i64> %7041 to <4 x i32>
  %7044 = bitcast <2 x i64> %7042 to <4 x i32>
  %7045 = add <4 x i32> %7043, <i32 1, i32 1, i32 1, i32 1>
  %7046 = icmp ugt <4 x i32> %7045, <i32 1, i32 1, i32 1, i32 1>
  %7047 = sext <4 x i1> %7046 to <4 x i32>
  %7048 = bitcast <4 x i32> %7047 to <16 x i8>
  %7049 = icmp slt <16 x i8> %7048, zeroinitializer
  %7050 = bitcast <16 x i1> %7049 to i16
  %7051 = zext i16 %7050 to i32
  %7052 = add <4 x i32> %7044, <i32 1, i32 1, i32 1, i32 1>
  %7053 = icmp ugt <4 x i32> %7052, <i32 1, i32 1, i32 1, i32 1>
  %7054 = sext <4 x i1> %7053 to <4 x i32>
  %7055 = bitcast <4 x i32> %7054 to <16 x i8>
  %7056 = icmp slt <16 x i8> %7055, zeroinitializer
  %7057 = bitcast <16 x i1> %7056 to i16
  %7058 = zext i16 %7057 to i32
  %7059 = sub nsw i32 0, %7051
  %7060 = icmp eq i32 %7058, %7059
  br i1 %7060, label %7061, label %7209

7061:                                             ; preds = %7024
  %7062 = shl <2 x i64> %6868, <i64 1, i64 1>
  %7063 = shl <2 x i64> %6871, <i64 1, i64 1>
  %7064 = shl <2 x i64> %6874, <i64 1, i64 1>
  %7065 = shl <2 x i64> %6877, <i64 1, i64 1>
  %7066 = bitcast <2 x i64> %7062 to <4 x i32>
  %7067 = shufflevector <4 x i32> %7066, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7068 = bitcast <4 x i32> %7067 to <2 x i64>
  %7069 = bitcast <2 x i64> %7063 to <4 x i32>
  %7070 = shufflevector <4 x i32> %7069, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7071 = bitcast <4 x i32> %7070 to <2 x i64>
  %7072 = bitcast <2 x i64> %7064 to <4 x i32>
  %7073 = shufflevector <4 x i32> %7072, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7074 = bitcast <4 x i32> %7073 to <2 x i64>
  %7075 = bitcast <2 x i64> %7065 to <4 x i32>
  %7076 = shufflevector <4 x i32> %7075, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7077 = bitcast <4 x i32> %7076 to <2 x i64>
  %7078 = shufflevector <2 x i64> %7068, <2 x i64> %7071, <2 x i32> <i32 0, i32 2>
  %7079 = shufflevector <2 x i64> %7074, <2 x i64> %7077, <2 x i32> <i32 0, i32 2>
  %7080 = bitcast <2 x i64> %7078 to <4 x i32>
  %7081 = bitcast <2 x i64> %7079 to <4 x i32>
  %7082 = add <4 x i32> %7080, <i32 1, i32 1, i32 1, i32 1>
  %7083 = icmp ugt <4 x i32> %7082, <i32 1, i32 1, i32 1, i32 1>
  %7084 = sext <4 x i1> %7083 to <4 x i32>
  %7085 = bitcast <4 x i32> %7084 to <16 x i8>
  %7086 = icmp slt <16 x i8> %7085, zeroinitializer
  %7087 = bitcast <16 x i1> %7086 to i16
  %7088 = zext i16 %7087 to i32
  %7089 = add <4 x i32> %7081, <i32 1, i32 1, i32 1, i32 1>
  %7090 = icmp ugt <4 x i32> %7089, <i32 1, i32 1, i32 1, i32 1>
  %7091 = sext <4 x i1> %7090 to <4 x i32>
  %7092 = bitcast <4 x i32> %7091 to <16 x i8>
  %7093 = icmp slt <16 x i8> %7092, zeroinitializer
  %7094 = bitcast <16 x i1> %7093 to i16
  %7095 = zext i16 %7094 to i32
  %7096 = sub nsw i32 0, %7088
  %7097 = icmp eq i32 %7095, %7096
  br i1 %7097, label %7098, label %7209

7098:                                             ; preds = %7061
  %7099 = shl nuw nsw <2 x i64> %6880, <i64 1, i64 1>
  %7100 = shl nuw nsw <2 x i64> %6883, <i64 1, i64 1>
  %7101 = shl nuw nsw <2 x i64> %6886, <i64 1, i64 1>
  %7102 = shl nuw nsw <2 x i64> %6889, <i64 1, i64 1>
  %7103 = bitcast <2 x i64> %7099 to <4 x i32>
  %7104 = shufflevector <4 x i32> %7103, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7105 = bitcast <4 x i32> %7104 to <2 x i64>
  %7106 = bitcast <2 x i64> %7100 to <4 x i32>
  %7107 = shufflevector <4 x i32> %7106, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7108 = bitcast <4 x i32> %7107 to <2 x i64>
  %7109 = bitcast <2 x i64> %7101 to <4 x i32>
  %7110 = shufflevector <4 x i32> %7109, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7111 = bitcast <4 x i32> %7110 to <2 x i64>
  %7112 = bitcast <2 x i64> %7102 to <4 x i32>
  %7113 = shufflevector <4 x i32> %7112, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7114 = bitcast <4 x i32> %7113 to <2 x i64>
  %7115 = shufflevector <2 x i64> %7105, <2 x i64> %7108, <2 x i32> <i32 0, i32 2>
  %7116 = shufflevector <2 x i64> %7111, <2 x i64> %7114, <2 x i32> <i32 0, i32 2>
  %7117 = bitcast <2 x i64> %7115 to <4 x i32>
  %7118 = bitcast <2 x i64> %7116 to <4 x i32>
  %7119 = add <4 x i32> %7117, <i32 1, i32 1, i32 1, i32 1>
  %7120 = icmp ugt <4 x i32> %7119, <i32 1, i32 1, i32 1, i32 1>
  %7121 = sext <4 x i1> %7120 to <4 x i32>
  %7122 = bitcast <4 x i32> %7121 to <16 x i8>
  %7123 = icmp slt <16 x i8> %7122, zeroinitializer
  %7124 = bitcast <16 x i1> %7123 to i16
  %7125 = zext i16 %7124 to i32
  %7126 = add <4 x i32> %7118, <i32 1, i32 1, i32 1, i32 1>
  %7127 = icmp ugt <4 x i32> %7126, <i32 1, i32 1, i32 1, i32 1>
  %7128 = sext <4 x i1> %7127 to <4 x i32>
  %7129 = bitcast <4 x i32> %7128 to <16 x i8>
  %7130 = icmp slt <16 x i8> %7129, zeroinitializer
  %7131 = bitcast <16 x i1> %7130 to i16
  %7132 = zext i16 %7131 to i32
  %7133 = sub nsw i32 0, %7125
  %7134 = icmp eq i32 %7132, %7133
  br i1 %7134, label %7135, label %7209

7135:                                             ; preds = %7098
  %7136 = shl <2 x i64> %6892, <i64 1, i64 1>
  %7137 = shl <2 x i64> %6895, <i64 1, i64 1>
  %7138 = shl <2 x i64> %6898, <i64 1, i64 1>
  %7139 = shl <2 x i64> %6901, <i64 1, i64 1>
  %7140 = bitcast <2 x i64> %7136 to <4 x i32>
  %7141 = shufflevector <4 x i32> %7140, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7142 = bitcast <4 x i32> %7141 to <2 x i64>
  %7143 = bitcast <2 x i64> %7137 to <4 x i32>
  %7144 = shufflevector <4 x i32> %7143, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7145 = bitcast <4 x i32> %7144 to <2 x i64>
  %7146 = bitcast <2 x i64> %7138 to <4 x i32>
  %7147 = shufflevector <4 x i32> %7146, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7148 = bitcast <4 x i32> %7147 to <2 x i64>
  %7149 = bitcast <2 x i64> %7139 to <4 x i32>
  %7150 = shufflevector <4 x i32> %7149, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7151 = bitcast <4 x i32> %7150 to <2 x i64>
  %7152 = shufflevector <2 x i64> %7142, <2 x i64> %7145, <2 x i32> <i32 0, i32 2>
  %7153 = shufflevector <2 x i64> %7148, <2 x i64> %7151, <2 x i32> <i32 0, i32 2>
  %7154 = bitcast <2 x i64> %7152 to <4 x i32>
  %7155 = bitcast <2 x i64> %7153 to <4 x i32>
  %7156 = add <4 x i32> %7154, <i32 1, i32 1, i32 1, i32 1>
  %7157 = icmp ugt <4 x i32> %7156, <i32 1, i32 1, i32 1, i32 1>
  %7158 = sext <4 x i1> %7157 to <4 x i32>
  %7159 = bitcast <4 x i32> %7158 to <16 x i8>
  %7160 = icmp slt <16 x i8> %7159, zeroinitializer
  %7161 = bitcast <16 x i1> %7160 to i16
  %7162 = zext i16 %7161 to i32
  %7163 = add <4 x i32> %7155, <i32 1, i32 1, i32 1, i32 1>
  %7164 = icmp ugt <4 x i32> %7163, <i32 1, i32 1, i32 1, i32 1>
  %7165 = sext <4 x i1> %7164 to <4 x i32>
  %7166 = bitcast <4 x i32> %7165 to <16 x i8>
  %7167 = icmp slt <16 x i8> %7166, zeroinitializer
  %7168 = bitcast <16 x i1> %7167 to i16
  %7169 = zext i16 %7168 to i32
  %7170 = sub nsw i32 0, %7162
  %7171 = icmp eq i32 %7169, %7170
  br i1 %7171, label %7172, label %7209

7172:                                             ; preds = %7135
  %7173 = shl nuw nsw <2 x i64> %6904, <i64 1, i64 1>
  %7174 = shl nuw nsw <2 x i64> %6907, <i64 1, i64 1>
  %7175 = shl nuw nsw <2 x i64> %6910, <i64 1, i64 1>
  %7176 = shl nuw nsw <2 x i64> %6913, <i64 1, i64 1>
  %7177 = bitcast <2 x i64> %7173 to <4 x i32>
  %7178 = shufflevector <4 x i32> %7177, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7179 = bitcast <4 x i32> %7178 to <2 x i64>
  %7180 = bitcast <2 x i64> %7174 to <4 x i32>
  %7181 = shufflevector <4 x i32> %7180, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7182 = bitcast <4 x i32> %7181 to <2 x i64>
  %7183 = bitcast <2 x i64> %7175 to <4 x i32>
  %7184 = shufflevector <4 x i32> %7183, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7185 = bitcast <4 x i32> %7184 to <2 x i64>
  %7186 = bitcast <2 x i64> %7176 to <4 x i32>
  %7187 = shufflevector <4 x i32> %7186, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7188 = bitcast <4 x i32> %7187 to <2 x i64>
  %7189 = shufflevector <2 x i64> %7179, <2 x i64> %7182, <2 x i32> <i32 0, i32 2>
  %7190 = shufflevector <2 x i64> %7185, <2 x i64> %7188, <2 x i32> <i32 0, i32 2>
  %7191 = bitcast <2 x i64> %7189 to <4 x i32>
  %7192 = bitcast <2 x i64> %7190 to <4 x i32>
  %7193 = add <4 x i32> %7191, <i32 1, i32 1, i32 1, i32 1>
  %7194 = icmp ugt <4 x i32> %7193, <i32 1, i32 1, i32 1, i32 1>
  %7195 = sext <4 x i1> %7194 to <4 x i32>
  %7196 = bitcast <4 x i32> %7195 to <16 x i8>
  %7197 = icmp slt <16 x i8> %7196, zeroinitializer
  %7198 = bitcast <16 x i1> %7197 to i16
  %7199 = zext i16 %7198 to i32
  %7200 = add <4 x i32> %7192, <i32 1, i32 1, i32 1, i32 1>
  %7201 = icmp ugt <4 x i32> %7200, <i32 1, i32 1, i32 1, i32 1>
  %7202 = sext <4 x i1> %7201 to <4 x i32>
  %7203 = bitcast <4 x i32> %7202 to <16 x i8>
  %7204 = icmp slt <16 x i8> %7203, zeroinitializer
  %7205 = bitcast <16 x i1> %7204 to i16
  %7206 = zext i16 %7205 to i32
  %7207 = sub nsw i32 0, %7199
  %7208 = icmp eq i32 %7206, %7207
  br i1 %7208, label %7425, label %7209

7209:                                             ; preds = %7135, %7098, %7061, %7024, %6987, %6950, %6737, %7172
  %7210 = bitcast [32 x i64]* %4 to i8*
  %7211 = bitcast [32 x i64]* %5 to i8*
  %7212 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %7213 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %7214 = bitcast [32 x i64]* %5 to <2 x i64>*
  %7215 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %7216 = bitcast i64* %7215 to <2 x i64>*
  %7217 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %7218 = bitcast i64* %7217 to <2 x i64>*
  %7219 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %7220 = bitcast i64* %7219 to <2 x i64>*
  %7221 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %7222 = bitcast i64* %7221 to <2 x i64>*
  %7223 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %7224 = bitcast i64* %7223 to <2 x i64>*
  %7225 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %7226 = bitcast i64* %7225 to <2 x i64>*
  %7227 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %7228 = bitcast i64* %7227 to <2 x i64>*
  %7229 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %7230 = bitcast i64* %7229 to <2 x i64>*
  %7231 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %7232 = bitcast i64* %7231 to <2 x i64>*
  %7233 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %7234 = bitcast i64* %7233 to <2 x i64>*
  %7235 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %7236 = bitcast i64* %7235 to <2 x i64>*
  %7237 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %7238 = bitcast i64* %7237 to <2 x i64>*
  %7239 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %7240 = bitcast i64* %7239 to <2 x i64>*
  %7241 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %7242 = bitcast i64* %7241 to <2 x i64>*
  %7243 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %7244 = bitcast i64* %7243 to <2 x i64>*
  br label %7245

7245:                                             ; preds = %7278, %7209
  %7246 = phi i64 [ 0, %7209 ], [ %7423, %7278 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %7210) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %7210, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %7211) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %7211, i8 -86, i64 256, i1 false) #6
  br label %7247

7247:                                             ; preds = %7247, %7245
  %7248 = phi i64 [ 0, %7245 ], [ %7276, %7247 ]
  %7249 = shl i64 %7248, 5
  %7250 = add nuw nsw i64 %7249, %7246
  %7251 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %7250
  %7252 = load i16, i16* %7251, align 2
  %7253 = sext i16 %7252 to i64
  %7254 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %7248
  store i64 %7253, i64* %7254, align 16
  %7255 = or i64 %7248, 1
  %7256 = shl i64 %7255, 5
  %7257 = add nuw nsw i64 %7256, %7246
  %7258 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %7257
  %7259 = load i16, i16* %7258, align 2
  %7260 = sext i16 %7259 to i64
  %7261 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %7255
  store i64 %7260, i64* %7261, align 8
  %7262 = or i64 %7248, 2
  %7263 = shl i64 %7262, 5
  %7264 = add nuw nsw i64 %7263, %7246
  %7265 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %7264
  %7266 = load i16, i16* %7265, align 2
  %7267 = sext i16 %7266 to i64
  %7268 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %7262
  store i64 %7267, i64* %7268, align 16
  %7269 = or i64 %7248, 3
  %7270 = shl i64 %7269, 5
  %7271 = add nuw nsw i64 %7270, %7246
  %7272 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %7271
  %7273 = load i16, i16* %7272, align 2
  %7274 = sext i16 %7273 to i64
  %7275 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %7269
  store i64 %7274, i64* %7275, align 8
  %7276 = add nuw nsw i64 %7248, 4
  %7277 = icmp eq i64 %7276, 32
  br i1 %7277, label %7278, label %7247

7278:                                             ; preds = %7247
  call void @vpx_fdct32(i64* nonnull %7212, i64* nonnull %7213, i32 0) #6
  %7279 = shl i64 %7246, 5
  %7280 = load <2 x i64>, <2 x i64>* %7214, align 16
  %7281 = add nsw <2 x i64> %7280, <i64 1, i64 1>
  %7282 = lshr <2 x i64> %7280, <i64 63, i64 63>
  %7283 = add nsw <2 x i64> %7281, %7282
  %7284 = lshr <2 x i64> %7283, <i64 2, i64 2>
  %7285 = trunc <2 x i64> %7284 to <2 x i32>
  %7286 = getelementptr inbounds i32, i32* %1, i64 %7279
  %7287 = bitcast i32* %7286 to <2 x i32>*
  store <2 x i32> %7285, <2 x i32>* %7287, align 4
  %7288 = load <2 x i64>, <2 x i64>* %7216, align 16
  %7289 = add nsw <2 x i64> %7288, <i64 1, i64 1>
  %7290 = lshr <2 x i64> %7288, <i64 63, i64 63>
  %7291 = add nsw <2 x i64> %7289, %7290
  %7292 = lshr <2 x i64> %7291, <i64 2, i64 2>
  %7293 = trunc <2 x i64> %7292 to <2 x i32>
  %7294 = or i64 %7279, 2
  %7295 = getelementptr inbounds i32, i32* %1, i64 %7294
  %7296 = bitcast i32* %7295 to <2 x i32>*
  store <2 x i32> %7293, <2 x i32>* %7296, align 4
  %7297 = load <2 x i64>, <2 x i64>* %7218, align 16
  %7298 = add nsw <2 x i64> %7297, <i64 1, i64 1>
  %7299 = lshr <2 x i64> %7297, <i64 63, i64 63>
  %7300 = add nsw <2 x i64> %7298, %7299
  %7301 = lshr <2 x i64> %7300, <i64 2, i64 2>
  %7302 = trunc <2 x i64> %7301 to <2 x i32>
  %7303 = or i64 %7279, 4
  %7304 = getelementptr inbounds i32, i32* %1, i64 %7303
  %7305 = bitcast i32* %7304 to <2 x i32>*
  store <2 x i32> %7302, <2 x i32>* %7305, align 4
  %7306 = load <2 x i64>, <2 x i64>* %7220, align 16
  %7307 = add nsw <2 x i64> %7306, <i64 1, i64 1>
  %7308 = lshr <2 x i64> %7306, <i64 63, i64 63>
  %7309 = add nsw <2 x i64> %7307, %7308
  %7310 = lshr <2 x i64> %7309, <i64 2, i64 2>
  %7311 = trunc <2 x i64> %7310 to <2 x i32>
  %7312 = or i64 %7279, 6
  %7313 = getelementptr inbounds i32, i32* %1, i64 %7312
  %7314 = bitcast i32* %7313 to <2 x i32>*
  store <2 x i32> %7311, <2 x i32>* %7314, align 4
  %7315 = load <2 x i64>, <2 x i64>* %7222, align 16
  %7316 = add nsw <2 x i64> %7315, <i64 1, i64 1>
  %7317 = lshr <2 x i64> %7315, <i64 63, i64 63>
  %7318 = add nsw <2 x i64> %7316, %7317
  %7319 = lshr <2 x i64> %7318, <i64 2, i64 2>
  %7320 = trunc <2 x i64> %7319 to <2 x i32>
  %7321 = or i64 %7279, 8
  %7322 = getelementptr inbounds i32, i32* %1, i64 %7321
  %7323 = bitcast i32* %7322 to <2 x i32>*
  store <2 x i32> %7320, <2 x i32>* %7323, align 4
  %7324 = load <2 x i64>, <2 x i64>* %7224, align 16
  %7325 = add nsw <2 x i64> %7324, <i64 1, i64 1>
  %7326 = lshr <2 x i64> %7324, <i64 63, i64 63>
  %7327 = add nsw <2 x i64> %7325, %7326
  %7328 = lshr <2 x i64> %7327, <i64 2, i64 2>
  %7329 = trunc <2 x i64> %7328 to <2 x i32>
  %7330 = or i64 %7279, 10
  %7331 = getelementptr inbounds i32, i32* %1, i64 %7330
  %7332 = bitcast i32* %7331 to <2 x i32>*
  store <2 x i32> %7329, <2 x i32>* %7332, align 4
  %7333 = load <2 x i64>, <2 x i64>* %7226, align 16
  %7334 = add nsw <2 x i64> %7333, <i64 1, i64 1>
  %7335 = lshr <2 x i64> %7333, <i64 63, i64 63>
  %7336 = add nsw <2 x i64> %7334, %7335
  %7337 = lshr <2 x i64> %7336, <i64 2, i64 2>
  %7338 = trunc <2 x i64> %7337 to <2 x i32>
  %7339 = or i64 %7279, 12
  %7340 = getelementptr inbounds i32, i32* %1, i64 %7339
  %7341 = bitcast i32* %7340 to <2 x i32>*
  store <2 x i32> %7338, <2 x i32>* %7341, align 4
  %7342 = load <2 x i64>, <2 x i64>* %7228, align 16
  %7343 = add nsw <2 x i64> %7342, <i64 1, i64 1>
  %7344 = lshr <2 x i64> %7342, <i64 63, i64 63>
  %7345 = add nsw <2 x i64> %7343, %7344
  %7346 = lshr <2 x i64> %7345, <i64 2, i64 2>
  %7347 = trunc <2 x i64> %7346 to <2 x i32>
  %7348 = or i64 %7279, 14
  %7349 = getelementptr inbounds i32, i32* %1, i64 %7348
  %7350 = bitcast i32* %7349 to <2 x i32>*
  store <2 x i32> %7347, <2 x i32>* %7350, align 4
  %7351 = load <2 x i64>, <2 x i64>* %7230, align 16
  %7352 = add nsw <2 x i64> %7351, <i64 1, i64 1>
  %7353 = lshr <2 x i64> %7351, <i64 63, i64 63>
  %7354 = add nsw <2 x i64> %7352, %7353
  %7355 = lshr <2 x i64> %7354, <i64 2, i64 2>
  %7356 = trunc <2 x i64> %7355 to <2 x i32>
  %7357 = or i64 %7279, 16
  %7358 = getelementptr inbounds i32, i32* %1, i64 %7357
  %7359 = bitcast i32* %7358 to <2 x i32>*
  store <2 x i32> %7356, <2 x i32>* %7359, align 4
  %7360 = load <2 x i64>, <2 x i64>* %7232, align 16
  %7361 = add nsw <2 x i64> %7360, <i64 1, i64 1>
  %7362 = lshr <2 x i64> %7360, <i64 63, i64 63>
  %7363 = add nsw <2 x i64> %7361, %7362
  %7364 = lshr <2 x i64> %7363, <i64 2, i64 2>
  %7365 = trunc <2 x i64> %7364 to <2 x i32>
  %7366 = or i64 %7279, 18
  %7367 = getelementptr inbounds i32, i32* %1, i64 %7366
  %7368 = bitcast i32* %7367 to <2 x i32>*
  store <2 x i32> %7365, <2 x i32>* %7368, align 4
  %7369 = load <2 x i64>, <2 x i64>* %7234, align 16
  %7370 = add nsw <2 x i64> %7369, <i64 1, i64 1>
  %7371 = lshr <2 x i64> %7369, <i64 63, i64 63>
  %7372 = add nsw <2 x i64> %7370, %7371
  %7373 = lshr <2 x i64> %7372, <i64 2, i64 2>
  %7374 = trunc <2 x i64> %7373 to <2 x i32>
  %7375 = or i64 %7279, 20
  %7376 = getelementptr inbounds i32, i32* %1, i64 %7375
  %7377 = bitcast i32* %7376 to <2 x i32>*
  store <2 x i32> %7374, <2 x i32>* %7377, align 4
  %7378 = load <2 x i64>, <2 x i64>* %7236, align 16
  %7379 = add nsw <2 x i64> %7378, <i64 1, i64 1>
  %7380 = lshr <2 x i64> %7378, <i64 63, i64 63>
  %7381 = add nsw <2 x i64> %7379, %7380
  %7382 = lshr <2 x i64> %7381, <i64 2, i64 2>
  %7383 = trunc <2 x i64> %7382 to <2 x i32>
  %7384 = or i64 %7279, 22
  %7385 = getelementptr inbounds i32, i32* %1, i64 %7384
  %7386 = bitcast i32* %7385 to <2 x i32>*
  store <2 x i32> %7383, <2 x i32>* %7386, align 4
  %7387 = load <2 x i64>, <2 x i64>* %7238, align 16
  %7388 = add nsw <2 x i64> %7387, <i64 1, i64 1>
  %7389 = lshr <2 x i64> %7387, <i64 63, i64 63>
  %7390 = add nsw <2 x i64> %7388, %7389
  %7391 = lshr <2 x i64> %7390, <i64 2, i64 2>
  %7392 = trunc <2 x i64> %7391 to <2 x i32>
  %7393 = or i64 %7279, 24
  %7394 = getelementptr inbounds i32, i32* %1, i64 %7393
  %7395 = bitcast i32* %7394 to <2 x i32>*
  store <2 x i32> %7392, <2 x i32>* %7395, align 4
  %7396 = load <2 x i64>, <2 x i64>* %7240, align 16
  %7397 = add nsw <2 x i64> %7396, <i64 1, i64 1>
  %7398 = lshr <2 x i64> %7396, <i64 63, i64 63>
  %7399 = add nsw <2 x i64> %7397, %7398
  %7400 = lshr <2 x i64> %7399, <i64 2, i64 2>
  %7401 = trunc <2 x i64> %7400 to <2 x i32>
  %7402 = or i64 %7279, 26
  %7403 = getelementptr inbounds i32, i32* %1, i64 %7402
  %7404 = bitcast i32* %7403 to <2 x i32>*
  store <2 x i32> %7401, <2 x i32>* %7404, align 4
  %7405 = load <2 x i64>, <2 x i64>* %7242, align 16
  %7406 = add nsw <2 x i64> %7405, <i64 1, i64 1>
  %7407 = lshr <2 x i64> %7405, <i64 63, i64 63>
  %7408 = add nsw <2 x i64> %7406, %7407
  %7409 = lshr <2 x i64> %7408, <i64 2, i64 2>
  %7410 = trunc <2 x i64> %7409 to <2 x i32>
  %7411 = or i64 %7279, 28
  %7412 = getelementptr inbounds i32, i32* %1, i64 %7411
  %7413 = bitcast i32* %7412 to <2 x i32>*
  store <2 x i32> %7410, <2 x i32>* %7413, align 4
  %7414 = load <2 x i64>, <2 x i64>* %7244, align 16
  %7415 = add nsw <2 x i64> %7414, <i64 1, i64 1>
  %7416 = lshr <2 x i64> %7414, <i64 63, i64 63>
  %7417 = add nsw <2 x i64> %7415, %7416
  %7418 = lshr <2 x i64> %7417, <i64 2, i64 2>
  %7419 = trunc <2 x i64> %7418 to <2 x i32>
  %7420 = or i64 %7279, 30
  %7421 = getelementptr inbounds i32, i32* %1, i64 %7420
  %7422 = bitcast i32* %7421 to <2 x i32>*
  store <2 x i32> %7419, <2 x i32>* %7422, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %7211) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %7210) #6
  %7423 = add nuw nsw i64 %7246, 1
  %7424 = icmp eq i64 %7423, 32
  br i1 %7424, label %11292, label %7245

7425:                                             ; preds = %7172
  %7426 = bitcast <2 x i64> %6790 to <4 x i32>
  %7427 = shufflevector <4 x i32> %7426, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %7428 = bitcast <4 x i32> %7427 to <2 x i64>
  %7429 = bitcast <2 x i64> %6795 to <4 x i32>
  %7430 = shufflevector <4 x i32> %7429, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %7431 = bitcast <4 x i32> %7430 to <2 x i64>
  %7432 = shufflevector <2 x i64> %7428, <2 x i64> %7431, <2 x i32> <i32 0, i32 2>
  %7433 = bitcast <2 x i64> %6800 to <4 x i32>
  %7434 = shufflevector <4 x i32> %7433, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %7435 = bitcast <4 x i32> %7434 to <2 x i64>
  %7436 = bitcast <2 x i64> %6805 to <4 x i32>
  %7437 = shufflevector <4 x i32> %7436, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %7438 = bitcast <4 x i32> %7437 to <2 x i64>
  %7439 = shufflevector <2 x i64> %7435, <2 x i64> %7438, <2 x i32> <i32 0, i32 2>
  %7440 = bitcast <2 x i64> %6810 to <4 x i32>
  %7441 = shufflevector <4 x i32> %7440, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %7442 = bitcast <4 x i32> %7441 to <2 x i64>
  %7443 = bitcast <2 x i64> %6815 to <4 x i32>
  %7444 = shufflevector <4 x i32> %7443, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %7445 = bitcast <4 x i32> %7444 to <2 x i64>
  %7446 = shufflevector <2 x i64> %7442, <2 x i64> %7445, <2 x i32> <i32 0, i32 2>
  %7447 = bitcast <2 x i64> %6820 to <4 x i32>
  %7448 = shufflevector <4 x i32> %7447, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %7449 = bitcast <4 x i32> %7448 to <2 x i64>
  %7450 = bitcast <2 x i64> %6825 to <4 x i32>
  %7451 = shufflevector <4 x i32> %7450, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %7452 = bitcast <4 x i32> %7451 to <2 x i64>
  %7453 = shufflevector <2 x i64> %7449, <2 x i64> %7452, <2 x i32> <i32 0, i32 2>
  %7454 = bitcast <2 x i64> %6830 to <4 x i32>
  %7455 = shufflevector <4 x i32> %7454, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %7456 = bitcast <4 x i32> %7455 to <2 x i64>
  %7457 = bitcast <2 x i64> %6835 to <4 x i32>
  %7458 = shufflevector <4 x i32> %7457, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %7459 = bitcast <4 x i32> %7458 to <2 x i64>
  %7460 = shufflevector <2 x i64> %7456, <2 x i64> %7459, <2 x i32> <i32 0, i32 2>
  %7461 = bitcast <2 x i64> %6840 to <4 x i32>
  %7462 = shufflevector <4 x i32> %7461, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %7463 = bitcast <4 x i32> %7462 to <2 x i64>
  %7464 = bitcast <2 x i64> %6845 to <4 x i32>
  %7465 = shufflevector <4 x i32> %7464, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %7466 = bitcast <4 x i32> %7465 to <2 x i64>
  %7467 = shufflevector <2 x i64> %7463, <2 x i64> %7466, <2 x i32> <i32 0, i32 2>
  %7468 = bitcast <2 x i64> %6850 to <4 x i32>
  %7469 = shufflevector <4 x i32> %7468, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %7470 = bitcast <4 x i32> %7469 to <2 x i64>
  %7471 = bitcast <2 x i64> %6855 to <4 x i32>
  %7472 = shufflevector <4 x i32> %7471, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %7473 = bitcast <4 x i32> %7472 to <2 x i64>
  %7474 = shufflevector <2 x i64> %7470, <2 x i64> %7473, <2 x i32> <i32 0, i32 2>
  %7475 = bitcast <2 x i64> %6860 to <4 x i32>
  %7476 = shufflevector <4 x i32> %7475, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %7477 = bitcast <4 x i32> %7476 to <2 x i64>
  %7478 = bitcast <2 x i64> %6865 to <4 x i32>
  %7479 = shufflevector <4 x i32> %7478, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %7480 = bitcast <4 x i32> %7479 to <2 x i64>
  %7481 = shufflevector <2 x i64> %7477, <2 x i64> %7480, <2 x i32> <i32 0, i32 2>
  %7482 = bitcast <2 x i64> %6868 to <4 x i32>
  %7483 = shufflevector <4 x i32> %7482, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %7484 = bitcast <4 x i32> %7483 to <2 x i64>
  %7485 = bitcast <2 x i64> %6871 to <4 x i32>
  %7486 = shufflevector <4 x i32> %7485, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %7487 = bitcast <4 x i32> %7486 to <2 x i64>
  %7488 = shufflevector <2 x i64> %7484, <2 x i64> %7487, <2 x i32> <i32 0, i32 2>
  %7489 = bitcast <2 x i64> %6874 to <4 x i32>
  %7490 = shufflevector <4 x i32> %7489, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %7491 = bitcast <4 x i32> %7490 to <2 x i64>
  %7492 = bitcast <2 x i64> %6877 to <4 x i32>
  %7493 = shufflevector <4 x i32> %7492, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %7494 = bitcast <4 x i32> %7493 to <2 x i64>
  %7495 = shufflevector <2 x i64> %7491, <2 x i64> %7494, <2 x i32> <i32 0, i32 2>
  %7496 = bitcast <2 x i64> %6880 to <4 x i32>
  %7497 = shufflevector <4 x i32> %7496, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %7498 = bitcast <4 x i32> %7497 to <2 x i64>
  %7499 = bitcast <2 x i64> %6883 to <4 x i32>
  %7500 = shufflevector <4 x i32> %7499, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %7501 = bitcast <4 x i32> %7500 to <2 x i64>
  %7502 = shufflevector <2 x i64> %7498, <2 x i64> %7501, <2 x i32> <i32 0, i32 2>
  %7503 = bitcast <2 x i64> %6886 to <4 x i32>
  %7504 = shufflevector <4 x i32> %7503, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %7505 = bitcast <4 x i32> %7504 to <2 x i64>
  %7506 = bitcast <2 x i64> %6889 to <4 x i32>
  %7507 = shufflevector <4 x i32> %7506, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %7508 = bitcast <4 x i32> %7507 to <2 x i64>
  %7509 = shufflevector <2 x i64> %7505, <2 x i64> %7508, <2 x i32> <i32 0, i32 2>
  %7510 = bitcast <2 x i64> %6892 to <4 x i32>
  %7511 = shufflevector <4 x i32> %7510, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %7512 = bitcast <4 x i32> %7511 to <2 x i64>
  %7513 = bitcast <2 x i64> %6895 to <4 x i32>
  %7514 = shufflevector <4 x i32> %7513, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %7515 = bitcast <4 x i32> %7514 to <2 x i64>
  %7516 = shufflevector <2 x i64> %7512, <2 x i64> %7515, <2 x i32> <i32 0, i32 2>
  %7517 = bitcast <2 x i64> %6898 to <4 x i32>
  %7518 = shufflevector <4 x i32> %7517, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %7519 = bitcast <4 x i32> %7518 to <2 x i64>
  %7520 = bitcast <2 x i64> %6901 to <4 x i32>
  %7521 = shufflevector <4 x i32> %7520, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %7522 = bitcast <4 x i32> %7521 to <2 x i64>
  %7523 = shufflevector <2 x i64> %7519, <2 x i64> %7522, <2 x i32> <i32 0, i32 2>
  %7524 = bitcast <2 x i64> %6904 to <4 x i32>
  %7525 = shufflevector <4 x i32> %7524, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %7526 = bitcast <4 x i32> %7525 to <2 x i64>
  %7527 = bitcast <2 x i64> %6907 to <4 x i32>
  %7528 = shufflevector <4 x i32> %7527, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %7529 = bitcast <4 x i32> %7528 to <2 x i64>
  %7530 = shufflevector <2 x i64> %7526, <2 x i64> %7529, <2 x i32> <i32 0, i32 2>
  %7531 = bitcast <2 x i64> %6910 to <4 x i32>
  %7532 = shufflevector <4 x i32> %7531, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %7533 = bitcast <4 x i32> %7532 to <2 x i64>
  %7534 = bitcast <2 x i64> %6913 to <4 x i32>
  %7535 = shufflevector <4 x i32> %7534, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %7536 = bitcast <4 x i32> %7535 to <2 x i64>
  %7537 = shufflevector <2 x i64> %7533, <2 x i64> %7536, <2 x i32> <i32 0, i32 2>
  %7538 = bitcast <2 x i64> %7432 to <4 x i32>
  %7539 = add <4 x i32> %7538, <i32 8192, i32 8192, i32 8192, i32 8192>
  %7540 = bitcast <2 x i64> %7439 to <4 x i32>
  %7541 = add <4 x i32> %7540, <i32 8192, i32 8192, i32 8192, i32 8192>
  %7542 = bitcast <2 x i64> %7446 to <4 x i32>
  %7543 = add <4 x i32> %7542, <i32 8192, i32 8192, i32 8192, i32 8192>
  %7544 = bitcast <2 x i64> %7453 to <4 x i32>
  %7545 = add <4 x i32> %7544, <i32 8192, i32 8192, i32 8192, i32 8192>
  %7546 = bitcast <2 x i64> %7460 to <4 x i32>
  %7547 = add <4 x i32> %7546, <i32 8192, i32 8192, i32 8192, i32 8192>
  %7548 = bitcast <2 x i64> %7467 to <4 x i32>
  %7549 = add <4 x i32> %7548, <i32 8192, i32 8192, i32 8192, i32 8192>
  %7550 = bitcast <2 x i64> %7474 to <4 x i32>
  %7551 = add <4 x i32> %7550, <i32 8192, i32 8192, i32 8192, i32 8192>
  %7552 = bitcast <2 x i64> %7481 to <4 x i32>
  %7553 = add <4 x i32> %7552, <i32 8192, i32 8192, i32 8192, i32 8192>
  %7554 = bitcast <2 x i64> %7488 to <4 x i32>
  %7555 = add <4 x i32> %7554, <i32 8192, i32 8192, i32 8192, i32 8192>
  %7556 = bitcast <2 x i64> %7495 to <4 x i32>
  %7557 = add <4 x i32> %7556, <i32 8192, i32 8192, i32 8192, i32 8192>
  %7558 = bitcast <2 x i64> %7502 to <4 x i32>
  %7559 = add <4 x i32> %7558, <i32 8192, i32 8192, i32 8192, i32 8192>
  %7560 = bitcast <2 x i64> %7509 to <4 x i32>
  %7561 = add <4 x i32> %7560, <i32 8192, i32 8192, i32 8192, i32 8192>
  %7562 = bitcast <2 x i64> %7516 to <4 x i32>
  %7563 = add <4 x i32> %7562, <i32 8192, i32 8192, i32 8192, i32 8192>
  %7564 = bitcast <2 x i64> %7523 to <4 x i32>
  %7565 = add <4 x i32> %7564, <i32 8192, i32 8192, i32 8192, i32 8192>
  %7566 = bitcast <2 x i64> %7530 to <4 x i32>
  %7567 = add <4 x i32> %7566, <i32 8192, i32 8192, i32 8192, i32 8192>
  %7568 = bitcast <2 x i64> %7537 to <4 x i32>
  %7569 = add <4 x i32> %7568, <i32 8192, i32 8192, i32 8192, i32 8192>
  %7570 = ashr <4 x i32> %7539, <i32 14, i32 14, i32 14, i32 14>
  %7571 = ashr <4 x i32> %7541, <i32 14, i32 14, i32 14, i32 14>
  %7572 = ashr <4 x i32> %7543, <i32 14, i32 14, i32 14, i32 14>
  %7573 = ashr <4 x i32> %7545, <i32 14, i32 14, i32 14, i32 14>
  %7574 = ashr <4 x i32> %7547, <i32 14, i32 14, i32 14, i32 14>
  %7575 = ashr <4 x i32> %7549, <i32 14, i32 14, i32 14, i32 14>
  %7576 = ashr <4 x i32> %7551, <i32 14, i32 14, i32 14, i32 14>
  %7577 = ashr <4 x i32> %7553, <i32 14, i32 14, i32 14, i32 14>
  %7578 = ashr <4 x i32> %7555, <i32 14, i32 14, i32 14, i32 14>
  %7579 = ashr <4 x i32> %7557, <i32 14, i32 14, i32 14, i32 14>
  %7580 = ashr <4 x i32> %7559, <i32 14, i32 14, i32 14, i32 14>
  %7581 = ashr <4 x i32> %7561, <i32 14, i32 14, i32 14, i32 14>
  %7582 = ashr <4 x i32> %7563, <i32 14, i32 14, i32 14, i32 14>
  %7583 = ashr <4 x i32> %7565, <i32 14, i32 14, i32 14, i32 14>
  %7584 = ashr <4 x i32> %7567, <i32 14, i32 14, i32 14, i32 14>
  %7585 = ashr <4 x i32> %7569, <i32 14, i32 14, i32 14, i32 14>
  %7586 = shufflevector <4 x i32> %6738, <4 x i32> %6752, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %7587 = bitcast <4 x i32> %7586 to <2 x i64>
  %7588 = shufflevector <4 x i32> %6738, <4 x i32> %6752, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %7589 = bitcast <4 x i32> %7588 to <2 x i64>
  %7590 = shufflevector <4 x i32> %6739, <4 x i32> %6753, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %7591 = bitcast <4 x i32> %7590 to <2 x i64>
  %7592 = shufflevector <4 x i32> %6739, <4 x i32> %6753, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %7593 = bitcast <4 x i32> %7592 to <2 x i64>
  %7594 = shufflevector <4 x i32> %6740, <4 x i32> %6750, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %7595 = bitcast <4 x i32> %7594 to <2 x i64>
  %7596 = shufflevector <4 x i32> %6740, <4 x i32> %6750, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %7597 = bitcast <4 x i32> %7596 to <2 x i64>
  %7598 = shufflevector <4 x i32> %6741, <4 x i32> %6751, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %7599 = bitcast <4 x i32> %7598 to <2 x i64>
  %7600 = shufflevector <4 x i32> %6741, <4 x i32> %6751, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %7601 = bitcast <4 x i32> %7600 to <2 x i64>
  %7602 = shufflevector <4 x i32> %6742, <4 x i32> %6748, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %7603 = bitcast <4 x i32> %7602 to <2 x i64>
  %7604 = shufflevector <4 x i32> %6742, <4 x i32> %6748, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %7605 = bitcast <4 x i32> %7604 to <2 x i64>
  %7606 = shufflevector <4 x i32> %6743, <4 x i32> %6749, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %7607 = bitcast <4 x i32> %7606 to <2 x i64>
  %7608 = shufflevector <4 x i32> %6743, <4 x i32> %6749, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %7609 = bitcast <4 x i32> %7608 to <2 x i64>
  %7610 = shufflevector <4 x i32> %6744, <4 x i32> %6746, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %7611 = bitcast <4 x i32> %7610 to <2 x i64>
  %7612 = shufflevector <4 x i32> %6744, <4 x i32> %6746, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %7613 = bitcast <4 x i32> %7612 to <2 x i64>
  %7614 = shufflevector <4 x i32> %6745, <4 x i32> %6747, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %7615 = bitcast <4 x i32> %7614 to <2 x i64>
  %7616 = shufflevector <4 x i32> %6745, <4 x i32> %6747, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %7617 = bitcast <4 x i32> %7616 to <2 x i64>
  %7618 = and <2 x i64> %7587, <i64 4294967295, i64 4294967295>
  %7619 = mul nuw nsw <2 x i64> %7618, <i64 1606, i64 1606>
  %7620 = lshr <2 x i64> %7587, <i64 32, i64 32>
  %7621 = mul nuw nsw <2 x i64> %7620, <i64 16305, i64 16305>
  %7622 = add nuw nsw <2 x i64> %7621, %7619
  %7623 = and <2 x i64> %7589, <i64 4294967295, i64 4294967295>
  %7624 = mul nuw nsw <2 x i64> %7623, <i64 1606, i64 1606>
  %7625 = lshr <2 x i64> %7589, <i64 32, i64 32>
  %7626 = mul nuw nsw <2 x i64> %7625, <i64 16305, i64 16305>
  %7627 = add nuw nsw <2 x i64> %7626, %7624
  %7628 = and <2 x i64> %7591, <i64 4294967295, i64 4294967295>
  %7629 = mul nuw nsw <2 x i64> %7628, <i64 1606, i64 1606>
  %7630 = lshr <2 x i64> %7591, <i64 32, i64 32>
  %7631 = mul nuw nsw <2 x i64> %7630, <i64 16305, i64 16305>
  %7632 = add nuw nsw <2 x i64> %7631, %7629
  %7633 = and <2 x i64> %7593, <i64 4294967295, i64 4294967295>
  %7634 = mul nuw nsw <2 x i64> %7633, <i64 1606, i64 1606>
  %7635 = lshr <2 x i64> %7593, <i64 32, i64 32>
  %7636 = mul nuw nsw <2 x i64> %7635, <i64 16305, i64 16305>
  %7637 = add nuw nsw <2 x i64> %7636, %7634
  %7638 = and <2 x i64> %7595, <i64 4294967295, i64 4294967295>
  %7639 = mul nuw nsw <2 x i64> %7638, <i64 12665, i64 12665>
  %7640 = lshr <2 x i64> %7595, <i64 32, i64 32>
  %7641 = mul nuw nsw <2 x i64> %7640, <i64 10394, i64 10394>
  %7642 = add nuw nsw <2 x i64> %7641, %7639
  %7643 = and <2 x i64> %7597, <i64 4294967295, i64 4294967295>
  %7644 = mul nuw nsw <2 x i64> %7643, <i64 12665, i64 12665>
  %7645 = lshr <2 x i64> %7597, <i64 32, i64 32>
  %7646 = mul nuw nsw <2 x i64> %7645, <i64 10394, i64 10394>
  %7647 = add nuw nsw <2 x i64> %7646, %7644
  %7648 = and <2 x i64> %7599, <i64 4294967295, i64 4294967295>
  %7649 = mul nuw nsw <2 x i64> %7648, <i64 12665, i64 12665>
  %7650 = lshr <2 x i64> %7599, <i64 32, i64 32>
  %7651 = mul nuw nsw <2 x i64> %7650, <i64 10394, i64 10394>
  %7652 = add nuw nsw <2 x i64> %7651, %7649
  %7653 = and <2 x i64> %7601, <i64 4294967295, i64 4294967295>
  %7654 = mul nuw nsw <2 x i64> %7653, <i64 12665, i64 12665>
  %7655 = lshr <2 x i64> %7601, <i64 32, i64 32>
  %7656 = mul nuw nsw <2 x i64> %7655, <i64 10394, i64 10394>
  %7657 = add nuw nsw <2 x i64> %7656, %7654
  %7658 = and <2 x i64> %7603, <i64 4294967295, i64 4294967295>
  %7659 = mul nuw nsw <2 x i64> %7658, <i64 7723, i64 7723>
  %7660 = lshr <2 x i64> %7603, <i64 32, i64 32>
  %7661 = mul nuw nsw <2 x i64> %7660, <i64 14449, i64 14449>
  %7662 = add nuw nsw <2 x i64> %7661, %7659
  %7663 = and <2 x i64> %7605, <i64 4294967295, i64 4294967295>
  %7664 = mul nuw nsw <2 x i64> %7663, <i64 7723, i64 7723>
  %7665 = lshr <2 x i64> %7605, <i64 32, i64 32>
  %7666 = mul nuw nsw <2 x i64> %7665, <i64 14449, i64 14449>
  %7667 = add nuw nsw <2 x i64> %7666, %7664
  %7668 = and <2 x i64> %7607, <i64 4294967295, i64 4294967295>
  %7669 = mul nuw nsw <2 x i64> %7668, <i64 7723, i64 7723>
  %7670 = lshr <2 x i64> %7607, <i64 32, i64 32>
  %7671 = mul nuw nsw <2 x i64> %7670, <i64 14449, i64 14449>
  %7672 = add nuw nsw <2 x i64> %7671, %7669
  %7673 = and <2 x i64> %7609, <i64 4294967295, i64 4294967295>
  %7674 = mul nuw nsw <2 x i64> %7673, <i64 7723, i64 7723>
  %7675 = lshr <2 x i64> %7609, <i64 32, i64 32>
  %7676 = mul nuw nsw <2 x i64> %7675, <i64 14449, i64 14449>
  %7677 = add nuw nsw <2 x i64> %7676, %7674
  %7678 = and <2 x i64> %7611, <i64 4294967295, i64 4294967295>
  %7679 = mul nuw nsw <2 x i64> %7678, <i64 15679, i64 15679>
  %7680 = lshr <2 x i64> %7611, <i64 32, i64 32>
  %7681 = mul nuw nsw <2 x i64> %7680, <i64 4756, i64 4756>
  %7682 = add nuw nsw <2 x i64> %7681, %7679
  %7683 = and <2 x i64> %7613, <i64 4294967295, i64 4294967295>
  %7684 = mul nuw nsw <2 x i64> %7683, <i64 15679, i64 15679>
  %7685 = lshr <2 x i64> %7613, <i64 32, i64 32>
  %7686 = mul nuw nsw <2 x i64> %7685, <i64 4756, i64 4756>
  %7687 = add nuw nsw <2 x i64> %7686, %7684
  %7688 = and <2 x i64> %7615, <i64 4294967295, i64 4294967295>
  %7689 = mul nuw nsw <2 x i64> %7688, <i64 15679, i64 15679>
  %7690 = lshr <2 x i64> %7615, <i64 32, i64 32>
  %7691 = mul nuw nsw <2 x i64> %7690, <i64 4756, i64 4756>
  %7692 = add nuw nsw <2 x i64> %7691, %7689
  %7693 = and <2 x i64> %7617, <i64 4294967295, i64 4294967295>
  %7694 = mul nuw nsw <2 x i64> %7693, <i64 15679, i64 15679>
  %7695 = lshr <2 x i64> %7617, <i64 32, i64 32>
  %7696 = mul nuw nsw <2 x i64> %7695, <i64 4756, i64 4756>
  %7697 = add nuw nsw <2 x i64> %7696, %7694
  %7698 = mul nuw <2 x i64> %7678, <i64 4294962540, i64 4294962540>
  %7699 = mul nuw nsw <2 x i64> %7680, <i64 15679, i64 15679>
  %7700 = add <2 x i64> %7699, %7698
  %7701 = mul nuw <2 x i64> %7683, <i64 4294962540, i64 4294962540>
  %7702 = mul nuw nsw <2 x i64> %7685, <i64 15679, i64 15679>
  %7703 = add <2 x i64> %7702, %7701
  %7704 = mul nuw <2 x i64> %7688, <i64 4294962540, i64 4294962540>
  %7705 = mul nuw nsw <2 x i64> %7690, <i64 15679, i64 15679>
  %7706 = add <2 x i64> %7705, %7704
  %7707 = mul nuw <2 x i64> %7693, <i64 4294962540, i64 4294962540>
  %7708 = mul nuw nsw <2 x i64> %7695, <i64 15679, i64 15679>
  %7709 = add <2 x i64> %7708, %7707
  %7710 = mul nuw <2 x i64> %7658, <i64 4294952847, i64 4294952847>
  %7711 = mul nuw nsw <2 x i64> %7660, <i64 7723, i64 7723>
  %7712 = add <2 x i64> %7711, %7710
  %7713 = mul nuw <2 x i64> %7663, <i64 4294952847, i64 4294952847>
  %7714 = mul nuw nsw <2 x i64> %7665, <i64 7723, i64 7723>
  %7715 = add <2 x i64> %7714, %7713
  %7716 = mul nuw <2 x i64> %7668, <i64 4294952847, i64 4294952847>
  %7717 = mul nuw nsw <2 x i64> %7670, <i64 7723, i64 7723>
  %7718 = add <2 x i64> %7717, %7716
  %7719 = mul nuw <2 x i64> %7673, <i64 4294952847, i64 4294952847>
  %7720 = mul nuw nsw <2 x i64> %7675, <i64 7723, i64 7723>
  %7721 = add <2 x i64> %7720, %7719
  %7722 = mul nuw <2 x i64> %7638, <i64 4294956902, i64 4294956902>
  %7723 = mul nuw nsw <2 x i64> %7640, <i64 12665, i64 12665>
  %7724 = add <2 x i64> %7723, %7722
  %7725 = mul nuw <2 x i64> %7643, <i64 4294956902, i64 4294956902>
  %7726 = mul nuw nsw <2 x i64> %7645, <i64 12665, i64 12665>
  %7727 = add <2 x i64> %7726, %7725
  %7728 = mul nuw <2 x i64> %7648, <i64 4294956902, i64 4294956902>
  %7729 = mul nuw nsw <2 x i64> %7650, <i64 12665, i64 12665>
  %7730 = add <2 x i64> %7729, %7728
  %7731 = mul nuw <2 x i64> %7653, <i64 4294956902, i64 4294956902>
  %7732 = mul nuw nsw <2 x i64> %7655, <i64 12665, i64 12665>
  %7733 = add <2 x i64> %7732, %7731
  %7734 = mul nuw <2 x i64> %7618, <i64 4294950991, i64 4294950991>
  %7735 = mul nuw nsw <2 x i64> %7620, <i64 1606, i64 1606>
  %7736 = add <2 x i64> %7735, %7734
  %7737 = mul nuw <2 x i64> %7623, <i64 4294950991, i64 4294950991>
  %7738 = mul nuw nsw <2 x i64> %7625, <i64 1606, i64 1606>
  %7739 = add <2 x i64> %7738, %7737
  %7740 = mul nuw <2 x i64> %7628, <i64 4294950991, i64 4294950991>
  %7741 = mul nuw nsw <2 x i64> %7630, <i64 1606, i64 1606>
  %7742 = add <2 x i64> %7741, %7740
  %7743 = mul nuw <2 x i64> %7633, <i64 4294950991, i64 4294950991>
  %7744 = mul nuw nsw <2 x i64> %7635, <i64 1606, i64 1606>
  %7745 = add <2 x i64> %7744, %7743
  %7746 = shl nuw nsw <2 x i64> %7622, <i64 1, i64 1>
  %7747 = shl nuw nsw <2 x i64> %7627, <i64 1, i64 1>
  %7748 = shl nuw nsw <2 x i64> %7632, <i64 1, i64 1>
  %7749 = shl nuw nsw <2 x i64> %7637, <i64 1, i64 1>
  %7750 = bitcast <2 x i64> %7746 to <4 x i32>
  %7751 = shufflevector <4 x i32> %7750, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7752 = bitcast <4 x i32> %7751 to <2 x i64>
  %7753 = bitcast <2 x i64> %7747 to <4 x i32>
  %7754 = shufflevector <4 x i32> %7753, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7755 = bitcast <4 x i32> %7754 to <2 x i64>
  %7756 = bitcast <2 x i64> %7748 to <4 x i32>
  %7757 = shufflevector <4 x i32> %7756, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7758 = bitcast <4 x i32> %7757 to <2 x i64>
  %7759 = bitcast <2 x i64> %7749 to <4 x i32>
  %7760 = shufflevector <4 x i32> %7759, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7761 = bitcast <4 x i32> %7760 to <2 x i64>
  %7762 = shufflevector <2 x i64> %7752, <2 x i64> %7755, <2 x i32> <i32 0, i32 2>
  %7763 = shufflevector <2 x i64> %7758, <2 x i64> %7761, <2 x i32> <i32 0, i32 2>
  %7764 = bitcast <2 x i64> %7762 to <4 x i32>
  %7765 = bitcast <2 x i64> %7763 to <4 x i32>
  %7766 = add <4 x i32> %7764, <i32 1, i32 1, i32 1, i32 1>
  %7767 = icmp ugt <4 x i32> %7766, <i32 1, i32 1, i32 1, i32 1>
  %7768 = sext <4 x i1> %7767 to <4 x i32>
  %7769 = bitcast <4 x i32> %7768 to <16 x i8>
  %7770 = icmp slt <16 x i8> %7769, zeroinitializer
  %7771 = bitcast <16 x i1> %7770 to i16
  %7772 = zext i16 %7771 to i32
  %7773 = add <4 x i32> %7765, <i32 1, i32 1, i32 1, i32 1>
  %7774 = icmp ugt <4 x i32> %7773, <i32 1, i32 1, i32 1, i32 1>
  %7775 = sext <4 x i1> %7774 to <4 x i32>
  %7776 = bitcast <4 x i32> %7775 to <16 x i8>
  %7777 = icmp slt <16 x i8> %7776, zeroinitializer
  %7778 = bitcast <16 x i1> %7777 to i16
  %7779 = zext i16 %7778 to i32
  %7780 = sub nsw i32 0, %7772
  %7781 = icmp eq i32 %7779, %7780
  br i1 %7781, label %7782, label %8041

7782:                                             ; preds = %7425
  %7783 = shl nuw nsw <2 x i64> %7642, <i64 1, i64 1>
  %7784 = shl nuw nsw <2 x i64> %7647, <i64 1, i64 1>
  %7785 = shl nuw nsw <2 x i64> %7652, <i64 1, i64 1>
  %7786 = shl nuw nsw <2 x i64> %7657, <i64 1, i64 1>
  %7787 = bitcast <2 x i64> %7783 to <4 x i32>
  %7788 = shufflevector <4 x i32> %7787, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7789 = bitcast <4 x i32> %7788 to <2 x i64>
  %7790 = bitcast <2 x i64> %7784 to <4 x i32>
  %7791 = shufflevector <4 x i32> %7790, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7792 = bitcast <4 x i32> %7791 to <2 x i64>
  %7793 = bitcast <2 x i64> %7785 to <4 x i32>
  %7794 = shufflevector <4 x i32> %7793, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7795 = bitcast <4 x i32> %7794 to <2 x i64>
  %7796 = bitcast <2 x i64> %7786 to <4 x i32>
  %7797 = shufflevector <4 x i32> %7796, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7798 = bitcast <4 x i32> %7797 to <2 x i64>
  %7799 = shufflevector <2 x i64> %7789, <2 x i64> %7792, <2 x i32> <i32 0, i32 2>
  %7800 = shufflevector <2 x i64> %7795, <2 x i64> %7798, <2 x i32> <i32 0, i32 2>
  %7801 = bitcast <2 x i64> %7799 to <4 x i32>
  %7802 = bitcast <2 x i64> %7800 to <4 x i32>
  %7803 = add <4 x i32> %7801, <i32 1, i32 1, i32 1, i32 1>
  %7804 = icmp ugt <4 x i32> %7803, <i32 1, i32 1, i32 1, i32 1>
  %7805 = sext <4 x i1> %7804 to <4 x i32>
  %7806 = bitcast <4 x i32> %7805 to <16 x i8>
  %7807 = icmp slt <16 x i8> %7806, zeroinitializer
  %7808 = bitcast <16 x i1> %7807 to i16
  %7809 = zext i16 %7808 to i32
  %7810 = add <4 x i32> %7802, <i32 1, i32 1, i32 1, i32 1>
  %7811 = icmp ugt <4 x i32> %7810, <i32 1, i32 1, i32 1, i32 1>
  %7812 = sext <4 x i1> %7811 to <4 x i32>
  %7813 = bitcast <4 x i32> %7812 to <16 x i8>
  %7814 = icmp slt <16 x i8> %7813, zeroinitializer
  %7815 = bitcast <16 x i1> %7814 to i16
  %7816 = zext i16 %7815 to i32
  %7817 = sub nsw i32 0, %7809
  %7818 = icmp eq i32 %7816, %7817
  br i1 %7818, label %7819, label %8041

7819:                                             ; preds = %7782
  %7820 = shl nuw nsw <2 x i64> %7662, <i64 1, i64 1>
  %7821 = shl nuw nsw <2 x i64> %7667, <i64 1, i64 1>
  %7822 = shl nuw nsw <2 x i64> %7672, <i64 1, i64 1>
  %7823 = shl nuw nsw <2 x i64> %7677, <i64 1, i64 1>
  %7824 = bitcast <2 x i64> %7820 to <4 x i32>
  %7825 = shufflevector <4 x i32> %7824, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7826 = bitcast <4 x i32> %7825 to <2 x i64>
  %7827 = bitcast <2 x i64> %7821 to <4 x i32>
  %7828 = shufflevector <4 x i32> %7827, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7829 = bitcast <4 x i32> %7828 to <2 x i64>
  %7830 = bitcast <2 x i64> %7822 to <4 x i32>
  %7831 = shufflevector <4 x i32> %7830, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7832 = bitcast <4 x i32> %7831 to <2 x i64>
  %7833 = bitcast <2 x i64> %7823 to <4 x i32>
  %7834 = shufflevector <4 x i32> %7833, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7835 = bitcast <4 x i32> %7834 to <2 x i64>
  %7836 = shufflevector <2 x i64> %7826, <2 x i64> %7829, <2 x i32> <i32 0, i32 2>
  %7837 = shufflevector <2 x i64> %7832, <2 x i64> %7835, <2 x i32> <i32 0, i32 2>
  %7838 = bitcast <2 x i64> %7836 to <4 x i32>
  %7839 = bitcast <2 x i64> %7837 to <4 x i32>
  %7840 = add <4 x i32> %7838, <i32 1, i32 1, i32 1, i32 1>
  %7841 = icmp ugt <4 x i32> %7840, <i32 1, i32 1, i32 1, i32 1>
  %7842 = sext <4 x i1> %7841 to <4 x i32>
  %7843 = bitcast <4 x i32> %7842 to <16 x i8>
  %7844 = icmp slt <16 x i8> %7843, zeroinitializer
  %7845 = bitcast <16 x i1> %7844 to i16
  %7846 = zext i16 %7845 to i32
  %7847 = add <4 x i32> %7839, <i32 1, i32 1, i32 1, i32 1>
  %7848 = icmp ugt <4 x i32> %7847, <i32 1, i32 1, i32 1, i32 1>
  %7849 = sext <4 x i1> %7848 to <4 x i32>
  %7850 = bitcast <4 x i32> %7849 to <16 x i8>
  %7851 = icmp slt <16 x i8> %7850, zeroinitializer
  %7852 = bitcast <16 x i1> %7851 to i16
  %7853 = zext i16 %7852 to i32
  %7854 = sub nsw i32 0, %7846
  %7855 = icmp eq i32 %7853, %7854
  br i1 %7855, label %7856, label %8041

7856:                                             ; preds = %7819
  %7857 = shl nuw nsw <2 x i64> %7682, <i64 1, i64 1>
  %7858 = shl nuw nsw <2 x i64> %7687, <i64 1, i64 1>
  %7859 = shl nuw nsw <2 x i64> %7692, <i64 1, i64 1>
  %7860 = shl nuw nsw <2 x i64> %7697, <i64 1, i64 1>
  %7861 = bitcast <2 x i64> %7857 to <4 x i32>
  %7862 = shufflevector <4 x i32> %7861, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7863 = bitcast <4 x i32> %7862 to <2 x i64>
  %7864 = bitcast <2 x i64> %7858 to <4 x i32>
  %7865 = shufflevector <4 x i32> %7864, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7866 = bitcast <4 x i32> %7865 to <2 x i64>
  %7867 = bitcast <2 x i64> %7859 to <4 x i32>
  %7868 = shufflevector <4 x i32> %7867, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7869 = bitcast <4 x i32> %7868 to <2 x i64>
  %7870 = bitcast <2 x i64> %7860 to <4 x i32>
  %7871 = shufflevector <4 x i32> %7870, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7872 = bitcast <4 x i32> %7871 to <2 x i64>
  %7873 = shufflevector <2 x i64> %7863, <2 x i64> %7866, <2 x i32> <i32 0, i32 2>
  %7874 = shufflevector <2 x i64> %7869, <2 x i64> %7872, <2 x i32> <i32 0, i32 2>
  %7875 = bitcast <2 x i64> %7873 to <4 x i32>
  %7876 = bitcast <2 x i64> %7874 to <4 x i32>
  %7877 = add <4 x i32> %7875, <i32 1, i32 1, i32 1, i32 1>
  %7878 = icmp ugt <4 x i32> %7877, <i32 1, i32 1, i32 1, i32 1>
  %7879 = sext <4 x i1> %7878 to <4 x i32>
  %7880 = bitcast <4 x i32> %7879 to <16 x i8>
  %7881 = icmp slt <16 x i8> %7880, zeroinitializer
  %7882 = bitcast <16 x i1> %7881 to i16
  %7883 = zext i16 %7882 to i32
  %7884 = add <4 x i32> %7876, <i32 1, i32 1, i32 1, i32 1>
  %7885 = icmp ugt <4 x i32> %7884, <i32 1, i32 1, i32 1, i32 1>
  %7886 = sext <4 x i1> %7885 to <4 x i32>
  %7887 = bitcast <4 x i32> %7886 to <16 x i8>
  %7888 = icmp slt <16 x i8> %7887, zeroinitializer
  %7889 = bitcast <16 x i1> %7888 to i16
  %7890 = zext i16 %7889 to i32
  %7891 = sub nsw i32 0, %7883
  %7892 = icmp eq i32 %7890, %7891
  br i1 %7892, label %7893, label %8041

7893:                                             ; preds = %7856
  %7894 = shl <2 x i64> %7700, <i64 1, i64 1>
  %7895 = shl <2 x i64> %7703, <i64 1, i64 1>
  %7896 = shl <2 x i64> %7706, <i64 1, i64 1>
  %7897 = shl <2 x i64> %7709, <i64 1, i64 1>
  %7898 = bitcast <2 x i64> %7894 to <4 x i32>
  %7899 = shufflevector <4 x i32> %7898, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7900 = bitcast <4 x i32> %7899 to <2 x i64>
  %7901 = bitcast <2 x i64> %7895 to <4 x i32>
  %7902 = shufflevector <4 x i32> %7901, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7903 = bitcast <4 x i32> %7902 to <2 x i64>
  %7904 = bitcast <2 x i64> %7896 to <4 x i32>
  %7905 = shufflevector <4 x i32> %7904, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7906 = bitcast <4 x i32> %7905 to <2 x i64>
  %7907 = bitcast <2 x i64> %7897 to <4 x i32>
  %7908 = shufflevector <4 x i32> %7907, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7909 = bitcast <4 x i32> %7908 to <2 x i64>
  %7910 = shufflevector <2 x i64> %7900, <2 x i64> %7903, <2 x i32> <i32 0, i32 2>
  %7911 = shufflevector <2 x i64> %7906, <2 x i64> %7909, <2 x i32> <i32 0, i32 2>
  %7912 = bitcast <2 x i64> %7910 to <4 x i32>
  %7913 = bitcast <2 x i64> %7911 to <4 x i32>
  %7914 = add <4 x i32> %7912, <i32 1, i32 1, i32 1, i32 1>
  %7915 = icmp ugt <4 x i32> %7914, <i32 1, i32 1, i32 1, i32 1>
  %7916 = sext <4 x i1> %7915 to <4 x i32>
  %7917 = bitcast <4 x i32> %7916 to <16 x i8>
  %7918 = icmp slt <16 x i8> %7917, zeroinitializer
  %7919 = bitcast <16 x i1> %7918 to i16
  %7920 = zext i16 %7919 to i32
  %7921 = add <4 x i32> %7913, <i32 1, i32 1, i32 1, i32 1>
  %7922 = icmp ugt <4 x i32> %7921, <i32 1, i32 1, i32 1, i32 1>
  %7923 = sext <4 x i1> %7922 to <4 x i32>
  %7924 = bitcast <4 x i32> %7923 to <16 x i8>
  %7925 = icmp slt <16 x i8> %7924, zeroinitializer
  %7926 = bitcast <16 x i1> %7925 to i16
  %7927 = zext i16 %7926 to i32
  %7928 = sub nsw i32 0, %7920
  %7929 = icmp eq i32 %7927, %7928
  br i1 %7929, label %7930, label %8041

7930:                                             ; preds = %7893
  %7931 = shl <2 x i64> %7712, <i64 1, i64 1>
  %7932 = shl <2 x i64> %7715, <i64 1, i64 1>
  %7933 = shl <2 x i64> %7718, <i64 1, i64 1>
  %7934 = shl <2 x i64> %7721, <i64 1, i64 1>
  %7935 = bitcast <2 x i64> %7931 to <4 x i32>
  %7936 = shufflevector <4 x i32> %7935, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7937 = bitcast <4 x i32> %7936 to <2 x i64>
  %7938 = bitcast <2 x i64> %7932 to <4 x i32>
  %7939 = shufflevector <4 x i32> %7938, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7940 = bitcast <4 x i32> %7939 to <2 x i64>
  %7941 = bitcast <2 x i64> %7933 to <4 x i32>
  %7942 = shufflevector <4 x i32> %7941, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7943 = bitcast <4 x i32> %7942 to <2 x i64>
  %7944 = bitcast <2 x i64> %7934 to <4 x i32>
  %7945 = shufflevector <4 x i32> %7944, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7946 = bitcast <4 x i32> %7945 to <2 x i64>
  %7947 = shufflevector <2 x i64> %7937, <2 x i64> %7940, <2 x i32> <i32 0, i32 2>
  %7948 = shufflevector <2 x i64> %7943, <2 x i64> %7946, <2 x i32> <i32 0, i32 2>
  %7949 = bitcast <2 x i64> %7947 to <4 x i32>
  %7950 = bitcast <2 x i64> %7948 to <4 x i32>
  %7951 = add <4 x i32> %7949, <i32 1, i32 1, i32 1, i32 1>
  %7952 = icmp ugt <4 x i32> %7951, <i32 1, i32 1, i32 1, i32 1>
  %7953 = sext <4 x i1> %7952 to <4 x i32>
  %7954 = bitcast <4 x i32> %7953 to <16 x i8>
  %7955 = icmp slt <16 x i8> %7954, zeroinitializer
  %7956 = bitcast <16 x i1> %7955 to i16
  %7957 = zext i16 %7956 to i32
  %7958 = add <4 x i32> %7950, <i32 1, i32 1, i32 1, i32 1>
  %7959 = icmp ugt <4 x i32> %7958, <i32 1, i32 1, i32 1, i32 1>
  %7960 = sext <4 x i1> %7959 to <4 x i32>
  %7961 = bitcast <4 x i32> %7960 to <16 x i8>
  %7962 = icmp slt <16 x i8> %7961, zeroinitializer
  %7963 = bitcast <16 x i1> %7962 to i16
  %7964 = zext i16 %7963 to i32
  %7965 = sub nsw i32 0, %7957
  %7966 = icmp eq i32 %7964, %7965
  br i1 %7966, label %7967, label %8041

7967:                                             ; preds = %7930
  %7968 = shl <2 x i64> %7724, <i64 1, i64 1>
  %7969 = shl <2 x i64> %7727, <i64 1, i64 1>
  %7970 = shl <2 x i64> %7730, <i64 1, i64 1>
  %7971 = shl <2 x i64> %7733, <i64 1, i64 1>
  %7972 = bitcast <2 x i64> %7968 to <4 x i32>
  %7973 = shufflevector <4 x i32> %7972, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7974 = bitcast <4 x i32> %7973 to <2 x i64>
  %7975 = bitcast <2 x i64> %7969 to <4 x i32>
  %7976 = shufflevector <4 x i32> %7975, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7977 = bitcast <4 x i32> %7976 to <2 x i64>
  %7978 = bitcast <2 x i64> %7970 to <4 x i32>
  %7979 = shufflevector <4 x i32> %7978, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7980 = bitcast <4 x i32> %7979 to <2 x i64>
  %7981 = bitcast <2 x i64> %7971 to <4 x i32>
  %7982 = shufflevector <4 x i32> %7981, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %7983 = bitcast <4 x i32> %7982 to <2 x i64>
  %7984 = shufflevector <2 x i64> %7974, <2 x i64> %7977, <2 x i32> <i32 0, i32 2>
  %7985 = shufflevector <2 x i64> %7980, <2 x i64> %7983, <2 x i32> <i32 0, i32 2>
  %7986 = bitcast <2 x i64> %7984 to <4 x i32>
  %7987 = bitcast <2 x i64> %7985 to <4 x i32>
  %7988 = add <4 x i32> %7986, <i32 1, i32 1, i32 1, i32 1>
  %7989 = icmp ugt <4 x i32> %7988, <i32 1, i32 1, i32 1, i32 1>
  %7990 = sext <4 x i1> %7989 to <4 x i32>
  %7991 = bitcast <4 x i32> %7990 to <16 x i8>
  %7992 = icmp slt <16 x i8> %7991, zeroinitializer
  %7993 = bitcast <16 x i1> %7992 to i16
  %7994 = zext i16 %7993 to i32
  %7995 = add <4 x i32> %7987, <i32 1, i32 1, i32 1, i32 1>
  %7996 = icmp ugt <4 x i32> %7995, <i32 1, i32 1, i32 1, i32 1>
  %7997 = sext <4 x i1> %7996 to <4 x i32>
  %7998 = bitcast <4 x i32> %7997 to <16 x i8>
  %7999 = icmp slt <16 x i8> %7998, zeroinitializer
  %8000 = bitcast <16 x i1> %7999 to i16
  %8001 = zext i16 %8000 to i32
  %8002 = sub nsw i32 0, %7994
  %8003 = icmp eq i32 %8001, %8002
  br i1 %8003, label %8004, label %8041

8004:                                             ; preds = %7967
  %8005 = shl <2 x i64> %7736, <i64 1, i64 1>
  %8006 = shl <2 x i64> %7739, <i64 1, i64 1>
  %8007 = shl <2 x i64> %7742, <i64 1, i64 1>
  %8008 = shl <2 x i64> %7745, <i64 1, i64 1>
  %8009 = bitcast <2 x i64> %8005 to <4 x i32>
  %8010 = shufflevector <4 x i32> %8009, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %8011 = bitcast <4 x i32> %8010 to <2 x i64>
  %8012 = bitcast <2 x i64> %8006 to <4 x i32>
  %8013 = shufflevector <4 x i32> %8012, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %8014 = bitcast <4 x i32> %8013 to <2 x i64>
  %8015 = bitcast <2 x i64> %8007 to <4 x i32>
  %8016 = shufflevector <4 x i32> %8015, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %8017 = bitcast <4 x i32> %8016 to <2 x i64>
  %8018 = bitcast <2 x i64> %8008 to <4 x i32>
  %8019 = shufflevector <4 x i32> %8018, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %8020 = bitcast <4 x i32> %8019 to <2 x i64>
  %8021 = shufflevector <2 x i64> %8011, <2 x i64> %8014, <2 x i32> <i32 0, i32 2>
  %8022 = shufflevector <2 x i64> %8017, <2 x i64> %8020, <2 x i32> <i32 0, i32 2>
  %8023 = bitcast <2 x i64> %8021 to <4 x i32>
  %8024 = bitcast <2 x i64> %8022 to <4 x i32>
  %8025 = add <4 x i32> %8023, <i32 1, i32 1, i32 1, i32 1>
  %8026 = icmp ugt <4 x i32> %8025, <i32 1, i32 1, i32 1, i32 1>
  %8027 = sext <4 x i1> %8026 to <4 x i32>
  %8028 = bitcast <4 x i32> %8027 to <16 x i8>
  %8029 = icmp slt <16 x i8> %8028, zeroinitializer
  %8030 = bitcast <16 x i1> %8029 to i16
  %8031 = zext i16 %8030 to i32
  %8032 = add <4 x i32> %8024, <i32 1, i32 1, i32 1, i32 1>
  %8033 = icmp ugt <4 x i32> %8032, <i32 1, i32 1, i32 1, i32 1>
  %8034 = sext <4 x i1> %8033 to <4 x i32>
  %8035 = bitcast <4 x i32> %8034 to <16 x i8>
  %8036 = icmp slt <16 x i8> %8035, zeroinitializer
  %8037 = bitcast <16 x i1> %8036 to i16
  %8038 = zext i16 %8037 to i32
  %8039 = sub nsw i32 0, %8031
  %8040 = icmp eq i32 %8038, %8039
  br i1 %8040, label %8257, label %8041

8041:                                             ; preds = %7967, %7930, %7893, %7856, %7819, %7782, %7425, %8004
  %8042 = bitcast [32 x i64]* %4 to i8*
  %8043 = bitcast [32 x i64]* %5 to i8*
  %8044 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %8045 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %8046 = bitcast [32 x i64]* %5 to <2 x i64>*
  %8047 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %8048 = bitcast i64* %8047 to <2 x i64>*
  %8049 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %8050 = bitcast i64* %8049 to <2 x i64>*
  %8051 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %8052 = bitcast i64* %8051 to <2 x i64>*
  %8053 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %8054 = bitcast i64* %8053 to <2 x i64>*
  %8055 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %8056 = bitcast i64* %8055 to <2 x i64>*
  %8057 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %8058 = bitcast i64* %8057 to <2 x i64>*
  %8059 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %8060 = bitcast i64* %8059 to <2 x i64>*
  %8061 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %8062 = bitcast i64* %8061 to <2 x i64>*
  %8063 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %8064 = bitcast i64* %8063 to <2 x i64>*
  %8065 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %8066 = bitcast i64* %8065 to <2 x i64>*
  %8067 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %8068 = bitcast i64* %8067 to <2 x i64>*
  %8069 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %8070 = bitcast i64* %8069 to <2 x i64>*
  %8071 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %8072 = bitcast i64* %8071 to <2 x i64>*
  %8073 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %8074 = bitcast i64* %8073 to <2 x i64>*
  %8075 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %8076 = bitcast i64* %8075 to <2 x i64>*
  br label %8077

8077:                                             ; preds = %8110, %8041
  %8078 = phi i64 [ 0, %8041 ], [ %8255, %8110 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %8042) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %8042, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %8043) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %8043, i8 -86, i64 256, i1 false) #6
  br label %8079

8079:                                             ; preds = %8079, %8077
  %8080 = phi i64 [ 0, %8077 ], [ %8108, %8079 ]
  %8081 = shl i64 %8080, 5
  %8082 = add nuw nsw i64 %8081, %8078
  %8083 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %8082
  %8084 = load i16, i16* %8083, align 2
  %8085 = sext i16 %8084 to i64
  %8086 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %8080
  store i64 %8085, i64* %8086, align 16
  %8087 = or i64 %8080, 1
  %8088 = shl i64 %8087, 5
  %8089 = add nuw nsw i64 %8088, %8078
  %8090 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %8089
  %8091 = load i16, i16* %8090, align 2
  %8092 = sext i16 %8091 to i64
  %8093 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %8087
  store i64 %8092, i64* %8093, align 8
  %8094 = or i64 %8080, 2
  %8095 = shl i64 %8094, 5
  %8096 = add nuw nsw i64 %8095, %8078
  %8097 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %8096
  %8098 = load i16, i16* %8097, align 2
  %8099 = sext i16 %8098 to i64
  %8100 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %8094
  store i64 %8099, i64* %8100, align 16
  %8101 = or i64 %8080, 3
  %8102 = shl i64 %8101, 5
  %8103 = add nuw nsw i64 %8102, %8078
  %8104 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %8103
  %8105 = load i16, i16* %8104, align 2
  %8106 = sext i16 %8105 to i64
  %8107 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %8101
  store i64 %8106, i64* %8107, align 8
  %8108 = add nuw nsw i64 %8080, 4
  %8109 = icmp eq i64 %8108, 32
  br i1 %8109, label %8110, label %8079

8110:                                             ; preds = %8079
  call void @vpx_fdct32(i64* nonnull %8044, i64* nonnull %8045, i32 0) #6
  %8111 = shl i64 %8078, 5
  %8112 = load <2 x i64>, <2 x i64>* %8046, align 16
  %8113 = add nsw <2 x i64> %8112, <i64 1, i64 1>
  %8114 = lshr <2 x i64> %8112, <i64 63, i64 63>
  %8115 = add nsw <2 x i64> %8113, %8114
  %8116 = lshr <2 x i64> %8115, <i64 2, i64 2>
  %8117 = trunc <2 x i64> %8116 to <2 x i32>
  %8118 = getelementptr inbounds i32, i32* %1, i64 %8111
  %8119 = bitcast i32* %8118 to <2 x i32>*
  store <2 x i32> %8117, <2 x i32>* %8119, align 4
  %8120 = load <2 x i64>, <2 x i64>* %8048, align 16
  %8121 = add nsw <2 x i64> %8120, <i64 1, i64 1>
  %8122 = lshr <2 x i64> %8120, <i64 63, i64 63>
  %8123 = add nsw <2 x i64> %8121, %8122
  %8124 = lshr <2 x i64> %8123, <i64 2, i64 2>
  %8125 = trunc <2 x i64> %8124 to <2 x i32>
  %8126 = or i64 %8111, 2
  %8127 = getelementptr inbounds i32, i32* %1, i64 %8126
  %8128 = bitcast i32* %8127 to <2 x i32>*
  store <2 x i32> %8125, <2 x i32>* %8128, align 4
  %8129 = load <2 x i64>, <2 x i64>* %8050, align 16
  %8130 = add nsw <2 x i64> %8129, <i64 1, i64 1>
  %8131 = lshr <2 x i64> %8129, <i64 63, i64 63>
  %8132 = add nsw <2 x i64> %8130, %8131
  %8133 = lshr <2 x i64> %8132, <i64 2, i64 2>
  %8134 = trunc <2 x i64> %8133 to <2 x i32>
  %8135 = or i64 %8111, 4
  %8136 = getelementptr inbounds i32, i32* %1, i64 %8135
  %8137 = bitcast i32* %8136 to <2 x i32>*
  store <2 x i32> %8134, <2 x i32>* %8137, align 4
  %8138 = load <2 x i64>, <2 x i64>* %8052, align 16
  %8139 = add nsw <2 x i64> %8138, <i64 1, i64 1>
  %8140 = lshr <2 x i64> %8138, <i64 63, i64 63>
  %8141 = add nsw <2 x i64> %8139, %8140
  %8142 = lshr <2 x i64> %8141, <i64 2, i64 2>
  %8143 = trunc <2 x i64> %8142 to <2 x i32>
  %8144 = or i64 %8111, 6
  %8145 = getelementptr inbounds i32, i32* %1, i64 %8144
  %8146 = bitcast i32* %8145 to <2 x i32>*
  store <2 x i32> %8143, <2 x i32>* %8146, align 4
  %8147 = load <2 x i64>, <2 x i64>* %8054, align 16
  %8148 = add nsw <2 x i64> %8147, <i64 1, i64 1>
  %8149 = lshr <2 x i64> %8147, <i64 63, i64 63>
  %8150 = add nsw <2 x i64> %8148, %8149
  %8151 = lshr <2 x i64> %8150, <i64 2, i64 2>
  %8152 = trunc <2 x i64> %8151 to <2 x i32>
  %8153 = or i64 %8111, 8
  %8154 = getelementptr inbounds i32, i32* %1, i64 %8153
  %8155 = bitcast i32* %8154 to <2 x i32>*
  store <2 x i32> %8152, <2 x i32>* %8155, align 4
  %8156 = load <2 x i64>, <2 x i64>* %8056, align 16
  %8157 = add nsw <2 x i64> %8156, <i64 1, i64 1>
  %8158 = lshr <2 x i64> %8156, <i64 63, i64 63>
  %8159 = add nsw <2 x i64> %8157, %8158
  %8160 = lshr <2 x i64> %8159, <i64 2, i64 2>
  %8161 = trunc <2 x i64> %8160 to <2 x i32>
  %8162 = or i64 %8111, 10
  %8163 = getelementptr inbounds i32, i32* %1, i64 %8162
  %8164 = bitcast i32* %8163 to <2 x i32>*
  store <2 x i32> %8161, <2 x i32>* %8164, align 4
  %8165 = load <2 x i64>, <2 x i64>* %8058, align 16
  %8166 = add nsw <2 x i64> %8165, <i64 1, i64 1>
  %8167 = lshr <2 x i64> %8165, <i64 63, i64 63>
  %8168 = add nsw <2 x i64> %8166, %8167
  %8169 = lshr <2 x i64> %8168, <i64 2, i64 2>
  %8170 = trunc <2 x i64> %8169 to <2 x i32>
  %8171 = or i64 %8111, 12
  %8172 = getelementptr inbounds i32, i32* %1, i64 %8171
  %8173 = bitcast i32* %8172 to <2 x i32>*
  store <2 x i32> %8170, <2 x i32>* %8173, align 4
  %8174 = load <2 x i64>, <2 x i64>* %8060, align 16
  %8175 = add nsw <2 x i64> %8174, <i64 1, i64 1>
  %8176 = lshr <2 x i64> %8174, <i64 63, i64 63>
  %8177 = add nsw <2 x i64> %8175, %8176
  %8178 = lshr <2 x i64> %8177, <i64 2, i64 2>
  %8179 = trunc <2 x i64> %8178 to <2 x i32>
  %8180 = or i64 %8111, 14
  %8181 = getelementptr inbounds i32, i32* %1, i64 %8180
  %8182 = bitcast i32* %8181 to <2 x i32>*
  store <2 x i32> %8179, <2 x i32>* %8182, align 4
  %8183 = load <2 x i64>, <2 x i64>* %8062, align 16
  %8184 = add nsw <2 x i64> %8183, <i64 1, i64 1>
  %8185 = lshr <2 x i64> %8183, <i64 63, i64 63>
  %8186 = add nsw <2 x i64> %8184, %8185
  %8187 = lshr <2 x i64> %8186, <i64 2, i64 2>
  %8188 = trunc <2 x i64> %8187 to <2 x i32>
  %8189 = or i64 %8111, 16
  %8190 = getelementptr inbounds i32, i32* %1, i64 %8189
  %8191 = bitcast i32* %8190 to <2 x i32>*
  store <2 x i32> %8188, <2 x i32>* %8191, align 4
  %8192 = load <2 x i64>, <2 x i64>* %8064, align 16
  %8193 = add nsw <2 x i64> %8192, <i64 1, i64 1>
  %8194 = lshr <2 x i64> %8192, <i64 63, i64 63>
  %8195 = add nsw <2 x i64> %8193, %8194
  %8196 = lshr <2 x i64> %8195, <i64 2, i64 2>
  %8197 = trunc <2 x i64> %8196 to <2 x i32>
  %8198 = or i64 %8111, 18
  %8199 = getelementptr inbounds i32, i32* %1, i64 %8198
  %8200 = bitcast i32* %8199 to <2 x i32>*
  store <2 x i32> %8197, <2 x i32>* %8200, align 4
  %8201 = load <2 x i64>, <2 x i64>* %8066, align 16
  %8202 = add nsw <2 x i64> %8201, <i64 1, i64 1>
  %8203 = lshr <2 x i64> %8201, <i64 63, i64 63>
  %8204 = add nsw <2 x i64> %8202, %8203
  %8205 = lshr <2 x i64> %8204, <i64 2, i64 2>
  %8206 = trunc <2 x i64> %8205 to <2 x i32>
  %8207 = or i64 %8111, 20
  %8208 = getelementptr inbounds i32, i32* %1, i64 %8207
  %8209 = bitcast i32* %8208 to <2 x i32>*
  store <2 x i32> %8206, <2 x i32>* %8209, align 4
  %8210 = load <2 x i64>, <2 x i64>* %8068, align 16
  %8211 = add nsw <2 x i64> %8210, <i64 1, i64 1>
  %8212 = lshr <2 x i64> %8210, <i64 63, i64 63>
  %8213 = add nsw <2 x i64> %8211, %8212
  %8214 = lshr <2 x i64> %8213, <i64 2, i64 2>
  %8215 = trunc <2 x i64> %8214 to <2 x i32>
  %8216 = or i64 %8111, 22
  %8217 = getelementptr inbounds i32, i32* %1, i64 %8216
  %8218 = bitcast i32* %8217 to <2 x i32>*
  store <2 x i32> %8215, <2 x i32>* %8218, align 4
  %8219 = load <2 x i64>, <2 x i64>* %8070, align 16
  %8220 = add nsw <2 x i64> %8219, <i64 1, i64 1>
  %8221 = lshr <2 x i64> %8219, <i64 63, i64 63>
  %8222 = add nsw <2 x i64> %8220, %8221
  %8223 = lshr <2 x i64> %8222, <i64 2, i64 2>
  %8224 = trunc <2 x i64> %8223 to <2 x i32>
  %8225 = or i64 %8111, 24
  %8226 = getelementptr inbounds i32, i32* %1, i64 %8225
  %8227 = bitcast i32* %8226 to <2 x i32>*
  store <2 x i32> %8224, <2 x i32>* %8227, align 4
  %8228 = load <2 x i64>, <2 x i64>* %8072, align 16
  %8229 = add nsw <2 x i64> %8228, <i64 1, i64 1>
  %8230 = lshr <2 x i64> %8228, <i64 63, i64 63>
  %8231 = add nsw <2 x i64> %8229, %8230
  %8232 = lshr <2 x i64> %8231, <i64 2, i64 2>
  %8233 = trunc <2 x i64> %8232 to <2 x i32>
  %8234 = or i64 %8111, 26
  %8235 = getelementptr inbounds i32, i32* %1, i64 %8234
  %8236 = bitcast i32* %8235 to <2 x i32>*
  store <2 x i32> %8233, <2 x i32>* %8236, align 4
  %8237 = load <2 x i64>, <2 x i64>* %8074, align 16
  %8238 = add nsw <2 x i64> %8237, <i64 1, i64 1>
  %8239 = lshr <2 x i64> %8237, <i64 63, i64 63>
  %8240 = add nsw <2 x i64> %8238, %8239
  %8241 = lshr <2 x i64> %8240, <i64 2, i64 2>
  %8242 = trunc <2 x i64> %8241 to <2 x i32>
  %8243 = or i64 %8111, 28
  %8244 = getelementptr inbounds i32, i32* %1, i64 %8243
  %8245 = bitcast i32* %8244 to <2 x i32>*
  store <2 x i32> %8242, <2 x i32>* %8245, align 4
  %8246 = load <2 x i64>, <2 x i64>* %8076, align 16
  %8247 = add nsw <2 x i64> %8246, <i64 1, i64 1>
  %8248 = lshr <2 x i64> %8246, <i64 63, i64 63>
  %8249 = add nsw <2 x i64> %8247, %8248
  %8250 = lshr <2 x i64> %8249, <i64 2, i64 2>
  %8251 = trunc <2 x i64> %8250 to <2 x i32>
  %8252 = or i64 %8111, 30
  %8253 = getelementptr inbounds i32, i32* %1, i64 %8252
  %8254 = bitcast i32* %8253 to <2 x i32>*
  store <2 x i32> %8251, <2 x i32>* %8254, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %8043) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %8042) #6
  %8255 = add nuw nsw i64 %8078, 1
  %8256 = icmp eq i64 %8255, 32
  br i1 %8256, label %11292, label %8077

8257:                                             ; preds = %8004
  %8258 = bitcast <2 x i64> %7622 to <4 x i32>
  %8259 = shufflevector <4 x i32> %8258, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %8260 = bitcast <4 x i32> %8259 to <2 x i64>
  %8261 = bitcast <2 x i64> %7627 to <4 x i32>
  %8262 = shufflevector <4 x i32> %8261, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %8263 = bitcast <4 x i32> %8262 to <2 x i64>
  %8264 = shufflevector <2 x i64> %8260, <2 x i64> %8263, <2 x i32> <i32 0, i32 2>
  %8265 = bitcast <2 x i64> %7632 to <4 x i32>
  %8266 = shufflevector <4 x i32> %8265, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %8267 = bitcast <4 x i32> %8266 to <2 x i64>
  %8268 = bitcast <2 x i64> %7637 to <4 x i32>
  %8269 = shufflevector <4 x i32> %8268, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %8270 = bitcast <4 x i32> %8269 to <2 x i64>
  %8271 = shufflevector <2 x i64> %8267, <2 x i64> %8270, <2 x i32> <i32 0, i32 2>
  %8272 = bitcast <2 x i64> %7642 to <4 x i32>
  %8273 = shufflevector <4 x i32> %8272, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %8274 = bitcast <4 x i32> %8273 to <2 x i64>
  %8275 = bitcast <2 x i64> %7647 to <4 x i32>
  %8276 = shufflevector <4 x i32> %8275, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %8277 = bitcast <4 x i32> %8276 to <2 x i64>
  %8278 = shufflevector <2 x i64> %8274, <2 x i64> %8277, <2 x i32> <i32 0, i32 2>
  %8279 = bitcast <2 x i64> %7652 to <4 x i32>
  %8280 = shufflevector <4 x i32> %8279, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %8281 = bitcast <4 x i32> %8280 to <2 x i64>
  %8282 = bitcast <2 x i64> %7657 to <4 x i32>
  %8283 = shufflevector <4 x i32> %8282, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %8284 = bitcast <4 x i32> %8283 to <2 x i64>
  %8285 = shufflevector <2 x i64> %8281, <2 x i64> %8284, <2 x i32> <i32 0, i32 2>
  %8286 = bitcast <2 x i64> %7662 to <4 x i32>
  %8287 = shufflevector <4 x i32> %8286, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %8288 = bitcast <4 x i32> %8287 to <2 x i64>
  %8289 = bitcast <2 x i64> %7667 to <4 x i32>
  %8290 = shufflevector <4 x i32> %8289, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %8291 = bitcast <4 x i32> %8290 to <2 x i64>
  %8292 = shufflevector <2 x i64> %8288, <2 x i64> %8291, <2 x i32> <i32 0, i32 2>
  %8293 = bitcast <2 x i64> %7672 to <4 x i32>
  %8294 = shufflevector <4 x i32> %8293, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %8295 = bitcast <4 x i32> %8294 to <2 x i64>
  %8296 = bitcast <2 x i64> %7677 to <4 x i32>
  %8297 = shufflevector <4 x i32> %8296, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %8298 = bitcast <4 x i32> %8297 to <2 x i64>
  %8299 = shufflevector <2 x i64> %8295, <2 x i64> %8298, <2 x i32> <i32 0, i32 2>
  %8300 = bitcast <2 x i64> %7682 to <4 x i32>
  %8301 = shufflevector <4 x i32> %8300, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %8302 = bitcast <4 x i32> %8301 to <2 x i64>
  %8303 = bitcast <2 x i64> %7687 to <4 x i32>
  %8304 = shufflevector <4 x i32> %8303, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %8305 = bitcast <4 x i32> %8304 to <2 x i64>
  %8306 = shufflevector <2 x i64> %8302, <2 x i64> %8305, <2 x i32> <i32 0, i32 2>
  %8307 = bitcast <2 x i64> %7692 to <4 x i32>
  %8308 = shufflevector <4 x i32> %8307, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %8309 = bitcast <4 x i32> %8308 to <2 x i64>
  %8310 = bitcast <2 x i64> %7697 to <4 x i32>
  %8311 = shufflevector <4 x i32> %8310, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %8312 = bitcast <4 x i32> %8311 to <2 x i64>
  %8313 = shufflevector <2 x i64> %8309, <2 x i64> %8312, <2 x i32> <i32 0, i32 2>
  %8314 = bitcast <2 x i64> %7700 to <4 x i32>
  %8315 = shufflevector <4 x i32> %8314, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %8316 = bitcast <4 x i32> %8315 to <2 x i64>
  %8317 = bitcast <2 x i64> %7703 to <4 x i32>
  %8318 = shufflevector <4 x i32> %8317, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %8319 = bitcast <4 x i32> %8318 to <2 x i64>
  %8320 = shufflevector <2 x i64> %8316, <2 x i64> %8319, <2 x i32> <i32 0, i32 2>
  %8321 = bitcast <2 x i64> %7706 to <4 x i32>
  %8322 = shufflevector <4 x i32> %8321, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %8323 = bitcast <4 x i32> %8322 to <2 x i64>
  %8324 = bitcast <2 x i64> %7709 to <4 x i32>
  %8325 = shufflevector <4 x i32> %8324, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %8326 = bitcast <4 x i32> %8325 to <2 x i64>
  %8327 = shufflevector <2 x i64> %8323, <2 x i64> %8326, <2 x i32> <i32 0, i32 2>
  %8328 = bitcast <2 x i64> %7712 to <4 x i32>
  %8329 = shufflevector <4 x i32> %8328, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %8330 = bitcast <4 x i32> %8329 to <2 x i64>
  %8331 = bitcast <2 x i64> %7715 to <4 x i32>
  %8332 = shufflevector <4 x i32> %8331, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %8333 = bitcast <4 x i32> %8332 to <2 x i64>
  %8334 = shufflevector <2 x i64> %8330, <2 x i64> %8333, <2 x i32> <i32 0, i32 2>
  %8335 = bitcast <2 x i64> %7718 to <4 x i32>
  %8336 = shufflevector <4 x i32> %8335, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %8337 = bitcast <4 x i32> %8336 to <2 x i64>
  %8338 = bitcast <2 x i64> %7721 to <4 x i32>
  %8339 = shufflevector <4 x i32> %8338, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %8340 = bitcast <4 x i32> %8339 to <2 x i64>
  %8341 = shufflevector <2 x i64> %8337, <2 x i64> %8340, <2 x i32> <i32 0, i32 2>
  %8342 = bitcast <2 x i64> %7724 to <4 x i32>
  %8343 = shufflevector <4 x i32> %8342, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %8344 = bitcast <4 x i32> %8343 to <2 x i64>
  %8345 = bitcast <2 x i64> %7727 to <4 x i32>
  %8346 = shufflevector <4 x i32> %8345, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %8347 = bitcast <4 x i32> %8346 to <2 x i64>
  %8348 = shufflevector <2 x i64> %8344, <2 x i64> %8347, <2 x i32> <i32 0, i32 2>
  %8349 = bitcast <2 x i64> %7730 to <4 x i32>
  %8350 = shufflevector <4 x i32> %8349, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %8351 = bitcast <4 x i32> %8350 to <2 x i64>
  %8352 = bitcast <2 x i64> %7733 to <4 x i32>
  %8353 = shufflevector <4 x i32> %8352, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %8354 = bitcast <4 x i32> %8353 to <2 x i64>
  %8355 = shufflevector <2 x i64> %8351, <2 x i64> %8354, <2 x i32> <i32 0, i32 2>
  %8356 = bitcast <2 x i64> %7736 to <4 x i32>
  %8357 = shufflevector <4 x i32> %8356, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %8358 = bitcast <4 x i32> %8357 to <2 x i64>
  %8359 = bitcast <2 x i64> %7739 to <4 x i32>
  %8360 = shufflevector <4 x i32> %8359, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %8361 = bitcast <4 x i32> %8360 to <2 x i64>
  %8362 = shufflevector <2 x i64> %8358, <2 x i64> %8361, <2 x i32> <i32 0, i32 2>
  %8363 = bitcast <2 x i64> %7742 to <4 x i32>
  %8364 = shufflevector <4 x i32> %8363, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %8365 = bitcast <4 x i32> %8364 to <2 x i64>
  %8366 = bitcast <2 x i64> %7745 to <4 x i32>
  %8367 = shufflevector <4 x i32> %8366, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %8368 = bitcast <4 x i32> %8367 to <2 x i64>
  %8369 = shufflevector <2 x i64> %8365, <2 x i64> %8368, <2 x i32> <i32 0, i32 2>
  %8370 = bitcast <2 x i64> %8264 to <4 x i32>
  %8371 = add <4 x i32> %8370, <i32 8192, i32 8192, i32 8192, i32 8192>
  %8372 = bitcast <2 x i64> %8271 to <4 x i32>
  %8373 = add <4 x i32> %8372, <i32 8192, i32 8192, i32 8192, i32 8192>
  %8374 = bitcast <2 x i64> %8278 to <4 x i32>
  %8375 = add <4 x i32> %8374, <i32 8192, i32 8192, i32 8192, i32 8192>
  %8376 = bitcast <2 x i64> %8285 to <4 x i32>
  %8377 = add <4 x i32> %8376, <i32 8192, i32 8192, i32 8192, i32 8192>
  %8378 = bitcast <2 x i64> %8292 to <4 x i32>
  %8379 = add <4 x i32> %8378, <i32 8192, i32 8192, i32 8192, i32 8192>
  %8380 = bitcast <2 x i64> %8299 to <4 x i32>
  %8381 = add <4 x i32> %8380, <i32 8192, i32 8192, i32 8192, i32 8192>
  %8382 = bitcast <2 x i64> %8306 to <4 x i32>
  %8383 = add <4 x i32> %8382, <i32 8192, i32 8192, i32 8192, i32 8192>
  %8384 = bitcast <2 x i64> %8313 to <4 x i32>
  %8385 = add <4 x i32> %8384, <i32 8192, i32 8192, i32 8192, i32 8192>
  %8386 = bitcast <2 x i64> %8320 to <4 x i32>
  %8387 = add <4 x i32> %8386, <i32 8192, i32 8192, i32 8192, i32 8192>
  %8388 = bitcast <2 x i64> %8327 to <4 x i32>
  %8389 = add <4 x i32> %8388, <i32 8192, i32 8192, i32 8192, i32 8192>
  %8390 = bitcast <2 x i64> %8334 to <4 x i32>
  %8391 = add <4 x i32> %8390, <i32 8192, i32 8192, i32 8192, i32 8192>
  %8392 = bitcast <2 x i64> %8341 to <4 x i32>
  %8393 = add <4 x i32> %8392, <i32 8192, i32 8192, i32 8192, i32 8192>
  %8394 = bitcast <2 x i64> %8348 to <4 x i32>
  %8395 = add <4 x i32> %8394, <i32 8192, i32 8192, i32 8192, i32 8192>
  %8396 = bitcast <2 x i64> %8355 to <4 x i32>
  %8397 = add <4 x i32> %8396, <i32 8192, i32 8192, i32 8192, i32 8192>
  %8398 = bitcast <2 x i64> %8362 to <4 x i32>
  %8399 = add <4 x i32> %8398, <i32 8192, i32 8192, i32 8192, i32 8192>
  %8400 = bitcast <2 x i64> %8369 to <4 x i32>
  %8401 = add <4 x i32> %8400, <i32 8192, i32 8192, i32 8192, i32 8192>
  %8402 = ashr <4 x i32> %8371, <i32 14, i32 14, i32 14, i32 14>
  %8403 = ashr <4 x i32> %8373, <i32 14, i32 14, i32 14, i32 14>
  %8404 = ashr <4 x i32> %8375, <i32 14, i32 14, i32 14, i32 14>
  %8405 = ashr <4 x i32> %8377, <i32 14, i32 14, i32 14, i32 14>
  %8406 = ashr <4 x i32> %8379, <i32 14, i32 14, i32 14, i32 14>
  %8407 = ashr <4 x i32> %8381, <i32 14, i32 14, i32 14, i32 14>
  %8408 = ashr <4 x i32> %8383, <i32 14, i32 14, i32 14, i32 14>
  %8409 = ashr <4 x i32> %8385, <i32 14, i32 14, i32 14, i32 14>
  %8410 = ashr <4 x i32> %8387, <i32 14, i32 14, i32 14, i32 14>
  %8411 = ashr <4 x i32> %8389, <i32 14, i32 14, i32 14, i32 14>
  %8412 = ashr <4 x i32> %8391, <i32 14, i32 14, i32 14, i32 14>
  %8413 = ashr <4 x i32> %8393, <i32 14, i32 14, i32 14, i32 14>
  %8414 = ashr <4 x i32> %8395, <i32 14, i32 14, i32 14, i32 14>
  %8415 = ashr <4 x i32> %8397, <i32 14, i32 14, i32 14, i32 14>
  %8416 = ashr <4 x i32> %8399, <i32 14, i32 14, i32 14, i32 14>
  %8417 = ashr <4 x i32> %8401, <i32 14, i32 14, i32 14, i32 14>
  %8418 = lshr <4 x i32> %8371, <i32 31, i32 31, i32 31, i32 31>
  %8419 = lshr <4 x i32> %8373, <i32 31, i32 31, i32 31, i32 31>
  %8420 = lshr <4 x i32> %8375, <i32 31, i32 31, i32 31, i32 31>
  %8421 = lshr <4 x i32> %8377, <i32 31, i32 31, i32 31, i32 31>
  %8422 = lshr <4 x i32> %8379, <i32 31, i32 31, i32 31, i32 31>
  %8423 = lshr <4 x i32> %8381, <i32 31, i32 31, i32 31, i32 31>
  %8424 = lshr <4 x i32> %8383, <i32 31, i32 31, i32 31, i32 31>
  %8425 = lshr <4 x i32> %8385, <i32 31, i32 31, i32 31, i32 31>
  %8426 = lshr <4 x i32> %8387, <i32 31, i32 31, i32 31, i32 31>
  %8427 = lshr <4 x i32> %8389, <i32 31, i32 31, i32 31, i32 31>
  %8428 = lshr <4 x i32> %8391, <i32 31, i32 31, i32 31, i32 31>
  %8429 = lshr <4 x i32> %8393, <i32 31, i32 31, i32 31, i32 31>
  %8430 = lshr <4 x i32> %8395, <i32 31, i32 31, i32 31, i32 31>
  %8431 = lshr <4 x i32> %8397, <i32 31, i32 31, i32 31, i32 31>
  %8432 = lshr <4 x i32> %8399, <i32 31, i32 31, i32 31, i32 31>
  %8433 = lshr <4 x i32> %8401, <i32 31, i32 31, i32 31, i32 31>
  %8434 = add nuw nsw <4 x i32> %8418, <i32 1, i32 1, i32 1, i32 1>
  %8435 = add nsw <4 x i32> %8434, %8402
  %8436 = add nuw nsw <4 x i32> %8419, <i32 1, i32 1, i32 1, i32 1>
  %8437 = add nsw <4 x i32> %8436, %8403
  %8438 = add nuw nsw <4 x i32> %8420, <i32 1, i32 1, i32 1, i32 1>
  %8439 = add nsw <4 x i32> %8438, %8404
  %8440 = add nuw nsw <4 x i32> %8421, <i32 1, i32 1, i32 1, i32 1>
  %8441 = add nsw <4 x i32> %8440, %8405
  %8442 = add nuw nsw <4 x i32> %8422, <i32 1, i32 1, i32 1, i32 1>
  %8443 = add nsw <4 x i32> %8442, %8406
  %8444 = add nuw nsw <4 x i32> %8423, <i32 1, i32 1, i32 1, i32 1>
  %8445 = add nsw <4 x i32> %8444, %8407
  %8446 = add nuw nsw <4 x i32> %8424, <i32 1, i32 1, i32 1, i32 1>
  %8447 = add nsw <4 x i32> %8446, %8408
  %8448 = add nuw nsw <4 x i32> %8425, <i32 1, i32 1, i32 1, i32 1>
  %8449 = add nsw <4 x i32> %8448, %8409
  %8450 = add nuw nsw <4 x i32> %8426, <i32 1, i32 1, i32 1, i32 1>
  %8451 = add nsw <4 x i32> %8450, %8410
  %8452 = add nuw nsw <4 x i32> %8427, <i32 1, i32 1, i32 1, i32 1>
  %8453 = add nsw <4 x i32> %8452, %8411
  %8454 = add nuw nsw <4 x i32> %8428, <i32 1, i32 1, i32 1, i32 1>
  %8455 = add nsw <4 x i32> %8454, %8412
  %8456 = add nuw nsw <4 x i32> %8429, <i32 1, i32 1, i32 1, i32 1>
  %8457 = add nsw <4 x i32> %8456, %8413
  %8458 = add nuw nsw <4 x i32> %8430, <i32 1, i32 1, i32 1, i32 1>
  %8459 = add nsw <4 x i32> %8458, %8414
  %8460 = add nuw nsw <4 x i32> %8431, <i32 1, i32 1, i32 1, i32 1>
  %8461 = add nsw <4 x i32> %8460, %8415
  %8462 = add nuw nsw <4 x i32> %8432, <i32 1, i32 1, i32 1, i32 1>
  %8463 = add nsw <4 x i32> %8462, %8416
  %8464 = add nuw nsw <4 x i32> %8433, <i32 1, i32 1, i32 1, i32 1>
  %8465 = add nsw <4 x i32> %8464, %8417
  %8466 = ashr <4 x i32> %8435, <i32 2, i32 2, i32 2, i32 2>
  %8467 = ashr <4 x i32> %8437, <i32 2, i32 2, i32 2, i32 2>
  %8468 = ashr <4 x i32> %8439, <i32 2, i32 2, i32 2, i32 2>
  %8469 = ashr <4 x i32> %8441, <i32 2, i32 2, i32 2, i32 2>
  %8470 = ashr <4 x i32> %8443, <i32 2, i32 2, i32 2, i32 2>
  %8471 = ashr <4 x i32> %8445, <i32 2, i32 2, i32 2, i32 2>
  %8472 = ashr <4 x i32> %8447, <i32 2, i32 2, i32 2, i32 2>
  %8473 = ashr <4 x i32> %8449, <i32 2, i32 2, i32 2, i32 2>
  %8474 = ashr <4 x i32> %8451, <i32 2, i32 2, i32 2, i32 2>
  %8475 = ashr <4 x i32> %8453, <i32 2, i32 2, i32 2, i32 2>
  %8476 = ashr <4 x i32> %8455, <i32 2, i32 2, i32 2, i32 2>
  %8477 = ashr <4 x i32> %8457, <i32 2, i32 2, i32 2, i32 2>
  %8478 = ashr <4 x i32> %8459, <i32 2, i32 2, i32 2, i32 2>
  %8479 = ashr <4 x i32> %8461, <i32 2, i32 2, i32 2, i32 2>
  %8480 = ashr <4 x i32> %8463, <i32 2, i32 2, i32 2, i32 2>
  %8481 = ashr <4 x i32> %8465, <i32 2, i32 2, i32 2, i32 2>
  %8482 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %8466, <4 x i32> %8467) #6
  store <8 x i16> %8482, <8 x i16>* %48, align 16
  %8483 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %8468, <4 x i32> %8469) #6
  store <8 x i16> %8483, <8 x i16>* %50, align 16
  %8484 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %8470, <4 x i32> %8471) #6
  store <8 x i16> %8484, <8 x i16>* %52, align 16
  %8485 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %8472, <4 x i32> %8473) #6
  store <8 x i16> %8485, <8 x i16>* %54, align 16
  %8486 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %8474, <4 x i32> %8475) #6
  store <8 x i16> %8486, <8 x i16>* %56, align 16
  %8487 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %8476, <4 x i32> %8477) #6
  store <8 x i16> %8487, <8 x i16>* %58, align 16
  %8488 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %8478, <4 x i32> %8479) #6
  store <8 x i16> %8488, <8 x i16>* %60, align 16
  %8489 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %8480, <4 x i32> %8481) #6
  store <8 x i16> %8489, <8 x i16>* %62, align 16
  %8490 = add <8 x i16> %8482, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %8491 = icmp ult <8 x i16> %8490, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %8492 = add <8 x i16> %8483, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %8493 = icmp ult <8 x i16> %8492, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %8494 = add <8 x i16> %8484, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %8495 = icmp ult <8 x i16> %8494, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %8496 = add <8 x i16> %8485, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %8497 = icmp ult <8 x i16> %8496, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %8498 = or <8 x i1> %8493, %8491
  %8499 = or <8 x i1> %8498, %8495
  %8500 = or <8 x i1> %8499, %8497
  %8501 = sext <8 x i1> %8500 to <8 x i16>
  %8502 = bitcast <8 x i16> %8501 to <16 x i8>
  %8503 = icmp slt <16 x i8> %8502, zeroinitializer
  %8504 = bitcast <16 x i1> %8503 to i16
  %8505 = zext i16 %8504 to i32
  %8506 = add <8 x i16> %8486, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %8507 = icmp ult <8 x i16> %8506, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %8508 = add <8 x i16> %8487, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %8509 = icmp ult <8 x i16> %8508, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %8510 = add <8 x i16> %8488, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %8511 = icmp ult <8 x i16> %8510, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %8512 = add <8 x i16> %8489, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %8513 = icmp ult <8 x i16> %8512, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %8514 = or <8 x i1> %8509, %8507
  %8515 = or <8 x i1> %8514, %8511
  %8516 = or <8 x i1> %8515, %8513
  %8517 = sext <8 x i1> %8516 to <8 x i16>
  %8518 = bitcast <8 x i16> %8517 to <16 x i8>
  %8519 = icmp slt <16 x i8> %8518, zeroinitializer
  %8520 = bitcast <16 x i1> %8519 to i16
  %8521 = zext i16 %8520 to i32
  %8522 = sub nsw i32 0, %8505
  %8523 = icmp eq i32 %8521, %8522
  br i1 %8523, label %8740, label %8524

8524:                                             ; preds = %8257
  %8525 = bitcast [32 x i64]* %4 to i8*
  %8526 = bitcast [32 x i64]* %5 to i8*
  %8527 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %8528 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %8529 = bitcast [32 x i64]* %5 to <2 x i64>*
  %8530 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %8531 = bitcast i64* %8530 to <2 x i64>*
  %8532 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %8533 = bitcast i64* %8532 to <2 x i64>*
  %8534 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %8535 = bitcast i64* %8534 to <2 x i64>*
  %8536 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %8537 = bitcast i64* %8536 to <2 x i64>*
  %8538 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %8539 = bitcast i64* %8538 to <2 x i64>*
  %8540 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %8541 = bitcast i64* %8540 to <2 x i64>*
  %8542 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %8543 = bitcast i64* %8542 to <2 x i64>*
  %8544 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %8545 = bitcast i64* %8544 to <2 x i64>*
  %8546 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %8547 = bitcast i64* %8546 to <2 x i64>*
  %8548 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %8549 = bitcast i64* %8548 to <2 x i64>*
  %8550 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %8551 = bitcast i64* %8550 to <2 x i64>*
  %8552 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %8553 = bitcast i64* %8552 to <2 x i64>*
  %8554 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %8555 = bitcast i64* %8554 to <2 x i64>*
  %8556 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %8557 = bitcast i64* %8556 to <2 x i64>*
  %8558 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %8559 = bitcast i64* %8558 to <2 x i64>*
  br label %8560

8560:                                             ; preds = %8593, %8524
  %8561 = phi i64 [ 0, %8524 ], [ %8738, %8593 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %8525) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %8525, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %8526) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %8526, i8 -86, i64 256, i1 false) #6
  br label %8562

8562:                                             ; preds = %8562, %8560
  %8563 = phi i64 [ 0, %8560 ], [ %8591, %8562 ]
  %8564 = shl i64 %8563, 5
  %8565 = add nuw nsw i64 %8564, %8561
  %8566 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %8565
  %8567 = load i16, i16* %8566, align 2
  %8568 = sext i16 %8567 to i64
  %8569 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %8563
  store i64 %8568, i64* %8569, align 16
  %8570 = or i64 %8563, 1
  %8571 = shl i64 %8570, 5
  %8572 = add nuw nsw i64 %8571, %8561
  %8573 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %8572
  %8574 = load i16, i16* %8573, align 2
  %8575 = sext i16 %8574 to i64
  %8576 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %8570
  store i64 %8575, i64* %8576, align 8
  %8577 = or i64 %8563, 2
  %8578 = shl i64 %8577, 5
  %8579 = add nuw nsw i64 %8578, %8561
  %8580 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %8579
  %8581 = load i16, i16* %8580, align 2
  %8582 = sext i16 %8581 to i64
  %8583 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %8577
  store i64 %8582, i64* %8583, align 16
  %8584 = or i64 %8563, 3
  %8585 = shl i64 %8584, 5
  %8586 = add nuw nsw i64 %8585, %8561
  %8587 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %8586
  %8588 = load i16, i16* %8587, align 2
  %8589 = sext i16 %8588 to i64
  %8590 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %8584
  store i64 %8589, i64* %8590, align 8
  %8591 = add nuw nsw i64 %8563, 4
  %8592 = icmp eq i64 %8591, 32
  br i1 %8592, label %8593, label %8562

8593:                                             ; preds = %8562
  call void @vpx_fdct32(i64* nonnull %8527, i64* nonnull %8528, i32 0) #6
  %8594 = shl i64 %8561, 5
  %8595 = load <2 x i64>, <2 x i64>* %8529, align 16
  %8596 = add nsw <2 x i64> %8595, <i64 1, i64 1>
  %8597 = lshr <2 x i64> %8595, <i64 63, i64 63>
  %8598 = add nsw <2 x i64> %8596, %8597
  %8599 = lshr <2 x i64> %8598, <i64 2, i64 2>
  %8600 = trunc <2 x i64> %8599 to <2 x i32>
  %8601 = getelementptr inbounds i32, i32* %1, i64 %8594
  %8602 = bitcast i32* %8601 to <2 x i32>*
  store <2 x i32> %8600, <2 x i32>* %8602, align 4
  %8603 = load <2 x i64>, <2 x i64>* %8531, align 16
  %8604 = add nsw <2 x i64> %8603, <i64 1, i64 1>
  %8605 = lshr <2 x i64> %8603, <i64 63, i64 63>
  %8606 = add nsw <2 x i64> %8604, %8605
  %8607 = lshr <2 x i64> %8606, <i64 2, i64 2>
  %8608 = trunc <2 x i64> %8607 to <2 x i32>
  %8609 = or i64 %8594, 2
  %8610 = getelementptr inbounds i32, i32* %1, i64 %8609
  %8611 = bitcast i32* %8610 to <2 x i32>*
  store <2 x i32> %8608, <2 x i32>* %8611, align 4
  %8612 = load <2 x i64>, <2 x i64>* %8533, align 16
  %8613 = add nsw <2 x i64> %8612, <i64 1, i64 1>
  %8614 = lshr <2 x i64> %8612, <i64 63, i64 63>
  %8615 = add nsw <2 x i64> %8613, %8614
  %8616 = lshr <2 x i64> %8615, <i64 2, i64 2>
  %8617 = trunc <2 x i64> %8616 to <2 x i32>
  %8618 = or i64 %8594, 4
  %8619 = getelementptr inbounds i32, i32* %1, i64 %8618
  %8620 = bitcast i32* %8619 to <2 x i32>*
  store <2 x i32> %8617, <2 x i32>* %8620, align 4
  %8621 = load <2 x i64>, <2 x i64>* %8535, align 16
  %8622 = add nsw <2 x i64> %8621, <i64 1, i64 1>
  %8623 = lshr <2 x i64> %8621, <i64 63, i64 63>
  %8624 = add nsw <2 x i64> %8622, %8623
  %8625 = lshr <2 x i64> %8624, <i64 2, i64 2>
  %8626 = trunc <2 x i64> %8625 to <2 x i32>
  %8627 = or i64 %8594, 6
  %8628 = getelementptr inbounds i32, i32* %1, i64 %8627
  %8629 = bitcast i32* %8628 to <2 x i32>*
  store <2 x i32> %8626, <2 x i32>* %8629, align 4
  %8630 = load <2 x i64>, <2 x i64>* %8537, align 16
  %8631 = add nsw <2 x i64> %8630, <i64 1, i64 1>
  %8632 = lshr <2 x i64> %8630, <i64 63, i64 63>
  %8633 = add nsw <2 x i64> %8631, %8632
  %8634 = lshr <2 x i64> %8633, <i64 2, i64 2>
  %8635 = trunc <2 x i64> %8634 to <2 x i32>
  %8636 = or i64 %8594, 8
  %8637 = getelementptr inbounds i32, i32* %1, i64 %8636
  %8638 = bitcast i32* %8637 to <2 x i32>*
  store <2 x i32> %8635, <2 x i32>* %8638, align 4
  %8639 = load <2 x i64>, <2 x i64>* %8539, align 16
  %8640 = add nsw <2 x i64> %8639, <i64 1, i64 1>
  %8641 = lshr <2 x i64> %8639, <i64 63, i64 63>
  %8642 = add nsw <2 x i64> %8640, %8641
  %8643 = lshr <2 x i64> %8642, <i64 2, i64 2>
  %8644 = trunc <2 x i64> %8643 to <2 x i32>
  %8645 = or i64 %8594, 10
  %8646 = getelementptr inbounds i32, i32* %1, i64 %8645
  %8647 = bitcast i32* %8646 to <2 x i32>*
  store <2 x i32> %8644, <2 x i32>* %8647, align 4
  %8648 = load <2 x i64>, <2 x i64>* %8541, align 16
  %8649 = add nsw <2 x i64> %8648, <i64 1, i64 1>
  %8650 = lshr <2 x i64> %8648, <i64 63, i64 63>
  %8651 = add nsw <2 x i64> %8649, %8650
  %8652 = lshr <2 x i64> %8651, <i64 2, i64 2>
  %8653 = trunc <2 x i64> %8652 to <2 x i32>
  %8654 = or i64 %8594, 12
  %8655 = getelementptr inbounds i32, i32* %1, i64 %8654
  %8656 = bitcast i32* %8655 to <2 x i32>*
  store <2 x i32> %8653, <2 x i32>* %8656, align 4
  %8657 = load <2 x i64>, <2 x i64>* %8543, align 16
  %8658 = add nsw <2 x i64> %8657, <i64 1, i64 1>
  %8659 = lshr <2 x i64> %8657, <i64 63, i64 63>
  %8660 = add nsw <2 x i64> %8658, %8659
  %8661 = lshr <2 x i64> %8660, <i64 2, i64 2>
  %8662 = trunc <2 x i64> %8661 to <2 x i32>
  %8663 = or i64 %8594, 14
  %8664 = getelementptr inbounds i32, i32* %1, i64 %8663
  %8665 = bitcast i32* %8664 to <2 x i32>*
  store <2 x i32> %8662, <2 x i32>* %8665, align 4
  %8666 = load <2 x i64>, <2 x i64>* %8545, align 16
  %8667 = add nsw <2 x i64> %8666, <i64 1, i64 1>
  %8668 = lshr <2 x i64> %8666, <i64 63, i64 63>
  %8669 = add nsw <2 x i64> %8667, %8668
  %8670 = lshr <2 x i64> %8669, <i64 2, i64 2>
  %8671 = trunc <2 x i64> %8670 to <2 x i32>
  %8672 = or i64 %8594, 16
  %8673 = getelementptr inbounds i32, i32* %1, i64 %8672
  %8674 = bitcast i32* %8673 to <2 x i32>*
  store <2 x i32> %8671, <2 x i32>* %8674, align 4
  %8675 = load <2 x i64>, <2 x i64>* %8547, align 16
  %8676 = add nsw <2 x i64> %8675, <i64 1, i64 1>
  %8677 = lshr <2 x i64> %8675, <i64 63, i64 63>
  %8678 = add nsw <2 x i64> %8676, %8677
  %8679 = lshr <2 x i64> %8678, <i64 2, i64 2>
  %8680 = trunc <2 x i64> %8679 to <2 x i32>
  %8681 = or i64 %8594, 18
  %8682 = getelementptr inbounds i32, i32* %1, i64 %8681
  %8683 = bitcast i32* %8682 to <2 x i32>*
  store <2 x i32> %8680, <2 x i32>* %8683, align 4
  %8684 = load <2 x i64>, <2 x i64>* %8549, align 16
  %8685 = add nsw <2 x i64> %8684, <i64 1, i64 1>
  %8686 = lshr <2 x i64> %8684, <i64 63, i64 63>
  %8687 = add nsw <2 x i64> %8685, %8686
  %8688 = lshr <2 x i64> %8687, <i64 2, i64 2>
  %8689 = trunc <2 x i64> %8688 to <2 x i32>
  %8690 = or i64 %8594, 20
  %8691 = getelementptr inbounds i32, i32* %1, i64 %8690
  %8692 = bitcast i32* %8691 to <2 x i32>*
  store <2 x i32> %8689, <2 x i32>* %8692, align 4
  %8693 = load <2 x i64>, <2 x i64>* %8551, align 16
  %8694 = add nsw <2 x i64> %8693, <i64 1, i64 1>
  %8695 = lshr <2 x i64> %8693, <i64 63, i64 63>
  %8696 = add nsw <2 x i64> %8694, %8695
  %8697 = lshr <2 x i64> %8696, <i64 2, i64 2>
  %8698 = trunc <2 x i64> %8697 to <2 x i32>
  %8699 = or i64 %8594, 22
  %8700 = getelementptr inbounds i32, i32* %1, i64 %8699
  %8701 = bitcast i32* %8700 to <2 x i32>*
  store <2 x i32> %8698, <2 x i32>* %8701, align 4
  %8702 = load <2 x i64>, <2 x i64>* %8553, align 16
  %8703 = add nsw <2 x i64> %8702, <i64 1, i64 1>
  %8704 = lshr <2 x i64> %8702, <i64 63, i64 63>
  %8705 = add nsw <2 x i64> %8703, %8704
  %8706 = lshr <2 x i64> %8705, <i64 2, i64 2>
  %8707 = trunc <2 x i64> %8706 to <2 x i32>
  %8708 = or i64 %8594, 24
  %8709 = getelementptr inbounds i32, i32* %1, i64 %8708
  %8710 = bitcast i32* %8709 to <2 x i32>*
  store <2 x i32> %8707, <2 x i32>* %8710, align 4
  %8711 = load <2 x i64>, <2 x i64>* %8555, align 16
  %8712 = add nsw <2 x i64> %8711, <i64 1, i64 1>
  %8713 = lshr <2 x i64> %8711, <i64 63, i64 63>
  %8714 = add nsw <2 x i64> %8712, %8713
  %8715 = lshr <2 x i64> %8714, <i64 2, i64 2>
  %8716 = trunc <2 x i64> %8715 to <2 x i32>
  %8717 = or i64 %8594, 26
  %8718 = getelementptr inbounds i32, i32* %1, i64 %8717
  %8719 = bitcast i32* %8718 to <2 x i32>*
  store <2 x i32> %8716, <2 x i32>* %8719, align 4
  %8720 = load <2 x i64>, <2 x i64>* %8557, align 16
  %8721 = add nsw <2 x i64> %8720, <i64 1, i64 1>
  %8722 = lshr <2 x i64> %8720, <i64 63, i64 63>
  %8723 = add nsw <2 x i64> %8721, %8722
  %8724 = lshr <2 x i64> %8723, <i64 2, i64 2>
  %8725 = trunc <2 x i64> %8724 to <2 x i32>
  %8726 = or i64 %8594, 28
  %8727 = getelementptr inbounds i32, i32* %1, i64 %8726
  %8728 = bitcast i32* %8727 to <2 x i32>*
  store <2 x i32> %8725, <2 x i32>* %8728, align 4
  %8729 = load <2 x i64>, <2 x i64>* %8559, align 16
  %8730 = add nsw <2 x i64> %8729, <i64 1, i64 1>
  %8731 = lshr <2 x i64> %8729, <i64 63, i64 63>
  %8732 = add nsw <2 x i64> %8730, %8731
  %8733 = lshr <2 x i64> %8732, <i64 2, i64 2>
  %8734 = trunc <2 x i64> %8733 to <2 x i32>
  %8735 = or i64 %8594, 30
  %8736 = getelementptr inbounds i32, i32* %1, i64 %8735
  %8737 = bitcast i32* %8736 to <2 x i32>*
  store <2 x i32> %8734, <2 x i32>* %8737, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %8526) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %8525) #6
  %8738 = add nuw nsw i64 %8561, 1
  %8739 = icmp eq i64 %8738, 32
  br i1 %8739, label %11292, label %8560

8740:                                             ; preds = %8257
  %8741 = add <4 x i32> %7570, %5913
  %8742 = add <4 x i32> %7571, %5914
  %8743 = sub <4 x i32> %5913, %7570
  %8744 = sub <4 x i32> %5914, %7571
  %8745 = sub <4 x i32> %5919, %7572
  %8746 = sub <4 x i32> %5920, %7573
  %8747 = add <4 x i32> %7572, %5919
  %8748 = add <4 x i32> %7573, %5920
  %8749 = add <4 x i32> %7574, %5921
  %8750 = add <4 x i32> %7575, %5922
  %8751 = sub <4 x i32> %5921, %7574
  %8752 = sub <4 x i32> %5922, %7575
  %8753 = sub <4 x i32> %5927, %7576
  %8754 = sub <4 x i32> %5928, %7577
  %8755 = add <4 x i32> %7576, %5927
  %8756 = add <4 x i32> %7577, %5928
  %8757 = add <4 x i32> %7578, %5929
  %8758 = add <4 x i32> %7579, %5930
  %8759 = sub <4 x i32> %5929, %7578
  %8760 = sub <4 x i32> %5930, %7579
  %8761 = sub <4 x i32> %5935, %7580
  %8762 = sub <4 x i32> %5936, %7581
  %8763 = add <4 x i32> %7580, %5935
  %8764 = add <4 x i32> %7581, %5936
  %8765 = add <4 x i32> %7582, %5937
  %8766 = add <4 x i32> %7583, %5938
  %8767 = sub <4 x i32> %5937, %7582
  %8768 = sub <4 x i32> %5938, %7583
  %8769 = sub <4 x i32> %5943, %7584
  %8770 = sub <4 x i32> %5944, %7585
  %8771 = add <4 x i32> %7584, %5943
  %8772 = add <4 x i32> %7585, %5944
  %8773 = shufflevector <4 x i32> %8741, <4 x i32> %8771, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %8774 = bitcast <4 x i32> %8773 to <2 x i64>
  %8775 = shufflevector <4 x i32> %8741, <4 x i32> %8771, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %8776 = bitcast <4 x i32> %8775 to <2 x i64>
  %8777 = shufflevector <4 x i32> %8742, <4 x i32> %8772, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %8778 = bitcast <4 x i32> %8777 to <2 x i64>
  %8779 = shufflevector <4 x i32> %8742, <4 x i32> %8772, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %8780 = bitcast <4 x i32> %8779 to <2 x i64>
  %8781 = shufflevector <4 x i32> %8743, <4 x i32> %8769, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %8782 = bitcast <4 x i32> %8781 to <2 x i64>
  %8783 = shufflevector <4 x i32> %8743, <4 x i32> %8769, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %8784 = bitcast <4 x i32> %8783 to <2 x i64>
  %8785 = shufflevector <4 x i32> %8744, <4 x i32> %8770, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %8786 = bitcast <4 x i32> %8785 to <2 x i64>
  %8787 = shufflevector <4 x i32> %8744, <4 x i32> %8770, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %8788 = bitcast <4 x i32> %8787 to <2 x i64>
  %8789 = shufflevector <4 x i32> %8745, <4 x i32> %8767, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %8790 = bitcast <4 x i32> %8789 to <2 x i64>
  %8791 = shufflevector <4 x i32> %8745, <4 x i32> %8767, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %8792 = bitcast <4 x i32> %8791 to <2 x i64>
  %8793 = shufflevector <4 x i32> %8746, <4 x i32> %8768, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %8794 = bitcast <4 x i32> %8793 to <2 x i64>
  %8795 = shufflevector <4 x i32> %8746, <4 x i32> %8768, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %8796 = bitcast <4 x i32> %8795 to <2 x i64>
  %8797 = shufflevector <4 x i32> %8747, <4 x i32> %8765, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %8798 = bitcast <4 x i32> %8797 to <2 x i64>
  %8799 = shufflevector <4 x i32> %8747, <4 x i32> %8765, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %8800 = bitcast <4 x i32> %8799 to <2 x i64>
  %8801 = shufflevector <4 x i32> %8748, <4 x i32> %8766, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %8802 = bitcast <4 x i32> %8801 to <2 x i64>
  %8803 = shufflevector <4 x i32> %8748, <4 x i32> %8766, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %8804 = bitcast <4 x i32> %8803 to <2 x i64>
  %8805 = and <2 x i64> %8774, <i64 4294967295, i64 4294967295>
  %8806 = mul nuw nsw <2 x i64> %8805, <i64 804, i64 804>
  %8807 = lshr <2 x i64> %8774, <i64 32, i64 32>
  %8808 = mul nuw nsw <2 x i64> %8807, <i64 16364, i64 16364>
  %8809 = add nuw nsw <2 x i64> %8808, %8806
  %8810 = and <2 x i64> %8776, <i64 4294967295, i64 4294967295>
  %8811 = mul nuw nsw <2 x i64> %8810, <i64 804, i64 804>
  %8812 = lshr <2 x i64> %8776, <i64 32, i64 32>
  %8813 = mul nuw nsw <2 x i64> %8812, <i64 16364, i64 16364>
  %8814 = add nuw nsw <2 x i64> %8813, %8811
  %8815 = and <2 x i64> %8778, <i64 4294967295, i64 4294967295>
  %8816 = mul nuw nsw <2 x i64> %8815, <i64 804, i64 804>
  %8817 = lshr <2 x i64> %8778, <i64 32, i64 32>
  %8818 = mul nuw nsw <2 x i64> %8817, <i64 16364, i64 16364>
  %8819 = add nuw nsw <2 x i64> %8818, %8816
  %8820 = and <2 x i64> %8780, <i64 4294967295, i64 4294967295>
  %8821 = mul nuw nsw <2 x i64> %8820, <i64 804, i64 804>
  %8822 = lshr <2 x i64> %8780, <i64 32, i64 32>
  %8823 = mul nuw nsw <2 x i64> %8822, <i64 16364, i64 16364>
  %8824 = add nuw nsw <2 x i64> %8823, %8821
  %8825 = and <2 x i64> %8782, <i64 4294967295, i64 4294967295>
  %8826 = mul nuw nsw <2 x i64> %8825, <i64 12140, i64 12140>
  %8827 = lshr <2 x i64> %8782, <i64 32, i64 32>
  %8828 = mul nuw nsw <2 x i64> %8827, <i64 11003, i64 11003>
  %8829 = add nuw nsw <2 x i64> %8828, %8826
  %8830 = and <2 x i64> %8784, <i64 4294967295, i64 4294967295>
  %8831 = mul nuw nsw <2 x i64> %8830, <i64 12140, i64 12140>
  %8832 = lshr <2 x i64> %8784, <i64 32, i64 32>
  %8833 = mul nuw nsw <2 x i64> %8832, <i64 11003, i64 11003>
  %8834 = add nuw nsw <2 x i64> %8833, %8831
  %8835 = and <2 x i64> %8786, <i64 4294967295, i64 4294967295>
  %8836 = mul nuw nsw <2 x i64> %8835, <i64 12140, i64 12140>
  %8837 = lshr <2 x i64> %8786, <i64 32, i64 32>
  %8838 = mul nuw nsw <2 x i64> %8837, <i64 11003, i64 11003>
  %8839 = add nuw nsw <2 x i64> %8838, %8836
  %8840 = and <2 x i64> %8788, <i64 4294967295, i64 4294967295>
  %8841 = mul nuw nsw <2 x i64> %8840, <i64 12140, i64 12140>
  %8842 = lshr <2 x i64> %8788, <i64 32, i64 32>
  %8843 = mul nuw nsw <2 x i64> %8842, <i64 11003, i64 11003>
  %8844 = add nuw nsw <2 x i64> %8843, %8841
  %8845 = and <2 x i64> %8790, <i64 4294967295, i64 4294967295>
  %8846 = mul nuw nsw <2 x i64> %8845, <i64 7005, i64 7005>
  %8847 = lshr <2 x i64> %8790, <i64 32, i64 32>
  %8848 = mul nuw nsw <2 x i64> %8847, <i64 14811, i64 14811>
  %8849 = add nuw nsw <2 x i64> %8848, %8846
  %8850 = and <2 x i64> %8792, <i64 4294967295, i64 4294967295>
  %8851 = mul nuw nsw <2 x i64> %8850, <i64 7005, i64 7005>
  %8852 = lshr <2 x i64> %8792, <i64 32, i64 32>
  %8853 = mul nuw nsw <2 x i64> %8852, <i64 14811, i64 14811>
  %8854 = add nuw nsw <2 x i64> %8853, %8851
  %8855 = and <2 x i64> %8794, <i64 4294967295, i64 4294967295>
  %8856 = mul nuw nsw <2 x i64> %8855, <i64 7005, i64 7005>
  %8857 = lshr <2 x i64> %8794, <i64 32, i64 32>
  %8858 = mul nuw nsw <2 x i64> %8857, <i64 14811, i64 14811>
  %8859 = add nuw nsw <2 x i64> %8858, %8856
  %8860 = and <2 x i64> %8796, <i64 4294967295, i64 4294967295>
  %8861 = mul nuw nsw <2 x i64> %8860, <i64 7005, i64 7005>
  %8862 = lshr <2 x i64> %8796, <i64 32, i64 32>
  %8863 = mul nuw nsw <2 x i64> %8862, <i64 14811, i64 14811>
  %8864 = add nuw nsw <2 x i64> %8863, %8861
  %8865 = and <2 x i64> %8798, <i64 4294967295, i64 4294967295>
  %8866 = mul nuw nsw <2 x i64> %8865, <i64 15426, i64 15426>
  %8867 = lshr <2 x i64> %8798, <i64 32, i64 32>
  %8868 = mul nuw nsw <2 x i64> %8867, <i64 5520, i64 5520>
  %8869 = add nuw nsw <2 x i64> %8868, %8866
  %8870 = and <2 x i64> %8800, <i64 4294967295, i64 4294967295>
  %8871 = mul nuw nsw <2 x i64> %8870, <i64 15426, i64 15426>
  %8872 = lshr <2 x i64> %8800, <i64 32, i64 32>
  %8873 = mul nuw nsw <2 x i64> %8872, <i64 5520, i64 5520>
  %8874 = add nuw nsw <2 x i64> %8873, %8871
  %8875 = and <2 x i64> %8802, <i64 4294967295, i64 4294967295>
  %8876 = mul nuw nsw <2 x i64> %8875, <i64 15426, i64 15426>
  %8877 = lshr <2 x i64> %8802, <i64 32, i64 32>
  %8878 = mul nuw nsw <2 x i64> %8877, <i64 5520, i64 5520>
  %8879 = add nuw nsw <2 x i64> %8878, %8876
  %8880 = and <2 x i64> %8804, <i64 4294967295, i64 4294967295>
  %8881 = mul nuw nsw <2 x i64> %8880, <i64 15426, i64 15426>
  %8882 = lshr <2 x i64> %8804, <i64 32, i64 32>
  %8883 = mul nuw nsw <2 x i64> %8882, <i64 5520, i64 5520>
  %8884 = add nuw nsw <2 x i64> %8883, %8881
  %8885 = mul nuw <2 x i64> %8865, <i64 4294961776, i64 4294961776>
  %8886 = mul nuw nsw <2 x i64> %8867, <i64 15426, i64 15426>
  %8887 = add <2 x i64> %8886, %8885
  %8888 = mul nuw <2 x i64> %8870, <i64 4294961776, i64 4294961776>
  %8889 = mul nuw nsw <2 x i64> %8872, <i64 15426, i64 15426>
  %8890 = add <2 x i64> %8889, %8888
  %8891 = mul nuw <2 x i64> %8875, <i64 4294961776, i64 4294961776>
  %8892 = mul nuw nsw <2 x i64> %8877, <i64 15426, i64 15426>
  %8893 = add <2 x i64> %8892, %8891
  %8894 = mul nuw <2 x i64> %8880, <i64 4294961776, i64 4294961776>
  %8895 = mul nuw nsw <2 x i64> %8882, <i64 15426, i64 15426>
  %8896 = add <2 x i64> %8895, %8894
  %8897 = mul nuw <2 x i64> %8845, <i64 4294952485, i64 4294952485>
  %8898 = mul nuw nsw <2 x i64> %8847, <i64 7005, i64 7005>
  %8899 = add <2 x i64> %8898, %8897
  %8900 = mul nuw <2 x i64> %8850, <i64 4294952485, i64 4294952485>
  %8901 = mul nuw nsw <2 x i64> %8852, <i64 7005, i64 7005>
  %8902 = add <2 x i64> %8901, %8900
  %8903 = mul nuw <2 x i64> %8855, <i64 4294952485, i64 4294952485>
  %8904 = mul nuw nsw <2 x i64> %8857, <i64 7005, i64 7005>
  %8905 = add <2 x i64> %8904, %8903
  %8906 = mul nuw <2 x i64> %8860, <i64 4294952485, i64 4294952485>
  %8907 = mul nuw nsw <2 x i64> %8862, <i64 7005, i64 7005>
  %8908 = add <2 x i64> %8907, %8906
  %8909 = mul nuw <2 x i64> %8825, <i64 4294956293, i64 4294956293>
  %8910 = mul nuw nsw <2 x i64> %8827, <i64 12140, i64 12140>
  %8911 = add <2 x i64> %8910, %8909
  %8912 = mul nuw <2 x i64> %8830, <i64 4294956293, i64 4294956293>
  %8913 = mul nuw nsw <2 x i64> %8832, <i64 12140, i64 12140>
  %8914 = add <2 x i64> %8913, %8912
  %8915 = mul nuw <2 x i64> %8835, <i64 4294956293, i64 4294956293>
  %8916 = mul nuw nsw <2 x i64> %8837, <i64 12140, i64 12140>
  %8917 = add <2 x i64> %8916, %8915
  %8918 = mul nuw <2 x i64> %8840, <i64 4294956293, i64 4294956293>
  %8919 = mul nuw nsw <2 x i64> %8842, <i64 12140, i64 12140>
  %8920 = add <2 x i64> %8919, %8918
  %8921 = mul nuw <2 x i64> %8805, <i64 4294950932, i64 4294950932>
  %8922 = mul nuw nsw <2 x i64> %8807, <i64 804, i64 804>
  %8923 = add <2 x i64> %8922, %8921
  %8924 = mul nuw <2 x i64> %8810, <i64 4294950932, i64 4294950932>
  %8925 = mul nuw nsw <2 x i64> %8812, <i64 804, i64 804>
  %8926 = add <2 x i64> %8925, %8924
  %8927 = mul nuw <2 x i64> %8815, <i64 4294950932, i64 4294950932>
  %8928 = mul nuw nsw <2 x i64> %8817, <i64 804, i64 804>
  %8929 = add <2 x i64> %8928, %8927
  %8930 = mul nuw <2 x i64> %8820, <i64 4294950932, i64 4294950932>
  %8931 = mul nuw nsw <2 x i64> %8822, <i64 804, i64 804>
  %8932 = add <2 x i64> %8931, %8930
  %8933 = shl nuw nsw <2 x i64> %8809, <i64 1, i64 1>
  %8934 = shl nuw nsw <2 x i64> %8814, <i64 1, i64 1>
  %8935 = shl nuw nsw <2 x i64> %8819, <i64 1, i64 1>
  %8936 = shl nuw nsw <2 x i64> %8824, <i64 1, i64 1>
  %8937 = bitcast <2 x i64> %8933 to <4 x i32>
  %8938 = shufflevector <4 x i32> %8937, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %8939 = bitcast <4 x i32> %8938 to <2 x i64>
  %8940 = bitcast <2 x i64> %8934 to <4 x i32>
  %8941 = shufflevector <4 x i32> %8940, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %8942 = bitcast <4 x i32> %8941 to <2 x i64>
  %8943 = bitcast <2 x i64> %8935 to <4 x i32>
  %8944 = shufflevector <4 x i32> %8943, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %8945 = bitcast <4 x i32> %8944 to <2 x i64>
  %8946 = bitcast <2 x i64> %8936 to <4 x i32>
  %8947 = shufflevector <4 x i32> %8946, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %8948 = bitcast <4 x i32> %8947 to <2 x i64>
  %8949 = shufflevector <2 x i64> %8939, <2 x i64> %8942, <2 x i32> <i32 0, i32 2>
  %8950 = shufflevector <2 x i64> %8945, <2 x i64> %8948, <2 x i32> <i32 0, i32 2>
  %8951 = bitcast <2 x i64> %8949 to <4 x i32>
  %8952 = bitcast <2 x i64> %8950 to <4 x i32>
  %8953 = add <4 x i32> %8951, <i32 1, i32 1, i32 1, i32 1>
  %8954 = icmp ugt <4 x i32> %8953, <i32 1, i32 1, i32 1, i32 1>
  %8955 = sext <4 x i1> %8954 to <4 x i32>
  %8956 = bitcast <4 x i32> %8955 to <16 x i8>
  %8957 = icmp slt <16 x i8> %8956, zeroinitializer
  %8958 = bitcast <16 x i1> %8957 to i16
  %8959 = zext i16 %8958 to i32
  %8960 = add <4 x i32> %8952, <i32 1, i32 1, i32 1, i32 1>
  %8961 = icmp ugt <4 x i32> %8960, <i32 1, i32 1, i32 1, i32 1>
  %8962 = sext <4 x i1> %8961 to <4 x i32>
  %8963 = bitcast <4 x i32> %8962 to <16 x i8>
  %8964 = icmp slt <16 x i8> %8963, zeroinitializer
  %8965 = bitcast <16 x i1> %8964 to i16
  %8966 = zext i16 %8965 to i32
  %8967 = sub nsw i32 0, %8959
  %8968 = icmp eq i32 %8966, %8967
  br i1 %8968, label %8969, label %9228

8969:                                             ; preds = %8740
  %8970 = shl nuw nsw <2 x i64> %8829, <i64 1, i64 1>
  %8971 = shl nuw nsw <2 x i64> %8834, <i64 1, i64 1>
  %8972 = shl nuw nsw <2 x i64> %8839, <i64 1, i64 1>
  %8973 = shl nuw nsw <2 x i64> %8844, <i64 1, i64 1>
  %8974 = bitcast <2 x i64> %8970 to <4 x i32>
  %8975 = shufflevector <4 x i32> %8974, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %8976 = bitcast <4 x i32> %8975 to <2 x i64>
  %8977 = bitcast <2 x i64> %8971 to <4 x i32>
  %8978 = shufflevector <4 x i32> %8977, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %8979 = bitcast <4 x i32> %8978 to <2 x i64>
  %8980 = bitcast <2 x i64> %8972 to <4 x i32>
  %8981 = shufflevector <4 x i32> %8980, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %8982 = bitcast <4 x i32> %8981 to <2 x i64>
  %8983 = bitcast <2 x i64> %8973 to <4 x i32>
  %8984 = shufflevector <4 x i32> %8983, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %8985 = bitcast <4 x i32> %8984 to <2 x i64>
  %8986 = shufflevector <2 x i64> %8976, <2 x i64> %8979, <2 x i32> <i32 0, i32 2>
  %8987 = shufflevector <2 x i64> %8982, <2 x i64> %8985, <2 x i32> <i32 0, i32 2>
  %8988 = bitcast <2 x i64> %8986 to <4 x i32>
  %8989 = bitcast <2 x i64> %8987 to <4 x i32>
  %8990 = add <4 x i32> %8988, <i32 1, i32 1, i32 1, i32 1>
  %8991 = icmp ugt <4 x i32> %8990, <i32 1, i32 1, i32 1, i32 1>
  %8992 = sext <4 x i1> %8991 to <4 x i32>
  %8993 = bitcast <4 x i32> %8992 to <16 x i8>
  %8994 = icmp slt <16 x i8> %8993, zeroinitializer
  %8995 = bitcast <16 x i1> %8994 to i16
  %8996 = zext i16 %8995 to i32
  %8997 = add <4 x i32> %8989, <i32 1, i32 1, i32 1, i32 1>
  %8998 = icmp ugt <4 x i32> %8997, <i32 1, i32 1, i32 1, i32 1>
  %8999 = sext <4 x i1> %8998 to <4 x i32>
  %9000 = bitcast <4 x i32> %8999 to <16 x i8>
  %9001 = icmp slt <16 x i8> %9000, zeroinitializer
  %9002 = bitcast <16 x i1> %9001 to i16
  %9003 = zext i16 %9002 to i32
  %9004 = sub nsw i32 0, %8996
  %9005 = icmp eq i32 %9003, %9004
  br i1 %9005, label %9006, label %9228

9006:                                             ; preds = %8969
  %9007 = shl nuw nsw <2 x i64> %8849, <i64 1, i64 1>
  %9008 = shl nuw nsw <2 x i64> %8854, <i64 1, i64 1>
  %9009 = shl nuw nsw <2 x i64> %8859, <i64 1, i64 1>
  %9010 = shl nuw nsw <2 x i64> %8864, <i64 1, i64 1>
  %9011 = bitcast <2 x i64> %9007 to <4 x i32>
  %9012 = shufflevector <4 x i32> %9011, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %9013 = bitcast <4 x i32> %9012 to <2 x i64>
  %9014 = bitcast <2 x i64> %9008 to <4 x i32>
  %9015 = shufflevector <4 x i32> %9014, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %9016 = bitcast <4 x i32> %9015 to <2 x i64>
  %9017 = bitcast <2 x i64> %9009 to <4 x i32>
  %9018 = shufflevector <4 x i32> %9017, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %9019 = bitcast <4 x i32> %9018 to <2 x i64>
  %9020 = bitcast <2 x i64> %9010 to <4 x i32>
  %9021 = shufflevector <4 x i32> %9020, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %9022 = bitcast <4 x i32> %9021 to <2 x i64>
  %9023 = shufflevector <2 x i64> %9013, <2 x i64> %9016, <2 x i32> <i32 0, i32 2>
  %9024 = shufflevector <2 x i64> %9019, <2 x i64> %9022, <2 x i32> <i32 0, i32 2>
  %9025 = bitcast <2 x i64> %9023 to <4 x i32>
  %9026 = bitcast <2 x i64> %9024 to <4 x i32>
  %9027 = add <4 x i32> %9025, <i32 1, i32 1, i32 1, i32 1>
  %9028 = icmp ugt <4 x i32> %9027, <i32 1, i32 1, i32 1, i32 1>
  %9029 = sext <4 x i1> %9028 to <4 x i32>
  %9030 = bitcast <4 x i32> %9029 to <16 x i8>
  %9031 = icmp slt <16 x i8> %9030, zeroinitializer
  %9032 = bitcast <16 x i1> %9031 to i16
  %9033 = zext i16 %9032 to i32
  %9034 = add <4 x i32> %9026, <i32 1, i32 1, i32 1, i32 1>
  %9035 = icmp ugt <4 x i32> %9034, <i32 1, i32 1, i32 1, i32 1>
  %9036 = sext <4 x i1> %9035 to <4 x i32>
  %9037 = bitcast <4 x i32> %9036 to <16 x i8>
  %9038 = icmp slt <16 x i8> %9037, zeroinitializer
  %9039 = bitcast <16 x i1> %9038 to i16
  %9040 = zext i16 %9039 to i32
  %9041 = sub nsw i32 0, %9033
  %9042 = icmp eq i32 %9040, %9041
  br i1 %9042, label %9043, label %9228

9043:                                             ; preds = %9006
  %9044 = shl nuw nsw <2 x i64> %8869, <i64 1, i64 1>
  %9045 = shl nuw nsw <2 x i64> %8874, <i64 1, i64 1>
  %9046 = shl nuw nsw <2 x i64> %8879, <i64 1, i64 1>
  %9047 = shl nuw nsw <2 x i64> %8884, <i64 1, i64 1>
  %9048 = bitcast <2 x i64> %9044 to <4 x i32>
  %9049 = shufflevector <4 x i32> %9048, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %9050 = bitcast <4 x i32> %9049 to <2 x i64>
  %9051 = bitcast <2 x i64> %9045 to <4 x i32>
  %9052 = shufflevector <4 x i32> %9051, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %9053 = bitcast <4 x i32> %9052 to <2 x i64>
  %9054 = bitcast <2 x i64> %9046 to <4 x i32>
  %9055 = shufflevector <4 x i32> %9054, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %9056 = bitcast <4 x i32> %9055 to <2 x i64>
  %9057 = bitcast <2 x i64> %9047 to <4 x i32>
  %9058 = shufflevector <4 x i32> %9057, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %9059 = bitcast <4 x i32> %9058 to <2 x i64>
  %9060 = shufflevector <2 x i64> %9050, <2 x i64> %9053, <2 x i32> <i32 0, i32 2>
  %9061 = shufflevector <2 x i64> %9056, <2 x i64> %9059, <2 x i32> <i32 0, i32 2>
  %9062 = bitcast <2 x i64> %9060 to <4 x i32>
  %9063 = bitcast <2 x i64> %9061 to <4 x i32>
  %9064 = add <4 x i32> %9062, <i32 1, i32 1, i32 1, i32 1>
  %9065 = icmp ugt <4 x i32> %9064, <i32 1, i32 1, i32 1, i32 1>
  %9066 = sext <4 x i1> %9065 to <4 x i32>
  %9067 = bitcast <4 x i32> %9066 to <16 x i8>
  %9068 = icmp slt <16 x i8> %9067, zeroinitializer
  %9069 = bitcast <16 x i1> %9068 to i16
  %9070 = zext i16 %9069 to i32
  %9071 = add <4 x i32> %9063, <i32 1, i32 1, i32 1, i32 1>
  %9072 = icmp ugt <4 x i32> %9071, <i32 1, i32 1, i32 1, i32 1>
  %9073 = sext <4 x i1> %9072 to <4 x i32>
  %9074 = bitcast <4 x i32> %9073 to <16 x i8>
  %9075 = icmp slt <16 x i8> %9074, zeroinitializer
  %9076 = bitcast <16 x i1> %9075 to i16
  %9077 = zext i16 %9076 to i32
  %9078 = sub nsw i32 0, %9070
  %9079 = icmp eq i32 %9077, %9078
  br i1 %9079, label %9080, label %9228

9080:                                             ; preds = %9043
  %9081 = shl <2 x i64> %8887, <i64 1, i64 1>
  %9082 = shl <2 x i64> %8890, <i64 1, i64 1>
  %9083 = shl <2 x i64> %8893, <i64 1, i64 1>
  %9084 = shl <2 x i64> %8896, <i64 1, i64 1>
  %9085 = bitcast <2 x i64> %9081 to <4 x i32>
  %9086 = shufflevector <4 x i32> %9085, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %9087 = bitcast <4 x i32> %9086 to <2 x i64>
  %9088 = bitcast <2 x i64> %9082 to <4 x i32>
  %9089 = shufflevector <4 x i32> %9088, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %9090 = bitcast <4 x i32> %9089 to <2 x i64>
  %9091 = bitcast <2 x i64> %9083 to <4 x i32>
  %9092 = shufflevector <4 x i32> %9091, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %9093 = bitcast <4 x i32> %9092 to <2 x i64>
  %9094 = bitcast <2 x i64> %9084 to <4 x i32>
  %9095 = shufflevector <4 x i32> %9094, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %9096 = bitcast <4 x i32> %9095 to <2 x i64>
  %9097 = shufflevector <2 x i64> %9087, <2 x i64> %9090, <2 x i32> <i32 0, i32 2>
  %9098 = shufflevector <2 x i64> %9093, <2 x i64> %9096, <2 x i32> <i32 0, i32 2>
  %9099 = bitcast <2 x i64> %9097 to <4 x i32>
  %9100 = bitcast <2 x i64> %9098 to <4 x i32>
  %9101 = add <4 x i32> %9099, <i32 1, i32 1, i32 1, i32 1>
  %9102 = icmp ugt <4 x i32> %9101, <i32 1, i32 1, i32 1, i32 1>
  %9103 = sext <4 x i1> %9102 to <4 x i32>
  %9104 = bitcast <4 x i32> %9103 to <16 x i8>
  %9105 = icmp slt <16 x i8> %9104, zeroinitializer
  %9106 = bitcast <16 x i1> %9105 to i16
  %9107 = zext i16 %9106 to i32
  %9108 = add <4 x i32> %9100, <i32 1, i32 1, i32 1, i32 1>
  %9109 = icmp ugt <4 x i32> %9108, <i32 1, i32 1, i32 1, i32 1>
  %9110 = sext <4 x i1> %9109 to <4 x i32>
  %9111 = bitcast <4 x i32> %9110 to <16 x i8>
  %9112 = icmp slt <16 x i8> %9111, zeroinitializer
  %9113 = bitcast <16 x i1> %9112 to i16
  %9114 = zext i16 %9113 to i32
  %9115 = sub nsw i32 0, %9107
  %9116 = icmp eq i32 %9114, %9115
  br i1 %9116, label %9117, label %9228

9117:                                             ; preds = %9080
  %9118 = shl <2 x i64> %8899, <i64 1, i64 1>
  %9119 = shl <2 x i64> %8902, <i64 1, i64 1>
  %9120 = shl <2 x i64> %8905, <i64 1, i64 1>
  %9121 = shl <2 x i64> %8908, <i64 1, i64 1>
  %9122 = bitcast <2 x i64> %9118 to <4 x i32>
  %9123 = shufflevector <4 x i32> %9122, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %9124 = bitcast <4 x i32> %9123 to <2 x i64>
  %9125 = bitcast <2 x i64> %9119 to <4 x i32>
  %9126 = shufflevector <4 x i32> %9125, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %9127 = bitcast <4 x i32> %9126 to <2 x i64>
  %9128 = bitcast <2 x i64> %9120 to <4 x i32>
  %9129 = shufflevector <4 x i32> %9128, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %9130 = bitcast <4 x i32> %9129 to <2 x i64>
  %9131 = bitcast <2 x i64> %9121 to <4 x i32>
  %9132 = shufflevector <4 x i32> %9131, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %9133 = bitcast <4 x i32> %9132 to <2 x i64>
  %9134 = shufflevector <2 x i64> %9124, <2 x i64> %9127, <2 x i32> <i32 0, i32 2>
  %9135 = shufflevector <2 x i64> %9130, <2 x i64> %9133, <2 x i32> <i32 0, i32 2>
  %9136 = bitcast <2 x i64> %9134 to <4 x i32>
  %9137 = bitcast <2 x i64> %9135 to <4 x i32>
  %9138 = add <4 x i32> %9136, <i32 1, i32 1, i32 1, i32 1>
  %9139 = icmp ugt <4 x i32> %9138, <i32 1, i32 1, i32 1, i32 1>
  %9140 = sext <4 x i1> %9139 to <4 x i32>
  %9141 = bitcast <4 x i32> %9140 to <16 x i8>
  %9142 = icmp slt <16 x i8> %9141, zeroinitializer
  %9143 = bitcast <16 x i1> %9142 to i16
  %9144 = zext i16 %9143 to i32
  %9145 = add <4 x i32> %9137, <i32 1, i32 1, i32 1, i32 1>
  %9146 = icmp ugt <4 x i32> %9145, <i32 1, i32 1, i32 1, i32 1>
  %9147 = sext <4 x i1> %9146 to <4 x i32>
  %9148 = bitcast <4 x i32> %9147 to <16 x i8>
  %9149 = icmp slt <16 x i8> %9148, zeroinitializer
  %9150 = bitcast <16 x i1> %9149 to i16
  %9151 = zext i16 %9150 to i32
  %9152 = sub nsw i32 0, %9144
  %9153 = icmp eq i32 %9151, %9152
  br i1 %9153, label %9154, label %9228

9154:                                             ; preds = %9117
  %9155 = shl <2 x i64> %8911, <i64 1, i64 1>
  %9156 = shl <2 x i64> %8914, <i64 1, i64 1>
  %9157 = shl <2 x i64> %8917, <i64 1, i64 1>
  %9158 = shl <2 x i64> %8920, <i64 1, i64 1>
  %9159 = bitcast <2 x i64> %9155 to <4 x i32>
  %9160 = shufflevector <4 x i32> %9159, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %9161 = bitcast <4 x i32> %9160 to <2 x i64>
  %9162 = bitcast <2 x i64> %9156 to <4 x i32>
  %9163 = shufflevector <4 x i32> %9162, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %9164 = bitcast <4 x i32> %9163 to <2 x i64>
  %9165 = bitcast <2 x i64> %9157 to <4 x i32>
  %9166 = shufflevector <4 x i32> %9165, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %9167 = bitcast <4 x i32> %9166 to <2 x i64>
  %9168 = bitcast <2 x i64> %9158 to <4 x i32>
  %9169 = shufflevector <4 x i32> %9168, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %9170 = bitcast <4 x i32> %9169 to <2 x i64>
  %9171 = shufflevector <2 x i64> %9161, <2 x i64> %9164, <2 x i32> <i32 0, i32 2>
  %9172 = shufflevector <2 x i64> %9167, <2 x i64> %9170, <2 x i32> <i32 0, i32 2>
  %9173 = bitcast <2 x i64> %9171 to <4 x i32>
  %9174 = bitcast <2 x i64> %9172 to <4 x i32>
  %9175 = add <4 x i32> %9173, <i32 1, i32 1, i32 1, i32 1>
  %9176 = icmp ugt <4 x i32> %9175, <i32 1, i32 1, i32 1, i32 1>
  %9177 = sext <4 x i1> %9176 to <4 x i32>
  %9178 = bitcast <4 x i32> %9177 to <16 x i8>
  %9179 = icmp slt <16 x i8> %9178, zeroinitializer
  %9180 = bitcast <16 x i1> %9179 to i16
  %9181 = zext i16 %9180 to i32
  %9182 = add <4 x i32> %9174, <i32 1, i32 1, i32 1, i32 1>
  %9183 = icmp ugt <4 x i32> %9182, <i32 1, i32 1, i32 1, i32 1>
  %9184 = sext <4 x i1> %9183 to <4 x i32>
  %9185 = bitcast <4 x i32> %9184 to <16 x i8>
  %9186 = icmp slt <16 x i8> %9185, zeroinitializer
  %9187 = bitcast <16 x i1> %9186 to i16
  %9188 = zext i16 %9187 to i32
  %9189 = sub nsw i32 0, %9181
  %9190 = icmp eq i32 %9188, %9189
  br i1 %9190, label %9191, label %9228

9191:                                             ; preds = %9154
  %9192 = shl <2 x i64> %8923, <i64 1, i64 1>
  %9193 = shl <2 x i64> %8926, <i64 1, i64 1>
  %9194 = shl <2 x i64> %8929, <i64 1, i64 1>
  %9195 = shl <2 x i64> %8932, <i64 1, i64 1>
  %9196 = bitcast <2 x i64> %9192 to <4 x i32>
  %9197 = shufflevector <4 x i32> %9196, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %9198 = bitcast <4 x i32> %9197 to <2 x i64>
  %9199 = bitcast <2 x i64> %9193 to <4 x i32>
  %9200 = shufflevector <4 x i32> %9199, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %9201 = bitcast <4 x i32> %9200 to <2 x i64>
  %9202 = bitcast <2 x i64> %9194 to <4 x i32>
  %9203 = shufflevector <4 x i32> %9202, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %9204 = bitcast <4 x i32> %9203 to <2 x i64>
  %9205 = bitcast <2 x i64> %9195 to <4 x i32>
  %9206 = shufflevector <4 x i32> %9205, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %9207 = bitcast <4 x i32> %9206 to <2 x i64>
  %9208 = shufflevector <2 x i64> %9198, <2 x i64> %9201, <2 x i32> <i32 0, i32 2>
  %9209 = shufflevector <2 x i64> %9204, <2 x i64> %9207, <2 x i32> <i32 0, i32 2>
  %9210 = bitcast <2 x i64> %9208 to <4 x i32>
  %9211 = bitcast <2 x i64> %9209 to <4 x i32>
  %9212 = add <4 x i32> %9210, <i32 1, i32 1, i32 1, i32 1>
  %9213 = icmp ugt <4 x i32> %9212, <i32 1, i32 1, i32 1, i32 1>
  %9214 = sext <4 x i1> %9213 to <4 x i32>
  %9215 = bitcast <4 x i32> %9214 to <16 x i8>
  %9216 = icmp slt <16 x i8> %9215, zeroinitializer
  %9217 = bitcast <16 x i1> %9216 to i16
  %9218 = zext i16 %9217 to i32
  %9219 = add <4 x i32> %9211, <i32 1, i32 1, i32 1, i32 1>
  %9220 = icmp ugt <4 x i32> %9219, <i32 1, i32 1, i32 1, i32 1>
  %9221 = sext <4 x i1> %9220 to <4 x i32>
  %9222 = bitcast <4 x i32> %9221 to <16 x i8>
  %9223 = icmp slt <16 x i8> %9222, zeroinitializer
  %9224 = bitcast <16 x i1> %9223 to i16
  %9225 = zext i16 %9224 to i32
  %9226 = sub nsw i32 0, %9218
  %9227 = icmp eq i32 %9225, %9226
  br i1 %9227, label %9444, label %9228

9228:                                             ; preds = %9154, %9117, %9080, %9043, %9006, %8969, %8740, %9191
  %9229 = bitcast [32 x i64]* %4 to i8*
  %9230 = bitcast [32 x i64]* %5 to i8*
  %9231 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %9232 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %9233 = bitcast [32 x i64]* %5 to <2 x i64>*
  %9234 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %9235 = bitcast i64* %9234 to <2 x i64>*
  %9236 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %9237 = bitcast i64* %9236 to <2 x i64>*
  %9238 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %9239 = bitcast i64* %9238 to <2 x i64>*
  %9240 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %9241 = bitcast i64* %9240 to <2 x i64>*
  %9242 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %9243 = bitcast i64* %9242 to <2 x i64>*
  %9244 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %9245 = bitcast i64* %9244 to <2 x i64>*
  %9246 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %9247 = bitcast i64* %9246 to <2 x i64>*
  %9248 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %9249 = bitcast i64* %9248 to <2 x i64>*
  %9250 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %9251 = bitcast i64* %9250 to <2 x i64>*
  %9252 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %9253 = bitcast i64* %9252 to <2 x i64>*
  %9254 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %9255 = bitcast i64* %9254 to <2 x i64>*
  %9256 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %9257 = bitcast i64* %9256 to <2 x i64>*
  %9258 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %9259 = bitcast i64* %9258 to <2 x i64>*
  %9260 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %9261 = bitcast i64* %9260 to <2 x i64>*
  %9262 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %9263 = bitcast i64* %9262 to <2 x i64>*
  br label %9264

9264:                                             ; preds = %9297, %9228
  %9265 = phi i64 [ 0, %9228 ], [ %9442, %9297 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %9229) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %9229, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %9230) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %9230, i8 -86, i64 256, i1 false) #6
  br label %9266

9266:                                             ; preds = %9266, %9264
  %9267 = phi i64 [ 0, %9264 ], [ %9295, %9266 ]
  %9268 = shl i64 %9267, 5
  %9269 = add nuw nsw i64 %9268, %9265
  %9270 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %9269
  %9271 = load i16, i16* %9270, align 2
  %9272 = sext i16 %9271 to i64
  %9273 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %9267
  store i64 %9272, i64* %9273, align 16
  %9274 = or i64 %9267, 1
  %9275 = shl i64 %9274, 5
  %9276 = add nuw nsw i64 %9275, %9265
  %9277 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %9276
  %9278 = load i16, i16* %9277, align 2
  %9279 = sext i16 %9278 to i64
  %9280 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %9274
  store i64 %9279, i64* %9280, align 8
  %9281 = or i64 %9267, 2
  %9282 = shl i64 %9281, 5
  %9283 = add nuw nsw i64 %9282, %9265
  %9284 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %9283
  %9285 = load i16, i16* %9284, align 2
  %9286 = sext i16 %9285 to i64
  %9287 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %9281
  store i64 %9286, i64* %9287, align 16
  %9288 = or i64 %9267, 3
  %9289 = shl i64 %9288, 5
  %9290 = add nuw nsw i64 %9289, %9265
  %9291 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %9290
  %9292 = load i16, i16* %9291, align 2
  %9293 = sext i16 %9292 to i64
  %9294 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %9288
  store i64 %9293, i64* %9294, align 8
  %9295 = add nuw nsw i64 %9267, 4
  %9296 = icmp eq i64 %9295, 32
  br i1 %9296, label %9297, label %9266

9297:                                             ; preds = %9266
  call void @vpx_fdct32(i64* nonnull %9231, i64* nonnull %9232, i32 0) #6
  %9298 = shl i64 %9265, 5
  %9299 = load <2 x i64>, <2 x i64>* %9233, align 16
  %9300 = add nsw <2 x i64> %9299, <i64 1, i64 1>
  %9301 = lshr <2 x i64> %9299, <i64 63, i64 63>
  %9302 = add nsw <2 x i64> %9300, %9301
  %9303 = lshr <2 x i64> %9302, <i64 2, i64 2>
  %9304 = trunc <2 x i64> %9303 to <2 x i32>
  %9305 = getelementptr inbounds i32, i32* %1, i64 %9298
  %9306 = bitcast i32* %9305 to <2 x i32>*
  store <2 x i32> %9304, <2 x i32>* %9306, align 4
  %9307 = load <2 x i64>, <2 x i64>* %9235, align 16
  %9308 = add nsw <2 x i64> %9307, <i64 1, i64 1>
  %9309 = lshr <2 x i64> %9307, <i64 63, i64 63>
  %9310 = add nsw <2 x i64> %9308, %9309
  %9311 = lshr <2 x i64> %9310, <i64 2, i64 2>
  %9312 = trunc <2 x i64> %9311 to <2 x i32>
  %9313 = or i64 %9298, 2
  %9314 = getelementptr inbounds i32, i32* %1, i64 %9313
  %9315 = bitcast i32* %9314 to <2 x i32>*
  store <2 x i32> %9312, <2 x i32>* %9315, align 4
  %9316 = load <2 x i64>, <2 x i64>* %9237, align 16
  %9317 = add nsw <2 x i64> %9316, <i64 1, i64 1>
  %9318 = lshr <2 x i64> %9316, <i64 63, i64 63>
  %9319 = add nsw <2 x i64> %9317, %9318
  %9320 = lshr <2 x i64> %9319, <i64 2, i64 2>
  %9321 = trunc <2 x i64> %9320 to <2 x i32>
  %9322 = or i64 %9298, 4
  %9323 = getelementptr inbounds i32, i32* %1, i64 %9322
  %9324 = bitcast i32* %9323 to <2 x i32>*
  store <2 x i32> %9321, <2 x i32>* %9324, align 4
  %9325 = load <2 x i64>, <2 x i64>* %9239, align 16
  %9326 = add nsw <2 x i64> %9325, <i64 1, i64 1>
  %9327 = lshr <2 x i64> %9325, <i64 63, i64 63>
  %9328 = add nsw <2 x i64> %9326, %9327
  %9329 = lshr <2 x i64> %9328, <i64 2, i64 2>
  %9330 = trunc <2 x i64> %9329 to <2 x i32>
  %9331 = or i64 %9298, 6
  %9332 = getelementptr inbounds i32, i32* %1, i64 %9331
  %9333 = bitcast i32* %9332 to <2 x i32>*
  store <2 x i32> %9330, <2 x i32>* %9333, align 4
  %9334 = load <2 x i64>, <2 x i64>* %9241, align 16
  %9335 = add nsw <2 x i64> %9334, <i64 1, i64 1>
  %9336 = lshr <2 x i64> %9334, <i64 63, i64 63>
  %9337 = add nsw <2 x i64> %9335, %9336
  %9338 = lshr <2 x i64> %9337, <i64 2, i64 2>
  %9339 = trunc <2 x i64> %9338 to <2 x i32>
  %9340 = or i64 %9298, 8
  %9341 = getelementptr inbounds i32, i32* %1, i64 %9340
  %9342 = bitcast i32* %9341 to <2 x i32>*
  store <2 x i32> %9339, <2 x i32>* %9342, align 4
  %9343 = load <2 x i64>, <2 x i64>* %9243, align 16
  %9344 = add nsw <2 x i64> %9343, <i64 1, i64 1>
  %9345 = lshr <2 x i64> %9343, <i64 63, i64 63>
  %9346 = add nsw <2 x i64> %9344, %9345
  %9347 = lshr <2 x i64> %9346, <i64 2, i64 2>
  %9348 = trunc <2 x i64> %9347 to <2 x i32>
  %9349 = or i64 %9298, 10
  %9350 = getelementptr inbounds i32, i32* %1, i64 %9349
  %9351 = bitcast i32* %9350 to <2 x i32>*
  store <2 x i32> %9348, <2 x i32>* %9351, align 4
  %9352 = load <2 x i64>, <2 x i64>* %9245, align 16
  %9353 = add nsw <2 x i64> %9352, <i64 1, i64 1>
  %9354 = lshr <2 x i64> %9352, <i64 63, i64 63>
  %9355 = add nsw <2 x i64> %9353, %9354
  %9356 = lshr <2 x i64> %9355, <i64 2, i64 2>
  %9357 = trunc <2 x i64> %9356 to <2 x i32>
  %9358 = or i64 %9298, 12
  %9359 = getelementptr inbounds i32, i32* %1, i64 %9358
  %9360 = bitcast i32* %9359 to <2 x i32>*
  store <2 x i32> %9357, <2 x i32>* %9360, align 4
  %9361 = load <2 x i64>, <2 x i64>* %9247, align 16
  %9362 = add nsw <2 x i64> %9361, <i64 1, i64 1>
  %9363 = lshr <2 x i64> %9361, <i64 63, i64 63>
  %9364 = add nsw <2 x i64> %9362, %9363
  %9365 = lshr <2 x i64> %9364, <i64 2, i64 2>
  %9366 = trunc <2 x i64> %9365 to <2 x i32>
  %9367 = or i64 %9298, 14
  %9368 = getelementptr inbounds i32, i32* %1, i64 %9367
  %9369 = bitcast i32* %9368 to <2 x i32>*
  store <2 x i32> %9366, <2 x i32>* %9369, align 4
  %9370 = load <2 x i64>, <2 x i64>* %9249, align 16
  %9371 = add nsw <2 x i64> %9370, <i64 1, i64 1>
  %9372 = lshr <2 x i64> %9370, <i64 63, i64 63>
  %9373 = add nsw <2 x i64> %9371, %9372
  %9374 = lshr <2 x i64> %9373, <i64 2, i64 2>
  %9375 = trunc <2 x i64> %9374 to <2 x i32>
  %9376 = or i64 %9298, 16
  %9377 = getelementptr inbounds i32, i32* %1, i64 %9376
  %9378 = bitcast i32* %9377 to <2 x i32>*
  store <2 x i32> %9375, <2 x i32>* %9378, align 4
  %9379 = load <2 x i64>, <2 x i64>* %9251, align 16
  %9380 = add nsw <2 x i64> %9379, <i64 1, i64 1>
  %9381 = lshr <2 x i64> %9379, <i64 63, i64 63>
  %9382 = add nsw <2 x i64> %9380, %9381
  %9383 = lshr <2 x i64> %9382, <i64 2, i64 2>
  %9384 = trunc <2 x i64> %9383 to <2 x i32>
  %9385 = or i64 %9298, 18
  %9386 = getelementptr inbounds i32, i32* %1, i64 %9385
  %9387 = bitcast i32* %9386 to <2 x i32>*
  store <2 x i32> %9384, <2 x i32>* %9387, align 4
  %9388 = load <2 x i64>, <2 x i64>* %9253, align 16
  %9389 = add nsw <2 x i64> %9388, <i64 1, i64 1>
  %9390 = lshr <2 x i64> %9388, <i64 63, i64 63>
  %9391 = add nsw <2 x i64> %9389, %9390
  %9392 = lshr <2 x i64> %9391, <i64 2, i64 2>
  %9393 = trunc <2 x i64> %9392 to <2 x i32>
  %9394 = or i64 %9298, 20
  %9395 = getelementptr inbounds i32, i32* %1, i64 %9394
  %9396 = bitcast i32* %9395 to <2 x i32>*
  store <2 x i32> %9393, <2 x i32>* %9396, align 4
  %9397 = load <2 x i64>, <2 x i64>* %9255, align 16
  %9398 = add nsw <2 x i64> %9397, <i64 1, i64 1>
  %9399 = lshr <2 x i64> %9397, <i64 63, i64 63>
  %9400 = add nsw <2 x i64> %9398, %9399
  %9401 = lshr <2 x i64> %9400, <i64 2, i64 2>
  %9402 = trunc <2 x i64> %9401 to <2 x i32>
  %9403 = or i64 %9298, 22
  %9404 = getelementptr inbounds i32, i32* %1, i64 %9403
  %9405 = bitcast i32* %9404 to <2 x i32>*
  store <2 x i32> %9402, <2 x i32>* %9405, align 4
  %9406 = load <2 x i64>, <2 x i64>* %9257, align 16
  %9407 = add nsw <2 x i64> %9406, <i64 1, i64 1>
  %9408 = lshr <2 x i64> %9406, <i64 63, i64 63>
  %9409 = add nsw <2 x i64> %9407, %9408
  %9410 = lshr <2 x i64> %9409, <i64 2, i64 2>
  %9411 = trunc <2 x i64> %9410 to <2 x i32>
  %9412 = or i64 %9298, 24
  %9413 = getelementptr inbounds i32, i32* %1, i64 %9412
  %9414 = bitcast i32* %9413 to <2 x i32>*
  store <2 x i32> %9411, <2 x i32>* %9414, align 4
  %9415 = load <2 x i64>, <2 x i64>* %9259, align 16
  %9416 = add nsw <2 x i64> %9415, <i64 1, i64 1>
  %9417 = lshr <2 x i64> %9415, <i64 63, i64 63>
  %9418 = add nsw <2 x i64> %9416, %9417
  %9419 = lshr <2 x i64> %9418, <i64 2, i64 2>
  %9420 = trunc <2 x i64> %9419 to <2 x i32>
  %9421 = or i64 %9298, 26
  %9422 = getelementptr inbounds i32, i32* %1, i64 %9421
  %9423 = bitcast i32* %9422 to <2 x i32>*
  store <2 x i32> %9420, <2 x i32>* %9423, align 4
  %9424 = load <2 x i64>, <2 x i64>* %9261, align 16
  %9425 = add nsw <2 x i64> %9424, <i64 1, i64 1>
  %9426 = lshr <2 x i64> %9424, <i64 63, i64 63>
  %9427 = add nsw <2 x i64> %9425, %9426
  %9428 = lshr <2 x i64> %9427, <i64 2, i64 2>
  %9429 = trunc <2 x i64> %9428 to <2 x i32>
  %9430 = or i64 %9298, 28
  %9431 = getelementptr inbounds i32, i32* %1, i64 %9430
  %9432 = bitcast i32* %9431 to <2 x i32>*
  store <2 x i32> %9429, <2 x i32>* %9432, align 4
  %9433 = load <2 x i64>, <2 x i64>* %9263, align 16
  %9434 = add nsw <2 x i64> %9433, <i64 1, i64 1>
  %9435 = lshr <2 x i64> %9433, <i64 63, i64 63>
  %9436 = add nsw <2 x i64> %9434, %9435
  %9437 = lshr <2 x i64> %9436, <i64 2, i64 2>
  %9438 = trunc <2 x i64> %9437 to <2 x i32>
  %9439 = or i64 %9298, 30
  %9440 = getelementptr inbounds i32, i32* %1, i64 %9439
  %9441 = bitcast i32* %9440 to <2 x i32>*
  store <2 x i32> %9438, <2 x i32>* %9441, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %9230) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %9229) #6
  %9442 = add nuw nsw i64 %9265, 1
  %9443 = icmp eq i64 %9442, 32
  br i1 %9443, label %11292, label %9264

9444:                                             ; preds = %9191
  %9445 = bitcast <2 x i64> %8809 to <4 x i32>
  %9446 = shufflevector <4 x i32> %9445, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %9447 = bitcast <4 x i32> %9446 to <2 x i64>
  %9448 = bitcast <2 x i64> %8814 to <4 x i32>
  %9449 = shufflevector <4 x i32> %9448, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %9450 = bitcast <4 x i32> %9449 to <2 x i64>
  %9451 = shufflevector <2 x i64> %9447, <2 x i64> %9450, <2 x i32> <i32 0, i32 2>
  %9452 = bitcast <2 x i64> %8819 to <4 x i32>
  %9453 = shufflevector <4 x i32> %9452, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %9454 = bitcast <4 x i32> %9453 to <2 x i64>
  %9455 = bitcast <2 x i64> %8824 to <4 x i32>
  %9456 = shufflevector <4 x i32> %9455, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %9457 = bitcast <4 x i32> %9456 to <2 x i64>
  %9458 = shufflevector <2 x i64> %9454, <2 x i64> %9457, <2 x i32> <i32 0, i32 2>
  %9459 = bitcast <2 x i64> %8829 to <4 x i32>
  %9460 = shufflevector <4 x i32> %9459, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %9461 = bitcast <4 x i32> %9460 to <2 x i64>
  %9462 = bitcast <2 x i64> %8834 to <4 x i32>
  %9463 = shufflevector <4 x i32> %9462, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %9464 = bitcast <4 x i32> %9463 to <2 x i64>
  %9465 = shufflevector <2 x i64> %9461, <2 x i64> %9464, <2 x i32> <i32 0, i32 2>
  %9466 = bitcast <2 x i64> %8839 to <4 x i32>
  %9467 = shufflevector <4 x i32> %9466, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %9468 = bitcast <4 x i32> %9467 to <2 x i64>
  %9469 = bitcast <2 x i64> %8844 to <4 x i32>
  %9470 = shufflevector <4 x i32> %9469, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %9471 = bitcast <4 x i32> %9470 to <2 x i64>
  %9472 = shufflevector <2 x i64> %9468, <2 x i64> %9471, <2 x i32> <i32 0, i32 2>
  %9473 = bitcast <2 x i64> %8849 to <4 x i32>
  %9474 = shufflevector <4 x i32> %9473, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %9475 = bitcast <4 x i32> %9474 to <2 x i64>
  %9476 = bitcast <2 x i64> %8854 to <4 x i32>
  %9477 = shufflevector <4 x i32> %9476, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %9478 = bitcast <4 x i32> %9477 to <2 x i64>
  %9479 = shufflevector <2 x i64> %9475, <2 x i64> %9478, <2 x i32> <i32 0, i32 2>
  %9480 = bitcast <2 x i64> %8859 to <4 x i32>
  %9481 = shufflevector <4 x i32> %9480, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %9482 = bitcast <4 x i32> %9481 to <2 x i64>
  %9483 = bitcast <2 x i64> %8864 to <4 x i32>
  %9484 = shufflevector <4 x i32> %9483, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %9485 = bitcast <4 x i32> %9484 to <2 x i64>
  %9486 = shufflevector <2 x i64> %9482, <2 x i64> %9485, <2 x i32> <i32 0, i32 2>
  %9487 = bitcast <2 x i64> %8869 to <4 x i32>
  %9488 = shufflevector <4 x i32> %9487, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %9489 = bitcast <4 x i32> %9488 to <2 x i64>
  %9490 = bitcast <2 x i64> %8874 to <4 x i32>
  %9491 = shufflevector <4 x i32> %9490, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %9492 = bitcast <4 x i32> %9491 to <2 x i64>
  %9493 = shufflevector <2 x i64> %9489, <2 x i64> %9492, <2 x i32> <i32 0, i32 2>
  %9494 = bitcast <2 x i64> %8879 to <4 x i32>
  %9495 = shufflevector <4 x i32> %9494, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %9496 = bitcast <4 x i32> %9495 to <2 x i64>
  %9497 = bitcast <2 x i64> %8884 to <4 x i32>
  %9498 = shufflevector <4 x i32> %9497, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %9499 = bitcast <4 x i32> %9498 to <2 x i64>
  %9500 = shufflevector <2 x i64> %9496, <2 x i64> %9499, <2 x i32> <i32 0, i32 2>
  %9501 = bitcast <2 x i64> %8887 to <4 x i32>
  %9502 = shufflevector <4 x i32> %9501, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %9503 = bitcast <4 x i32> %9502 to <2 x i64>
  %9504 = bitcast <2 x i64> %8890 to <4 x i32>
  %9505 = shufflevector <4 x i32> %9504, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %9506 = bitcast <4 x i32> %9505 to <2 x i64>
  %9507 = shufflevector <2 x i64> %9503, <2 x i64> %9506, <2 x i32> <i32 0, i32 2>
  %9508 = bitcast <2 x i64> %8893 to <4 x i32>
  %9509 = shufflevector <4 x i32> %9508, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %9510 = bitcast <4 x i32> %9509 to <2 x i64>
  %9511 = bitcast <2 x i64> %8896 to <4 x i32>
  %9512 = shufflevector <4 x i32> %9511, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %9513 = bitcast <4 x i32> %9512 to <2 x i64>
  %9514 = shufflevector <2 x i64> %9510, <2 x i64> %9513, <2 x i32> <i32 0, i32 2>
  %9515 = bitcast <2 x i64> %8899 to <4 x i32>
  %9516 = shufflevector <4 x i32> %9515, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %9517 = bitcast <4 x i32> %9516 to <2 x i64>
  %9518 = bitcast <2 x i64> %8902 to <4 x i32>
  %9519 = shufflevector <4 x i32> %9518, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %9520 = bitcast <4 x i32> %9519 to <2 x i64>
  %9521 = shufflevector <2 x i64> %9517, <2 x i64> %9520, <2 x i32> <i32 0, i32 2>
  %9522 = bitcast <2 x i64> %8905 to <4 x i32>
  %9523 = shufflevector <4 x i32> %9522, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %9524 = bitcast <4 x i32> %9523 to <2 x i64>
  %9525 = bitcast <2 x i64> %8908 to <4 x i32>
  %9526 = shufflevector <4 x i32> %9525, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %9527 = bitcast <4 x i32> %9526 to <2 x i64>
  %9528 = shufflevector <2 x i64> %9524, <2 x i64> %9527, <2 x i32> <i32 0, i32 2>
  %9529 = bitcast <2 x i64> %8911 to <4 x i32>
  %9530 = shufflevector <4 x i32> %9529, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %9531 = bitcast <4 x i32> %9530 to <2 x i64>
  %9532 = bitcast <2 x i64> %8914 to <4 x i32>
  %9533 = shufflevector <4 x i32> %9532, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %9534 = bitcast <4 x i32> %9533 to <2 x i64>
  %9535 = shufflevector <2 x i64> %9531, <2 x i64> %9534, <2 x i32> <i32 0, i32 2>
  %9536 = bitcast <2 x i64> %8917 to <4 x i32>
  %9537 = shufflevector <4 x i32> %9536, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %9538 = bitcast <4 x i32> %9537 to <2 x i64>
  %9539 = bitcast <2 x i64> %8920 to <4 x i32>
  %9540 = shufflevector <4 x i32> %9539, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %9541 = bitcast <4 x i32> %9540 to <2 x i64>
  %9542 = shufflevector <2 x i64> %9538, <2 x i64> %9541, <2 x i32> <i32 0, i32 2>
  %9543 = bitcast <2 x i64> %8923 to <4 x i32>
  %9544 = shufflevector <4 x i32> %9543, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %9545 = bitcast <4 x i32> %9544 to <2 x i64>
  %9546 = bitcast <2 x i64> %8926 to <4 x i32>
  %9547 = shufflevector <4 x i32> %9546, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %9548 = bitcast <4 x i32> %9547 to <2 x i64>
  %9549 = shufflevector <2 x i64> %9545, <2 x i64> %9548, <2 x i32> <i32 0, i32 2>
  %9550 = bitcast <2 x i64> %8929 to <4 x i32>
  %9551 = shufflevector <4 x i32> %9550, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %9552 = bitcast <4 x i32> %9551 to <2 x i64>
  %9553 = bitcast <2 x i64> %8932 to <4 x i32>
  %9554 = shufflevector <4 x i32> %9553, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %9555 = bitcast <4 x i32> %9554 to <2 x i64>
  %9556 = shufflevector <2 x i64> %9552, <2 x i64> %9555, <2 x i32> <i32 0, i32 2>
  %9557 = bitcast <2 x i64> %9451 to <4 x i32>
  %9558 = add <4 x i32> %9557, <i32 8192, i32 8192, i32 8192, i32 8192>
  %9559 = bitcast <2 x i64> %9458 to <4 x i32>
  %9560 = add <4 x i32> %9559, <i32 8192, i32 8192, i32 8192, i32 8192>
  %9561 = bitcast <2 x i64> %9465 to <4 x i32>
  %9562 = add <4 x i32> %9561, <i32 8192, i32 8192, i32 8192, i32 8192>
  %9563 = bitcast <2 x i64> %9472 to <4 x i32>
  %9564 = add <4 x i32> %9563, <i32 8192, i32 8192, i32 8192, i32 8192>
  %9565 = bitcast <2 x i64> %9479 to <4 x i32>
  %9566 = add <4 x i32> %9565, <i32 8192, i32 8192, i32 8192, i32 8192>
  %9567 = bitcast <2 x i64> %9486 to <4 x i32>
  %9568 = add <4 x i32> %9567, <i32 8192, i32 8192, i32 8192, i32 8192>
  %9569 = bitcast <2 x i64> %9493 to <4 x i32>
  %9570 = add <4 x i32> %9569, <i32 8192, i32 8192, i32 8192, i32 8192>
  %9571 = bitcast <2 x i64> %9500 to <4 x i32>
  %9572 = add <4 x i32> %9571, <i32 8192, i32 8192, i32 8192, i32 8192>
  %9573 = bitcast <2 x i64> %9507 to <4 x i32>
  %9574 = add <4 x i32> %9573, <i32 8192, i32 8192, i32 8192, i32 8192>
  %9575 = bitcast <2 x i64> %9514 to <4 x i32>
  %9576 = add <4 x i32> %9575, <i32 8192, i32 8192, i32 8192, i32 8192>
  %9577 = bitcast <2 x i64> %9521 to <4 x i32>
  %9578 = add <4 x i32> %9577, <i32 8192, i32 8192, i32 8192, i32 8192>
  %9579 = bitcast <2 x i64> %9528 to <4 x i32>
  %9580 = add <4 x i32> %9579, <i32 8192, i32 8192, i32 8192, i32 8192>
  %9581 = bitcast <2 x i64> %9535 to <4 x i32>
  %9582 = add <4 x i32> %9581, <i32 8192, i32 8192, i32 8192, i32 8192>
  %9583 = bitcast <2 x i64> %9542 to <4 x i32>
  %9584 = add <4 x i32> %9583, <i32 8192, i32 8192, i32 8192, i32 8192>
  %9585 = bitcast <2 x i64> %9549 to <4 x i32>
  %9586 = add <4 x i32> %9585, <i32 8192, i32 8192, i32 8192, i32 8192>
  %9587 = bitcast <2 x i64> %9556 to <4 x i32>
  %9588 = add <4 x i32> %9587, <i32 8192, i32 8192, i32 8192, i32 8192>
  %9589 = ashr <4 x i32> %9558, <i32 14, i32 14, i32 14, i32 14>
  %9590 = ashr <4 x i32> %9560, <i32 14, i32 14, i32 14, i32 14>
  %9591 = ashr <4 x i32> %9562, <i32 14, i32 14, i32 14, i32 14>
  %9592 = ashr <4 x i32> %9564, <i32 14, i32 14, i32 14, i32 14>
  %9593 = ashr <4 x i32> %9566, <i32 14, i32 14, i32 14, i32 14>
  %9594 = ashr <4 x i32> %9568, <i32 14, i32 14, i32 14, i32 14>
  %9595 = ashr <4 x i32> %9570, <i32 14, i32 14, i32 14, i32 14>
  %9596 = ashr <4 x i32> %9572, <i32 14, i32 14, i32 14, i32 14>
  %9597 = ashr <4 x i32> %9574, <i32 14, i32 14, i32 14, i32 14>
  %9598 = ashr <4 x i32> %9576, <i32 14, i32 14, i32 14, i32 14>
  %9599 = ashr <4 x i32> %9578, <i32 14, i32 14, i32 14, i32 14>
  %9600 = ashr <4 x i32> %9580, <i32 14, i32 14, i32 14, i32 14>
  %9601 = ashr <4 x i32> %9582, <i32 14, i32 14, i32 14, i32 14>
  %9602 = ashr <4 x i32> %9584, <i32 14, i32 14, i32 14, i32 14>
  %9603 = ashr <4 x i32> %9586, <i32 14, i32 14, i32 14, i32 14>
  %9604 = ashr <4 x i32> %9588, <i32 14, i32 14, i32 14, i32 14>
  %9605 = lshr <4 x i32> %9558, <i32 31, i32 31, i32 31, i32 31>
  %9606 = lshr <4 x i32> %9560, <i32 31, i32 31, i32 31, i32 31>
  %9607 = lshr <4 x i32> %9562, <i32 31, i32 31, i32 31, i32 31>
  %9608 = lshr <4 x i32> %9564, <i32 31, i32 31, i32 31, i32 31>
  %9609 = lshr <4 x i32> %9566, <i32 31, i32 31, i32 31, i32 31>
  %9610 = lshr <4 x i32> %9568, <i32 31, i32 31, i32 31, i32 31>
  %9611 = lshr <4 x i32> %9570, <i32 31, i32 31, i32 31, i32 31>
  %9612 = lshr <4 x i32> %9572, <i32 31, i32 31, i32 31, i32 31>
  %9613 = lshr <4 x i32> %9574, <i32 31, i32 31, i32 31, i32 31>
  %9614 = lshr <4 x i32> %9576, <i32 31, i32 31, i32 31, i32 31>
  %9615 = lshr <4 x i32> %9578, <i32 31, i32 31, i32 31, i32 31>
  %9616 = lshr <4 x i32> %9580, <i32 31, i32 31, i32 31, i32 31>
  %9617 = lshr <4 x i32> %9582, <i32 31, i32 31, i32 31, i32 31>
  %9618 = lshr <4 x i32> %9584, <i32 31, i32 31, i32 31, i32 31>
  %9619 = lshr <4 x i32> %9586, <i32 31, i32 31, i32 31, i32 31>
  %9620 = lshr <4 x i32> %9588, <i32 31, i32 31, i32 31, i32 31>
  %9621 = add nuw nsw <4 x i32> %9605, <i32 1, i32 1, i32 1, i32 1>
  %9622 = add nsw <4 x i32> %9621, %9589
  %9623 = add nuw nsw <4 x i32> %9606, <i32 1, i32 1, i32 1, i32 1>
  %9624 = add nsw <4 x i32> %9623, %9590
  %9625 = add nuw nsw <4 x i32> %9607, <i32 1, i32 1, i32 1, i32 1>
  %9626 = add nsw <4 x i32> %9625, %9591
  %9627 = add nuw nsw <4 x i32> %9608, <i32 1, i32 1, i32 1, i32 1>
  %9628 = add nsw <4 x i32> %9627, %9592
  %9629 = add nuw nsw <4 x i32> %9609, <i32 1, i32 1, i32 1, i32 1>
  %9630 = add nsw <4 x i32> %9629, %9593
  %9631 = add nuw nsw <4 x i32> %9610, <i32 1, i32 1, i32 1, i32 1>
  %9632 = add nsw <4 x i32> %9631, %9594
  %9633 = add nuw nsw <4 x i32> %9611, <i32 1, i32 1, i32 1, i32 1>
  %9634 = add nsw <4 x i32> %9633, %9595
  %9635 = add nuw nsw <4 x i32> %9612, <i32 1, i32 1, i32 1, i32 1>
  %9636 = add nsw <4 x i32> %9635, %9596
  %9637 = add nuw nsw <4 x i32> %9613, <i32 1, i32 1, i32 1, i32 1>
  %9638 = add nsw <4 x i32> %9637, %9597
  %9639 = add nuw nsw <4 x i32> %9614, <i32 1, i32 1, i32 1, i32 1>
  %9640 = add nsw <4 x i32> %9639, %9598
  %9641 = add nuw nsw <4 x i32> %9615, <i32 1, i32 1, i32 1, i32 1>
  %9642 = add nsw <4 x i32> %9641, %9599
  %9643 = add nuw nsw <4 x i32> %9616, <i32 1, i32 1, i32 1, i32 1>
  %9644 = add nsw <4 x i32> %9643, %9600
  %9645 = add nuw nsw <4 x i32> %9617, <i32 1, i32 1, i32 1, i32 1>
  %9646 = add nsw <4 x i32> %9645, %9601
  %9647 = add nuw nsw <4 x i32> %9618, <i32 1, i32 1, i32 1, i32 1>
  %9648 = add nsw <4 x i32> %9647, %9602
  %9649 = add nuw nsw <4 x i32> %9619, <i32 1, i32 1, i32 1, i32 1>
  %9650 = add nsw <4 x i32> %9649, %9603
  %9651 = add nuw nsw <4 x i32> %9620, <i32 1, i32 1, i32 1, i32 1>
  %9652 = add nsw <4 x i32> %9651, %9604
  %9653 = ashr <4 x i32> %9622, <i32 2, i32 2, i32 2, i32 2>
  %9654 = ashr <4 x i32> %9624, <i32 2, i32 2, i32 2, i32 2>
  %9655 = ashr <4 x i32> %9626, <i32 2, i32 2, i32 2, i32 2>
  %9656 = ashr <4 x i32> %9628, <i32 2, i32 2, i32 2, i32 2>
  %9657 = ashr <4 x i32> %9630, <i32 2, i32 2, i32 2, i32 2>
  %9658 = ashr <4 x i32> %9632, <i32 2, i32 2, i32 2, i32 2>
  %9659 = ashr <4 x i32> %9634, <i32 2, i32 2, i32 2, i32 2>
  %9660 = ashr <4 x i32> %9636, <i32 2, i32 2, i32 2, i32 2>
  %9661 = ashr <4 x i32> %9638, <i32 2, i32 2, i32 2, i32 2>
  %9662 = ashr <4 x i32> %9640, <i32 2, i32 2, i32 2, i32 2>
  %9663 = ashr <4 x i32> %9642, <i32 2, i32 2, i32 2, i32 2>
  %9664 = ashr <4 x i32> %9644, <i32 2, i32 2, i32 2, i32 2>
  %9665 = ashr <4 x i32> %9646, <i32 2, i32 2, i32 2, i32 2>
  %9666 = ashr <4 x i32> %9648, <i32 2, i32 2, i32 2, i32 2>
  %9667 = ashr <4 x i32> %9650, <i32 2, i32 2, i32 2, i32 2>
  %9668 = ashr <4 x i32> %9652, <i32 2, i32 2, i32 2, i32 2>
  %9669 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %9653, <4 x i32> %9654) #6
  store <8 x i16> %9669, <8 x i16>* %64, align 16
  %9670 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %9655, <4 x i32> %9656) #6
  store <8 x i16> %9670, <8 x i16>* %66, align 16
  %9671 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %9657, <4 x i32> %9658) #6
  store <8 x i16> %9671, <8 x i16>* %68, align 16
  %9672 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %9659, <4 x i32> %9660) #6
  store <8 x i16> %9672, <8 x i16>* %70, align 16
  %9673 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %9661, <4 x i32> %9662) #6
  store <8 x i16> %9673, <8 x i16>* %72, align 16
  %9674 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %9663, <4 x i32> %9664) #6
  store <8 x i16> %9674, <8 x i16>* %74, align 16
  %9675 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %9665, <4 x i32> %9666) #6
  store <8 x i16> %9675, <8 x i16>* %76, align 16
  %9676 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %9667, <4 x i32> %9668) #6
  store <8 x i16> %9676, <8 x i16>* %78, align 16
  %9677 = add <8 x i16> %9669, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %9678 = icmp ult <8 x i16> %9677, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %9679 = add <8 x i16> %9670, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %9680 = icmp ult <8 x i16> %9679, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %9681 = add <8 x i16> %9671, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %9682 = icmp ult <8 x i16> %9681, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %9683 = add <8 x i16> %9672, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %9684 = icmp ult <8 x i16> %9683, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %9685 = or <8 x i1> %9680, %9678
  %9686 = or <8 x i1> %9685, %9682
  %9687 = or <8 x i1> %9686, %9684
  %9688 = sext <8 x i1> %9687 to <8 x i16>
  %9689 = bitcast <8 x i16> %9688 to <16 x i8>
  %9690 = icmp slt <16 x i8> %9689, zeroinitializer
  %9691 = bitcast <16 x i1> %9690 to i16
  %9692 = zext i16 %9691 to i32
  %9693 = add <8 x i16> %9673, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %9694 = icmp ult <8 x i16> %9693, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %9695 = add <8 x i16> %9674, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %9696 = icmp ult <8 x i16> %9695, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %9697 = add <8 x i16> %9675, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %9698 = icmp ult <8 x i16> %9697, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %9699 = add <8 x i16> %9676, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %9700 = icmp ult <8 x i16> %9699, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %9701 = or <8 x i1> %9696, %9694
  %9702 = or <8 x i1> %9701, %9698
  %9703 = or <8 x i1> %9702, %9700
  %9704 = sext <8 x i1> %9703 to <8 x i16>
  %9705 = bitcast <8 x i16> %9704 to <16 x i8>
  %9706 = icmp slt <16 x i8> %9705, zeroinitializer
  %9707 = bitcast <16 x i1> %9706 to i16
  %9708 = zext i16 %9707 to i32
  %9709 = sub nsw i32 0, %9692
  %9710 = icmp eq i32 %9708, %9709
  br i1 %9710, label %9927, label %9711

9711:                                             ; preds = %9444
  %9712 = bitcast [32 x i64]* %4 to i8*
  %9713 = bitcast [32 x i64]* %5 to i8*
  %9714 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %9715 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %9716 = bitcast [32 x i64]* %5 to <2 x i64>*
  %9717 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %9718 = bitcast i64* %9717 to <2 x i64>*
  %9719 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %9720 = bitcast i64* %9719 to <2 x i64>*
  %9721 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %9722 = bitcast i64* %9721 to <2 x i64>*
  %9723 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %9724 = bitcast i64* %9723 to <2 x i64>*
  %9725 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %9726 = bitcast i64* %9725 to <2 x i64>*
  %9727 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %9728 = bitcast i64* %9727 to <2 x i64>*
  %9729 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %9730 = bitcast i64* %9729 to <2 x i64>*
  %9731 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %9732 = bitcast i64* %9731 to <2 x i64>*
  %9733 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %9734 = bitcast i64* %9733 to <2 x i64>*
  %9735 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %9736 = bitcast i64* %9735 to <2 x i64>*
  %9737 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %9738 = bitcast i64* %9737 to <2 x i64>*
  %9739 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %9740 = bitcast i64* %9739 to <2 x i64>*
  %9741 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %9742 = bitcast i64* %9741 to <2 x i64>*
  %9743 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %9744 = bitcast i64* %9743 to <2 x i64>*
  %9745 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %9746 = bitcast i64* %9745 to <2 x i64>*
  br label %9747

9747:                                             ; preds = %9780, %9711
  %9748 = phi i64 [ 0, %9711 ], [ %9925, %9780 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %9712) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %9712, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %9713) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %9713, i8 -86, i64 256, i1 false) #6
  br label %9749

9749:                                             ; preds = %9749, %9747
  %9750 = phi i64 [ 0, %9747 ], [ %9778, %9749 ]
  %9751 = shl i64 %9750, 5
  %9752 = add nuw nsw i64 %9751, %9748
  %9753 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %9752
  %9754 = load i16, i16* %9753, align 2
  %9755 = sext i16 %9754 to i64
  %9756 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %9750
  store i64 %9755, i64* %9756, align 16
  %9757 = or i64 %9750, 1
  %9758 = shl i64 %9757, 5
  %9759 = add nuw nsw i64 %9758, %9748
  %9760 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %9759
  %9761 = load i16, i16* %9760, align 2
  %9762 = sext i16 %9761 to i64
  %9763 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %9757
  store i64 %9762, i64* %9763, align 8
  %9764 = or i64 %9750, 2
  %9765 = shl i64 %9764, 5
  %9766 = add nuw nsw i64 %9765, %9748
  %9767 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %9766
  %9768 = load i16, i16* %9767, align 2
  %9769 = sext i16 %9768 to i64
  %9770 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %9764
  store i64 %9769, i64* %9770, align 16
  %9771 = or i64 %9750, 3
  %9772 = shl i64 %9771, 5
  %9773 = add nuw nsw i64 %9772, %9748
  %9774 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %9773
  %9775 = load i16, i16* %9774, align 2
  %9776 = sext i16 %9775 to i64
  %9777 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %9771
  store i64 %9776, i64* %9777, align 8
  %9778 = add nuw nsw i64 %9750, 4
  %9779 = icmp eq i64 %9778, 32
  br i1 %9779, label %9780, label %9749

9780:                                             ; preds = %9749
  call void @vpx_fdct32(i64* nonnull %9714, i64* nonnull %9715, i32 0) #6
  %9781 = shl i64 %9748, 5
  %9782 = load <2 x i64>, <2 x i64>* %9716, align 16
  %9783 = add nsw <2 x i64> %9782, <i64 1, i64 1>
  %9784 = lshr <2 x i64> %9782, <i64 63, i64 63>
  %9785 = add nsw <2 x i64> %9783, %9784
  %9786 = lshr <2 x i64> %9785, <i64 2, i64 2>
  %9787 = trunc <2 x i64> %9786 to <2 x i32>
  %9788 = getelementptr inbounds i32, i32* %1, i64 %9781
  %9789 = bitcast i32* %9788 to <2 x i32>*
  store <2 x i32> %9787, <2 x i32>* %9789, align 4
  %9790 = load <2 x i64>, <2 x i64>* %9718, align 16
  %9791 = add nsw <2 x i64> %9790, <i64 1, i64 1>
  %9792 = lshr <2 x i64> %9790, <i64 63, i64 63>
  %9793 = add nsw <2 x i64> %9791, %9792
  %9794 = lshr <2 x i64> %9793, <i64 2, i64 2>
  %9795 = trunc <2 x i64> %9794 to <2 x i32>
  %9796 = or i64 %9781, 2
  %9797 = getelementptr inbounds i32, i32* %1, i64 %9796
  %9798 = bitcast i32* %9797 to <2 x i32>*
  store <2 x i32> %9795, <2 x i32>* %9798, align 4
  %9799 = load <2 x i64>, <2 x i64>* %9720, align 16
  %9800 = add nsw <2 x i64> %9799, <i64 1, i64 1>
  %9801 = lshr <2 x i64> %9799, <i64 63, i64 63>
  %9802 = add nsw <2 x i64> %9800, %9801
  %9803 = lshr <2 x i64> %9802, <i64 2, i64 2>
  %9804 = trunc <2 x i64> %9803 to <2 x i32>
  %9805 = or i64 %9781, 4
  %9806 = getelementptr inbounds i32, i32* %1, i64 %9805
  %9807 = bitcast i32* %9806 to <2 x i32>*
  store <2 x i32> %9804, <2 x i32>* %9807, align 4
  %9808 = load <2 x i64>, <2 x i64>* %9722, align 16
  %9809 = add nsw <2 x i64> %9808, <i64 1, i64 1>
  %9810 = lshr <2 x i64> %9808, <i64 63, i64 63>
  %9811 = add nsw <2 x i64> %9809, %9810
  %9812 = lshr <2 x i64> %9811, <i64 2, i64 2>
  %9813 = trunc <2 x i64> %9812 to <2 x i32>
  %9814 = or i64 %9781, 6
  %9815 = getelementptr inbounds i32, i32* %1, i64 %9814
  %9816 = bitcast i32* %9815 to <2 x i32>*
  store <2 x i32> %9813, <2 x i32>* %9816, align 4
  %9817 = load <2 x i64>, <2 x i64>* %9724, align 16
  %9818 = add nsw <2 x i64> %9817, <i64 1, i64 1>
  %9819 = lshr <2 x i64> %9817, <i64 63, i64 63>
  %9820 = add nsw <2 x i64> %9818, %9819
  %9821 = lshr <2 x i64> %9820, <i64 2, i64 2>
  %9822 = trunc <2 x i64> %9821 to <2 x i32>
  %9823 = or i64 %9781, 8
  %9824 = getelementptr inbounds i32, i32* %1, i64 %9823
  %9825 = bitcast i32* %9824 to <2 x i32>*
  store <2 x i32> %9822, <2 x i32>* %9825, align 4
  %9826 = load <2 x i64>, <2 x i64>* %9726, align 16
  %9827 = add nsw <2 x i64> %9826, <i64 1, i64 1>
  %9828 = lshr <2 x i64> %9826, <i64 63, i64 63>
  %9829 = add nsw <2 x i64> %9827, %9828
  %9830 = lshr <2 x i64> %9829, <i64 2, i64 2>
  %9831 = trunc <2 x i64> %9830 to <2 x i32>
  %9832 = or i64 %9781, 10
  %9833 = getelementptr inbounds i32, i32* %1, i64 %9832
  %9834 = bitcast i32* %9833 to <2 x i32>*
  store <2 x i32> %9831, <2 x i32>* %9834, align 4
  %9835 = load <2 x i64>, <2 x i64>* %9728, align 16
  %9836 = add nsw <2 x i64> %9835, <i64 1, i64 1>
  %9837 = lshr <2 x i64> %9835, <i64 63, i64 63>
  %9838 = add nsw <2 x i64> %9836, %9837
  %9839 = lshr <2 x i64> %9838, <i64 2, i64 2>
  %9840 = trunc <2 x i64> %9839 to <2 x i32>
  %9841 = or i64 %9781, 12
  %9842 = getelementptr inbounds i32, i32* %1, i64 %9841
  %9843 = bitcast i32* %9842 to <2 x i32>*
  store <2 x i32> %9840, <2 x i32>* %9843, align 4
  %9844 = load <2 x i64>, <2 x i64>* %9730, align 16
  %9845 = add nsw <2 x i64> %9844, <i64 1, i64 1>
  %9846 = lshr <2 x i64> %9844, <i64 63, i64 63>
  %9847 = add nsw <2 x i64> %9845, %9846
  %9848 = lshr <2 x i64> %9847, <i64 2, i64 2>
  %9849 = trunc <2 x i64> %9848 to <2 x i32>
  %9850 = or i64 %9781, 14
  %9851 = getelementptr inbounds i32, i32* %1, i64 %9850
  %9852 = bitcast i32* %9851 to <2 x i32>*
  store <2 x i32> %9849, <2 x i32>* %9852, align 4
  %9853 = load <2 x i64>, <2 x i64>* %9732, align 16
  %9854 = add nsw <2 x i64> %9853, <i64 1, i64 1>
  %9855 = lshr <2 x i64> %9853, <i64 63, i64 63>
  %9856 = add nsw <2 x i64> %9854, %9855
  %9857 = lshr <2 x i64> %9856, <i64 2, i64 2>
  %9858 = trunc <2 x i64> %9857 to <2 x i32>
  %9859 = or i64 %9781, 16
  %9860 = getelementptr inbounds i32, i32* %1, i64 %9859
  %9861 = bitcast i32* %9860 to <2 x i32>*
  store <2 x i32> %9858, <2 x i32>* %9861, align 4
  %9862 = load <2 x i64>, <2 x i64>* %9734, align 16
  %9863 = add nsw <2 x i64> %9862, <i64 1, i64 1>
  %9864 = lshr <2 x i64> %9862, <i64 63, i64 63>
  %9865 = add nsw <2 x i64> %9863, %9864
  %9866 = lshr <2 x i64> %9865, <i64 2, i64 2>
  %9867 = trunc <2 x i64> %9866 to <2 x i32>
  %9868 = or i64 %9781, 18
  %9869 = getelementptr inbounds i32, i32* %1, i64 %9868
  %9870 = bitcast i32* %9869 to <2 x i32>*
  store <2 x i32> %9867, <2 x i32>* %9870, align 4
  %9871 = load <2 x i64>, <2 x i64>* %9736, align 16
  %9872 = add nsw <2 x i64> %9871, <i64 1, i64 1>
  %9873 = lshr <2 x i64> %9871, <i64 63, i64 63>
  %9874 = add nsw <2 x i64> %9872, %9873
  %9875 = lshr <2 x i64> %9874, <i64 2, i64 2>
  %9876 = trunc <2 x i64> %9875 to <2 x i32>
  %9877 = or i64 %9781, 20
  %9878 = getelementptr inbounds i32, i32* %1, i64 %9877
  %9879 = bitcast i32* %9878 to <2 x i32>*
  store <2 x i32> %9876, <2 x i32>* %9879, align 4
  %9880 = load <2 x i64>, <2 x i64>* %9738, align 16
  %9881 = add nsw <2 x i64> %9880, <i64 1, i64 1>
  %9882 = lshr <2 x i64> %9880, <i64 63, i64 63>
  %9883 = add nsw <2 x i64> %9881, %9882
  %9884 = lshr <2 x i64> %9883, <i64 2, i64 2>
  %9885 = trunc <2 x i64> %9884 to <2 x i32>
  %9886 = or i64 %9781, 22
  %9887 = getelementptr inbounds i32, i32* %1, i64 %9886
  %9888 = bitcast i32* %9887 to <2 x i32>*
  store <2 x i32> %9885, <2 x i32>* %9888, align 4
  %9889 = load <2 x i64>, <2 x i64>* %9740, align 16
  %9890 = add nsw <2 x i64> %9889, <i64 1, i64 1>
  %9891 = lshr <2 x i64> %9889, <i64 63, i64 63>
  %9892 = add nsw <2 x i64> %9890, %9891
  %9893 = lshr <2 x i64> %9892, <i64 2, i64 2>
  %9894 = trunc <2 x i64> %9893 to <2 x i32>
  %9895 = or i64 %9781, 24
  %9896 = getelementptr inbounds i32, i32* %1, i64 %9895
  %9897 = bitcast i32* %9896 to <2 x i32>*
  store <2 x i32> %9894, <2 x i32>* %9897, align 4
  %9898 = load <2 x i64>, <2 x i64>* %9742, align 16
  %9899 = add nsw <2 x i64> %9898, <i64 1, i64 1>
  %9900 = lshr <2 x i64> %9898, <i64 63, i64 63>
  %9901 = add nsw <2 x i64> %9899, %9900
  %9902 = lshr <2 x i64> %9901, <i64 2, i64 2>
  %9903 = trunc <2 x i64> %9902 to <2 x i32>
  %9904 = or i64 %9781, 26
  %9905 = getelementptr inbounds i32, i32* %1, i64 %9904
  %9906 = bitcast i32* %9905 to <2 x i32>*
  store <2 x i32> %9903, <2 x i32>* %9906, align 4
  %9907 = load <2 x i64>, <2 x i64>* %9744, align 16
  %9908 = add nsw <2 x i64> %9907, <i64 1, i64 1>
  %9909 = lshr <2 x i64> %9907, <i64 63, i64 63>
  %9910 = add nsw <2 x i64> %9908, %9909
  %9911 = lshr <2 x i64> %9910, <i64 2, i64 2>
  %9912 = trunc <2 x i64> %9911 to <2 x i32>
  %9913 = or i64 %9781, 28
  %9914 = getelementptr inbounds i32, i32* %1, i64 %9913
  %9915 = bitcast i32* %9914 to <2 x i32>*
  store <2 x i32> %9912, <2 x i32>* %9915, align 4
  %9916 = load <2 x i64>, <2 x i64>* %9746, align 16
  %9917 = add nsw <2 x i64> %9916, <i64 1, i64 1>
  %9918 = lshr <2 x i64> %9916, <i64 63, i64 63>
  %9919 = add nsw <2 x i64> %9917, %9918
  %9920 = lshr <2 x i64> %9919, <i64 2, i64 2>
  %9921 = trunc <2 x i64> %9920 to <2 x i32>
  %9922 = or i64 %9781, 30
  %9923 = getelementptr inbounds i32, i32* %1, i64 %9922
  %9924 = bitcast i32* %9923 to <2 x i32>*
  store <2 x i32> %9921, <2 x i32>* %9924, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %9713) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %9712) #6
  %9925 = add nuw nsw i64 %9748, 1
  %9926 = icmp eq i64 %9925, 32
  br i1 %9926, label %11292, label %9747

9927:                                             ; preds = %9444
  %9928 = shufflevector <4 x i32> %8749, <4 x i32> %8763, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %9929 = bitcast <4 x i32> %9928 to <2 x i64>
  %9930 = shufflevector <4 x i32> %8749, <4 x i32> %8763, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %9931 = bitcast <4 x i32> %9930 to <2 x i64>
  %9932 = shufflevector <4 x i32> %8750, <4 x i32> %8764, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %9933 = bitcast <4 x i32> %9932 to <2 x i64>
  %9934 = shufflevector <4 x i32> %8750, <4 x i32> %8764, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %9935 = bitcast <4 x i32> %9934 to <2 x i64>
  %9936 = shufflevector <4 x i32> %8751, <4 x i32> %8761, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %9937 = bitcast <4 x i32> %9936 to <2 x i64>
  %9938 = shufflevector <4 x i32> %8751, <4 x i32> %8761, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %9939 = bitcast <4 x i32> %9938 to <2 x i64>
  %9940 = shufflevector <4 x i32> %8752, <4 x i32> %8762, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %9941 = bitcast <4 x i32> %9940 to <2 x i64>
  %9942 = shufflevector <4 x i32> %8752, <4 x i32> %8762, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %9943 = bitcast <4 x i32> %9942 to <2 x i64>
  %9944 = shufflevector <4 x i32> %8753, <4 x i32> %8759, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %9945 = bitcast <4 x i32> %9944 to <2 x i64>
  %9946 = shufflevector <4 x i32> %8753, <4 x i32> %8759, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %9947 = bitcast <4 x i32> %9946 to <2 x i64>
  %9948 = shufflevector <4 x i32> %8754, <4 x i32> %8760, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %9949 = bitcast <4 x i32> %9948 to <2 x i64>
  %9950 = shufflevector <4 x i32> %8754, <4 x i32> %8760, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %9951 = bitcast <4 x i32> %9950 to <2 x i64>
  %9952 = shufflevector <4 x i32> %8755, <4 x i32> %8757, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %9953 = bitcast <4 x i32> %9952 to <2 x i64>
  %9954 = shufflevector <4 x i32> %8755, <4 x i32> %8757, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %9955 = bitcast <4 x i32> %9954 to <2 x i64>
  %9956 = shufflevector <4 x i32> %8756, <4 x i32> %8758, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %9957 = bitcast <4 x i32> %9956 to <2 x i64>
  %9958 = shufflevector <4 x i32> %8756, <4 x i32> %8758, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %9959 = bitcast <4 x i32> %9958 to <2 x i64>
  %9960 = and <2 x i64> %9929, <i64 4294967295, i64 4294967295>
  %9961 = mul nuw nsw <2 x i64> %9960, <i64 3981, i64 3981>
  %9962 = lshr <2 x i64> %9929, <i64 32, i64 32>
  %9963 = mul nuw nsw <2 x i64> %9962, <i64 15893, i64 15893>
  %9964 = add nuw nsw <2 x i64> %9963, %9961
  %9965 = and <2 x i64> %9931, <i64 4294967295, i64 4294967295>
  %9966 = mul nuw nsw <2 x i64> %9965, <i64 3981, i64 3981>
  %9967 = lshr <2 x i64> %9931, <i64 32, i64 32>
  %9968 = mul nuw nsw <2 x i64> %9967, <i64 15893, i64 15893>
  %9969 = add nuw nsw <2 x i64> %9968, %9966
  %9970 = and <2 x i64> %9933, <i64 4294967295, i64 4294967295>
  %9971 = mul nuw nsw <2 x i64> %9970, <i64 3981, i64 3981>
  %9972 = lshr <2 x i64> %9933, <i64 32, i64 32>
  %9973 = mul nuw nsw <2 x i64> %9972, <i64 15893, i64 15893>
  %9974 = add nuw nsw <2 x i64> %9973, %9971
  %9975 = and <2 x i64> %9935, <i64 4294967295, i64 4294967295>
  %9976 = mul nuw nsw <2 x i64> %9975, <i64 3981, i64 3981>
  %9977 = lshr <2 x i64> %9935, <i64 32, i64 32>
  %9978 = mul nuw nsw <2 x i64> %9977, <i64 15893, i64 15893>
  %9979 = add nuw nsw <2 x i64> %9978, %9976
  %9980 = and <2 x i64> %9937, <i64 4294967295, i64 4294967295>
  %9981 = mul nuw nsw <2 x i64> %9980, <i64 14053, i64 14053>
  %9982 = lshr <2 x i64> %9937, <i64 32, i64 32>
  %9983 = mul nuw nsw <2 x i64> %9982, <i64 8423, i64 8423>
  %9984 = add nuw nsw <2 x i64> %9983, %9981
  %9985 = and <2 x i64> %9939, <i64 4294967295, i64 4294967295>
  %9986 = mul nuw nsw <2 x i64> %9985, <i64 14053, i64 14053>
  %9987 = lshr <2 x i64> %9939, <i64 32, i64 32>
  %9988 = mul nuw nsw <2 x i64> %9987, <i64 8423, i64 8423>
  %9989 = add nuw nsw <2 x i64> %9988, %9986
  %9990 = and <2 x i64> %9941, <i64 4294967295, i64 4294967295>
  %9991 = mul nuw nsw <2 x i64> %9990, <i64 14053, i64 14053>
  %9992 = lshr <2 x i64> %9941, <i64 32, i64 32>
  %9993 = mul nuw nsw <2 x i64> %9992, <i64 8423, i64 8423>
  %9994 = add nuw nsw <2 x i64> %9993, %9991
  %9995 = and <2 x i64> %9943, <i64 4294967295, i64 4294967295>
  %9996 = mul nuw nsw <2 x i64> %9995, <i64 14053, i64 14053>
  %9997 = lshr <2 x i64> %9943, <i64 32, i64 32>
  %9998 = mul nuw nsw <2 x i64> %9997, <i64 8423, i64 8423>
  %9999 = add nuw nsw <2 x i64> %9998, %9996
  %10000 = and <2 x i64> %9945, <i64 4294967295, i64 4294967295>
  %10001 = mul nuw nsw <2 x i64> %10000, <i64 9760, i64 9760>
  %10002 = lshr <2 x i64> %9945, <i64 32, i64 32>
  %10003 = mul nuw nsw <2 x i64> %10002, <i64 13160, i64 13160>
  %10004 = add nuw nsw <2 x i64> %10003, %10001
  %10005 = and <2 x i64> %9947, <i64 4294967295, i64 4294967295>
  %10006 = mul nuw nsw <2 x i64> %10005, <i64 9760, i64 9760>
  %10007 = lshr <2 x i64> %9947, <i64 32, i64 32>
  %10008 = mul nuw nsw <2 x i64> %10007, <i64 13160, i64 13160>
  %10009 = add nuw nsw <2 x i64> %10008, %10006
  %10010 = and <2 x i64> %9949, <i64 4294967295, i64 4294967295>
  %10011 = mul nuw nsw <2 x i64> %10010, <i64 9760, i64 9760>
  %10012 = lshr <2 x i64> %9949, <i64 32, i64 32>
  %10013 = mul nuw nsw <2 x i64> %10012, <i64 13160, i64 13160>
  %10014 = add nuw nsw <2 x i64> %10013, %10011
  %10015 = and <2 x i64> %9951, <i64 4294967295, i64 4294967295>
  %10016 = mul nuw nsw <2 x i64> %10015, <i64 9760, i64 9760>
  %10017 = lshr <2 x i64> %9951, <i64 32, i64 32>
  %10018 = mul nuw nsw <2 x i64> %10017, <i64 13160, i64 13160>
  %10019 = add nuw nsw <2 x i64> %10018, %10016
  %10020 = and <2 x i64> %9953, <i64 4294967295, i64 4294967295>
  %10021 = mul nuw nsw <2 x i64> %10020, <i64 16207, i64 16207>
  %10022 = lshr <2 x i64> %9953, <i64 32, i64 32>
  %10023 = mul nuw nsw <2 x i64> %10022, <i64 2404, i64 2404>
  %10024 = add nuw nsw <2 x i64> %10023, %10021
  %10025 = and <2 x i64> %9955, <i64 4294967295, i64 4294967295>
  %10026 = mul nuw nsw <2 x i64> %10025, <i64 16207, i64 16207>
  %10027 = lshr <2 x i64> %9955, <i64 32, i64 32>
  %10028 = mul nuw nsw <2 x i64> %10027, <i64 2404, i64 2404>
  %10029 = add nuw nsw <2 x i64> %10028, %10026
  %10030 = and <2 x i64> %9957, <i64 4294967295, i64 4294967295>
  %10031 = mul nuw nsw <2 x i64> %10030, <i64 16207, i64 16207>
  %10032 = lshr <2 x i64> %9957, <i64 32, i64 32>
  %10033 = mul nuw nsw <2 x i64> %10032, <i64 2404, i64 2404>
  %10034 = add nuw nsw <2 x i64> %10033, %10031
  %10035 = and <2 x i64> %9959, <i64 4294967295, i64 4294967295>
  %10036 = mul nuw nsw <2 x i64> %10035, <i64 16207, i64 16207>
  %10037 = lshr <2 x i64> %9959, <i64 32, i64 32>
  %10038 = mul nuw nsw <2 x i64> %10037, <i64 2404, i64 2404>
  %10039 = add nuw nsw <2 x i64> %10038, %10036
  %10040 = mul nuw <2 x i64> %10020, <i64 4294964892, i64 4294964892>
  %10041 = mul nuw nsw <2 x i64> %10022, <i64 16207, i64 16207>
  %10042 = add <2 x i64> %10041, %10040
  %10043 = mul nuw <2 x i64> %10025, <i64 4294964892, i64 4294964892>
  %10044 = mul nuw nsw <2 x i64> %10027, <i64 16207, i64 16207>
  %10045 = add <2 x i64> %10044, %10043
  %10046 = mul nuw <2 x i64> %10030, <i64 4294964892, i64 4294964892>
  %10047 = mul nuw nsw <2 x i64> %10032, <i64 16207, i64 16207>
  %10048 = add <2 x i64> %10047, %10046
  %10049 = mul nuw <2 x i64> %10035, <i64 4294964892, i64 4294964892>
  %10050 = mul nuw nsw <2 x i64> %10037, <i64 16207, i64 16207>
  %10051 = add <2 x i64> %10050, %10049
  %10052 = mul nuw <2 x i64> %10000, <i64 4294954136, i64 4294954136>
  %10053 = mul nuw nsw <2 x i64> %10002, <i64 9760, i64 9760>
  %10054 = add <2 x i64> %10053, %10052
  %10055 = mul nuw <2 x i64> %10005, <i64 4294954136, i64 4294954136>
  %10056 = mul nuw nsw <2 x i64> %10007, <i64 9760, i64 9760>
  %10057 = add <2 x i64> %10056, %10055
  %10058 = mul nuw <2 x i64> %10010, <i64 4294954136, i64 4294954136>
  %10059 = mul nuw nsw <2 x i64> %10012, <i64 9760, i64 9760>
  %10060 = add <2 x i64> %10059, %10058
  %10061 = mul nuw <2 x i64> %10015, <i64 4294954136, i64 4294954136>
  %10062 = mul nuw nsw <2 x i64> %10017, <i64 9760, i64 9760>
  %10063 = add <2 x i64> %10062, %10061
  %10064 = mul nuw <2 x i64> %9980, <i64 4294958873, i64 4294958873>
  %10065 = mul nuw nsw <2 x i64> %9982, <i64 14053, i64 14053>
  %10066 = add <2 x i64> %10065, %10064
  %10067 = mul nuw <2 x i64> %9985, <i64 4294958873, i64 4294958873>
  %10068 = mul nuw nsw <2 x i64> %9987, <i64 14053, i64 14053>
  %10069 = add <2 x i64> %10068, %10067
  %10070 = mul nuw <2 x i64> %9990, <i64 4294958873, i64 4294958873>
  %10071 = mul nuw nsw <2 x i64> %9992, <i64 14053, i64 14053>
  %10072 = add <2 x i64> %10071, %10070
  %10073 = mul nuw <2 x i64> %9995, <i64 4294958873, i64 4294958873>
  %10074 = mul nuw nsw <2 x i64> %9997, <i64 14053, i64 14053>
  %10075 = add <2 x i64> %10074, %10073
  %10076 = mul nuw <2 x i64> %9960, <i64 4294951403, i64 4294951403>
  %10077 = mul nuw nsw <2 x i64> %9962, <i64 3981, i64 3981>
  %10078 = add <2 x i64> %10077, %10076
  %10079 = mul nuw <2 x i64> %9965, <i64 4294951403, i64 4294951403>
  %10080 = mul nuw nsw <2 x i64> %9967, <i64 3981, i64 3981>
  %10081 = add <2 x i64> %10080, %10079
  %10082 = mul nuw <2 x i64> %9970, <i64 4294951403, i64 4294951403>
  %10083 = mul nuw nsw <2 x i64> %9972, <i64 3981, i64 3981>
  %10084 = add <2 x i64> %10083, %10082
  %10085 = mul nuw <2 x i64> %9975, <i64 4294951403, i64 4294951403>
  %10086 = mul nuw nsw <2 x i64> %9977, <i64 3981, i64 3981>
  %10087 = add <2 x i64> %10086, %10085
  %10088 = shl nuw nsw <2 x i64> %9964, <i64 1, i64 1>
  %10089 = shl nuw nsw <2 x i64> %9969, <i64 1, i64 1>
  %10090 = shl nuw nsw <2 x i64> %9974, <i64 1, i64 1>
  %10091 = shl nuw nsw <2 x i64> %9979, <i64 1, i64 1>
  %10092 = bitcast <2 x i64> %10088 to <4 x i32>
  %10093 = shufflevector <4 x i32> %10092, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %10094 = bitcast <4 x i32> %10093 to <2 x i64>
  %10095 = bitcast <2 x i64> %10089 to <4 x i32>
  %10096 = shufflevector <4 x i32> %10095, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %10097 = bitcast <4 x i32> %10096 to <2 x i64>
  %10098 = bitcast <2 x i64> %10090 to <4 x i32>
  %10099 = shufflevector <4 x i32> %10098, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %10100 = bitcast <4 x i32> %10099 to <2 x i64>
  %10101 = bitcast <2 x i64> %10091 to <4 x i32>
  %10102 = shufflevector <4 x i32> %10101, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %10103 = bitcast <4 x i32> %10102 to <2 x i64>
  %10104 = shufflevector <2 x i64> %10094, <2 x i64> %10097, <2 x i32> <i32 0, i32 2>
  %10105 = shufflevector <2 x i64> %10100, <2 x i64> %10103, <2 x i32> <i32 0, i32 2>
  %10106 = bitcast <2 x i64> %10104 to <4 x i32>
  %10107 = bitcast <2 x i64> %10105 to <4 x i32>
  %10108 = add <4 x i32> %10106, <i32 1, i32 1, i32 1, i32 1>
  %10109 = icmp ugt <4 x i32> %10108, <i32 1, i32 1, i32 1, i32 1>
  %10110 = sext <4 x i1> %10109 to <4 x i32>
  %10111 = bitcast <4 x i32> %10110 to <16 x i8>
  %10112 = icmp slt <16 x i8> %10111, zeroinitializer
  %10113 = bitcast <16 x i1> %10112 to i16
  %10114 = zext i16 %10113 to i32
  %10115 = add <4 x i32> %10107, <i32 1, i32 1, i32 1, i32 1>
  %10116 = icmp ugt <4 x i32> %10115, <i32 1, i32 1, i32 1, i32 1>
  %10117 = sext <4 x i1> %10116 to <4 x i32>
  %10118 = bitcast <4 x i32> %10117 to <16 x i8>
  %10119 = icmp slt <16 x i8> %10118, zeroinitializer
  %10120 = bitcast <16 x i1> %10119 to i16
  %10121 = zext i16 %10120 to i32
  %10122 = sub nsw i32 0, %10114
  %10123 = icmp eq i32 %10121, %10122
  br i1 %10123, label %10124, label %10383

10124:                                            ; preds = %9927
  %10125 = shl nuw nsw <2 x i64> %9984, <i64 1, i64 1>
  %10126 = shl nuw nsw <2 x i64> %9989, <i64 1, i64 1>
  %10127 = shl nuw nsw <2 x i64> %9994, <i64 1, i64 1>
  %10128 = shl nuw nsw <2 x i64> %9999, <i64 1, i64 1>
  %10129 = bitcast <2 x i64> %10125 to <4 x i32>
  %10130 = shufflevector <4 x i32> %10129, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %10131 = bitcast <4 x i32> %10130 to <2 x i64>
  %10132 = bitcast <2 x i64> %10126 to <4 x i32>
  %10133 = shufflevector <4 x i32> %10132, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %10134 = bitcast <4 x i32> %10133 to <2 x i64>
  %10135 = bitcast <2 x i64> %10127 to <4 x i32>
  %10136 = shufflevector <4 x i32> %10135, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %10137 = bitcast <4 x i32> %10136 to <2 x i64>
  %10138 = bitcast <2 x i64> %10128 to <4 x i32>
  %10139 = shufflevector <4 x i32> %10138, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %10140 = bitcast <4 x i32> %10139 to <2 x i64>
  %10141 = shufflevector <2 x i64> %10131, <2 x i64> %10134, <2 x i32> <i32 0, i32 2>
  %10142 = shufflevector <2 x i64> %10137, <2 x i64> %10140, <2 x i32> <i32 0, i32 2>
  %10143 = bitcast <2 x i64> %10141 to <4 x i32>
  %10144 = bitcast <2 x i64> %10142 to <4 x i32>
  %10145 = add <4 x i32> %10143, <i32 1, i32 1, i32 1, i32 1>
  %10146 = icmp ugt <4 x i32> %10145, <i32 1, i32 1, i32 1, i32 1>
  %10147 = sext <4 x i1> %10146 to <4 x i32>
  %10148 = bitcast <4 x i32> %10147 to <16 x i8>
  %10149 = icmp slt <16 x i8> %10148, zeroinitializer
  %10150 = bitcast <16 x i1> %10149 to i16
  %10151 = zext i16 %10150 to i32
  %10152 = add <4 x i32> %10144, <i32 1, i32 1, i32 1, i32 1>
  %10153 = icmp ugt <4 x i32> %10152, <i32 1, i32 1, i32 1, i32 1>
  %10154 = sext <4 x i1> %10153 to <4 x i32>
  %10155 = bitcast <4 x i32> %10154 to <16 x i8>
  %10156 = icmp slt <16 x i8> %10155, zeroinitializer
  %10157 = bitcast <16 x i1> %10156 to i16
  %10158 = zext i16 %10157 to i32
  %10159 = sub nsw i32 0, %10151
  %10160 = icmp eq i32 %10158, %10159
  br i1 %10160, label %10161, label %10383

10161:                                            ; preds = %10124
  %10162 = shl nuw nsw <2 x i64> %10004, <i64 1, i64 1>
  %10163 = shl nuw nsw <2 x i64> %10009, <i64 1, i64 1>
  %10164 = shl nuw nsw <2 x i64> %10014, <i64 1, i64 1>
  %10165 = shl nuw nsw <2 x i64> %10019, <i64 1, i64 1>
  %10166 = bitcast <2 x i64> %10162 to <4 x i32>
  %10167 = shufflevector <4 x i32> %10166, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %10168 = bitcast <4 x i32> %10167 to <2 x i64>
  %10169 = bitcast <2 x i64> %10163 to <4 x i32>
  %10170 = shufflevector <4 x i32> %10169, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %10171 = bitcast <4 x i32> %10170 to <2 x i64>
  %10172 = bitcast <2 x i64> %10164 to <4 x i32>
  %10173 = shufflevector <4 x i32> %10172, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %10174 = bitcast <4 x i32> %10173 to <2 x i64>
  %10175 = bitcast <2 x i64> %10165 to <4 x i32>
  %10176 = shufflevector <4 x i32> %10175, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %10177 = bitcast <4 x i32> %10176 to <2 x i64>
  %10178 = shufflevector <2 x i64> %10168, <2 x i64> %10171, <2 x i32> <i32 0, i32 2>
  %10179 = shufflevector <2 x i64> %10174, <2 x i64> %10177, <2 x i32> <i32 0, i32 2>
  %10180 = bitcast <2 x i64> %10178 to <4 x i32>
  %10181 = bitcast <2 x i64> %10179 to <4 x i32>
  %10182 = add <4 x i32> %10180, <i32 1, i32 1, i32 1, i32 1>
  %10183 = icmp ugt <4 x i32> %10182, <i32 1, i32 1, i32 1, i32 1>
  %10184 = sext <4 x i1> %10183 to <4 x i32>
  %10185 = bitcast <4 x i32> %10184 to <16 x i8>
  %10186 = icmp slt <16 x i8> %10185, zeroinitializer
  %10187 = bitcast <16 x i1> %10186 to i16
  %10188 = zext i16 %10187 to i32
  %10189 = add <4 x i32> %10181, <i32 1, i32 1, i32 1, i32 1>
  %10190 = icmp ugt <4 x i32> %10189, <i32 1, i32 1, i32 1, i32 1>
  %10191 = sext <4 x i1> %10190 to <4 x i32>
  %10192 = bitcast <4 x i32> %10191 to <16 x i8>
  %10193 = icmp slt <16 x i8> %10192, zeroinitializer
  %10194 = bitcast <16 x i1> %10193 to i16
  %10195 = zext i16 %10194 to i32
  %10196 = sub nsw i32 0, %10188
  %10197 = icmp eq i32 %10195, %10196
  br i1 %10197, label %10198, label %10383

10198:                                            ; preds = %10161
  %10199 = shl nuw nsw <2 x i64> %10024, <i64 1, i64 1>
  %10200 = shl nuw nsw <2 x i64> %10029, <i64 1, i64 1>
  %10201 = shl nuw nsw <2 x i64> %10034, <i64 1, i64 1>
  %10202 = shl nuw nsw <2 x i64> %10039, <i64 1, i64 1>
  %10203 = bitcast <2 x i64> %10199 to <4 x i32>
  %10204 = shufflevector <4 x i32> %10203, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %10205 = bitcast <4 x i32> %10204 to <2 x i64>
  %10206 = bitcast <2 x i64> %10200 to <4 x i32>
  %10207 = shufflevector <4 x i32> %10206, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %10208 = bitcast <4 x i32> %10207 to <2 x i64>
  %10209 = bitcast <2 x i64> %10201 to <4 x i32>
  %10210 = shufflevector <4 x i32> %10209, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %10211 = bitcast <4 x i32> %10210 to <2 x i64>
  %10212 = bitcast <2 x i64> %10202 to <4 x i32>
  %10213 = shufflevector <4 x i32> %10212, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %10214 = bitcast <4 x i32> %10213 to <2 x i64>
  %10215 = shufflevector <2 x i64> %10205, <2 x i64> %10208, <2 x i32> <i32 0, i32 2>
  %10216 = shufflevector <2 x i64> %10211, <2 x i64> %10214, <2 x i32> <i32 0, i32 2>
  %10217 = bitcast <2 x i64> %10215 to <4 x i32>
  %10218 = bitcast <2 x i64> %10216 to <4 x i32>
  %10219 = add <4 x i32> %10217, <i32 1, i32 1, i32 1, i32 1>
  %10220 = icmp ugt <4 x i32> %10219, <i32 1, i32 1, i32 1, i32 1>
  %10221 = sext <4 x i1> %10220 to <4 x i32>
  %10222 = bitcast <4 x i32> %10221 to <16 x i8>
  %10223 = icmp slt <16 x i8> %10222, zeroinitializer
  %10224 = bitcast <16 x i1> %10223 to i16
  %10225 = zext i16 %10224 to i32
  %10226 = add <4 x i32> %10218, <i32 1, i32 1, i32 1, i32 1>
  %10227 = icmp ugt <4 x i32> %10226, <i32 1, i32 1, i32 1, i32 1>
  %10228 = sext <4 x i1> %10227 to <4 x i32>
  %10229 = bitcast <4 x i32> %10228 to <16 x i8>
  %10230 = icmp slt <16 x i8> %10229, zeroinitializer
  %10231 = bitcast <16 x i1> %10230 to i16
  %10232 = zext i16 %10231 to i32
  %10233 = sub nsw i32 0, %10225
  %10234 = icmp eq i32 %10232, %10233
  br i1 %10234, label %10235, label %10383

10235:                                            ; preds = %10198
  %10236 = shl <2 x i64> %10042, <i64 1, i64 1>
  %10237 = shl <2 x i64> %10045, <i64 1, i64 1>
  %10238 = shl <2 x i64> %10048, <i64 1, i64 1>
  %10239 = shl <2 x i64> %10051, <i64 1, i64 1>
  %10240 = bitcast <2 x i64> %10236 to <4 x i32>
  %10241 = shufflevector <4 x i32> %10240, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %10242 = bitcast <4 x i32> %10241 to <2 x i64>
  %10243 = bitcast <2 x i64> %10237 to <4 x i32>
  %10244 = shufflevector <4 x i32> %10243, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %10245 = bitcast <4 x i32> %10244 to <2 x i64>
  %10246 = bitcast <2 x i64> %10238 to <4 x i32>
  %10247 = shufflevector <4 x i32> %10246, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %10248 = bitcast <4 x i32> %10247 to <2 x i64>
  %10249 = bitcast <2 x i64> %10239 to <4 x i32>
  %10250 = shufflevector <4 x i32> %10249, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %10251 = bitcast <4 x i32> %10250 to <2 x i64>
  %10252 = shufflevector <2 x i64> %10242, <2 x i64> %10245, <2 x i32> <i32 0, i32 2>
  %10253 = shufflevector <2 x i64> %10248, <2 x i64> %10251, <2 x i32> <i32 0, i32 2>
  %10254 = bitcast <2 x i64> %10252 to <4 x i32>
  %10255 = bitcast <2 x i64> %10253 to <4 x i32>
  %10256 = add <4 x i32> %10254, <i32 1, i32 1, i32 1, i32 1>
  %10257 = icmp ugt <4 x i32> %10256, <i32 1, i32 1, i32 1, i32 1>
  %10258 = sext <4 x i1> %10257 to <4 x i32>
  %10259 = bitcast <4 x i32> %10258 to <16 x i8>
  %10260 = icmp slt <16 x i8> %10259, zeroinitializer
  %10261 = bitcast <16 x i1> %10260 to i16
  %10262 = zext i16 %10261 to i32
  %10263 = add <4 x i32> %10255, <i32 1, i32 1, i32 1, i32 1>
  %10264 = icmp ugt <4 x i32> %10263, <i32 1, i32 1, i32 1, i32 1>
  %10265 = sext <4 x i1> %10264 to <4 x i32>
  %10266 = bitcast <4 x i32> %10265 to <16 x i8>
  %10267 = icmp slt <16 x i8> %10266, zeroinitializer
  %10268 = bitcast <16 x i1> %10267 to i16
  %10269 = zext i16 %10268 to i32
  %10270 = sub nsw i32 0, %10262
  %10271 = icmp eq i32 %10269, %10270
  br i1 %10271, label %10272, label %10383

10272:                                            ; preds = %10235
  %10273 = shl <2 x i64> %10054, <i64 1, i64 1>
  %10274 = shl <2 x i64> %10057, <i64 1, i64 1>
  %10275 = shl <2 x i64> %10060, <i64 1, i64 1>
  %10276 = shl <2 x i64> %10063, <i64 1, i64 1>
  %10277 = bitcast <2 x i64> %10273 to <4 x i32>
  %10278 = shufflevector <4 x i32> %10277, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %10279 = bitcast <4 x i32> %10278 to <2 x i64>
  %10280 = bitcast <2 x i64> %10274 to <4 x i32>
  %10281 = shufflevector <4 x i32> %10280, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %10282 = bitcast <4 x i32> %10281 to <2 x i64>
  %10283 = bitcast <2 x i64> %10275 to <4 x i32>
  %10284 = shufflevector <4 x i32> %10283, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %10285 = bitcast <4 x i32> %10284 to <2 x i64>
  %10286 = bitcast <2 x i64> %10276 to <4 x i32>
  %10287 = shufflevector <4 x i32> %10286, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %10288 = bitcast <4 x i32> %10287 to <2 x i64>
  %10289 = shufflevector <2 x i64> %10279, <2 x i64> %10282, <2 x i32> <i32 0, i32 2>
  %10290 = shufflevector <2 x i64> %10285, <2 x i64> %10288, <2 x i32> <i32 0, i32 2>
  %10291 = bitcast <2 x i64> %10289 to <4 x i32>
  %10292 = bitcast <2 x i64> %10290 to <4 x i32>
  %10293 = add <4 x i32> %10291, <i32 1, i32 1, i32 1, i32 1>
  %10294 = icmp ugt <4 x i32> %10293, <i32 1, i32 1, i32 1, i32 1>
  %10295 = sext <4 x i1> %10294 to <4 x i32>
  %10296 = bitcast <4 x i32> %10295 to <16 x i8>
  %10297 = icmp slt <16 x i8> %10296, zeroinitializer
  %10298 = bitcast <16 x i1> %10297 to i16
  %10299 = zext i16 %10298 to i32
  %10300 = add <4 x i32> %10292, <i32 1, i32 1, i32 1, i32 1>
  %10301 = icmp ugt <4 x i32> %10300, <i32 1, i32 1, i32 1, i32 1>
  %10302 = sext <4 x i1> %10301 to <4 x i32>
  %10303 = bitcast <4 x i32> %10302 to <16 x i8>
  %10304 = icmp slt <16 x i8> %10303, zeroinitializer
  %10305 = bitcast <16 x i1> %10304 to i16
  %10306 = zext i16 %10305 to i32
  %10307 = sub nsw i32 0, %10299
  %10308 = icmp eq i32 %10306, %10307
  br i1 %10308, label %10309, label %10383

10309:                                            ; preds = %10272
  %10310 = shl <2 x i64> %10066, <i64 1, i64 1>
  %10311 = shl <2 x i64> %10069, <i64 1, i64 1>
  %10312 = shl <2 x i64> %10072, <i64 1, i64 1>
  %10313 = shl <2 x i64> %10075, <i64 1, i64 1>
  %10314 = bitcast <2 x i64> %10310 to <4 x i32>
  %10315 = shufflevector <4 x i32> %10314, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %10316 = bitcast <4 x i32> %10315 to <2 x i64>
  %10317 = bitcast <2 x i64> %10311 to <4 x i32>
  %10318 = shufflevector <4 x i32> %10317, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %10319 = bitcast <4 x i32> %10318 to <2 x i64>
  %10320 = bitcast <2 x i64> %10312 to <4 x i32>
  %10321 = shufflevector <4 x i32> %10320, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %10322 = bitcast <4 x i32> %10321 to <2 x i64>
  %10323 = bitcast <2 x i64> %10313 to <4 x i32>
  %10324 = shufflevector <4 x i32> %10323, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %10325 = bitcast <4 x i32> %10324 to <2 x i64>
  %10326 = shufflevector <2 x i64> %10316, <2 x i64> %10319, <2 x i32> <i32 0, i32 2>
  %10327 = shufflevector <2 x i64> %10322, <2 x i64> %10325, <2 x i32> <i32 0, i32 2>
  %10328 = bitcast <2 x i64> %10326 to <4 x i32>
  %10329 = bitcast <2 x i64> %10327 to <4 x i32>
  %10330 = add <4 x i32> %10328, <i32 1, i32 1, i32 1, i32 1>
  %10331 = icmp ugt <4 x i32> %10330, <i32 1, i32 1, i32 1, i32 1>
  %10332 = sext <4 x i1> %10331 to <4 x i32>
  %10333 = bitcast <4 x i32> %10332 to <16 x i8>
  %10334 = icmp slt <16 x i8> %10333, zeroinitializer
  %10335 = bitcast <16 x i1> %10334 to i16
  %10336 = zext i16 %10335 to i32
  %10337 = add <4 x i32> %10329, <i32 1, i32 1, i32 1, i32 1>
  %10338 = icmp ugt <4 x i32> %10337, <i32 1, i32 1, i32 1, i32 1>
  %10339 = sext <4 x i1> %10338 to <4 x i32>
  %10340 = bitcast <4 x i32> %10339 to <16 x i8>
  %10341 = icmp slt <16 x i8> %10340, zeroinitializer
  %10342 = bitcast <16 x i1> %10341 to i16
  %10343 = zext i16 %10342 to i32
  %10344 = sub nsw i32 0, %10336
  %10345 = icmp eq i32 %10343, %10344
  br i1 %10345, label %10346, label %10383

10346:                                            ; preds = %10309
  %10347 = shl <2 x i64> %10078, <i64 1, i64 1>
  %10348 = shl <2 x i64> %10081, <i64 1, i64 1>
  %10349 = shl <2 x i64> %10084, <i64 1, i64 1>
  %10350 = shl <2 x i64> %10087, <i64 1, i64 1>
  %10351 = bitcast <2 x i64> %10347 to <4 x i32>
  %10352 = shufflevector <4 x i32> %10351, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %10353 = bitcast <4 x i32> %10352 to <2 x i64>
  %10354 = bitcast <2 x i64> %10348 to <4 x i32>
  %10355 = shufflevector <4 x i32> %10354, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %10356 = bitcast <4 x i32> %10355 to <2 x i64>
  %10357 = bitcast <2 x i64> %10349 to <4 x i32>
  %10358 = shufflevector <4 x i32> %10357, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %10359 = bitcast <4 x i32> %10358 to <2 x i64>
  %10360 = bitcast <2 x i64> %10350 to <4 x i32>
  %10361 = shufflevector <4 x i32> %10360, <4 x i32> undef, <4 x i32> <i32 1, i32 3, i32 undef, i32 undef>
  %10362 = bitcast <4 x i32> %10361 to <2 x i64>
  %10363 = shufflevector <2 x i64> %10353, <2 x i64> %10356, <2 x i32> <i32 0, i32 2>
  %10364 = shufflevector <2 x i64> %10359, <2 x i64> %10362, <2 x i32> <i32 0, i32 2>
  %10365 = bitcast <2 x i64> %10363 to <4 x i32>
  %10366 = bitcast <2 x i64> %10364 to <4 x i32>
  %10367 = add <4 x i32> %10365, <i32 1, i32 1, i32 1, i32 1>
  %10368 = icmp ugt <4 x i32> %10367, <i32 1, i32 1, i32 1, i32 1>
  %10369 = sext <4 x i1> %10368 to <4 x i32>
  %10370 = bitcast <4 x i32> %10369 to <16 x i8>
  %10371 = icmp slt <16 x i8> %10370, zeroinitializer
  %10372 = bitcast <16 x i1> %10371 to i16
  %10373 = zext i16 %10372 to i32
  %10374 = add <4 x i32> %10366, <i32 1, i32 1, i32 1, i32 1>
  %10375 = icmp ugt <4 x i32> %10374, <i32 1, i32 1, i32 1, i32 1>
  %10376 = sext <4 x i1> %10375 to <4 x i32>
  %10377 = bitcast <4 x i32> %10376 to <16 x i8>
  %10378 = icmp slt <16 x i8> %10377, zeroinitializer
  %10379 = bitcast <16 x i1> %10378 to i16
  %10380 = zext i16 %10379 to i32
  %10381 = sub nsw i32 0, %10373
  %10382 = icmp eq i32 %10380, %10381
  br i1 %10382, label %10599, label %10383

10383:                                            ; preds = %10309, %10272, %10235, %10198, %10161, %10124, %9927, %10346
  %10384 = bitcast [32 x i64]* %4 to i8*
  %10385 = bitcast [32 x i64]* %5 to i8*
  %10386 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %10387 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %10388 = bitcast [32 x i64]* %5 to <2 x i64>*
  %10389 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %10390 = bitcast i64* %10389 to <2 x i64>*
  %10391 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %10392 = bitcast i64* %10391 to <2 x i64>*
  %10393 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %10394 = bitcast i64* %10393 to <2 x i64>*
  %10395 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %10396 = bitcast i64* %10395 to <2 x i64>*
  %10397 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %10398 = bitcast i64* %10397 to <2 x i64>*
  %10399 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %10400 = bitcast i64* %10399 to <2 x i64>*
  %10401 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %10402 = bitcast i64* %10401 to <2 x i64>*
  %10403 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %10404 = bitcast i64* %10403 to <2 x i64>*
  %10405 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %10406 = bitcast i64* %10405 to <2 x i64>*
  %10407 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %10408 = bitcast i64* %10407 to <2 x i64>*
  %10409 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %10410 = bitcast i64* %10409 to <2 x i64>*
  %10411 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %10412 = bitcast i64* %10411 to <2 x i64>*
  %10413 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %10414 = bitcast i64* %10413 to <2 x i64>*
  %10415 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %10416 = bitcast i64* %10415 to <2 x i64>*
  %10417 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %10418 = bitcast i64* %10417 to <2 x i64>*
  br label %10419

10419:                                            ; preds = %10452, %10383
  %10420 = phi i64 [ 0, %10383 ], [ %10597, %10452 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %10384) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %10384, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %10385) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %10385, i8 -86, i64 256, i1 false) #6
  br label %10421

10421:                                            ; preds = %10421, %10419
  %10422 = phi i64 [ 0, %10419 ], [ %10450, %10421 ]
  %10423 = shl i64 %10422, 5
  %10424 = add nuw nsw i64 %10423, %10420
  %10425 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %10424
  %10426 = load i16, i16* %10425, align 2
  %10427 = sext i16 %10426 to i64
  %10428 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %10422
  store i64 %10427, i64* %10428, align 16
  %10429 = or i64 %10422, 1
  %10430 = shl i64 %10429, 5
  %10431 = add nuw nsw i64 %10430, %10420
  %10432 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %10431
  %10433 = load i16, i16* %10432, align 2
  %10434 = sext i16 %10433 to i64
  %10435 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %10429
  store i64 %10434, i64* %10435, align 8
  %10436 = or i64 %10422, 2
  %10437 = shl i64 %10436, 5
  %10438 = add nuw nsw i64 %10437, %10420
  %10439 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %10438
  %10440 = load i16, i16* %10439, align 2
  %10441 = sext i16 %10440 to i64
  %10442 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %10436
  store i64 %10441, i64* %10442, align 16
  %10443 = or i64 %10422, 3
  %10444 = shl i64 %10443, 5
  %10445 = add nuw nsw i64 %10444, %10420
  %10446 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %10445
  %10447 = load i16, i16* %10446, align 2
  %10448 = sext i16 %10447 to i64
  %10449 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %10443
  store i64 %10448, i64* %10449, align 8
  %10450 = add nuw nsw i64 %10422, 4
  %10451 = icmp eq i64 %10450, 32
  br i1 %10451, label %10452, label %10421

10452:                                            ; preds = %10421
  call void @vpx_fdct32(i64* nonnull %10386, i64* nonnull %10387, i32 0) #6
  %10453 = shl i64 %10420, 5
  %10454 = load <2 x i64>, <2 x i64>* %10388, align 16
  %10455 = add nsw <2 x i64> %10454, <i64 1, i64 1>
  %10456 = lshr <2 x i64> %10454, <i64 63, i64 63>
  %10457 = add nsw <2 x i64> %10455, %10456
  %10458 = lshr <2 x i64> %10457, <i64 2, i64 2>
  %10459 = trunc <2 x i64> %10458 to <2 x i32>
  %10460 = getelementptr inbounds i32, i32* %1, i64 %10453
  %10461 = bitcast i32* %10460 to <2 x i32>*
  store <2 x i32> %10459, <2 x i32>* %10461, align 4
  %10462 = load <2 x i64>, <2 x i64>* %10390, align 16
  %10463 = add nsw <2 x i64> %10462, <i64 1, i64 1>
  %10464 = lshr <2 x i64> %10462, <i64 63, i64 63>
  %10465 = add nsw <2 x i64> %10463, %10464
  %10466 = lshr <2 x i64> %10465, <i64 2, i64 2>
  %10467 = trunc <2 x i64> %10466 to <2 x i32>
  %10468 = or i64 %10453, 2
  %10469 = getelementptr inbounds i32, i32* %1, i64 %10468
  %10470 = bitcast i32* %10469 to <2 x i32>*
  store <2 x i32> %10467, <2 x i32>* %10470, align 4
  %10471 = load <2 x i64>, <2 x i64>* %10392, align 16
  %10472 = add nsw <2 x i64> %10471, <i64 1, i64 1>
  %10473 = lshr <2 x i64> %10471, <i64 63, i64 63>
  %10474 = add nsw <2 x i64> %10472, %10473
  %10475 = lshr <2 x i64> %10474, <i64 2, i64 2>
  %10476 = trunc <2 x i64> %10475 to <2 x i32>
  %10477 = or i64 %10453, 4
  %10478 = getelementptr inbounds i32, i32* %1, i64 %10477
  %10479 = bitcast i32* %10478 to <2 x i32>*
  store <2 x i32> %10476, <2 x i32>* %10479, align 4
  %10480 = load <2 x i64>, <2 x i64>* %10394, align 16
  %10481 = add nsw <2 x i64> %10480, <i64 1, i64 1>
  %10482 = lshr <2 x i64> %10480, <i64 63, i64 63>
  %10483 = add nsw <2 x i64> %10481, %10482
  %10484 = lshr <2 x i64> %10483, <i64 2, i64 2>
  %10485 = trunc <2 x i64> %10484 to <2 x i32>
  %10486 = or i64 %10453, 6
  %10487 = getelementptr inbounds i32, i32* %1, i64 %10486
  %10488 = bitcast i32* %10487 to <2 x i32>*
  store <2 x i32> %10485, <2 x i32>* %10488, align 4
  %10489 = load <2 x i64>, <2 x i64>* %10396, align 16
  %10490 = add nsw <2 x i64> %10489, <i64 1, i64 1>
  %10491 = lshr <2 x i64> %10489, <i64 63, i64 63>
  %10492 = add nsw <2 x i64> %10490, %10491
  %10493 = lshr <2 x i64> %10492, <i64 2, i64 2>
  %10494 = trunc <2 x i64> %10493 to <2 x i32>
  %10495 = or i64 %10453, 8
  %10496 = getelementptr inbounds i32, i32* %1, i64 %10495
  %10497 = bitcast i32* %10496 to <2 x i32>*
  store <2 x i32> %10494, <2 x i32>* %10497, align 4
  %10498 = load <2 x i64>, <2 x i64>* %10398, align 16
  %10499 = add nsw <2 x i64> %10498, <i64 1, i64 1>
  %10500 = lshr <2 x i64> %10498, <i64 63, i64 63>
  %10501 = add nsw <2 x i64> %10499, %10500
  %10502 = lshr <2 x i64> %10501, <i64 2, i64 2>
  %10503 = trunc <2 x i64> %10502 to <2 x i32>
  %10504 = or i64 %10453, 10
  %10505 = getelementptr inbounds i32, i32* %1, i64 %10504
  %10506 = bitcast i32* %10505 to <2 x i32>*
  store <2 x i32> %10503, <2 x i32>* %10506, align 4
  %10507 = load <2 x i64>, <2 x i64>* %10400, align 16
  %10508 = add nsw <2 x i64> %10507, <i64 1, i64 1>
  %10509 = lshr <2 x i64> %10507, <i64 63, i64 63>
  %10510 = add nsw <2 x i64> %10508, %10509
  %10511 = lshr <2 x i64> %10510, <i64 2, i64 2>
  %10512 = trunc <2 x i64> %10511 to <2 x i32>
  %10513 = or i64 %10453, 12
  %10514 = getelementptr inbounds i32, i32* %1, i64 %10513
  %10515 = bitcast i32* %10514 to <2 x i32>*
  store <2 x i32> %10512, <2 x i32>* %10515, align 4
  %10516 = load <2 x i64>, <2 x i64>* %10402, align 16
  %10517 = add nsw <2 x i64> %10516, <i64 1, i64 1>
  %10518 = lshr <2 x i64> %10516, <i64 63, i64 63>
  %10519 = add nsw <2 x i64> %10517, %10518
  %10520 = lshr <2 x i64> %10519, <i64 2, i64 2>
  %10521 = trunc <2 x i64> %10520 to <2 x i32>
  %10522 = or i64 %10453, 14
  %10523 = getelementptr inbounds i32, i32* %1, i64 %10522
  %10524 = bitcast i32* %10523 to <2 x i32>*
  store <2 x i32> %10521, <2 x i32>* %10524, align 4
  %10525 = load <2 x i64>, <2 x i64>* %10404, align 16
  %10526 = add nsw <2 x i64> %10525, <i64 1, i64 1>
  %10527 = lshr <2 x i64> %10525, <i64 63, i64 63>
  %10528 = add nsw <2 x i64> %10526, %10527
  %10529 = lshr <2 x i64> %10528, <i64 2, i64 2>
  %10530 = trunc <2 x i64> %10529 to <2 x i32>
  %10531 = or i64 %10453, 16
  %10532 = getelementptr inbounds i32, i32* %1, i64 %10531
  %10533 = bitcast i32* %10532 to <2 x i32>*
  store <2 x i32> %10530, <2 x i32>* %10533, align 4
  %10534 = load <2 x i64>, <2 x i64>* %10406, align 16
  %10535 = add nsw <2 x i64> %10534, <i64 1, i64 1>
  %10536 = lshr <2 x i64> %10534, <i64 63, i64 63>
  %10537 = add nsw <2 x i64> %10535, %10536
  %10538 = lshr <2 x i64> %10537, <i64 2, i64 2>
  %10539 = trunc <2 x i64> %10538 to <2 x i32>
  %10540 = or i64 %10453, 18
  %10541 = getelementptr inbounds i32, i32* %1, i64 %10540
  %10542 = bitcast i32* %10541 to <2 x i32>*
  store <2 x i32> %10539, <2 x i32>* %10542, align 4
  %10543 = load <2 x i64>, <2 x i64>* %10408, align 16
  %10544 = add nsw <2 x i64> %10543, <i64 1, i64 1>
  %10545 = lshr <2 x i64> %10543, <i64 63, i64 63>
  %10546 = add nsw <2 x i64> %10544, %10545
  %10547 = lshr <2 x i64> %10546, <i64 2, i64 2>
  %10548 = trunc <2 x i64> %10547 to <2 x i32>
  %10549 = or i64 %10453, 20
  %10550 = getelementptr inbounds i32, i32* %1, i64 %10549
  %10551 = bitcast i32* %10550 to <2 x i32>*
  store <2 x i32> %10548, <2 x i32>* %10551, align 4
  %10552 = load <2 x i64>, <2 x i64>* %10410, align 16
  %10553 = add nsw <2 x i64> %10552, <i64 1, i64 1>
  %10554 = lshr <2 x i64> %10552, <i64 63, i64 63>
  %10555 = add nsw <2 x i64> %10553, %10554
  %10556 = lshr <2 x i64> %10555, <i64 2, i64 2>
  %10557 = trunc <2 x i64> %10556 to <2 x i32>
  %10558 = or i64 %10453, 22
  %10559 = getelementptr inbounds i32, i32* %1, i64 %10558
  %10560 = bitcast i32* %10559 to <2 x i32>*
  store <2 x i32> %10557, <2 x i32>* %10560, align 4
  %10561 = load <2 x i64>, <2 x i64>* %10412, align 16
  %10562 = add nsw <2 x i64> %10561, <i64 1, i64 1>
  %10563 = lshr <2 x i64> %10561, <i64 63, i64 63>
  %10564 = add nsw <2 x i64> %10562, %10563
  %10565 = lshr <2 x i64> %10564, <i64 2, i64 2>
  %10566 = trunc <2 x i64> %10565 to <2 x i32>
  %10567 = or i64 %10453, 24
  %10568 = getelementptr inbounds i32, i32* %1, i64 %10567
  %10569 = bitcast i32* %10568 to <2 x i32>*
  store <2 x i32> %10566, <2 x i32>* %10569, align 4
  %10570 = load <2 x i64>, <2 x i64>* %10414, align 16
  %10571 = add nsw <2 x i64> %10570, <i64 1, i64 1>
  %10572 = lshr <2 x i64> %10570, <i64 63, i64 63>
  %10573 = add nsw <2 x i64> %10571, %10572
  %10574 = lshr <2 x i64> %10573, <i64 2, i64 2>
  %10575 = trunc <2 x i64> %10574 to <2 x i32>
  %10576 = or i64 %10453, 26
  %10577 = getelementptr inbounds i32, i32* %1, i64 %10576
  %10578 = bitcast i32* %10577 to <2 x i32>*
  store <2 x i32> %10575, <2 x i32>* %10578, align 4
  %10579 = load <2 x i64>, <2 x i64>* %10416, align 16
  %10580 = add nsw <2 x i64> %10579, <i64 1, i64 1>
  %10581 = lshr <2 x i64> %10579, <i64 63, i64 63>
  %10582 = add nsw <2 x i64> %10580, %10581
  %10583 = lshr <2 x i64> %10582, <i64 2, i64 2>
  %10584 = trunc <2 x i64> %10583 to <2 x i32>
  %10585 = or i64 %10453, 28
  %10586 = getelementptr inbounds i32, i32* %1, i64 %10585
  %10587 = bitcast i32* %10586 to <2 x i32>*
  store <2 x i32> %10584, <2 x i32>* %10587, align 4
  %10588 = load <2 x i64>, <2 x i64>* %10418, align 16
  %10589 = add nsw <2 x i64> %10588, <i64 1, i64 1>
  %10590 = lshr <2 x i64> %10588, <i64 63, i64 63>
  %10591 = add nsw <2 x i64> %10589, %10590
  %10592 = lshr <2 x i64> %10591, <i64 2, i64 2>
  %10593 = trunc <2 x i64> %10592 to <2 x i32>
  %10594 = or i64 %10453, 30
  %10595 = getelementptr inbounds i32, i32* %1, i64 %10594
  %10596 = bitcast i32* %10595 to <2 x i32>*
  store <2 x i32> %10593, <2 x i32>* %10596, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %10385) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %10384) #6
  %10597 = add nuw nsw i64 %10420, 1
  %10598 = icmp eq i64 %10597, 32
  br i1 %10598, label %11292, label %10419

10599:                                            ; preds = %10346
  %10600 = bitcast <2 x i64> %9964 to <4 x i32>
  %10601 = shufflevector <4 x i32> %10600, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %10602 = bitcast <4 x i32> %10601 to <2 x i64>
  %10603 = bitcast <2 x i64> %9969 to <4 x i32>
  %10604 = shufflevector <4 x i32> %10603, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %10605 = bitcast <4 x i32> %10604 to <2 x i64>
  %10606 = shufflevector <2 x i64> %10602, <2 x i64> %10605, <2 x i32> <i32 0, i32 2>
  %10607 = bitcast <2 x i64> %9974 to <4 x i32>
  %10608 = shufflevector <4 x i32> %10607, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %10609 = bitcast <4 x i32> %10608 to <2 x i64>
  %10610 = bitcast <2 x i64> %9979 to <4 x i32>
  %10611 = shufflevector <4 x i32> %10610, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %10612 = bitcast <4 x i32> %10611 to <2 x i64>
  %10613 = shufflevector <2 x i64> %10609, <2 x i64> %10612, <2 x i32> <i32 0, i32 2>
  %10614 = bitcast <2 x i64> %9984 to <4 x i32>
  %10615 = shufflevector <4 x i32> %10614, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %10616 = bitcast <4 x i32> %10615 to <2 x i64>
  %10617 = bitcast <2 x i64> %9989 to <4 x i32>
  %10618 = shufflevector <4 x i32> %10617, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %10619 = bitcast <4 x i32> %10618 to <2 x i64>
  %10620 = shufflevector <2 x i64> %10616, <2 x i64> %10619, <2 x i32> <i32 0, i32 2>
  %10621 = bitcast <2 x i64> %9994 to <4 x i32>
  %10622 = shufflevector <4 x i32> %10621, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %10623 = bitcast <4 x i32> %10622 to <2 x i64>
  %10624 = bitcast <2 x i64> %9999 to <4 x i32>
  %10625 = shufflevector <4 x i32> %10624, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %10626 = bitcast <4 x i32> %10625 to <2 x i64>
  %10627 = shufflevector <2 x i64> %10623, <2 x i64> %10626, <2 x i32> <i32 0, i32 2>
  %10628 = bitcast <2 x i64> %10004 to <4 x i32>
  %10629 = shufflevector <4 x i32> %10628, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %10630 = bitcast <4 x i32> %10629 to <2 x i64>
  %10631 = bitcast <2 x i64> %10009 to <4 x i32>
  %10632 = shufflevector <4 x i32> %10631, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %10633 = bitcast <4 x i32> %10632 to <2 x i64>
  %10634 = shufflevector <2 x i64> %10630, <2 x i64> %10633, <2 x i32> <i32 0, i32 2>
  %10635 = bitcast <2 x i64> %10014 to <4 x i32>
  %10636 = shufflevector <4 x i32> %10635, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %10637 = bitcast <4 x i32> %10636 to <2 x i64>
  %10638 = bitcast <2 x i64> %10019 to <4 x i32>
  %10639 = shufflevector <4 x i32> %10638, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %10640 = bitcast <4 x i32> %10639 to <2 x i64>
  %10641 = shufflevector <2 x i64> %10637, <2 x i64> %10640, <2 x i32> <i32 0, i32 2>
  %10642 = bitcast <2 x i64> %10024 to <4 x i32>
  %10643 = shufflevector <4 x i32> %10642, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %10644 = bitcast <4 x i32> %10643 to <2 x i64>
  %10645 = bitcast <2 x i64> %10029 to <4 x i32>
  %10646 = shufflevector <4 x i32> %10645, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %10647 = bitcast <4 x i32> %10646 to <2 x i64>
  %10648 = shufflevector <2 x i64> %10644, <2 x i64> %10647, <2 x i32> <i32 0, i32 2>
  %10649 = bitcast <2 x i64> %10034 to <4 x i32>
  %10650 = shufflevector <4 x i32> %10649, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %10651 = bitcast <4 x i32> %10650 to <2 x i64>
  %10652 = bitcast <2 x i64> %10039 to <4 x i32>
  %10653 = shufflevector <4 x i32> %10652, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %10654 = bitcast <4 x i32> %10653 to <2 x i64>
  %10655 = shufflevector <2 x i64> %10651, <2 x i64> %10654, <2 x i32> <i32 0, i32 2>
  %10656 = bitcast <2 x i64> %10042 to <4 x i32>
  %10657 = shufflevector <4 x i32> %10656, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %10658 = bitcast <4 x i32> %10657 to <2 x i64>
  %10659 = bitcast <2 x i64> %10045 to <4 x i32>
  %10660 = shufflevector <4 x i32> %10659, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %10661 = bitcast <4 x i32> %10660 to <2 x i64>
  %10662 = shufflevector <2 x i64> %10658, <2 x i64> %10661, <2 x i32> <i32 0, i32 2>
  %10663 = bitcast <2 x i64> %10048 to <4 x i32>
  %10664 = shufflevector <4 x i32> %10663, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %10665 = bitcast <4 x i32> %10664 to <2 x i64>
  %10666 = bitcast <2 x i64> %10051 to <4 x i32>
  %10667 = shufflevector <4 x i32> %10666, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %10668 = bitcast <4 x i32> %10667 to <2 x i64>
  %10669 = shufflevector <2 x i64> %10665, <2 x i64> %10668, <2 x i32> <i32 0, i32 2>
  %10670 = bitcast <2 x i64> %10054 to <4 x i32>
  %10671 = shufflevector <4 x i32> %10670, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %10672 = bitcast <4 x i32> %10671 to <2 x i64>
  %10673 = bitcast <2 x i64> %10057 to <4 x i32>
  %10674 = shufflevector <4 x i32> %10673, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %10675 = bitcast <4 x i32> %10674 to <2 x i64>
  %10676 = shufflevector <2 x i64> %10672, <2 x i64> %10675, <2 x i32> <i32 0, i32 2>
  %10677 = bitcast <2 x i64> %10060 to <4 x i32>
  %10678 = shufflevector <4 x i32> %10677, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %10679 = bitcast <4 x i32> %10678 to <2 x i64>
  %10680 = bitcast <2 x i64> %10063 to <4 x i32>
  %10681 = shufflevector <4 x i32> %10680, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %10682 = bitcast <4 x i32> %10681 to <2 x i64>
  %10683 = shufflevector <2 x i64> %10679, <2 x i64> %10682, <2 x i32> <i32 0, i32 2>
  %10684 = bitcast <2 x i64> %10066 to <4 x i32>
  %10685 = shufflevector <4 x i32> %10684, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %10686 = bitcast <4 x i32> %10685 to <2 x i64>
  %10687 = bitcast <2 x i64> %10069 to <4 x i32>
  %10688 = shufflevector <4 x i32> %10687, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %10689 = bitcast <4 x i32> %10688 to <2 x i64>
  %10690 = shufflevector <2 x i64> %10686, <2 x i64> %10689, <2 x i32> <i32 0, i32 2>
  %10691 = bitcast <2 x i64> %10072 to <4 x i32>
  %10692 = shufflevector <4 x i32> %10691, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %10693 = bitcast <4 x i32> %10692 to <2 x i64>
  %10694 = bitcast <2 x i64> %10075 to <4 x i32>
  %10695 = shufflevector <4 x i32> %10694, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %10696 = bitcast <4 x i32> %10695 to <2 x i64>
  %10697 = shufflevector <2 x i64> %10693, <2 x i64> %10696, <2 x i32> <i32 0, i32 2>
  %10698 = bitcast <2 x i64> %10078 to <4 x i32>
  %10699 = shufflevector <4 x i32> %10698, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %10700 = bitcast <4 x i32> %10699 to <2 x i64>
  %10701 = bitcast <2 x i64> %10081 to <4 x i32>
  %10702 = shufflevector <4 x i32> %10701, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %10703 = bitcast <4 x i32> %10702 to <2 x i64>
  %10704 = shufflevector <2 x i64> %10700, <2 x i64> %10703, <2 x i32> <i32 0, i32 2>
  %10705 = bitcast <2 x i64> %10084 to <4 x i32>
  %10706 = shufflevector <4 x i32> %10705, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %10707 = bitcast <4 x i32> %10706 to <2 x i64>
  %10708 = bitcast <2 x i64> %10087 to <4 x i32>
  %10709 = shufflevector <4 x i32> %10708, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %10710 = bitcast <4 x i32> %10709 to <2 x i64>
  %10711 = shufflevector <2 x i64> %10707, <2 x i64> %10710, <2 x i32> <i32 0, i32 2>
  %10712 = bitcast <2 x i64> %10606 to <4 x i32>
  %10713 = add <4 x i32> %10712, <i32 8192, i32 8192, i32 8192, i32 8192>
  %10714 = bitcast <2 x i64> %10613 to <4 x i32>
  %10715 = add <4 x i32> %10714, <i32 8192, i32 8192, i32 8192, i32 8192>
  %10716 = bitcast <2 x i64> %10620 to <4 x i32>
  %10717 = add <4 x i32> %10716, <i32 8192, i32 8192, i32 8192, i32 8192>
  %10718 = bitcast <2 x i64> %10627 to <4 x i32>
  %10719 = add <4 x i32> %10718, <i32 8192, i32 8192, i32 8192, i32 8192>
  %10720 = bitcast <2 x i64> %10634 to <4 x i32>
  %10721 = add <4 x i32> %10720, <i32 8192, i32 8192, i32 8192, i32 8192>
  %10722 = bitcast <2 x i64> %10641 to <4 x i32>
  %10723 = add <4 x i32> %10722, <i32 8192, i32 8192, i32 8192, i32 8192>
  %10724 = bitcast <2 x i64> %10648 to <4 x i32>
  %10725 = add <4 x i32> %10724, <i32 8192, i32 8192, i32 8192, i32 8192>
  %10726 = bitcast <2 x i64> %10655 to <4 x i32>
  %10727 = add <4 x i32> %10726, <i32 8192, i32 8192, i32 8192, i32 8192>
  %10728 = bitcast <2 x i64> %10662 to <4 x i32>
  %10729 = add <4 x i32> %10728, <i32 8192, i32 8192, i32 8192, i32 8192>
  %10730 = bitcast <2 x i64> %10669 to <4 x i32>
  %10731 = add <4 x i32> %10730, <i32 8192, i32 8192, i32 8192, i32 8192>
  %10732 = bitcast <2 x i64> %10676 to <4 x i32>
  %10733 = add <4 x i32> %10732, <i32 8192, i32 8192, i32 8192, i32 8192>
  %10734 = bitcast <2 x i64> %10683 to <4 x i32>
  %10735 = add <4 x i32> %10734, <i32 8192, i32 8192, i32 8192, i32 8192>
  %10736 = bitcast <2 x i64> %10690 to <4 x i32>
  %10737 = add <4 x i32> %10736, <i32 8192, i32 8192, i32 8192, i32 8192>
  %10738 = bitcast <2 x i64> %10697 to <4 x i32>
  %10739 = add <4 x i32> %10738, <i32 8192, i32 8192, i32 8192, i32 8192>
  %10740 = bitcast <2 x i64> %10704 to <4 x i32>
  %10741 = add <4 x i32> %10740, <i32 8192, i32 8192, i32 8192, i32 8192>
  %10742 = bitcast <2 x i64> %10711 to <4 x i32>
  %10743 = add <4 x i32> %10742, <i32 8192, i32 8192, i32 8192, i32 8192>
  %10744 = ashr <4 x i32> %10713, <i32 14, i32 14, i32 14, i32 14>
  %10745 = ashr <4 x i32> %10715, <i32 14, i32 14, i32 14, i32 14>
  %10746 = ashr <4 x i32> %10717, <i32 14, i32 14, i32 14, i32 14>
  %10747 = ashr <4 x i32> %10719, <i32 14, i32 14, i32 14, i32 14>
  %10748 = ashr <4 x i32> %10721, <i32 14, i32 14, i32 14, i32 14>
  %10749 = ashr <4 x i32> %10723, <i32 14, i32 14, i32 14, i32 14>
  %10750 = ashr <4 x i32> %10725, <i32 14, i32 14, i32 14, i32 14>
  %10751 = ashr <4 x i32> %10727, <i32 14, i32 14, i32 14, i32 14>
  %10752 = ashr <4 x i32> %10729, <i32 14, i32 14, i32 14, i32 14>
  %10753 = ashr <4 x i32> %10731, <i32 14, i32 14, i32 14, i32 14>
  %10754 = ashr <4 x i32> %10733, <i32 14, i32 14, i32 14, i32 14>
  %10755 = ashr <4 x i32> %10735, <i32 14, i32 14, i32 14, i32 14>
  %10756 = ashr <4 x i32> %10737, <i32 14, i32 14, i32 14, i32 14>
  %10757 = ashr <4 x i32> %10739, <i32 14, i32 14, i32 14, i32 14>
  %10758 = ashr <4 x i32> %10741, <i32 14, i32 14, i32 14, i32 14>
  %10759 = ashr <4 x i32> %10743, <i32 14, i32 14, i32 14, i32 14>
  %10760 = ashr <4 x i32> %10713, <i32 31, i32 31, i32 31, i32 31>
  %10761 = ashr <4 x i32> %10715, <i32 31, i32 31, i32 31, i32 31>
  %10762 = ashr <4 x i32> %10717, <i32 31, i32 31, i32 31, i32 31>
  %10763 = ashr <4 x i32> %10719, <i32 31, i32 31, i32 31, i32 31>
  %10764 = ashr <4 x i32> %10721, <i32 31, i32 31, i32 31, i32 31>
  %10765 = ashr <4 x i32> %10723, <i32 31, i32 31, i32 31, i32 31>
  %10766 = ashr <4 x i32> %10725, <i32 31, i32 31, i32 31, i32 31>
  %10767 = ashr <4 x i32> %10727, <i32 31, i32 31, i32 31, i32 31>
  %10768 = ashr <4 x i32> %10729, <i32 31, i32 31, i32 31, i32 31>
  %10769 = ashr <4 x i32> %10731, <i32 31, i32 31, i32 31, i32 31>
  %10770 = ashr <4 x i32> %10733, <i32 31, i32 31, i32 31, i32 31>
  %10771 = ashr <4 x i32> %10735, <i32 31, i32 31, i32 31, i32 31>
  %10772 = ashr <4 x i32> %10737, <i32 31, i32 31, i32 31, i32 31>
  %10773 = ashr <4 x i32> %10739, <i32 31, i32 31, i32 31, i32 31>
  %10774 = ashr <4 x i32> %10741, <i32 31, i32 31, i32 31, i32 31>
  %10775 = ashr <4 x i32> %10743, <i32 31, i32 31, i32 31, i32 31>
  %10776 = sub nsw <4 x i32> %10744, %10760
  %10777 = sub nsw <4 x i32> %10745, %10761
  %10778 = sub nsw <4 x i32> %10746, %10762
  %10779 = sub nsw <4 x i32> %10747, %10763
  %10780 = sub nsw <4 x i32> %10748, %10764
  %10781 = sub nsw <4 x i32> %10749, %10765
  %10782 = sub nsw <4 x i32> %10750, %10766
  %10783 = sub nsw <4 x i32> %10751, %10767
  %10784 = sub nsw <4 x i32> %10752, %10768
  %10785 = sub nsw <4 x i32> %10753, %10769
  %10786 = sub nsw <4 x i32> %10754, %10770
  %10787 = sub nsw <4 x i32> %10755, %10771
  %10788 = sub nsw <4 x i32> %10756, %10772
  %10789 = sub nsw <4 x i32> %10757, %10773
  %10790 = sub nsw <4 x i32> %10758, %10774
  %10791 = sub nsw <4 x i32> %10759, %10775
  %10792 = add nsw <4 x i32> %10776, <i32 1, i32 1, i32 1, i32 1>
  %10793 = add nsw <4 x i32> %10777, <i32 1, i32 1, i32 1, i32 1>
  %10794 = add nsw <4 x i32> %10778, <i32 1, i32 1, i32 1, i32 1>
  %10795 = add nsw <4 x i32> %10779, <i32 1, i32 1, i32 1, i32 1>
  %10796 = add nsw <4 x i32> %10780, <i32 1, i32 1, i32 1, i32 1>
  %10797 = add nsw <4 x i32> %10781, <i32 1, i32 1, i32 1, i32 1>
  %10798 = add nsw <4 x i32> %10782, <i32 1, i32 1, i32 1, i32 1>
  %10799 = add nsw <4 x i32> %10783, <i32 1, i32 1, i32 1, i32 1>
  %10800 = add nsw <4 x i32> %10784, <i32 1, i32 1, i32 1, i32 1>
  %10801 = add nsw <4 x i32> %10785, <i32 1, i32 1, i32 1, i32 1>
  %10802 = add nsw <4 x i32> %10786, <i32 1, i32 1, i32 1, i32 1>
  %10803 = add nsw <4 x i32> %10787, <i32 1, i32 1, i32 1, i32 1>
  %10804 = add nsw <4 x i32> %10788, <i32 1, i32 1, i32 1, i32 1>
  %10805 = add nsw <4 x i32> %10789, <i32 1, i32 1, i32 1, i32 1>
  %10806 = add nsw <4 x i32> %10790, <i32 1, i32 1, i32 1, i32 1>
  %10807 = add nsw <4 x i32> %10791, <i32 1, i32 1, i32 1, i32 1>
  %10808 = ashr <4 x i32> %10792, <i32 2, i32 2, i32 2, i32 2>
  %10809 = ashr <4 x i32> %10793, <i32 2, i32 2, i32 2, i32 2>
  %10810 = ashr <4 x i32> %10794, <i32 2, i32 2, i32 2, i32 2>
  %10811 = ashr <4 x i32> %10795, <i32 2, i32 2, i32 2, i32 2>
  %10812 = ashr <4 x i32> %10796, <i32 2, i32 2, i32 2, i32 2>
  %10813 = ashr <4 x i32> %10797, <i32 2, i32 2, i32 2, i32 2>
  %10814 = ashr <4 x i32> %10798, <i32 2, i32 2, i32 2, i32 2>
  %10815 = ashr <4 x i32> %10799, <i32 2, i32 2, i32 2, i32 2>
  %10816 = ashr <4 x i32> %10800, <i32 2, i32 2, i32 2, i32 2>
  %10817 = ashr <4 x i32> %10801, <i32 2, i32 2, i32 2, i32 2>
  %10818 = ashr <4 x i32> %10802, <i32 2, i32 2, i32 2, i32 2>
  %10819 = ashr <4 x i32> %10803, <i32 2, i32 2, i32 2, i32 2>
  %10820 = ashr <4 x i32> %10804, <i32 2, i32 2, i32 2, i32 2>
  %10821 = ashr <4 x i32> %10805, <i32 2, i32 2, i32 2, i32 2>
  %10822 = ashr <4 x i32> %10806, <i32 2, i32 2, i32 2, i32 2>
  %10823 = ashr <4 x i32> %10807, <i32 2, i32 2, i32 2, i32 2>
  %10824 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %10808, <4 x i32> %10809) #6
  store <8 x i16> %10824, <8 x i16>* %80, align 16
  %10825 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %10810, <4 x i32> %10811) #6
  store <8 x i16> %10825, <8 x i16>* %82, align 16
  %10826 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %10812, <4 x i32> %10813) #6
  store <8 x i16> %10826, <8 x i16>* %84, align 16
  %10827 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %10814, <4 x i32> %10815) #6
  store <8 x i16> %10827, <8 x i16>* %86, align 16
  %10828 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %10816, <4 x i32> %10817) #6
  store <8 x i16> %10828, <8 x i16>* %88, align 16
  %10829 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %10818, <4 x i32> %10819) #6
  store <8 x i16> %10829, <8 x i16>* %90, align 16
  %10830 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %10820, <4 x i32> %10821) #6
  store <8 x i16> %10830, <8 x i16>* %92, align 16
  %10831 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %10822, <4 x i32> %10823) #6
  store <8 x i16> %10831, <8 x i16>* %94, align 16
  %10832 = add <8 x i16> %10824, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %10833 = icmp ult <8 x i16> %10832, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %10834 = add <8 x i16> %10825, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %10835 = icmp ult <8 x i16> %10834, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %10836 = add <8 x i16> %10826, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %10837 = icmp ult <8 x i16> %10836, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %10838 = add <8 x i16> %10827, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %10839 = icmp ult <8 x i16> %10838, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %10840 = or <8 x i1> %10835, %10833
  %10841 = or <8 x i1> %10840, %10837
  %10842 = or <8 x i1> %10841, %10839
  %10843 = sext <8 x i1> %10842 to <8 x i16>
  %10844 = bitcast <8 x i16> %10843 to <16 x i8>
  %10845 = icmp slt <16 x i8> %10844, zeroinitializer
  %10846 = bitcast <16 x i1> %10845 to i16
  %10847 = zext i16 %10846 to i32
  %10848 = add <8 x i16> %10828, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %10849 = icmp ult <8 x i16> %10848, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %10850 = add <8 x i16> %10829, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %10851 = icmp ult <8 x i16> %10850, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %10852 = add <8 x i16> %10830, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %10853 = icmp ult <8 x i16> %10852, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %10854 = add <8 x i16> %10831, <i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767, i16 -32767>
  %10855 = icmp ult <8 x i16> %10854, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %10856 = or <8 x i1> %10851, %10849
  %10857 = or <8 x i1> %10856, %10853
  %10858 = or <8 x i1> %10857, %10855
  %10859 = sext <8 x i1> %10858 to <8 x i16>
  %10860 = bitcast <8 x i16> %10859 to <16 x i8>
  %10861 = icmp slt <16 x i8> %10860, zeroinitializer
  %10862 = bitcast <16 x i1> %10861 to i16
  %10863 = zext i16 %10862 to i32
  %10864 = sub nsw i32 0, %10847
  %10865 = icmp eq i32 %10863, %10864
  br i1 %10865, label %11082, label %10866

10866:                                            ; preds = %10599
  %10867 = bitcast [32 x i64]* %4 to i8*
  %10868 = bitcast [32 x i64]* %5 to i8*
  %10869 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 0
  %10870 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %10871 = bitcast [32 x i64]* %5 to <2 x i64>*
  %10872 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %10873 = bitcast i64* %10872 to <2 x i64>*
  %10874 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %10875 = bitcast i64* %10874 to <2 x i64>*
  %10876 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %10877 = bitcast i64* %10876 to <2 x i64>*
  %10878 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %10879 = bitcast i64* %10878 to <2 x i64>*
  %10880 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %10881 = bitcast i64* %10880 to <2 x i64>*
  %10882 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %10883 = bitcast i64* %10882 to <2 x i64>*
  %10884 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %10885 = bitcast i64* %10884 to <2 x i64>*
  %10886 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %10887 = bitcast i64* %10886 to <2 x i64>*
  %10888 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %10889 = bitcast i64* %10888 to <2 x i64>*
  %10890 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %10891 = bitcast i64* %10890 to <2 x i64>*
  %10892 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %10893 = bitcast i64* %10892 to <2 x i64>*
  %10894 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %10895 = bitcast i64* %10894 to <2 x i64>*
  %10896 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %10897 = bitcast i64* %10896 to <2 x i64>*
  %10898 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %10899 = bitcast i64* %10898 to <2 x i64>*
  %10900 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %10901 = bitcast i64* %10900 to <2 x i64>*
  br label %10902

10902:                                            ; preds = %10935, %10866
  %10903 = phi i64 [ 0, %10866 ], [ %11080, %10935 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %10867) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %10867, i8 -86, i64 256, i1 false) #6
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %10868) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %10868, i8 -86, i64 256, i1 false) #6
  br label %10904

10904:                                            ; preds = %10904, %10902
  %10905 = phi i64 [ 0, %10902 ], [ %10933, %10904 ]
  %10906 = shl i64 %10905, 5
  %10907 = add nuw nsw i64 %10906, %10903
  %10908 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %10907
  %10909 = load i16, i16* %10908, align 2
  %10910 = sext i16 %10909 to i64
  %10911 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %10905
  store i64 %10910, i64* %10911, align 16
  %10912 = or i64 %10905, 1
  %10913 = shl i64 %10912, 5
  %10914 = add nuw nsw i64 %10913, %10903
  %10915 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %10914
  %10916 = load i16, i16* %10915, align 2
  %10917 = sext i16 %10916 to i64
  %10918 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %10912
  store i64 %10917, i64* %10918, align 8
  %10919 = or i64 %10905, 2
  %10920 = shl i64 %10919, 5
  %10921 = add nuw nsw i64 %10920, %10903
  %10922 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %10921
  %10923 = load i16, i16* %10922, align 2
  %10924 = sext i16 %10923 to i64
  %10925 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %10919
  store i64 %10924, i64* %10925, align 16
  %10926 = or i64 %10905, 3
  %10927 = shl i64 %10926, 5
  %10928 = add nuw nsw i64 %10927, %10903
  %10929 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %10928
  %10930 = load i16, i16* %10929, align 2
  %10931 = sext i16 %10930 to i64
  %10932 = getelementptr inbounds [32 x i64], [32 x i64]* %4, i64 0, i64 %10926
  store i64 %10931, i64* %10932, align 8
  %10933 = add nuw nsw i64 %10905, 4
  %10934 = icmp eq i64 %10933, 32
  br i1 %10934, label %10935, label %10904

10935:                                            ; preds = %10904
  call void @vpx_fdct32(i64* nonnull %10869, i64* nonnull %10870, i32 0) #6
  %10936 = shl i64 %10903, 5
  %10937 = load <2 x i64>, <2 x i64>* %10871, align 16
  %10938 = add nsw <2 x i64> %10937, <i64 1, i64 1>
  %10939 = lshr <2 x i64> %10937, <i64 63, i64 63>
  %10940 = add nsw <2 x i64> %10938, %10939
  %10941 = lshr <2 x i64> %10940, <i64 2, i64 2>
  %10942 = trunc <2 x i64> %10941 to <2 x i32>
  %10943 = getelementptr inbounds i32, i32* %1, i64 %10936
  %10944 = bitcast i32* %10943 to <2 x i32>*
  store <2 x i32> %10942, <2 x i32>* %10944, align 4
  %10945 = load <2 x i64>, <2 x i64>* %10873, align 16
  %10946 = add nsw <2 x i64> %10945, <i64 1, i64 1>
  %10947 = lshr <2 x i64> %10945, <i64 63, i64 63>
  %10948 = add nsw <2 x i64> %10946, %10947
  %10949 = lshr <2 x i64> %10948, <i64 2, i64 2>
  %10950 = trunc <2 x i64> %10949 to <2 x i32>
  %10951 = or i64 %10936, 2
  %10952 = getelementptr inbounds i32, i32* %1, i64 %10951
  %10953 = bitcast i32* %10952 to <2 x i32>*
  store <2 x i32> %10950, <2 x i32>* %10953, align 4
  %10954 = load <2 x i64>, <2 x i64>* %10875, align 16
  %10955 = add nsw <2 x i64> %10954, <i64 1, i64 1>
  %10956 = lshr <2 x i64> %10954, <i64 63, i64 63>
  %10957 = add nsw <2 x i64> %10955, %10956
  %10958 = lshr <2 x i64> %10957, <i64 2, i64 2>
  %10959 = trunc <2 x i64> %10958 to <2 x i32>
  %10960 = or i64 %10936, 4
  %10961 = getelementptr inbounds i32, i32* %1, i64 %10960
  %10962 = bitcast i32* %10961 to <2 x i32>*
  store <2 x i32> %10959, <2 x i32>* %10962, align 4
  %10963 = load <2 x i64>, <2 x i64>* %10877, align 16
  %10964 = add nsw <2 x i64> %10963, <i64 1, i64 1>
  %10965 = lshr <2 x i64> %10963, <i64 63, i64 63>
  %10966 = add nsw <2 x i64> %10964, %10965
  %10967 = lshr <2 x i64> %10966, <i64 2, i64 2>
  %10968 = trunc <2 x i64> %10967 to <2 x i32>
  %10969 = or i64 %10936, 6
  %10970 = getelementptr inbounds i32, i32* %1, i64 %10969
  %10971 = bitcast i32* %10970 to <2 x i32>*
  store <2 x i32> %10968, <2 x i32>* %10971, align 4
  %10972 = load <2 x i64>, <2 x i64>* %10879, align 16
  %10973 = add nsw <2 x i64> %10972, <i64 1, i64 1>
  %10974 = lshr <2 x i64> %10972, <i64 63, i64 63>
  %10975 = add nsw <2 x i64> %10973, %10974
  %10976 = lshr <2 x i64> %10975, <i64 2, i64 2>
  %10977 = trunc <2 x i64> %10976 to <2 x i32>
  %10978 = or i64 %10936, 8
  %10979 = getelementptr inbounds i32, i32* %1, i64 %10978
  %10980 = bitcast i32* %10979 to <2 x i32>*
  store <2 x i32> %10977, <2 x i32>* %10980, align 4
  %10981 = load <2 x i64>, <2 x i64>* %10881, align 16
  %10982 = add nsw <2 x i64> %10981, <i64 1, i64 1>
  %10983 = lshr <2 x i64> %10981, <i64 63, i64 63>
  %10984 = add nsw <2 x i64> %10982, %10983
  %10985 = lshr <2 x i64> %10984, <i64 2, i64 2>
  %10986 = trunc <2 x i64> %10985 to <2 x i32>
  %10987 = or i64 %10936, 10
  %10988 = getelementptr inbounds i32, i32* %1, i64 %10987
  %10989 = bitcast i32* %10988 to <2 x i32>*
  store <2 x i32> %10986, <2 x i32>* %10989, align 4
  %10990 = load <2 x i64>, <2 x i64>* %10883, align 16
  %10991 = add nsw <2 x i64> %10990, <i64 1, i64 1>
  %10992 = lshr <2 x i64> %10990, <i64 63, i64 63>
  %10993 = add nsw <2 x i64> %10991, %10992
  %10994 = lshr <2 x i64> %10993, <i64 2, i64 2>
  %10995 = trunc <2 x i64> %10994 to <2 x i32>
  %10996 = or i64 %10936, 12
  %10997 = getelementptr inbounds i32, i32* %1, i64 %10996
  %10998 = bitcast i32* %10997 to <2 x i32>*
  store <2 x i32> %10995, <2 x i32>* %10998, align 4
  %10999 = load <2 x i64>, <2 x i64>* %10885, align 16
  %11000 = add nsw <2 x i64> %10999, <i64 1, i64 1>
  %11001 = lshr <2 x i64> %10999, <i64 63, i64 63>
  %11002 = add nsw <2 x i64> %11000, %11001
  %11003 = lshr <2 x i64> %11002, <i64 2, i64 2>
  %11004 = trunc <2 x i64> %11003 to <2 x i32>
  %11005 = or i64 %10936, 14
  %11006 = getelementptr inbounds i32, i32* %1, i64 %11005
  %11007 = bitcast i32* %11006 to <2 x i32>*
  store <2 x i32> %11004, <2 x i32>* %11007, align 4
  %11008 = load <2 x i64>, <2 x i64>* %10887, align 16
  %11009 = add nsw <2 x i64> %11008, <i64 1, i64 1>
  %11010 = lshr <2 x i64> %11008, <i64 63, i64 63>
  %11011 = add nsw <2 x i64> %11009, %11010
  %11012 = lshr <2 x i64> %11011, <i64 2, i64 2>
  %11013 = trunc <2 x i64> %11012 to <2 x i32>
  %11014 = or i64 %10936, 16
  %11015 = getelementptr inbounds i32, i32* %1, i64 %11014
  %11016 = bitcast i32* %11015 to <2 x i32>*
  store <2 x i32> %11013, <2 x i32>* %11016, align 4
  %11017 = load <2 x i64>, <2 x i64>* %10889, align 16
  %11018 = add nsw <2 x i64> %11017, <i64 1, i64 1>
  %11019 = lshr <2 x i64> %11017, <i64 63, i64 63>
  %11020 = add nsw <2 x i64> %11018, %11019
  %11021 = lshr <2 x i64> %11020, <i64 2, i64 2>
  %11022 = trunc <2 x i64> %11021 to <2 x i32>
  %11023 = or i64 %10936, 18
  %11024 = getelementptr inbounds i32, i32* %1, i64 %11023
  %11025 = bitcast i32* %11024 to <2 x i32>*
  store <2 x i32> %11022, <2 x i32>* %11025, align 4
  %11026 = load <2 x i64>, <2 x i64>* %10891, align 16
  %11027 = add nsw <2 x i64> %11026, <i64 1, i64 1>
  %11028 = lshr <2 x i64> %11026, <i64 63, i64 63>
  %11029 = add nsw <2 x i64> %11027, %11028
  %11030 = lshr <2 x i64> %11029, <i64 2, i64 2>
  %11031 = trunc <2 x i64> %11030 to <2 x i32>
  %11032 = or i64 %10936, 20
  %11033 = getelementptr inbounds i32, i32* %1, i64 %11032
  %11034 = bitcast i32* %11033 to <2 x i32>*
  store <2 x i32> %11031, <2 x i32>* %11034, align 4
  %11035 = load <2 x i64>, <2 x i64>* %10893, align 16
  %11036 = add nsw <2 x i64> %11035, <i64 1, i64 1>
  %11037 = lshr <2 x i64> %11035, <i64 63, i64 63>
  %11038 = add nsw <2 x i64> %11036, %11037
  %11039 = lshr <2 x i64> %11038, <i64 2, i64 2>
  %11040 = trunc <2 x i64> %11039 to <2 x i32>
  %11041 = or i64 %10936, 22
  %11042 = getelementptr inbounds i32, i32* %1, i64 %11041
  %11043 = bitcast i32* %11042 to <2 x i32>*
  store <2 x i32> %11040, <2 x i32>* %11043, align 4
  %11044 = load <2 x i64>, <2 x i64>* %10895, align 16
  %11045 = add nsw <2 x i64> %11044, <i64 1, i64 1>
  %11046 = lshr <2 x i64> %11044, <i64 63, i64 63>
  %11047 = add nsw <2 x i64> %11045, %11046
  %11048 = lshr <2 x i64> %11047, <i64 2, i64 2>
  %11049 = trunc <2 x i64> %11048 to <2 x i32>
  %11050 = or i64 %10936, 24
  %11051 = getelementptr inbounds i32, i32* %1, i64 %11050
  %11052 = bitcast i32* %11051 to <2 x i32>*
  store <2 x i32> %11049, <2 x i32>* %11052, align 4
  %11053 = load <2 x i64>, <2 x i64>* %10897, align 16
  %11054 = add nsw <2 x i64> %11053, <i64 1, i64 1>
  %11055 = lshr <2 x i64> %11053, <i64 63, i64 63>
  %11056 = add nsw <2 x i64> %11054, %11055
  %11057 = lshr <2 x i64> %11056, <i64 2, i64 2>
  %11058 = trunc <2 x i64> %11057 to <2 x i32>
  %11059 = or i64 %10936, 26
  %11060 = getelementptr inbounds i32, i32* %1, i64 %11059
  %11061 = bitcast i32* %11060 to <2 x i32>*
  store <2 x i32> %11058, <2 x i32>* %11061, align 4
  %11062 = load <2 x i64>, <2 x i64>* %10899, align 16
  %11063 = add nsw <2 x i64> %11062, <i64 1, i64 1>
  %11064 = lshr <2 x i64> %11062, <i64 63, i64 63>
  %11065 = add nsw <2 x i64> %11063, %11064
  %11066 = lshr <2 x i64> %11065, <i64 2, i64 2>
  %11067 = trunc <2 x i64> %11066 to <2 x i32>
  %11068 = or i64 %10936, 28
  %11069 = getelementptr inbounds i32, i32* %1, i64 %11068
  %11070 = bitcast i32* %11069 to <2 x i32>*
  store <2 x i32> %11067, <2 x i32>* %11070, align 4
  %11071 = load <2 x i64>, <2 x i64>* %10901, align 16
  %11072 = add nsw <2 x i64> %11071, <i64 1, i64 1>
  %11073 = lshr <2 x i64> %11071, <i64 63, i64 63>
  %11074 = add nsw <2 x i64> %11072, %11073
  %11075 = lshr <2 x i64> %11074, <i64 2, i64 2>
  %11076 = trunc <2 x i64> %11075 to <2 x i32>
  %11077 = or i64 %10936, 30
  %11078 = getelementptr inbounds i32, i32* %1, i64 %11077
  %11079 = bitcast i32* %11078 to <2 x i32>*
  store <2 x i32> %11076, <2 x i32>* %11079, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %10868) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %10867) #6
  %11080 = add nuw nsw i64 %10903, 1
  %11081 = icmp eq i64 %11080, 32
  br i1 %11081, label %11292, label %10902

11082:                                            ; preds = %10599, %3149
  %11083 = shl nsw i64 %99, 5
  %11084 = getelementptr inbounds [1024 x i16], [1024 x i16]* %6, i64 0, i64 %11083
  %11085 = getelementptr inbounds i32, i32* %1, i64 %11083
  br label %11086

11086:                                            ; preds = %11284, %11082
  %11087 = phi i64 [ 0, %11082 ], [ %11287, %11284 ]
  %11088 = phi i32* [ %11085, %11082 ], [ %11286, %11284 ]
  %11089 = phi i16* [ %11084, %11082 ], [ %11285, %11284 ]
  %11090 = shl nsw i64 %11087, 3
  %11091 = getelementptr inbounds [32 x <2 x i64>], [32 x <2 x i64>]* %7, i64 0, i64 %11090
  %11092 = bitcast <2 x i64>* %11091 to <8 x i16>*
  %11093 = load <8 x i16>, <8 x i16>* %11092, align 16
  %11094 = getelementptr inbounds <2 x i64>, <2 x i64>* %11091, i64 1
  %11095 = bitcast <2 x i64>* %11094 to <8 x i16>*
  %11096 = load <8 x i16>, <8 x i16>* %11095, align 16
  %11097 = shufflevector <8 x i16> %11093, <8 x i16> %11096, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %11098 = getelementptr inbounds <2 x i64>, <2 x i64>* %11091, i64 2
  %11099 = bitcast <2 x i64>* %11098 to <8 x i16>*
  %11100 = load <8 x i16>, <8 x i16>* %11099, align 16
  %11101 = getelementptr inbounds <2 x i64>, <2 x i64>* %11091, i64 3
  %11102 = bitcast <2 x i64>* %11101 to <8 x i16>*
  %11103 = load <8 x i16>, <8 x i16>* %11102, align 16
  %11104 = shufflevector <8 x i16> %11100, <8 x i16> %11103, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %11105 = shufflevector <8 x i16> %11093, <8 x i16> %11096, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %11106 = shufflevector <8 x i16> %11100, <8 x i16> %11103, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %11107 = getelementptr inbounds <2 x i64>, <2 x i64>* %11091, i64 4
  %11108 = bitcast <2 x i64>* %11107 to <8 x i16>*
  %11109 = load <8 x i16>, <8 x i16>* %11108, align 16
  %11110 = getelementptr inbounds <2 x i64>, <2 x i64>* %11091, i64 5
  %11111 = bitcast <2 x i64>* %11110 to <8 x i16>*
  %11112 = load <8 x i16>, <8 x i16>* %11111, align 16
  %11113 = shufflevector <8 x i16> %11109, <8 x i16> %11112, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %11114 = getelementptr inbounds <2 x i64>, <2 x i64>* %11091, i64 6
  %11115 = bitcast <2 x i64>* %11114 to <8 x i16>*
  %11116 = load <8 x i16>, <8 x i16>* %11115, align 16
  %11117 = getelementptr inbounds <2 x i64>, <2 x i64>* %11091, i64 7
  %11118 = bitcast <2 x i64>* %11117 to <8 x i16>*
  %11119 = load <8 x i16>, <8 x i16>* %11118, align 16
  %11120 = shufflevector <8 x i16> %11116, <8 x i16> %11119, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %11121 = shufflevector <8 x i16> %11109, <8 x i16> %11112, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %11122 = shufflevector <8 x i16> %11116, <8 x i16> %11119, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %11123 = bitcast <8 x i16> %11097 to <4 x i32>
  %11124 = bitcast <8 x i16> %11104 to <4 x i32>
  %11125 = shufflevector <4 x i32> %11123, <4 x i32> %11124, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %11126 = bitcast <4 x i32> %11125 to <2 x i64>
  %11127 = bitcast <8 x i16> %11105 to <4 x i32>
  %11128 = bitcast <8 x i16> %11106 to <4 x i32>
  %11129 = shufflevector <4 x i32> %11127, <4 x i32> %11128, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %11130 = bitcast <4 x i32> %11129 to <2 x i64>
  %11131 = shufflevector <4 x i32> %11123, <4 x i32> %11124, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %11132 = bitcast <4 x i32> %11131 to <2 x i64>
  %11133 = shufflevector <4 x i32> %11127, <4 x i32> %11128, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %11134 = bitcast <4 x i32> %11133 to <2 x i64>
  %11135 = bitcast <8 x i16> %11113 to <4 x i32>
  %11136 = bitcast <8 x i16> %11120 to <4 x i32>
  %11137 = shufflevector <4 x i32> %11135, <4 x i32> %11136, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %11138 = bitcast <4 x i32> %11137 to <2 x i64>
  %11139 = bitcast <8 x i16> %11121 to <4 x i32>
  %11140 = bitcast <8 x i16> %11122 to <4 x i32>
  %11141 = shufflevector <4 x i32> %11139, <4 x i32> %11140, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %11142 = bitcast <4 x i32> %11141 to <2 x i64>
  %11143 = shufflevector <4 x i32> %11135, <4 x i32> %11136, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %11144 = bitcast <4 x i32> %11143 to <2 x i64>
  %11145 = shufflevector <4 x i32> %11139, <4 x i32> %11140, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %11146 = bitcast <4 x i32> %11145 to <2 x i64>
  %11147 = shufflevector <2 x i64> %11126, <2 x i64> %11138, <2 x i32> <i32 0, i32 2>
  %11148 = shufflevector <2 x i64> %11126, <2 x i64> %11138, <2 x i32> <i32 1, i32 3>
  %11149 = shufflevector <2 x i64> %11132, <2 x i64> %11144, <2 x i32> <i32 0, i32 2>
  %11150 = shufflevector <2 x i64> %11132, <2 x i64> %11144, <2 x i32> <i32 1, i32 3>
  %11151 = shufflevector <2 x i64> %11130, <2 x i64> %11142, <2 x i32> <i32 0, i32 2>
  %11152 = shufflevector <2 x i64> %11130, <2 x i64> %11142, <2 x i32> <i32 1, i32 3>
  %11153 = shufflevector <2 x i64> %11134, <2 x i64> %11146, <2 x i32> <i32 0, i32 2>
  %11154 = shufflevector <2 x i64> %11134, <2 x i64> %11146, <2 x i32> <i32 1, i32 3>
  %11155 = bitcast <2 x i64> %11147 to <8 x i16>
  br i1 %97, label %11156, label %11220

11156:                                            ; preds = %11086
  %11157 = icmp sgt <8 x i16> %11155, zeroinitializer
  %11158 = bitcast <2 x i64> %11148 to <8 x i16>
  %11159 = icmp sgt <8 x i16> %11158, zeroinitializer
  %11160 = bitcast <2 x i64> %11149 to <8 x i16>
  %11161 = icmp sgt <8 x i16> %11160, zeroinitializer
  %11162 = bitcast <2 x i64> %11150 to <8 x i16>
  %11163 = icmp sgt <8 x i16> %11162, zeroinitializer
  %11164 = bitcast <2 x i64> %11151 to <8 x i16>
  %11165 = icmp sgt <8 x i16> %11164, zeroinitializer
  %11166 = bitcast <2 x i64> %11152 to <8 x i16>
  %11167 = icmp sgt <8 x i16> %11166, zeroinitializer
  %11168 = bitcast <2 x i64> %11153 to <8 x i16>
  %11169 = icmp sgt <8 x i16> %11168, zeroinitializer
  %11170 = bitcast <2 x i64> %11154 to <8 x i16>
  %11171 = icmp sgt <8 x i16> %11170, zeroinitializer
  %11172 = zext <8 x i1> %11157 to <8 x i16>
  %11173 = zext <8 x i1> %11159 to <8 x i16>
  %11174 = zext <8 x i1> %11161 to <8 x i16>
  %11175 = zext <8 x i1> %11163 to <8 x i16>
  %11176 = zext <8 x i1> %11165 to <8 x i16>
  %11177 = zext <8 x i1> %11167 to <8 x i16>
  %11178 = zext <8 x i1> %11169 to <8 x i16>
  %11179 = zext <8 x i1> %11171 to <8 x i16>
  %11180 = add <8 x i16> %11155, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %11181 = add <8 x i16> %11180, %11172
  %11182 = add <8 x i16> %11158, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %11183 = add <8 x i16> %11182, %11173
  %11184 = add <8 x i16> %11160, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %11185 = add <8 x i16> %11184, %11174
  %11186 = add <8 x i16> %11162, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %11187 = add <8 x i16> %11186, %11175
  %11188 = add <8 x i16> %11164, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %11189 = add <8 x i16> %11188, %11176
  %11190 = add <8 x i16> %11166, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %11191 = add <8 x i16> %11190, %11177
  %11192 = add <8 x i16> %11168, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %11193 = add <8 x i16> %11192, %11178
  %11194 = add <8 x i16> %11170, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %11195 = add <8 x i16> %11194, %11179
  %11196 = ashr <8 x i16> %11181, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %11197 = ashr <8 x i16> %11183, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %11198 = ashr <8 x i16> %11185, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %11199 = ashr <8 x i16> %11187, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %11200 = ashr <8 x i16> %11189, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %11201 = ashr <8 x i16> %11191, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %11202 = ashr <8 x i16> %11193, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %11203 = ashr <8 x i16> %11195, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %11204 = bitcast i16* %11089 to <8 x i16>*
  store <8 x i16> %11196, <8 x i16>* %11204, align 1
  %11205 = getelementptr inbounds i16, i16* %11089, i64 32
  %11206 = bitcast i16* %11205 to <8 x i16>*
  store <8 x i16> %11197, <8 x i16>* %11206, align 1
  %11207 = getelementptr inbounds i16, i16* %11089, i64 64
  %11208 = bitcast i16* %11207 to <8 x i16>*
  store <8 x i16> %11198, <8 x i16>* %11208, align 1
  %11209 = getelementptr inbounds i16, i16* %11089, i64 96
  %11210 = bitcast i16* %11209 to <8 x i16>*
  store <8 x i16> %11199, <8 x i16>* %11210, align 1
  %11211 = getelementptr inbounds i16, i16* %11089, i64 128
  %11212 = bitcast i16* %11211 to <8 x i16>*
  store <8 x i16> %11200, <8 x i16>* %11212, align 1
  %11213 = getelementptr inbounds i16, i16* %11089, i64 160
  %11214 = bitcast i16* %11213 to <8 x i16>*
  store <8 x i16> %11201, <8 x i16>* %11214, align 1
  %11215 = getelementptr inbounds i16, i16* %11089, i64 192
  %11216 = bitcast i16* %11215 to <8 x i16>*
  store <8 x i16> %11202, <8 x i16>* %11216, align 1
  %11217 = getelementptr inbounds i16, i16* %11089, i64 224
  %11218 = bitcast i16* %11217 to <8 x i16>*
  store <8 x i16> %11203, <8 x i16>* %11218, align 1
  %11219 = getelementptr inbounds i16, i16* %11089, i64 8
  br label %11284

11220:                                            ; preds = %11086
  %11221 = ashr <8 x i16> %11155, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %11222 = shufflevector <8 x i16> %11155, <8 x i16> %11221, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %11223 = shufflevector <8 x i16> %11155, <8 x i16> %11221, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %11224 = bitcast i32* %11088 to <8 x i16>*
  store <8 x i16> %11222, <8 x i16>* %11224, align 1
  %11225 = getelementptr inbounds i32, i32* %11088, i64 4
  %11226 = bitcast i32* %11225 to <8 x i16>*
  store <8 x i16> %11223, <8 x i16>* %11226, align 1
  %11227 = getelementptr inbounds i32, i32* %11088, i64 32
  %11228 = bitcast <2 x i64> %11148 to <8 x i16>
  %11229 = ashr <8 x i16> %11228, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %11230 = shufflevector <8 x i16> %11228, <8 x i16> %11229, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %11231 = shufflevector <8 x i16> %11228, <8 x i16> %11229, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %11232 = bitcast i32* %11227 to <8 x i16>*
  store <8 x i16> %11230, <8 x i16>* %11232, align 1
  %11233 = getelementptr inbounds i32, i32* %11088, i64 36
  %11234 = bitcast i32* %11233 to <8 x i16>*
  store <8 x i16> %11231, <8 x i16>* %11234, align 1
  %11235 = getelementptr inbounds i32, i32* %11088, i64 64
  %11236 = bitcast <2 x i64> %11149 to <8 x i16>
  %11237 = ashr <8 x i16> %11236, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %11238 = shufflevector <8 x i16> %11236, <8 x i16> %11237, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %11239 = shufflevector <8 x i16> %11236, <8 x i16> %11237, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %11240 = bitcast i32* %11235 to <8 x i16>*
  store <8 x i16> %11238, <8 x i16>* %11240, align 1
  %11241 = getelementptr inbounds i32, i32* %11088, i64 68
  %11242 = bitcast i32* %11241 to <8 x i16>*
  store <8 x i16> %11239, <8 x i16>* %11242, align 1
  %11243 = getelementptr inbounds i32, i32* %11088, i64 96
  %11244 = bitcast <2 x i64> %11150 to <8 x i16>
  %11245 = ashr <8 x i16> %11244, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %11246 = shufflevector <8 x i16> %11244, <8 x i16> %11245, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %11247 = shufflevector <8 x i16> %11244, <8 x i16> %11245, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %11248 = bitcast i32* %11243 to <8 x i16>*
  store <8 x i16> %11246, <8 x i16>* %11248, align 1
  %11249 = getelementptr inbounds i32, i32* %11088, i64 100
  %11250 = bitcast i32* %11249 to <8 x i16>*
  store <8 x i16> %11247, <8 x i16>* %11250, align 1
  %11251 = getelementptr inbounds i32, i32* %11088, i64 128
  %11252 = bitcast <2 x i64> %11151 to <8 x i16>
  %11253 = ashr <8 x i16> %11252, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %11254 = shufflevector <8 x i16> %11252, <8 x i16> %11253, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %11255 = shufflevector <8 x i16> %11252, <8 x i16> %11253, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %11256 = bitcast i32* %11251 to <8 x i16>*
  store <8 x i16> %11254, <8 x i16>* %11256, align 1
  %11257 = getelementptr inbounds i32, i32* %11088, i64 132
  %11258 = bitcast i32* %11257 to <8 x i16>*
  store <8 x i16> %11255, <8 x i16>* %11258, align 1
  %11259 = getelementptr inbounds i32, i32* %11088, i64 160
  %11260 = bitcast <2 x i64> %11152 to <8 x i16>
  %11261 = ashr <8 x i16> %11260, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %11262 = shufflevector <8 x i16> %11260, <8 x i16> %11261, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %11263 = shufflevector <8 x i16> %11260, <8 x i16> %11261, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %11264 = bitcast i32* %11259 to <8 x i16>*
  store <8 x i16> %11262, <8 x i16>* %11264, align 1
  %11265 = getelementptr inbounds i32, i32* %11088, i64 164
  %11266 = bitcast i32* %11265 to <8 x i16>*
  store <8 x i16> %11263, <8 x i16>* %11266, align 1
  %11267 = getelementptr inbounds i32, i32* %11088, i64 192
  %11268 = bitcast <2 x i64> %11153 to <8 x i16>
  %11269 = ashr <8 x i16> %11268, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %11270 = shufflevector <8 x i16> %11268, <8 x i16> %11269, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %11271 = shufflevector <8 x i16> %11268, <8 x i16> %11269, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %11272 = bitcast i32* %11267 to <8 x i16>*
  store <8 x i16> %11270, <8 x i16>* %11272, align 1
  %11273 = getelementptr inbounds i32, i32* %11088, i64 196
  %11274 = bitcast i32* %11273 to <8 x i16>*
  store <8 x i16> %11271, <8 x i16>* %11274, align 1
  %11275 = getelementptr inbounds i32, i32* %11088, i64 224
  %11276 = bitcast <2 x i64> %11154 to <8 x i16>
  %11277 = ashr <8 x i16> %11276, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %11278 = shufflevector <8 x i16> %11276, <8 x i16> %11277, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %11279 = shufflevector <8 x i16> %11276, <8 x i16> %11277, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %11280 = bitcast i32* %11275 to <8 x i16>*
  store <8 x i16> %11278, <8 x i16>* %11280, align 1
  %11281 = getelementptr inbounds i32, i32* %11088, i64 228
  %11282 = bitcast i32* %11281 to <8 x i16>*
  store <8 x i16> %11279, <8 x i16>* %11282, align 1
  %11283 = getelementptr inbounds i32, i32* %11088, i64 8
  br label %11284

11284:                                            ; preds = %11220, %11156
  %11285 = phi i16* [ %11219, %11156 ], [ %11089, %11220 ]
  %11286 = phi i32* [ %11088, %11156 ], [ %11283, %11220 ]
  %11287 = add nuw nsw i64 %11087, 1
  %11288 = icmp eq i64 %11287, 4
  br i1 %11288, label %11289, label %11086

11289:                                            ; preds = %11284
  call void @llvm.lifetime.end.p0i8(i64 512, i8* nonnull %11) #6
  %11290 = add nuw nsw i64 %99, 8
  %11291 = icmp ult i64 %11290, 32
  br i1 %11291, label %98, label %11293

11292:                                            ; preds = %397, %680, %963, %1246, %1586, %1903, %3572, %4284, %4892, %5241, %5685, %6241, %6590, %7278, %8110, %8593, %9297, %9780, %10452, %10935, %1833, %2094, %2235, %2322, %2472, %2663, %2757, %3048, %3248, %3148, %2957, %2857, %2713, %2572, %2522, %2450, %2350, %2144, %1516
  call void @llvm.lifetime.end.p0i8(i64 512, i8* nonnull %11) #6
  br label %11296

11293:                                            ; preds = %11289
  %11294 = add nuw nsw i32 %96, 1
  %11295 = icmp eq i32 %11294, 2
  br i1 %11295, label %11296, label %95

11296:                                            ; preds = %11293, %11292
  call void @llvm.lifetime.end.p0i8(i64 2048, i8* nonnull %10) #6
  ret void
}

declare void @vpx_highbd_fdct32x32_c(i16*, i32*, i32) local_unnamed_addr #3

; Function Attrs: nounwind readnone
declare <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16>, <8 x i16>) #4

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32>, <4 x i32>) #4

; Function Attrs: nounwind readnone speculatable
declare <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16>, <8 x i16>) #5

; Function Attrs: nounwind readnone speculatable
declare <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16>, <8 x i16>) #5

declare void @vpx_fdct32(i64*, i64*, i32) local_unnamed_addr #3

attributes #0 = { nofree nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="128" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { argmemonly nounwind }
attributes #2 = { nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="128" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #3 = { "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #4 = { nounwind readnone }
attributes #5 = { nounwind readnone speculatable }
attributes #6 = { nounwind }

!llvm.module.flags = !{!0, !1}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 2}
