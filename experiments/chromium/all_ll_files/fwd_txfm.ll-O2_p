; ModuleID = '../../third_party/libvpx/source/libvpx/vpx_dsp/fwd_txfm.c'
source_filename = "../../third_party/libvpx/source/libvpx/vpx_dsp/fwd_txfm.c"
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

; Function Attrs: nounwind ssp uwtable
define hidden void @vpx_fdct4x4_c(i16* nocapture readonly, i32* nocapture, i32) local_unnamed_addr #0 {
  %4 = alloca [16 x i32], align 16
  %5 = bitcast [16 x i32]* %4 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %5) #3
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %5, i8 -86, i64 64, i1 false)
  %6 = getelementptr inbounds [16 x i32], [16 x i32]* %4, i64 0, i64 0
  %7 = sext i32 %2 to i64
  %8 = shl nsw i32 %2, 1
  %9 = sext i32 %8 to i64
  %10 = mul nsw i32 %2, 3
  %11 = sext i32 %10 to i64
  %12 = load i16, i16* %0, align 2
  %13 = sext i16 %12 to i32
  %14 = shl nsw i32 %13, 4
  %15 = sext i32 %14 to i64
  %16 = getelementptr inbounds i16, i16* %0, i64 %7
  %17 = load i16, i16* %16, align 2
  %18 = sext i16 %17 to i32
  %19 = shl nsw i32 %18, 4
  %20 = sext i32 %19 to i64
  %21 = getelementptr inbounds i16, i16* %0, i64 %9
  %22 = load i16, i16* %21, align 2
  %23 = sext i16 %22 to i32
  %24 = shl nsw i32 %23, 4
  %25 = sext i32 %24 to i64
  %26 = getelementptr inbounds i16, i16* %0, i64 %11
  %27 = load i16, i16* %26, align 2
  %28 = sext i16 %27 to i32
  %29 = shl nsw i32 %28, 4
  %30 = sext i32 %29 to i64
  %31 = icmp eq i16 %12, 0
  %32 = or i64 %15, 1
  %33 = select i1 %31, i64 0, i64 %32
  %34 = add nsw i64 %33, %30
  %35 = add nsw i64 %20, %25
  %36 = sub nsw i64 %20, %25
  %37 = sub nsw i64 %33, %30
  %38 = add nsw i64 %34, %35
  %39 = mul nsw i64 %38, 11585
  %40 = sub nsw i64 %34, %35
  %41 = mul nsw i64 %40, 11585
  %42 = add nsw i64 %39, 8192
  %43 = lshr i64 %42, 14
  %44 = trunc i64 %43 to i32
  store i32 %44, i32* %6, align 16
  %45 = add nsw i64 %41, 8192
  %46 = lshr i64 %45, 14
  %47 = trunc i64 %46 to i32
  %48 = getelementptr inbounds [16 x i32], [16 x i32]* %4, i64 0, i64 2
  store i32 %47, i32* %48, align 8
  %49 = mul nsw i64 %36, 6270
  %50 = mul nsw i64 %37, 15137
  %51 = mul nsw i64 %36, -15137
  %52 = mul nsw i64 %37, 6270
  %53 = add nsw i64 %49, 8192
  %54 = add nsw i64 %53, %50
  %55 = lshr i64 %54, 14
  %56 = trunc i64 %55 to i32
  %57 = getelementptr inbounds [16 x i32], [16 x i32]* %4, i64 0, i64 1
  store i32 %56, i32* %57, align 4
  %58 = add nsw i64 %51, 8192
  %59 = add nsw i64 %58, %52
  %60 = lshr i64 %59, 14
  %61 = trunc i64 %60 to i32
  %62 = getelementptr inbounds [16 x i32], [16 x i32]* %4, i64 0, i64 3
  store i32 %61, i32* %62, align 4
  %63 = getelementptr inbounds i16, i16* %0, i64 1
  %64 = getelementptr inbounds [16 x i32], [16 x i32]* %4, i64 0, i64 4
  %65 = load i16, i16* %63, align 2
  %66 = sext i16 %65 to i32
  %67 = shl nsw i32 %66, 4
  %68 = sext i32 %67 to i64
  %69 = getelementptr inbounds i16, i16* %63, i64 %7
  %70 = load i16, i16* %69, align 2
  %71 = sext i16 %70 to i32
  %72 = shl nsw i32 %71, 4
  %73 = sext i32 %72 to i64
  %74 = getelementptr inbounds i16, i16* %63, i64 %9
  %75 = load i16, i16* %74, align 2
  %76 = sext i16 %75 to i32
  %77 = shl nsw i32 %76, 4
  %78 = sext i32 %77 to i64
  %79 = getelementptr inbounds i16, i16* %63, i64 %11
  %80 = load i16, i16* %79, align 2
  %81 = sext i16 %80 to i32
  %82 = shl nsw i32 %81, 4
  %83 = sext i32 %82 to i64
  %84 = add nsw i64 %68, %83
  %85 = add nsw i64 %73, %78
  %86 = sub nsw i64 %73, %78
  %87 = sub nsw i64 %68, %83
  %88 = add nsw i64 %84, %85
  %89 = mul nsw i64 %88, 11585
  %90 = sub nsw i64 %84, %85
  %91 = mul nsw i64 %90, 11585
  %92 = add nsw i64 %89, 8192
  %93 = lshr i64 %92, 14
  %94 = trunc i64 %93 to i32
  store i32 %94, i32* %64, align 16
  %95 = add nsw i64 %91, 8192
  %96 = lshr i64 %95, 14
  %97 = trunc i64 %96 to i32
  %98 = getelementptr inbounds [16 x i32], [16 x i32]* %4, i64 0, i64 6
  store i32 %97, i32* %98, align 8
  %99 = mul nsw i64 %86, 6270
  %100 = mul nsw i64 %87, 15137
  %101 = mul nsw i64 %86, -15137
  %102 = mul nsw i64 %87, 6270
  %103 = add nsw i64 %99, 8192
  %104 = add nsw i64 %103, %100
  %105 = lshr i64 %104, 14
  %106 = trunc i64 %105 to i32
  %107 = getelementptr inbounds [16 x i32], [16 x i32]* %4, i64 0, i64 5
  store i32 %106, i32* %107, align 4
  %108 = add nsw i64 %101, 8192
  %109 = add nsw i64 %108, %102
  %110 = lshr i64 %109, 14
  %111 = trunc i64 %110 to i32
  %112 = getelementptr inbounds [16 x i32], [16 x i32]* %4, i64 0, i64 7
  store i32 %111, i32* %112, align 4
  %113 = getelementptr inbounds i16, i16* %0, i64 2
  %114 = getelementptr inbounds [16 x i32], [16 x i32]* %4, i64 0, i64 8
  %115 = load i16, i16* %113, align 2
  %116 = sext i16 %115 to i32
  %117 = shl nsw i32 %116, 4
  %118 = sext i32 %117 to i64
  %119 = getelementptr inbounds i16, i16* %113, i64 %7
  %120 = load i16, i16* %119, align 2
  %121 = sext i16 %120 to i32
  %122 = shl nsw i32 %121, 4
  %123 = sext i32 %122 to i64
  %124 = getelementptr inbounds i16, i16* %113, i64 %9
  %125 = load i16, i16* %124, align 2
  %126 = sext i16 %125 to i32
  %127 = shl nsw i32 %126, 4
  %128 = sext i32 %127 to i64
  %129 = getelementptr inbounds i16, i16* %113, i64 %11
  %130 = load i16, i16* %129, align 2
  %131 = sext i16 %130 to i32
  %132 = shl nsw i32 %131, 4
  %133 = sext i32 %132 to i64
  %134 = add nsw i64 %118, %133
  %135 = add nsw i64 %123, %128
  %136 = sub nsw i64 %123, %128
  %137 = sub nsw i64 %118, %133
  %138 = add nsw i64 %134, %135
  %139 = mul nsw i64 %138, 11585
  %140 = sub nsw i64 %134, %135
  %141 = mul nsw i64 %140, 11585
  %142 = add nsw i64 %139, 8192
  %143 = lshr i64 %142, 14
  %144 = trunc i64 %143 to i32
  store i32 %144, i32* %114, align 16
  %145 = add nsw i64 %141, 8192
  %146 = lshr i64 %145, 14
  %147 = trunc i64 %146 to i32
  %148 = getelementptr inbounds [16 x i32], [16 x i32]* %4, i64 0, i64 10
  store i32 %147, i32* %148, align 8
  %149 = mul nsw i64 %136, 6270
  %150 = mul nsw i64 %137, 15137
  %151 = mul nsw i64 %136, -15137
  %152 = mul nsw i64 %137, 6270
  %153 = add nsw i64 %149, 8192
  %154 = add nsw i64 %153, %150
  %155 = lshr i64 %154, 14
  %156 = trunc i64 %155 to i32
  %157 = getelementptr inbounds [16 x i32], [16 x i32]* %4, i64 0, i64 9
  store i32 %156, i32* %157, align 4
  %158 = add nsw i64 %151, 8192
  %159 = add nsw i64 %158, %152
  %160 = lshr i64 %159, 14
  %161 = trunc i64 %160 to i32
  %162 = getelementptr inbounds [16 x i32], [16 x i32]* %4, i64 0, i64 11
  store i32 %161, i32* %162, align 4
  %163 = getelementptr inbounds i16, i16* %0, i64 3
  %164 = getelementptr inbounds [16 x i32], [16 x i32]* %4, i64 0, i64 12
  %165 = load i16, i16* %163, align 2
  %166 = sext i16 %165 to i32
  %167 = shl nsw i32 %166, 4
  %168 = sext i32 %167 to i64
  %169 = getelementptr inbounds i16, i16* %163, i64 %7
  %170 = load i16, i16* %169, align 2
  %171 = sext i16 %170 to i32
  %172 = shl nsw i32 %171, 4
  %173 = sext i32 %172 to i64
  %174 = getelementptr inbounds i16, i16* %163, i64 %9
  %175 = load i16, i16* %174, align 2
  %176 = sext i16 %175 to i32
  %177 = shl nsw i32 %176, 4
  %178 = sext i32 %177 to i64
  %179 = getelementptr inbounds i16, i16* %163, i64 %11
  %180 = load i16, i16* %179, align 2
  %181 = sext i16 %180 to i32
  %182 = shl nsw i32 %181, 4
  %183 = sext i32 %182 to i64
  %184 = add nsw i64 %168, %183
  %185 = add nsw i64 %173, %178
  %186 = sub nsw i64 %173, %178
  %187 = sub nsw i64 %168, %183
  %188 = add nsw i64 %184, %185
  %189 = mul nsw i64 %188, 11585
  %190 = sub nsw i64 %184, %185
  %191 = mul nsw i64 %190, 11585
  %192 = add nsw i64 %189, 8192
  %193 = lshr i64 %192, 14
  %194 = trunc i64 %193 to i32
  store i32 %194, i32* %164, align 16
  %195 = add nsw i64 %191, 8192
  %196 = lshr i64 %195, 14
  %197 = trunc i64 %196 to i32
  %198 = getelementptr inbounds [16 x i32], [16 x i32]* %4, i64 0, i64 14
  store i32 %197, i32* %198, align 8
  %199 = mul nsw i64 %186, 6270
  %200 = mul nsw i64 %187, 15137
  %201 = mul nsw i64 %186, -15137
  %202 = mul nsw i64 %187, 6270
  %203 = add nsw i64 %199, 8192
  %204 = add nsw i64 %203, %200
  %205 = lshr i64 %204, 14
  %206 = trunc i64 %205 to i32
  %207 = getelementptr inbounds [16 x i32], [16 x i32]* %4, i64 0, i64 13
  store i32 %206, i32* %207, align 4
  %208 = add nsw i64 %201, 8192
  %209 = add nsw i64 %208, %202
  %210 = lshr i64 %209, 14
  %211 = trunc i64 %210 to i32
  %212 = getelementptr inbounds [16 x i32], [16 x i32]* %4, i64 0, i64 15
  store i32 %211, i32* %212, align 4
  %213 = load i32, i32* %6, align 16
  %214 = sext i32 %213 to i64
  %215 = getelementptr inbounds [16 x i32], [16 x i32]* %4, i64 0, i64 4
  %216 = load i32, i32* %215, align 16
  %217 = sext i32 %216 to i64
  %218 = getelementptr inbounds [16 x i32], [16 x i32]* %4, i64 0, i64 8
  %219 = load i32, i32* %218, align 16
  %220 = sext i32 %219 to i64
  %221 = getelementptr inbounds [16 x i32], [16 x i32]* %4, i64 0, i64 12
  %222 = load i32, i32* %221, align 16
  %223 = sext i32 %222 to i64
  %224 = getelementptr inbounds [16 x i32], [16 x i32]* %4, i64 0, i64 1
  %225 = add nsw i64 %214, %223
  %226 = add nsw i64 %217, %220
  %227 = sub nsw i64 %217, %220
  %228 = sub nsw i64 %214, %223
  %229 = add nsw i64 %225, %226
  %230 = mul nsw i64 %229, 11585
  %231 = sub nsw i64 %225, %226
  %232 = mul nsw i64 %231, 11585
  %233 = add nsw i64 %230, 8192
  %234 = lshr i64 %233, 14
  %235 = trunc i64 %234 to i32
  store i32 %235, i32* %1, align 4
  %236 = add nsw i64 %232, 8192
  %237 = lshr i64 %236, 14
  %238 = trunc i64 %237 to i32
  %239 = getelementptr inbounds i32, i32* %1, i64 2
  store i32 %238, i32* %239, align 4
  %240 = mul nsw i64 %227, 6270
  %241 = mul nsw i64 %228, 15137
  %242 = mul nsw i64 %227, -15137
  %243 = mul nsw i64 %228, 6270
  %244 = add nsw i64 %240, 8192
  %245 = add nsw i64 %244, %241
  %246 = lshr i64 %245, 14
  %247 = trunc i64 %246 to i32
  %248 = getelementptr inbounds i32, i32* %1, i64 1
  store i32 %247, i32* %248, align 4
  %249 = add nsw i64 %242, 8192
  %250 = add nsw i64 %249, %243
  %251 = lshr i64 %250, 14
  %252 = trunc i64 %251 to i32
  %253 = getelementptr inbounds i32, i32* %1, i64 3
  store i32 %252, i32* %253, align 4
  %254 = getelementptr inbounds i32, i32* %1, i64 4
  %255 = load i32, i32* %224, align 4
  %256 = sext i32 %255 to i64
  %257 = getelementptr inbounds [16 x i32], [16 x i32]* %4, i64 0, i64 5
  %258 = load i32, i32* %257, align 4
  %259 = sext i32 %258 to i64
  %260 = getelementptr inbounds [16 x i32], [16 x i32]* %4, i64 0, i64 9
  %261 = load i32, i32* %260, align 4
  %262 = sext i32 %261 to i64
  %263 = getelementptr inbounds [16 x i32], [16 x i32]* %4, i64 0, i64 13
  %264 = load i32, i32* %263, align 4
  %265 = sext i32 %264 to i64
  %266 = getelementptr inbounds [16 x i32], [16 x i32]* %4, i64 0, i64 2
  %267 = add nsw i64 %256, %265
  %268 = add nsw i64 %259, %262
  %269 = sub nsw i64 %259, %262
  %270 = sub nsw i64 %256, %265
  %271 = add nsw i64 %267, %268
  %272 = mul nsw i64 %271, 11585
  %273 = sub nsw i64 %267, %268
  %274 = mul nsw i64 %273, 11585
  %275 = add nsw i64 %272, 8192
  %276 = lshr i64 %275, 14
  %277 = trunc i64 %276 to i32
  store i32 %277, i32* %254, align 4
  %278 = add nsw i64 %274, 8192
  %279 = lshr i64 %278, 14
  %280 = trunc i64 %279 to i32
  %281 = getelementptr inbounds i32, i32* %1, i64 6
  store i32 %280, i32* %281, align 4
  %282 = mul nsw i64 %269, 6270
  %283 = mul nsw i64 %270, 15137
  %284 = mul nsw i64 %269, -15137
  %285 = mul nsw i64 %270, 6270
  %286 = add nsw i64 %282, 8192
  %287 = add nsw i64 %286, %283
  %288 = lshr i64 %287, 14
  %289 = trunc i64 %288 to i32
  %290 = getelementptr inbounds i32, i32* %1, i64 5
  store i32 %289, i32* %290, align 4
  %291 = add nsw i64 %284, 8192
  %292 = add nsw i64 %291, %285
  %293 = lshr i64 %292, 14
  %294 = trunc i64 %293 to i32
  %295 = getelementptr inbounds i32, i32* %1, i64 7
  store i32 %294, i32* %295, align 4
  %296 = getelementptr inbounds i32, i32* %1, i64 8
  %297 = load i32, i32* %266, align 8
  %298 = sext i32 %297 to i64
  %299 = getelementptr inbounds [16 x i32], [16 x i32]* %4, i64 0, i64 6
  %300 = load i32, i32* %299, align 8
  %301 = sext i32 %300 to i64
  %302 = getelementptr inbounds [16 x i32], [16 x i32]* %4, i64 0, i64 10
  %303 = load i32, i32* %302, align 8
  %304 = sext i32 %303 to i64
  %305 = getelementptr inbounds [16 x i32], [16 x i32]* %4, i64 0, i64 14
  %306 = load i32, i32* %305, align 8
  %307 = sext i32 %306 to i64
  %308 = getelementptr inbounds [16 x i32], [16 x i32]* %4, i64 0, i64 3
  %309 = add nsw i64 %298, %307
  %310 = add nsw i64 %301, %304
  %311 = sub nsw i64 %301, %304
  %312 = sub nsw i64 %298, %307
  %313 = add nsw i64 %309, %310
  %314 = mul nsw i64 %313, 11585
  %315 = sub nsw i64 %309, %310
  %316 = mul nsw i64 %315, 11585
  %317 = add nsw i64 %314, 8192
  %318 = lshr i64 %317, 14
  %319 = trunc i64 %318 to i32
  store i32 %319, i32* %296, align 4
  %320 = add nsw i64 %316, 8192
  %321 = lshr i64 %320, 14
  %322 = trunc i64 %321 to i32
  %323 = getelementptr inbounds i32, i32* %1, i64 10
  store i32 %322, i32* %323, align 4
  %324 = mul nsw i64 %311, 6270
  %325 = mul nsw i64 %312, 15137
  %326 = mul nsw i64 %311, -15137
  %327 = mul nsw i64 %312, 6270
  %328 = add nsw i64 %324, 8192
  %329 = add nsw i64 %328, %325
  %330 = lshr i64 %329, 14
  %331 = trunc i64 %330 to i32
  %332 = getelementptr inbounds i32, i32* %1, i64 9
  store i32 %331, i32* %332, align 4
  %333 = add nsw i64 %326, 8192
  %334 = add nsw i64 %333, %327
  %335 = lshr i64 %334, 14
  %336 = trunc i64 %335 to i32
  %337 = getelementptr inbounds i32, i32* %1, i64 11
  store i32 %336, i32* %337, align 4
  %338 = getelementptr inbounds i32, i32* %1, i64 12
  %339 = load i32, i32* %308, align 4
  %340 = sext i32 %339 to i64
  %341 = getelementptr inbounds [16 x i32], [16 x i32]* %4, i64 0, i64 7
  %342 = load i32, i32* %341, align 4
  %343 = sext i32 %342 to i64
  %344 = getelementptr inbounds [16 x i32], [16 x i32]* %4, i64 0, i64 11
  %345 = load i32, i32* %344, align 4
  %346 = sext i32 %345 to i64
  %347 = getelementptr inbounds [16 x i32], [16 x i32]* %4, i64 0, i64 15
  %348 = load i32, i32* %347, align 4
  %349 = sext i32 %348 to i64
  %350 = add nsw i64 %340, %349
  %351 = add nsw i64 %343, %346
  %352 = sub nsw i64 %343, %346
  %353 = sub nsw i64 %340, %349
  %354 = add nsw i64 %350, %351
  %355 = mul nsw i64 %354, 11585
  %356 = sub nsw i64 %350, %351
  %357 = mul nsw i64 %356, 11585
  %358 = add nsw i64 %355, 8192
  %359 = lshr i64 %358, 14
  %360 = trunc i64 %359 to i32
  store i32 %360, i32* %338, align 4
  %361 = add nsw i64 %357, 8192
  %362 = lshr i64 %361, 14
  %363 = trunc i64 %362 to i32
  %364 = getelementptr inbounds i32, i32* %1, i64 14
  store i32 %363, i32* %364, align 4
  %365 = mul nsw i64 %352, 6270
  %366 = mul nsw i64 %353, 15137
  %367 = mul nsw i64 %352, -15137
  %368 = mul nsw i64 %353, 6270
  %369 = add nsw i64 %365, 8192
  %370 = add nsw i64 %369, %366
  %371 = lshr i64 %370, 14
  %372 = trunc i64 %371 to i32
  %373 = getelementptr inbounds i32, i32* %1, i64 13
  store i32 %372, i32* %373, align 4
  %374 = add nsw i64 %367, 8192
  %375 = add nsw i64 %374, %368
  %376 = lshr i64 %375, 14
  %377 = trunc i64 %376 to i32
  %378 = getelementptr inbounds i32, i32* %1, i64 15
  store i32 %377, i32* %378, align 4
  %379 = bitcast i32* %1 to <4 x i32>*
  %380 = load <4 x i32>, <4 x i32>* %379, align 4
  %381 = add nsw <4 x i32> %380, <i32 1, i32 1, i32 1, i32 1>
  %382 = ashr <4 x i32> %381, <i32 2, i32 2, i32 2, i32 2>
  %383 = bitcast i32* %1 to <4 x i32>*
  store <4 x i32> %382, <4 x i32>* %383, align 4
  %384 = getelementptr inbounds i32, i32* %1, i64 4
  %385 = bitcast i32* %384 to <4 x i32>*
  %386 = load <4 x i32>, <4 x i32>* %385, align 4
  %387 = add nsw <4 x i32> %386, <i32 1, i32 1, i32 1, i32 1>
  %388 = ashr <4 x i32> %387, <i32 2, i32 2, i32 2, i32 2>
  %389 = bitcast i32* %384 to <4 x i32>*
  store <4 x i32> %388, <4 x i32>* %389, align 4
  %390 = getelementptr inbounds i32, i32* %1, i64 8
  %391 = bitcast i32* %390 to <4 x i32>*
  %392 = load <4 x i32>, <4 x i32>* %391, align 4
  %393 = add nsw <4 x i32> %392, <i32 1, i32 1, i32 1, i32 1>
  %394 = ashr <4 x i32> %393, <i32 2, i32 2, i32 2, i32 2>
  %395 = bitcast i32* %390 to <4 x i32>*
  store <4 x i32> %394, <4 x i32>* %395, align 4
  %396 = getelementptr inbounds i32, i32* %1, i64 12
  %397 = bitcast i32* %396 to <4 x i32>*
  %398 = load <4 x i32>, <4 x i32>* %397, align 4
  %399 = add nsw <4 x i32> %398, <i32 1, i32 1, i32 1, i32 1>
  %400 = ashr <4 x i32> %399, <i32 2, i32 2, i32 2, i32 2>
  %401 = bitcast i32* %396 to <4 x i32>*
  store <4 x i32> %400, <4 x i32>* %401, align 4
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %5) #3
  ret void
}

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #1

; Function Attrs: argmemonly nounwind
declare void @llvm.memset.p0i8.i64(i8* nocapture writeonly, i8, i64, i1 immarg) #1

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #1

; Function Attrs: nofree norecurse nounwind ssp uwtable
define hidden void @vpx_fdct4x4_1_c(i16* nocapture readonly, i32* nocapture, i32) local_unnamed_addr #2 {
  %4 = sext i32 %2 to i64
  %5 = load i16, i16* %0, align 2
  %6 = sext i16 %5 to i32
  %7 = getelementptr inbounds i16, i16* %0, i64 1
  %8 = load i16, i16* %7, align 2
  %9 = sext i16 %8 to i32
  %10 = add nsw i32 %6, %9
  %11 = getelementptr inbounds i16, i16* %0, i64 2
  %12 = load i16, i16* %11, align 2
  %13 = sext i16 %12 to i32
  %14 = add nsw i32 %10, %13
  %15 = getelementptr inbounds i16, i16* %0, i64 3
  %16 = load i16, i16* %15, align 2
  %17 = sext i16 %16 to i32
  %18 = add nsw i32 %14, %17
  %19 = getelementptr inbounds i16, i16* %0, i64 %4
  %20 = load i16, i16* %19, align 2
  %21 = sext i16 %20 to i32
  %22 = add nsw i32 %18, %21
  %23 = add nsw i64 %4, 1
  %24 = getelementptr inbounds i16, i16* %0, i64 %23
  %25 = load i16, i16* %24, align 2
  %26 = sext i16 %25 to i32
  %27 = add nsw i32 %22, %26
  %28 = add nsw i64 %4, 2
  %29 = getelementptr inbounds i16, i16* %0, i64 %28
  %30 = load i16, i16* %29, align 2
  %31 = sext i16 %30 to i32
  %32 = add nsw i32 %27, %31
  %33 = add nsw i64 %4, 3
  %34 = getelementptr inbounds i16, i16* %0, i64 %33
  %35 = load i16, i16* %34, align 2
  %36 = sext i16 %35 to i32
  %37 = add nsw i32 %32, %36
  %38 = shl nsw i64 %4, 1
  %39 = getelementptr inbounds i16, i16* %0, i64 %38
  %40 = load i16, i16* %39, align 2
  %41 = sext i16 %40 to i32
  %42 = add nsw i32 %37, %41
  %43 = or i64 %38, 1
  %44 = getelementptr inbounds i16, i16* %0, i64 %43
  %45 = load i16, i16* %44, align 2
  %46 = sext i16 %45 to i32
  %47 = add nsw i32 %42, %46
  %48 = add nsw i64 %38, 2
  %49 = getelementptr inbounds i16, i16* %0, i64 %48
  %50 = load i16, i16* %49, align 2
  %51 = sext i16 %50 to i32
  %52 = add nsw i32 %47, %51
  %53 = add nsw i64 %38, 3
  %54 = getelementptr inbounds i16, i16* %0, i64 %53
  %55 = load i16, i16* %54, align 2
  %56 = sext i16 %55 to i32
  %57 = add nsw i32 %52, %56
  %58 = mul nsw i64 %4, 3
  %59 = getelementptr inbounds i16, i16* %0, i64 %58
  %60 = load i16, i16* %59, align 2
  %61 = sext i16 %60 to i32
  %62 = add nsw i32 %57, %61
  %63 = add nsw i64 %58, 1
  %64 = getelementptr inbounds i16, i16* %0, i64 %63
  %65 = load i16, i16* %64, align 2
  %66 = sext i16 %65 to i32
  %67 = add nsw i32 %62, %66
  %68 = add nsw i64 %58, 2
  %69 = getelementptr inbounds i16, i16* %0, i64 %68
  %70 = load i16, i16* %69, align 2
  %71 = sext i16 %70 to i32
  %72 = add nsw i32 %67, %71
  %73 = add nsw i64 %58, 3
  %74 = getelementptr inbounds i16, i16* %0, i64 %73
  %75 = load i16, i16* %74, align 2
  %76 = sext i16 %75 to i32
  %77 = add nsw i32 %72, %76
  %78 = shl nsw i32 %77, 1
  store i32 %78, i32* %1, align 4
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @vpx_fdct8x8_c(i16* nocapture readonly, i32* nocapture, i32) local_unnamed_addr #0 {
  %4 = alloca [64 x i32], align 16
  %5 = bitcast [64 x i32]* %4 to i8*
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %5) #3
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %5, i8 -86, i64 256, i1 false)
  %6 = getelementptr inbounds [64 x i32], [64 x i32]* %4, i64 0, i64 0
  %7 = mul nsw i32 %2, 7
  %8 = sext i32 %7 to i64
  %9 = sext i32 %2 to i64
  %10 = mul nsw i32 %2, 6
  %11 = sext i32 %10 to i64
  %12 = shl nsw i32 %2, 1
  %13 = sext i32 %12 to i64
  %14 = mul nsw i32 %2, 5
  %15 = sext i32 %14 to i64
  %16 = mul nsw i32 %2, 3
  %17 = sext i32 %16 to i64
  %18 = shl nsw i32 %2, 2
  %19 = sext i32 %18 to i64
  br label %20

20:                                               ; preds = %188, %3
  %21 = phi i32* [ null, %3 ], [ %6, %188 ]
  %22 = phi i32* [ %6, %3 ], [ %1, %188 ]
  %23 = phi i32 [ 0, %3 ], [ %189, %188 ]
  %24 = phi i16* [ %0, %3 ], [ %98, %188 ]
  %25 = icmp eq i32 %23, 0
  br label %26

26:                                               ; preds = %97, %20
  %27 = phi i32* [ %21, %20 ], [ %99, %97 ]
  %28 = phi i32* [ %22, %20 ], [ %185, %97 ]
  %29 = phi i32 [ 0, %20 ], [ %186, %97 ]
  %30 = phi i16* [ %24, %20 ], [ %98, %97 ]
  br i1 %25, label %31, label %72

31:                                               ; preds = %26
  %32 = load i16, i16* %30, align 2
  %33 = sext i16 %32 to i32
  %34 = getelementptr inbounds i16, i16* %30, i64 %8
  %35 = load i16, i16* %34, align 2
  %36 = sext i16 %35 to i32
  %37 = add nsw i32 %36, %33
  %38 = shl nsw i32 %37, 2
  %39 = getelementptr inbounds i16, i16* %30, i64 %9
  %40 = load i16, i16* %39, align 2
  %41 = sext i16 %40 to i32
  %42 = getelementptr inbounds i16, i16* %30, i64 %11
  %43 = load i16, i16* %42, align 2
  %44 = sext i16 %43 to i32
  %45 = add nsw i32 %44, %41
  %46 = shl nsw i32 %45, 2
  %47 = getelementptr inbounds i16, i16* %30, i64 %13
  %48 = load i16, i16* %47, align 2
  %49 = sext i16 %48 to i32
  %50 = getelementptr inbounds i16, i16* %30, i64 %15
  %51 = load i16, i16* %50, align 2
  %52 = sext i16 %51 to i32
  %53 = add nsw i32 %52, %49
  %54 = shl nsw i32 %53, 2
  %55 = getelementptr inbounds i16, i16* %30, i64 %17
  %56 = load i16, i16* %55, align 2
  %57 = sext i16 %56 to i32
  %58 = getelementptr inbounds i16, i16* %30, i64 %19
  %59 = load i16, i16* %58, align 2
  %60 = sext i16 %59 to i32
  %61 = add nsw i32 %60, %57
  %62 = shl nsw i32 %61, 2
  %63 = sub nsw i32 %57, %60
  %64 = shl nsw i32 %63, 2
  %65 = sub nsw i32 %49, %52
  %66 = shl nsw i32 %65, 2
  %67 = sub nsw i32 %41, %44
  %68 = shl nsw i32 %67, 2
  %69 = sub nsw i32 %33, %36
  %70 = shl nsw i32 %69, 2
  %71 = getelementptr inbounds i16, i16* %30, i64 1
  br label %97

72:                                               ; preds = %26
  %73 = load i32, i32* %27, align 4
  %74 = getelementptr inbounds i32, i32* %27, i64 56
  %75 = load i32, i32* %74, align 4
  %76 = add nsw i32 %75, %73
  %77 = getelementptr inbounds i32, i32* %27, i64 8
  %78 = load i32, i32* %77, align 4
  %79 = getelementptr inbounds i32, i32* %27, i64 48
  %80 = load i32, i32* %79, align 4
  %81 = add nsw i32 %80, %78
  %82 = getelementptr inbounds i32, i32* %27, i64 16
  %83 = load i32, i32* %82, align 4
  %84 = getelementptr inbounds i32, i32* %27, i64 40
  %85 = load i32, i32* %84, align 4
  %86 = add nsw i32 %85, %83
  %87 = getelementptr inbounds i32, i32* %27, i64 24
  %88 = load i32, i32* %87, align 4
  %89 = getelementptr inbounds i32, i32* %27, i64 32
  %90 = load i32, i32* %89, align 4
  %91 = add nsw i32 %90, %88
  %92 = sub nsw i32 %88, %90
  %93 = sub nsw i32 %83, %85
  %94 = sub nsw i32 %78, %80
  %95 = sub nsw i32 %73, %75
  %96 = getelementptr inbounds i32, i32* %27, i64 1
  br label %97

97:                                               ; preds = %72, %31
  %98 = phi i16* [ %71, %31 ], [ %30, %72 ]
  %99 = phi i32* [ %27, %31 ], [ %96, %72 ]
  %100 = phi i32 [ %38, %31 ], [ %76, %72 ]
  %101 = phi i32 [ %46, %31 ], [ %81, %72 ]
  %102 = phi i32 [ %54, %31 ], [ %86, %72 ]
  %103 = phi i32 [ %62, %31 ], [ %91, %72 ]
  %104 = phi i32 [ %64, %31 ], [ %92, %72 ]
  %105 = phi i32 [ %66, %31 ], [ %93, %72 ]
  %106 = phi i32 [ %68, %31 ], [ %94, %72 ]
  %107 = phi i32 [ %70, %31 ], [ %95, %72 ]
  %108 = sext i32 %107 to i64
  %109 = sext i32 %106 to i64
  %110 = sext i32 %105 to i64
  %111 = sext i32 %104 to i64
  %112 = sext i32 %103 to i64
  %113 = sext i32 %102 to i64
  %114 = sext i32 %101 to i64
  %115 = sext i32 %100 to i64
  %116 = add nsw i64 %112, %115
  %117 = add nsw i64 %113, %114
  %118 = sub nsw i64 %114, %113
  %119 = sub nsw i64 %115, %112
  %120 = add nsw i64 %116, %117
  %121 = mul nsw i64 %120, 11585
  %122 = sub nsw i64 %116, %117
  %123 = mul nsw i64 %122, 11585
  %124 = mul nsw i64 %118, 6270
  %125 = mul nsw i64 %119, 15137
  %126 = mul nsw i64 %118, -15137
  %127 = mul nsw i64 %119, 6270
  %128 = add nsw i64 %121, 8192
  %129 = lshr i64 %128, 14
  %130 = trunc i64 %129 to i32
  store i32 %130, i32* %28, align 4
  %131 = add nsw i64 %124, 8192
  %132 = add nsw i64 %131, %125
  %133 = lshr i64 %132, 14
  %134 = trunc i64 %133 to i32
  %135 = getelementptr inbounds i32, i32* %28, i64 2
  store i32 %134, i32* %135, align 4
  %136 = add nsw i64 %123, 8192
  %137 = lshr i64 %136, 14
  %138 = trunc i64 %137 to i32
  %139 = getelementptr inbounds i32, i32* %28, i64 4
  store i32 %138, i32* %139, align 4
  %140 = add nsw i64 %126, 8192
  %141 = add nsw i64 %140, %127
  %142 = lshr i64 %141, 14
  %143 = trunc i64 %142 to i32
  %144 = getelementptr inbounds i32, i32* %28, i64 6
  store i32 %143, i32* %144, align 4
  %145 = sub nsw i64 %109, %110
  %146 = mul nsw i64 %145, 11585
  %147 = add nsw i64 %109, %110
  %148 = mul nsw i64 %147, 11585
  %149 = add nsw i64 %146, 8192
  %150 = ashr i64 %149, 14
  %151 = add nsw i64 %148, 8192
  %152 = ashr i64 %151, 14
  %153 = add nsw i64 %150, %111
  %154 = sub nsw i64 %111, %150
  %155 = sub nsw i64 %108, %152
  %156 = add nsw i64 %152, %108
  %157 = mul nsw i64 %153, 3196
  %158 = mul nsw i64 %156, 16069
  %159 = mul nsw i64 %154, 13623
  %160 = mul nsw i64 %155, 9102
  %161 = mul nsw i64 %155, 13623
  %162 = mul nsw i64 %154, -9102
  %163 = mul nsw i64 %156, 3196
  %164 = mul nsw i64 %153, -16069
  %165 = add nsw i64 %158, 8192
  %166 = add nsw i64 %165, %157
  %167 = lshr i64 %166, 14
  %168 = trunc i64 %167 to i32
  %169 = getelementptr inbounds i32, i32* %28, i64 1
  store i32 %168, i32* %169, align 4
  %170 = add nsw i64 %162, 8192
  %171 = add nsw i64 %170, %161
  %172 = lshr i64 %171, 14
  %173 = trunc i64 %172 to i32
  %174 = getelementptr inbounds i32, i32* %28, i64 3
  store i32 %173, i32* %174, align 4
  %175 = add nsw i64 %160, 8192
  %176 = add nsw i64 %175, %159
  %177 = lshr i64 %176, 14
  %178 = trunc i64 %177 to i32
  %179 = getelementptr inbounds i32, i32* %28, i64 5
  store i32 %178, i32* %179, align 4
  %180 = add nsw i64 %164, 8192
  %181 = add nsw i64 %180, %163
  %182 = lshr i64 %181, 14
  %183 = trunc i64 %182 to i32
  %184 = getelementptr inbounds i32, i32* %28, i64 7
  store i32 %183, i32* %184, align 4
  %185 = getelementptr inbounds i32, i32* %28, i64 8
  %186 = add nuw nsw i32 %29, 1
  %187 = icmp eq i32 %186, 8
  br i1 %187, label %188, label %26

188:                                              ; preds = %97
  %189 = add nuw nsw i32 %23, 1
  %190 = icmp eq i32 %189, 2
  br i1 %190, label %191, label %20

191:                                              ; preds = %188
  %192 = bitcast i32* %1 to <32 x i32>*
  %193 = load <32 x i32>, <32 x i32>* %192, align 4
  %194 = shufflevector <32 x i32> %193, <32 x i32> undef, <4 x i32> <i32 0, i32 8, i32 16, i32 24>
  %195 = shufflevector <32 x i32> %193, <32 x i32> undef, <4 x i32> <i32 1, i32 9, i32 17, i32 25>
  %196 = shufflevector <32 x i32> %193, <32 x i32> undef, <4 x i32> <i32 2, i32 10, i32 18, i32 26>
  %197 = shufflevector <32 x i32> %193, <32 x i32> undef, <4 x i32> <i32 3, i32 11, i32 19, i32 27>
  %198 = shufflevector <32 x i32> %193, <32 x i32> undef, <4 x i32> <i32 4, i32 12, i32 20, i32 28>
  %199 = shufflevector <32 x i32> %193, <32 x i32> undef, <4 x i32> <i32 5, i32 13, i32 21, i32 29>
  %200 = shufflevector <32 x i32> %193, <32 x i32> undef, <4 x i32> <i32 6, i32 14, i32 22, i32 30>
  %201 = shufflevector <32 x i32> %193, <32 x i32> undef, <4 x i32> <i32 7, i32 15, i32 23, i32 31>
  %202 = sdiv <4 x i32> %194, <i32 2, i32 2, i32 2, i32 2>
  %203 = sdiv <4 x i32> %195, <i32 2, i32 2, i32 2, i32 2>
  %204 = sdiv <4 x i32> %196, <i32 2, i32 2, i32 2, i32 2>
  %205 = sdiv <4 x i32> %197, <i32 2, i32 2, i32 2, i32 2>
  %206 = sdiv <4 x i32> %198, <i32 2, i32 2, i32 2, i32 2>
  %207 = sdiv <4 x i32> %199, <i32 2, i32 2, i32 2, i32 2>
  %208 = sdiv <4 x i32> %200, <i32 2, i32 2, i32 2, i32 2>
  %209 = sdiv <4 x i32> %201, <i32 2, i32 2, i32 2, i32 2>
  %210 = bitcast i32* %1 to <32 x i32>*
  %211 = shufflevector <4 x i32> %202, <4 x i32> %203, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %212 = shufflevector <4 x i32> %204, <4 x i32> %205, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %213 = shufflevector <4 x i32> %206, <4 x i32> %207, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %214 = shufflevector <4 x i32> %208, <4 x i32> %209, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %215 = shufflevector <8 x i32> %211, <8 x i32> %212, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %216 = shufflevector <8 x i32> %213, <8 x i32> %214, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %217 = shufflevector <16 x i32> %215, <16 x i32> %216, <32 x i32> <i32 0, i32 4, i32 8, i32 12, i32 16, i32 20, i32 24, i32 28, i32 1, i32 5, i32 9, i32 13, i32 17, i32 21, i32 25, i32 29, i32 2, i32 6, i32 10, i32 14, i32 18, i32 22, i32 26, i32 30, i32 3, i32 7, i32 11, i32 15, i32 19, i32 23, i32 27, i32 31>
  store <32 x i32> %217, <32 x i32>* %210, align 4
  %218 = getelementptr inbounds i32, i32* %1, i64 32
  %219 = bitcast i32* %218 to <32 x i32>*
  %220 = load <32 x i32>, <32 x i32>* %219, align 4
  %221 = shufflevector <32 x i32> %220, <32 x i32> undef, <4 x i32> <i32 0, i32 8, i32 16, i32 24>
  %222 = shufflevector <32 x i32> %220, <32 x i32> undef, <4 x i32> <i32 1, i32 9, i32 17, i32 25>
  %223 = shufflevector <32 x i32> %220, <32 x i32> undef, <4 x i32> <i32 2, i32 10, i32 18, i32 26>
  %224 = shufflevector <32 x i32> %220, <32 x i32> undef, <4 x i32> <i32 3, i32 11, i32 19, i32 27>
  %225 = shufflevector <32 x i32> %220, <32 x i32> undef, <4 x i32> <i32 4, i32 12, i32 20, i32 28>
  %226 = shufflevector <32 x i32> %220, <32 x i32> undef, <4 x i32> <i32 5, i32 13, i32 21, i32 29>
  %227 = shufflevector <32 x i32> %220, <32 x i32> undef, <4 x i32> <i32 6, i32 14, i32 22, i32 30>
  %228 = shufflevector <32 x i32> %220, <32 x i32> undef, <4 x i32> <i32 7, i32 15, i32 23, i32 31>
  %229 = sdiv <4 x i32> %221, <i32 2, i32 2, i32 2, i32 2>
  %230 = sdiv <4 x i32> %222, <i32 2, i32 2, i32 2, i32 2>
  %231 = sdiv <4 x i32> %223, <i32 2, i32 2, i32 2, i32 2>
  %232 = sdiv <4 x i32> %224, <i32 2, i32 2, i32 2, i32 2>
  %233 = sdiv <4 x i32> %225, <i32 2, i32 2, i32 2, i32 2>
  %234 = sdiv <4 x i32> %226, <i32 2, i32 2, i32 2, i32 2>
  %235 = sdiv <4 x i32> %227, <i32 2, i32 2, i32 2, i32 2>
  %236 = sdiv <4 x i32> %228, <i32 2, i32 2, i32 2, i32 2>
  %237 = getelementptr inbounds i32, i32* %1, i64 32
  %238 = bitcast i32* %237 to <32 x i32>*
  %239 = shufflevector <4 x i32> %229, <4 x i32> %230, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %240 = shufflevector <4 x i32> %231, <4 x i32> %232, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %241 = shufflevector <4 x i32> %233, <4 x i32> %234, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %242 = shufflevector <4 x i32> %235, <4 x i32> %236, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %243 = shufflevector <8 x i32> %239, <8 x i32> %240, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %244 = shufflevector <8 x i32> %241, <8 x i32> %242, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %245 = shufflevector <16 x i32> %243, <16 x i32> %244, <32 x i32> <i32 0, i32 4, i32 8, i32 12, i32 16, i32 20, i32 24, i32 28, i32 1, i32 5, i32 9, i32 13, i32 17, i32 21, i32 25, i32 29, i32 2, i32 6, i32 10, i32 14, i32 18, i32 22, i32 26, i32 30, i32 3, i32 7, i32 11, i32 15, i32 19, i32 23, i32 27, i32 31>
  store <32 x i32> %245, <32 x i32>* %238, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %5) #3
  ret void
}

; Function Attrs: nofree norecurse nounwind ssp uwtable
define hidden void @vpx_fdct8x8_1_c(i16* nocapture readonly, i32* nocapture, i32) local_unnamed_addr #2 {
  %4 = sext i32 %2 to i64
  %5 = bitcast i16* %0 to <8 x i16>*
  %6 = load <8 x i16>, <8 x i16>* %5, align 2
  %7 = sext <8 x i16> %6 to <8 x i32>
  %8 = shufflevector <8 x i32> %7, <8 x i32> undef, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 undef, i32 undef, i32 undef, i32 undef>
  %9 = add nsw <8 x i32> %8, %7
  %10 = shufflevector <8 x i32> %9, <8 x i32> undef, <8 x i32> <i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %11 = add nsw <8 x i32> %9, %10
  %12 = shufflevector <8 x i32> %11, <8 x i32> undef, <8 x i32> <i32 1, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %13 = add nsw <8 x i32> %11, %12
  %14 = extractelement <8 x i32> %13, i32 0
  %15 = getelementptr inbounds i16, i16* %0, i64 %4
  %16 = bitcast i16* %15 to <8 x i16>*
  %17 = load <8 x i16>, <8 x i16>* %16, align 2
  %18 = sext <8 x i16> %17 to <8 x i32>
  %19 = shufflevector <8 x i32> %18, <8 x i32> undef, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 undef, i32 undef, i32 undef, i32 undef>
  %20 = add nsw <8 x i32> %19, %18
  %21 = shufflevector <8 x i32> %20, <8 x i32> undef, <8 x i32> <i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %22 = add nsw <8 x i32> %20, %21
  %23 = shufflevector <8 x i32> %22, <8 x i32> undef, <8 x i32> <i32 1, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %24 = add nsw <8 x i32> %22, %23
  %25 = extractelement <8 x i32> %24, i32 0
  %26 = add nsw i32 %25, %14
  %27 = shl nsw i64 %4, 1
  %28 = getelementptr inbounds i16, i16* %0, i64 %27
  %29 = bitcast i16* %28 to <8 x i16>*
  %30 = load <8 x i16>, <8 x i16>* %29, align 2
  %31 = sext <8 x i16> %30 to <8 x i32>
  %32 = shufflevector <8 x i32> %31, <8 x i32> undef, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 undef, i32 undef, i32 undef, i32 undef>
  %33 = add nsw <8 x i32> %32, %31
  %34 = shufflevector <8 x i32> %33, <8 x i32> undef, <8 x i32> <i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %35 = add nsw <8 x i32> %33, %34
  %36 = shufflevector <8 x i32> %35, <8 x i32> undef, <8 x i32> <i32 1, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %37 = add nsw <8 x i32> %35, %36
  %38 = extractelement <8 x i32> %37, i32 0
  %39 = add nsw i32 %38, %26
  %40 = mul nsw i64 %4, 3
  %41 = getelementptr inbounds i16, i16* %0, i64 %40
  %42 = bitcast i16* %41 to <8 x i16>*
  %43 = load <8 x i16>, <8 x i16>* %42, align 2
  %44 = sext <8 x i16> %43 to <8 x i32>
  %45 = shufflevector <8 x i32> %44, <8 x i32> undef, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 undef, i32 undef, i32 undef, i32 undef>
  %46 = add nsw <8 x i32> %45, %44
  %47 = shufflevector <8 x i32> %46, <8 x i32> undef, <8 x i32> <i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %48 = add nsw <8 x i32> %46, %47
  %49 = shufflevector <8 x i32> %48, <8 x i32> undef, <8 x i32> <i32 1, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %50 = add nsw <8 x i32> %48, %49
  %51 = extractelement <8 x i32> %50, i32 0
  %52 = add nsw i32 %51, %39
  %53 = shl nsw i64 %4, 2
  %54 = getelementptr inbounds i16, i16* %0, i64 %53
  %55 = bitcast i16* %54 to <8 x i16>*
  %56 = load <8 x i16>, <8 x i16>* %55, align 2
  %57 = sext <8 x i16> %56 to <8 x i32>
  %58 = shufflevector <8 x i32> %57, <8 x i32> undef, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 undef, i32 undef, i32 undef, i32 undef>
  %59 = add nsw <8 x i32> %58, %57
  %60 = shufflevector <8 x i32> %59, <8 x i32> undef, <8 x i32> <i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %61 = add nsw <8 x i32> %59, %60
  %62 = shufflevector <8 x i32> %61, <8 x i32> undef, <8 x i32> <i32 1, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %63 = add nsw <8 x i32> %61, %62
  %64 = extractelement <8 x i32> %63, i32 0
  %65 = add nsw i32 %64, %52
  %66 = mul nsw i64 %4, 5
  %67 = getelementptr inbounds i16, i16* %0, i64 %66
  %68 = bitcast i16* %67 to <8 x i16>*
  %69 = load <8 x i16>, <8 x i16>* %68, align 2
  %70 = sext <8 x i16> %69 to <8 x i32>
  %71 = shufflevector <8 x i32> %70, <8 x i32> undef, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 undef, i32 undef, i32 undef, i32 undef>
  %72 = add nsw <8 x i32> %71, %70
  %73 = shufflevector <8 x i32> %72, <8 x i32> undef, <8 x i32> <i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %74 = add nsw <8 x i32> %72, %73
  %75 = shufflevector <8 x i32> %74, <8 x i32> undef, <8 x i32> <i32 1, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %76 = add nsw <8 x i32> %74, %75
  %77 = extractelement <8 x i32> %76, i32 0
  %78 = add nsw i32 %77, %65
  %79 = mul nsw i64 %4, 6
  %80 = getelementptr inbounds i16, i16* %0, i64 %79
  %81 = bitcast i16* %80 to <8 x i16>*
  %82 = load <8 x i16>, <8 x i16>* %81, align 2
  %83 = sext <8 x i16> %82 to <8 x i32>
  %84 = shufflevector <8 x i32> %83, <8 x i32> undef, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 undef, i32 undef, i32 undef, i32 undef>
  %85 = add nsw <8 x i32> %84, %83
  %86 = shufflevector <8 x i32> %85, <8 x i32> undef, <8 x i32> <i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %87 = add nsw <8 x i32> %85, %86
  %88 = shufflevector <8 x i32> %87, <8 x i32> undef, <8 x i32> <i32 1, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %89 = add nsw <8 x i32> %87, %88
  %90 = extractelement <8 x i32> %89, i32 0
  %91 = add nsw i32 %90, %78
  %92 = mul nsw i64 %4, 7
  %93 = getelementptr inbounds i16, i16* %0, i64 %92
  %94 = bitcast i16* %93 to <8 x i16>*
  %95 = load <8 x i16>, <8 x i16>* %94, align 2
  %96 = sext <8 x i16> %95 to <8 x i32>
  %97 = shufflevector <8 x i32> %96, <8 x i32> undef, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 undef, i32 undef, i32 undef, i32 undef>
  %98 = add nsw <8 x i32> %97, %96
  %99 = shufflevector <8 x i32> %98, <8 x i32> undef, <8 x i32> <i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %100 = add nsw <8 x i32> %98, %99
  %101 = shufflevector <8 x i32> %100, <8 x i32> undef, <8 x i32> <i32 1, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %102 = add nsw <8 x i32> %100, %101
  %103 = extractelement <8 x i32> %102, i32 0
  %104 = add nsw i32 %103, %91
  store i32 %104, i32* %1, align 4
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @vpx_fdct16x16_c(i16* nocapture readonly, i32* nocapture, i32) local_unnamed_addr #0 {
  %4 = alloca [256 x i32], align 16
  %5 = bitcast [256 x i32]* %4 to i8*
  call void @llvm.lifetime.start.p0i8(i64 1024, i8* nonnull %5) #3
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %5, i8 -86, i64 1024, i1 false)
  %6 = getelementptr inbounds [256 x i32], [256 x i32]* %4, i64 0, i64 0
  %7 = mul nsw i32 %2, 15
  %8 = sext i32 %7 to i64
  %9 = sext i32 %2 to i64
  %10 = mul nsw i32 %2, 14
  %11 = sext i32 %10 to i64
  %12 = shl nsw i32 %2, 1
  %13 = sext i32 %12 to i64
  %14 = mul nsw i32 %2, 13
  %15 = sext i32 %14 to i64
  %16 = mul nsw i32 %2, 3
  %17 = sext i32 %16 to i64
  %18 = mul nsw i32 %2, 12
  %19 = sext i32 %18 to i64
  %20 = shl nsw i32 %2, 2
  %21 = sext i32 %20 to i64
  %22 = mul nsw i32 %2, 11
  %23 = sext i32 %22 to i64
  %24 = mul nsw i32 %2, 5
  %25 = sext i32 %24 to i64
  %26 = mul nsw i32 %2, 10
  %27 = sext i32 %26 to i64
  %28 = mul nsw i32 %2, 6
  %29 = sext i32 %28 to i64
  %30 = mul nsw i32 %2, 9
  %31 = sext i32 %30 to i64
  %32 = mul nsw i32 %2, 7
  %33 = sext i32 %32 to i64
  %34 = shl nsw i32 %2, 3
  %35 = sext i32 %34 to i64
  br label %36

36:                                               ; preds = %431, %3
  %37 = phi i16* [ %0, %3 ], [ %427, %431 ]
  %38 = phi i32 [ 0, %3 ], [ %432, %431 ]
  %39 = phi i32* [ null, %3 ], [ %6, %431 ]
  %40 = phi i32* [ %6, %3 ], [ %1, %431 ]
  %41 = icmp eq i32 %38, 0
  br label %42

42:                                               ; preds = %208, %36
  %43 = phi i16* [ %37, %36 ], [ %427, %208 ]
  %44 = phi i32* [ %39, %36 ], [ %225, %208 ]
  %45 = phi i32* [ %40, %36 ], [ %428, %208 ]
  %46 = phi i32 [ 0, %36 ], [ %429, %208 ]
  br i1 %41, label %47, label %127

47:                                               ; preds = %42
  %48 = load i16, i16* %43, align 2
  %49 = sext i16 %48 to i32
  %50 = getelementptr inbounds i16, i16* %43, i64 %8
  %51 = load i16, i16* %50, align 2
  %52 = sext i16 %51 to i32
  %53 = add nsw i32 %52, %49
  %54 = shl nsw i32 %53, 2
  %55 = getelementptr inbounds i16, i16* %43, i64 %9
  %56 = load i16, i16* %55, align 2
  %57 = sext i16 %56 to i32
  %58 = getelementptr inbounds i16, i16* %43, i64 %11
  %59 = load i16, i16* %58, align 2
  %60 = sext i16 %59 to i32
  %61 = add nsw i32 %60, %57
  %62 = shl nsw i32 %61, 2
  %63 = getelementptr inbounds i16, i16* %43, i64 %13
  %64 = load i16, i16* %63, align 2
  %65 = sext i16 %64 to i32
  %66 = getelementptr inbounds i16, i16* %43, i64 %15
  %67 = load i16, i16* %66, align 2
  %68 = sext i16 %67 to i32
  %69 = add nsw i32 %68, %65
  %70 = shl nsw i32 %69, 2
  %71 = getelementptr inbounds i16, i16* %43, i64 %17
  %72 = load i16, i16* %71, align 2
  %73 = sext i16 %72 to i32
  %74 = getelementptr inbounds i16, i16* %43, i64 %19
  %75 = load i16, i16* %74, align 2
  %76 = sext i16 %75 to i32
  %77 = add nsw i32 %76, %73
  %78 = shl nsw i32 %77, 2
  %79 = getelementptr inbounds i16, i16* %43, i64 %21
  %80 = load i16, i16* %79, align 2
  %81 = sext i16 %80 to i32
  %82 = getelementptr inbounds i16, i16* %43, i64 %23
  %83 = load i16, i16* %82, align 2
  %84 = sext i16 %83 to i32
  %85 = add nsw i32 %84, %81
  %86 = shl nsw i32 %85, 2
  %87 = getelementptr inbounds i16, i16* %43, i64 %25
  %88 = load i16, i16* %87, align 2
  %89 = sext i16 %88 to i32
  %90 = getelementptr inbounds i16, i16* %43, i64 %27
  %91 = load i16, i16* %90, align 2
  %92 = sext i16 %91 to i32
  %93 = add nsw i32 %92, %89
  %94 = shl nsw i32 %93, 2
  %95 = getelementptr inbounds i16, i16* %43, i64 %29
  %96 = load i16, i16* %95, align 2
  %97 = sext i16 %96 to i32
  %98 = getelementptr inbounds i16, i16* %43, i64 %31
  %99 = load i16, i16* %98, align 2
  %100 = sext i16 %99 to i32
  %101 = add nsw i32 %100, %97
  %102 = shl nsw i32 %101, 2
  %103 = getelementptr inbounds i16, i16* %43, i64 %33
  %104 = load i16, i16* %103, align 2
  %105 = sext i16 %104 to i32
  %106 = getelementptr inbounds i16, i16* %43, i64 %35
  %107 = load i16, i16* %106, align 2
  %108 = sext i16 %107 to i32
  %109 = add nsw i32 %108, %105
  %110 = shl nsw i32 %109, 2
  %111 = sub nsw i32 %105, %108
  %112 = shl nsw i32 %111, 2
  %113 = sub nsw i32 %97, %100
  %114 = shl nsw i32 %113, 2
  %115 = sub nsw i32 %89, %92
  %116 = shl nsw i32 %115, 2
  %117 = sub nsw i32 %81, %84
  %118 = shl nsw i32 %117, 2
  %119 = sub nsw i32 %73, %76
  %120 = shl nsw i32 %119, 2
  %121 = sub nsw i32 %65, %68
  %122 = shl nsw i32 %121, 2
  %123 = sub nsw i32 %57, %60
  %124 = shl nsw i32 %123, 2
  %125 = sub nsw i32 %49, %52
  %126 = shl nsw i32 %125, 2
  br label %208

127:                                              ; preds = %42
  %128 = load i32, i32* %44, align 4
  %129 = add nsw i32 %128, 1
  %130 = ashr i32 %129, 2
  %131 = getelementptr inbounds i32, i32* %44, i64 240
  %132 = load i32, i32* %131, align 4
  %133 = add nsw i32 %132, 1
  %134 = ashr i32 %133, 2
  %135 = add nsw i32 %134, %130
  %136 = getelementptr inbounds i32, i32* %44, i64 16
  %137 = load i32, i32* %136, align 4
  %138 = add nsw i32 %137, 1
  %139 = ashr i32 %138, 2
  %140 = getelementptr inbounds i32, i32* %44, i64 224
  %141 = load i32, i32* %140, align 4
  %142 = add nsw i32 %141, 1
  %143 = ashr i32 %142, 2
  %144 = add nsw i32 %143, %139
  %145 = getelementptr inbounds i32, i32* %44, i64 32
  %146 = load i32, i32* %145, align 4
  %147 = add nsw i32 %146, 1
  %148 = ashr i32 %147, 2
  %149 = getelementptr inbounds i32, i32* %44, i64 208
  %150 = load i32, i32* %149, align 4
  %151 = add nsw i32 %150, 1
  %152 = ashr i32 %151, 2
  %153 = add nsw i32 %152, %148
  %154 = getelementptr inbounds i32, i32* %44, i64 48
  %155 = load i32, i32* %154, align 4
  %156 = add nsw i32 %155, 1
  %157 = ashr i32 %156, 2
  %158 = getelementptr inbounds i32, i32* %44, i64 192
  %159 = load i32, i32* %158, align 4
  %160 = add nsw i32 %159, 1
  %161 = ashr i32 %160, 2
  %162 = add nsw i32 %161, %157
  %163 = getelementptr inbounds i32, i32* %44, i64 64
  %164 = load i32, i32* %163, align 4
  %165 = add nsw i32 %164, 1
  %166 = ashr i32 %165, 2
  %167 = getelementptr inbounds i32, i32* %44, i64 176
  %168 = load i32, i32* %167, align 4
  %169 = add nsw i32 %168, 1
  %170 = ashr i32 %169, 2
  %171 = add nsw i32 %170, %166
  %172 = getelementptr inbounds i32, i32* %44, i64 80
  %173 = load i32, i32* %172, align 4
  %174 = add nsw i32 %173, 1
  %175 = ashr i32 %174, 2
  %176 = getelementptr inbounds i32, i32* %44, i64 160
  %177 = load i32, i32* %176, align 4
  %178 = add nsw i32 %177, 1
  %179 = ashr i32 %178, 2
  %180 = add nsw i32 %179, %175
  %181 = getelementptr inbounds i32, i32* %44, i64 96
  %182 = load i32, i32* %181, align 4
  %183 = add nsw i32 %182, 1
  %184 = ashr i32 %183, 2
  %185 = getelementptr inbounds i32, i32* %44, i64 144
  %186 = load i32, i32* %185, align 4
  %187 = add nsw i32 %186, 1
  %188 = ashr i32 %187, 2
  %189 = add nsw i32 %188, %184
  %190 = getelementptr inbounds i32, i32* %44, i64 112
  %191 = load i32, i32* %190, align 4
  %192 = add nsw i32 %191, 1
  %193 = ashr i32 %192, 2
  %194 = getelementptr inbounds i32, i32* %44, i64 128
  %195 = load i32, i32* %194, align 4
  %196 = add nsw i32 %195, 1
  %197 = ashr i32 %196, 2
  %198 = add nsw i32 %197, %193
  %199 = sub nsw i32 %193, %197
  %200 = sub nsw i32 %184, %188
  %201 = sub nsw i32 %175, %179
  %202 = sub nsw i32 %166, %170
  %203 = sub nsw i32 %157, %161
  %204 = sub nsw i32 %148, %152
  %205 = sub nsw i32 %139, %143
  %206 = sub nsw i32 %130, %134
  %207 = getelementptr inbounds i32, i32* %44, i64 1
  br label %208

208:                                              ; preds = %127, %47
  %209 = phi i32 [ %54, %47 ], [ %135, %127 ]
  %210 = phi i32 [ %62, %47 ], [ %144, %127 ]
  %211 = phi i32 [ %70, %47 ], [ %153, %127 ]
  %212 = phi i32 [ %78, %47 ], [ %162, %127 ]
  %213 = phi i32 [ %86, %47 ], [ %171, %127 ]
  %214 = phi i32 [ %94, %47 ], [ %180, %127 ]
  %215 = phi i32 [ %102, %47 ], [ %189, %127 ]
  %216 = phi i32 [ %110, %47 ], [ %198, %127 ]
  %217 = phi i32 [ %126, %47 ], [ %206, %127 ]
  %218 = phi i32 [ %124, %47 ], [ %205, %127 ]
  %219 = phi i32 [ %122, %47 ], [ %204, %127 ]
  %220 = phi i32 [ %120, %47 ], [ %203, %127 ]
  %221 = phi i32 [ %118, %47 ], [ %202, %127 ]
  %222 = phi i32 [ %116, %47 ], [ %201, %127 ]
  %223 = phi i32 [ %114, %47 ], [ %200, %127 ]
  %224 = phi i32 [ %112, %47 ], [ %199, %127 ]
  %225 = phi i32* [ %44, %47 ], [ %207, %127 ]
  %226 = sext i32 %224 to i64
  %227 = sext i32 %223 to i64
  %228 = sext i32 %222 to i64
  %229 = sext i32 %221 to i64
  %230 = sext i32 %220 to i64
  %231 = sext i32 %219 to i64
  %232 = sext i32 %218 to i64
  %233 = sext i32 %217 to i64
  %234 = sext i32 %216 to i64
  %235 = sext i32 %215 to i64
  %236 = sext i32 %214 to i64
  %237 = sext i32 %213 to i64
  %238 = sext i32 %212 to i64
  %239 = sext i32 %211 to i64
  %240 = sext i32 %210 to i64
  %241 = sext i32 %209 to i64
  %242 = add nsw i64 %234, %241
  %243 = add nsw i64 %235, %240
  %244 = add nsw i64 %236, %239
  %245 = add nsw i64 %237, %238
  %246 = sub nsw i64 %238, %237
  %247 = sub nsw i64 %239, %236
  %248 = sub nsw i64 %240, %235
  %249 = sub nsw i64 %241, %234
  %250 = add nsw i64 %242, %245
  %251 = add nsw i64 %243, %244
  %252 = sub nsw i64 %243, %244
  %253 = sub nsw i64 %242, %245
  %254 = add nsw i64 %250, %251
  %255 = mul nsw i64 %254, 11585
  %256 = sub nsw i64 %250, %251
  %257 = mul nsw i64 %256, 11585
  %258 = mul nsw i64 %253, 15137
  %259 = mul nsw i64 %252, 6270
  %260 = mul nsw i64 %253, 6270
  %261 = mul nsw i64 %252, -15137
  %262 = add nsw i64 %255, 8192
  %263 = lshr i64 %262, 14
  %264 = trunc i64 %263 to i32
  store i32 %264, i32* %45, align 4
  %265 = add nsw i64 %259, 8192
  %266 = add nsw i64 %265, %258
  %267 = lshr i64 %266, 14
  %268 = trunc i64 %267 to i32
  %269 = getelementptr inbounds i32, i32* %45, i64 4
  store i32 %268, i32* %269, align 4
  %270 = add nsw i64 %257, 8192
  %271 = lshr i64 %270, 14
  %272 = trunc i64 %271 to i32
  %273 = getelementptr inbounds i32, i32* %45, i64 8
  store i32 %272, i32* %273, align 4
  %274 = add nsw i64 %261, 8192
  %275 = add nsw i64 %274, %260
  %276 = lshr i64 %275, 14
  %277 = trunc i64 %276 to i32
  %278 = getelementptr inbounds i32, i32* %45, i64 12
  store i32 %277, i32* %278, align 4
  %279 = sub nsw i64 %248, %247
  %280 = mul nsw i64 %279, 11585
  %281 = add nsw i64 %248, %247
  %282 = mul nsw i64 %281, 11585
  %283 = add nsw i64 %280, 8192
  %284 = ashr i64 %283, 14
  %285 = add nsw i64 %282, 8192
  %286 = ashr i64 %285, 14
  %287 = add nsw i64 %284, %246
  %288 = sub nsw i64 %246, %284
  %289 = sub nsw i64 %249, %286
  %290 = add nsw i64 %286, %249
  %291 = mul nsw i64 %287, 3196
  %292 = mul nsw i64 %290, 16069
  %293 = mul nsw i64 %288, 13623
  %294 = mul nsw i64 %289, 9102
  %295 = mul nsw i64 %289, 13623
  %296 = mul nsw i64 %288, -9102
  %297 = mul nsw i64 %290, 3196
  %298 = mul nsw i64 %287, -16069
  %299 = add nsw i64 %292, 8192
  %300 = add nsw i64 %299, %291
  %301 = lshr i64 %300, 14
  %302 = trunc i64 %301 to i32
  %303 = getelementptr inbounds i32, i32* %45, i64 2
  store i32 %302, i32* %303, align 4
  %304 = add nsw i64 %296, 8192
  %305 = add nsw i64 %304, %295
  %306 = lshr i64 %305, 14
  %307 = trunc i64 %306 to i32
  %308 = getelementptr inbounds i32, i32* %45, i64 6
  store i32 %307, i32* %308, align 4
  %309 = add nsw i64 %294, 8192
  %310 = add nsw i64 %309, %293
  %311 = lshr i64 %310, 14
  %312 = trunc i64 %311 to i32
  %313 = getelementptr inbounds i32, i32* %45, i64 10
  store i32 %312, i32* %313, align 4
  %314 = add nsw i64 %298, 8192
  %315 = add nsw i64 %314, %297
  %316 = lshr i64 %315, 14
  %317 = trunc i64 %316 to i32
  %318 = getelementptr inbounds i32, i32* %45, i64 14
  store i32 %317, i32* %318, align 4
  %319 = sub nsw i64 %231, %228
  %320 = mul nsw i64 %319, 11585
  %321 = sub nsw i64 %230, %229
  %322 = mul nsw i64 %321, 11585
  %323 = add nsw i64 %320, 8192
  %324 = ashr i64 %323, 14
  %325 = add nsw i64 %322, 8192
  %326 = ashr i64 %325, 14
  %327 = add nsw i64 %229, %230
  %328 = mul nsw i64 %327, 11585
  %329 = add nsw i64 %228, %231
  %330 = mul nsw i64 %329, 11585
  %331 = add nsw i64 %328, 8192
  %332 = ashr i64 %331, 14
  %333 = add nsw i64 %330, 8192
  %334 = ashr i64 %333, 14
  %335 = add nsw i64 %326, %226
  %336 = add nsw i64 %324, %227
  %337 = sub nsw i64 %227, %324
  %338 = sub nsw i64 %226, %326
  %339 = sub nsw i64 %233, %332
  %340 = sub nsw i64 %232, %334
  %341 = add nsw i64 %334, %232
  %342 = add nsw i64 %332, %233
  %343 = mul nsw i64 %336, -15137
  %344 = mul nsw i64 %341, 6270
  %345 = mul nsw i64 %337, 6270
  %346 = mul nsw i64 %340, 15137
  %347 = add nsw i64 %344, 8192
  %348 = add nsw i64 %347, %343
  %349 = ashr i64 %348, 14
  %350 = add nsw i64 %346, 8192
  %351 = add nsw i64 %350, %345
  %352 = ashr i64 %351, 14
  %353 = mul nsw i64 %337, 15137
  %354 = mul nsw i64 %340, -6270
  %355 = mul nsw i64 %336, 6270
  %356 = mul nsw i64 %341, 15137
  %357 = add nsw i64 %354, 8192
  %358 = add nsw i64 %357, %353
  %359 = ashr i64 %358, 14
  %360 = add nsw i64 %356, 8192
  %361 = add nsw i64 %360, %355
  %362 = ashr i64 %361, 14
  %363 = add nsw i64 %349, %335
  %364 = sub nsw i64 %335, %349
  %365 = add nsw i64 %352, %338
  %366 = sub nsw i64 %338, %352
  %367 = sub nsw i64 %339, %359
  %368 = add nsw i64 %359, %339
  %369 = sub nsw i64 %342, %362
  %370 = add nsw i64 %362, %342
  %371 = mul nsw i64 %363, 1606
  %372 = mul nsw i64 %370, 16305
  %373 = mul nsw i64 %364, 12665
  %374 = mul nsw i64 %369, 10394
  %375 = add nsw i64 %372, 8192
  %376 = add nsw i64 %375, %371
  %377 = lshr i64 %376, 14
  %378 = trunc i64 %377 to i32
  %379 = getelementptr inbounds i32, i32* %45, i64 1
  store i32 %378, i32* %379, align 4
  %380 = add nsw i64 %374, 8192
  %381 = add nsw i64 %380, %373
  %382 = lshr i64 %381, 14
  %383 = trunc i64 %382 to i32
  %384 = getelementptr inbounds i32, i32* %45, i64 9
  store i32 %383, i32* %384, align 4
  %385 = mul nsw i64 %365, 7723
  %386 = mul nsw i64 %368, 14449
  %387 = mul nsw i64 %366, 15679
  %388 = mul nsw i64 %367, 4756
  %389 = add nsw i64 %386, 8192
  %390 = add nsw i64 %389, %385
  %391 = lshr i64 %390, 14
  %392 = trunc i64 %391 to i32
  %393 = getelementptr inbounds i32, i32* %45, i64 5
  store i32 %392, i32* %393, align 4
  %394 = add nsw i64 %388, 8192
  %395 = add nsw i64 %394, %387
  %396 = lshr i64 %395, 14
  %397 = trunc i64 %396 to i32
  %398 = getelementptr inbounds i32, i32* %45, i64 13
  store i32 %397, i32* %398, align 4
  %399 = mul nsw i64 %366, -4756
  %400 = mul nsw i64 %367, 15679
  %401 = mul nsw i64 %365, -14449
  %402 = mul nsw i64 %368, 7723
  %403 = add nsw i64 %400, 8192
  %404 = add nsw i64 %403, %399
  %405 = lshr i64 %404, 14
  %406 = trunc i64 %405 to i32
  %407 = getelementptr inbounds i32, i32* %45, i64 3
  store i32 %406, i32* %407, align 4
  %408 = add nsw i64 %402, 8192
  %409 = add nsw i64 %408, %401
  %410 = lshr i64 %409, 14
  %411 = trunc i64 %410 to i32
  %412 = getelementptr inbounds i32, i32* %45, i64 11
  store i32 %411, i32* %412, align 4
  %413 = mul nsw i64 %364, -10394
  %414 = mul nsw i64 %369, 12665
  %415 = mul nsw i64 %363, -16305
  %416 = mul nsw i64 %370, 1606
  %417 = add nsw i64 %414, 8192
  %418 = add nsw i64 %417, %413
  %419 = lshr i64 %418, 14
  %420 = trunc i64 %419 to i32
  %421 = getelementptr inbounds i32, i32* %45, i64 7
  store i32 %420, i32* %421, align 4
  %422 = add nsw i64 %416, 8192
  %423 = add nsw i64 %422, %415
  %424 = lshr i64 %423, 14
  %425 = trunc i64 %424 to i32
  %426 = getelementptr inbounds i32, i32* %45, i64 15
  store i32 %425, i32* %426, align 4
  %427 = getelementptr inbounds i16, i16* %43, i64 1
  %428 = getelementptr inbounds i32, i32* %45, i64 16
  %429 = add nuw nsw i32 %46, 1
  %430 = icmp eq i32 %429, 16
  br i1 %430, label %431, label %42

431:                                              ; preds = %208
  %432 = add nuw nsw i32 %38, 1
  %433 = icmp eq i32 %432, 2
  br i1 %433, label %434, label %36

434:                                              ; preds = %431
  call void @llvm.lifetime.end.p0i8(i64 1024, i8* nonnull %5) #3
  ret void
}

; Function Attrs: nofree norecurse nounwind ssp uwtable
define hidden void @vpx_fdct16x16_1_c(i16* nocapture readonly, i32* nocapture, i32) local_unnamed_addr #2 {
  %4 = sext i32 %2 to i64
  %5 = icmp eq i32 %2, 1
  br i1 %5, label %6, label %113

6:                                                ; preds = %3, %6
  %7 = phi i64 [ %105, %6 ], [ 0, %3 ]
  %8 = phi <4 x i32> [ %104, %6 ], [ zeroinitializer, %3 ]
  %9 = mul nsw i64 %7, %4
  %10 = getelementptr inbounds i16, i16* %0, i64 %9
  %11 = bitcast i16* %10 to <4 x i16>*
  %12 = load <4 x i16>, <4 x i16>* %11, align 2
  %13 = sext <4 x i16> %12 to <4 x i32>
  %14 = add nsw <4 x i32> %8, %13
  %15 = or i64 %9, 1
  %16 = getelementptr inbounds i16, i16* %0, i64 %15
  %17 = bitcast i16* %16 to <4 x i16>*
  %18 = load <4 x i16>, <4 x i16>* %17, align 2
  %19 = sext <4 x i16> %18 to <4 x i32>
  %20 = add nsw <4 x i32> %14, %19
  %21 = or i64 %9, 2
  %22 = getelementptr inbounds i16, i16* %0, i64 %21
  %23 = bitcast i16* %22 to <4 x i16>*
  %24 = load <4 x i16>, <4 x i16>* %23, align 2
  %25 = sext <4 x i16> %24 to <4 x i32>
  %26 = add nsw <4 x i32> %20, %25
  %27 = or i64 %9, 3
  %28 = getelementptr inbounds i16, i16* %0, i64 %27
  %29 = bitcast i16* %28 to <4 x i16>*
  %30 = load <4 x i16>, <4 x i16>* %29, align 2
  %31 = sext <4 x i16> %30 to <4 x i32>
  %32 = add nsw <4 x i32> %26, %31
  %33 = add nsw i64 %9, 4
  %34 = getelementptr inbounds i16, i16* %0, i64 %33
  %35 = bitcast i16* %34 to <4 x i16>*
  %36 = load <4 x i16>, <4 x i16>* %35, align 2
  %37 = sext <4 x i16> %36 to <4 x i32>
  %38 = add nsw <4 x i32> %32, %37
  %39 = add nsw i64 %9, 5
  %40 = getelementptr inbounds i16, i16* %0, i64 %39
  %41 = bitcast i16* %40 to <4 x i16>*
  %42 = load <4 x i16>, <4 x i16>* %41, align 2
  %43 = sext <4 x i16> %42 to <4 x i32>
  %44 = add nsw <4 x i32> %38, %43
  %45 = add nsw i64 %9, 6
  %46 = getelementptr inbounds i16, i16* %0, i64 %45
  %47 = bitcast i16* %46 to <4 x i16>*
  %48 = load <4 x i16>, <4 x i16>* %47, align 2
  %49 = sext <4 x i16> %48 to <4 x i32>
  %50 = add nsw <4 x i32> %44, %49
  %51 = add nsw i64 %9, 7
  %52 = getelementptr inbounds i16, i16* %0, i64 %51
  %53 = bitcast i16* %52 to <4 x i16>*
  %54 = load <4 x i16>, <4 x i16>* %53, align 2
  %55 = sext <4 x i16> %54 to <4 x i32>
  %56 = add nsw <4 x i32> %50, %55
  %57 = add nsw i64 %9, 8
  %58 = getelementptr inbounds i16, i16* %0, i64 %57
  %59 = bitcast i16* %58 to <4 x i16>*
  %60 = load <4 x i16>, <4 x i16>* %59, align 2
  %61 = sext <4 x i16> %60 to <4 x i32>
  %62 = add nsw <4 x i32> %56, %61
  %63 = add nsw i64 %9, 9
  %64 = getelementptr inbounds i16, i16* %0, i64 %63
  %65 = bitcast i16* %64 to <4 x i16>*
  %66 = load <4 x i16>, <4 x i16>* %65, align 2
  %67 = sext <4 x i16> %66 to <4 x i32>
  %68 = add nsw <4 x i32> %62, %67
  %69 = add nsw i64 %9, 10
  %70 = getelementptr inbounds i16, i16* %0, i64 %69
  %71 = bitcast i16* %70 to <4 x i16>*
  %72 = load <4 x i16>, <4 x i16>* %71, align 2
  %73 = sext <4 x i16> %72 to <4 x i32>
  %74 = add nsw <4 x i32> %68, %73
  %75 = add nsw i64 %9, 11
  %76 = getelementptr inbounds i16, i16* %0, i64 %75
  %77 = bitcast i16* %76 to <4 x i16>*
  %78 = load <4 x i16>, <4 x i16>* %77, align 2
  %79 = sext <4 x i16> %78 to <4 x i32>
  %80 = add nsw <4 x i32> %74, %79
  %81 = add nsw i64 %9, 12
  %82 = getelementptr inbounds i16, i16* %0, i64 %81
  %83 = bitcast i16* %82 to <4 x i16>*
  %84 = load <4 x i16>, <4 x i16>* %83, align 2
  %85 = sext <4 x i16> %84 to <4 x i32>
  %86 = add nsw <4 x i32> %80, %85
  %87 = add nsw i64 %9, 13
  %88 = getelementptr inbounds i16, i16* %0, i64 %87
  %89 = bitcast i16* %88 to <4 x i16>*
  %90 = load <4 x i16>, <4 x i16>* %89, align 2
  %91 = sext <4 x i16> %90 to <4 x i32>
  %92 = add nsw <4 x i32> %86, %91
  %93 = add nsw i64 %9, 14
  %94 = getelementptr inbounds i16, i16* %0, i64 %93
  %95 = bitcast i16* %94 to <4 x i16>*
  %96 = load <4 x i16>, <4 x i16>* %95, align 2
  %97 = sext <4 x i16> %96 to <4 x i32>
  %98 = add nsw <4 x i32> %92, %97
  %99 = add nsw i64 %9, 15
  %100 = getelementptr inbounds i16, i16* %0, i64 %99
  %101 = bitcast i16* %100 to <4 x i16>*
  %102 = load <4 x i16>, <4 x i16>* %101, align 2
  %103 = sext <4 x i16> %102 to <4 x i32>
  %104 = add nsw <4 x i32> %98, %103
  %105 = add i64 %7, 4
  %106 = icmp eq i64 %105, 16
  br i1 %106, label %107, label %6, !llvm.loop !2

107:                                              ; preds = %6
  %108 = shufflevector <4 x i32> %104, <4 x i32> undef, <4 x i32> <i32 2, i32 3, i32 undef, i32 undef>
  %109 = add <4 x i32> %104, %108
  %110 = shufflevector <4 x i32> %109, <4 x i32> undef, <4 x i32> <i32 1, i32 undef, i32 undef, i32 undef>
  %111 = add <4 x i32> %109, %110
  %112 = extractelement <4 x i32> %111, i32 0
  br label %133

113:                                              ; preds = %3, %113
  %114 = phi i64 [ %131, %113 ], [ 0, %3 ]
  %115 = phi i32 [ %130, %113 ], [ 0, %3 ]
  %116 = mul nsw i64 %114, %4
  %117 = getelementptr inbounds i16, i16* %0, i64 %116
  %118 = bitcast i16* %117 to <16 x i16>*
  %119 = load <16 x i16>, <16 x i16>* %118, align 2
  %120 = sext <16 x i16> %119 to <16 x i32>
  %121 = shufflevector <16 x i32> %120, <16 x i32> undef, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %122 = add nsw <16 x i32> %121, %120
  %123 = shufflevector <16 x i32> %122, <16 x i32> undef, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %124 = add nsw <16 x i32> %122, %123
  %125 = shufflevector <16 x i32> %124, <16 x i32> undef, <16 x i32> <i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %126 = add nsw <16 x i32> %124, %125
  %127 = shufflevector <16 x i32> %126, <16 x i32> undef, <16 x i32> <i32 1, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %128 = add nsw <16 x i32> %126, %127
  %129 = extractelement <16 x i32> %128, i32 0
  %130 = add nsw i32 %129, %115
  %131 = add nuw nsw i64 %114, 1
  %132 = icmp eq i64 %131, 16
  br i1 %132, label %133, label %113, !llvm.loop !4

133:                                              ; preds = %113, %107
  %134 = phi i32 [ %112, %107 ], [ %130, %113 ]
  %135 = ashr i32 %134, 1
  store i32 %135, i32* %1, align 4
  ret void
}

; Function Attrs: nofree norecurse nounwind ssp uwtable
define hidden void @vpx_fdct32(i64* readonly, i64*, i32) local_unnamed_addr #2 {
  %4 = load i64, i64* %0, align 8
  %5 = getelementptr inbounds i64, i64* %0, i64 31
  %6 = load i64, i64* %5, align 8
  %7 = add nsw i64 %6, %4
  %8 = getelementptr inbounds i64, i64* %0, i64 1
  %9 = load i64, i64* %8, align 8
  %10 = getelementptr inbounds i64, i64* %0, i64 30
  %11 = load i64, i64* %10, align 8
  %12 = add nsw i64 %11, %9
  %13 = getelementptr inbounds i64, i64* %0, i64 2
  %14 = load i64, i64* %13, align 8
  %15 = getelementptr inbounds i64, i64* %0, i64 29
  %16 = load i64, i64* %15, align 8
  %17 = add nsw i64 %16, %14
  %18 = getelementptr inbounds i64, i64* %0, i64 3
  %19 = load i64, i64* %18, align 8
  %20 = getelementptr inbounds i64, i64* %0, i64 28
  %21 = load i64, i64* %20, align 8
  %22 = add nsw i64 %21, %19
  %23 = getelementptr inbounds i64, i64* %0, i64 4
  %24 = load i64, i64* %23, align 8
  %25 = getelementptr inbounds i64, i64* %0, i64 27
  %26 = load i64, i64* %25, align 8
  %27 = add nsw i64 %26, %24
  %28 = getelementptr inbounds i64, i64* %0, i64 5
  %29 = load i64, i64* %28, align 8
  %30 = getelementptr inbounds i64, i64* %0, i64 26
  %31 = load i64, i64* %30, align 8
  %32 = add nsw i64 %31, %29
  %33 = getelementptr inbounds i64, i64* %0, i64 6
  %34 = load i64, i64* %33, align 8
  %35 = getelementptr inbounds i64, i64* %0, i64 25
  %36 = load i64, i64* %35, align 8
  %37 = add nsw i64 %36, %34
  %38 = getelementptr inbounds i64, i64* %0, i64 7
  %39 = load i64, i64* %38, align 8
  %40 = getelementptr inbounds i64, i64* %0, i64 24
  %41 = load i64, i64* %40, align 8
  %42 = add nsw i64 %41, %39
  %43 = getelementptr inbounds i64, i64* %0, i64 8
  %44 = load i64, i64* %43, align 8
  %45 = getelementptr inbounds i64, i64* %0, i64 23
  %46 = load i64, i64* %45, align 8
  %47 = add nsw i64 %46, %44
  %48 = getelementptr inbounds i64, i64* %0, i64 9
  %49 = load i64, i64* %48, align 8
  %50 = getelementptr inbounds i64, i64* %0, i64 22
  %51 = load i64, i64* %50, align 8
  %52 = add nsw i64 %51, %49
  %53 = getelementptr inbounds i64, i64* %0, i64 10
  %54 = load i64, i64* %53, align 8
  %55 = getelementptr inbounds i64, i64* %0, i64 21
  %56 = load i64, i64* %55, align 8
  %57 = add nsw i64 %56, %54
  %58 = getelementptr inbounds i64, i64* %0, i64 11
  %59 = load i64, i64* %58, align 8
  %60 = getelementptr inbounds i64, i64* %0, i64 20
  %61 = load i64, i64* %60, align 8
  %62 = add nsw i64 %61, %59
  %63 = getelementptr inbounds i64, i64* %0, i64 12
  %64 = load i64, i64* %63, align 8
  %65 = getelementptr inbounds i64, i64* %0, i64 19
  %66 = load i64, i64* %65, align 8
  %67 = add nsw i64 %66, %64
  %68 = getelementptr inbounds i64, i64* %0, i64 13
  %69 = load i64, i64* %68, align 8
  %70 = getelementptr inbounds i64, i64* %0, i64 18
  %71 = load i64, i64* %70, align 8
  %72 = add nsw i64 %71, %69
  %73 = getelementptr inbounds i64, i64* %0, i64 14
  %74 = load i64, i64* %73, align 8
  %75 = getelementptr inbounds i64, i64* %0, i64 17
  %76 = load i64, i64* %75, align 8
  %77 = add nsw i64 %76, %74
  %78 = getelementptr inbounds i64, i64* %0, i64 15
  %79 = load i64, i64* %78, align 8
  %80 = getelementptr inbounds i64, i64* %0, i64 16
  %81 = load i64, i64* %80, align 8
  %82 = add nsw i64 %81, %79
  %83 = sub i64 %79, %81
  %84 = sub i64 %74, %76
  %85 = sub i64 %69, %71
  %86 = sub i64 %64, %66
  %87 = sub i64 %59, %61
  %88 = sub i64 %54, %56
  %89 = sub i64 %49, %51
  %90 = sub i64 %44, %46
  %91 = sub i64 %39, %41
  %92 = sub i64 %34, %36
  %93 = sub i64 %29, %31
  %94 = sub i64 %24, %26
  %95 = sub i64 %19, %21
  %96 = sub i64 %14, %16
  %97 = sub i64 %9, %11
  %98 = sub i64 %4, %6
  %99 = add nsw i64 %82, %7
  store i64 %99, i64* %1, align 8
  %100 = add nsw i64 %77, %12
  %101 = getelementptr inbounds i64, i64* %1, i64 1
  store i64 %100, i64* %101, align 8
  %102 = add nsw i64 %72, %17
  %103 = getelementptr inbounds i64, i64* %1, i64 2
  store i64 %102, i64* %103, align 8
  %104 = add nsw i64 %67, %22
  %105 = getelementptr inbounds i64, i64* %1, i64 3
  store i64 %104, i64* %105, align 8
  %106 = add nsw i64 %62, %27
  %107 = getelementptr inbounds i64, i64* %1, i64 4
  store i64 %106, i64* %107, align 8
  %108 = add nsw i64 %57, %32
  %109 = getelementptr inbounds i64, i64* %1, i64 5
  store i64 %108, i64* %109, align 8
  %110 = add nsw i64 %52, %37
  %111 = getelementptr inbounds i64, i64* %1, i64 6
  store i64 %110, i64* %111, align 8
  %112 = add nsw i64 %47, %42
  %113 = getelementptr inbounds i64, i64* %1, i64 7
  store i64 %112, i64* %113, align 8
  %114 = sub i64 %42, %47
  %115 = getelementptr inbounds i64, i64* %1, i64 8
  store i64 %114, i64* %115, align 8
  %116 = sub i64 %37, %52
  %117 = getelementptr inbounds i64, i64* %1, i64 9
  store i64 %116, i64* %117, align 8
  %118 = sub i64 %32, %57
  %119 = getelementptr inbounds i64, i64* %1, i64 10
  store i64 %118, i64* %119, align 8
  %120 = sub i64 %27, %62
  %121 = getelementptr inbounds i64, i64* %1, i64 11
  store i64 %120, i64* %121, align 8
  %122 = sub i64 %22, %67
  %123 = getelementptr inbounds i64, i64* %1, i64 12
  store i64 %122, i64* %123, align 8
  %124 = sub i64 %17, %72
  %125 = getelementptr inbounds i64, i64* %1, i64 13
  store i64 %124, i64* %125, align 8
  %126 = sub i64 %12, %77
  %127 = getelementptr inbounds i64, i64* %1, i64 14
  store i64 %126, i64* %127, align 8
  %128 = sub i64 %7, %82
  %129 = getelementptr inbounds i64, i64* %1, i64 15
  store i64 %128, i64* %129, align 8
  %130 = getelementptr inbounds i64, i64* %1, i64 16
  store i64 %83, i64* %130, align 8
  %131 = getelementptr inbounds i64, i64* %1, i64 17
  store i64 %84, i64* %131, align 8
  %132 = getelementptr inbounds i64, i64* %1, i64 18
  store i64 %85, i64* %132, align 8
  %133 = getelementptr inbounds i64, i64* %1, i64 19
  store i64 %86, i64* %133, align 8
  %134 = sub i64 %94, %87
  %135 = mul nsw i64 %134, 11585
  %136 = add nsw i64 %135, 8192
  %137 = ashr i64 %136, 14
  %138 = getelementptr inbounds i64, i64* %1, i64 20
  store i64 %137, i64* %138, align 8
  %139 = sub i64 %93, %88
  %140 = mul nsw i64 %139, 11585
  %141 = add nsw i64 %140, 8192
  %142 = ashr i64 %141, 14
  %143 = getelementptr inbounds i64, i64* %1, i64 21
  store i64 %142, i64* %143, align 8
  %144 = sub i64 %92, %89
  %145 = mul nsw i64 %144, 11585
  %146 = add nsw i64 %145, 8192
  %147 = ashr i64 %146, 14
  %148 = getelementptr inbounds i64, i64* %1, i64 22
  store i64 %147, i64* %148, align 8
  %149 = sub i64 %91, %90
  %150 = mul nsw i64 %149, 11585
  %151 = add nsw i64 %150, 8192
  %152 = ashr i64 %151, 14
  %153 = getelementptr inbounds i64, i64* %1, i64 23
  store i64 %152, i64* %153, align 8
  %154 = add nsw i64 %90, %91
  %155 = mul nsw i64 %154, 11585
  %156 = add nsw i64 %155, 8192
  %157 = ashr i64 %156, 14
  %158 = getelementptr inbounds i64, i64* %1, i64 24
  store i64 %157, i64* %158, align 8
  %159 = add nsw i64 %89, %92
  %160 = mul nsw i64 %159, 11585
  %161 = add nsw i64 %160, 8192
  %162 = ashr i64 %161, 14
  %163 = getelementptr inbounds i64, i64* %1, i64 25
  store i64 %162, i64* %163, align 8
  %164 = add nsw i64 %88, %93
  %165 = mul nsw i64 %164, 11585
  %166 = add nsw i64 %165, 8192
  %167 = ashr i64 %166, 14
  %168 = getelementptr inbounds i64, i64* %1, i64 26
  store i64 %167, i64* %168, align 8
  %169 = add nsw i64 %87, %94
  %170 = mul nsw i64 %169, 11585
  %171 = add nsw i64 %170, 8192
  %172 = ashr i64 %171, 14
  %173 = getelementptr inbounds i64, i64* %1, i64 27
  store i64 %172, i64* %173, align 8
  %174 = getelementptr inbounds i64, i64* %1, i64 28
  store i64 %95, i64* %174, align 8
  %175 = getelementptr inbounds i64, i64* %1, i64 29
  store i64 %96, i64* %175, align 8
  %176 = getelementptr inbounds i64, i64* %1, i64 30
  store i64 %97, i64* %176, align 8
  %177 = getelementptr inbounds i64, i64* %1, i64 31
  store i64 %98, i64* %177, align 8
  %178 = icmp eq i32 %2, 0
  br i1 %178, label %316, label %179

179:                                              ; preds = %3
  %180 = add nsw i64 %99, 1
  %181 = lshr i64 %99, 63
  %182 = add nsw i64 %180, %181
  %183 = ashr i64 %182, 2
  store i64 %183, i64* %1, align 8
  %184 = add nsw i64 %100, 1
  %185 = lshr i64 %100, 63
  %186 = add nsw i64 %184, %185
  %187 = ashr i64 %186, 2
  store i64 %187, i64* %101, align 8
  %188 = add nsw i64 %102, 1
  %189 = lshr i64 %102, 63
  %190 = add nsw i64 %188, %189
  %191 = ashr i64 %190, 2
  store i64 %191, i64* %103, align 8
  %192 = add nsw i64 %104, 1
  %193 = lshr i64 %104, 63
  %194 = add nsw i64 %192, %193
  %195 = ashr i64 %194, 2
  store i64 %195, i64* %105, align 8
  %196 = add nsw i64 %106, 1
  %197 = lshr i64 %106, 63
  %198 = add nsw i64 %196, %197
  %199 = ashr i64 %198, 2
  store i64 %199, i64* %107, align 8
  %200 = add nsw i64 %108, 1
  %201 = lshr i64 %108, 63
  %202 = add nsw i64 %200, %201
  %203 = ashr i64 %202, 2
  store i64 %203, i64* %109, align 8
  %204 = add nsw i64 %110, 1
  %205 = lshr i64 %110, 63
  %206 = add nsw i64 %204, %205
  %207 = ashr i64 %206, 2
  store i64 %207, i64* %111, align 8
  %208 = add nsw i64 %112, 1
  %209 = lshr i64 %112, 63
  %210 = add nsw i64 %208, %209
  %211 = ashr i64 %210, 2
  store i64 %211, i64* %113, align 8
  %212 = add nsw i64 %114, 1
  %213 = lshr i64 %114, 63
  %214 = add nsw i64 %212, %213
  %215 = ashr i64 %214, 2
  store i64 %215, i64* %115, align 8
  %216 = add nsw i64 %116, 1
  %217 = lshr i64 %116, 63
  %218 = add nsw i64 %216, %217
  %219 = ashr i64 %218, 2
  store i64 %219, i64* %117, align 8
  %220 = add nsw i64 %118, 1
  %221 = lshr i64 %118, 63
  %222 = add nsw i64 %220, %221
  %223 = ashr i64 %222, 2
  store i64 %223, i64* %119, align 8
  %224 = add nsw i64 %120, 1
  %225 = lshr i64 %120, 63
  %226 = add nsw i64 %224, %225
  %227 = ashr i64 %226, 2
  store i64 %227, i64* %121, align 8
  %228 = add nsw i64 %122, 1
  %229 = lshr i64 %122, 63
  %230 = add nsw i64 %228, %229
  %231 = ashr i64 %230, 2
  store i64 %231, i64* %123, align 8
  %232 = add nsw i64 %124, 1
  %233 = lshr i64 %124, 63
  %234 = add nsw i64 %232, %233
  %235 = ashr i64 %234, 2
  store i64 %235, i64* %125, align 8
  %236 = add nsw i64 %126, 1
  %237 = lshr i64 %126, 63
  %238 = add nsw i64 %236, %237
  %239 = ashr i64 %238, 2
  store i64 %239, i64* %127, align 8
  %240 = add nsw i64 %128, 1
  %241 = lshr i64 %128, 63
  %242 = add nsw i64 %240, %241
  %243 = ashr i64 %242, 2
  store i64 %243, i64* %129, align 8
  %244 = add nsw i64 %83, 1
  %245 = lshr i64 %83, 63
  %246 = add nsw i64 %244, %245
  %247 = ashr i64 %246, 2
  store i64 %247, i64* %130, align 8
  %248 = add nsw i64 %84, 1
  %249 = lshr i64 %84, 63
  %250 = add nsw i64 %248, %249
  %251 = ashr i64 %250, 2
  store i64 %251, i64* %131, align 8
  %252 = add nsw i64 %85, 1
  %253 = lshr i64 %85, 63
  %254 = add nsw i64 %252, %253
  %255 = ashr i64 %254, 2
  store i64 %255, i64* %132, align 8
  %256 = add nsw i64 %86, 1
  %257 = lshr i64 %86, 63
  %258 = add nsw i64 %256, %257
  %259 = ashr i64 %258, 2
  store i64 %259, i64* %133, align 8
  %260 = add nsw i64 %137, 1
  %261 = lshr i64 %137, 63
  %262 = add nsw i64 %260, %261
  %263 = ashr i64 %262, 2
  store i64 %263, i64* %138, align 8
  %264 = add nsw i64 %142, 1
  %265 = lshr i64 %142, 63
  %266 = add nsw i64 %264, %265
  %267 = ashr i64 %266, 2
  store i64 %267, i64* %143, align 8
  %268 = add nsw i64 %147, 1
  %269 = lshr i64 %147, 63
  %270 = add nsw i64 %268, %269
  %271 = ashr i64 %270, 2
  store i64 %271, i64* %148, align 8
  %272 = add nsw i64 %152, 1
  %273 = lshr i64 %152, 63
  %274 = add nsw i64 %272, %273
  %275 = ashr i64 %274, 2
  store i64 %275, i64* %153, align 8
  %276 = add nsw i64 %157, 1
  %277 = lshr i64 %157, 63
  %278 = add nsw i64 %276, %277
  %279 = ashr i64 %278, 2
  store i64 %279, i64* %158, align 8
  %280 = add nsw i64 %162, 1
  %281 = lshr i64 %162, 63
  %282 = add nsw i64 %280, %281
  %283 = ashr i64 %282, 2
  store i64 %283, i64* %163, align 8
  %284 = add nsw i64 %167, 1
  %285 = lshr i64 %167, 63
  %286 = add nsw i64 %284, %285
  %287 = ashr i64 %286, 2
  store i64 %287, i64* %168, align 8
  %288 = add nsw i64 %172, 1
  %289 = lshr i64 %172, 63
  %290 = add nsw i64 %288, %289
  %291 = ashr i64 %290, 2
  store i64 %291, i64* %173, align 8
  %292 = add nsw i64 %95, 1
  %293 = lshr i64 %95, 63
  %294 = add nsw i64 %292, %293
  %295 = ashr i64 %294, 2
  store i64 %295, i64* %174, align 8
  %296 = add nsw i64 %96, 1
  %297 = lshr i64 %96, 63
  %298 = add nsw i64 %296, %297
  %299 = ashr i64 %298, 2
  store i64 %299, i64* %175, align 8
  %300 = add nsw i64 %97, 1
  %301 = lshr i64 %97, 63
  %302 = add nsw i64 %300, %301
  %303 = ashr i64 %302, 2
  store i64 %303, i64* %176, align 8
  %304 = add nsw i64 %98, 1
  %305 = lshr i64 %98, 63
  %306 = add nsw i64 %304, %305
  %307 = ashr i64 %306, 2
  store i64 %307, i64* %177, align 8
  %308 = load i64, i64* %113, align 8
  %309 = load i64, i64* %111, align 8
  %310 = load i64, i64* %109, align 8
  %311 = load i64, i64* %115, align 8
  %312 = load i64, i64* %117, align 8
  %313 = load i64, i64* %119, align 8
  %314 = load i64, i64* %121, align 8
  %315 = load i64, i64* %123, align 8
  br label %316

316:                                              ; preds = %3, %179
  %317 = phi i64 [ %95, %3 ], [ %295, %179 ]
  %318 = phi i64 [ %172, %3 ], [ %291, %179 ]
  %319 = phi i64 [ %96, %3 ], [ %299, %179 ]
  %320 = phi i64 [ %167, %3 ], [ %287, %179 ]
  %321 = phi i64 [ %97, %3 ], [ %303, %179 ]
  %322 = phi i64 [ %162, %3 ], [ %283, %179 ]
  %323 = phi i64 [ %98, %3 ], [ %307, %179 ]
  %324 = phi i64 [ %157, %3 ], [ %279, %179 ]
  %325 = phi i64 [ %137, %3 ], [ %263, %179 ]
  %326 = phi i64 [ %86, %3 ], [ %259, %179 ]
  %327 = phi i64 [ %142, %3 ], [ %267, %179 ]
  %328 = phi i64 [ %85, %3 ], [ %255, %179 ]
  %329 = phi i64 [ %147, %3 ], [ %271, %179 ]
  %330 = phi i64 [ %84, %3 ], [ %251, %179 ]
  %331 = phi i64 [ %152, %3 ], [ %275, %179 ]
  %332 = phi i64 [ %83, %3 ], [ %247, %179 ]
  %333 = phi i64 [ %128, %3 ], [ %243, %179 ]
  %334 = phi i64 [ %126, %3 ], [ %239, %179 ]
  %335 = phi i64 [ %122, %3 ], [ %315, %179 ]
  %336 = phi i64 [ %120, %3 ], [ %314, %179 ]
  %337 = phi i64 [ %124, %3 ], [ %235, %179 ]
  %338 = phi i64 [ %118, %3 ], [ %313, %179 ]
  %339 = phi i64 [ %116, %3 ], [ %312, %179 ]
  %340 = phi i64 [ %114, %3 ], [ %311, %179 ]
  %341 = phi i64 [ %108, %3 ], [ %310, %179 ]
  %342 = phi i64 [ %110, %3 ], [ %309, %179 ]
  %343 = phi i64 [ %112, %3 ], [ %308, %179 ]
  %344 = load i64, i64* %1, align 8
  %345 = add nsw i64 %343, %344
  %346 = load i64, i64* %101, align 8
  %347 = add nsw i64 %342, %346
  %348 = load i64, i64* %103, align 8
  %349 = add nsw i64 %341, %348
  %350 = load i64, i64* %105, align 8
  %351 = load i64, i64* %107, align 8
  %352 = add nsw i64 %351, %350
  %353 = sub i64 %350, %351
  %354 = sub i64 %348, %341
  %355 = sub i64 %346, %342
  %356 = sub i64 %344, %343
  %357 = sub i64 %337, %338
  %358 = mul nsw i64 %357, 11585
  %359 = add nsw i64 %358, 8192
  %360 = ashr i64 %359, 14
  %361 = sub i64 %335, %336
  %362 = mul nsw i64 %361, 11585
  %363 = add nsw i64 %362, 8192
  %364 = ashr i64 %363, 14
  %365 = add nsw i64 %335, %336
  %366 = mul nsw i64 %365, 11585
  %367 = add nsw i64 %366, 8192
  %368 = ashr i64 %367, 14
  %369 = add nsw i64 %337, %338
  %370 = mul nsw i64 %369, 11585
  %371 = add nsw i64 %370, 8192
  %372 = ashr i64 %371, 14
  %373 = add nsw i64 %331, %332
  %374 = add nsw i64 %329, %330
  %375 = add nsw i64 %327, %328
  %376 = add nsw i64 %325, %326
  %377 = sub i64 %326, %325
  %378 = sub i64 %328, %327
  %379 = sub i64 %330, %329
  %380 = sub i64 %332, %331
  %381 = sub i64 %323, %324
  %382 = sub i64 %321, %322
  %383 = sub i64 %319, %320
  %384 = sub i64 %317, %318
  %385 = add nsw i64 %317, %318
  %386 = add nsw i64 %319, %320
  %387 = add nsw i64 %321, %322
  %388 = add nsw i64 %323, %324
  %389 = add nsw i64 %352, %345
  store i64 %389, i64* %1, align 8
  %390 = add nsw i64 %349, %347
  store i64 %390, i64* %101, align 8
  %391 = sub i64 %347, %349
  store i64 %391, i64* %103, align 8
  %392 = sub i64 %345, %352
  store i64 %392, i64* %105, align 8
  store i64 %353, i64* %107, align 8
  %393 = sub i64 %355, %354
  %394 = mul nsw i64 %393, 11585
  %395 = add nsw i64 %394, 8192
  %396 = ashr i64 %395, 14
  store i64 %396, i64* %109, align 8
  %397 = add nsw i64 %354, %355
  %398 = mul nsw i64 %397, 11585
  %399 = add nsw i64 %398, 8192
  %400 = ashr i64 %399, 14
  store i64 %400, i64* %111, align 8
  store i64 %356, i64* %113, align 8
  %401 = add nsw i64 %364, %340
  store i64 %401, i64* %115, align 8
  %402 = add nsw i64 %360, %339
  store i64 %402, i64* %117, align 8
  %403 = sub i64 %339, %360
  store i64 %403, i64* %119, align 8
  %404 = sub i64 %340, %364
  store i64 %404, i64* %121, align 8
  %405 = sub i64 %333, %368
  store i64 %405, i64* %123, align 8
  %406 = sub i64 %334, %372
  store i64 %406, i64* %125, align 8
  %407 = add nsw i64 %372, %334
  store i64 %407, i64* %127, align 8
  %408 = add nsw i64 %368, %333
  store i64 %408, i64* %129, align 8
  store i64 %373, i64* %130, align 8
  store i64 %374, i64* %131, align 8
  %409 = mul nsw i64 %375, -15137
  %410 = mul nsw i64 %386, 6270
  %411 = add i64 %409, 8192
  %412 = add i64 %411, %410
  %413 = ashr i64 %412, 14
  store i64 %413, i64* %132, align 8
  %414 = mul nsw i64 %376, -15137
  %415 = mul nsw i64 %385, 6270
  %416 = add i64 %414, 8192
  %417 = add i64 %416, %415
  %418 = ashr i64 %417, 14
  store i64 %418, i64* %133, align 8
  %419 = mul nsw i64 %377, -6270
  %420 = mul nsw i64 %384, -15137
  %421 = add i64 %419, 8192
  %422 = add i64 %421, %420
  %423 = ashr i64 %422, 14
  store i64 %423, i64* %138, align 8
  %424 = mul nsw i64 %378, -6270
  %425 = mul nsw i64 %383, -15137
  %426 = add i64 %424, 8192
  %427 = add i64 %426, %425
  %428 = ashr i64 %427, 14
  store i64 %428, i64* %143, align 8
  store i64 %379, i64* %148, align 8
  store i64 %380, i64* %153, align 8
  store i64 %381, i64* %158, align 8
  store i64 %382, i64* %163, align 8
  %429 = mul nsw i64 %383, 6270
  %430 = mul nsw i64 %378, -15137
  %431 = add i64 %430, 8192
  %432 = add i64 %431, %429
  %433 = ashr i64 %432, 14
  store i64 %433, i64* %168, align 8
  %434 = mul nsw i64 %384, 6270
  %435 = mul nsw i64 %377, -15137
  %436 = add i64 %435, 8192
  %437 = add i64 %436, %434
  %438 = ashr i64 %437, 14
  store i64 %438, i64* %173, align 8
  %439 = mul nsw i64 %385, 15137
  %440 = mul nsw i64 %376, 6270
  %441 = add i64 %440, 8192
  %442 = add i64 %441, %439
  %443 = ashr i64 %442, 14
  store i64 %443, i64* %174, align 8
  %444 = mul nsw i64 %386, 15137
  %445 = mul nsw i64 %375, 6270
  %446 = add i64 %445, 8192
  %447 = add i64 %446, %444
  %448 = ashr i64 %447, 14
  store i64 %448, i64* %175, align 8
  store i64 %387, i64* %176, align 8
  store i64 %388, i64* %177, align 8
  %449 = add nsw i64 %389, %390
  %450 = mul nsw i64 %449, 11585
  %451 = add nsw i64 %450, 8192
  %452 = ashr i64 %451, 14
  %453 = sub i64 %389, %390
  %454 = mul nsw i64 %453, 11585
  %455 = add nsw i64 %454, 8192
  %456 = ashr i64 %455, 14
  %457 = mul nsw i64 %391, 6270
  %458 = mul nsw i64 %392, 15137
  %459 = add i64 %457, 8192
  %460 = add i64 %459, %458
  %461 = ashr i64 %460, 14
  %462 = mul nsw i64 %392, 6270
  %463 = mul i64 %391, -15137
  %464 = add i64 %463, 8192
  %465 = add i64 %464, %462
  %466 = ashr i64 %465, 14
  %467 = add nsw i64 %396, %353
  %468 = sub i64 %353, %396
  %469 = sub i64 %356, %400
  %470 = add nsw i64 %400, %356
  %471 = mul nsw i64 %402, -15137
  %472 = mul nsw i64 %407, 6270
  %473 = add i64 %472, 8192
  %474 = add i64 %473, %471
  %475 = ashr i64 %474, 14
  %476 = mul nsw i64 %403, -6270
  %477 = mul nsw i64 %406, -15137
  %478 = add i64 %477, 8192
  %479 = add i64 %478, %476
  %480 = ashr i64 %479, 14
  %481 = mul nsw i64 %406, 6270
  %482 = mul nsw i64 %403, -15137
  %483 = add i64 %482, 8192
  %484 = add i64 %483, %481
  %485 = ashr i64 %484, 14
  %486 = mul nsw i64 %407, 15137
  %487 = mul nsw i64 %402, 6270
  %488 = add i64 %487, 8192
  %489 = add i64 %488, %486
  %490 = ashr i64 %489, 14
  %491 = add nsw i64 %418, %373
  %492 = add nsw i64 %413, %374
  %493 = sub i64 %374, %413
  %494 = sub i64 %373, %418
  %495 = sub i64 %380, %423
  %496 = sub i64 %379, %428
  %497 = add nsw i64 %428, %379
  %498 = add nsw i64 %423, %380
  %499 = add nsw i64 %438, %381
  %500 = add nsw i64 %433, %382
  %501 = sub i64 %382, %433
  %502 = sub i64 %381, %438
  %503 = sub i64 %388, %443
  %504 = sub i64 %387, %448
  %505 = add nsw i64 %448, %387
  %506 = add nsw i64 %443, %388
  store i64 %452, i64* %1, align 8
  store i64 %456, i64* %101, align 8
  store i64 %461, i64* %103, align 8
  store i64 %466, i64* %105, align 8
  %507 = mul nsw i64 %467, 3196
  %508 = mul nsw i64 %470, 16069
  %509 = add i64 %508, 8192
  %510 = add i64 %509, %507
  %511 = ashr i64 %510, 14
  store i64 %511, i64* %107, align 8
  %512 = mul nsw i64 %468, 13623
  %513 = mul nsw i64 %469, 9102
  %514 = add i64 %513, 8192
  %515 = add i64 %514, %512
  %516 = ashr i64 %515, 14
  store i64 %516, i64* %109, align 8
  %517 = mul nsw i64 %469, 13623
  %518 = mul nsw i64 %468, -9102
  %519 = add i64 %518, 8192
  %520 = add i64 %519, %517
  %521 = ashr i64 %520, 14
  store i64 %521, i64* %111, align 8
  %522 = mul nsw i64 %470, 3196
  %523 = mul nsw i64 %467, -16069
  %524 = add i64 %523, 8192
  %525 = add i64 %524, %522
  %526 = ashr i64 %525, 14
  store i64 %526, i64* %113, align 8
  %527 = add nsw i64 %475, %401
  store i64 %527, i64* %115, align 8
  %528 = sub i64 %401, %475
  store i64 %528, i64* %117, align 8
  %529 = sub i64 %404, %480
  store i64 %529, i64* %119, align 8
  %530 = add nsw i64 %480, %404
  store i64 %530, i64* %121, align 8
  %531 = add nsw i64 %485, %405
  store i64 %531, i64* %123, align 8
  %532 = sub i64 %405, %485
  store i64 %532, i64* %125, align 8
  %533 = sub i64 %408, %490
  store i64 %533, i64* %127, align 8
  %534 = add nsw i64 %490, %408
  store i64 %534, i64* %129, align 8
  store i64 %491, i64* %130, align 8
  %535 = mul nsw i64 %492, -16069
  %536 = mul nsw i64 %505, 3196
  %537 = add i64 %536, 8192
  %538 = add i64 %537, %535
  %539 = ashr i64 %538, 14
  store i64 %539, i64* %131, align 8
  %540 = mul nsw i64 %493, -3196
  %541 = mul nsw i64 %504, -16069
  %542 = add i64 %541, 8192
  %543 = add i64 %542, %540
  %544 = ashr i64 %543, 14
  store i64 %544, i64* %132, align 8
  store i64 %494, i64* %133, align 8
  store i64 %495, i64* %138, align 8
  %545 = mul nsw i64 %496, -9102
  %546 = mul nsw i64 %501, 13623
  %547 = add i64 %546, 8192
  %548 = add i64 %547, %545
  %549 = ashr i64 %548, 14
  store i64 %549, i64* %143, align 8
  %550 = mul nsw i64 %497, -13623
  %551 = mul nsw i64 %500, -9102
  %552 = add i64 %551, 8192
  %553 = add i64 %552, %550
  %554 = ashr i64 %553, 14
  store i64 %498, i64* %153, align 8
  %555 = mul nsw i64 %500, 13623
  %556 = mul nsw i64 %497, -9102
  %557 = add i64 %556, 8192
  %558 = add i64 %557, %555
  %559 = ashr i64 %558, 14
  store i64 %559, i64* %163, align 8
  %560 = mul nsw i64 %501, 9102
  %561 = mul nsw i64 %496, 13623
  %562 = add i64 %561, 8192
  %563 = add i64 %562, %560
  %564 = ashr i64 %563, 14
  store i64 %502, i64* %173, align 8
  %565 = mul nsw i64 %504, 3196
  %566 = mul nsw i64 %493, -16069
  %567 = add i64 %566, 8192
  %568 = add i64 %567, %565
  %569 = ashr i64 %568, 14
  store i64 %569, i64* %175, align 8
  %570 = mul nsw i64 %505, 16069
  %571 = mul nsw i64 %492, 3196
  %572 = add i64 %571, 8192
  %573 = add i64 %572, %570
  %574 = ashr i64 %573, 14
  store i64 %506, i64* %177, align 8
  %575 = mul nsw i64 %527, 1606
  %576 = mul nsw i64 %534, 16305
  %577 = add i64 %576, 8192
  %578 = add i64 %577, %575
  %579 = ashr i64 %578, 14
  %580 = mul nsw i64 %528, 12665
  %581 = mul nsw i64 %533, 10394
  %582 = add i64 %581, 8192
  %583 = add i64 %582, %580
  %584 = ashr i64 %583, 14
  %585 = mul nsw i64 %529, 7723
  %586 = mul nsw i64 %532, 14449
  %587 = add i64 %586, 8192
  %588 = add i64 %587, %585
  %589 = ashr i64 %588, 14
  %590 = mul nsw i64 %530, 15679
  %591 = mul nsw i64 %531, 4756
  %592 = add i64 %591, 8192
  %593 = add i64 %592, %590
  %594 = ashr i64 %593, 14
  %595 = mul nsw i64 %531, 15679
  %596 = mul nsw i64 %530, -4756
  %597 = add i64 %596, 8192
  %598 = add i64 %597, %595
  %599 = ashr i64 %598, 14
  %600 = mul nsw i64 %532, 7723
  %601 = mul nsw i64 %529, -14449
  %602 = add i64 %601, 8192
  %603 = add i64 %602, %600
  %604 = ashr i64 %603, 14
  %605 = mul nsw i64 %533, 12665
  %606 = mul nsw i64 %528, -10394
  %607 = add i64 %606, 8192
  %608 = add i64 %607, %605
  %609 = ashr i64 %608, 14
  %610 = mul nsw i64 %534, 1606
  %611 = mul nsw i64 %527, -16305
  %612 = add i64 %611, 8192
  %613 = add i64 %612, %610
  %614 = ashr i64 %613, 14
  %615 = add nsw i64 %539, %491
  %616 = sub i64 %491, %539
  %617 = sub i64 %494, %544
  %618 = add nsw i64 %544, %494
  %619 = add nsw i64 %549, %495
  %620 = sub i64 %495, %549
  %621 = sub i64 %498, %554
  %622 = add nsw i64 %554, %498
  %623 = add nsw i64 %559, %499
  %624 = sub i64 %499, %559
  %625 = sub i64 %502, %564
  %626 = add nsw i64 %564, %502
  %627 = add nsw i64 %569, %503
  %628 = sub i64 %503, %569
  %629 = sub i64 %506, %574
  %630 = add nsw i64 %574, %506
  store i64 %452, i64* %1, align 8
  store i64 %456, i64* %130, align 8
  store i64 %461, i64* %115, align 8
  store i64 %466, i64* %158, align 8
  store i64 %511, i64* %107, align 8
  store i64 %516, i64* %138, align 8
  store i64 %521, i64* %123, align 8
  store i64 %526, i64* %174, align 8
  store i64 %579, i64* %103, align 8
  store i64 %584, i64* %132, align 8
  store i64 %589, i64* %119, align 8
  store i64 %594, i64* %168, align 8
  store i64 %599, i64* %111, align 8
  store i64 %604, i64* %148, align 8
  store i64 %609, i64* %127, align 8
  store i64 %614, i64* %176, align 8
  %631 = mul nsw i64 %615, 804
  %632 = mul nsw i64 %630, 16364
  %633 = add i64 %632, 8192
  %634 = add i64 %633, %631
  %635 = ashr i64 %634, 14
  store i64 %635, i64* %101, align 8
  %636 = mul nsw i64 %616, 12140
  %637 = mul nsw i64 %629, 11003
  %638 = add i64 %637, 8192
  %639 = add i64 %638, %636
  %640 = ashr i64 %639, 14
  store i64 %640, i64* %131, align 8
  %641 = mul nsw i64 %617, 7005
  %642 = mul nsw i64 %628, 14811
  %643 = add i64 %642, 8192
  %644 = add i64 %643, %641
  %645 = ashr i64 %644, 14
  store i64 %645, i64* %117, align 8
  %646 = mul nsw i64 %618, 15426
  %647 = mul nsw i64 %627, 5520
  %648 = add i64 %647, 8192
  %649 = add i64 %648, %646
  %650 = ashr i64 %649, 14
  store i64 %650, i64* %163, align 8
  %651 = mul nsw i64 %619, 3981
  %652 = mul nsw i64 %626, 15893
  %653 = add i64 %652, 8192
  %654 = add i64 %653, %651
  %655 = ashr i64 %654, 14
  store i64 %655, i64* %109, align 8
  %656 = mul nsw i64 %620, 14053
  %657 = mul nsw i64 %625, 8423
  %658 = add i64 %657, 8192
  %659 = add i64 %658, %656
  %660 = ashr i64 %659, 14
  store i64 %660, i64* %143, align 8
  %661 = mul nsw i64 %621, 9760
  %662 = mul nsw i64 %624, 13160
  %663 = add i64 %662, 8192
  %664 = add i64 %663, %661
  %665 = ashr i64 %664, 14
  store i64 %665, i64* %125, align 8
  %666 = mul nsw i64 %622, 16207
  %667 = mul nsw i64 %623, 2404
  %668 = add i64 %667, 8192
  %669 = add i64 %668, %666
  %670 = ashr i64 %669, 14
  store i64 %670, i64* %175, align 8
  %671 = mul nsw i64 %623, 16207
  %672 = mul nsw i64 %622, -2404
  %673 = add i64 %672, 8192
  %674 = add i64 %673, %671
  %675 = ashr i64 %674, 14
  store i64 %675, i64* %105, align 8
  %676 = mul nsw i64 %624, 9760
  %677 = mul nsw i64 %621, -13160
  %678 = add i64 %677, 8192
  %679 = add i64 %678, %676
  %680 = ashr i64 %679, 14
  store i64 %680, i64* %133, align 8
  %681 = mul nsw i64 %625, 14053
  %682 = mul nsw i64 %620, -8423
  %683 = add i64 %682, 8192
  %684 = add i64 %683, %681
  %685 = ashr i64 %684, 14
  store i64 %685, i64* %121, align 8
  %686 = mul nsw i64 %626, 3981
  %687 = mul nsw i64 %619, -15893
  %688 = add i64 %687, 8192
  %689 = add i64 %688, %686
  %690 = ashr i64 %689, 14
  store i64 %690, i64* %173, align 8
  %691 = mul nsw i64 %627, 15426
  %692 = mul nsw i64 %618, -5520
  %693 = add i64 %692, 8192
  %694 = add i64 %693, %691
  %695 = ashr i64 %694, 14
  store i64 %695, i64* %113, align 8
  %696 = mul nsw i64 %628, 7005
  %697 = mul nsw i64 %617, -14811
  %698 = add i64 %697, 8192
  %699 = add i64 %698, %696
  %700 = ashr i64 %699, 14
  store i64 %700, i64* %153, align 8
  %701 = mul nsw i64 %629, 12140
  %702 = mul nsw i64 %616, -11003
  %703 = add i64 %702, 8192
  %704 = add i64 %703, %701
  %705 = ashr i64 %704, 14
  store i64 %705, i64* %129, align 8
  %706 = mul nsw i64 %630, 804
  %707 = mul nsw i64 %615, -16364
  %708 = add i64 %707, 8192
  %709 = add i64 %708, %706
  %710 = ashr i64 %709, 14
  store i64 %710, i64* %177, align 8
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @vpx_fdct32x32_c(i16* nocapture readonly, i32* nocapture, i32) local_unnamed_addr #0 {
  %4 = alloca [1024 x i64], align 16
  %5 = alloca [32 x i64], align 16
  %6 = alloca [32 x i64], align 16
  %7 = alloca [32 x i64], align 16
  %8 = alloca [32 x i64], align 16
  %9 = bitcast [1024 x i64]* %4 to i8*
  call void @llvm.lifetime.start.p0i8(i64 8192, i8* nonnull %9) #3
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %9, i8 -86, i64 8192, i1 false)
  %10 = bitcast [32 x i64]* %5 to i8*
  %11 = bitcast [32 x i64]* %6 to i8*
  %12 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %13 = getelementptr inbounds [32 x i64], [32 x i64]* %6, i64 0, i64 0
  %14 = sext i32 %2 to i64
  %15 = icmp eq i32 %2, 1
  %16 = bitcast [32 x i64]* %5 to <2 x i64>*
  %17 = shl nsw i64 %14, 1
  %18 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %19 = bitcast i64* %18 to <2 x i64>*
  %20 = shl nsw i64 %14, 2
  %21 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %22 = bitcast i64* %21 to <2 x i64>*
  %23 = mul nsw i64 %14, 6
  %24 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %25 = bitcast i64* %24 to <2 x i64>*
  %26 = shl nsw i64 %14, 3
  %27 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %28 = bitcast i64* %27 to <2 x i64>*
  %29 = mul nsw i64 %14, 10
  %30 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %31 = bitcast i64* %30 to <2 x i64>*
  %32 = mul nsw i64 %14, 12
  %33 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %34 = bitcast i64* %33 to <2 x i64>*
  %35 = mul nsw i64 %14, 14
  %36 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %37 = bitcast i64* %36 to <2 x i64>*
  %38 = shl nsw i64 %14, 4
  %39 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %40 = bitcast i64* %39 to <2 x i64>*
  %41 = mul nsw i64 %14, 18
  %42 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %43 = bitcast i64* %42 to <2 x i64>*
  %44 = mul nsw i64 %14, 20
  %45 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %46 = bitcast i64* %45 to <2 x i64>*
  %47 = mul nsw i64 %14, 22
  %48 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %49 = bitcast i64* %48 to <2 x i64>*
  %50 = mul nsw i64 %14, 24
  %51 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %52 = bitcast i64* %51 to <2 x i64>*
  %53 = mul nsw i64 %14, 26
  %54 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %55 = bitcast i64* %54 to <2 x i64>*
  %56 = mul nsw i64 %14, 28
  %57 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %58 = bitcast i64* %57 to <2 x i64>*
  %59 = mul nsw i64 %14, 30
  %60 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %61 = bitcast i64* %60 to <2 x i64>*
  br label %98

62:                                               ; preds = %259
  %63 = bitcast [32 x i64]* %7 to i8*
  %64 = bitcast [32 x i64]* %8 to i8*
  %65 = getelementptr inbounds [32 x i64], [32 x i64]* %7, i64 0, i64 0
  %66 = getelementptr inbounds [32 x i64], [32 x i64]* %8, i64 0, i64 0
  %67 = bitcast [32 x i64]* %8 to <2 x i64>*
  %68 = getelementptr inbounds [32 x i64], [32 x i64]* %8, i64 0, i64 2
  %69 = bitcast i64* %68 to <2 x i64>*
  %70 = getelementptr inbounds [32 x i64], [32 x i64]* %8, i64 0, i64 4
  %71 = bitcast i64* %70 to <2 x i64>*
  %72 = getelementptr inbounds [32 x i64], [32 x i64]* %8, i64 0, i64 6
  %73 = bitcast i64* %72 to <2 x i64>*
  %74 = getelementptr inbounds [32 x i64], [32 x i64]* %8, i64 0, i64 8
  %75 = bitcast i64* %74 to <2 x i64>*
  %76 = getelementptr inbounds [32 x i64], [32 x i64]* %8, i64 0, i64 10
  %77 = bitcast i64* %76 to <2 x i64>*
  %78 = getelementptr inbounds [32 x i64], [32 x i64]* %8, i64 0, i64 12
  %79 = bitcast i64* %78 to <2 x i64>*
  %80 = getelementptr inbounds [32 x i64], [32 x i64]* %8, i64 0, i64 14
  %81 = bitcast i64* %80 to <2 x i64>*
  %82 = getelementptr inbounds [32 x i64], [32 x i64]* %8, i64 0, i64 16
  %83 = bitcast i64* %82 to <2 x i64>*
  %84 = getelementptr inbounds [32 x i64], [32 x i64]* %8, i64 0, i64 18
  %85 = bitcast i64* %84 to <2 x i64>*
  %86 = getelementptr inbounds [32 x i64], [32 x i64]* %8, i64 0, i64 20
  %87 = bitcast i64* %86 to <2 x i64>*
  %88 = getelementptr inbounds [32 x i64], [32 x i64]* %8, i64 0, i64 22
  %89 = bitcast i64* %88 to <2 x i64>*
  %90 = getelementptr inbounds [32 x i64], [32 x i64]* %8, i64 0, i64 24
  %91 = bitcast i64* %90 to <2 x i64>*
  %92 = getelementptr inbounds [32 x i64], [32 x i64]* %8, i64 0, i64 26
  %93 = bitcast i64* %92 to <2 x i64>*
  %94 = getelementptr inbounds [32 x i64], [32 x i64]* %8, i64 0, i64 28
  %95 = bitcast i64* %94 to <2 x i64>*
  %96 = getelementptr inbounds [32 x i64], [32 x i64]* %8, i64 0, i64 30
  %97 = bitcast i64* %96 to <2 x i64>*
  br label %262

98:                                               ; preds = %259, %3
  %99 = phi i64 [ 0, %3 ], [ %260, %259 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %10) #3
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %10, i8 -86, i64 256, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %11) #3
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %11, i8 -86, i64 256, i1 false)
  br i1 %15, label %100, label %212

100:                                              ; preds = %98
  %101 = getelementptr inbounds i16, i16* %0, i64 %99
  %102 = bitcast i16* %101 to <2 x i16>*
  %103 = load <2 x i16>, <2 x i16>* %102, align 2
  %104 = sext <2 x i16> %103 to <2 x i32>
  %105 = shl nsw <2 x i32> %104, <i32 2, i32 2>
  %106 = sext <2 x i32> %105 to <2 x i64>
  store <2 x i64> %106, <2 x i64>* %16, align 16
  %107 = add nsw i64 %17, %99
  %108 = getelementptr inbounds i16, i16* %0, i64 %107
  %109 = bitcast i16* %108 to <2 x i16>*
  %110 = load <2 x i16>, <2 x i16>* %109, align 2
  %111 = sext <2 x i16> %110 to <2 x i32>
  %112 = shl nsw <2 x i32> %111, <i32 2, i32 2>
  %113 = sext <2 x i32> %112 to <2 x i64>
  store <2 x i64> %113, <2 x i64>* %19, align 16
  %114 = add nsw i64 %20, %99
  %115 = getelementptr inbounds i16, i16* %0, i64 %114
  %116 = bitcast i16* %115 to <2 x i16>*
  %117 = load <2 x i16>, <2 x i16>* %116, align 2
  %118 = sext <2 x i16> %117 to <2 x i32>
  %119 = shl nsw <2 x i32> %118, <i32 2, i32 2>
  %120 = sext <2 x i32> %119 to <2 x i64>
  store <2 x i64> %120, <2 x i64>* %22, align 16
  %121 = add nsw i64 %23, %99
  %122 = getelementptr inbounds i16, i16* %0, i64 %121
  %123 = bitcast i16* %122 to <2 x i16>*
  %124 = load <2 x i16>, <2 x i16>* %123, align 2
  %125 = sext <2 x i16> %124 to <2 x i32>
  %126 = shl nsw <2 x i32> %125, <i32 2, i32 2>
  %127 = sext <2 x i32> %126 to <2 x i64>
  store <2 x i64> %127, <2 x i64>* %25, align 16
  %128 = add nsw i64 %26, %99
  %129 = getelementptr inbounds i16, i16* %0, i64 %128
  %130 = bitcast i16* %129 to <2 x i16>*
  %131 = load <2 x i16>, <2 x i16>* %130, align 2
  %132 = sext <2 x i16> %131 to <2 x i32>
  %133 = shl nsw <2 x i32> %132, <i32 2, i32 2>
  %134 = sext <2 x i32> %133 to <2 x i64>
  store <2 x i64> %134, <2 x i64>* %28, align 16
  %135 = add nsw i64 %29, %99
  %136 = getelementptr inbounds i16, i16* %0, i64 %135
  %137 = bitcast i16* %136 to <2 x i16>*
  %138 = load <2 x i16>, <2 x i16>* %137, align 2
  %139 = sext <2 x i16> %138 to <2 x i32>
  %140 = shl nsw <2 x i32> %139, <i32 2, i32 2>
  %141 = sext <2 x i32> %140 to <2 x i64>
  store <2 x i64> %141, <2 x i64>* %31, align 16
  %142 = add nsw i64 %32, %99
  %143 = getelementptr inbounds i16, i16* %0, i64 %142
  %144 = bitcast i16* %143 to <2 x i16>*
  %145 = load <2 x i16>, <2 x i16>* %144, align 2
  %146 = sext <2 x i16> %145 to <2 x i32>
  %147 = shl nsw <2 x i32> %146, <i32 2, i32 2>
  %148 = sext <2 x i32> %147 to <2 x i64>
  store <2 x i64> %148, <2 x i64>* %34, align 16
  %149 = add nsw i64 %35, %99
  %150 = getelementptr inbounds i16, i16* %0, i64 %149
  %151 = bitcast i16* %150 to <2 x i16>*
  %152 = load <2 x i16>, <2 x i16>* %151, align 2
  %153 = sext <2 x i16> %152 to <2 x i32>
  %154 = shl nsw <2 x i32> %153, <i32 2, i32 2>
  %155 = sext <2 x i32> %154 to <2 x i64>
  store <2 x i64> %155, <2 x i64>* %37, align 16
  %156 = add nsw i64 %38, %99
  %157 = getelementptr inbounds i16, i16* %0, i64 %156
  %158 = bitcast i16* %157 to <2 x i16>*
  %159 = load <2 x i16>, <2 x i16>* %158, align 2
  %160 = sext <2 x i16> %159 to <2 x i32>
  %161 = shl nsw <2 x i32> %160, <i32 2, i32 2>
  %162 = sext <2 x i32> %161 to <2 x i64>
  store <2 x i64> %162, <2 x i64>* %40, align 16
  %163 = add nsw i64 %41, %99
  %164 = getelementptr inbounds i16, i16* %0, i64 %163
  %165 = bitcast i16* %164 to <2 x i16>*
  %166 = load <2 x i16>, <2 x i16>* %165, align 2
  %167 = sext <2 x i16> %166 to <2 x i32>
  %168 = shl nsw <2 x i32> %167, <i32 2, i32 2>
  %169 = sext <2 x i32> %168 to <2 x i64>
  store <2 x i64> %169, <2 x i64>* %43, align 16
  %170 = add nsw i64 %44, %99
  %171 = getelementptr inbounds i16, i16* %0, i64 %170
  %172 = bitcast i16* %171 to <2 x i16>*
  %173 = load <2 x i16>, <2 x i16>* %172, align 2
  %174 = sext <2 x i16> %173 to <2 x i32>
  %175 = shl nsw <2 x i32> %174, <i32 2, i32 2>
  %176 = sext <2 x i32> %175 to <2 x i64>
  store <2 x i64> %176, <2 x i64>* %46, align 16
  %177 = add nsw i64 %47, %99
  %178 = getelementptr inbounds i16, i16* %0, i64 %177
  %179 = bitcast i16* %178 to <2 x i16>*
  %180 = load <2 x i16>, <2 x i16>* %179, align 2
  %181 = sext <2 x i16> %180 to <2 x i32>
  %182 = shl nsw <2 x i32> %181, <i32 2, i32 2>
  %183 = sext <2 x i32> %182 to <2 x i64>
  store <2 x i64> %183, <2 x i64>* %49, align 16
  %184 = add nsw i64 %50, %99
  %185 = getelementptr inbounds i16, i16* %0, i64 %184
  %186 = bitcast i16* %185 to <2 x i16>*
  %187 = load <2 x i16>, <2 x i16>* %186, align 2
  %188 = sext <2 x i16> %187 to <2 x i32>
  %189 = shl nsw <2 x i32> %188, <i32 2, i32 2>
  %190 = sext <2 x i32> %189 to <2 x i64>
  store <2 x i64> %190, <2 x i64>* %52, align 16
  %191 = add nsw i64 %53, %99
  %192 = getelementptr inbounds i16, i16* %0, i64 %191
  %193 = bitcast i16* %192 to <2 x i16>*
  %194 = load <2 x i16>, <2 x i16>* %193, align 2
  %195 = sext <2 x i16> %194 to <2 x i32>
  %196 = shl nsw <2 x i32> %195, <i32 2, i32 2>
  %197 = sext <2 x i32> %196 to <2 x i64>
  store <2 x i64> %197, <2 x i64>* %55, align 16
  %198 = add nsw i64 %56, %99
  %199 = getelementptr inbounds i16, i16* %0, i64 %198
  %200 = bitcast i16* %199 to <2 x i16>*
  %201 = load <2 x i16>, <2 x i16>* %200, align 2
  %202 = sext <2 x i16> %201 to <2 x i32>
  %203 = shl nsw <2 x i32> %202, <i32 2, i32 2>
  %204 = sext <2 x i32> %203 to <2 x i64>
  store <2 x i64> %204, <2 x i64>* %58, align 16
  %205 = add nsw i64 %59, %99
  %206 = getelementptr inbounds i16, i16* %0, i64 %205
  %207 = bitcast i16* %206 to <2 x i16>*
  %208 = load <2 x i16>, <2 x i16>* %207, align 2
  %209 = sext <2 x i16> %208 to <2 x i32>
  %210 = shl nsw <2 x i32> %209, <i32 2, i32 2>
  %211 = sext <2 x i32> %210 to <2 x i64>
  store <2 x i64> %211, <2 x i64>* %61, align 16
  br label %233

212:                                              ; preds = %98, %212
  %213 = phi i64 [ %231, %212 ], [ 0, %98 ]
  %214 = mul nsw i64 %213, %14
  %215 = add nsw i64 %214, %99
  %216 = getelementptr inbounds i16, i16* %0, i64 %215
  %217 = load i16, i16* %216, align 2
  %218 = sext i16 %217 to i32
  %219 = shl nsw i32 %218, 2
  %220 = sext i32 %219 to i64
  %221 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 %213
  store i64 %220, i64* %221, align 16
  %222 = or i64 %213, 1
  %223 = mul nsw i64 %222, %14
  %224 = add nsw i64 %223, %99
  %225 = getelementptr inbounds i16, i16* %0, i64 %224
  %226 = load i16, i16* %225, align 2
  %227 = sext i16 %226 to i32
  %228 = shl nsw i32 %227, 2
  %229 = sext i32 %228 to i64
  %230 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 %222
  store i64 %229, i64* %230, align 8
  %231 = add nuw nsw i64 %213, 2
  %232 = icmp eq i64 %231, 32
  br i1 %232, label %233, label %212, !llvm.loop !5

233:                                              ; preds = %212, %100
  call void @vpx_fdct32(i64* nonnull %12, i64* nonnull %13, i32 0)
  br label %234

234:                                              ; preds = %234, %233
  %235 = phi i64 [ 0, %233 ], [ %257, %234 ]
  %236 = getelementptr inbounds [32 x i64], [32 x i64]* %6, i64 0, i64 %235
  %237 = load i64, i64* %236, align 16
  %238 = add nsw i64 %237, 1
  %239 = icmp sgt i64 %237, 0
  %240 = zext i1 %239 to i64
  %241 = add nsw i64 %238, %240
  %242 = ashr i64 %241, 2
  %243 = shl i64 %235, 5
  %244 = add nuw nsw i64 %243, %99
  %245 = getelementptr inbounds [1024 x i64], [1024 x i64]* %4, i64 0, i64 %244
  store i64 %242, i64* %245, align 8
  %246 = or i64 %235, 1
  %247 = getelementptr inbounds [32 x i64], [32 x i64]* %6, i64 0, i64 %246
  %248 = load i64, i64* %247, align 8
  %249 = add nsw i64 %248, 1
  %250 = icmp sgt i64 %248, 0
  %251 = zext i1 %250 to i64
  %252 = add nsw i64 %249, %251
  %253 = ashr i64 %252, 2
  %254 = shl i64 %246, 5
  %255 = add nuw nsw i64 %254, %99
  %256 = getelementptr inbounds [1024 x i64], [1024 x i64]* %4, i64 0, i64 %255
  store i64 %253, i64* %256, align 8
  %257 = add nuw nsw i64 %235, 2
  %258 = icmp eq i64 %257, 32
  br i1 %258, label %259, label %234

259:                                              ; preds = %234
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %11) #3
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %10) #3
  %260 = add nuw nsw i64 %99, 1
  %261 = icmp eq i64 %260, 32
  br i1 %261, label %62, label %98

262:                                              ; preds = %262, %62
  %263 = phi i64 [ 0, %62 ], [ %410, %262 ]
  %264 = shl i64 %263, 5
  %265 = getelementptr [1024 x i64], [1024 x i64]* %4, i64 0, i64 %264
  %266 = bitcast i64* %265 to i8*
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %63) #3
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %64) #3
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %64, i8 -86, i64 256, i1 false)
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 16 %63, i8* align 16 %266, i64 256, i1 false)
  call void @vpx_fdct32(i64* nonnull %65, i64* nonnull %66, i32 0)
  %267 = load <2 x i64>, <2 x i64>* %67, align 16
  %268 = add nsw <2 x i64> %267, <i64 1, i64 1>
  %269 = lshr <2 x i64> %267, <i64 63, i64 63>
  %270 = add nsw <2 x i64> %268, %269
  %271 = lshr <2 x i64> %270, <i64 2, i64 2>
  %272 = trunc <2 x i64> %271 to <2 x i32>
  %273 = getelementptr inbounds i32, i32* %1, i64 %264
  %274 = bitcast i32* %273 to <2 x i32>*
  store <2 x i32> %272, <2 x i32>* %274, align 4
  %275 = load <2 x i64>, <2 x i64>* %69, align 16
  %276 = add nsw <2 x i64> %275, <i64 1, i64 1>
  %277 = lshr <2 x i64> %275, <i64 63, i64 63>
  %278 = add nsw <2 x i64> %276, %277
  %279 = lshr <2 x i64> %278, <i64 2, i64 2>
  %280 = trunc <2 x i64> %279 to <2 x i32>
  %281 = or i64 %264, 2
  %282 = getelementptr inbounds i32, i32* %1, i64 %281
  %283 = bitcast i32* %282 to <2 x i32>*
  store <2 x i32> %280, <2 x i32>* %283, align 4
  %284 = load <2 x i64>, <2 x i64>* %71, align 16
  %285 = add nsw <2 x i64> %284, <i64 1, i64 1>
  %286 = lshr <2 x i64> %284, <i64 63, i64 63>
  %287 = add nsw <2 x i64> %285, %286
  %288 = lshr <2 x i64> %287, <i64 2, i64 2>
  %289 = trunc <2 x i64> %288 to <2 x i32>
  %290 = or i64 %264, 4
  %291 = getelementptr inbounds i32, i32* %1, i64 %290
  %292 = bitcast i32* %291 to <2 x i32>*
  store <2 x i32> %289, <2 x i32>* %292, align 4
  %293 = load <2 x i64>, <2 x i64>* %73, align 16
  %294 = add nsw <2 x i64> %293, <i64 1, i64 1>
  %295 = lshr <2 x i64> %293, <i64 63, i64 63>
  %296 = add nsw <2 x i64> %294, %295
  %297 = lshr <2 x i64> %296, <i64 2, i64 2>
  %298 = trunc <2 x i64> %297 to <2 x i32>
  %299 = or i64 %264, 6
  %300 = getelementptr inbounds i32, i32* %1, i64 %299
  %301 = bitcast i32* %300 to <2 x i32>*
  store <2 x i32> %298, <2 x i32>* %301, align 4
  %302 = load <2 x i64>, <2 x i64>* %75, align 16
  %303 = add nsw <2 x i64> %302, <i64 1, i64 1>
  %304 = lshr <2 x i64> %302, <i64 63, i64 63>
  %305 = add nsw <2 x i64> %303, %304
  %306 = lshr <2 x i64> %305, <i64 2, i64 2>
  %307 = trunc <2 x i64> %306 to <2 x i32>
  %308 = or i64 %264, 8
  %309 = getelementptr inbounds i32, i32* %1, i64 %308
  %310 = bitcast i32* %309 to <2 x i32>*
  store <2 x i32> %307, <2 x i32>* %310, align 4
  %311 = load <2 x i64>, <2 x i64>* %77, align 16
  %312 = add nsw <2 x i64> %311, <i64 1, i64 1>
  %313 = lshr <2 x i64> %311, <i64 63, i64 63>
  %314 = add nsw <2 x i64> %312, %313
  %315 = lshr <2 x i64> %314, <i64 2, i64 2>
  %316 = trunc <2 x i64> %315 to <2 x i32>
  %317 = or i64 %264, 10
  %318 = getelementptr inbounds i32, i32* %1, i64 %317
  %319 = bitcast i32* %318 to <2 x i32>*
  store <2 x i32> %316, <2 x i32>* %319, align 4
  %320 = load <2 x i64>, <2 x i64>* %79, align 16
  %321 = add nsw <2 x i64> %320, <i64 1, i64 1>
  %322 = lshr <2 x i64> %320, <i64 63, i64 63>
  %323 = add nsw <2 x i64> %321, %322
  %324 = lshr <2 x i64> %323, <i64 2, i64 2>
  %325 = trunc <2 x i64> %324 to <2 x i32>
  %326 = or i64 %264, 12
  %327 = getelementptr inbounds i32, i32* %1, i64 %326
  %328 = bitcast i32* %327 to <2 x i32>*
  store <2 x i32> %325, <2 x i32>* %328, align 4
  %329 = load <2 x i64>, <2 x i64>* %81, align 16
  %330 = add nsw <2 x i64> %329, <i64 1, i64 1>
  %331 = lshr <2 x i64> %329, <i64 63, i64 63>
  %332 = add nsw <2 x i64> %330, %331
  %333 = lshr <2 x i64> %332, <i64 2, i64 2>
  %334 = trunc <2 x i64> %333 to <2 x i32>
  %335 = or i64 %264, 14
  %336 = getelementptr inbounds i32, i32* %1, i64 %335
  %337 = bitcast i32* %336 to <2 x i32>*
  store <2 x i32> %334, <2 x i32>* %337, align 4
  %338 = load <2 x i64>, <2 x i64>* %83, align 16
  %339 = add nsw <2 x i64> %338, <i64 1, i64 1>
  %340 = lshr <2 x i64> %338, <i64 63, i64 63>
  %341 = add nsw <2 x i64> %339, %340
  %342 = lshr <2 x i64> %341, <i64 2, i64 2>
  %343 = trunc <2 x i64> %342 to <2 x i32>
  %344 = or i64 %264, 16
  %345 = getelementptr inbounds i32, i32* %1, i64 %344
  %346 = bitcast i32* %345 to <2 x i32>*
  store <2 x i32> %343, <2 x i32>* %346, align 4
  %347 = load <2 x i64>, <2 x i64>* %85, align 16
  %348 = add nsw <2 x i64> %347, <i64 1, i64 1>
  %349 = lshr <2 x i64> %347, <i64 63, i64 63>
  %350 = add nsw <2 x i64> %348, %349
  %351 = lshr <2 x i64> %350, <i64 2, i64 2>
  %352 = trunc <2 x i64> %351 to <2 x i32>
  %353 = or i64 %264, 18
  %354 = getelementptr inbounds i32, i32* %1, i64 %353
  %355 = bitcast i32* %354 to <2 x i32>*
  store <2 x i32> %352, <2 x i32>* %355, align 4
  %356 = load <2 x i64>, <2 x i64>* %87, align 16
  %357 = add nsw <2 x i64> %356, <i64 1, i64 1>
  %358 = lshr <2 x i64> %356, <i64 63, i64 63>
  %359 = add nsw <2 x i64> %357, %358
  %360 = lshr <2 x i64> %359, <i64 2, i64 2>
  %361 = trunc <2 x i64> %360 to <2 x i32>
  %362 = or i64 %264, 20
  %363 = getelementptr inbounds i32, i32* %1, i64 %362
  %364 = bitcast i32* %363 to <2 x i32>*
  store <2 x i32> %361, <2 x i32>* %364, align 4
  %365 = load <2 x i64>, <2 x i64>* %89, align 16
  %366 = add nsw <2 x i64> %365, <i64 1, i64 1>
  %367 = lshr <2 x i64> %365, <i64 63, i64 63>
  %368 = add nsw <2 x i64> %366, %367
  %369 = lshr <2 x i64> %368, <i64 2, i64 2>
  %370 = trunc <2 x i64> %369 to <2 x i32>
  %371 = or i64 %264, 22
  %372 = getelementptr inbounds i32, i32* %1, i64 %371
  %373 = bitcast i32* %372 to <2 x i32>*
  store <2 x i32> %370, <2 x i32>* %373, align 4
  %374 = load <2 x i64>, <2 x i64>* %91, align 16
  %375 = add nsw <2 x i64> %374, <i64 1, i64 1>
  %376 = lshr <2 x i64> %374, <i64 63, i64 63>
  %377 = add nsw <2 x i64> %375, %376
  %378 = lshr <2 x i64> %377, <i64 2, i64 2>
  %379 = trunc <2 x i64> %378 to <2 x i32>
  %380 = or i64 %264, 24
  %381 = getelementptr inbounds i32, i32* %1, i64 %380
  %382 = bitcast i32* %381 to <2 x i32>*
  store <2 x i32> %379, <2 x i32>* %382, align 4
  %383 = load <2 x i64>, <2 x i64>* %93, align 16
  %384 = add nsw <2 x i64> %383, <i64 1, i64 1>
  %385 = lshr <2 x i64> %383, <i64 63, i64 63>
  %386 = add nsw <2 x i64> %384, %385
  %387 = lshr <2 x i64> %386, <i64 2, i64 2>
  %388 = trunc <2 x i64> %387 to <2 x i32>
  %389 = or i64 %264, 26
  %390 = getelementptr inbounds i32, i32* %1, i64 %389
  %391 = bitcast i32* %390 to <2 x i32>*
  store <2 x i32> %388, <2 x i32>* %391, align 4
  %392 = load <2 x i64>, <2 x i64>* %95, align 16
  %393 = add nsw <2 x i64> %392, <i64 1, i64 1>
  %394 = lshr <2 x i64> %392, <i64 63, i64 63>
  %395 = add nsw <2 x i64> %393, %394
  %396 = lshr <2 x i64> %395, <i64 2, i64 2>
  %397 = trunc <2 x i64> %396 to <2 x i32>
  %398 = or i64 %264, 28
  %399 = getelementptr inbounds i32, i32* %1, i64 %398
  %400 = bitcast i32* %399 to <2 x i32>*
  store <2 x i32> %397, <2 x i32>* %400, align 4
  %401 = load <2 x i64>, <2 x i64>* %97, align 16
  %402 = add nsw <2 x i64> %401, <i64 1, i64 1>
  %403 = lshr <2 x i64> %401, <i64 63, i64 63>
  %404 = add nsw <2 x i64> %402, %403
  %405 = lshr <2 x i64> %404, <i64 2, i64 2>
  %406 = trunc <2 x i64> %405 to <2 x i32>
  %407 = or i64 %264, 30
  %408 = getelementptr inbounds i32, i32* %1, i64 %407
  %409 = bitcast i32* %408 to <2 x i32>*
  store <2 x i32> %406, <2 x i32>* %409, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %64) #3
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %63) #3
  %410 = add nuw nsw i64 %263, 1
  %411 = icmp eq i64 %410, 32
  br i1 %411, label %412, label %262

412:                                              ; preds = %262
  call void @llvm.lifetime.end.p0i8(i64 8192, i8* nonnull %9) #3
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @vpx_fdct32x32_rd_c(i16* nocapture readonly, i32* nocapture, i32) local_unnamed_addr #0 {
  %4 = alloca [1024 x i64], align 16
  %5 = alloca [32 x i64], align 16
  %6 = alloca [32 x i64], align 16
  %7 = alloca [32 x i64], align 16
  %8 = alloca [32 x i64], align 16
  %9 = bitcast [1024 x i64]* %4 to i8*
  call void @llvm.lifetime.start.p0i8(i64 8192, i8* nonnull %9) #3
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %9, i8 -86, i64 8192, i1 false)
  %10 = bitcast [32 x i64]* %5 to i8*
  %11 = bitcast [32 x i64]* %6 to i8*
  %12 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 0
  %13 = getelementptr inbounds [32 x i64], [32 x i64]* %6, i64 0, i64 0
  %14 = sext i32 %2 to i64
  %15 = icmp eq i32 %2, 1
  %16 = bitcast [32 x i64]* %5 to <2 x i64>*
  %17 = shl nsw i64 %14, 1
  %18 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 2
  %19 = bitcast i64* %18 to <2 x i64>*
  %20 = shl nsw i64 %14, 2
  %21 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 4
  %22 = bitcast i64* %21 to <2 x i64>*
  %23 = mul nsw i64 %14, 6
  %24 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 6
  %25 = bitcast i64* %24 to <2 x i64>*
  %26 = shl nsw i64 %14, 3
  %27 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 8
  %28 = bitcast i64* %27 to <2 x i64>*
  %29 = mul nsw i64 %14, 10
  %30 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 10
  %31 = bitcast i64* %30 to <2 x i64>*
  %32 = mul nsw i64 %14, 12
  %33 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 12
  %34 = bitcast i64* %33 to <2 x i64>*
  %35 = mul nsw i64 %14, 14
  %36 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 14
  %37 = bitcast i64* %36 to <2 x i64>*
  %38 = shl nsw i64 %14, 4
  %39 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 16
  %40 = bitcast i64* %39 to <2 x i64>*
  %41 = mul nsw i64 %14, 18
  %42 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 18
  %43 = bitcast i64* %42 to <2 x i64>*
  %44 = mul nsw i64 %14, 20
  %45 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 20
  %46 = bitcast i64* %45 to <2 x i64>*
  %47 = mul nsw i64 %14, 22
  %48 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 22
  %49 = bitcast i64* %48 to <2 x i64>*
  %50 = mul nsw i64 %14, 24
  %51 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 24
  %52 = bitcast i64* %51 to <2 x i64>*
  %53 = mul nsw i64 %14, 26
  %54 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 26
  %55 = bitcast i64* %54 to <2 x i64>*
  %56 = mul nsw i64 %14, 28
  %57 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 28
  %58 = bitcast i64* %57 to <2 x i64>*
  %59 = mul nsw i64 %14, 30
  %60 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 30
  %61 = bitcast i64* %60 to <2 x i64>*
  br label %98

62:                                               ; preds = %259
  %63 = bitcast [32 x i64]* %7 to i8*
  %64 = bitcast [32 x i64]* %8 to i8*
  %65 = getelementptr inbounds [32 x i64], [32 x i64]* %7, i64 0, i64 0
  %66 = getelementptr inbounds [32 x i64], [32 x i64]* %8, i64 0, i64 0
  %67 = bitcast [32 x i64]* %8 to <2 x i64>*
  %68 = getelementptr inbounds [32 x i64], [32 x i64]* %8, i64 0, i64 2
  %69 = bitcast i64* %68 to <2 x i64>*
  %70 = getelementptr inbounds [32 x i64], [32 x i64]* %8, i64 0, i64 4
  %71 = bitcast i64* %70 to <2 x i64>*
  %72 = getelementptr inbounds [32 x i64], [32 x i64]* %8, i64 0, i64 6
  %73 = bitcast i64* %72 to <2 x i64>*
  %74 = getelementptr inbounds [32 x i64], [32 x i64]* %8, i64 0, i64 8
  %75 = bitcast i64* %74 to <2 x i64>*
  %76 = getelementptr inbounds [32 x i64], [32 x i64]* %8, i64 0, i64 10
  %77 = bitcast i64* %76 to <2 x i64>*
  %78 = getelementptr inbounds [32 x i64], [32 x i64]* %8, i64 0, i64 12
  %79 = bitcast i64* %78 to <2 x i64>*
  %80 = getelementptr inbounds [32 x i64], [32 x i64]* %8, i64 0, i64 14
  %81 = bitcast i64* %80 to <2 x i64>*
  %82 = getelementptr inbounds [32 x i64], [32 x i64]* %8, i64 0, i64 16
  %83 = bitcast i64* %82 to <2 x i64>*
  %84 = getelementptr inbounds [32 x i64], [32 x i64]* %8, i64 0, i64 18
  %85 = bitcast i64* %84 to <2 x i64>*
  %86 = getelementptr inbounds [32 x i64], [32 x i64]* %8, i64 0, i64 20
  %87 = bitcast i64* %86 to <2 x i64>*
  %88 = getelementptr inbounds [32 x i64], [32 x i64]* %8, i64 0, i64 22
  %89 = bitcast i64* %88 to <2 x i64>*
  %90 = getelementptr inbounds [32 x i64], [32 x i64]* %8, i64 0, i64 24
  %91 = bitcast i64* %90 to <2 x i64>*
  %92 = getelementptr inbounds [32 x i64], [32 x i64]* %8, i64 0, i64 26
  %93 = bitcast i64* %92 to <2 x i64>*
  %94 = getelementptr inbounds [32 x i64], [32 x i64]* %8, i64 0, i64 28
  %95 = bitcast i64* %94 to <2 x i64>*
  %96 = getelementptr inbounds [32 x i64], [32 x i64]* %8, i64 0, i64 30
  %97 = bitcast i64* %96 to <2 x i64>*
  br label %262

98:                                               ; preds = %259, %3
  %99 = phi i64 [ 0, %3 ], [ %260, %259 ]
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %10) #3
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %10, i8 -86, i64 256, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %11) #3
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %11, i8 -86, i64 256, i1 false)
  br i1 %15, label %100, label %212

100:                                              ; preds = %98
  %101 = getelementptr inbounds i16, i16* %0, i64 %99
  %102 = bitcast i16* %101 to <2 x i16>*
  %103 = load <2 x i16>, <2 x i16>* %102, align 2
  %104 = sext <2 x i16> %103 to <2 x i32>
  %105 = shl nsw <2 x i32> %104, <i32 2, i32 2>
  %106 = sext <2 x i32> %105 to <2 x i64>
  store <2 x i64> %106, <2 x i64>* %16, align 16
  %107 = add nsw i64 %17, %99
  %108 = getelementptr inbounds i16, i16* %0, i64 %107
  %109 = bitcast i16* %108 to <2 x i16>*
  %110 = load <2 x i16>, <2 x i16>* %109, align 2
  %111 = sext <2 x i16> %110 to <2 x i32>
  %112 = shl nsw <2 x i32> %111, <i32 2, i32 2>
  %113 = sext <2 x i32> %112 to <2 x i64>
  store <2 x i64> %113, <2 x i64>* %19, align 16
  %114 = add nsw i64 %20, %99
  %115 = getelementptr inbounds i16, i16* %0, i64 %114
  %116 = bitcast i16* %115 to <2 x i16>*
  %117 = load <2 x i16>, <2 x i16>* %116, align 2
  %118 = sext <2 x i16> %117 to <2 x i32>
  %119 = shl nsw <2 x i32> %118, <i32 2, i32 2>
  %120 = sext <2 x i32> %119 to <2 x i64>
  store <2 x i64> %120, <2 x i64>* %22, align 16
  %121 = add nsw i64 %23, %99
  %122 = getelementptr inbounds i16, i16* %0, i64 %121
  %123 = bitcast i16* %122 to <2 x i16>*
  %124 = load <2 x i16>, <2 x i16>* %123, align 2
  %125 = sext <2 x i16> %124 to <2 x i32>
  %126 = shl nsw <2 x i32> %125, <i32 2, i32 2>
  %127 = sext <2 x i32> %126 to <2 x i64>
  store <2 x i64> %127, <2 x i64>* %25, align 16
  %128 = add nsw i64 %26, %99
  %129 = getelementptr inbounds i16, i16* %0, i64 %128
  %130 = bitcast i16* %129 to <2 x i16>*
  %131 = load <2 x i16>, <2 x i16>* %130, align 2
  %132 = sext <2 x i16> %131 to <2 x i32>
  %133 = shl nsw <2 x i32> %132, <i32 2, i32 2>
  %134 = sext <2 x i32> %133 to <2 x i64>
  store <2 x i64> %134, <2 x i64>* %28, align 16
  %135 = add nsw i64 %29, %99
  %136 = getelementptr inbounds i16, i16* %0, i64 %135
  %137 = bitcast i16* %136 to <2 x i16>*
  %138 = load <2 x i16>, <2 x i16>* %137, align 2
  %139 = sext <2 x i16> %138 to <2 x i32>
  %140 = shl nsw <2 x i32> %139, <i32 2, i32 2>
  %141 = sext <2 x i32> %140 to <2 x i64>
  store <2 x i64> %141, <2 x i64>* %31, align 16
  %142 = add nsw i64 %32, %99
  %143 = getelementptr inbounds i16, i16* %0, i64 %142
  %144 = bitcast i16* %143 to <2 x i16>*
  %145 = load <2 x i16>, <2 x i16>* %144, align 2
  %146 = sext <2 x i16> %145 to <2 x i32>
  %147 = shl nsw <2 x i32> %146, <i32 2, i32 2>
  %148 = sext <2 x i32> %147 to <2 x i64>
  store <2 x i64> %148, <2 x i64>* %34, align 16
  %149 = add nsw i64 %35, %99
  %150 = getelementptr inbounds i16, i16* %0, i64 %149
  %151 = bitcast i16* %150 to <2 x i16>*
  %152 = load <2 x i16>, <2 x i16>* %151, align 2
  %153 = sext <2 x i16> %152 to <2 x i32>
  %154 = shl nsw <2 x i32> %153, <i32 2, i32 2>
  %155 = sext <2 x i32> %154 to <2 x i64>
  store <2 x i64> %155, <2 x i64>* %37, align 16
  %156 = add nsw i64 %38, %99
  %157 = getelementptr inbounds i16, i16* %0, i64 %156
  %158 = bitcast i16* %157 to <2 x i16>*
  %159 = load <2 x i16>, <2 x i16>* %158, align 2
  %160 = sext <2 x i16> %159 to <2 x i32>
  %161 = shl nsw <2 x i32> %160, <i32 2, i32 2>
  %162 = sext <2 x i32> %161 to <2 x i64>
  store <2 x i64> %162, <2 x i64>* %40, align 16
  %163 = add nsw i64 %41, %99
  %164 = getelementptr inbounds i16, i16* %0, i64 %163
  %165 = bitcast i16* %164 to <2 x i16>*
  %166 = load <2 x i16>, <2 x i16>* %165, align 2
  %167 = sext <2 x i16> %166 to <2 x i32>
  %168 = shl nsw <2 x i32> %167, <i32 2, i32 2>
  %169 = sext <2 x i32> %168 to <2 x i64>
  store <2 x i64> %169, <2 x i64>* %43, align 16
  %170 = add nsw i64 %44, %99
  %171 = getelementptr inbounds i16, i16* %0, i64 %170
  %172 = bitcast i16* %171 to <2 x i16>*
  %173 = load <2 x i16>, <2 x i16>* %172, align 2
  %174 = sext <2 x i16> %173 to <2 x i32>
  %175 = shl nsw <2 x i32> %174, <i32 2, i32 2>
  %176 = sext <2 x i32> %175 to <2 x i64>
  store <2 x i64> %176, <2 x i64>* %46, align 16
  %177 = add nsw i64 %47, %99
  %178 = getelementptr inbounds i16, i16* %0, i64 %177
  %179 = bitcast i16* %178 to <2 x i16>*
  %180 = load <2 x i16>, <2 x i16>* %179, align 2
  %181 = sext <2 x i16> %180 to <2 x i32>
  %182 = shl nsw <2 x i32> %181, <i32 2, i32 2>
  %183 = sext <2 x i32> %182 to <2 x i64>
  store <2 x i64> %183, <2 x i64>* %49, align 16
  %184 = add nsw i64 %50, %99
  %185 = getelementptr inbounds i16, i16* %0, i64 %184
  %186 = bitcast i16* %185 to <2 x i16>*
  %187 = load <2 x i16>, <2 x i16>* %186, align 2
  %188 = sext <2 x i16> %187 to <2 x i32>
  %189 = shl nsw <2 x i32> %188, <i32 2, i32 2>
  %190 = sext <2 x i32> %189 to <2 x i64>
  store <2 x i64> %190, <2 x i64>* %52, align 16
  %191 = add nsw i64 %53, %99
  %192 = getelementptr inbounds i16, i16* %0, i64 %191
  %193 = bitcast i16* %192 to <2 x i16>*
  %194 = load <2 x i16>, <2 x i16>* %193, align 2
  %195 = sext <2 x i16> %194 to <2 x i32>
  %196 = shl nsw <2 x i32> %195, <i32 2, i32 2>
  %197 = sext <2 x i32> %196 to <2 x i64>
  store <2 x i64> %197, <2 x i64>* %55, align 16
  %198 = add nsw i64 %56, %99
  %199 = getelementptr inbounds i16, i16* %0, i64 %198
  %200 = bitcast i16* %199 to <2 x i16>*
  %201 = load <2 x i16>, <2 x i16>* %200, align 2
  %202 = sext <2 x i16> %201 to <2 x i32>
  %203 = shl nsw <2 x i32> %202, <i32 2, i32 2>
  %204 = sext <2 x i32> %203 to <2 x i64>
  store <2 x i64> %204, <2 x i64>* %58, align 16
  %205 = add nsw i64 %59, %99
  %206 = getelementptr inbounds i16, i16* %0, i64 %205
  %207 = bitcast i16* %206 to <2 x i16>*
  %208 = load <2 x i16>, <2 x i16>* %207, align 2
  %209 = sext <2 x i16> %208 to <2 x i32>
  %210 = shl nsw <2 x i32> %209, <i32 2, i32 2>
  %211 = sext <2 x i32> %210 to <2 x i64>
  store <2 x i64> %211, <2 x i64>* %61, align 16
  br label %233

212:                                              ; preds = %98, %212
  %213 = phi i64 [ %231, %212 ], [ 0, %98 ]
  %214 = mul nsw i64 %213, %14
  %215 = add nsw i64 %214, %99
  %216 = getelementptr inbounds i16, i16* %0, i64 %215
  %217 = load i16, i16* %216, align 2
  %218 = sext i16 %217 to i32
  %219 = shl nsw i32 %218, 2
  %220 = sext i32 %219 to i64
  %221 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 %213
  store i64 %220, i64* %221, align 16
  %222 = or i64 %213, 1
  %223 = mul nsw i64 %222, %14
  %224 = add nsw i64 %223, %99
  %225 = getelementptr inbounds i16, i16* %0, i64 %224
  %226 = load i16, i16* %225, align 2
  %227 = sext i16 %226 to i32
  %228 = shl nsw i32 %227, 2
  %229 = sext i32 %228 to i64
  %230 = getelementptr inbounds [32 x i64], [32 x i64]* %5, i64 0, i64 %222
  store i64 %229, i64* %230, align 8
  %231 = add nuw nsw i64 %213, 2
  %232 = icmp eq i64 %231, 32
  br i1 %232, label %233, label %212, !llvm.loop !6

233:                                              ; preds = %212, %100
  call void @vpx_fdct32(i64* nonnull %12, i64* nonnull %13, i32 0)
  br label %234

234:                                              ; preds = %234, %233
  %235 = phi i64 [ 0, %233 ], [ %257, %234 ]
  %236 = getelementptr inbounds [32 x i64], [32 x i64]* %6, i64 0, i64 %235
  %237 = load i64, i64* %236, align 16
  %238 = add nsw i64 %237, 1
  %239 = icmp sgt i64 %237, 0
  %240 = zext i1 %239 to i64
  %241 = add nsw i64 %238, %240
  %242 = ashr i64 %241, 2
  %243 = shl i64 %235, 5
  %244 = add nuw nsw i64 %243, %99
  %245 = getelementptr inbounds [1024 x i64], [1024 x i64]* %4, i64 0, i64 %244
  store i64 %242, i64* %245, align 8
  %246 = or i64 %235, 1
  %247 = getelementptr inbounds [32 x i64], [32 x i64]* %6, i64 0, i64 %246
  %248 = load i64, i64* %247, align 8
  %249 = add nsw i64 %248, 1
  %250 = icmp sgt i64 %248, 0
  %251 = zext i1 %250 to i64
  %252 = add nsw i64 %249, %251
  %253 = ashr i64 %252, 2
  %254 = shl i64 %246, 5
  %255 = add nuw nsw i64 %254, %99
  %256 = getelementptr inbounds [1024 x i64], [1024 x i64]* %4, i64 0, i64 %255
  store i64 %253, i64* %256, align 8
  %257 = add nuw nsw i64 %235, 2
  %258 = icmp eq i64 %257, 32
  br i1 %258, label %259, label %234

259:                                              ; preds = %234
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %11) #3
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %10) #3
  %260 = add nuw nsw i64 %99, 1
  %261 = icmp eq i64 %260, 32
  br i1 %261, label %62, label %98

262:                                              ; preds = %262, %62
  %263 = phi i64 [ 0, %62 ], [ %346, %262 ]
  %264 = shl i64 %263, 5
  %265 = getelementptr [1024 x i64], [1024 x i64]* %4, i64 0, i64 %264
  %266 = bitcast i64* %265 to i8*
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %63) #3
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %64) #3
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %64, i8 -86, i64 256, i1 false)
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 16 %63, i8* align 16 %266, i64 256, i1 false)
  call void @vpx_fdct32(i64* nonnull %65, i64* nonnull %66, i32 1)
  %267 = load <2 x i64>, <2 x i64>* %67, align 16
  %268 = trunc <2 x i64> %267 to <2 x i32>
  %269 = getelementptr inbounds i32, i32* %1, i64 %264
  %270 = bitcast i32* %269 to <2 x i32>*
  store <2 x i32> %268, <2 x i32>* %270, align 4
  %271 = load <2 x i64>, <2 x i64>* %69, align 16
  %272 = trunc <2 x i64> %271 to <2 x i32>
  %273 = or i64 %264, 2
  %274 = getelementptr inbounds i32, i32* %1, i64 %273
  %275 = bitcast i32* %274 to <2 x i32>*
  store <2 x i32> %272, <2 x i32>* %275, align 4
  %276 = load <2 x i64>, <2 x i64>* %71, align 16
  %277 = trunc <2 x i64> %276 to <2 x i32>
  %278 = or i64 %264, 4
  %279 = getelementptr inbounds i32, i32* %1, i64 %278
  %280 = bitcast i32* %279 to <2 x i32>*
  store <2 x i32> %277, <2 x i32>* %280, align 4
  %281 = load <2 x i64>, <2 x i64>* %73, align 16
  %282 = trunc <2 x i64> %281 to <2 x i32>
  %283 = or i64 %264, 6
  %284 = getelementptr inbounds i32, i32* %1, i64 %283
  %285 = bitcast i32* %284 to <2 x i32>*
  store <2 x i32> %282, <2 x i32>* %285, align 4
  %286 = load <2 x i64>, <2 x i64>* %75, align 16
  %287 = trunc <2 x i64> %286 to <2 x i32>
  %288 = or i64 %264, 8
  %289 = getelementptr inbounds i32, i32* %1, i64 %288
  %290 = bitcast i32* %289 to <2 x i32>*
  store <2 x i32> %287, <2 x i32>* %290, align 4
  %291 = load <2 x i64>, <2 x i64>* %77, align 16
  %292 = trunc <2 x i64> %291 to <2 x i32>
  %293 = or i64 %264, 10
  %294 = getelementptr inbounds i32, i32* %1, i64 %293
  %295 = bitcast i32* %294 to <2 x i32>*
  store <2 x i32> %292, <2 x i32>* %295, align 4
  %296 = load <2 x i64>, <2 x i64>* %79, align 16
  %297 = trunc <2 x i64> %296 to <2 x i32>
  %298 = or i64 %264, 12
  %299 = getelementptr inbounds i32, i32* %1, i64 %298
  %300 = bitcast i32* %299 to <2 x i32>*
  store <2 x i32> %297, <2 x i32>* %300, align 4
  %301 = load <2 x i64>, <2 x i64>* %81, align 16
  %302 = trunc <2 x i64> %301 to <2 x i32>
  %303 = or i64 %264, 14
  %304 = getelementptr inbounds i32, i32* %1, i64 %303
  %305 = bitcast i32* %304 to <2 x i32>*
  store <2 x i32> %302, <2 x i32>* %305, align 4
  %306 = load <2 x i64>, <2 x i64>* %83, align 16
  %307 = trunc <2 x i64> %306 to <2 x i32>
  %308 = or i64 %264, 16
  %309 = getelementptr inbounds i32, i32* %1, i64 %308
  %310 = bitcast i32* %309 to <2 x i32>*
  store <2 x i32> %307, <2 x i32>* %310, align 4
  %311 = load <2 x i64>, <2 x i64>* %85, align 16
  %312 = trunc <2 x i64> %311 to <2 x i32>
  %313 = or i64 %264, 18
  %314 = getelementptr inbounds i32, i32* %1, i64 %313
  %315 = bitcast i32* %314 to <2 x i32>*
  store <2 x i32> %312, <2 x i32>* %315, align 4
  %316 = load <2 x i64>, <2 x i64>* %87, align 16
  %317 = trunc <2 x i64> %316 to <2 x i32>
  %318 = or i64 %264, 20
  %319 = getelementptr inbounds i32, i32* %1, i64 %318
  %320 = bitcast i32* %319 to <2 x i32>*
  store <2 x i32> %317, <2 x i32>* %320, align 4
  %321 = load <2 x i64>, <2 x i64>* %89, align 16
  %322 = trunc <2 x i64> %321 to <2 x i32>
  %323 = or i64 %264, 22
  %324 = getelementptr inbounds i32, i32* %1, i64 %323
  %325 = bitcast i32* %324 to <2 x i32>*
  store <2 x i32> %322, <2 x i32>* %325, align 4
  %326 = load <2 x i64>, <2 x i64>* %91, align 16
  %327 = trunc <2 x i64> %326 to <2 x i32>
  %328 = or i64 %264, 24
  %329 = getelementptr inbounds i32, i32* %1, i64 %328
  %330 = bitcast i32* %329 to <2 x i32>*
  store <2 x i32> %327, <2 x i32>* %330, align 4
  %331 = load <2 x i64>, <2 x i64>* %93, align 16
  %332 = trunc <2 x i64> %331 to <2 x i32>
  %333 = or i64 %264, 26
  %334 = getelementptr inbounds i32, i32* %1, i64 %333
  %335 = bitcast i32* %334 to <2 x i32>*
  store <2 x i32> %332, <2 x i32>* %335, align 4
  %336 = load <2 x i64>, <2 x i64>* %95, align 16
  %337 = trunc <2 x i64> %336 to <2 x i32>
  %338 = or i64 %264, 28
  %339 = getelementptr inbounds i32, i32* %1, i64 %338
  %340 = bitcast i32* %339 to <2 x i32>*
  store <2 x i32> %337, <2 x i32>* %340, align 4
  %341 = load <2 x i64>, <2 x i64>* %97, align 16
  %342 = trunc <2 x i64> %341 to <2 x i32>
  %343 = or i64 %264, 30
  %344 = getelementptr inbounds i32, i32* %1, i64 %343
  %345 = bitcast i32* %344 to <2 x i32>*
  store <2 x i32> %342, <2 x i32>* %345, align 4
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %64) #3
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %63) #3
  %346 = add nuw nsw i64 %263, 1
  %347 = icmp eq i64 %346, 32
  br i1 %347, label %348, label %262

348:                                              ; preds = %262
  call void @llvm.lifetime.end.p0i8(i64 8192, i8* nonnull %9) #3
  ret void
}

; Function Attrs: nofree norecurse nounwind ssp uwtable
define hidden void @vpx_fdct32x32_1_c(i16* nocapture readonly, i32* nocapture, i32) local_unnamed_addr #2 {
  %4 = sext i32 %2 to i64
  %5 = icmp eq i32 %2, 1
  br i1 %5, label %6, label %209

6:                                                ; preds = %3, %6
  %7 = phi i64 [ %201, %6 ], [ 0, %3 ]
  %8 = phi <4 x i32> [ %200, %6 ], [ zeroinitializer, %3 ]
  %9 = mul nsw i64 %7, %4
  %10 = getelementptr inbounds i16, i16* %0, i64 %9
  %11 = bitcast i16* %10 to <4 x i16>*
  %12 = load <4 x i16>, <4 x i16>* %11, align 2
  %13 = sext <4 x i16> %12 to <4 x i32>
  %14 = add nsw <4 x i32> %8, %13
  %15 = or i64 %9, 1
  %16 = getelementptr inbounds i16, i16* %0, i64 %15
  %17 = bitcast i16* %16 to <4 x i16>*
  %18 = load <4 x i16>, <4 x i16>* %17, align 2
  %19 = sext <4 x i16> %18 to <4 x i32>
  %20 = add nsw <4 x i32> %14, %19
  %21 = or i64 %9, 2
  %22 = getelementptr inbounds i16, i16* %0, i64 %21
  %23 = bitcast i16* %22 to <4 x i16>*
  %24 = load <4 x i16>, <4 x i16>* %23, align 2
  %25 = sext <4 x i16> %24 to <4 x i32>
  %26 = add nsw <4 x i32> %20, %25
  %27 = or i64 %9, 3
  %28 = getelementptr inbounds i16, i16* %0, i64 %27
  %29 = bitcast i16* %28 to <4 x i16>*
  %30 = load <4 x i16>, <4 x i16>* %29, align 2
  %31 = sext <4 x i16> %30 to <4 x i32>
  %32 = add nsw <4 x i32> %26, %31
  %33 = add nsw i64 %9, 4
  %34 = getelementptr inbounds i16, i16* %0, i64 %33
  %35 = bitcast i16* %34 to <4 x i16>*
  %36 = load <4 x i16>, <4 x i16>* %35, align 2
  %37 = sext <4 x i16> %36 to <4 x i32>
  %38 = add nsw <4 x i32> %32, %37
  %39 = add nsw i64 %9, 5
  %40 = getelementptr inbounds i16, i16* %0, i64 %39
  %41 = bitcast i16* %40 to <4 x i16>*
  %42 = load <4 x i16>, <4 x i16>* %41, align 2
  %43 = sext <4 x i16> %42 to <4 x i32>
  %44 = add nsw <4 x i32> %38, %43
  %45 = add nsw i64 %9, 6
  %46 = getelementptr inbounds i16, i16* %0, i64 %45
  %47 = bitcast i16* %46 to <4 x i16>*
  %48 = load <4 x i16>, <4 x i16>* %47, align 2
  %49 = sext <4 x i16> %48 to <4 x i32>
  %50 = add nsw <4 x i32> %44, %49
  %51 = add nsw i64 %9, 7
  %52 = getelementptr inbounds i16, i16* %0, i64 %51
  %53 = bitcast i16* %52 to <4 x i16>*
  %54 = load <4 x i16>, <4 x i16>* %53, align 2
  %55 = sext <4 x i16> %54 to <4 x i32>
  %56 = add nsw <4 x i32> %50, %55
  %57 = add nsw i64 %9, 8
  %58 = getelementptr inbounds i16, i16* %0, i64 %57
  %59 = bitcast i16* %58 to <4 x i16>*
  %60 = load <4 x i16>, <4 x i16>* %59, align 2
  %61 = sext <4 x i16> %60 to <4 x i32>
  %62 = add nsw <4 x i32> %56, %61
  %63 = add nsw i64 %9, 9
  %64 = getelementptr inbounds i16, i16* %0, i64 %63
  %65 = bitcast i16* %64 to <4 x i16>*
  %66 = load <4 x i16>, <4 x i16>* %65, align 2
  %67 = sext <4 x i16> %66 to <4 x i32>
  %68 = add nsw <4 x i32> %62, %67
  %69 = add nsw i64 %9, 10
  %70 = getelementptr inbounds i16, i16* %0, i64 %69
  %71 = bitcast i16* %70 to <4 x i16>*
  %72 = load <4 x i16>, <4 x i16>* %71, align 2
  %73 = sext <4 x i16> %72 to <4 x i32>
  %74 = add nsw <4 x i32> %68, %73
  %75 = add nsw i64 %9, 11
  %76 = getelementptr inbounds i16, i16* %0, i64 %75
  %77 = bitcast i16* %76 to <4 x i16>*
  %78 = load <4 x i16>, <4 x i16>* %77, align 2
  %79 = sext <4 x i16> %78 to <4 x i32>
  %80 = add nsw <4 x i32> %74, %79
  %81 = add nsw i64 %9, 12
  %82 = getelementptr inbounds i16, i16* %0, i64 %81
  %83 = bitcast i16* %82 to <4 x i16>*
  %84 = load <4 x i16>, <4 x i16>* %83, align 2
  %85 = sext <4 x i16> %84 to <4 x i32>
  %86 = add nsw <4 x i32> %80, %85
  %87 = add nsw i64 %9, 13
  %88 = getelementptr inbounds i16, i16* %0, i64 %87
  %89 = bitcast i16* %88 to <4 x i16>*
  %90 = load <4 x i16>, <4 x i16>* %89, align 2
  %91 = sext <4 x i16> %90 to <4 x i32>
  %92 = add nsw <4 x i32> %86, %91
  %93 = add nsw i64 %9, 14
  %94 = getelementptr inbounds i16, i16* %0, i64 %93
  %95 = bitcast i16* %94 to <4 x i16>*
  %96 = load <4 x i16>, <4 x i16>* %95, align 2
  %97 = sext <4 x i16> %96 to <4 x i32>
  %98 = add nsw <4 x i32> %92, %97
  %99 = add nsw i64 %9, 15
  %100 = getelementptr inbounds i16, i16* %0, i64 %99
  %101 = bitcast i16* %100 to <4 x i16>*
  %102 = load <4 x i16>, <4 x i16>* %101, align 2
  %103 = sext <4 x i16> %102 to <4 x i32>
  %104 = add nsw <4 x i32> %98, %103
  %105 = add nsw i64 %9, 16
  %106 = getelementptr inbounds i16, i16* %0, i64 %105
  %107 = bitcast i16* %106 to <4 x i16>*
  %108 = load <4 x i16>, <4 x i16>* %107, align 2
  %109 = sext <4 x i16> %108 to <4 x i32>
  %110 = add nsw <4 x i32> %104, %109
  %111 = add nsw i64 %9, 17
  %112 = getelementptr inbounds i16, i16* %0, i64 %111
  %113 = bitcast i16* %112 to <4 x i16>*
  %114 = load <4 x i16>, <4 x i16>* %113, align 2
  %115 = sext <4 x i16> %114 to <4 x i32>
  %116 = add nsw <4 x i32> %110, %115
  %117 = add nsw i64 %9, 18
  %118 = getelementptr inbounds i16, i16* %0, i64 %117
  %119 = bitcast i16* %118 to <4 x i16>*
  %120 = load <4 x i16>, <4 x i16>* %119, align 2
  %121 = sext <4 x i16> %120 to <4 x i32>
  %122 = add nsw <4 x i32> %116, %121
  %123 = add nsw i64 %9, 19
  %124 = getelementptr inbounds i16, i16* %0, i64 %123
  %125 = bitcast i16* %124 to <4 x i16>*
  %126 = load <4 x i16>, <4 x i16>* %125, align 2
  %127 = sext <4 x i16> %126 to <4 x i32>
  %128 = add nsw <4 x i32> %122, %127
  %129 = add nsw i64 %9, 20
  %130 = getelementptr inbounds i16, i16* %0, i64 %129
  %131 = bitcast i16* %130 to <4 x i16>*
  %132 = load <4 x i16>, <4 x i16>* %131, align 2
  %133 = sext <4 x i16> %132 to <4 x i32>
  %134 = add nsw <4 x i32> %128, %133
  %135 = add nsw i64 %9, 21
  %136 = getelementptr inbounds i16, i16* %0, i64 %135
  %137 = bitcast i16* %136 to <4 x i16>*
  %138 = load <4 x i16>, <4 x i16>* %137, align 2
  %139 = sext <4 x i16> %138 to <4 x i32>
  %140 = add nsw <4 x i32> %134, %139
  %141 = add nsw i64 %9, 22
  %142 = getelementptr inbounds i16, i16* %0, i64 %141
  %143 = bitcast i16* %142 to <4 x i16>*
  %144 = load <4 x i16>, <4 x i16>* %143, align 2
  %145 = sext <4 x i16> %144 to <4 x i32>
  %146 = add nsw <4 x i32> %140, %145
  %147 = add nsw i64 %9, 23
  %148 = getelementptr inbounds i16, i16* %0, i64 %147
  %149 = bitcast i16* %148 to <4 x i16>*
  %150 = load <4 x i16>, <4 x i16>* %149, align 2
  %151 = sext <4 x i16> %150 to <4 x i32>
  %152 = add nsw <4 x i32> %146, %151
  %153 = add nsw i64 %9, 24
  %154 = getelementptr inbounds i16, i16* %0, i64 %153
  %155 = bitcast i16* %154 to <4 x i16>*
  %156 = load <4 x i16>, <4 x i16>* %155, align 2
  %157 = sext <4 x i16> %156 to <4 x i32>
  %158 = add nsw <4 x i32> %152, %157
  %159 = add nsw i64 %9, 25
  %160 = getelementptr inbounds i16, i16* %0, i64 %159
  %161 = bitcast i16* %160 to <4 x i16>*
  %162 = load <4 x i16>, <4 x i16>* %161, align 2
  %163 = sext <4 x i16> %162 to <4 x i32>
  %164 = add nsw <4 x i32> %158, %163
  %165 = add nsw i64 %9, 26
  %166 = getelementptr inbounds i16, i16* %0, i64 %165
  %167 = bitcast i16* %166 to <4 x i16>*
  %168 = load <4 x i16>, <4 x i16>* %167, align 2
  %169 = sext <4 x i16> %168 to <4 x i32>
  %170 = add nsw <4 x i32> %164, %169
  %171 = add nsw i64 %9, 27
  %172 = getelementptr inbounds i16, i16* %0, i64 %171
  %173 = bitcast i16* %172 to <4 x i16>*
  %174 = load <4 x i16>, <4 x i16>* %173, align 2
  %175 = sext <4 x i16> %174 to <4 x i32>
  %176 = add nsw <4 x i32> %170, %175
  %177 = add nsw i64 %9, 28
  %178 = getelementptr inbounds i16, i16* %0, i64 %177
  %179 = bitcast i16* %178 to <4 x i16>*
  %180 = load <4 x i16>, <4 x i16>* %179, align 2
  %181 = sext <4 x i16> %180 to <4 x i32>
  %182 = add nsw <4 x i32> %176, %181
  %183 = add nsw i64 %9, 29
  %184 = getelementptr inbounds i16, i16* %0, i64 %183
  %185 = bitcast i16* %184 to <4 x i16>*
  %186 = load <4 x i16>, <4 x i16>* %185, align 2
  %187 = sext <4 x i16> %186 to <4 x i32>
  %188 = add nsw <4 x i32> %182, %187
  %189 = add nsw i64 %9, 30
  %190 = getelementptr inbounds i16, i16* %0, i64 %189
  %191 = bitcast i16* %190 to <4 x i16>*
  %192 = load <4 x i16>, <4 x i16>* %191, align 2
  %193 = sext <4 x i16> %192 to <4 x i32>
  %194 = add nsw <4 x i32> %188, %193
  %195 = add nsw i64 %9, 31
  %196 = getelementptr inbounds i16, i16* %0, i64 %195
  %197 = bitcast i16* %196 to <4 x i16>*
  %198 = load <4 x i16>, <4 x i16>* %197, align 2
  %199 = sext <4 x i16> %198 to <4 x i32>
  %200 = add nsw <4 x i32> %194, %199
  %201 = add i64 %7, 4
  %202 = icmp eq i64 %201, 32
  br i1 %202, label %203, label %6, !llvm.loop !7

203:                                              ; preds = %6
  %204 = shufflevector <4 x i32> %200, <4 x i32> undef, <4 x i32> <i32 2, i32 3, i32 undef, i32 undef>
  %205 = add <4 x i32> %200, %204
  %206 = shufflevector <4 x i32> %205, <4 x i32> undef, <4 x i32> <i32 1, i32 undef, i32 undef, i32 undef>
  %207 = add <4 x i32> %205, %206
  %208 = extractelement <4 x i32> %207, i32 0
  br label %231

209:                                              ; preds = %3, %209
  %210 = phi i64 [ %229, %209 ], [ 0, %3 ]
  %211 = phi i32 [ %228, %209 ], [ 0, %3 ]
  %212 = mul nsw i64 %210, %4
  %213 = getelementptr inbounds i16, i16* %0, i64 %212
  %214 = bitcast i16* %213 to <32 x i16>*
  %215 = load <32 x i16>, <32 x i16>* %214, align 2
  %216 = sext <32 x i16> %215 to <32 x i32>
  %217 = shufflevector <32 x i32> %216, <32 x i32> undef, <32 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %218 = add nsw <32 x i32> %217, %216
  %219 = shufflevector <32 x i32> %218, <32 x i32> undef, <32 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %220 = add nsw <32 x i32> %218, %219
  %221 = shufflevector <32 x i32> %220, <32 x i32> undef, <32 x i32> <i32 4, i32 5, i32 6, i32 7, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %222 = add nsw <32 x i32> %220, %221
  %223 = shufflevector <32 x i32> %222, <32 x i32> undef, <32 x i32> <i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %224 = add nsw <32 x i32> %222, %223
  %225 = shufflevector <32 x i32> %224, <32 x i32> undef, <32 x i32> <i32 1, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %226 = add nsw <32 x i32> %224, %225
  %227 = extractelement <32 x i32> %226, i32 0
  %228 = add nsw i32 %227, %211
  %229 = add nuw nsw i64 %210, 1
  %230 = icmp eq i64 %229, 32
  br i1 %230, label %231, label %209, !llvm.loop !8

231:                                              ; preds = %209, %203
  %232 = phi i32 [ %208, %203 ], [ %228, %209 ]
  %233 = ashr i32 %232, 3
  store i32 %233, i32* %1, align 4
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @vpx_highbd_fdct4x4_c(i16* nocapture readonly, i32* nocapture, i32) local_unnamed_addr #0 {
  tail call void @vpx_fdct4x4_c(i16* %0, i32* %1, i32 %2)
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @vpx_highbd_fdct8x8_c(i16* nocapture readonly, i32* nocapture, i32) local_unnamed_addr #0 {
  tail call void @vpx_fdct8x8_c(i16* %0, i32* %1, i32 %2)
  ret void
}

; Function Attrs: nofree norecurse nounwind ssp uwtable
define hidden void @vpx_highbd_fdct8x8_1_c(i16* nocapture readonly, i32* nocapture, i32) local_unnamed_addr #2 {
  %4 = sext i32 %2 to i64
  %5 = bitcast i16* %0 to <8 x i16>*
  %6 = load <8 x i16>, <8 x i16>* %5, align 2
  %7 = sext <8 x i16> %6 to <8 x i32>
  %8 = shufflevector <8 x i32> %7, <8 x i32> undef, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 undef, i32 undef, i32 undef, i32 undef>
  %9 = add nsw <8 x i32> %8, %7
  %10 = shufflevector <8 x i32> %9, <8 x i32> undef, <8 x i32> <i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %11 = add nsw <8 x i32> %9, %10
  %12 = shufflevector <8 x i32> %11, <8 x i32> undef, <8 x i32> <i32 1, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %13 = add nsw <8 x i32> %11, %12
  %14 = extractelement <8 x i32> %13, i32 0
  %15 = getelementptr inbounds i16, i16* %0, i64 %4
  %16 = bitcast i16* %15 to <8 x i16>*
  %17 = load <8 x i16>, <8 x i16>* %16, align 2
  %18 = sext <8 x i16> %17 to <8 x i32>
  %19 = shufflevector <8 x i32> %18, <8 x i32> undef, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 undef, i32 undef, i32 undef, i32 undef>
  %20 = add nsw <8 x i32> %19, %18
  %21 = shufflevector <8 x i32> %20, <8 x i32> undef, <8 x i32> <i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %22 = add nsw <8 x i32> %20, %21
  %23 = shufflevector <8 x i32> %22, <8 x i32> undef, <8 x i32> <i32 1, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %24 = add nsw <8 x i32> %22, %23
  %25 = extractelement <8 x i32> %24, i32 0
  %26 = add nsw i32 %25, %14
  %27 = shl nsw i64 %4, 1
  %28 = getelementptr inbounds i16, i16* %0, i64 %27
  %29 = bitcast i16* %28 to <8 x i16>*
  %30 = load <8 x i16>, <8 x i16>* %29, align 2
  %31 = sext <8 x i16> %30 to <8 x i32>
  %32 = shufflevector <8 x i32> %31, <8 x i32> undef, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 undef, i32 undef, i32 undef, i32 undef>
  %33 = add nsw <8 x i32> %32, %31
  %34 = shufflevector <8 x i32> %33, <8 x i32> undef, <8 x i32> <i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %35 = add nsw <8 x i32> %33, %34
  %36 = shufflevector <8 x i32> %35, <8 x i32> undef, <8 x i32> <i32 1, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %37 = add nsw <8 x i32> %35, %36
  %38 = extractelement <8 x i32> %37, i32 0
  %39 = add nsw i32 %38, %26
  %40 = mul nsw i64 %4, 3
  %41 = getelementptr inbounds i16, i16* %0, i64 %40
  %42 = bitcast i16* %41 to <8 x i16>*
  %43 = load <8 x i16>, <8 x i16>* %42, align 2
  %44 = sext <8 x i16> %43 to <8 x i32>
  %45 = shufflevector <8 x i32> %44, <8 x i32> undef, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 undef, i32 undef, i32 undef, i32 undef>
  %46 = add nsw <8 x i32> %45, %44
  %47 = shufflevector <8 x i32> %46, <8 x i32> undef, <8 x i32> <i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %48 = add nsw <8 x i32> %46, %47
  %49 = shufflevector <8 x i32> %48, <8 x i32> undef, <8 x i32> <i32 1, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %50 = add nsw <8 x i32> %48, %49
  %51 = extractelement <8 x i32> %50, i32 0
  %52 = add nsw i32 %51, %39
  %53 = shl nsw i64 %4, 2
  %54 = getelementptr inbounds i16, i16* %0, i64 %53
  %55 = bitcast i16* %54 to <8 x i16>*
  %56 = load <8 x i16>, <8 x i16>* %55, align 2
  %57 = sext <8 x i16> %56 to <8 x i32>
  %58 = shufflevector <8 x i32> %57, <8 x i32> undef, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 undef, i32 undef, i32 undef, i32 undef>
  %59 = add nsw <8 x i32> %58, %57
  %60 = shufflevector <8 x i32> %59, <8 x i32> undef, <8 x i32> <i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %61 = add nsw <8 x i32> %59, %60
  %62 = shufflevector <8 x i32> %61, <8 x i32> undef, <8 x i32> <i32 1, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %63 = add nsw <8 x i32> %61, %62
  %64 = extractelement <8 x i32> %63, i32 0
  %65 = add nsw i32 %64, %52
  %66 = mul nsw i64 %4, 5
  %67 = getelementptr inbounds i16, i16* %0, i64 %66
  %68 = bitcast i16* %67 to <8 x i16>*
  %69 = load <8 x i16>, <8 x i16>* %68, align 2
  %70 = sext <8 x i16> %69 to <8 x i32>
  %71 = shufflevector <8 x i32> %70, <8 x i32> undef, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 undef, i32 undef, i32 undef, i32 undef>
  %72 = add nsw <8 x i32> %71, %70
  %73 = shufflevector <8 x i32> %72, <8 x i32> undef, <8 x i32> <i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %74 = add nsw <8 x i32> %72, %73
  %75 = shufflevector <8 x i32> %74, <8 x i32> undef, <8 x i32> <i32 1, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %76 = add nsw <8 x i32> %74, %75
  %77 = extractelement <8 x i32> %76, i32 0
  %78 = add nsw i32 %77, %65
  %79 = mul nsw i64 %4, 6
  %80 = getelementptr inbounds i16, i16* %0, i64 %79
  %81 = bitcast i16* %80 to <8 x i16>*
  %82 = load <8 x i16>, <8 x i16>* %81, align 2
  %83 = sext <8 x i16> %82 to <8 x i32>
  %84 = shufflevector <8 x i32> %83, <8 x i32> undef, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 undef, i32 undef, i32 undef, i32 undef>
  %85 = add nsw <8 x i32> %84, %83
  %86 = shufflevector <8 x i32> %85, <8 x i32> undef, <8 x i32> <i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %87 = add nsw <8 x i32> %85, %86
  %88 = shufflevector <8 x i32> %87, <8 x i32> undef, <8 x i32> <i32 1, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %89 = add nsw <8 x i32> %87, %88
  %90 = extractelement <8 x i32> %89, i32 0
  %91 = add nsw i32 %90, %78
  %92 = mul nsw i64 %4, 7
  %93 = getelementptr inbounds i16, i16* %0, i64 %92
  %94 = bitcast i16* %93 to <8 x i16>*
  %95 = load <8 x i16>, <8 x i16>* %94, align 2
  %96 = sext <8 x i16> %95 to <8 x i32>
  %97 = shufflevector <8 x i32> %96, <8 x i32> undef, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 undef, i32 undef, i32 undef, i32 undef>
  %98 = add nsw <8 x i32> %97, %96
  %99 = shufflevector <8 x i32> %98, <8 x i32> undef, <8 x i32> <i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %100 = add nsw <8 x i32> %98, %99
  %101 = shufflevector <8 x i32> %100, <8 x i32> undef, <8 x i32> <i32 1, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %102 = add nsw <8 x i32> %100, %101
  %103 = extractelement <8 x i32> %102, i32 0
  %104 = add nsw i32 %103, %91
  store i32 %104, i32* %1, align 4
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @vpx_highbd_fdct16x16_c(i16* nocapture readonly, i32* nocapture, i32) local_unnamed_addr #0 {
  tail call void @vpx_fdct16x16_c(i16* %0, i32* %1, i32 %2)
  ret void
}

; Function Attrs: nofree norecurse nounwind ssp uwtable
define hidden void @vpx_highbd_fdct16x16_1_c(i16* nocapture readonly, i32* nocapture, i32) local_unnamed_addr #2 {
  tail call void @vpx_fdct16x16_1_c(i16* %0, i32* %1, i32 %2)
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @vpx_highbd_fdct32x32_c(i16* nocapture readonly, i32* nocapture, i32) local_unnamed_addr #0 {
  tail call void @vpx_fdct32x32_c(i16* %0, i32* %1, i32 %2)
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @vpx_highbd_fdct32x32_rd_c(i16* nocapture readonly, i32* nocapture, i32) local_unnamed_addr #0 {
  tail call void @vpx_fdct32x32_rd_c(i16* %0, i32* %1, i32 %2)
  ret void
}

; Function Attrs: nofree norecurse nounwind ssp uwtable
define hidden void @vpx_highbd_fdct32x32_1_c(i16* nocapture readonly, i32* nocapture, i32) local_unnamed_addr #2 {
  tail call void @vpx_fdct32x32_1_c(i16* %0, i32* %1, i32 %2)
  ret void
}

; Function Attrs: argmemonly nounwind
declare void @llvm.memcpy.p0i8.p0i8.i64(i8* nocapture writeonly, i8* nocapture readonly, i64, i1 immarg) #1

attributes #0 = { nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { argmemonly nounwind }
attributes #2 = { nofree norecurse nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #3 = { nounwind }

!llvm.module.flags = !{!0, !1}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 2}
!2 = distinct !{!2, !3}
!3 = !{!"llvm.loop.isvectorized", i32 1}
!4 = distinct !{!4, !3}
!5 = distinct !{!5, !3}
!6 = distinct !{!6, !3}
!7 = distinct !{!7, !3}
!8 = distinct !{!8, !3}
