; ModuleID = '../../third_party/dawn/src/dawn_native/vulkan/UtilsVulkan.cpp'
source_filename = "../../third_party/dawn/src/dawn_native/vulkan/UtilsVulkan.cpp"
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

%"class.std::__1::__bit_iterator" = type <{ i64*, i32, [4 x i8] }>
%"class.BitSetIterator<6, unsigned char>::Iterator" = type { %"class.std::__1::bitset", i64, i64 }
%"class.std::__1::bitset" = type { %"class.std::__1::__bitset" }
%"class.std::__1::__bitset" = type { i64 }
%"class.dawn_native::EnumMaskIterator<dawn_native::Aspect>::Iterator" = type { %"class.BitSetIterator<6, unsigned char>::Iterator" }
%"struct.dawn_native::TextureCopy" = type <{ %class.Ref, i32, %"struct.dawn_native::Origin3D", i8, [7 x i8] }>
%class.Ref = type { %class.RefBase }
%class.RefBase = type { %"class.dawn_native::TextureBase"* }
%"class.dawn_native::TextureBase" = type { %"class.dawn_native::ObjectBase", i32, %"struct.dawn_native::Format"*, %"struct.dawn_native::Extent3D", i32, i32, i32, i32, %"class.std::__1::vector" }
%"class.dawn_native::ObjectBase" = type { %class.RefCounted, %"class.dawn_native::DeviceBase"* }
%class.RefCounted = type { i32 (...)**, %"struct.std::__1::atomic" }
%"struct.std::__1::atomic" = type { %"struct.std::__1::__atomic_base" }
%"struct.std::__1::__atomic_base" = type { %"struct.std::__1::__atomic_base.0" }
%"struct.std::__1::__atomic_base.0" = type { %"struct.std::__1::__cxx_atomic_impl" }
%"struct.std::__1::__cxx_atomic_impl" = type { %"struct.std::__1::__cxx_atomic_base_impl" }
%"struct.std::__1::__cxx_atomic_base_impl" = type { i64 }
%"class.dawn_native::DeviceBase" = type opaque
%"struct.dawn_native::Format" = type { i32, i8, i8, i8, i8, i8, %"struct.std::__1::array" }
%"struct.std::__1::array" = type { [3 x %"struct.dawn_native::AspectInfo"] }
%"struct.dawn_native::AspectInfo" = type { %"struct.dawn_native::TexelBlockInfo", i32, i8, i32 }
%"struct.dawn_native::TexelBlockInfo" = type { i32, i32, i32 }
%"struct.dawn_native::Extent3D" = type { i32, i32, i32 }
%"class.std::__1::vector" = type { i64*, i64, %"class.std::__1::__compressed_pair" }
%"class.std::__1::__compressed_pair" = type { %"struct.std::__1::__compressed_pair_elem" }
%"struct.std::__1::__compressed_pair_elem" = type { i64 }
%"struct.dawn_native::Origin3D" = type { i32, i32, i32 }
%struct.VkBufferImageCopy = type { i64, i32, i32, %struct.VkImageSubresourceLayers, %struct.VkOffset3D, %struct.VkExtent3D }
%struct.VkImageSubresourceLayers = type { i32, i32, i32, i32 }
%struct.VkOffset3D = type { i32, i32, i32 }
%struct.VkExtent3D = type { i32, i32, i32 }
%"struct.dawn_native::BufferCopy" = type { %class.Ref.2, i64, i32, i32 }
%class.Ref.2 = type { %class.RefBase.3 }
%class.RefBase.3 = type { %"class.dawn_native::BufferBase"* }
%"class.dawn_native::BufferBase" = type opaque
%"struct.dawn_native::TextureDataLayout" = type { %"struct.dawn_native::ChainedStruct"*, i64, i32, i32 }
%"struct.dawn_native::ChainedStruct" = type <{ %"struct.dawn_native::ChainedStruct"*, i32, [4 x i8] }>
%"class.dawn_native::vulkan::Texture" = type { %"class.dawn_native::TextureBase", %"class.dawn_native::vulkan::detail::VkHandle", %"class.dawn_native::ResourceMemoryAllocation", %"class.dawn_native::vulkan::detail::VkHandle.4", i32, i32, i32, i32, %"class.dawn_native::vulkan::detail::VkHandle.5", %"class.std::__1::vector.6", %"class.dawn_native::SubresourceStorage" }
%"class.dawn_native::vulkan::detail::VkHandle" = type { %struct.VkImage_T* }
%struct.VkImage_T = type opaque
%"class.dawn_native::ResourceMemoryAllocation" = type { i32 (...)**, %"struct.dawn_native::AllocationInfo", i64, %"class.dawn_native::ResourceHeapBase"*, i8* }
%"struct.dawn_native::AllocationInfo" = type <{ i64, i32, [4 x i8] }>
%"class.dawn_native::ResourceHeapBase" = type opaque
%"class.dawn_native::vulkan::detail::VkHandle.4" = type { %struct.VkDeviceMemory_T* }
%struct.VkDeviceMemory_T = type opaque
%"class.dawn_native::vulkan::detail::VkHandle.5" = type { %struct.VkSemaphore_T* }
%struct.VkSemaphore_T = type opaque
%"class.std::__1::vector.6" = type { %"class.std::__1::__vector_base" }
%"class.std::__1::__vector_base" = type { %"class.dawn_native::vulkan::detail::VkHandle.5"*, %"class.dawn_native::vulkan::detail::VkHandle.5"*, %"class.std::__1::__compressed_pair.7" }
%"class.std::__1::__compressed_pair.7" = type { %"struct.std::__1::__compressed_pair_elem.8" }
%"struct.std::__1::__compressed_pair_elem.8" = type { %"class.dawn_native::vulkan::detail::VkHandle.5"* }
%"class.dawn_native::SubresourceStorage" = type { i8, i8, i16, %"struct.std::__1::array.12", %"struct.std::__1::array.13", %"class.std::__1::unique_ptr", %"class.std::__1::unique_ptr.17" }
%"struct.std::__1::array.12" = type { [2 x i8] }
%"struct.std::__1::array.13" = type { [2 x i32] }
%"class.std::__1::unique_ptr" = type { %"class.std::__1::__compressed_pair.14" }
%"class.std::__1::__compressed_pair.14" = type { %"struct.std::__1::__compressed_pair_elem.15" }
%"struct.std::__1::__compressed_pair_elem.15" = type { i8* }
%"class.std::__1::unique_ptr.17" = type { %"class.std::__1::__compressed_pair.18" }
%"class.std::__1::__compressed_pair.18" = type { %"struct.std::__1::__compressed_pair_elem.19" }
%"struct.std::__1::__compressed_pair_elem.19" = type { i32* }

$_ZNSt3__116__copy_unalignedINS_8__bitsetILm1ELm6EEELb0EEENS_14__bit_iteratorIT_Lb0EXLi0EEEENS3_IS4_XT0_EXLi0EEEES6_S5_ = comdat any

$_ZZN14BitSetIteratorILm6EhE8Iterator10getNextBitEvE8wordMask = comdat any

@_ZZN14BitSetIteratorILm6EhE8Iterator10getNextBitEvE8wordMask = linkonce_odr hidden local_unnamed_addr global { i64 } { i64 63 }, comdat, align 8
@switch.table._ZN11dawn_native6vulkan17ToVulkanCompareOpEN4wgpu15CompareFunctionE = private unnamed_addr constant [8 x i32] [i32 0, i32 1, i32 3, i32 4, i32 6, i32 2, i32 5, i32 7], align 4

; Function Attrs: norecurse nounwind readnone ssp uwtable
define hidden i32 @_ZN11dawn_native6vulkan17ToVulkanCompareOpEN4wgpu15CompareFunctionE(i32) local_unnamed_addr #0 {
  %2 = add i32 %0, -1
  %3 = sext i32 %2 to i64
  %4 = getelementptr inbounds [8 x i32], [8 x i32]* @switch.table._ZN11dawn_native6vulkan17ToVulkanCompareOpEN4wgpu15CompareFunctionE, i64 0, i64 %3
  %5 = load i32, i32* %4, align 4
  ret i32 %5
}

; Function Attrs: nounwind ssp uwtable
define hidden i32 @_ZN11dawn_native6vulkan16VulkanAspectMaskERKNS_6AspectE(i8* nocapture readonly dereferenceable(1)) local_unnamed_addr #1 {
  %2 = alloca %"class.std::__1::__bit_iterator", align 8
  %3 = alloca %"class.std::__1::__bit_iterator", align 8
  %4 = alloca %"class.std::__1::__bit_iterator", align 8
  %5 = alloca %"class.std::__1::__bit_iterator", align 8
  %6 = alloca %"class.std::__1::__bit_iterator", align 8
  %7 = alloca %"class.std::__1::__bit_iterator", align 8
  %8 = alloca %"class.std::__1::__bit_iterator", align 8
  %9 = alloca %"class.std::__1::__bit_iterator", align 8
  %10 = alloca %"class.BitSetIterator<6, unsigned char>::Iterator", align 8
  %11 = alloca %"class.dawn_native::EnumMaskIterator<dawn_native::Aspect>::Iterator", align 8
  %12 = load i8, i8* %0, align 1, !noalias !2
  %13 = and i8 %12, 63
  %14 = zext i8 %13 to i64
  %15 = bitcast %"class.dawn_native::EnumMaskIterator<dawn_native::Aspect>::Iterator"* %11 to i8*
  call void @llvm.lifetime.start.p0i8(i64 24, i8* nonnull %15) #5
  %16 = getelementptr inbounds %"class.dawn_native::EnumMaskIterator<dawn_native::Aspect>::Iterator", %"class.dawn_native::EnumMaskIterator<dawn_native::Aspect>::Iterator"* %11, i64 0, i32 0, i32 0, i32 0, i32 0
  %17 = getelementptr inbounds %"class.dawn_native::EnumMaskIterator<dawn_native::Aspect>::Iterator", %"class.dawn_native::EnumMaskIterator<dawn_native::Aspect>::Iterator"* %11, i64 0, i32 0, i32 1
  %18 = getelementptr inbounds %"class.dawn_native::EnumMaskIterator<dawn_native::Aspect>::Iterator", %"class.dawn_native::EnumMaskIterator<dawn_native::Aspect>::Iterator"* %11, i64 0, i32 0, i32 2
  %19 = bitcast %"class.BitSetIterator<6, unsigned char>::Iterator"* %10 to i8*
  %20 = bitcast %"class.dawn_native::EnumMaskIterator<dawn_native::Aspect>::Iterator"* %11 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %20, i8 -86, i64 24, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 24, i8* nonnull %19) #5, !noalias !5
  %21 = getelementptr inbounds %"class.BitSetIterator<6, unsigned char>::Iterator", %"class.BitSetIterator<6, unsigned char>::Iterator"* %10, i64 0, i32 0, i32 0, i32 0
  store i64 %14, i64* %21, align 8, !alias.scope !8, !noalias !5
  %22 = getelementptr inbounds %"class.BitSetIterator<6, unsigned char>::Iterator", %"class.BitSetIterator<6, unsigned char>::Iterator"* %10, i64 0, i32 1
  %23 = bitcast i64* %22 to i8*
  call void @llvm.memset.p0i8.i64(i8* align 8 %23, i8 0, i64 16, i1 false) #5, !alias.scope !8, !noalias !5
  %24 = icmp eq i8 %13, 0
  %25 = getelementptr inbounds %"class.BitSetIterator<6, unsigned char>::Iterator", %"class.BitSetIterator<6, unsigned char>::Iterator"* %10, i64 0, i32 2
  br i1 %24, label %58, label %26

26:                                               ; preds = %1
  %27 = ptrtoint %"class.BitSetIterator<6, unsigned char>::Iterator"* %10 to i64
  %28 = bitcast %"class.std::__1::__bit_iterator"* %9 to i8*
  %29 = bitcast %"class.std::__1::__bit_iterator"* %6 to i8*
  %30 = bitcast %"class.std::__1::__bit_iterator"* %7 to i8*
  %31 = bitcast %"class.std::__1::__bit_iterator"* %8 to i8*
  %32 = bitcast %"class.std::__1::__bit_iterator"* %6 to i64*
  %33 = getelementptr inbounds %"class.std::__1::__bit_iterator", %"class.std::__1::__bit_iterator"* %6, i64 0, i32 1
  %34 = bitcast %"class.std::__1::__bit_iterator"* %7 to i64*
  %35 = getelementptr inbounds %"class.std::__1::__bit_iterator", %"class.std::__1::__bit_iterator"* %7, i64 0, i32 1
  %36 = bitcast %"class.std::__1::__bit_iterator"* %8 to i64*
  %37 = getelementptr inbounds %"class.std::__1::__bit_iterator", %"class.std::__1::__bit_iterator"* %8, i64 0, i32 1
  br label %38

38:                                               ; preds = %50, %26
  %39 = phi i64 [ %14, %26 ], [ %52, %50 ]
  %40 = load i64, i64* getelementptr inbounds ({ i64 }, { i64 }* @_ZZN14BitSetIteratorILm6EhE8Iterator10getNextBitEvE8wordMask, i64 0, i32 0), align 8, !noalias !5
  %41 = and i64 %40, %39
  %42 = and i64 %41, 4294967295
  %43 = icmp eq i64 %42, 0
  br i1 %43, label %50, label %44

44:                                               ; preds = %38
  %45 = trunc i64 %41 to i32
  %46 = call i32 @_Z11ScanForwardj(i32 %45) #5, !noalias !5
  %47 = zext i32 %46 to i64
  %48 = load i64, i64* %25, align 8, !noalias !5
  %49 = add i64 %48, %47
  br label %56

50:                                               ; preds = %38
  call void @llvm.lifetime.start.p0i8(i64 16, i8* nonnull %28) #5, !noalias !5
  call void @llvm.lifetime.start.p0i8(i64 16, i8* nonnull %29) #5, !noalias !5
  call void @llvm.lifetime.start.p0i8(i64 16, i8* nonnull %30) #5, !noalias !5
  call void @llvm.lifetime.start.p0i8(i64 16, i8* nonnull %31) #5, !noalias !5
  store i64 %27, i64* %32, align 8, !noalias !11
  store i32 6, i32* %33, align 8, !noalias !11
  store i64 %27, i64* %34, align 8, !noalias !11
  store i32 6, i32* %35, align 8, !noalias !11
  store i64 %27, i64* %36, align 8, !noalias !11
  store i32 0, i32* %37, align 8, !noalias !11
  call void @_ZNSt3__116__copy_unalignedINS_8__bitsetILm1ELm6EEELb0EEENS_14__bit_iteratorIT_Lb0EXLi0EEEENS3_IS4_XT0_EXLi0EEEES6_S5_(%"class.std::__1::__bit_iterator"* nonnull sret %9, %"class.std::__1::__bit_iterator"* nonnull %6, %"class.std::__1::__bit_iterator"* nonnull %7, %"class.std::__1::__bit_iterator"* nonnull %8) #5, !noalias !5
  call void @llvm.lifetime.end.p0i8(i64 16, i8* nonnull %29) #5, !noalias !5
  call void @llvm.lifetime.end.p0i8(i64 16, i8* nonnull %30) #5, !noalias !5
  call void @llvm.lifetime.end.p0i8(i64 16, i8* nonnull %31) #5, !noalias !5
  call void @llvm.lifetime.end.p0i8(i64 16, i8* nonnull %28) #5, !noalias !5
  %51 = load i64, i64* %21, align 8, !noalias !5
  %52 = and i64 %51, -64
  store i64 %52, i64* %21, align 8, !noalias !5
  %53 = load i64, i64* %25, align 8, !noalias !5
  %54 = add i64 %53, 32
  store i64 %54, i64* %25, align 8, !noalias !5
  %55 = icmp ult i64 %54, 6
  br i1 %55, label %38, label %56

56:                                               ; preds = %50, %44
  %57 = phi i64 [ %49, %44 ], [ 0, %50 ]
  store i64 %57, i64* %22, align 8, !alias.scope !8, !noalias !5
  br label %59

58:                                               ; preds = %1
  store i64 32, i64* %25, align 8, !alias.scope !8, !noalias !5
  br label %59

59:                                               ; preds = %56, %58
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 8 %15, i8* nonnull align 8 %19, i64 24, i1 false) #5
  call void @llvm.lifetime.end.p0i8(i64 24, i8* nonnull %19) #5, !noalias !5
  %60 = ptrtoint %"class.dawn_native::EnumMaskIterator<dawn_native::Aspect>::Iterator"* %11 to i64
  %61 = bitcast %"class.std::__1::__bit_iterator"* %5 to i8*
  %62 = bitcast %"class.std::__1::__bit_iterator"* %2 to i8*
  %63 = bitcast %"class.std::__1::__bit_iterator"* %3 to i8*
  %64 = bitcast %"class.std::__1::__bit_iterator"* %4 to i8*
  %65 = bitcast %"class.std::__1::__bit_iterator"* %2 to i64*
  %66 = getelementptr inbounds %"class.std::__1::__bit_iterator", %"class.std::__1::__bit_iterator"* %2, i64 0, i32 1
  %67 = bitcast %"class.std::__1::__bit_iterator"* %3 to i64*
  %68 = getelementptr inbounds %"class.std::__1::__bit_iterator", %"class.std::__1::__bit_iterator"* %3, i64 0, i32 1
  %69 = bitcast %"class.std::__1::__bit_iterator"* %4 to i64*
  %70 = getelementptr inbounds %"class.std::__1::__bit_iterator", %"class.std::__1::__bit_iterator"* %4, i64 0, i32 1
  %71 = load i64, i64* %18, align 8
  br label %72

72:                                               ; preds = %124, %59
  %73 = phi i64 [ %71, %59 ], [ %125, %124 ]
  %74 = phi i32 [ 0, %59 ], [ %96, %124 ]
  %75 = icmp eq i64 %73, 32
  br i1 %75, label %76, label %81

76:                                               ; preds = %72
  %77 = load i64, i64* %16, align 8
  %78 = and i64 %77, 63
  %79 = icmp eq i64 %78, 0
  br i1 %79, label %80, label %81

80:                                               ; preds = %76
  call void @llvm.lifetime.end.p0i8(i64 24, i8* nonnull %15) #5
  ret i32 %74

81:                                               ; preds = %76, %72
  %82 = load i64, i64* %17, align 8
  %83 = trunc i64 %82 to i32
  %84 = and i32 %83, 255
  %85 = shl i32 1, %84
  %86 = trunc i32 %85 to i8
  switch i8 %86, label %95 [
    i8 1, label %87
    i8 2, label %89
    i8 4, label %91
    i8 32, label %93
  ]

87:                                               ; preds = %81
  %88 = or i32 %74, 1
  br label %95

89:                                               ; preds = %81
  %90 = or i32 %74, 2
  br label %95

91:                                               ; preds = %81
  %92 = or i32 %74, 4
  br label %95

93:                                               ; preds = %81
  %94 = or i32 %74, 6
  br label %95

95:                                               ; preds = %81, %93, %91, %89, %87
  %96 = phi i32 [ %74, %81 ], [ %94, %93 ], [ %92, %91 ], [ %90, %89 ], [ %88, %87 ]
  %97 = sub i64 %82, %73
  %98 = icmp ugt i64 %97, 5
  br i1 %98, label %99, label %100

99:                                               ; preds = %95
  call void @abort() #6
  unreachable

100:                                              ; preds = %95
  %101 = shl i64 1, %97
  %102 = xor i64 %101, -1
  %103 = load i64, i64* %16, align 8
  %104 = and i64 %103, %102
  store i64 %104, i64* %16, align 8
  %105 = icmp ult i64 %73, 6
  br i1 %105, label %106, label %124

106:                                              ; preds = %100, %118
  %107 = phi i64 [ %120, %118 ], [ %104, %100 ]
  %108 = load i64, i64* getelementptr inbounds ({ i64 }, { i64 }* @_ZZN14BitSetIteratorILm6EhE8Iterator10getNextBitEvE8wordMask, i64 0, i32 0), align 8
  %109 = and i64 %108, %107
  %110 = and i64 %109, 4294967295
  %111 = icmp eq i64 %110, 0
  br i1 %111, label %118, label %112

112:                                              ; preds = %106
  %113 = trunc i64 %109 to i32
  %114 = call i32 @_Z11ScanForwardj(i32 %113) #5
  %115 = zext i32 %114 to i64
  %116 = load i64, i64* %18, align 8
  %117 = add i64 %116, %115
  br label %124

118:                                              ; preds = %106
  call void @llvm.lifetime.start.p0i8(i64 16, i8* nonnull %61) #5
  call void @llvm.lifetime.start.p0i8(i64 16, i8* nonnull %62) #5
  call void @llvm.lifetime.start.p0i8(i64 16, i8* nonnull %63) #5
  call void @llvm.lifetime.start.p0i8(i64 16, i8* nonnull %64) #5
  store i64 %60, i64* %65, align 8, !noalias !14
  store i32 6, i32* %66, align 8, !noalias !14
  store i64 %60, i64* %67, align 8, !noalias !14
  store i32 6, i32* %68, align 8, !noalias !14
  store i64 %60, i64* %69, align 8, !noalias !14
  store i32 0, i32* %70, align 8, !noalias !14
  call void @_ZNSt3__116__copy_unalignedINS_8__bitsetILm1ELm6EEELb0EEENS_14__bit_iteratorIT_Lb0EXLi0EEEENS3_IS4_XT0_EXLi0EEEES6_S5_(%"class.std::__1::__bit_iterator"* nonnull sret %5, %"class.std::__1::__bit_iterator"* nonnull %2, %"class.std::__1::__bit_iterator"* nonnull %3, %"class.std::__1::__bit_iterator"* nonnull %4) #5
  call void @llvm.lifetime.end.p0i8(i64 16, i8* nonnull %62) #5
  call void @llvm.lifetime.end.p0i8(i64 16, i8* nonnull %63) #5
  call void @llvm.lifetime.end.p0i8(i64 16, i8* nonnull %64) #5
  call void @llvm.lifetime.end.p0i8(i64 16, i8* nonnull %61) #5
  %119 = load i64, i64* %16, align 8
  %120 = and i64 %119, -64
  store i64 %120, i64* %16, align 8
  %121 = load i64, i64* %18, align 8
  %122 = add i64 %121, 32
  store i64 %122, i64* %18, align 8
  %123 = icmp ult i64 %122, 6
  br i1 %123, label %106, label %124

124:                                              ; preds = %118, %100, %112
  %125 = phi i64 [ %116, %112 ], [ %73, %100 ], [ %122, %118 ]
  %126 = phi i64 [ %117, %112 ], [ 0, %100 ], [ 0, %118 ]
  store i64 %126, i64* %17, align 8
  br label %72
}

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #2

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #2

; Function Attrs: nounwind ssp uwtable
define hidden { i64, i32 } @_ZN11dawn_native6vulkan24ComputeTextureCopyExtentERKNS_11TextureCopyERKNS_8Extent3DE(%"struct.dawn_native::TextureCopy"* nocapture readonly dereferenceable(32), %"struct.dawn_native::Extent3D"* nocapture readonly dereferenceable(12)) local_unnamed_addr #1 {
  %3 = bitcast %"struct.dawn_native::Extent3D"* %1 to i64*
  %4 = load i64, i64* %3, align 4
  %5 = trunc i64 %4 to i32
  %6 = lshr i64 %4, 32
  %7 = trunc i64 %6 to i32
  %8 = getelementptr inbounds %"struct.dawn_native::Extent3D", %"struct.dawn_native::Extent3D"* %1, i64 0, i32 2
  %9 = load i32, i32* %8, align 4
  %10 = getelementptr inbounds %"struct.dawn_native::TextureCopy", %"struct.dawn_native::TextureCopy"* %0, i64 0, i32 0, i32 0, i32 0
  %11 = load %"class.dawn_native::TextureBase"*, %"class.dawn_native::TextureBase"** %10, align 8
  %12 = getelementptr inbounds %"struct.dawn_native::TextureCopy", %"struct.dawn_native::TextureCopy"* %0, i64 0, i32 1
  %13 = load i32, i32* %12, align 8
  %14 = tail call { i64, i32 } @_ZNK11dawn_native11TextureBase22GetMipLevelVirtualSizeEj(%"class.dawn_native::TextureBase"* %11, i32 %13) #5
  %15 = extractvalue { i64, i32 } %14, 0
  %16 = trunc i64 %15 to i32
  %17 = lshr i64 %15, 32
  %18 = trunc i64 %17 to i32
  %19 = getelementptr inbounds %"struct.dawn_native::TextureCopy", %"struct.dawn_native::TextureCopy"* %0, i64 0, i32 2, i32 0
  %20 = load i32, i32* %19, align 4
  %21 = getelementptr inbounds %"struct.dawn_native::Extent3D", %"struct.dawn_native::Extent3D"* %1, i64 0, i32 0
  %22 = load i32, i32* %21, align 4
  %23 = add i32 %22, %20
  %24 = icmp ugt i32 %23, %16
  %25 = sub i32 %16, %20
  %26 = select i1 %24, i32 %25, i32 %5
  %27 = getelementptr inbounds %"struct.dawn_native::TextureCopy", %"struct.dawn_native::TextureCopy"* %0, i64 0, i32 2, i32 1
  %28 = load i32, i32* %27, align 4
  %29 = getelementptr inbounds %"struct.dawn_native::Extent3D", %"struct.dawn_native::Extent3D"* %1, i64 0, i32 1
  %30 = load i32, i32* %29, align 4
  %31 = add i32 %30, %28
  %32 = icmp ugt i32 %31, %18
  %33 = sub i32 %18, %28
  %34 = select i1 %32, i32 %33, i32 %7
  %35 = zext i32 %34 to i64
  %36 = shl nuw i64 %35, 32
  %37 = zext i32 %26 to i64
  %38 = or i64 %36, %37
  %39 = insertvalue { i64, i32 } undef, i64 %38, 0
  %40 = insertvalue { i64, i32 } %39, i32 %9, 1
  ret { i64, i32 } %40
}

; Function Attrs: argmemonly nounwind
declare void @llvm.memcpy.p0i8.p0i8.i64(i8* nocapture writeonly, i8* nocapture readonly, i64, i1 immarg) #2

declare { i64, i32 } @_ZNK11dawn_native11TextureBase22GetMipLevelVirtualSizeEj(%"class.dawn_native::TextureBase"*, i32) local_unnamed_addr #3

; Function Attrs: nounwind ssp uwtable
define hidden void @_ZN11dawn_native6vulkan28ComputeBufferImageCopyRegionERKNS_10BufferCopyERKNS_11TextureCopyERKNS_8Extent3DE(%struct.VkBufferImageCopy* noalias sret, %"struct.dawn_native::BufferCopy"* nocapture readonly dereferenceable(24), %"struct.dawn_native::TextureCopy"* nocapture readonly dereferenceable(32), %"struct.dawn_native::Extent3D"* nocapture readonly dereferenceable(12)) local_unnamed_addr #1 {
  %5 = alloca %"struct.dawn_native::TextureDataLayout", align 8
  %6 = bitcast %"struct.dawn_native::TextureDataLayout"* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 24, i8* nonnull %6) #5
  %7 = getelementptr inbounds %"struct.dawn_native::TextureDataLayout", %"struct.dawn_native::TextureDataLayout"* %5, i64 0, i32 1
  %8 = getelementptr inbounds %"struct.dawn_native::TextureDataLayout", %"struct.dawn_native::TextureDataLayout"* %5, i64 0, i32 2
  %9 = getelementptr inbounds %"struct.dawn_native::TextureDataLayout", %"struct.dawn_native::TextureDataLayout"* %5, i64 0, i32 3
  %10 = bitcast %"struct.dawn_native::TextureDataLayout"* %5 to i64*
  store i64 0, i64* %10, align 8
  %11 = getelementptr inbounds %"struct.dawn_native::BufferCopy", %"struct.dawn_native::BufferCopy"* %1, i64 0, i32 1
  %12 = load i64, i64* %11, align 8
  store i64 %12, i64* %7, align 8
  %13 = getelementptr inbounds %"struct.dawn_native::BufferCopy", %"struct.dawn_native::BufferCopy"* %1, i64 0, i32 3
  %14 = load i32, i32* %13, align 4
  store i32 %14, i32* %9, align 4
  %15 = getelementptr inbounds %"struct.dawn_native::BufferCopy", %"struct.dawn_native::BufferCopy"* %1, i64 0, i32 2
  %16 = load i32, i32* %15, align 8
  store i32 %16, i32* %8, align 8
  call void @_ZN11dawn_native6vulkan28ComputeBufferImageCopyRegionERKNS_17TextureDataLayoutERKNS_11TextureCopyERKNS_8Extent3DE(%struct.VkBufferImageCopy* sret %0, %"struct.dawn_native::TextureDataLayout"* nonnull dereferenceable(24) %5, %"struct.dawn_native::TextureCopy"* dereferenceable(32) %2, %"struct.dawn_native::Extent3D"* dereferenceable(12) %3)
  call void @llvm.lifetime.end.p0i8(i64 24, i8* nonnull %6) #5
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @_ZN11dawn_native6vulkan28ComputeBufferImageCopyRegionERKNS_17TextureDataLayoutERKNS_11TextureCopyERKNS_8Extent3DE(%struct.VkBufferImageCopy* noalias sret, %"struct.dawn_native::TextureDataLayout"* nocapture readonly dereferenceable(24), %"struct.dawn_native::TextureCopy"* nocapture readonly dereferenceable(32), %"struct.dawn_native::Extent3D"* nocapture readonly dereferenceable(12)) local_unnamed_addr #1 {
  %5 = getelementptr inbounds %"struct.dawn_native::TextureCopy", %"struct.dawn_native::TextureCopy"* %2, i64 0, i32 0, i32 0, i32 0
  %6 = bitcast %"struct.dawn_native::TextureCopy"* %2 to %"class.dawn_native::vulkan::Texture"**
  %7 = load %"class.dawn_native::vulkan::Texture"*, %"class.dawn_native::vulkan::Texture"** %6, align 8
  %8 = getelementptr inbounds %struct.VkBufferImageCopy, %struct.VkBufferImageCopy* %0, i64 0, i32 1
  %9 = bitcast i32* %8 to i8*
  tail call void @llvm.memset.p0i8.i64(i8* align 8 %9, i8 -86, i64 48, i1 false)
  %10 = getelementptr inbounds %"struct.dawn_native::TextureDataLayout", %"struct.dawn_native::TextureDataLayout"* %1, i64 0, i32 1
  %11 = load i64, i64* %10, align 8
  %12 = getelementptr inbounds %struct.VkBufferImageCopy, %struct.VkBufferImageCopy* %0, i64 0, i32 0
  store i64 %11, i64* %12, align 8
  %13 = getelementptr inbounds %"class.dawn_native::vulkan::Texture", %"class.dawn_native::vulkan::Texture"* %7, i64 0, i32 0
  %14 = tail call dereferenceable(84) %"struct.dawn_native::Format"* @_ZNK11dawn_native11TextureBase9GetFormatEv(%"class.dawn_native::TextureBase"* %13) #5
  %15 = getelementptr inbounds %"struct.dawn_native::TextureCopy", %"struct.dawn_native::TextureCopy"* %2, i64 0, i32 3
  %16 = load i8, i8* %15, align 8
  %17 = tail call dereferenceable(24) %"struct.dawn_native::AspectInfo"* @_ZNK11dawn_native6Format13GetAspectInfoENS_6AspectE(%"struct.dawn_native::Format"* %14, i8 zeroext %16) #5
  %18 = getelementptr inbounds %"struct.dawn_native::TextureDataLayout", %"struct.dawn_native::TextureDataLayout"* %1, i64 0, i32 2
  %19 = load i32, i32* %18, align 8
  %20 = getelementptr inbounds %"struct.dawn_native::AspectInfo", %"struct.dawn_native::AspectInfo"* %17, i64 0, i32 0, i32 0
  %21 = load i32, i32* %20, align 4
  %22 = udiv i32 %19, %21
  %23 = getelementptr inbounds %"struct.dawn_native::AspectInfo", %"struct.dawn_native::AspectInfo"* %17, i64 0, i32 0, i32 1
  %24 = load i32, i32* %23, align 4
  %25 = mul i32 %24, %22
  %26 = getelementptr inbounds %struct.VkBufferImageCopy, %struct.VkBufferImageCopy* %0, i64 0, i32 1
  store i32 %25, i32* %26, align 8
  %27 = getelementptr inbounds %"struct.dawn_native::TextureDataLayout", %"struct.dawn_native::TextureDataLayout"* %1, i64 0, i32 3
  %28 = load i32, i32* %27, align 4
  %29 = getelementptr inbounds %"struct.dawn_native::AspectInfo", %"struct.dawn_native::AspectInfo"* %17, i64 0, i32 0, i32 2
  %30 = load i32, i32* %29, align 4
  %31 = mul i32 %30, %28
  %32 = getelementptr inbounds %struct.VkBufferImageCopy, %struct.VkBufferImageCopy* %0, i64 0, i32 2
  store i32 %31, i32* %32, align 4
  %33 = tail call i32 @_ZN11dawn_native6vulkan16VulkanAspectMaskERKNS_6AspectE(i8* dereferenceable(1) %15)
  %34 = getelementptr inbounds %struct.VkBufferImageCopy, %struct.VkBufferImageCopy* %0, i64 0, i32 3, i32 0
  store i32 %33, i32* %34, align 8
  %35 = getelementptr inbounds %"struct.dawn_native::TextureCopy", %"struct.dawn_native::TextureCopy"* %2, i64 0, i32 1
  %36 = load i32, i32* %35, align 8
  %37 = getelementptr inbounds %struct.VkBufferImageCopy, %struct.VkBufferImageCopy* %0, i64 0, i32 3, i32 1
  store i32 %36, i32* %37, align 4
  %38 = load %"class.dawn_native::TextureBase"*, %"class.dawn_native::TextureBase"** %5, align 8
  %39 = tail call i32 @_ZNK11dawn_native11TextureBase12GetDimensionEv(%"class.dawn_native::TextureBase"* %38) #5
  switch i32 %39, label %126 [
    i32 1, label %40
    i32 2, label %77
  ]

40:                                               ; preds = %4
  %41 = getelementptr inbounds %"struct.dawn_native::TextureCopy", %"struct.dawn_native::TextureCopy"* %2, i64 0, i32 2, i32 0
  %42 = load i32, i32* %41, align 4
  %43 = getelementptr inbounds %struct.VkBufferImageCopy, %struct.VkBufferImageCopy* %0, i64 0, i32 4, i32 0
  store i32 %42, i32* %43, align 8
  %44 = getelementptr inbounds %"struct.dawn_native::TextureCopy", %"struct.dawn_native::TextureCopy"* %2, i64 0, i32 2, i32 1
  %45 = load i32, i32* %44, align 4
  %46 = getelementptr inbounds %struct.VkBufferImageCopy, %struct.VkBufferImageCopy* %0, i64 0, i32 4, i32 1
  store i32 %45, i32* %46, align 4
  %47 = getelementptr inbounds %struct.VkBufferImageCopy, %struct.VkBufferImageCopy* %0, i64 0, i32 4, i32 2
  store i32 0, i32* %47, align 8
  %48 = getelementptr inbounds %"struct.dawn_native::TextureCopy", %"struct.dawn_native::TextureCopy"* %2, i64 0, i32 2, i32 2
  %49 = load i32, i32* %48, align 4
  %50 = getelementptr inbounds %struct.VkBufferImageCopy, %struct.VkBufferImageCopy* %0, i64 0, i32 3, i32 2
  store i32 %49, i32* %50, align 8
  %51 = getelementptr inbounds %"struct.dawn_native::Extent3D", %"struct.dawn_native::Extent3D"* %3, i64 0, i32 2
  %52 = load i32, i32* %51, align 4
  %53 = getelementptr inbounds %struct.VkBufferImageCopy, %struct.VkBufferImageCopy* %0, i64 0, i32 3, i32 3
  store i32 %52, i32* %53, align 4
  %54 = bitcast %"struct.dawn_native::Extent3D"* %3 to i64*
  %55 = load i64, i64* %54, align 4
  %56 = trunc i64 %55 to i32
  %57 = load %"class.dawn_native::TextureBase"*, %"class.dawn_native::TextureBase"** %5, align 8
  %58 = load i32, i32* %35, align 8
  %59 = tail call { i64, i32 } @_ZNK11dawn_native11TextureBase22GetMipLevelVirtualSizeEj(%"class.dawn_native::TextureBase"* %57, i32 %58) #5
  %60 = extractvalue { i64, i32 } %59, 0
  %61 = trunc i64 %60 to i32
  %62 = lshr i64 %60, 32
  %63 = trunc i64 %62 to i32
  %64 = load i32, i32* %41, align 4
  %65 = getelementptr inbounds %"struct.dawn_native::Extent3D", %"struct.dawn_native::Extent3D"* %3, i64 0, i32 0
  %66 = load i32, i32* %65, align 4
  %67 = add i32 %66, %64
  %68 = icmp ugt i32 %67, %61
  %69 = sub i32 %61, %64
  %70 = select i1 %68, i32 %69, i32 %56
  %71 = load i32, i32* %44, align 4
  %72 = getelementptr inbounds %"struct.dawn_native::Extent3D", %"struct.dawn_native::Extent3D"* %3, i64 0, i32 1
  %73 = load i32, i32* %72, align 4
  %74 = add i32 %73, %71
  %75 = icmp ugt i32 %74, %63
  %76 = sub i32 %63, %71
  br label %114

77:                                               ; preds = %4
  %78 = getelementptr inbounds %"struct.dawn_native::TextureCopy", %"struct.dawn_native::TextureCopy"* %2, i64 0, i32 2, i32 0
  %79 = load i32, i32* %78, align 4
  %80 = getelementptr inbounds %struct.VkBufferImageCopy, %struct.VkBufferImageCopy* %0, i64 0, i32 4, i32 0
  store i32 %79, i32* %80, align 8
  %81 = getelementptr inbounds %"struct.dawn_native::TextureCopy", %"struct.dawn_native::TextureCopy"* %2, i64 0, i32 2, i32 1
  %82 = load i32, i32* %81, align 4
  %83 = getelementptr inbounds %struct.VkBufferImageCopy, %struct.VkBufferImageCopy* %0, i64 0, i32 4, i32 1
  store i32 %82, i32* %83, align 4
  %84 = getelementptr inbounds %"struct.dawn_native::TextureCopy", %"struct.dawn_native::TextureCopy"* %2, i64 0, i32 2, i32 2
  %85 = load i32, i32* %84, align 4
  %86 = getelementptr inbounds %struct.VkBufferImageCopy, %struct.VkBufferImageCopy* %0, i64 0, i32 4, i32 2
  store i32 %85, i32* %86, align 8
  %87 = getelementptr inbounds %struct.VkBufferImageCopy, %struct.VkBufferImageCopy* %0, i64 0, i32 3, i32 2
  store i32 0, i32* %87, align 8
  %88 = getelementptr inbounds %struct.VkBufferImageCopy, %struct.VkBufferImageCopy* %0, i64 0, i32 3, i32 3
  store i32 1, i32* %88, align 4
  %89 = bitcast %"struct.dawn_native::Extent3D"* %3 to i64*
  %90 = load i64, i64* %89, align 4
  %91 = trunc i64 %90 to i32
  %92 = getelementptr inbounds %"struct.dawn_native::Extent3D", %"struct.dawn_native::Extent3D"* %3, i64 0, i32 2
  %93 = load i32, i32* %92, align 4
  %94 = load %"class.dawn_native::TextureBase"*, %"class.dawn_native::TextureBase"** %5, align 8
  %95 = load i32, i32* %35, align 8
  %96 = tail call { i64, i32 } @_ZNK11dawn_native11TextureBase22GetMipLevelVirtualSizeEj(%"class.dawn_native::TextureBase"* %94, i32 %95) #5
  %97 = extractvalue { i64, i32 } %96, 0
  %98 = trunc i64 %97 to i32
  %99 = lshr i64 %97, 32
  %100 = trunc i64 %99 to i32
  %101 = load i32, i32* %78, align 4
  %102 = getelementptr inbounds %"struct.dawn_native::Extent3D", %"struct.dawn_native::Extent3D"* %3, i64 0, i32 0
  %103 = load i32, i32* %102, align 4
  %104 = add i32 %103, %101
  %105 = icmp ugt i32 %104, %98
  %106 = sub i32 %98, %101
  %107 = select i1 %105, i32 %106, i32 %91
  %108 = load i32, i32* %81, align 4
  %109 = getelementptr inbounds %"struct.dawn_native::Extent3D", %"struct.dawn_native::Extent3D"* %3, i64 0, i32 1
  %110 = load i32, i32* %109, align 4
  %111 = add i32 %110, %108
  %112 = icmp ugt i32 %111, %100
  %113 = sub i32 %100, %108
  br label %114

114:                                              ; preds = %40, %77
  %115 = phi i64 [ %90, %77 ], [ %55, %40 ]
  %116 = phi i32 [ %113, %77 ], [ %76, %40 ]
  %117 = phi i1 [ %112, %77 ], [ %75, %40 ]
  %118 = phi i32 [ %107, %77 ], [ %70, %40 ]
  %119 = phi i32 [ %93, %77 ], [ 1, %40 ]
  %120 = lshr i64 %115, 32
  %121 = trunc i64 %120 to i32
  %122 = select i1 %117, i32 %116, i32 %121
  %123 = getelementptr inbounds %struct.VkBufferImageCopy, %struct.VkBufferImageCopy* %0, i64 0, i32 5, i32 0
  store i32 %118, i32* %123, align 4
  %124 = getelementptr inbounds %struct.VkBufferImageCopy, %struct.VkBufferImageCopy* %0, i64 0, i32 5, i32 1
  store i32 %122, i32* %124, align 4
  %125 = getelementptr inbounds %struct.VkBufferImageCopy, %struct.VkBufferImageCopy* %0, i64 0, i32 5, i32 2
  store i32 %119, i32* %125, align 4
  br label %126

126:                                              ; preds = %114, %4
  ret void
}

; Function Attrs: argmemonly nounwind
declare void @llvm.memset.p0i8.i64(i8* nocapture writeonly, i8, i64, i1 immarg) #2

declare dereferenceable(84) %"struct.dawn_native::Format"* @_ZNK11dawn_native11TextureBase9GetFormatEv(%"class.dawn_native::TextureBase"*) local_unnamed_addr #3

declare dereferenceable(24) %"struct.dawn_native::AspectInfo"* @_ZNK11dawn_native6Format13GetAspectInfoENS_6AspectE(%"struct.dawn_native::Format"*, i8 zeroext) local_unnamed_addr #3

declare i32 @_ZNK11dawn_native11TextureBase12GetDimensionEv(%"class.dawn_native::TextureBase"*) local_unnamed_addr #3

declare i32 @_Z11ScanForwardj(i32) local_unnamed_addr #3

; Function Attrs: nounwind ssp uwtable
define linkonce_odr hidden void @_ZNSt3__116__copy_unalignedINS_8__bitsetILm1ELm6EEELb0EEENS_14__bit_iteratorIT_Lb0EXLi0EEEENS3_IS4_XT0_EXLi0EEEES6_S5_(%"class.std::__1::__bit_iterator"* noalias sret, %"class.std::__1::__bit_iterator"*, %"class.std::__1::__bit_iterator"*, %"class.std::__1::__bit_iterator"*) local_unnamed_addr #1 comdat {
  %5 = bitcast %"class.std::__1::__bit_iterator"* %2 to i64*
  %6 = load i64, i64* %5, align 8
  %7 = bitcast %"class.std::__1::__bit_iterator"* %1 to i64*
  %8 = load i64, i64* %7, align 8
  %9 = sub i64 %6, %8
  %10 = shl i64 %9, 3
  %11 = getelementptr inbounds %"class.std::__1::__bit_iterator", %"class.std::__1::__bit_iterator"* %2, i64 0, i32 1
  %12 = load i32, i32* %11, align 8
  %13 = zext i32 %12 to i64
  %14 = add nsw i64 %10, %13
  %15 = getelementptr inbounds %"class.std::__1::__bit_iterator", %"class.std::__1::__bit_iterator"* %1, i64 0, i32 1
  %16 = load i32, i32* %15, align 8
  %17 = zext i32 %16 to i64
  %18 = sub i64 %14, %17
  %19 = icmp sgt i64 %18, 0
  %20 = inttoptr i64 %8 to i64*
  br i1 %19, label %21, label %185

21:                                               ; preds = %4
  %22 = icmp eq i32 %16, 0
  br i1 %22, label %93, label %23

23:                                               ; preds = %21
  %24 = sub i32 64, %16
  %25 = zext i32 %24 to i64
  %26 = icmp slt i64 %18, %25
  %27 = select i1 %26, i64 %18, i64 %25
  %28 = sub nsw i64 %18, %27
  %29 = shl i64 -1, %17
  %30 = sub nsw i64 %25, %27
  %31 = lshr i64 -1, %30
  %32 = and i64 %31, %29
  %33 = getelementptr inbounds %"class.std::__1::__bit_iterator", %"class.std::__1::__bit_iterator"* %1, i64 0, i32 0
  %34 = load i64, i64* %20, align 8
  %35 = and i64 %32, %34
  %36 = getelementptr inbounds %"class.std::__1::__bit_iterator", %"class.std::__1::__bit_iterator"* %3, i64 0, i32 1
  %37 = load i32, i32* %36, align 8
  %38 = sub i32 64, %37
  %39 = zext i32 %38 to i64
  %40 = icmp ugt i64 %27, %39
  %41 = select i1 %40, i64 %39, i64 %27
  %42 = zext i32 %37 to i64
  %43 = shl i64 -1, %42
  %44 = sub nsw i64 %39, %41
  %45 = lshr i64 -1, %44
  %46 = and i64 %45, %43
  %47 = xor i64 %46, -1
  %48 = getelementptr inbounds %"class.std::__1::__bit_iterator", %"class.std::__1::__bit_iterator"* %3, i64 0, i32 0
  %49 = load i64*, i64** %48, align 8
  %50 = load i64, i64* %49, align 8
  %51 = and i64 %50, %47
  store i64 %51, i64* %49, align 8
  %52 = load i32, i32* %36, align 8
  %53 = load i32, i32* %15, align 8
  %54 = icmp ugt i32 %52, %53
  %55 = sub i32 %53, %52
  %56 = zext i32 %55 to i64
  %57 = lshr i64 %35, %56
  %58 = sub i32 %52, %53
  %59 = zext i32 %58 to i64
  %60 = shl i64 %35, %59
  %61 = select i1 %54, i64 %60, i64 %57
  %62 = load i64*, i64** %48, align 8
  %63 = load i64, i64* %62, align 8
  %64 = or i64 %63, %61
  store i64 %64, i64* %62, align 8
  %65 = load i32, i32* %36, align 8
  %66 = zext i32 %65 to i64
  %67 = add nuw nsw i64 %41, %66
  %68 = lshr i64 %67, 6
  %69 = load i64*, i64** %48, align 8
  %70 = getelementptr inbounds i64, i64* %69, i64 %68
  store i64* %70, i64** %48, align 8
  %71 = trunc i64 %41 to i32
  %72 = add i32 %65, %71
  %73 = and i32 %72, 63
  store i32 %73, i32* %36, align 8
  %74 = sub i64 %27, %41
  %75 = icmp sgt i64 %74, 0
  br i1 %75, label %76, label %90

76:                                               ; preds = %23
  %77 = sub nsw i64 64, %74
  %78 = lshr i64 -1, %77
  %79 = xor i64 %78, -1
  %80 = load i64, i64* %70, align 8
  %81 = and i64 %80, %79
  store i64 %81, i64* %70, align 8
  %82 = load i32, i32* %15, align 8
  %83 = zext i32 %82 to i64
  %84 = add nuw nsw i64 %41, %83
  %85 = lshr i64 %35, %84
  %86 = load i64*, i64** %48, align 8
  %87 = load i64, i64* %86, align 8
  %88 = or i64 %85, %87
  store i64 %88, i64* %86, align 8
  %89 = trunc i64 %74 to i32
  store i32 %89, i32* %36, align 8
  br label %90

90:                                               ; preds = %76, %23
  %91 = load i64*, i64** %33, align 8
  %92 = getelementptr inbounds i64, i64* %91, i64 1
  store i64* %92, i64** %33, align 8
  br label %93

93:                                               ; preds = %21, %90
  %94 = phi i64* [ %20, %21 ], [ %92, %90 ]
  %95 = phi i64 [ %18, %21 ], [ %28, %90 ]
  %96 = getelementptr inbounds %"class.std::__1::__bit_iterator", %"class.std::__1::__bit_iterator"* %3, i64 0, i32 1
  %97 = load i32, i32* %96, align 8
  %98 = sub i32 64, %97
  %99 = zext i32 %97 to i64
  %100 = shl i64 -1, %99
  %101 = icmp sgt i64 %95, 63
  br i1 %101, label %102, label %134

102:                                              ; preds = %93
  %103 = getelementptr inbounds %"class.std::__1::__bit_iterator", %"class.std::__1::__bit_iterator"* %1, i64 0, i32 0
  %104 = xor i64 %100, -1
  %105 = getelementptr inbounds %"class.std::__1::__bit_iterator", %"class.std::__1::__bit_iterator"* %3, i64 0, i32 0
  %106 = zext i32 %98 to i64
  br label %107

107:                                              ; preds = %102, %107
  %108 = phi i64* [ %94, %102 ], [ %130, %107 ]
  %109 = phi i64 [ %95, %102 ], [ %128, %107 ]
  %110 = load i64, i64* %108, align 8
  %111 = load i64*, i64** %105, align 8
  %112 = load i64, i64* %111, align 8
  %113 = and i64 %112, %104
  store i64 %113, i64* %111, align 8
  %114 = load i32, i32* %96, align 8
  %115 = zext i32 %114 to i64
  %116 = shl i64 %110, %115
  %117 = load i64*, i64** %105, align 8
  %118 = load i64, i64* %117, align 8
  %119 = or i64 %118, %116
  store i64 %119, i64* %117, align 8
  %120 = load i64*, i64** %105, align 8
  %121 = getelementptr inbounds i64, i64* %120, i64 1
  store i64* %121, i64** %105, align 8
  %122 = load i64, i64* %121, align 8
  %123 = and i64 %122, %100
  store i64 %123, i64* %121, align 8
  %124 = lshr i64 %110, %106
  %125 = load i64*, i64** %105, align 8
  %126 = load i64, i64* %125, align 8
  %127 = or i64 %126, %124
  store i64 %127, i64* %125, align 8
  %128 = add nsw i64 %109, -64
  %129 = load i64*, i64** %103, align 8
  %130 = getelementptr inbounds i64, i64* %129, i64 1
  store i64* %130, i64** %103, align 8
  %131 = icmp sgt i64 %128, 63
  br i1 %131, label %107, label %132

132:                                              ; preds = %107
  %133 = and i64 %95, 63
  br label %134

134:                                              ; preds = %132, %93
  %135 = phi i64* [ %94, %93 ], [ %130, %132 ]
  %136 = phi i64 [ %95, %93 ], [ %133, %132 ]
  %137 = icmp sgt i64 %136, 0
  br i1 %137, label %138, label %185

138:                                              ; preds = %134
  %139 = sub nuw nsw i64 64, %136
  %140 = lshr i64 -1, %139
  %141 = load i64, i64* %135, align 8
  %142 = and i64 %141, %140
  %143 = zext i32 %98 to i64
  %144 = icmp sgt i64 %136, %143
  %145 = select i1 %144, i64 %143, i64 %136
  %146 = load i32, i32* %96, align 8
  %147 = zext i32 %146 to i64
  %148 = shl i64 -1, %147
  %149 = sub nsw i64 %143, %145
  %150 = lshr i64 -1, %149
  %151 = and i64 %148, %150
  %152 = xor i64 %151, -1
  %153 = getelementptr inbounds %"class.std::__1::__bit_iterator", %"class.std::__1::__bit_iterator"* %3, i64 0, i32 0
  %154 = load i64*, i64** %153, align 8
  %155 = load i64, i64* %154, align 8
  %156 = and i64 %155, %152
  store i64 %156, i64* %154, align 8
  %157 = load i32, i32* %96, align 8
  %158 = zext i32 %157 to i64
  %159 = shl i64 %142, %158
  %160 = load i64*, i64** %153, align 8
  %161 = load i64, i64* %160, align 8
  %162 = or i64 %161, %159
  store i64 %162, i64* %160, align 8
  %163 = load i32, i32* %96, align 8
  %164 = zext i32 %163 to i64
  %165 = add nuw nsw i64 %145, %164
  %166 = lshr i64 %165, 6
  %167 = load i64*, i64** %153, align 8
  %168 = getelementptr inbounds i64, i64* %167, i64 %166
  store i64* %168, i64** %153, align 8
  %169 = trunc i64 %145 to i32
  %170 = add i32 %163, %169
  %171 = and i32 %170, 63
  store i32 %171, i32* %96, align 8
  %172 = sub nsw i64 %136, %145
  %173 = icmp sgt i64 %172, 0
  br i1 %173, label %174, label %185

174:                                              ; preds = %138
  %175 = sub nuw nsw i64 64, %172
  %176 = lshr i64 -1, %175
  %177 = xor i64 %176, -1
  %178 = load i64, i64* %168, align 8
  %179 = and i64 %178, %177
  store i64 %179, i64* %168, align 8
  %180 = lshr i64 %142, %145
  %181 = load i64*, i64** %153, align 8
  %182 = load i64, i64* %181, align 8
  %183 = or i64 %182, %180
  store i64 %183, i64* %181, align 8
  %184 = trunc i64 %172 to i32
  store i32 %184, i32* %96, align 8
  br label %185

185:                                              ; preds = %134, %174, %138, %4
  %186 = bitcast %"class.std::__1::__bit_iterator"* %3 to i64*
  %187 = load i64, i64* %186, align 8
  %188 = bitcast %"class.std::__1::__bit_iterator"* %0 to i64*
  store i64 %187, i64* %188, align 8
  %189 = getelementptr inbounds %"class.std::__1::__bit_iterator", %"class.std::__1::__bit_iterator"* %0, i64 0, i32 1
  %190 = getelementptr inbounds %"class.std::__1::__bit_iterator", %"class.std::__1::__bit_iterator"* %3, i64 0, i32 1
  %191 = load i32, i32* %190, align 8
  store i32 %191, i32* %189, align 8
  ret void
}

; Function Attrs: noreturn nounwind
declare void @abort() local_unnamed_addr #4

attributes #0 = { norecurse nounwind readnone ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #2 = { argmemonly nounwind }
attributes #3 = { "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #4 = { noreturn nounwind "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #5 = { nounwind }
attributes #6 = { noreturn nounwind }

!llvm.module.flags = !{!0, !1}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 2}
!2 = !{!3}
!3 = distinct !{!3, !4, !"_ZN11dawn_native15IterateEnumMaskINS_6AspectEEENS_16EnumMaskIteratorIT_EERKS3_: argument 0"}
!4 = distinct !{!4, !"_ZN11dawn_native15IterateEnumMaskINS_6AspectEEENS_16EnumMaskIteratorIT_EERKS3_"}
!5 = !{!6}
!6 = distinct !{!6, !7, !"_ZNK11dawn_native16EnumMaskIteratorINS_6AspectEE5beginEv: argument 0"}
!7 = distinct !{!7, !"_ZNK11dawn_native16EnumMaskIteratorINS_6AspectEE5beginEv"}
!8 = !{!9}
!9 = distinct !{!9, !10, !"_ZNK14BitSetIteratorILm6EhE5beginEv: argument 0"}
!10 = distinct !{!10, !"_ZNK14BitSetIteratorILm6EhE5beginEv"}
!11 = !{!12, !6}
!12 = distinct !{!12, !13, !"_ZNSt3__14copyINS_8__bitsetILm1ELm6EEELb0EEENS_14__bit_iteratorIT_Lb0EXLi0EEEENS3_IS4_XT0_EXLi0EEEES6_S5_: argument 0"}
!13 = distinct !{!13, !"_ZNSt3__14copyINS_8__bitsetILm1ELm6EEELb0EEENS_14__bit_iteratorIT_Lb0EXLi0EEEENS3_IS4_XT0_EXLi0EEEES6_S5_"}
!14 = !{!15}
!15 = distinct !{!15, !16, !"_ZNSt3__14copyINS_8__bitsetILm1ELm6EEELb0EEENS_14__bit_iteratorIT_Lb0EXLi0EEEENS3_IS4_XT0_EXLi0EEEES6_S5_: argument 0"}
!16 = distinct !{!16, !"_ZNSt3__14copyINS_8__bitsetILm1ELm6EEELb0EEENS_14__bit_iteratorIT_Lb0EXLi0EEEENS3_IS4_XT0_EXLi0EEEES6_S5_"}
