; ModuleID = '../../third_party/libaom/source/libaom/av1/encoder/x86/av1_fwd_txfm1d_sse4.c'
source_filename = "../../third_party/libaom/source/libaom/av1/encoder/x86/av1_fwd_txfm1d_sse4.c"
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

module asm ".symver exp, exp@GLIBC_2.2.5"
module asm ".symver exp2, exp2@GLIBC_2.2.5"
module asm ".symver exp2f, exp2f@GLIBC_2.2.5"
module asm ".symver expf, expf@GLIBC_2.2.5"
module asm ".symver lgamma, lgamma@GLIBC_2.2.5"
module asm ".symver lgammaf, lgammaf@GLIBC_2.2.5"
module asm ".symver lgammal, lgammal@GLIBC_2.2.5"
module asm ".symver log, log@GLIBC_2.2.5"
module asm ".symver log2, log2@GLIBC_2.2.5"
module asm ".symver log2f, log2f@GLIBC_2.2.5"
module asm ".symver logf, logf@GLIBC_2.2.5"
module asm ".symver pow, pow@GLIBC_2.2.5"
module asm ".symver powf, powf@GLIBC_2.2.5"

@av1_cospi_arr_data = external local_unnamed_addr constant [7 x [64 x i32]], align 16

; Function Attrs: nofree nounwind ssp uwtable
define hidden void @av1_fdct32_sse4_1(<2 x i64>* readonly, <2 x i64>*, i32, i32) local_unnamed_addr #0 {
  %5 = mul nsw i32 %3, 31
  %6 = bitcast <2 x i64>* %0 to <4 x i32>*
  %7 = load <4 x i32>, <4 x i32>* %6, align 16
  %8 = sext i32 %5 to i64
  %9 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %8
  %10 = bitcast <2 x i64>* %9 to <4 x i32>*
  %11 = load <4 x i32>, <4 x i32>* %10, align 16
  %12 = add <4 x i32> %11, %7
  %13 = sub <4 x i32> %7, %11
  %14 = mul i32 %3, 30
  %15 = sext i32 %3 to i64
  %16 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %15
  %17 = bitcast <2 x i64>* %16 to <4 x i32>*
  %18 = load <4 x i32>, <4 x i32>* %17, align 16
  %19 = sext i32 %14 to i64
  %20 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %19
  %21 = bitcast <2 x i64>* %20 to <4 x i32>*
  %22 = load <4 x i32>, <4 x i32>* %21, align 16
  %23 = add <4 x i32> %22, %18
  %24 = sub <4 x i32> %18, %22
  %25 = shl nsw i32 %3, 1
  %26 = mul i32 %3, 29
  %27 = sext i32 %25 to i64
  %28 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %27
  %29 = bitcast <2 x i64>* %28 to <4 x i32>*
  %30 = load <4 x i32>, <4 x i32>* %29, align 16
  %31 = sext i32 %26 to i64
  %32 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %31
  %33 = bitcast <2 x i64>* %32 to <4 x i32>*
  %34 = load <4 x i32>, <4 x i32>* %33, align 16
  %35 = add <4 x i32> %34, %30
  %36 = sub <4 x i32> %30, %34
  %37 = mul nsw i32 %3, 3
  %38 = mul i32 %3, 28
  %39 = sext i32 %37 to i64
  %40 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %39
  %41 = bitcast <2 x i64>* %40 to <4 x i32>*
  %42 = load <4 x i32>, <4 x i32>* %41, align 16
  %43 = sext i32 %38 to i64
  %44 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %43
  %45 = bitcast <2 x i64>* %44 to <4 x i32>*
  %46 = load <4 x i32>, <4 x i32>* %45, align 16
  %47 = add <4 x i32> %46, %42
  %48 = sub <4 x i32> %42, %46
  %49 = shl nsw i32 %3, 2
  %50 = mul i32 %3, 27
  %51 = sext i32 %49 to i64
  %52 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %51
  %53 = bitcast <2 x i64>* %52 to <4 x i32>*
  %54 = load <4 x i32>, <4 x i32>* %53, align 16
  %55 = sext i32 %50 to i64
  %56 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %55
  %57 = bitcast <2 x i64>* %56 to <4 x i32>*
  %58 = load <4 x i32>, <4 x i32>* %57, align 16
  %59 = add <4 x i32> %58, %54
  %60 = sub <4 x i32> %54, %58
  %61 = mul nsw i32 %3, 5
  %62 = mul i32 %3, 26
  %63 = sext i32 %61 to i64
  %64 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %63
  %65 = bitcast <2 x i64>* %64 to <4 x i32>*
  %66 = load <4 x i32>, <4 x i32>* %65, align 16
  %67 = sext i32 %62 to i64
  %68 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %67
  %69 = bitcast <2 x i64>* %68 to <4 x i32>*
  %70 = load <4 x i32>, <4 x i32>* %69, align 16
  %71 = add <4 x i32> %70, %66
  %72 = sub <4 x i32> %66, %70
  %73 = mul nsw i32 %3, 6
  %74 = mul i32 %3, 25
  %75 = sext i32 %73 to i64
  %76 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %75
  %77 = bitcast <2 x i64>* %76 to <4 x i32>*
  %78 = load <4 x i32>, <4 x i32>* %77, align 16
  %79 = sext i32 %74 to i64
  %80 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %79
  %81 = bitcast <2 x i64>* %80 to <4 x i32>*
  %82 = load <4 x i32>, <4 x i32>* %81, align 16
  %83 = add <4 x i32> %82, %78
  %84 = sub <4 x i32> %78, %82
  %85 = mul nsw i32 %3, 7
  %86 = mul i32 %3, 24
  %87 = sext i32 %85 to i64
  %88 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %87
  %89 = bitcast <2 x i64>* %88 to <4 x i32>*
  %90 = load <4 x i32>, <4 x i32>* %89, align 16
  %91 = sext i32 %86 to i64
  %92 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %91
  %93 = bitcast <2 x i64>* %92 to <4 x i32>*
  %94 = load <4 x i32>, <4 x i32>* %93, align 16
  %95 = add <4 x i32> %94, %90
  %96 = sub <4 x i32> %90, %94
  %97 = shl nsw i32 %3, 3
  %98 = mul i32 %3, 23
  %99 = sext i32 %97 to i64
  %100 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %99
  %101 = bitcast <2 x i64>* %100 to <4 x i32>*
  %102 = load <4 x i32>, <4 x i32>* %101, align 16
  %103 = sext i32 %98 to i64
  %104 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %103
  %105 = bitcast <2 x i64>* %104 to <4 x i32>*
  %106 = load <4 x i32>, <4 x i32>* %105, align 16
  %107 = add <4 x i32> %106, %102
  %108 = sub <4 x i32> %102, %106
  %109 = mul nsw i32 %3, 9
  %110 = mul i32 %3, 22
  %111 = sext i32 %109 to i64
  %112 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %111
  %113 = bitcast <2 x i64>* %112 to <4 x i32>*
  %114 = load <4 x i32>, <4 x i32>* %113, align 16
  %115 = sext i32 %110 to i64
  %116 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %115
  %117 = bitcast <2 x i64>* %116 to <4 x i32>*
  %118 = load <4 x i32>, <4 x i32>* %117, align 16
  %119 = add <4 x i32> %118, %114
  %120 = sub <4 x i32> %114, %118
  %121 = mul nsw i32 %3, 10
  %122 = mul i32 %3, 21
  %123 = sext i32 %121 to i64
  %124 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %123
  %125 = bitcast <2 x i64>* %124 to <4 x i32>*
  %126 = load <4 x i32>, <4 x i32>* %125, align 16
  %127 = sext i32 %122 to i64
  %128 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %127
  %129 = bitcast <2 x i64>* %128 to <4 x i32>*
  %130 = load <4 x i32>, <4 x i32>* %129, align 16
  %131 = add <4 x i32> %130, %126
  %132 = sub <4 x i32> %126, %130
  %133 = mul nsw i32 %3, 11
  %134 = mul i32 %3, 20
  %135 = sext i32 %133 to i64
  %136 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %135
  %137 = bitcast <2 x i64>* %136 to <4 x i32>*
  %138 = load <4 x i32>, <4 x i32>* %137, align 16
  %139 = sext i32 %134 to i64
  %140 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %139
  %141 = bitcast <2 x i64>* %140 to <4 x i32>*
  %142 = load <4 x i32>, <4 x i32>* %141, align 16
  %143 = add <4 x i32> %142, %138
  %144 = sub <4 x i32> %138, %142
  %145 = mul nsw i32 %3, 12
  %146 = mul i32 %3, 19
  %147 = sext i32 %145 to i64
  %148 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %147
  %149 = bitcast <2 x i64>* %148 to <4 x i32>*
  %150 = load <4 x i32>, <4 x i32>* %149, align 16
  %151 = sext i32 %146 to i64
  %152 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %151
  %153 = bitcast <2 x i64>* %152 to <4 x i32>*
  %154 = load <4 x i32>, <4 x i32>* %153, align 16
  %155 = add <4 x i32> %154, %150
  %156 = sub <4 x i32> %150, %154
  %157 = mul nsw i32 %3, 13
  %158 = mul i32 %3, 18
  %159 = sext i32 %157 to i64
  %160 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %159
  %161 = bitcast <2 x i64>* %160 to <4 x i32>*
  %162 = load <4 x i32>, <4 x i32>* %161, align 16
  %163 = sext i32 %158 to i64
  %164 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %163
  %165 = bitcast <2 x i64>* %164 to <4 x i32>*
  %166 = load <4 x i32>, <4 x i32>* %165, align 16
  %167 = add <4 x i32> %166, %162
  %168 = sub <4 x i32> %162, %166
  %169 = mul nsw i32 %3, 14
  %170 = mul i32 %3, 17
  %171 = sext i32 %169 to i64
  %172 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %171
  %173 = bitcast <2 x i64>* %172 to <4 x i32>*
  %174 = load <4 x i32>, <4 x i32>* %173, align 16
  %175 = sext i32 %170 to i64
  %176 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %175
  %177 = bitcast <2 x i64>* %176 to <4 x i32>*
  %178 = load <4 x i32>, <4 x i32>* %177, align 16
  %179 = add <4 x i32> %178, %174
  %180 = sub <4 x i32> %174, %178
  %181 = mul nsw i32 %3, 15
  %182 = shl i32 %3, 4
  %183 = sext i32 %181 to i64
  %184 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %183
  %185 = bitcast <2 x i64>* %184 to <4 x i32>*
  %186 = load <4 x i32>, <4 x i32>* %185, align 16
  %187 = sext i32 %182 to i64
  %188 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %187
  %189 = bitcast <2 x i64>* %188 to <4 x i32>*
  %190 = load <4 x i32>, <4 x i32>* %189, align 16
  %191 = add <4 x i32> %190, %186
  %192 = sub <4 x i32> %186, %190
  %193 = add nsw i32 %2, -10
  %194 = sext i32 %193 to i64
  %195 = add <4 x i32> %191, %12
  %196 = sub <4 x i32> %12, %191
  %197 = add <4 x i32> %179, %23
  %198 = sub <4 x i32> %23, %179
  %199 = add <4 x i32> %167, %35
  %200 = sub <4 x i32> %35, %167
  %201 = add <4 x i32> %155, %47
  %202 = sub <4 x i32> %47, %155
  %203 = add <4 x i32> %143, %59
  %204 = sub <4 x i32> %59, %143
  %205 = add <4 x i32> %131, %71
  %206 = sub <4 x i32> %71, %131
  %207 = add <4 x i32> %119, %83
  %208 = sub <4 x i32> %83, %119
  %209 = add <4 x i32> %107, %95
  %210 = sub <4 x i32> %95, %107
  %211 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %194, i64 32
  %212 = load i32, i32* %211, align 16
  %213 = sub nsw i32 0, %212
  %214 = insertelement <4 x i32> undef, i32 %213, i32 0
  %215 = shufflevector <4 x i32> %214, <4 x i32> undef, <4 x i32> zeroinitializer
  %216 = insertelement <4 x i32> undef, i32 %212, i32 0
  %217 = shufflevector <4 x i32> %216, <4 x i32> undef, <4 x i32> zeroinitializer
  %218 = mul <4 x i32> %215, %144
  %219 = mul <4 x i32> %217, %60
  %220 = add nsw i32 %2, -1
  %221 = shl i32 1, %220
  %222 = insertelement <4 x i32> undef, i32 %221, i32 0
  %223 = shufflevector <4 x i32> %222, <4 x i32> undef, <4 x i32> zeroinitializer
  %224 = add <4 x i32> %218, %223
  %225 = add <4 x i32> %224, %219
  %226 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %225, i32 %2) #5
  %227 = mul <4 x i32> %217, %144
  %228 = mul <4 x i32> %60, %215
  %229 = add <4 x i32> %227, %223
  %230 = sub <4 x i32> %229, %228
  %231 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %230, i32 %2) #5
  %232 = mul <4 x i32> %215, %132
  %233 = mul <4 x i32> %217, %72
  %234 = add <4 x i32> %232, %223
  %235 = add <4 x i32> %234, %233
  %236 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %235, i32 %2) #5
  %237 = mul <4 x i32> %217, %132
  %238 = mul <4 x i32> %72, %215
  %239 = add <4 x i32> %237, %223
  %240 = sub <4 x i32> %239, %238
  %241 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %240, i32 %2) #5
  %242 = mul <4 x i32> %215, %120
  %243 = mul <4 x i32> %217, %84
  %244 = add <4 x i32> %242, %223
  %245 = add <4 x i32> %244, %243
  %246 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %245, i32 %2) #5
  %247 = mul <4 x i32> %217, %120
  %248 = mul <4 x i32> %84, %215
  %249 = add <4 x i32> %247, %223
  %250 = sub <4 x i32> %249, %248
  %251 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %250, i32 %2) #5
  %252 = mul <4 x i32> %215, %108
  %253 = mul <4 x i32> %217, %96
  %254 = add <4 x i32> %252, %223
  %255 = add <4 x i32> %254, %253
  %256 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %255, i32 %2) #5
  %257 = mul <4 x i32> %217, %108
  %258 = mul <4 x i32> %96, %215
  %259 = add <4 x i32> %257, %223
  %260 = sub <4 x i32> %259, %258
  %261 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %260, i32 %2) #5
  %262 = add <4 x i32> %195, %209
  %263 = sub <4 x i32> %195, %209
  %264 = add <4 x i32> %197, %207
  %265 = sub <4 x i32> %197, %207
  %266 = add <4 x i32> %199, %205
  %267 = sub <4 x i32> %199, %205
  %268 = add <4 x i32> %201, %203
  %269 = sub <4 x i32> %201, %203
  %270 = mul <4 x i32> %215, %206
  %271 = mul <4 x i32> %217, %200
  %272 = add <4 x i32> %270, %223
  %273 = add <4 x i32> %272, %271
  %274 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %273, i32 %2) #5
  %275 = mul <4 x i32> %217, %206
  %276 = mul <4 x i32> %200, %215
  %277 = add <4 x i32> %275, %223
  %278 = sub <4 x i32> %277, %276
  %279 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %278, i32 %2) #5
  %280 = mul <4 x i32> %215, %204
  %281 = mul <4 x i32> %217, %202
  %282 = add <4 x i32> %280, %223
  %283 = add <4 x i32> %282, %281
  %284 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %283, i32 %2) #5
  %285 = mul <4 x i32> %217, %204
  %286 = mul <4 x i32> %202, %215
  %287 = add <4 x i32> %285, %223
  %288 = sub <4 x i32> %287, %286
  %289 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %288, i32 %2) #5
  %290 = add <4 x i32> %256, %192
  %291 = sub <4 x i32> %192, %256
  %292 = add <4 x i32> %246, %180
  %293 = sub <4 x i32> %180, %246
  %294 = add <4 x i32> %236, %168
  %295 = sub <4 x i32> %168, %236
  %296 = add <4 x i32> %226, %156
  %297 = sub <4 x i32> %156, %226
  %298 = sub <4 x i32> %13, %261
  %299 = add <4 x i32> %261, %13
  %300 = sub <4 x i32> %24, %251
  %301 = add <4 x i32> %251, %24
  %302 = sub <4 x i32> %36, %241
  %303 = add <4 x i32> %241, %36
  %304 = sub <4 x i32> %48, %231
  %305 = add <4 x i32> %231, %48
  %306 = add <4 x i32> %262, %268
  %307 = sub <4 x i32> %262, %268
  %308 = add <4 x i32> %264, %266
  %309 = sub <4 x i32> %264, %266
  %310 = mul <4 x i32> %215, %267
  %311 = mul <4 x i32> %217, %265
  %312 = add <4 x i32> %310, %223
  %313 = add <4 x i32> %312, %311
  %314 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %313, i32 %2) #5
  %315 = mul <4 x i32> %217, %267
  %316 = mul <4 x i32> %265, %215
  %317 = add <4 x i32> %315, %223
  %318 = sub <4 x i32> %317, %316
  %319 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %318, i32 %2) #5
  %320 = add <4 x i32> %284, %210
  %321 = sub <4 x i32> %210, %284
  %322 = add <4 x i32> %274, %208
  %323 = sub <4 x i32> %208, %274
  %324 = sub <4 x i32> %196, %289
  %325 = add <4 x i32> %289, %196
  %326 = sub <4 x i32> %198, %279
  %327 = add <4 x i32> %279, %198
  %328 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %194, i64 16
  %329 = load i32, i32* %328, align 16
  %330 = sub nsw i32 0, %329
  %331 = insertelement <4 x i32> undef, i32 %330, i32 0
  %332 = shufflevector <4 x i32> %331, <4 x i32> undef, <4 x i32> zeroinitializer
  %333 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %194, i64 48
  %334 = load i32, i32* %333, align 16
  %335 = insertelement <4 x i32> undef, i32 %334, i32 0
  %336 = shufflevector <4 x i32> %335, <4 x i32> undef, <4 x i32> zeroinitializer
  %337 = mul <4 x i32> %332, %294
  %338 = mul <4 x i32> %336, %303
  %339 = add <4 x i32> %337, %223
  %340 = add <4 x i32> %339, %338
  %341 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %340, i32 %2) #5
  %342 = mul <4 x i32> %336, %294
  %343 = mul <4 x i32> %303, %332
  %344 = sub <4 x i32> %223, %343
  %345 = add <4 x i32> %344, %342
  %346 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %345, i32 %2) #5
  %347 = mul <4 x i32> %332, %296
  %348 = mul <4 x i32> %336, %305
  %349 = add <4 x i32> %347, %223
  %350 = add <4 x i32> %349, %348
  %351 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %350, i32 %2) #5
  %352 = mul <4 x i32> %336, %296
  %353 = mul <4 x i32> %305, %332
  %354 = sub <4 x i32> %223, %353
  %355 = add <4 x i32> %354, %352
  %356 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %355, i32 %2) #5
  %357 = sub nsw i32 0, %334
  %358 = insertelement <4 x i32> undef, i32 %357, i32 0
  %359 = shufflevector <4 x i32> %358, <4 x i32> undef, <4 x i32> zeroinitializer
  %360 = mul <4 x i32> %359, %297
  %361 = mul <4 x i32> %332, %304
  %362 = add <4 x i32> %361, %223
  %363 = add <4 x i32> %362, %360
  %364 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %363, i32 %2) #5
  %365 = mul <4 x i32> %332, %297
  %366 = mul <4 x i32> %304, %359
  %367 = add <4 x i32> %365, %223
  %368 = sub <4 x i32> %367, %366
  %369 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %368, i32 %2) #5
  %370 = mul <4 x i32> %359, %295
  %371 = mul <4 x i32> %332, %302
  %372 = add <4 x i32> %371, %223
  %373 = add <4 x i32> %372, %370
  %374 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %373, i32 %2) #5
  %375 = mul <4 x i32> %332, %295
  %376 = mul <4 x i32> %302, %359
  %377 = add <4 x i32> %375, %223
  %378 = sub <4 x i32> %377, %376
  %379 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %378, i32 %2) #5
  %380 = mul <4 x i32> %306, %217
  %381 = mul <4 x i32> %217, %308
  %382 = add <4 x i32> %381, %223
  %383 = add <4 x i32> %382, %380
  %384 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %383, i32 %2) #5
  %385 = sub <4 x i32> %223, %381
  %386 = add <4 x i32> %385, %380
  %387 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %386, i32 %2) #5
  %388 = insertelement <4 x i32> undef, i32 %329, i32 0
  %389 = shufflevector <4 x i32> %388, <4 x i32> undef, <4 x i32> zeroinitializer
  %390 = mul <4 x i32> %389, %307
  %391 = mul <4 x i32> %336, %309
  %392 = add <4 x i32> %390, %223
  %393 = add <4 x i32> %392, %391
  %394 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %393, i32 %2) #5
  %395 = mul <4 x i32> %336, %307
  %396 = mul <4 x i32> %309, %389
  %397 = sub <4 x i32> %223, %396
  %398 = add <4 x i32> %397, %395
  %399 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %398, i32 %2) #5
  %400 = add <4 x i32> %314, %269
  %401 = sub <4 x i32> %269, %314
  %402 = sub <4 x i32> %263, %319
  %403 = add <4 x i32> %319, %263
  %404 = mul <4 x i32> %332, %322
  %405 = mul <4 x i32> %336, %327
  %406 = add <4 x i32> %404, %223
  %407 = add <4 x i32> %406, %405
  %408 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %407, i32 %2) #5
  %409 = mul <4 x i32> %336, %322
  %410 = mul <4 x i32> %327, %332
  %411 = sub <4 x i32> %223, %410
  %412 = add <4 x i32> %411, %409
  %413 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %412, i32 %2) #5
  %414 = mul <4 x i32> %359, %323
  %415 = mul <4 x i32> %332, %326
  %416 = add <4 x i32> %415, %223
  %417 = add <4 x i32> %416, %414
  %418 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %417, i32 %2) #5
  %419 = mul <4 x i32> %332, %323
  %420 = mul <4 x i32> %326, %359
  %421 = add <4 x i32> %419, %223
  %422 = sub <4 x i32> %421, %420
  %423 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %422, i32 %2) #5
  %424 = add <4 x i32> %351, %290
  %425 = sub <4 x i32> %290, %351
  %426 = add <4 x i32> %341, %292
  %427 = sub <4 x i32> %292, %341
  %428 = sub <4 x i32> %291, %364
  %429 = add <4 x i32> %364, %291
  %430 = sub <4 x i32> %293, %374
  %431 = add <4 x i32> %374, %293
  %432 = add <4 x i32> %369, %298
  %433 = sub <4 x i32> %298, %369
  %434 = add <4 x i32> %379, %300
  %435 = sub <4 x i32> %300, %379
  %436 = sub <4 x i32> %299, %356
  %437 = add <4 x i32> %356, %299
  %438 = sub <4 x i32> %301, %346
  %439 = add <4 x i32> %346, %301
  %440 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %194, i64 8
  %441 = load i32, i32* %440, align 16
  %442 = insertelement <4 x i32> undef, i32 %441, i32 0
  %443 = shufflevector <4 x i32> %442, <4 x i32> undef, <4 x i32> zeroinitializer
  %444 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %194, i64 56
  %445 = load i32, i32* %444, align 16
  %446 = insertelement <4 x i32> undef, i32 %445, i32 0
  %447 = shufflevector <4 x i32> %446, <4 x i32> undef, <4 x i32> zeroinitializer
  %448 = mul <4 x i32> %443, %403
  %449 = mul <4 x i32> %447, %400
  %450 = add <4 x i32> %448, %223
  %451 = add <4 x i32> %450, %449
  %452 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %451, i32 %2) #5
  %453 = mul <4 x i32> %447, %403
  %454 = mul <4 x i32> %400, %443
  %455 = sub <4 x i32> %223, %454
  %456 = add <4 x i32> %455, %453
  %457 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %456, i32 %2) #5
  %458 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %194, i64 40
  %459 = load i32, i32* %458, align 16
  %460 = insertelement <4 x i32> undef, i32 %459, i32 0
  %461 = shufflevector <4 x i32> %460, <4 x i32> undef, <4 x i32> zeroinitializer
  %462 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %194, i64 24
  %463 = load i32, i32* %462, align 16
  %464 = insertelement <4 x i32> undef, i32 %463, i32 0
  %465 = shufflevector <4 x i32> %464, <4 x i32> undef, <4 x i32> zeroinitializer
  %466 = mul <4 x i32> %461, %402
  %467 = mul <4 x i32> %465, %401
  %468 = add <4 x i32> %466, %223
  %469 = add <4 x i32> %468, %467
  %470 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %469, i32 %2) #5
  %471 = mul <4 x i32> %465, %402
  %472 = mul <4 x i32> %401, %461
  %473 = sub <4 x i32> %223, %472
  %474 = add <4 x i32> %473, %471
  %475 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %474, i32 %2) #5
  %476 = add <4 x i32> %408, %320
  %477 = sub <4 x i32> %320, %408
  %478 = sub <4 x i32> %321, %418
  %479 = add <4 x i32> %418, %321
  %480 = add <4 x i32> %423, %324
  %481 = sub <4 x i32> %324, %423
  %482 = sub <4 x i32> %325, %413
  %483 = add <4 x i32> %413, %325
  %484 = sub nsw i32 0, %441
  %485 = insertelement <4 x i32> undef, i32 %484, i32 0
  %486 = shufflevector <4 x i32> %485, <4 x i32> undef, <4 x i32> zeroinitializer
  %487 = mul <4 x i32> %486, %426
  %488 = mul <4 x i32> %447, %439
  %489 = add <4 x i32> %487, %223
  %490 = add <4 x i32> %489, %488
  %491 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %490, i32 %2) #5
  %492 = mul <4 x i32> %447, %426
  %493 = mul <4 x i32> %439, %486
  %494 = sub <4 x i32> %223, %493
  %495 = add <4 x i32> %494, %492
  %496 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %495, i32 %2) #5
  %497 = sub nsw i32 0, %445
  %498 = insertelement <4 x i32> undef, i32 %497, i32 0
  %499 = shufflevector <4 x i32> %498, <4 x i32> undef, <4 x i32> zeroinitializer
  %500 = mul <4 x i32> %499, %427
  %501 = mul <4 x i32> %486, %438
  %502 = add <4 x i32> %501, %223
  %503 = add <4 x i32> %502, %500
  %504 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %503, i32 %2) #5
  %505 = mul <4 x i32> %486, %427
  %506 = mul <4 x i32> %438, %499
  %507 = add <4 x i32> %505, %223
  %508 = sub <4 x i32> %507, %506
  %509 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %508, i32 %2) #5
  %510 = sub nsw i32 0, %459
  %511 = insertelement <4 x i32> undef, i32 %510, i32 0
  %512 = shufflevector <4 x i32> %511, <4 x i32> undef, <4 x i32> zeroinitializer
  %513 = mul <4 x i32> %512, %430
  %514 = mul <4 x i32> %465, %435
  %515 = add <4 x i32> %513, %223
  %516 = add <4 x i32> %515, %514
  %517 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %516, i32 %2) #5
  %518 = mul <4 x i32> %465, %430
  %519 = mul <4 x i32> %435, %512
  %520 = sub <4 x i32> %223, %519
  %521 = add <4 x i32> %520, %518
  %522 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %521, i32 %2) #5
  %523 = sub nsw i32 0, %463
  %524 = insertelement <4 x i32> undef, i32 %523, i32 0
  %525 = shufflevector <4 x i32> %524, <4 x i32> undef, <4 x i32> zeroinitializer
  %526 = mul <4 x i32> %525, %431
  %527 = mul <4 x i32> %512, %434
  %528 = add <4 x i32> %527, %223
  %529 = add <4 x i32> %528, %526
  %530 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %529, i32 %2) #5
  %531 = mul <4 x i32> %512, %431
  %532 = mul <4 x i32> %434, %525
  %533 = add <4 x i32> %531, %223
  %534 = sub <4 x i32> %533, %532
  %535 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %534, i32 %2) #5
  %536 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %194, i64 4
  %537 = load i32, i32* %536, align 16
  %538 = insertelement <4 x i32> undef, i32 %537, i32 0
  %539 = shufflevector <4 x i32> %538, <4 x i32> undef, <4 x i32> zeroinitializer
  %540 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %194, i64 60
  %541 = load i32, i32* %540, align 16
  %542 = insertelement <4 x i32> undef, i32 %541, i32 0
  %543 = shufflevector <4 x i32> %542, <4 x i32> undef, <4 x i32> zeroinitializer
  %544 = mul <4 x i32> %539, %483
  %545 = mul <4 x i32> %543, %476
  %546 = add <4 x i32> %544, %223
  %547 = add <4 x i32> %546, %545
  %548 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %547, i32 %2) #5
  %549 = mul <4 x i32> %543, %483
  %550 = mul <4 x i32> %476, %539
  %551 = sub <4 x i32> %223, %550
  %552 = add <4 x i32> %551, %549
  %553 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %552, i32 %2) #5
  %554 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %194, i64 36
  %555 = load i32, i32* %554, align 16
  %556 = insertelement <4 x i32> undef, i32 %555, i32 0
  %557 = shufflevector <4 x i32> %556, <4 x i32> undef, <4 x i32> zeroinitializer
  %558 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %194, i64 28
  %559 = load i32, i32* %558, align 16
  %560 = insertelement <4 x i32> undef, i32 %559, i32 0
  %561 = shufflevector <4 x i32> %560, <4 x i32> undef, <4 x i32> zeroinitializer
  %562 = mul <4 x i32> %557, %482
  %563 = mul <4 x i32> %561, %477
  %564 = add <4 x i32> %562, %223
  %565 = add <4 x i32> %564, %563
  %566 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %565, i32 %2) #5
  %567 = mul <4 x i32> %561, %482
  %568 = mul <4 x i32> %477, %557
  %569 = sub <4 x i32> %223, %568
  %570 = add <4 x i32> %569, %567
  %571 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %570, i32 %2) #5
  %572 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %194, i64 20
  %573 = load i32, i32* %572, align 16
  %574 = insertelement <4 x i32> undef, i32 %573, i32 0
  %575 = shufflevector <4 x i32> %574, <4 x i32> undef, <4 x i32> zeroinitializer
  %576 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %194, i64 44
  %577 = load i32, i32* %576, align 16
  %578 = insertelement <4 x i32> undef, i32 %577, i32 0
  %579 = shufflevector <4 x i32> %578, <4 x i32> undef, <4 x i32> zeroinitializer
  %580 = mul <4 x i32> %575, %481
  %581 = mul <4 x i32> %579, %478
  %582 = add <4 x i32> %580, %223
  %583 = add <4 x i32> %582, %581
  %584 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %583, i32 %2) #5
  %585 = mul <4 x i32> %579, %481
  %586 = mul <4 x i32> %478, %575
  %587 = sub <4 x i32> %223, %586
  %588 = add <4 x i32> %587, %585
  %589 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %588, i32 %2) #5
  %590 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %194, i64 52
  %591 = load i32, i32* %590, align 16
  %592 = insertelement <4 x i32> undef, i32 %591, i32 0
  %593 = shufflevector <4 x i32> %592, <4 x i32> undef, <4 x i32> zeroinitializer
  %594 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %194, i64 12
  %595 = load i32, i32* %594, align 16
  %596 = insertelement <4 x i32> undef, i32 %595, i32 0
  %597 = shufflevector <4 x i32> %596, <4 x i32> undef, <4 x i32> zeroinitializer
  %598 = mul <4 x i32> %593, %480
  %599 = mul <4 x i32> %597, %479
  %600 = add <4 x i32> %598, %223
  %601 = add <4 x i32> %600, %599
  %602 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %601, i32 %2) #5
  %603 = mul <4 x i32> %597, %480
  %604 = mul <4 x i32> %479, %593
  %605 = sub <4 x i32> %223, %604
  %606 = add <4 x i32> %605, %603
  %607 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %606, i32 %2) #5
  %608 = add <4 x i32> %491, %424
  %609 = sub <4 x i32> %424, %491
  %610 = sub <4 x i32> %425, %504
  %611 = add <4 x i32> %504, %425
  %612 = add <4 x i32> %517, %428
  %613 = sub <4 x i32> %428, %517
  %614 = sub <4 x i32> %429, %530
  %615 = add <4 x i32> %530, %429
  %616 = add <4 x i32> %535, %432
  %617 = sub <4 x i32> %432, %535
  %618 = sub <4 x i32> %433, %522
  %619 = add <4 x i32> %522, %433
  %620 = add <4 x i32> %509, %436
  %621 = sub <4 x i32> %436, %509
  %622 = sub <4 x i32> %437, %496
  %623 = add <4 x i32> %496, %437
  %624 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %194, i64 2
  %625 = load i32, i32* %624, align 8
  %626 = insertelement <4 x i32> undef, i32 %625, i32 0
  %627 = shufflevector <4 x i32> %626, <4 x i32> undef, <4 x i32> zeroinitializer
  %628 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %194, i64 62
  %629 = load i32, i32* %628, align 8
  %630 = insertelement <4 x i32> undef, i32 %629, i32 0
  %631 = shufflevector <4 x i32> %630, <4 x i32> undef, <4 x i32> zeroinitializer
  %632 = mul <4 x i32> %627, %623
  %633 = mul <4 x i32> %631, %608
  %634 = add <4 x i32> %632, %223
  %635 = add <4 x i32> %634, %633
  %636 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %635, i32 %2) #5
  %637 = mul <4 x i32> %631, %623
  %638 = mul <4 x i32> %608, %627
  %639 = sub <4 x i32> %223, %638
  %640 = add <4 x i32> %639, %637
  %641 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %640, i32 %2) #5
  %642 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %194, i64 34
  %643 = load i32, i32* %642, align 8
  %644 = insertelement <4 x i32> undef, i32 %643, i32 0
  %645 = shufflevector <4 x i32> %644, <4 x i32> undef, <4 x i32> zeroinitializer
  %646 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %194, i64 30
  %647 = load i32, i32* %646, align 8
  %648 = insertelement <4 x i32> undef, i32 %647, i32 0
  %649 = shufflevector <4 x i32> %648, <4 x i32> undef, <4 x i32> zeroinitializer
  %650 = mul <4 x i32> %645, %622
  %651 = mul <4 x i32> %649, %609
  %652 = add <4 x i32> %650, %223
  %653 = add <4 x i32> %652, %651
  %654 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %653, i32 %2) #5
  %655 = mul <4 x i32> %649, %622
  %656 = mul <4 x i32> %609, %645
  %657 = sub <4 x i32> %223, %656
  %658 = add <4 x i32> %657, %655
  %659 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %658, i32 %2) #5
  %660 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %194, i64 18
  %661 = load i32, i32* %660, align 8
  %662 = insertelement <4 x i32> undef, i32 %661, i32 0
  %663 = shufflevector <4 x i32> %662, <4 x i32> undef, <4 x i32> zeroinitializer
  %664 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %194, i64 46
  %665 = load i32, i32* %664, align 8
  %666 = insertelement <4 x i32> undef, i32 %665, i32 0
  %667 = shufflevector <4 x i32> %666, <4 x i32> undef, <4 x i32> zeroinitializer
  %668 = mul <4 x i32> %663, %621
  %669 = mul <4 x i32> %667, %610
  %670 = add <4 x i32> %668, %223
  %671 = add <4 x i32> %670, %669
  %672 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %671, i32 %2) #5
  %673 = mul <4 x i32> %667, %621
  %674 = mul <4 x i32> %610, %663
  %675 = sub <4 x i32> %223, %674
  %676 = add <4 x i32> %675, %673
  %677 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %676, i32 %2) #5
  %678 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %194, i64 50
  %679 = load i32, i32* %678, align 8
  %680 = insertelement <4 x i32> undef, i32 %679, i32 0
  %681 = shufflevector <4 x i32> %680, <4 x i32> undef, <4 x i32> zeroinitializer
  %682 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %194, i64 14
  %683 = load i32, i32* %682, align 8
  %684 = insertelement <4 x i32> undef, i32 %683, i32 0
  %685 = shufflevector <4 x i32> %684, <4 x i32> undef, <4 x i32> zeroinitializer
  %686 = mul <4 x i32> %681, %620
  %687 = mul <4 x i32> %685, %611
  %688 = add <4 x i32> %686, %223
  %689 = add <4 x i32> %688, %687
  %690 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %689, i32 %2) #5
  %691 = mul <4 x i32> %685, %620
  %692 = mul <4 x i32> %611, %681
  %693 = sub <4 x i32> %223, %692
  %694 = add <4 x i32> %693, %691
  %695 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %694, i32 %2) #5
  %696 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %194, i64 10
  %697 = load i32, i32* %696, align 8
  %698 = insertelement <4 x i32> undef, i32 %697, i32 0
  %699 = shufflevector <4 x i32> %698, <4 x i32> undef, <4 x i32> zeroinitializer
  %700 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %194, i64 54
  %701 = load i32, i32* %700, align 8
  %702 = insertelement <4 x i32> undef, i32 %701, i32 0
  %703 = shufflevector <4 x i32> %702, <4 x i32> undef, <4 x i32> zeroinitializer
  %704 = mul <4 x i32> %699, %619
  %705 = mul <4 x i32> %703, %612
  %706 = add <4 x i32> %704, %223
  %707 = add <4 x i32> %706, %705
  %708 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %707, i32 %2) #5
  %709 = mul <4 x i32> %703, %619
  %710 = mul <4 x i32> %612, %699
  %711 = sub <4 x i32> %223, %710
  %712 = add <4 x i32> %711, %709
  %713 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %712, i32 %2) #5
  %714 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %194, i64 42
  %715 = load i32, i32* %714, align 8
  %716 = insertelement <4 x i32> undef, i32 %715, i32 0
  %717 = shufflevector <4 x i32> %716, <4 x i32> undef, <4 x i32> zeroinitializer
  %718 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %194, i64 22
  %719 = load i32, i32* %718, align 8
  %720 = insertelement <4 x i32> undef, i32 %719, i32 0
  %721 = shufflevector <4 x i32> %720, <4 x i32> undef, <4 x i32> zeroinitializer
  %722 = mul <4 x i32> %717, %618
  %723 = mul <4 x i32> %721, %613
  %724 = add <4 x i32> %722, %223
  %725 = add <4 x i32> %724, %723
  %726 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %725, i32 %2) #5
  %727 = mul <4 x i32> %721, %618
  %728 = mul <4 x i32> %613, %717
  %729 = sub <4 x i32> %223, %728
  %730 = add <4 x i32> %729, %727
  %731 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %730, i32 %2) #5
  %732 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %194, i64 26
  %733 = load i32, i32* %732, align 8
  %734 = insertelement <4 x i32> undef, i32 %733, i32 0
  %735 = shufflevector <4 x i32> %734, <4 x i32> undef, <4 x i32> zeroinitializer
  %736 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %194, i64 38
  %737 = load i32, i32* %736, align 8
  %738 = insertelement <4 x i32> undef, i32 %737, i32 0
  %739 = shufflevector <4 x i32> %738, <4 x i32> undef, <4 x i32> zeroinitializer
  %740 = mul <4 x i32> %735, %617
  %741 = mul <4 x i32> %739, %614
  %742 = add <4 x i32> %740, %223
  %743 = add <4 x i32> %742, %741
  %744 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %743, i32 %2) #5
  %745 = mul <4 x i32> %739, %617
  %746 = mul <4 x i32> %614, %735
  %747 = sub <4 x i32> %223, %746
  %748 = add <4 x i32> %747, %745
  %749 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %748, i32 %2) #5
  %750 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %194, i64 58
  %751 = load i32, i32* %750, align 8
  %752 = insertelement <4 x i32> undef, i32 %751, i32 0
  %753 = shufflevector <4 x i32> %752, <4 x i32> undef, <4 x i32> zeroinitializer
  %754 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %194, i64 6
  %755 = load i32, i32* %754, align 8
  %756 = insertelement <4 x i32> undef, i32 %755, i32 0
  %757 = shufflevector <4 x i32> %756, <4 x i32> undef, <4 x i32> zeroinitializer
  %758 = mul <4 x i32> %753, %616
  %759 = mul <4 x i32> %757, %615
  %760 = add <4 x i32> %758, %223
  %761 = add <4 x i32> %760, %759
  %762 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %761, i32 %2) #5
  %763 = mul <4 x i32> %757, %616
  %764 = mul <4 x i32> %615, %753
  %765 = sub <4 x i32> %223, %764
  %766 = add <4 x i32> %765, %763
  %767 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %766, i32 %2) #5
  %768 = bitcast <2 x i64>* %1 to <4 x i32>*
  store <4 x i32> %384, <4 x i32>* %768, align 16
  %769 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %8
  %770 = bitcast <2 x i64>* %769 to <4 x i32>*
  store <4 x i32> %641, <4 x i32>* %770, align 16
  %771 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %15
  %772 = bitcast <2 x i64>* %771 to <4 x i32>*
  store <4 x i32> %636, <4 x i32>* %772, align 16
  %773 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %19
  %774 = bitcast <2 x i64>* %773 to <4 x i32>*
  store <4 x i32> %553, <4 x i32>* %774, align 16
  %775 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %27
  %776 = bitcast <2 x i64>* %775 to <4 x i32>*
  store <4 x i32> %548, <4 x i32>* %776, align 16
  %777 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %31
  %778 = bitcast <2 x i64>* %777 to <4 x i32>*
  store <4 x i32> %762, <4 x i32>* %778, align 16
  %779 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %39
  %780 = bitcast <2 x i64>* %779 to <4 x i32>*
  store <4 x i32> %767, <4 x i32>* %780, align 16
  %781 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %43
  %782 = bitcast <2 x i64>* %781 to <4 x i32>*
  store <4 x i32> %457, <4 x i32>* %782, align 16
  %783 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %51
  %784 = bitcast <2 x i64>* %783 to <4 x i32>*
  store <4 x i32> %452, <4 x i32>* %784, align 16
  %785 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %55
  %786 = bitcast <2 x i64>* %785 to <4 x i32>*
  store <4 x i32> %713, <4 x i32>* %786, align 16
  %787 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %63
  %788 = bitcast <2 x i64>* %787 to <4 x i32>*
  store <4 x i32> %708, <4 x i32>* %788, align 16
  %789 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %67
  %790 = bitcast <2 x i64>* %789 to <4 x i32>*
  store <4 x i32> %602, <4 x i32>* %790, align 16
  %791 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %75
  %792 = bitcast <2 x i64>* %791 to <4 x i32>*
  store <4 x i32> %607, <4 x i32>* %792, align 16
  %793 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %79
  %794 = bitcast <2 x i64>* %793 to <4 x i32>*
  store <4 x i32> %690, <4 x i32>* %794, align 16
  %795 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %87
  %796 = bitcast <2 x i64>* %795 to <4 x i32>*
  store <4 x i32> %695, <4 x i32>* %796, align 16
  %797 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %91
  %798 = bitcast <2 x i64>* %797 to <4 x i32>*
  store <4 x i32> %399, <4 x i32>* %798, align 16
  %799 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %99
  %800 = bitcast <2 x i64>* %799 to <4 x i32>*
  store <4 x i32> %394, <4 x i32>* %800, align 16
  %801 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %103
  %802 = bitcast <2 x i64>* %801 to <4 x i32>*
  store <4 x i32> %677, <4 x i32>* %802, align 16
  %803 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %111
  %804 = bitcast <2 x i64>* %803 to <4 x i32>*
  store <4 x i32> %672, <4 x i32>* %804, align 16
  %805 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %115
  %806 = bitcast <2 x i64>* %805 to <4 x i32>*
  store <4 x i32> %589, <4 x i32>* %806, align 16
  %807 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %123
  %808 = bitcast <2 x i64>* %807 to <4 x i32>*
  store <4 x i32> %584, <4 x i32>* %808, align 16
  %809 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %127
  %810 = bitcast <2 x i64>* %809 to <4 x i32>*
  store <4 x i32> %726, <4 x i32>* %810, align 16
  %811 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %135
  %812 = bitcast <2 x i64>* %811 to <4 x i32>*
  store <4 x i32> %731, <4 x i32>* %812, align 16
  %813 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %139
  %814 = bitcast <2 x i64>* %813 to <4 x i32>*
  store <4 x i32> %470, <4 x i32>* %814, align 16
  %815 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %147
  %816 = bitcast <2 x i64>* %815 to <4 x i32>*
  store <4 x i32> %475, <4 x i32>* %816, align 16
  %817 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %151
  %818 = bitcast <2 x i64>* %817 to <4 x i32>*
  store <4 x i32> %749, <4 x i32>* %818, align 16
  %819 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %159
  %820 = bitcast <2 x i64>* %819 to <4 x i32>*
  store <4 x i32> %744, <4 x i32>* %820, align 16
  %821 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %163
  %822 = bitcast <2 x i64>* %821 to <4 x i32>*
  store <4 x i32> %566, <4 x i32>* %822, align 16
  %823 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %171
  %824 = bitcast <2 x i64>* %823 to <4 x i32>*
  store <4 x i32> %571, <4 x i32>* %824, align 16
  %825 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %175
  %826 = bitcast <2 x i64>* %825 to <4 x i32>*
  store <4 x i32> %654, <4 x i32>* %826, align 16
  %827 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %183
  %828 = bitcast <2 x i64>* %827 to <4 x i32>*
  store <4 x i32> %659, <4 x i32>* %828, align 16
  %829 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %187
  %830 = bitcast <2 x i64>* %829 to <4 x i32>*
  store <4 x i32> %387, <4 x i32>* %830, align 16
  ret void
}

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #1

; Function Attrs: argmemonly nounwind
declare void @llvm.memset.p0i8.i64(i8* nocapture writeonly, i8, i64, i1 immarg) #1

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #1

; Function Attrs: nounwind ssp uwtable
define hidden void @av1_fadst4_sse4_1(<2 x i64>* nocapture readonly, <2 x i64>* nocapture, i8 signext, i8* nocapture readnone) local_unnamed_addr #2 {
  %5 = alloca [4 x <2 x i64>], align 16
  %6 = bitcast [4 x <2 x i64>]* %5 to i8*
  %7 = alloca [4 x <2 x i64>], align 16
  %8 = bitcast [4 x <2 x i64>]* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %6) #5
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %6, i8 -86, i64 64, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 64, i8* nonnull %8) #5
  %9 = getelementptr inbounds [4 x <2 x i64>], [4 x <2 x i64>]* %5, i64 0, i64 3
  %10 = getelementptr inbounds [4 x <2 x i64>], [4 x <2 x i64>]* %7, i64 0, i64 1
  %11 = getelementptr inbounds [4 x <2 x i64>], [4 x <2 x i64>]* %5, i64 0, i64 1
  %12 = getelementptr inbounds [4 x <2 x i64>], [4 x <2 x i64>]* %7, i64 0, i64 2
  %13 = getelementptr inbounds [4 x <2 x i64>], [4 x <2 x i64>]* %5, i64 0, i64 2
  %14 = getelementptr inbounds [4 x <2 x i64>], [4 x <2 x i64>]* %7, i64 0, i64 3
  %15 = sext i8 %2 to i32
  %16 = add nsw i32 %15, -10
  %17 = sext i32 %16 to i64
  %18 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %17, i64 8
  %19 = load i32, i32* %18, align 16
  %20 = insertelement <4 x i32> undef, i32 %19, i32 0
  %21 = shufflevector <4 x i32> %20, <4 x i32> undef, <4 x i32> zeroinitializer
  %22 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %17, i64 56
  %23 = load i32, i32* %22, align 16
  %24 = insertelement <4 x i32> undef, i32 %23, i32 0
  %25 = shufflevector <4 x i32> %24, <4 x i32> undef, <4 x i32> zeroinitializer
  %26 = add nsw i32 %15, -1
  %27 = shl i32 1, %26
  %28 = insertelement <4 x i32> undef, i32 %27, i32 0
  %29 = shufflevector <4 x i32> %28, <4 x i32> undef, <4 x i32> zeroinitializer
  %30 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %17, i64 40
  %31 = load i32, i32* %30, align 16
  %32 = insertelement <4 x i32> undef, i32 %31, i32 0
  %33 = shufflevector <4 x i32> %32, <4 x i32> undef, <4 x i32> zeroinitializer
  %34 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %17, i64 24
  %35 = load i32, i32* %34, align 16
  %36 = insertelement <4 x i32> undef, i32 %35, i32 0
  %37 = shufflevector <4 x i32> %36, <4 x i32> undef, <4 x i32> zeroinitializer
  %38 = bitcast [4 x <2 x i64>]* %7 to <4 x i32>*
  %39 = bitcast <2 x i64>* %12 to <4 x i32>*
  %40 = bitcast <2 x i64>* %10 to <4 x i32>*
  %41 = bitcast <2 x i64>* %14 to <4 x i32>*
  %42 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %17, i64 32
  %43 = load i32, i32* %42, align 16
  %44 = insertelement <4 x i32> undef, i32 %43, i32 0
  %45 = shufflevector <4 x i32> %44, <4 x i32> undef, <4 x i32> zeroinitializer
  %46 = bitcast <2 x i64>* %1 to i8*
  %47 = bitcast <2 x i64>* %0 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 16 %6, i8* align 16 %47, i64 64, i1 false)
  %48 = bitcast <2 x i64>* %9 to <4 x i32>*
  %49 = load <4 x i32>, <4 x i32>* %48, align 16
  %50 = bitcast [4 x <2 x i64>]* %5 to <4 x i32>*
  %51 = load <4 x i32>, <4 x i32>* %50, align 16
  %52 = bitcast <2 x i64>* %11 to <4 x i32>*
  %53 = load <4 x i32>, <4 x i32>* %52, align 16
  %54 = bitcast <2 x i64>* %13 to <4 x i32>*
  %55 = load <4 x i32>, <4 x i32>* %54, align 16
  %56 = mul <4 x i32> %21, %49
  %57 = mul <4 x i32> %25, %51
  %58 = add <4 x i32> %56, %29
  %59 = add <4 x i32> %58, %57
  %60 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %59, i32 %15) #5
  %61 = mul <4 x i32> %25, %49
  %62 = mul <4 x i32> %21, %51
  %63 = sub <4 x i32> %29, %62
  %64 = add <4 x i32> %63, %61
  %65 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %64, i32 %15) #5
  %66 = mul <4 x i32> %33, %53
  %67 = mul <4 x i32> %37, %55
  %68 = add <4 x i32> %66, %29
  %69 = add <4 x i32> %68, %67
  %70 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %69, i32 %15) #5
  %71 = mul <4 x i32> %37, %53
  %72 = mul <4 x i32> %33, %55
  %73 = sub <4 x i32> %29, %72
  %74 = add <4 x i32> %73, %71
  %75 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %74, i32 %15) #5
  %76 = add <4 x i32> %70, %60
  %77 = sub <4 x i32> %60, %70
  %78 = add <4 x i32> %75, %65
  %79 = sub <4 x i32> %65, %75
  %80 = mul <4 x i32> %45, %77
  %81 = mul <4 x i32> %45, %79
  %82 = add <4 x i32> %80, %29
  %83 = add <4 x i32> %82, %81
  %84 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %83, i32 %15) #5
  %85 = sub <4 x i32> %29, %81
  %86 = add <4 x i32> %85, %80
  %87 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %86, i32 %15) #5
  store <4 x i32> %76, <4 x i32>* %38, align 16
  %88 = sub <4 x i32> zeroinitializer, %84
  store <4 x i32> %88, <4 x i32>* %40, align 16
  store <4 x i32> %87, <4 x i32>* %39, align 16
  %89 = sub <4 x i32> zeroinitializer, %78
  store <4 x i32> %89, <4 x i32>* %41, align 16
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 16 %46, i8* nonnull align 16 %8, i64 64, i1 false)
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %8) #5
  call void @llvm.lifetime.end.p0i8(i64 64, i8* nonnull %6) #5
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define hidden void @av1_fdct64_sse4_1(<2 x i64>* readonly, <2 x i64>*, i8 signext, i32, i32) local_unnamed_addr #0 {
  %6 = sext i8 %2 to i32
  %7 = add nsw i32 %6, -10
  %8 = sext i32 %7 to i64
  %9 = add nsw i32 %6, -1
  %10 = shl i32 1, %9
  %11 = insertelement <4 x i32> undef, i32 %10, i32 0
  %12 = shufflevector <4 x i32> %11, <4 x i32> undef, <4 x i32> zeroinitializer
  %13 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 32
  %14 = load i32, i32* %13, align 16
  %15 = sub nsw i32 0, %14
  %16 = insertelement <4 x i32> undef, i32 %15, i32 0
  %17 = shufflevector <4 x i32> %16, <4 x i32> undef, <4 x i32> zeroinitializer
  %18 = insertelement <4 x i32> undef, i32 %14, i32 0
  %19 = shufflevector <4 x i32> %18, <4 x i32> undef, <4 x i32> zeroinitializer
  %20 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 16
  %21 = load i32, i32* %20, align 16
  %22 = sub nsw i32 0, %21
  %23 = insertelement <4 x i32> undef, i32 %22, i32 0
  %24 = shufflevector <4 x i32> %23, <4 x i32> undef, <4 x i32> zeroinitializer
  %25 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 48
  %26 = load i32, i32* %25, align 16
  %27 = insertelement <4 x i32> undef, i32 %26, i32 0
  %28 = shufflevector <4 x i32> %27, <4 x i32> undef, <4 x i32> zeroinitializer
  %29 = sub nsw i32 0, %26
  %30 = insertelement <4 x i32> undef, i32 %29, i32 0
  %31 = shufflevector <4 x i32> %30, <4 x i32> undef, <4 x i32> zeroinitializer
  %32 = insertelement <4 x i32> undef, i32 %21, i32 0
  %33 = shufflevector <4 x i32> %32, <4 x i32> undef, <4 x i32> zeroinitializer
  %34 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 8
  %35 = load i32, i32* %34, align 16
  %36 = sub nsw i32 0, %35
  %37 = insertelement <4 x i32> undef, i32 %36, i32 0
  %38 = shufflevector <4 x i32> %37, <4 x i32> undef, <4 x i32> zeroinitializer
  %39 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 56
  %40 = load i32, i32* %39, align 16
  %41 = insertelement <4 x i32> undef, i32 %40, i32 0
  %42 = shufflevector <4 x i32> %41, <4 x i32> undef, <4 x i32> zeroinitializer
  %43 = sub nsw i32 0, %40
  %44 = insertelement <4 x i32> undef, i32 %43, i32 0
  %45 = shufflevector <4 x i32> %44, <4 x i32> undef, <4 x i32> zeroinitializer
  %46 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 40
  %47 = load i32, i32* %46, align 16
  %48 = sub nsw i32 0, %47
  %49 = insertelement <4 x i32> undef, i32 %48, i32 0
  %50 = shufflevector <4 x i32> %49, <4 x i32> undef, <4 x i32> zeroinitializer
  %51 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 24
  %52 = load i32, i32* %51, align 16
  %53 = insertelement <4 x i32> undef, i32 %52, i32 0
  %54 = shufflevector <4 x i32> %53, <4 x i32> undef, <4 x i32> zeroinitializer
  %55 = sub nsw i32 0, %52
  %56 = insertelement <4 x i32> undef, i32 %55, i32 0
  %57 = shufflevector <4 x i32> %56, <4 x i32> undef, <4 x i32> zeroinitializer
  %58 = insertelement <4 x i32> undef, i32 %35, i32 0
  %59 = shufflevector <4 x i32> %58, <4 x i32> undef, <4 x i32> zeroinitializer
  %60 = insertelement <4 x i32> undef, i32 %47, i32 0
  %61 = shufflevector <4 x i32> %60, <4 x i32> undef, <4 x i32> zeroinitializer
  %62 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 60
  %63 = load i32, i32* %62, align 16
  %64 = insertelement <4 x i32> undef, i32 %63, i32 0
  %65 = shufflevector <4 x i32> %64, <4 x i32> undef, <4 x i32> zeroinitializer
  %66 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 4
  %67 = load i32, i32* %66, align 16
  %68 = insertelement <4 x i32> undef, i32 %67, i32 0
  %69 = shufflevector <4 x i32> %68, <4 x i32> undef, <4 x i32> zeroinitializer
  %70 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 28
  %71 = load i32, i32* %70, align 16
  %72 = insertelement <4 x i32> undef, i32 %71, i32 0
  %73 = shufflevector <4 x i32> %72, <4 x i32> undef, <4 x i32> zeroinitializer
  %74 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 36
  %75 = load i32, i32* %74, align 16
  %76 = insertelement <4 x i32> undef, i32 %75, i32 0
  %77 = shufflevector <4 x i32> %76, <4 x i32> undef, <4 x i32> zeroinitializer
  %78 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 44
  %79 = load i32, i32* %78, align 16
  %80 = insertelement <4 x i32> undef, i32 %79, i32 0
  %81 = shufflevector <4 x i32> %80, <4 x i32> undef, <4 x i32> zeroinitializer
  %82 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 20
  %83 = load i32, i32* %82, align 16
  %84 = insertelement <4 x i32> undef, i32 %83, i32 0
  %85 = shufflevector <4 x i32> %84, <4 x i32> undef, <4 x i32> zeroinitializer
  %86 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 12
  %87 = load i32, i32* %86, align 16
  %88 = insertelement <4 x i32> undef, i32 %87, i32 0
  %89 = shufflevector <4 x i32> %88, <4 x i32> undef, <4 x i32> zeroinitializer
  %90 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 52
  %91 = load i32, i32* %90, align 16
  %92 = insertelement <4 x i32> undef, i32 %91, i32 0
  %93 = shufflevector <4 x i32> %92, <4 x i32> undef, <4 x i32> zeroinitializer
  %94 = sub nsw i32 0, %67
  %95 = insertelement <4 x i32> undef, i32 %94, i32 0
  %96 = shufflevector <4 x i32> %95, <4 x i32> undef, <4 x i32> zeroinitializer
  %97 = sub nsw i32 0, %63
  %98 = insertelement <4 x i32> undef, i32 %97, i32 0
  %99 = shufflevector <4 x i32> %98, <4 x i32> undef, <4 x i32> zeroinitializer
  %100 = sub nsw i32 0, %75
  %101 = insertelement <4 x i32> undef, i32 %100, i32 0
  %102 = shufflevector <4 x i32> %101, <4 x i32> undef, <4 x i32> zeroinitializer
  %103 = sub nsw i32 0, %71
  %104 = insertelement <4 x i32> undef, i32 %103, i32 0
  %105 = shufflevector <4 x i32> %104, <4 x i32> undef, <4 x i32> zeroinitializer
  %106 = sub nsw i32 0, %83
  %107 = insertelement <4 x i32> undef, i32 %106, i32 0
  %108 = shufflevector <4 x i32> %107, <4 x i32> undef, <4 x i32> zeroinitializer
  %109 = sub nsw i32 0, %79
  %110 = insertelement <4 x i32> undef, i32 %109, i32 0
  %111 = shufflevector <4 x i32> %110, <4 x i32> undef, <4 x i32> zeroinitializer
  %112 = sub nsw i32 0, %91
  %113 = insertelement <4 x i32> undef, i32 %112, i32 0
  %114 = shufflevector <4 x i32> %113, <4 x i32> undef, <4 x i32> zeroinitializer
  %115 = sub nsw i32 0, %87
  %116 = insertelement <4 x i32> undef, i32 %115, i32 0
  %117 = shufflevector <4 x i32> %116, <4 x i32> undef, <4 x i32> zeroinitializer
  %118 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 62
  %119 = load i32, i32* %118, align 8
  %120 = insertelement <4 x i32> undef, i32 %119, i32 0
  %121 = shufflevector <4 x i32> %120, <4 x i32> undef, <4 x i32> zeroinitializer
  %122 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 2
  %123 = load i32, i32* %122, align 8
  %124 = insertelement <4 x i32> undef, i32 %123, i32 0
  %125 = shufflevector <4 x i32> %124, <4 x i32> undef, <4 x i32> zeroinitializer
  %126 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 30
  %127 = load i32, i32* %126, align 8
  %128 = insertelement <4 x i32> undef, i32 %127, i32 0
  %129 = shufflevector <4 x i32> %128, <4 x i32> undef, <4 x i32> zeroinitializer
  %130 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 34
  %131 = load i32, i32* %130, align 8
  %132 = insertelement <4 x i32> undef, i32 %131, i32 0
  %133 = shufflevector <4 x i32> %132, <4 x i32> undef, <4 x i32> zeroinitializer
  %134 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 46
  %135 = load i32, i32* %134, align 8
  %136 = insertelement <4 x i32> undef, i32 %135, i32 0
  %137 = shufflevector <4 x i32> %136, <4 x i32> undef, <4 x i32> zeroinitializer
  %138 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 18
  %139 = load i32, i32* %138, align 8
  %140 = insertelement <4 x i32> undef, i32 %139, i32 0
  %141 = shufflevector <4 x i32> %140, <4 x i32> undef, <4 x i32> zeroinitializer
  %142 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 14
  %143 = load i32, i32* %142, align 8
  %144 = insertelement <4 x i32> undef, i32 %143, i32 0
  %145 = shufflevector <4 x i32> %144, <4 x i32> undef, <4 x i32> zeroinitializer
  %146 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 50
  %147 = load i32, i32* %146, align 8
  %148 = insertelement <4 x i32> undef, i32 %147, i32 0
  %149 = shufflevector <4 x i32> %148, <4 x i32> undef, <4 x i32> zeroinitializer
  %150 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 54
  %151 = load i32, i32* %150, align 8
  %152 = insertelement <4 x i32> undef, i32 %151, i32 0
  %153 = shufflevector <4 x i32> %152, <4 x i32> undef, <4 x i32> zeroinitializer
  %154 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 10
  %155 = load i32, i32* %154, align 8
  %156 = insertelement <4 x i32> undef, i32 %155, i32 0
  %157 = shufflevector <4 x i32> %156, <4 x i32> undef, <4 x i32> zeroinitializer
  %158 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 22
  %159 = load i32, i32* %158, align 8
  %160 = insertelement <4 x i32> undef, i32 %159, i32 0
  %161 = shufflevector <4 x i32> %160, <4 x i32> undef, <4 x i32> zeroinitializer
  %162 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 42
  %163 = load i32, i32* %162, align 8
  %164 = insertelement <4 x i32> undef, i32 %163, i32 0
  %165 = shufflevector <4 x i32> %164, <4 x i32> undef, <4 x i32> zeroinitializer
  %166 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 38
  %167 = load i32, i32* %166, align 8
  %168 = insertelement <4 x i32> undef, i32 %167, i32 0
  %169 = shufflevector <4 x i32> %168, <4 x i32> undef, <4 x i32> zeroinitializer
  %170 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 26
  %171 = load i32, i32* %170, align 8
  %172 = insertelement <4 x i32> undef, i32 %171, i32 0
  %173 = shufflevector <4 x i32> %172, <4 x i32> undef, <4 x i32> zeroinitializer
  %174 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 6
  %175 = load i32, i32* %174, align 8
  %176 = insertelement <4 x i32> undef, i32 %175, i32 0
  %177 = shufflevector <4 x i32> %176, <4 x i32> undef, <4 x i32> zeroinitializer
  %178 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 58
  %179 = load i32, i32* %178, align 8
  %180 = insertelement <4 x i32> undef, i32 %179, i32 0
  %181 = shufflevector <4 x i32> %180, <4 x i32> undef, <4 x i32> zeroinitializer
  %182 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 63
  %183 = load i32, i32* %182, align 4
  %184 = insertelement <4 x i32> undef, i32 %183, i32 0
  %185 = shufflevector <4 x i32> %184, <4 x i32> undef, <4 x i32> zeroinitializer
  %186 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 1
  %187 = load i32, i32* %186, align 4
  %188 = insertelement <4 x i32> undef, i32 %187, i32 0
  %189 = shufflevector <4 x i32> %188, <4 x i32> undef, <4 x i32> zeroinitializer
  %190 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 31
  %191 = load i32, i32* %190, align 4
  %192 = insertelement <4 x i32> undef, i32 %191, i32 0
  %193 = shufflevector <4 x i32> %192, <4 x i32> undef, <4 x i32> zeroinitializer
  %194 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 33
  %195 = load i32, i32* %194, align 4
  %196 = insertelement <4 x i32> undef, i32 %195, i32 0
  %197 = shufflevector <4 x i32> %196, <4 x i32> undef, <4 x i32> zeroinitializer
  %198 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 47
  %199 = load i32, i32* %198, align 4
  %200 = insertelement <4 x i32> undef, i32 %199, i32 0
  %201 = shufflevector <4 x i32> %200, <4 x i32> undef, <4 x i32> zeroinitializer
  %202 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 17
  %203 = load i32, i32* %202, align 4
  %204 = insertelement <4 x i32> undef, i32 %203, i32 0
  %205 = shufflevector <4 x i32> %204, <4 x i32> undef, <4 x i32> zeroinitializer
  %206 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 15
  %207 = load i32, i32* %206, align 4
  %208 = insertelement <4 x i32> undef, i32 %207, i32 0
  %209 = shufflevector <4 x i32> %208, <4 x i32> undef, <4 x i32> zeroinitializer
  %210 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 49
  %211 = load i32, i32* %210, align 4
  %212 = insertelement <4 x i32> undef, i32 %211, i32 0
  %213 = shufflevector <4 x i32> %212, <4 x i32> undef, <4 x i32> zeroinitializer
  %214 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 55
  %215 = load i32, i32* %214, align 4
  %216 = insertelement <4 x i32> undef, i32 %215, i32 0
  %217 = shufflevector <4 x i32> %216, <4 x i32> undef, <4 x i32> zeroinitializer
  %218 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 9
  %219 = load i32, i32* %218, align 4
  %220 = insertelement <4 x i32> undef, i32 %219, i32 0
  %221 = shufflevector <4 x i32> %220, <4 x i32> undef, <4 x i32> zeroinitializer
  %222 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 23
  %223 = load i32, i32* %222, align 4
  %224 = insertelement <4 x i32> undef, i32 %223, i32 0
  %225 = shufflevector <4 x i32> %224, <4 x i32> undef, <4 x i32> zeroinitializer
  %226 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 41
  %227 = load i32, i32* %226, align 4
  %228 = insertelement <4 x i32> undef, i32 %227, i32 0
  %229 = shufflevector <4 x i32> %228, <4 x i32> undef, <4 x i32> zeroinitializer
  %230 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 39
  %231 = load i32, i32* %230, align 4
  %232 = insertelement <4 x i32> undef, i32 %231, i32 0
  %233 = shufflevector <4 x i32> %232, <4 x i32> undef, <4 x i32> zeroinitializer
  %234 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 25
  %235 = load i32, i32* %234, align 4
  %236 = insertelement <4 x i32> undef, i32 %235, i32 0
  %237 = shufflevector <4 x i32> %236, <4 x i32> undef, <4 x i32> zeroinitializer
  %238 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 7
  %239 = load i32, i32* %238, align 4
  %240 = insertelement <4 x i32> undef, i32 %239, i32 0
  %241 = shufflevector <4 x i32> %240, <4 x i32> undef, <4 x i32> zeroinitializer
  %242 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 57
  %243 = load i32, i32* %242, align 4
  %244 = insertelement <4 x i32> undef, i32 %243, i32 0
  %245 = shufflevector <4 x i32> %244, <4 x i32> undef, <4 x i32> zeroinitializer
  %246 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 59
  %247 = load i32, i32* %246, align 4
  %248 = insertelement <4 x i32> undef, i32 %247, i32 0
  %249 = shufflevector <4 x i32> %248, <4 x i32> undef, <4 x i32> zeroinitializer
  %250 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 5
  %251 = load i32, i32* %250, align 4
  %252 = insertelement <4 x i32> undef, i32 %251, i32 0
  %253 = shufflevector <4 x i32> %252, <4 x i32> undef, <4 x i32> zeroinitializer
  %254 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 27
  %255 = load i32, i32* %254, align 4
  %256 = insertelement <4 x i32> undef, i32 %255, i32 0
  %257 = shufflevector <4 x i32> %256, <4 x i32> undef, <4 x i32> zeroinitializer
  %258 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 37
  %259 = load i32, i32* %258, align 4
  %260 = insertelement <4 x i32> undef, i32 %259, i32 0
  %261 = shufflevector <4 x i32> %260, <4 x i32> undef, <4 x i32> zeroinitializer
  %262 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 43
  %263 = load i32, i32* %262, align 4
  %264 = insertelement <4 x i32> undef, i32 %263, i32 0
  %265 = shufflevector <4 x i32> %264, <4 x i32> undef, <4 x i32> zeroinitializer
  %266 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 21
  %267 = load i32, i32* %266, align 4
  %268 = insertelement <4 x i32> undef, i32 %267, i32 0
  %269 = shufflevector <4 x i32> %268, <4 x i32> undef, <4 x i32> zeroinitializer
  %270 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 11
  %271 = load i32, i32* %270, align 4
  %272 = insertelement <4 x i32> undef, i32 %271, i32 0
  %273 = shufflevector <4 x i32> %272, <4 x i32> undef, <4 x i32> zeroinitializer
  %274 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 53
  %275 = load i32, i32* %274, align 4
  %276 = insertelement <4 x i32> undef, i32 %275, i32 0
  %277 = shufflevector <4 x i32> %276, <4 x i32> undef, <4 x i32> zeroinitializer
  %278 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 51
  %279 = load i32, i32* %278, align 4
  %280 = insertelement <4 x i32> undef, i32 %279, i32 0
  %281 = shufflevector <4 x i32> %280, <4 x i32> undef, <4 x i32> zeroinitializer
  %282 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 13
  %283 = load i32, i32* %282, align 4
  %284 = insertelement <4 x i32> undef, i32 %283, i32 0
  %285 = shufflevector <4 x i32> %284, <4 x i32> undef, <4 x i32> zeroinitializer
  %286 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 19
  %287 = load i32, i32* %286, align 4
  %288 = insertelement <4 x i32> undef, i32 %287, i32 0
  %289 = shufflevector <4 x i32> %288, <4 x i32> undef, <4 x i32> zeroinitializer
  %290 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 45
  %291 = load i32, i32* %290, align 4
  %292 = insertelement <4 x i32> undef, i32 %291, i32 0
  %293 = shufflevector <4 x i32> %292, <4 x i32> undef, <4 x i32> zeroinitializer
  %294 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 35
  %295 = load i32, i32* %294, align 4
  %296 = insertelement <4 x i32> undef, i32 %295, i32 0
  %297 = shufflevector <4 x i32> %296, <4 x i32> undef, <4 x i32> zeroinitializer
  %298 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 29
  %299 = load i32, i32* %298, align 4
  %300 = insertelement <4 x i32> undef, i32 %299, i32 0
  %301 = shufflevector <4 x i32> %300, <4 x i32> undef, <4 x i32> zeroinitializer
  %302 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 3
  %303 = load i32, i32* %302, align 4
  %304 = insertelement <4 x i32> undef, i32 %303, i32 0
  %305 = shufflevector <4 x i32> %304, <4 x i32> undef, <4 x i32> zeroinitializer
  %306 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 61
  %307 = load i32, i32* %306, align 4
  %308 = insertelement <4 x i32> undef, i32 %307, i32 0
  %309 = shufflevector <4 x i32> %308, <4 x i32> undef, <4 x i32> zeroinitializer
  %310 = mul nsw i32 %3, 63
  %311 = bitcast <2 x i64>* %0 to <4 x i32>*
  %312 = load <4 x i32>, <4 x i32>* %311, align 16
  %313 = sext i32 %310 to i64
  %314 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %313
  %315 = bitcast <2 x i64>* %314 to <4 x i32>*
  %316 = load <4 x i32>, <4 x i32>* %315, align 16
  %317 = add <4 x i32> %316, %312
  %318 = sub <4 x i32> %312, %316
  %319 = mul i32 %3, 62
  %320 = sext i32 %3 to i64
  %321 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %320
  %322 = bitcast <2 x i64>* %321 to <4 x i32>*
  %323 = load <4 x i32>, <4 x i32>* %322, align 16
  %324 = sext i32 %319 to i64
  %325 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %324
  %326 = bitcast <2 x i64>* %325 to <4 x i32>*
  %327 = load <4 x i32>, <4 x i32>* %326, align 16
  %328 = add <4 x i32> %327, %323
  %329 = sub <4 x i32> %323, %327
  %330 = shl nsw i32 %3, 1
  %331 = mul i32 %3, 61
  %332 = sext i32 %330 to i64
  %333 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %332
  %334 = bitcast <2 x i64>* %333 to <4 x i32>*
  %335 = load <4 x i32>, <4 x i32>* %334, align 16
  %336 = sext i32 %331 to i64
  %337 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %336
  %338 = bitcast <2 x i64>* %337 to <4 x i32>*
  %339 = load <4 x i32>, <4 x i32>* %338, align 16
  %340 = add <4 x i32> %339, %335
  %341 = sub <4 x i32> %335, %339
  %342 = mul nsw i32 %3, 3
  %343 = mul i32 %3, 60
  %344 = sext i32 %342 to i64
  %345 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %344
  %346 = bitcast <2 x i64>* %345 to <4 x i32>*
  %347 = load <4 x i32>, <4 x i32>* %346, align 16
  %348 = sext i32 %343 to i64
  %349 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %348
  %350 = bitcast <2 x i64>* %349 to <4 x i32>*
  %351 = load <4 x i32>, <4 x i32>* %350, align 16
  %352 = add <4 x i32> %351, %347
  %353 = sub <4 x i32> %347, %351
  %354 = shl nsw i32 %3, 2
  %355 = mul i32 %3, 59
  %356 = sext i32 %354 to i64
  %357 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %356
  %358 = bitcast <2 x i64>* %357 to <4 x i32>*
  %359 = load <4 x i32>, <4 x i32>* %358, align 16
  %360 = sext i32 %355 to i64
  %361 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %360
  %362 = bitcast <2 x i64>* %361 to <4 x i32>*
  %363 = load <4 x i32>, <4 x i32>* %362, align 16
  %364 = add <4 x i32> %363, %359
  %365 = sub <4 x i32> %359, %363
  %366 = mul nsw i32 %3, 5
  %367 = mul i32 %3, 58
  %368 = sext i32 %366 to i64
  %369 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %368
  %370 = bitcast <2 x i64>* %369 to <4 x i32>*
  %371 = load <4 x i32>, <4 x i32>* %370, align 16
  %372 = sext i32 %367 to i64
  %373 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %372
  %374 = bitcast <2 x i64>* %373 to <4 x i32>*
  %375 = load <4 x i32>, <4 x i32>* %374, align 16
  %376 = add <4 x i32> %375, %371
  %377 = sub <4 x i32> %371, %375
  %378 = mul nsw i32 %3, 6
  %379 = mul i32 %3, 57
  %380 = sext i32 %378 to i64
  %381 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %380
  %382 = bitcast <2 x i64>* %381 to <4 x i32>*
  %383 = load <4 x i32>, <4 x i32>* %382, align 16
  %384 = sext i32 %379 to i64
  %385 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %384
  %386 = bitcast <2 x i64>* %385 to <4 x i32>*
  %387 = load <4 x i32>, <4 x i32>* %386, align 16
  %388 = add <4 x i32> %387, %383
  %389 = sub <4 x i32> %383, %387
  %390 = mul nsw i32 %3, 7
  %391 = mul i32 %3, 56
  %392 = sext i32 %390 to i64
  %393 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %392
  %394 = bitcast <2 x i64>* %393 to <4 x i32>*
  %395 = load <4 x i32>, <4 x i32>* %394, align 16
  %396 = sext i32 %391 to i64
  %397 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %396
  %398 = bitcast <2 x i64>* %397 to <4 x i32>*
  %399 = load <4 x i32>, <4 x i32>* %398, align 16
  %400 = add <4 x i32> %399, %395
  %401 = sub <4 x i32> %395, %399
  %402 = shl nsw i32 %3, 3
  %403 = mul i32 %3, 55
  %404 = sext i32 %402 to i64
  %405 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %404
  %406 = bitcast <2 x i64>* %405 to <4 x i32>*
  %407 = load <4 x i32>, <4 x i32>* %406, align 16
  %408 = sext i32 %403 to i64
  %409 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %408
  %410 = bitcast <2 x i64>* %409 to <4 x i32>*
  %411 = load <4 x i32>, <4 x i32>* %410, align 16
  %412 = add <4 x i32> %411, %407
  %413 = sub <4 x i32> %407, %411
  %414 = mul nsw i32 %3, 9
  %415 = mul i32 %3, 54
  %416 = sext i32 %414 to i64
  %417 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %416
  %418 = bitcast <2 x i64>* %417 to <4 x i32>*
  %419 = load <4 x i32>, <4 x i32>* %418, align 16
  %420 = sext i32 %415 to i64
  %421 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %420
  %422 = bitcast <2 x i64>* %421 to <4 x i32>*
  %423 = load <4 x i32>, <4 x i32>* %422, align 16
  %424 = add <4 x i32> %423, %419
  %425 = sub <4 x i32> %419, %423
  %426 = mul nsw i32 %3, 10
  %427 = mul i32 %3, 53
  %428 = sext i32 %426 to i64
  %429 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %428
  %430 = bitcast <2 x i64>* %429 to <4 x i32>*
  %431 = load <4 x i32>, <4 x i32>* %430, align 16
  %432 = sext i32 %427 to i64
  %433 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %432
  %434 = bitcast <2 x i64>* %433 to <4 x i32>*
  %435 = load <4 x i32>, <4 x i32>* %434, align 16
  %436 = add <4 x i32> %435, %431
  %437 = sub <4 x i32> %431, %435
  %438 = mul nsw i32 %3, 11
  %439 = mul i32 %3, 52
  %440 = sext i32 %438 to i64
  %441 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %440
  %442 = bitcast <2 x i64>* %441 to <4 x i32>*
  %443 = load <4 x i32>, <4 x i32>* %442, align 16
  %444 = sext i32 %439 to i64
  %445 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %444
  %446 = bitcast <2 x i64>* %445 to <4 x i32>*
  %447 = load <4 x i32>, <4 x i32>* %446, align 16
  %448 = add <4 x i32> %447, %443
  %449 = sub <4 x i32> %443, %447
  %450 = mul nsw i32 %3, 12
  %451 = mul i32 %3, 51
  %452 = sext i32 %450 to i64
  %453 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %452
  %454 = bitcast <2 x i64>* %453 to <4 x i32>*
  %455 = load <4 x i32>, <4 x i32>* %454, align 16
  %456 = sext i32 %451 to i64
  %457 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %456
  %458 = bitcast <2 x i64>* %457 to <4 x i32>*
  %459 = load <4 x i32>, <4 x i32>* %458, align 16
  %460 = add <4 x i32> %459, %455
  %461 = sub <4 x i32> %455, %459
  %462 = mul nsw i32 %3, 13
  %463 = mul i32 %3, 50
  %464 = sext i32 %462 to i64
  %465 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %464
  %466 = bitcast <2 x i64>* %465 to <4 x i32>*
  %467 = load <4 x i32>, <4 x i32>* %466, align 16
  %468 = sext i32 %463 to i64
  %469 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %468
  %470 = bitcast <2 x i64>* %469 to <4 x i32>*
  %471 = load <4 x i32>, <4 x i32>* %470, align 16
  %472 = add <4 x i32> %471, %467
  %473 = sub <4 x i32> %467, %471
  %474 = mul nsw i32 %3, 14
  %475 = mul i32 %3, 49
  %476 = sext i32 %474 to i64
  %477 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %476
  %478 = bitcast <2 x i64>* %477 to <4 x i32>*
  %479 = load <4 x i32>, <4 x i32>* %478, align 16
  %480 = sext i32 %475 to i64
  %481 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %480
  %482 = bitcast <2 x i64>* %481 to <4 x i32>*
  %483 = load <4 x i32>, <4 x i32>* %482, align 16
  %484 = add <4 x i32> %483, %479
  %485 = sub <4 x i32> %479, %483
  %486 = mul nsw i32 %3, 15
  %487 = mul i32 %3, 48
  %488 = sext i32 %486 to i64
  %489 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %488
  %490 = bitcast <2 x i64>* %489 to <4 x i32>*
  %491 = load <4 x i32>, <4 x i32>* %490, align 16
  %492 = sext i32 %487 to i64
  %493 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %492
  %494 = bitcast <2 x i64>* %493 to <4 x i32>*
  %495 = load <4 x i32>, <4 x i32>* %494, align 16
  %496 = add <4 x i32> %495, %491
  %497 = sub <4 x i32> %491, %495
  %498 = shl nsw i32 %3, 4
  %499 = mul i32 %3, 47
  %500 = sext i32 %498 to i64
  %501 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %500
  %502 = bitcast <2 x i64>* %501 to <4 x i32>*
  %503 = load <4 x i32>, <4 x i32>* %502, align 16
  %504 = sext i32 %499 to i64
  %505 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %504
  %506 = bitcast <2 x i64>* %505 to <4 x i32>*
  %507 = load <4 x i32>, <4 x i32>* %506, align 16
  %508 = add <4 x i32> %507, %503
  %509 = sub <4 x i32> %503, %507
  %510 = mul nsw i32 %3, 17
  %511 = mul i32 %3, 46
  %512 = sext i32 %510 to i64
  %513 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %512
  %514 = bitcast <2 x i64>* %513 to <4 x i32>*
  %515 = load <4 x i32>, <4 x i32>* %514, align 16
  %516 = sext i32 %511 to i64
  %517 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %516
  %518 = bitcast <2 x i64>* %517 to <4 x i32>*
  %519 = load <4 x i32>, <4 x i32>* %518, align 16
  %520 = add <4 x i32> %519, %515
  %521 = sub <4 x i32> %515, %519
  %522 = mul nsw i32 %3, 18
  %523 = mul i32 %3, 45
  %524 = sext i32 %522 to i64
  %525 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %524
  %526 = bitcast <2 x i64>* %525 to <4 x i32>*
  %527 = load <4 x i32>, <4 x i32>* %526, align 16
  %528 = sext i32 %523 to i64
  %529 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %528
  %530 = bitcast <2 x i64>* %529 to <4 x i32>*
  %531 = load <4 x i32>, <4 x i32>* %530, align 16
  %532 = add <4 x i32> %531, %527
  %533 = sub <4 x i32> %527, %531
  %534 = mul nsw i32 %3, 19
  %535 = mul i32 %3, 44
  %536 = sext i32 %534 to i64
  %537 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %536
  %538 = bitcast <2 x i64>* %537 to <4 x i32>*
  %539 = load <4 x i32>, <4 x i32>* %538, align 16
  %540 = sext i32 %535 to i64
  %541 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %540
  %542 = bitcast <2 x i64>* %541 to <4 x i32>*
  %543 = load <4 x i32>, <4 x i32>* %542, align 16
  %544 = add <4 x i32> %543, %539
  %545 = sub <4 x i32> %539, %543
  %546 = mul nsw i32 %3, 20
  %547 = mul i32 %3, 43
  %548 = sext i32 %546 to i64
  %549 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %548
  %550 = bitcast <2 x i64>* %549 to <4 x i32>*
  %551 = load <4 x i32>, <4 x i32>* %550, align 16
  %552 = sext i32 %547 to i64
  %553 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %552
  %554 = bitcast <2 x i64>* %553 to <4 x i32>*
  %555 = load <4 x i32>, <4 x i32>* %554, align 16
  %556 = add <4 x i32> %555, %551
  %557 = sub <4 x i32> %551, %555
  %558 = mul nsw i32 %3, 21
  %559 = mul i32 %3, 42
  %560 = sext i32 %558 to i64
  %561 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %560
  %562 = bitcast <2 x i64>* %561 to <4 x i32>*
  %563 = load <4 x i32>, <4 x i32>* %562, align 16
  %564 = sext i32 %559 to i64
  %565 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %564
  %566 = bitcast <2 x i64>* %565 to <4 x i32>*
  %567 = load <4 x i32>, <4 x i32>* %566, align 16
  %568 = add <4 x i32> %567, %563
  %569 = sub <4 x i32> %563, %567
  %570 = mul nsw i32 %3, 22
  %571 = mul i32 %3, 41
  %572 = sext i32 %570 to i64
  %573 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %572
  %574 = bitcast <2 x i64>* %573 to <4 x i32>*
  %575 = load <4 x i32>, <4 x i32>* %574, align 16
  %576 = sext i32 %571 to i64
  %577 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %576
  %578 = bitcast <2 x i64>* %577 to <4 x i32>*
  %579 = load <4 x i32>, <4 x i32>* %578, align 16
  %580 = add <4 x i32> %579, %575
  %581 = sub <4 x i32> %575, %579
  %582 = mul nsw i32 %3, 23
  %583 = mul i32 %3, 40
  %584 = sext i32 %582 to i64
  %585 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %584
  %586 = bitcast <2 x i64>* %585 to <4 x i32>*
  %587 = load <4 x i32>, <4 x i32>* %586, align 16
  %588 = sext i32 %583 to i64
  %589 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %588
  %590 = bitcast <2 x i64>* %589 to <4 x i32>*
  %591 = load <4 x i32>, <4 x i32>* %590, align 16
  %592 = add <4 x i32> %591, %587
  %593 = sub <4 x i32> %587, %591
  %594 = mul nsw i32 %3, 24
  %595 = mul i32 %3, 39
  %596 = sext i32 %594 to i64
  %597 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %596
  %598 = bitcast <2 x i64>* %597 to <4 x i32>*
  %599 = load <4 x i32>, <4 x i32>* %598, align 16
  %600 = sext i32 %595 to i64
  %601 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %600
  %602 = bitcast <2 x i64>* %601 to <4 x i32>*
  %603 = load <4 x i32>, <4 x i32>* %602, align 16
  %604 = add <4 x i32> %603, %599
  %605 = sub <4 x i32> %599, %603
  %606 = mul nsw i32 %3, 25
  %607 = mul i32 %3, 38
  %608 = sext i32 %606 to i64
  %609 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %608
  %610 = bitcast <2 x i64>* %609 to <4 x i32>*
  %611 = load <4 x i32>, <4 x i32>* %610, align 16
  %612 = sext i32 %607 to i64
  %613 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %612
  %614 = bitcast <2 x i64>* %613 to <4 x i32>*
  %615 = load <4 x i32>, <4 x i32>* %614, align 16
  %616 = add <4 x i32> %615, %611
  %617 = sub <4 x i32> %611, %615
  %618 = mul nsw i32 %3, 26
  %619 = mul i32 %3, 37
  %620 = sext i32 %618 to i64
  %621 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %620
  %622 = bitcast <2 x i64>* %621 to <4 x i32>*
  %623 = load <4 x i32>, <4 x i32>* %622, align 16
  %624 = sext i32 %619 to i64
  %625 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %624
  %626 = bitcast <2 x i64>* %625 to <4 x i32>*
  %627 = load <4 x i32>, <4 x i32>* %626, align 16
  %628 = add <4 x i32> %627, %623
  %629 = sub <4 x i32> %623, %627
  %630 = mul nsw i32 %3, 27
  %631 = mul i32 %3, 36
  %632 = sext i32 %630 to i64
  %633 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %632
  %634 = bitcast <2 x i64>* %633 to <4 x i32>*
  %635 = load <4 x i32>, <4 x i32>* %634, align 16
  %636 = sext i32 %631 to i64
  %637 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %636
  %638 = bitcast <2 x i64>* %637 to <4 x i32>*
  %639 = load <4 x i32>, <4 x i32>* %638, align 16
  %640 = add <4 x i32> %639, %635
  %641 = sub <4 x i32> %635, %639
  %642 = mul nsw i32 %3, 28
  %643 = mul i32 %3, 35
  %644 = sext i32 %642 to i64
  %645 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %644
  %646 = bitcast <2 x i64>* %645 to <4 x i32>*
  %647 = load <4 x i32>, <4 x i32>* %646, align 16
  %648 = sext i32 %643 to i64
  %649 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %648
  %650 = bitcast <2 x i64>* %649 to <4 x i32>*
  %651 = load <4 x i32>, <4 x i32>* %650, align 16
  %652 = add <4 x i32> %651, %647
  %653 = sub <4 x i32> %647, %651
  %654 = mul nsw i32 %3, 29
  %655 = mul i32 %3, 34
  %656 = sext i32 %654 to i64
  %657 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %656
  %658 = bitcast <2 x i64>* %657 to <4 x i32>*
  %659 = load <4 x i32>, <4 x i32>* %658, align 16
  %660 = sext i32 %655 to i64
  %661 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %660
  %662 = bitcast <2 x i64>* %661 to <4 x i32>*
  %663 = load <4 x i32>, <4 x i32>* %662, align 16
  %664 = add <4 x i32> %663, %659
  %665 = sub <4 x i32> %659, %663
  %666 = mul nsw i32 %3, 30
  %667 = mul i32 %3, 33
  %668 = sext i32 %666 to i64
  %669 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %668
  %670 = bitcast <2 x i64>* %669 to <4 x i32>*
  %671 = load <4 x i32>, <4 x i32>* %670, align 16
  %672 = sext i32 %667 to i64
  %673 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %672
  %674 = bitcast <2 x i64>* %673 to <4 x i32>*
  %675 = load <4 x i32>, <4 x i32>* %674, align 16
  %676 = add <4 x i32> %675, %671
  %677 = sub <4 x i32> %671, %675
  %678 = mul nsw i32 %3, 31
  %679 = shl i32 %3, 5
  %680 = sext i32 %678 to i64
  %681 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %680
  %682 = bitcast <2 x i64>* %681 to <4 x i32>*
  %683 = load <4 x i32>, <4 x i32>* %682, align 16
  %684 = sext i32 %679 to i64
  %685 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %684
  %686 = bitcast <2 x i64>* %685 to <4 x i32>*
  %687 = load <4 x i32>, <4 x i32>* %686, align 16
  %688 = add <4 x i32> %687, %683
  %689 = sub <4 x i32> %683, %687
  %690 = add <4 x i32> %688, %317
  %691 = sub <4 x i32> %317, %688
  %692 = add <4 x i32> %676, %328
  %693 = sub <4 x i32> %328, %676
  %694 = add <4 x i32> %664, %340
  %695 = sub <4 x i32> %340, %664
  %696 = add <4 x i32> %652, %352
  %697 = sub <4 x i32> %352, %652
  %698 = add <4 x i32> %640, %364
  %699 = sub <4 x i32> %364, %640
  %700 = add <4 x i32> %628, %376
  %701 = sub <4 x i32> %376, %628
  %702 = add <4 x i32> %616, %388
  %703 = sub <4 x i32> %388, %616
  %704 = add <4 x i32> %604, %400
  %705 = sub <4 x i32> %400, %604
  %706 = add <4 x i32> %592, %412
  %707 = sub <4 x i32> %412, %592
  %708 = add <4 x i32> %580, %424
  %709 = sub <4 x i32> %424, %580
  %710 = add <4 x i32> %568, %436
  %711 = sub <4 x i32> %436, %568
  %712 = add <4 x i32> %556, %448
  %713 = sub <4 x i32> %448, %556
  %714 = add <4 x i32> %544, %460
  %715 = sub <4 x i32> %460, %544
  %716 = add <4 x i32> %532, %472
  %717 = sub <4 x i32> %472, %532
  %718 = add <4 x i32> %520, %484
  %719 = sub <4 x i32> %484, %520
  %720 = add <4 x i32> %508, %496
  %721 = sub <4 x i32> %496, %508
  %722 = mul <4 x i32> %593, %17
  %723 = mul <4 x i32> %413, %19
  %724 = add <4 x i32> %723, %12
  %725 = add <4 x i32> %724, %722
  %726 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %725, i32 %6) #5
  %727 = mul <4 x i32> %593, %19
  %728 = mul <4 x i32> %17, %413
  %729 = sub <4 x i32> %12, %728
  %730 = add <4 x i32> %729, %727
  %731 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %730, i32 %6) #5
  %732 = mul <4 x i32> %581, %17
  %733 = mul <4 x i32> %425, %19
  %734 = add <4 x i32> %733, %12
  %735 = add <4 x i32> %734, %732
  %736 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %735, i32 %6) #5
  %737 = mul <4 x i32> %581, %19
  %738 = mul <4 x i32> %17, %425
  %739 = sub <4 x i32> %12, %738
  %740 = add <4 x i32> %739, %737
  %741 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %740, i32 %6) #5
  %742 = mul <4 x i32> %569, %17
  %743 = mul <4 x i32> %437, %19
  %744 = add <4 x i32> %743, %12
  %745 = add <4 x i32> %744, %742
  %746 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %745, i32 %6) #5
  %747 = mul <4 x i32> %569, %19
  %748 = mul <4 x i32> %17, %437
  %749 = sub <4 x i32> %12, %748
  %750 = add <4 x i32> %749, %747
  %751 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %750, i32 %6) #5
  %752 = mul <4 x i32> %557, %17
  %753 = mul <4 x i32> %449, %19
  %754 = add <4 x i32> %753, %12
  %755 = add <4 x i32> %754, %752
  %756 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %755, i32 %6) #5
  %757 = mul <4 x i32> %557, %19
  %758 = mul <4 x i32> %17, %449
  %759 = sub <4 x i32> %12, %758
  %760 = add <4 x i32> %759, %757
  %761 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %760, i32 %6) #5
  %762 = mul <4 x i32> %545, %17
  %763 = mul <4 x i32> %461, %19
  %764 = add <4 x i32> %763, %12
  %765 = add <4 x i32> %764, %762
  %766 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %765, i32 %6) #5
  %767 = mul <4 x i32> %545, %19
  %768 = mul <4 x i32> %17, %461
  %769 = sub <4 x i32> %12, %768
  %770 = add <4 x i32> %769, %767
  %771 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %770, i32 %6) #5
  %772 = mul <4 x i32> %533, %17
  %773 = mul <4 x i32> %473, %19
  %774 = add <4 x i32> %773, %12
  %775 = add <4 x i32> %774, %772
  %776 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %775, i32 %6) #5
  %777 = mul <4 x i32> %533, %19
  %778 = mul <4 x i32> %17, %473
  %779 = sub <4 x i32> %12, %778
  %780 = add <4 x i32> %779, %777
  %781 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %780, i32 %6) #5
  %782 = mul <4 x i32> %521, %17
  %783 = mul <4 x i32> %485, %19
  %784 = add <4 x i32> %783, %12
  %785 = add <4 x i32> %784, %782
  %786 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %785, i32 %6) #5
  %787 = mul <4 x i32> %521, %19
  %788 = mul <4 x i32> %17, %485
  %789 = sub <4 x i32> %12, %788
  %790 = add <4 x i32> %789, %787
  %791 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %790, i32 %6) #5
  %792 = mul <4 x i32> %509, %17
  %793 = mul <4 x i32> %497, %19
  %794 = add <4 x i32> %793, %12
  %795 = add <4 x i32> %794, %792
  %796 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %795, i32 %6) #5
  %797 = mul <4 x i32> %509, %19
  %798 = mul <4 x i32> %17, %497
  %799 = sub <4 x i32> %12, %798
  %800 = add <4 x i32> %799, %797
  %801 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %800, i32 %6) #5
  %802 = add <4 x i32> %690, %720
  %803 = sub <4 x i32> %690, %720
  %804 = add <4 x i32> %692, %718
  %805 = sub <4 x i32> %692, %718
  %806 = add <4 x i32> %694, %716
  %807 = sub <4 x i32> %694, %716
  %808 = add <4 x i32> %696, %714
  %809 = sub <4 x i32> %696, %714
  %810 = add <4 x i32> %698, %712
  %811 = sub <4 x i32> %698, %712
  %812 = add <4 x i32> %700, %710
  %813 = sub <4 x i32> %700, %710
  %814 = add <4 x i32> %702, %708
  %815 = sub <4 x i32> %702, %708
  %816 = add <4 x i32> %704, %706
  %817 = sub <4 x i32> %704, %706
  %818 = mul <4 x i32> %713, %17
  %819 = mul <4 x i32> %699, %19
  %820 = add <4 x i32> %818, %12
  %821 = add <4 x i32> %820, %819
  %822 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %821, i32 %6) #5
  %823 = mul <4 x i32> %713, %19
  %824 = mul <4 x i32> %17, %699
  %825 = add <4 x i32> %823, %12
  %826 = sub <4 x i32> %825, %824
  %827 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %826, i32 %6) #5
  %828 = mul <4 x i32> %711, %17
  %829 = mul <4 x i32> %701, %19
  %830 = add <4 x i32> %828, %12
  %831 = add <4 x i32> %830, %829
  %832 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %831, i32 %6) #5
  %833 = mul <4 x i32> %711, %19
  %834 = mul <4 x i32> %17, %701
  %835 = add <4 x i32> %833, %12
  %836 = sub <4 x i32> %835, %834
  %837 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %836, i32 %6) #5
  %838 = mul <4 x i32> %709, %17
  %839 = mul <4 x i32> %703, %19
  %840 = add <4 x i32> %838, %12
  %841 = add <4 x i32> %840, %839
  %842 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %841, i32 %6) #5
  %843 = mul <4 x i32> %709, %19
  %844 = mul <4 x i32> %17, %703
  %845 = add <4 x i32> %843, %12
  %846 = sub <4 x i32> %845, %844
  %847 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %846, i32 %6) #5
  %848 = mul <4 x i32> %707, %17
  %849 = mul <4 x i32> %705, %19
  %850 = add <4 x i32> %848, %12
  %851 = add <4 x i32> %850, %849
  %852 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %851, i32 %6) #5
  %853 = mul <4 x i32> %707, %19
  %854 = mul <4 x i32> %17, %705
  %855 = add <4 x i32> %853, %12
  %856 = sub <4 x i32> %855, %854
  %857 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %856, i32 %6) #5
  %858 = add <4 x i32> %796, %689
  %859 = sub <4 x i32> %689, %796
  %860 = add <4 x i32> %786, %677
  %861 = sub <4 x i32> %677, %786
  %862 = add <4 x i32> %776, %665
  %863 = sub <4 x i32> %665, %776
  %864 = add <4 x i32> %766, %653
  %865 = sub <4 x i32> %653, %766
  %866 = add <4 x i32> %756, %641
  %867 = sub <4 x i32> %641, %756
  %868 = add <4 x i32> %746, %629
  %869 = sub <4 x i32> %629, %746
  %870 = add <4 x i32> %736, %617
  %871 = sub <4 x i32> %617, %736
  %872 = add <4 x i32> %726, %605
  %873 = sub <4 x i32> %605, %726
  %874 = sub <4 x i32> %318, %801
  %875 = add <4 x i32> %801, %318
  %876 = sub <4 x i32> %329, %791
  %877 = add <4 x i32> %791, %329
  %878 = sub <4 x i32> %341, %781
  %879 = add <4 x i32> %781, %341
  %880 = sub <4 x i32> %353, %771
  %881 = add <4 x i32> %771, %353
  %882 = sub <4 x i32> %365, %761
  %883 = add <4 x i32> %761, %365
  %884 = sub <4 x i32> %377, %751
  %885 = add <4 x i32> %751, %377
  %886 = sub <4 x i32> %389, %741
  %887 = add <4 x i32> %741, %389
  %888 = sub <4 x i32> %401, %731
  %889 = add <4 x i32> %731, %401
  %890 = add <4 x i32> %802, %816
  %891 = sub <4 x i32> %802, %816
  %892 = add <4 x i32> %804, %814
  %893 = sub <4 x i32> %804, %814
  %894 = add <4 x i32> %806, %812
  %895 = sub <4 x i32> %806, %812
  %896 = add <4 x i32> %808, %810
  %897 = sub <4 x i32> %808, %810
  %898 = mul <4 x i32> %813, %17
  %899 = mul <4 x i32> %807, %19
  %900 = add <4 x i32> %898, %12
  %901 = add <4 x i32> %900, %899
  %902 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %901, i32 %6) #5
  %903 = mul <4 x i32> %813, %19
  %904 = mul <4 x i32> %17, %807
  %905 = add <4 x i32> %903, %12
  %906 = sub <4 x i32> %905, %904
  %907 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %906, i32 %6) #5
  %908 = mul <4 x i32> %811, %17
  %909 = mul <4 x i32> %809, %19
  %910 = add <4 x i32> %908, %12
  %911 = add <4 x i32> %910, %909
  %912 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %911, i32 %6) #5
  %913 = mul <4 x i32> %811, %19
  %914 = mul <4 x i32> %17, %809
  %915 = add <4 x i32> %913, %12
  %916 = sub <4 x i32> %915, %914
  %917 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %916, i32 %6) #5
  %918 = add <4 x i32> %852, %721
  %919 = sub <4 x i32> %721, %852
  %920 = add <4 x i32> %842, %719
  %921 = sub <4 x i32> %719, %842
  %922 = add <4 x i32> %832, %717
  %923 = sub <4 x i32> %717, %832
  %924 = add <4 x i32> %822, %715
  %925 = sub <4 x i32> %715, %822
  %926 = sub <4 x i32> %691, %857
  %927 = add <4 x i32> %857, %691
  %928 = sub <4 x i32> %693, %847
  %929 = add <4 x i32> %847, %693
  %930 = sub <4 x i32> %695, %837
  %931 = add <4 x i32> %837, %695
  %932 = sub <4 x i32> %697, %827
  %933 = add <4 x i32> %827, %697
  %934 = mul <4 x i32> %866, %24
  %935 = mul <4 x i32> %883, %28
  %936 = add <4 x i32> %934, %12
  %937 = add <4 x i32> %936, %935
  %938 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %937, i32 %6) #5
  %939 = mul <4 x i32> %866, %28
  %940 = mul <4 x i32> %24, %883
  %941 = add <4 x i32> %939, %12
  %942 = sub <4 x i32> %941, %940
  %943 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %942, i32 %6) #5
  %944 = mul <4 x i32> %868, %24
  %945 = mul <4 x i32> %885, %28
  %946 = add <4 x i32> %944, %12
  %947 = add <4 x i32> %946, %945
  %948 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %947, i32 %6) #5
  %949 = mul <4 x i32> %868, %28
  %950 = mul <4 x i32> %24, %885
  %951 = add <4 x i32> %949, %12
  %952 = sub <4 x i32> %951, %950
  %953 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %952, i32 %6) #5
  %954 = mul <4 x i32> %870, %24
  %955 = mul <4 x i32> %887, %28
  %956 = add <4 x i32> %954, %12
  %957 = add <4 x i32> %956, %955
  %958 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %957, i32 %6) #5
  %959 = mul <4 x i32> %870, %28
  %960 = mul <4 x i32> %24, %887
  %961 = add <4 x i32> %959, %12
  %962 = sub <4 x i32> %961, %960
  %963 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %962, i32 %6) #5
  %964 = mul <4 x i32> %872, %24
  %965 = mul <4 x i32> %889, %28
  %966 = add <4 x i32> %964, %12
  %967 = add <4 x i32> %966, %965
  %968 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %967, i32 %6) #5
  %969 = mul <4 x i32> %872, %28
  %970 = mul <4 x i32> %24, %889
  %971 = add <4 x i32> %969, %12
  %972 = sub <4 x i32> %971, %970
  %973 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %972, i32 %6) #5
  %974 = mul <4 x i32> %873, %31
  %975 = mul <4 x i32> %888, %24
  %976 = add <4 x i32> %974, %12
  %977 = add <4 x i32> %976, %975
  %978 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %977, i32 %6) #5
  %979 = mul <4 x i32> %873, %24
  %980 = mul <4 x i32> %31, %888
  %981 = add <4 x i32> %979, %12
  %982 = sub <4 x i32> %981, %980
  %983 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %982, i32 %6) #5
  %984 = mul <4 x i32> %871, %31
  %985 = mul <4 x i32> %886, %24
  %986 = add <4 x i32> %984, %12
  %987 = add <4 x i32> %986, %985
  %988 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %987, i32 %6) #5
  %989 = mul <4 x i32> %871, %24
  %990 = mul <4 x i32> %31, %886
  %991 = add <4 x i32> %989, %12
  %992 = sub <4 x i32> %991, %990
  %993 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %992, i32 %6) #5
  %994 = mul <4 x i32> %869, %31
  %995 = mul <4 x i32> %884, %24
  %996 = add <4 x i32> %994, %12
  %997 = add <4 x i32> %996, %995
  %998 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %997, i32 %6) #5
  %999 = mul <4 x i32> %869, %24
  %1000 = mul <4 x i32> %31, %884
  %1001 = add <4 x i32> %999, %12
  %1002 = sub <4 x i32> %1001, %1000
  %1003 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1002, i32 %6) #5
  %1004 = mul <4 x i32> %867, %31
  %1005 = mul <4 x i32> %882, %24
  %1006 = add <4 x i32> %1004, %12
  %1007 = add <4 x i32> %1006, %1005
  %1008 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1007, i32 %6) #5
  %1009 = mul <4 x i32> %867, %24
  %1010 = mul <4 x i32> %31, %882
  %1011 = add <4 x i32> %1009, %12
  %1012 = sub <4 x i32> %1011, %1010
  %1013 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1012, i32 %6) #5
  %1014 = add <4 x i32> %890, %896
  %1015 = sub <4 x i32> %890, %896
  %1016 = add <4 x i32> %892, %894
  %1017 = sub <4 x i32> %892, %894
  %1018 = mul <4 x i32> %895, %17
  %1019 = mul <4 x i32> %893, %19
  %1020 = add <4 x i32> %1018, %12
  %1021 = add <4 x i32> %1020, %1019
  %1022 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1021, i32 %6) #5
  %1023 = mul <4 x i32> %895, %19
  %1024 = mul <4 x i32> %17, %893
  %1025 = add <4 x i32> %1023, %12
  %1026 = sub <4 x i32> %1025, %1024
  %1027 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1026, i32 %6) #5
  %1028 = add <4 x i32> %912, %817
  %1029 = sub <4 x i32> %817, %912
  %1030 = add <4 x i32> %902, %815
  %1031 = sub <4 x i32> %815, %902
  %1032 = sub <4 x i32> %803, %917
  %1033 = add <4 x i32> %917, %803
  %1034 = sub <4 x i32> %805, %907
  %1035 = add <4 x i32> %907, %805
  %1036 = mul <4 x i32> %922, %24
  %1037 = mul <4 x i32> %931, %28
  %1038 = add <4 x i32> %1036, %12
  %1039 = add <4 x i32> %1038, %1037
  %1040 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1039, i32 %6) #5
  %1041 = mul <4 x i32> %922, %28
  %1042 = mul <4 x i32> %24, %931
  %1043 = add <4 x i32> %1041, %12
  %1044 = sub <4 x i32> %1043, %1042
  %1045 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1044, i32 %6) #5
  %1046 = mul <4 x i32> %924, %24
  %1047 = mul <4 x i32> %933, %28
  %1048 = add <4 x i32> %1046, %12
  %1049 = add <4 x i32> %1048, %1047
  %1050 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1049, i32 %6) #5
  %1051 = mul <4 x i32> %924, %28
  %1052 = mul <4 x i32> %24, %933
  %1053 = add <4 x i32> %1051, %12
  %1054 = sub <4 x i32> %1053, %1052
  %1055 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1054, i32 %6) #5
  %1056 = mul <4 x i32> %925, %31
  %1057 = mul <4 x i32> %932, %24
  %1058 = add <4 x i32> %1056, %12
  %1059 = add <4 x i32> %1058, %1057
  %1060 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1059, i32 %6) #5
  %1061 = mul <4 x i32> %925, %24
  %1062 = mul <4 x i32> %31, %932
  %1063 = add <4 x i32> %1061, %12
  %1064 = sub <4 x i32> %1063, %1062
  %1065 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1064, i32 %6) #5
  %1066 = mul <4 x i32> %923, %31
  %1067 = mul <4 x i32> %930, %24
  %1068 = add <4 x i32> %1066, %12
  %1069 = add <4 x i32> %1068, %1067
  %1070 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1069, i32 %6) #5
  %1071 = mul <4 x i32> %923, %24
  %1072 = mul <4 x i32> %31, %930
  %1073 = add <4 x i32> %1071, %12
  %1074 = sub <4 x i32> %1073, %1072
  %1075 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1074, i32 %6) #5
  %1076 = add <4 x i32> %968, %858
  %1077 = sub <4 x i32> %858, %968
  %1078 = add <4 x i32> %958, %860
  %1079 = sub <4 x i32> %860, %958
  %1080 = add <4 x i32> %948, %862
  %1081 = sub <4 x i32> %862, %948
  %1082 = add <4 x i32> %938, %864
  %1083 = sub <4 x i32> %864, %938
  %1084 = sub <4 x i32> %859, %978
  %1085 = add <4 x i32> %978, %859
  %1086 = sub <4 x i32> %861, %988
  %1087 = add <4 x i32> %988, %861
  %1088 = sub <4 x i32> %863, %998
  %1089 = add <4 x i32> %998, %863
  %1090 = sub <4 x i32> %865, %1008
  %1091 = add <4 x i32> %1008, %865
  %1092 = add <4 x i32> %983, %874
  %1093 = sub <4 x i32> %874, %983
  %1094 = add <4 x i32> %993, %876
  %1095 = sub <4 x i32> %876, %993
  %1096 = add <4 x i32> %1003, %878
  %1097 = sub <4 x i32> %878, %1003
  %1098 = add <4 x i32> %1013, %880
  %1099 = sub <4 x i32> %880, %1013
  %1100 = sub <4 x i32> %875, %973
  %1101 = add <4 x i32> %973, %875
  %1102 = sub <4 x i32> %877, %963
  %1103 = add <4 x i32> %963, %877
  %1104 = sub <4 x i32> %879, %953
  %1105 = add <4 x i32> %953, %879
  %1106 = sub <4 x i32> %881, %943
  %1107 = add <4 x i32> %943, %881
  %1108 = mul <4 x i32> %1014, %19
  %1109 = mul <4 x i32> %1016, %19
  %1110 = add <4 x i32> %1109, %12
  %1111 = add <4 x i32> %1110, %1108
  %1112 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1111, i32 %6) #5
  %1113 = sub <4 x i32> %12, %1109
  %1114 = add <4 x i32> %1113, %1108
  %1115 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1114, i32 %6) #5
  %1116 = mul <4 x i32> %1015, %33
  %1117 = mul <4 x i32> %1017, %28
  %1118 = add <4 x i32> %1117, %12
  %1119 = add <4 x i32> %1118, %1116
  %1120 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1119, i32 %6) #5
  %1121 = mul <4 x i32> %1015, %28
  %1122 = mul <4 x i32> %33, %1017
  %1123 = sub <4 x i32> %12, %1122
  %1124 = add <4 x i32> %1123, %1121
  %1125 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1124, i32 %6) #5
  %1126 = add <4 x i32> %1022, %897
  %1127 = sub <4 x i32> %897, %1022
  %1128 = sub <4 x i32> %891, %1027
  %1129 = add <4 x i32> %1027, %891
  %1130 = mul <4 x i32> %1030, %24
  %1131 = mul <4 x i32> %1035, %28
  %1132 = add <4 x i32> %1130, %12
  %1133 = add <4 x i32> %1132, %1131
  %1134 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1133, i32 %6) #5
  %1135 = mul <4 x i32> %1030, %28
  %1136 = mul <4 x i32> %24, %1035
  %1137 = add <4 x i32> %1135, %12
  %1138 = sub <4 x i32> %1137, %1136
  %1139 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1138, i32 %6) #5
  %1140 = mul <4 x i32> %1031, %31
  %1141 = mul <4 x i32> %1034, %24
  %1142 = add <4 x i32> %1140, %12
  %1143 = add <4 x i32> %1142, %1141
  %1144 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1143, i32 %6) #5
  %1145 = mul <4 x i32> %1031, %24
  %1146 = mul <4 x i32> %31, %1034
  %1147 = add <4 x i32> %1145, %12
  %1148 = sub <4 x i32> %1147, %1146
  %1149 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1148, i32 %6) #5
  %1150 = add <4 x i32> %1050, %918
  %1151 = sub <4 x i32> %918, %1050
  %1152 = add <4 x i32> %1040, %920
  %1153 = sub <4 x i32> %920, %1040
  %1154 = sub <4 x i32> %919, %1060
  %1155 = add <4 x i32> %1060, %919
  %1156 = sub <4 x i32> %921, %1070
  %1157 = add <4 x i32> %1070, %921
  %1158 = add <4 x i32> %1065, %926
  %1159 = sub <4 x i32> %926, %1065
  %1160 = add <4 x i32> %1075, %928
  %1161 = sub <4 x i32> %928, %1075
  %1162 = sub <4 x i32> %927, %1055
  %1163 = add <4 x i32> %1055, %927
  %1164 = sub <4 x i32> %929, %1045
  %1165 = add <4 x i32> %1045, %929
  %1166 = mul <4 x i32> %1080, %38
  %1167 = mul <4 x i32> %1105, %42
  %1168 = add <4 x i32> %1166, %12
  %1169 = add <4 x i32> %1168, %1167
  %1170 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1169, i32 %6) #5
  %1171 = mul <4 x i32> %1080, %42
  %1172 = mul <4 x i32> %38, %1105
  %1173 = add <4 x i32> %1171, %12
  %1174 = sub <4 x i32> %1173, %1172
  %1175 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1174, i32 %6) #5
  %1176 = mul <4 x i32> %1082, %38
  %1177 = mul <4 x i32> %1107, %42
  %1178 = add <4 x i32> %1176, %12
  %1179 = add <4 x i32> %1178, %1177
  %1180 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1179, i32 %6) #5
  %1181 = mul <4 x i32> %1082, %42
  %1182 = mul <4 x i32> %38, %1107
  %1183 = add <4 x i32> %1181, %12
  %1184 = sub <4 x i32> %1183, %1182
  %1185 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1184, i32 %6) #5
  %1186 = mul <4 x i32> %1083, %45
  %1187 = mul <4 x i32> %1106, %38
  %1188 = add <4 x i32> %1186, %12
  %1189 = add <4 x i32> %1188, %1187
  %1190 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1189, i32 %6) #5
  %1191 = mul <4 x i32> %1083, %38
  %1192 = mul <4 x i32> %45, %1106
  %1193 = add <4 x i32> %1191, %12
  %1194 = sub <4 x i32> %1193, %1192
  %1195 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1194, i32 %6) #5
  %1196 = mul <4 x i32> %1081, %45
  %1197 = mul <4 x i32> %1104, %38
  %1198 = add <4 x i32> %1196, %12
  %1199 = add <4 x i32> %1198, %1197
  %1200 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1199, i32 %6) #5
  %1201 = mul <4 x i32> %1081, %38
  %1202 = mul <4 x i32> %45, %1104
  %1203 = add <4 x i32> %1201, %12
  %1204 = sub <4 x i32> %1203, %1202
  %1205 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1204, i32 %6) #5
  %1206 = mul <4 x i32> %1088, %50
  %1207 = mul <4 x i32> %1097, %54
  %1208 = add <4 x i32> %1206, %12
  %1209 = add <4 x i32> %1208, %1207
  %1210 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1209, i32 %6) #5
  %1211 = mul <4 x i32> %1088, %54
  %1212 = mul <4 x i32> %50, %1097
  %1213 = add <4 x i32> %1211, %12
  %1214 = sub <4 x i32> %1213, %1212
  %1215 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1214, i32 %6) #5
  %1216 = mul <4 x i32> %1090, %50
  %1217 = mul <4 x i32> %1099, %54
  %1218 = add <4 x i32> %1216, %12
  %1219 = add <4 x i32> %1218, %1217
  %1220 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1219, i32 %6) #5
  %1221 = mul <4 x i32> %1090, %54
  %1222 = mul <4 x i32> %50, %1099
  %1223 = add <4 x i32> %1221, %12
  %1224 = sub <4 x i32> %1223, %1222
  %1225 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1224, i32 %6) #5
  %1226 = mul <4 x i32> %1091, %57
  %1227 = mul <4 x i32> %1098, %50
  %1228 = add <4 x i32> %1226, %12
  %1229 = add <4 x i32> %1228, %1227
  %1230 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1229, i32 %6) #5
  %1231 = mul <4 x i32> %1091, %50
  %1232 = mul <4 x i32> %57, %1098
  %1233 = add <4 x i32> %1231, %12
  %1234 = sub <4 x i32> %1233, %1232
  %1235 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1234, i32 %6) #5
  %1236 = mul <4 x i32> %1089, %57
  %1237 = mul <4 x i32> %1096, %50
  %1238 = add <4 x i32> %1236, %12
  %1239 = add <4 x i32> %1238, %1237
  %1240 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1239, i32 %6) #5
  %1241 = mul <4 x i32> %1089, %50
  %1242 = mul <4 x i32> %57, %1096
  %1243 = add <4 x i32> %1241, %12
  %1244 = sub <4 x i32> %1243, %1242
  %1245 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1244, i32 %6) #5
  %1246 = mul <4 x i32> %1129, %59
  %1247 = mul <4 x i32> %1126, %42
  %1248 = add <4 x i32> %1247, %12
  %1249 = add <4 x i32> %1248, %1246
  %1250 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1249, i32 %6) #5
  %1251 = mul <4 x i32> %1129, %42
  %1252 = mul <4 x i32> %59, %1126
  %1253 = sub <4 x i32> %12, %1252
  %1254 = add <4 x i32> %1253, %1251
  %1255 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1254, i32 %6) #5
  %1256 = mul <4 x i32> %1128, %61
  %1257 = mul <4 x i32> %1127, %54
  %1258 = add <4 x i32> %1257, %12
  %1259 = add <4 x i32> %1258, %1256
  %1260 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1259, i32 %6) #5
  %1261 = mul <4 x i32> %1128, %54
  %1262 = mul <4 x i32> %61, %1127
  %1263 = sub <4 x i32> %12, %1262
  %1264 = add <4 x i32> %1263, %1261
  %1265 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1264, i32 %6) #5
  %1266 = add <4 x i32> %1134, %1028
  %1267 = sub <4 x i32> %1028, %1134
  %1268 = sub <4 x i32> %1029, %1144
  %1269 = add <4 x i32> %1144, %1029
  %1270 = add <4 x i32> %1149, %1032
  %1271 = sub <4 x i32> %1032, %1149
  %1272 = sub <4 x i32> %1033, %1139
  %1273 = add <4 x i32> %1139, %1033
  %1274 = mul <4 x i32> %1152, %38
  %1275 = mul <4 x i32> %1165, %42
  %1276 = add <4 x i32> %1274, %12
  %1277 = add <4 x i32> %1276, %1275
  %1278 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1277, i32 %6) #5
  %1279 = mul <4 x i32> %1152, %42
  %1280 = mul <4 x i32> %38, %1165
  %1281 = add <4 x i32> %1279, %12
  %1282 = sub <4 x i32> %1281, %1280
  %1283 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1282, i32 %6) #5
  %1284 = mul <4 x i32> %1153, %45
  %1285 = mul <4 x i32> %1164, %38
  %1286 = add <4 x i32> %1284, %12
  %1287 = add <4 x i32> %1286, %1285
  %1288 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1287, i32 %6) #5
  %1289 = mul <4 x i32> %1153, %38
  %1290 = mul <4 x i32> %45, %1164
  %1291 = add <4 x i32> %1289, %12
  %1292 = sub <4 x i32> %1291, %1290
  %1293 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1292, i32 %6) #5
  %1294 = mul <4 x i32> %1156, %50
  %1295 = mul <4 x i32> %1161, %54
  %1296 = add <4 x i32> %1294, %12
  %1297 = add <4 x i32> %1296, %1295
  %1298 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1297, i32 %6) #5
  %1299 = mul <4 x i32> %1156, %54
  %1300 = mul <4 x i32> %50, %1161
  %1301 = add <4 x i32> %1299, %12
  %1302 = sub <4 x i32> %1301, %1300
  %1303 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1302, i32 %6) #5
  %1304 = mul <4 x i32> %1157, %57
  %1305 = mul <4 x i32> %1160, %50
  %1306 = add <4 x i32> %1304, %12
  %1307 = add <4 x i32> %1306, %1305
  %1308 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1307, i32 %6) #5
  %1309 = mul <4 x i32> %1157, %50
  %1310 = mul <4 x i32> %57, %1160
  %1311 = add <4 x i32> %1309, %12
  %1312 = sub <4 x i32> %1311, %1310
  %1313 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1312, i32 %6) #5
  %1314 = add <4 x i32> %1180, %1076
  %1315 = sub <4 x i32> %1076, %1180
  %1316 = add <4 x i32> %1170, %1078
  %1317 = sub <4 x i32> %1078, %1170
  %1318 = sub <4 x i32> %1077, %1190
  %1319 = add <4 x i32> %1190, %1077
  %1320 = sub <4 x i32> %1079, %1200
  %1321 = add <4 x i32> %1200, %1079
  %1322 = add <4 x i32> %1220, %1084
  %1323 = sub <4 x i32> %1084, %1220
  %1324 = add <4 x i32> %1210, %1086
  %1325 = sub <4 x i32> %1086, %1210
  %1326 = sub <4 x i32> %1085, %1230
  %1327 = add <4 x i32> %1230, %1085
  %1328 = sub <4 x i32> %1087, %1240
  %1329 = add <4 x i32> %1240, %1087
  %1330 = add <4 x i32> %1235, %1092
  %1331 = sub <4 x i32> %1092, %1235
  %1332 = add <4 x i32> %1245, %1094
  %1333 = sub <4 x i32> %1094, %1245
  %1334 = sub <4 x i32> %1093, %1225
  %1335 = add <4 x i32> %1225, %1093
  %1336 = sub <4 x i32> %1095, %1215
  %1337 = add <4 x i32> %1215, %1095
  %1338 = add <4 x i32> %1195, %1100
  %1339 = sub <4 x i32> %1100, %1195
  %1340 = add <4 x i32> %1205, %1102
  %1341 = sub <4 x i32> %1102, %1205
  %1342 = sub <4 x i32> %1101, %1185
  %1343 = add <4 x i32> %1185, %1101
  %1344 = sub <4 x i32> %1103, %1175
  %1345 = add <4 x i32> %1175, %1103
  %1346 = mul <4 x i32> %1273, %69
  %1347 = mul <4 x i32> %1266, %65
  %1348 = add <4 x i32> %1347, %12
  %1349 = add <4 x i32> %1348, %1346
  %1350 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1349, i32 %6) #5
  %1351 = mul <4 x i32> %1273, %65
  %1352 = mul <4 x i32> %69, %1266
  %1353 = sub <4 x i32> %12, %1352
  %1354 = add <4 x i32> %1353, %1351
  %1355 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1354, i32 %6) #5
  %1356 = mul <4 x i32> %1272, %77
  %1357 = mul <4 x i32> %1267, %73
  %1358 = add <4 x i32> %1357, %12
  %1359 = add <4 x i32> %1358, %1356
  %1360 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1359, i32 %6) #5
  %1361 = mul <4 x i32> %1272, %73
  %1362 = mul <4 x i32> %77, %1267
  %1363 = sub <4 x i32> %12, %1362
  %1364 = add <4 x i32> %1363, %1361
  %1365 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1364, i32 %6) #5
  %1366 = mul <4 x i32> %1271, %85
  %1367 = mul <4 x i32> %1268, %81
  %1368 = add <4 x i32> %1367, %12
  %1369 = add <4 x i32> %1368, %1366
  %1370 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1369, i32 %6) #5
  %1371 = mul <4 x i32> %1271, %81
  %1372 = mul <4 x i32> %85, %1268
  %1373 = sub <4 x i32> %12, %1372
  %1374 = add <4 x i32> %1373, %1371
  %1375 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1374, i32 %6) #5
  %1376 = mul <4 x i32> %1270, %93
  %1377 = mul <4 x i32> %1269, %89
  %1378 = add <4 x i32> %1377, %12
  %1379 = add <4 x i32> %1378, %1376
  %1380 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1379, i32 %6) #5
  %1381 = mul <4 x i32> %1270, %89
  %1382 = mul <4 x i32> %93, %1269
  %1383 = sub <4 x i32> %12, %1382
  %1384 = add <4 x i32> %1383, %1381
  %1385 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1384, i32 %6) #5
  %1386 = add <4 x i32> %1278, %1150
  %1387 = sub <4 x i32> %1150, %1278
  %1388 = sub <4 x i32> %1151, %1288
  %1389 = add <4 x i32> %1288, %1151
  %1390 = add <4 x i32> %1298, %1154
  %1391 = sub <4 x i32> %1154, %1298
  %1392 = sub <4 x i32> %1155, %1308
  %1393 = add <4 x i32> %1308, %1155
  %1394 = add <4 x i32> %1313, %1158
  %1395 = sub <4 x i32> %1158, %1313
  %1396 = sub <4 x i32> %1159, %1303
  %1397 = add <4 x i32> %1303, %1159
  %1398 = add <4 x i32> %1293, %1162
  %1399 = sub <4 x i32> %1162, %1293
  %1400 = sub <4 x i32> %1163, %1283
  %1401 = add <4 x i32> %1283, %1163
  %1402 = mul <4 x i32> %1316, %96
  %1403 = mul <4 x i32> %1345, %65
  %1404 = add <4 x i32> %1402, %12
  %1405 = add <4 x i32> %1404, %1403
  %1406 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1405, i32 %6) #5
  %1407 = mul <4 x i32> %1316, %65
  %1408 = mul <4 x i32> %96, %1345
  %1409 = add <4 x i32> %1407, %12
  %1410 = sub <4 x i32> %1409, %1408
  %1411 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1410, i32 %6) #5
  %1412 = mul <4 x i32> %1317, %99
  %1413 = mul <4 x i32> %1344, %96
  %1414 = add <4 x i32> %1412, %12
  %1415 = add <4 x i32> %1414, %1413
  %1416 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1415, i32 %6) #5
  %1417 = mul <4 x i32> %1317, %96
  %1418 = mul <4 x i32> %99, %1344
  %1419 = add <4 x i32> %1417, %12
  %1420 = sub <4 x i32> %1419, %1418
  %1421 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1420, i32 %6) #5
  %1422 = mul <4 x i32> %1320, %102
  %1423 = mul <4 x i32> %1341, %73
  %1424 = add <4 x i32> %1422, %12
  %1425 = add <4 x i32> %1424, %1423
  %1426 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1425, i32 %6) #5
  %1427 = mul <4 x i32> %1320, %73
  %1428 = mul <4 x i32> %102, %1341
  %1429 = add <4 x i32> %1427, %12
  %1430 = sub <4 x i32> %1429, %1428
  %1431 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1430, i32 %6) #5
  %1432 = mul <4 x i32> %1321, %105
  %1433 = mul <4 x i32> %1340, %102
  %1434 = add <4 x i32> %1432, %12
  %1435 = add <4 x i32> %1434, %1433
  %1436 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1435, i32 %6) #5
  %1437 = mul <4 x i32> %1321, %102
  %1438 = mul <4 x i32> %105, %1340
  %1439 = add <4 x i32> %1437, %12
  %1440 = sub <4 x i32> %1439, %1438
  %1441 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1440, i32 %6) #5
  %1442 = mul <4 x i32> %1324, %108
  %1443 = mul <4 x i32> %1337, %81
  %1444 = add <4 x i32> %1442, %12
  %1445 = add <4 x i32> %1444, %1443
  %1446 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1445, i32 %6) #5
  %1447 = mul <4 x i32> %1324, %81
  %1448 = mul <4 x i32> %108, %1337
  %1449 = add <4 x i32> %1447, %12
  %1450 = sub <4 x i32> %1449, %1448
  %1451 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1450, i32 %6) #5
  %1452 = mul <4 x i32> %1325, %111
  %1453 = mul <4 x i32> %1336, %108
  %1454 = add <4 x i32> %1452, %12
  %1455 = add <4 x i32> %1454, %1453
  %1456 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1455, i32 %6) #5
  %1457 = mul <4 x i32> %1325, %108
  %1458 = mul <4 x i32> %111, %1336
  %1459 = add <4 x i32> %1457, %12
  %1460 = sub <4 x i32> %1459, %1458
  %1461 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1460, i32 %6) #5
  %1462 = mul <4 x i32> %1328, %114
  %1463 = mul <4 x i32> %1333, %89
  %1464 = add <4 x i32> %1462, %12
  %1465 = add <4 x i32> %1464, %1463
  %1466 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1465, i32 %6) #5
  %1467 = mul <4 x i32> %1328, %89
  %1468 = mul <4 x i32> %114, %1333
  %1469 = add <4 x i32> %1467, %12
  %1470 = sub <4 x i32> %1469, %1468
  %1471 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1470, i32 %6) #5
  %1472 = mul <4 x i32> %1329, %117
  %1473 = mul <4 x i32> %1332, %114
  %1474 = add <4 x i32> %1472, %12
  %1475 = add <4 x i32> %1474, %1473
  %1476 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1475, i32 %6) #5
  %1477 = mul <4 x i32> %1329, %114
  %1478 = mul <4 x i32> %117, %1332
  %1479 = add <4 x i32> %1477, %12
  %1480 = sub <4 x i32> %1479, %1478
  %1481 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1480, i32 %6) #5
  %1482 = mul <4 x i32> %1401, %125
  %1483 = mul <4 x i32> %1386, %121
  %1484 = add <4 x i32> %1483, %12
  %1485 = add <4 x i32> %1484, %1482
  %1486 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1485, i32 %6) #5
  %1487 = mul <4 x i32> %1401, %121
  %1488 = mul <4 x i32> %125, %1386
  %1489 = sub <4 x i32> %12, %1488
  %1490 = add <4 x i32> %1489, %1487
  %1491 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1490, i32 %6) #5
  %1492 = mul <4 x i32> %1400, %133
  %1493 = mul <4 x i32> %1387, %129
  %1494 = add <4 x i32> %1493, %12
  %1495 = add <4 x i32> %1494, %1492
  %1496 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1495, i32 %6) #5
  %1497 = mul <4 x i32> %1400, %129
  %1498 = mul <4 x i32> %133, %1387
  %1499 = sub <4 x i32> %12, %1498
  %1500 = add <4 x i32> %1499, %1497
  %1501 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1500, i32 %6) #5
  %1502 = mul <4 x i32> %1399, %141
  %1503 = mul <4 x i32> %1388, %137
  %1504 = add <4 x i32> %1503, %12
  %1505 = add <4 x i32> %1504, %1502
  %1506 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1505, i32 %6) #5
  %1507 = mul <4 x i32> %1399, %137
  %1508 = mul <4 x i32> %141, %1388
  %1509 = sub <4 x i32> %12, %1508
  %1510 = add <4 x i32> %1509, %1507
  %1511 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1510, i32 %6) #5
  %1512 = mul <4 x i32> %1398, %149
  %1513 = mul <4 x i32> %1389, %145
  %1514 = add <4 x i32> %1513, %12
  %1515 = add <4 x i32> %1514, %1512
  %1516 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1515, i32 %6) #5
  %1517 = mul <4 x i32> %1398, %145
  %1518 = mul <4 x i32> %149, %1389
  %1519 = sub <4 x i32> %12, %1518
  %1520 = add <4 x i32> %1519, %1517
  %1521 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1520, i32 %6) #5
  %1522 = mul <4 x i32> %1397, %157
  %1523 = mul <4 x i32> %1390, %153
  %1524 = add <4 x i32> %1523, %12
  %1525 = add <4 x i32> %1524, %1522
  %1526 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1525, i32 %6) #5
  %1527 = mul <4 x i32> %1397, %153
  %1528 = mul <4 x i32> %157, %1390
  %1529 = sub <4 x i32> %12, %1528
  %1530 = add <4 x i32> %1529, %1527
  %1531 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1530, i32 %6) #5
  %1532 = mul <4 x i32> %1396, %165
  %1533 = mul <4 x i32> %1391, %161
  %1534 = add <4 x i32> %1533, %12
  %1535 = add <4 x i32> %1534, %1532
  %1536 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1535, i32 %6) #5
  %1537 = mul <4 x i32> %1396, %161
  %1538 = mul <4 x i32> %165, %1391
  %1539 = sub <4 x i32> %12, %1538
  %1540 = add <4 x i32> %1539, %1537
  %1541 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1540, i32 %6) #5
  %1542 = mul <4 x i32> %1395, %173
  %1543 = mul <4 x i32> %1392, %169
  %1544 = add <4 x i32> %1543, %12
  %1545 = add <4 x i32> %1544, %1542
  %1546 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1545, i32 %6) #5
  %1547 = mul <4 x i32> %1395, %169
  %1548 = mul <4 x i32> %173, %1392
  %1549 = sub <4 x i32> %12, %1548
  %1550 = add <4 x i32> %1549, %1547
  %1551 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1550, i32 %6) #5
  %1552 = mul <4 x i32> %1394, %181
  %1553 = mul <4 x i32> %1393, %177
  %1554 = add <4 x i32> %1553, %12
  %1555 = add <4 x i32> %1554, %1552
  %1556 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1555, i32 %6) #5
  %1557 = mul <4 x i32> %1394, %177
  %1558 = mul <4 x i32> %181, %1393
  %1559 = sub <4 x i32> %12, %1558
  %1560 = add <4 x i32> %1559, %1557
  %1561 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1560, i32 %6) #5
  %1562 = add <4 x i32> %1406, %1314
  %1563 = sub <4 x i32> %1314, %1406
  %1564 = sub <4 x i32> %1315, %1416
  %1565 = add <4 x i32> %1416, %1315
  %1566 = add <4 x i32> %1426, %1318
  %1567 = sub <4 x i32> %1318, %1426
  %1568 = sub <4 x i32> %1319, %1436
  %1569 = add <4 x i32> %1436, %1319
  %1570 = add <4 x i32> %1446, %1322
  %1571 = sub <4 x i32> %1322, %1446
  %1572 = sub <4 x i32> %1323, %1456
  %1573 = add <4 x i32> %1456, %1323
  %1574 = add <4 x i32> %1466, %1326
  %1575 = sub <4 x i32> %1326, %1466
  %1576 = sub <4 x i32> %1327, %1476
  %1577 = add <4 x i32> %1476, %1327
  %1578 = add <4 x i32> %1481, %1330
  %1579 = sub <4 x i32> %1330, %1481
  %1580 = sub <4 x i32> %1331, %1471
  %1581 = add <4 x i32> %1471, %1331
  %1582 = add <4 x i32> %1461, %1334
  %1583 = sub <4 x i32> %1334, %1461
  %1584 = sub <4 x i32> %1335, %1451
  %1585 = add <4 x i32> %1451, %1335
  %1586 = add <4 x i32> %1441, %1338
  %1587 = sub <4 x i32> %1338, %1441
  %1588 = sub <4 x i32> %1339, %1431
  %1589 = add <4 x i32> %1431, %1339
  %1590 = add <4 x i32> %1421, %1342
  %1591 = sub <4 x i32> %1342, %1421
  %1592 = sub <4 x i32> %1343, %1411
  %1593 = add <4 x i32> %1411, %1343
  %1594 = mul <4 x i32> %1593, %189
  %1595 = mul <4 x i32> %1562, %185
  %1596 = add <4 x i32> %1595, %12
  %1597 = add <4 x i32> %1596, %1594
  %1598 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1597, i32 %6) #5
  %1599 = mul <4 x i32> %1593, %185
  %1600 = mul <4 x i32> %189, %1562
  %1601 = sub <4 x i32> %12, %1600
  %1602 = add <4 x i32> %1601, %1599
  %1603 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1602, i32 %6) #5
  %1604 = mul <4 x i32> %1592, %197
  %1605 = mul <4 x i32> %1563, %193
  %1606 = add <4 x i32> %1605, %12
  %1607 = add <4 x i32> %1606, %1604
  %1608 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1607, i32 %6) #5
  %1609 = mul <4 x i32> %1592, %193
  %1610 = mul <4 x i32> %197, %1563
  %1611 = sub <4 x i32> %12, %1610
  %1612 = add <4 x i32> %1611, %1609
  %1613 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1612, i32 %6) #5
  %1614 = mul <4 x i32> %1591, %205
  %1615 = mul <4 x i32> %1564, %201
  %1616 = add <4 x i32> %1615, %12
  %1617 = add <4 x i32> %1616, %1614
  %1618 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1617, i32 %6) #5
  %1619 = mul <4 x i32> %1591, %201
  %1620 = mul <4 x i32> %205, %1564
  %1621 = sub <4 x i32> %12, %1620
  %1622 = add <4 x i32> %1621, %1619
  %1623 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1622, i32 %6) #5
  %1624 = mul <4 x i32> %1590, %213
  %1625 = mul <4 x i32> %1565, %209
  %1626 = add <4 x i32> %1625, %12
  %1627 = add <4 x i32> %1626, %1624
  %1628 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1627, i32 %6) #5
  %1629 = mul <4 x i32> %1590, %209
  %1630 = mul <4 x i32> %213, %1565
  %1631 = sub <4 x i32> %12, %1630
  %1632 = add <4 x i32> %1631, %1629
  %1633 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1632, i32 %6) #5
  %1634 = mul <4 x i32> %1589, %221
  %1635 = mul <4 x i32> %1566, %217
  %1636 = add <4 x i32> %1635, %12
  %1637 = add <4 x i32> %1636, %1634
  %1638 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1637, i32 %6) #5
  %1639 = mul <4 x i32> %1589, %217
  %1640 = mul <4 x i32> %221, %1566
  %1641 = sub <4 x i32> %12, %1640
  %1642 = add <4 x i32> %1641, %1639
  %1643 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1642, i32 %6) #5
  %1644 = mul <4 x i32> %1588, %229
  %1645 = mul <4 x i32> %1567, %225
  %1646 = add <4 x i32> %1645, %12
  %1647 = add <4 x i32> %1646, %1644
  %1648 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1647, i32 %6) #5
  %1649 = mul <4 x i32> %1588, %225
  %1650 = mul <4 x i32> %229, %1567
  %1651 = sub <4 x i32> %12, %1650
  %1652 = add <4 x i32> %1651, %1649
  %1653 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1652, i32 %6) #5
  %1654 = mul <4 x i32> %1587, %237
  %1655 = mul <4 x i32> %1568, %233
  %1656 = add <4 x i32> %1655, %12
  %1657 = add <4 x i32> %1656, %1654
  %1658 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1657, i32 %6) #5
  %1659 = mul <4 x i32> %1587, %233
  %1660 = mul <4 x i32> %237, %1568
  %1661 = sub <4 x i32> %12, %1660
  %1662 = add <4 x i32> %1661, %1659
  %1663 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1662, i32 %6) #5
  %1664 = mul <4 x i32> %1586, %245
  %1665 = mul <4 x i32> %1569, %241
  %1666 = add <4 x i32> %1665, %12
  %1667 = add <4 x i32> %1666, %1664
  %1668 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1667, i32 %6) #5
  %1669 = mul <4 x i32> %1586, %241
  %1670 = mul <4 x i32> %245, %1569
  %1671 = sub <4 x i32> %12, %1670
  %1672 = add <4 x i32> %1671, %1669
  %1673 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1672, i32 %6) #5
  %1674 = mul <4 x i32> %1585, %253
  %1675 = mul <4 x i32> %1570, %249
  %1676 = add <4 x i32> %1675, %12
  %1677 = add <4 x i32> %1676, %1674
  %1678 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1677, i32 %6) #5
  %1679 = mul <4 x i32> %1585, %249
  %1680 = mul <4 x i32> %253, %1570
  %1681 = sub <4 x i32> %12, %1680
  %1682 = add <4 x i32> %1681, %1679
  %1683 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1682, i32 %6) #5
  %1684 = mul <4 x i32> %1584, %261
  %1685 = mul <4 x i32> %1571, %257
  %1686 = add <4 x i32> %1685, %12
  %1687 = add <4 x i32> %1686, %1684
  %1688 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1687, i32 %6) #5
  %1689 = mul <4 x i32> %1584, %257
  %1690 = mul <4 x i32> %261, %1571
  %1691 = sub <4 x i32> %12, %1690
  %1692 = add <4 x i32> %1691, %1689
  %1693 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1692, i32 %6) #5
  %1694 = mul <4 x i32> %1583, %269
  %1695 = mul <4 x i32> %1572, %265
  %1696 = add <4 x i32> %1695, %12
  %1697 = add <4 x i32> %1696, %1694
  %1698 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1697, i32 %6) #5
  %1699 = mul <4 x i32> %1583, %265
  %1700 = mul <4 x i32> %269, %1572
  %1701 = sub <4 x i32> %12, %1700
  %1702 = add <4 x i32> %1701, %1699
  %1703 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1702, i32 %6) #5
  %1704 = mul <4 x i32> %1582, %277
  %1705 = mul <4 x i32> %1573, %273
  %1706 = add <4 x i32> %1705, %12
  %1707 = add <4 x i32> %1706, %1704
  %1708 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1707, i32 %6) #5
  %1709 = mul <4 x i32> %1582, %273
  %1710 = mul <4 x i32> %277, %1573
  %1711 = sub <4 x i32> %12, %1710
  %1712 = add <4 x i32> %1711, %1709
  %1713 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1712, i32 %6) #5
  %1714 = mul <4 x i32> %1581, %285
  %1715 = mul <4 x i32> %1574, %281
  %1716 = add <4 x i32> %1715, %12
  %1717 = add <4 x i32> %1716, %1714
  %1718 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1717, i32 %6) #5
  %1719 = mul <4 x i32> %1581, %281
  %1720 = mul <4 x i32> %285, %1574
  %1721 = sub <4 x i32> %12, %1720
  %1722 = add <4 x i32> %1721, %1719
  %1723 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1722, i32 %6) #5
  %1724 = mul <4 x i32> %1580, %293
  %1725 = mul <4 x i32> %1575, %289
  %1726 = add <4 x i32> %1725, %12
  %1727 = add <4 x i32> %1726, %1724
  %1728 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1727, i32 %6) #5
  %1729 = mul <4 x i32> %1580, %289
  %1730 = mul <4 x i32> %293, %1575
  %1731 = sub <4 x i32> %12, %1730
  %1732 = add <4 x i32> %1731, %1729
  %1733 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1732, i32 %6) #5
  %1734 = mul <4 x i32> %1579, %301
  %1735 = mul <4 x i32> %1576, %297
  %1736 = add <4 x i32> %1735, %12
  %1737 = add <4 x i32> %1736, %1734
  %1738 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1737, i32 %6) #5
  %1739 = mul <4 x i32> %1579, %297
  %1740 = mul <4 x i32> %301, %1576
  %1741 = sub <4 x i32> %12, %1740
  %1742 = add <4 x i32> %1741, %1739
  %1743 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1742, i32 %6) #5
  %1744 = mul <4 x i32> %1578, %309
  %1745 = mul <4 x i32> %1577, %305
  %1746 = add <4 x i32> %1745, %12
  %1747 = add <4 x i32> %1746, %1744
  %1748 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1747, i32 %6) #5
  %1749 = mul <4 x i32> %1578, %305
  %1750 = mul <4 x i32> %309, %1577
  %1751 = sub <4 x i32> %12, %1750
  %1752 = add <4 x i32> %1751, %1749
  %1753 = tail call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %1752, i32 %6) #5
  %1754 = mul nsw i32 %4, 63
  %1755 = bitcast <2 x i64>* %1 to <4 x i32>*
  store <4 x i32> %1112, <4 x i32>* %1755, align 16
  %1756 = sext i32 %1754 to i64
  %1757 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1756
  %1758 = bitcast <2 x i64>* %1757 to <4 x i32>*
  store <4 x i32> %1603, <4 x i32>* %1758, align 16
  %1759 = mul i32 %4, 62
  %1760 = sext i32 %4 to i64
  %1761 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1760
  %1762 = bitcast <2 x i64>* %1761 to <4 x i32>*
  store <4 x i32> %1598, <4 x i32>* %1762, align 16
  %1763 = sext i32 %1759 to i64
  %1764 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1763
  %1765 = bitcast <2 x i64>* %1764 to <4 x i32>*
  store <4 x i32> %1491, <4 x i32>* %1765, align 16
  %1766 = shl nsw i32 %4, 1
  %1767 = mul i32 %4, 61
  %1768 = sext i32 %1766 to i64
  %1769 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1768
  %1770 = bitcast <2 x i64>* %1769 to <4 x i32>*
  store <4 x i32> %1486, <4 x i32>* %1770, align 16
  %1771 = sext i32 %1767 to i64
  %1772 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1771
  %1773 = bitcast <2 x i64>* %1772 to <4 x i32>*
  store <4 x i32> %1748, <4 x i32>* %1773, align 16
  %1774 = mul nsw i32 %4, 3
  %1775 = mul i32 %4, 60
  %1776 = sext i32 %1774 to i64
  %1777 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1776
  %1778 = bitcast <2 x i64>* %1777 to <4 x i32>*
  store <4 x i32> %1753, <4 x i32>* %1778, align 16
  %1779 = sext i32 %1775 to i64
  %1780 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1779
  %1781 = bitcast <2 x i64>* %1780 to <4 x i32>*
  store <4 x i32> %1355, <4 x i32>* %1781, align 16
  %1782 = shl nsw i32 %4, 2
  %1783 = mul i32 %4, 59
  %1784 = sext i32 %1782 to i64
  %1785 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1784
  %1786 = bitcast <2 x i64>* %1785 to <4 x i32>*
  store <4 x i32> %1350, <4 x i32>* %1786, align 16
  %1787 = sext i32 %1783 to i64
  %1788 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1787
  %1789 = bitcast <2 x i64>* %1788 to <4 x i32>*
  store <4 x i32> %1683, <4 x i32>* %1789, align 16
  %1790 = mul nsw i32 %4, 5
  %1791 = mul i32 %4, 58
  %1792 = sext i32 %1790 to i64
  %1793 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1792
  %1794 = bitcast <2 x i64>* %1793 to <4 x i32>*
  store <4 x i32> %1678, <4 x i32>* %1794, align 16
  %1795 = sext i32 %1791 to i64
  %1796 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1795
  %1797 = bitcast <2 x i64>* %1796 to <4 x i32>*
  store <4 x i32> %1556, <4 x i32>* %1797, align 16
  %1798 = mul nsw i32 %4, 6
  %1799 = mul i32 %4, 57
  %1800 = sext i32 %1798 to i64
  %1801 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1800
  %1802 = bitcast <2 x i64>* %1801 to <4 x i32>*
  store <4 x i32> %1561, <4 x i32>* %1802, align 16
  %1803 = sext i32 %1799 to i64
  %1804 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1803
  %1805 = bitcast <2 x i64>* %1804 to <4 x i32>*
  store <4 x i32> %1668, <4 x i32>* %1805, align 16
  %1806 = mul nsw i32 %4, 7
  %1807 = mul i32 %4, 56
  %1808 = sext i32 %1806 to i64
  %1809 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1808
  %1810 = bitcast <2 x i64>* %1809 to <4 x i32>*
  store <4 x i32> %1673, <4 x i32>* %1810, align 16
  %1811 = sext i32 %1807 to i64
  %1812 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1811
  %1813 = bitcast <2 x i64>* %1812 to <4 x i32>*
  store <4 x i32> %1255, <4 x i32>* %1813, align 16
  %1814 = shl nsw i32 %4, 3
  %1815 = mul i32 %4, 55
  %1816 = sext i32 %1814 to i64
  %1817 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1816
  %1818 = bitcast <2 x i64>* %1817 to <4 x i32>*
  store <4 x i32> %1250, <4 x i32>* %1818, align 16
  %1819 = sext i32 %1815 to i64
  %1820 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1819
  %1821 = bitcast <2 x i64>* %1820 to <4 x i32>*
  store <4 x i32> %1643, <4 x i32>* %1821, align 16
  %1822 = mul nsw i32 %4, 9
  %1823 = mul i32 %4, 54
  %1824 = sext i32 %1822 to i64
  %1825 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1824
  %1826 = bitcast <2 x i64>* %1825 to <4 x i32>*
  store <4 x i32> %1638, <4 x i32>* %1826, align 16
  %1827 = sext i32 %1823 to i64
  %1828 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1827
  %1829 = bitcast <2 x i64>* %1828 to <4 x i32>*
  store <4 x i32> %1531, <4 x i32>* %1829, align 16
  %1830 = mul nsw i32 %4, 10
  %1831 = mul i32 %4, 53
  %1832 = sext i32 %1830 to i64
  %1833 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1832
  %1834 = bitcast <2 x i64>* %1833 to <4 x i32>*
  store <4 x i32> %1526, <4 x i32>* %1834, align 16
  %1835 = sext i32 %1831 to i64
  %1836 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1835
  %1837 = bitcast <2 x i64>* %1836 to <4 x i32>*
  store <4 x i32> %1708, <4 x i32>* %1837, align 16
  %1838 = mul nsw i32 %4, 11
  %1839 = mul i32 %4, 52
  %1840 = sext i32 %1838 to i64
  %1841 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1840
  %1842 = bitcast <2 x i64>* %1841 to <4 x i32>*
  store <4 x i32> %1713, <4 x i32>* %1842, align 16
  %1843 = sext i32 %1839 to i64
  %1844 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1843
  %1845 = bitcast <2 x i64>* %1844 to <4 x i32>*
  store <4 x i32> %1380, <4 x i32>* %1845, align 16
  %1846 = mul nsw i32 %4, 12
  %1847 = mul i32 %4, 51
  %1848 = sext i32 %1846 to i64
  %1849 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1848
  %1850 = bitcast <2 x i64>* %1849 to <4 x i32>*
  store <4 x i32> %1385, <4 x i32>* %1850, align 16
  %1851 = sext i32 %1847 to i64
  %1852 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1851
  %1853 = bitcast <2 x i64>* %1852 to <4 x i32>*
  store <4 x i32> %1723, <4 x i32>* %1853, align 16
  %1854 = mul nsw i32 %4, 13
  %1855 = mul i32 %4, 50
  %1856 = sext i32 %1854 to i64
  %1857 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1856
  %1858 = bitcast <2 x i64>* %1857 to <4 x i32>*
  store <4 x i32> %1718, <4 x i32>* %1858, align 16
  %1859 = sext i32 %1855 to i64
  %1860 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1859
  %1861 = bitcast <2 x i64>* %1860 to <4 x i32>*
  store <4 x i32> %1516, <4 x i32>* %1861, align 16
  %1862 = mul nsw i32 %4, 14
  %1863 = mul i32 %4, 49
  %1864 = sext i32 %1862 to i64
  %1865 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1864
  %1866 = bitcast <2 x i64>* %1865 to <4 x i32>*
  store <4 x i32> %1521, <4 x i32>* %1866, align 16
  %1867 = sext i32 %1863 to i64
  %1868 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1867
  %1869 = bitcast <2 x i64>* %1868 to <4 x i32>*
  store <4 x i32> %1628, <4 x i32>* %1869, align 16
  %1870 = mul nsw i32 %4, 15
  %1871 = mul i32 %4, 48
  %1872 = sext i32 %1870 to i64
  %1873 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1872
  %1874 = bitcast <2 x i64>* %1873 to <4 x i32>*
  store <4 x i32> %1633, <4 x i32>* %1874, align 16
  %1875 = sext i32 %1871 to i64
  %1876 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1875
  %1877 = bitcast <2 x i64>* %1876 to <4 x i32>*
  store <4 x i32> %1125, <4 x i32>* %1877, align 16
  %1878 = shl nsw i32 %4, 4
  %1879 = mul i32 %4, 47
  %1880 = sext i32 %1878 to i64
  %1881 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1880
  %1882 = bitcast <2 x i64>* %1881 to <4 x i32>*
  store <4 x i32> %1120, <4 x i32>* %1882, align 16
  %1883 = sext i32 %1879 to i64
  %1884 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1883
  %1885 = bitcast <2 x i64>* %1884 to <4 x i32>*
  store <4 x i32> %1623, <4 x i32>* %1885, align 16
  %1886 = mul nsw i32 %4, 17
  %1887 = mul i32 %4, 46
  %1888 = sext i32 %1886 to i64
  %1889 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1888
  %1890 = bitcast <2 x i64>* %1889 to <4 x i32>*
  store <4 x i32> %1618, <4 x i32>* %1890, align 16
  %1891 = sext i32 %1887 to i64
  %1892 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1891
  %1893 = bitcast <2 x i64>* %1892 to <4 x i32>*
  store <4 x i32> %1511, <4 x i32>* %1893, align 16
  %1894 = mul nsw i32 %4, 18
  %1895 = mul i32 %4, 45
  %1896 = sext i32 %1894 to i64
  %1897 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1896
  %1898 = bitcast <2 x i64>* %1897 to <4 x i32>*
  store <4 x i32> %1506, <4 x i32>* %1898, align 16
  %1899 = sext i32 %1895 to i64
  %1900 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1899
  %1901 = bitcast <2 x i64>* %1900 to <4 x i32>*
  store <4 x i32> %1728, <4 x i32>* %1901, align 16
  %1902 = mul nsw i32 %4, 19
  %1903 = mul i32 %4, 44
  %1904 = sext i32 %1902 to i64
  %1905 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1904
  %1906 = bitcast <2 x i64>* %1905 to <4 x i32>*
  store <4 x i32> %1733, <4 x i32>* %1906, align 16
  %1907 = sext i32 %1903 to i64
  %1908 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1907
  %1909 = bitcast <2 x i64>* %1908 to <4 x i32>*
  store <4 x i32> %1375, <4 x i32>* %1909, align 16
  %1910 = mul nsw i32 %4, 20
  %1911 = mul i32 %4, 43
  %1912 = sext i32 %1910 to i64
  %1913 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1912
  %1914 = bitcast <2 x i64>* %1913 to <4 x i32>*
  store <4 x i32> %1370, <4 x i32>* %1914, align 16
  %1915 = sext i32 %1911 to i64
  %1916 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1915
  %1917 = bitcast <2 x i64>* %1916 to <4 x i32>*
  store <4 x i32> %1703, <4 x i32>* %1917, align 16
  %1918 = mul nsw i32 %4, 21
  %1919 = mul i32 %4, 42
  %1920 = sext i32 %1918 to i64
  %1921 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1920
  %1922 = bitcast <2 x i64>* %1921 to <4 x i32>*
  store <4 x i32> %1698, <4 x i32>* %1922, align 16
  %1923 = sext i32 %1919 to i64
  %1924 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1923
  %1925 = bitcast <2 x i64>* %1924 to <4 x i32>*
  store <4 x i32> %1536, <4 x i32>* %1925, align 16
  %1926 = mul nsw i32 %4, 22
  %1927 = mul i32 %4, 41
  %1928 = sext i32 %1926 to i64
  %1929 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1928
  %1930 = bitcast <2 x i64>* %1929 to <4 x i32>*
  store <4 x i32> %1541, <4 x i32>* %1930, align 16
  %1931 = sext i32 %1927 to i64
  %1932 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1931
  %1933 = bitcast <2 x i64>* %1932 to <4 x i32>*
  store <4 x i32> %1648, <4 x i32>* %1933, align 16
  %1934 = mul nsw i32 %4, 23
  %1935 = mul i32 %4, 40
  %1936 = sext i32 %1934 to i64
  %1937 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1936
  %1938 = bitcast <2 x i64>* %1937 to <4 x i32>*
  store <4 x i32> %1653, <4 x i32>* %1938, align 16
  %1939 = sext i32 %1935 to i64
  %1940 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1939
  %1941 = bitcast <2 x i64>* %1940 to <4 x i32>*
  store <4 x i32> %1260, <4 x i32>* %1941, align 16
  %1942 = mul nsw i32 %4, 24
  %1943 = mul i32 %4, 39
  %1944 = sext i32 %1942 to i64
  %1945 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1944
  %1946 = bitcast <2 x i64>* %1945 to <4 x i32>*
  store <4 x i32> %1265, <4 x i32>* %1946, align 16
  %1947 = sext i32 %1943 to i64
  %1948 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1947
  %1949 = bitcast <2 x i64>* %1948 to <4 x i32>*
  store <4 x i32> %1663, <4 x i32>* %1949, align 16
  %1950 = mul nsw i32 %4, 25
  %1951 = mul i32 %4, 38
  %1952 = sext i32 %1950 to i64
  %1953 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1952
  %1954 = bitcast <2 x i64>* %1953 to <4 x i32>*
  store <4 x i32> %1658, <4 x i32>* %1954, align 16
  %1955 = sext i32 %1951 to i64
  %1956 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1955
  %1957 = bitcast <2 x i64>* %1956 to <4 x i32>*
  store <4 x i32> %1551, <4 x i32>* %1957, align 16
  %1958 = mul nsw i32 %4, 26
  %1959 = mul i32 %4, 37
  %1960 = sext i32 %1958 to i64
  %1961 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1960
  %1962 = bitcast <2 x i64>* %1961 to <4 x i32>*
  store <4 x i32> %1546, <4 x i32>* %1962, align 16
  %1963 = sext i32 %1959 to i64
  %1964 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1963
  %1965 = bitcast <2 x i64>* %1964 to <4 x i32>*
  store <4 x i32> %1688, <4 x i32>* %1965, align 16
  %1966 = mul nsw i32 %4, 27
  %1967 = mul i32 %4, 36
  %1968 = sext i32 %1966 to i64
  %1969 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1968
  %1970 = bitcast <2 x i64>* %1969 to <4 x i32>*
  store <4 x i32> %1693, <4 x i32>* %1970, align 16
  %1971 = sext i32 %1967 to i64
  %1972 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1971
  %1973 = bitcast <2 x i64>* %1972 to <4 x i32>*
  store <4 x i32> %1360, <4 x i32>* %1973, align 16
  %1974 = mul nsw i32 %4, 28
  %1975 = mul i32 %4, 35
  %1976 = sext i32 %1974 to i64
  %1977 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1976
  %1978 = bitcast <2 x i64>* %1977 to <4 x i32>*
  store <4 x i32> %1365, <4 x i32>* %1978, align 16
  %1979 = sext i32 %1975 to i64
  %1980 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1979
  %1981 = bitcast <2 x i64>* %1980 to <4 x i32>*
  store <4 x i32> %1743, <4 x i32>* %1981, align 16
  %1982 = mul nsw i32 %4, 29
  %1983 = mul i32 %4, 34
  %1984 = sext i32 %1982 to i64
  %1985 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1984
  %1986 = bitcast <2 x i64>* %1985 to <4 x i32>*
  store <4 x i32> %1738, <4 x i32>* %1986, align 16
  %1987 = sext i32 %1983 to i64
  %1988 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1987
  %1989 = bitcast <2 x i64>* %1988 to <4 x i32>*
  store <4 x i32> %1496, <4 x i32>* %1989, align 16
  %1990 = mul nsw i32 %4, 30
  %1991 = mul i32 %4, 33
  %1992 = sext i32 %1990 to i64
  %1993 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1992
  %1994 = bitcast <2 x i64>* %1993 to <4 x i32>*
  store <4 x i32> %1501, <4 x i32>* %1994, align 16
  %1995 = sext i32 %1991 to i64
  %1996 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %1995
  %1997 = bitcast <2 x i64>* %1996 to <4 x i32>*
  store <4 x i32> %1608, <4 x i32>* %1997, align 16
  %1998 = mul nsw i32 %4, 31
  %1999 = shl i32 %4, 5
  %2000 = sext i32 %1998 to i64
  %2001 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %2000
  %2002 = bitcast <2 x i64>* %2001 to <4 x i32>*
  store <4 x i32> %1613, <4 x i32>* %2002, align 16
  %2003 = sext i32 %1999 to i64
  %2004 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %2003
  %2005 = bitcast <2 x i64>* %2004 to <4 x i32>*
  store <4 x i32> %1115, <4 x i32>* %2005, align 16
  ret void
}

; Function Attrs: nofree norecurse nounwind ssp uwtable
define hidden void @av1_idtx32_sse4_1(<2 x i64>* nocapture readonly, <2 x i64>* nocapture, i32, i32) local_unnamed_addr #3 {
  %5 = sext i32 %3 to i64
  br label %7

6:                                                ; preds = %7
  ret void

7:                                                ; preds = %7, %4
  %8 = phi i64 [ 0, %4 ], [ %24, %7 ]
  %9 = mul nsw i64 %8, %5
  %10 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %9
  %11 = bitcast <2 x i64>* %10 to <4 x i32>*
  %12 = load <4 x i32>, <4 x i32>* %11, align 16
  %13 = shl <4 x i32> %12, <i32 2, i32 2, i32 2, i32 2>
  %14 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %9
  %15 = bitcast <2 x i64>* %14 to <4 x i32>*
  store <4 x i32> %13, <4 x i32>* %15, align 16
  %16 = or i64 %8, 1
  %17 = mul nsw i64 %16, %5
  %18 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 %17
  %19 = bitcast <2 x i64>* %18 to <4 x i32>*
  %20 = load <4 x i32>, <4 x i32>* %19, align 16
  %21 = shl <4 x i32> %20, <i32 2, i32 2, i32 2, i32 2>
  %22 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 %17
  %23 = bitcast <2 x i64>* %22 to <4 x i32>*
  store <4 x i32> %21, <4 x i32>* %23, align 16
  %24 = add nuw nsw i64 %8, 2
  %25 = icmp eq i64 %24, 32
  br i1 %25, label %6, label %7
}

; Function Attrs: nounwind readnone
declare <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32>, i32) #4

; Function Attrs: argmemonly nounwind
declare void @llvm.memcpy.p0i8.p0i8.i64(i8* nocapture writeonly, i8* nocapture readonly, i64, i1 immarg) #1

attributes #0 = { nofree nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="128" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+sse4.1,+ssse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { argmemonly nounwind }
attributes #2 = { nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="128" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+sse4.1,+ssse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #3 = { nofree norecurse nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="128" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+sse4.1,+ssse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #4 = { nounwind readnone }
attributes #5 = { nounwind }

!llvm.module.flags = !{!0, !1}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 2}
