; ModuleID = '../../third_party/libvpx/source/libvpx/vp9/encoder/x86/vp9_dct_intrin_sse2.c'
source_filename = "../../third_party/libvpx/source/libvpx/vp9/encoder/x86/vp9_dct_intrin_sse2.c"
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

; Function Attrs: nounwind ssp uwtable
define hidden void @vp9_fht4x4_sse2(i16*, i32*, i32, i32) local_unnamed_addr #0 {
  switch i32 %3, label %244 [
    i32 0, label %5
    i32 1, label %6
    i32 2, label %125
  ]

5:                                                ; preds = %4
  tail call void @vpx_fdct4x4_sse2(i16* %0, i32* %1, i32 %2) #6
  br label %374

6:                                                ; preds = %4
  %7 = bitcast i16* %0 to i64*
  %8 = load i64, i64* %7, align 1
  %9 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %8, i32 0
  %10 = sext i32 %2 to i64
  %11 = getelementptr inbounds i16, i16* %0, i64 %10
  %12 = bitcast i16* %11 to i64*
  %13 = load i64, i64* %12, align 1
  %14 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %13, i32 0
  %15 = shl nsw i32 %2, 1
  %16 = sext i32 %15 to i64
  %17 = getelementptr inbounds i16, i16* %0, i64 %16
  %18 = bitcast i16* %17 to i64*
  %19 = load i64, i64* %18, align 1
  %20 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %19, i32 0
  %21 = mul nsw i32 %2, 3
  %22 = sext i32 %21 to i64
  %23 = getelementptr inbounds i16, i16* %0, i64 %22
  %24 = bitcast i16* %23 to i64*
  %25 = load i64, i64* %24, align 1
  %26 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %25, i32 0
  %27 = bitcast <2 x i64> %9 to <8 x i16>
  %28 = shl <8 x i16> %27, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %29 = bitcast <2 x i64> %14 to <8 x i16>
  %30 = shl <8 x i16> %29, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %31 = bitcast <2 x i64> %20 to <8 x i16>
  %32 = shl <8 x i16> %31, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %33 = bitcast <2 x i64> %26 to <8 x i16>
  %34 = shl <8 x i16> %33, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %35 = icmp eq <8 x i16> %28, <i16 0, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %36 = or <8 x i16> %28, <i16 1, i16 0, i16 0, i16 0, i16 0, i16 0, i16 0, i16 0>
  %37 = zext <8 x i1> %35 to <8 x i16>
  %38 = sub <8 x i16> %36, %37
  %39 = add <8 x i16> %38, %30
  %40 = shufflevector <8 x i16> %38, <8 x i16> %30, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %41 = shufflevector <8 x i16> %32, <8 x i16> %34, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %42 = shufflevector <8 x i16> %39, <8 x i16> <i16 0, i16 0, i16 0, i16 0, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %43 = shufflevector <8 x i16> %32, <8 x i16> <i16 0, i16 0, i16 0, i16 0, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %44 = shufflevector <8 x i16> %34, <8 x i16> <i16 0, i16 0, i16 0, i16 0, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %45 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %40, <8 x i16> <i16 5283, i16 9929, i16 5283, i16 9929, i16 5283, i16 9929, i16 5283, i16 9929>) #6
  %46 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %41, <8 x i16> <i16 13377, i16 15212, i16 13377, i16 15212, i16 13377, i16 15212, i16 13377, i16 15212>) #6
  %47 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %42, <8 x i16> <i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377>) #6
  %48 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %40, <8 x i16> <i16 15212, i16 -5283, i16 15212, i16 -5283, i16 15212, i16 -5283, i16 15212, i16 -5283>) #6
  %49 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %41, <8 x i16> <i16 -13377, i16 9929, i16 -13377, i16 9929, i16 -13377, i16 9929, i16 -13377, i16 9929>) #6
  %50 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %43, <8 x i16> <i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377>) #6
  %51 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %44, <8 x i16> <i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377>) #6
  %52 = add <4 x i32> %46, %45
  %53 = add <4 x i32> %49, %48
  %54 = mul <4 x i32> %50, <i32 3, i32 3, i32 3, i32 3>
  %55 = add <4 x i32> %52, <i32 8192, i32 8192, i32 8192, i32 8192>
  %56 = add <4 x i32> %47, <i32 8192, i32 8192, i32 8192, i32 8192>
  %57 = sub <4 x i32> %56, %51
  %58 = add <4 x i32> %53, <i32 8192, i32 8192, i32 8192, i32 8192>
  %59 = sub <4 x i32> <i32 8192, i32 8192, i32 8192, i32 8192>, %52
  %60 = add <4 x i32> %59, %53
  %61 = add <4 x i32> %60, %54
  %62 = ashr <4 x i32> %55, <i32 14, i32 14, i32 14, i32 14>
  %63 = ashr <4 x i32> %57, <i32 14, i32 14, i32 14, i32 14>
  %64 = ashr <4 x i32> %58, <i32 14, i32 14, i32 14, i32 14>
  %65 = ashr <4 x i32> %61, <i32 14, i32 14, i32 14, i32 14>
  %66 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %62, <4 x i32> %64) #6
  %67 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %63, <4 x i32> %65) #6
  %68 = shufflevector <8 x i16> %66, <8 x i16> %67, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %69 = shufflevector <8 x i16> %66, <8 x i16> %67, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %70 = bitcast <8 x i16> %68 to <4 x i32>
  %71 = bitcast <8 x i16> %69 to <4 x i32>
  %72 = shufflevector <4 x i32> %70, <4 x i32> %71, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %73 = bitcast <4 x i32> %72 to <2 x i64>
  %74 = shufflevector <4 x i32> %70, <4 x i32> %71, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %75 = bitcast <4 x i32> %74 to <2 x i64>
  %76 = shufflevector <2 x i64> %73, <2 x i64> undef, <2 x i32> <i32 1, i32 undef>
  %77 = shufflevector <2 x i64> %75, <2 x i64> undef, <2 x i32> <i32 1, i32 undef>
  %78 = bitcast <4 x i32> %72 to <8 x i16>
  %79 = bitcast <2 x i64> %76 to <8 x i16>
  %80 = shufflevector <8 x i16> %78, <8 x i16> %79, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %81 = bitcast <2 x i64> %77 to <8 x i16>
  %82 = bitcast <4 x i32> %74 to <8 x i16>
  %83 = shufflevector <8 x i16> %81, <8 x i16> %82, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %84 = add <8 x i16> %83, %80
  %85 = sub <8 x i16> %80, %83
  %86 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %84, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %87 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %84, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %88 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %85, <8 x i16> <i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270>) #6
  %89 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %85, <8 x i16> <i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137>) #6
  %90 = add <4 x i32> %86, <i32 8192, i32 8192, i32 8192, i32 8192>
  %91 = add <4 x i32> %87, <i32 8192, i32 8192, i32 8192, i32 8192>
  %92 = add <4 x i32> %88, <i32 8192, i32 8192, i32 8192, i32 8192>
  %93 = add <4 x i32> %89, <i32 8192, i32 8192, i32 8192, i32 8192>
  %94 = ashr <4 x i32> %90, <i32 14, i32 14, i32 14, i32 14>
  %95 = ashr <4 x i32> %91, <i32 14, i32 14, i32 14, i32 14>
  %96 = ashr <4 x i32> %92, <i32 14, i32 14, i32 14, i32 14>
  %97 = ashr <4 x i32> %93, <i32 14, i32 14, i32 14, i32 14>
  %98 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %94, <4 x i32> %95) #6
  %99 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %96, <4 x i32> %97) #6
  %100 = shufflevector <8 x i16> %98, <8 x i16> %99, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %101 = shufflevector <8 x i16> %98, <8 x i16> %99, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %102 = bitcast <8 x i16> %100 to <4 x i32>
  %103 = bitcast <8 x i16> %101 to <4 x i32>
  %104 = shufflevector <4 x i32> %102, <4 x i32> %103, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %105 = shufflevector <4 x i32> %102, <4 x i32> %103, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %106 = bitcast <4 x i32> %104 to <8 x i16>
  %107 = add <8 x i16> %106, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %108 = bitcast <4 x i32> %105 to <8 x i16>
  %109 = add <8 x i16> %108, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %110 = ashr <8 x i16> %107, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %111 = ashr <8 x i16> %109, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %112 = ashr <8 x i16> %107, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %113 = shufflevector <8 x i16> %110, <8 x i16> %112, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %114 = shufflevector <8 x i16> %110, <8 x i16> %112, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %115 = bitcast i32* %1 to <8 x i16>*
  store <8 x i16> %113, <8 x i16>* %115, align 16
  %116 = getelementptr inbounds i32, i32* %1, i64 4
  %117 = bitcast i32* %116 to <8 x i16>*
  store <8 x i16> %114, <8 x i16>* %117, align 16
  %118 = getelementptr inbounds i32, i32* %1, i64 8
  %119 = ashr <8 x i16> %109, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %120 = shufflevector <8 x i16> %111, <8 x i16> %119, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %121 = shufflevector <8 x i16> %111, <8 x i16> %119, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %122 = bitcast i32* %118 to <8 x i16>*
  store <8 x i16> %120, <8 x i16>* %122, align 16
  %123 = getelementptr inbounds i32, i32* %1, i64 12
  %124 = bitcast i32* %123 to <8 x i16>*
  store <8 x i16> %121, <8 x i16>* %124, align 16
  br label %374

125:                                              ; preds = %4
  %126 = bitcast i16* %0 to i64*
  %127 = load i64, i64* %126, align 1
  %128 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %127, i32 0
  %129 = sext i32 %2 to i64
  %130 = getelementptr inbounds i16, i16* %0, i64 %129
  %131 = bitcast i16* %130 to i64*
  %132 = load i64, i64* %131, align 1
  %133 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %132, i32 0
  %134 = shl nsw i32 %2, 1
  %135 = sext i32 %134 to i64
  %136 = getelementptr inbounds i16, i16* %0, i64 %135
  %137 = bitcast i16* %136 to i64*
  %138 = load i64, i64* %137, align 1
  %139 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %138, i32 0
  %140 = mul nsw i32 %2, 3
  %141 = sext i32 %140 to i64
  %142 = getelementptr inbounds i16, i16* %0, i64 %141
  %143 = bitcast i16* %142 to i64*
  %144 = load i64, i64* %143, align 1
  %145 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %144, i32 0
  %146 = bitcast <2 x i64> %128 to <8 x i16>
  %147 = shl <8 x i16> %146, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %148 = bitcast <2 x i64> %133 to <8 x i16>
  %149 = shl <8 x i16> %148, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %150 = bitcast <2 x i64> %139 to <8 x i16>
  %151 = shl <8 x i16> %150, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %152 = bitcast <2 x i64> %145 to <8 x i16>
  %153 = shl <8 x i16> %152, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %154 = icmp eq <8 x i16> %147, <i16 0, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %155 = or <8 x i16> %147, <i16 1, i16 0, i16 0, i16 0, i16 undef, i16 undef, i16 undef, i16 undef>
  %156 = zext <8 x i1> %154 to <8 x i16>
  %157 = sub <8 x i16> %155, %156
  %158 = shufflevector <8 x i16> %157, <8 x i16> %149, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %159 = shufflevector <8 x i16> %153, <8 x i16> %151, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %160 = add <8 x i16> %159, %158
  %161 = sub <8 x i16> %158, %159
  %162 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %160, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %163 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %160, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %164 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %161, <8 x i16> <i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270>) #6
  %165 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %161, <8 x i16> <i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137>) #6
  %166 = add <4 x i32> %162, <i32 8192, i32 8192, i32 8192, i32 8192>
  %167 = add <4 x i32> %163, <i32 8192, i32 8192, i32 8192, i32 8192>
  %168 = add <4 x i32> %164, <i32 8192, i32 8192, i32 8192, i32 8192>
  %169 = add <4 x i32> %165, <i32 8192, i32 8192, i32 8192, i32 8192>
  %170 = ashr <4 x i32> %166, <i32 14, i32 14, i32 14, i32 14>
  %171 = ashr <4 x i32> %167, <i32 14, i32 14, i32 14, i32 14>
  %172 = ashr <4 x i32> %168, <i32 14, i32 14, i32 14, i32 14>
  %173 = ashr <4 x i32> %169, <i32 14, i32 14, i32 14, i32 14>
  %174 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %170, <4 x i32> %171) #6
  %175 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %172, <4 x i32> %173) #6
  %176 = shufflevector <8 x i16> %174, <8 x i16> %175, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %177 = shufflevector <8 x i16> %174, <8 x i16> %175, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %178 = bitcast <8 x i16> %176 to <4 x i32>
  %179 = bitcast <8 x i16> %177 to <4 x i32>
  %180 = shufflevector <4 x i32> %178, <4 x i32> %179, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %181 = bitcast <4 x i32> %180 to <2 x i64>
  %182 = shufflevector <4 x i32> %178, <4 x i32> %179, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %183 = bitcast <4 x i32> %182 to <2 x i64>
  %184 = shufflevector <2 x i64> %181, <2 x i64> undef, <2 x i32> <i32 1, i32 1>
  %185 = shufflevector <2 x i64> %183, <2 x i64> undef, <2 x i32> <i32 1, i32 1>
  %186 = bitcast <4 x i32> %180 to <8 x i16>
  %187 = bitcast <2 x i64> %184 to <8 x i16>
  %188 = add <8 x i16> %187, %186
  %189 = shufflevector <8 x i16> %186, <8 x i16> %187, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %190 = bitcast <4 x i32> %182 to <8 x i16>
  %191 = bitcast <2 x i64> %185 to <8 x i16>
  %192 = shufflevector <8 x i16> %190, <8 x i16> %191, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %193 = shufflevector <8 x i16> %188, <8 x i16> <i16 0, i16 0, i16 0, i16 0, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %194 = shufflevector <8 x i16> %190, <8 x i16> <i16 0, i16 0, i16 0, i16 0, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %195 = shufflevector <8 x i16> %191, <8 x i16> <i16 0, i16 0, i16 0, i16 0, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %196 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %189, <8 x i16> <i16 5283, i16 9929, i16 5283, i16 9929, i16 5283, i16 9929, i16 5283, i16 9929>) #6
  %197 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %192, <8 x i16> <i16 13377, i16 15212, i16 13377, i16 15212, i16 13377, i16 15212, i16 13377, i16 15212>) #6
  %198 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %193, <8 x i16> <i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377>) #6
  %199 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %189, <8 x i16> <i16 15212, i16 -5283, i16 15212, i16 -5283, i16 15212, i16 -5283, i16 15212, i16 -5283>) #6
  %200 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %192, <8 x i16> <i16 -13377, i16 9929, i16 -13377, i16 9929, i16 -13377, i16 9929, i16 -13377, i16 9929>) #6
  %201 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %194, <8 x i16> <i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377>) #6
  %202 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %195, <8 x i16> <i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377>) #6
  %203 = add <4 x i32> %197, %196
  %204 = add <4 x i32> %200, %199
  %205 = mul <4 x i32> %201, <i32 3, i32 3, i32 3, i32 3>
  %206 = add <4 x i32> %203, <i32 8192, i32 8192, i32 8192, i32 8192>
  %207 = add <4 x i32> %198, <i32 8192, i32 8192, i32 8192, i32 8192>
  %208 = sub <4 x i32> %207, %202
  %209 = add <4 x i32> %204, <i32 8192, i32 8192, i32 8192, i32 8192>
  %210 = sub <4 x i32> <i32 8192, i32 8192, i32 8192, i32 8192>, %203
  %211 = add <4 x i32> %210, %204
  %212 = add <4 x i32> %211, %205
  %213 = ashr <4 x i32> %206, <i32 14, i32 14, i32 14, i32 14>
  %214 = ashr <4 x i32> %208, <i32 14, i32 14, i32 14, i32 14>
  %215 = ashr <4 x i32> %209, <i32 14, i32 14, i32 14, i32 14>
  %216 = ashr <4 x i32> %212, <i32 14, i32 14, i32 14, i32 14>
  %217 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %213, <4 x i32> %215) #6
  %218 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %214, <4 x i32> %216) #6
  %219 = shufflevector <8 x i16> %217, <8 x i16> %218, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %220 = shufflevector <8 x i16> %217, <8 x i16> %218, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %221 = bitcast <8 x i16> %219 to <4 x i32>
  %222 = bitcast <8 x i16> %220 to <4 x i32>
  %223 = shufflevector <4 x i32> %221, <4 x i32> %222, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %224 = shufflevector <4 x i32> %221, <4 x i32> %222, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %225 = bitcast <4 x i32> %223 to <8 x i16>
  %226 = add <8 x i16> %225, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %227 = bitcast <4 x i32> %224 to <8 x i16>
  %228 = add <8 x i16> %227, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %229 = ashr <8 x i16> %226, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %230 = ashr <8 x i16> %228, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %231 = ashr <8 x i16> %226, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %232 = shufflevector <8 x i16> %229, <8 x i16> %231, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %233 = shufflevector <8 x i16> %229, <8 x i16> %231, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %234 = bitcast i32* %1 to <8 x i16>*
  store <8 x i16> %232, <8 x i16>* %234, align 16
  %235 = getelementptr inbounds i32, i32* %1, i64 4
  %236 = bitcast i32* %235 to <8 x i16>*
  store <8 x i16> %233, <8 x i16>* %236, align 16
  %237 = getelementptr inbounds i32, i32* %1, i64 8
  %238 = ashr <8 x i16> %228, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %239 = shufflevector <8 x i16> %230, <8 x i16> %238, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %240 = shufflevector <8 x i16> %230, <8 x i16> %238, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %241 = bitcast i32* %237 to <8 x i16>*
  store <8 x i16> %239, <8 x i16>* %241, align 16
  %242 = getelementptr inbounds i32, i32* %1, i64 12
  %243 = bitcast i32* %242 to <8 x i16>*
  store <8 x i16> %240, <8 x i16>* %243, align 16
  br label %374

244:                                              ; preds = %4
  %245 = bitcast i16* %0 to i64*
  %246 = load i64, i64* %245, align 1
  %247 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %246, i32 0
  %248 = sext i32 %2 to i64
  %249 = getelementptr inbounds i16, i16* %0, i64 %248
  %250 = bitcast i16* %249 to i64*
  %251 = load i64, i64* %250, align 1
  %252 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %251, i32 0
  %253 = shl nsw i32 %2, 1
  %254 = sext i32 %253 to i64
  %255 = getelementptr inbounds i16, i16* %0, i64 %254
  %256 = bitcast i16* %255 to i64*
  %257 = load i64, i64* %256, align 1
  %258 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %257, i32 0
  %259 = mul nsw i32 %2, 3
  %260 = sext i32 %259 to i64
  %261 = getelementptr inbounds i16, i16* %0, i64 %260
  %262 = bitcast i16* %261 to i64*
  %263 = load i64, i64* %262, align 1
  %264 = insertelement <2 x i64> <i64 undef, i64 0>, i64 %263, i32 0
  %265 = bitcast <2 x i64> %247 to <8 x i16>
  %266 = shl <8 x i16> %265, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %267 = bitcast <2 x i64> %252 to <8 x i16>
  %268 = shl <8 x i16> %267, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %269 = bitcast <2 x i64> %258 to <8 x i16>
  %270 = shl <8 x i16> %269, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %271 = bitcast <2 x i64> %264 to <8 x i16>
  %272 = shl <8 x i16> %271, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %273 = icmp eq <8 x i16> %266, <i16 0, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %274 = or <8 x i16> %266, <i16 1, i16 0, i16 0, i16 0, i16 0, i16 0, i16 0, i16 0>
  %275 = zext <8 x i1> %273 to <8 x i16>
  %276 = sub <8 x i16> %274, %275
  %277 = add <8 x i16> %276, %268
  %278 = shufflevector <8 x i16> %276, <8 x i16> %268, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %279 = shufflevector <8 x i16> %270, <8 x i16> %272, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %280 = shufflevector <8 x i16> %277, <8 x i16> <i16 0, i16 0, i16 0, i16 0, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %281 = shufflevector <8 x i16> %270, <8 x i16> <i16 0, i16 0, i16 0, i16 0, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %282 = shufflevector <8 x i16> %272, <8 x i16> <i16 0, i16 0, i16 0, i16 0, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %283 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %278, <8 x i16> <i16 5283, i16 9929, i16 5283, i16 9929, i16 5283, i16 9929, i16 5283, i16 9929>) #6
  %284 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %279, <8 x i16> <i16 13377, i16 15212, i16 13377, i16 15212, i16 13377, i16 15212, i16 13377, i16 15212>) #6
  %285 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %280, <8 x i16> <i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377>) #6
  %286 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %278, <8 x i16> <i16 15212, i16 -5283, i16 15212, i16 -5283, i16 15212, i16 -5283, i16 15212, i16 -5283>) #6
  %287 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %279, <8 x i16> <i16 -13377, i16 9929, i16 -13377, i16 9929, i16 -13377, i16 9929, i16 -13377, i16 9929>) #6
  %288 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %281, <8 x i16> <i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377>) #6
  %289 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %282, <8 x i16> <i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377>) #6
  %290 = add <4 x i32> %284, %283
  %291 = add <4 x i32> %287, %286
  %292 = mul <4 x i32> %288, <i32 3, i32 3, i32 3, i32 3>
  %293 = add <4 x i32> %290, <i32 8192, i32 8192, i32 8192, i32 8192>
  %294 = add <4 x i32> %285, <i32 8192, i32 8192, i32 8192, i32 8192>
  %295 = sub <4 x i32> %294, %289
  %296 = add <4 x i32> %291, <i32 8192, i32 8192, i32 8192, i32 8192>
  %297 = sub <4 x i32> <i32 8192, i32 8192, i32 8192, i32 8192>, %290
  %298 = add <4 x i32> %297, %291
  %299 = add <4 x i32> %298, %292
  %300 = ashr <4 x i32> %293, <i32 14, i32 14, i32 14, i32 14>
  %301 = ashr <4 x i32> %295, <i32 14, i32 14, i32 14, i32 14>
  %302 = ashr <4 x i32> %296, <i32 14, i32 14, i32 14, i32 14>
  %303 = ashr <4 x i32> %299, <i32 14, i32 14, i32 14, i32 14>
  %304 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %300, <4 x i32> %302) #6
  %305 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %301, <4 x i32> %303) #6
  %306 = shufflevector <8 x i16> %304, <8 x i16> %305, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %307 = shufflevector <8 x i16> %304, <8 x i16> %305, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %308 = bitcast <8 x i16> %306 to <4 x i32>
  %309 = bitcast <8 x i16> %307 to <4 x i32>
  %310 = shufflevector <4 x i32> %308, <4 x i32> %309, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %311 = bitcast <4 x i32> %310 to <2 x i64>
  %312 = shufflevector <4 x i32> %308, <4 x i32> %309, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %313 = bitcast <4 x i32> %312 to <2 x i64>
  %314 = shufflevector <2 x i64> %311, <2 x i64> undef, <2 x i32> <i32 1, i32 1>
  %315 = shufflevector <2 x i64> %313, <2 x i64> undef, <2 x i32> <i32 1, i32 1>
  %316 = bitcast <4 x i32> %310 to <8 x i16>
  %317 = bitcast <2 x i64> %314 to <8 x i16>
  %318 = add <8 x i16> %317, %316
  %319 = shufflevector <8 x i16> %316, <8 x i16> %317, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %320 = bitcast <4 x i32> %312 to <8 x i16>
  %321 = bitcast <2 x i64> %315 to <8 x i16>
  %322 = shufflevector <8 x i16> %320, <8 x i16> %321, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %323 = shufflevector <8 x i16> %318, <8 x i16> <i16 0, i16 0, i16 0, i16 0, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %324 = shufflevector <8 x i16> %320, <8 x i16> <i16 0, i16 0, i16 0, i16 0, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %325 = shufflevector <8 x i16> %321, <8 x i16> <i16 0, i16 0, i16 0, i16 0, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %326 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %319, <8 x i16> <i16 5283, i16 9929, i16 5283, i16 9929, i16 5283, i16 9929, i16 5283, i16 9929>) #6
  %327 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %322, <8 x i16> <i16 13377, i16 15212, i16 13377, i16 15212, i16 13377, i16 15212, i16 13377, i16 15212>) #6
  %328 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %323, <8 x i16> <i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377>) #6
  %329 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %319, <8 x i16> <i16 15212, i16 -5283, i16 15212, i16 -5283, i16 15212, i16 -5283, i16 15212, i16 -5283>) #6
  %330 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %322, <8 x i16> <i16 -13377, i16 9929, i16 -13377, i16 9929, i16 -13377, i16 9929, i16 -13377, i16 9929>) #6
  %331 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %324, <8 x i16> <i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377>) #6
  %332 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %325, <8 x i16> <i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377, i16 13377>) #6
  %333 = add <4 x i32> %327, %326
  %334 = add <4 x i32> %330, %329
  %335 = mul <4 x i32> %331, <i32 3, i32 3, i32 3, i32 3>
  %336 = add <4 x i32> %333, <i32 8192, i32 8192, i32 8192, i32 8192>
  %337 = add <4 x i32> %328, <i32 8192, i32 8192, i32 8192, i32 8192>
  %338 = sub <4 x i32> %337, %332
  %339 = add <4 x i32> %334, <i32 8192, i32 8192, i32 8192, i32 8192>
  %340 = sub <4 x i32> <i32 8192, i32 8192, i32 8192, i32 8192>, %333
  %341 = add <4 x i32> %340, %334
  %342 = add <4 x i32> %341, %335
  %343 = ashr <4 x i32> %336, <i32 14, i32 14, i32 14, i32 14>
  %344 = ashr <4 x i32> %338, <i32 14, i32 14, i32 14, i32 14>
  %345 = ashr <4 x i32> %339, <i32 14, i32 14, i32 14, i32 14>
  %346 = ashr <4 x i32> %342, <i32 14, i32 14, i32 14, i32 14>
  %347 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %343, <4 x i32> %345) #6
  %348 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %344, <4 x i32> %346) #6
  %349 = shufflevector <8 x i16> %347, <8 x i16> %348, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %350 = shufflevector <8 x i16> %347, <8 x i16> %348, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %351 = bitcast <8 x i16> %349 to <4 x i32>
  %352 = bitcast <8 x i16> %350 to <4 x i32>
  %353 = shufflevector <4 x i32> %351, <4 x i32> %352, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %354 = shufflevector <4 x i32> %351, <4 x i32> %352, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %355 = bitcast <4 x i32> %353 to <8 x i16>
  %356 = add <8 x i16> %355, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %357 = bitcast <4 x i32> %354 to <8 x i16>
  %358 = add <8 x i16> %357, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %359 = ashr <8 x i16> %356, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %360 = ashr <8 x i16> %358, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %361 = ashr <8 x i16> %356, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %362 = shufflevector <8 x i16> %359, <8 x i16> %361, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %363 = shufflevector <8 x i16> %359, <8 x i16> %361, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %364 = bitcast i32* %1 to <8 x i16>*
  store <8 x i16> %362, <8 x i16>* %364, align 16
  %365 = getelementptr inbounds i32, i32* %1, i64 4
  %366 = bitcast i32* %365 to <8 x i16>*
  store <8 x i16> %363, <8 x i16>* %366, align 16
  %367 = getelementptr inbounds i32, i32* %1, i64 8
  %368 = ashr <8 x i16> %358, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %369 = shufflevector <8 x i16> %360, <8 x i16> %368, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %370 = shufflevector <8 x i16> %360, <8 x i16> %368, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %371 = bitcast i32* %367 to <8 x i16>*
  store <8 x i16> %369, <8 x i16>* %371, align 16
  %372 = getelementptr inbounds i32, i32* %1, i64 12
  %373 = bitcast i32* %372 to <8 x i16>*
  store <8 x i16> %370, <8 x i16>* %373, align 16
  br label %374

374:                                              ; preds = %244, %125, %6, %5
  ret void
}

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #1

; Function Attrs: argmemonly nounwind
declare void @llvm.memset.p0i8.i64(i8* nocapture writeonly, i8, i64, i1 immarg) #1

declare void @vpx_fdct4x4_sse2(i16*, i32*, i32) local_unnamed_addr #2

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #1

; Function Attrs: nounwind ssp uwtable
define hidden void @vp9_fht8x8_sse2(i16*, i32*, i32, i32) local_unnamed_addr #0 {
  %5 = alloca [8 x <2 x i64>], align 16
  %6 = bitcast [8 x <2 x i64>]* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 128, i8* nonnull %6) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %6, i8 -86, i64 128, i1 false)
  switch i32 %3, label %584 [
    i32 0, label %7
    i32 1, label %8
    i32 2, label %300
  ]

7:                                                ; preds = %4
  tail call void @vpx_fdct8x8_sse2(i16* %0, i32* %1, i32 %2) #6
  br label %732

8:                                                ; preds = %4
  %9 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %5, i64 0, i64 0
  %10 = bitcast i16* %0 to <8 x i16>*
  %11 = load <8 x i16>, <8 x i16>* %10, align 16
  %12 = sext i32 %2 to i64
  %13 = getelementptr inbounds i16, i16* %0, i64 %12
  %14 = bitcast i16* %13 to <8 x i16>*
  %15 = load <8 x i16>, <8 x i16>* %14, align 16
  %16 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %5, i64 0, i64 1
  %17 = shl nsw i32 %2, 1
  %18 = sext i32 %17 to i64
  %19 = getelementptr inbounds i16, i16* %0, i64 %18
  %20 = bitcast i16* %19 to <8 x i16>*
  %21 = load <8 x i16>, <8 x i16>* %20, align 16
  %22 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %5, i64 0, i64 2
  %23 = mul nsw i32 %2, 3
  %24 = sext i32 %23 to i64
  %25 = getelementptr inbounds i16, i16* %0, i64 %24
  %26 = bitcast i16* %25 to <8 x i16>*
  %27 = load <8 x i16>, <8 x i16>* %26, align 16
  %28 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %5, i64 0, i64 3
  %29 = shl nsw i32 %2, 2
  %30 = sext i32 %29 to i64
  %31 = getelementptr inbounds i16, i16* %0, i64 %30
  %32 = bitcast i16* %31 to <8 x i16>*
  %33 = load <8 x i16>, <8 x i16>* %32, align 16
  %34 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %5, i64 0, i64 4
  %35 = mul nsw i32 %2, 5
  %36 = sext i32 %35 to i64
  %37 = getelementptr inbounds i16, i16* %0, i64 %36
  %38 = bitcast i16* %37 to <8 x i16>*
  %39 = load <8 x i16>, <8 x i16>* %38, align 16
  %40 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %5, i64 0, i64 5
  %41 = mul nsw i32 %2, 6
  %42 = sext i32 %41 to i64
  %43 = getelementptr inbounds i16, i16* %0, i64 %42
  %44 = bitcast i16* %43 to <8 x i16>*
  %45 = load <8 x i16>, <8 x i16>* %44, align 16
  %46 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %5, i64 0, i64 6
  %47 = mul nsw i32 %2, 7
  %48 = sext i32 %47 to i64
  %49 = getelementptr inbounds i16, i16* %0, i64 %48
  %50 = bitcast i16* %49 to <8 x i16>*
  %51 = load <8 x i16>, <8 x i16>* %50, align 16
  %52 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %5, i64 0, i64 7
  %53 = shl <8 x i16> %11, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %54 = bitcast [8 x <2 x i64>]* %5 to <8 x i16>*
  store <8 x i16> %53, <8 x i16>* %54, align 16
  %55 = shl <8 x i16> %15, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %56 = bitcast <2 x i64>* %16 to <8 x i16>*
  store <8 x i16> %55, <8 x i16>* %56, align 16
  %57 = shl <8 x i16> %21, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %58 = bitcast <2 x i64>* %22 to <8 x i16>*
  store <8 x i16> %57, <8 x i16>* %58, align 16
  %59 = shl <8 x i16> %27, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %60 = bitcast <2 x i64>* %28 to <8 x i16>*
  store <8 x i16> %59, <8 x i16>* %60, align 16
  %61 = shl <8 x i16> %33, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %62 = bitcast <2 x i64>* %34 to <8 x i16>*
  store <8 x i16> %61, <8 x i16>* %62, align 16
  %63 = shl <8 x i16> %39, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %64 = bitcast <2 x i64>* %40 to <8 x i16>*
  store <8 x i16> %63, <8 x i16>* %64, align 16
  %65 = shl <8 x i16> %45, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %66 = bitcast <2 x i64>* %46 to <8 x i16>*
  store <8 x i16> %65, <8 x i16>* %66, align 16
  %67 = shl <8 x i16> %51, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %68 = bitcast <2 x i64>* %52 to <8 x i16>*
  store <8 x i16> %67, <8 x i16>* %68, align 16
  call fastcc void @fadst8_sse2(<2 x i64>* nonnull %9)
  %69 = load <8 x i16>, <8 x i16>* %54, align 16
  %70 = load <8 x i16>, <8 x i16>* %68, align 16
  %71 = add <8 x i16> %70, %69
  %72 = load <8 x i16>, <8 x i16>* %56, align 16
  %73 = load <8 x i16>, <8 x i16>* %66, align 16
  %74 = add <8 x i16> %73, %72
  %75 = load <8 x i16>, <8 x i16>* %58, align 16
  %76 = load <8 x i16>, <8 x i16>* %64, align 16
  %77 = add <8 x i16> %76, %75
  %78 = load <8 x i16>, <8 x i16>* %60, align 16
  %79 = load <8 x i16>, <8 x i16>* %62, align 16
  %80 = add <8 x i16> %79, %78
  %81 = sub <8 x i16> %78, %79
  %82 = sub <8 x i16> %75, %76
  %83 = sub <8 x i16> %72, %73
  %84 = sub <8 x i16> %69, %70
  %85 = add <8 x i16> %80, %71
  %86 = add <8 x i16> %77, %74
  %87 = sub <8 x i16> %74, %77
  %88 = sub <8 x i16> %71, %80
  %89 = shufflevector <8 x i16> %85, <8 x i16> %86, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %90 = shufflevector <8 x i16> %85, <8 x i16> %86, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %91 = shufflevector <8 x i16> %87, <8 x i16> %88, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %92 = shufflevector <8 x i16> %87, <8 x i16> %88, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %93 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %89, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %94 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %90, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %95 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %89, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %96 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %90, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %97 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %91, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %98 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %92, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %99 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %91, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %100 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %92, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %101 = add <4 x i32> %93, <i32 8192, i32 8192, i32 8192, i32 8192>
  %102 = add <4 x i32> %94, <i32 8192, i32 8192, i32 8192, i32 8192>
  %103 = add <4 x i32> %95, <i32 8192, i32 8192, i32 8192, i32 8192>
  %104 = add <4 x i32> %96, <i32 8192, i32 8192, i32 8192, i32 8192>
  %105 = add <4 x i32> %97, <i32 8192, i32 8192, i32 8192, i32 8192>
  %106 = add <4 x i32> %98, <i32 8192, i32 8192, i32 8192, i32 8192>
  %107 = add <4 x i32> %99, <i32 8192, i32 8192, i32 8192, i32 8192>
  %108 = add <4 x i32> %100, <i32 8192, i32 8192, i32 8192, i32 8192>
  %109 = ashr <4 x i32> %101, <i32 14, i32 14, i32 14, i32 14>
  %110 = ashr <4 x i32> %102, <i32 14, i32 14, i32 14, i32 14>
  %111 = ashr <4 x i32> %103, <i32 14, i32 14, i32 14, i32 14>
  %112 = ashr <4 x i32> %104, <i32 14, i32 14, i32 14, i32 14>
  %113 = ashr <4 x i32> %105, <i32 14, i32 14, i32 14, i32 14>
  %114 = ashr <4 x i32> %106, <i32 14, i32 14, i32 14, i32 14>
  %115 = ashr <4 x i32> %107, <i32 14, i32 14, i32 14, i32 14>
  %116 = ashr <4 x i32> %108, <i32 14, i32 14, i32 14, i32 14>
  %117 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %109, <4 x i32> %110) #6
  %118 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %113, <4 x i32> %114) #6
  %119 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %111, <4 x i32> %112) #6
  %120 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %115, <4 x i32> %116) #6
  %121 = shufflevector <8 x i16> %83, <8 x i16> %82, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %122 = shufflevector <8 x i16> %83, <8 x i16> %82, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %123 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %121, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %124 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %122, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %125 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %121, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %126 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %122, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %127 = add <4 x i32> %123, <i32 8192, i32 8192, i32 8192, i32 8192>
  %128 = add <4 x i32> %124, <i32 8192, i32 8192, i32 8192, i32 8192>
  %129 = add <4 x i32> %125, <i32 8192, i32 8192, i32 8192, i32 8192>
  %130 = add <4 x i32> %126, <i32 8192, i32 8192, i32 8192, i32 8192>
  %131 = ashr <4 x i32> %127, <i32 14, i32 14, i32 14, i32 14>
  %132 = ashr <4 x i32> %128, <i32 14, i32 14, i32 14, i32 14>
  %133 = ashr <4 x i32> %129, <i32 14, i32 14, i32 14, i32 14>
  %134 = ashr <4 x i32> %130, <i32 14, i32 14, i32 14, i32 14>
  %135 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %131, <4 x i32> %132) #6
  %136 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %133, <4 x i32> %134) #6
  %137 = add <8 x i16> %135, %81
  %138 = sub <8 x i16> %81, %135
  %139 = sub <8 x i16> %84, %136
  %140 = add <8 x i16> %136, %84
  %141 = shufflevector <8 x i16> %137, <8 x i16> %140, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %142 = shufflevector <8 x i16> %137, <8 x i16> %140, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %143 = shufflevector <8 x i16> %138, <8 x i16> %139, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %144 = shufflevector <8 x i16> %138, <8 x i16> %139, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %145 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %141, <8 x i16> <i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069>) #6
  %146 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %142, <8 x i16> <i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069>) #6
  %147 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %143, <8 x i16> <i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102>) #6
  %148 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %144, <8 x i16> <i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102>) #6
  %149 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %143, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %150 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %144, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %151 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %141, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %152 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %142, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %153 = add <4 x i32> %145, <i32 8192, i32 8192, i32 8192, i32 8192>
  %154 = add <4 x i32> %146, <i32 8192, i32 8192, i32 8192, i32 8192>
  %155 = add <4 x i32> %147, <i32 8192, i32 8192, i32 8192, i32 8192>
  %156 = add <4 x i32> %148, <i32 8192, i32 8192, i32 8192, i32 8192>
  %157 = add <4 x i32> %149, <i32 8192, i32 8192, i32 8192, i32 8192>
  %158 = add <4 x i32> %150, <i32 8192, i32 8192, i32 8192, i32 8192>
  %159 = add <4 x i32> %151, <i32 8192, i32 8192, i32 8192, i32 8192>
  %160 = add <4 x i32> %152, <i32 8192, i32 8192, i32 8192, i32 8192>
  %161 = ashr <4 x i32> %153, <i32 14, i32 14, i32 14, i32 14>
  %162 = ashr <4 x i32> %154, <i32 14, i32 14, i32 14, i32 14>
  %163 = ashr <4 x i32> %155, <i32 14, i32 14, i32 14, i32 14>
  %164 = ashr <4 x i32> %156, <i32 14, i32 14, i32 14, i32 14>
  %165 = ashr <4 x i32> %157, <i32 14, i32 14, i32 14, i32 14>
  %166 = ashr <4 x i32> %158, <i32 14, i32 14, i32 14, i32 14>
  %167 = ashr <4 x i32> %159, <i32 14, i32 14, i32 14, i32 14>
  %168 = ashr <4 x i32> %160, <i32 14, i32 14, i32 14, i32 14>
  %169 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %161, <4 x i32> %162) #6
  %170 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %165, <4 x i32> %166) #6
  %171 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %163, <4 x i32> %164) #6
  %172 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %167, <4 x i32> %168) #6
  %173 = shufflevector <8 x i16> %117, <8 x i16> %169, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %174 = shufflevector <8 x i16> %118, <8 x i16> %170, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %175 = shufflevector <8 x i16> %119, <8 x i16> %171, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %176 = shufflevector <8 x i16> %120, <8 x i16> %172, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %177 = shufflevector <8 x i16> %117, <8 x i16> %169, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %178 = shufflevector <8 x i16> %118, <8 x i16> %170, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %179 = shufflevector <8 x i16> %119, <8 x i16> %171, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %180 = shufflevector <8 x i16> %120, <8 x i16> %172, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %181 = bitcast <8 x i16> %173 to <4 x i32>
  %182 = bitcast <8 x i16> %174 to <4 x i32>
  %183 = shufflevector <4 x i32> %181, <4 x i32> %182, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %184 = bitcast <4 x i32> %183 to <2 x i64>
  %185 = bitcast <8 x i16> %175 to <4 x i32>
  %186 = bitcast <8 x i16> %176 to <4 x i32>
  %187 = shufflevector <4 x i32> %185, <4 x i32> %186, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %188 = bitcast <4 x i32> %187 to <2 x i64>
  %189 = bitcast <8 x i16> %177 to <4 x i32>
  %190 = bitcast <8 x i16> %178 to <4 x i32>
  %191 = shufflevector <4 x i32> %189, <4 x i32> %190, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %192 = bitcast <4 x i32> %191 to <2 x i64>
  %193 = bitcast <8 x i16> %179 to <4 x i32>
  %194 = bitcast <8 x i16> %180 to <4 x i32>
  %195 = shufflevector <4 x i32> %193, <4 x i32> %194, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %196 = bitcast <4 x i32> %195 to <2 x i64>
  %197 = shufflevector <4 x i32> %181, <4 x i32> %182, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %198 = bitcast <4 x i32> %197 to <2 x i64>
  %199 = shufflevector <4 x i32> %185, <4 x i32> %186, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %200 = bitcast <4 x i32> %199 to <2 x i64>
  %201 = shufflevector <4 x i32> %189, <4 x i32> %190, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %202 = bitcast <4 x i32> %201 to <2 x i64>
  %203 = shufflevector <4 x i32> %193, <4 x i32> %194, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %204 = bitcast <4 x i32> %203 to <2 x i64>
  %205 = shufflevector <2 x i64> %184, <2 x i64> %188, <2 x i32> <i32 0, i32 2>
  %206 = shufflevector <2 x i64> %184, <2 x i64> %188, <2 x i32> <i32 1, i32 3>
  %207 = shufflevector <2 x i64> %198, <2 x i64> %200, <2 x i32> <i32 0, i32 2>
  %208 = shufflevector <2 x i64> %198, <2 x i64> %200, <2 x i32> <i32 1, i32 3>
  %209 = shufflevector <2 x i64> %192, <2 x i64> %196, <2 x i32> <i32 0, i32 2>
  %210 = shufflevector <2 x i64> %192, <2 x i64> %196, <2 x i32> <i32 1, i32 3>
  %211 = shufflevector <2 x i64> %202, <2 x i64> %204, <2 x i32> <i32 0, i32 2>
  %212 = shufflevector <2 x i64> %202, <2 x i64> %204, <2 x i32> <i32 1, i32 3>
  %213 = bitcast <2 x i64> %205 to <8 x i16>
  %214 = ashr <8 x i16> %213, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %215 = bitcast <2 x i64> %206 to <8 x i16>
  %216 = ashr <8 x i16> %215, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %217 = bitcast <2 x i64> %207 to <8 x i16>
  %218 = ashr <8 x i16> %217, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %219 = bitcast <2 x i64> %208 to <8 x i16>
  %220 = ashr <8 x i16> %219, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %221 = bitcast <2 x i64> %209 to <8 x i16>
  %222 = ashr <8 x i16> %221, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %223 = bitcast <2 x i64> %210 to <8 x i16>
  %224 = ashr <8 x i16> %223, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %225 = bitcast <2 x i64> %211 to <8 x i16>
  %226 = ashr <8 x i16> %225, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %227 = bitcast <2 x i64> %212 to <8 x i16>
  %228 = ashr <8 x i16> %227, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %229 = sub <8 x i16> %213, %214
  %230 = sub <8 x i16> %215, %216
  %231 = sub <8 x i16> %217, %218
  %232 = sub <8 x i16> %219, %220
  %233 = sub <8 x i16> %221, %222
  %234 = sub <8 x i16> %223, %224
  %235 = sub <8 x i16> %225, %226
  %236 = sub <8 x i16> %227, %228
  %237 = ashr <8 x i16> %229, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  store <8 x i16> %237, <8 x i16>* %54, align 16
  %238 = ashr <8 x i16> %230, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  store <8 x i16> %238, <8 x i16>* %56, align 16
  %239 = ashr <8 x i16> %231, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  store <8 x i16> %239, <8 x i16>* %58, align 16
  %240 = ashr <8 x i16> %232, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  store <8 x i16> %240, <8 x i16>* %60, align 16
  %241 = ashr <8 x i16> %233, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  store <8 x i16> %241, <8 x i16>* %62, align 16
  %242 = ashr <8 x i16> %234, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  store <8 x i16> %242, <8 x i16>* %64, align 16
  %243 = ashr <8 x i16> %235, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  store <8 x i16> %243, <8 x i16>* %66, align 16
  %244 = ashr <8 x i16> %236, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  store <8 x i16> %244, <8 x i16>* %68, align 16
  %245 = ashr <8 x i16> %229, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %246 = shufflevector <8 x i16> %237, <8 x i16> %245, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %247 = shufflevector <8 x i16> %237, <8 x i16> %245, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %248 = bitcast i32* %1 to <8 x i16>*
  store <8 x i16> %246, <8 x i16>* %248, align 16
  %249 = getelementptr inbounds i32, i32* %1, i64 4
  %250 = bitcast i32* %249 to <8 x i16>*
  store <8 x i16> %247, <8 x i16>* %250, align 16
  %251 = getelementptr inbounds i32, i32* %1, i64 8
  %252 = ashr <8 x i16> %230, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %253 = shufflevector <8 x i16> %238, <8 x i16> %252, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %254 = shufflevector <8 x i16> %238, <8 x i16> %252, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %255 = bitcast i32* %251 to <8 x i16>*
  store <8 x i16> %253, <8 x i16>* %255, align 16
  %256 = getelementptr inbounds i32, i32* %1, i64 12
  %257 = bitcast i32* %256 to <8 x i16>*
  store <8 x i16> %254, <8 x i16>* %257, align 16
  %258 = getelementptr inbounds i32, i32* %1, i64 16
  %259 = ashr <8 x i16> %231, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %260 = shufflevector <8 x i16> %239, <8 x i16> %259, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %261 = shufflevector <8 x i16> %239, <8 x i16> %259, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %262 = bitcast i32* %258 to <8 x i16>*
  store <8 x i16> %260, <8 x i16>* %262, align 16
  %263 = getelementptr inbounds i32, i32* %1, i64 20
  %264 = bitcast i32* %263 to <8 x i16>*
  store <8 x i16> %261, <8 x i16>* %264, align 16
  %265 = getelementptr inbounds i32, i32* %1, i64 24
  %266 = ashr <8 x i16> %232, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %267 = shufflevector <8 x i16> %240, <8 x i16> %266, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %268 = shufflevector <8 x i16> %240, <8 x i16> %266, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %269 = bitcast i32* %265 to <8 x i16>*
  store <8 x i16> %267, <8 x i16>* %269, align 16
  %270 = getelementptr inbounds i32, i32* %1, i64 28
  %271 = bitcast i32* %270 to <8 x i16>*
  store <8 x i16> %268, <8 x i16>* %271, align 16
  %272 = getelementptr inbounds i32, i32* %1, i64 32
  %273 = ashr <8 x i16> %233, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %274 = shufflevector <8 x i16> %241, <8 x i16> %273, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %275 = shufflevector <8 x i16> %241, <8 x i16> %273, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %276 = bitcast i32* %272 to <8 x i16>*
  store <8 x i16> %274, <8 x i16>* %276, align 16
  %277 = getelementptr inbounds i32, i32* %1, i64 36
  %278 = bitcast i32* %277 to <8 x i16>*
  store <8 x i16> %275, <8 x i16>* %278, align 16
  %279 = getelementptr inbounds i32, i32* %1, i64 40
  %280 = ashr <8 x i16> %234, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %281 = shufflevector <8 x i16> %242, <8 x i16> %280, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %282 = shufflevector <8 x i16> %242, <8 x i16> %280, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %283 = bitcast i32* %279 to <8 x i16>*
  store <8 x i16> %281, <8 x i16>* %283, align 16
  %284 = getelementptr inbounds i32, i32* %1, i64 44
  %285 = bitcast i32* %284 to <8 x i16>*
  store <8 x i16> %282, <8 x i16>* %285, align 16
  %286 = getelementptr inbounds i32, i32* %1, i64 48
  %287 = ashr <8 x i16> %235, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %288 = shufflevector <8 x i16> %243, <8 x i16> %287, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %289 = shufflevector <8 x i16> %243, <8 x i16> %287, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %290 = bitcast i32* %286 to <8 x i16>*
  store <8 x i16> %288, <8 x i16>* %290, align 16
  %291 = getelementptr inbounds i32, i32* %1, i64 52
  %292 = bitcast i32* %291 to <8 x i16>*
  store <8 x i16> %289, <8 x i16>* %292, align 16
  %293 = getelementptr inbounds i32, i32* %1, i64 56
  %294 = ashr <8 x i16> %236, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %295 = shufflevector <8 x i16> %244, <8 x i16> %294, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %296 = shufflevector <8 x i16> %244, <8 x i16> %294, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %297 = bitcast i32* %293 to <8 x i16>*
  store <8 x i16> %295, <8 x i16>* %297, align 16
  %298 = getelementptr inbounds i32, i32* %1, i64 60
  %299 = bitcast i32* %298 to <8 x i16>*
  store <8 x i16> %296, <8 x i16>* %299, align 16
  br label %732

300:                                              ; preds = %4
  %301 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %5, i64 0, i64 0
  %302 = bitcast i16* %0 to <8 x i16>*
  %303 = load <8 x i16>, <8 x i16>* %302, align 16
  %304 = sext i32 %2 to i64
  %305 = getelementptr inbounds i16, i16* %0, i64 %304
  %306 = bitcast i16* %305 to <8 x i16>*
  %307 = load <8 x i16>, <8 x i16>* %306, align 16
  %308 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %5, i64 0, i64 1
  %309 = shl nsw i32 %2, 1
  %310 = sext i32 %309 to i64
  %311 = getelementptr inbounds i16, i16* %0, i64 %310
  %312 = bitcast i16* %311 to <8 x i16>*
  %313 = load <8 x i16>, <8 x i16>* %312, align 16
  %314 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %5, i64 0, i64 2
  %315 = mul nsw i32 %2, 3
  %316 = sext i32 %315 to i64
  %317 = getelementptr inbounds i16, i16* %0, i64 %316
  %318 = bitcast i16* %317 to <8 x i16>*
  %319 = load <8 x i16>, <8 x i16>* %318, align 16
  %320 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %5, i64 0, i64 3
  %321 = shl nsw i32 %2, 2
  %322 = sext i32 %321 to i64
  %323 = getelementptr inbounds i16, i16* %0, i64 %322
  %324 = bitcast i16* %323 to <8 x i16>*
  %325 = load <8 x i16>, <8 x i16>* %324, align 16
  %326 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %5, i64 0, i64 4
  %327 = mul nsw i32 %2, 5
  %328 = sext i32 %327 to i64
  %329 = getelementptr inbounds i16, i16* %0, i64 %328
  %330 = bitcast i16* %329 to <8 x i16>*
  %331 = load <8 x i16>, <8 x i16>* %330, align 16
  %332 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %5, i64 0, i64 5
  %333 = mul nsw i32 %2, 6
  %334 = sext i32 %333 to i64
  %335 = getelementptr inbounds i16, i16* %0, i64 %334
  %336 = bitcast i16* %335 to <8 x i16>*
  %337 = load <8 x i16>, <8 x i16>* %336, align 16
  %338 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %5, i64 0, i64 6
  %339 = mul nsw i32 %2, 7
  %340 = sext i32 %339 to i64
  %341 = getelementptr inbounds i16, i16* %0, i64 %340
  %342 = bitcast i16* %341 to <8 x i16>*
  %343 = load <8 x i16>, <8 x i16>* %342, align 16
  %344 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %5, i64 0, i64 7
  %345 = shl <8 x i16> %303, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %346 = bitcast [8 x <2 x i64>]* %5 to <8 x i16>*
  store <8 x i16> %345, <8 x i16>* %346, align 16
  %347 = shl <8 x i16> %307, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %348 = bitcast <2 x i64>* %308 to <8 x i16>*
  store <8 x i16> %347, <8 x i16>* %348, align 16
  %349 = shl <8 x i16> %313, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %350 = bitcast <2 x i64>* %314 to <8 x i16>*
  store <8 x i16> %349, <8 x i16>* %350, align 16
  %351 = shl <8 x i16> %319, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %352 = bitcast <2 x i64>* %320 to <8 x i16>*
  store <8 x i16> %351, <8 x i16>* %352, align 16
  %353 = shl <8 x i16> %325, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %354 = bitcast <2 x i64>* %326 to <8 x i16>*
  store <8 x i16> %353, <8 x i16>* %354, align 16
  %355 = shl <8 x i16> %331, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %356 = bitcast <2 x i64>* %332 to <8 x i16>*
  store <8 x i16> %355, <8 x i16>* %356, align 16
  %357 = shl <8 x i16> %337, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %358 = bitcast <2 x i64>* %338 to <8 x i16>*
  store <8 x i16> %357, <8 x i16>* %358, align 16
  %359 = shl <8 x i16> %343, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %360 = bitcast <2 x i64>* %344 to <8 x i16>*
  store <8 x i16> %359, <8 x i16>* %360, align 16
  %361 = add <8 x i16> %359, %345
  %362 = add <8 x i16> %357, %347
  %363 = add <8 x i16> %355, %349
  %364 = add <8 x i16> %353, %351
  %365 = sub <8 x i16> %351, %353
  %366 = sub <8 x i16> %349, %355
  %367 = sub <8 x i16> %347, %357
  %368 = sub <8 x i16> %345, %359
  %369 = add <8 x i16> %361, %364
  %370 = add <8 x i16> %362, %363
  %371 = sub <8 x i16> %362, %363
  %372 = sub <8 x i16> %361, %364
  %373 = shufflevector <8 x i16> %369, <8 x i16> %370, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %374 = shufflevector <8 x i16> %369, <8 x i16> %370, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %375 = shufflevector <8 x i16> %371, <8 x i16> %372, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %376 = shufflevector <8 x i16> %371, <8 x i16> %372, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %377 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %373, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %378 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %374, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %379 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %373, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %380 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %374, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %381 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %375, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %382 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %376, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %383 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %375, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %384 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %376, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %385 = add <4 x i32> %377, <i32 8192, i32 8192, i32 8192, i32 8192>
  %386 = add <4 x i32> %378, <i32 8192, i32 8192, i32 8192, i32 8192>
  %387 = add <4 x i32> %379, <i32 8192, i32 8192, i32 8192, i32 8192>
  %388 = add <4 x i32> %380, <i32 8192, i32 8192, i32 8192, i32 8192>
  %389 = add <4 x i32> %381, <i32 8192, i32 8192, i32 8192, i32 8192>
  %390 = add <4 x i32> %382, <i32 8192, i32 8192, i32 8192, i32 8192>
  %391 = add <4 x i32> %383, <i32 8192, i32 8192, i32 8192, i32 8192>
  %392 = add <4 x i32> %384, <i32 8192, i32 8192, i32 8192, i32 8192>
  %393 = ashr <4 x i32> %385, <i32 14, i32 14, i32 14, i32 14>
  %394 = ashr <4 x i32> %386, <i32 14, i32 14, i32 14, i32 14>
  %395 = ashr <4 x i32> %387, <i32 14, i32 14, i32 14, i32 14>
  %396 = ashr <4 x i32> %388, <i32 14, i32 14, i32 14, i32 14>
  %397 = ashr <4 x i32> %389, <i32 14, i32 14, i32 14, i32 14>
  %398 = ashr <4 x i32> %390, <i32 14, i32 14, i32 14, i32 14>
  %399 = ashr <4 x i32> %391, <i32 14, i32 14, i32 14, i32 14>
  %400 = ashr <4 x i32> %392, <i32 14, i32 14, i32 14, i32 14>
  %401 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %393, <4 x i32> %394) #6
  %402 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %397, <4 x i32> %398) #6
  %403 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %395, <4 x i32> %396) #6
  %404 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %399, <4 x i32> %400) #6
  %405 = shufflevector <8 x i16> %367, <8 x i16> %366, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %406 = shufflevector <8 x i16> %367, <8 x i16> %366, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %407 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %405, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %408 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %406, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %409 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %405, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %410 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %406, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %411 = add <4 x i32> %407, <i32 8192, i32 8192, i32 8192, i32 8192>
  %412 = add <4 x i32> %408, <i32 8192, i32 8192, i32 8192, i32 8192>
  %413 = add <4 x i32> %409, <i32 8192, i32 8192, i32 8192, i32 8192>
  %414 = add <4 x i32> %410, <i32 8192, i32 8192, i32 8192, i32 8192>
  %415 = ashr <4 x i32> %411, <i32 14, i32 14, i32 14, i32 14>
  %416 = ashr <4 x i32> %412, <i32 14, i32 14, i32 14, i32 14>
  %417 = ashr <4 x i32> %413, <i32 14, i32 14, i32 14, i32 14>
  %418 = ashr <4 x i32> %414, <i32 14, i32 14, i32 14, i32 14>
  %419 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %415, <4 x i32> %416) #6
  %420 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %417, <4 x i32> %418) #6
  %421 = add <8 x i16> %419, %365
  %422 = sub <8 x i16> %365, %419
  %423 = sub <8 x i16> %368, %420
  %424 = add <8 x i16> %420, %368
  %425 = shufflevector <8 x i16> %421, <8 x i16> %424, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %426 = shufflevector <8 x i16> %421, <8 x i16> %424, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %427 = shufflevector <8 x i16> %422, <8 x i16> %423, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %428 = shufflevector <8 x i16> %422, <8 x i16> %423, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %429 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %425, <8 x i16> <i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069>) #6
  %430 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %426, <8 x i16> <i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069>) #6
  %431 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %427, <8 x i16> <i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102>) #6
  %432 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %428, <8 x i16> <i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102>) #6
  %433 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %427, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %434 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %428, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %435 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %425, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %436 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %426, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %437 = add <4 x i32> %429, <i32 8192, i32 8192, i32 8192, i32 8192>
  %438 = add <4 x i32> %430, <i32 8192, i32 8192, i32 8192, i32 8192>
  %439 = add <4 x i32> %431, <i32 8192, i32 8192, i32 8192, i32 8192>
  %440 = add <4 x i32> %432, <i32 8192, i32 8192, i32 8192, i32 8192>
  %441 = add <4 x i32> %433, <i32 8192, i32 8192, i32 8192, i32 8192>
  %442 = add <4 x i32> %434, <i32 8192, i32 8192, i32 8192, i32 8192>
  %443 = add <4 x i32> %435, <i32 8192, i32 8192, i32 8192, i32 8192>
  %444 = add <4 x i32> %436, <i32 8192, i32 8192, i32 8192, i32 8192>
  %445 = ashr <4 x i32> %437, <i32 14, i32 14, i32 14, i32 14>
  %446 = ashr <4 x i32> %438, <i32 14, i32 14, i32 14, i32 14>
  %447 = ashr <4 x i32> %439, <i32 14, i32 14, i32 14, i32 14>
  %448 = ashr <4 x i32> %440, <i32 14, i32 14, i32 14, i32 14>
  %449 = ashr <4 x i32> %441, <i32 14, i32 14, i32 14, i32 14>
  %450 = ashr <4 x i32> %442, <i32 14, i32 14, i32 14, i32 14>
  %451 = ashr <4 x i32> %443, <i32 14, i32 14, i32 14, i32 14>
  %452 = ashr <4 x i32> %444, <i32 14, i32 14, i32 14, i32 14>
  %453 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %445, <4 x i32> %446) #6
  %454 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %449, <4 x i32> %450) #6
  %455 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %447, <4 x i32> %448) #6
  %456 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %451, <4 x i32> %452) #6
  %457 = shufflevector <8 x i16> %401, <8 x i16> %453, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %458 = shufflevector <8 x i16> %402, <8 x i16> %454, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %459 = shufflevector <8 x i16> %403, <8 x i16> %455, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %460 = shufflevector <8 x i16> %404, <8 x i16> %456, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %461 = shufflevector <8 x i16> %401, <8 x i16> %453, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %462 = shufflevector <8 x i16> %402, <8 x i16> %454, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %463 = shufflevector <8 x i16> %403, <8 x i16> %455, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %464 = shufflevector <8 x i16> %404, <8 x i16> %456, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %465 = bitcast <8 x i16> %457 to <4 x i32>
  %466 = bitcast <8 x i16> %458 to <4 x i32>
  %467 = shufflevector <4 x i32> %465, <4 x i32> %466, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %468 = bitcast <4 x i32> %467 to <2 x i64>
  %469 = bitcast <8 x i16> %459 to <4 x i32>
  %470 = bitcast <8 x i16> %460 to <4 x i32>
  %471 = shufflevector <4 x i32> %469, <4 x i32> %470, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %472 = bitcast <4 x i32> %471 to <2 x i64>
  %473 = bitcast <8 x i16> %461 to <4 x i32>
  %474 = bitcast <8 x i16> %462 to <4 x i32>
  %475 = shufflevector <4 x i32> %473, <4 x i32> %474, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %476 = bitcast <4 x i32> %475 to <2 x i64>
  %477 = bitcast <8 x i16> %463 to <4 x i32>
  %478 = bitcast <8 x i16> %464 to <4 x i32>
  %479 = shufflevector <4 x i32> %477, <4 x i32> %478, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %480 = bitcast <4 x i32> %479 to <2 x i64>
  %481 = shufflevector <4 x i32> %465, <4 x i32> %466, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %482 = bitcast <4 x i32> %481 to <2 x i64>
  %483 = shufflevector <4 x i32> %469, <4 x i32> %470, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %484 = bitcast <4 x i32> %483 to <2 x i64>
  %485 = shufflevector <4 x i32> %473, <4 x i32> %474, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %486 = bitcast <4 x i32> %485 to <2 x i64>
  %487 = shufflevector <4 x i32> %477, <4 x i32> %478, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %488 = bitcast <4 x i32> %487 to <2 x i64>
  %489 = shufflevector <2 x i64> %468, <2 x i64> %472, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %489, <2 x i64>* %301, align 16
  %490 = shufflevector <2 x i64> %468, <2 x i64> %472, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %490, <2 x i64>* %308, align 16
  %491 = shufflevector <2 x i64> %482, <2 x i64> %484, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %491, <2 x i64>* %314, align 16
  %492 = shufflevector <2 x i64> %482, <2 x i64> %484, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %492, <2 x i64>* %320, align 16
  %493 = shufflevector <2 x i64> %476, <2 x i64> %480, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %493, <2 x i64>* %326, align 16
  %494 = shufflevector <2 x i64> %476, <2 x i64> %480, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %494, <2 x i64>* %332, align 16
  %495 = shufflevector <2 x i64> %486, <2 x i64> %488, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %495, <2 x i64>* %338, align 16
  %496 = shufflevector <2 x i64> %486, <2 x i64> %488, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %496, <2 x i64>* %344, align 16
  call fastcc void @fadst8_sse2(<2 x i64>* nonnull %301)
  %497 = load <8 x i16>, <8 x i16>* %346, align 16
  %498 = ashr <8 x i16> %497, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %499 = load <8 x i16>, <8 x i16>* %348, align 16
  %500 = ashr <8 x i16> %499, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %501 = load <8 x i16>, <8 x i16>* %350, align 16
  %502 = ashr <8 x i16> %501, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %503 = load <8 x i16>, <8 x i16>* %352, align 16
  %504 = ashr <8 x i16> %503, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %505 = load <8 x i16>, <8 x i16>* %354, align 16
  %506 = ashr <8 x i16> %505, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %507 = load <8 x i16>, <8 x i16>* %356, align 16
  %508 = ashr <8 x i16> %507, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %509 = load <8 x i16>, <8 x i16>* %358, align 16
  %510 = ashr <8 x i16> %509, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %511 = load <8 x i16>, <8 x i16>* %360, align 16
  %512 = ashr <8 x i16> %511, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %513 = sub <8 x i16> %497, %498
  %514 = sub <8 x i16> %499, %500
  %515 = sub <8 x i16> %501, %502
  %516 = sub <8 x i16> %503, %504
  %517 = sub <8 x i16> %505, %506
  %518 = sub <8 x i16> %507, %508
  %519 = sub <8 x i16> %509, %510
  %520 = sub <8 x i16> %511, %512
  %521 = ashr <8 x i16> %513, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  store <8 x i16> %521, <8 x i16>* %346, align 16
  %522 = ashr <8 x i16> %514, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  store <8 x i16> %522, <8 x i16>* %348, align 16
  %523 = ashr <8 x i16> %515, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  store <8 x i16> %523, <8 x i16>* %350, align 16
  %524 = ashr <8 x i16> %516, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  store <8 x i16> %524, <8 x i16>* %352, align 16
  %525 = ashr <8 x i16> %517, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  store <8 x i16> %525, <8 x i16>* %354, align 16
  %526 = ashr <8 x i16> %518, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  store <8 x i16> %526, <8 x i16>* %356, align 16
  %527 = ashr <8 x i16> %519, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  store <8 x i16> %527, <8 x i16>* %358, align 16
  %528 = ashr <8 x i16> %520, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  store <8 x i16> %528, <8 x i16>* %360, align 16
  %529 = ashr <8 x i16> %513, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %530 = shufflevector <8 x i16> %521, <8 x i16> %529, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %531 = shufflevector <8 x i16> %521, <8 x i16> %529, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %532 = bitcast i32* %1 to <8 x i16>*
  store <8 x i16> %530, <8 x i16>* %532, align 16
  %533 = getelementptr inbounds i32, i32* %1, i64 4
  %534 = bitcast i32* %533 to <8 x i16>*
  store <8 x i16> %531, <8 x i16>* %534, align 16
  %535 = getelementptr inbounds i32, i32* %1, i64 8
  %536 = ashr <8 x i16> %514, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %537 = shufflevector <8 x i16> %522, <8 x i16> %536, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %538 = shufflevector <8 x i16> %522, <8 x i16> %536, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %539 = bitcast i32* %535 to <8 x i16>*
  store <8 x i16> %537, <8 x i16>* %539, align 16
  %540 = getelementptr inbounds i32, i32* %1, i64 12
  %541 = bitcast i32* %540 to <8 x i16>*
  store <8 x i16> %538, <8 x i16>* %541, align 16
  %542 = getelementptr inbounds i32, i32* %1, i64 16
  %543 = ashr <8 x i16> %515, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %544 = shufflevector <8 x i16> %523, <8 x i16> %543, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %545 = shufflevector <8 x i16> %523, <8 x i16> %543, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %546 = bitcast i32* %542 to <8 x i16>*
  store <8 x i16> %544, <8 x i16>* %546, align 16
  %547 = getelementptr inbounds i32, i32* %1, i64 20
  %548 = bitcast i32* %547 to <8 x i16>*
  store <8 x i16> %545, <8 x i16>* %548, align 16
  %549 = getelementptr inbounds i32, i32* %1, i64 24
  %550 = ashr <8 x i16> %516, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %551 = shufflevector <8 x i16> %524, <8 x i16> %550, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %552 = shufflevector <8 x i16> %524, <8 x i16> %550, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %553 = bitcast i32* %549 to <8 x i16>*
  store <8 x i16> %551, <8 x i16>* %553, align 16
  %554 = getelementptr inbounds i32, i32* %1, i64 28
  %555 = bitcast i32* %554 to <8 x i16>*
  store <8 x i16> %552, <8 x i16>* %555, align 16
  %556 = getelementptr inbounds i32, i32* %1, i64 32
  %557 = ashr <8 x i16> %517, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %558 = shufflevector <8 x i16> %525, <8 x i16> %557, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %559 = shufflevector <8 x i16> %525, <8 x i16> %557, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %560 = bitcast i32* %556 to <8 x i16>*
  store <8 x i16> %558, <8 x i16>* %560, align 16
  %561 = getelementptr inbounds i32, i32* %1, i64 36
  %562 = bitcast i32* %561 to <8 x i16>*
  store <8 x i16> %559, <8 x i16>* %562, align 16
  %563 = getelementptr inbounds i32, i32* %1, i64 40
  %564 = ashr <8 x i16> %518, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %565 = shufflevector <8 x i16> %526, <8 x i16> %564, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %566 = shufflevector <8 x i16> %526, <8 x i16> %564, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %567 = bitcast i32* %563 to <8 x i16>*
  store <8 x i16> %565, <8 x i16>* %567, align 16
  %568 = getelementptr inbounds i32, i32* %1, i64 44
  %569 = bitcast i32* %568 to <8 x i16>*
  store <8 x i16> %566, <8 x i16>* %569, align 16
  %570 = getelementptr inbounds i32, i32* %1, i64 48
  %571 = ashr <8 x i16> %519, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %572 = shufflevector <8 x i16> %527, <8 x i16> %571, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %573 = shufflevector <8 x i16> %527, <8 x i16> %571, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %574 = bitcast i32* %570 to <8 x i16>*
  store <8 x i16> %572, <8 x i16>* %574, align 16
  %575 = getelementptr inbounds i32, i32* %1, i64 52
  %576 = bitcast i32* %575 to <8 x i16>*
  store <8 x i16> %573, <8 x i16>* %576, align 16
  %577 = getelementptr inbounds i32, i32* %1, i64 56
  %578 = ashr <8 x i16> %520, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %579 = shufflevector <8 x i16> %528, <8 x i16> %578, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %580 = shufflevector <8 x i16> %528, <8 x i16> %578, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %581 = bitcast i32* %577 to <8 x i16>*
  store <8 x i16> %579, <8 x i16>* %581, align 16
  %582 = getelementptr inbounds i32, i32* %1, i64 60
  %583 = bitcast i32* %582 to <8 x i16>*
  store <8 x i16> %580, <8 x i16>* %583, align 16
  br label %732

584:                                              ; preds = %4
  %585 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %5, i64 0, i64 0
  %586 = bitcast i16* %0 to <8 x i16>*
  %587 = load <8 x i16>, <8 x i16>* %586, align 16
  %588 = sext i32 %2 to i64
  %589 = getelementptr inbounds i16, i16* %0, i64 %588
  %590 = bitcast i16* %589 to <8 x i16>*
  %591 = load <8 x i16>, <8 x i16>* %590, align 16
  %592 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %5, i64 0, i64 1
  %593 = shl nsw i32 %2, 1
  %594 = sext i32 %593 to i64
  %595 = getelementptr inbounds i16, i16* %0, i64 %594
  %596 = bitcast i16* %595 to <8 x i16>*
  %597 = load <8 x i16>, <8 x i16>* %596, align 16
  %598 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %5, i64 0, i64 2
  %599 = mul nsw i32 %2, 3
  %600 = sext i32 %599 to i64
  %601 = getelementptr inbounds i16, i16* %0, i64 %600
  %602 = bitcast i16* %601 to <8 x i16>*
  %603 = load <8 x i16>, <8 x i16>* %602, align 16
  %604 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %5, i64 0, i64 3
  %605 = shl nsw i32 %2, 2
  %606 = sext i32 %605 to i64
  %607 = getelementptr inbounds i16, i16* %0, i64 %606
  %608 = bitcast i16* %607 to <8 x i16>*
  %609 = load <8 x i16>, <8 x i16>* %608, align 16
  %610 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %5, i64 0, i64 4
  %611 = mul nsw i32 %2, 5
  %612 = sext i32 %611 to i64
  %613 = getelementptr inbounds i16, i16* %0, i64 %612
  %614 = bitcast i16* %613 to <8 x i16>*
  %615 = load <8 x i16>, <8 x i16>* %614, align 16
  %616 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %5, i64 0, i64 5
  %617 = mul nsw i32 %2, 6
  %618 = sext i32 %617 to i64
  %619 = getelementptr inbounds i16, i16* %0, i64 %618
  %620 = bitcast i16* %619 to <8 x i16>*
  %621 = load <8 x i16>, <8 x i16>* %620, align 16
  %622 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %5, i64 0, i64 6
  %623 = mul nsw i32 %2, 7
  %624 = sext i32 %623 to i64
  %625 = getelementptr inbounds i16, i16* %0, i64 %624
  %626 = bitcast i16* %625 to <8 x i16>*
  %627 = load <8 x i16>, <8 x i16>* %626, align 16
  %628 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %5, i64 0, i64 7
  %629 = shl <8 x i16> %587, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %630 = bitcast [8 x <2 x i64>]* %5 to <8 x i16>*
  store <8 x i16> %629, <8 x i16>* %630, align 16
  %631 = shl <8 x i16> %591, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %632 = bitcast <2 x i64>* %592 to <8 x i16>*
  store <8 x i16> %631, <8 x i16>* %632, align 16
  %633 = shl <8 x i16> %597, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %634 = bitcast <2 x i64>* %598 to <8 x i16>*
  store <8 x i16> %633, <8 x i16>* %634, align 16
  %635 = shl <8 x i16> %603, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %636 = bitcast <2 x i64>* %604 to <8 x i16>*
  store <8 x i16> %635, <8 x i16>* %636, align 16
  %637 = shl <8 x i16> %609, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %638 = bitcast <2 x i64>* %610 to <8 x i16>*
  store <8 x i16> %637, <8 x i16>* %638, align 16
  %639 = shl <8 x i16> %615, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %640 = bitcast <2 x i64>* %616 to <8 x i16>*
  store <8 x i16> %639, <8 x i16>* %640, align 16
  %641 = shl <8 x i16> %621, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %642 = bitcast <2 x i64>* %622 to <8 x i16>*
  store <8 x i16> %641, <8 x i16>* %642, align 16
  %643 = shl <8 x i16> %627, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %644 = bitcast <2 x i64>* %628 to <8 x i16>*
  store <8 x i16> %643, <8 x i16>* %644, align 16
  call fastcc void @fadst8_sse2(<2 x i64>* nonnull %585)
  call fastcc void @fadst8_sse2(<2 x i64>* nonnull %585)
  %645 = load <8 x i16>, <8 x i16>* %630, align 16
  %646 = ashr <8 x i16> %645, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %647 = load <8 x i16>, <8 x i16>* %632, align 16
  %648 = ashr <8 x i16> %647, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %649 = load <8 x i16>, <8 x i16>* %634, align 16
  %650 = ashr <8 x i16> %649, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %651 = load <8 x i16>, <8 x i16>* %636, align 16
  %652 = ashr <8 x i16> %651, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %653 = load <8 x i16>, <8 x i16>* %638, align 16
  %654 = ashr <8 x i16> %653, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %655 = load <8 x i16>, <8 x i16>* %640, align 16
  %656 = ashr <8 x i16> %655, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %657 = load <8 x i16>, <8 x i16>* %642, align 16
  %658 = ashr <8 x i16> %657, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %659 = load <8 x i16>, <8 x i16>* %644, align 16
  %660 = ashr <8 x i16> %659, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %661 = sub <8 x i16> %645, %646
  %662 = sub <8 x i16> %647, %648
  %663 = sub <8 x i16> %649, %650
  %664 = sub <8 x i16> %651, %652
  %665 = sub <8 x i16> %653, %654
  %666 = sub <8 x i16> %655, %656
  %667 = sub <8 x i16> %657, %658
  %668 = sub <8 x i16> %659, %660
  %669 = ashr <8 x i16> %661, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  store <8 x i16> %669, <8 x i16>* %630, align 16
  %670 = ashr <8 x i16> %662, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  store <8 x i16> %670, <8 x i16>* %632, align 16
  %671 = ashr <8 x i16> %663, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  store <8 x i16> %671, <8 x i16>* %634, align 16
  %672 = ashr <8 x i16> %664, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  store <8 x i16> %672, <8 x i16>* %636, align 16
  %673 = ashr <8 x i16> %665, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  store <8 x i16> %673, <8 x i16>* %638, align 16
  %674 = ashr <8 x i16> %666, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  store <8 x i16> %674, <8 x i16>* %640, align 16
  %675 = ashr <8 x i16> %667, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  store <8 x i16> %675, <8 x i16>* %642, align 16
  %676 = ashr <8 x i16> %668, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  store <8 x i16> %676, <8 x i16>* %644, align 16
  %677 = ashr <8 x i16> %661, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %678 = shufflevector <8 x i16> %669, <8 x i16> %677, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %679 = shufflevector <8 x i16> %669, <8 x i16> %677, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %680 = bitcast i32* %1 to <8 x i16>*
  store <8 x i16> %678, <8 x i16>* %680, align 16
  %681 = getelementptr inbounds i32, i32* %1, i64 4
  %682 = bitcast i32* %681 to <8 x i16>*
  store <8 x i16> %679, <8 x i16>* %682, align 16
  %683 = getelementptr inbounds i32, i32* %1, i64 8
  %684 = ashr <8 x i16> %662, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %685 = shufflevector <8 x i16> %670, <8 x i16> %684, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %686 = shufflevector <8 x i16> %670, <8 x i16> %684, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %687 = bitcast i32* %683 to <8 x i16>*
  store <8 x i16> %685, <8 x i16>* %687, align 16
  %688 = getelementptr inbounds i32, i32* %1, i64 12
  %689 = bitcast i32* %688 to <8 x i16>*
  store <8 x i16> %686, <8 x i16>* %689, align 16
  %690 = getelementptr inbounds i32, i32* %1, i64 16
  %691 = ashr <8 x i16> %663, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %692 = shufflevector <8 x i16> %671, <8 x i16> %691, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %693 = shufflevector <8 x i16> %671, <8 x i16> %691, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %694 = bitcast i32* %690 to <8 x i16>*
  store <8 x i16> %692, <8 x i16>* %694, align 16
  %695 = getelementptr inbounds i32, i32* %1, i64 20
  %696 = bitcast i32* %695 to <8 x i16>*
  store <8 x i16> %693, <8 x i16>* %696, align 16
  %697 = getelementptr inbounds i32, i32* %1, i64 24
  %698 = ashr <8 x i16> %664, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %699 = shufflevector <8 x i16> %672, <8 x i16> %698, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %700 = shufflevector <8 x i16> %672, <8 x i16> %698, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %701 = bitcast i32* %697 to <8 x i16>*
  store <8 x i16> %699, <8 x i16>* %701, align 16
  %702 = getelementptr inbounds i32, i32* %1, i64 28
  %703 = bitcast i32* %702 to <8 x i16>*
  store <8 x i16> %700, <8 x i16>* %703, align 16
  %704 = getelementptr inbounds i32, i32* %1, i64 32
  %705 = ashr <8 x i16> %665, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %706 = shufflevector <8 x i16> %673, <8 x i16> %705, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %707 = shufflevector <8 x i16> %673, <8 x i16> %705, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %708 = bitcast i32* %704 to <8 x i16>*
  store <8 x i16> %706, <8 x i16>* %708, align 16
  %709 = getelementptr inbounds i32, i32* %1, i64 36
  %710 = bitcast i32* %709 to <8 x i16>*
  store <8 x i16> %707, <8 x i16>* %710, align 16
  %711 = getelementptr inbounds i32, i32* %1, i64 40
  %712 = ashr <8 x i16> %666, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %713 = shufflevector <8 x i16> %674, <8 x i16> %712, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %714 = shufflevector <8 x i16> %674, <8 x i16> %712, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %715 = bitcast i32* %711 to <8 x i16>*
  store <8 x i16> %713, <8 x i16>* %715, align 16
  %716 = getelementptr inbounds i32, i32* %1, i64 44
  %717 = bitcast i32* %716 to <8 x i16>*
  store <8 x i16> %714, <8 x i16>* %717, align 16
  %718 = getelementptr inbounds i32, i32* %1, i64 48
  %719 = ashr <8 x i16> %667, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %720 = shufflevector <8 x i16> %675, <8 x i16> %719, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %721 = shufflevector <8 x i16> %675, <8 x i16> %719, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %722 = bitcast i32* %718 to <8 x i16>*
  store <8 x i16> %720, <8 x i16>* %722, align 16
  %723 = getelementptr inbounds i32, i32* %1, i64 52
  %724 = bitcast i32* %723 to <8 x i16>*
  store <8 x i16> %721, <8 x i16>* %724, align 16
  %725 = getelementptr inbounds i32, i32* %1, i64 56
  %726 = ashr <8 x i16> %668, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %727 = shufflevector <8 x i16> %676, <8 x i16> %726, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %728 = shufflevector <8 x i16> %676, <8 x i16> %726, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %729 = bitcast i32* %725 to <8 x i16>*
  store <8 x i16> %727, <8 x i16>* %729, align 16
  %730 = getelementptr inbounds i32, i32* %1, i64 60
  %731 = bitcast i32* %730 to <8 x i16>*
  store <8 x i16> %728, <8 x i16>* %731, align 16
  br label %732

732:                                              ; preds = %584, %300, %8, %7
  call void @llvm.lifetime.end.p0i8(i64 128, i8* nonnull %6) #6
  ret void
}

declare void @vpx_fdct8x8_sse2(i16*, i32*, i32) local_unnamed_addr #2

; Function Attrs: nofree nounwind ssp uwtable
define internal fastcc void @fadst8_sse2(<2 x i64>*) unnamed_addr #3 {
  %2 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 7
  %3 = bitcast <2 x i64>* %2 to <8 x i16>*
  %4 = load <8 x i16>, <8 x i16>* %3, align 16
  %5 = bitcast <2 x i64>* %0 to <8 x i16>*
  %6 = load <8 x i16>, <8 x i16>* %5, align 16
  %7 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 5
  %8 = bitcast <2 x i64>* %7 to <8 x i16>*
  %9 = load <8 x i16>, <8 x i16>* %8, align 16
  %10 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %11 = bitcast <2 x i64>* %10 to <8 x i16>*
  %12 = load <8 x i16>, <8 x i16>* %11, align 16
  %13 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %14 = bitcast <2 x i64>* %13 to <8 x i16>*
  %15 = load <8 x i16>, <8 x i16>* %14, align 16
  %16 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 4
  %17 = bitcast <2 x i64>* %16 to <8 x i16>*
  %18 = load <8 x i16>, <8 x i16>* %17, align 16
  %19 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %20 = bitcast <2 x i64>* %19 to <8 x i16>*
  %21 = load <8 x i16>, <8 x i16>* %20, align 16
  %22 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 6
  %23 = bitcast <2 x i64>* %22 to <8 x i16>*
  %24 = load <8 x i16>, <8 x i16>* %23, align 16
  %25 = shufflevector <8 x i16> %4, <8 x i16> %6, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %26 = shufflevector <8 x i16> %4, <8 x i16> %6, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %27 = shufflevector <8 x i16> %9, <8 x i16> %12, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %28 = shufflevector <8 x i16> %9, <8 x i16> %12, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %29 = shufflevector <8 x i16> %15, <8 x i16> %18, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %30 = shufflevector <8 x i16> %15, <8 x i16> %18, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %31 = shufflevector <8 x i16> %21, <8 x i16> %24, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %32 = shufflevector <8 x i16> %21, <8 x i16> %24, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %33 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %25, <8 x i16> <i16 16305, i16 1606, i16 16305, i16 1606, i16 16305, i16 1606, i16 16305, i16 1606>) #6
  %34 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %26, <8 x i16> <i16 16305, i16 1606, i16 16305, i16 1606, i16 16305, i16 1606, i16 16305, i16 1606>) #6
  %35 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %25, <8 x i16> <i16 1606, i16 -16305, i16 1606, i16 -16305, i16 1606, i16 -16305, i16 1606, i16 -16305>) #6
  %36 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %26, <8 x i16> <i16 1606, i16 -16305, i16 1606, i16 -16305, i16 1606, i16 -16305, i16 1606, i16 -16305>) #6
  %37 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %27, <8 x i16> <i16 14449, i16 7723, i16 14449, i16 7723, i16 14449, i16 7723, i16 14449, i16 7723>) #6
  %38 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %28, <8 x i16> <i16 14449, i16 7723, i16 14449, i16 7723, i16 14449, i16 7723, i16 14449, i16 7723>) #6
  %39 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %27, <8 x i16> <i16 7723, i16 -14449, i16 7723, i16 -14449, i16 7723, i16 -14449, i16 7723, i16 -14449>) #6
  %40 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %28, <8 x i16> <i16 7723, i16 -14449, i16 7723, i16 -14449, i16 7723, i16 -14449, i16 7723, i16 -14449>) #6
  %41 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %29, <8 x i16> <i16 10394, i16 12665, i16 10394, i16 12665, i16 10394, i16 12665, i16 10394, i16 12665>) #6
  %42 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %30, <8 x i16> <i16 10394, i16 12665, i16 10394, i16 12665, i16 10394, i16 12665, i16 10394, i16 12665>) #6
  %43 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %29, <8 x i16> <i16 12665, i16 -10394, i16 12665, i16 -10394, i16 12665, i16 -10394, i16 12665, i16 -10394>) #6
  %44 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %30, <8 x i16> <i16 12665, i16 -10394, i16 12665, i16 -10394, i16 12665, i16 -10394, i16 12665, i16 -10394>) #6
  %45 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %31, <8 x i16> <i16 4756, i16 15679, i16 4756, i16 15679, i16 4756, i16 15679, i16 4756, i16 15679>) #6
  %46 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %32, <8 x i16> <i16 4756, i16 15679, i16 4756, i16 15679, i16 4756, i16 15679, i16 4756, i16 15679>) #6
  %47 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %31, <8 x i16> <i16 15679, i16 -4756, i16 15679, i16 -4756, i16 15679, i16 -4756, i16 15679, i16 -4756>) #6
  %48 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %32, <8 x i16> <i16 15679, i16 -4756, i16 15679, i16 -4756, i16 15679, i16 -4756, i16 15679, i16 -4756>) #6
  %49 = add <4 x i32> %33, <i32 8192, i32 8192, i32 8192, i32 8192>
  %50 = add <4 x i32> %49, %41
  %51 = add <4 x i32> %34, <i32 8192, i32 8192, i32 8192, i32 8192>
  %52 = add <4 x i32> %51, %42
  %53 = add <4 x i32> %35, <i32 8192, i32 8192, i32 8192, i32 8192>
  %54 = add <4 x i32> %53, %43
  %55 = add <4 x i32> %36, <i32 8192, i32 8192, i32 8192, i32 8192>
  %56 = add <4 x i32> %55, %44
  %57 = add <4 x i32> %37, <i32 8192, i32 8192, i32 8192, i32 8192>
  %58 = add <4 x i32> %57, %45
  %59 = add <4 x i32> %38, <i32 8192, i32 8192, i32 8192, i32 8192>
  %60 = add <4 x i32> %59, %46
  %61 = add <4 x i32> %39, <i32 8192, i32 8192, i32 8192, i32 8192>
  %62 = add <4 x i32> %61, %47
  %63 = add <4 x i32> %40, <i32 8192, i32 8192, i32 8192, i32 8192>
  %64 = add <4 x i32> %63, %48
  %65 = sub <4 x i32> %49, %41
  %66 = sub <4 x i32> %51, %42
  %67 = sub <4 x i32> %53, %43
  %68 = sub <4 x i32> %55, %44
  %69 = sub <4 x i32> %57, %45
  %70 = sub <4 x i32> %59, %46
  %71 = sub <4 x i32> %61, %47
  %72 = sub <4 x i32> %63, %48
  %73 = ashr <4 x i32> %50, <i32 14, i32 14, i32 14, i32 14>
  %74 = ashr <4 x i32> %52, <i32 14, i32 14, i32 14, i32 14>
  %75 = ashr <4 x i32> %54, <i32 14, i32 14, i32 14, i32 14>
  %76 = ashr <4 x i32> %56, <i32 14, i32 14, i32 14, i32 14>
  %77 = ashr <4 x i32> %58, <i32 14, i32 14, i32 14, i32 14>
  %78 = ashr <4 x i32> %60, <i32 14, i32 14, i32 14, i32 14>
  %79 = ashr <4 x i32> %62, <i32 14, i32 14, i32 14, i32 14>
  %80 = ashr <4 x i32> %64, <i32 14, i32 14, i32 14, i32 14>
  %81 = ashr <4 x i32> %65, <i32 14, i32 14, i32 14, i32 14>
  %82 = ashr <4 x i32> %66, <i32 14, i32 14, i32 14, i32 14>
  %83 = ashr <4 x i32> %67, <i32 14, i32 14, i32 14, i32 14>
  %84 = ashr <4 x i32> %68, <i32 14, i32 14, i32 14, i32 14>
  %85 = ashr <4 x i32> %69, <i32 14, i32 14, i32 14, i32 14>
  %86 = ashr <4 x i32> %70, <i32 14, i32 14, i32 14, i32 14>
  %87 = ashr <4 x i32> %71, <i32 14, i32 14, i32 14, i32 14>
  %88 = ashr <4 x i32> %72, <i32 14, i32 14, i32 14, i32 14>
  %89 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %73, <4 x i32> %74) #6
  %90 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %75, <4 x i32> %76) #6
  %91 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %77, <4 x i32> %78) #6
  %92 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %79, <4 x i32> %80) #6
  %93 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %81, <4 x i32> %82) #6
  %94 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %83, <4 x i32> %84) #6
  %95 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %85, <4 x i32> %86) #6
  %96 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %87, <4 x i32> %88) #6
  %97 = add <8 x i16> %91, %89
  %98 = add <8 x i16> %90, %92
  %99 = sub <8 x i16> %89, %91
  %100 = sub <8 x i16> %90, %92
  %101 = shufflevector <8 x i16> %93, <8 x i16> %94, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %102 = shufflevector <8 x i16> %93, <8 x i16> %94, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %103 = shufflevector <8 x i16> %95, <8 x i16> %96, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %104 = shufflevector <8 x i16> %95, <8 x i16> %96, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %105 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %101, <8 x i16> <i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270>) #6
  %106 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %102, <8 x i16> <i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270>) #6
  %107 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %101, <8 x i16> <i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137>) #6
  %108 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %102, <8 x i16> <i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137>) #6
  %109 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %103, <8 x i16> <i16 -6270, i16 15137, i16 -6270, i16 15137, i16 -6270, i16 15137, i16 -6270, i16 15137>) #6
  %110 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %104, <8 x i16> <i16 -6270, i16 15137, i16 -6270, i16 15137, i16 -6270, i16 15137, i16 -6270, i16 15137>) #6
  %111 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %103, <8 x i16> <i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270>) #6
  %112 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %104, <8 x i16> <i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270>) #6
  %113 = add <4 x i32> %105, <i32 8192, i32 8192, i32 8192, i32 8192>
  %114 = add <4 x i32> %113, %109
  %115 = add <4 x i32> %106, <i32 8192, i32 8192, i32 8192, i32 8192>
  %116 = add <4 x i32> %115, %110
  %117 = add <4 x i32> %107, <i32 8192, i32 8192, i32 8192, i32 8192>
  %118 = add <4 x i32> %117, %111
  %119 = add <4 x i32> %108, <i32 8192, i32 8192, i32 8192, i32 8192>
  %120 = add <4 x i32> %119, %112
  %121 = sub <4 x i32> %113, %109
  %122 = sub <4 x i32> %115, %110
  %123 = sub <4 x i32> %117, %111
  %124 = sub <4 x i32> %119, %112
  %125 = ashr <4 x i32> %114, <i32 14, i32 14, i32 14, i32 14>
  %126 = ashr <4 x i32> %116, <i32 14, i32 14, i32 14, i32 14>
  %127 = ashr <4 x i32> %118, <i32 14, i32 14, i32 14, i32 14>
  %128 = ashr <4 x i32> %120, <i32 14, i32 14, i32 14, i32 14>
  %129 = ashr <4 x i32> %121, <i32 14, i32 14, i32 14, i32 14>
  %130 = ashr <4 x i32> %122, <i32 14, i32 14, i32 14, i32 14>
  %131 = ashr <4 x i32> %123, <i32 14, i32 14, i32 14, i32 14>
  %132 = ashr <4 x i32> %124, <i32 14, i32 14, i32 14, i32 14>
  %133 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %125, <4 x i32> %126) #6
  %134 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %127, <4 x i32> %128) #6
  %135 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %129, <4 x i32> %130) #6
  %136 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %131, <4 x i32> %132) #6
  %137 = shufflevector <8 x i16> %99, <8 x i16> %100, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %138 = shufflevector <8 x i16> %99, <8 x i16> %100, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %139 = shufflevector <8 x i16> %135, <8 x i16> %136, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %140 = shufflevector <8 x i16> %135, <8 x i16> %136, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %141 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %137, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %142 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %138, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %143 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %137, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %144 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %138, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %145 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %139, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %146 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %140, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %147 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %139, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %148 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %140, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %149 = add <4 x i32> %141, <i32 8192, i32 8192, i32 8192, i32 8192>
  %150 = add <4 x i32> %142, <i32 8192, i32 8192, i32 8192, i32 8192>
  %151 = add <4 x i32> %143, <i32 8192, i32 8192, i32 8192, i32 8192>
  %152 = add <4 x i32> %144, <i32 8192, i32 8192, i32 8192, i32 8192>
  %153 = add <4 x i32> %145, <i32 8192, i32 8192, i32 8192, i32 8192>
  %154 = add <4 x i32> %146, <i32 8192, i32 8192, i32 8192, i32 8192>
  %155 = add <4 x i32> %147, <i32 8192, i32 8192, i32 8192, i32 8192>
  %156 = add <4 x i32> %148, <i32 8192, i32 8192, i32 8192, i32 8192>
  %157 = ashr <4 x i32> %149, <i32 14, i32 14, i32 14, i32 14>
  %158 = ashr <4 x i32> %150, <i32 14, i32 14, i32 14, i32 14>
  %159 = ashr <4 x i32> %151, <i32 14, i32 14, i32 14, i32 14>
  %160 = ashr <4 x i32> %152, <i32 14, i32 14, i32 14, i32 14>
  %161 = ashr <4 x i32> %153, <i32 14, i32 14, i32 14, i32 14>
  %162 = ashr <4 x i32> %154, <i32 14, i32 14, i32 14, i32 14>
  %163 = ashr <4 x i32> %155, <i32 14, i32 14, i32 14, i32 14>
  %164 = ashr <4 x i32> %156, <i32 14, i32 14, i32 14, i32 14>
  %165 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %157, <4 x i32> %158) #6
  %166 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %159, <4 x i32> %160) #6
  %167 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %161, <4 x i32> %162) #6
  %168 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %163, <4 x i32> %164) #6
  %169 = sub <8 x i16> zeroinitializer, %133
  %170 = sub <8 x i16> zeroinitializer, %165
  %171 = sub <8 x i16> zeroinitializer, %168
  %172 = sub <8 x i16> zeroinitializer, %98
  %173 = shufflevector <8 x i16> %97, <8 x i16> %169, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %174 = shufflevector <8 x i16> %167, <8 x i16> %170, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %175 = shufflevector <8 x i16> %166, <8 x i16> %171, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %176 = shufflevector <8 x i16> %134, <8 x i16> %172, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %177 = shufflevector <8 x i16> %97, <8 x i16> %169, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %178 = shufflevector <8 x i16> %167, <8 x i16> %170, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %179 = shufflevector <8 x i16> %166, <8 x i16> %171, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %180 = shufflevector <8 x i16> %134, <8 x i16> %172, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %181 = bitcast <8 x i16> %173 to <4 x i32>
  %182 = bitcast <8 x i16> %174 to <4 x i32>
  %183 = shufflevector <4 x i32> %181, <4 x i32> %182, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %184 = bitcast <4 x i32> %183 to <2 x i64>
  %185 = bitcast <8 x i16> %175 to <4 x i32>
  %186 = bitcast <8 x i16> %176 to <4 x i32>
  %187 = shufflevector <4 x i32> %185, <4 x i32> %186, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %188 = bitcast <4 x i32> %187 to <2 x i64>
  %189 = bitcast <8 x i16> %177 to <4 x i32>
  %190 = bitcast <8 x i16> %178 to <4 x i32>
  %191 = shufflevector <4 x i32> %189, <4 x i32> %190, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %192 = bitcast <4 x i32> %191 to <2 x i64>
  %193 = bitcast <8 x i16> %179 to <4 x i32>
  %194 = bitcast <8 x i16> %180 to <4 x i32>
  %195 = shufflevector <4 x i32> %193, <4 x i32> %194, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %196 = bitcast <4 x i32> %195 to <2 x i64>
  %197 = shufflevector <4 x i32> %181, <4 x i32> %182, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %198 = bitcast <4 x i32> %197 to <2 x i64>
  %199 = shufflevector <4 x i32> %185, <4 x i32> %186, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %200 = bitcast <4 x i32> %199 to <2 x i64>
  %201 = shufflevector <4 x i32> %189, <4 x i32> %190, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %202 = bitcast <4 x i32> %201 to <2 x i64>
  %203 = shufflevector <4 x i32> %193, <4 x i32> %194, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %204 = bitcast <4 x i32> %203 to <2 x i64>
  %205 = shufflevector <2 x i64> %184, <2 x i64> %188, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %205, <2 x i64>* %0, align 16
  %206 = shufflevector <2 x i64> %184, <2 x i64> %188, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %206, <2 x i64>* %19, align 16
  %207 = shufflevector <2 x i64> %198, <2 x i64> %200, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %207, <2 x i64>* %10, align 16
  %208 = shufflevector <2 x i64> %198, <2 x i64> %200, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %208, <2 x i64>* %13, align 16
  %209 = shufflevector <2 x i64> %192, <2 x i64> %196, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %209, <2 x i64>* %16, align 16
  %210 = shufflevector <2 x i64> %192, <2 x i64> %196, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %210, <2 x i64>* %7, align 16
  %211 = shufflevector <2 x i64> %202, <2 x i64> %204, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %211, <2 x i64>* %22, align 16
  %212 = shufflevector <2 x i64> %202, <2 x i64> %204, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %212, <2 x i64>* %2, align 16
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @vp9_fht16x16_sse2(i16*, i32*, i32, i32) local_unnamed_addr #0 {
  %5 = alloca [16 x <2 x i64>], align 16
  %6 = alloca [16 x <2 x i64>], align 16
  %7 = bitcast [16 x <2 x i64>]* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %7) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %7, i8 -86, i64 256, i1 false)
  %8 = bitcast [16 x <2 x i64>]* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %8) #6
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %8, i8 -86, i64 256, i1 false)
  switch i32 %3, label %1512 [
    i32 0, label %9
    i32 1, label %10
    i32 2, label %761
  ]

9:                                                ; preds = %4
  tail call void @vpx_fdct16x16_sse2(i16* %0, i32* %1, i32 %2) #6
  br label %2263

10:                                               ; preds = %4
  %11 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 0
  %12 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 0
  %13 = bitcast i16* %0 to <8 x i16>*
  %14 = load <8 x i16>, <8 x i16>* %13, align 16
  %15 = sext i32 %2 to i64
  %16 = getelementptr inbounds i16, i16* %0, i64 %15
  %17 = bitcast i16* %16 to <8 x i16>*
  %18 = load <8 x i16>, <8 x i16>* %17, align 16
  %19 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 1
  %20 = shl nsw i32 %2, 1
  %21 = sext i32 %20 to i64
  %22 = getelementptr inbounds i16, i16* %0, i64 %21
  %23 = bitcast i16* %22 to <8 x i16>*
  %24 = load <8 x i16>, <8 x i16>* %23, align 16
  %25 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 2
  %26 = mul nsw i32 %2, 3
  %27 = sext i32 %26 to i64
  %28 = getelementptr inbounds i16, i16* %0, i64 %27
  %29 = bitcast i16* %28 to <8 x i16>*
  %30 = load <8 x i16>, <8 x i16>* %29, align 16
  %31 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 3
  %32 = shl nsw i32 %2, 2
  %33 = sext i32 %32 to i64
  %34 = getelementptr inbounds i16, i16* %0, i64 %33
  %35 = bitcast i16* %34 to <8 x i16>*
  %36 = load <8 x i16>, <8 x i16>* %35, align 16
  %37 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 4
  %38 = mul nsw i32 %2, 5
  %39 = sext i32 %38 to i64
  %40 = getelementptr inbounds i16, i16* %0, i64 %39
  %41 = bitcast i16* %40 to <8 x i16>*
  %42 = load <8 x i16>, <8 x i16>* %41, align 16
  %43 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 5
  %44 = mul nsw i32 %2, 6
  %45 = sext i32 %44 to i64
  %46 = getelementptr inbounds i16, i16* %0, i64 %45
  %47 = bitcast i16* %46 to <8 x i16>*
  %48 = load <8 x i16>, <8 x i16>* %47, align 16
  %49 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 6
  %50 = mul nsw i32 %2, 7
  %51 = sext i32 %50 to i64
  %52 = getelementptr inbounds i16, i16* %0, i64 %51
  %53 = bitcast i16* %52 to <8 x i16>*
  %54 = load <8 x i16>, <8 x i16>* %53, align 16
  %55 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 7
  %56 = shl <8 x i16> %14, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %57 = bitcast [16 x <2 x i64>]* %5 to <8 x i16>*
  store <8 x i16> %56, <8 x i16>* %57, align 16
  %58 = shl <8 x i16> %18, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %59 = bitcast <2 x i64>* %19 to <8 x i16>*
  store <8 x i16> %58, <8 x i16>* %59, align 16
  %60 = shl <8 x i16> %24, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %61 = bitcast <2 x i64>* %25 to <8 x i16>*
  store <8 x i16> %60, <8 x i16>* %61, align 16
  %62 = shl <8 x i16> %30, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %63 = bitcast <2 x i64>* %31 to <8 x i16>*
  store <8 x i16> %62, <8 x i16>* %63, align 16
  %64 = shl <8 x i16> %36, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %65 = bitcast <2 x i64>* %37 to <8 x i16>*
  store <8 x i16> %64, <8 x i16>* %65, align 16
  %66 = shl <8 x i16> %42, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %67 = bitcast <2 x i64>* %43 to <8 x i16>*
  store <8 x i16> %66, <8 x i16>* %67, align 16
  %68 = shl <8 x i16> %48, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %69 = bitcast <2 x i64>* %49 to <8 x i16>*
  store <8 x i16> %68, <8 x i16>* %69, align 16
  %70 = shl <8 x i16> %54, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %71 = bitcast <2 x i64>* %55 to <8 x i16>*
  store <8 x i16> %70, <8 x i16>* %71, align 16
  %72 = shl nsw i32 %2, 3
  %73 = sext i32 %72 to i64
  %74 = getelementptr inbounds i16, i16* %0, i64 %73
  %75 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 8
  %76 = bitcast i16* %74 to <8 x i16>*
  %77 = load <8 x i16>, <8 x i16>* %76, align 16
  %78 = getelementptr inbounds i16, i16* %74, i64 %15
  %79 = bitcast i16* %78 to <8 x i16>*
  %80 = load <8 x i16>, <8 x i16>* %79, align 16
  %81 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 9
  %82 = getelementptr inbounds i16, i16* %74, i64 %21
  %83 = bitcast i16* %82 to <8 x i16>*
  %84 = load <8 x i16>, <8 x i16>* %83, align 16
  %85 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 10
  %86 = getelementptr inbounds i16, i16* %74, i64 %27
  %87 = bitcast i16* %86 to <8 x i16>*
  %88 = load <8 x i16>, <8 x i16>* %87, align 16
  %89 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 11
  %90 = getelementptr inbounds i16, i16* %74, i64 %33
  %91 = bitcast i16* %90 to <8 x i16>*
  %92 = load <8 x i16>, <8 x i16>* %91, align 16
  %93 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 12
  %94 = getelementptr inbounds i16, i16* %74, i64 %39
  %95 = bitcast i16* %94 to <8 x i16>*
  %96 = load <8 x i16>, <8 x i16>* %95, align 16
  %97 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 13
  %98 = getelementptr inbounds i16, i16* %74, i64 %45
  %99 = bitcast i16* %98 to <8 x i16>*
  %100 = load <8 x i16>, <8 x i16>* %99, align 16
  %101 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 14
  %102 = getelementptr inbounds i16, i16* %74, i64 %51
  %103 = bitcast i16* %102 to <8 x i16>*
  %104 = load <8 x i16>, <8 x i16>* %103, align 16
  %105 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 15
  %106 = shl <8 x i16> %77, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %107 = bitcast <2 x i64>* %75 to <8 x i16>*
  store <8 x i16> %106, <8 x i16>* %107, align 16
  %108 = shl <8 x i16> %80, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %109 = bitcast <2 x i64>* %81 to <8 x i16>*
  store <8 x i16> %108, <8 x i16>* %109, align 16
  %110 = shl <8 x i16> %84, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %111 = bitcast <2 x i64>* %85 to <8 x i16>*
  store <8 x i16> %110, <8 x i16>* %111, align 16
  %112 = shl <8 x i16> %88, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %113 = bitcast <2 x i64>* %89 to <8 x i16>*
  store <8 x i16> %112, <8 x i16>* %113, align 16
  %114 = shl <8 x i16> %92, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %115 = bitcast <2 x i64>* %93 to <8 x i16>*
  store <8 x i16> %114, <8 x i16>* %115, align 16
  %116 = shl <8 x i16> %96, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %117 = bitcast <2 x i64>* %97 to <8 x i16>*
  store <8 x i16> %116, <8 x i16>* %117, align 16
  %118 = shl <8 x i16> %100, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %119 = bitcast <2 x i64>* %101 to <8 x i16>*
  store <8 x i16> %118, <8 x i16>* %119, align 16
  %120 = shl <8 x i16> %104, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %121 = bitcast <2 x i64>* %105 to <8 x i16>*
  store <8 x i16> %120, <8 x i16>* %121, align 16
  %122 = getelementptr inbounds i16, i16* %0, i64 8
  %123 = bitcast i16* %122 to <8 x i16>*
  %124 = load <8 x i16>, <8 x i16>* %123, align 16
  %125 = getelementptr inbounds i16, i16* %122, i64 %15
  %126 = bitcast i16* %125 to <8 x i16>*
  %127 = load <8 x i16>, <8 x i16>* %126, align 16
  %128 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 1
  %129 = getelementptr inbounds i16, i16* %122, i64 %21
  %130 = bitcast i16* %129 to <8 x i16>*
  %131 = load <8 x i16>, <8 x i16>* %130, align 16
  %132 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 2
  %133 = getelementptr inbounds i16, i16* %122, i64 %27
  %134 = bitcast i16* %133 to <8 x i16>*
  %135 = load <8 x i16>, <8 x i16>* %134, align 16
  %136 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 3
  %137 = getelementptr inbounds i16, i16* %122, i64 %33
  %138 = bitcast i16* %137 to <8 x i16>*
  %139 = load <8 x i16>, <8 x i16>* %138, align 16
  %140 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 4
  %141 = getelementptr inbounds i16, i16* %122, i64 %39
  %142 = bitcast i16* %141 to <8 x i16>*
  %143 = load <8 x i16>, <8 x i16>* %142, align 16
  %144 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 5
  %145 = getelementptr inbounds i16, i16* %122, i64 %45
  %146 = bitcast i16* %145 to <8 x i16>*
  %147 = load <8 x i16>, <8 x i16>* %146, align 16
  %148 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 6
  %149 = getelementptr inbounds i16, i16* %122, i64 %51
  %150 = bitcast i16* %149 to <8 x i16>*
  %151 = load <8 x i16>, <8 x i16>* %150, align 16
  %152 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 7
  %153 = shl <8 x i16> %124, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %154 = bitcast [16 x <2 x i64>]* %6 to <8 x i16>*
  store <8 x i16> %153, <8 x i16>* %154, align 16
  %155 = shl <8 x i16> %127, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %156 = bitcast <2 x i64>* %128 to <8 x i16>*
  store <8 x i16> %155, <8 x i16>* %156, align 16
  %157 = shl <8 x i16> %131, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %158 = bitcast <2 x i64>* %132 to <8 x i16>*
  store <8 x i16> %157, <8 x i16>* %158, align 16
  %159 = shl <8 x i16> %135, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %160 = bitcast <2 x i64>* %136 to <8 x i16>*
  store <8 x i16> %159, <8 x i16>* %160, align 16
  %161 = shl <8 x i16> %139, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %162 = bitcast <2 x i64>* %140 to <8 x i16>*
  store <8 x i16> %161, <8 x i16>* %162, align 16
  %163 = shl <8 x i16> %143, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %164 = bitcast <2 x i64>* %144 to <8 x i16>*
  store <8 x i16> %163, <8 x i16>* %164, align 16
  %165 = shl <8 x i16> %147, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %166 = bitcast <2 x i64>* %148 to <8 x i16>*
  store <8 x i16> %165, <8 x i16>* %166, align 16
  %167 = shl <8 x i16> %151, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %168 = bitcast <2 x i64>* %152 to <8 x i16>*
  store <8 x i16> %167, <8 x i16>* %168, align 16
  %169 = getelementptr inbounds i16, i16* %122, i64 %73
  %170 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 8
  %171 = bitcast i16* %169 to <8 x i16>*
  %172 = load <8 x i16>, <8 x i16>* %171, align 16
  %173 = getelementptr inbounds i16, i16* %169, i64 %15
  %174 = bitcast i16* %173 to <8 x i16>*
  %175 = load <8 x i16>, <8 x i16>* %174, align 16
  %176 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 9
  %177 = getelementptr inbounds i16, i16* %169, i64 %21
  %178 = bitcast i16* %177 to <8 x i16>*
  %179 = load <8 x i16>, <8 x i16>* %178, align 16
  %180 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 10
  %181 = getelementptr inbounds i16, i16* %169, i64 %27
  %182 = bitcast i16* %181 to <8 x i16>*
  %183 = load <8 x i16>, <8 x i16>* %182, align 16
  %184 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 11
  %185 = getelementptr inbounds i16, i16* %169, i64 %33
  %186 = bitcast i16* %185 to <8 x i16>*
  %187 = load <8 x i16>, <8 x i16>* %186, align 16
  %188 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 12
  %189 = getelementptr inbounds i16, i16* %169, i64 %39
  %190 = bitcast i16* %189 to <8 x i16>*
  %191 = load <8 x i16>, <8 x i16>* %190, align 16
  %192 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 13
  %193 = getelementptr inbounds i16, i16* %169, i64 %45
  %194 = bitcast i16* %193 to <8 x i16>*
  %195 = load <8 x i16>, <8 x i16>* %194, align 16
  %196 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 14
  %197 = getelementptr inbounds i16, i16* %169, i64 %51
  %198 = bitcast i16* %197 to <8 x i16>*
  %199 = load <8 x i16>, <8 x i16>* %198, align 16
  %200 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 15
  %201 = shl <8 x i16> %172, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %202 = bitcast <2 x i64>* %170 to <8 x i16>*
  store <8 x i16> %201, <8 x i16>* %202, align 16
  %203 = shl <8 x i16> %175, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %204 = bitcast <2 x i64>* %176 to <8 x i16>*
  store <8 x i16> %203, <8 x i16>* %204, align 16
  %205 = shl <8 x i16> %179, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %206 = bitcast <2 x i64>* %180 to <8 x i16>*
  store <8 x i16> %205, <8 x i16>* %206, align 16
  %207 = shl <8 x i16> %183, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %208 = bitcast <2 x i64>* %184 to <8 x i16>*
  store <8 x i16> %207, <8 x i16>* %208, align 16
  %209 = shl <8 x i16> %187, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %210 = bitcast <2 x i64>* %188 to <8 x i16>*
  store <8 x i16> %209, <8 x i16>* %210, align 16
  %211 = shl <8 x i16> %191, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %212 = bitcast <2 x i64>* %192 to <8 x i16>*
  store <8 x i16> %211, <8 x i16>* %212, align 16
  %213 = shl <8 x i16> %195, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %214 = bitcast <2 x i64>* %196 to <8 x i16>*
  store <8 x i16> %213, <8 x i16>* %214, align 16
  %215 = shl <8 x i16> %199, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %216 = bitcast <2 x i64>* %200 to <8 x i16>*
  store <8 x i16> %215, <8 x i16>* %216, align 16
  call fastcc void @fadst16_8col(<2 x i64>* nonnull %11) #6
  call fastcc void @fadst16_8col(<2 x i64>* nonnull %12) #6
  %217 = load <8 x i16>, <8 x i16>* %57, align 16
  %218 = load <8 x i16>, <8 x i16>* %59, align 16
  %219 = shufflevector <8 x i16> %217, <8 x i16> %218, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %220 = load <8 x i16>, <8 x i16>* %61, align 16
  %221 = load <8 x i16>, <8 x i16>* %63, align 16
  %222 = shufflevector <8 x i16> %220, <8 x i16> %221, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %223 = load <8 x i16>, <8 x i16>* %65, align 16
  %224 = load <8 x i16>, <8 x i16>* %67, align 16
  %225 = shufflevector <8 x i16> %223, <8 x i16> %224, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %226 = load <8 x i16>, <8 x i16>* %69, align 16
  %227 = load <8 x i16>, <8 x i16>* %71, align 16
  %228 = shufflevector <8 x i16> %226, <8 x i16> %227, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %229 = shufflevector <8 x i16> %217, <8 x i16> %218, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %230 = shufflevector <8 x i16> %220, <8 x i16> %221, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %231 = shufflevector <8 x i16> %223, <8 x i16> %224, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %232 = shufflevector <8 x i16> %226, <8 x i16> %227, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %233 = bitcast <8 x i16> %219 to <4 x i32>
  %234 = bitcast <8 x i16> %222 to <4 x i32>
  %235 = shufflevector <4 x i32> %233, <4 x i32> %234, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %236 = bitcast <4 x i32> %235 to <2 x i64>
  %237 = bitcast <8 x i16> %225 to <4 x i32>
  %238 = bitcast <8 x i16> %228 to <4 x i32>
  %239 = shufflevector <4 x i32> %237, <4 x i32> %238, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %240 = bitcast <4 x i32> %239 to <2 x i64>
  %241 = bitcast <8 x i16> %229 to <4 x i32>
  %242 = bitcast <8 x i16> %230 to <4 x i32>
  %243 = shufflevector <4 x i32> %241, <4 x i32> %242, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %244 = bitcast <4 x i32> %243 to <2 x i64>
  %245 = bitcast <8 x i16> %231 to <4 x i32>
  %246 = bitcast <8 x i16> %232 to <4 x i32>
  %247 = shufflevector <4 x i32> %245, <4 x i32> %246, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %248 = bitcast <4 x i32> %247 to <2 x i64>
  %249 = shufflevector <4 x i32> %233, <4 x i32> %234, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %250 = bitcast <4 x i32> %249 to <2 x i64>
  %251 = shufflevector <4 x i32> %237, <4 x i32> %238, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %252 = bitcast <4 x i32> %251 to <2 x i64>
  %253 = shufflevector <4 x i32> %241, <4 x i32> %242, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %254 = bitcast <4 x i32> %253 to <2 x i64>
  %255 = shufflevector <4 x i32> %245, <4 x i32> %246, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %256 = bitcast <4 x i32> %255 to <2 x i64>
  %257 = shufflevector <2 x i64> %236, <2 x i64> %240, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %257, <2 x i64>* %11, align 16
  %258 = shufflevector <2 x i64> %236, <2 x i64> %240, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %258, <2 x i64>* %19, align 16
  %259 = shufflevector <2 x i64> %250, <2 x i64> %252, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %259, <2 x i64>* %25, align 16
  %260 = shufflevector <2 x i64> %250, <2 x i64> %252, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %260, <2 x i64>* %31, align 16
  %261 = shufflevector <2 x i64> %244, <2 x i64> %248, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %261, <2 x i64>* %37, align 16
  %262 = shufflevector <2 x i64> %244, <2 x i64> %248, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %262, <2 x i64>* %43, align 16
  %263 = shufflevector <2 x i64> %254, <2 x i64> %256, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %263, <2 x i64>* %49, align 16
  %264 = shufflevector <2 x i64> %254, <2 x i64> %256, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %264, <2 x i64>* %55, align 16
  %265 = load <8 x i16>, <8 x i16>* %154, align 16
  %266 = load <8 x i16>, <8 x i16>* %156, align 16
  %267 = shufflevector <8 x i16> %265, <8 x i16> %266, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %268 = load <8 x i16>, <8 x i16>* %158, align 16
  %269 = load <8 x i16>, <8 x i16>* %160, align 16
  %270 = shufflevector <8 x i16> %268, <8 x i16> %269, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %271 = load <8 x i16>, <8 x i16>* %162, align 16
  %272 = load <8 x i16>, <8 x i16>* %164, align 16
  %273 = shufflevector <8 x i16> %271, <8 x i16> %272, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %274 = load <8 x i16>, <8 x i16>* %166, align 16
  %275 = load <8 x i16>, <8 x i16>* %168, align 16
  %276 = shufflevector <8 x i16> %274, <8 x i16> %275, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %277 = shufflevector <8 x i16> %265, <8 x i16> %266, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %278 = shufflevector <8 x i16> %268, <8 x i16> %269, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %279 = shufflevector <8 x i16> %271, <8 x i16> %272, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %280 = shufflevector <8 x i16> %274, <8 x i16> %275, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %281 = bitcast <8 x i16> %267 to <4 x i32>
  %282 = bitcast <8 x i16> %270 to <4 x i32>
  %283 = shufflevector <4 x i32> %281, <4 x i32> %282, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %284 = bitcast <4 x i32> %283 to <2 x i64>
  %285 = bitcast <8 x i16> %273 to <4 x i32>
  %286 = bitcast <8 x i16> %276 to <4 x i32>
  %287 = shufflevector <4 x i32> %285, <4 x i32> %286, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %288 = bitcast <4 x i32> %287 to <2 x i64>
  %289 = bitcast <8 x i16> %277 to <4 x i32>
  %290 = bitcast <8 x i16> %278 to <4 x i32>
  %291 = shufflevector <4 x i32> %289, <4 x i32> %290, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %292 = bitcast <4 x i32> %291 to <2 x i64>
  %293 = bitcast <8 x i16> %279 to <4 x i32>
  %294 = bitcast <8 x i16> %280 to <4 x i32>
  %295 = shufflevector <4 x i32> %293, <4 x i32> %294, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %296 = bitcast <4 x i32> %295 to <2 x i64>
  %297 = shufflevector <4 x i32> %281, <4 x i32> %282, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %298 = bitcast <4 x i32> %297 to <2 x i64>
  %299 = shufflevector <4 x i32> %285, <4 x i32> %286, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %300 = bitcast <4 x i32> %299 to <2 x i64>
  %301 = shufflevector <4 x i32> %289, <4 x i32> %290, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %302 = bitcast <4 x i32> %301 to <2 x i64>
  %303 = shufflevector <4 x i32> %293, <4 x i32> %294, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %304 = bitcast <4 x i32> %303 to <2 x i64>
  %305 = shufflevector <2 x i64> %284, <2 x i64> %288, <2 x i32> <i32 0, i32 2>
  %306 = shufflevector <2 x i64> %284, <2 x i64> %288, <2 x i32> <i32 1, i32 3>
  %307 = shufflevector <2 x i64> %298, <2 x i64> %300, <2 x i32> <i32 0, i32 2>
  %308 = shufflevector <2 x i64> %298, <2 x i64> %300, <2 x i32> <i32 1, i32 3>
  %309 = shufflevector <2 x i64> %292, <2 x i64> %296, <2 x i32> <i32 0, i32 2>
  %310 = shufflevector <2 x i64> %292, <2 x i64> %296, <2 x i32> <i32 1, i32 3>
  %311 = shufflevector <2 x i64> %302, <2 x i64> %304, <2 x i32> <i32 0, i32 2>
  %312 = shufflevector <2 x i64> %302, <2 x i64> %304, <2 x i32> <i32 1, i32 3>
  %313 = load <8 x i16>, <8 x i16>* %107, align 16
  %314 = load <8 x i16>, <8 x i16>* %109, align 16
  %315 = shufflevector <8 x i16> %313, <8 x i16> %314, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %316 = load <8 x i16>, <8 x i16>* %111, align 16
  %317 = load <8 x i16>, <8 x i16>* %113, align 16
  %318 = shufflevector <8 x i16> %316, <8 x i16> %317, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %319 = load <8 x i16>, <8 x i16>* %115, align 16
  %320 = load <8 x i16>, <8 x i16>* %117, align 16
  %321 = shufflevector <8 x i16> %319, <8 x i16> %320, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %322 = load <8 x i16>, <8 x i16>* %119, align 16
  %323 = load <8 x i16>, <8 x i16>* %121, align 16
  %324 = shufflevector <8 x i16> %322, <8 x i16> %323, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %325 = shufflevector <8 x i16> %313, <8 x i16> %314, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %326 = shufflevector <8 x i16> %316, <8 x i16> %317, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %327 = shufflevector <8 x i16> %319, <8 x i16> %320, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %328 = shufflevector <8 x i16> %322, <8 x i16> %323, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %329 = bitcast <8 x i16> %315 to <4 x i32>
  %330 = bitcast <8 x i16> %318 to <4 x i32>
  %331 = shufflevector <4 x i32> %329, <4 x i32> %330, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %332 = bitcast <4 x i32> %331 to <2 x i64>
  %333 = bitcast <8 x i16> %321 to <4 x i32>
  %334 = bitcast <8 x i16> %324 to <4 x i32>
  %335 = shufflevector <4 x i32> %333, <4 x i32> %334, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %336 = bitcast <4 x i32> %335 to <2 x i64>
  %337 = bitcast <8 x i16> %325 to <4 x i32>
  %338 = bitcast <8 x i16> %326 to <4 x i32>
  %339 = shufflevector <4 x i32> %337, <4 x i32> %338, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %340 = bitcast <4 x i32> %339 to <2 x i64>
  %341 = bitcast <8 x i16> %327 to <4 x i32>
  %342 = bitcast <8 x i16> %328 to <4 x i32>
  %343 = shufflevector <4 x i32> %341, <4 x i32> %342, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %344 = bitcast <4 x i32> %343 to <2 x i64>
  %345 = shufflevector <4 x i32> %329, <4 x i32> %330, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %346 = bitcast <4 x i32> %345 to <2 x i64>
  %347 = shufflevector <4 x i32> %333, <4 x i32> %334, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %348 = bitcast <4 x i32> %347 to <2 x i64>
  %349 = shufflevector <4 x i32> %337, <4 x i32> %338, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %350 = bitcast <4 x i32> %349 to <2 x i64>
  %351 = shufflevector <4 x i32> %341, <4 x i32> %342, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %352 = bitcast <4 x i32> %351 to <2 x i64>
  %353 = shufflevector <2 x i64> %332, <2 x i64> %336, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %353, <2 x i64>* %12, align 16
  %354 = shufflevector <2 x i64> %332, <2 x i64> %336, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %354, <2 x i64>* %128, align 16
  %355 = shufflevector <2 x i64> %346, <2 x i64> %348, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %355, <2 x i64>* %132, align 16
  %356 = shufflevector <2 x i64> %346, <2 x i64> %348, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %356, <2 x i64>* %136, align 16
  %357 = shufflevector <2 x i64> %340, <2 x i64> %344, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %357, <2 x i64>* %140, align 16
  %358 = shufflevector <2 x i64> %340, <2 x i64> %344, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %358, <2 x i64>* %144, align 16
  %359 = shufflevector <2 x i64> %350, <2 x i64> %352, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %359, <2 x i64>* %148, align 16
  %360 = shufflevector <2 x i64> %350, <2 x i64> %352, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %360, <2 x i64>* %152, align 16
  %361 = load <8 x i16>, <8 x i16>* %202, align 16
  %362 = load <8 x i16>, <8 x i16>* %204, align 16
  %363 = shufflevector <8 x i16> %361, <8 x i16> %362, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %364 = load <8 x i16>, <8 x i16>* %206, align 16
  %365 = load <8 x i16>, <8 x i16>* %208, align 16
  %366 = shufflevector <8 x i16> %364, <8 x i16> %365, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %367 = load <8 x i16>, <8 x i16>* %210, align 16
  %368 = load <8 x i16>, <8 x i16>* %212, align 16
  %369 = shufflevector <8 x i16> %367, <8 x i16> %368, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %370 = load <8 x i16>, <8 x i16>* %214, align 16
  %371 = load <8 x i16>, <8 x i16>* %216, align 16
  %372 = shufflevector <8 x i16> %370, <8 x i16> %371, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %373 = shufflevector <8 x i16> %361, <8 x i16> %362, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %374 = shufflevector <8 x i16> %364, <8 x i16> %365, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %375 = shufflevector <8 x i16> %367, <8 x i16> %368, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %376 = shufflevector <8 x i16> %370, <8 x i16> %371, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %377 = bitcast <8 x i16> %363 to <4 x i32>
  %378 = bitcast <8 x i16> %366 to <4 x i32>
  %379 = shufflevector <4 x i32> %377, <4 x i32> %378, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %380 = bitcast <4 x i32> %379 to <2 x i64>
  %381 = bitcast <8 x i16> %369 to <4 x i32>
  %382 = bitcast <8 x i16> %372 to <4 x i32>
  %383 = shufflevector <4 x i32> %381, <4 x i32> %382, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %384 = bitcast <4 x i32> %383 to <2 x i64>
  %385 = bitcast <8 x i16> %373 to <4 x i32>
  %386 = bitcast <8 x i16> %374 to <4 x i32>
  %387 = shufflevector <4 x i32> %385, <4 x i32> %386, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %388 = bitcast <4 x i32> %387 to <2 x i64>
  %389 = bitcast <8 x i16> %375 to <4 x i32>
  %390 = bitcast <8 x i16> %376 to <4 x i32>
  %391 = shufflevector <4 x i32> %389, <4 x i32> %390, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %392 = bitcast <4 x i32> %391 to <2 x i64>
  %393 = shufflevector <4 x i32> %377, <4 x i32> %378, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %394 = bitcast <4 x i32> %393 to <2 x i64>
  %395 = shufflevector <4 x i32> %381, <4 x i32> %382, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %396 = bitcast <4 x i32> %395 to <2 x i64>
  %397 = shufflevector <4 x i32> %385, <4 x i32> %386, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %398 = bitcast <4 x i32> %397 to <2 x i64>
  %399 = shufflevector <4 x i32> %389, <4 x i32> %390, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %400 = bitcast <4 x i32> %399 to <2 x i64>
  %401 = shufflevector <2 x i64> %380, <2 x i64> %384, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %401, <2 x i64>* %170, align 16
  %402 = shufflevector <2 x i64> %380, <2 x i64> %384, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %402, <2 x i64>* %176, align 16
  %403 = shufflevector <2 x i64> %394, <2 x i64> %396, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %403, <2 x i64>* %180, align 16
  %404 = shufflevector <2 x i64> %394, <2 x i64> %396, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %404, <2 x i64>* %184, align 16
  %405 = shufflevector <2 x i64> %388, <2 x i64> %392, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %405, <2 x i64>* %188, align 16
  %406 = shufflevector <2 x i64> %388, <2 x i64> %392, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %406, <2 x i64>* %192, align 16
  %407 = shufflevector <2 x i64> %398, <2 x i64> %400, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %407, <2 x i64>* %196, align 16
  %408 = shufflevector <2 x i64> %398, <2 x i64> %400, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %408, <2 x i64>* %200, align 16
  %409 = load <8 x i16>, <8 x i16>* %57, align 16
  %410 = load <8 x i16>, <8 x i16>* %59, align 16
  %411 = load <8 x i16>, <8 x i16>* %61, align 16
  %412 = load <8 x i16>, <8 x i16>* %63, align 16
  %413 = load <8 x i16>, <8 x i16>* %65, align 16
  %414 = load <8 x i16>, <8 x i16>* %67, align 16
  %415 = load <8 x i16>, <8 x i16>* %69, align 16
  %416 = load <8 x i16>, <8 x i16>* %71, align 16
  %417 = add <8 x i16> %409, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %418 = add <8 x i16> %410, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %419 = add <8 x i16> %411, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %420 = add <8 x i16> %412, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %421 = add <8 x i16> %413, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %422 = add <8 x i16> %414, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %423 = add <8 x i16> %415, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %424 = add <8 x i16> %416, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %425 = lshr <8 x i16> %409, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %426 = add <8 x i16> %417, %425
  %427 = lshr <8 x i16> %410, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %428 = add <8 x i16> %418, %427
  %429 = lshr <8 x i16> %411, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %430 = add <8 x i16> %419, %429
  %431 = lshr <8 x i16> %412, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %432 = add <8 x i16> %420, %431
  %433 = lshr <8 x i16> %413, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %434 = add <8 x i16> %421, %433
  %435 = lshr <8 x i16> %414, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %436 = add <8 x i16> %422, %435
  %437 = lshr <8 x i16> %415, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %438 = add <8 x i16> %423, %437
  %439 = lshr <8 x i16> %416, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %440 = add <8 x i16> %424, %439
  %441 = ashr <8 x i16> %426, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %441, <8 x i16>* %57, align 16
  %442 = ashr <8 x i16> %428, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %442, <8 x i16>* %59, align 16
  %443 = ashr <8 x i16> %430, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %443, <8 x i16>* %61, align 16
  %444 = ashr <8 x i16> %432, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %444, <8 x i16>* %63, align 16
  %445 = ashr <8 x i16> %434, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %445, <8 x i16>* %65, align 16
  %446 = ashr <8 x i16> %436, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %446, <8 x i16>* %67, align 16
  %447 = ashr <8 x i16> %438, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %447, <8 x i16>* %69, align 16
  %448 = ashr <8 x i16> %440, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %448, <8 x i16>* %71, align 16
  %449 = bitcast <2 x i64> %305 to <8 x i16>
  %450 = ashr <8 x i16> %449, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %451 = bitcast <2 x i64> %306 to <8 x i16>
  %452 = ashr <8 x i16> %451, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %453 = bitcast <2 x i64> %307 to <8 x i16>
  %454 = ashr <8 x i16> %453, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %455 = bitcast <2 x i64> %308 to <8 x i16>
  %456 = ashr <8 x i16> %455, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %457 = bitcast <2 x i64> %309 to <8 x i16>
  %458 = ashr <8 x i16> %457, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %459 = bitcast <2 x i64> %310 to <8 x i16>
  %460 = ashr <8 x i16> %459, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %461 = bitcast <2 x i64> %311 to <8 x i16>
  %462 = ashr <8 x i16> %461, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %463 = bitcast <2 x i64> %312 to <8 x i16>
  %464 = ashr <8 x i16> %463, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %465 = add <8 x i16> %449, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %466 = add <8 x i16> %451, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %467 = add <8 x i16> %453, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %468 = add <8 x i16> %455, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %469 = add <8 x i16> %457, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %470 = add <8 x i16> %459, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %471 = add <8 x i16> %461, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %472 = add <8 x i16> %463, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %473 = sub <8 x i16> %465, %450
  %474 = sub <8 x i16> %466, %452
  %475 = sub <8 x i16> %467, %454
  %476 = sub <8 x i16> %468, %456
  %477 = sub <8 x i16> %469, %458
  %478 = sub <8 x i16> %470, %460
  %479 = sub <8 x i16> %471, %462
  %480 = sub <8 x i16> %472, %464
  %481 = ashr <8 x i16> %473, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %481, <8 x i16>* %107, align 16
  %482 = ashr <8 x i16> %474, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %482, <8 x i16>* %109, align 16
  %483 = ashr <8 x i16> %475, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %483, <8 x i16>* %111, align 16
  %484 = ashr <8 x i16> %476, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %484, <8 x i16>* %113, align 16
  %485 = ashr <8 x i16> %477, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %485, <8 x i16>* %115, align 16
  %486 = ashr <8 x i16> %478, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %486, <8 x i16>* %117, align 16
  %487 = ashr <8 x i16> %479, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %487, <8 x i16>* %119, align 16
  %488 = ashr <8 x i16> %480, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %488, <8 x i16>* %121, align 16
  %489 = load <8 x i16>, <8 x i16>* %154, align 16
  %490 = ashr <8 x i16> %489, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %491 = load <8 x i16>, <8 x i16>* %156, align 16
  %492 = ashr <8 x i16> %491, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %493 = load <8 x i16>, <8 x i16>* %158, align 16
  %494 = ashr <8 x i16> %493, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %495 = load <8 x i16>, <8 x i16>* %160, align 16
  %496 = ashr <8 x i16> %495, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %497 = load <8 x i16>, <8 x i16>* %162, align 16
  %498 = ashr <8 x i16> %497, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %499 = load <8 x i16>, <8 x i16>* %164, align 16
  %500 = ashr <8 x i16> %499, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %501 = load <8 x i16>, <8 x i16>* %166, align 16
  %502 = ashr <8 x i16> %501, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %503 = load <8 x i16>, <8 x i16>* %168, align 16
  %504 = ashr <8 x i16> %503, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %505 = add <8 x i16> %489, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %506 = add <8 x i16> %491, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %507 = add <8 x i16> %493, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %508 = add <8 x i16> %495, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %509 = add <8 x i16> %497, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %510 = add <8 x i16> %499, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %511 = add <8 x i16> %501, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %512 = add <8 x i16> %503, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %513 = sub <8 x i16> %505, %490
  %514 = sub <8 x i16> %506, %492
  %515 = sub <8 x i16> %507, %494
  %516 = sub <8 x i16> %508, %496
  %517 = sub <8 x i16> %509, %498
  %518 = sub <8 x i16> %510, %500
  %519 = sub <8 x i16> %511, %502
  %520 = sub <8 x i16> %512, %504
  %521 = ashr <8 x i16> %513, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %521, <8 x i16>* %154, align 16
  %522 = ashr <8 x i16> %514, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %522, <8 x i16>* %156, align 16
  %523 = ashr <8 x i16> %515, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %523, <8 x i16>* %158, align 16
  %524 = ashr <8 x i16> %516, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %524, <8 x i16>* %160, align 16
  %525 = ashr <8 x i16> %517, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %525, <8 x i16>* %162, align 16
  %526 = ashr <8 x i16> %518, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %526, <8 x i16>* %164, align 16
  %527 = ashr <8 x i16> %519, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %527, <8 x i16>* %166, align 16
  %528 = ashr <8 x i16> %520, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %528, <8 x i16>* %168, align 16
  %529 = load <8 x i16>, <8 x i16>* %202, align 16
  %530 = load <8 x i16>, <8 x i16>* %204, align 16
  %531 = load <8 x i16>, <8 x i16>* %206, align 16
  %532 = load <8 x i16>, <8 x i16>* %208, align 16
  %533 = load <8 x i16>, <8 x i16>* %210, align 16
  %534 = load <8 x i16>, <8 x i16>* %212, align 16
  %535 = load <8 x i16>, <8 x i16>* %214, align 16
  %536 = load <8 x i16>, <8 x i16>* %216, align 16
  %537 = add <8 x i16> %529, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %538 = add <8 x i16> %530, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %539 = add <8 x i16> %531, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %540 = add <8 x i16> %532, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %541 = add <8 x i16> %533, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %542 = add <8 x i16> %534, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %543 = add <8 x i16> %535, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %544 = add <8 x i16> %536, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %545 = lshr <8 x i16> %529, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %546 = add <8 x i16> %537, %545
  %547 = lshr <8 x i16> %530, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %548 = add <8 x i16> %538, %547
  %549 = lshr <8 x i16> %531, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %550 = add <8 x i16> %539, %549
  %551 = lshr <8 x i16> %532, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %552 = add <8 x i16> %540, %551
  %553 = lshr <8 x i16> %533, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %554 = add <8 x i16> %541, %553
  %555 = lshr <8 x i16> %534, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %556 = add <8 x i16> %542, %555
  %557 = lshr <8 x i16> %535, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %558 = add <8 x i16> %543, %557
  %559 = lshr <8 x i16> %536, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %560 = add <8 x i16> %544, %559
  %561 = ashr <8 x i16> %546, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %561, <8 x i16>* %202, align 16
  %562 = ashr <8 x i16> %548, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %562, <8 x i16>* %204, align 16
  %563 = ashr <8 x i16> %550, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %563, <8 x i16>* %206, align 16
  %564 = ashr <8 x i16> %552, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %564, <8 x i16>* %208, align 16
  %565 = ashr <8 x i16> %554, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %565, <8 x i16>* %210, align 16
  %566 = ashr <8 x i16> %556, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %566, <8 x i16>* %212, align 16
  %567 = ashr <8 x i16> %558, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %567, <8 x i16>* %214, align 16
  %568 = ashr <8 x i16> %560, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %568, <8 x i16>* %216, align 16
  call fastcc void @fdct16_8col(<2 x i64>* nonnull %11) #6
  call fastcc void @fdct16_8col(<2 x i64>* nonnull %12) #6
  %569 = load <8 x i16>, <8 x i16>* %57, align 16
  %570 = load <8 x i16>, <8 x i16>* %59, align 16
  %571 = shufflevector <8 x i16> %569, <8 x i16> %570, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %572 = load <8 x i16>, <8 x i16>* %61, align 16
  %573 = load <8 x i16>, <8 x i16>* %63, align 16
  %574 = shufflevector <8 x i16> %572, <8 x i16> %573, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %575 = load <8 x i16>, <8 x i16>* %65, align 16
  %576 = load <8 x i16>, <8 x i16>* %67, align 16
  %577 = shufflevector <8 x i16> %575, <8 x i16> %576, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %578 = load <8 x i16>, <8 x i16>* %69, align 16
  %579 = load <8 x i16>, <8 x i16>* %71, align 16
  %580 = shufflevector <8 x i16> %578, <8 x i16> %579, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %581 = shufflevector <8 x i16> %569, <8 x i16> %570, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %582 = shufflevector <8 x i16> %572, <8 x i16> %573, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %583 = shufflevector <8 x i16> %575, <8 x i16> %576, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %584 = shufflevector <8 x i16> %578, <8 x i16> %579, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %585 = bitcast <8 x i16> %571 to <4 x i32>
  %586 = bitcast <8 x i16> %574 to <4 x i32>
  %587 = shufflevector <4 x i32> %585, <4 x i32> %586, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %588 = bitcast <4 x i32> %587 to <2 x i64>
  %589 = bitcast <8 x i16> %577 to <4 x i32>
  %590 = bitcast <8 x i16> %580 to <4 x i32>
  %591 = shufflevector <4 x i32> %589, <4 x i32> %590, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %592 = bitcast <4 x i32> %591 to <2 x i64>
  %593 = bitcast <8 x i16> %581 to <4 x i32>
  %594 = bitcast <8 x i16> %582 to <4 x i32>
  %595 = shufflevector <4 x i32> %593, <4 x i32> %594, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %596 = bitcast <4 x i32> %595 to <2 x i64>
  %597 = bitcast <8 x i16> %583 to <4 x i32>
  %598 = bitcast <8 x i16> %584 to <4 x i32>
  %599 = shufflevector <4 x i32> %597, <4 x i32> %598, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %600 = bitcast <4 x i32> %599 to <2 x i64>
  %601 = shufflevector <4 x i32> %585, <4 x i32> %586, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %602 = bitcast <4 x i32> %601 to <2 x i64>
  %603 = shufflevector <4 x i32> %589, <4 x i32> %590, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %604 = bitcast <4 x i32> %603 to <2 x i64>
  %605 = shufflevector <4 x i32> %593, <4 x i32> %594, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %606 = bitcast <4 x i32> %605 to <2 x i64>
  %607 = shufflevector <4 x i32> %597, <4 x i32> %598, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %608 = bitcast <4 x i32> %607 to <2 x i64>
  %609 = shufflevector <2 x i64> %588, <2 x i64> %592, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %609, <2 x i64>* %11, align 16
  %610 = shufflevector <2 x i64> %588, <2 x i64> %592, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %610, <2 x i64>* %19, align 16
  %611 = shufflevector <2 x i64> %602, <2 x i64> %604, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %611, <2 x i64>* %25, align 16
  %612 = shufflevector <2 x i64> %602, <2 x i64> %604, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %612, <2 x i64>* %31, align 16
  %613 = shufflevector <2 x i64> %596, <2 x i64> %600, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %613, <2 x i64>* %37, align 16
  %614 = shufflevector <2 x i64> %596, <2 x i64> %600, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %614, <2 x i64>* %43, align 16
  %615 = shufflevector <2 x i64> %606, <2 x i64> %608, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %615, <2 x i64>* %49, align 16
  %616 = shufflevector <2 x i64> %606, <2 x i64> %608, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %616, <2 x i64>* %55, align 16
  %617 = load <8 x i16>, <8 x i16>* %154, align 16
  %618 = load <8 x i16>, <8 x i16>* %156, align 16
  %619 = shufflevector <8 x i16> %617, <8 x i16> %618, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %620 = load <8 x i16>, <8 x i16>* %158, align 16
  %621 = load <8 x i16>, <8 x i16>* %160, align 16
  %622 = shufflevector <8 x i16> %620, <8 x i16> %621, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %623 = load <8 x i16>, <8 x i16>* %162, align 16
  %624 = load <8 x i16>, <8 x i16>* %164, align 16
  %625 = shufflevector <8 x i16> %623, <8 x i16> %624, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %626 = load <8 x i16>, <8 x i16>* %166, align 16
  %627 = load <8 x i16>, <8 x i16>* %168, align 16
  %628 = shufflevector <8 x i16> %626, <8 x i16> %627, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %629 = shufflevector <8 x i16> %617, <8 x i16> %618, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %630 = shufflevector <8 x i16> %620, <8 x i16> %621, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %631 = shufflevector <8 x i16> %623, <8 x i16> %624, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %632 = shufflevector <8 x i16> %626, <8 x i16> %627, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %633 = bitcast <8 x i16> %619 to <4 x i32>
  %634 = bitcast <8 x i16> %622 to <4 x i32>
  %635 = shufflevector <4 x i32> %633, <4 x i32> %634, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %636 = bitcast <4 x i32> %635 to <2 x i64>
  %637 = bitcast <8 x i16> %625 to <4 x i32>
  %638 = bitcast <8 x i16> %628 to <4 x i32>
  %639 = shufflevector <4 x i32> %637, <4 x i32> %638, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %640 = bitcast <4 x i32> %639 to <2 x i64>
  %641 = bitcast <8 x i16> %629 to <4 x i32>
  %642 = bitcast <8 x i16> %630 to <4 x i32>
  %643 = shufflevector <4 x i32> %641, <4 x i32> %642, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %644 = bitcast <4 x i32> %643 to <2 x i64>
  %645 = bitcast <8 x i16> %631 to <4 x i32>
  %646 = bitcast <8 x i16> %632 to <4 x i32>
  %647 = shufflevector <4 x i32> %645, <4 x i32> %646, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %648 = bitcast <4 x i32> %647 to <2 x i64>
  %649 = shufflevector <4 x i32> %633, <4 x i32> %634, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %650 = bitcast <4 x i32> %649 to <2 x i64>
  %651 = shufflevector <4 x i32> %637, <4 x i32> %638, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %652 = bitcast <4 x i32> %651 to <2 x i64>
  %653 = shufflevector <4 x i32> %641, <4 x i32> %642, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %654 = bitcast <4 x i32> %653 to <2 x i64>
  %655 = shufflevector <4 x i32> %645, <4 x i32> %646, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %656 = bitcast <4 x i32> %655 to <2 x i64>
  %657 = shufflevector <2 x i64> %636, <2 x i64> %640, <2 x i32> <i32 0, i32 2>
  %658 = shufflevector <2 x i64> %636, <2 x i64> %640, <2 x i32> <i32 1, i32 3>
  %659 = shufflevector <2 x i64> %650, <2 x i64> %652, <2 x i32> <i32 0, i32 2>
  %660 = shufflevector <2 x i64> %650, <2 x i64> %652, <2 x i32> <i32 1, i32 3>
  %661 = shufflevector <2 x i64> %644, <2 x i64> %648, <2 x i32> <i32 0, i32 2>
  %662 = shufflevector <2 x i64> %644, <2 x i64> %648, <2 x i32> <i32 1, i32 3>
  %663 = shufflevector <2 x i64> %654, <2 x i64> %656, <2 x i32> <i32 0, i32 2>
  %664 = shufflevector <2 x i64> %654, <2 x i64> %656, <2 x i32> <i32 1, i32 3>
  %665 = load <8 x i16>, <8 x i16>* %107, align 16
  %666 = load <8 x i16>, <8 x i16>* %109, align 16
  %667 = shufflevector <8 x i16> %665, <8 x i16> %666, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %668 = load <8 x i16>, <8 x i16>* %111, align 16
  %669 = load <8 x i16>, <8 x i16>* %113, align 16
  %670 = shufflevector <8 x i16> %668, <8 x i16> %669, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %671 = load <8 x i16>, <8 x i16>* %115, align 16
  %672 = load <8 x i16>, <8 x i16>* %117, align 16
  %673 = shufflevector <8 x i16> %671, <8 x i16> %672, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %674 = load <8 x i16>, <8 x i16>* %119, align 16
  %675 = load <8 x i16>, <8 x i16>* %121, align 16
  %676 = shufflevector <8 x i16> %674, <8 x i16> %675, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %677 = shufflevector <8 x i16> %665, <8 x i16> %666, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %678 = shufflevector <8 x i16> %668, <8 x i16> %669, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %679 = shufflevector <8 x i16> %671, <8 x i16> %672, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %680 = shufflevector <8 x i16> %674, <8 x i16> %675, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %681 = bitcast <8 x i16> %667 to <4 x i32>
  %682 = bitcast <8 x i16> %670 to <4 x i32>
  %683 = shufflevector <4 x i32> %681, <4 x i32> %682, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %684 = bitcast <4 x i32> %683 to <2 x i64>
  %685 = bitcast <8 x i16> %673 to <4 x i32>
  %686 = bitcast <8 x i16> %676 to <4 x i32>
  %687 = shufflevector <4 x i32> %685, <4 x i32> %686, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %688 = bitcast <4 x i32> %687 to <2 x i64>
  %689 = bitcast <8 x i16> %677 to <4 x i32>
  %690 = bitcast <8 x i16> %678 to <4 x i32>
  %691 = shufflevector <4 x i32> %689, <4 x i32> %690, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %692 = bitcast <4 x i32> %691 to <2 x i64>
  %693 = bitcast <8 x i16> %679 to <4 x i32>
  %694 = bitcast <8 x i16> %680 to <4 x i32>
  %695 = shufflevector <4 x i32> %693, <4 x i32> %694, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %696 = bitcast <4 x i32> %695 to <2 x i64>
  %697 = shufflevector <4 x i32> %681, <4 x i32> %682, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %698 = bitcast <4 x i32> %697 to <2 x i64>
  %699 = shufflevector <4 x i32> %685, <4 x i32> %686, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %700 = bitcast <4 x i32> %699 to <2 x i64>
  %701 = shufflevector <4 x i32> %689, <4 x i32> %690, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %702 = bitcast <4 x i32> %701 to <2 x i64>
  %703 = shufflevector <4 x i32> %693, <4 x i32> %694, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %704 = bitcast <4 x i32> %703 to <2 x i64>
  %705 = shufflevector <2 x i64> %684, <2 x i64> %688, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %705, <2 x i64>* %12, align 16
  %706 = shufflevector <2 x i64> %684, <2 x i64> %688, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %706, <2 x i64>* %128, align 16
  %707 = shufflevector <2 x i64> %698, <2 x i64> %700, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %707, <2 x i64>* %132, align 16
  %708 = shufflevector <2 x i64> %698, <2 x i64> %700, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %708, <2 x i64>* %136, align 16
  %709 = shufflevector <2 x i64> %692, <2 x i64> %696, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %709, <2 x i64>* %140, align 16
  %710 = shufflevector <2 x i64> %692, <2 x i64> %696, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %710, <2 x i64>* %144, align 16
  %711 = shufflevector <2 x i64> %702, <2 x i64> %704, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %711, <2 x i64>* %148, align 16
  %712 = shufflevector <2 x i64> %702, <2 x i64> %704, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %712, <2 x i64>* %152, align 16
  %713 = load <8 x i16>, <8 x i16>* %202, align 16
  %714 = load <8 x i16>, <8 x i16>* %204, align 16
  %715 = shufflevector <8 x i16> %713, <8 x i16> %714, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %716 = load <8 x i16>, <8 x i16>* %206, align 16
  %717 = load <8 x i16>, <8 x i16>* %208, align 16
  %718 = shufflevector <8 x i16> %716, <8 x i16> %717, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %719 = load <8 x i16>, <8 x i16>* %210, align 16
  %720 = load <8 x i16>, <8 x i16>* %212, align 16
  %721 = shufflevector <8 x i16> %719, <8 x i16> %720, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %722 = load <8 x i16>, <8 x i16>* %214, align 16
  %723 = load <8 x i16>, <8 x i16>* %216, align 16
  %724 = shufflevector <8 x i16> %722, <8 x i16> %723, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %725 = shufflevector <8 x i16> %713, <8 x i16> %714, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %726 = shufflevector <8 x i16> %716, <8 x i16> %717, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %727 = shufflevector <8 x i16> %719, <8 x i16> %720, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %728 = shufflevector <8 x i16> %722, <8 x i16> %723, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %729 = bitcast <8 x i16> %715 to <4 x i32>
  %730 = bitcast <8 x i16> %718 to <4 x i32>
  %731 = shufflevector <4 x i32> %729, <4 x i32> %730, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %732 = bitcast <4 x i32> %731 to <2 x i64>
  %733 = bitcast <8 x i16> %721 to <4 x i32>
  %734 = bitcast <8 x i16> %724 to <4 x i32>
  %735 = shufflevector <4 x i32> %733, <4 x i32> %734, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %736 = bitcast <4 x i32> %735 to <2 x i64>
  %737 = bitcast <8 x i16> %725 to <4 x i32>
  %738 = bitcast <8 x i16> %726 to <4 x i32>
  %739 = shufflevector <4 x i32> %737, <4 x i32> %738, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %740 = bitcast <4 x i32> %739 to <2 x i64>
  %741 = bitcast <8 x i16> %727 to <4 x i32>
  %742 = bitcast <8 x i16> %728 to <4 x i32>
  %743 = shufflevector <4 x i32> %741, <4 x i32> %742, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %744 = bitcast <4 x i32> %743 to <2 x i64>
  %745 = shufflevector <4 x i32> %729, <4 x i32> %730, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %746 = bitcast <4 x i32> %745 to <2 x i64>
  %747 = shufflevector <4 x i32> %733, <4 x i32> %734, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %748 = bitcast <4 x i32> %747 to <2 x i64>
  %749 = shufflevector <4 x i32> %737, <4 x i32> %738, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %750 = bitcast <4 x i32> %749 to <2 x i64>
  %751 = shufflevector <4 x i32> %741, <4 x i32> %742, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %752 = bitcast <4 x i32> %751 to <2 x i64>
  %753 = shufflevector <2 x i64> %732, <2 x i64> %736, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %753, <2 x i64>* %170, align 16
  %754 = shufflevector <2 x i64> %732, <2 x i64> %736, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %754, <2 x i64>* %176, align 16
  %755 = shufflevector <2 x i64> %746, <2 x i64> %748, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %755, <2 x i64>* %180, align 16
  %756 = shufflevector <2 x i64> %746, <2 x i64> %748, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %756, <2 x i64>* %184, align 16
  %757 = shufflevector <2 x i64> %740, <2 x i64> %744, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %757, <2 x i64>* %188, align 16
  %758 = shufflevector <2 x i64> %740, <2 x i64> %744, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %758, <2 x i64>* %192, align 16
  %759 = shufflevector <2 x i64> %750, <2 x i64> %752, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %759, <2 x i64>* %196, align 16
  %760 = shufflevector <2 x i64> %750, <2 x i64> %752, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %760, <2 x i64>* %200, align 16
  store <2 x i64> %657, <2 x i64>* %75, align 16
  store <2 x i64> %658, <2 x i64>* %81, align 16
  store <2 x i64> %659, <2 x i64>* %85, align 16
  store <2 x i64> %660, <2 x i64>* %89, align 16
  store <2 x i64> %661, <2 x i64>* %93, align 16
  store <2 x i64> %662, <2 x i64>* %97, align 16
  store <2 x i64> %663, <2 x i64>* %101, align 16
  store <2 x i64> %664, <2 x i64>* %105, align 16
  call fastcc void @write_buffer_16x16(i32* %1, <2 x i64>* nonnull %11, <2 x i64>* nonnull %12)
  br label %2263

761:                                              ; preds = %4
  %762 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 0
  %763 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 0
  %764 = bitcast i16* %0 to <8 x i16>*
  %765 = load <8 x i16>, <8 x i16>* %764, align 16
  %766 = sext i32 %2 to i64
  %767 = getelementptr inbounds i16, i16* %0, i64 %766
  %768 = bitcast i16* %767 to <8 x i16>*
  %769 = load <8 x i16>, <8 x i16>* %768, align 16
  %770 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 1
  %771 = shl nsw i32 %2, 1
  %772 = sext i32 %771 to i64
  %773 = getelementptr inbounds i16, i16* %0, i64 %772
  %774 = bitcast i16* %773 to <8 x i16>*
  %775 = load <8 x i16>, <8 x i16>* %774, align 16
  %776 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 2
  %777 = mul nsw i32 %2, 3
  %778 = sext i32 %777 to i64
  %779 = getelementptr inbounds i16, i16* %0, i64 %778
  %780 = bitcast i16* %779 to <8 x i16>*
  %781 = load <8 x i16>, <8 x i16>* %780, align 16
  %782 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 3
  %783 = shl nsw i32 %2, 2
  %784 = sext i32 %783 to i64
  %785 = getelementptr inbounds i16, i16* %0, i64 %784
  %786 = bitcast i16* %785 to <8 x i16>*
  %787 = load <8 x i16>, <8 x i16>* %786, align 16
  %788 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 4
  %789 = mul nsw i32 %2, 5
  %790 = sext i32 %789 to i64
  %791 = getelementptr inbounds i16, i16* %0, i64 %790
  %792 = bitcast i16* %791 to <8 x i16>*
  %793 = load <8 x i16>, <8 x i16>* %792, align 16
  %794 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 5
  %795 = mul nsw i32 %2, 6
  %796 = sext i32 %795 to i64
  %797 = getelementptr inbounds i16, i16* %0, i64 %796
  %798 = bitcast i16* %797 to <8 x i16>*
  %799 = load <8 x i16>, <8 x i16>* %798, align 16
  %800 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 6
  %801 = mul nsw i32 %2, 7
  %802 = sext i32 %801 to i64
  %803 = getelementptr inbounds i16, i16* %0, i64 %802
  %804 = bitcast i16* %803 to <8 x i16>*
  %805 = load <8 x i16>, <8 x i16>* %804, align 16
  %806 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 7
  %807 = shl <8 x i16> %765, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %808 = bitcast [16 x <2 x i64>]* %5 to <8 x i16>*
  store <8 x i16> %807, <8 x i16>* %808, align 16
  %809 = shl <8 x i16> %769, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %810 = bitcast <2 x i64>* %770 to <8 x i16>*
  store <8 x i16> %809, <8 x i16>* %810, align 16
  %811 = shl <8 x i16> %775, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %812 = bitcast <2 x i64>* %776 to <8 x i16>*
  store <8 x i16> %811, <8 x i16>* %812, align 16
  %813 = shl <8 x i16> %781, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %814 = bitcast <2 x i64>* %782 to <8 x i16>*
  store <8 x i16> %813, <8 x i16>* %814, align 16
  %815 = shl <8 x i16> %787, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %816 = bitcast <2 x i64>* %788 to <8 x i16>*
  store <8 x i16> %815, <8 x i16>* %816, align 16
  %817 = shl <8 x i16> %793, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %818 = bitcast <2 x i64>* %794 to <8 x i16>*
  store <8 x i16> %817, <8 x i16>* %818, align 16
  %819 = shl <8 x i16> %799, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %820 = bitcast <2 x i64>* %800 to <8 x i16>*
  store <8 x i16> %819, <8 x i16>* %820, align 16
  %821 = shl <8 x i16> %805, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %822 = bitcast <2 x i64>* %806 to <8 x i16>*
  store <8 x i16> %821, <8 x i16>* %822, align 16
  %823 = shl nsw i32 %2, 3
  %824 = sext i32 %823 to i64
  %825 = getelementptr inbounds i16, i16* %0, i64 %824
  %826 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 8
  %827 = bitcast i16* %825 to <8 x i16>*
  %828 = load <8 x i16>, <8 x i16>* %827, align 16
  %829 = getelementptr inbounds i16, i16* %825, i64 %766
  %830 = bitcast i16* %829 to <8 x i16>*
  %831 = load <8 x i16>, <8 x i16>* %830, align 16
  %832 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 9
  %833 = getelementptr inbounds i16, i16* %825, i64 %772
  %834 = bitcast i16* %833 to <8 x i16>*
  %835 = load <8 x i16>, <8 x i16>* %834, align 16
  %836 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 10
  %837 = getelementptr inbounds i16, i16* %825, i64 %778
  %838 = bitcast i16* %837 to <8 x i16>*
  %839 = load <8 x i16>, <8 x i16>* %838, align 16
  %840 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 11
  %841 = getelementptr inbounds i16, i16* %825, i64 %784
  %842 = bitcast i16* %841 to <8 x i16>*
  %843 = load <8 x i16>, <8 x i16>* %842, align 16
  %844 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 12
  %845 = getelementptr inbounds i16, i16* %825, i64 %790
  %846 = bitcast i16* %845 to <8 x i16>*
  %847 = load <8 x i16>, <8 x i16>* %846, align 16
  %848 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 13
  %849 = getelementptr inbounds i16, i16* %825, i64 %796
  %850 = bitcast i16* %849 to <8 x i16>*
  %851 = load <8 x i16>, <8 x i16>* %850, align 16
  %852 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 14
  %853 = getelementptr inbounds i16, i16* %825, i64 %802
  %854 = bitcast i16* %853 to <8 x i16>*
  %855 = load <8 x i16>, <8 x i16>* %854, align 16
  %856 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 15
  %857 = shl <8 x i16> %828, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %858 = bitcast <2 x i64>* %826 to <8 x i16>*
  store <8 x i16> %857, <8 x i16>* %858, align 16
  %859 = shl <8 x i16> %831, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %860 = bitcast <2 x i64>* %832 to <8 x i16>*
  store <8 x i16> %859, <8 x i16>* %860, align 16
  %861 = shl <8 x i16> %835, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %862 = bitcast <2 x i64>* %836 to <8 x i16>*
  store <8 x i16> %861, <8 x i16>* %862, align 16
  %863 = shl <8 x i16> %839, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %864 = bitcast <2 x i64>* %840 to <8 x i16>*
  store <8 x i16> %863, <8 x i16>* %864, align 16
  %865 = shl <8 x i16> %843, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %866 = bitcast <2 x i64>* %844 to <8 x i16>*
  store <8 x i16> %865, <8 x i16>* %866, align 16
  %867 = shl <8 x i16> %847, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %868 = bitcast <2 x i64>* %848 to <8 x i16>*
  store <8 x i16> %867, <8 x i16>* %868, align 16
  %869 = shl <8 x i16> %851, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %870 = bitcast <2 x i64>* %852 to <8 x i16>*
  store <8 x i16> %869, <8 x i16>* %870, align 16
  %871 = shl <8 x i16> %855, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %872 = bitcast <2 x i64>* %856 to <8 x i16>*
  store <8 x i16> %871, <8 x i16>* %872, align 16
  %873 = getelementptr inbounds i16, i16* %0, i64 8
  %874 = bitcast i16* %873 to <8 x i16>*
  %875 = load <8 x i16>, <8 x i16>* %874, align 16
  %876 = getelementptr inbounds i16, i16* %873, i64 %766
  %877 = bitcast i16* %876 to <8 x i16>*
  %878 = load <8 x i16>, <8 x i16>* %877, align 16
  %879 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 1
  %880 = getelementptr inbounds i16, i16* %873, i64 %772
  %881 = bitcast i16* %880 to <8 x i16>*
  %882 = load <8 x i16>, <8 x i16>* %881, align 16
  %883 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 2
  %884 = getelementptr inbounds i16, i16* %873, i64 %778
  %885 = bitcast i16* %884 to <8 x i16>*
  %886 = load <8 x i16>, <8 x i16>* %885, align 16
  %887 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 3
  %888 = getelementptr inbounds i16, i16* %873, i64 %784
  %889 = bitcast i16* %888 to <8 x i16>*
  %890 = load <8 x i16>, <8 x i16>* %889, align 16
  %891 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 4
  %892 = getelementptr inbounds i16, i16* %873, i64 %790
  %893 = bitcast i16* %892 to <8 x i16>*
  %894 = load <8 x i16>, <8 x i16>* %893, align 16
  %895 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 5
  %896 = getelementptr inbounds i16, i16* %873, i64 %796
  %897 = bitcast i16* %896 to <8 x i16>*
  %898 = load <8 x i16>, <8 x i16>* %897, align 16
  %899 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 6
  %900 = getelementptr inbounds i16, i16* %873, i64 %802
  %901 = bitcast i16* %900 to <8 x i16>*
  %902 = load <8 x i16>, <8 x i16>* %901, align 16
  %903 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 7
  %904 = shl <8 x i16> %875, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %905 = bitcast [16 x <2 x i64>]* %6 to <8 x i16>*
  store <8 x i16> %904, <8 x i16>* %905, align 16
  %906 = shl <8 x i16> %878, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %907 = bitcast <2 x i64>* %879 to <8 x i16>*
  store <8 x i16> %906, <8 x i16>* %907, align 16
  %908 = shl <8 x i16> %882, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %909 = bitcast <2 x i64>* %883 to <8 x i16>*
  store <8 x i16> %908, <8 x i16>* %909, align 16
  %910 = shl <8 x i16> %886, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %911 = bitcast <2 x i64>* %887 to <8 x i16>*
  store <8 x i16> %910, <8 x i16>* %911, align 16
  %912 = shl <8 x i16> %890, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %913 = bitcast <2 x i64>* %891 to <8 x i16>*
  store <8 x i16> %912, <8 x i16>* %913, align 16
  %914 = shl <8 x i16> %894, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %915 = bitcast <2 x i64>* %895 to <8 x i16>*
  store <8 x i16> %914, <8 x i16>* %915, align 16
  %916 = shl <8 x i16> %898, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %917 = bitcast <2 x i64>* %899 to <8 x i16>*
  store <8 x i16> %916, <8 x i16>* %917, align 16
  %918 = shl <8 x i16> %902, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %919 = bitcast <2 x i64>* %903 to <8 x i16>*
  store <8 x i16> %918, <8 x i16>* %919, align 16
  %920 = getelementptr inbounds i16, i16* %873, i64 %824
  %921 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 8
  %922 = bitcast i16* %920 to <8 x i16>*
  %923 = load <8 x i16>, <8 x i16>* %922, align 16
  %924 = getelementptr inbounds i16, i16* %920, i64 %766
  %925 = bitcast i16* %924 to <8 x i16>*
  %926 = load <8 x i16>, <8 x i16>* %925, align 16
  %927 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 9
  %928 = getelementptr inbounds i16, i16* %920, i64 %772
  %929 = bitcast i16* %928 to <8 x i16>*
  %930 = load <8 x i16>, <8 x i16>* %929, align 16
  %931 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 10
  %932 = getelementptr inbounds i16, i16* %920, i64 %778
  %933 = bitcast i16* %932 to <8 x i16>*
  %934 = load <8 x i16>, <8 x i16>* %933, align 16
  %935 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 11
  %936 = getelementptr inbounds i16, i16* %920, i64 %784
  %937 = bitcast i16* %936 to <8 x i16>*
  %938 = load <8 x i16>, <8 x i16>* %937, align 16
  %939 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 12
  %940 = getelementptr inbounds i16, i16* %920, i64 %790
  %941 = bitcast i16* %940 to <8 x i16>*
  %942 = load <8 x i16>, <8 x i16>* %941, align 16
  %943 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 13
  %944 = getelementptr inbounds i16, i16* %920, i64 %796
  %945 = bitcast i16* %944 to <8 x i16>*
  %946 = load <8 x i16>, <8 x i16>* %945, align 16
  %947 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 14
  %948 = getelementptr inbounds i16, i16* %920, i64 %802
  %949 = bitcast i16* %948 to <8 x i16>*
  %950 = load <8 x i16>, <8 x i16>* %949, align 16
  %951 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 15
  %952 = shl <8 x i16> %923, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %953 = bitcast <2 x i64>* %921 to <8 x i16>*
  store <8 x i16> %952, <8 x i16>* %953, align 16
  %954 = shl <8 x i16> %926, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %955 = bitcast <2 x i64>* %927 to <8 x i16>*
  store <8 x i16> %954, <8 x i16>* %955, align 16
  %956 = shl <8 x i16> %930, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %957 = bitcast <2 x i64>* %931 to <8 x i16>*
  store <8 x i16> %956, <8 x i16>* %957, align 16
  %958 = shl <8 x i16> %934, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %959 = bitcast <2 x i64>* %935 to <8 x i16>*
  store <8 x i16> %958, <8 x i16>* %959, align 16
  %960 = shl <8 x i16> %938, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %961 = bitcast <2 x i64>* %939 to <8 x i16>*
  store <8 x i16> %960, <8 x i16>* %961, align 16
  %962 = shl <8 x i16> %942, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %963 = bitcast <2 x i64>* %943 to <8 x i16>*
  store <8 x i16> %962, <8 x i16>* %963, align 16
  %964 = shl <8 x i16> %946, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %965 = bitcast <2 x i64>* %947 to <8 x i16>*
  store <8 x i16> %964, <8 x i16>* %965, align 16
  %966 = shl <8 x i16> %950, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %967 = bitcast <2 x i64>* %951 to <8 x i16>*
  store <8 x i16> %966, <8 x i16>* %967, align 16
  call fastcc void @fdct16_8col(<2 x i64>* nonnull %762) #6
  call fastcc void @fdct16_8col(<2 x i64>* nonnull %763) #6
  %968 = load <8 x i16>, <8 x i16>* %808, align 16
  %969 = load <8 x i16>, <8 x i16>* %810, align 16
  %970 = shufflevector <8 x i16> %968, <8 x i16> %969, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %971 = load <8 x i16>, <8 x i16>* %812, align 16
  %972 = load <8 x i16>, <8 x i16>* %814, align 16
  %973 = shufflevector <8 x i16> %971, <8 x i16> %972, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %974 = load <8 x i16>, <8 x i16>* %816, align 16
  %975 = load <8 x i16>, <8 x i16>* %818, align 16
  %976 = shufflevector <8 x i16> %974, <8 x i16> %975, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %977 = load <8 x i16>, <8 x i16>* %820, align 16
  %978 = load <8 x i16>, <8 x i16>* %822, align 16
  %979 = shufflevector <8 x i16> %977, <8 x i16> %978, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %980 = shufflevector <8 x i16> %968, <8 x i16> %969, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %981 = shufflevector <8 x i16> %971, <8 x i16> %972, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %982 = shufflevector <8 x i16> %974, <8 x i16> %975, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %983 = shufflevector <8 x i16> %977, <8 x i16> %978, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %984 = bitcast <8 x i16> %970 to <4 x i32>
  %985 = bitcast <8 x i16> %973 to <4 x i32>
  %986 = shufflevector <4 x i32> %984, <4 x i32> %985, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %987 = bitcast <4 x i32> %986 to <2 x i64>
  %988 = bitcast <8 x i16> %976 to <4 x i32>
  %989 = bitcast <8 x i16> %979 to <4 x i32>
  %990 = shufflevector <4 x i32> %988, <4 x i32> %989, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %991 = bitcast <4 x i32> %990 to <2 x i64>
  %992 = bitcast <8 x i16> %980 to <4 x i32>
  %993 = bitcast <8 x i16> %981 to <4 x i32>
  %994 = shufflevector <4 x i32> %992, <4 x i32> %993, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %995 = bitcast <4 x i32> %994 to <2 x i64>
  %996 = bitcast <8 x i16> %982 to <4 x i32>
  %997 = bitcast <8 x i16> %983 to <4 x i32>
  %998 = shufflevector <4 x i32> %996, <4 x i32> %997, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %999 = bitcast <4 x i32> %998 to <2 x i64>
  %1000 = shufflevector <4 x i32> %984, <4 x i32> %985, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1001 = bitcast <4 x i32> %1000 to <2 x i64>
  %1002 = shufflevector <4 x i32> %988, <4 x i32> %989, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1003 = bitcast <4 x i32> %1002 to <2 x i64>
  %1004 = shufflevector <4 x i32> %992, <4 x i32> %993, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1005 = bitcast <4 x i32> %1004 to <2 x i64>
  %1006 = shufflevector <4 x i32> %996, <4 x i32> %997, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1007 = bitcast <4 x i32> %1006 to <2 x i64>
  %1008 = shufflevector <2 x i64> %987, <2 x i64> %991, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1008, <2 x i64>* %762, align 16
  %1009 = shufflevector <2 x i64> %987, <2 x i64> %991, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1009, <2 x i64>* %770, align 16
  %1010 = shufflevector <2 x i64> %1001, <2 x i64> %1003, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1010, <2 x i64>* %776, align 16
  %1011 = shufflevector <2 x i64> %1001, <2 x i64> %1003, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1011, <2 x i64>* %782, align 16
  %1012 = shufflevector <2 x i64> %995, <2 x i64> %999, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1012, <2 x i64>* %788, align 16
  %1013 = shufflevector <2 x i64> %995, <2 x i64> %999, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1013, <2 x i64>* %794, align 16
  %1014 = shufflevector <2 x i64> %1005, <2 x i64> %1007, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1014, <2 x i64>* %800, align 16
  %1015 = shufflevector <2 x i64> %1005, <2 x i64> %1007, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1015, <2 x i64>* %806, align 16
  %1016 = load <8 x i16>, <8 x i16>* %905, align 16
  %1017 = load <8 x i16>, <8 x i16>* %907, align 16
  %1018 = shufflevector <8 x i16> %1016, <8 x i16> %1017, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1019 = load <8 x i16>, <8 x i16>* %909, align 16
  %1020 = load <8 x i16>, <8 x i16>* %911, align 16
  %1021 = shufflevector <8 x i16> %1019, <8 x i16> %1020, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1022 = load <8 x i16>, <8 x i16>* %913, align 16
  %1023 = load <8 x i16>, <8 x i16>* %915, align 16
  %1024 = shufflevector <8 x i16> %1022, <8 x i16> %1023, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1025 = load <8 x i16>, <8 x i16>* %917, align 16
  %1026 = load <8 x i16>, <8 x i16>* %919, align 16
  %1027 = shufflevector <8 x i16> %1025, <8 x i16> %1026, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1028 = shufflevector <8 x i16> %1016, <8 x i16> %1017, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1029 = shufflevector <8 x i16> %1019, <8 x i16> %1020, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1030 = shufflevector <8 x i16> %1022, <8 x i16> %1023, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1031 = shufflevector <8 x i16> %1025, <8 x i16> %1026, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1032 = bitcast <8 x i16> %1018 to <4 x i32>
  %1033 = bitcast <8 x i16> %1021 to <4 x i32>
  %1034 = shufflevector <4 x i32> %1032, <4 x i32> %1033, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1035 = bitcast <4 x i32> %1034 to <2 x i64>
  %1036 = bitcast <8 x i16> %1024 to <4 x i32>
  %1037 = bitcast <8 x i16> %1027 to <4 x i32>
  %1038 = shufflevector <4 x i32> %1036, <4 x i32> %1037, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1039 = bitcast <4 x i32> %1038 to <2 x i64>
  %1040 = bitcast <8 x i16> %1028 to <4 x i32>
  %1041 = bitcast <8 x i16> %1029 to <4 x i32>
  %1042 = shufflevector <4 x i32> %1040, <4 x i32> %1041, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1043 = bitcast <4 x i32> %1042 to <2 x i64>
  %1044 = bitcast <8 x i16> %1030 to <4 x i32>
  %1045 = bitcast <8 x i16> %1031 to <4 x i32>
  %1046 = shufflevector <4 x i32> %1044, <4 x i32> %1045, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1047 = bitcast <4 x i32> %1046 to <2 x i64>
  %1048 = shufflevector <4 x i32> %1032, <4 x i32> %1033, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1049 = bitcast <4 x i32> %1048 to <2 x i64>
  %1050 = shufflevector <4 x i32> %1036, <4 x i32> %1037, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1051 = bitcast <4 x i32> %1050 to <2 x i64>
  %1052 = shufflevector <4 x i32> %1040, <4 x i32> %1041, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1053 = bitcast <4 x i32> %1052 to <2 x i64>
  %1054 = shufflevector <4 x i32> %1044, <4 x i32> %1045, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1055 = bitcast <4 x i32> %1054 to <2 x i64>
  %1056 = shufflevector <2 x i64> %1035, <2 x i64> %1039, <2 x i32> <i32 0, i32 2>
  %1057 = shufflevector <2 x i64> %1035, <2 x i64> %1039, <2 x i32> <i32 1, i32 3>
  %1058 = shufflevector <2 x i64> %1049, <2 x i64> %1051, <2 x i32> <i32 0, i32 2>
  %1059 = shufflevector <2 x i64> %1049, <2 x i64> %1051, <2 x i32> <i32 1, i32 3>
  %1060 = shufflevector <2 x i64> %1043, <2 x i64> %1047, <2 x i32> <i32 0, i32 2>
  %1061 = shufflevector <2 x i64> %1043, <2 x i64> %1047, <2 x i32> <i32 1, i32 3>
  %1062 = shufflevector <2 x i64> %1053, <2 x i64> %1055, <2 x i32> <i32 0, i32 2>
  %1063 = shufflevector <2 x i64> %1053, <2 x i64> %1055, <2 x i32> <i32 1, i32 3>
  %1064 = load <8 x i16>, <8 x i16>* %858, align 16
  %1065 = load <8 x i16>, <8 x i16>* %860, align 16
  %1066 = shufflevector <8 x i16> %1064, <8 x i16> %1065, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1067 = load <8 x i16>, <8 x i16>* %862, align 16
  %1068 = load <8 x i16>, <8 x i16>* %864, align 16
  %1069 = shufflevector <8 x i16> %1067, <8 x i16> %1068, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1070 = load <8 x i16>, <8 x i16>* %866, align 16
  %1071 = load <8 x i16>, <8 x i16>* %868, align 16
  %1072 = shufflevector <8 x i16> %1070, <8 x i16> %1071, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1073 = load <8 x i16>, <8 x i16>* %870, align 16
  %1074 = load <8 x i16>, <8 x i16>* %872, align 16
  %1075 = shufflevector <8 x i16> %1073, <8 x i16> %1074, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1076 = shufflevector <8 x i16> %1064, <8 x i16> %1065, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1077 = shufflevector <8 x i16> %1067, <8 x i16> %1068, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1078 = shufflevector <8 x i16> %1070, <8 x i16> %1071, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1079 = shufflevector <8 x i16> %1073, <8 x i16> %1074, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1080 = bitcast <8 x i16> %1066 to <4 x i32>
  %1081 = bitcast <8 x i16> %1069 to <4 x i32>
  %1082 = shufflevector <4 x i32> %1080, <4 x i32> %1081, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1083 = bitcast <4 x i32> %1082 to <2 x i64>
  %1084 = bitcast <8 x i16> %1072 to <4 x i32>
  %1085 = bitcast <8 x i16> %1075 to <4 x i32>
  %1086 = shufflevector <4 x i32> %1084, <4 x i32> %1085, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1087 = bitcast <4 x i32> %1086 to <2 x i64>
  %1088 = bitcast <8 x i16> %1076 to <4 x i32>
  %1089 = bitcast <8 x i16> %1077 to <4 x i32>
  %1090 = shufflevector <4 x i32> %1088, <4 x i32> %1089, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1091 = bitcast <4 x i32> %1090 to <2 x i64>
  %1092 = bitcast <8 x i16> %1078 to <4 x i32>
  %1093 = bitcast <8 x i16> %1079 to <4 x i32>
  %1094 = shufflevector <4 x i32> %1092, <4 x i32> %1093, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1095 = bitcast <4 x i32> %1094 to <2 x i64>
  %1096 = shufflevector <4 x i32> %1080, <4 x i32> %1081, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1097 = bitcast <4 x i32> %1096 to <2 x i64>
  %1098 = shufflevector <4 x i32> %1084, <4 x i32> %1085, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1099 = bitcast <4 x i32> %1098 to <2 x i64>
  %1100 = shufflevector <4 x i32> %1088, <4 x i32> %1089, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1101 = bitcast <4 x i32> %1100 to <2 x i64>
  %1102 = shufflevector <4 x i32> %1092, <4 x i32> %1093, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1103 = bitcast <4 x i32> %1102 to <2 x i64>
  %1104 = shufflevector <2 x i64> %1083, <2 x i64> %1087, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1104, <2 x i64>* %763, align 16
  %1105 = shufflevector <2 x i64> %1083, <2 x i64> %1087, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1105, <2 x i64>* %879, align 16
  %1106 = shufflevector <2 x i64> %1097, <2 x i64> %1099, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1106, <2 x i64>* %883, align 16
  %1107 = shufflevector <2 x i64> %1097, <2 x i64> %1099, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1107, <2 x i64>* %887, align 16
  %1108 = shufflevector <2 x i64> %1091, <2 x i64> %1095, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1108, <2 x i64>* %891, align 16
  %1109 = shufflevector <2 x i64> %1091, <2 x i64> %1095, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1109, <2 x i64>* %895, align 16
  %1110 = shufflevector <2 x i64> %1101, <2 x i64> %1103, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1110, <2 x i64>* %899, align 16
  %1111 = shufflevector <2 x i64> %1101, <2 x i64> %1103, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1111, <2 x i64>* %903, align 16
  %1112 = load <8 x i16>, <8 x i16>* %953, align 16
  %1113 = load <8 x i16>, <8 x i16>* %955, align 16
  %1114 = shufflevector <8 x i16> %1112, <8 x i16> %1113, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1115 = load <8 x i16>, <8 x i16>* %957, align 16
  %1116 = load <8 x i16>, <8 x i16>* %959, align 16
  %1117 = shufflevector <8 x i16> %1115, <8 x i16> %1116, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1118 = load <8 x i16>, <8 x i16>* %961, align 16
  %1119 = load <8 x i16>, <8 x i16>* %963, align 16
  %1120 = shufflevector <8 x i16> %1118, <8 x i16> %1119, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1121 = load <8 x i16>, <8 x i16>* %965, align 16
  %1122 = load <8 x i16>, <8 x i16>* %967, align 16
  %1123 = shufflevector <8 x i16> %1121, <8 x i16> %1122, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1124 = shufflevector <8 x i16> %1112, <8 x i16> %1113, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1125 = shufflevector <8 x i16> %1115, <8 x i16> %1116, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1126 = shufflevector <8 x i16> %1118, <8 x i16> %1119, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1127 = shufflevector <8 x i16> %1121, <8 x i16> %1122, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1128 = bitcast <8 x i16> %1114 to <4 x i32>
  %1129 = bitcast <8 x i16> %1117 to <4 x i32>
  %1130 = shufflevector <4 x i32> %1128, <4 x i32> %1129, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1131 = bitcast <4 x i32> %1130 to <2 x i64>
  %1132 = bitcast <8 x i16> %1120 to <4 x i32>
  %1133 = bitcast <8 x i16> %1123 to <4 x i32>
  %1134 = shufflevector <4 x i32> %1132, <4 x i32> %1133, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1135 = bitcast <4 x i32> %1134 to <2 x i64>
  %1136 = bitcast <8 x i16> %1124 to <4 x i32>
  %1137 = bitcast <8 x i16> %1125 to <4 x i32>
  %1138 = shufflevector <4 x i32> %1136, <4 x i32> %1137, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1139 = bitcast <4 x i32> %1138 to <2 x i64>
  %1140 = bitcast <8 x i16> %1126 to <4 x i32>
  %1141 = bitcast <8 x i16> %1127 to <4 x i32>
  %1142 = shufflevector <4 x i32> %1140, <4 x i32> %1141, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1143 = bitcast <4 x i32> %1142 to <2 x i64>
  %1144 = shufflevector <4 x i32> %1128, <4 x i32> %1129, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1145 = bitcast <4 x i32> %1144 to <2 x i64>
  %1146 = shufflevector <4 x i32> %1132, <4 x i32> %1133, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1147 = bitcast <4 x i32> %1146 to <2 x i64>
  %1148 = shufflevector <4 x i32> %1136, <4 x i32> %1137, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1149 = bitcast <4 x i32> %1148 to <2 x i64>
  %1150 = shufflevector <4 x i32> %1140, <4 x i32> %1141, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1151 = bitcast <4 x i32> %1150 to <2 x i64>
  %1152 = shufflevector <2 x i64> %1131, <2 x i64> %1135, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1152, <2 x i64>* %921, align 16
  %1153 = shufflevector <2 x i64> %1131, <2 x i64> %1135, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1153, <2 x i64>* %927, align 16
  %1154 = shufflevector <2 x i64> %1145, <2 x i64> %1147, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1154, <2 x i64>* %931, align 16
  %1155 = shufflevector <2 x i64> %1145, <2 x i64> %1147, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1155, <2 x i64>* %935, align 16
  %1156 = shufflevector <2 x i64> %1139, <2 x i64> %1143, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1156, <2 x i64>* %939, align 16
  %1157 = shufflevector <2 x i64> %1139, <2 x i64> %1143, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1157, <2 x i64>* %943, align 16
  %1158 = shufflevector <2 x i64> %1149, <2 x i64> %1151, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1158, <2 x i64>* %947, align 16
  %1159 = shufflevector <2 x i64> %1149, <2 x i64> %1151, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1159, <2 x i64>* %951, align 16
  %1160 = load <8 x i16>, <8 x i16>* %808, align 16
  %1161 = load <8 x i16>, <8 x i16>* %810, align 16
  %1162 = load <8 x i16>, <8 x i16>* %812, align 16
  %1163 = load <8 x i16>, <8 x i16>* %814, align 16
  %1164 = load <8 x i16>, <8 x i16>* %816, align 16
  %1165 = load <8 x i16>, <8 x i16>* %818, align 16
  %1166 = load <8 x i16>, <8 x i16>* %820, align 16
  %1167 = load <8 x i16>, <8 x i16>* %822, align 16
  %1168 = add <8 x i16> %1160, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1169 = add <8 x i16> %1161, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1170 = add <8 x i16> %1162, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1171 = add <8 x i16> %1163, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1172 = add <8 x i16> %1164, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1173 = add <8 x i16> %1165, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1174 = add <8 x i16> %1166, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1175 = add <8 x i16> %1167, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1176 = lshr <8 x i16> %1160, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1177 = add <8 x i16> %1168, %1176
  %1178 = lshr <8 x i16> %1161, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1179 = add <8 x i16> %1169, %1178
  %1180 = lshr <8 x i16> %1162, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1181 = add <8 x i16> %1170, %1180
  %1182 = lshr <8 x i16> %1163, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1183 = add <8 x i16> %1171, %1182
  %1184 = lshr <8 x i16> %1164, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1185 = add <8 x i16> %1172, %1184
  %1186 = lshr <8 x i16> %1165, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1187 = add <8 x i16> %1173, %1186
  %1188 = lshr <8 x i16> %1166, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1189 = add <8 x i16> %1174, %1188
  %1190 = lshr <8 x i16> %1167, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1191 = add <8 x i16> %1175, %1190
  %1192 = ashr <8 x i16> %1177, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1192, <8 x i16>* %808, align 16
  %1193 = ashr <8 x i16> %1179, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1193, <8 x i16>* %810, align 16
  %1194 = ashr <8 x i16> %1181, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1194, <8 x i16>* %812, align 16
  %1195 = ashr <8 x i16> %1183, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1195, <8 x i16>* %814, align 16
  %1196 = ashr <8 x i16> %1185, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1196, <8 x i16>* %816, align 16
  %1197 = ashr <8 x i16> %1187, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1197, <8 x i16>* %818, align 16
  %1198 = ashr <8 x i16> %1189, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1198, <8 x i16>* %820, align 16
  %1199 = ashr <8 x i16> %1191, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1199, <8 x i16>* %822, align 16
  %1200 = bitcast <2 x i64> %1056 to <8 x i16>
  %1201 = ashr <8 x i16> %1200, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1202 = bitcast <2 x i64> %1057 to <8 x i16>
  %1203 = ashr <8 x i16> %1202, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1204 = bitcast <2 x i64> %1058 to <8 x i16>
  %1205 = ashr <8 x i16> %1204, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1206 = bitcast <2 x i64> %1059 to <8 x i16>
  %1207 = ashr <8 x i16> %1206, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1208 = bitcast <2 x i64> %1060 to <8 x i16>
  %1209 = ashr <8 x i16> %1208, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1210 = bitcast <2 x i64> %1061 to <8 x i16>
  %1211 = ashr <8 x i16> %1210, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1212 = bitcast <2 x i64> %1062 to <8 x i16>
  %1213 = ashr <8 x i16> %1212, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1214 = bitcast <2 x i64> %1063 to <8 x i16>
  %1215 = ashr <8 x i16> %1214, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1216 = add <8 x i16> %1200, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1217 = add <8 x i16> %1202, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1218 = add <8 x i16> %1204, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1219 = add <8 x i16> %1206, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1220 = add <8 x i16> %1208, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1221 = add <8 x i16> %1210, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1222 = add <8 x i16> %1212, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1223 = add <8 x i16> %1214, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1224 = sub <8 x i16> %1216, %1201
  %1225 = sub <8 x i16> %1217, %1203
  %1226 = sub <8 x i16> %1218, %1205
  %1227 = sub <8 x i16> %1219, %1207
  %1228 = sub <8 x i16> %1220, %1209
  %1229 = sub <8 x i16> %1221, %1211
  %1230 = sub <8 x i16> %1222, %1213
  %1231 = sub <8 x i16> %1223, %1215
  %1232 = ashr <8 x i16> %1224, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1232, <8 x i16>* %858, align 16
  %1233 = ashr <8 x i16> %1225, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1233, <8 x i16>* %860, align 16
  %1234 = ashr <8 x i16> %1226, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1234, <8 x i16>* %862, align 16
  %1235 = ashr <8 x i16> %1227, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1235, <8 x i16>* %864, align 16
  %1236 = ashr <8 x i16> %1228, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1236, <8 x i16>* %866, align 16
  %1237 = ashr <8 x i16> %1229, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1237, <8 x i16>* %868, align 16
  %1238 = ashr <8 x i16> %1230, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1238, <8 x i16>* %870, align 16
  %1239 = ashr <8 x i16> %1231, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1239, <8 x i16>* %872, align 16
  %1240 = load <8 x i16>, <8 x i16>* %905, align 16
  %1241 = ashr <8 x i16> %1240, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1242 = load <8 x i16>, <8 x i16>* %907, align 16
  %1243 = ashr <8 x i16> %1242, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1244 = load <8 x i16>, <8 x i16>* %909, align 16
  %1245 = ashr <8 x i16> %1244, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1246 = load <8 x i16>, <8 x i16>* %911, align 16
  %1247 = ashr <8 x i16> %1246, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1248 = load <8 x i16>, <8 x i16>* %913, align 16
  %1249 = ashr <8 x i16> %1248, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1250 = load <8 x i16>, <8 x i16>* %915, align 16
  %1251 = ashr <8 x i16> %1250, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1252 = load <8 x i16>, <8 x i16>* %917, align 16
  %1253 = ashr <8 x i16> %1252, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1254 = load <8 x i16>, <8 x i16>* %919, align 16
  %1255 = ashr <8 x i16> %1254, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1256 = add <8 x i16> %1240, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1257 = add <8 x i16> %1242, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1258 = add <8 x i16> %1244, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1259 = add <8 x i16> %1246, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1260 = add <8 x i16> %1248, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1261 = add <8 x i16> %1250, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1262 = add <8 x i16> %1252, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1263 = add <8 x i16> %1254, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1264 = sub <8 x i16> %1256, %1241
  %1265 = sub <8 x i16> %1257, %1243
  %1266 = sub <8 x i16> %1258, %1245
  %1267 = sub <8 x i16> %1259, %1247
  %1268 = sub <8 x i16> %1260, %1249
  %1269 = sub <8 x i16> %1261, %1251
  %1270 = sub <8 x i16> %1262, %1253
  %1271 = sub <8 x i16> %1263, %1255
  %1272 = ashr <8 x i16> %1264, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1272, <8 x i16>* %905, align 16
  %1273 = ashr <8 x i16> %1265, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1273, <8 x i16>* %907, align 16
  %1274 = ashr <8 x i16> %1266, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1274, <8 x i16>* %909, align 16
  %1275 = ashr <8 x i16> %1267, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1275, <8 x i16>* %911, align 16
  %1276 = ashr <8 x i16> %1268, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1276, <8 x i16>* %913, align 16
  %1277 = ashr <8 x i16> %1269, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1277, <8 x i16>* %915, align 16
  %1278 = ashr <8 x i16> %1270, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1278, <8 x i16>* %917, align 16
  %1279 = ashr <8 x i16> %1271, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1279, <8 x i16>* %919, align 16
  %1280 = load <8 x i16>, <8 x i16>* %953, align 16
  %1281 = load <8 x i16>, <8 x i16>* %955, align 16
  %1282 = load <8 x i16>, <8 x i16>* %957, align 16
  %1283 = load <8 x i16>, <8 x i16>* %959, align 16
  %1284 = load <8 x i16>, <8 x i16>* %961, align 16
  %1285 = load <8 x i16>, <8 x i16>* %963, align 16
  %1286 = load <8 x i16>, <8 x i16>* %965, align 16
  %1287 = load <8 x i16>, <8 x i16>* %967, align 16
  %1288 = add <8 x i16> %1280, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1289 = add <8 x i16> %1281, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1290 = add <8 x i16> %1282, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1291 = add <8 x i16> %1283, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1292 = add <8 x i16> %1284, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1293 = add <8 x i16> %1285, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1294 = add <8 x i16> %1286, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1295 = add <8 x i16> %1287, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1296 = lshr <8 x i16> %1280, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1297 = add <8 x i16> %1288, %1296
  %1298 = lshr <8 x i16> %1281, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1299 = add <8 x i16> %1289, %1298
  %1300 = lshr <8 x i16> %1282, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1301 = add <8 x i16> %1290, %1300
  %1302 = lshr <8 x i16> %1283, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1303 = add <8 x i16> %1291, %1302
  %1304 = lshr <8 x i16> %1284, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1305 = add <8 x i16> %1292, %1304
  %1306 = lshr <8 x i16> %1285, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1307 = add <8 x i16> %1293, %1306
  %1308 = lshr <8 x i16> %1286, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1309 = add <8 x i16> %1294, %1308
  %1310 = lshr <8 x i16> %1287, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1311 = add <8 x i16> %1295, %1310
  %1312 = ashr <8 x i16> %1297, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1312, <8 x i16>* %953, align 16
  %1313 = ashr <8 x i16> %1299, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1313, <8 x i16>* %955, align 16
  %1314 = ashr <8 x i16> %1301, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1314, <8 x i16>* %957, align 16
  %1315 = ashr <8 x i16> %1303, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1315, <8 x i16>* %959, align 16
  %1316 = ashr <8 x i16> %1305, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1316, <8 x i16>* %961, align 16
  %1317 = ashr <8 x i16> %1307, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1317, <8 x i16>* %963, align 16
  %1318 = ashr <8 x i16> %1309, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1318, <8 x i16>* %965, align 16
  %1319 = ashr <8 x i16> %1311, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1319, <8 x i16>* %967, align 16
  call fastcc void @fadst16_8col(<2 x i64>* nonnull %762) #6
  call fastcc void @fadst16_8col(<2 x i64>* nonnull %763) #6
  %1320 = load <8 x i16>, <8 x i16>* %808, align 16
  %1321 = load <8 x i16>, <8 x i16>* %810, align 16
  %1322 = shufflevector <8 x i16> %1320, <8 x i16> %1321, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1323 = load <8 x i16>, <8 x i16>* %812, align 16
  %1324 = load <8 x i16>, <8 x i16>* %814, align 16
  %1325 = shufflevector <8 x i16> %1323, <8 x i16> %1324, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1326 = load <8 x i16>, <8 x i16>* %816, align 16
  %1327 = load <8 x i16>, <8 x i16>* %818, align 16
  %1328 = shufflevector <8 x i16> %1326, <8 x i16> %1327, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1329 = load <8 x i16>, <8 x i16>* %820, align 16
  %1330 = load <8 x i16>, <8 x i16>* %822, align 16
  %1331 = shufflevector <8 x i16> %1329, <8 x i16> %1330, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1332 = shufflevector <8 x i16> %1320, <8 x i16> %1321, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1333 = shufflevector <8 x i16> %1323, <8 x i16> %1324, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1334 = shufflevector <8 x i16> %1326, <8 x i16> %1327, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1335 = shufflevector <8 x i16> %1329, <8 x i16> %1330, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1336 = bitcast <8 x i16> %1322 to <4 x i32>
  %1337 = bitcast <8 x i16> %1325 to <4 x i32>
  %1338 = shufflevector <4 x i32> %1336, <4 x i32> %1337, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1339 = bitcast <4 x i32> %1338 to <2 x i64>
  %1340 = bitcast <8 x i16> %1328 to <4 x i32>
  %1341 = bitcast <8 x i16> %1331 to <4 x i32>
  %1342 = shufflevector <4 x i32> %1340, <4 x i32> %1341, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1343 = bitcast <4 x i32> %1342 to <2 x i64>
  %1344 = bitcast <8 x i16> %1332 to <4 x i32>
  %1345 = bitcast <8 x i16> %1333 to <4 x i32>
  %1346 = shufflevector <4 x i32> %1344, <4 x i32> %1345, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1347 = bitcast <4 x i32> %1346 to <2 x i64>
  %1348 = bitcast <8 x i16> %1334 to <4 x i32>
  %1349 = bitcast <8 x i16> %1335 to <4 x i32>
  %1350 = shufflevector <4 x i32> %1348, <4 x i32> %1349, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1351 = bitcast <4 x i32> %1350 to <2 x i64>
  %1352 = shufflevector <4 x i32> %1336, <4 x i32> %1337, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1353 = bitcast <4 x i32> %1352 to <2 x i64>
  %1354 = shufflevector <4 x i32> %1340, <4 x i32> %1341, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1355 = bitcast <4 x i32> %1354 to <2 x i64>
  %1356 = shufflevector <4 x i32> %1344, <4 x i32> %1345, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1357 = bitcast <4 x i32> %1356 to <2 x i64>
  %1358 = shufflevector <4 x i32> %1348, <4 x i32> %1349, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1359 = bitcast <4 x i32> %1358 to <2 x i64>
  %1360 = shufflevector <2 x i64> %1339, <2 x i64> %1343, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1360, <2 x i64>* %762, align 16
  %1361 = shufflevector <2 x i64> %1339, <2 x i64> %1343, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1361, <2 x i64>* %770, align 16
  %1362 = shufflevector <2 x i64> %1353, <2 x i64> %1355, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1362, <2 x i64>* %776, align 16
  %1363 = shufflevector <2 x i64> %1353, <2 x i64> %1355, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1363, <2 x i64>* %782, align 16
  %1364 = shufflevector <2 x i64> %1347, <2 x i64> %1351, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1364, <2 x i64>* %788, align 16
  %1365 = shufflevector <2 x i64> %1347, <2 x i64> %1351, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1365, <2 x i64>* %794, align 16
  %1366 = shufflevector <2 x i64> %1357, <2 x i64> %1359, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1366, <2 x i64>* %800, align 16
  %1367 = shufflevector <2 x i64> %1357, <2 x i64> %1359, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1367, <2 x i64>* %806, align 16
  %1368 = load <8 x i16>, <8 x i16>* %905, align 16
  %1369 = load <8 x i16>, <8 x i16>* %907, align 16
  %1370 = shufflevector <8 x i16> %1368, <8 x i16> %1369, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1371 = load <8 x i16>, <8 x i16>* %909, align 16
  %1372 = load <8 x i16>, <8 x i16>* %911, align 16
  %1373 = shufflevector <8 x i16> %1371, <8 x i16> %1372, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1374 = load <8 x i16>, <8 x i16>* %913, align 16
  %1375 = load <8 x i16>, <8 x i16>* %915, align 16
  %1376 = shufflevector <8 x i16> %1374, <8 x i16> %1375, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1377 = load <8 x i16>, <8 x i16>* %917, align 16
  %1378 = load <8 x i16>, <8 x i16>* %919, align 16
  %1379 = shufflevector <8 x i16> %1377, <8 x i16> %1378, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1380 = shufflevector <8 x i16> %1368, <8 x i16> %1369, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1381 = shufflevector <8 x i16> %1371, <8 x i16> %1372, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1382 = shufflevector <8 x i16> %1374, <8 x i16> %1375, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1383 = shufflevector <8 x i16> %1377, <8 x i16> %1378, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1384 = bitcast <8 x i16> %1370 to <4 x i32>
  %1385 = bitcast <8 x i16> %1373 to <4 x i32>
  %1386 = shufflevector <4 x i32> %1384, <4 x i32> %1385, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1387 = bitcast <4 x i32> %1386 to <2 x i64>
  %1388 = bitcast <8 x i16> %1376 to <4 x i32>
  %1389 = bitcast <8 x i16> %1379 to <4 x i32>
  %1390 = shufflevector <4 x i32> %1388, <4 x i32> %1389, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1391 = bitcast <4 x i32> %1390 to <2 x i64>
  %1392 = bitcast <8 x i16> %1380 to <4 x i32>
  %1393 = bitcast <8 x i16> %1381 to <4 x i32>
  %1394 = shufflevector <4 x i32> %1392, <4 x i32> %1393, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1395 = bitcast <4 x i32> %1394 to <2 x i64>
  %1396 = bitcast <8 x i16> %1382 to <4 x i32>
  %1397 = bitcast <8 x i16> %1383 to <4 x i32>
  %1398 = shufflevector <4 x i32> %1396, <4 x i32> %1397, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1399 = bitcast <4 x i32> %1398 to <2 x i64>
  %1400 = shufflevector <4 x i32> %1384, <4 x i32> %1385, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1401 = bitcast <4 x i32> %1400 to <2 x i64>
  %1402 = shufflevector <4 x i32> %1388, <4 x i32> %1389, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1403 = bitcast <4 x i32> %1402 to <2 x i64>
  %1404 = shufflevector <4 x i32> %1392, <4 x i32> %1393, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1405 = bitcast <4 x i32> %1404 to <2 x i64>
  %1406 = shufflevector <4 x i32> %1396, <4 x i32> %1397, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1407 = bitcast <4 x i32> %1406 to <2 x i64>
  %1408 = shufflevector <2 x i64> %1387, <2 x i64> %1391, <2 x i32> <i32 0, i32 2>
  %1409 = shufflevector <2 x i64> %1387, <2 x i64> %1391, <2 x i32> <i32 1, i32 3>
  %1410 = shufflevector <2 x i64> %1401, <2 x i64> %1403, <2 x i32> <i32 0, i32 2>
  %1411 = shufflevector <2 x i64> %1401, <2 x i64> %1403, <2 x i32> <i32 1, i32 3>
  %1412 = shufflevector <2 x i64> %1395, <2 x i64> %1399, <2 x i32> <i32 0, i32 2>
  %1413 = shufflevector <2 x i64> %1395, <2 x i64> %1399, <2 x i32> <i32 1, i32 3>
  %1414 = shufflevector <2 x i64> %1405, <2 x i64> %1407, <2 x i32> <i32 0, i32 2>
  %1415 = shufflevector <2 x i64> %1405, <2 x i64> %1407, <2 x i32> <i32 1, i32 3>
  %1416 = load <8 x i16>, <8 x i16>* %858, align 16
  %1417 = load <8 x i16>, <8 x i16>* %860, align 16
  %1418 = shufflevector <8 x i16> %1416, <8 x i16> %1417, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1419 = load <8 x i16>, <8 x i16>* %862, align 16
  %1420 = load <8 x i16>, <8 x i16>* %864, align 16
  %1421 = shufflevector <8 x i16> %1419, <8 x i16> %1420, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1422 = load <8 x i16>, <8 x i16>* %866, align 16
  %1423 = load <8 x i16>, <8 x i16>* %868, align 16
  %1424 = shufflevector <8 x i16> %1422, <8 x i16> %1423, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1425 = load <8 x i16>, <8 x i16>* %870, align 16
  %1426 = load <8 x i16>, <8 x i16>* %872, align 16
  %1427 = shufflevector <8 x i16> %1425, <8 x i16> %1426, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1428 = shufflevector <8 x i16> %1416, <8 x i16> %1417, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1429 = shufflevector <8 x i16> %1419, <8 x i16> %1420, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1430 = shufflevector <8 x i16> %1422, <8 x i16> %1423, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1431 = shufflevector <8 x i16> %1425, <8 x i16> %1426, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1432 = bitcast <8 x i16> %1418 to <4 x i32>
  %1433 = bitcast <8 x i16> %1421 to <4 x i32>
  %1434 = shufflevector <4 x i32> %1432, <4 x i32> %1433, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1435 = bitcast <4 x i32> %1434 to <2 x i64>
  %1436 = bitcast <8 x i16> %1424 to <4 x i32>
  %1437 = bitcast <8 x i16> %1427 to <4 x i32>
  %1438 = shufflevector <4 x i32> %1436, <4 x i32> %1437, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1439 = bitcast <4 x i32> %1438 to <2 x i64>
  %1440 = bitcast <8 x i16> %1428 to <4 x i32>
  %1441 = bitcast <8 x i16> %1429 to <4 x i32>
  %1442 = shufflevector <4 x i32> %1440, <4 x i32> %1441, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1443 = bitcast <4 x i32> %1442 to <2 x i64>
  %1444 = bitcast <8 x i16> %1430 to <4 x i32>
  %1445 = bitcast <8 x i16> %1431 to <4 x i32>
  %1446 = shufflevector <4 x i32> %1444, <4 x i32> %1445, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1447 = bitcast <4 x i32> %1446 to <2 x i64>
  %1448 = shufflevector <4 x i32> %1432, <4 x i32> %1433, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1449 = bitcast <4 x i32> %1448 to <2 x i64>
  %1450 = shufflevector <4 x i32> %1436, <4 x i32> %1437, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1451 = bitcast <4 x i32> %1450 to <2 x i64>
  %1452 = shufflevector <4 x i32> %1440, <4 x i32> %1441, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1453 = bitcast <4 x i32> %1452 to <2 x i64>
  %1454 = shufflevector <4 x i32> %1444, <4 x i32> %1445, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1455 = bitcast <4 x i32> %1454 to <2 x i64>
  %1456 = shufflevector <2 x i64> %1435, <2 x i64> %1439, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1456, <2 x i64>* %763, align 16
  %1457 = shufflevector <2 x i64> %1435, <2 x i64> %1439, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1457, <2 x i64>* %879, align 16
  %1458 = shufflevector <2 x i64> %1449, <2 x i64> %1451, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1458, <2 x i64>* %883, align 16
  %1459 = shufflevector <2 x i64> %1449, <2 x i64> %1451, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1459, <2 x i64>* %887, align 16
  %1460 = shufflevector <2 x i64> %1443, <2 x i64> %1447, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1460, <2 x i64>* %891, align 16
  %1461 = shufflevector <2 x i64> %1443, <2 x i64> %1447, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1461, <2 x i64>* %895, align 16
  %1462 = shufflevector <2 x i64> %1453, <2 x i64> %1455, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1462, <2 x i64>* %899, align 16
  %1463 = shufflevector <2 x i64> %1453, <2 x i64> %1455, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1463, <2 x i64>* %903, align 16
  %1464 = load <8 x i16>, <8 x i16>* %953, align 16
  %1465 = load <8 x i16>, <8 x i16>* %955, align 16
  %1466 = shufflevector <8 x i16> %1464, <8 x i16> %1465, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1467 = load <8 x i16>, <8 x i16>* %957, align 16
  %1468 = load <8 x i16>, <8 x i16>* %959, align 16
  %1469 = shufflevector <8 x i16> %1467, <8 x i16> %1468, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1470 = load <8 x i16>, <8 x i16>* %961, align 16
  %1471 = load <8 x i16>, <8 x i16>* %963, align 16
  %1472 = shufflevector <8 x i16> %1470, <8 x i16> %1471, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1473 = load <8 x i16>, <8 x i16>* %965, align 16
  %1474 = load <8 x i16>, <8 x i16>* %967, align 16
  %1475 = shufflevector <8 x i16> %1473, <8 x i16> %1474, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1476 = shufflevector <8 x i16> %1464, <8 x i16> %1465, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1477 = shufflevector <8 x i16> %1467, <8 x i16> %1468, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1478 = shufflevector <8 x i16> %1470, <8 x i16> %1471, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1479 = shufflevector <8 x i16> %1473, <8 x i16> %1474, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1480 = bitcast <8 x i16> %1466 to <4 x i32>
  %1481 = bitcast <8 x i16> %1469 to <4 x i32>
  %1482 = shufflevector <4 x i32> %1480, <4 x i32> %1481, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1483 = bitcast <4 x i32> %1482 to <2 x i64>
  %1484 = bitcast <8 x i16> %1472 to <4 x i32>
  %1485 = bitcast <8 x i16> %1475 to <4 x i32>
  %1486 = shufflevector <4 x i32> %1484, <4 x i32> %1485, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1487 = bitcast <4 x i32> %1486 to <2 x i64>
  %1488 = bitcast <8 x i16> %1476 to <4 x i32>
  %1489 = bitcast <8 x i16> %1477 to <4 x i32>
  %1490 = shufflevector <4 x i32> %1488, <4 x i32> %1489, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1491 = bitcast <4 x i32> %1490 to <2 x i64>
  %1492 = bitcast <8 x i16> %1478 to <4 x i32>
  %1493 = bitcast <8 x i16> %1479 to <4 x i32>
  %1494 = shufflevector <4 x i32> %1492, <4 x i32> %1493, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1495 = bitcast <4 x i32> %1494 to <2 x i64>
  %1496 = shufflevector <4 x i32> %1480, <4 x i32> %1481, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1497 = bitcast <4 x i32> %1496 to <2 x i64>
  %1498 = shufflevector <4 x i32> %1484, <4 x i32> %1485, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1499 = bitcast <4 x i32> %1498 to <2 x i64>
  %1500 = shufflevector <4 x i32> %1488, <4 x i32> %1489, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1501 = bitcast <4 x i32> %1500 to <2 x i64>
  %1502 = shufflevector <4 x i32> %1492, <4 x i32> %1493, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1503 = bitcast <4 x i32> %1502 to <2 x i64>
  %1504 = shufflevector <2 x i64> %1483, <2 x i64> %1487, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1504, <2 x i64>* %921, align 16
  %1505 = shufflevector <2 x i64> %1483, <2 x i64> %1487, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1505, <2 x i64>* %927, align 16
  %1506 = shufflevector <2 x i64> %1497, <2 x i64> %1499, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1506, <2 x i64>* %931, align 16
  %1507 = shufflevector <2 x i64> %1497, <2 x i64> %1499, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1507, <2 x i64>* %935, align 16
  %1508 = shufflevector <2 x i64> %1491, <2 x i64> %1495, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1508, <2 x i64>* %939, align 16
  %1509 = shufflevector <2 x i64> %1491, <2 x i64> %1495, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1509, <2 x i64>* %943, align 16
  %1510 = shufflevector <2 x i64> %1501, <2 x i64> %1503, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1510, <2 x i64>* %947, align 16
  %1511 = shufflevector <2 x i64> %1501, <2 x i64> %1503, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1511, <2 x i64>* %951, align 16
  store <2 x i64> %1408, <2 x i64>* %826, align 16
  store <2 x i64> %1409, <2 x i64>* %832, align 16
  store <2 x i64> %1410, <2 x i64>* %836, align 16
  store <2 x i64> %1411, <2 x i64>* %840, align 16
  store <2 x i64> %1412, <2 x i64>* %844, align 16
  store <2 x i64> %1413, <2 x i64>* %848, align 16
  store <2 x i64> %1414, <2 x i64>* %852, align 16
  store <2 x i64> %1415, <2 x i64>* %856, align 16
  call fastcc void @write_buffer_16x16(i32* %1, <2 x i64>* nonnull %762, <2 x i64>* nonnull %763)
  br label %2263

1512:                                             ; preds = %4
  %1513 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 0
  %1514 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 0
  %1515 = bitcast i16* %0 to <8 x i16>*
  %1516 = load <8 x i16>, <8 x i16>* %1515, align 16
  %1517 = sext i32 %2 to i64
  %1518 = getelementptr inbounds i16, i16* %0, i64 %1517
  %1519 = bitcast i16* %1518 to <8 x i16>*
  %1520 = load <8 x i16>, <8 x i16>* %1519, align 16
  %1521 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 1
  %1522 = shl nsw i32 %2, 1
  %1523 = sext i32 %1522 to i64
  %1524 = getelementptr inbounds i16, i16* %0, i64 %1523
  %1525 = bitcast i16* %1524 to <8 x i16>*
  %1526 = load <8 x i16>, <8 x i16>* %1525, align 16
  %1527 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 2
  %1528 = mul nsw i32 %2, 3
  %1529 = sext i32 %1528 to i64
  %1530 = getelementptr inbounds i16, i16* %0, i64 %1529
  %1531 = bitcast i16* %1530 to <8 x i16>*
  %1532 = load <8 x i16>, <8 x i16>* %1531, align 16
  %1533 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 3
  %1534 = shl nsw i32 %2, 2
  %1535 = sext i32 %1534 to i64
  %1536 = getelementptr inbounds i16, i16* %0, i64 %1535
  %1537 = bitcast i16* %1536 to <8 x i16>*
  %1538 = load <8 x i16>, <8 x i16>* %1537, align 16
  %1539 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 4
  %1540 = mul nsw i32 %2, 5
  %1541 = sext i32 %1540 to i64
  %1542 = getelementptr inbounds i16, i16* %0, i64 %1541
  %1543 = bitcast i16* %1542 to <8 x i16>*
  %1544 = load <8 x i16>, <8 x i16>* %1543, align 16
  %1545 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 5
  %1546 = mul nsw i32 %2, 6
  %1547 = sext i32 %1546 to i64
  %1548 = getelementptr inbounds i16, i16* %0, i64 %1547
  %1549 = bitcast i16* %1548 to <8 x i16>*
  %1550 = load <8 x i16>, <8 x i16>* %1549, align 16
  %1551 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 6
  %1552 = mul nsw i32 %2, 7
  %1553 = sext i32 %1552 to i64
  %1554 = getelementptr inbounds i16, i16* %0, i64 %1553
  %1555 = bitcast i16* %1554 to <8 x i16>*
  %1556 = load <8 x i16>, <8 x i16>* %1555, align 16
  %1557 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 7
  %1558 = shl <8 x i16> %1516, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1559 = bitcast [16 x <2 x i64>]* %5 to <8 x i16>*
  store <8 x i16> %1558, <8 x i16>* %1559, align 16
  %1560 = shl <8 x i16> %1520, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1561 = bitcast <2 x i64>* %1521 to <8 x i16>*
  store <8 x i16> %1560, <8 x i16>* %1561, align 16
  %1562 = shl <8 x i16> %1526, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1563 = bitcast <2 x i64>* %1527 to <8 x i16>*
  store <8 x i16> %1562, <8 x i16>* %1563, align 16
  %1564 = shl <8 x i16> %1532, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1565 = bitcast <2 x i64>* %1533 to <8 x i16>*
  store <8 x i16> %1564, <8 x i16>* %1565, align 16
  %1566 = shl <8 x i16> %1538, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1567 = bitcast <2 x i64>* %1539 to <8 x i16>*
  store <8 x i16> %1566, <8 x i16>* %1567, align 16
  %1568 = shl <8 x i16> %1544, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1569 = bitcast <2 x i64>* %1545 to <8 x i16>*
  store <8 x i16> %1568, <8 x i16>* %1569, align 16
  %1570 = shl <8 x i16> %1550, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1571 = bitcast <2 x i64>* %1551 to <8 x i16>*
  store <8 x i16> %1570, <8 x i16>* %1571, align 16
  %1572 = shl <8 x i16> %1556, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1573 = bitcast <2 x i64>* %1557 to <8 x i16>*
  store <8 x i16> %1572, <8 x i16>* %1573, align 16
  %1574 = shl nsw i32 %2, 3
  %1575 = sext i32 %1574 to i64
  %1576 = getelementptr inbounds i16, i16* %0, i64 %1575
  %1577 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 8
  %1578 = bitcast i16* %1576 to <8 x i16>*
  %1579 = load <8 x i16>, <8 x i16>* %1578, align 16
  %1580 = getelementptr inbounds i16, i16* %1576, i64 %1517
  %1581 = bitcast i16* %1580 to <8 x i16>*
  %1582 = load <8 x i16>, <8 x i16>* %1581, align 16
  %1583 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 9
  %1584 = getelementptr inbounds i16, i16* %1576, i64 %1523
  %1585 = bitcast i16* %1584 to <8 x i16>*
  %1586 = load <8 x i16>, <8 x i16>* %1585, align 16
  %1587 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 10
  %1588 = getelementptr inbounds i16, i16* %1576, i64 %1529
  %1589 = bitcast i16* %1588 to <8 x i16>*
  %1590 = load <8 x i16>, <8 x i16>* %1589, align 16
  %1591 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 11
  %1592 = getelementptr inbounds i16, i16* %1576, i64 %1535
  %1593 = bitcast i16* %1592 to <8 x i16>*
  %1594 = load <8 x i16>, <8 x i16>* %1593, align 16
  %1595 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 12
  %1596 = getelementptr inbounds i16, i16* %1576, i64 %1541
  %1597 = bitcast i16* %1596 to <8 x i16>*
  %1598 = load <8 x i16>, <8 x i16>* %1597, align 16
  %1599 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 13
  %1600 = getelementptr inbounds i16, i16* %1576, i64 %1547
  %1601 = bitcast i16* %1600 to <8 x i16>*
  %1602 = load <8 x i16>, <8 x i16>* %1601, align 16
  %1603 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 14
  %1604 = getelementptr inbounds i16, i16* %1576, i64 %1553
  %1605 = bitcast i16* %1604 to <8 x i16>*
  %1606 = load <8 x i16>, <8 x i16>* %1605, align 16
  %1607 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 15
  %1608 = shl <8 x i16> %1579, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1609 = bitcast <2 x i64>* %1577 to <8 x i16>*
  store <8 x i16> %1608, <8 x i16>* %1609, align 16
  %1610 = shl <8 x i16> %1582, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1611 = bitcast <2 x i64>* %1583 to <8 x i16>*
  store <8 x i16> %1610, <8 x i16>* %1611, align 16
  %1612 = shl <8 x i16> %1586, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1613 = bitcast <2 x i64>* %1587 to <8 x i16>*
  store <8 x i16> %1612, <8 x i16>* %1613, align 16
  %1614 = shl <8 x i16> %1590, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1615 = bitcast <2 x i64>* %1591 to <8 x i16>*
  store <8 x i16> %1614, <8 x i16>* %1615, align 16
  %1616 = shl <8 x i16> %1594, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1617 = bitcast <2 x i64>* %1595 to <8 x i16>*
  store <8 x i16> %1616, <8 x i16>* %1617, align 16
  %1618 = shl <8 x i16> %1598, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1619 = bitcast <2 x i64>* %1599 to <8 x i16>*
  store <8 x i16> %1618, <8 x i16>* %1619, align 16
  %1620 = shl <8 x i16> %1602, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1621 = bitcast <2 x i64>* %1603 to <8 x i16>*
  store <8 x i16> %1620, <8 x i16>* %1621, align 16
  %1622 = shl <8 x i16> %1606, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1623 = bitcast <2 x i64>* %1607 to <8 x i16>*
  store <8 x i16> %1622, <8 x i16>* %1623, align 16
  %1624 = getelementptr inbounds i16, i16* %0, i64 8
  %1625 = bitcast i16* %1624 to <8 x i16>*
  %1626 = load <8 x i16>, <8 x i16>* %1625, align 16
  %1627 = getelementptr inbounds i16, i16* %1624, i64 %1517
  %1628 = bitcast i16* %1627 to <8 x i16>*
  %1629 = load <8 x i16>, <8 x i16>* %1628, align 16
  %1630 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 1
  %1631 = getelementptr inbounds i16, i16* %1624, i64 %1523
  %1632 = bitcast i16* %1631 to <8 x i16>*
  %1633 = load <8 x i16>, <8 x i16>* %1632, align 16
  %1634 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 2
  %1635 = getelementptr inbounds i16, i16* %1624, i64 %1529
  %1636 = bitcast i16* %1635 to <8 x i16>*
  %1637 = load <8 x i16>, <8 x i16>* %1636, align 16
  %1638 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 3
  %1639 = getelementptr inbounds i16, i16* %1624, i64 %1535
  %1640 = bitcast i16* %1639 to <8 x i16>*
  %1641 = load <8 x i16>, <8 x i16>* %1640, align 16
  %1642 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 4
  %1643 = getelementptr inbounds i16, i16* %1624, i64 %1541
  %1644 = bitcast i16* %1643 to <8 x i16>*
  %1645 = load <8 x i16>, <8 x i16>* %1644, align 16
  %1646 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 5
  %1647 = getelementptr inbounds i16, i16* %1624, i64 %1547
  %1648 = bitcast i16* %1647 to <8 x i16>*
  %1649 = load <8 x i16>, <8 x i16>* %1648, align 16
  %1650 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 6
  %1651 = getelementptr inbounds i16, i16* %1624, i64 %1553
  %1652 = bitcast i16* %1651 to <8 x i16>*
  %1653 = load <8 x i16>, <8 x i16>* %1652, align 16
  %1654 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 7
  %1655 = shl <8 x i16> %1626, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1656 = bitcast [16 x <2 x i64>]* %6 to <8 x i16>*
  store <8 x i16> %1655, <8 x i16>* %1656, align 16
  %1657 = shl <8 x i16> %1629, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1658 = bitcast <2 x i64>* %1630 to <8 x i16>*
  store <8 x i16> %1657, <8 x i16>* %1658, align 16
  %1659 = shl <8 x i16> %1633, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1660 = bitcast <2 x i64>* %1634 to <8 x i16>*
  store <8 x i16> %1659, <8 x i16>* %1660, align 16
  %1661 = shl <8 x i16> %1637, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1662 = bitcast <2 x i64>* %1638 to <8 x i16>*
  store <8 x i16> %1661, <8 x i16>* %1662, align 16
  %1663 = shl <8 x i16> %1641, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1664 = bitcast <2 x i64>* %1642 to <8 x i16>*
  store <8 x i16> %1663, <8 x i16>* %1664, align 16
  %1665 = shl <8 x i16> %1645, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1666 = bitcast <2 x i64>* %1646 to <8 x i16>*
  store <8 x i16> %1665, <8 x i16>* %1666, align 16
  %1667 = shl <8 x i16> %1649, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1668 = bitcast <2 x i64>* %1650 to <8 x i16>*
  store <8 x i16> %1667, <8 x i16>* %1668, align 16
  %1669 = shl <8 x i16> %1653, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1670 = bitcast <2 x i64>* %1654 to <8 x i16>*
  store <8 x i16> %1669, <8 x i16>* %1670, align 16
  %1671 = getelementptr inbounds i16, i16* %1624, i64 %1575
  %1672 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 8
  %1673 = bitcast i16* %1671 to <8 x i16>*
  %1674 = load <8 x i16>, <8 x i16>* %1673, align 16
  %1675 = getelementptr inbounds i16, i16* %1671, i64 %1517
  %1676 = bitcast i16* %1675 to <8 x i16>*
  %1677 = load <8 x i16>, <8 x i16>* %1676, align 16
  %1678 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 9
  %1679 = getelementptr inbounds i16, i16* %1671, i64 %1523
  %1680 = bitcast i16* %1679 to <8 x i16>*
  %1681 = load <8 x i16>, <8 x i16>* %1680, align 16
  %1682 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 10
  %1683 = getelementptr inbounds i16, i16* %1671, i64 %1529
  %1684 = bitcast i16* %1683 to <8 x i16>*
  %1685 = load <8 x i16>, <8 x i16>* %1684, align 16
  %1686 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 11
  %1687 = getelementptr inbounds i16, i16* %1671, i64 %1535
  %1688 = bitcast i16* %1687 to <8 x i16>*
  %1689 = load <8 x i16>, <8 x i16>* %1688, align 16
  %1690 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 12
  %1691 = getelementptr inbounds i16, i16* %1671, i64 %1541
  %1692 = bitcast i16* %1691 to <8 x i16>*
  %1693 = load <8 x i16>, <8 x i16>* %1692, align 16
  %1694 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 13
  %1695 = getelementptr inbounds i16, i16* %1671, i64 %1547
  %1696 = bitcast i16* %1695 to <8 x i16>*
  %1697 = load <8 x i16>, <8 x i16>* %1696, align 16
  %1698 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 14
  %1699 = getelementptr inbounds i16, i16* %1671, i64 %1553
  %1700 = bitcast i16* %1699 to <8 x i16>*
  %1701 = load <8 x i16>, <8 x i16>* %1700, align 16
  %1702 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %6, i64 0, i64 15
  %1703 = shl <8 x i16> %1674, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1704 = bitcast <2 x i64>* %1672 to <8 x i16>*
  store <8 x i16> %1703, <8 x i16>* %1704, align 16
  %1705 = shl <8 x i16> %1677, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1706 = bitcast <2 x i64>* %1678 to <8 x i16>*
  store <8 x i16> %1705, <8 x i16>* %1706, align 16
  %1707 = shl <8 x i16> %1681, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1708 = bitcast <2 x i64>* %1682 to <8 x i16>*
  store <8 x i16> %1707, <8 x i16>* %1708, align 16
  %1709 = shl <8 x i16> %1685, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1710 = bitcast <2 x i64>* %1686 to <8 x i16>*
  store <8 x i16> %1709, <8 x i16>* %1710, align 16
  %1711 = shl <8 x i16> %1689, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1712 = bitcast <2 x i64>* %1690 to <8 x i16>*
  store <8 x i16> %1711, <8 x i16>* %1712, align 16
  %1713 = shl <8 x i16> %1693, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1714 = bitcast <2 x i64>* %1694 to <8 x i16>*
  store <8 x i16> %1713, <8 x i16>* %1714, align 16
  %1715 = shl <8 x i16> %1697, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1716 = bitcast <2 x i64>* %1698 to <8 x i16>*
  store <8 x i16> %1715, <8 x i16>* %1716, align 16
  %1717 = shl <8 x i16> %1701, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  %1718 = bitcast <2 x i64>* %1702 to <8 x i16>*
  store <8 x i16> %1717, <8 x i16>* %1718, align 16
  call fastcc void @fadst16_8col(<2 x i64>* nonnull %1513) #6
  call fastcc void @fadst16_8col(<2 x i64>* nonnull %1514) #6
  %1719 = load <8 x i16>, <8 x i16>* %1559, align 16
  %1720 = load <8 x i16>, <8 x i16>* %1561, align 16
  %1721 = shufflevector <8 x i16> %1719, <8 x i16> %1720, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1722 = load <8 x i16>, <8 x i16>* %1563, align 16
  %1723 = load <8 x i16>, <8 x i16>* %1565, align 16
  %1724 = shufflevector <8 x i16> %1722, <8 x i16> %1723, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1725 = load <8 x i16>, <8 x i16>* %1567, align 16
  %1726 = load <8 x i16>, <8 x i16>* %1569, align 16
  %1727 = shufflevector <8 x i16> %1725, <8 x i16> %1726, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1728 = load <8 x i16>, <8 x i16>* %1571, align 16
  %1729 = load <8 x i16>, <8 x i16>* %1573, align 16
  %1730 = shufflevector <8 x i16> %1728, <8 x i16> %1729, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1731 = shufflevector <8 x i16> %1719, <8 x i16> %1720, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1732 = shufflevector <8 x i16> %1722, <8 x i16> %1723, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1733 = shufflevector <8 x i16> %1725, <8 x i16> %1726, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1734 = shufflevector <8 x i16> %1728, <8 x i16> %1729, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1735 = bitcast <8 x i16> %1721 to <4 x i32>
  %1736 = bitcast <8 x i16> %1724 to <4 x i32>
  %1737 = shufflevector <4 x i32> %1735, <4 x i32> %1736, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1738 = bitcast <4 x i32> %1737 to <2 x i64>
  %1739 = bitcast <8 x i16> %1727 to <4 x i32>
  %1740 = bitcast <8 x i16> %1730 to <4 x i32>
  %1741 = shufflevector <4 x i32> %1739, <4 x i32> %1740, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1742 = bitcast <4 x i32> %1741 to <2 x i64>
  %1743 = bitcast <8 x i16> %1731 to <4 x i32>
  %1744 = bitcast <8 x i16> %1732 to <4 x i32>
  %1745 = shufflevector <4 x i32> %1743, <4 x i32> %1744, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1746 = bitcast <4 x i32> %1745 to <2 x i64>
  %1747 = bitcast <8 x i16> %1733 to <4 x i32>
  %1748 = bitcast <8 x i16> %1734 to <4 x i32>
  %1749 = shufflevector <4 x i32> %1747, <4 x i32> %1748, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1750 = bitcast <4 x i32> %1749 to <2 x i64>
  %1751 = shufflevector <4 x i32> %1735, <4 x i32> %1736, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1752 = bitcast <4 x i32> %1751 to <2 x i64>
  %1753 = shufflevector <4 x i32> %1739, <4 x i32> %1740, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1754 = bitcast <4 x i32> %1753 to <2 x i64>
  %1755 = shufflevector <4 x i32> %1743, <4 x i32> %1744, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1756 = bitcast <4 x i32> %1755 to <2 x i64>
  %1757 = shufflevector <4 x i32> %1747, <4 x i32> %1748, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1758 = bitcast <4 x i32> %1757 to <2 x i64>
  %1759 = shufflevector <2 x i64> %1738, <2 x i64> %1742, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1759, <2 x i64>* %1513, align 16
  %1760 = shufflevector <2 x i64> %1738, <2 x i64> %1742, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1760, <2 x i64>* %1521, align 16
  %1761 = shufflevector <2 x i64> %1752, <2 x i64> %1754, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1761, <2 x i64>* %1527, align 16
  %1762 = shufflevector <2 x i64> %1752, <2 x i64> %1754, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1762, <2 x i64>* %1533, align 16
  %1763 = shufflevector <2 x i64> %1746, <2 x i64> %1750, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1763, <2 x i64>* %1539, align 16
  %1764 = shufflevector <2 x i64> %1746, <2 x i64> %1750, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1764, <2 x i64>* %1545, align 16
  %1765 = shufflevector <2 x i64> %1756, <2 x i64> %1758, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1765, <2 x i64>* %1551, align 16
  %1766 = shufflevector <2 x i64> %1756, <2 x i64> %1758, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1766, <2 x i64>* %1557, align 16
  %1767 = load <8 x i16>, <8 x i16>* %1656, align 16
  %1768 = load <8 x i16>, <8 x i16>* %1658, align 16
  %1769 = shufflevector <8 x i16> %1767, <8 x i16> %1768, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1770 = load <8 x i16>, <8 x i16>* %1660, align 16
  %1771 = load <8 x i16>, <8 x i16>* %1662, align 16
  %1772 = shufflevector <8 x i16> %1770, <8 x i16> %1771, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1773 = load <8 x i16>, <8 x i16>* %1664, align 16
  %1774 = load <8 x i16>, <8 x i16>* %1666, align 16
  %1775 = shufflevector <8 x i16> %1773, <8 x i16> %1774, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1776 = load <8 x i16>, <8 x i16>* %1668, align 16
  %1777 = load <8 x i16>, <8 x i16>* %1670, align 16
  %1778 = shufflevector <8 x i16> %1776, <8 x i16> %1777, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1779 = shufflevector <8 x i16> %1767, <8 x i16> %1768, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1780 = shufflevector <8 x i16> %1770, <8 x i16> %1771, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1781 = shufflevector <8 x i16> %1773, <8 x i16> %1774, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1782 = shufflevector <8 x i16> %1776, <8 x i16> %1777, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1783 = bitcast <8 x i16> %1769 to <4 x i32>
  %1784 = bitcast <8 x i16> %1772 to <4 x i32>
  %1785 = shufflevector <4 x i32> %1783, <4 x i32> %1784, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1786 = bitcast <4 x i32> %1785 to <2 x i64>
  %1787 = bitcast <8 x i16> %1775 to <4 x i32>
  %1788 = bitcast <8 x i16> %1778 to <4 x i32>
  %1789 = shufflevector <4 x i32> %1787, <4 x i32> %1788, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1790 = bitcast <4 x i32> %1789 to <2 x i64>
  %1791 = bitcast <8 x i16> %1779 to <4 x i32>
  %1792 = bitcast <8 x i16> %1780 to <4 x i32>
  %1793 = shufflevector <4 x i32> %1791, <4 x i32> %1792, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1794 = bitcast <4 x i32> %1793 to <2 x i64>
  %1795 = bitcast <8 x i16> %1781 to <4 x i32>
  %1796 = bitcast <8 x i16> %1782 to <4 x i32>
  %1797 = shufflevector <4 x i32> %1795, <4 x i32> %1796, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1798 = bitcast <4 x i32> %1797 to <2 x i64>
  %1799 = shufflevector <4 x i32> %1783, <4 x i32> %1784, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1800 = bitcast <4 x i32> %1799 to <2 x i64>
  %1801 = shufflevector <4 x i32> %1787, <4 x i32> %1788, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1802 = bitcast <4 x i32> %1801 to <2 x i64>
  %1803 = shufflevector <4 x i32> %1791, <4 x i32> %1792, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1804 = bitcast <4 x i32> %1803 to <2 x i64>
  %1805 = shufflevector <4 x i32> %1795, <4 x i32> %1796, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1806 = bitcast <4 x i32> %1805 to <2 x i64>
  %1807 = shufflevector <2 x i64> %1786, <2 x i64> %1790, <2 x i32> <i32 0, i32 2>
  %1808 = shufflevector <2 x i64> %1786, <2 x i64> %1790, <2 x i32> <i32 1, i32 3>
  %1809 = shufflevector <2 x i64> %1800, <2 x i64> %1802, <2 x i32> <i32 0, i32 2>
  %1810 = shufflevector <2 x i64> %1800, <2 x i64> %1802, <2 x i32> <i32 1, i32 3>
  %1811 = shufflevector <2 x i64> %1794, <2 x i64> %1798, <2 x i32> <i32 0, i32 2>
  %1812 = shufflevector <2 x i64> %1794, <2 x i64> %1798, <2 x i32> <i32 1, i32 3>
  %1813 = shufflevector <2 x i64> %1804, <2 x i64> %1806, <2 x i32> <i32 0, i32 2>
  %1814 = shufflevector <2 x i64> %1804, <2 x i64> %1806, <2 x i32> <i32 1, i32 3>
  %1815 = load <8 x i16>, <8 x i16>* %1609, align 16
  %1816 = load <8 x i16>, <8 x i16>* %1611, align 16
  %1817 = shufflevector <8 x i16> %1815, <8 x i16> %1816, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1818 = load <8 x i16>, <8 x i16>* %1613, align 16
  %1819 = load <8 x i16>, <8 x i16>* %1615, align 16
  %1820 = shufflevector <8 x i16> %1818, <8 x i16> %1819, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1821 = load <8 x i16>, <8 x i16>* %1617, align 16
  %1822 = load <8 x i16>, <8 x i16>* %1619, align 16
  %1823 = shufflevector <8 x i16> %1821, <8 x i16> %1822, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1824 = load <8 x i16>, <8 x i16>* %1621, align 16
  %1825 = load <8 x i16>, <8 x i16>* %1623, align 16
  %1826 = shufflevector <8 x i16> %1824, <8 x i16> %1825, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1827 = shufflevector <8 x i16> %1815, <8 x i16> %1816, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1828 = shufflevector <8 x i16> %1818, <8 x i16> %1819, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1829 = shufflevector <8 x i16> %1821, <8 x i16> %1822, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1830 = shufflevector <8 x i16> %1824, <8 x i16> %1825, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1831 = bitcast <8 x i16> %1817 to <4 x i32>
  %1832 = bitcast <8 x i16> %1820 to <4 x i32>
  %1833 = shufflevector <4 x i32> %1831, <4 x i32> %1832, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1834 = bitcast <4 x i32> %1833 to <2 x i64>
  %1835 = bitcast <8 x i16> %1823 to <4 x i32>
  %1836 = bitcast <8 x i16> %1826 to <4 x i32>
  %1837 = shufflevector <4 x i32> %1835, <4 x i32> %1836, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1838 = bitcast <4 x i32> %1837 to <2 x i64>
  %1839 = bitcast <8 x i16> %1827 to <4 x i32>
  %1840 = bitcast <8 x i16> %1828 to <4 x i32>
  %1841 = shufflevector <4 x i32> %1839, <4 x i32> %1840, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1842 = bitcast <4 x i32> %1841 to <2 x i64>
  %1843 = bitcast <8 x i16> %1829 to <4 x i32>
  %1844 = bitcast <8 x i16> %1830 to <4 x i32>
  %1845 = shufflevector <4 x i32> %1843, <4 x i32> %1844, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1846 = bitcast <4 x i32> %1845 to <2 x i64>
  %1847 = shufflevector <4 x i32> %1831, <4 x i32> %1832, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1848 = bitcast <4 x i32> %1847 to <2 x i64>
  %1849 = shufflevector <4 x i32> %1835, <4 x i32> %1836, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1850 = bitcast <4 x i32> %1849 to <2 x i64>
  %1851 = shufflevector <4 x i32> %1839, <4 x i32> %1840, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1852 = bitcast <4 x i32> %1851 to <2 x i64>
  %1853 = shufflevector <4 x i32> %1843, <4 x i32> %1844, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1854 = bitcast <4 x i32> %1853 to <2 x i64>
  %1855 = shufflevector <2 x i64> %1834, <2 x i64> %1838, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1855, <2 x i64>* %1514, align 16
  %1856 = shufflevector <2 x i64> %1834, <2 x i64> %1838, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1856, <2 x i64>* %1630, align 16
  %1857 = shufflevector <2 x i64> %1848, <2 x i64> %1850, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1857, <2 x i64>* %1634, align 16
  %1858 = shufflevector <2 x i64> %1848, <2 x i64> %1850, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1858, <2 x i64>* %1638, align 16
  %1859 = shufflevector <2 x i64> %1842, <2 x i64> %1846, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1859, <2 x i64>* %1642, align 16
  %1860 = shufflevector <2 x i64> %1842, <2 x i64> %1846, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1860, <2 x i64>* %1646, align 16
  %1861 = shufflevector <2 x i64> %1852, <2 x i64> %1854, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1861, <2 x i64>* %1650, align 16
  %1862 = shufflevector <2 x i64> %1852, <2 x i64> %1854, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1862, <2 x i64>* %1654, align 16
  %1863 = load <8 x i16>, <8 x i16>* %1704, align 16
  %1864 = load <8 x i16>, <8 x i16>* %1706, align 16
  %1865 = shufflevector <8 x i16> %1863, <8 x i16> %1864, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1866 = load <8 x i16>, <8 x i16>* %1708, align 16
  %1867 = load <8 x i16>, <8 x i16>* %1710, align 16
  %1868 = shufflevector <8 x i16> %1866, <8 x i16> %1867, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1869 = load <8 x i16>, <8 x i16>* %1712, align 16
  %1870 = load <8 x i16>, <8 x i16>* %1714, align 16
  %1871 = shufflevector <8 x i16> %1869, <8 x i16> %1870, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1872 = load <8 x i16>, <8 x i16>* %1716, align 16
  %1873 = load <8 x i16>, <8 x i16>* %1718, align 16
  %1874 = shufflevector <8 x i16> %1872, <8 x i16> %1873, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %1875 = shufflevector <8 x i16> %1863, <8 x i16> %1864, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1876 = shufflevector <8 x i16> %1866, <8 x i16> %1867, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1877 = shufflevector <8 x i16> %1869, <8 x i16> %1870, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1878 = shufflevector <8 x i16> %1872, <8 x i16> %1873, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %1879 = bitcast <8 x i16> %1865 to <4 x i32>
  %1880 = bitcast <8 x i16> %1868 to <4 x i32>
  %1881 = shufflevector <4 x i32> %1879, <4 x i32> %1880, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1882 = bitcast <4 x i32> %1881 to <2 x i64>
  %1883 = bitcast <8 x i16> %1871 to <4 x i32>
  %1884 = bitcast <8 x i16> %1874 to <4 x i32>
  %1885 = shufflevector <4 x i32> %1883, <4 x i32> %1884, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1886 = bitcast <4 x i32> %1885 to <2 x i64>
  %1887 = bitcast <8 x i16> %1875 to <4 x i32>
  %1888 = bitcast <8 x i16> %1876 to <4 x i32>
  %1889 = shufflevector <4 x i32> %1887, <4 x i32> %1888, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1890 = bitcast <4 x i32> %1889 to <2 x i64>
  %1891 = bitcast <8 x i16> %1877 to <4 x i32>
  %1892 = bitcast <8 x i16> %1878 to <4 x i32>
  %1893 = shufflevector <4 x i32> %1891, <4 x i32> %1892, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %1894 = bitcast <4 x i32> %1893 to <2 x i64>
  %1895 = shufflevector <4 x i32> %1879, <4 x i32> %1880, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1896 = bitcast <4 x i32> %1895 to <2 x i64>
  %1897 = shufflevector <4 x i32> %1883, <4 x i32> %1884, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1898 = bitcast <4 x i32> %1897 to <2 x i64>
  %1899 = shufflevector <4 x i32> %1887, <4 x i32> %1888, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1900 = bitcast <4 x i32> %1899 to <2 x i64>
  %1901 = shufflevector <4 x i32> %1891, <4 x i32> %1892, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %1902 = bitcast <4 x i32> %1901 to <2 x i64>
  %1903 = shufflevector <2 x i64> %1882, <2 x i64> %1886, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1903, <2 x i64>* %1672, align 16
  %1904 = shufflevector <2 x i64> %1882, <2 x i64> %1886, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1904, <2 x i64>* %1678, align 16
  %1905 = shufflevector <2 x i64> %1896, <2 x i64> %1898, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1905, <2 x i64>* %1682, align 16
  %1906 = shufflevector <2 x i64> %1896, <2 x i64> %1898, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1906, <2 x i64>* %1686, align 16
  %1907 = shufflevector <2 x i64> %1890, <2 x i64> %1894, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1907, <2 x i64>* %1690, align 16
  %1908 = shufflevector <2 x i64> %1890, <2 x i64> %1894, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1908, <2 x i64>* %1694, align 16
  %1909 = shufflevector <2 x i64> %1900, <2 x i64> %1902, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1909, <2 x i64>* %1698, align 16
  %1910 = shufflevector <2 x i64> %1900, <2 x i64> %1902, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %1910, <2 x i64>* %1702, align 16
  %1911 = load <8 x i16>, <8 x i16>* %1559, align 16
  %1912 = load <8 x i16>, <8 x i16>* %1561, align 16
  %1913 = load <8 x i16>, <8 x i16>* %1563, align 16
  %1914 = load <8 x i16>, <8 x i16>* %1565, align 16
  %1915 = load <8 x i16>, <8 x i16>* %1567, align 16
  %1916 = load <8 x i16>, <8 x i16>* %1569, align 16
  %1917 = load <8 x i16>, <8 x i16>* %1571, align 16
  %1918 = load <8 x i16>, <8 x i16>* %1573, align 16
  %1919 = add <8 x i16> %1911, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1920 = add <8 x i16> %1912, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1921 = add <8 x i16> %1913, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1922 = add <8 x i16> %1914, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1923 = add <8 x i16> %1915, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1924 = add <8 x i16> %1916, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1925 = add <8 x i16> %1917, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1926 = add <8 x i16> %1918, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1927 = lshr <8 x i16> %1911, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1928 = add <8 x i16> %1919, %1927
  %1929 = lshr <8 x i16> %1912, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1930 = add <8 x i16> %1920, %1929
  %1931 = lshr <8 x i16> %1913, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1932 = add <8 x i16> %1921, %1931
  %1933 = lshr <8 x i16> %1914, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1934 = add <8 x i16> %1922, %1933
  %1935 = lshr <8 x i16> %1915, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1936 = add <8 x i16> %1923, %1935
  %1937 = lshr <8 x i16> %1916, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1938 = add <8 x i16> %1924, %1937
  %1939 = lshr <8 x i16> %1917, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1940 = add <8 x i16> %1925, %1939
  %1941 = lshr <8 x i16> %1918, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1942 = add <8 x i16> %1926, %1941
  %1943 = ashr <8 x i16> %1928, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1943, <8 x i16>* %1559, align 16
  %1944 = ashr <8 x i16> %1930, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1944, <8 x i16>* %1561, align 16
  %1945 = ashr <8 x i16> %1932, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1945, <8 x i16>* %1563, align 16
  %1946 = ashr <8 x i16> %1934, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1946, <8 x i16>* %1565, align 16
  %1947 = ashr <8 x i16> %1936, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1947, <8 x i16>* %1567, align 16
  %1948 = ashr <8 x i16> %1938, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1948, <8 x i16>* %1569, align 16
  %1949 = ashr <8 x i16> %1940, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1949, <8 x i16>* %1571, align 16
  %1950 = ashr <8 x i16> %1942, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1950, <8 x i16>* %1573, align 16
  %1951 = bitcast <2 x i64> %1807 to <8 x i16>
  %1952 = ashr <8 x i16> %1951, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1953 = bitcast <2 x i64> %1808 to <8 x i16>
  %1954 = ashr <8 x i16> %1953, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1955 = bitcast <2 x i64> %1809 to <8 x i16>
  %1956 = ashr <8 x i16> %1955, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1957 = bitcast <2 x i64> %1810 to <8 x i16>
  %1958 = ashr <8 x i16> %1957, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1959 = bitcast <2 x i64> %1811 to <8 x i16>
  %1960 = ashr <8 x i16> %1959, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1961 = bitcast <2 x i64> %1812 to <8 x i16>
  %1962 = ashr <8 x i16> %1961, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1963 = bitcast <2 x i64> %1813 to <8 x i16>
  %1964 = ashr <8 x i16> %1963, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1965 = bitcast <2 x i64> %1814 to <8 x i16>
  %1966 = ashr <8 x i16> %1965, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1967 = add <8 x i16> %1951, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1968 = add <8 x i16> %1953, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1969 = add <8 x i16> %1955, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1970 = add <8 x i16> %1957, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1971 = add <8 x i16> %1959, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1972 = add <8 x i16> %1961, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1973 = add <8 x i16> %1963, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1974 = add <8 x i16> %1965, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %1975 = sub <8 x i16> %1967, %1952
  %1976 = sub <8 x i16> %1968, %1954
  %1977 = sub <8 x i16> %1969, %1956
  %1978 = sub <8 x i16> %1970, %1958
  %1979 = sub <8 x i16> %1971, %1960
  %1980 = sub <8 x i16> %1972, %1962
  %1981 = sub <8 x i16> %1973, %1964
  %1982 = sub <8 x i16> %1974, %1966
  %1983 = ashr <8 x i16> %1975, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1983, <8 x i16>* %1609, align 16
  %1984 = ashr <8 x i16> %1976, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1984, <8 x i16>* %1611, align 16
  %1985 = ashr <8 x i16> %1977, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1985, <8 x i16>* %1613, align 16
  %1986 = ashr <8 x i16> %1978, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1986, <8 x i16>* %1615, align 16
  %1987 = ashr <8 x i16> %1979, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1987, <8 x i16>* %1617, align 16
  %1988 = ashr <8 x i16> %1980, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1988, <8 x i16>* %1619, align 16
  %1989 = ashr <8 x i16> %1981, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1989, <8 x i16>* %1621, align 16
  %1990 = ashr <8 x i16> %1982, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %1990, <8 x i16>* %1623, align 16
  %1991 = load <8 x i16>, <8 x i16>* %1656, align 16
  %1992 = ashr <8 x i16> %1991, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1993 = load <8 x i16>, <8 x i16>* %1658, align 16
  %1994 = ashr <8 x i16> %1993, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1995 = load <8 x i16>, <8 x i16>* %1660, align 16
  %1996 = ashr <8 x i16> %1995, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1997 = load <8 x i16>, <8 x i16>* %1662, align 16
  %1998 = ashr <8 x i16> %1997, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %1999 = load <8 x i16>, <8 x i16>* %1664, align 16
  %2000 = ashr <8 x i16> %1999, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %2001 = load <8 x i16>, <8 x i16>* %1666, align 16
  %2002 = ashr <8 x i16> %2001, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %2003 = load <8 x i16>, <8 x i16>* %1668, align 16
  %2004 = ashr <8 x i16> %2003, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %2005 = load <8 x i16>, <8 x i16>* %1670, align 16
  %2006 = ashr <8 x i16> %2005, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %2007 = add <8 x i16> %1991, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2008 = add <8 x i16> %1993, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2009 = add <8 x i16> %1995, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2010 = add <8 x i16> %1997, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2011 = add <8 x i16> %1999, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2012 = add <8 x i16> %2001, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2013 = add <8 x i16> %2003, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2014 = add <8 x i16> %2005, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2015 = sub <8 x i16> %2007, %1992
  %2016 = sub <8 x i16> %2008, %1994
  %2017 = sub <8 x i16> %2009, %1996
  %2018 = sub <8 x i16> %2010, %1998
  %2019 = sub <8 x i16> %2011, %2000
  %2020 = sub <8 x i16> %2012, %2002
  %2021 = sub <8 x i16> %2013, %2004
  %2022 = sub <8 x i16> %2014, %2006
  %2023 = ashr <8 x i16> %2015, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %2023, <8 x i16>* %1656, align 16
  %2024 = ashr <8 x i16> %2016, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %2024, <8 x i16>* %1658, align 16
  %2025 = ashr <8 x i16> %2017, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %2025, <8 x i16>* %1660, align 16
  %2026 = ashr <8 x i16> %2018, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %2026, <8 x i16>* %1662, align 16
  %2027 = ashr <8 x i16> %2019, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %2027, <8 x i16>* %1664, align 16
  %2028 = ashr <8 x i16> %2020, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %2028, <8 x i16>* %1666, align 16
  %2029 = ashr <8 x i16> %2021, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %2029, <8 x i16>* %1668, align 16
  %2030 = ashr <8 x i16> %2022, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %2030, <8 x i16>* %1670, align 16
  %2031 = load <8 x i16>, <8 x i16>* %1704, align 16
  %2032 = load <8 x i16>, <8 x i16>* %1706, align 16
  %2033 = load <8 x i16>, <8 x i16>* %1708, align 16
  %2034 = load <8 x i16>, <8 x i16>* %1710, align 16
  %2035 = load <8 x i16>, <8 x i16>* %1712, align 16
  %2036 = load <8 x i16>, <8 x i16>* %1714, align 16
  %2037 = load <8 x i16>, <8 x i16>* %1716, align 16
  %2038 = load <8 x i16>, <8 x i16>* %1718, align 16
  %2039 = add <8 x i16> %2031, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2040 = add <8 x i16> %2032, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2041 = add <8 x i16> %2033, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2042 = add <8 x i16> %2034, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2043 = add <8 x i16> %2035, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2044 = add <8 x i16> %2036, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2045 = add <8 x i16> %2037, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2046 = add <8 x i16> %2038, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %2047 = lshr <8 x i16> %2031, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %2048 = add <8 x i16> %2039, %2047
  %2049 = lshr <8 x i16> %2032, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %2050 = add <8 x i16> %2040, %2049
  %2051 = lshr <8 x i16> %2033, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %2052 = add <8 x i16> %2041, %2051
  %2053 = lshr <8 x i16> %2034, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %2054 = add <8 x i16> %2042, %2053
  %2055 = lshr <8 x i16> %2035, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %2056 = add <8 x i16> %2043, %2055
  %2057 = lshr <8 x i16> %2036, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %2058 = add <8 x i16> %2044, %2057
  %2059 = lshr <8 x i16> %2037, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %2060 = add <8 x i16> %2045, %2059
  %2061 = lshr <8 x i16> %2038, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %2062 = add <8 x i16> %2046, %2061
  %2063 = ashr <8 x i16> %2048, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %2063, <8 x i16>* %1704, align 16
  %2064 = ashr <8 x i16> %2050, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %2064, <8 x i16>* %1706, align 16
  %2065 = ashr <8 x i16> %2052, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %2065, <8 x i16>* %1708, align 16
  %2066 = ashr <8 x i16> %2054, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %2066, <8 x i16>* %1710, align 16
  %2067 = ashr <8 x i16> %2056, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %2067, <8 x i16>* %1712, align 16
  %2068 = ashr <8 x i16> %2058, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %2068, <8 x i16>* %1714, align 16
  %2069 = ashr <8 x i16> %2060, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %2069, <8 x i16>* %1716, align 16
  %2070 = ashr <8 x i16> %2062, <i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2, i16 2>
  store <8 x i16> %2070, <8 x i16>* %1718, align 16
  call fastcc void @fadst16_8col(<2 x i64>* nonnull %1513) #6
  call fastcc void @fadst16_8col(<2 x i64>* nonnull %1514) #6
  %2071 = load <8 x i16>, <8 x i16>* %1559, align 16
  %2072 = load <8 x i16>, <8 x i16>* %1561, align 16
  %2073 = shufflevector <8 x i16> %2071, <8 x i16> %2072, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2074 = load <8 x i16>, <8 x i16>* %1563, align 16
  %2075 = load <8 x i16>, <8 x i16>* %1565, align 16
  %2076 = shufflevector <8 x i16> %2074, <8 x i16> %2075, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2077 = load <8 x i16>, <8 x i16>* %1567, align 16
  %2078 = load <8 x i16>, <8 x i16>* %1569, align 16
  %2079 = shufflevector <8 x i16> %2077, <8 x i16> %2078, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2080 = load <8 x i16>, <8 x i16>* %1571, align 16
  %2081 = load <8 x i16>, <8 x i16>* %1573, align 16
  %2082 = shufflevector <8 x i16> %2080, <8 x i16> %2081, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2083 = shufflevector <8 x i16> %2071, <8 x i16> %2072, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2084 = shufflevector <8 x i16> %2074, <8 x i16> %2075, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2085 = shufflevector <8 x i16> %2077, <8 x i16> %2078, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2086 = shufflevector <8 x i16> %2080, <8 x i16> %2081, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2087 = bitcast <8 x i16> %2073 to <4 x i32>
  %2088 = bitcast <8 x i16> %2076 to <4 x i32>
  %2089 = shufflevector <4 x i32> %2087, <4 x i32> %2088, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2090 = bitcast <4 x i32> %2089 to <2 x i64>
  %2091 = bitcast <8 x i16> %2079 to <4 x i32>
  %2092 = bitcast <8 x i16> %2082 to <4 x i32>
  %2093 = shufflevector <4 x i32> %2091, <4 x i32> %2092, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2094 = bitcast <4 x i32> %2093 to <2 x i64>
  %2095 = bitcast <8 x i16> %2083 to <4 x i32>
  %2096 = bitcast <8 x i16> %2084 to <4 x i32>
  %2097 = shufflevector <4 x i32> %2095, <4 x i32> %2096, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2098 = bitcast <4 x i32> %2097 to <2 x i64>
  %2099 = bitcast <8 x i16> %2085 to <4 x i32>
  %2100 = bitcast <8 x i16> %2086 to <4 x i32>
  %2101 = shufflevector <4 x i32> %2099, <4 x i32> %2100, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2102 = bitcast <4 x i32> %2101 to <2 x i64>
  %2103 = shufflevector <4 x i32> %2087, <4 x i32> %2088, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2104 = bitcast <4 x i32> %2103 to <2 x i64>
  %2105 = shufflevector <4 x i32> %2091, <4 x i32> %2092, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2106 = bitcast <4 x i32> %2105 to <2 x i64>
  %2107 = shufflevector <4 x i32> %2095, <4 x i32> %2096, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2108 = bitcast <4 x i32> %2107 to <2 x i64>
  %2109 = shufflevector <4 x i32> %2099, <4 x i32> %2100, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2110 = bitcast <4 x i32> %2109 to <2 x i64>
  %2111 = shufflevector <2 x i64> %2090, <2 x i64> %2094, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %2111, <2 x i64>* %1513, align 16
  %2112 = shufflevector <2 x i64> %2090, <2 x i64> %2094, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %2112, <2 x i64>* %1521, align 16
  %2113 = shufflevector <2 x i64> %2104, <2 x i64> %2106, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %2113, <2 x i64>* %1527, align 16
  %2114 = shufflevector <2 x i64> %2104, <2 x i64> %2106, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %2114, <2 x i64>* %1533, align 16
  %2115 = shufflevector <2 x i64> %2098, <2 x i64> %2102, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %2115, <2 x i64>* %1539, align 16
  %2116 = shufflevector <2 x i64> %2098, <2 x i64> %2102, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %2116, <2 x i64>* %1545, align 16
  %2117 = shufflevector <2 x i64> %2108, <2 x i64> %2110, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %2117, <2 x i64>* %1551, align 16
  %2118 = shufflevector <2 x i64> %2108, <2 x i64> %2110, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %2118, <2 x i64>* %1557, align 16
  %2119 = load <8 x i16>, <8 x i16>* %1656, align 16
  %2120 = load <8 x i16>, <8 x i16>* %1658, align 16
  %2121 = shufflevector <8 x i16> %2119, <8 x i16> %2120, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2122 = load <8 x i16>, <8 x i16>* %1660, align 16
  %2123 = load <8 x i16>, <8 x i16>* %1662, align 16
  %2124 = shufflevector <8 x i16> %2122, <8 x i16> %2123, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2125 = load <8 x i16>, <8 x i16>* %1664, align 16
  %2126 = load <8 x i16>, <8 x i16>* %1666, align 16
  %2127 = shufflevector <8 x i16> %2125, <8 x i16> %2126, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2128 = load <8 x i16>, <8 x i16>* %1668, align 16
  %2129 = load <8 x i16>, <8 x i16>* %1670, align 16
  %2130 = shufflevector <8 x i16> %2128, <8 x i16> %2129, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2131 = shufflevector <8 x i16> %2119, <8 x i16> %2120, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2132 = shufflevector <8 x i16> %2122, <8 x i16> %2123, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2133 = shufflevector <8 x i16> %2125, <8 x i16> %2126, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2134 = shufflevector <8 x i16> %2128, <8 x i16> %2129, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2135 = bitcast <8 x i16> %2121 to <4 x i32>
  %2136 = bitcast <8 x i16> %2124 to <4 x i32>
  %2137 = shufflevector <4 x i32> %2135, <4 x i32> %2136, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2138 = bitcast <4 x i32> %2137 to <2 x i64>
  %2139 = bitcast <8 x i16> %2127 to <4 x i32>
  %2140 = bitcast <8 x i16> %2130 to <4 x i32>
  %2141 = shufflevector <4 x i32> %2139, <4 x i32> %2140, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2142 = bitcast <4 x i32> %2141 to <2 x i64>
  %2143 = bitcast <8 x i16> %2131 to <4 x i32>
  %2144 = bitcast <8 x i16> %2132 to <4 x i32>
  %2145 = shufflevector <4 x i32> %2143, <4 x i32> %2144, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2146 = bitcast <4 x i32> %2145 to <2 x i64>
  %2147 = bitcast <8 x i16> %2133 to <4 x i32>
  %2148 = bitcast <8 x i16> %2134 to <4 x i32>
  %2149 = shufflevector <4 x i32> %2147, <4 x i32> %2148, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2150 = bitcast <4 x i32> %2149 to <2 x i64>
  %2151 = shufflevector <4 x i32> %2135, <4 x i32> %2136, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2152 = bitcast <4 x i32> %2151 to <2 x i64>
  %2153 = shufflevector <4 x i32> %2139, <4 x i32> %2140, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2154 = bitcast <4 x i32> %2153 to <2 x i64>
  %2155 = shufflevector <4 x i32> %2143, <4 x i32> %2144, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2156 = bitcast <4 x i32> %2155 to <2 x i64>
  %2157 = shufflevector <4 x i32> %2147, <4 x i32> %2148, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2158 = bitcast <4 x i32> %2157 to <2 x i64>
  %2159 = shufflevector <2 x i64> %2138, <2 x i64> %2142, <2 x i32> <i32 0, i32 2>
  %2160 = shufflevector <2 x i64> %2138, <2 x i64> %2142, <2 x i32> <i32 1, i32 3>
  %2161 = shufflevector <2 x i64> %2152, <2 x i64> %2154, <2 x i32> <i32 0, i32 2>
  %2162 = shufflevector <2 x i64> %2152, <2 x i64> %2154, <2 x i32> <i32 1, i32 3>
  %2163 = shufflevector <2 x i64> %2146, <2 x i64> %2150, <2 x i32> <i32 0, i32 2>
  %2164 = shufflevector <2 x i64> %2146, <2 x i64> %2150, <2 x i32> <i32 1, i32 3>
  %2165 = shufflevector <2 x i64> %2156, <2 x i64> %2158, <2 x i32> <i32 0, i32 2>
  %2166 = shufflevector <2 x i64> %2156, <2 x i64> %2158, <2 x i32> <i32 1, i32 3>
  %2167 = load <8 x i16>, <8 x i16>* %1609, align 16
  %2168 = load <8 x i16>, <8 x i16>* %1611, align 16
  %2169 = shufflevector <8 x i16> %2167, <8 x i16> %2168, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2170 = load <8 x i16>, <8 x i16>* %1613, align 16
  %2171 = load <8 x i16>, <8 x i16>* %1615, align 16
  %2172 = shufflevector <8 x i16> %2170, <8 x i16> %2171, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2173 = load <8 x i16>, <8 x i16>* %1617, align 16
  %2174 = load <8 x i16>, <8 x i16>* %1619, align 16
  %2175 = shufflevector <8 x i16> %2173, <8 x i16> %2174, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2176 = load <8 x i16>, <8 x i16>* %1621, align 16
  %2177 = load <8 x i16>, <8 x i16>* %1623, align 16
  %2178 = shufflevector <8 x i16> %2176, <8 x i16> %2177, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2179 = shufflevector <8 x i16> %2167, <8 x i16> %2168, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2180 = shufflevector <8 x i16> %2170, <8 x i16> %2171, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2181 = shufflevector <8 x i16> %2173, <8 x i16> %2174, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2182 = shufflevector <8 x i16> %2176, <8 x i16> %2177, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2183 = bitcast <8 x i16> %2169 to <4 x i32>
  %2184 = bitcast <8 x i16> %2172 to <4 x i32>
  %2185 = shufflevector <4 x i32> %2183, <4 x i32> %2184, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2186 = bitcast <4 x i32> %2185 to <2 x i64>
  %2187 = bitcast <8 x i16> %2175 to <4 x i32>
  %2188 = bitcast <8 x i16> %2178 to <4 x i32>
  %2189 = shufflevector <4 x i32> %2187, <4 x i32> %2188, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2190 = bitcast <4 x i32> %2189 to <2 x i64>
  %2191 = bitcast <8 x i16> %2179 to <4 x i32>
  %2192 = bitcast <8 x i16> %2180 to <4 x i32>
  %2193 = shufflevector <4 x i32> %2191, <4 x i32> %2192, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2194 = bitcast <4 x i32> %2193 to <2 x i64>
  %2195 = bitcast <8 x i16> %2181 to <4 x i32>
  %2196 = bitcast <8 x i16> %2182 to <4 x i32>
  %2197 = shufflevector <4 x i32> %2195, <4 x i32> %2196, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2198 = bitcast <4 x i32> %2197 to <2 x i64>
  %2199 = shufflevector <4 x i32> %2183, <4 x i32> %2184, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2200 = bitcast <4 x i32> %2199 to <2 x i64>
  %2201 = shufflevector <4 x i32> %2187, <4 x i32> %2188, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2202 = bitcast <4 x i32> %2201 to <2 x i64>
  %2203 = shufflevector <4 x i32> %2191, <4 x i32> %2192, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2204 = bitcast <4 x i32> %2203 to <2 x i64>
  %2205 = shufflevector <4 x i32> %2195, <4 x i32> %2196, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2206 = bitcast <4 x i32> %2205 to <2 x i64>
  %2207 = shufflevector <2 x i64> %2186, <2 x i64> %2190, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %2207, <2 x i64>* %1514, align 16
  %2208 = shufflevector <2 x i64> %2186, <2 x i64> %2190, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %2208, <2 x i64>* %1630, align 16
  %2209 = shufflevector <2 x i64> %2200, <2 x i64> %2202, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %2209, <2 x i64>* %1634, align 16
  %2210 = shufflevector <2 x i64> %2200, <2 x i64> %2202, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %2210, <2 x i64>* %1638, align 16
  %2211 = shufflevector <2 x i64> %2194, <2 x i64> %2198, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %2211, <2 x i64>* %1642, align 16
  %2212 = shufflevector <2 x i64> %2194, <2 x i64> %2198, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %2212, <2 x i64>* %1646, align 16
  %2213 = shufflevector <2 x i64> %2204, <2 x i64> %2206, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %2213, <2 x i64>* %1650, align 16
  %2214 = shufflevector <2 x i64> %2204, <2 x i64> %2206, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %2214, <2 x i64>* %1654, align 16
  %2215 = load <8 x i16>, <8 x i16>* %1704, align 16
  %2216 = load <8 x i16>, <8 x i16>* %1706, align 16
  %2217 = shufflevector <8 x i16> %2215, <8 x i16> %2216, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2218 = load <8 x i16>, <8 x i16>* %1708, align 16
  %2219 = load <8 x i16>, <8 x i16>* %1710, align 16
  %2220 = shufflevector <8 x i16> %2218, <8 x i16> %2219, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2221 = load <8 x i16>, <8 x i16>* %1712, align 16
  %2222 = load <8 x i16>, <8 x i16>* %1714, align 16
  %2223 = shufflevector <8 x i16> %2221, <8 x i16> %2222, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2224 = load <8 x i16>, <8 x i16>* %1716, align 16
  %2225 = load <8 x i16>, <8 x i16>* %1718, align 16
  %2226 = shufflevector <8 x i16> %2224, <8 x i16> %2225, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %2227 = shufflevector <8 x i16> %2215, <8 x i16> %2216, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2228 = shufflevector <8 x i16> %2218, <8 x i16> %2219, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2229 = shufflevector <8 x i16> %2221, <8 x i16> %2222, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2230 = shufflevector <8 x i16> %2224, <8 x i16> %2225, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %2231 = bitcast <8 x i16> %2217 to <4 x i32>
  %2232 = bitcast <8 x i16> %2220 to <4 x i32>
  %2233 = shufflevector <4 x i32> %2231, <4 x i32> %2232, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2234 = bitcast <4 x i32> %2233 to <2 x i64>
  %2235 = bitcast <8 x i16> %2223 to <4 x i32>
  %2236 = bitcast <8 x i16> %2226 to <4 x i32>
  %2237 = shufflevector <4 x i32> %2235, <4 x i32> %2236, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2238 = bitcast <4 x i32> %2237 to <2 x i64>
  %2239 = bitcast <8 x i16> %2227 to <4 x i32>
  %2240 = bitcast <8 x i16> %2228 to <4 x i32>
  %2241 = shufflevector <4 x i32> %2239, <4 x i32> %2240, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2242 = bitcast <4 x i32> %2241 to <2 x i64>
  %2243 = bitcast <8 x i16> %2229 to <4 x i32>
  %2244 = bitcast <8 x i16> %2230 to <4 x i32>
  %2245 = shufflevector <4 x i32> %2243, <4 x i32> %2244, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %2246 = bitcast <4 x i32> %2245 to <2 x i64>
  %2247 = shufflevector <4 x i32> %2231, <4 x i32> %2232, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2248 = bitcast <4 x i32> %2247 to <2 x i64>
  %2249 = shufflevector <4 x i32> %2235, <4 x i32> %2236, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2250 = bitcast <4 x i32> %2249 to <2 x i64>
  %2251 = shufflevector <4 x i32> %2239, <4 x i32> %2240, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2252 = bitcast <4 x i32> %2251 to <2 x i64>
  %2253 = shufflevector <4 x i32> %2243, <4 x i32> %2244, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %2254 = bitcast <4 x i32> %2253 to <2 x i64>
  %2255 = shufflevector <2 x i64> %2234, <2 x i64> %2238, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %2255, <2 x i64>* %1672, align 16
  %2256 = shufflevector <2 x i64> %2234, <2 x i64> %2238, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %2256, <2 x i64>* %1678, align 16
  %2257 = shufflevector <2 x i64> %2248, <2 x i64> %2250, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %2257, <2 x i64>* %1682, align 16
  %2258 = shufflevector <2 x i64> %2248, <2 x i64> %2250, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %2258, <2 x i64>* %1686, align 16
  %2259 = shufflevector <2 x i64> %2242, <2 x i64> %2246, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %2259, <2 x i64>* %1690, align 16
  %2260 = shufflevector <2 x i64> %2242, <2 x i64> %2246, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %2260, <2 x i64>* %1694, align 16
  %2261 = shufflevector <2 x i64> %2252, <2 x i64> %2254, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %2261, <2 x i64>* %1698, align 16
  %2262 = shufflevector <2 x i64> %2252, <2 x i64> %2254, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %2262, <2 x i64>* %1702, align 16
  store <2 x i64> %2159, <2 x i64>* %1577, align 16
  store <2 x i64> %2160, <2 x i64>* %1583, align 16
  store <2 x i64> %2161, <2 x i64>* %1587, align 16
  store <2 x i64> %2162, <2 x i64>* %1591, align 16
  store <2 x i64> %2163, <2 x i64>* %1595, align 16
  store <2 x i64> %2164, <2 x i64>* %1599, align 16
  store <2 x i64> %2165, <2 x i64>* %1603, align 16
  store <2 x i64> %2166, <2 x i64>* %1607, align 16
  call fastcc void @write_buffer_16x16(i32* %1, <2 x i64>* nonnull %1513, <2 x i64>* nonnull %1514)
  br label %2263

2263:                                             ; preds = %1512, %761, %10, %9
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %8) #6
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %7) #6
  ret void
}

declare void @vpx_fdct16x16_sse2(i16*, i32*, i32) local_unnamed_addr #2

; Function Attrs: inlinehint nofree norecurse nounwind ssp uwtable
define internal fastcc void @write_buffer_16x16(i32* nocapture, <2 x i64>* nocapture readonly, <2 x i64>* nocapture readonly) unnamed_addr #4 {
  %4 = bitcast <2 x i64>* %1 to <8 x i16>*
  %5 = load <8 x i16>, <8 x i16>* %4, align 16
  %6 = ashr <8 x i16> %5, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %7 = shufflevector <8 x i16> %5, <8 x i16> %6, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %8 = shufflevector <8 x i16> %5, <8 x i16> %6, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %9 = bitcast i32* %0 to <8 x i16>*
  store <8 x i16> %7, <8 x i16>* %9, align 16
  %10 = getelementptr inbounds i32, i32* %0, i64 4
  %11 = bitcast i32* %10 to <8 x i16>*
  store <8 x i16> %8, <8 x i16>* %11, align 16
  %12 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 1
  %13 = getelementptr inbounds i32, i32* %0, i64 16
  %14 = bitcast <2 x i64>* %12 to <8 x i16>*
  %15 = load <8 x i16>, <8 x i16>* %14, align 16
  %16 = ashr <8 x i16> %15, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %17 = shufflevector <8 x i16> %15, <8 x i16> %16, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %18 = shufflevector <8 x i16> %15, <8 x i16> %16, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %19 = bitcast i32* %13 to <8 x i16>*
  store <8 x i16> %17, <8 x i16>* %19, align 16
  %20 = getelementptr inbounds i32, i32* %0, i64 20
  %21 = bitcast i32* %20 to <8 x i16>*
  store <8 x i16> %18, <8 x i16>* %21, align 16
  %22 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 2
  %23 = getelementptr inbounds i32, i32* %0, i64 32
  %24 = bitcast <2 x i64>* %22 to <8 x i16>*
  %25 = load <8 x i16>, <8 x i16>* %24, align 16
  %26 = ashr <8 x i16> %25, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %27 = shufflevector <8 x i16> %25, <8 x i16> %26, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %28 = shufflevector <8 x i16> %25, <8 x i16> %26, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %29 = bitcast i32* %23 to <8 x i16>*
  store <8 x i16> %27, <8 x i16>* %29, align 16
  %30 = getelementptr inbounds i32, i32* %0, i64 36
  %31 = bitcast i32* %30 to <8 x i16>*
  store <8 x i16> %28, <8 x i16>* %31, align 16
  %32 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 3
  %33 = getelementptr inbounds i32, i32* %0, i64 48
  %34 = bitcast <2 x i64>* %32 to <8 x i16>*
  %35 = load <8 x i16>, <8 x i16>* %34, align 16
  %36 = ashr <8 x i16> %35, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %37 = shufflevector <8 x i16> %35, <8 x i16> %36, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %38 = shufflevector <8 x i16> %35, <8 x i16> %36, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %39 = bitcast i32* %33 to <8 x i16>*
  store <8 x i16> %37, <8 x i16>* %39, align 16
  %40 = getelementptr inbounds i32, i32* %0, i64 52
  %41 = bitcast i32* %40 to <8 x i16>*
  store <8 x i16> %38, <8 x i16>* %41, align 16
  %42 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 4
  %43 = getelementptr inbounds i32, i32* %0, i64 64
  %44 = bitcast <2 x i64>* %42 to <8 x i16>*
  %45 = load <8 x i16>, <8 x i16>* %44, align 16
  %46 = ashr <8 x i16> %45, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %47 = shufflevector <8 x i16> %45, <8 x i16> %46, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %48 = shufflevector <8 x i16> %45, <8 x i16> %46, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %49 = bitcast i32* %43 to <8 x i16>*
  store <8 x i16> %47, <8 x i16>* %49, align 16
  %50 = getelementptr inbounds i32, i32* %0, i64 68
  %51 = bitcast i32* %50 to <8 x i16>*
  store <8 x i16> %48, <8 x i16>* %51, align 16
  %52 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 5
  %53 = getelementptr inbounds i32, i32* %0, i64 80
  %54 = bitcast <2 x i64>* %52 to <8 x i16>*
  %55 = load <8 x i16>, <8 x i16>* %54, align 16
  %56 = ashr <8 x i16> %55, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %57 = shufflevector <8 x i16> %55, <8 x i16> %56, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %58 = shufflevector <8 x i16> %55, <8 x i16> %56, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %59 = bitcast i32* %53 to <8 x i16>*
  store <8 x i16> %57, <8 x i16>* %59, align 16
  %60 = getelementptr inbounds i32, i32* %0, i64 84
  %61 = bitcast i32* %60 to <8 x i16>*
  store <8 x i16> %58, <8 x i16>* %61, align 16
  %62 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 6
  %63 = getelementptr inbounds i32, i32* %0, i64 96
  %64 = bitcast <2 x i64>* %62 to <8 x i16>*
  %65 = load <8 x i16>, <8 x i16>* %64, align 16
  %66 = ashr <8 x i16> %65, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %67 = shufflevector <8 x i16> %65, <8 x i16> %66, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %68 = shufflevector <8 x i16> %65, <8 x i16> %66, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %69 = bitcast i32* %63 to <8 x i16>*
  store <8 x i16> %67, <8 x i16>* %69, align 16
  %70 = getelementptr inbounds i32, i32* %0, i64 100
  %71 = bitcast i32* %70 to <8 x i16>*
  store <8 x i16> %68, <8 x i16>* %71, align 16
  %72 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 7
  %73 = getelementptr inbounds i32, i32* %0, i64 112
  %74 = bitcast <2 x i64>* %72 to <8 x i16>*
  %75 = load <8 x i16>, <8 x i16>* %74, align 16
  %76 = ashr <8 x i16> %75, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %77 = shufflevector <8 x i16> %75, <8 x i16> %76, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %78 = shufflevector <8 x i16> %75, <8 x i16> %76, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %79 = bitcast i32* %73 to <8 x i16>*
  store <8 x i16> %77, <8 x i16>* %79, align 16
  %80 = getelementptr inbounds i32, i32* %0, i64 116
  %81 = bitcast i32* %80 to <8 x i16>*
  store <8 x i16> %78, <8 x i16>* %81, align 16
  %82 = getelementptr inbounds i32, i32* %0, i64 128
  %83 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 8
  %84 = bitcast <2 x i64>* %83 to <8 x i16>*
  %85 = load <8 x i16>, <8 x i16>* %84, align 16
  %86 = ashr <8 x i16> %85, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %87 = shufflevector <8 x i16> %85, <8 x i16> %86, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %88 = shufflevector <8 x i16> %85, <8 x i16> %86, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %89 = bitcast i32* %82 to <8 x i16>*
  store <8 x i16> %87, <8 x i16>* %89, align 16
  %90 = getelementptr inbounds i32, i32* %0, i64 132
  %91 = bitcast i32* %90 to <8 x i16>*
  store <8 x i16> %88, <8 x i16>* %91, align 16
  %92 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 9
  %93 = getelementptr inbounds i32, i32* %0, i64 144
  %94 = bitcast <2 x i64>* %92 to <8 x i16>*
  %95 = load <8 x i16>, <8 x i16>* %94, align 16
  %96 = ashr <8 x i16> %95, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %97 = shufflevector <8 x i16> %95, <8 x i16> %96, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %98 = shufflevector <8 x i16> %95, <8 x i16> %96, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %99 = bitcast i32* %93 to <8 x i16>*
  store <8 x i16> %97, <8 x i16>* %99, align 16
  %100 = getelementptr inbounds i32, i32* %0, i64 148
  %101 = bitcast i32* %100 to <8 x i16>*
  store <8 x i16> %98, <8 x i16>* %101, align 16
  %102 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 10
  %103 = getelementptr inbounds i32, i32* %0, i64 160
  %104 = bitcast <2 x i64>* %102 to <8 x i16>*
  %105 = load <8 x i16>, <8 x i16>* %104, align 16
  %106 = ashr <8 x i16> %105, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %107 = shufflevector <8 x i16> %105, <8 x i16> %106, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %108 = shufflevector <8 x i16> %105, <8 x i16> %106, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %109 = bitcast i32* %103 to <8 x i16>*
  store <8 x i16> %107, <8 x i16>* %109, align 16
  %110 = getelementptr inbounds i32, i32* %0, i64 164
  %111 = bitcast i32* %110 to <8 x i16>*
  store <8 x i16> %108, <8 x i16>* %111, align 16
  %112 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 11
  %113 = getelementptr inbounds i32, i32* %0, i64 176
  %114 = bitcast <2 x i64>* %112 to <8 x i16>*
  %115 = load <8 x i16>, <8 x i16>* %114, align 16
  %116 = ashr <8 x i16> %115, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %117 = shufflevector <8 x i16> %115, <8 x i16> %116, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %118 = shufflevector <8 x i16> %115, <8 x i16> %116, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %119 = bitcast i32* %113 to <8 x i16>*
  store <8 x i16> %117, <8 x i16>* %119, align 16
  %120 = getelementptr inbounds i32, i32* %0, i64 180
  %121 = bitcast i32* %120 to <8 x i16>*
  store <8 x i16> %118, <8 x i16>* %121, align 16
  %122 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 12
  %123 = getelementptr inbounds i32, i32* %0, i64 192
  %124 = bitcast <2 x i64>* %122 to <8 x i16>*
  %125 = load <8 x i16>, <8 x i16>* %124, align 16
  %126 = ashr <8 x i16> %125, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %127 = shufflevector <8 x i16> %125, <8 x i16> %126, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %128 = shufflevector <8 x i16> %125, <8 x i16> %126, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %129 = bitcast i32* %123 to <8 x i16>*
  store <8 x i16> %127, <8 x i16>* %129, align 16
  %130 = getelementptr inbounds i32, i32* %0, i64 196
  %131 = bitcast i32* %130 to <8 x i16>*
  store <8 x i16> %128, <8 x i16>* %131, align 16
  %132 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 13
  %133 = getelementptr inbounds i32, i32* %0, i64 208
  %134 = bitcast <2 x i64>* %132 to <8 x i16>*
  %135 = load <8 x i16>, <8 x i16>* %134, align 16
  %136 = ashr <8 x i16> %135, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %137 = shufflevector <8 x i16> %135, <8 x i16> %136, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %138 = shufflevector <8 x i16> %135, <8 x i16> %136, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %139 = bitcast i32* %133 to <8 x i16>*
  store <8 x i16> %137, <8 x i16>* %139, align 16
  %140 = getelementptr inbounds i32, i32* %0, i64 212
  %141 = bitcast i32* %140 to <8 x i16>*
  store <8 x i16> %138, <8 x i16>* %141, align 16
  %142 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 14
  %143 = getelementptr inbounds i32, i32* %0, i64 224
  %144 = bitcast <2 x i64>* %142 to <8 x i16>*
  %145 = load <8 x i16>, <8 x i16>* %144, align 16
  %146 = ashr <8 x i16> %145, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %147 = shufflevector <8 x i16> %145, <8 x i16> %146, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %148 = shufflevector <8 x i16> %145, <8 x i16> %146, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %149 = bitcast i32* %143 to <8 x i16>*
  store <8 x i16> %147, <8 x i16>* %149, align 16
  %150 = getelementptr inbounds i32, i32* %0, i64 228
  %151 = bitcast i32* %150 to <8 x i16>*
  store <8 x i16> %148, <8 x i16>* %151, align 16
  %152 = getelementptr inbounds <2 x i64>, <2 x i64>* %1, i64 15
  %153 = getelementptr inbounds i32, i32* %0, i64 240
  %154 = bitcast <2 x i64>* %152 to <8 x i16>*
  %155 = load <8 x i16>, <8 x i16>* %154, align 16
  %156 = ashr <8 x i16> %155, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %157 = shufflevector <8 x i16> %155, <8 x i16> %156, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %158 = shufflevector <8 x i16> %155, <8 x i16> %156, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %159 = bitcast i32* %153 to <8 x i16>*
  store <8 x i16> %157, <8 x i16>* %159, align 16
  %160 = getelementptr inbounds i32, i32* %0, i64 244
  %161 = bitcast i32* %160 to <8 x i16>*
  store <8 x i16> %158, <8 x i16>* %161, align 16
  %162 = getelementptr inbounds i32, i32* %0, i64 8
  %163 = bitcast <2 x i64>* %2 to <8 x i16>*
  %164 = load <8 x i16>, <8 x i16>* %163, align 16
  %165 = ashr <8 x i16> %164, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %166 = shufflevector <8 x i16> %164, <8 x i16> %165, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %167 = shufflevector <8 x i16> %164, <8 x i16> %165, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %168 = bitcast i32* %162 to <8 x i16>*
  store <8 x i16> %166, <8 x i16>* %168, align 16
  %169 = getelementptr inbounds i32, i32* %0, i64 12
  %170 = bitcast i32* %169 to <8 x i16>*
  store <8 x i16> %167, <8 x i16>* %170, align 16
  %171 = getelementptr inbounds <2 x i64>, <2 x i64>* %2, i64 1
  %172 = getelementptr inbounds i32, i32* %0, i64 24
  %173 = bitcast <2 x i64>* %171 to <8 x i16>*
  %174 = load <8 x i16>, <8 x i16>* %173, align 16
  %175 = ashr <8 x i16> %174, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %176 = shufflevector <8 x i16> %174, <8 x i16> %175, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %177 = shufflevector <8 x i16> %174, <8 x i16> %175, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %178 = bitcast i32* %172 to <8 x i16>*
  store <8 x i16> %176, <8 x i16>* %178, align 16
  %179 = getelementptr inbounds i32, i32* %0, i64 28
  %180 = bitcast i32* %179 to <8 x i16>*
  store <8 x i16> %177, <8 x i16>* %180, align 16
  %181 = getelementptr inbounds <2 x i64>, <2 x i64>* %2, i64 2
  %182 = getelementptr inbounds i32, i32* %0, i64 40
  %183 = bitcast <2 x i64>* %181 to <8 x i16>*
  %184 = load <8 x i16>, <8 x i16>* %183, align 16
  %185 = ashr <8 x i16> %184, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %186 = shufflevector <8 x i16> %184, <8 x i16> %185, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %187 = shufflevector <8 x i16> %184, <8 x i16> %185, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %188 = bitcast i32* %182 to <8 x i16>*
  store <8 x i16> %186, <8 x i16>* %188, align 16
  %189 = getelementptr inbounds i32, i32* %0, i64 44
  %190 = bitcast i32* %189 to <8 x i16>*
  store <8 x i16> %187, <8 x i16>* %190, align 16
  %191 = getelementptr inbounds <2 x i64>, <2 x i64>* %2, i64 3
  %192 = getelementptr inbounds i32, i32* %0, i64 56
  %193 = bitcast <2 x i64>* %191 to <8 x i16>*
  %194 = load <8 x i16>, <8 x i16>* %193, align 16
  %195 = ashr <8 x i16> %194, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %196 = shufflevector <8 x i16> %194, <8 x i16> %195, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %197 = shufflevector <8 x i16> %194, <8 x i16> %195, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %198 = bitcast i32* %192 to <8 x i16>*
  store <8 x i16> %196, <8 x i16>* %198, align 16
  %199 = getelementptr inbounds i32, i32* %0, i64 60
  %200 = bitcast i32* %199 to <8 x i16>*
  store <8 x i16> %197, <8 x i16>* %200, align 16
  %201 = getelementptr inbounds <2 x i64>, <2 x i64>* %2, i64 4
  %202 = getelementptr inbounds i32, i32* %0, i64 72
  %203 = bitcast <2 x i64>* %201 to <8 x i16>*
  %204 = load <8 x i16>, <8 x i16>* %203, align 16
  %205 = ashr <8 x i16> %204, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %206 = shufflevector <8 x i16> %204, <8 x i16> %205, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %207 = shufflevector <8 x i16> %204, <8 x i16> %205, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %208 = bitcast i32* %202 to <8 x i16>*
  store <8 x i16> %206, <8 x i16>* %208, align 16
  %209 = getelementptr inbounds i32, i32* %0, i64 76
  %210 = bitcast i32* %209 to <8 x i16>*
  store <8 x i16> %207, <8 x i16>* %210, align 16
  %211 = getelementptr inbounds <2 x i64>, <2 x i64>* %2, i64 5
  %212 = getelementptr inbounds i32, i32* %0, i64 88
  %213 = bitcast <2 x i64>* %211 to <8 x i16>*
  %214 = load <8 x i16>, <8 x i16>* %213, align 16
  %215 = ashr <8 x i16> %214, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %216 = shufflevector <8 x i16> %214, <8 x i16> %215, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %217 = shufflevector <8 x i16> %214, <8 x i16> %215, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %218 = bitcast i32* %212 to <8 x i16>*
  store <8 x i16> %216, <8 x i16>* %218, align 16
  %219 = getelementptr inbounds i32, i32* %0, i64 92
  %220 = bitcast i32* %219 to <8 x i16>*
  store <8 x i16> %217, <8 x i16>* %220, align 16
  %221 = getelementptr inbounds <2 x i64>, <2 x i64>* %2, i64 6
  %222 = getelementptr inbounds i32, i32* %0, i64 104
  %223 = bitcast <2 x i64>* %221 to <8 x i16>*
  %224 = load <8 x i16>, <8 x i16>* %223, align 16
  %225 = ashr <8 x i16> %224, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %226 = shufflevector <8 x i16> %224, <8 x i16> %225, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %227 = shufflevector <8 x i16> %224, <8 x i16> %225, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %228 = bitcast i32* %222 to <8 x i16>*
  store <8 x i16> %226, <8 x i16>* %228, align 16
  %229 = getelementptr inbounds i32, i32* %0, i64 108
  %230 = bitcast i32* %229 to <8 x i16>*
  store <8 x i16> %227, <8 x i16>* %230, align 16
  %231 = getelementptr inbounds <2 x i64>, <2 x i64>* %2, i64 7
  %232 = getelementptr inbounds i32, i32* %0, i64 120
  %233 = bitcast <2 x i64>* %231 to <8 x i16>*
  %234 = load <8 x i16>, <8 x i16>* %233, align 16
  %235 = ashr <8 x i16> %234, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %236 = shufflevector <8 x i16> %234, <8 x i16> %235, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %237 = shufflevector <8 x i16> %234, <8 x i16> %235, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %238 = bitcast i32* %232 to <8 x i16>*
  store <8 x i16> %236, <8 x i16>* %238, align 16
  %239 = getelementptr inbounds i32, i32* %0, i64 124
  %240 = bitcast i32* %239 to <8 x i16>*
  store <8 x i16> %237, <8 x i16>* %240, align 16
  %241 = getelementptr inbounds i32, i32* %0, i64 136
  %242 = getelementptr inbounds <2 x i64>, <2 x i64>* %2, i64 8
  %243 = bitcast <2 x i64>* %242 to <8 x i16>*
  %244 = load <8 x i16>, <8 x i16>* %243, align 16
  %245 = ashr <8 x i16> %244, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %246 = shufflevector <8 x i16> %244, <8 x i16> %245, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %247 = shufflevector <8 x i16> %244, <8 x i16> %245, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %248 = bitcast i32* %241 to <8 x i16>*
  store <8 x i16> %246, <8 x i16>* %248, align 16
  %249 = getelementptr inbounds i32, i32* %0, i64 140
  %250 = bitcast i32* %249 to <8 x i16>*
  store <8 x i16> %247, <8 x i16>* %250, align 16
  %251 = getelementptr inbounds <2 x i64>, <2 x i64>* %2, i64 9
  %252 = getelementptr inbounds i32, i32* %0, i64 152
  %253 = bitcast <2 x i64>* %251 to <8 x i16>*
  %254 = load <8 x i16>, <8 x i16>* %253, align 16
  %255 = ashr <8 x i16> %254, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %256 = shufflevector <8 x i16> %254, <8 x i16> %255, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %257 = shufflevector <8 x i16> %254, <8 x i16> %255, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %258 = bitcast i32* %252 to <8 x i16>*
  store <8 x i16> %256, <8 x i16>* %258, align 16
  %259 = getelementptr inbounds i32, i32* %0, i64 156
  %260 = bitcast i32* %259 to <8 x i16>*
  store <8 x i16> %257, <8 x i16>* %260, align 16
  %261 = getelementptr inbounds <2 x i64>, <2 x i64>* %2, i64 10
  %262 = getelementptr inbounds i32, i32* %0, i64 168
  %263 = bitcast <2 x i64>* %261 to <8 x i16>*
  %264 = load <8 x i16>, <8 x i16>* %263, align 16
  %265 = ashr <8 x i16> %264, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %266 = shufflevector <8 x i16> %264, <8 x i16> %265, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %267 = shufflevector <8 x i16> %264, <8 x i16> %265, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %268 = bitcast i32* %262 to <8 x i16>*
  store <8 x i16> %266, <8 x i16>* %268, align 16
  %269 = getelementptr inbounds i32, i32* %0, i64 172
  %270 = bitcast i32* %269 to <8 x i16>*
  store <8 x i16> %267, <8 x i16>* %270, align 16
  %271 = getelementptr inbounds <2 x i64>, <2 x i64>* %2, i64 11
  %272 = getelementptr inbounds i32, i32* %0, i64 184
  %273 = bitcast <2 x i64>* %271 to <8 x i16>*
  %274 = load <8 x i16>, <8 x i16>* %273, align 16
  %275 = ashr <8 x i16> %274, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %276 = shufflevector <8 x i16> %274, <8 x i16> %275, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %277 = shufflevector <8 x i16> %274, <8 x i16> %275, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %278 = bitcast i32* %272 to <8 x i16>*
  store <8 x i16> %276, <8 x i16>* %278, align 16
  %279 = getelementptr inbounds i32, i32* %0, i64 188
  %280 = bitcast i32* %279 to <8 x i16>*
  store <8 x i16> %277, <8 x i16>* %280, align 16
  %281 = getelementptr inbounds <2 x i64>, <2 x i64>* %2, i64 12
  %282 = getelementptr inbounds i32, i32* %0, i64 200
  %283 = bitcast <2 x i64>* %281 to <8 x i16>*
  %284 = load <8 x i16>, <8 x i16>* %283, align 16
  %285 = ashr <8 x i16> %284, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %286 = shufflevector <8 x i16> %284, <8 x i16> %285, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %287 = shufflevector <8 x i16> %284, <8 x i16> %285, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %288 = bitcast i32* %282 to <8 x i16>*
  store <8 x i16> %286, <8 x i16>* %288, align 16
  %289 = getelementptr inbounds i32, i32* %0, i64 204
  %290 = bitcast i32* %289 to <8 x i16>*
  store <8 x i16> %287, <8 x i16>* %290, align 16
  %291 = getelementptr inbounds <2 x i64>, <2 x i64>* %2, i64 13
  %292 = getelementptr inbounds i32, i32* %0, i64 216
  %293 = bitcast <2 x i64>* %291 to <8 x i16>*
  %294 = load <8 x i16>, <8 x i16>* %293, align 16
  %295 = ashr <8 x i16> %294, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %296 = shufflevector <8 x i16> %294, <8 x i16> %295, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %297 = shufflevector <8 x i16> %294, <8 x i16> %295, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %298 = bitcast i32* %292 to <8 x i16>*
  store <8 x i16> %296, <8 x i16>* %298, align 16
  %299 = getelementptr inbounds i32, i32* %0, i64 220
  %300 = bitcast i32* %299 to <8 x i16>*
  store <8 x i16> %297, <8 x i16>* %300, align 16
  %301 = getelementptr inbounds <2 x i64>, <2 x i64>* %2, i64 14
  %302 = getelementptr inbounds i32, i32* %0, i64 232
  %303 = bitcast <2 x i64>* %301 to <8 x i16>*
  %304 = load <8 x i16>, <8 x i16>* %303, align 16
  %305 = ashr <8 x i16> %304, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %306 = shufflevector <8 x i16> %304, <8 x i16> %305, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %307 = shufflevector <8 x i16> %304, <8 x i16> %305, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %308 = bitcast i32* %302 to <8 x i16>*
  store <8 x i16> %306, <8 x i16>* %308, align 16
  %309 = getelementptr inbounds i32, i32* %0, i64 236
  %310 = bitcast i32* %309 to <8 x i16>*
  store <8 x i16> %307, <8 x i16>* %310, align 16
  %311 = getelementptr inbounds <2 x i64>, <2 x i64>* %2, i64 15
  %312 = getelementptr inbounds i32, i32* %0, i64 248
  %313 = bitcast <2 x i64>* %311 to <8 x i16>*
  %314 = load <8 x i16>, <8 x i16>* %313, align 16
  %315 = ashr <8 x i16> %314, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %316 = shufflevector <8 x i16> %314, <8 x i16> %315, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %317 = shufflevector <8 x i16> %314, <8 x i16> %315, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %318 = bitcast i32* %312 to <8 x i16>*
  store <8 x i16> %316, <8 x i16>* %318, align 16
  %319 = getelementptr inbounds i32, i32* %0, i64 252
  %320 = bitcast i32* %319 to <8 x i16>*
  store <8 x i16> %317, <8 x i16>* %320, align 16
  ret void
}

; Function Attrs: nounwind readnone
declare <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16>, <8 x i16>) #5

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32>, <4 x i32>) #5

; Function Attrs: nofree nounwind ssp uwtable
define internal fastcc void @fadst16_8col(<2 x i64>* nocapture) unnamed_addr #3 {
  %2 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 15
  %3 = bitcast <2 x i64>* %2 to <8 x i16>*
  %4 = load <8 x i16>, <8 x i16>* %3, align 16
  %5 = bitcast <2 x i64>* %0 to <8 x i16>*
  %6 = load <8 x i16>, <8 x i16>* %5, align 16
  %7 = shufflevector <8 x i16> %4, <8 x i16> %6, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %8 = shufflevector <8 x i16> %4, <8 x i16> %6, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %9 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 13
  %10 = bitcast <2 x i64>* %9 to <8 x i16>*
  %11 = load <8 x i16>, <8 x i16>* %10, align 16
  %12 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %13 = bitcast <2 x i64>* %12 to <8 x i16>*
  %14 = load <8 x i16>, <8 x i16>* %13, align 16
  %15 = shufflevector <8 x i16> %11, <8 x i16> %14, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %16 = shufflevector <8 x i16> %11, <8 x i16> %14, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %17 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 11
  %18 = bitcast <2 x i64>* %17 to <8 x i16>*
  %19 = load <8 x i16>, <8 x i16>* %18, align 16
  %20 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 4
  %21 = bitcast <2 x i64>* %20 to <8 x i16>*
  %22 = load <8 x i16>, <8 x i16>* %21, align 16
  %23 = shufflevector <8 x i16> %19, <8 x i16> %22, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %24 = shufflevector <8 x i16> %19, <8 x i16> %22, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %25 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 9
  %26 = bitcast <2 x i64>* %25 to <8 x i16>*
  %27 = load <8 x i16>, <8 x i16>* %26, align 16
  %28 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 6
  %29 = bitcast <2 x i64>* %28 to <8 x i16>*
  %30 = load <8 x i16>, <8 x i16>* %29, align 16
  %31 = shufflevector <8 x i16> %27, <8 x i16> %30, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %32 = shufflevector <8 x i16> %27, <8 x i16> %30, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %33 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 7
  %34 = bitcast <2 x i64>* %33 to <8 x i16>*
  %35 = load <8 x i16>, <8 x i16>* %34, align 16
  %36 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 8
  %37 = bitcast <2 x i64>* %36 to <8 x i16>*
  %38 = load <8 x i16>, <8 x i16>* %37, align 16
  %39 = shufflevector <8 x i16> %35, <8 x i16> %38, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %40 = shufflevector <8 x i16> %35, <8 x i16> %38, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %41 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 5
  %42 = bitcast <2 x i64>* %41 to <8 x i16>*
  %43 = load <8 x i16>, <8 x i16>* %42, align 16
  %44 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 10
  %45 = bitcast <2 x i64>* %44 to <8 x i16>*
  %46 = load <8 x i16>, <8 x i16>* %45, align 16
  %47 = shufflevector <8 x i16> %43, <8 x i16> %46, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %48 = shufflevector <8 x i16> %43, <8 x i16> %46, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %49 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %50 = bitcast <2 x i64>* %49 to <8 x i16>*
  %51 = load <8 x i16>, <8 x i16>* %50, align 16
  %52 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 12
  %53 = bitcast <2 x i64>* %52 to <8 x i16>*
  %54 = load <8 x i16>, <8 x i16>* %53, align 16
  %55 = shufflevector <8 x i16> %51, <8 x i16> %54, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %56 = shufflevector <8 x i16> %51, <8 x i16> %54, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %57 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %58 = bitcast <2 x i64>* %57 to <8 x i16>*
  %59 = load <8 x i16>, <8 x i16>* %58, align 16
  %60 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 14
  %61 = bitcast <2 x i64>* %60 to <8 x i16>*
  %62 = load <8 x i16>, <8 x i16>* %61, align 16
  %63 = shufflevector <8 x i16> %59, <8 x i16> %62, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %64 = shufflevector <8 x i16> %59, <8 x i16> %62, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %65 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %7, <8 x i16> <i16 16364, i16 804, i16 16364, i16 804, i16 16364, i16 804, i16 16364, i16 804>) #6
  %66 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %8, <8 x i16> <i16 16364, i16 804, i16 16364, i16 804, i16 16364, i16 804, i16 16364, i16 804>) #6
  %67 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %7, <8 x i16> <i16 804, i16 -16364, i16 804, i16 -16364, i16 804, i16 -16364, i16 804, i16 -16364>) #6
  %68 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %8, <8 x i16> <i16 804, i16 -16364, i16 804, i16 -16364, i16 804, i16 -16364, i16 804, i16 -16364>) #6
  %69 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %15, <8 x i16> <i16 15893, i16 3981, i16 15893, i16 3981, i16 15893, i16 3981, i16 15893, i16 3981>) #6
  %70 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %16, <8 x i16> <i16 15893, i16 3981, i16 15893, i16 3981, i16 15893, i16 3981, i16 15893, i16 3981>) #6
  %71 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %15, <8 x i16> <i16 3981, i16 -15893, i16 3981, i16 -15893, i16 3981, i16 -15893, i16 3981, i16 -15893>) #6
  %72 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %16, <8 x i16> <i16 3981, i16 -15893, i16 3981, i16 -15893, i16 3981, i16 -15893, i16 3981, i16 -15893>) #6
  %73 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %23, <8 x i16> <i16 14811, i16 7005, i16 14811, i16 7005, i16 14811, i16 7005, i16 14811, i16 7005>) #6
  %74 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %24, <8 x i16> <i16 14811, i16 7005, i16 14811, i16 7005, i16 14811, i16 7005, i16 14811, i16 7005>) #6
  %75 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %23, <8 x i16> <i16 7005, i16 -14811, i16 7005, i16 -14811, i16 7005, i16 -14811, i16 7005, i16 -14811>) #6
  %76 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %24, <8 x i16> <i16 7005, i16 -14811, i16 7005, i16 -14811, i16 7005, i16 -14811, i16 7005, i16 -14811>) #6
  %77 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %31, <8 x i16> <i16 13160, i16 9760, i16 13160, i16 9760, i16 13160, i16 9760, i16 13160, i16 9760>) #6
  %78 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %32, <8 x i16> <i16 13160, i16 9760, i16 13160, i16 9760, i16 13160, i16 9760, i16 13160, i16 9760>) #6
  %79 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %31, <8 x i16> <i16 9760, i16 -13160, i16 9760, i16 -13160, i16 9760, i16 -13160, i16 9760, i16 -13160>) #6
  %80 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %32, <8 x i16> <i16 9760, i16 -13160, i16 9760, i16 -13160, i16 9760, i16 -13160, i16 9760, i16 -13160>) #6
  %81 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %39, <8 x i16> <i16 11003, i16 12140, i16 11003, i16 12140, i16 11003, i16 12140, i16 11003, i16 12140>) #6
  %82 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %40, <8 x i16> <i16 11003, i16 12140, i16 11003, i16 12140, i16 11003, i16 12140, i16 11003, i16 12140>) #6
  %83 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %39, <8 x i16> <i16 12140, i16 -11003, i16 12140, i16 -11003, i16 12140, i16 -11003, i16 12140, i16 -11003>) #6
  %84 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %40, <8 x i16> <i16 12140, i16 -11003, i16 12140, i16 -11003, i16 12140, i16 -11003, i16 12140, i16 -11003>) #6
  %85 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %47, <8 x i16> <i16 8423, i16 14053, i16 8423, i16 14053, i16 8423, i16 14053, i16 8423, i16 14053>) #6
  %86 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %48, <8 x i16> <i16 8423, i16 14053, i16 8423, i16 14053, i16 8423, i16 14053, i16 8423, i16 14053>) #6
  %87 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %47, <8 x i16> <i16 14053, i16 -8423, i16 14053, i16 -8423, i16 14053, i16 -8423, i16 14053, i16 -8423>) #6
  %88 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %48, <8 x i16> <i16 14053, i16 -8423, i16 14053, i16 -8423, i16 14053, i16 -8423, i16 14053, i16 -8423>) #6
  %89 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %55, <8 x i16> <i16 5520, i16 15426, i16 5520, i16 15426, i16 5520, i16 15426, i16 5520, i16 15426>) #6
  %90 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %56, <8 x i16> <i16 5520, i16 15426, i16 5520, i16 15426, i16 5520, i16 15426, i16 5520, i16 15426>) #6
  %91 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %55, <8 x i16> <i16 15426, i16 -5520, i16 15426, i16 -5520, i16 15426, i16 -5520, i16 15426, i16 -5520>) #6
  %92 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %56, <8 x i16> <i16 15426, i16 -5520, i16 15426, i16 -5520, i16 15426, i16 -5520, i16 15426, i16 -5520>) #6
  %93 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %63, <8 x i16> <i16 2404, i16 16207, i16 2404, i16 16207, i16 2404, i16 16207, i16 2404, i16 16207>) #6
  %94 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %64, <8 x i16> <i16 2404, i16 16207, i16 2404, i16 16207, i16 2404, i16 16207, i16 2404, i16 16207>) #6
  %95 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %63, <8 x i16> <i16 16207, i16 -2404, i16 16207, i16 -2404, i16 16207, i16 -2404, i16 16207, i16 -2404>) #6
  %96 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %64, <8 x i16> <i16 16207, i16 -2404, i16 16207, i16 -2404, i16 16207, i16 -2404, i16 16207, i16 -2404>) #6
  %97 = add <4 x i32> %65, <i32 8192, i32 8192, i32 8192, i32 8192>
  %98 = add <4 x i32> %97, %81
  %99 = add <4 x i32> %66, <i32 8192, i32 8192, i32 8192, i32 8192>
  %100 = add <4 x i32> %99, %82
  %101 = add <4 x i32> %67, <i32 8192, i32 8192, i32 8192, i32 8192>
  %102 = add <4 x i32> %101, %83
  %103 = add <4 x i32> %68, <i32 8192, i32 8192, i32 8192, i32 8192>
  %104 = add <4 x i32> %103, %84
  %105 = add <4 x i32> %69, <i32 8192, i32 8192, i32 8192, i32 8192>
  %106 = add <4 x i32> %105, %85
  %107 = add <4 x i32> %70, <i32 8192, i32 8192, i32 8192, i32 8192>
  %108 = add <4 x i32> %107, %86
  %109 = add <4 x i32> %71, <i32 8192, i32 8192, i32 8192, i32 8192>
  %110 = add <4 x i32> %109, %87
  %111 = add <4 x i32> %72, <i32 8192, i32 8192, i32 8192, i32 8192>
  %112 = add <4 x i32> %111, %88
  %113 = add <4 x i32> %73, <i32 8192, i32 8192, i32 8192, i32 8192>
  %114 = add <4 x i32> %113, %89
  %115 = add <4 x i32> %74, <i32 8192, i32 8192, i32 8192, i32 8192>
  %116 = add <4 x i32> %115, %90
  %117 = add <4 x i32> %75, <i32 8192, i32 8192, i32 8192, i32 8192>
  %118 = add <4 x i32> %117, %91
  %119 = add <4 x i32> %76, <i32 8192, i32 8192, i32 8192, i32 8192>
  %120 = add <4 x i32> %119, %92
  %121 = add <4 x i32> %77, <i32 8192, i32 8192, i32 8192, i32 8192>
  %122 = add <4 x i32> %121, %93
  %123 = add <4 x i32> %78, <i32 8192, i32 8192, i32 8192, i32 8192>
  %124 = add <4 x i32> %123, %94
  %125 = add <4 x i32> %79, <i32 8192, i32 8192, i32 8192, i32 8192>
  %126 = add <4 x i32> %125, %95
  %127 = add <4 x i32> %80, <i32 8192, i32 8192, i32 8192, i32 8192>
  %128 = add <4 x i32> %127, %96
  %129 = sub <4 x i32> %97, %81
  %130 = sub <4 x i32> %99, %82
  %131 = sub <4 x i32> %101, %83
  %132 = sub <4 x i32> %103, %84
  %133 = sub <4 x i32> %105, %85
  %134 = sub <4 x i32> %107, %86
  %135 = sub <4 x i32> %109, %87
  %136 = sub <4 x i32> %111, %88
  %137 = sub <4 x i32> %113, %89
  %138 = sub <4 x i32> %115, %90
  %139 = sub <4 x i32> %117, %91
  %140 = sub <4 x i32> %119, %92
  %141 = sub <4 x i32> %121, %93
  %142 = sub <4 x i32> %123, %94
  %143 = sub <4 x i32> %125, %95
  %144 = sub <4 x i32> %127, %96
  %145 = ashr <4 x i32> %98, <i32 14, i32 14, i32 14, i32 14>
  %146 = ashr <4 x i32> %100, <i32 14, i32 14, i32 14, i32 14>
  %147 = ashr <4 x i32> %102, <i32 14, i32 14, i32 14, i32 14>
  %148 = ashr <4 x i32> %104, <i32 14, i32 14, i32 14, i32 14>
  %149 = ashr <4 x i32> %106, <i32 14, i32 14, i32 14, i32 14>
  %150 = ashr <4 x i32> %108, <i32 14, i32 14, i32 14, i32 14>
  %151 = ashr <4 x i32> %110, <i32 14, i32 14, i32 14, i32 14>
  %152 = ashr <4 x i32> %112, <i32 14, i32 14, i32 14, i32 14>
  %153 = ashr <4 x i32> %114, <i32 14, i32 14, i32 14, i32 14>
  %154 = ashr <4 x i32> %116, <i32 14, i32 14, i32 14, i32 14>
  %155 = ashr <4 x i32> %118, <i32 14, i32 14, i32 14, i32 14>
  %156 = ashr <4 x i32> %120, <i32 14, i32 14, i32 14, i32 14>
  %157 = ashr <4 x i32> %122, <i32 14, i32 14, i32 14, i32 14>
  %158 = ashr <4 x i32> %124, <i32 14, i32 14, i32 14, i32 14>
  %159 = ashr <4 x i32> %126, <i32 14, i32 14, i32 14, i32 14>
  %160 = ashr <4 x i32> %128, <i32 14, i32 14, i32 14, i32 14>
  %161 = ashr <4 x i32> %129, <i32 14, i32 14, i32 14, i32 14>
  %162 = ashr <4 x i32> %130, <i32 14, i32 14, i32 14, i32 14>
  %163 = ashr <4 x i32> %131, <i32 14, i32 14, i32 14, i32 14>
  %164 = ashr <4 x i32> %132, <i32 14, i32 14, i32 14, i32 14>
  %165 = ashr <4 x i32> %133, <i32 14, i32 14, i32 14, i32 14>
  %166 = ashr <4 x i32> %134, <i32 14, i32 14, i32 14, i32 14>
  %167 = ashr <4 x i32> %135, <i32 14, i32 14, i32 14, i32 14>
  %168 = ashr <4 x i32> %136, <i32 14, i32 14, i32 14, i32 14>
  %169 = ashr <4 x i32> %137, <i32 14, i32 14, i32 14, i32 14>
  %170 = ashr <4 x i32> %138, <i32 14, i32 14, i32 14, i32 14>
  %171 = ashr <4 x i32> %139, <i32 14, i32 14, i32 14, i32 14>
  %172 = ashr <4 x i32> %140, <i32 14, i32 14, i32 14, i32 14>
  %173 = ashr <4 x i32> %141, <i32 14, i32 14, i32 14, i32 14>
  %174 = ashr <4 x i32> %142, <i32 14, i32 14, i32 14, i32 14>
  %175 = ashr <4 x i32> %143, <i32 14, i32 14, i32 14, i32 14>
  %176 = ashr <4 x i32> %144, <i32 14, i32 14, i32 14, i32 14>
  %177 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %145, <4 x i32> %146) #6
  %178 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %147, <4 x i32> %148) #6
  %179 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %149, <4 x i32> %150) #6
  %180 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %151, <4 x i32> %152) #6
  %181 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %153, <4 x i32> %154) #6
  %182 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %155, <4 x i32> %156) #6
  %183 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %157, <4 x i32> %158) #6
  %184 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %159, <4 x i32> %160) #6
  %185 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %161, <4 x i32> %162) #6
  %186 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %163, <4 x i32> %164) #6
  %187 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %165, <4 x i32> %166) #6
  %188 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %167, <4 x i32> %168) #6
  %189 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %169, <4 x i32> %170) #6
  %190 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %171, <4 x i32> %172) #6
  %191 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %173, <4 x i32> %174) #6
  %192 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %175, <4 x i32> %176) #6
  %193 = shufflevector <8 x i16> %185, <8 x i16> %186, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %194 = shufflevector <8 x i16> %185, <8 x i16> %186, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %195 = shufflevector <8 x i16> %187, <8 x i16> %188, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %196 = shufflevector <8 x i16> %187, <8 x i16> %188, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %197 = shufflevector <8 x i16> %189, <8 x i16> %190, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %198 = shufflevector <8 x i16> %189, <8 x i16> %190, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %199 = shufflevector <8 x i16> %191, <8 x i16> %192, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %200 = shufflevector <8 x i16> %191, <8 x i16> %192, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %201 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %193, <8 x i16> <i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196>) #6
  %202 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %194, <8 x i16> <i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196>) #6
  %203 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %193, <8 x i16> <i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069>) #6
  %204 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %194, <8 x i16> <i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069>) #6
  %205 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %195, <8 x i16> <i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623>) #6
  %206 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %196, <8 x i16> <i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623>) #6
  %207 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %195, <8 x i16> <i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102>) #6
  %208 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %196, <8 x i16> <i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102>) #6
  %209 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %197, <8 x i16> <i16 -3196, i16 16069, i16 -3196, i16 16069, i16 -3196, i16 16069, i16 -3196, i16 16069>) #6
  %210 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %198, <8 x i16> <i16 -3196, i16 16069, i16 -3196, i16 16069, i16 -3196, i16 16069, i16 -3196, i16 16069>) #6
  %211 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %197, <8 x i16> <i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196>) #6
  %212 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %198, <8 x i16> <i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196>) #6
  %213 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %199, <8 x i16> <i16 -13623, i16 9102, i16 -13623, i16 9102, i16 -13623, i16 9102, i16 -13623, i16 9102>) #6
  %214 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %200, <8 x i16> <i16 -13623, i16 9102, i16 -13623, i16 9102, i16 -13623, i16 9102, i16 -13623, i16 9102>) #6
  %215 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %199, <8 x i16> <i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623>) #6
  %216 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %200, <8 x i16> <i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623>) #6
  %217 = add <4 x i32> %201, <i32 8192, i32 8192, i32 8192, i32 8192>
  %218 = add <4 x i32> %217, %209
  %219 = add <4 x i32> %202, <i32 8192, i32 8192, i32 8192, i32 8192>
  %220 = add <4 x i32> %219, %210
  %221 = add <4 x i32> %203, <i32 8192, i32 8192, i32 8192, i32 8192>
  %222 = add <4 x i32> %221, %211
  %223 = add <4 x i32> %204, <i32 8192, i32 8192, i32 8192, i32 8192>
  %224 = add <4 x i32> %223, %212
  %225 = add <4 x i32> %205, <i32 8192, i32 8192, i32 8192, i32 8192>
  %226 = add <4 x i32> %225, %213
  %227 = add <4 x i32> %206, <i32 8192, i32 8192, i32 8192, i32 8192>
  %228 = add <4 x i32> %227, %214
  %229 = add <4 x i32> %207, <i32 8192, i32 8192, i32 8192, i32 8192>
  %230 = add <4 x i32> %229, %215
  %231 = add <4 x i32> %208, <i32 8192, i32 8192, i32 8192, i32 8192>
  %232 = add <4 x i32> %231, %216
  %233 = sub <4 x i32> %217, %209
  %234 = sub <4 x i32> %219, %210
  %235 = sub <4 x i32> %221, %211
  %236 = sub <4 x i32> %223, %212
  %237 = sub <4 x i32> %225, %213
  %238 = sub <4 x i32> %227, %214
  %239 = sub <4 x i32> %229, %215
  %240 = sub <4 x i32> %231, %216
  %241 = ashr <4 x i32> %218, <i32 14, i32 14, i32 14, i32 14>
  %242 = ashr <4 x i32> %220, <i32 14, i32 14, i32 14, i32 14>
  %243 = ashr <4 x i32> %222, <i32 14, i32 14, i32 14, i32 14>
  %244 = ashr <4 x i32> %224, <i32 14, i32 14, i32 14, i32 14>
  %245 = ashr <4 x i32> %226, <i32 14, i32 14, i32 14, i32 14>
  %246 = ashr <4 x i32> %228, <i32 14, i32 14, i32 14, i32 14>
  %247 = ashr <4 x i32> %230, <i32 14, i32 14, i32 14, i32 14>
  %248 = ashr <4 x i32> %232, <i32 14, i32 14, i32 14, i32 14>
  %249 = ashr <4 x i32> %233, <i32 14, i32 14, i32 14, i32 14>
  %250 = ashr <4 x i32> %234, <i32 14, i32 14, i32 14, i32 14>
  %251 = ashr <4 x i32> %235, <i32 14, i32 14, i32 14, i32 14>
  %252 = ashr <4 x i32> %236, <i32 14, i32 14, i32 14, i32 14>
  %253 = ashr <4 x i32> %237, <i32 14, i32 14, i32 14, i32 14>
  %254 = ashr <4 x i32> %238, <i32 14, i32 14, i32 14, i32 14>
  %255 = ashr <4 x i32> %239, <i32 14, i32 14, i32 14, i32 14>
  %256 = ashr <4 x i32> %240, <i32 14, i32 14, i32 14, i32 14>
  %257 = add <8 x i16> %181, %177
  %258 = add <8 x i16> %182, %178
  %259 = add <8 x i16> %183, %179
  %260 = add <8 x i16> %184, %180
  %261 = sub <8 x i16> %177, %181
  %262 = sub <8 x i16> %178, %182
  %263 = sub <8 x i16> %179, %183
  %264 = sub <8 x i16> %180, %184
  %265 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %241, <4 x i32> %242) #6
  %266 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %243, <4 x i32> %244) #6
  %267 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %245, <4 x i32> %246) #6
  %268 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %247, <4 x i32> %248) #6
  %269 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %249, <4 x i32> %250) #6
  %270 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %251, <4 x i32> %252) #6
  %271 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %253, <4 x i32> %254) #6
  %272 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %255, <4 x i32> %256) #6
  %273 = shufflevector <8 x i16> %261, <8 x i16> %262, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %274 = shufflevector <8 x i16> %261, <8 x i16> %262, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %275 = shufflevector <8 x i16> %263, <8 x i16> %264, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %276 = shufflevector <8 x i16> %263, <8 x i16> %264, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %277 = shufflevector <8 x i16> %269, <8 x i16> %270, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %278 = shufflevector <8 x i16> %269, <8 x i16> %270, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %279 = shufflevector <8 x i16> %271, <8 x i16> %272, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %280 = shufflevector <8 x i16> %271, <8 x i16> %272, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %281 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %273, <8 x i16> <i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270>) #6
  %282 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %274, <8 x i16> <i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270>) #6
  %283 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %273, <8 x i16> <i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137>) #6
  %284 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %274, <8 x i16> <i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137>) #6
  %285 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %275, <8 x i16> <i16 -6270, i16 15137, i16 -6270, i16 15137, i16 -6270, i16 15137, i16 -6270, i16 15137>) #6
  %286 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %276, <8 x i16> <i16 -6270, i16 15137, i16 -6270, i16 15137, i16 -6270, i16 15137, i16 -6270, i16 15137>) #6
  %287 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %275, <8 x i16> <i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270>) #6
  %288 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %276, <8 x i16> <i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270>) #6
  %289 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %277, <8 x i16> <i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270>) #6
  %290 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %278, <8 x i16> <i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270>) #6
  %291 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %277, <8 x i16> <i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137>) #6
  %292 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %278, <8 x i16> <i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137>) #6
  %293 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %279, <8 x i16> <i16 -6270, i16 15137, i16 -6270, i16 15137, i16 -6270, i16 15137, i16 -6270, i16 15137>) #6
  %294 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %280, <8 x i16> <i16 -6270, i16 15137, i16 -6270, i16 15137, i16 -6270, i16 15137, i16 -6270, i16 15137>) #6
  %295 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %279, <8 x i16> <i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270>) #6
  %296 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %280, <8 x i16> <i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270>) #6
  %297 = add <4 x i32> %281, <i32 8192, i32 8192, i32 8192, i32 8192>
  %298 = add <4 x i32> %297, %285
  %299 = add <4 x i32> %282, <i32 8192, i32 8192, i32 8192, i32 8192>
  %300 = add <4 x i32> %299, %286
  %301 = add <4 x i32> %283, <i32 8192, i32 8192, i32 8192, i32 8192>
  %302 = add <4 x i32> %301, %287
  %303 = add <4 x i32> %284, <i32 8192, i32 8192, i32 8192, i32 8192>
  %304 = add <4 x i32> %303, %288
  %305 = sub <4 x i32> %297, %285
  %306 = sub <4 x i32> %299, %286
  %307 = sub <4 x i32> %301, %287
  %308 = sub <4 x i32> %303, %288
  %309 = add <4 x i32> %289, <i32 8192, i32 8192, i32 8192, i32 8192>
  %310 = add <4 x i32> %309, %293
  %311 = add <4 x i32> %290, <i32 8192, i32 8192, i32 8192, i32 8192>
  %312 = add <4 x i32> %311, %294
  %313 = add <4 x i32> %291, <i32 8192, i32 8192, i32 8192, i32 8192>
  %314 = add <4 x i32> %313, %295
  %315 = add <4 x i32> %292, <i32 8192, i32 8192, i32 8192, i32 8192>
  %316 = add <4 x i32> %315, %296
  %317 = sub <4 x i32> %309, %293
  %318 = sub <4 x i32> %311, %294
  %319 = sub <4 x i32> %313, %295
  %320 = sub <4 x i32> %315, %296
  %321 = ashr <4 x i32> %298, <i32 14, i32 14, i32 14, i32 14>
  %322 = ashr <4 x i32> %300, <i32 14, i32 14, i32 14, i32 14>
  %323 = ashr <4 x i32> %302, <i32 14, i32 14, i32 14, i32 14>
  %324 = ashr <4 x i32> %304, <i32 14, i32 14, i32 14, i32 14>
  %325 = ashr <4 x i32> %305, <i32 14, i32 14, i32 14, i32 14>
  %326 = ashr <4 x i32> %306, <i32 14, i32 14, i32 14, i32 14>
  %327 = ashr <4 x i32> %307, <i32 14, i32 14, i32 14, i32 14>
  %328 = ashr <4 x i32> %308, <i32 14, i32 14, i32 14, i32 14>
  %329 = ashr <4 x i32> %310, <i32 14, i32 14, i32 14, i32 14>
  %330 = ashr <4 x i32> %312, <i32 14, i32 14, i32 14, i32 14>
  %331 = ashr <4 x i32> %314, <i32 14, i32 14, i32 14, i32 14>
  %332 = ashr <4 x i32> %316, <i32 14, i32 14, i32 14, i32 14>
  %333 = ashr <4 x i32> %317, <i32 14, i32 14, i32 14, i32 14>
  %334 = ashr <4 x i32> %318, <i32 14, i32 14, i32 14, i32 14>
  %335 = ashr <4 x i32> %319, <i32 14, i32 14, i32 14, i32 14>
  %336 = ashr <4 x i32> %320, <i32 14, i32 14, i32 14, i32 14>
  %337 = add <8 x i16> %259, %257
  %338 = add <8 x i16> %258, %260
  %339 = sub <8 x i16> %257, %259
  %340 = sub <8 x i16> %258, %260
  %341 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %321, <4 x i32> %322) #6
  %342 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %323, <4 x i32> %324) #6
  %343 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %325, <4 x i32> %326) #6
  %344 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %327, <4 x i32> %328) #6
  %345 = add <8 x i16> %265, %267
  %346 = add <8 x i16> %268, %266
  %347 = sub <8 x i16> %265, %267
  %348 = sub <8 x i16> %266, %268
  %349 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %329, <4 x i32> %330) #6
  %350 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %331, <4 x i32> %332) #6
  %351 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %333, <4 x i32> %334) #6
  %352 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %335, <4 x i32> %336) #6
  %353 = shufflevector <8 x i16> %339, <8 x i16> %340, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %354 = shufflevector <8 x i16> %339, <8 x i16> %340, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %355 = shufflevector <8 x i16> %343, <8 x i16> %344, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %356 = shufflevector <8 x i16> %343, <8 x i16> %344, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %357 = shufflevector <8 x i16> %347, <8 x i16> %348, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %358 = shufflevector <8 x i16> %347, <8 x i16> %348, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %359 = shufflevector <8 x i16> %351, <8 x i16> %352, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %360 = shufflevector <8 x i16> %351, <8 x i16> %352, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %361 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %353, <8 x i16> <i16 -11585, i16 -11585, i16 -11585, i16 -11585, i16 -11585, i16 -11585, i16 -11585, i16 -11585>) #6
  %362 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %354, <8 x i16> <i16 -11585, i16 -11585, i16 -11585, i16 -11585, i16 -11585, i16 -11585, i16 -11585, i16 -11585>) #6
  %363 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %353, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %364 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %354, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %365 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %355, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %366 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %356, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %367 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %355, <8 x i16> <i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585>) #6
  %368 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %356, <8 x i16> <i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585>) #6
  %369 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %357, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %370 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %358, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %371 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %357, <8 x i16> <i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585>) #6
  %372 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %358, <8 x i16> <i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585>) #6
  %373 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %359, <8 x i16> <i16 -11585, i16 -11585, i16 -11585, i16 -11585, i16 -11585, i16 -11585, i16 -11585, i16 -11585>) #6
  %374 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %360, <8 x i16> <i16 -11585, i16 -11585, i16 -11585, i16 -11585, i16 -11585, i16 -11585, i16 -11585, i16 -11585>) #6
  %375 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %359, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %376 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %360, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %377 = add <4 x i32> %361, <i32 8192, i32 8192, i32 8192, i32 8192>
  %378 = add <4 x i32> %362, <i32 8192, i32 8192, i32 8192, i32 8192>
  %379 = add <4 x i32> %363, <i32 8192, i32 8192, i32 8192, i32 8192>
  %380 = add <4 x i32> %364, <i32 8192, i32 8192, i32 8192, i32 8192>
  %381 = add <4 x i32> %365, <i32 8192, i32 8192, i32 8192, i32 8192>
  %382 = add <4 x i32> %366, <i32 8192, i32 8192, i32 8192, i32 8192>
  %383 = add <4 x i32> %367, <i32 8192, i32 8192, i32 8192, i32 8192>
  %384 = add <4 x i32> %368, <i32 8192, i32 8192, i32 8192, i32 8192>
  %385 = add <4 x i32> %369, <i32 8192, i32 8192, i32 8192, i32 8192>
  %386 = add <4 x i32> %370, <i32 8192, i32 8192, i32 8192, i32 8192>
  %387 = add <4 x i32> %371, <i32 8192, i32 8192, i32 8192, i32 8192>
  %388 = add <4 x i32> %372, <i32 8192, i32 8192, i32 8192, i32 8192>
  %389 = add <4 x i32> %373, <i32 8192, i32 8192, i32 8192, i32 8192>
  %390 = add <4 x i32> %374, <i32 8192, i32 8192, i32 8192, i32 8192>
  %391 = add <4 x i32> %375, <i32 8192, i32 8192, i32 8192, i32 8192>
  %392 = add <4 x i32> %376, <i32 8192, i32 8192, i32 8192, i32 8192>
  %393 = ashr <4 x i32> %377, <i32 14, i32 14, i32 14, i32 14>
  %394 = ashr <4 x i32> %378, <i32 14, i32 14, i32 14, i32 14>
  %395 = ashr <4 x i32> %379, <i32 14, i32 14, i32 14, i32 14>
  %396 = ashr <4 x i32> %380, <i32 14, i32 14, i32 14, i32 14>
  %397 = ashr <4 x i32> %381, <i32 14, i32 14, i32 14, i32 14>
  %398 = ashr <4 x i32> %382, <i32 14, i32 14, i32 14, i32 14>
  %399 = ashr <4 x i32> %383, <i32 14, i32 14, i32 14, i32 14>
  %400 = ashr <4 x i32> %384, <i32 14, i32 14, i32 14, i32 14>
  %401 = ashr <4 x i32> %385, <i32 14, i32 14, i32 14, i32 14>
  %402 = ashr <4 x i32> %386, <i32 14, i32 14, i32 14, i32 14>
  %403 = ashr <4 x i32> %387, <i32 14, i32 14, i32 14, i32 14>
  %404 = ashr <4 x i32> %388, <i32 14, i32 14, i32 14, i32 14>
  %405 = ashr <4 x i32> %389, <i32 14, i32 14, i32 14, i32 14>
  %406 = ashr <4 x i32> %390, <i32 14, i32 14, i32 14, i32 14>
  %407 = ashr <4 x i32> %391, <i32 14, i32 14, i32 14, i32 14>
  %408 = ashr <4 x i32> %392, <i32 14, i32 14, i32 14, i32 14>
  store <8 x i16> %337, <8 x i16>* %5, align 16
  %409 = sub <8 x i16> zeroinitializer, %345
  store <8 x i16> %409, <8 x i16>* %58, align 16
  store <8 x i16> %349, <8 x i16>* %13, align 16
  %410 = sub <8 x i16> zeroinitializer, %341
  store <8 x i16> %410, <8 x i16>* %50, align 16
  %411 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %397, <4 x i32> %398) #6
  store <8 x i16> %411, <8 x i16>* %21, align 16
  %412 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %405, <4 x i32> %406) #6
  store <8 x i16> %412, <8 x i16>* %42, align 16
  %413 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %401, <4 x i32> %402) #6
  store <8 x i16> %413, <8 x i16>* %29, align 16
  %414 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %393, <4 x i32> %394) #6
  store <8 x i16> %414, <8 x i16>* %34, align 16
  %415 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %395, <4 x i32> %396) #6
  store <8 x i16> %415, <8 x i16>* %37, align 16
  %416 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %403, <4 x i32> %404) #6
  store <8 x i16> %416, <8 x i16>* %26, align 16
  %417 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %407, <4 x i32> %408) #6
  store <8 x i16> %417, <8 x i16>* %45, align 16
  %418 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %399, <4 x i32> %400) #6
  store <8 x i16> %418, <8 x i16>* %18, align 16
  store <8 x i16> %342, <8 x i16>* %53, align 16
  %419 = sub <8 x i16> zeroinitializer, %350
  store <8 x i16> %419, <8 x i16>* %10, align 16
  store <8 x i16> %346, <8 x i16>* %61, align 16
  %420 = sub <8 x i16> zeroinitializer, %338
  store <8 x i16> %420, <8 x i16>* %3, align 16
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define internal fastcc void @fdct16_8col(<2 x i64>* nocapture) unnamed_addr #3 {
  %2 = bitcast <2 x i64>* %0 to <8 x i16>*
  %3 = load <8 x i16>, <8 x i16>* %2, align 16
  %4 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 15
  %5 = bitcast <2 x i64>* %4 to <8 x i16>*
  %6 = load <8 x i16>, <8 x i16>* %5, align 16
  %7 = add <8 x i16> %6, %3
  %8 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %9 = bitcast <2 x i64>* %8 to <8 x i16>*
  %10 = load <8 x i16>, <8 x i16>* %9, align 16
  %11 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 14
  %12 = bitcast <2 x i64>* %11 to <8 x i16>*
  %13 = load <8 x i16>, <8 x i16>* %12, align 16
  %14 = add <8 x i16> %13, %10
  %15 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %16 = bitcast <2 x i64>* %15 to <8 x i16>*
  %17 = load <8 x i16>, <8 x i16>* %16, align 16
  %18 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 13
  %19 = bitcast <2 x i64>* %18 to <8 x i16>*
  %20 = load <8 x i16>, <8 x i16>* %19, align 16
  %21 = add <8 x i16> %20, %17
  %22 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %23 = bitcast <2 x i64>* %22 to <8 x i16>*
  %24 = load <8 x i16>, <8 x i16>* %23, align 16
  %25 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 12
  %26 = bitcast <2 x i64>* %25 to <8 x i16>*
  %27 = load <8 x i16>, <8 x i16>* %26, align 16
  %28 = add <8 x i16> %27, %24
  %29 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 4
  %30 = bitcast <2 x i64>* %29 to <8 x i16>*
  %31 = load <8 x i16>, <8 x i16>* %30, align 16
  %32 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 11
  %33 = bitcast <2 x i64>* %32 to <8 x i16>*
  %34 = load <8 x i16>, <8 x i16>* %33, align 16
  %35 = add <8 x i16> %34, %31
  %36 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 5
  %37 = bitcast <2 x i64>* %36 to <8 x i16>*
  %38 = load <8 x i16>, <8 x i16>* %37, align 16
  %39 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 10
  %40 = bitcast <2 x i64>* %39 to <8 x i16>*
  %41 = load <8 x i16>, <8 x i16>* %40, align 16
  %42 = add <8 x i16> %41, %38
  %43 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 6
  %44 = bitcast <2 x i64>* %43 to <8 x i16>*
  %45 = load <8 x i16>, <8 x i16>* %44, align 16
  %46 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 9
  %47 = bitcast <2 x i64>* %46 to <8 x i16>*
  %48 = load <8 x i16>, <8 x i16>* %47, align 16
  %49 = add <8 x i16> %48, %45
  %50 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 7
  %51 = bitcast <2 x i64>* %50 to <8 x i16>*
  %52 = load <8 x i16>, <8 x i16>* %51, align 16
  %53 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 8
  %54 = bitcast <2 x i64>* %53 to <8 x i16>*
  %55 = load <8 x i16>, <8 x i16>* %54, align 16
  %56 = add <8 x i16> %55, %52
  %57 = sub <8 x i16> %52, %55
  %58 = sub <8 x i16> %45, %48
  %59 = sub <8 x i16> %38, %41
  %60 = sub <8 x i16> %31, %34
  %61 = sub <8 x i16> %24, %27
  %62 = sub <8 x i16> %17, %20
  %63 = sub <8 x i16> %10, %13
  %64 = sub <8 x i16> %3, %6
  %65 = add <8 x i16> %56, %7
  %66 = add <8 x i16> %49, %14
  %67 = add <8 x i16> %42, %21
  %68 = add <8 x i16> %35, %28
  %69 = sub <8 x i16> %28, %35
  %70 = sub <8 x i16> %21, %42
  %71 = sub <8 x i16> %14, %49
  %72 = sub <8 x i16> %7, %56
  %73 = add <8 x i16> %65, %68
  %74 = add <8 x i16> %66, %67
  %75 = sub <8 x i16> %66, %67
  %76 = sub <8 x i16> %65, %68
  %77 = shufflevector <8 x i16> %73, <8 x i16> %74, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %78 = shufflevector <8 x i16> %73, <8 x i16> %74, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %79 = shufflevector <8 x i16> %75, <8 x i16> %76, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %80 = shufflevector <8 x i16> %75, <8 x i16> %76, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %81 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %77, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %82 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %78, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %83 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %77, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %84 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %78, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %85 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %79, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %86 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %80, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %87 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %79, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %88 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %80, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %89 = add <4 x i32> %81, <i32 8192, i32 8192, i32 8192, i32 8192>
  %90 = add <4 x i32> %82, <i32 8192, i32 8192, i32 8192, i32 8192>
  %91 = add <4 x i32> %83, <i32 8192, i32 8192, i32 8192, i32 8192>
  %92 = add <4 x i32> %84, <i32 8192, i32 8192, i32 8192, i32 8192>
  %93 = add <4 x i32> %85, <i32 8192, i32 8192, i32 8192, i32 8192>
  %94 = add <4 x i32> %86, <i32 8192, i32 8192, i32 8192, i32 8192>
  %95 = add <4 x i32> %87, <i32 8192, i32 8192, i32 8192, i32 8192>
  %96 = add <4 x i32> %88, <i32 8192, i32 8192, i32 8192, i32 8192>
  %97 = ashr <4 x i32> %89, <i32 14, i32 14, i32 14, i32 14>
  %98 = ashr <4 x i32> %90, <i32 14, i32 14, i32 14, i32 14>
  %99 = ashr <4 x i32> %91, <i32 14, i32 14, i32 14, i32 14>
  %100 = ashr <4 x i32> %92, <i32 14, i32 14, i32 14, i32 14>
  %101 = ashr <4 x i32> %93, <i32 14, i32 14, i32 14, i32 14>
  %102 = ashr <4 x i32> %94, <i32 14, i32 14, i32 14, i32 14>
  %103 = ashr <4 x i32> %95, <i32 14, i32 14, i32 14, i32 14>
  %104 = ashr <4 x i32> %96, <i32 14, i32 14, i32 14, i32 14>
  %105 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %97, <4 x i32> %98) #6
  store <8 x i16> %105, <8 x i16>* %2, align 16
  %106 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %101, <4 x i32> %102) #6
  store <8 x i16> %106, <8 x i16>* %30, align 16
  %107 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %99, <4 x i32> %100) #6
  store <8 x i16> %107, <8 x i16>* %54, align 16
  %108 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %103, <4 x i32> %104) #6
  store <8 x i16> %108, <8 x i16>* %26, align 16
  %109 = shufflevector <8 x i16> %70, <8 x i16> %71, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %110 = shufflevector <8 x i16> %70, <8 x i16> %71, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %111 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %109, <8 x i16> <i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585>) #6
  %112 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %110, <8 x i16> <i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585>) #6
  %113 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %109, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %114 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %110, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %115 = add <4 x i32> %111, <i32 8192, i32 8192, i32 8192, i32 8192>
  %116 = add <4 x i32> %112, <i32 8192, i32 8192, i32 8192, i32 8192>
  %117 = add <4 x i32> %113, <i32 8192, i32 8192, i32 8192, i32 8192>
  %118 = add <4 x i32> %114, <i32 8192, i32 8192, i32 8192, i32 8192>
  %119 = ashr <4 x i32> %115, <i32 14, i32 14, i32 14, i32 14>
  %120 = ashr <4 x i32> %116, <i32 14, i32 14, i32 14, i32 14>
  %121 = ashr <4 x i32> %117, <i32 14, i32 14, i32 14, i32 14>
  %122 = ashr <4 x i32> %118, <i32 14, i32 14, i32 14, i32 14>
  %123 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %119, <4 x i32> %120) #6
  %124 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %121, <4 x i32> %122) #6
  %125 = add <8 x i16> %123, %69
  %126 = sub <8 x i16> %69, %123
  %127 = sub <8 x i16> %72, %124
  %128 = add <8 x i16> %124, %72
  %129 = shufflevector <8 x i16> %125, <8 x i16> %128, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %130 = shufflevector <8 x i16> %125, <8 x i16> %128, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %131 = shufflevector <8 x i16> %126, <8 x i16> %127, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %132 = shufflevector <8 x i16> %126, <8 x i16> %127, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %133 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %129, <8 x i16> <i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069>) #6
  %134 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %130, <8 x i16> <i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069>) #6
  %135 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %131, <8 x i16> <i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102>) #6
  %136 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %132, <8 x i16> <i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102>) #6
  %137 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %131, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %138 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %132, <8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>) #6
  %139 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %129, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %140 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %130, <8 x i16> <i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196>) #6
  %141 = add <4 x i32> %133, <i32 8192, i32 8192, i32 8192, i32 8192>
  %142 = add <4 x i32> %134, <i32 8192, i32 8192, i32 8192, i32 8192>
  %143 = add <4 x i32> %135, <i32 8192, i32 8192, i32 8192, i32 8192>
  %144 = add <4 x i32> %136, <i32 8192, i32 8192, i32 8192, i32 8192>
  %145 = add <4 x i32> %137, <i32 8192, i32 8192, i32 8192, i32 8192>
  %146 = add <4 x i32> %138, <i32 8192, i32 8192, i32 8192, i32 8192>
  %147 = add <4 x i32> %139, <i32 8192, i32 8192, i32 8192, i32 8192>
  %148 = add <4 x i32> %140, <i32 8192, i32 8192, i32 8192, i32 8192>
  %149 = ashr <4 x i32> %141, <i32 14, i32 14, i32 14, i32 14>
  %150 = ashr <4 x i32> %142, <i32 14, i32 14, i32 14, i32 14>
  %151 = ashr <4 x i32> %143, <i32 14, i32 14, i32 14, i32 14>
  %152 = ashr <4 x i32> %144, <i32 14, i32 14, i32 14, i32 14>
  %153 = ashr <4 x i32> %145, <i32 14, i32 14, i32 14, i32 14>
  %154 = ashr <4 x i32> %146, <i32 14, i32 14, i32 14, i32 14>
  %155 = ashr <4 x i32> %147, <i32 14, i32 14, i32 14, i32 14>
  %156 = ashr <4 x i32> %148, <i32 14, i32 14, i32 14, i32 14>
  %157 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %149, <4 x i32> %150) #6
  store <8 x i16> %157, <8 x i16>* %16, align 16
  %158 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %153, <4 x i32> %154) #6
  store <8 x i16> %158, <8 x i16>* %44, align 16
  %159 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %151, <4 x i32> %152) #6
  store <8 x i16> %159, <8 x i16>* %40, align 16
  %160 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %155, <4 x i32> %156) #6
  store <8 x i16> %160, <8 x i16>* %12, align 16
  %161 = shufflevector <8 x i16> %59, <8 x i16> %62, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %162 = shufflevector <8 x i16> %59, <8 x i16> %62, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %163 = shufflevector <8 x i16> %60, <8 x i16> %61, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %164 = shufflevector <8 x i16> %60, <8 x i16> %61, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %165 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %161, <8 x i16> <i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585>) #6
  %166 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %162, <8 x i16> <i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585>) #6
  %167 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %163, <8 x i16> <i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585>) #6
  %168 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %164, <8 x i16> <i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585>) #6
  %169 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %163, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %170 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %164, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %171 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %161, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %172 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %162, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %173 = add <4 x i32> %165, <i32 8192, i32 8192, i32 8192, i32 8192>
  %174 = add <4 x i32> %166, <i32 8192, i32 8192, i32 8192, i32 8192>
  %175 = add <4 x i32> %167, <i32 8192, i32 8192, i32 8192, i32 8192>
  %176 = add <4 x i32> %168, <i32 8192, i32 8192, i32 8192, i32 8192>
  %177 = add <4 x i32> %169, <i32 8192, i32 8192, i32 8192, i32 8192>
  %178 = add <4 x i32> %170, <i32 8192, i32 8192, i32 8192, i32 8192>
  %179 = add <4 x i32> %171, <i32 8192, i32 8192, i32 8192, i32 8192>
  %180 = add <4 x i32> %172, <i32 8192, i32 8192, i32 8192, i32 8192>
  %181 = ashr <4 x i32> %173, <i32 14, i32 14, i32 14, i32 14>
  %182 = ashr <4 x i32> %174, <i32 14, i32 14, i32 14, i32 14>
  %183 = ashr <4 x i32> %175, <i32 14, i32 14, i32 14, i32 14>
  %184 = ashr <4 x i32> %176, <i32 14, i32 14, i32 14, i32 14>
  %185 = ashr <4 x i32> %177, <i32 14, i32 14, i32 14, i32 14>
  %186 = ashr <4 x i32> %178, <i32 14, i32 14, i32 14, i32 14>
  %187 = ashr <4 x i32> %179, <i32 14, i32 14, i32 14, i32 14>
  %188 = ashr <4 x i32> %180, <i32 14, i32 14, i32 14, i32 14>
  %189 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %181, <4 x i32> %182) #6
  %190 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %183, <4 x i32> %184) #6
  %191 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %185, <4 x i32> %186) #6
  %192 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %187, <4 x i32> %188) #6
  %193 = add <8 x i16> %190, %57
  %194 = add <8 x i16> %189, %58
  %195 = sub <8 x i16> %58, %189
  %196 = sub <8 x i16> %57, %190
  %197 = sub <8 x i16> %64, %191
  %198 = sub <8 x i16> %63, %192
  %199 = add <8 x i16> %192, %63
  %200 = add <8 x i16> %191, %64
  %201 = shufflevector <8 x i16> %194, <8 x i16> %199, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %202 = shufflevector <8 x i16> %194, <8 x i16> %199, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %203 = shufflevector <8 x i16> %195, <8 x i16> %198, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %204 = shufflevector <8 x i16> %195, <8 x i16> %198, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %205 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %201, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %206 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %202, <8 x i16> <i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270>) #6
  %207 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %203, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %208 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %204, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %209 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %203, <8 x i16> <i16 15137, i16 -6270, i16 15137, i16 -6270, i16 15137, i16 -6270, i16 15137, i16 -6270>) #6
  %210 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %204, <8 x i16> <i16 15137, i16 -6270, i16 15137, i16 -6270, i16 15137, i16 -6270, i16 15137, i16 -6270>) #6
  %211 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %201, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %212 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %202, <8 x i16> <i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137>) #6
  %213 = add <4 x i32> %205, <i32 8192, i32 8192, i32 8192, i32 8192>
  %214 = add <4 x i32> %206, <i32 8192, i32 8192, i32 8192, i32 8192>
  %215 = add <4 x i32> %207, <i32 8192, i32 8192, i32 8192, i32 8192>
  %216 = add <4 x i32> %208, <i32 8192, i32 8192, i32 8192, i32 8192>
  %217 = add <4 x i32> %209, <i32 8192, i32 8192, i32 8192, i32 8192>
  %218 = add <4 x i32> %210, <i32 8192, i32 8192, i32 8192, i32 8192>
  %219 = add <4 x i32> %211, <i32 8192, i32 8192, i32 8192, i32 8192>
  %220 = add <4 x i32> %212, <i32 8192, i32 8192, i32 8192, i32 8192>
  %221 = ashr <4 x i32> %213, <i32 14, i32 14, i32 14, i32 14>
  %222 = ashr <4 x i32> %214, <i32 14, i32 14, i32 14, i32 14>
  %223 = ashr <4 x i32> %215, <i32 14, i32 14, i32 14, i32 14>
  %224 = ashr <4 x i32> %216, <i32 14, i32 14, i32 14, i32 14>
  %225 = ashr <4 x i32> %217, <i32 14, i32 14, i32 14, i32 14>
  %226 = ashr <4 x i32> %218, <i32 14, i32 14, i32 14, i32 14>
  %227 = ashr <4 x i32> %219, <i32 14, i32 14, i32 14, i32 14>
  %228 = ashr <4 x i32> %220, <i32 14, i32 14, i32 14, i32 14>
  %229 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %221, <4 x i32> %222) #6
  %230 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %223, <4 x i32> %224) #6
  %231 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %225, <4 x i32> %226) #6
  %232 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %227, <4 x i32> %228) #6
  %233 = add <8 x i16> %229, %193
  %234 = sub <8 x i16> %193, %229
  %235 = add <8 x i16> %230, %196
  %236 = sub <8 x i16> %196, %230
  %237 = sub <8 x i16> %197, %231
  %238 = add <8 x i16> %231, %197
  %239 = sub <8 x i16> %200, %232
  %240 = add <8 x i16> %232, %200
  %241 = shufflevector <8 x i16> %233, <8 x i16> %240, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %242 = shufflevector <8 x i16> %233, <8 x i16> %240, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %243 = shufflevector <8 x i16> %234, <8 x i16> %239, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %244 = shufflevector <8 x i16> %234, <8 x i16> %239, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %245 = shufflevector <8 x i16> %235, <8 x i16> %238, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %246 = shufflevector <8 x i16> %235, <8 x i16> %238, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %247 = shufflevector <8 x i16> %236, <8 x i16> %237, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %248 = shufflevector <8 x i16> %236, <8 x i16> %237, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %249 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %241, <8 x i16> <i16 1606, i16 16305, i16 1606, i16 16305, i16 1606, i16 16305, i16 1606, i16 16305>) #6
  %250 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %242, <8 x i16> <i16 1606, i16 16305, i16 1606, i16 16305, i16 1606, i16 16305, i16 1606, i16 16305>) #6
  %251 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %243, <8 x i16> <i16 12665, i16 10394, i16 12665, i16 10394, i16 12665, i16 10394, i16 12665, i16 10394>) #6
  %252 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %244, <8 x i16> <i16 12665, i16 10394, i16 12665, i16 10394, i16 12665, i16 10394, i16 12665, i16 10394>) #6
  %253 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %245, <8 x i16> <i16 7723, i16 14449, i16 7723, i16 14449, i16 7723, i16 14449, i16 7723, i16 14449>) #6
  %254 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %246, <8 x i16> <i16 7723, i16 14449, i16 7723, i16 14449, i16 7723, i16 14449, i16 7723, i16 14449>) #6
  %255 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %247, <8 x i16> <i16 15679, i16 4756, i16 15679, i16 4756, i16 15679, i16 4756, i16 15679, i16 4756>) #6
  %256 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %248, <8 x i16> <i16 15679, i16 4756, i16 15679, i16 4756, i16 15679, i16 4756, i16 15679, i16 4756>) #6
  %257 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %247, <8 x i16> <i16 -4756, i16 15679, i16 -4756, i16 15679, i16 -4756, i16 15679, i16 -4756, i16 15679>) #6
  %258 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %248, <8 x i16> <i16 -4756, i16 15679, i16 -4756, i16 15679, i16 -4756, i16 15679, i16 -4756, i16 15679>) #6
  %259 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %245, <8 x i16> <i16 -14449, i16 7723, i16 -14449, i16 7723, i16 -14449, i16 7723, i16 -14449, i16 7723>) #6
  %260 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %246, <8 x i16> <i16 -14449, i16 7723, i16 -14449, i16 7723, i16 -14449, i16 7723, i16 -14449, i16 7723>) #6
  %261 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %243, <8 x i16> <i16 -10394, i16 12665, i16 -10394, i16 12665, i16 -10394, i16 12665, i16 -10394, i16 12665>) #6
  %262 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %244, <8 x i16> <i16 -10394, i16 12665, i16 -10394, i16 12665, i16 -10394, i16 12665, i16 -10394, i16 12665>) #6
  %263 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %241, <8 x i16> <i16 -16305, i16 1606, i16 -16305, i16 1606, i16 -16305, i16 1606, i16 -16305, i16 1606>) #6
  %264 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %242, <8 x i16> <i16 -16305, i16 1606, i16 -16305, i16 1606, i16 -16305, i16 1606, i16 -16305, i16 1606>) #6
  %265 = add <4 x i32> %249, <i32 8192, i32 8192, i32 8192, i32 8192>
  %266 = add <4 x i32> %250, <i32 8192, i32 8192, i32 8192, i32 8192>
  %267 = add <4 x i32> %251, <i32 8192, i32 8192, i32 8192, i32 8192>
  %268 = add <4 x i32> %252, <i32 8192, i32 8192, i32 8192, i32 8192>
  %269 = add <4 x i32> %253, <i32 8192, i32 8192, i32 8192, i32 8192>
  %270 = add <4 x i32> %254, <i32 8192, i32 8192, i32 8192, i32 8192>
  %271 = add <4 x i32> %255, <i32 8192, i32 8192, i32 8192, i32 8192>
  %272 = add <4 x i32> %256, <i32 8192, i32 8192, i32 8192, i32 8192>
  %273 = add <4 x i32> %257, <i32 8192, i32 8192, i32 8192, i32 8192>
  %274 = add <4 x i32> %258, <i32 8192, i32 8192, i32 8192, i32 8192>
  %275 = add <4 x i32> %259, <i32 8192, i32 8192, i32 8192, i32 8192>
  %276 = add <4 x i32> %260, <i32 8192, i32 8192, i32 8192, i32 8192>
  %277 = add <4 x i32> %261, <i32 8192, i32 8192, i32 8192, i32 8192>
  %278 = add <4 x i32> %262, <i32 8192, i32 8192, i32 8192, i32 8192>
  %279 = add <4 x i32> %263, <i32 8192, i32 8192, i32 8192, i32 8192>
  %280 = add <4 x i32> %264, <i32 8192, i32 8192, i32 8192, i32 8192>
  %281 = ashr <4 x i32> %265, <i32 14, i32 14, i32 14, i32 14>
  %282 = ashr <4 x i32> %266, <i32 14, i32 14, i32 14, i32 14>
  %283 = ashr <4 x i32> %267, <i32 14, i32 14, i32 14, i32 14>
  %284 = ashr <4 x i32> %268, <i32 14, i32 14, i32 14, i32 14>
  %285 = ashr <4 x i32> %269, <i32 14, i32 14, i32 14, i32 14>
  %286 = ashr <4 x i32> %270, <i32 14, i32 14, i32 14, i32 14>
  %287 = ashr <4 x i32> %271, <i32 14, i32 14, i32 14, i32 14>
  %288 = ashr <4 x i32> %272, <i32 14, i32 14, i32 14, i32 14>
  %289 = ashr <4 x i32> %273, <i32 14, i32 14, i32 14, i32 14>
  %290 = ashr <4 x i32> %274, <i32 14, i32 14, i32 14, i32 14>
  %291 = ashr <4 x i32> %275, <i32 14, i32 14, i32 14, i32 14>
  %292 = ashr <4 x i32> %276, <i32 14, i32 14, i32 14, i32 14>
  %293 = ashr <4 x i32> %277, <i32 14, i32 14, i32 14, i32 14>
  %294 = ashr <4 x i32> %278, <i32 14, i32 14, i32 14, i32 14>
  %295 = ashr <4 x i32> %279, <i32 14, i32 14, i32 14, i32 14>
  %296 = ashr <4 x i32> %280, <i32 14, i32 14, i32 14, i32 14>
  %297 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %281, <4 x i32> %282) #6
  store <8 x i16> %297, <8 x i16>* %9, align 16
  %298 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %283, <4 x i32> %284) #6
  store <8 x i16> %298, <8 x i16>* %47, align 16
  %299 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %285, <4 x i32> %286) #6
  store <8 x i16> %299, <8 x i16>* %37, align 16
  %300 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %287, <4 x i32> %288) #6
  store <8 x i16> %300, <8 x i16>* %19, align 16
  %301 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %289, <4 x i32> %290) #6
  store <8 x i16> %301, <8 x i16>* %23, align 16
  %302 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %291, <4 x i32> %292) #6
  store <8 x i16> %302, <8 x i16>* %33, align 16
  %303 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %293, <4 x i32> %294) #6
  store <8 x i16> %303, <8 x i16>* %51, align 16
  %304 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %295, <4 x i32> %296) #6
  store <8 x i16> %304, <8 x i16>* %5, align 16
  ret void
}

attributes #0 = { nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="128" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { argmemonly nounwind }
attributes #2 = { "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #3 = { nofree nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="128" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #4 = { inlinehint nofree norecurse nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="128" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #5 = { nounwind readnone }
attributes #6 = { nounwind }

!llvm.module.flags = !{!0, !1}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 2}
