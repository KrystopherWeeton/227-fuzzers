; ModuleID = '../../third_party/libaom/source/libaom/av1/common/cdef_block_avx2.c'
source_filename = "../../third_party/libaom/source/libaom/av1/common/cdef_block_avx2.c"
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

module asm ".symver exp, exp@GLIBC_2.2.5"
module asm ".symver exp2, exp2@GLIBC_2.2.5"
module asm ".symver exp2f, exp2f@GLIBC_2.2.5"
module asm ".symver expf, expf@GLIBC_2.2.5"
module asm ".symver lgamma, lgamma@GLIBC_2.2.5"
module asm ".symver lgammaf, lgammaf@GLIBC_2.2.5"
module asm ".symver lgammal, lgammal@GLIBC_2.2.5"
module asm ".symver log, log@GLIBC_2.2.5"
module asm ".symver log2, log2@GLIBC_2.2.5"
module asm ".symver log2f, log2f@GLIBC_2.2.5"
module asm ".symver logf, logf@GLIBC_2.2.5"
module asm ".symver pow, pow@GLIBC_2.2.5"
module asm ".symver powf, powf@GLIBC_2.2.5"

@cdef_directions = external local_unnamed_addr constant [8 x [2 x i32]], align 16
@cdef_pri_taps = external local_unnamed_addr constant [2 x [2 x i32]], align 16
@cdef_sec_taps = external local_unnamed_addr constant [2 x i32], align 4

; Function Attrs: nounwind ssp uwtable
define hidden i32 @cdef_find_dir_avx2(i16*, i32, i32* nocapture, i32) local_unnamed_addr #0 {
  %5 = alloca [8 x i32], align 16
  %6 = alloca [8 x <2 x i64>], align 16
  %7 = bitcast [8 x i32]* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %7) #9
  %8 = getelementptr inbounds [8 x i32], [8 x i32]* %5, i64 0, i64 4
  %9 = bitcast [8 x <2 x i64>]* %6 to i8*
  %10 = bitcast [8 x i32]* %5 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %10, i8 -86, i64 32, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 128, i8* nonnull %9) #9
  %11 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %3, i32 0
  %12 = bitcast <4 x i32> %11 to <8 x i16>
  %13 = sext i32 %1 to i64
  %14 = bitcast i16* %0 to i8*
  %15 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %14) #9
  %16 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 0
  %17 = bitcast <16 x i8> %15 to <8 x i16>
  %18 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %17, <8 x i16> %12) #9
  %19 = add <8 x i16> %18, <i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128>
  %20 = bitcast [8 x <2 x i64>]* %6 to <8 x i16>*
  store <8 x i16> %19, <8 x i16>* %20, align 16
  %21 = getelementptr inbounds i16, i16* %0, i64 %13
  %22 = bitcast i16* %21 to i8*
  %23 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %22) #9
  %24 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 1
  %25 = bitcast <16 x i8> %23 to <8 x i16>
  %26 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %25, <8 x i16> %12) #9
  %27 = add <8 x i16> %26, <i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128>
  %28 = bitcast <2 x i64>* %24 to <8 x i16>*
  store <8 x i16> %27, <8 x i16>* %28, align 16
  %29 = shl nsw i64 %13, 1
  %30 = getelementptr inbounds i16, i16* %0, i64 %29
  %31 = bitcast i16* %30 to i8*
  %32 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %31) #9
  %33 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 2
  %34 = bitcast <16 x i8> %32 to <8 x i16>
  %35 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %34, <8 x i16> %12) #9
  %36 = add <8 x i16> %35, <i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128>
  %37 = bitcast <2 x i64>* %33 to <8 x i16>*
  store <8 x i16> %36, <8 x i16>* %37, align 16
  %38 = mul nsw i64 %13, 3
  %39 = getelementptr inbounds i16, i16* %0, i64 %38
  %40 = bitcast i16* %39 to i8*
  %41 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %40) #9
  %42 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 3
  %43 = bitcast <16 x i8> %41 to <8 x i16>
  %44 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %43, <8 x i16> %12) #9
  %45 = add <8 x i16> %44, <i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128>
  %46 = bitcast <2 x i64>* %42 to <8 x i16>*
  store <8 x i16> %45, <8 x i16>* %46, align 16
  %47 = shl nsw i64 %13, 2
  %48 = getelementptr inbounds i16, i16* %0, i64 %47
  %49 = bitcast i16* %48 to i8*
  %50 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %49) #9
  %51 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 4
  %52 = bitcast <16 x i8> %50 to <8 x i16>
  %53 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %52, <8 x i16> %12) #9
  %54 = add <8 x i16> %53, <i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128>
  %55 = bitcast <2 x i64>* %51 to <8 x i16>*
  store <8 x i16> %54, <8 x i16>* %55, align 16
  %56 = mul nsw i64 %13, 5
  %57 = getelementptr inbounds i16, i16* %0, i64 %56
  %58 = bitcast i16* %57 to i8*
  %59 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %58) #9
  %60 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 5
  %61 = bitcast <16 x i8> %59 to <8 x i16>
  %62 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %61, <8 x i16> %12) #9
  %63 = add <8 x i16> %62, <i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128>
  %64 = bitcast <2 x i64>* %60 to <8 x i16>*
  store <8 x i16> %63, <8 x i16>* %64, align 16
  %65 = mul nsw i64 %13, 6
  %66 = getelementptr inbounds i16, i16* %0, i64 %65
  %67 = bitcast i16* %66 to i8*
  %68 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %67) #9
  %69 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 6
  %70 = bitcast <16 x i8> %68 to <8 x i16>
  %71 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %70, <8 x i16> %12) #9
  %72 = add <8 x i16> %71, <i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128>
  %73 = bitcast <2 x i64>* %69 to <8 x i16>*
  store <8 x i16> %72, <8 x i16>* %73, align 16
  %74 = mul nsw i64 %13, 7
  %75 = getelementptr inbounds i16, i16* %0, i64 %74
  %76 = bitcast i16* %75 to i8*
  %77 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %76) #9
  %78 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 7
  %79 = bitcast <16 x i8> %77 to <8 x i16>
  %80 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %79, <8 x i16> %12) #9
  %81 = add <8 x i16> %80, <i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128>
  %82 = bitcast <2 x i64>* %78 to <8 x i16>*
  store <8 x i16> %81, <8 x i16>* %82, align 16
  %83 = bitcast <8 x i16> %19 to <16 x i8>
  %84 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %83, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %85 = shufflevector <16 x i8> %83, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %86 = bitcast <8 x i16> %27 to <16 x i8>
  %87 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %86, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19>
  %88 = bitcast <16 x i8> %84 to <8 x i16>
  %89 = bitcast <16 x i8> %87 to <8 x i16>
  %90 = add <8 x i16> %89, %88
  %91 = shufflevector <16 x i8> %86, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19>
  %92 = bitcast <16 x i8> %85 to <8 x i16>
  %93 = bitcast <16 x i8> %91 to <8 x i16>
  %94 = add <8 x i16> %93, %92
  %95 = add <8 x i16> %27, %19
  %96 = bitcast <8 x i16> %95 to <16 x i8>
  %97 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %96, <16 x i32> <i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21>
  %98 = shufflevector <16 x i8> %96, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21>
  %99 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %96, <16 x i32> <i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27>
  %100 = shufflevector <16 x i8> %96, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27>
  %101 = bitcast <8 x i16> %36 to <16 x i8>
  %102 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %101, <16 x i32> <i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21>
  %103 = bitcast <16 x i8> %102 to <8 x i16>
  %104 = add <8 x i16> %90, %103
  %105 = shufflevector <16 x i8> %101, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21>
  %106 = bitcast <16 x i8> %105 to <8 x i16>
  %107 = add <8 x i16> %94, %106
  %108 = bitcast <8 x i16> %45 to <16 x i8>
  %109 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %108, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %110 = bitcast <16 x i8> %109 to <8 x i16>
  %111 = add <8 x i16> %104, %110
  %112 = shufflevector <16 x i8> %108, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %113 = bitcast <16 x i8> %112 to <8 x i16>
  %114 = add <8 x i16> %107, %113
  %115 = add <8 x i16> %45, %36
  %116 = bitcast <8 x i16> %115 to <16 x i8>
  %117 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %116, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %118 = bitcast <16 x i8> %97 to <8 x i16>
  %119 = bitcast <16 x i8> %117 to <8 x i16>
  %120 = add <8 x i16> %119, %118
  %121 = shufflevector <16 x i8> %116, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %122 = bitcast <16 x i8> %98 to <8 x i16>
  %123 = bitcast <16 x i8> %121 to <8 x i16>
  %124 = add <8 x i16> %123, %122
  %125 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %116, <16 x i32> <i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25>
  %126 = bitcast <16 x i8> %99 to <8 x i16>
  %127 = bitcast <16 x i8> %125 to <8 x i16>
  %128 = add <8 x i16> %127, %126
  %129 = shufflevector <16 x i8> %116, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25>
  %130 = bitcast <16 x i8> %100 to <8 x i16>
  %131 = bitcast <16 x i8> %129 to <8 x i16>
  %132 = add <8 x i16> %131, %130
  %133 = add <8 x i16> %115, %95
  %134 = bitcast <8 x i16> %54 to <16 x i8>
  %135 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %134, <16 x i32> <i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25>
  %136 = bitcast <16 x i8> %135 to <8 x i16>
  %137 = add <8 x i16> %111, %136
  %138 = shufflevector <16 x i8> %134, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25>
  %139 = bitcast <16 x i8> %138 to <8 x i16>
  %140 = add <8 x i16> %114, %139
  %141 = bitcast <8 x i16> %63 to <16 x i8>
  %142 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %141, <16 x i32> <i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27>
  %143 = bitcast <16 x i8> %142 to <8 x i16>
  %144 = add <8 x i16> %137, %143
  %145 = shufflevector <16 x i8> %141, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27>
  %146 = bitcast <16 x i8> %145 to <8 x i16>
  %147 = add <8 x i16> %140, %146
  %148 = add <8 x i16> %63, %54
  %149 = bitcast <8 x i16> %148 to <16 x i8>
  %150 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %149, <16 x i32> <i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25>
  %151 = bitcast <16 x i8> %150 to <8 x i16>
  %152 = add <8 x i16> %120, %151
  %153 = shufflevector <16 x i8> %149, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25>
  %154 = bitcast <16 x i8> %153 to <8 x i16>
  %155 = add <8 x i16> %124, %154
  %156 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %149, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %157 = bitcast <16 x i8> %156 to <8 x i16>
  %158 = add <8 x i16> %128, %157
  %159 = shufflevector <16 x i8> %149, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %160 = bitcast <16 x i8> %159 to <8 x i16>
  %161 = add <8 x i16> %132, %160
  %162 = add <8 x i16> %133, %148
  %163 = bitcast <2 x i64>* %69 to <16 x i8>*
  %164 = load <16 x i8>, <16 x i8>* %163, align 16
  %165 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0>, <16 x i8> %164, <16 x i32> <i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29>
  %166 = bitcast <16 x i8> %165 to <8 x i16>
  %167 = shufflevector <16 x i8> %164, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef>, <16 x i32> <i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29>
  %168 = bitcast <16 x i8> %167 to <8 x i16>
  %169 = add <8 x i16> %147, %168
  %170 = load <8 x i16>, <8 x i16>* %82, align 16
  %171 = add <8 x i16> %144, %170
  %172 = add <8 x i16> %171, %166
  %173 = bitcast <16 x i8> %164 to <8 x i16>
  %174 = add <8 x i16> %170, %173
  %175 = bitcast <8 x i16> %174 to <16 x i8>
  %176 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %175, <16 x i32> <i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27>
  %177 = bitcast <16 x i8> %176 to <8 x i16>
  %178 = add <8 x i16> %152, %177
  %179 = shufflevector <16 x i8> %175, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27>
  %180 = bitcast <16 x i8> %179 to <8 x i16>
  %181 = add <8 x i16> %155, %180
  %182 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %175, <16 x i32> <i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21>
  %183 = bitcast <16 x i8> %182 to <8 x i16>
  %184 = add <8 x i16> %158, %183
  %185 = shufflevector <16 x i8> %175, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21>
  %186 = bitcast <16 x i8> %185 to <8 x i16>
  %187 = add <8 x i16> %161, %186
  %188 = add <8 x i16> %162, %174
  %189 = bitcast <8 x i16> %169 to <16 x i8>
  %190 = shufflevector <16 x i8> %189, <16 x i8> undef, <16 x i32> <i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15>
  %191 = bitcast <16 x i8> %190 to <8 x i16>
  %192 = shufflevector <8 x i16> %172, <8 x i16> %191, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %193 = shufflevector <8 x i16> %172, <8 x i16> %191, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %194 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %192, <8 x i16> %192) #9
  %195 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %193, <8 x i16> %193) #9
  %196 = mul <4 x i32> %194, <i32 840, i32 420, i32 280, i32 210>
  %197 = mul <4 x i32> %195, <i32 168, i32 140, i32 120, i32 105>
  %198 = add <4 x i32> %197, %196
  %199 = bitcast <8 x i16> %187 to <16 x i8>
  %200 = shufflevector <16 x i8> %199, <16 x i8> undef, <16 x i32> <i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15>
  %201 = bitcast <16 x i8> %200 to <8 x i16>
  %202 = shufflevector <8 x i16> %184, <8 x i16> %201, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %203 = shufflevector <8 x i16> %184, <8 x i16> %201, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %204 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %202, <8 x i16> %202) #9
  %205 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %203, <8 x i16> %203) #9
  %206 = mul <4 x i32> %204, <i32 0, i32 0, i32 420, i32 210>
  %207 = mul <4 x i32> %205, <i32 140, i32 105, i32 105, i32 105>
  %208 = add <4 x i32> %207, %206
  %209 = bitcast <8 x i16> %181 to <16 x i8>
  %210 = shufflevector <16 x i8> %209, <16 x i8> undef, <16 x i32> <i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15>
  %211 = bitcast <16 x i8> %210 to <8 x i16>
  %212 = shufflevector <8 x i16> %178, <8 x i16> %211, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %213 = shufflevector <8 x i16> %178, <8 x i16> %211, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %214 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %212, <8 x i16> %212) #9
  %215 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %213, <8 x i16> %213) #9
  %216 = mul <4 x i32> %214, <i32 0, i32 0, i32 420, i32 210>
  %217 = mul <4 x i32> %215, <i32 140, i32 105, i32 105, i32 105>
  %218 = add <4 x i32> %217, %216
  %219 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %188, <8 x i16> %188) #9
  %220 = mul <4 x i32> %219, <i32 105, i32 105, i32 105, i32 105>
  %221 = shufflevector <4 x i32> %198, <4 x i32> %218, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %222 = bitcast <4 x i32> %221 to <2 x i64>
  %223 = shufflevector <4 x i32> %220, <4 x i32> %208, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %224 = bitcast <4 x i32> %223 to <2 x i64>
  %225 = shufflevector <4 x i32> %198, <4 x i32> %218, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %226 = bitcast <4 x i32> %225 to <2 x i64>
  %227 = shufflevector <4 x i32> %220, <4 x i32> %208, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %228 = bitcast <4 x i32> %227 to <2 x i64>
  %229 = shufflevector <2 x i64> %222, <2 x i64> %224, <2 x i32> <i32 0, i32 2>
  %230 = shufflevector <2 x i64> %222, <2 x i64> %224, <2 x i32> <i32 1, i32 3>
  %231 = shufflevector <2 x i64> %226, <2 x i64> %228, <2 x i32> <i32 0, i32 2>
  %232 = shufflevector <2 x i64> %226, <2 x i64> %228, <2 x i32> <i32 1, i32 3>
  %233 = bitcast <2 x i64> %229 to <4 x i32>
  %234 = bitcast <2 x i64> %230 to <4 x i32>
  %235 = bitcast <2 x i64> %231 to <4 x i32>
  %236 = bitcast <2 x i64> %232 to <4 x i32>
  %237 = add <4 x i32> %236, %235
  %238 = add <4 x i32> %237, %233
  %239 = add <4 x i32> %238, %234
  %240 = bitcast i32* %8 to <4 x i32>*
  store <4 x i32> %239, <4 x i32>* %240, align 16
  %241 = load <8 x i16>, <8 x i16>* %28, align 16
  %242 = bitcast [8 x <2 x i64>]* %6 to <8 x i16>*
  %243 = load <8 x i16>, <8 x i16>* %242, align 16
  %244 = shufflevector <8 x i16> %243, <8 x i16> %241, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %245 = load <8 x i16>, <8 x i16>* %46, align 16
  %246 = load <8 x i16>, <8 x i16>* %37, align 16
  %247 = shufflevector <8 x i16> %246, <8 x i16> %245, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %248 = shufflevector <8 x i16> %243, <8 x i16> %241, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %249 = shufflevector <8 x i16> %246, <8 x i16> %245, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %250 = load <8 x i16>, <8 x i16>* %64, align 16
  %251 = load <8 x i16>, <8 x i16>* %55, align 16
  %252 = shufflevector <8 x i16> %251, <8 x i16> %250, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %253 = bitcast <16 x i8> %164 to <8 x i16>
  %254 = shufflevector <8 x i16> %253, <8 x i16> %170, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %255 = shufflevector <8 x i16> %251, <8 x i16> %250, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %256 = shufflevector <8 x i16> %253, <8 x i16> %170, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %257 = bitcast <8 x i16> %244 to <4 x i32>
  %258 = bitcast <8 x i16> %247 to <4 x i32>
  %259 = shufflevector <4 x i32> %257, <4 x i32> %258, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %260 = bitcast <4 x i32> %259 to <2 x i64>
  %261 = bitcast <8 x i16> %252 to <4 x i32>
  %262 = bitcast <8 x i16> %254 to <4 x i32>
  %263 = shufflevector <4 x i32> %261, <4 x i32> %262, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %264 = bitcast <4 x i32> %263 to <2 x i64>
  %265 = shufflevector <4 x i32> %257, <4 x i32> %258, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %266 = bitcast <4 x i32> %265 to <2 x i64>
  %267 = shufflevector <4 x i32> %261, <4 x i32> %262, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %268 = bitcast <4 x i32> %267 to <2 x i64>
  %269 = bitcast <8 x i16> %248 to <4 x i32>
  %270 = bitcast <8 x i16> %249 to <4 x i32>
  %271 = shufflevector <4 x i32> %269, <4 x i32> %270, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %272 = bitcast <4 x i32> %271 to <2 x i64>
  %273 = bitcast <8 x i16> %255 to <4 x i32>
  %274 = bitcast <8 x i16> %256 to <4 x i32>
  %275 = shufflevector <4 x i32> %273, <4 x i32> %274, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %276 = bitcast <4 x i32> %275 to <2 x i64>
  %277 = shufflevector <4 x i32> %269, <4 x i32> %270, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %278 = bitcast <4 x i32> %277 to <2 x i64>
  %279 = shufflevector <4 x i32> %273, <4 x i32> %274, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %280 = bitcast <4 x i32> %279 to <2 x i64>
  %281 = shufflevector <2 x i64> %260, <2 x i64> %264, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %281, <2 x i64>* %78, align 16
  %282 = shufflevector <2 x i64> %260, <2 x i64> %264, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %282, <2 x i64>* %69, align 16
  %283 = shufflevector <2 x i64> %266, <2 x i64> %268, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %283, <2 x i64>* %60, align 16
  %284 = shufflevector <2 x i64> %266, <2 x i64> %268, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %284, <2 x i64>* %51, align 16
  %285 = shufflevector <2 x i64> %272, <2 x i64> %276, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %285, <2 x i64>* %42, align 16
  %286 = shufflevector <2 x i64> %272, <2 x i64> %276, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %286, <2 x i64>* %33, align 16
  %287 = shufflevector <2 x i64> %278, <2 x i64> %280, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %287, <2 x i64>* %24, align 16
  %288 = shufflevector <2 x i64> %278, <2 x i64> %280, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %288, <2 x i64>* %16, align 16
  %289 = bitcast <2 x i64> %288 to <16 x i8>
  %290 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %289, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %291 = shufflevector <16 x i8> %289, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %292 = bitcast <2 x i64> %287 to <16 x i8>
  %293 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %292, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19>
  %294 = bitcast <16 x i8> %290 to <8 x i16>
  %295 = bitcast <16 x i8> %293 to <8 x i16>
  %296 = add <8 x i16> %295, %294
  %297 = shufflevector <16 x i8> %292, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19>
  %298 = bitcast <16 x i8> %291 to <8 x i16>
  %299 = bitcast <16 x i8> %297 to <8 x i16>
  %300 = add <8 x i16> %299, %298
  %301 = bitcast <2 x i64> %288 to <8 x i16>
  %302 = bitcast <2 x i64> %287 to <8 x i16>
  %303 = add <8 x i16> %302, %301
  %304 = bitcast <8 x i16> %303 to <16 x i8>
  %305 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %304, <16 x i32> <i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21>
  %306 = shufflevector <16 x i8> %304, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21>
  %307 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %304, <16 x i32> <i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27>
  %308 = shufflevector <16 x i8> %304, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27>
  %309 = bitcast <2 x i64> %286 to <16 x i8>
  %310 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %309, <16 x i32> <i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21>
  %311 = bitcast <16 x i8> %310 to <8 x i16>
  %312 = add <8 x i16> %296, %311
  %313 = shufflevector <16 x i8> %309, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21>
  %314 = bitcast <16 x i8> %313 to <8 x i16>
  %315 = add <8 x i16> %300, %314
  %316 = bitcast <2 x i64> %285 to <16 x i8>
  %317 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %316, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %318 = bitcast <16 x i8> %317 to <8 x i16>
  %319 = add <8 x i16> %312, %318
  %320 = shufflevector <16 x i8> %316, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %321 = bitcast <16 x i8> %320 to <8 x i16>
  %322 = add <8 x i16> %315, %321
  %323 = bitcast <2 x i64> %286 to <8 x i16>
  %324 = bitcast <2 x i64> %285 to <8 x i16>
  %325 = add <8 x i16> %324, %323
  %326 = bitcast <8 x i16> %325 to <16 x i8>
  %327 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %326, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %328 = bitcast <16 x i8> %305 to <8 x i16>
  %329 = bitcast <16 x i8> %327 to <8 x i16>
  %330 = add <8 x i16> %329, %328
  %331 = shufflevector <16 x i8> %326, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %332 = bitcast <16 x i8> %306 to <8 x i16>
  %333 = bitcast <16 x i8> %331 to <8 x i16>
  %334 = add <8 x i16> %333, %332
  %335 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %326, <16 x i32> <i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25>
  %336 = bitcast <16 x i8> %307 to <8 x i16>
  %337 = bitcast <16 x i8> %335 to <8 x i16>
  %338 = add <8 x i16> %337, %336
  %339 = shufflevector <16 x i8> %326, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25>
  %340 = bitcast <16 x i8> %308 to <8 x i16>
  %341 = bitcast <16 x i8> %339 to <8 x i16>
  %342 = add <8 x i16> %341, %340
  %343 = add <8 x i16> %325, %303
  %344 = bitcast <2 x i64> %284 to <16 x i8>
  %345 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %344, <16 x i32> <i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25>
  %346 = bitcast <16 x i8> %345 to <8 x i16>
  %347 = add <8 x i16> %319, %346
  %348 = shufflevector <16 x i8> %344, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25>
  %349 = bitcast <16 x i8> %348 to <8 x i16>
  %350 = add <8 x i16> %322, %349
  %351 = bitcast <2 x i64> %283 to <16 x i8>
  %352 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %351, <16 x i32> <i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27>
  %353 = bitcast <16 x i8> %352 to <8 x i16>
  %354 = add <8 x i16> %347, %353
  %355 = shufflevector <16 x i8> %351, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27>
  %356 = bitcast <16 x i8> %355 to <8 x i16>
  %357 = add <8 x i16> %350, %356
  %358 = bitcast <2 x i64> %284 to <8 x i16>
  %359 = bitcast <2 x i64> %283 to <8 x i16>
  %360 = add <8 x i16> %359, %358
  %361 = bitcast <8 x i16> %360 to <16 x i8>
  %362 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %361, <16 x i32> <i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25>
  %363 = bitcast <16 x i8> %362 to <8 x i16>
  %364 = add <8 x i16> %330, %363
  %365 = shufflevector <16 x i8> %361, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25>
  %366 = bitcast <16 x i8> %365 to <8 x i16>
  %367 = add <8 x i16> %334, %366
  %368 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %361, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %369 = bitcast <16 x i8> %368 to <8 x i16>
  %370 = add <8 x i16> %338, %369
  %371 = shufflevector <16 x i8> %361, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %372 = bitcast <16 x i8> %371 to <8 x i16>
  %373 = add <8 x i16> %342, %372
  %374 = add <8 x i16> %343, %360
  %375 = load <16 x i8>, <16 x i8>* %163, align 16
  %376 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0>, <16 x i8> %375, <16 x i32> <i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29>
  %377 = bitcast <16 x i8> %376 to <8 x i16>
  %378 = shufflevector <16 x i8> %375, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef>, <16 x i32> <i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29>
  %379 = bitcast <16 x i8> %378 to <8 x i16>
  %380 = add <8 x i16> %357, %379
  %381 = load <8 x i16>, <8 x i16>* %82, align 16
  %382 = add <8 x i16> %354, %381
  %383 = add <8 x i16> %382, %377
  %384 = bitcast <16 x i8> %375 to <8 x i16>
  %385 = add <8 x i16> %381, %384
  %386 = bitcast <8 x i16> %385 to <16 x i8>
  %387 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %386, <16 x i32> <i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27>
  %388 = bitcast <16 x i8> %387 to <8 x i16>
  %389 = add <8 x i16> %364, %388
  %390 = shufflevector <16 x i8> %386, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27>
  %391 = bitcast <16 x i8> %390 to <8 x i16>
  %392 = add <8 x i16> %367, %391
  %393 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %386, <16 x i32> <i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21>
  %394 = bitcast <16 x i8> %393 to <8 x i16>
  %395 = add <8 x i16> %370, %394
  %396 = shufflevector <16 x i8> %386, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21>
  %397 = bitcast <16 x i8> %396 to <8 x i16>
  %398 = add <8 x i16> %373, %397
  %399 = add <8 x i16> %374, %385
  %400 = bitcast <8 x i16> %380 to <16 x i8>
  %401 = shufflevector <16 x i8> %400, <16 x i8> undef, <16 x i32> <i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15>
  %402 = bitcast <16 x i8> %401 to <8 x i16>
  %403 = shufflevector <8 x i16> %383, <8 x i16> %402, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %404 = shufflevector <8 x i16> %383, <8 x i16> %402, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %405 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %403, <8 x i16> %403) #9
  %406 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %404, <8 x i16> %404) #9
  %407 = mul <4 x i32> %405, <i32 840, i32 420, i32 280, i32 210>
  %408 = mul <4 x i32> %406, <i32 168, i32 140, i32 120, i32 105>
  %409 = add <4 x i32> %408, %407
  %410 = bitcast <8 x i16> %398 to <16 x i8>
  %411 = shufflevector <16 x i8> %410, <16 x i8> undef, <16 x i32> <i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15>
  %412 = bitcast <16 x i8> %411 to <8 x i16>
  %413 = shufflevector <8 x i16> %395, <8 x i16> %412, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %414 = shufflevector <8 x i16> %395, <8 x i16> %412, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %415 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %413, <8 x i16> %413) #9
  %416 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %414, <8 x i16> %414) #9
  %417 = mul <4 x i32> %415, <i32 0, i32 0, i32 420, i32 210>
  %418 = mul <4 x i32> %416, <i32 140, i32 105, i32 105, i32 105>
  %419 = add <4 x i32> %418, %417
  %420 = bitcast <8 x i16> %392 to <16 x i8>
  %421 = shufflevector <16 x i8> %420, <16 x i8> undef, <16 x i32> <i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15>
  %422 = bitcast <16 x i8> %421 to <8 x i16>
  %423 = shufflevector <8 x i16> %389, <8 x i16> %422, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %424 = shufflevector <8 x i16> %389, <8 x i16> %422, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %425 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %423, <8 x i16> %423) #9
  %426 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %424, <8 x i16> %424) #9
  %427 = mul <4 x i32> %425, <i32 0, i32 0, i32 420, i32 210>
  %428 = mul <4 x i32> %426, <i32 140, i32 105, i32 105, i32 105>
  %429 = add <4 x i32> %428, %427
  %430 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %399, <8 x i16> %399) #9
  %431 = mul <4 x i32> %430, <i32 105, i32 105, i32 105, i32 105>
  %432 = shufflevector <4 x i32> %409, <4 x i32> %429, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %433 = bitcast <4 x i32> %432 to <2 x i64>
  %434 = shufflevector <4 x i32> %431, <4 x i32> %419, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %435 = bitcast <4 x i32> %434 to <2 x i64>
  %436 = shufflevector <4 x i32> %409, <4 x i32> %429, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %437 = bitcast <4 x i32> %436 to <2 x i64>
  %438 = shufflevector <4 x i32> %431, <4 x i32> %419, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %439 = bitcast <4 x i32> %438 to <2 x i64>
  %440 = shufflevector <2 x i64> %433, <2 x i64> %435, <2 x i32> <i32 0, i32 2>
  %441 = shufflevector <2 x i64> %433, <2 x i64> %435, <2 x i32> <i32 1, i32 3>
  %442 = shufflevector <2 x i64> %437, <2 x i64> %439, <2 x i32> <i32 0, i32 2>
  %443 = shufflevector <2 x i64> %437, <2 x i64> %439, <2 x i32> <i32 1, i32 3>
  %444 = bitcast <2 x i64> %440 to <4 x i32>
  %445 = bitcast <2 x i64> %441 to <4 x i32>
  %446 = bitcast <2 x i64> %442 to <4 x i32>
  %447 = bitcast <2 x i64> %443 to <4 x i32>
  %448 = add <4 x i32> %447, %446
  %449 = add <4 x i32> %448, %444
  %450 = add <4 x i32> %449, %445
  %451 = bitcast [8 x i32]* %5 to <4 x i32>*
  store <4 x i32> %450, <4 x i32>* %451, align 16
  %452 = icmp sgt <4 x i32> %450, %239
  %453 = select <4 x i1> %452, <4 x i32> %450, <4 x i32> %239
  %454 = bitcast <4 x i32> %453 to <16 x i8>
  %455 = shufflevector <16 x i8> %454, <16 x i8> undef, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %456 = bitcast <16 x i8> %455 to <4 x i32>
  %457 = icmp sgt <4 x i32> %453, %456
  %458 = select <4 x i1> %457, <4 x i32> %453, <4 x i32> %456
  %459 = bitcast <4 x i32> %458 to <16 x i8>
  %460 = shufflevector <16 x i8> %459, <16 x i8> undef, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 0, i32 1, i32 2, i32 3>
  %461 = bitcast <16 x i8> %460 to <4 x i32>
  %462 = icmp sgt <4 x i32> %458, %461
  %463 = select <4 x i1> %462, <4 x i32> %458, <4 x i32> %461
  %464 = extractelement <4 x i32> %463, i32 0
  %465 = icmp eq <4 x i32> %463, %239
  %466 = sext <4 x i1> %465 to <4 x i32>
  %467 = icmp eq <4 x i32> %463, %450
  %468 = sext <4 x i1> %467 to <4 x i32>
  %469 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %468, <4 x i32> %466) #9
  %470 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %469, <8 x i16> %469) #9
  %471 = icmp slt <16 x i8> %470, zeroinitializer
  %472 = bitcast <16 x i1> %471 to i16
  %473 = zext i16 %472 to i32
  %474 = add nsw i32 %473, -1
  %475 = xor i32 %474, %473
  %476 = tail call i32 @llvm.ctlz.i32(i32 %475, i1 true) #9, !range !2
  %477 = xor i32 %476, 31
  %478 = add nuw nsw i32 %477, 4
  %479 = and i32 %478, 7
  %480 = zext i32 %479 to i64
  %481 = getelementptr inbounds [8 x i32], [8 x i32]* %5, i64 0, i64 %480
  %482 = load i32, i32* %481, align 4
  %483 = sub nsw i32 %464, %482
  %484 = ashr i32 %483, 10
  store i32 %484, i32* %2, align 4
  call void @llvm.lifetime.end.p0i8(i64 128, i8* nonnull %9) #9
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %7) #9
  ret i32 %477
}

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #1

; Function Attrs: argmemonly nounwind
declare void @llvm.memset.p0i8.i64(i8* nocapture writeonly, i8, i64, i1 immarg) #1

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #1

; Function Attrs: nofree nounwind ssp uwtable
define hidden void @cdef_filter_block_4x4_8_avx2(i8* nocapture, i32, i16* readonly, i32, i32, i32, i32, i32, i32) local_unnamed_addr #2 {
  %10 = sext i32 %5 to i64
  %11 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %10, i64 0
  %12 = load i32, i32* %11, align 8
  %13 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %10, i64 1
  %14 = load i32, i32* %13, align 4
  %15 = add nsw i32 %5, 2
  %16 = and i32 %15, 7
  %17 = zext i32 %16 to i64
  %18 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %17, i64 0
  %19 = load i32, i32* %18, align 8
  %20 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %17, i64 1
  %21 = load i32, i32* %20, align 4
  %22 = add nsw i32 %5, 6
  %23 = and i32 %22, 7
  %24 = zext i32 %23 to i64
  %25 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %24, i64 0
  %26 = load i32, i32* %25, align 8
  %27 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %24, i64 1
  %28 = load i32, i32* %27, align 4
  %29 = lshr i32 %3, %8
  %30 = and i32 %29, 1
  %31 = zext i32 %30 to i64
  %32 = getelementptr inbounds [2 x [2 x i32]], [2 x [2 x i32]]* @cdef_pri_taps, i64 0, i64 %31, i64 0
  %33 = icmp ne i32 %3, 0
  br i1 %33, label %34, label %40

34:                                               ; preds = %9
  %35 = tail call i32 @llvm.ctlz.i32(i32 %3, i1 true) #9, !range !2
  %36 = xor i32 %35, 31
  %37 = icmp sgt i32 %36, %6
  %38 = sub nsw i32 %6, %36
  %39 = select i1 %37, i32 0, i32 %38
  br label %40

40:                                               ; preds = %34, %9
  %41 = phi i32 [ %6, %9 ], [ %39, %34 ]
  %42 = icmp ne i32 %4, 0
  br i1 %42, label %43, label %49

43:                                               ; preds = %40
  %44 = tail call i32 @llvm.ctlz.i32(i32 %4, i1 true) #9, !range !2
  %45 = xor i32 %44, 31
  %46 = icmp sgt i32 %45, %7
  %47 = sub nsw i32 %7, %45
  %48 = select i1 %46, i32 0, i32 %47
  br label %49

49:                                               ; preds = %43, %40
  %50 = phi i32 [ %7, %40 ], [ %48, %43 ]
  %51 = bitcast i16* %2 to i64*
  %52 = load i64, i64* %51, align 1
  %53 = getelementptr inbounds i16, i16* %2, i64 144
  %54 = bitcast i16* %53 to i64*
  %55 = load i64, i64* %54, align 1
  %56 = insertelement <2 x i64> undef, i64 %55, i32 0
  %57 = getelementptr inbounds i16, i16* %2, i64 288
  %58 = bitcast i16* %57 to i64*
  %59 = load i64, i64* %58, align 1
  %60 = getelementptr inbounds i16, i16* %2, i64 432
  %61 = bitcast i16* %60 to i64*
  %62 = load i64, i64* %61, align 1
  %63 = insertelement <2 x i64> undef, i64 %62, i32 0
  %64 = insertelement <2 x i64> %56, i64 %52, i32 1
  %65 = insertelement <2 x i64> %63, i64 %59, i32 1
  %66 = shufflevector <2 x i64> %65, <2 x i64> %64, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  br i1 %33, label %67, label %326

67:                                               ; preds = %49
  %68 = sext i32 %12 to i64
  %69 = getelementptr inbounds i16, i16* %2, i64 %68
  %70 = bitcast i16* %69 to i64*
  %71 = load i64, i64* %70, align 1
  %72 = add nsw i32 %12, 144
  %73 = sext i32 %72 to i64
  %74 = getelementptr inbounds i16, i16* %2, i64 %73
  %75 = bitcast i16* %74 to i64*
  %76 = load i64, i64* %75, align 1
  %77 = insertelement <2 x i64> undef, i64 %76, i32 0
  %78 = add nsw i32 %12, 288
  %79 = sext i32 %78 to i64
  %80 = getelementptr inbounds i16, i16* %2, i64 %79
  %81 = bitcast i16* %80 to i64*
  %82 = load i64, i64* %81, align 1
  %83 = add nsw i32 %12, 432
  %84 = sext i32 %83 to i64
  %85 = getelementptr inbounds i16, i16* %2, i64 %84
  %86 = bitcast i16* %85 to i64*
  %87 = load i64, i64* %86, align 1
  %88 = insertelement <2 x i64> undef, i64 %87, i32 0
  %89 = insertelement <2 x i64> %77, i64 %71, i32 1
  %90 = insertelement <2 x i64> %88, i64 %82, i32 1
  %91 = shufflevector <2 x i64> %90, <2 x i64> %89, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %92 = bitcast <4 x i64> %91 to <16 x i16>
  %93 = icmp eq <16 x i16> %92, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %94 = sext <16 x i1> %93 to <16 x i16>
  %95 = bitcast <16 x i16> %94 to <4 x i64>
  %96 = xor <4 x i64> %95, <i64 -1, i64 -1, i64 -1, i64 -1>
  %97 = and <4 x i64> %91, %96
  %98 = bitcast <4 x i64> %66 to <16 x i16>
  %99 = bitcast <4 x i64> %97 to <16 x i16>
  %100 = icmp sgt <16 x i16> %98, %99
  %101 = select <16 x i1> %100, <16 x i16> %98, <16 x i16> %99
  %102 = icmp sgt <16 x i16> %92, %98
  %103 = select <16 x i1> %102, <16 x i16> %98, <16 x i16> %92
  %104 = sub <16 x i16> %92, %98
  %105 = bitcast <16 x i16> %104 to <4 x i64>
  %106 = shufflevector <4 x i64> %105, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %107 = shufflevector <4 x i64> %105, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %108 = bitcast <2 x i64> %107 to <8 x i16>
  %109 = bitcast <2 x i64> %106 to <8 x i16>
  %110 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %108, <8 x i16> %109) #9
  %111 = ashr <16 x i8> %110, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %112 = sub <16 x i8> zeroinitializer, %110
  %113 = icmp slt <16 x i8> %110, zeroinitializer
  %114 = select <16 x i1> %113, <16 x i8> %112, <16 x i8> %110
  %115 = trunc i32 %3 to i8
  %116 = insertelement <16 x i8> undef, i8 %115, i32 0
  %117 = shufflevector <16 x i8> %116, <16 x i8> undef, <16 x i32> zeroinitializer
  %118 = lshr i32 255, %41
  %119 = trunc i32 %118 to i8
  %120 = insertelement <16 x i8> undef, i8 %119, i32 0
  %121 = shufflevector <16 x i8> %120, <16 x i8> undef, <16 x i32> zeroinitializer
  %122 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %41, i32 0
  %123 = bitcast <16 x i8> %114 to <8 x i16>
  %124 = bitcast <4 x i32> %122 to <8 x i16>
  %125 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %123, <8 x i16> %124) #9
  %126 = bitcast <8 x i16> %125 to <16 x i8>
  %127 = and <16 x i8> %121, %126
  %128 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %117, <16 x i8> %127) #9
  %129 = icmp ult <16 x i8> %114, %128
  %130 = select <16 x i1> %129, <16 x i8> %114, <16 x i8> %128
  %131 = add <16 x i8> %130, %111
  %132 = xor <16 x i8> %131, %111
  %133 = sub nsw i32 0, %12
  %134 = sext i32 %133 to i64
  %135 = getelementptr inbounds i16, i16* %2, i64 %134
  %136 = bitcast i16* %135 to i64*
  %137 = load i64, i64* %136, align 1
  %138 = sub nsw i32 144, %12
  %139 = sext i32 %138 to i64
  %140 = getelementptr inbounds i16, i16* %2, i64 %139
  %141 = bitcast i16* %140 to i64*
  %142 = load i64, i64* %141, align 1
  %143 = insertelement <2 x i64> undef, i64 %142, i32 0
  %144 = sub nsw i32 288, %12
  %145 = sext i32 %144 to i64
  %146 = getelementptr inbounds i16, i16* %2, i64 %145
  %147 = bitcast i16* %146 to i64*
  %148 = load i64, i64* %147, align 1
  %149 = sub nsw i32 432, %12
  %150 = sext i32 %149 to i64
  %151 = getelementptr inbounds i16, i16* %2, i64 %150
  %152 = bitcast i16* %151 to i64*
  %153 = load i64, i64* %152, align 1
  %154 = insertelement <2 x i64> undef, i64 %153, i32 0
  %155 = insertelement <2 x i64> %143, i64 %137, i32 1
  %156 = insertelement <2 x i64> %154, i64 %148, i32 1
  %157 = shufflevector <2 x i64> %156, <2 x i64> %155, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %158 = bitcast <4 x i64> %157 to <16 x i16>
  %159 = icmp eq <16 x i16> %158, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %160 = sext <16 x i1> %159 to <16 x i16>
  %161 = bitcast <16 x i16> %160 to <4 x i64>
  %162 = xor <4 x i64> %161, <i64 -1, i64 -1, i64 -1, i64 -1>
  %163 = and <4 x i64> %157, %162
  %164 = bitcast <4 x i64> %163 to <16 x i16>
  %165 = icmp sgt <16 x i16> %101, %164
  %166 = select <16 x i1> %165, <16 x i16> %101, <16 x i16> %164
  %167 = icmp slt <16 x i16> %103, %158
  %168 = select <16 x i1> %167, <16 x i16> %103, <16 x i16> %158
  %169 = sub <16 x i16> %158, %98
  %170 = bitcast <16 x i16> %169 to <4 x i64>
  %171 = shufflevector <4 x i64> %170, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %172 = shufflevector <4 x i64> %170, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %173 = bitcast <2 x i64> %172 to <8 x i16>
  %174 = bitcast <2 x i64> %171 to <8 x i16>
  %175 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %173, <8 x i16> %174) #9
  %176 = ashr <16 x i8> %175, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %177 = sub <16 x i8> zeroinitializer, %175
  %178 = icmp slt <16 x i8> %175, zeroinitializer
  %179 = select <16 x i1> %178, <16 x i8> %177, <16 x i8> %175
  %180 = bitcast <16 x i8> %179 to <8 x i16>
  %181 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %180, <8 x i16> %124) #9
  %182 = bitcast <8 x i16> %181 to <16 x i8>
  %183 = and <16 x i8> %121, %182
  %184 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %117, <16 x i8> %183) #9
  %185 = icmp ult <16 x i8> %179, %184
  %186 = select <16 x i1> %185, <16 x i8> %179, <16 x i8> %184
  %187 = add <16 x i8> %186, %176
  %188 = xor <16 x i8> %187, %176
  %189 = load i32, i32* %32, align 8
  %190 = trunc i32 %189 to i8
  %191 = insertelement <32 x i8> undef, i8 %190, i32 0
  %192 = shufflevector <32 x i8> %191, <32 x i8> undef, <32 x i32> zeroinitializer
  %193 = shufflevector <16 x i8> %188, <16 x i8> %132, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %194 = bitcast <16 x i8> %193 to <2 x i64>
  %195 = shufflevector <16 x i8> %188, <16 x i8> %132, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %196 = bitcast <16 x i8> %195 to <2 x i64>
  %197 = shufflevector <2 x i64> %196, <2 x i64> %194, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %198 = bitcast <4 x i64> %197 to <32 x i8>
  %199 = tail call <16 x i16> @llvm.x86.avx2.pmadd.ub.sw(<32 x i8> %192, <32 x i8> %198) #9
  %200 = sext i32 %14 to i64
  %201 = getelementptr inbounds i16, i16* %2, i64 %200
  %202 = bitcast i16* %201 to i64*
  %203 = load i64, i64* %202, align 1
  %204 = add nsw i32 %14, 144
  %205 = sext i32 %204 to i64
  %206 = getelementptr inbounds i16, i16* %2, i64 %205
  %207 = bitcast i16* %206 to i64*
  %208 = load i64, i64* %207, align 1
  %209 = insertelement <2 x i64> undef, i64 %208, i32 0
  %210 = add nsw i32 %14, 288
  %211 = sext i32 %210 to i64
  %212 = getelementptr inbounds i16, i16* %2, i64 %211
  %213 = bitcast i16* %212 to i64*
  %214 = load i64, i64* %213, align 1
  %215 = add nsw i32 %14, 432
  %216 = sext i32 %215 to i64
  %217 = getelementptr inbounds i16, i16* %2, i64 %216
  %218 = bitcast i16* %217 to i64*
  %219 = load i64, i64* %218, align 1
  %220 = insertelement <2 x i64> undef, i64 %219, i32 0
  %221 = insertelement <2 x i64> %209, i64 %203, i32 1
  %222 = insertelement <2 x i64> %220, i64 %214, i32 1
  %223 = shufflevector <2 x i64> %222, <2 x i64> %221, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %224 = bitcast <4 x i64> %223 to <16 x i16>
  %225 = icmp eq <16 x i16> %224, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %226 = sext <16 x i1> %225 to <16 x i16>
  %227 = bitcast <16 x i16> %226 to <4 x i64>
  %228 = xor <4 x i64> %227, <i64 -1, i64 -1, i64 -1, i64 -1>
  %229 = and <4 x i64> %223, %228
  %230 = bitcast <4 x i64> %229 to <16 x i16>
  %231 = icmp sgt <16 x i16> %166, %230
  %232 = select <16 x i1> %231, <16 x i16> %166, <16 x i16> %230
  %233 = icmp slt <16 x i16> %168, %224
  %234 = select <16 x i1> %233, <16 x i16> %168, <16 x i16> %224
  %235 = sub <16 x i16> %224, %98
  %236 = bitcast <16 x i16> %235 to <4 x i64>
  %237 = shufflevector <4 x i64> %236, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %238 = shufflevector <4 x i64> %236, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %239 = bitcast <2 x i64> %238 to <8 x i16>
  %240 = bitcast <2 x i64> %237 to <8 x i16>
  %241 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %239, <8 x i16> %240) #9
  %242 = ashr <16 x i8> %241, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %243 = sub <16 x i8> zeroinitializer, %241
  %244 = icmp slt <16 x i8> %241, zeroinitializer
  %245 = select <16 x i1> %244, <16 x i8> %243, <16 x i8> %241
  %246 = bitcast <16 x i8> %245 to <8 x i16>
  %247 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %246, <8 x i16> %124) #9
  %248 = bitcast <8 x i16> %247 to <16 x i8>
  %249 = and <16 x i8> %121, %248
  %250 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %117, <16 x i8> %249) #9
  %251 = icmp ult <16 x i8> %245, %250
  %252 = select <16 x i1> %251, <16 x i8> %245, <16 x i8> %250
  %253 = add <16 x i8> %252, %242
  %254 = xor <16 x i8> %253, %242
  %255 = sub nsw i32 0, %14
  %256 = sext i32 %255 to i64
  %257 = getelementptr inbounds i16, i16* %2, i64 %256
  %258 = bitcast i16* %257 to i64*
  %259 = load i64, i64* %258, align 1
  %260 = sub nsw i32 144, %14
  %261 = sext i32 %260 to i64
  %262 = getelementptr inbounds i16, i16* %2, i64 %261
  %263 = bitcast i16* %262 to i64*
  %264 = load i64, i64* %263, align 1
  %265 = insertelement <2 x i64> undef, i64 %264, i32 0
  %266 = sub nsw i32 288, %14
  %267 = sext i32 %266 to i64
  %268 = getelementptr inbounds i16, i16* %2, i64 %267
  %269 = bitcast i16* %268 to i64*
  %270 = load i64, i64* %269, align 1
  %271 = sub nsw i32 432, %14
  %272 = sext i32 %271 to i64
  %273 = getelementptr inbounds i16, i16* %2, i64 %272
  %274 = bitcast i16* %273 to i64*
  %275 = load i64, i64* %274, align 1
  %276 = insertelement <2 x i64> undef, i64 %275, i32 0
  %277 = insertelement <2 x i64> %265, i64 %259, i32 1
  %278 = insertelement <2 x i64> %276, i64 %270, i32 1
  %279 = shufflevector <2 x i64> %278, <2 x i64> %277, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %280 = bitcast <4 x i64> %279 to <16 x i16>
  %281 = icmp eq <16 x i16> %280, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %282 = sext <16 x i1> %281 to <16 x i16>
  %283 = bitcast <16 x i16> %282 to <4 x i64>
  %284 = xor <4 x i64> %283, <i64 -1, i64 -1, i64 -1, i64 -1>
  %285 = and <4 x i64> %279, %284
  %286 = bitcast <4 x i64> %285 to <16 x i16>
  %287 = icmp sgt <16 x i16> %232, %286
  %288 = select <16 x i1> %287, <16 x i16> %232, <16 x i16> %286
  %289 = bitcast <16 x i16> %288 to <4 x i64>
  %290 = icmp slt <16 x i16> %234, %280
  %291 = select <16 x i1> %290, <16 x i16> %234, <16 x i16> %280
  %292 = bitcast <16 x i16> %291 to <4 x i64>
  %293 = sub <16 x i16> %280, %98
  %294 = bitcast <16 x i16> %293 to <4 x i64>
  %295 = shufflevector <4 x i64> %294, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %296 = shufflevector <4 x i64> %294, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %297 = bitcast <2 x i64> %296 to <8 x i16>
  %298 = bitcast <2 x i64> %295 to <8 x i16>
  %299 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %297, <8 x i16> %298) #9
  %300 = ashr <16 x i8> %299, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %301 = sub <16 x i8> zeroinitializer, %299
  %302 = icmp slt <16 x i8> %299, zeroinitializer
  %303 = select <16 x i1> %302, <16 x i8> %301, <16 x i8> %299
  %304 = bitcast <16 x i8> %303 to <8 x i16>
  %305 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %304, <8 x i16> %124) #9
  %306 = bitcast <8 x i16> %305 to <16 x i8>
  %307 = and <16 x i8> %121, %306
  %308 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %117, <16 x i8> %307) #9
  %309 = icmp ult <16 x i8> %303, %308
  %310 = select <16 x i1> %309, <16 x i8> %303, <16 x i8> %308
  %311 = add <16 x i8> %310, %300
  %312 = xor <16 x i8> %311, %300
  %313 = getelementptr inbounds [2 x [2 x i32]], [2 x [2 x i32]]* @cdef_pri_taps, i64 0, i64 %31, i64 1
  %314 = load i32, i32* %313, align 4
  %315 = trunc i32 %314 to i8
  %316 = insertelement <32 x i8> undef, i8 %315, i32 0
  %317 = shufflevector <32 x i8> %316, <32 x i8> undef, <32 x i32> zeroinitializer
  %318 = shufflevector <16 x i8> %312, <16 x i8> %254, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %319 = bitcast <16 x i8> %318 to <2 x i64>
  %320 = shufflevector <16 x i8> %312, <16 x i8> %254, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %321 = bitcast <16 x i8> %320 to <2 x i64>
  %322 = shufflevector <2 x i64> %321, <2 x i64> %319, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %323 = bitcast <4 x i64> %322 to <32 x i8>
  %324 = tail call <16 x i16> @llvm.x86.avx2.pmadd.ub.sw(<32 x i8> %317, <32 x i8> %323) #9
  %325 = add <16 x i16> %324, %199
  br label %326

326:                                              ; preds = %67, %49
  %327 = phi <16 x i16> [ %325, %67 ], [ zeroinitializer, %49 ]
  %328 = phi <4 x i64> [ %289, %67 ], [ %66, %49 ]
  %329 = phi <4 x i64> [ %292, %67 ], [ %66, %49 ]
  br i1 %42, label %332, label %330

330:                                              ; preds = %326
  %331 = bitcast <4 x i64> %66 to <16 x i16>
  br label %819

332:                                              ; preds = %326
  %333 = sext i32 %19 to i64
  %334 = getelementptr inbounds i16, i16* %2, i64 %333
  %335 = bitcast i16* %334 to i64*
  %336 = load i64, i64* %335, align 1
  %337 = add nsw i32 %19, 144
  %338 = sext i32 %337 to i64
  %339 = getelementptr inbounds i16, i16* %2, i64 %338
  %340 = bitcast i16* %339 to i64*
  %341 = load i64, i64* %340, align 1
  %342 = insertelement <2 x i64> undef, i64 %341, i32 0
  %343 = add nsw i32 %19, 288
  %344 = sext i32 %343 to i64
  %345 = getelementptr inbounds i16, i16* %2, i64 %344
  %346 = bitcast i16* %345 to i64*
  %347 = load i64, i64* %346, align 1
  %348 = add nsw i32 %19, 432
  %349 = sext i32 %348 to i64
  %350 = getelementptr inbounds i16, i16* %2, i64 %349
  %351 = bitcast i16* %350 to i64*
  %352 = load i64, i64* %351, align 1
  %353 = insertelement <2 x i64> undef, i64 %352, i32 0
  %354 = insertelement <2 x i64> %342, i64 %336, i32 1
  %355 = insertelement <2 x i64> %353, i64 %347, i32 1
  %356 = shufflevector <2 x i64> %355, <2 x i64> %354, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %357 = bitcast <4 x i64> %356 to <16 x i16>
  %358 = icmp eq <16 x i16> %357, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %359 = sext <16 x i1> %358 to <16 x i16>
  %360 = bitcast <16 x i16> %359 to <4 x i64>
  %361 = xor <4 x i64> %360, <i64 -1, i64 -1, i64 -1, i64 -1>
  %362 = and <4 x i64> %356, %361
  %363 = bitcast <4 x i64> %328 to <16 x i16>
  %364 = bitcast <4 x i64> %362 to <16 x i16>
  %365 = icmp sgt <16 x i16> %363, %364
  %366 = select <16 x i1> %365, <16 x i16> %363, <16 x i16> %364
  %367 = bitcast <4 x i64> %329 to <16 x i16>
  %368 = icmp slt <16 x i16> %367, %357
  %369 = select <16 x i1> %368, <16 x i16> %367, <16 x i16> %357
  %370 = bitcast <4 x i64> %66 to <16 x i16>
  %371 = sub <16 x i16> %357, %370
  %372 = bitcast <16 x i16> %371 to <4 x i64>
  %373 = shufflevector <4 x i64> %372, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %374 = shufflevector <4 x i64> %372, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %375 = bitcast <2 x i64> %374 to <8 x i16>
  %376 = bitcast <2 x i64> %373 to <8 x i16>
  %377 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %375, <8 x i16> %376) #9
  %378 = ashr <16 x i8> %377, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %379 = sub <16 x i8> zeroinitializer, %377
  %380 = icmp slt <16 x i8> %377, zeroinitializer
  %381 = select <16 x i1> %380, <16 x i8> %379, <16 x i8> %377
  %382 = trunc i32 %4 to i8
  %383 = insertelement <16 x i8> undef, i8 %382, i32 0
  %384 = shufflevector <16 x i8> %383, <16 x i8> undef, <16 x i32> zeroinitializer
  %385 = lshr i32 255, %50
  %386 = trunc i32 %385 to i8
  %387 = insertelement <16 x i8> undef, i8 %386, i32 0
  %388 = shufflevector <16 x i8> %387, <16 x i8> undef, <16 x i32> zeroinitializer
  %389 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %50, i32 0
  %390 = bitcast <16 x i8> %381 to <8 x i16>
  %391 = bitcast <4 x i32> %389 to <8 x i16>
  %392 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %390, <8 x i16> %391) #9
  %393 = bitcast <8 x i16> %392 to <16 x i8>
  %394 = and <16 x i8> %388, %393
  %395 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %384, <16 x i8> %394) #9
  %396 = icmp ult <16 x i8> %381, %395
  %397 = select <16 x i1> %396, <16 x i8> %381, <16 x i8> %395
  %398 = add <16 x i8> %397, %378
  %399 = xor <16 x i8> %398, %378
  %400 = sub nsw i32 0, %19
  %401 = sext i32 %400 to i64
  %402 = getelementptr inbounds i16, i16* %2, i64 %401
  %403 = bitcast i16* %402 to i64*
  %404 = load i64, i64* %403, align 1
  %405 = sub nsw i32 144, %19
  %406 = sext i32 %405 to i64
  %407 = getelementptr inbounds i16, i16* %2, i64 %406
  %408 = bitcast i16* %407 to i64*
  %409 = load i64, i64* %408, align 1
  %410 = insertelement <2 x i64> undef, i64 %409, i32 0
  %411 = sub nsw i32 288, %19
  %412 = sext i32 %411 to i64
  %413 = getelementptr inbounds i16, i16* %2, i64 %412
  %414 = bitcast i16* %413 to i64*
  %415 = load i64, i64* %414, align 1
  %416 = sub nsw i32 432, %19
  %417 = sext i32 %416 to i64
  %418 = getelementptr inbounds i16, i16* %2, i64 %417
  %419 = bitcast i16* %418 to i64*
  %420 = load i64, i64* %419, align 1
  %421 = insertelement <2 x i64> undef, i64 %420, i32 0
  %422 = insertelement <2 x i64> %410, i64 %404, i32 1
  %423 = insertelement <2 x i64> %421, i64 %415, i32 1
  %424 = shufflevector <2 x i64> %423, <2 x i64> %422, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %425 = bitcast <4 x i64> %424 to <16 x i16>
  %426 = icmp eq <16 x i16> %425, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %427 = sext <16 x i1> %426 to <16 x i16>
  %428 = bitcast <16 x i16> %427 to <4 x i64>
  %429 = xor <4 x i64> %428, <i64 -1, i64 -1, i64 -1, i64 -1>
  %430 = and <4 x i64> %424, %429
  %431 = bitcast <4 x i64> %430 to <16 x i16>
  %432 = icmp sgt <16 x i16> %366, %431
  %433 = select <16 x i1> %432, <16 x i16> %366, <16 x i16> %431
  %434 = icmp slt <16 x i16> %369, %425
  %435 = select <16 x i1> %434, <16 x i16> %369, <16 x i16> %425
  %436 = sub <16 x i16> %425, %370
  %437 = bitcast <16 x i16> %436 to <4 x i64>
  %438 = shufflevector <4 x i64> %437, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %439 = shufflevector <4 x i64> %437, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %440 = bitcast <2 x i64> %439 to <8 x i16>
  %441 = bitcast <2 x i64> %438 to <8 x i16>
  %442 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %440, <8 x i16> %441) #9
  %443 = ashr <16 x i8> %442, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %444 = sub <16 x i8> zeroinitializer, %442
  %445 = icmp slt <16 x i8> %442, zeroinitializer
  %446 = select <16 x i1> %445, <16 x i8> %444, <16 x i8> %442
  %447 = bitcast <16 x i8> %446 to <8 x i16>
  %448 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %447, <8 x i16> %391) #9
  %449 = bitcast <8 x i16> %448 to <16 x i8>
  %450 = and <16 x i8> %388, %449
  %451 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %384, <16 x i8> %450) #9
  %452 = icmp ult <16 x i8> %446, %451
  %453 = select <16 x i1> %452, <16 x i8> %446, <16 x i8> %451
  %454 = add <16 x i8> %453, %443
  %455 = xor <16 x i8> %454, %443
  %456 = sext i32 %26 to i64
  %457 = getelementptr inbounds i16, i16* %2, i64 %456
  %458 = bitcast i16* %457 to i64*
  %459 = load i64, i64* %458, align 1
  %460 = add nsw i32 %26, 144
  %461 = sext i32 %460 to i64
  %462 = getelementptr inbounds i16, i16* %2, i64 %461
  %463 = bitcast i16* %462 to i64*
  %464 = load i64, i64* %463, align 1
  %465 = insertelement <2 x i64> undef, i64 %464, i32 0
  %466 = add nsw i32 %26, 288
  %467 = sext i32 %466 to i64
  %468 = getelementptr inbounds i16, i16* %2, i64 %467
  %469 = bitcast i16* %468 to i64*
  %470 = load i64, i64* %469, align 1
  %471 = add nsw i32 %26, 432
  %472 = sext i32 %471 to i64
  %473 = getelementptr inbounds i16, i16* %2, i64 %472
  %474 = bitcast i16* %473 to i64*
  %475 = load i64, i64* %474, align 1
  %476 = insertelement <2 x i64> undef, i64 %475, i32 0
  %477 = insertelement <2 x i64> %465, i64 %459, i32 1
  %478 = insertelement <2 x i64> %476, i64 %470, i32 1
  %479 = shufflevector <2 x i64> %478, <2 x i64> %477, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %480 = bitcast <4 x i64> %479 to <16 x i16>
  %481 = icmp eq <16 x i16> %480, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %482 = sext <16 x i1> %481 to <16 x i16>
  %483 = bitcast <16 x i16> %482 to <4 x i64>
  %484 = xor <4 x i64> %483, <i64 -1, i64 -1, i64 -1, i64 -1>
  %485 = and <4 x i64> %479, %484
  %486 = bitcast <4 x i64> %485 to <16 x i16>
  %487 = icmp sgt <16 x i16> %433, %486
  %488 = select <16 x i1> %487, <16 x i16> %433, <16 x i16> %486
  %489 = icmp slt <16 x i16> %435, %480
  %490 = select <16 x i1> %489, <16 x i16> %435, <16 x i16> %480
  %491 = sub <16 x i16> %480, %370
  %492 = bitcast <16 x i16> %491 to <4 x i64>
  %493 = shufflevector <4 x i64> %492, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %494 = shufflevector <4 x i64> %492, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %495 = bitcast <2 x i64> %494 to <8 x i16>
  %496 = bitcast <2 x i64> %493 to <8 x i16>
  %497 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %495, <8 x i16> %496) #9
  %498 = ashr <16 x i8> %497, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %499 = sub <16 x i8> zeroinitializer, %497
  %500 = icmp slt <16 x i8> %497, zeroinitializer
  %501 = select <16 x i1> %500, <16 x i8> %499, <16 x i8> %497
  %502 = bitcast <16 x i8> %501 to <8 x i16>
  %503 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %502, <8 x i16> %391) #9
  %504 = bitcast <8 x i16> %503 to <16 x i8>
  %505 = and <16 x i8> %388, %504
  %506 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %384, <16 x i8> %505) #9
  %507 = icmp ult <16 x i8> %501, %506
  %508 = select <16 x i1> %507, <16 x i8> %501, <16 x i8> %506
  %509 = add <16 x i8> %508, %498
  %510 = xor <16 x i8> %509, %498
  %511 = sub nsw i32 0, %26
  %512 = sext i32 %511 to i64
  %513 = getelementptr inbounds i16, i16* %2, i64 %512
  %514 = bitcast i16* %513 to i64*
  %515 = load i64, i64* %514, align 1
  %516 = sub nsw i32 144, %26
  %517 = sext i32 %516 to i64
  %518 = getelementptr inbounds i16, i16* %2, i64 %517
  %519 = bitcast i16* %518 to i64*
  %520 = load i64, i64* %519, align 1
  %521 = insertelement <2 x i64> undef, i64 %520, i32 0
  %522 = sub nsw i32 288, %26
  %523 = sext i32 %522 to i64
  %524 = getelementptr inbounds i16, i16* %2, i64 %523
  %525 = bitcast i16* %524 to i64*
  %526 = load i64, i64* %525, align 1
  %527 = sub nsw i32 432, %26
  %528 = sext i32 %527 to i64
  %529 = getelementptr inbounds i16, i16* %2, i64 %528
  %530 = bitcast i16* %529 to i64*
  %531 = load i64, i64* %530, align 1
  %532 = insertelement <2 x i64> undef, i64 %531, i32 0
  %533 = insertelement <2 x i64> %521, i64 %515, i32 1
  %534 = insertelement <2 x i64> %532, i64 %526, i32 1
  %535 = shufflevector <2 x i64> %534, <2 x i64> %533, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %536 = bitcast <4 x i64> %535 to <16 x i16>
  %537 = icmp eq <16 x i16> %536, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %538 = sext <16 x i1> %537 to <16 x i16>
  %539 = bitcast <16 x i16> %538 to <4 x i64>
  %540 = xor <4 x i64> %539, <i64 -1, i64 -1, i64 -1, i64 -1>
  %541 = and <4 x i64> %535, %540
  %542 = bitcast <4 x i64> %541 to <16 x i16>
  %543 = icmp sgt <16 x i16> %488, %542
  %544 = select <16 x i1> %543, <16 x i16> %488, <16 x i16> %542
  %545 = icmp slt <16 x i16> %490, %536
  %546 = select <16 x i1> %545, <16 x i16> %490, <16 x i16> %536
  %547 = sub <16 x i16> %536, %370
  %548 = bitcast <16 x i16> %547 to <4 x i64>
  %549 = shufflevector <4 x i64> %548, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %550 = shufflevector <4 x i64> %548, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %551 = bitcast <2 x i64> %550 to <8 x i16>
  %552 = bitcast <2 x i64> %549 to <8 x i16>
  %553 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %551, <8 x i16> %552) #9
  %554 = ashr <16 x i8> %553, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %555 = sub <16 x i8> zeroinitializer, %553
  %556 = icmp slt <16 x i8> %553, zeroinitializer
  %557 = select <16 x i1> %556, <16 x i8> %555, <16 x i8> %553
  %558 = bitcast <16 x i8> %557 to <8 x i16>
  %559 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %558, <8 x i16> %391) #9
  %560 = bitcast <8 x i16> %559 to <16 x i8>
  %561 = and <16 x i8> %388, %560
  %562 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %384, <16 x i8> %561) #9
  %563 = icmp ult <16 x i8> %557, %562
  %564 = select <16 x i1> %563, <16 x i8> %557, <16 x i8> %562
  %565 = add <16 x i8> %564, %554
  %566 = xor <16 x i8> %565, %554
  %567 = add <16 x i8> %455, %399
  %568 = add <16 x i8> %566, %510
  %569 = load i32, i32* getelementptr inbounds ([2 x i32], [2 x i32]* @cdef_sec_taps, i64 0, i64 0), align 4
  %570 = trunc i32 %569 to i8
  %571 = insertelement <32 x i8> undef, i8 %570, i32 0
  %572 = shufflevector <32 x i8> %571, <32 x i8> undef, <32 x i32> zeroinitializer
  %573 = shufflevector <16 x i8> %568, <16 x i8> %567, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %574 = bitcast <16 x i8> %573 to <2 x i64>
  %575 = shufflevector <16 x i8> %568, <16 x i8> %567, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %576 = bitcast <16 x i8> %575 to <2 x i64>
  %577 = shufflevector <2 x i64> %576, <2 x i64> %574, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %578 = bitcast <4 x i64> %577 to <32 x i8>
  %579 = tail call <16 x i16> @llvm.x86.avx2.pmadd.ub.sw(<32 x i8> %572, <32 x i8> %578) #9
  %580 = add <16 x i16> %579, %327
  %581 = sext i32 %21 to i64
  %582 = getelementptr inbounds i16, i16* %2, i64 %581
  %583 = bitcast i16* %582 to i64*
  %584 = load i64, i64* %583, align 1
  %585 = add nsw i32 %21, 144
  %586 = sext i32 %585 to i64
  %587 = getelementptr inbounds i16, i16* %2, i64 %586
  %588 = bitcast i16* %587 to i64*
  %589 = load i64, i64* %588, align 1
  %590 = insertelement <2 x i64> undef, i64 %589, i32 0
  %591 = add nsw i32 %21, 288
  %592 = sext i32 %591 to i64
  %593 = getelementptr inbounds i16, i16* %2, i64 %592
  %594 = bitcast i16* %593 to i64*
  %595 = load i64, i64* %594, align 1
  %596 = add nsw i32 %21, 432
  %597 = sext i32 %596 to i64
  %598 = getelementptr inbounds i16, i16* %2, i64 %597
  %599 = bitcast i16* %598 to i64*
  %600 = load i64, i64* %599, align 1
  %601 = insertelement <2 x i64> undef, i64 %600, i32 0
  %602 = insertelement <2 x i64> %590, i64 %584, i32 1
  %603 = insertelement <2 x i64> %601, i64 %595, i32 1
  %604 = shufflevector <2 x i64> %603, <2 x i64> %602, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %605 = bitcast <4 x i64> %604 to <16 x i16>
  %606 = icmp eq <16 x i16> %605, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %607 = sext <16 x i1> %606 to <16 x i16>
  %608 = bitcast <16 x i16> %607 to <4 x i64>
  %609 = xor <4 x i64> %608, <i64 -1, i64 -1, i64 -1, i64 -1>
  %610 = and <4 x i64> %604, %609
  %611 = bitcast <4 x i64> %610 to <16 x i16>
  %612 = icmp sgt <16 x i16> %544, %611
  %613 = select <16 x i1> %612, <16 x i16> %544, <16 x i16> %611
  %614 = icmp slt <16 x i16> %546, %605
  %615 = select <16 x i1> %614, <16 x i16> %546, <16 x i16> %605
  %616 = sub <16 x i16> %605, %370
  %617 = bitcast <16 x i16> %616 to <4 x i64>
  %618 = shufflevector <4 x i64> %617, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %619 = shufflevector <4 x i64> %617, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %620 = bitcast <2 x i64> %619 to <8 x i16>
  %621 = bitcast <2 x i64> %618 to <8 x i16>
  %622 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %620, <8 x i16> %621) #9
  %623 = ashr <16 x i8> %622, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %624 = sub <16 x i8> zeroinitializer, %622
  %625 = icmp slt <16 x i8> %622, zeroinitializer
  %626 = select <16 x i1> %625, <16 x i8> %624, <16 x i8> %622
  %627 = bitcast <16 x i8> %626 to <8 x i16>
  %628 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %627, <8 x i16> %391) #9
  %629 = bitcast <8 x i16> %628 to <16 x i8>
  %630 = and <16 x i8> %388, %629
  %631 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %384, <16 x i8> %630) #9
  %632 = icmp ult <16 x i8> %626, %631
  %633 = select <16 x i1> %632, <16 x i8> %626, <16 x i8> %631
  %634 = add <16 x i8> %633, %623
  %635 = xor <16 x i8> %634, %623
  %636 = sub nsw i32 0, %21
  %637 = sext i32 %636 to i64
  %638 = getelementptr inbounds i16, i16* %2, i64 %637
  %639 = bitcast i16* %638 to i64*
  %640 = load i64, i64* %639, align 1
  %641 = sub nsw i32 144, %21
  %642 = sext i32 %641 to i64
  %643 = getelementptr inbounds i16, i16* %2, i64 %642
  %644 = bitcast i16* %643 to i64*
  %645 = load i64, i64* %644, align 1
  %646 = insertelement <2 x i64> undef, i64 %645, i32 0
  %647 = sub nsw i32 288, %21
  %648 = sext i32 %647 to i64
  %649 = getelementptr inbounds i16, i16* %2, i64 %648
  %650 = bitcast i16* %649 to i64*
  %651 = load i64, i64* %650, align 1
  %652 = sub nsw i32 432, %21
  %653 = sext i32 %652 to i64
  %654 = getelementptr inbounds i16, i16* %2, i64 %653
  %655 = bitcast i16* %654 to i64*
  %656 = load i64, i64* %655, align 1
  %657 = insertelement <2 x i64> undef, i64 %656, i32 0
  %658 = insertelement <2 x i64> %646, i64 %640, i32 1
  %659 = insertelement <2 x i64> %657, i64 %651, i32 1
  %660 = shufflevector <2 x i64> %659, <2 x i64> %658, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %661 = bitcast <4 x i64> %660 to <16 x i16>
  %662 = icmp eq <16 x i16> %661, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %663 = sext <16 x i1> %662 to <16 x i16>
  %664 = bitcast <16 x i16> %663 to <4 x i64>
  %665 = xor <4 x i64> %664, <i64 -1, i64 -1, i64 -1, i64 -1>
  %666 = and <4 x i64> %660, %665
  %667 = bitcast <4 x i64> %666 to <16 x i16>
  %668 = icmp sgt <16 x i16> %613, %667
  %669 = select <16 x i1> %668, <16 x i16> %613, <16 x i16> %667
  %670 = icmp slt <16 x i16> %615, %661
  %671 = select <16 x i1> %670, <16 x i16> %615, <16 x i16> %661
  %672 = sub <16 x i16> %661, %370
  %673 = bitcast <16 x i16> %672 to <4 x i64>
  %674 = shufflevector <4 x i64> %673, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %675 = shufflevector <4 x i64> %673, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %676 = bitcast <2 x i64> %675 to <8 x i16>
  %677 = bitcast <2 x i64> %674 to <8 x i16>
  %678 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %676, <8 x i16> %677) #9
  %679 = ashr <16 x i8> %678, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %680 = sub <16 x i8> zeroinitializer, %678
  %681 = icmp slt <16 x i8> %678, zeroinitializer
  %682 = select <16 x i1> %681, <16 x i8> %680, <16 x i8> %678
  %683 = bitcast <16 x i8> %682 to <8 x i16>
  %684 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %683, <8 x i16> %391) #9
  %685 = bitcast <8 x i16> %684 to <16 x i8>
  %686 = and <16 x i8> %388, %685
  %687 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %384, <16 x i8> %686) #9
  %688 = icmp ult <16 x i8> %682, %687
  %689 = select <16 x i1> %688, <16 x i8> %682, <16 x i8> %687
  %690 = add <16 x i8> %689, %679
  %691 = xor <16 x i8> %690, %679
  %692 = sext i32 %28 to i64
  %693 = getelementptr inbounds i16, i16* %2, i64 %692
  %694 = bitcast i16* %693 to i64*
  %695 = load i64, i64* %694, align 1
  %696 = add nsw i32 %28, 144
  %697 = sext i32 %696 to i64
  %698 = getelementptr inbounds i16, i16* %2, i64 %697
  %699 = bitcast i16* %698 to i64*
  %700 = load i64, i64* %699, align 1
  %701 = insertelement <2 x i64> undef, i64 %700, i32 0
  %702 = add nsw i32 %28, 288
  %703 = sext i32 %702 to i64
  %704 = getelementptr inbounds i16, i16* %2, i64 %703
  %705 = bitcast i16* %704 to i64*
  %706 = load i64, i64* %705, align 1
  %707 = add nsw i32 %28, 432
  %708 = sext i32 %707 to i64
  %709 = getelementptr inbounds i16, i16* %2, i64 %708
  %710 = bitcast i16* %709 to i64*
  %711 = load i64, i64* %710, align 1
  %712 = insertelement <2 x i64> undef, i64 %711, i32 0
  %713 = insertelement <2 x i64> %701, i64 %695, i32 1
  %714 = insertelement <2 x i64> %712, i64 %706, i32 1
  %715 = shufflevector <2 x i64> %714, <2 x i64> %713, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %716 = bitcast <4 x i64> %715 to <16 x i16>
  %717 = icmp eq <16 x i16> %716, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %718 = sext <16 x i1> %717 to <16 x i16>
  %719 = bitcast <16 x i16> %718 to <4 x i64>
  %720 = xor <4 x i64> %719, <i64 -1, i64 -1, i64 -1, i64 -1>
  %721 = and <4 x i64> %715, %720
  %722 = bitcast <4 x i64> %721 to <16 x i16>
  %723 = icmp sgt <16 x i16> %669, %722
  %724 = select <16 x i1> %723, <16 x i16> %669, <16 x i16> %722
  %725 = icmp slt <16 x i16> %671, %716
  %726 = select <16 x i1> %725, <16 x i16> %671, <16 x i16> %716
  %727 = sub <16 x i16> %716, %370
  %728 = bitcast <16 x i16> %727 to <4 x i64>
  %729 = shufflevector <4 x i64> %728, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %730 = shufflevector <4 x i64> %728, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %731 = bitcast <2 x i64> %730 to <8 x i16>
  %732 = bitcast <2 x i64> %729 to <8 x i16>
  %733 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %731, <8 x i16> %732) #9
  %734 = ashr <16 x i8> %733, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %735 = sub <16 x i8> zeroinitializer, %733
  %736 = icmp slt <16 x i8> %733, zeroinitializer
  %737 = select <16 x i1> %736, <16 x i8> %735, <16 x i8> %733
  %738 = bitcast <16 x i8> %737 to <8 x i16>
  %739 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %738, <8 x i16> %391) #9
  %740 = bitcast <8 x i16> %739 to <16 x i8>
  %741 = and <16 x i8> %388, %740
  %742 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %384, <16 x i8> %741) #9
  %743 = icmp ult <16 x i8> %737, %742
  %744 = select <16 x i1> %743, <16 x i8> %737, <16 x i8> %742
  %745 = add <16 x i8> %744, %734
  %746 = xor <16 x i8> %745, %734
  %747 = sub nsw i32 0, %28
  %748 = sext i32 %747 to i64
  %749 = getelementptr inbounds i16, i16* %2, i64 %748
  %750 = bitcast i16* %749 to i64*
  %751 = load i64, i64* %750, align 1
  %752 = sub nsw i32 144, %28
  %753 = sext i32 %752 to i64
  %754 = getelementptr inbounds i16, i16* %2, i64 %753
  %755 = bitcast i16* %754 to i64*
  %756 = load i64, i64* %755, align 1
  %757 = insertelement <2 x i64> undef, i64 %756, i32 0
  %758 = sub nsw i32 288, %28
  %759 = sext i32 %758 to i64
  %760 = getelementptr inbounds i16, i16* %2, i64 %759
  %761 = bitcast i16* %760 to i64*
  %762 = load i64, i64* %761, align 1
  %763 = sub nsw i32 432, %28
  %764 = sext i32 %763 to i64
  %765 = getelementptr inbounds i16, i16* %2, i64 %764
  %766 = bitcast i16* %765 to i64*
  %767 = load i64, i64* %766, align 1
  %768 = insertelement <2 x i64> undef, i64 %767, i32 0
  %769 = insertelement <2 x i64> %757, i64 %751, i32 1
  %770 = insertelement <2 x i64> %768, i64 %762, i32 1
  %771 = shufflevector <2 x i64> %770, <2 x i64> %769, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %772 = bitcast <4 x i64> %771 to <16 x i16>
  %773 = icmp eq <16 x i16> %772, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %774 = sext <16 x i1> %773 to <16 x i16>
  %775 = bitcast <16 x i16> %774 to <4 x i64>
  %776 = xor <4 x i64> %775, <i64 -1, i64 -1, i64 -1, i64 -1>
  %777 = and <4 x i64> %771, %776
  %778 = bitcast <4 x i64> %777 to <16 x i16>
  %779 = icmp sgt <16 x i16> %724, %778
  %780 = select <16 x i1> %779, <16 x i16> %724, <16 x i16> %778
  %781 = bitcast <16 x i16> %780 to <4 x i64>
  %782 = icmp slt <16 x i16> %726, %772
  %783 = select <16 x i1> %782, <16 x i16> %726, <16 x i16> %772
  %784 = bitcast <16 x i16> %783 to <4 x i64>
  %785 = sub <16 x i16> %772, %370
  %786 = bitcast <16 x i16> %785 to <4 x i64>
  %787 = shufflevector <4 x i64> %786, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %788 = shufflevector <4 x i64> %786, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %789 = bitcast <2 x i64> %788 to <8 x i16>
  %790 = bitcast <2 x i64> %787 to <8 x i16>
  %791 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %789, <8 x i16> %790) #9
  %792 = ashr <16 x i8> %791, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %793 = sub <16 x i8> zeroinitializer, %791
  %794 = icmp slt <16 x i8> %791, zeroinitializer
  %795 = select <16 x i1> %794, <16 x i8> %793, <16 x i8> %791
  %796 = bitcast <16 x i8> %795 to <8 x i16>
  %797 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %796, <8 x i16> %391) #9
  %798 = bitcast <8 x i16> %797 to <16 x i8>
  %799 = and <16 x i8> %388, %798
  %800 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %384, <16 x i8> %799) #9
  %801 = icmp ult <16 x i8> %795, %800
  %802 = select <16 x i1> %801, <16 x i8> %795, <16 x i8> %800
  %803 = add <16 x i8> %802, %792
  %804 = xor <16 x i8> %803, %792
  %805 = add <16 x i8> %691, %635
  %806 = add <16 x i8> %804, %746
  %807 = load i32, i32* getelementptr inbounds ([2 x i32], [2 x i32]* @cdef_sec_taps, i64 0, i64 1), align 4
  %808 = trunc i32 %807 to i8
  %809 = insertelement <32 x i8> undef, i8 %808, i32 0
  %810 = shufflevector <32 x i8> %809, <32 x i8> undef, <32 x i32> zeroinitializer
  %811 = shufflevector <16 x i8> %806, <16 x i8> %805, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %812 = bitcast <16 x i8> %811 to <2 x i64>
  %813 = shufflevector <16 x i8> %806, <16 x i8> %805, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %814 = bitcast <16 x i8> %813 to <2 x i64>
  %815 = shufflevector <2 x i64> %814, <2 x i64> %812, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %816 = bitcast <4 x i64> %815 to <32 x i8>
  %817 = tail call <16 x i16> @llvm.x86.avx2.pmadd.ub.sw(<32 x i8> %810, <32 x i8> %816) #9
  %818 = add <16 x i16> %580, %817
  br label %819

819:                                              ; preds = %330, %332
  %820 = phi <16 x i16> [ %331, %330 ], [ %370, %332 ]
  %821 = phi <16 x i16> [ %327, %330 ], [ %818, %332 ]
  %822 = phi <4 x i64> [ %328, %330 ], [ %781, %332 ]
  %823 = phi <4 x i64> [ %329, %330 ], [ %784, %332 ]
  %824 = ashr <16 x i16> %821, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %825 = add <16 x i16> %821, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %826 = add <16 x i16> %825, %824
  %827 = ashr <16 x i16> %826, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %828 = add <16 x i16> %827, %820
  %829 = bitcast <4 x i64> %823 to <16 x i16>
  %830 = icmp sgt <16 x i16> %828, %829
  %831 = select <16 x i1> %830, <16 x i16> %828, <16 x i16> %829
  %832 = bitcast <4 x i64> %822 to <16 x i16>
  %833 = icmp slt <16 x i16> %831, %832
  %834 = select <16 x i1> %833, <16 x i16> %831, <16 x i16> %832
  %835 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %834, <16 x i16> undef) #9
  %836 = bitcast <32 x i8> %835 to <4 x i64>
  %837 = shufflevector <4 x i64> %836, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  %838 = bitcast <2 x i64> %837 to <16 x i8>
  %839 = shufflevector <16 x i8> %838, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %840 = shufflevector <16 x i8> %839, <16 x i8> undef, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %841 = bitcast <16 x i8> %840 to <4 x i32>
  %842 = extractelement <4 x i32> %841, i32 0
  %843 = bitcast i8* %0 to i32*
  store i32 %842, i32* %843, align 4
  %844 = sext i32 %1 to i64
  %845 = getelementptr inbounds i8, i8* %0, i64 %844
  %846 = bitcast <16 x i8> %839 to <4 x i32>
  %847 = extractelement <4 x i32> %846, i32 0
  %848 = bitcast i8* %845 to i32*
  store i32 %847, i32* %848, align 4
  %849 = shl nsw i32 %1, 1
  %850 = sext i32 %849 to i64
  %851 = getelementptr inbounds i8, i8* %0, i64 %850
  %852 = insertelement <2 x i64> %837, i64 0, i32 1
  %853 = bitcast <2 x i64> %852 to <16 x i8>
  %854 = shufflevector <16 x i8> %853, <16 x i8> undef, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %855 = bitcast <16 x i8> %854 to <4 x i32>
  %856 = extractelement <4 x i32> %855, i32 0
  %857 = bitcast i8* %851 to i32*
  store i32 %856, i32* %857, align 4
  %858 = mul nsw i32 %1, 3
  %859 = sext i32 %858 to i64
  %860 = getelementptr inbounds i8, i8* %0, i64 %859
  %861 = bitcast <2 x i64> %852 to <4 x i32>
  %862 = extractelement <4 x i32> %861, i32 0
  %863 = bitcast i8* %860 to i32*
  store i32 %862, i32* %863, align 4
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define hidden void @cdef_filter_block_8x8_8_avx2(i8* nocapture, i32, i16*, i32, i32, i32, i32, i32, i32) local_unnamed_addr #2 {
  %10 = sext i32 %5 to i64
  %11 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %10, i64 0
  %12 = load i32, i32* %11, align 8
  %13 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %10, i64 1
  %14 = load i32, i32* %13, align 4
  %15 = add nsw i32 %5, 2
  %16 = and i32 %15, 7
  %17 = zext i32 %16 to i64
  %18 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %17, i64 0
  %19 = load i32, i32* %18, align 8
  %20 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %17, i64 1
  %21 = load i32, i32* %20, align 4
  %22 = add nsw i32 %5, 6
  %23 = and i32 %22, 7
  %24 = zext i32 %23 to i64
  %25 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %24, i64 0
  %26 = load i32, i32* %25, align 8
  %27 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %24, i64 1
  %28 = load i32, i32* %27, align 4
  %29 = lshr i32 %3, %8
  %30 = and i32 %29, 1
  %31 = zext i32 %30 to i64
  %32 = getelementptr inbounds [2 x [2 x i32]], [2 x [2 x i32]]* @cdef_pri_taps, i64 0, i64 %31, i64 0
  %33 = icmp eq i32 %3, 0
  br i1 %33, label %40, label %34

34:                                               ; preds = %9
  %35 = tail call i32 @llvm.ctlz.i32(i32 %3, i1 true) #9, !range !2
  %36 = xor i32 %35, 31
  %37 = icmp sgt i32 %36, %6
  %38 = sub nsw i32 %6, %36
  %39 = select i1 %37, i32 0, i32 %38
  br label %40

40:                                               ; preds = %34, %9
  %41 = phi i32 [ %6, %9 ], [ %39, %34 ]
  %42 = icmp eq i32 %4, 0
  br i1 %42, label %49, label %43

43:                                               ; preds = %40
  %44 = tail call i32 @llvm.ctlz.i32(i32 %4, i1 true) #9, !range !2
  %45 = xor i32 %44, 31
  %46 = icmp sgt i32 %45, %7
  %47 = sub nsw i32 %7, %45
  %48 = select i1 %46, i32 0, i32 %47
  br label %49

49:                                               ; preds = %43, %40
  %50 = phi i32 [ %7, %40 ], [ %48, %43 ]
  %51 = trunc i32 %3 to i8
  %52 = insertelement <16 x i8> undef, i8 %51, i32 0
  %53 = shufflevector <16 x i8> %52, <16 x i8> undef, <16 x i32> zeroinitializer
  %54 = lshr i32 255, %41
  %55 = trunc i32 %54 to i8
  %56 = insertelement <16 x i8> undef, i8 %55, i32 0
  %57 = shufflevector <16 x i8> %56, <16 x i8> undef, <16 x i32> zeroinitializer
  %58 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %41, i32 0
  %59 = bitcast <4 x i32> %58 to <8 x i16>
  %60 = load i32, i32* %32, align 8
  %61 = trunc i32 %60 to i8
  %62 = insertelement <32 x i8> undef, i8 %61, i32 0
  %63 = shufflevector <32 x i8> %62, <32 x i8> undef, <32 x i32> zeroinitializer
  %64 = getelementptr inbounds [2 x [2 x i32]], [2 x [2 x i32]]* @cdef_pri_taps, i64 0, i64 %31, i64 1
  %65 = load i32, i32* %64, align 4
  %66 = trunc i32 %65 to i8
  %67 = insertelement <32 x i8> undef, i8 %66, i32 0
  %68 = shufflevector <32 x i8> %67, <32 x i8> undef, <32 x i32> zeroinitializer
  %69 = trunc i32 %4 to i8
  %70 = insertelement <16 x i8> undef, i8 %69, i32 0
  %71 = shufflevector <16 x i8> %70, <16 x i8> undef, <16 x i32> zeroinitializer
  %72 = lshr i32 255, %50
  %73 = trunc i32 %72 to i8
  %74 = insertelement <16 x i8> undef, i8 %73, i32 0
  %75 = shufflevector <16 x i8> %74, <16 x i8> undef, <16 x i32> zeroinitializer
  %76 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %50, i32 0
  %77 = bitcast <4 x i32> %76 to <8 x i16>
  %78 = load i32, i32* getelementptr inbounds ([2 x i32], [2 x i32]* @cdef_sec_taps, i64 0, i64 0), align 4
  %79 = trunc i32 %78 to i8
  %80 = insertelement <32 x i8> undef, i8 %79, i32 0
  %81 = shufflevector <32 x i8> %80, <32 x i8> undef, <32 x i32> zeroinitializer
  %82 = load i32, i32* getelementptr inbounds ([2 x i32], [2 x i32]* @cdef_sec_taps, i64 0, i64 1), align 4
  %83 = trunc i32 %82 to i8
  %84 = insertelement <32 x i8> undef, i8 %83, i32 0
  %85 = shufflevector <32 x i8> %84, <32 x i8> undef, <32 x i32> zeroinitializer
  %86 = sext i32 %1 to i64
  %87 = sext i32 %12 to i64
  %88 = sext i32 %14 to i64
  %89 = sext i32 %19 to i64
  %90 = sext i32 %26 to i64
  %91 = sext i32 %21 to i64
  %92 = sext i32 %28 to i64
  br label %93

93:                                               ; preds = %49, %93
  %94 = phi i64 [ 0, %49 ], [ %668, %93 ]
  %95 = mul nuw nsw i64 %94, 144
  %96 = getelementptr inbounds i16, i16* %2, i64 %95
  %97 = bitcast i16* %96 to <2 x i64>*
  %98 = load <2 x i64>, <2 x i64>* %97, align 16
  %99 = or i64 %94, 1
  %100 = mul nuw nsw i64 %99, 144
  %101 = getelementptr inbounds i16, i16* %2, i64 %100
  %102 = bitcast i16* %101 to <2 x i64>*
  %103 = load <2 x i64>, <2 x i64>* %102, align 16
  %104 = shufflevector <2 x i64> %103, <2 x i64> %98, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %105 = add nsw i64 %95, %87
  %106 = getelementptr inbounds i16, i16* %2, i64 %105
  %107 = bitcast i16* %106 to i8*
  %108 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %107) #9
  %109 = bitcast <16 x i8> %108 to <2 x i64>
  %110 = add nsw i64 %100, %87
  %111 = getelementptr inbounds i16, i16* %2, i64 %110
  %112 = bitcast i16* %111 to i8*
  %113 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %112) #9
  %114 = bitcast <16 x i8> %113 to <2 x i64>
  %115 = shufflevector <2 x i64> %114, <2 x i64> %109, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %116 = bitcast <4 x i64> %115 to <16 x i16>
  %117 = icmp eq <16 x i16> %116, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %118 = sext <16 x i1> %117 to <16 x i16>
  %119 = bitcast <16 x i16> %118 to <4 x i64>
  %120 = xor <4 x i64> %119, <i64 -1, i64 -1, i64 -1, i64 -1>
  %121 = and <4 x i64> %115, %120
  %122 = bitcast <4 x i64> %104 to <16 x i16>
  %123 = bitcast <4 x i64> %121 to <16 x i16>
  %124 = icmp sgt <16 x i16> %122, %123
  %125 = select <16 x i1> %124, <16 x i16> %122, <16 x i16> %123
  %126 = icmp sgt <16 x i16> %116, %122
  %127 = select <16 x i1> %126, <16 x i16> %122, <16 x i16> %116
  %128 = sub <16 x i16> %116, %122
  %129 = bitcast <16 x i16> %128 to <4 x i64>
  %130 = shufflevector <4 x i64> %129, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %131 = shufflevector <4 x i64> %129, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %132 = bitcast <2 x i64> %131 to <8 x i16>
  %133 = bitcast <2 x i64> %130 to <8 x i16>
  %134 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %132, <8 x i16> %133) #9
  %135 = ashr <16 x i8> %134, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %136 = sub <16 x i8> zeroinitializer, %134
  %137 = icmp slt <16 x i8> %134, zeroinitializer
  %138 = select <16 x i1> %137, <16 x i8> %136, <16 x i8> %134
  %139 = bitcast <16 x i8> %138 to <8 x i16>
  %140 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %139, <8 x i16> %59) #9
  %141 = bitcast <8 x i16> %140 to <16 x i8>
  %142 = and <16 x i8> %57, %141
  %143 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %53, <16 x i8> %142) #9
  %144 = icmp ult <16 x i8> %138, %143
  %145 = select <16 x i1> %144, <16 x i8> %138, <16 x i8> %143
  %146 = add <16 x i8> %145, %135
  %147 = xor <16 x i8> %146, %135
  %148 = sub nsw i64 %95, %87
  %149 = getelementptr inbounds i16, i16* %2, i64 %148
  %150 = bitcast i16* %149 to i8*
  %151 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %150) #9
  %152 = bitcast <16 x i8> %151 to <2 x i64>
  %153 = sub nsw i64 %100, %87
  %154 = getelementptr inbounds i16, i16* %2, i64 %153
  %155 = bitcast i16* %154 to i8*
  %156 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %155) #9
  %157 = bitcast <16 x i8> %156 to <2 x i64>
  %158 = shufflevector <2 x i64> %157, <2 x i64> %152, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %159 = bitcast <4 x i64> %158 to <16 x i16>
  %160 = icmp eq <16 x i16> %159, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %161 = sext <16 x i1> %160 to <16 x i16>
  %162 = bitcast <16 x i16> %161 to <4 x i64>
  %163 = xor <4 x i64> %162, <i64 -1, i64 -1, i64 -1, i64 -1>
  %164 = and <4 x i64> %158, %163
  %165 = bitcast <4 x i64> %164 to <16 x i16>
  %166 = icmp sgt <16 x i16> %125, %165
  %167 = select <16 x i1> %166, <16 x i16> %125, <16 x i16> %165
  %168 = icmp slt <16 x i16> %127, %159
  %169 = select <16 x i1> %168, <16 x i16> %127, <16 x i16> %159
  %170 = sub <16 x i16> %159, %122
  %171 = bitcast <16 x i16> %170 to <4 x i64>
  %172 = shufflevector <4 x i64> %171, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %173 = shufflevector <4 x i64> %171, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %174 = bitcast <2 x i64> %173 to <8 x i16>
  %175 = bitcast <2 x i64> %172 to <8 x i16>
  %176 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %174, <8 x i16> %175) #9
  %177 = ashr <16 x i8> %176, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %178 = sub <16 x i8> zeroinitializer, %176
  %179 = icmp slt <16 x i8> %176, zeroinitializer
  %180 = select <16 x i1> %179, <16 x i8> %178, <16 x i8> %176
  %181 = bitcast <16 x i8> %180 to <8 x i16>
  %182 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %181, <8 x i16> %59) #9
  %183 = bitcast <8 x i16> %182 to <16 x i8>
  %184 = and <16 x i8> %57, %183
  %185 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %53, <16 x i8> %184) #9
  %186 = icmp ult <16 x i8> %180, %185
  %187 = select <16 x i1> %186, <16 x i8> %180, <16 x i8> %185
  %188 = add <16 x i8> %187, %177
  %189 = xor <16 x i8> %188, %177
  %190 = shufflevector <16 x i8> %189, <16 x i8> %147, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %191 = bitcast <16 x i8> %190 to <2 x i64>
  %192 = shufflevector <16 x i8> %189, <16 x i8> %147, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %193 = bitcast <16 x i8> %192 to <2 x i64>
  %194 = shufflevector <2 x i64> %193, <2 x i64> %191, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %195 = bitcast <4 x i64> %194 to <32 x i8>
  %196 = tail call <16 x i16> @llvm.x86.avx2.pmadd.ub.sw(<32 x i8> %63, <32 x i8> %195) #9
  %197 = add nsw i64 %95, %88
  %198 = getelementptr inbounds i16, i16* %2, i64 %197
  %199 = bitcast i16* %198 to i8*
  %200 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %199) #9
  %201 = bitcast <16 x i8> %200 to <2 x i64>
  %202 = add nsw i64 %100, %88
  %203 = getelementptr inbounds i16, i16* %2, i64 %202
  %204 = bitcast i16* %203 to i8*
  %205 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %204) #9
  %206 = bitcast <16 x i8> %205 to <2 x i64>
  %207 = shufflevector <2 x i64> %206, <2 x i64> %201, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %208 = bitcast <4 x i64> %207 to <16 x i16>
  %209 = icmp eq <16 x i16> %208, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %210 = sext <16 x i1> %209 to <16 x i16>
  %211 = bitcast <16 x i16> %210 to <4 x i64>
  %212 = xor <4 x i64> %211, <i64 -1, i64 -1, i64 -1, i64 -1>
  %213 = and <4 x i64> %207, %212
  %214 = bitcast <4 x i64> %213 to <16 x i16>
  %215 = icmp sgt <16 x i16> %167, %214
  %216 = select <16 x i1> %215, <16 x i16> %167, <16 x i16> %214
  %217 = icmp slt <16 x i16> %169, %208
  %218 = select <16 x i1> %217, <16 x i16> %169, <16 x i16> %208
  %219 = sub <16 x i16> %208, %122
  %220 = bitcast <16 x i16> %219 to <4 x i64>
  %221 = shufflevector <4 x i64> %220, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %222 = shufflevector <4 x i64> %220, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %223 = bitcast <2 x i64> %222 to <8 x i16>
  %224 = bitcast <2 x i64> %221 to <8 x i16>
  %225 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %223, <8 x i16> %224) #9
  %226 = ashr <16 x i8> %225, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %227 = sub <16 x i8> zeroinitializer, %225
  %228 = icmp slt <16 x i8> %225, zeroinitializer
  %229 = select <16 x i1> %228, <16 x i8> %227, <16 x i8> %225
  %230 = bitcast <16 x i8> %229 to <8 x i16>
  %231 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %230, <8 x i16> %59) #9
  %232 = bitcast <8 x i16> %231 to <16 x i8>
  %233 = and <16 x i8> %57, %232
  %234 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %53, <16 x i8> %233) #9
  %235 = icmp ult <16 x i8> %229, %234
  %236 = select <16 x i1> %235, <16 x i8> %229, <16 x i8> %234
  %237 = add <16 x i8> %236, %226
  %238 = xor <16 x i8> %237, %226
  %239 = sub nsw i64 %95, %88
  %240 = getelementptr inbounds i16, i16* %2, i64 %239
  %241 = bitcast i16* %240 to i8*
  %242 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %241) #9
  %243 = bitcast <16 x i8> %242 to <2 x i64>
  %244 = sub nsw i64 %100, %88
  %245 = getelementptr inbounds i16, i16* %2, i64 %244
  %246 = bitcast i16* %245 to i8*
  %247 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %246) #9
  %248 = bitcast <16 x i8> %247 to <2 x i64>
  %249 = shufflevector <2 x i64> %248, <2 x i64> %243, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %250 = bitcast <4 x i64> %249 to <16 x i16>
  %251 = icmp eq <16 x i16> %250, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %252 = sext <16 x i1> %251 to <16 x i16>
  %253 = bitcast <16 x i16> %252 to <4 x i64>
  %254 = xor <4 x i64> %253, <i64 -1, i64 -1, i64 -1, i64 -1>
  %255 = and <4 x i64> %249, %254
  %256 = bitcast <4 x i64> %255 to <16 x i16>
  %257 = icmp sgt <16 x i16> %216, %256
  %258 = select <16 x i1> %257, <16 x i16> %216, <16 x i16> %256
  %259 = icmp slt <16 x i16> %218, %250
  %260 = select <16 x i1> %259, <16 x i16> %218, <16 x i16> %250
  %261 = sub <16 x i16> %250, %122
  %262 = bitcast <16 x i16> %261 to <4 x i64>
  %263 = shufflevector <4 x i64> %262, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %264 = shufflevector <4 x i64> %262, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %265 = bitcast <2 x i64> %264 to <8 x i16>
  %266 = bitcast <2 x i64> %263 to <8 x i16>
  %267 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %265, <8 x i16> %266) #9
  %268 = ashr <16 x i8> %267, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %269 = sub <16 x i8> zeroinitializer, %267
  %270 = icmp slt <16 x i8> %267, zeroinitializer
  %271 = select <16 x i1> %270, <16 x i8> %269, <16 x i8> %267
  %272 = bitcast <16 x i8> %271 to <8 x i16>
  %273 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %272, <8 x i16> %59) #9
  %274 = bitcast <8 x i16> %273 to <16 x i8>
  %275 = and <16 x i8> %57, %274
  %276 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %53, <16 x i8> %275) #9
  %277 = icmp ult <16 x i8> %271, %276
  %278 = select <16 x i1> %277, <16 x i8> %271, <16 x i8> %276
  %279 = add <16 x i8> %278, %268
  %280 = xor <16 x i8> %279, %268
  %281 = shufflevector <16 x i8> %280, <16 x i8> %238, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %282 = bitcast <16 x i8> %281 to <2 x i64>
  %283 = shufflevector <16 x i8> %280, <16 x i8> %238, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %284 = bitcast <16 x i8> %283 to <2 x i64>
  %285 = shufflevector <2 x i64> %284, <2 x i64> %282, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %286 = bitcast <4 x i64> %285 to <32 x i8>
  %287 = tail call <16 x i16> @llvm.x86.avx2.pmadd.ub.sw(<32 x i8> %68, <32 x i8> %286) #9
  %288 = add <16 x i16> %287, %196
  %289 = add nsw i64 %95, %89
  %290 = getelementptr inbounds i16, i16* %2, i64 %289
  %291 = bitcast i16* %290 to i8*
  %292 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %291) #9
  %293 = bitcast <16 x i8> %292 to <2 x i64>
  %294 = add nsw i64 %100, %89
  %295 = getelementptr inbounds i16, i16* %2, i64 %294
  %296 = bitcast i16* %295 to i8*
  %297 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %296) #9
  %298 = bitcast <16 x i8> %297 to <2 x i64>
  %299 = shufflevector <2 x i64> %298, <2 x i64> %293, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %300 = bitcast <4 x i64> %299 to <16 x i16>
  %301 = icmp eq <16 x i16> %300, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %302 = sext <16 x i1> %301 to <16 x i16>
  %303 = bitcast <16 x i16> %302 to <4 x i64>
  %304 = xor <4 x i64> %303, <i64 -1, i64 -1, i64 -1, i64 -1>
  %305 = and <4 x i64> %299, %304
  %306 = bitcast <4 x i64> %305 to <16 x i16>
  %307 = icmp sgt <16 x i16> %258, %306
  %308 = select <16 x i1> %307, <16 x i16> %258, <16 x i16> %306
  %309 = icmp slt <16 x i16> %260, %300
  %310 = select <16 x i1> %309, <16 x i16> %260, <16 x i16> %300
  %311 = sub <16 x i16> %300, %122
  %312 = bitcast <16 x i16> %311 to <4 x i64>
  %313 = shufflevector <4 x i64> %312, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %314 = shufflevector <4 x i64> %312, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %315 = bitcast <2 x i64> %314 to <8 x i16>
  %316 = bitcast <2 x i64> %313 to <8 x i16>
  %317 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %315, <8 x i16> %316) #9
  %318 = ashr <16 x i8> %317, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %319 = sub <16 x i8> zeroinitializer, %317
  %320 = icmp slt <16 x i8> %317, zeroinitializer
  %321 = select <16 x i1> %320, <16 x i8> %319, <16 x i8> %317
  %322 = bitcast <16 x i8> %321 to <8 x i16>
  %323 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %322, <8 x i16> %77) #9
  %324 = bitcast <8 x i16> %323 to <16 x i8>
  %325 = and <16 x i8> %75, %324
  %326 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %71, <16 x i8> %325) #9
  %327 = icmp ult <16 x i8> %321, %326
  %328 = select <16 x i1> %327, <16 x i8> %321, <16 x i8> %326
  %329 = add <16 x i8> %328, %318
  %330 = xor <16 x i8> %329, %318
  %331 = sub nsw i64 %95, %89
  %332 = getelementptr inbounds i16, i16* %2, i64 %331
  %333 = bitcast i16* %332 to i8*
  %334 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %333) #9
  %335 = bitcast <16 x i8> %334 to <2 x i64>
  %336 = sub nsw i64 %100, %89
  %337 = getelementptr inbounds i16, i16* %2, i64 %336
  %338 = bitcast i16* %337 to i8*
  %339 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %338) #9
  %340 = bitcast <16 x i8> %339 to <2 x i64>
  %341 = shufflevector <2 x i64> %340, <2 x i64> %335, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %342 = bitcast <4 x i64> %341 to <16 x i16>
  %343 = icmp eq <16 x i16> %342, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %344 = sext <16 x i1> %343 to <16 x i16>
  %345 = bitcast <16 x i16> %344 to <4 x i64>
  %346 = xor <4 x i64> %345, <i64 -1, i64 -1, i64 -1, i64 -1>
  %347 = and <4 x i64> %341, %346
  %348 = bitcast <4 x i64> %347 to <16 x i16>
  %349 = icmp sgt <16 x i16> %308, %348
  %350 = select <16 x i1> %349, <16 x i16> %308, <16 x i16> %348
  %351 = icmp slt <16 x i16> %310, %342
  %352 = select <16 x i1> %351, <16 x i16> %310, <16 x i16> %342
  %353 = sub <16 x i16> %342, %122
  %354 = bitcast <16 x i16> %353 to <4 x i64>
  %355 = shufflevector <4 x i64> %354, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %356 = shufflevector <4 x i64> %354, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %357 = bitcast <2 x i64> %356 to <8 x i16>
  %358 = bitcast <2 x i64> %355 to <8 x i16>
  %359 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %357, <8 x i16> %358) #9
  %360 = ashr <16 x i8> %359, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %361 = sub <16 x i8> zeroinitializer, %359
  %362 = icmp slt <16 x i8> %359, zeroinitializer
  %363 = select <16 x i1> %362, <16 x i8> %361, <16 x i8> %359
  %364 = bitcast <16 x i8> %363 to <8 x i16>
  %365 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %364, <8 x i16> %77) #9
  %366 = bitcast <8 x i16> %365 to <16 x i8>
  %367 = and <16 x i8> %75, %366
  %368 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %71, <16 x i8> %367) #9
  %369 = icmp ult <16 x i8> %363, %368
  %370 = select <16 x i1> %369, <16 x i8> %363, <16 x i8> %368
  %371 = add <16 x i8> %370, %360
  %372 = xor <16 x i8> %371, %360
  %373 = add nsw i64 %95, %90
  %374 = getelementptr inbounds i16, i16* %2, i64 %373
  %375 = bitcast i16* %374 to i8*
  %376 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %375) #9
  %377 = bitcast <16 x i8> %376 to <2 x i64>
  %378 = add nsw i64 %100, %90
  %379 = getelementptr inbounds i16, i16* %2, i64 %378
  %380 = bitcast i16* %379 to i8*
  %381 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %380) #9
  %382 = bitcast <16 x i8> %381 to <2 x i64>
  %383 = shufflevector <2 x i64> %382, <2 x i64> %377, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %384 = bitcast <4 x i64> %383 to <16 x i16>
  %385 = icmp eq <16 x i16> %384, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %386 = sext <16 x i1> %385 to <16 x i16>
  %387 = bitcast <16 x i16> %386 to <4 x i64>
  %388 = xor <4 x i64> %387, <i64 -1, i64 -1, i64 -1, i64 -1>
  %389 = and <4 x i64> %383, %388
  %390 = bitcast <4 x i64> %389 to <16 x i16>
  %391 = icmp sgt <16 x i16> %350, %390
  %392 = select <16 x i1> %391, <16 x i16> %350, <16 x i16> %390
  %393 = icmp slt <16 x i16> %352, %384
  %394 = select <16 x i1> %393, <16 x i16> %352, <16 x i16> %384
  %395 = sub <16 x i16> %384, %122
  %396 = bitcast <16 x i16> %395 to <4 x i64>
  %397 = shufflevector <4 x i64> %396, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %398 = shufflevector <4 x i64> %396, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %399 = bitcast <2 x i64> %398 to <8 x i16>
  %400 = bitcast <2 x i64> %397 to <8 x i16>
  %401 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %399, <8 x i16> %400) #9
  %402 = ashr <16 x i8> %401, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %403 = sub <16 x i8> zeroinitializer, %401
  %404 = icmp slt <16 x i8> %401, zeroinitializer
  %405 = select <16 x i1> %404, <16 x i8> %403, <16 x i8> %401
  %406 = bitcast <16 x i8> %405 to <8 x i16>
  %407 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %406, <8 x i16> %77) #9
  %408 = bitcast <8 x i16> %407 to <16 x i8>
  %409 = and <16 x i8> %75, %408
  %410 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %71, <16 x i8> %409) #9
  %411 = icmp ult <16 x i8> %405, %410
  %412 = select <16 x i1> %411, <16 x i8> %405, <16 x i8> %410
  %413 = add <16 x i8> %412, %402
  %414 = xor <16 x i8> %413, %402
  %415 = sub nsw i64 %95, %90
  %416 = getelementptr inbounds i16, i16* %2, i64 %415
  %417 = bitcast i16* %416 to i8*
  %418 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %417) #9
  %419 = bitcast <16 x i8> %418 to <2 x i64>
  %420 = sub nsw i64 %100, %90
  %421 = getelementptr inbounds i16, i16* %2, i64 %420
  %422 = bitcast i16* %421 to i8*
  %423 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %422) #9
  %424 = bitcast <16 x i8> %423 to <2 x i64>
  %425 = shufflevector <2 x i64> %424, <2 x i64> %419, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %426 = bitcast <4 x i64> %425 to <16 x i16>
  %427 = icmp eq <16 x i16> %426, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %428 = sext <16 x i1> %427 to <16 x i16>
  %429 = bitcast <16 x i16> %428 to <4 x i64>
  %430 = xor <4 x i64> %429, <i64 -1, i64 -1, i64 -1, i64 -1>
  %431 = and <4 x i64> %425, %430
  %432 = bitcast <4 x i64> %431 to <16 x i16>
  %433 = icmp sgt <16 x i16> %392, %432
  %434 = select <16 x i1> %433, <16 x i16> %392, <16 x i16> %432
  %435 = icmp slt <16 x i16> %394, %426
  %436 = select <16 x i1> %435, <16 x i16> %394, <16 x i16> %426
  %437 = sub <16 x i16> %426, %122
  %438 = bitcast <16 x i16> %437 to <4 x i64>
  %439 = shufflevector <4 x i64> %438, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %440 = shufflevector <4 x i64> %438, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %441 = bitcast <2 x i64> %440 to <8 x i16>
  %442 = bitcast <2 x i64> %439 to <8 x i16>
  %443 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %441, <8 x i16> %442) #9
  %444 = ashr <16 x i8> %443, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %445 = sub <16 x i8> zeroinitializer, %443
  %446 = icmp slt <16 x i8> %443, zeroinitializer
  %447 = select <16 x i1> %446, <16 x i8> %445, <16 x i8> %443
  %448 = bitcast <16 x i8> %447 to <8 x i16>
  %449 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %448, <8 x i16> %77) #9
  %450 = bitcast <8 x i16> %449 to <16 x i8>
  %451 = and <16 x i8> %75, %450
  %452 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %71, <16 x i8> %451) #9
  %453 = icmp ult <16 x i8> %447, %452
  %454 = select <16 x i1> %453, <16 x i8> %447, <16 x i8> %452
  %455 = add <16 x i8> %454, %444
  %456 = xor <16 x i8> %455, %444
  %457 = add <16 x i8> %372, %330
  %458 = add <16 x i8> %456, %414
  %459 = shufflevector <16 x i8> %458, <16 x i8> %457, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %460 = bitcast <16 x i8> %459 to <2 x i64>
  %461 = shufflevector <16 x i8> %458, <16 x i8> %457, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %462 = bitcast <16 x i8> %461 to <2 x i64>
  %463 = shufflevector <2 x i64> %462, <2 x i64> %460, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %464 = bitcast <4 x i64> %463 to <32 x i8>
  %465 = tail call <16 x i16> @llvm.x86.avx2.pmadd.ub.sw(<32 x i8> %81, <32 x i8> %464) #9
  %466 = add <16 x i16> %288, %465
  %467 = add nsw i64 %95, %91
  %468 = getelementptr inbounds i16, i16* %2, i64 %467
  %469 = bitcast i16* %468 to i8*
  %470 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %469) #9
  %471 = bitcast <16 x i8> %470 to <2 x i64>
  %472 = add nsw i64 %100, %91
  %473 = getelementptr inbounds i16, i16* %2, i64 %472
  %474 = bitcast i16* %473 to i8*
  %475 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %474) #9
  %476 = bitcast <16 x i8> %475 to <2 x i64>
  %477 = shufflevector <2 x i64> %476, <2 x i64> %471, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %478 = bitcast <4 x i64> %477 to <16 x i16>
  %479 = icmp eq <16 x i16> %478, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %480 = sext <16 x i1> %479 to <16 x i16>
  %481 = bitcast <16 x i16> %480 to <4 x i64>
  %482 = xor <4 x i64> %481, <i64 -1, i64 -1, i64 -1, i64 -1>
  %483 = and <4 x i64> %477, %482
  %484 = bitcast <4 x i64> %483 to <16 x i16>
  %485 = icmp sgt <16 x i16> %434, %484
  %486 = select <16 x i1> %485, <16 x i16> %434, <16 x i16> %484
  %487 = icmp slt <16 x i16> %436, %478
  %488 = select <16 x i1> %487, <16 x i16> %436, <16 x i16> %478
  %489 = sub <16 x i16> %478, %122
  %490 = bitcast <16 x i16> %489 to <4 x i64>
  %491 = shufflevector <4 x i64> %490, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %492 = shufflevector <4 x i64> %490, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %493 = bitcast <2 x i64> %492 to <8 x i16>
  %494 = bitcast <2 x i64> %491 to <8 x i16>
  %495 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %493, <8 x i16> %494) #9
  %496 = ashr <16 x i8> %495, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %497 = sub <16 x i8> zeroinitializer, %495
  %498 = icmp slt <16 x i8> %495, zeroinitializer
  %499 = select <16 x i1> %498, <16 x i8> %497, <16 x i8> %495
  %500 = bitcast <16 x i8> %499 to <8 x i16>
  %501 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %500, <8 x i16> %77) #9
  %502 = bitcast <8 x i16> %501 to <16 x i8>
  %503 = and <16 x i8> %75, %502
  %504 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %71, <16 x i8> %503) #9
  %505 = icmp ult <16 x i8> %499, %504
  %506 = select <16 x i1> %505, <16 x i8> %499, <16 x i8> %504
  %507 = add <16 x i8> %506, %496
  %508 = xor <16 x i8> %507, %496
  %509 = sub nsw i64 %95, %91
  %510 = getelementptr inbounds i16, i16* %2, i64 %509
  %511 = bitcast i16* %510 to i8*
  %512 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %511) #9
  %513 = bitcast <16 x i8> %512 to <2 x i64>
  %514 = sub nsw i64 %100, %91
  %515 = getelementptr inbounds i16, i16* %2, i64 %514
  %516 = bitcast i16* %515 to i8*
  %517 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %516) #9
  %518 = bitcast <16 x i8> %517 to <2 x i64>
  %519 = shufflevector <2 x i64> %518, <2 x i64> %513, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %520 = bitcast <4 x i64> %519 to <16 x i16>
  %521 = icmp eq <16 x i16> %520, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %522 = sext <16 x i1> %521 to <16 x i16>
  %523 = bitcast <16 x i16> %522 to <4 x i64>
  %524 = xor <4 x i64> %523, <i64 -1, i64 -1, i64 -1, i64 -1>
  %525 = and <4 x i64> %519, %524
  %526 = bitcast <4 x i64> %525 to <16 x i16>
  %527 = icmp sgt <16 x i16> %486, %526
  %528 = select <16 x i1> %527, <16 x i16> %486, <16 x i16> %526
  %529 = icmp slt <16 x i16> %488, %520
  %530 = select <16 x i1> %529, <16 x i16> %488, <16 x i16> %520
  %531 = sub <16 x i16> %520, %122
  %532 = bitcast <16 x i16> %531 to <4 x i64>
  %533 = shufflevector <4 x i64> %532, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %534 = shufflevector <4 x i64> %532, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %535 = bitcast <2 x i64> %534 to <8 x i16>
  %536 = bitcast <2 x i64> %533 to <8 x i16>
  %537 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %535, <8 x i16> %536) #9
  %538 = ashr <16 x i8> %537, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %539 = sub <16 x i8> zeroinitializer, %537
  %540 = icmp slt <16 x i8> %537, zeroinitializer
  %541 = select <16 x i1> %540, <16 x i8> %539, <16 x i8> %537
  %542 = bitcast <16 x i8> %541 to <8 x i16>
  %543 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %542, <8 x i16> %77) #9
  %544 = bitcast <8 x i16> %543 to <16 x i8>
  %545 = and <16 x i8> %75, %544
  %546 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %71, <16 x i8> %545) #9
  %547 = icmp ult <16 x i8> %541, %546
  %548 = select <16 x i1> %547, <16 x i8> %541, <16 x i8> %546
  %549 = add <16 x i8> %548, %538
  %550 = xor <16 x i8> %549, %538
  %551 = add nsw i64 %95, %92
  %552 = getelementptr inbounds i16, i16* %2, i64 %551
  %553 = bitcast i16* %552 to i8*
  %554 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %553) #9
  %555 = bitcast <16 x i8> %554 to <2 x i64>
  %556 = add nsw i64 %100, %92
  %557 = getelementptr inbounds i16, i16* %2, i64 %556
  %558 = bitcast i16* %557 to i8*
  %559 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %558) #9
  %560 = bitcast <16 x i8> %559 to <2 x i64>
  %561 = shufflevector <2 x i64> %560, <2 x i64> %555, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %562 = bitcast <4 x i64> %561 to <16 x i16>
  %563 = icmp eq <16 x i16> %562, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %564 = sext <16 x i1> %563 to <16 x i16>
  %565 = bitcast <16 x i16> %564 to <4 x i64>
  %566 = xor <4 x i64> %565, <i64 -1, i64 -1, i64 -1, i64 -1>
  %567 = and <4 x i64> %561, %566
  %568 = bitcast <4 x i64> %567 to <16 x i16>
  %569 = icmp sgt <16 x i16> %528, %568
  %570 = select <16 x i1> %569, <16 x i16> %528, <16 x i16> %568
  %571 = icmp slt <16 x i16> %530, %562
  %572 = select <16 x i1> %571, <16 x i16> %530, <16 x i16> %562
  %573 = sub <16 x i16> %562, %122
  %574 = bitcast <16 x i16> %573 to <4 x i64>
  %575 = shufflevector <4 x i64> %574, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %576 = shufflevector <4 x i64> %574, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %577 = bitcast <2 x i64> %576 to <8 x i16>
  %578 = bitcast <2 x i64> %575 to <8 x i16>
  %579 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %577, <8 x i16> %578) #9
  %580 = ashr <16 x i8> %579, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %581 = sub <16 x i8> zeroinitializer, %579
  %582 = icmp slt <16 x i8> %579, zeroinitializer
  %583 = select <16 x i1> %582, <16 x i8> %581, <16 x i8> %579
  %584 = bitcast <16 x i8> %583 to <8 x i16>
  %585 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %584, <8 x i16> %77) #9
  %586 = bitcast <8 x i16> %585 to <16 x i8>
  %587 = and <16 x i8> %75, %586
  %588 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %71, <16 x i8> %587) #9
  %589 = icmp ult <16 x i8> %583, %588
  %590 = select <16 x i1> %589, <16 x i8> %583, <16 x i8> %588
  %591 = add <16 x i8> %590, %580
  %592 = xor <16 x i8> %591, %580
  %593 = sub nsw i64 %95, %92
  %594 = getelementptr inbounds i16, i16* %2, i64 %593
  %595 = bitcast i16* %594 to i8*
  %596 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %595) #9
  %597 = bitcast <16 x i8> %596 to <2 x i64>
  %598 = sub nsw i64 %100, %92
  %599 = getelementptr inbounds i16, i16* %2, i64 %598
  %600 = bitcast i16* %599 to i8*
  %601 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %600) #9
  %602 = bitcast <16 x i8> %601 to <2 x i64>
  %603 = shufflevector <2 x i64> %602, <2 x i64> %597, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %604 = bitcast <4 x i64> %603 to <16 x i16>
  %605 = icmp eq <16 x i16> %604, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %606 = sext <16 x i1> %605 to <16 x i16>
  %607 = bitcast <16 x i16> %606 to <4 x i64>
  %608 = xor <4 x i64> %607, <i64 -1, i64 -1, i64 -1, i64 -1>
  %609 = and <4 x i64> %603, %608
  %610 = bitcast <4 x i64> %609 to <16 x i16>
  %611 = icmp sgt <16 x i16> %570, %610
  %612 = select <16 x i1> %611, <16 x i16> %570, <16 x i16> %610
  %613 = icmp slt <16 x i16> %572, %604
  %614 = select <16 x i1> %613, <16 x i16> %572, <16 x i16> %604
  %615 = sub <16 x i16> %604, %122
  %616 = bitcast <16 x i16> %615 to <4 x i64>
  %617 = shufflevector <4 x i64> %616, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %618 = shufflevector <4 x i64> %616, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %619 = bitcast <2 x i64> %618 to <8 x i16>
  %620 = bitcast <2 x i64> %617 to <8 x i16>
  %621 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %619, <8 x i16> %620) #9
  %622 = ashr <16 x i8> %621, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %623 = sub <16 x i8> zeroinitializer, %621
  %624 = icmp slt <16 x i8> %621, zeroinitializer
  %625 = select <16 x i1> %624, <16 x i8> %623, <16 x i8> %621
  %626 = bitcast <16 x i8> %625 to <8 x i16>
  %627 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %626, <8 x i16> %77) #9
  %628 = bitcast <8 x i16> %627 to <16 x i8>
  %629 = and <16 x i8> %75, %628
  %630 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %71, <16 x i8> %629) #9
  %631 = icmp ult <16 x i8> %625, %630
  %632 = select <16 x i1> %631, <16 x i8> %625, <16 x i8> %630
  %633 = add <16 x i8> %632, %622
  %634 = xor <16 x i8> %633, %622
  %635 = add <16 x i8> %550, %508
  %636 = add <16 x i8> %634, %592
  %637 = shufflevector <16 x i8> %636, <16 x i8> %635, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %638 = bitcast <16 x i8> %637 to <2 x i64>
  %639 = shufflevector <16 x i8> %636, <16 x i8> %635, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %640 = bitcast <16 x i8> %639 to <2 x i64>
  %641 = shufflevector <2 x i64> %640, <2 x i64> %638, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %642 = bitcast <4 x i64> %641 to <32 x i8>
  %643 = tail call <16 x i16> @llvm.x86.avx2.pmadd.ub.sw(<32 x i8> %85, <32 x i8> %642) #9
  %644 = add <16 x i16> %466, %643
  %645 = ashr <16 x i16> %644, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %646 = add <16 x i16> %644, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %647 = add <16 x i16> %646, %645
  %648 = ashr <16 x i16> %647, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %649 = add <16 x i16> %648, %122
  %650 = icmp sgt <16 x i16> %649, %614
  %651 = select <16 x i1> %650, <16 x i16> %649, <16 x i16> %614
  %652 = icmp slt <16 x i16> %651, %612
  %653 = select <16 x i1> %652, <16 x i16> %651, <16 x i16> %612
  %654 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %653, <16 x i16> undef) #9
  %655 = bitcast <32 x i8> %654 to <4 x i64>
  %656 = shufflevector <4 x i64> %655, <4 x i64> undef, <2 x i32> <i32 undef, i32 2>
  %657 = mul nsw i64 %94, %86
  %658 = getelementptr inbounds i8, i8* %0, i64 %657
  %659 = bitcast <2 x i64> %656 to <16 x i8>
  %660 = shufflevector <16 x i8> %659, <16 x i8> undef, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %661 = bitcast <16 x i8> %660 to <2 x i64>
  %662 = extractelement <2 x i64> %661, i32 0
  %663 = bitcast i8* %658 to i64*
  store i64 %662, i64* %663, align 1
  %664 = mul nsw i64 %99, %86
  %665 = getelementptr inbounds i8, i8* %0, i64 %664
  %666 = extractelement <4 x i64> %655, i32 0
  %667 = bitcast i8* %665 to i64*
  store i64 %666, i64* %667, align 1
  %668 = add nuw nsw i64 %94, 2
  %669 = icmp ult i64 %668, 8
  br i1 %669, label %93, label %670

670:                                              ; preds = %93
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define hidden void @cdef_filter_block_4x4_16_avx2(i16* nocapture, i32, i16* readonly, i32, i32, i32, i32, i32, i32) local_unnamed_addr #2 {
  %10 = sext i32 %5 to i64
  %11 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %10, i64 0
  %12 = load i32, i32* %11, align 8
  %13 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %10, i64 1
  %14 = load i32, i32* %13, align 4
  %15 = add nsw i32 %5, 2
  %16 = and i32 %15, 7
  %17 = zext i32 %16 to i64
  %18 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %17, i64 0
  %19 = load i32, i32* %18, align 8
  %20 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %17, i64 1
  %21 = load i32, i32* %20, align 4
  %22 = add nsw i32 %5, 6
  %23 = and i32 %22, 7
  %24 = zext i32 %23 to i64
  %25 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %24, i64 0
  %26 = load i32, i32* %25, align 8
  %27 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %24, i64 1
  %28 = load i32, i32* %27, align 4
  %29 = lshr i32 %3, %8
  %30 = and i32 %29, 1
  %31 = zext i32 %30 to i64
  %32 = getelementptr inbounds [2 x [2 x i32]], [2 x [2 x i32]]* @cdef_pri_taps, i64 0, i64 %31, i64 0
  %33 = icmp eq i32 %3, 0
  br i1 %33, label %40, label %34

34:                                               ; preds = %9
  %35 = tail call i32 @llvm.ctlz.i32(i32 %3, i1 true) #9, !range !2
  %36 = xor i32 %35, 31
  %37 = icmp sgt i32 %36, %6
  %38 = sub nsw i32 %6, %36
  %39 = select i1 %37, i32 0, i32 %38
  br label %40

40:                                               ; preds = %34, %9
  %41 = phi i32 [ %6, %9 ], [ %39, %34 ]
  %42 = icmp eq i32 %4, 0
  br i1 %42, label %49, label %43

43:                                               ; preds = %40
  %44 = tail call i32 @llvm.ctlz.i32(i32 %4, i1 true) #9, !range !2
  %45 = xor i32 %44, 31
  %46 = icmp sgt i32 %45, %7
  %47 = sub nsw i32 %7, %45
  %48 = select i1 %46, i32 0, i32 %47
  br label %49

49:                                               ; preds = %43, %40
  %50 = phi i32 [ %7, %40 ], [ %48, %43 ]
  %51 = trunc i32 %3 to i16
  %52 = insertelement <16 x i16> undef, i16 %51, i32 0
  %53 = shufflevector <16 x i16> %52, <16 x i16> undef, <16 x i32> zeroinitializer
  %54 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %41, i32 0
  %55 = bitcast <4 x i32> %54 to <8 x i16>
  %56 = load i32, i32* %32, align 8
  %57 = trunc i32 %56 to i16
  %58 = insertelement <16 x i16> undef, i16 %57, i32 0
  %59 = shufflevector <16 x i16> %58, <16 x i16> undef, <16 x i32> zeroinitializer
  %60 = getelementptr inbounds [2 x [2 x i32]], [2 x [2 x i32]]* @cdef_pri_taps, i64 0, i64 %31, i64 1
  %61 = load i32, i32* %60, align 4
  %62 = trunc i32 %61 to i16
  %63 = insertelement <16 x i16> undef, i16 %62, i32 0
  %64 = shufflevector <16 x i16> %63, <16 x i16> undef, <16 x i32> zeroinitializer
  %65 = trunc i32 %4 to i16
  %66 = insertelement <16 x i16> undef, i16 %65, i32 0
  %67 = shufflevector <16 x i16> %66, <16 x i16> undef, <16 x i32> zeroinitializer
  %68 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %50, i32 0
  %69 = bitcast <4 x i32> %68 to <8 x i16>
  %70 = load i32, i32* getelementptr inbounds ([2 x i32], [2 x i32]* @cdef_sec_taps, i64 0, i64 0), align 4
  %71 = trunc i32 %70 to i16
  %72 = insertelement <16 x i16> undef, i16 %71, i32 0
  %73 = shufflevector <16 x i16> %72, <16 x i16> undef, <16 x i32> zeroinitializer
  %74 = load i32, i32* getelementptr inbounds ([2 x i32], [2 x i32]* @cdef_sec_taps, i64 0, i64 1), align 4
  %75 = trunc i32 %74 to i16
  %76 = insertelement <16 x i16> undef, i16 %75, i32 0
  %77 = shufflevector <16 x i16> %76, <16 x i16> undef, <16 x i32> zeroinitializer
  %78 = bitcast i16* %2 to i64*
  %79 = load i64, i64* %78, align 1
  %80 = getelementptr inbounds i16, i16* %2, i64 144
  %81 = bitcast i16* %80 to i64*
  %82 = load i64, i64* %81, align 1
  %83 = insertelement <2 x i64> undef, i64 %82, i32 0
  %84 = getelementptr inbounds i16, i16* %2, i64 288
  %85 = bitcast i16* %84 to i64*
  %86 = load i64, i64* %85, align 1
  %87 = getelementptr inbounds i16, i16* %2, i64 432
  %88 = bitcast i16* %87 to i64*
  %89 = load i64, i64* %88, align 1
  %90 = insertelement <2 x i64> undef, i64 %89, i32 0
  %91 = insertelement <2 x i64> %83, i64 %79, i32 1
  %92 = insertelement <2 x i64> %90, i64 %86, i32 1
  %93 = shufflevector <2 x i64> %92, <2 x i64> %91, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %94 = sext i32 %12 to i64
  %95 = getelementptr inbounds i16, i16* %2, i64 %94
  %96 = bitcast i16* %95 to i64*
  %97 = load i64, i64* %96, align 1
  %98 = add nsw i32 %12, 144
  %99 = sext i32 %98 to i64
  %100 = getelementptr inbounds i16, i16* %2, i64 %99
  %101 = bitcast i16* %100 to i64*
  %102 = load i64, i64* %101, align 1
  %103 = insertelement <2 x i64> undef, i64 %102, i32 0
  %104 = add nsw i32 %12, 288
  %105 = sext i32 %104 to i64
  %106 = getelementptr inbounds i16, i16* %2, i64 %105
  %107 = bitcast i16* %106 to i64*
  %108 = load i64, i64* %107, align 1
  %109 = add nsw i32 %12, 432
  %110 = sext i32 %109 to i64
  %111 = getelementptr inbounds i16, i16* %2, i64 %110
  %112 = bitcast i16* %111 to i64*
  %113 = load i64, i64* %112, align 1
  %114 = insertelement <2 x i64> undef, i64 %113, i32 0
  %115 = insertelement <2 x i64> %103, i64 %97, i32 1
  %116 = insertelement <2 x i64> %114, i64 %108, i32 1
  %117 = shufflevector <2 x i64> %116, <2 x i64> %115, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %118 = sub nsw i32 0, %12
  %119 = sext i32 %118 to i64
  %120 = getelementptr inbounds i16, i16* %2, i64 %119
  %121 = bitcast i16* %120 to i64*
  %122 = load i64, i64* %121, align 1
  %123 = sub nsw i32 144, %12
  %124 = sext i32 %123 to i64
  %125 = getelementptr inbounds i16, i16* %2, i64 %124
  %126 = bitcast i16* %125 to i64*
  %127 = load i64, i64* %126, align 1
  %128 = insertelement <2 x i64> undef, i64 %127, i32 0
  %129 = sub nsw i32 288, %12
  %130 = sext i32 %129 to i64
  %131 = getelementptr inbounds i16, i16* %2, i64 %130
  %132 = bitcast i16* %131 to i64*
  %133 = load i64, i64* %132, align 1
  %134 = sub nsw i32 432, %12
  %135 = sext i32 %134 to i64
  %136 = getelementptr inbounds i16, i16* %2, i64 %135
  %137 = bitcast i16* %136 to i64*
  %138 = load i64, i64* %137, align 1
  %139 = insertelement <2 x i64> undef, i64 %138, i32 0
  %140 = insertelement <2 x i64> %128, i64 %122, i32 1
  %141 = insertelement <2 x i64> %139, i64 %133, i32 1
  %142 = shufflevector <2 x i64> %141, <2 x i64> %140, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %143 = bitcast <4 x i64> %117 to <16 x i16>
  %144 = icmp eq <16 x i16> %143, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %145 = sext <16 x i1> %144 to <16 x i16>
  %146 = bitcast <16 x i16> %145 to <4 x i64>
  %147 = xor <4 x i64> %146, <i64 -1, i64 -1, i64 -1, i64 -1>
  %148 = and <4 x i64> %117, %147
  %149 = bitcast <4 x i64> %93 to <16 x i16>
  %150 = bitcast <4 x i64> %148 to <16 x i16>
  %151 = icmp sgt <16 x i16> %149, %150
  %152 = select <16 x i1> %151, <16 x i16> %149, <16 x i16> %150
  %153 = bitcast <4 x i64> %142 to <16 x i16>
  %154 = icmp eq <16 x i16> %153, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %155 = sext <16 x i1> %154 to <16 x i16>
  %156 = bitcast <16 x i16> %155 to <4 x i64>
  %157 = xor <4 x i64> %156, <i64 -1, i64 -1, i64 -1, i64 -1>
  %158 = and <4 x i64> %142, %157
  %159 = bitcast <4 x i64> %158 to <16 x i16>
  %160 = icmp sgt <16 x i16> %152, %159
  %161 = select <16 x i1> %160, <16 x i16> %152, <16 x i16> %159
  %162 = icmp sgt <16 x i16> %143, %149
  %163 = select <16 x i1> %162, <16 x i16> %149, <16 x i16> %143
  %164 = icmp slt <16 x i16> %163, %153
  %165 = select <16 x i1> %164, <16 x i16> %163, <16 x i16> %153
  %166 = sub <16 x i16> %143, %149
  %167 = ashr <16 x i16> %166, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %168 = sub <16 x i16> zeroinitializer, %166
  %169 = icmp slt <16 x i16> %166, zeroinitializer
  %170 = select <16 x i1> %169, <16 x i16> %168, <16 x i16> %166
  %171 = tail call <16 x i16> @llvm.x86.avx2.psrl.w(<16 x i16> %170, <8 x i16> %55) #9
  %172 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %53, <16 x i16> %171) #9
  %173 = icmp slt <16 x i16> %170, %172
  %174 = select <16 x i1> %173, <16 x i16> %170, <16 x i16> %172
  %175 = add <16 x i16> %174, %167
  %176 = xor <16 x i16> %175, %167
  %177 = sub <16 x i16> %153, %149
  %178 = ashr <16 x i16> %177, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %179 = sub <16 x i16> zeroinitializer, %177
  %180 = icmp slt <16 x i16> %177, zeroinitializer
  %181 = select <16 x i1> %180, <16 x i16> %179, <16 x i16> %177
  %182 = tail call <16 x i16> @llvm.x86.avx2.psrl.w(<16 x i16> %181, <8 x i16> %55) #9
  %183 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %53, <16 x i16> %182) #9
  %184 = icmp slt <16 x i16> %181, %183
  %185 = select <16 x i1> %184, <16 x i16> %181, <16 x i16> %183
  %186 = add <16 x i16> %185, %178
  %187 = xor <16 x i16> %186, %178
  %188 = add <16 x i16> %187, %176
  %189 = mul <16 x i16> %188, %59
  %190 = sext i32 %14 to i64
  %191 = getelementptr inbounds i16, i16* %2, i64 %190
  %192 = bitcast i16* %191 to i64*
  %193 = load i64, i64* %192, align 1
  %194 = add nsw i32 %14, 144
  %195 = sext i32 %194 to i64
  %196 = getelementptr inbounds i16, i16* %2, i64 %195
  %197 = bitcast i16* %196 to i64*
  %198 = load i64, i64* %197, align 1
  %199 = insertelement <2 x i64> undef, i64 %198, i32 0
  %200 = add nsw i32 %14, 288
  %201 = sext i32 %200 to i64
  %202 = getelementptr inbounds i16, i16* %2, i64 %201
  %203 = bitcast i16* %202 to i64*
  %204 = load i64, i64* %203, align 1
  %205 = add nsw i32 %14, 432
  %206 = sext i32 %205 to i64
  %207 = getelementptr inbounds i16, i16* %2, i64 %206
  %208 = bitcast i16* %207 to i64*
  %209 = load i64, i64* %208, align 1
  %210 = insertelement <2 x i64> undef, i64 %209, i32 0
  %211 = insertelement <2 x i64> %199, i64 %193, i32 1
  %212 = insertelement <2 x i64> %210, i64 %204, i32 1
  %213 = shufflevector <2 x i64> %212, <2 x i64> %211, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %214 = sub nsw i32 0, %14
  %215 = sext i32 %214 to i64
  %216 = getelementptr inbounds i16, i16* %2, i64 %215
  %217 = bitcast i16* %216 to i64*
  %218 = load i64, i64* %217, align 1
  %219 = sub nsw i32 144, %14
  %220 = sext i32 %219 to i64
  %221 = getelementptr inbounds i16, i16* %2, i64 %220
  %222 = bitcast i16* %221 to i64*
  %223 = load i64, i64* %222, align 1
  %224 = insertelement <2 x i64> undef, i64 %223, i32 0
  %225 = sub nsw i32 288, %14
  %226 = sext i32 %225 to i64
  %227 = getelementptr inbounds i16, i16* %2, i64 %226
  %228 = bitcast i16* %227 to i64*
  %229 = load i64, i64* %228, align 1
  %230 = sub nsw i32 432, %14
  %231 = sext i32 %230 to i64
  %232 = getelementptr inbounds i16, i16* %2, i64 %231
  %233 = bitcast i16* %232 to i64*
  %234 = load i64, i64* %233, align 1
  %235 = insertelement <2 x i64> undef, i64 %234, i32 0
  %236 = insertelement <2 x i64> %224, i64 %218, i32 1
  %237 = insertelement <2 x i64> %235, i64 %229, i32 1
  %238 = shufflevector <2 x i64> %237, <2 x i64> %236, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %239 = bitcast <4 x i64> %213 to <16 x i16>
  %240 = icmp eq <16 x i16> %239, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %241 = sext <16 x i1> %240 to <16 x i16>
  %242 = bitcast <16 x i16> %241 to <4 x i64>
  %243 = xor <4 x i64> %242, <i64 -1, i64 -1, i64 -1, i64 -1>
  %244 = and <4 x i64> %213, %243
  %245 = bitcast <4 x i64> %244 to <16 x i16>
  %246 = icmp sgt <16 x i16> %161, %245
  %247 = select <16 x i1> %246, <16 x i16> %161, <16 x i16> %245
  %248 = bitcast <4 x i64> %238 to <16 x i16>
  %249 = icmp eq <16 x i16> %248, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %250 = sext <16 x i1> %249 to <16 x i16>
  %251 = bitcast <16 x i16> %250 to <4 x i64>
  %252 = xor <4 x i64> %251, <i64 -1, i64 -1, i64 -1, i64 -1>
  %253 = and <4 x i64> %238, %252
  %254 = bitcast <4 x i64> %253 to <16 x i16>
  %255 = icmp sgt <16 x i16> %247, %254
  %256 = select <16 x i1> %255, <16 x i16> %247, <16 x i16> %254
  %257 = icmp slt <16 x i16> %165, %239
  %258 = select <16 x i1> %257, <16 x i16> %165, <16 x i16> %239
  %259 = icmp slt <16 x i16> %258, %248
  %260 = select <16 x i1> %259, <16 x i16> %258, <16 x i16> %248
  %261 = sub <16 x i16> %239, %149
  %262 = ashr <16 x i16> %261, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %263 = sub <16 x i16> zeroinitializer, %261
  %264 = icmp slt <16 x i16> %261, zeroinitializer
  %265 = select <16 x i1> %264, <16 x i16> %263, <16 x i16> %261
  %266 = tail call <16 x i16> @llvm.x86.avx2.psrl.w(<16 x i16> %265, <8 x i16> %55) #9
  %267 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %53, <16 x i16> %266) #9
  %268 = icmp slt <16 x i16> %265, %267
  %269 = select <16 x i1> %268, <16 x i16> %265, <16 x i16> %267
  %270 = add <16 x i16> %269, %262
  %271 = xor <16 x i16> %270, %262
  %272 = sub <16 x i16> %248, %149
  %273 = ashr <16 x i16> %272, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %274 = sub <16 x i16> zeroinitializer, %272
  %275 = icmp slt <16 x i16> %272, zeroinitializer
  %276 = select <16 x i1> %275, <16 x i16> %274, <16 x i16> %272
  %277 = tail call <16 x i16> @llvm.x86.avx2.psrl.w(<16 x i16> %276, <8 x i16> %55) #9
  %278 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %53, <16 x i16> %277) #9
  %279 = icmp slt <16 x i16> %276, %278
  %280 = select <16 x i1> %279, <16 x i16> %276, <16 x i16> %278
  %281 = add <16 x i16> %280, %273
  %282 = xor <16 x i16> %281, %273
  %283 = add <16 x i16> %282, %271
  %284 = mul <16 x i16> %283, %64
  %285 = add <16 x i16> %284, %189
  %286 = sext i32 %19 to i64
  %287 = getelementptr inbounds i16, i16* %2, i64 %286
  %288 = bitcast i16* %287 to i64*
  %289 = load i64, i64* %288, align 1
  %290 = add nsw i32 %19, 144
  %291 = sext i32 %290 to i64
  %292 = getelementptr inbounds i16, i16* %2, i64 %291
  %293 = bitcast i16* %292 to i64*
  %294 = load i64, i64* %293, align 1
  %295 = insertelement <2 x i64> undef, i64 %294, i32 0
  %296 = add nsw i32 %19, 288
  %297 = sext i32 %296 to i64
  %298 = getelementptr inbounds i16, i16* %2, i64 %297
  %299 = bitcast i16* %298 to i64*
  %300 = load i64, i64* %299, align 1
  %301 = add nsw i32 %19, 432
  %302 = sext i32 %301 to i64
  %303 = getelementptr inbounds i16, i16* %2, i64 %302
  %304 = bitcast i16* %303 to i64*
  %305 = load i64, i64* %304, align 1
  %306 = insertelement <2 x i64> undef, i64 %305, i32 0
  %307 = insertelement <2 x i64> %295, i64 %289, i32 1
  %308 = insertelement <2 x i64> %306, i64 %300, i32 1
  %309 = shufflevector <2 x i64> %308, <2 x i64> %307, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %310 = sub nsw i32 0, %19
  %311 = sext i32 %310 to i64
  %312 = getelementptr inbounds i16, i16* %2, i64 %311
  %313 = bitcast i16* %312 to i64*
  %314 = load i64, i64* %313, align 1
  %315 = sub nsw i32 144, %19
  %316 = sext i32 %315 to i64
  %317 = getelementptr inbounds i16, i16* %2, i64 %316
  %318 = bitcast i16* %317 to i64*
  %319 = load i64, i64* %318, align 1
  %320 = insertelement <2 x i64> undef, i64 %319, i32 0
  %321 = sub nsw i32 288, %19
  %322 = sext i32 %321 to i64
  %323 = getelementptr inbounds i16, i16* %2, i64 %322
  %324 = bitcast i16* %323 to i64*
  %325 = load i64, i64* %324, align 1
  %326 = sub nsw i32 432, %19
  %327 = sext i32 %326 to i64
  %328 = getelementptr inbounds i16, i16* %2, i64 %327
  %329 = bitcast i16* %328 to i64*
  %330 = load i64, i64* %329, align 1
  %331 = insertelement <2 x i64> undef, i64 %330, i32 0
  %332 = insertelement <2 x i64> %320, i64 %314, i32 1
  %333 = insertelement <2 x i64> %331, i64 %325, i32 1
  %334 = shufflevector <2 x i64> %333, <2 x i64> %332, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %335 = sext i32 %26 to i64
  %336 = getelementptr inbounds i16, i16* %2, i64 %335
  %337 = bitcast i16* %336 to i64*
  %338 = load i64, i64* %337, align 1
  %339 = add nsw i32 %26, 144
  %340 = sext i32 %339 to i64
  %341 = getelementptr inbounds i16, i16* %2, i64 %340
  %342 = bitcast i16* %341 to i64*
  %343 = load i64, i64* %342, align 1
  %344 = insertelement <2 x i64> undef, i64 %343, i32 0
  %345 = add nsw i32 %26, 288
  %346 = sext i32 %345 to i64
  %347 = getelementptr inbounds i16, i16* %2, i64 %346
  %348 = bitcast i16* %347 to i64*
  %349 = load i64, i64* %348, align 1
  %350 = add nsw i32 %26, 432
  %351 = sext i32 %350 to i64
  %352 = getelementptr inbounds i16, i16* %2, i64 %351
  %353 = bitcast i16* %352 to i64*
  %354 = load i64, i64* %353, align 1
  %355 = insertelement <2 x i64> undef, i64 %354, i32 0
  %356 = insertelement <2 x i64> %344, i64 %338, i32 1
  %357 = insertelement <2 x i64> %355, i64 %349, i32 1
  %358 = shufflevector <2 x i64> %357, <2 x i64> %356, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %359 = sub nsw i32 0, %26
  %360 = sext i32 %359 to i64
  %361 = getelementptr inbounds i16, i16* %2, i64 %360
  %362 = bitcast i16* %361 to i64*
  %363 = load i64, i64* %362, align 1
  %364 = sub nsw i32 144, %26
  %365 = sext i32 %364 to i64
  %366 = getelementptr inbounds i16, i16* %2, i64 %365
  %367 = bitcast i16* %366 to i64*
  %368 = load i64, i64* %367, align 1
  %369 = insertelement <2 x i64> undef, i64 %368, i32 0
  %370 = sub nsw i32 288, %26
  %371 = sext i32 %370 to i64
  %372 = getelementptr inbounds i16, i16* %2, i64 %371
  %373 = bitcast i16* %372 to i64*
  %374 = load i64, i64* %373, align 1
  %375 = sub nsw i32 432, %26
  %376 = sext i32 %375 to i64
  %377 = getelementptr inbounds i16, i16* %2, i64 %376
  %378 = bitcast i16* %377 to i64*
  %379 = load i64, i64* %378, align 1
  %380 = insertelement <2 x i64> undef, i64 %379, i32 0
  %381 = insertelement <2 x i64> %369, i64 %363, i32 1
  %382 = insertelement <2 x i64> %380, i64 %374, i32 1
  %383 = shufflevector <2 x i64> %382, <2 x i64> %381, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %384 = bitcast <4 x i64> %309 to <16 x i16>
  %385 = icmp eq <16 x i16> %384, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %386 = sext <16 x i1> %385 to <16 x i16>
  %387 = bitcast <16 x i16> %386 to <4 x i64>
  %388 = xor <4 x i64> %387, <i64 -1, i64 -1, i64 -1, i64 -1>
  %389 = and <4 x i64> %309, %388
  %390 = bitcast <4 x i64> %389 to <16 x i16>
  %391 = icmp sgt <16 x i16> %256, %390
  %392 = select <16 x i1> %391, <16 x i16> %256, <16 x i16> %390
  %393 = bitcast <4 x i64> %334 to <16 x i16>
  %394 = icmp eq <16 x i16> %393, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %395 = sext <16 x i1> %394 to <16 x i16>
  %396 = bitcast <16 x i16> %395 to <4 x i64>
  %397 = xor <4 x i64> %396, <i64 -1, i64 -1, i64 -1, i64 -1>
  %398 = and <4 x i64> %334, %397
  %399 = bitcast <4 x i64> %398 to <16 x i16>
  %400 = icmp sgt <16 x i16> %392, %399
  %401 = select <16 x i1> %400, <16 x i16> %392, <16 x i16> %399
  %402 = bitcast <4 x i64> %358 to <16 x i16>
  %403 = icmp eq <16 x i16> %402, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %404 = sext <16 x i1> %403 to <16 x i16>
  %405 = bitcast <16 x i16> %404 to <4 x i64>
  %406 = xor <4 x i64> %405, <i64 -1, i64 -1, i64 -1, i64 -1>
  %407 = and <4 x i64> %358, %406
  %408 = bitcast <4 x i64> %407 to <16 x i16>
  %409 = icmp sgt <16 x i16> %401, %408
  %410 = select <16 x i1> %409, <16 x i16> %401, <16 x i16> %408
  %411 = bitcast <4 x i64> %383 to <16 x i16>
  %412 = icmp eq <16 x i16> %411, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %413 = sext <16 x i1> %412 to <16 x i16>
  %414 = bitcast <16 x i16> %413 to <4 x i64>
  %415 = xor <4 x i64> %414, <i64 -1, i64 -1, i64 -1, i64 -1>
  %416 = and <4 x i64> %383, %415
  %417 = bitcast <4 x i64> %416 to <16 x i16>
  %418 = icmp sgt <16 x i16> %410, %417
  %419 = select <16 x i1> %418, <16 x i16> %410, <16 x i16> %417
  %420 = icmp slt <16 x i16> %260, %384
  %421 = select <16 x i1> %420, <16 x i16> %260, <16 x i16> %384
  %422 = icmp slt <16 x i16> %421, %393
  %423 = select <16 x i1> %422, <16 x i16> %421, <16 x i16> %393
  %424 = icmp slt <16 x i16> %423, %402
  %425 = select <16 x i1> %424, <16 x i16> %423, <16 x i16> %402
  %426 = icmp slt <16 x i16> %425, %411
  %427 = select <16 x i1> %426, <16 x i16> %425, <16 x i16> %411
  %428 = sub <16 x i16> %384, %149
  %429 = ashr <16 x i16> %428, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %430 = sub <16 x i16> zeroinitializer, %428
  %431 = icmp slt <16 x i16> %428, zeroinitializer
  %432 = select <16 x i1> %431, <16 x i16> %430, <16 x i16> %428
  %433 = tail call <16 x i16> @llvm.x86.avx2.psrl.w(<16 x i16> %432, <8 x i16> %69) #9
  %434 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %67, <16 x i16> %433) #9
  %435 = icmp slt <16 x i16> %432, %434
  %436 = select <16 x i1> %435, <16 x i16> %432, <16 x i16> %434
  %437 = add <16 x i16> %436, %429
  %438 = xor <16 x i16> %437, %429
  %439 = sub <16 x i16> %393, %149
  %440 = ashr <16 x i16> %439, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %441 = sub <16 x i16> zeroinitializer, %439
  %442 = icmp slt <16 x i16> %439, zeroinitializer
  %443 = select <16 x i1> %442, <16 x i16> %441, <16 x i16> %439
  %444 = tail call <16 x i16> @llvm.x86.avx2.psrl.w(<16 x i16> %443, <8 x i16> %69) #9
  %445 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %67, <16 x i16> %444) #9
  %446 = icmp slt <16 x i16> %443, %445
  %447 = select <16 x i1> %446, <16 x i16> %443, <16 x i16> %445
  %448 = add <16 x i16> %447, %440
  %449 = xor <16 x i16> %448, %440
  %450 = sub <16 x i16> %402, %149
  %451 = ashr <16 x i16> %450, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %452 = sub <16 x i16> zeroinitializer, %450
  %453 = icmp slt <16 x i16> %450, zeroinitializer
  %454 = select <16 x i1> %453, <16 x i16> %452, <16 x i16> %450
  %455 = tail call <16 x i16> @llvm.x86.avx2.psrl.w(<16 x i16> %454, <8 x i16> %69) #9
  %456 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %67, <16 x i16> %455) #9
  %457 = icmp slt <16 x i16> %454, %456
  %458 = select <16 x i1> %457, <16 x i16> %454, <16 x i16> %456
  %459 = add <16 x i16> %458, %451
  %460 = xor <16 x i16> %459, %451
  %461 = sub <16 x i16> %411, %149
  %462 = ashr <16 x i16> %461, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %463 = sub <16 x i16> zeroinitializer, %461
  %464 = icmp slt <16 x i16> %461, zeroinitializer
  %465 = select <16 x i1> %464, <16 x i16> %463, <16 x i16> %461
  %466 = tail call <16 x i16> @llvm.x86.avx2.psrl.w(<16 x i16> %465, <8 x i16> %69) #9
  %467 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %67, <16 x i16> %466) #9
  %468 = icmp slt <16 x i16> %465, %467
  %469 = select <16 x i1> %468, <16 x i16> %465, <16 x i16> %467
  %470 = add <16 x i16> %469, %462
  %471 = xor <16 x i16> %470, %462
  %472 = add <16 x i16> %449, %438
  %473 = add <16 x i16> %472, %460
  %474 = add <16 x i16> %473, %471
  %475 = mul <16 x i16> %474, %73
  %476 = add <16 x i16> %285, %475
  %477 = sext i32 %21 to i64
  %478 = getelementptr inbounds i16, i16* %2, i64 %477
  %479 = bitcast i16* %478 to i64*
  %480 = load i64, i64* %479, align 1
  %481 = add nsw i32 %21, 144
  %482 = sext i32 %481 to i64
  %483 = getelementptr inbounds i16, i16* %2, i64 %482
  %484 = bitcast i16* %483 to i64*
  %485 = load i64, i64* %484, align 1
  %486 = insertelement <2 x i64> undef, i64 %485, i32 0
  %487 = add nsw i32 %21, 288
  %488 = sext i32 %487 to i64
  %489 = getelementptr inbounds i16, i16* %2, i64 %488
  %490 = bitcast i16* %489 to i64*
  %491 = load i64, i64* %490, align 1
  %492 = add nsw i32 %21, 432
  %493 = sext i32 %492 to i64
  %494 = getelementptr inbounds i16, i16* %2, i64 %493
  %495 = bitcast i16* %494 to i64*
  %496 = load i64, i64* %495, align 1
  %497 = insertelement <2 x i64> undef, i64 %496, i32 0
  %498 = insertelement <2 x i64> %486, i64 %480, i32 1
  %499 = insertelement <2 x i64> %497, i64 %491, i32 1
  %500 = shufflevector <2 x i64> %499, <2 x i64> %498, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %501 = sub nsw i32 0, %21
  %502 = sext i32 %501 to i64
  %503 = getelementptr inbounds i16, i16* %2, i64 %502
  %504 = bitcast i16* %503 to i64*
  %505 = load i64, i64* %504, align 1
  %506 = sub nsw i32 144, %21
  %507 = sext i32 %506 to i64
  %508 = getelementptr inbounds i16, i16* %2, i64 %507
  %509 = bitcast i16* %508 to i64*
  %510 = load i64, i64* %509, align 1
  %511 = insertelement <2 x i64> undef, i64 %510, i32 0
  %512 = sub nsw i32 288, %21
  %513 = sext i32 %512 to i64
  %514 = getelementptr inbounds i16, i16* %2, i64 %513
  %515 = bitcast i16* %514 to i64*
  %516 = load i64, i64* %515, align 1
  %517 = sub nsw i32 432, %21
  %518 = sext i32 %517 to i64
  %519 = getelementptr inbounds i16, i16* %2, i64 %518
  %520 = bitcast i16* %519 to i64*
  %521 = load i64, i64* %520, align 1
  %522 = insertelement <2 x i64> undef, i64 %521, i32 0
  %523 = insertelement <2 x i64> %511, i64 %505, i32 1
  %524 = insertelement <2 x i64> %522, i64 %516, i32 1
  %525 = shufflevector <2 x i64> %524, <2 x i64> %523, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %526 = sext i32 %28 to i64
  %527 = getelementptr inbounds i16, i16* %2, i64 %526
  %528 = bitcast i16* %527 to i64*
  %529 = load i64, i64* %528, align 1
  %530 = add nsw i32 %28, 144
  %531 = sext i32 %530 to i64
  %532 = getelementptr inbounds i16, i16* %2, i64 %531
  %533 = bitcast i16* %532 to i64*
  %534 = load i64, i64* %533, align 1
  %535 = insertelement <2 x i64> undef, i64 %534, i32 0
  %536 = add nsw i32 %28, 288
  %537 = sext i32 %536 to i64
  %538 = getelementptr inbounds i16, i16* %2, i64 %537
  %539 = bitcast i16* %538 to i64*
  %540 = load i64, i64* %539, align 1
  %541 = add nsw i32 %28, 432
  %542 = sext i32 %541 to i64
  %543 = getelementptr inbounds i16, i16* %2, i64 %542
  %544 = bitcast i16* %543 to i64*
  %545 = load i64, i64* %544, align 1
  %546 = insertelement <2 x i64> undef, i64 %545, i32 0
  %547 = insertelement <2 x i64> %535, i64 %529, i32 1
  %548 = insertelement <2 x i64> %546, i64 %540, i32 1
  %549 = shufflevector <2 x i64> %548, <2 x i64> %547, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %550 = sub nsw i32 0, %28
  %551 = sext i32 %550 to i64
  %552 = getelementptr inbounds i16, i16* %2, i64 %551
  %553 = bitcast i16* %552 to i64*
  %554 = load i64, i64* %553, align 1
  %555 = sub nsw i32 144, %28
  %556 = sext i32 %555 to i64
  %557 = getelementptr inbounds i16, i16* %2, i64 %556
  %558 = bitcast i16* %557 to i64*
  %559 = load i64, i64* %558, align 1
  %560 = insertelement <2 x i64> undef, i64 %559, i32 0
  %561 = sub nsw i32 288, %28
  %562 = sext i32 %561 to i64
  %563 = getelementptr inbounds i16, i16* %2, i64 %562
  %564 = bitcast i16* %563 to i64*
  %565 = load i64, i64* %564, align 1
  %566 = sub nsw i32 432, %28
  %567 = sext i32 %566 to i64
  %568 = getelementptr inbounds i16, i16* %2, i64 %567
  %569 = bitcast i16* %568 to i64*
  %570 = load i64, i64* %569, align 1
  %571 = insertelement <2 x i64> undef, i64 %570, i32 0
  %572 = insertelement <2 x i64> %560, i64 %554, i32 1
  %573 = insertelement <2 x i64> %571, i64 %565, i32 1
  %574 = shufflevector <2 x i64> %573, <2 x i64> %572, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %575 = bitcast <4 x i64> %500 to <16 x i16>
  %576 = icmp eq <16 x i16> %575, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %577 = sext <16 x i1> %576 to <16 x i16>
  %578 = bitcast <16 x i16> %577 to <4 x i64>
  %579 = xor <4 x i64> %578, <i64 -1, i64 -1, i64 -1, i64 -1>
  %580 = and <4 x i64> %500, %579
  %581 = bitcast <4 x i64> %580 to <16 x i16>
  %582 = icmp sgt <16 x i16> %419, %581
  %583 = select <16 x i1> %582, <16 x i16> %419, <16 x i16> %581
  %584 = bitcast <4 x i64> %525 to <16 x i16>
  %585 = icmp eq <16 x i16> %584, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %586 = sext <16 x i1> %585 to <16 x i16>
  %587 = bitcast <16 x i16> %586 to <4 x i64>
  %588 = xor <4 x i64> %587, <i64 -1, i64 -1, i64 -1, i64 -1>
  %589 = and <4 x i64> %525, %588
  %590 = bitcast <4 x i64> %589 to <16 x i16>
  %591 = icmp sgt <16 x i16> %583, %590
  %592 = select <16 x i1> %591, <16 x i16> %583, <16 x i16> %590
  %593 = bitcast <4 x i64> %549 to <16 x i16>
  %594 = icmp eq <16 x i16> %593, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %595 = sext <16 x i1> %594 to <16 x i16>
  %596 = bitcast <16 x i16> %595 to <4 x i64>
  %597 = xor <4 x i64> %596, <i64 -1, i64 -1, i64 -1, i64 -1>
  %598 = and <4 x i64> %549, %597
  %599 = bitcast <4 x i64> %598 to <16 x i16>
  %600 = icmp sgt <16 x i16> %592, %599
  %601 = select <16 x i1> %600, <16 x i16> %592, <16 x i16> %599
  %602 = bitcast <4 x i64> %574 to <16 x i16>
  %603 = icmp eq <16 x i16> %602, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %604 = sext <16 x i1> %603 to <16 x i16>
  %605 = bitcast <16 x i16> %604 to <4 x i64>
  %606 = xor <4 x i64> %605, <i64 -1, i64 -1, i64 -1, i64 -1>
  %607 = and <4 x i64> %574, %606
  %608 = bitcast <4 x i64> %607 to <16 x i16>
  %609 = icmp sgt <16 x i16> %601, %608
  %610 = select <16 x i1> %609, <16 x i16> %601, <16 x i16> %608
  %611 = icmp slt <16 x i16> %427, %575
  %612 = select <16 x i1> %611, <16 x i16> %427, <16 x i16> %575
  %613 = icmp slt <16 x i16> %612, %584
  %614 = select <16 x i1> %613, <16 x i16> %612, <16 x i16> %584
  %615 = icmp slt <16 x i16> %614, %593
  %616 = select <16 x i1> %615, <16 x i16> %614, <16 x i16> %593
  %617 = icmp slt <16 x i16> %616, %602
  %618 = select <16 x i1> %617, <16 x i16> %616, <16 x i16> %602
  %619 = sub <16 x i16> %575, %149
  %620 = ashr <16 x i16> %619, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %621 = sub <16 x i16> zeroinitializer, %619
  %622 = icmp slt <16 x i16> %619, zeroinitializer
  %623 = select <16 x i1> %622, <16 x i16> %621, <16 x i16> %619
  %624 = tail call <16 x i16> @llvm.x86.avx2.psrl.w(<16 x i16> %623, <8 x i16> %69) #9
  %625 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %67, <16 x i16> %624) #9
  %626 = icmp slt <16 x i16> %623, %625
  %627 = select <16 x i1> %626, <16 x i16> %623, <16 x i16> %625
  %628 = add <16 x i16> %627, %620
  %629 = xor <16 x i16> %628, %620
  %630 = sub <16 x i16> %584, %149
  %631 = ashr <16 x i16> %630, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %632 = sub <16 x i16> zeroinitializer, %630
  %633 = icmp slt <16 x i16> %630, zeroinitializer
  %634 = select <16 x i1> %633, <16 x i16> %632, <16 x i16> %630
  %635 = tail call <16 x i16> @llvm.x86.avx2.psrl.w(<16 x i16> %634, <8 x i16> %69) #9
  %636 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %67, <16 x i16> %635) #9
  %637 = icmp slt <16 x i16> %634, %636
  %638 = select <16 x i1> %637, <16 x i16> %634, <16 x i16> %636
  %639 = add <16 x i16> %638, %631
  %640 = xor <16 x i16> %639, %631
  %641 = sub <16 x i16> %593, %149
  %642 = ashr <16 x i16> %641, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %643 = sub <16 x i16> zeroinitializer, %641
  %644 = icmp slt <16 x i16> %641, zeroinitializer
  %645 = select <16 x i1> %644, <16 x i16> %643, <16 x i16> %641
  %646 = tail call <16 x i16> @llvm.x86.avx2.psrl.w(<16 x i16> %645, <8 x i16> %69) #9
  %647 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %67, <16 x i16> %646) #9
  %648 = icmp slt <16 x i16> %645, %647
  %649 = select <16 x i1> %648, <16 x i16> %645, <16 x i16> %647
  %650 = add <16 x i16> %649, %642
  %651 = xor <16 x i16> %650, %642
  %652 = sub <16 x i16> %602, %149
  %653 = ashr <16 x i16> %652, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %654 = sub <16 x i16> zeroinitializer, %652
  %655 = icmp slt <16 x i16> %652, zeroinitializer
  %656 = select <16 x i1> %655, <16 x i16> %654, <16 x i16> %652
  %657 = tail call <16 x i16> @llvm.x86.avx2.psrl.w(<16 x i16> %656, <8 x i16> %69) #9
  %658 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %67, <16 x i16> %657) #9
  %659 = icmp slt <16 x i16> %656, %658
  %660 = select <16 x i1> %659, <16 x i16> %656, <16 x i16> %658
  %661 = add <16 x i16> %660, %653
  %662 = xor <16 x i16> %661, %653
  %663 = add <16 x i16> %640, %629
  %664 = add <16 x i16> %663, %651
  %665 = add <16 x i16> %664, %662
  %666 = mul <16 x i16> %665, %77
  %667 = add <16 x i16> %476, %666
  %668 = ashr <16 x i16> %667, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %669 = add <16 x i16> %667, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %670 = add <16 x i16> %669, %668
  %671 = ashr <16 x i16> %670, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %672 = add <16 x i16> %671, %149
  %673 = icmp sgt <16 x i16> %672, %618
  %674 = select <16 x i1> %673, <16 x i16> %672, <16 x i16> %618
  %675 = icmp slt <16 x i16> %674, %610
  %676 = select <16 x i1> %675, <16 x i16> %674, <16 x i16> %610
  %677 = bitcast <16 x i16> %676 to <4 x i64>
  %678 = shufflevector <4 x i64> %677, <4 x i64> undef, <2 x i32> <i32 undef, i32 3>
  %679 = bitcast <2 x i64> %678 to <16 x i8>
  %680 = shufflevector <16 x i8> %679, <16 x i8> undef, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %681 = bitcast <16 x i8> %680 to <2 x i64>
  %682 = extractelement <2 x i64> %681, i32 0
  %683 = bitcast i16* %0 to i64*
  store i64 %682, i64* %683, align 1
  %684 = sext i32 %1 to i64
  %685 = getelementptr inbounds i16, i16* %0, i64 %684
  %686 = extractelement <4 x i64> %677, i32 2
  %687 = bitcast i16* %685 to i64*
  store i64 %686, i64* %687, align 1
  %688 = shl nsw i32 %1, 1
  %689 = sext i32 %688 to i64
  %690 = getelementptr inbounds i16, i16* %0, i64 %689
  %691 = shufflevector <4 x i64> %677, <4 x i64> undef, <2 x i32> <i32 undef, i32 1>
  %692 = bitcast <2 x i64> %691 to <16 x i8>
  %693 = shufflevector <16 x i8> %692, <16 x i8> undef, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %694 = bitcast <16 x i8> %693 to <2 x i64>
  %695 = extractelement <2 x i64> %694, i32 0
  %696 = bitcast i16* %690 to i64*
  store i64 %695, i64* %696, align 1
  %697 = mul nsw i32 %1, 3
  %698 = sext i32 %697 to i64
  %699 = getelementptr inbounds i16, i16* %0, i64 %698
  %700 = extractelement <4 x i64> %677, i32 0
  %701 = bitcast i16* %699 to i64*
  store i64 %700, i64* %701, align 1
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define hidden void @cdef_filter_block_8x8_16_avx2(i16* nocapture, i32, i16*, i32, i32, i32, i32, i32, i32) local_unnamed_addr #2 {
  %10 = sext i32 %5 to i64
  %11 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %10, i64 0
  %12 = load i32, i32* %11, align 8
  %13 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %10, i64 1
  %14 = load i32, i32* %13, align 4
  %15 = add nsw i32 %5, 2
  %16 = and i32 %15, 7
  %17 = zext i32 %16 to i64
  %18 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %17, i64 0
  %19 = load i32, i32* %18, align 8
  %20 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %17, i64 1
  %21 = load i32, i32* %20, align 4
  %22 = add nsw i32 %5, 6
  %23 = and i32 %22, 7
  %24 = zext i32 %23 to i64
  %25 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %24, i64 0
  %26 = load i32, i32* %25, align 8
  %27 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %24, i64 1
  %28 = load i32, i32* %27, align 4
  %29 = lshr i32 %3, %8
  %30 = and i32 %29, 1
  %31 = zext i32 %30 to i64
  %32 = getelementptr inbounds [2 x [2 x i32]], [2 x [2 x i32]]* @cdef_pri_taps, i64 0, i64 %31, i64 0
  %33 = icmp eq i32 %3, 0
  br i1 %33, label %40, label %34

34:                                               ; preds = %9
  %35 = tail call i32 @llvm.ctlz.i32(i32 %3, i1 true) #9, !range !2
  %36 = xor i32 %35, 31
  %37 = icmp sgt i32 %36, %6
  %38 = sub nsw i32 %6, %36
  %39 = select i1 %37, i32 0, i32 %38
  br label %40

40:                                               ; preds = %34, %9
  %41 = phi i32 [ %6, %9 ], [ %39, %34 ]
  %42 = icmp eq i32 %4, 0
  br i1 %42, label %49, label %43

43:                                               ; preds = %40
  %44 = tail call i32 @llvm.ctlz.i32(i32 %4, i1 true) #9, !range !2
  %45 = xor i32 %44, 31
  %46 = icmp sgt i32 %45, %7
  %47 = sub nsw i32 %7, %45
  %48 = select i1 %46, i32 0, i32 %47
  br label %49

49:                                               ; preds = %43, %40
  %50 = phi i32 [ %7, %40 ], [ %48, %43 ]
  %51 = trunc i32 %3 to i16
  %52 = insertelement <16 x i16> undef, i16 %51, i32 0
  %53 = shufflevector <16 x i16> %52, <16 x i16> undef, <16 x i32> zeroinitializer
  %54 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %41, i32 0
  %55 = bitcast <4 x i32> %54 to <8 x i16>
  %56 = load i32, i32* %32, align 8
  %57 = trunc i32 %56 to i16
  %58 = insertelement <16 x i16> undef, i16 %57, i32 0
  %59 = shufflevector <16 x i16> %58, <16 x i16> undef, <16 x i32> zeroinitializer
  %60 = getelementptr inbounds [2 x [2 x i32]], [2 x [2 x i32]]* @cdef_pri_taps, i64 0, i64 %31, i64 1
  %61 = load i32, i32* %60, align 4
  %62 = trunc i32 %61 to i16
  %63 = insertelement <16 x i16> undef, i16 %62, i32 0
  %64 = shufflevector <16 x i16> %63, <16 x i16> undef, <16 x i32> zeroinitializer
  %65 = trunc i32 %4 to i16
  %66 = insertelement <16 x i16> undef, i16 %65, i32 0
  %67 = shufflevector <16 x i16> %66, <16 x i16> undef, <16 x i32> zeroinitializer
  %68 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %50, i32 0
  %69 = bitcast <4 x i32> %68 to <8 x i16>
  %70 = load i32, i32* getelementptr inbounds ([2 x i32], [2 x i32]* @cdef_sec_taps, i64 0, i64 0), align 4
  %71 = trunc i32 %70 to i16
  %72 = insertelement <16 x i16> undef, i16 %71, i32 0
  %73 = shufflevector <16 x i16> %72, <16 x i16> undef, <16 x i32> zeroinitializer
  %74 = load i32, i32* getelementptr inbounds ([2 x i32], [2 x i32]* @cdef_sec_taps, i64 0, i64 1), align 4
  %75 = trunc i32 %74 to i16
  %76 = insertelement <16 x i16> undef, i16 %75, i32 0
  %77 = shufflevector <16 x i16> %76, <16 x i16> undef, <16 x i32> zeroinitializer
  %78 = sext i32 %1 to i64
  %79 = sext i32 %12 to i64
  %80 = sext i32 %14 to i64
  %81 = sext i32 %19 to i64
  %82 = sext i32 %26 to i64
  %83 = sext i32 %21 to i64
  %84 = sext i32 %28 to i64
  br label %85

85:                                               ; preds = %49, %85
  %86 = phi i64 [ 0, %49 ], [ %527, %85 ]
  %87 = mul nuw nsw i64 %86, 144
  %88 = getelementptr inbounds i16, i16* %2, i64 %87
  %89 = bitcast i16* %88 to <2 x i64>*
  %90 = load <2 x i64>, <2 x i64>* %89, align 16
  %91 = or i64 %86, 1
  %92 = mul nuw nsw i64 %91, 144
  %93 = getelementptr inbounds i16, i16* %2, i64 %92
  %94 = bitcast i16* %93 to <2 x i64>*
  %95 = load <2 x i64>, <2 x i64>* %94, align 16
  %96 = shufflevector <2 x i64> %95, <2 x i64> %90, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %97 = add nsw i64 %87, %79
  %98 = getelementptr inbounds i16, i16* %2, i64 %97
  %99 = bitcast i16* %98 to i8*
  %100 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %99) #9
  %101 = bitcast <16 x i8> %100 to <2 x i64>
  %102 = add nsw i64 %92, %79
  %103 = getelementptr inbounds i16, i16* %2, i64 %102
  %104 = bitcast i16* %103 to i8*
  %105 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %104) #9
  %106 = bitcast <16 x i8> %105 to <2 x i64>
  %107 = shufflevector <2 x i64> %106, <2 x i64> %101, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %108 = sub nsw i64 %87, %79
  %109 = getelementptr inbounds i16, i16* %2, i64 %108
  %110 = bitcast i16* %109 to i8*
  %111 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %110) #9
  %112 = bitcast <16 x i8> %111 to <2 x i64>
  %113 = sub nsw i64 %92, %79
  %114 = getelementptr inbounds i16, i16* %2, i64 %113
  %115 = bitcast i16* %114 to i8*
  %116 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %115) #9
  %117 = bitcast <16 x i8> %116 to <2 x i64>
  %118 = shufflevector <2 x i64> %117, <2 x i64> %112, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %119 = bitcast <4 x i64> %107 to <16 x i16>
  %120 = icmp eq <16 x i16> %119, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %121 = sext <16 x i1> %120 to <16 x i16>
  %122 = bitcast <16 x i16> %121 to <4 x i64>
  %123 = xor <4 x i64> %122, <i64 -1, i64 -1, i64 -1, i64 -1>
  %124 = and <4 x i64> %107, %123
  %125 = bitcast <4 x i64> %96 to <16 x i16>
  %126 = bitcast <4 x i64> %124 to <16 x i16>
  %127 = icmp sgt <16 x i16> %125, %126
  %128 = select <16 x i1> %127, <16 x i16> %125, <16 x i16> %126
  %129 = bitcast <4 x i64> %118 to <16 x i16>
  %130 = icmp eq <16 x i16> %129, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %131 = sext <16 x i1> %130 to <16 x i16>
  %132 = bitcast <16 x i16> %131 to <4 x i64>
  %133 = xor <4 x i64> %132, <i64 -1, i64 -1, i64 -1, i64 -1>
  %134 = and <4 x i64> %118, %133
  %135 = bitcast <4 x i64> %134 to <16 x i16>
  %136 = icmp sgt <16 x i16> %128, %135
  %137 = select <16 x i1> %136, <16 x i16> %128, <16 x i16> %135
  %138 = icmp sgt <16 x i16> %119, %125
  %139 = select <16 x i1> %138, <16 x i16> %125, <16 x i16> %119
  %140 = icmp slt <16 x i16> %139, %129
  %141 = select <16 x i1> %140, <16 x i16> %139, <16 x i16> %129
  %142 = sub <16 x i16> %119, %125
  %143 = ashr <16 x i16> %142, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %144 = sub <16 x i16> zeroinitializer, %142
  %145 = icmp slt <16 x i16> %142, zeroinitializer
  %146 = select <16 x i1> %145, <16 x i16> %144, <16 x i16> %142
  %147 = tail call <16 x i16> @llvm.x86.avx2.psrl.w(<16 x i16> %146, <8 x i16> %55) #9
  %148 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %53, <16 x i16> %147) #9
  %149 = icmp slt <16 x i16> %146, %148
  %150 = select <16 x i1> %149, <16 x i16> %146, <16 x i16> %148
  %151 = add <16 x i16> %150, %143
  %152 = xor <16 x i16> %151, %143
  %153 = sub <16 x i16> %129, %125
  %154 = ashr <16 x i16> %153, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %155 = sub <16 x i16> zeroinitializer, %153
  %156 = icmp slt <16 x i16> %153, zeroinitializer
  %157 = select <16 x i1> %156, <16 x i16> %155, <16 x i16> %153
  %158 = tail call <16 x i16> @llvm.x86.avx2.psrl.w(<16 x i16> %157, <8 x i16> %55) #9
  %159 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %53, <16 x i16> %158) #9
  %160 = icmp slt <16 x i16> %157, %159
  %161 = select <16 x i1> %160, <16 x i16> %157, <16 x i16> %159
  %162 = add <16 x i16> %161, %154
  %163 = xor <16 x i16> %162, %154
  %164 = add <16 x i16> %163, %152
  %165 = mul <16 x i16> %164, %59
  %166 = add nsw i64 %87, %80
  %167 = getelementptr inbounds i16, i16* %2, i64 %166
  %168 = bitcast i16* %167 to i8*
  %169 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %168) #9
  %170 = bitcast <16 x i8> %169 to <2 x i64>
  %171 = add nsw i64 %92, %80
  %172 = getelementptr inbounds i16, i16* %2, i64 %171
  %173 = bitcast i16* %172 to i8*
  %174 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %173) #9
  %175 = bitcast <16 x i8> %174 to <2 x i64>
  %176 = shufflevector <2 x i64> %175, <2 x i64> %170, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %177 = sub nsw i64 %87, %80
  %178 = getelementptr inbounds i16, i16* %2, i64 %177
  %179 = bitcast i16* %178 to i8*
  %180 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %179) #9
  %181 = bitcast <16 x i8> %180 to <2 x i64>
  %182 = sub nsw i64 %92, %80
  %183 = getelementptr inbounds i16, i16* %2, i64 %182
  %184 = bitcast i16* %183 to i8*
  %185 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %184) #9
  %186 = bitcast <16 x i8> %185 to <2 x i64>
  %187 = shufflevector <2 x i64> %186, <2 x i64> %181, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %188 = bitcast <4 x i64> %176 to <16 x i16>
  %189 = icmp eq <16 x i16> %188, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %190 = sext <16 x i1> %189 to <16 x i16>
  %191 = bitcast <16 x i16> %190 to <4 x i64>
  %192 = xor <4 x i64> %191, <i64 -1, i64 -1, i64 -1, i64 -1>
  %193 = and <4 x i64> %176, %192
  %194 = bitcast <4 x i64> %193 to <16 x i16>
  %195 = icmp sgt <16 x i16> %137, %194
  %196 = select <16 x i1> %195, <16 x i16> %137, <16 x i16> %194
  %197 = bitcast <4 x i64> %187 to <16 x i16>
  %198 = icmp eq <16 x i16> %197, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %199 = sext <16 x i1> %198 to <16 x i16>
  %200 = bitcast <16 x i16> %199 to <4 x i64>
  %201 = xor <4 x i64> %200, <i64 -1, i64 -1, i64 -1, i64 -1>
  %202 = and <4 x i64> %187, %201
  %203 = bitcast <4 x i64> %202 to <16 x i16>
  %204 = icmp sgt <16 x i16> %196, %203
  %205 = select <16 x i1> %204, <16 x i16> %196, <16 x i16> %203
  %206 = icmp slt <16 x i16> %141, %188
  %207 = select <16 x i1> %206, <16 x i16> %141, <16 x i16> %188
  %208 = icmp slt <16 x i16> %207, %197
  %209 = select <16 x i1> %208, <16 x i16> %207, <16 x i16> %197
  %210 = sub <16 x i16> %188, %125
  %211 = ashr <16 x i16> %210, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %212 = sub <16 x i16> zeroinitializer, %210
  %213 = icmp slt <16 x i16> %210, zeroinitializer
  %214 = select <16 x i1> %213, <16 x i16> %212, <16 x i16> %210
  %215 = tail call <16 x i16> @llvm.x86.avx2.psrl.w(<16 x i16> %214, <8 x i16> %55) #9
  %216 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %53, <16 x i16> %215) #9
  %217 = icmp slt <16 x i16> %214, %216
  %218 = select <16 x i1> %217, <16 x i16> %214, <16 x i16> %216
  %219 = add <16 x i16> %218, %211
  %220 = xor <16 x i16> %219, %211
  %221 = sub <16 x i16> %197, %125
  %222 = ashr <16 x i16> %221, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %223 = sub <16 x i16> zeroinitializer, %221
  %224 = icmp slt <16 x i16> %221, zeroinitializer
  %225 = select <16 x i1> %224, <16 x i16> %223, <16 x i16> %221
  %226 = tail call <16 x i16> @llvm.x86.avx2.psrl.w(<16 x i16> %225, <8 x i16> %55) #9
  %227 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %53, <16 x i16> %226) #9
  %228 = icmp slt <16 x i16> %225, %227
  %229 = select <16 x i1> %228, <16 x i16> %225, <16 x i16> %227
  %230 = add <16 x i16> %229, %222
  %231 = xor <16 x i16> %230, %222
  %232 = add <16 x i16> %231, %220
  %233 = mul <16 x i16> %232, %64
  %234 = add <16 x i16> %233, %165
  %235 = add nsw i64 %87, %81
  %236 = getelementptr inbounds i16, i16* %2, i64 %235
  %237 = bitcast i16* %236 to i8*
  %238 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %237) #9
  %239 = bitcast <16 x i8> %238 to <2 x i64>
  %240 = add nsw i64 %92, %81
  %241 = getelementptr inbounds i16, i16* %2, i64 %240
  %242 = bitcast i16* %241 to i8*
  %243 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %242) #9
  %244 = bitcast <16 x i8> %243 to <2 x i64>
  %245 = shufflevector <2 x i64> %244, <2 x i64> %239, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %246 = sub nsw i64 %87, %81
  %247 = getelementptr inbounds i16, i16* %2, i64 %246
  %248 = bitcast i16* %247 to i8*
  %249 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %248) #9
  %250 = bitcast <16 x i8> %249 to <2 x i64>
  %251 = sub nsw i64 %92, %81
  %252 = getelementptr inbounds i16, i16* %2, i64 %251
  %253 = bitcast i16* %252 to i8*
  %254 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %253) #9
  %255 = bitcast <16 x i8> %254 to <2 x i64>
  %256 = shufflevector <2 x i64> %255, <2 x i64> %250, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %257 = add nsw i64 %87, %82
  %258 = getelementptr inbounds i16, i16* %2, i64 %257
  %259 = bitcast i16* %258 to i8*
  %260 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %259) #9
  %261 = bitcast <16 x i8> %260 to <2 x i64>
  %262 = add nsw i64 %92, %82
  %263 = getelementptr inbounds i16, i16* %2, i64 %262
  %264 = bitcast i16* %263 to i8*
  %265 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %264) #9
  %266 = bitcast <16 x i8> %265 to <2 x i64>
  %267 = shufflevector <2 x i64> %266, <2 x i64> %261, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %268 = sub nsw i64 %87, %82
  %269 = getelementptr inbounds i16, i16* %2, i64 %268
  %270 = bitcast i16* %269 to i8*
  %271 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %270) #9
  %272 = bitcast <16 x i8> %271 to <2 x i64>
  %273 = sub nsw i64 %92, %82
  %274 = getelementptr inbounds i16, i16* %2, i64 %273
  %275 = bitcast i16* %274 to i8*
  %276 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %275) #9
  %277 = bitcast <16 x i8> %276 to <2 x i64>
  %278 = shufflevector <2 x i64> %277, <2 x i64> %272, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %279 = bitcast <4 x i64> %245 to <16 x i16>
  %280 = icmp eq <16 x i16> %279, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %281 = sext <16 x i1> %280 to <16 x i16>
  %282 = bitcast <16 x i16> %281 to <4 x i64>
  %283 = xor <4 x i64> %282, <i64 -1, i64 -1, i64 -1, i64 -1>
  %284 = and <4 x i64> %245, %283
  %285 = bitcast <4 x i64> %284 to <16 x i16>
  %286 = icmp sgt <16 x i16> %205, %285
  %287 = select <16 x i1> %286, <16 x i16> %205, <16 x i16> %285
  %288 = bitcast <4 x i64> %256 to <16 x i16>
  %289 = icmp eq <16 x i16> %288, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %290 = sext <16 x i1> %289 to <16 x i16>
  %291 = bitcast <16 x i16> %290 to <4 x i64>
  %292 = xor <4 x i64> %291, <i64 -1, i64 -1, i64 -1, i64 -1>
  %293 = and <4 x i64> %256, %292
  %294 = bitcast <4 x i64> %293 to <16 x i16>
  %295 = icmp sgt <16 x i16> %287, %294
  %296 = select <16 x i1> %295, <16 x i16> %287, <16 x i16> %294
  %297 = bitcast <4 x i64> %267 to <16 x i16>
  %298 = icmp eq <16 x i16> %297, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %299 = sext <16 x i1> %298 to <16 x i16>
  %300 = bitcast <16 x i16> %299 to <4 x i64>
  %301 = xor <4 x i64> %300, <i64 -1, i64 -1, i64 -1, i64 -1>
  %302 = and <4 x i64> %267, %301
  %303 = bitcast <4 x i64> %302 to <16 x i16>
  %304 = icmp sgt <16 x i16> %296, %303
  %305 = select <16 x i1> %304, <16 x i16> %296, <16 x i16> %303
  %306 = bitcast <4 x i64> %278 to <16 x i16>
  %307 = icmp eq <16 x i16> %306, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %308 = sext <16 x i1> %307 to <16 x i16>
  %309 = bitcast <16 x i16> %308 to <4 x i64>
  %310 = xor <4 x i64> %309, <i64 -1, i64 -1, i64 -1, i64 -1>
  %311 = and <4 x i64> %278, %310
  %312 = bitcast <4 x i64> %311 to <16 x i16>
  %313 = icmp sgt <16 x i16> %305, %312
  %314 = select <16 x i1> %313, <16 x i16> %305, <16 x i16> %312
  %315 = icmp slt <16 x i16> %209, %279
  %316 = select <16 x i1> %315, <16 x i16> %209, <16 x i16> %279
  %317 = icmp slt <16 x i16> %316, %288
  %318 = select <16 x i1> %317, <16 x i16> %316, <16 x i16> %288
  %319 = icmp slt <16 x i16> %318, %297
  %320 = select <16 x i1> %319, <16 x i16> %318, <16 x i16> %297
  %321 = icmp slt <16 x i16> %320, %306
  %322 = select <16 x i1> %321, <16 x i16> %320, <16 x i16> %306
  %323 = sub <16 x i16> %279, %125
  %324 = ashr <16 x i16> %323, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %325 = sub <16 x i16> zeroinitializer, %323
  %326 = icmp slt <16 x i16> %323, zeroinitializer
  %327 = select <16 x i1> %326, <16 x i16> %325, <16 x i16> %323
  %328 = tail call <16 x i16> @llvm.x86.avx2.psrl.w(<16 x i16> %327, <8 x i16> %69) #9
  %329 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %67, <16 x i16> %328) #9
  %330 = icmp slt <16 x i16> %327, %329
  %331 = select <16 x i1> %330, <16 x i16> %327, <16 x i16> %329
  %332 = add <16 x i16> %331, %324
  %333 = xor <16 x i16> %332, %324
  %334 = sub <16 x i16> %288, %125
  %335 = ashr <16 x i16> %334, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %336 = sub <16 x i16> zeroinitializer, %334
  %337 = icmp slt <16 x i16> %334, zeroinitializer
  %338 = select <16 x i1> %337, <16 x i16> %336, <16 x i16> %334
  %339 = tail call <16 x i16> @llvm.x86.avx2.psrl.w(<16 x i16> %338, <8 x i16> %69) #9
  %340 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %67, <16 x i16> %339) #9
  %341 = icmp slt <16 x i16> %338, %340
  %342 = select <16 x i1> %341, <16 x i16> %338, <16 x i16> %340
  %343 = add <16 x i16> %342, %335
  %344 = xor <16 x i16> %343, %335
  %345 = sub <16 x i16> %297, %125
  %346 = ashr <16 x i16> %345, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %347 = sub <16 x i16> zeroinitializer, %345
  %348 = icmp slt <16 x i16> %345, zeroinitializer
  %349 = select <16 x i1> %348, <16 x i16> %347, <16 x i16> %345
  %350 = tail call <16 x i16> @llvm.x86.avx2.psrl.w(<16 x i16> %349, <8 x i16> %69) #9
  %351 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %67, <16 x i16> %350) #9
  %352 = icmp slt <16 x i16> %349, %351
  %353 = select <16 x i1> %352, <16 x i16> %349, <16 x i16> %351
  %354 = add <16 x i16> %353, %346
  %355 = xor <16 x i16> %354, %346
  %356 = sub <16 x i16> %306, %125
  %357 = ashr <16 x i16> %356, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %358 = sub <16 x i16> zeroinitializer, %356
  %359 = icmp slt <16 x i16> %356, zeroinitializer
  %360 = select <16 x i1> %359, <16 x i16> %358, <16 x i16> %356
  %361 = tail call <16 x i16> @llvm.x86.avx2.psrl.w(<16 x i16> %360, <8 x i16> %69) #9
  %362 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %67, <16 x i16> %361) #9
  %363 = icmp slt <16 x i16> %360, %362
  %364 = select <16 x i1> %363, <16 x i16> %360, <16 x i16> %362
  %365 = add <16 x i16> %364, %357
  %366 = xor <16 x i16> %365, %357
  %367 = add <16 x i16> %344, %333
  %368 = add <16 x i16> %367, %355
  %369 = add <16 x i16> %368, %366
  %370 = mul <16 x i16> %369, %73
  %371 = add <16 x i16> %234, %370
  %372 = add nsw i64 %87, %83
  %373 = getelementptr inbounds i16, i16* %2, i64 %372
  %374 = bitcast i16* %373 to i8*
  %375 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %374) #9
  %376 = bitcast <16 x i8> %375 to <2 x i64>
  %377 = add nsw i64 %92, %83
  %378 = getelementptr inbounds i16, i16* %2, i64 %377
  %379 = bitcast i16* %378 to i8*
  %380 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %379) #9
  %381 = bitcast <16 x i8> %380 to <2 x i64>
  %382 = shufflevector <2 x i64> %381, <2 x i64> %376, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %383 = sub nsw i64 %87, %83
  %384 = getelementptr inbounds i16, i16* %2, i64 %383
  %385 = bitcast i16* %384 to i8*
  %386 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %385) #9
  %387 = bitcast <16 x i8> %386 to <2 x i64>
  %388 = sub nsw i64 %92, %83
  %389 = getelementptr inbounds i16, i16* %2, i64 %388
  %390 = bitcast i16* %389 to i8*
  %391 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %390) #9
  %392 = bitcast <16 x i8> %391 to <2 x i64>
  %393 = shufflevector <2 x i64> %392, <2 x i64> %387, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %394 = add nsw i64 %87, %84
  %395 = getelementptr inbounds i16, i16* %2, i64 %394
  %396 = bitcast i16* %395 to i8*
  %397 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %396) #9
  %398 = bitcast <16 x i8> %397 to <2 x i64>
  %399 = add nsw i64 %92, %84
  %400 = getelementptr inbounds i16, i16* %2, i64 %399
  %401 = bitcast i16* %400 to i8*
  %402 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %401) #9
  %403 = bitcast <16 x i8> %402 to <2 x i64>
  %404 = shufflevector <2 x i64> %403, <2 x i64> %398, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %405 = sub nsw i64 %87, %84
  %406 = getelementptr inbounds i16, i16* %2, i64 %405
  %407 = bitcast i16* %406 to i8*
  %408 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %407) #9
  %409 = bitcast <16 x i8> %408 to <2 x i64>
  %410 = sub nsw i64 %92, %84
  %411 = getelementptr inbounds i16, i16* %2, i64 %410
  %412 = bitcast i16* %411 to i8*
  %413 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %412) #9
  %414 = bitcast <16 x i8> %413 to <2 x i64>
  %415 = shufflevector <2 x i64> %414, <2 x i64> %409, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %416 = bitcast <4 x i64> %382 to <16 x i16>
  %417 = icmp eq <16 x i16> %416, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %418 = sext <16 x i1> %417 to <16 x i16>
  %419 = bitcast <16 x i16> %418 to <4 x i64>
  %420 = xor <4 x i64> %419, <i64 -1, i64 -1, i64 -1, i64 -1>
  %421 = and <4 x i64> %382, %420
  %422 = bitcast <4 x i64> %421 to <16 x i16>
  %423 = icmp sgt <16 x i16> %314, %422
  %424 = select <16 x i1> %423, <16 x i16> %314, <16 x i16> %422
  %425 = bitcast <4 x i64> %393 to <16 x i16>
  %426 = icmp eq <16 x i16> %425, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %427 = sext <16 x i1> %426 to <16 x i16>
  %428 = bitcast <16 x i16> %427 to <4 x i64>
  %429 = xor <4 x i64> %428, <i64 -1, i64 -1, i64 -1, i64 -1>
  %430 = and <4 x i64> %393, %429
  %431 = bitcast <4 x i64> %430 to <16 x i16>
  %432 = icmp sgt <16 x i16> %424, %431
  %433 = select <16 x i1> %432, <16 x i16> %424, <16 x i16> %431
  %434 = bitcast <4 x i64> %404 to <16 x i16>
  %435 = icmp eq <16 x i16> %434, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %436 = sext <16 x i1> %435 to <16 x i16>
  %437 = bitcast <16 x i16> %436 to <4 x i64>
  %438 = xor <4 x i64> %437, <i64 -1, i64 -1, i64 -1, i64 -1>
  %439 = and <4 x i64> %404, %438
  %440 = bitcast <4 x i64> %439 to <16 x i16>
  %441 = icmp sgt <16 x i16> %433, %440
  %442 = select <16 x i1> %441, <16 x i16> %433, <16 x i16> %440
  %443 = bitcast <4 x i64> %415 to <16 x i16>
  %444 = icmp eq <16 x i16> %443, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %445 = sext <16 x i1> %444 to <16 x i16>
  %446 = bitcast <16 x i16> %445 to <4 x i64>
  %447 = xor <4 x i64> %446, <i64 -1, i64 -1, i64 -1, i64 -1>
  %448 = and <4 x i64> %415, %447
  %449 = bitcast <4 x i64> %448 to <16 x i16>
  %450 = icmp sgt <16 x i16> %442, %449
  %451 = select <16 x i1> %450, <16 x i16> %442, <16 x i16> %449
  %452 = icmp slt <16 x i16> %322, %416
  %453 = select <16 x i1> %452, <16 x i16> %322, <16 x i16> %416
  %454 = icmp slt <16 x i16> %453, %425
  %455 = select <16 x i1> %454, <16 x i16> %453, <16 x i16> %425
  %456 = icmp slt <16 x i16> %455, %434
  %457 = select <16 x i1> %456, <16 x i16> %455, <16 x i16> %434
  %458 = icmp slt <16 x i16> %457, %443
  %459 = select <16 x i1> %458, <16 x i16> %457, <16 x i16> %443
  %460 = sub <16 x i16> %416, %125
  %461 = ashr <16 x i16> %460, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %462 = sub <16 x i16> zeroinitializer, %460
  %463 = icmp slt <16 x i16> %460, zeroinitializer
  %464 = select <16 x i1> %463, <16 x i16> %462, <16 x i16> %460
  %465 = tail call <16 x i16> @llvm.x86.avx2.psrl.w(<16 x i16> %464, <8 x i16> %69) #9
  %466 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %67, <16 x i16> %465) #9
  %467 = icmp slt <16 x i16> %464, %466
  %468 = select <16 x i1> %467, <16 x i16> %464, <16 x i16> %466
  %469 = add <16 x i16> %468, %461
  %470 = xor <16 x i16> %469, %461
  %471 = sub <16 x i16> %425, %125
  %472 = ashr <16 x i16> %471, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %473 = sub <16 x i16> zeroinitializer, %471
  %474 = icmp slt <16 x i16> %471, zeroinitializer
  %475 = select <16 x i1> %474, <16 x i16> %473, <16 x i16> %471
  %476 = tail call <16 x i16> @llvm.x86.avx2.psrl.w(<16 x i16> %475, <8 x i16> %69) #9
  %477 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %67, <16 x i16> %476) #9
  %478 = icmp slt <16 x i16> %475, %477
  %479 = select <16 x i1> %478, <16 x i16> %475, <16 x i16> %477
  %480 = add <16 x i16> %479, %472
  %481 = xor <16 x i16> %480, %472
  %482 = sub <16 x i16> %434, %125
  %483 = ashr <16 x i16> %482, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %484 = sub <16 x i16> zeroinitializer, %482
  %485 = icmp slt <16 x i16> %482, zeroinitializer
  %486 = select <16 x i1> %485, <16 x i16> %484, <16 x i16> %482
  %487 = tail call <16 x i16> @llvm.x86.avx2.psrl.w(<16 x i16> %486, <8 x i16> %69) #9
  %488 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %67, <16 x i16> %487) #9
  %489 = icmp slt <16 x i16> %486, %488
  %490 = select <16 x i1> %489, <16 x i16> %486, <16 x i16> %488
  %491 = add <16 x i16> %490, %483
  %492 = xor <16 x i16> %491, %483
  %493 = sub <16 x i16> %443, %125
  %494 = ashr <16 x i16> %493, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %495 = sub <16 x i16> zeroinitializer, %493
  %496 = icmp slt <16 x i16> %493, zeroinitializer
  %497 = select <16 x i1> %496, <16 x i16> %495, <16 x i16> %493
  %498 = tail call <16 x i16> @llvm.x86.avx2.psrl.w(<16 x i16> %497, <8 x i16> %69) #9
  %499 = tail call <16 x i16> @llvm.usub.sat.v16i16(<16 x i16> %67, <16 x i16> %498) #9
  %500 = icmp slt <16 x i16> %497, %499
  %501 = select <16 x i1> %500, <16 x i16> %497, <16 x i16> %499
  %502 = add <16 x i16> %501, %494
  %503 = xor <16 x i16> %502, %494
  %504 = add <16 x i16> %481, %470
  %505 = add <16 x i16> %504, %492
  %506 = add <16 x i16> %505, %503
  %507 = mul <16 x i16> %506, %77
  %508 = add <16 x i16> %371, %507
  %509 = ashr <16 x i16> %508, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %510 = add <16 x i16> %508, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %511 = add <16 x i16> %510, %509
  %512 = ashr <16 x i16> %511, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %513 = add <16 x i16> %512, %125
  %514 = icmp sgt <16 x i16> %513, %459
  %515 = select <16 x i1> %514, <16 x i16> %513, <16 x i16> %459
  %516 = icmp slt <16 x i16> %515, %451
  %517 = select <16 x i1> %516, <16 x i16> %515, <16 x i16> %451
  %518 = bitcast <16 x i16> %517 to <4 x i64>
  %519 = mul nsw i64 %86, %78
  %520 = getelementptr inbounds i16, i16* %0, i64 %519
  %521 = shufflevector <4 x i64> %518, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %522 = bitcast i16* %520 to <2 x i64>*
  store <2 x i64> %521, <2 x i64>* %522, align 1
  %523 = mul nsw i64 %91, %78
  %524 = getelementptr inbounds i16, i16* %0, i64 %523
  %525 = shufflevector <4 x i64> %518, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %526 = bitcast i16* %524 to <2 x i64>*
  store <2 x i64> %525, <2 x i64>* %526, align 1
  %527 = add nuw nsw i64 %86, 2
  %528 = icmp ult i64 %527, 8
  br i1 %528, label %85, label %529

529:                                              ; preds = %85
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define hidden void @cdef_filter_block_avx2(i8*, i16* nocapture, i32, i16*, i32, i32, i32, i32, i32, i32, i32) local_unnamed_addr #3 {
  %12 = icmp eq i8* %0, null
  %13 = icmp eq i32 %9, 3
  br i1 %12, label %28, label %14

14:                                               ; preds = %11
  br i1 %13, label %15, label %16

15:                                               ; preds = %14
  tail call void @cdef_filter_block_8x8_8_avx2(i8* nonnull %0, i32 %2, i16* %3, i32 %4, i32 %5, i32 %6, i32 %7, i32 %8, i32 %10)
  br label %42

16:                                               ; preds = %14
  %17 = icmp eq i32 %9, 1
  br i1 %17, label %18, label %23

18:                                               ; preds = %16
  tail call void @cdef_filter_block_4x4_8_avx2(i8* nonnull %0, i32 %2, i16* %3, i32 %4, i32 %5, i32 %6, i32 %7, i32 %8, i32 %10)
  %19 = shl nsw i32 %2, 2
  %20 = sext i32 %19 to i64
  %21 = getelementptr inbounds i8, i8* %0, i64 %20
  %22 = getelementptr inbounds i16, i16* %3, i64 576
  tail call void @cdef_filter_block_4x4_8_avx2(i8* %21, i32 %2, i16* %22, i32 %4, i32 %5, i32 %6, i32 %7, i32 %8, i32 %10)
  br label %42

23:                                               ; preds = %16
  %24 = icmp eq i32 %9, 2
  tail call void @cdef_filter_block_4x4_8_avx2(i8* nonnull %0, i32 %2, i16* %3, i32 %4, i32 %5, i32 %6, i32 %7, i32 %8, i32 %10)
  br i1 %24, label %25, label %42

25:                                               ; preds = %23
  %26 = getelementptr inbounds i8, i8* %0, i64 4
  %27 = getelementptr inbounds i16, i16* %3, i64 4
  tail call void @cdef_filter_block_4x4_8_avx2(i8* %26, i32 %2, i16* %27, i32 %4, i32 %5, i32 %6, i32 %7, i32 %8, i32 %10)
  br label %42

28:                                               ; preds = %11
  br i1 %13, label %29, label %30

29:                                               ; preds = %28
  tail call void @cdef_filter_block_8x8_16_avx2(i16* %1, i32 %2, i16* %3, i32 %4, i32 %5, i32 %6, i32 %7, i32 %8, i32 %10)
  br label %42

30:                                               ; preds = %28
  %31 = icmp eq i32 %9, 1
  br i1 %31, label %32, label %37

32:                                               ; preds = %30
  tail call void @cdef_filter_block_4x4_16_avx2(i16* %1, i32 %2, i16* %3, i32 %4, i32 %5, i32 %6, i32 %7, i32 %8, i32 %10)
  %33 = shl nsw i32 %2, 2
  %34 = sext i32 %33 to i64
  %35 = getelementptr inbounds i16, i16* %1, i64 %34
  %36 = getelementptr inbounds i16, i16* %3, i64 576
  tail call void @cdef_filter_block_4x4_16_avx2(i16* %35, i32 %2, i16* %36, i32 %4, i32 %5, i32 %6, i32 %7, i32 %8, i32 %10)
  br label %42

37:                                               ; preds = %30
  %38 = icmp eq i32 %9, 2
  tail call void @cdef_filter_block_4x4_16_avx2(i16* %1, i32 %2, i16* %3, i32 %4, i32 %5, i32 %6, i32 %7, i32 %8, i32 %10)
  br i1 %38, label %39, label %42

39:                                               ; preds = %37
  %40 = getelementptr inbounds i16, i16* %1, i64 4
  %41 = getelementptr inbounds i16, i16* %3, i64 4
  tail call void @cdef_filter_block_4x4_16_avx2(i16* %40, i32 %2, i16* %41, i32 %4, i32 %5, i32 %6, i32 %7, i32 %8, i32 %10)
  br label %42

42:                                               ; preds = %29, %39, %37, %32, %15, %25, %23, %18
  ret void
}

; Function Attrs: nofree norecurse nounwind ssp uwtable
define hidden void @cdef_copy_rect8_8bit_to_16bit_avx2(i16* nocapture, i32, i8* nocapture readonly, i32, i32, i32) local_unnamed_addr #4 {
  %7 = icmp sgt i32 %4, 0
  br i1 %7, label %8, label %258

8:                                                ; preds = %6
  %9 = and i32 %5, -8
  %10 = icmp sgt i32 %9, 0
  %11 = sext i32 %9 to i64
  %12 = sext i32 %1 to i64
  %13 = sext i32 %3 to i64
  %14 = zext i32 %4 to i64
  %15 = zext i32 %5 to i64
  %16 = add nsw i64 %11, -1
  %17 = lshr i64 %16, 3
  %18 = add nuw nsw i64 %17, 1
  %19 = and i64 %18, 1
  %20 = icmp eq i64 %17, 0
  %21 = sub nuw nsw i64 %18, %19
  %22 = icmp eq i64 %19, 0
  br label %23

23:                                               ; preds = %255, %8
  %24 = phi i64 [ 0, %8 ], [ %256, %255 ]
  %25 = mul i64 %24, %12
  %26 = add i64 %25, %15
  %27 = getelementptr i16, i16* %0, i64 %26
  %28 = bitcast i16* %27 to i8*
  %29 = mul i64 %24, %13
  %30 = getelementptr i8, i8* %2, i64 %29
  %31 = add i64 %29, %15
  %32 = getelementptr i8, i8* %2, i64 %31
  br i1 %10, label %33, label %54

33:                                               ; preds = %23
  %34 = mul nsw i64 %24, %13
  %35 = mul nsw i64 %24, %12
  br i1 %20, label %36, label %197

36:                                               ; preds = %197, %33
  %37 = phi i64 [ undef, %33 ], [ %221, %197 ]
  %38 = phi i64 [ 0, %33 ], [ %221, %197 ]
  br i1 %22, label %51, label %39

39:                                               ; preds = %36
  %40 = add nsw i64 %38, %34
  %41 = getelementptr inbounds i8, i8* %2, i64 %40
  %42 = bitcast i8* %41 to i64*
  %43 = load i64, i64* %42, align 1
  %44 = insertelement <2 x i64> undef, i64 %43, i32 0
  %45 = add nsw i64 %38, %35
  %46 = getelementptr inbounds i16, i16* %0, i64 %45
  %47 = bitcast <2 x i64> %44 to <16 x i8>
  %48 = shufflevector <16 x i8> %47, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %49 = bitcast i16* %46 to <16 x i8>*
  store <16 x i8> %48, <16 x i8>* %49, align 1
  %50 = add nuw nsw i64 %38, 8
  br label %51

51:                                               ; preds = %36, %39
  %52 = phi i64 [ %37, %36 ], [ %50, %39 ]
  %53 = trunc i64 %52 to i32
  br label %54

54:                                               ; preds = %51, %23
  %55 = phi i32 [ 0, %23 ], [ %53, %51 ]
  %56 = icmp slt i32 %55, %5
  br i1 %56, label %57, label %255

57:                                               ; preds = %54
  %58 = mul nsw i64 %24, %13
  %59 = mul nsw i64 %24, %12
  %60 = zext i32 %55 to i64
  %61 = sub nsw i64 %15, %60
  %62 = icmp ult i64 %61, 64
  br i1 %62, label %63, label %85

63:                                               ; preds = %195, %85, %57
  %64 = phi i64 [ %60, %85 ], [ %60, %57 ], [ %95, %195 ]
  %65 = sub nsw i64 %15, %64
  %66 = xor i64 %64, -1
  %67 = add nsw i64 %66, %15
  %68 = and i64 %65, 3
  %69 = icmp eq i64 %68, 0
  br i1 %69, label %82, label %70

70:                                               ; preds = %63, %70
  %71 = phi i64 [ %79, %70 ], [ %64, %63 ]
  %72 = phi i64 [ %80, %70 ], [ %68, %63 ]
  %73 = add nsw i64 %71, %58
  %74 = getelementptr inbounds i8, i8* %2, i64 %73
  %75 = load i8, i8* %74, align 1
  %76 = zext i8 %75 to i16
  %77 = add nsw i64 %71, %59
  %78 = getelementptr inbounds i16, i16* %0, i64 %77
  store i16 %76, i16* %78, align 2
  %79 = add nuw nsw i64 %71, 1
  %80 = add i64 %72, -1
  %81 = icmp eq i64 %80, 0
  br i1 %81, label %82, label %70, !llvm.loop !3

82:                                               ; preds = %70, %63
  %83 = phi i64 [ %64, %63 ], [ %79, %70 ]
  %84 = icmp ult i64 %67, 3
  br i1 %84, label %255, label %224

85:                                               ; preds = %57
  %86 = add nsw i64 %25, %60
  %87 = getelementptr i16, i16* %0, i64 %86
  %88 = bitcast i16* %87 to i8*
  %89 = getelementptr i8, i8* %30, i64 %60
  %90 = icmp ugt i8* %32, %88
  %91 = icmp ult i8* %89, %28
  %92 = and i1 %90, %91
  br i1 %92, label %63, label %93

93:                                               ; preds = %85
  %94 = and i64 %61, -64
  %95 = add nsw i64 %94, %60
  %96 = add nsw i64 %94, -64
  %97 = lshr exact i64 %96, 6
  %98 = add nuw nsw i64 %97, 1
  %99 = and i64 %98, 1
  %100 = icmp eq i64 %96, 0
  br i1 %100, label %164, label %101

101:                                              ; preds = %93
  %102 = sub nuw nsw i64 %98, %99
  br label %103

103:                                              ; preds = %103, %101
  %104 = phi i64 [ 0, %101 ], [ %161, %103 ]
  %105 = phi i64 [ %102, %101 ], [ %162, %103 ]
  %106 = add i64 %104, %60
  %107 = add nsw i64 %106, %58
  %108 = getelementptr inbounds i8, i8* %2, i64 %107
  %109 = bitcast i8* %108 to <16 x i8>*
  %110 = load <16 x i8>, <16 x i8>* %109, align 1, !alias.scope !5
  %111 = getelementptr inbounds i8, i8* %108, i64 16
  %112 = bitcast i8* %111 to <16 x i8>*
  %113 = load <16 x i8>, <16 x i8>* %112, align 1, !alias.scope !5
  %114 = getelementptr inbounds i8, i8* %108, i64 32
  %115 = bitcast i8* %114 to <16 x i8>*
  %116 = load <16 x i8>, <16 x i8>* %115, align 1, !alias.scope !5
  %117 = getelementptr inbounds i8, i8* %108, i64 48
  %118 = bitcast i8* %117 to <16 x i8>*
  %119 = load <16 x i8>, <16 x i8>* %118, align 1, !alias.scope !5
  %120 = zext <16 x i8> %110 to <16 x i16>
  %121 = zext <16 x i8> %113 to <16 x i16>
  %122 = zext <16 x i8> %116 to <16 x i16>
  %123 = zext <16 x i8> %119 to <16 x i16>
  %124 = add nsw i64 %106, %59
  %125 = getelementptr inbounds i16, i16* %0, i64 %124
  %126 = bitcast i16* %125 to <16 x i16>*
  store <16 x i16> %120, <16 x i16>* %126, align 2, !alias.scope !8, !noalias !5
  %127 = getelementptr inbounds i16, i16* %125, i64 16
  %128 = bitcast i16* %127 to <16 x i16>*
  store <16 x i16> %121, <16 x i16>* %128, align 2, !alias.scope !8, !noalias !5
  %129 = getelementptr inbounds i16, i16* %125, i64 32
  %130 = bitcast i16* %129 to <16 x i16>*
  store <16 x i16> %122, <16 x i16>* %130, align 2, !alias.scope !8, !noalias !5
  %131 = getelementptr inbounds i16, i16* %125, i64 48
  %132 = bitcast i16* %131 to <16 x i16>*
  store <16 x i16> %123, <16 x i16>* %132, align 2, !alias.scope !8, !noalias !5
  %133 = or i64 %104, 64
  %134 = add i64 %133, %60
  %135 = add nsw i64 %134, %58
  %136 = getelementptr inbounds i8, i8* %2, i64 %135
  %137 = bitcast i8* %136 to <16 x i8>*
  %138 = load <16 x i8>, <16 x i8>* %137, align 1, !alias.scope !5
  %139 = getelementptr inbounds i8, i8* %136, i64 16
  %140 = bitcast i8* %139 to <16 x i8>*
  %141 = load <16 x i8>, <16 x i8>* %140, align 1, !alias.scope !5
  %142 = getelementptr inbounds i8, i8* %136, i64 32
  %143 = bitcast i8* %142 to <16 x i8>*
  %144 = load <16 x i8>, <16 x i8>* %143, align 1, !alias.scope !5
  %145 = getelementptr inbounds i8, i8* %136, i64 48
  %146 = bitcast i8* %145 to <16 x i8>*
  %147 = load <16 x i8>, <16 x i8>* %146, align 1, !alias.scope !5
  %148 = zext <16 x i8> %138 to <16 x i16>
  %149 = zext <16 x i8> %141 to <16 x i16>
  %150 = zext <16 x i8> %144 to <16 x i16>
  %151 = zext <16 x i8> %147 to <16 x i16>
  %152 = add nsw i64 %134, %59
  %153 = getelementptr inbounds i16, i16* %0, i64 %152
  %154 = bitcast i16* %153 to <16 x i16>*
  store <16 x i16> %148, <16 x i16>* %154, align 2, !alias.scope !8, !noalias !5
  %155 = getelementptr inbounds i16, i16* %153, i64 16
  %156 = bitcast i16* %155 to <16 x i16>*
  store <16 x i16> %149, <16 x i16>* %156, align 2, !alias.scope !8, !noalias !5
  %157 = getelementptr inbounds i16, i16* %153, i64 32
  %158 = bitcast i16* %157 to <16 x i16>*
  store <16 x i16> %150, <16 x i16>* %158, align 2, !alias.scope !8, !noalias !5
  %159 = getelementptr inbounds i16, i16* %153, i64 48
  %160 = bitcast i16* %159 to <16 x i16>*
  store <16 x i16> %151, <16 x i16>* %160, align 2, !alias.scope !8, !noalias !5
  %161 = add i64 %104, 128
  %162 = add i64 %105, -2
  %163 = icmp eq i64 %162, 0
  br i1 %163, label %164, label %103, !llvm.loop !10

164:                                              ; preds = %103, %93
  %165 = phi i64 [ 0, %93 ], [ %161, %103 ]
  %166 = icmp eq i64 %99, 0
  br i1 %166, label %195, label %167

167:                                              ; preds = %164
  %168 = add i64 %165, %60
  %169 = add nsw i64 %168, %58
  %170 = getelementptr inbounds i8, i8* %2, i64 %169
  %171 = bitcast i8* %170 to <16 x i8>*
  %172 = load <16 x i8>, <16 x i8>* %171, align 1, !alias.scope !5
  %173 = getelementptr inbounds i8, i8* %170, i64 16
  %174 = bitcast i8* %173 to <16 x i8>*
  %175 = load <16 x i8>, <16 x i8>* %174, align 1, !alias.scope !5
  %176 = getelementptr inbounds i8, i8* %170, i64 32
  %177 = bitcast i8* %176 to <16 x i8>*
  %178 = load <16 x i8>, <16 x i8>* %177, align 1, !alias.scope !5
  %179 = getelementptr inbounds i8, i8* %170, i64 48
  %180 = bitcast i8* %179 to <16 x i8>*
  %181 = load <16 x i8>, <16 x i8>* %180, align 1, !alias.scope !5
  %182 = zext <16 x i8> %172 to <16 x i16>
  %183 = zext <16 x i8> %175 to <16 x i16>
  %184 = zext <16 x i8> %178 to <16 x i16>
  %185 = zext <16 x i8> %181 to <16 x i16>
  %186 = add nsw i64 %168, %59
  %187 = getelementptr inbounds i16, i16* %0, i64 %186
  %188 = bitcast i16* %187 to <16 x i16>*
  store <16 x i16> %182, <16 x i16>* %188, align 2, !alias.scope !8, !noalias !5
  %189 = getelementptr inbounds i16, i16* %187, i64 16
  %190 = bitcast i16* %189 to <16 x i16>*
  store <16 x i16> %183, <16 x i16>* %190, align 2, !alias.scope !8, !noalias !5
  %191 = getelementptr inbounds i16, i16* %187, i64 32
  %192 = bitcast i16* %191 to <16 x i16>*
  store <16 x i16> %184, <16 x i16>* %192, align 2, !alias.scope !8, !noalias !5
  %193 = getelementptr inbounds i16, i16* %187, i64 48
  %194 = bitcast i16* %193 to <16 x i16>*
  store <16 x i16> %185, <16 x i16>* %194, align 2, !alias.scope !8, !noalias !5
  br label %195

195:                                              ; preds = %164, %167
  %196 = icmp eq i64 %61, %94
  br i1 %196, label %255, label %63

197:                                              ; preds = %33, %197
  %198 = phi i64 [ %221, %197 ], [ 0, %33 ]
  %199 = phi i64 [ %222, %197 ], [ %21, %33 ]
  %200 = add nsw i64 %198, %34
  %201 = getelementptr inbounds i8, i8* %2, i64 %200
  %202 = bitcast i8* %201 to i64*
  %203 = load i64, i64* %202, align 1
  %204 = insertelement <2 x i64> undef, i64 %203, i32 0
  %205 = add nsw i64 %198, %35
  %206 = getelementptr inbounds i16, i16* %0, i64 %205
  %207 = bitcast <2 x i64> %204 to <16 x i8>
  %208 = shufflevector <16 x i8> %207, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %209 = bitcast i16* %206 to <16 x i8>*
  store <16 x i8> %208, <16 x i8>* %209, align 1
  %210 = or i64 %198, 8
  %211 = add nsw i64 %210, %34
  %212 = getelementptr inbounds i8, i8* %2, i64 %211
  %213 = bitcast i8* %212 to i64*
  %214 = load i64, i64* %213, align 1
  %215 = insertelement <2 x i64> undef, i64 %214, i32 0
  %216 = add nsw i64 %210, %35
  %217 = getelementptr inbounds i16, i16* %0, i64 %216
  %218 = bitcast <2 x i64> %215 to <16 x i8>
  %219 = shufflevector <16 x i8> %218, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %220 = bitcast i16* %217 to <16 x i8>*
  store <16 x i8> %219, <16 x i8>* %220, align 1
  %221 = add nuw nsw i64 %198, 16
  %222 = add i64 %199, -2
  %223 = icmp eq i64 %222, 0
  br i1 %223, label %36, label %197

224:                                              ; preds = %82, %224
  %225 = phi i64 [ %253, %224 ], [ %83, %82 ]
  %226 = add nsw i64 %225, %58
  %227 = getelementptr inbounds i8, i8* %2, i64 %226
  %228 = load i8, i8* %227, align 1
  %229 = zext i8 %228 to i16
  %230 = add nsw i64 %225, %59
  %231 = getelementptr inbounds i16, i16* %0, i64 %230
  store i16 %229, i16* %231, align 2
  %232 = add nuw nsw i64 %225, 1
  %233 = add nsw i64 %232, %58
  %234 = getelementptr inbounds i8, i8* %2, i64 %233
  %235 = load i8, i8* %234, align 1
  %236 = zext i8 %235 to i16
  %237 = add nsw i64 %232, %59
  %238 = getelementptr inbounds i16, i16* %0, i64 %237
  store i16 %236, i16* %238, align 2
  %239 = add nuw nsw i64 %225, 2
  %240 = add nsw i64 %239, %58
  %241 = getelementptr inbounds i8, i8* %2, i64 %240
  %242 = load i8, i8* %241, align 1
  %243 = zext i8 %242 to i16
  %244 = add nsw i64 %239, %59
  %245 = getelementptr inbounds i16, i16* %0, i64 %244
  store i16 %243, i16* %245, align 2
  %246 = add nuw nsw i64 %225, 3
  %247 = add nsw i64 %246, %58
  %248 = getelementptr inbounds i8, i8* %2, i64 %247
  %249 = load i8, i8* %248, align 1
  %250 = zext i8 %249 to i16
  %251 = add nsw i64 %246, %59
  %252 = getelementptr inbounds i16, i16* %0, i64 %251
  store i16 %250, i16* %252, align 2
  %253 = add nuw nsw i64 %225, 4
  %254 = icmp eq i64 %253, %15
  br i1 %254, label %255, label %224, !llvm.loop !12

255:                                              ; preds = %82, %224, %195, %54
  %256 = add nuw nsw i64 %24, 1
  %257 = icmp eq i64 %256, %14
  br i1 %257, label %258, label %23

258:                                              ; preds = %255, %6
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define hidden void @cdef_copy_rect8_16bit_to_16bit_avx2(i16* nocapture, i32, i16*, i32, i32, i32) local_unnamed_addr #5 {
  %7 = icmp sgt i32 %4, 0
  br i1 %7, label %8, label %250

8:                                                ; preds = %6
  %9 = and i32 %5, -8
  %10 = icmp sgt i32 %9, 0
  %11 = sext i32 %9 to i64
  %12 = sext i32 %1 to i64
  %13 = sext i32 %3 to i64
  %14 = zext i32 %4 to i64
  %15 = zext i32 %5 to i64
  %16 = add nsw i64 %11, -1
  %17 = lshr i64 %16, 3
  %18 = add nuw nsw i64 %17, 1
  %19 = and i64 %18, 3
  %20 = icmp ult i64 %16, 24
  %21 = sub nsw i64 %18, %19
  %22 = icmp eq i64 %19, 0
  br label %23

23:                                               ; preds = %247, %8
  %24 = phi i64 [ 0, %8 ], [ %248, %247 ]
  %25 = mul i64 %24, %12
  %26 = add i64 %25, %15
  %27 = getelementptr i16, i16* %0, i64 %26
  %28 = mul i64 %24, %13
  %29 = add i64 %28, %15
  %30 = getelementptr i16, i16* %2, i64 %29
  br i1 %10, label %31, label %53

31:                                               ; preds = %23
  %32 = mul nsw i64 %24, %13
  %33 = mul nsw i64 %24, %12
  br i1 %20, label %34, label %183

34:                                               ; preds = %183, %31
  %35 = phi i64 [ undef, %31 ], [ %217, %183 ]
  %36 = phi i64 [ 0, %31 ], [ %217, %183 ]
  br i1 %22, label %50, label %37

37:                                               ; preds = %34, %37
  %38 = phi i64 [ %47, %37 ], [ %36, %34 ]
  %39 = phi i64 [ %48, %37 ], [ %19, %34 ]
  %40 = add nsw i64 %38, %32
  %41 = getelementptr inbounds i16, i16* %2, i64 %40
  %42 = bitcast i16* %41 to i8*
  %43 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %42) #9
  %44 = add nsw i64 %38, %33
  %45 = getelementptr inbounds i16, i16* %0, i64 %44
  %46 = bitcast i16* %45 to <16 x i8>*
  store <16 x i8> %43, <16 x i8>* %46, align 1
  %47 = add nuw nsw i64 %38, 8
  %48 = add i64 %39, -1
  %49 = icmp eq i64 %48, 0
  br i1 %49, label %50, label %37, !llvm.loop !13

50:                                               ; preds = %37, %34
  %51 = phi i64 [ %35, %34 ], [ %47, %37 ]
  %52 = trunc i64 %51 to i32
  br label %53

53:                                               ; preds = %50, %23
  %54 = phi i32 [ 0, %23 ], [ %52, %50 ]
  %55 = icmp slt i32 %54, %5
  br i1 %55, label %56, label %247

56:                                               ; preds = %53
  %57 = mul nsw i64 %24, %13
  %58 = mul nsw i64 %24, %12
  %59 = zext i32 %54 to i64
  %60 = sub nsw i64 %15, %59
  %61 = icmp ult i64 %60, 64
  br i1 %61, label %62, label %83

62:                                               ; preds = %181, %83, %56
  %63 = phi i64 [ %59, %83 ], [ %59, %56 ], [ %93, %181 ]
  %64 = sub nsw i64 %15, %63
  %65 = xor i64 %63, -1
  %66 = add nsw i64 %65, %15
  %67 = and i64 %64, 3
  %68 = icmp eq i64 %67, 0
  br i1 %68, label %80, label %69

69:                                               ; preds = %62, %69
  %70 = phi i64 [ %77, %69 ], [ %63, %62 ]
  %71 = phi i64 [ %78, %69 ], [ %67, %62 ]
  %72 = add nsw i64 %70, %57
  %73 = getelementptr inbounds i16, i16* %2, i64 %72
  %74 = load i16, i16* %73, align 2
  %75 = add nsw i64 %70, %58
  %76 = getelementptr inbounds i16, i16* %0, i64 %75
  store i16 %74, i16* %76, align 2
  %77 = add nuw nsw i64 %70, 1
  %78 = add i64 %71, -1
  %79 = icmp eq i64 %78, 0
  br i1 %79, label %80, label %69, !llvm.loop !14

80:                                               ; preds = %69, %62
  %81 = phi i64 [ %63, %62 ], [ %77, %69 ]
  %82 = icmp ult i64 %66, 3
  br i1 %82, label %247, label %220

83:                                               ; preds = %56
  %84 = add nsw i64 %25, %59
  %85 = getelementptr i16, i16* %0, i64 %84
  %86 = add nsw i64 %28, %59
  %87 = getelementptr i16, i16* %2, i64 %86
  %88 = icmp ult i16* %85, %30
  %89 = icmp ult i16* %87, %27
  %90 = and i1 %88, %89
  br i1 %90, label %62, label %91

91:                                               ; preds = %83
  %92 = and i64 %60, -64
  %93 = add nsw i64 %92, %59
  %94 = add nsw i64 %92, -64
  %95 = lshr exact i64 %94, 6
  %96 = add nuw nsw i64 %95, 1
  %97 = and i64 %96, 1
  %98 = icmp eq i64 %94, 0
  br i1 %98, label %154, label %99

99:                                               ; preds = %91
  %100 = sub nuw nsw i64 %96, %97
  br label %101

101:                                              ; preds = %101, %99
  %102 = phi i64 [ 0, %99 ], [ %151, %101 ]
  %103 = phi i64 [ %100, %99 ], [ %152, %101 ]
  %104 = add i64 %102, %59
  %105 = add nsw i64 %104, %57
  %106 = getelementptr inbounds i16, i16* %2, i64 %105
  %107 = bitcast i16* %106 to <16 x i16>*
  %108 = load <16 x i16>, <16 x i16>* %107, align 2, !alias.scope !15
  %109 = getelementptr inbounds i16, i16* %106, i64 16
  %110 = bitcast i16* %109 to <16 x i16>*
  %111 = load <16 x i16>, <16 x i16>* %110, align 2, !alias.scope !15
  %112 = getelementptr inbounds i16, i16* %106, i64 32
  %113 = bitcast i16* %112 to <16 x i16>*
  %114 = load <16 x i16>, <16 x i16>* %113, align 2, !alias.scope !15
  %115 = getelementptr inbounds i16, i16* %106, i64 48
  %116 = bitcast i16* %115 to <16 x i16>*
  %117 = load <16 x i16>, <16 x i16>* %116, align 2, !alias.scope !15
  %118 = add nsw i64 %104, %58
  %119 = getelementptr inbounds i16, i16* %0, i64 %118
  %120 = bitcast i16* %119 to <16 x i16>*
  store <16 x i16> %108, <16 x i16>* %120, align 2, !alias.scope !18, !noalias !15
  %121 = getelementptr inbounds i16, i16* %119, i64 16
  %122 = bitcast i16* %121 to <16 x i16>*
  store <16 x i16> %111, <16 x i16>* %122, align 2, !alias.scope !18, !noalias !15
  %123 = getelementptr inbounds i16, i16* %119, i64 32
  %124 = bitcast i16* %123 to <16 x i16>*
  store <16 x i16> %114, <16 x i16>* %124, align 2, !alias.scope !18, !noalias !15
  %125 = getelementptr inbounds i16, i16* %119, i64 48
  %126 = bitcast i16* %125 to <16 x i16>*
  store <16 x i16> %117, <16 x i16>* %126, align 2, !alias.scope !18, !noalias !15
  %127 = or i64 %102, 64
  %128 = add i64 %127, %59
  %129 = add nsw i64 %128, %57
  %130 = getelementptr inbounds i16, i16* %2, i64 %129
  %131 = bitcast i16* %130 to <16 x i16>*
  %132 = load <16 x i16>, <16 x i16>* %131, align 2, !alias.scope !15
  %133 = getelementptr inbounds i16, i16* %130, i64 16
  %134 = bitcast i16* %133 to <16 x i16>*
  %135 = load <16 x i16>, <16 x i16>* %134, align 2, !alias.scope !15
  %136 = getelementptr inbounds i16, i16* %130, i64 32
  %137 = bitcast i16* %136 to <16 x i16>*
  %138 = load <16 x i16>, <16 x i16>* %137, align 2, !alias.scope !15
  %139 = getelementptr inbounds i16, i16* %130, i64 48
  %140 = bitcast i16* %139 to <16 x i16>*
  %141 = load <16 x i16>, <16 x i16>* %140, align 2, !alias.scope !15
  %142 = add nsw i64 %128, %58
  %143 = getelementptr inbounds i16, i16* %0, i64 %142
  %144 = bitcast i16* %143 to <16 x i16>*
  store <16 x i16> %132, <16 x i16>* %144, align 2, !alias.scope !18, !noalias !15
  %145 = getelementptr inbounds i16, i16* %143, i64 16
  %146 = bitcast i16* %145 to <16 x i16>*
  store <16 x i16> %135, <16 x i16>* %146, align 2, !alias.scope !18, !noalias !15
  %147 = getelementptr inbounds i16, i16* %143, i64 32
  %148 = bitcast i16* %147 to <16 x i16>*
  store <16 x i16> %138, <16 x i16>* %148, align 2, !alias.scope !18, !noalias !15
  %149 = getelementptr inbounds i16, i16* %143, i64 48
  %150 = bitcast i16* %149 to <16 x i16>*
  store <16 x i16> %141, <16 x i16>* %150, align 2, !alias.scope !18, !noalias !15
  %151 = add i64 %102, 128
  %152 = add i64 %103, -2
  %153 = icmp eq i64 %152, 0
  br i1 %153, label %154, label %101, !llvm.loop !20

154:                                              ; preds = %101, %91
  %155 = phi i64 [ 0, %91 ], [ %151, %101 ]
  %156 = icmp eq i64 %97, 0
  br i1 %156, label %181, label %157

157:                                              ; preds = %154
  %158 = add i64 %155, %59
  %159 = add nsw i64 %158, %57
  %160 = getelementptr inbounds i16, i16* %2, i64 %159
  %161 = bitcast i16* %160 to <16 x i16>*
  %162 = load <16 x i16>, <16 x i16>* %161, align 2, !alias.scope !15
  %163 = getelementptr inbounds i16, i16* %160, i64 16
  %164 = bitcast i16* %163 to <16 x i16>*
  %165 = load <16 x i16>, <16 x i16>* %164, align 2, !alias.scope !15
  %166 = getelementptr inbounds i16, i16* %160, i64 32
  %167 = bitcast i16* %166 to <16 x i16>*
  %168 = load <16 x i16>, <16 x i16>* %167, align 2, !alias.scope !15
  %169 = getelementptr inbounds i16, i16* %160, i64 48
  %170 = bitcast i16* %169 to <16 x i16>*
  %171 = load <16 x i16>, <16 x i16>* %170, align 2, !alias.scope !15
  %172 = add nsw i64 %158, %58
  %173 = getelementptr inbounds i16, i16* %0, i64 %172
  %174 = bitcast i16* %173 to <16 x i16>*
  store <16 x i16> %162, <16 x i16>* %174, align 2, !alias.scope !18, !noalias !15
  %175 = getelementptr inbounds i16, i16* %173, i64 16
  %176 = bitcast i16* %175 to <16 x i16>*
  store <16 x i16> %165, <16 x i16>* %176, align 2, !alias.scope !18, !noalias !15
  %177 = getelementptr inbounds i16, i16* %173, i64 32
  %178 = bitcast i16* %177 to <16 x i16>*
  store <16 x i16> %168, <16 x i16>* %178, align 2, !alias.scope !18, !noalias !15
  %179 = getelementptr inbounds i16, i16* %173, i64 48
  %180 = bitcast i16* %179 to <16 x i16>*
  store <16 x i16> %171, <16 x i16>* %180, align 2, !alias.scope !18, !noalias !15
  br label %181

181:                                              ; preds = %154, %157
  %182 = icmp eq i64 %60, %92
  br i1 %182, label %247, label %62

183:                                              ; preds = %31, %183
  %184 = phi i64 [ %217, %183 ], [ 0, %31 ]
  %185 = phi i64 [ %218, %183 ], [ %21, %31 ]
  %186 = add nsw i64 %184, %32
  %187 = getelementptr inbounds i16, i16* %2, i64 %186
  %188 = bitcast i16* %187 to i8*
  %189 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %188) #9
  %190 = add nsw i64 %184, %33
  %191 = getelementptr inbounds i16, i16* %0, i64 %190
  %192 = bitcast i16* %191 to <16 x i8>*
  store <16 x i8> %189, <16 x i8>* %192, align 1
  %193 = or i64 %184, 8
  %194 = add nsw i64 %193, %32
  %195 = getelementptr inbounds i16, i16* %2, i64 %194
  %196 = bitcast i16* %195 to i8*
  %197 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %196) #9
  %198 = add nsw i64 %193, %33
  %199 = getelementptr inbounds i16, i16* %0, i64 %198
  %200 = bitcast i16* %199 to <16 x i8>*
  store <16 x i8> %197, <16 x i8>* %200, align 1
  %201 = or i64 %184, 16
  %202 = add nsw i64 %201, %32
  %203 = getelementptr inbounds i16, i16* %2, i64 %202
  %204 = bitcast i16* %203 to i8*
  %205 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %204) #9
  %206 = add nsw i64 %201, %33
  %207 = getelementptr inbounds i16, i16* %0, i64 %206
  %208 = bitcast i16* %207 to <16 x i8>*
  store <16 x i8> %205, <16 x i8>* %208, align 1
  %209 = or i64 %184, 24
  %210 = add nsw i64 %209, %32
  %211 = getelementptr inbounds i16, i16* %2, i64 %210
  %212 = bitcast i16* %211 to i8*
  %213 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %212) #9
  %214 = add nsw i64 %209, %33
  %215 = getelementptr inbounds i16, i16* %0, i64 %214
  %216 = bitcast i16* %215 to <16 x i8>*
  store <16 x i8> %213, <16 x i8>* %216, align 1
  %217 = add nuw nsw i64 %184, 32
  %218 = add i64 %185, -4
  %219 = icmp eq i64 %218, 0
  br i1 %219, label %34, label %183

220:                                              ; preds = %80, %220
  %221 = phi i64 [ %245, %220 ], [ %81, %80 ]
  %222 = add nsw i64 %221, %57
  %223 = getelementptr inbounds i16, i16* %2, i64 %222
  %224 = load i16, i16* %223, align 2
  %225 = add nsw i64 %221, %58
  %226 = getelementptr inbounds i16, i16* %0, i64 %225
  store i16 %224, i16* %226, align 2
  %227 = add nuw nsw i64 %221, 1
  %228 = add nsw i64 %227, %57
  %229 = getelementptr inbounds i16, i16* %2, i64 %228
  %230 = load i16, i16* %229, align 2
  %231 = add nsw i64 %227, %58
  %232 = getelementptr inbounds i16, i16* %0, i64 %231
  store i16 %230, i16* %232, align 2
  %233 = add nuw nsw i64 %221, 2
  %234 = add nsw i64 %233, %57
  %235 = getelementptr inbounds i16, i16* %2, i64 %234
  %236 = load i16, i16* %235, align 2
  %237 = add nsw i64 %233, %58
  %238 = getelementptr inbounds i16, i16* %0, i64 %237
  store i16 %236, i16* %238, align 2
  %239 = add nuw nsw i64 %221, 3
  %240 = add nsw i64 %239, %57
  %241 = getelementptr inbounds i16, i16* %2, i64 %240
  %242 = load i16, i16* %241, align 2
  %243 = add nsw i64 %239, %58
  %244 = getelementptr inbounds i16, i16* %0, i64 %243
  store i16 %242, i16* %244, align 2
  %245 = add nuw nsw i64 %221, 4
  %246 = icmp eq i64 %245, %15
  br i1 %246, label %247, label %220, !llvm.loop !21

247:                                              ; preds = %80, %220, %181, %53
  %248 = add nuw nsw i64 %24, 1
  %249 = icmp eq i64 %248, %14
  br i1 %249, label %250, label %23

250:                                              ; preds = %247, %6
  ret void
}

; Function Attrs: nounwind readonly
declare <16 x i8> @llvm.x86.sse3.ldu.dq(i8*) #6

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16>, <8 x i16>) #7

; Function Attrs: nounwind readnone
declare <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16>, <8 x i16>) #7

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32>, <4 x i32>) #7

; Function Attrs: nounwind readnone
declare <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16>, <8 x i16>) #7

; Function Attrs: nounwind readnone speculatable
declare i32 @llvm.ctlz.i32(i32, i1 immarg) #8

; Function Attrs: nounwind readnone speculatable
declare <16 x i8> @llvm.usub.sat.v16i8(<16 x i8>, <16 x i8>) #8

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16>, <8 x i16>) #7

; Function Attrs: nounwind readnone
declare <16 x i16> @llvm.x86.avx2.pmadd.ub.sw(<32 x i8>, <32 x i8>) #7

; Function Attrs: nounwind readnone
declare <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16>, <16 x i16>) #7

; Function Attrs: nounwind readnone speculatable
declare <16 x i16> @llvm.usub.sat.v16i16(<16 x i16>, <16 x i16>) #8

; Function Attrs: nounwind readnone
declare <16 x i16> @llvm.x86.avx2.psrl.w(<16 x i16>, <8 x i16>) #7

attributes #0 = { nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="128" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+avx,+avx2,+cx8,+fxsr,+mmx,+popcnt,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { argmemonly nounwind }
attributes #2 = { nofree nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="256" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+avx,+avx2,+cx8,+fxsr,+mmx,+popcnt,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #3 = { nofree nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+avx,+avx2,+cx8,+fxsr,+mmx,+popcnt,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #4 = { nofree norecurse nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="128" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+avx,+avx2,+cx8,+fxsr,+mmx,+popcnt,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #5 = { nofree nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="128" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+avx,+avx2,+cx8,+fxsr,+mmx,+popcnt,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #6 = { nounwind readonly }
attributes #7 = { nounwind readnone }
attributes #8 = { nounwind readnone speculatable }
attributes #9 = { nounwind }

!llvm.module.flags = !{!0, !1}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 2}
!2 = !{i32 0, i32 33}
!3 = distinct !{!3, !4}
!4 = !{!"llvm.loop.unroll.disable"}
!5 = !{!6}
!6 = distinct !{!6, !7}
!7 = distinct !{!7, !"LVerDomain"}
!8 = !{!9}
!9 = distinct !{!9, !7}
!10 = distinct !{!10, !11}
!11 = !{!"llvm.loop.isvectorized", i32 1}
!12 = distinct !{!12, !11}
!13 = distinct !{!13, !4}
!14 = distinct !{!14, !4}
!15 = !{!16}
!16 = distinct !{!16, !17}
!17 = distinct !{!17, !"LVerDomain"}
!18 = !{!19}
!19 = distinct !{!19, !17}
!20 = distinct !{!20, !11}
!21 = distinct !{!21, !11}
