; ModuleID = '../../third_party/libaom/source/libaom/av1/common/x86/av1_inv_txfm_avx2.c'
source_filename = "../../third_party/libaom/source/libaom/av1/common/x86/av1_inv_txfm_avx2.c"
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

module asm ".symver exp, exp@GLIBC_2.2.5"
module asm ".symver exp2, exp2@GLIBC_2.2.5"
module asm ".symver exp2f, exp2f@GLIBC_2.2.5"
module asm ".symver expf, expf@GLIBC_2.2.5"
module asm ".symver lgamma, lgamma@GLIBC_2.2.5"
module asm ".symver lgammaf, lgammaf@GLIBC_2.2.5"
module asm ".symver lgammal, lgammal@GLIBC_2.2.5"
module asm ".symver log, log@GLIBC_2.2.5"
module asm ".symver log2, log2@GLIBC_2.2.5"
module asm ".symver log2f, log2f@GLIBC_2.2.5"
module asm ".symver logf, logf@GLIBC_2.2.5"
module asm ".symver pow, pow@GLIBC_2.2.5"
module asm ".symver powf, powf@GLIBC_2.2.5"

%struct.txfm_param = type { i8, i8, i32, i32, i32, i8, i32 }

@av1_inv_txfm_shift_ls = external local_unnamed_addr global [19 x i8*], align 16
@av1_inv_cos_bit_col = external local_unnamed_addr constant [5 x [5 x i8]], align 16
@av1_inv_cos_bit_row = external local_unnamed_addr constant [5 x [5 x i8]], align 16
@tx_size_wide = internal unnamed_addr constant [19 x i32] [i32 4, i32 8, i32 16, i32 32, i32 64, i32 4, i32 8, i32 8, i32 16, i32 16, i32 32, i32 32, i32 64, i32 4, i32 16, i32 8, i32 32, i32 16, i32 64], align 16
@tx_size_high = internal unnamed_addr constant [19 x i32] [i32 4, i32 8, i32 16, i32 32, i32 64, i32 8, i32 4, i32 16, i32 8, i32 32, i32 16, i32 64, i32 32, i32 16, i32 4, i32 32, i32 8, i32 64, i32 16], align 16
@lowbd_txfm_all_1d_zeros_idx = internal unnamed_addr constant [32 x i32] [i32 0, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 2, i32 2, i32 2, i32 2, i32 2, i32 2, i32 2, i32 2, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3], align 16
@lowbd_txfm_all_1d_zeros_w16_arr = internal unnamed_addr constant [5 x [3 x [4 x void (<4 x i64>*, <4 x i64>*, i8)*]]] [[3 x [4 x void (<4 x i64>*, <4 x i64>*, i8)*]] zeroinitializer, [3 x [4 x void (<4 x i64>*, <4 x i64>*, i8)*]] zeroinitializer, [3 x [4 x void (<4 x i64>*, <4 x i64>*, i8)*]] [[4 x void (<4 x i64>*, <4 x i64>*, i8)*] [void (<4 x i64>*, <4 x i64>*, i8)* @idct16_low1_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @idct16_low8_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @idct16_avx2, void (<4 x i64>*, <4 x i64>*, i8)* null], [4 x void (<4 x i64>*, <4 x i64>*, i8)*] [void (<4 x i64>*, <4 x i64>*, i8)* @iadst16_low1_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @iadst16_low8_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @iadst16_avx2, void (<4 x i64>*, <4 x i64>*, i8)* null], [4 x void (<4 x i64>*, <4 x i64>*, i8)*] zeroinitializer], [3 x [4 x void (<4 x i64>*, <4 x i64>*, i8)*]] [[4 x void (<4 x i64>*, <4 x i64>*, i8)*] [void (<4 x i64>*, <4 x i64>*, i8)* @idct32_low1_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @idct32_low8_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @idct32_low16_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @idct32_avx2], [4 x void (<4 x i64>*, <4 x i64>*, i8)*] zeroinitializer, [4 x void (<4 x i64>*, <4 x i64>*, i8)*] zeroinitializer], [3 x [4 x void (<4 x i64>*, <4 x i64>*, i8)*]] [[4 x void (<4 x i64>*, <4 x i64>*, i8)*] [void (<4 x i64>*, <4 x i64>*, i8)* @idct64_low1_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @idct64_low8_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @idct64_low16_avx2, void (<4 x i64>*, <4 x i64>*, i8)* @idct64_low32_avx2], [4 x void (<4 x i64>*, <4 x i64>*, i8)*] zeroinitializer, [4 x void (<4 x i64>*, <4 x i64>*, i8)*] zeroinitializer]], align 16
@hitx_1d_tab = internal unnamed_addr constant [16 x i8] c"\00\00\01\01\00\01\01\01\01\02\02\00\02\01\02\01", align 16
@vitx_1d_tab = internal unnamed_addr constant [16 x i8] c"\00\01\00\01\01\00\01\01\01\02\00\02\01\02\01\02", align 16
@tx_size_wide_log2_eob = internal unnamed_addr constant [19 x i32] [i32 2, i32 3, i32 4, i32 5, i32 5, i32 2, i32 3, i32 3, i32 4, i32 4, i32 5, i32 5, i32 5, i32 2, i32 4, i32 3, i32 5, i32 4, i32 5], align 16
@av1_eob_to_eobxy_default = internal unnamed_addr constant [19 x i16*] [i16* null, i16* getelementptr inbounds ([8 x i16], [8 x i16]* @av1_eob_to_eobxy_8x8_default, i32 0, i32 0), i16* getelementptr inbounds ([16 x i16], [16 x i16]* @av1_eob_to_eobxy_16x16_default, i32 0, i32 0), i16* getelementptr inbounds ([32 x i16], [32 x i16]* @av1_eob_to_eobxy_32x32_default, i32 0, i32 0), i16* getelementptr inbounds ([32 x i16], [32 x i16]* @av1_eob_to_eobxy_32x32_default, i32 0, i32 0), i16* null, i16* null, i16* getelementptr inbounds ([16 x i16], [16 x i16]* @av1_eob_to_eobxy_8x16_default, i32 0, i32 0), i16* getelementptr inbounds ([8 x i16], [8 x i16]* @av1_eob_to_eobxy_16x8_default, i32 0, i32 0), i16* getelementptr inbounds ([32 x i16], [32 x i16]* @av1_eob_to_eobxy_16x32_default, i32 0, i32 0), i16* getelementptr inbounds ([16 x i16], [16 x i16]* @av1_eob_to_eobxy_32x16_default, i32 0, i32 0), i16* getelementptr inbounds ([32 x i16], [32 x i16]* @av1_eob_to_eobxy_32x32_default, i32 0, i32 0), i16* getelementptr inbounds ([32 x i16], [32 x i16]* @av1_eob_to_eobxy_32x32_default, i32 0, i32 0), i16* null, i16* null, i16* getelementptr inbounds ([32 x i16], [32 x i16]* @av1_eob_to_eobxy_8x32_default, i32 0, i32 0), i16* getelementptr inbounds ([8 x i16], [8 x i16]* @av1_eob_to_eobxy_32x8_default, i32 0, i32 0), i16* getelementptr inbounds ([32 x i16], [32 x i16]* @av1_eob_to_eobxy_16x32_default, i32 0, i32 0), i16* getelementptr inbounds ([16 x i16], [16 x i16]* @av1_eob_to_eobxy_32x16_default, i32 0, i32 0)], align 16
@av1_eob_to_eobxy_8x8_default = internal constant [8 x i16] [i16 1799, i16 1799, i16 1799, i16 1799, i16 1799, i16 1799, i16 1799, i16 1799], align 16
@av1_eob_to_eobxy_16x16_default = internal constant [16 x i16] [i16 1799, i16 1799, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855], align 16
@av1_eob_to_eobxy_32x32_default = internal constant [32 x i16] [i16 1799, i16 3855, i16 3855, i16 3855, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967], align 16
@av1_eob_to_eobxy_8x16_default = internal constant [16 x i16] [i16 1799, i16 1799, i16 1799, i16 1799, i16 1799, i16 3847, i16 3847, i16 3847, i16 3847, i16 3847, i16 3847, i16 3847, i16 3847, i16 3847, i16 3847, i16 3847], align 16
@av1_eob_to_eobxy_16x8_default = internal constant [8 x i16] [i16 1799, i16 1799, i16 1807, i16 1807, i16 1807, i16 1807, i16 1807, i16 1807], align 16
@av1_eob_to_eobxy_16x32_default = internal constant [32 x i16] [i16 1799, i16 1799, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951], align 16
@av1_eob_to_eobxy_32x16_default = internal constant [16 x i16] [i16 1799, i16 3855, i16 3855, i16 3855, i16 3871, i16 3871, i16 3871, i16 3871, i16 3871, i16 3871, i16 3871, i16 3871, i16 3871, i16 3871, i16 3871, i16 3871], align 16
@av1_eob_to_eobxy_8x32_default = internal constant [32 x i16] [i16 1799, i16 1799, i16 1799, i16 1799, i16 1799, i16 3847, i16 3847, i16 3847, i16 3847, i16 3847, i16 3847, i16 3847, i16 3847, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943], align 16
@av1_eob_to_eobxy_32x8_default = internal constant [8 x i16] [i16 1799, i16 1807, i16 1807, i16 1823, i16 1823, i16 1823, i16 1823, i16 1823], align 16
@tx_size_wide_log2 = internal unnamed_addr constant [19 x i32] [i32 2, i32 3, i32 4, i32 5, i32 6, i32 2, i32 3, i32 3, i32 4, i32 4, i32 5, i32 5, i32 6, i32 2, i32 4, i32 3, i32 5, i32 4, i32 6], align 16
@tx_size_high_log2 = internal unnamed_addr constant [19 x i32] [i32 2, i32 3, i32 4, i32 5, i32 6, i32 3, i32 2, i32 4, i32 3, i32 5, i32 4, i32 6, i32 5, i32 4, i32 2, i32 5, i32 3, i32 6, i32 4], align 16
@av1_cospi_arr_data = external local_unnamed_addr constant [7 x [64 x i32]], align 16
@NewSqrt2list = internal unnamed_addr constant [5 x i32] [i32 5793, i32 8192, i32 11586, i32 16384, i32 23172], align 16
@eob_fill = internal unnamed_addr constant [32 x i32] [i32 0, i32 7, i32 7, i32 7, i32 7, i32 7, i32 7, i32 7, i32 15, i32 15, i32 15, i32 15, i32 15, i32 15, i32 15, i32 15, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31, i32 31], align 16
@switch.table.av1_lowbd_inv_txfm2d_add_avx2 = private unnamed_addr constant [5 x i32] [i32 1, i32 0, i32 1, i32 0, i32 1], align 4
@switch.table.av1_lowbd_inv_txfm2d_add_avx2.1 = private unnamed_addr constant [5 x i32] [i32 0, i32 1, i32 1, i32 1, i32 0], align 4
@switch.table.av1_lowbd_inv_txfm2d_add_avx2.2 = private unnamed_addr constant [6 x i32] [i32 1, i32 1, i32 1, i32 0, i32 0, i32 1], align 4

; Function Attrs: nounwind ssp uwtable
define hidden void @av1_lowbd_inv_txfm2d_add_avx2(i32*, i8*, i32, i8 zeroext, i8 zeroext, i32) local_unnamed_addr #0 {
  %7 = alloca [64 x <4 x i64>], align 32
  %8 = alloca [32 x <4 x i64>], align 32
  %9 = alloca [1024 x <4 x i64>], align 32
  %10 = alloca [64 x <4 x i64>], align 32
  switch i8 %4, label %12 [
    i8 0, label %11
    i8 1, label %11
    i8 5, label %11
    i8 6, label %11
    i8 7, label %11
    i8 8, label %11
    i8 13, label %11
    i8 14, label %11
    i8 15, label %11
    i8 16, label %11
  ]

11:                                               ; preds = %6, %6, %6, %6, %6, %6, %6, %6, %6, %6
  tail call void @av1_lowbd_inv_txfm2d_add_ssse3(i32* %0, i8* %1, i32 %2, i8 zeroext %3, i8 zeroext %4, i32 %5) #9
  br label %2237

12:                                               ; preds = %6
  switch i8 %3, label %2236 [
    i8 0, label %13
    i8 1, label %13
    i8 2, label %13
    i8 3, label %13
    i8 4, label %13
    i8 5, label %13
    i8 6, label %13
    i8 7, label %13
    i8 8, label %13
    i8 9, label %940
    i8 10, label %1100
    i8 12, label %1100
    i8 14, label %1100
    i8 11, label %1294
    i8 13, label %1294
    i8 15, label %1294
  ]

13:                                               ; preds = %12, %12, %12, %12, %12, %12, %12, %12, %12
  %14 = bitcast [1024 x <4 x i64>]* %9 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32768, i8* nonnull %14) #9
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %14, i8 -86, i64 32768, i1 false) #9
  %15 = icmp eq i32 %5, 1
  %16 = zext i8 %4 to i64
  br i1 %15, label %30, label %17

17:                                               ; preds = %13
  %18 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_wide_log2_eob, i64 0, i64 %16
  %19 = load i32, i32* %18, align 4
  %20 = add nsw i32 %5, -1
  %21 = ashr i32 %20, %19
  %22 = getelementptr inbounds [19 x i16*], [19 x i16*]* @av1_eob_to_eobxy_default, i64 0, i64 %16
  %23 = load i16*, i16** %22, align 8
  %24 = sext i32 %21 to i64
  %25 = getelementptr inbounds i16, i16* %23, i64 %24
  %26 = load i16, i16* %25, align 2
  %27 = sext i16 %26 to i32
  %28 = and i32 %27, 255
  %29 = ashr i32 %27, 8
  br label %30

30:                                               ; preds = %17, %13
  %31 = phi i32 [ %28, %17 ], [ 0, %13 ]
  %32 = phi i32 [ %29, %17 ], [ 0, %13 ]
  %33 = getelementptr inbounds [19 x i8*], [19 x i8*]* @av1_inv_txfm_shift_ls, i64 0, i64 %16
  %34 = load i8*, i8** %33, align 8
  %35 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_wide_log2, i64 0, i64 %16
  %36 = load i32, i32* %35, align 4
  %37 = add nsw i32 %36, -2
  %38 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_high_log2, i64 0, i64 %16
  %39 = load i32, i32* %38, align 4
  %40 = add nsw i32 %39, -2
  %41 = sext i32 %37 to i64
  %42 = sext i32 %40 to i64
  %43 = getelementptr inbounds [5 x [5 x i8]], [5 x [5 x i8]]* @av1_inv_cos_bit_col, i64 0, i64 %41, i64 %42
  %44 = load i8, i8* %43, align 1
  %45 = getelementptr inbounds [5 x [5 x i8]], [5 x [5 x i8]]* @av1_inv_cos_bit_row, i64 0, i64 %41, i64 %42
  %46 = load i8, i8* %45, align 1
  %47 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_wide, i64 0, i64 %16
  %48 = load i32, i32* %47, align 4
  %49 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_high, i64 0, i64 %16
  %50 = load i32, i32* %49, align 4
  %51 = ashr i32 %48, 4
  %52 = add nuw nsw i32 %31, 16
  %53 = lshr i32 %52, 4
  %54 = add nsw i32 %32, 16
  %55 = ashr i32 %54, 4
  %56 = icmp slt i32 %48, 32
  %57 = select i1 %56, i32 %48, i32 32
  %58 = icmp eq i32 %48, %50
  br i1 %58, label %74, label %59

59:                                               ; preds = %30
  %60 = icmp sgt i32 %48, %50
  br i1 %60, label %61, label %67

61:                                               ; preds = %59
  %62 = shl nsw i32 %50, 1
  %63 = icmp eq i32 %62, %48
  br i1 %63, label %74, label %64

64:                                               ; preds = %61
  %65 = shl nsw i32 %50, 2
  %66 = icmp eq i32 %65, %48
  br i1 %66, label %74, label %73

67:                                               ; preds = %59
  %68 = shl nsw i32 %48, 1
  %69 = icmp eq i32 %68, %50
  br i1 %69, label %74, label %70

70:                                               ; preds = %67
  %71 = shl nsw i32 %48, 2
  %72 = icmp eq i32 %71, %50
  br i1 %72, label %74, label %73

73:                                               ; preds = %70, %64
  br label %74

74:                                               ; preds = %73, %70, %67, %64, %61, %30
  %75 = phi i32 [ 0, %73 ], [ 0, %30 ], [ 1, %61 ], [ 2, %64 ], [ -1, %67 ], [ -2, %70 ]
  %76 = zext i32 %31 to i64
  %77 = getelementptr inbounds [32 x i32], [32 x i32]* @lowbd_txfm_all_1d_zeros_idx, i64 0, i64 %76
  %78 = load i32, i32* %77, align 4
  %79 = sext i32 %32 to i64
  %80 = getelementptr inbounds [32 x i32], [32 x i32]* @lowbd_txfm_all_1d_zeros_idx, i64 0, i64 %79
  %81 = load i32, i32* %80, align 4
  %82 = zext i8 %3 to i64
  %83 = getelementptr inbounds [16 x i8], [16 x i8]* @hitx_1d_tab, i64 0, i64 %82
  %84 = load i8, i8* %83, align 1
  %85 = zext i8 %84 to i64
  %86 = sext i32 %78 to i64
  %87 = getelementptr inbounds [5 x [3 x [4 x void (<4 x i64>*, <4 x i64>*, i8)*]]], [5 x [3 x [4 x void (<4 x i64>*, <4 x i64>*, i8)*]]]* @lowbd_txfm_all_1d_zeros_w16_arr, i64 0, i64 %41, i64 %85, i64 %86
  %88 = load void (<4 x i64>*, <4 x i64>*, i8)*, void (<4 x i64>*, <4 x i64>*, i8)** %87, align 8
  %89 = getelementptr inbounds [16 x i8], [16 x i8]* @vitx_1d_tab, i64 0, i64 %82
  %90 = load i8, i8* %89, align 1
  %91 = zext i8 %90 to i64
  %92 = sext i32 %81 to i64
  %93 = getelementptr inbounds [5 x [3 x [4 x void (<4 x i64>*, <4 x i64>*, i8)*]]], [5 x [3 x [4 x void (<4 x i64>*, <4 x i64>*, i8)*]]]* @lowbd_txfm_all_1d_zeros_w16_arr, i64 0, i64 %42, i64 %91, i64 %92
  %94 = load void (<4 x i64>*, <4 x i64>*, i8)*, void (<4 x i64>*, <4 x i64>*, i8)** %93, align 8
  %95 = add i8 %3, -4
  %96 = icmp ult i8 %95, 5
  br i1 %96, label %97, label %104

97:                                               ; preds = %74
  %98 = sext i8 %95 to i64
  %99 = getelementptr inbounds [5 x i32], [5 x i32]* @switch.table.av1_lowbd_inv_txfm2d_add_avx2, i64 0, i64 %98
  %100 = load i32, i32* %99, align 4
  %101 = sext i8 %95 to i64
  %102 = getelementptr inbounds [5 x i32], [5 x i32]* @switch.table.av1_lowbd_inv_txfm2d_add_avx2.1, i64 0, i64 %101
  %103 = load i32, i32* %102, align 4
  br label %104

104:                                              ; preds = %97, %74
  %105 = phi i32 [ 0, %74 ], [ %100, %97 ]
  %106 = phi i32 [ 0, %74 ], [ %103, %97 ]
  %107 = load i8, i8* %34, align 1
  %108 = sext i8 %107 to i32
  %109 = add nsw i32 %108, 15
  %110 = shl i32 1, %109
  %111 = trunc i32 %110 to i16
  %112 = insertelement <16 x i16> undef, i16 %111, i32 0
  %113 = shufflevector <16 x i16> %112, <16 x i16> undef, <16 x i32> zeroinitializer
  %114 = icmp sgt i32 %54, 15
  br i1 %114, label %118, label %115

115:                                              ; preds = %104
  %116 = lshr i64 483100, %16
  %117 = and i64 %116, 1
  br label %137

118:                                              ; preds = %104
  %119 = bitcast [64 x <4 x i64>]* %10 to i8*
  %120 = sext i32 %57 to i64
  %121 = zext i32 %57 to i64
  %122 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %10, i64 0, i64 0
  %123 = icmp eq i32 %106, 0
  %124 = lshr i64 483100, %16
  %125 = and i64 %124, 1
  %126 = icmp eq i64 %125, 0
  %127 = sext i32 %48 to i64
  %128 = sext i32 %51 to i64
  %129 = sext i32 %50 to i64
  %130 = sext i32 %55 to i64
  %131 = zext i32 %53 to i64
  %132 = add nsw i64 %121, -1
  %133 = and i64 %121, 3
  %134 = icmp ult i64 %132, 3
  %135 = sub nsw i64 %121, %133
  %136 = icmp eq i64 %133, 0
  br label %151

137:                                              ; preds = %850, %115
  %138 = phi i64 [ %117, %115 ], [ %125, %850 ]
  %139 = getelementptr inbounds i8, i8* %34, i64 1
  %140 = load i8, i8* %139, align 1
  %141 = sext i8 %140 to i32
  %142 = add nsw i32 %141, 15
  %143 = shl i32 1, %142
  %144 = trunc i32 %143 to i16
  %145 = insertelement <16 x i16> undef, i16 %144, i32 0
  %146 = shufflevector <16 x i16> %145, <16 x i16> undef, <16 x i32> zeroinitializer
  %147 = icmp eq i64 %138, 0
  br i1 %147, label %939, label %148

148:                                              ; preds = %137
  %149 = sext i32 %50 to i64
  %150 = sext i32 %51 to i64
  br label %864

151:                                              ; preds = %850, %118
  %152 = phi i64 [ 0, %118 ], [ %851, %850 ]
  call void @llvm.lifetime.start.p0i8(i64 2048, i8* nonnull %119) #9
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %119, i8 -86, i64 2048, i1 false) #9
  %153 = shl i64 %152, 4
  %154 = mul nsw i64 %153, %120
  %155 = getelementptr inbounds i32, i32* %0, i64 %154
  br label %158

156:                                              ; preds = %192
  switch i32 %75, label %423 [
    i32 -1, label %157
    i32 1, label %157
  ]

157:                                              ; preds = %156, %156
  br i1 %134, label %411, label %386

158:                                              ; preds = %192, %151
  %159 = phi i64 [ 0, %151 ], [ %384, %192 ]
  %160 = shl nsw i64 %159, 4
  %161 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %10, i64 0, i64 %160
  %162 = getelementptr inbounds i32, i32* %155, i64 %160
  br label %163

163:                                              ; preds = %163, %158
  %164 = phi i64 [ 0, %158 ], [ %190, %163 ]
  %165 = mul nsw i64 %164, %120
  %166 = getelementptr inbounds i32, i32* %162, i64 %165
  %167 = bitcast i32* %166 to i8*
  %168 = call <32 x i8> @llvm.x86.avx.ldu.dq.256(i8* %167) #9
  %169 = getelementptr inbounds i32, i32* %166, i64 8
  %170 = bitcast i32* %169 to <8 x i32>*
  %171 = load <8 x i32>, <8 x i32>* %170, align 32
  %172 = bitcast <32 x i8> %168 to <8 x i32>
  %173 = call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %172, <8 x i32> %171) #9
  %174 = bitcast <16 x i16> %173 to <4 x i64>
  %175 = shufflevector <4 x i64> %174, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %176 = getelementptr inbounds <4 x i64>, <4 x i64>* %161, i64 %164
  store <4 x i64> %175, <4 x i64>* %176, align 32
  %177 = or i64 %164, 1
  %178 = mul nsw i64 %177, %120
  %179 = getelementptr inbounds i32, i32* %162, i64 %178
  %180 = bitcast i32* %179 to i8*
  %181 = call <32 x i8> @llvm.x86.avx.ldu.dq.256(i8* %180) #9
  %182 = getelementptr inbounds i32, i32* %179, i64 8
  %183 = bitcast i32* %182 to <8 x i32>*
  %184 = load <8 x i32>, <8 x i32>* %183, align 32
  %185 = bitcast <32 x i8> %181 to <8 x i32>
  %186 = call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %185, <8 x i32> %184) #9
  %187 = bitcast <16 x i16> %186 to <4 x i64>
  %188 = shufflevector <4 x i64> %187, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %189 = getelementptr inbounds <4 x i64>, <4 x i64>* %161, i64 %177
  store <4 x i64> %188, <4 x i64>* %189, align 32
  %190 = add nuw nsw i64 %164, 2
  %191 = icmp eq i64 %190, 16
  br i1 %191, label %192, label %163

192:                                              ; preds = %163
  %193 = bitcast <4 x i64>* %161 to <2 x i64>*
  %194 = load <2 x i64>, <2 x i64>* %193, align 32
  %195 = getelementptr inbounds <4 x i64>, <4 x i64>* %161, i64 8
  %196 = bitcast <4 x i64>* %195 to <2 x i64>*
  %197 = load <2 x i64>, <2 x i64>* %196, align 32
  %198 = shufflevector <2 x i64> %194, <2 x i64> %197, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %199 = getelementptr inbounds <4 x i64>, <4 x i64>* %161, i64 1
  %200 = bitcast <4 x i64>* %199 to <2 x i64>*
  %201 = load <2 x i64>, <2 x i64>* %200, align 32
  %202 = getelementptr inbounds <4 x i64>, <4 x i64>* %161, i64 9
  %203 = bitcast <4 x i64>* %202 to <2 x i64>*
  %204 = load <2 x i64>, <2 x i64>* %203, align 32
  %205 = shufflevector <2 x i64> %201, <2 x i64> %204, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %206 = getelementptr inbounds <4 x i64>, <4 x i64>* %161, i64 2
  %207 = bitcast <4 x i64>* %206 to <2 x i64>*
  %208 = load <2 x i64>, <2 x i64>* %207, align 32
  %209 = getelementptr inbounds <4 x i64>, <4 x i64>* %161, i64 10
  %210 = bitcast <4 x i64>* %209 to <2 x i64>*
  %211 = load <2 x i64>, <2 x i64>* %210, align 32
  %212 = shufflevector <2 x i64> %208, <2 x i64> %211, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %213 = getelementptr inbounds <4 x i64>, <4 x i64>* %161, i64 3
  %214 = bitcast <4 x i64>* %213 to <2 x i64>*
  %215 = load <2 x i64>, <2 x i64>* %214, align 32
  %216 = getelementptr inbounds <4 x i64>, <4 x i64>* %161, i64 11
  %217 = bitcast <4 x i64>* %216 to <2 x i64>*
  %218 = load <2 x i64>, <2 x i64>* %217, align 32
  %219 = shufflevector <2 x i64> %215, <2 x i64> %218, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %220 = getelementptr inbounds <4 x i64>, <4 x i64>* %161, i64 4
  %221 = bitcast <4 x i64>* %220 to <2 x i64>*
  %222 = load <2 x i64>, <2 x i64>* %221, align 32
  %223 = getelementptr inbounds <4 x i64>, <4 x i64>* %161, i64 12
  %224 = bitcast <4 x i64>* %223 to <2 x i64>*
  %225 = load <2 x i64>, <2 x i64>* %224, align 32
  %226 = shufflevector <2 x i64> %222, <2 x i64> %225, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %227 = getelementptr inbounds <4 x i64>, <4 x i64>* %161, i64 5
  %228 = bitcast <4 x i64>* %227 to <2 x i64>*
  %229 = load <2 x i64>, <2 x i64>* %228, align 32
  %230 = getelementptr inbounds <4 x i64>, <4 x i64>* %161, i64 13
  %231 = bitcast <4 x i64>* %230 to <2 x i64>*
  %232 = load <2 x i64>, <2 x i64>* %231, align 32
  %233 = shufflevector <2 x i64> %229, <2 x i64> %232, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %234 = getelementptr inbounds <4 x i64>, <4 x i64>* %161, i64 6
  %235 = bitcast <4 x i64>* %234 to <2 x i64>*
  %236 = load <2 x i64>, <2 x i64>* %235, align 32
  %237 = getelementptr inbounds <4 x i64>, <4 x i64>* %161, i64 14
  %238 = bitcast <4 x i64>* %237 to <2 x i64>*
  %239 = load <2 x i64>, <2 x i64>* %238, align 32
  %240 = shufflevector <2 x i64> %236, <2 x i64> %239, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %241 = getelementptr inbounds <4 x i64>, <4 x i64>* %161, i64 7
  %242 = bitcast <4 x i64>* %241 to <2 x i64>*
  %243 = load <2 x i64>, <2 x i64>* %242, align 32
  %244 = getelementptr inbounds <4 x i64>, <4 x i64>* %161, i64 15
  %245 = bitcast <4 x i64>* %244 to <2 x i64>*
  %246 = load <2 x i64>, <2 x i64>* %245, align 32
  %247 = shufflevector <2 x i64> %243, <2 x i64> %246, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %248 = getelementptr inbounds <2 x i64>, <2 x i64>* %193, i64 1
  %249 = load <2 x i64>, <2 x i64>* %248, align 16
  %250 = getelementptr inbounds <2 x i64>, <2 x i64>* %196, i64 1
  %251 = load <2 x i64>, <2 x i64>* %250, align 16
  %252 = shufflevector <2 x i64> %249, <2 x i64> %251, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %253 = getelementptr inbounds <2 x i64>, <2 x i64>* %200, i64 1
  %254 = load <2 x i64>, <2 x i64>* %253, align 16
  %255 = getelementptr inbounds <2 x i64>, <2 x i64>* %203, i64 1
  %256 = load <2 x i64>, <2 x i64>* %255, align 16
  %257 = shufflevector <2 x i64> %254, <2 x i64> %256, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %258 = getelementptr inbounds <2 x i64>, <2 x i64>* %207, i64 1
  %259 = load <2 x i64>, <2 x i64>* %258, align 16
  %260 = getelementptr inbounds <2 x i64>, <2 x i64>* %210, i64 1
  %261 = load <2 x i64>, <2 x i64>* %260, align 16
  %262 = shufflevector <2 x i64> %259, <2 x i64> %261, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %263 = getelementptr inbounds <2 x i64>, <2 x i64>* %214, i64 1
  %264 = load <2 x i64>, <2 x i64>* %263, align 16
  %265 = getelementptr inbounds <2 x i64>, <2 x i64>* %217, i64 1
  %266 = load <2 x i64>, <2 x i64>* %265, align 16
  %267 = shufflevector <2 x i64> %264, <2 x i64> %266, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %268 = getelementptr inbounds <2 x i64>, <2 x i64>* %221, i64 1
  %269 = load <2 x i64>, <2 x i64>* %268, align 16
  %270 = getelementptr inbounds <2 x i64>, <2 x i64>* %224, i64 1
  %271 = load <2 x i64>, <2 x i64>* %270, align 16
  %272 = shufflevector <2 x i64> %269, <2 x i64> %271, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %273 = getelementptr inbounds <2 x i64>, <2 x i64>* %228, i64 1
  %274 = load <2 x i64>, <2 x i64>* %273, align 16
  %275 = getelementptr inbounds <2 x i64>, <2 x i64>* %231, i64 1
  %276 = load <2 x i64>, <2 x i64>* %275, align 16
  %277 = shufflevector <2 x i64> %274, <2 x i64> %276, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %278 = getelementptr inbounds <2 x i64>, <2 x i64>* %235, i64 1
  %279 = load <2 x i64>, <2 x i64>* %278, align 16
  %280 = getelementptr inbounds <2 x i64>, <2 x i64>* %238, i64 1
  %281 = load <2 x i64>, <2 x i64>* %280, align 16
  %282 = shufflevector <2 x i64> %279, <2 x i64> %281, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %283 = getelementptr inbounds <2 x i64>, <2 x i64>* %242, i64 1
  %284 = load <2 x i64>, <2 x i64>* %283, align 16
  %285 = getelementptr inbounds <2 x i64>, <2 x i64>* %245, i64 1
  %286 = load <2 x i64>, <2 x i64>* %285, align 16
  %287 = shufflevector <2 x i64> %284, <2 x i64> %286, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %288 = bitcast <4 x i64> %198 to <16 x i16>
  %289 = bitcast <4 x i64> %205 to <16 x i16>
  %290 = shufflevector <16 x i16> %288, <16 x i16> %289, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %291 = shufflevector <16 x i16> %288, <16 x i16> %289, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %292 = bitcast <4 x i64> %212 to <16 x i16>
  %293 = bitcast <4 x i64> %219 to <16 x i16>
  %294 = shufflevector <16 x i16> %292, <16 x i16> %293, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %295 = shufflevector <16 x i16> %292, <16 x i16> %293, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %296 = bitcast <4 x i64> %226 to <16 x i16>
  %297 = bitcast <4 x i64> %233 to <16 x i16>
  %298 = shufflevector <16 x i16> %296, <16 x i16> %297, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %299 = shufflevector <16 x i16> %296, <16 x i16> %297, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %300 = bitcast <4 x i64> %240 to <16 x i16>
  %301 = bitcast <4 x i64> %247 to <16 x i16>
  %302 = shufflevector <16 x i16> %300, <16 x i16> %301, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %303 = shufflevector <16 x i16> %300, <16 x i16> %301, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %304 = bitcast <16 x i16> %290 to <8 x i32>
  %305 = bitcast <16 x i16> %294 to <8 x i32>
  %306 = shufflevector <8 x i32> %304, <8 x i32> %305, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %307 = shufflevector <8 x i32> %304, <8 x i32> %305, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %308 = bitcast <16 x i16> %298 to <8 x i32>
  %309 = bitcast <16 x i16> %302 to <8 x i32>
  %310 = shufflevector <8 x i32> %308, <8 x i32> %309, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %311 = shufflevector <8 x i32> %308, <8 x i32> %309, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %312 = bitcast <16 x i16> %291 to <8 x i32>
  %313 = bitcast <16 x i16> %295 to <8 x i32>
  %314 = shufflevector <8 x i32> %312, <8 x i32> %313, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %315 = shufflevector <8 x i32> %312, <8 x i32> %313, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %316 = bitcast <16 x i16> %299 to <8 x i32>
  %317 = bitcast <16 x i16> %303 to <8 x i32>
  %318 = shufflevector <8 x i32> %316, <8 x i32> %317, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %319 = shufflevector <8 x i32> %316, <8 x i32> %317, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %320 = bitcast <8 x i32> %306 to <4 x i64>
  %321 = bitcast <8 x i32> %310 to <4 x i64>
  %322 = shufflevector <4 x i64> %320, <4 x i64> %321, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %322, <4 x i64>* %161, align 32
  %323 = shufflevector <4 x i64> %320, <4 x i64> %321, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %323, <4 x i64>* %199, align 32
  %324 = bitcast <8 x i32> %314 to <4 x i64>
  %325 = bitcast <8 x i32> %318 to <4 x i64>
  %326 = shufflevector <4 x i64> %324, <4 x i64> %325, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %326, <4 x i64>* %220, align 32
  %327 = shufflevector <4 x i64> %324, <4 x i64> %325, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %327, <4 x i64>* %227, align 32
  %328 = bitcast <8 x i32> %307 to <4 x i64>
  %329 = bitcast <8 x i32> %311 to <4 x i64>
  %330 = shufflevector <4 x i64> %328, <4 x i64> %329, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %330, <4 x i64>* %206, align 32
  %331 = shufflevector <4 x i64> %328, <4 x i64> %329, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %331, <4 x i64>* %213, align 32
  %332 = bitcast <8 x i32> %315 to <4 x i64>
  %333 = bitcast <8 x i32> %319 to <4 x i64>
  %334 = shufflevector <4 x i64> %332, <4 x i64> %333, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %334, <4 x i64>* %234, align 32
  %335 = shufflevector <4 x i64> %332, <4 x i64> %333, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %335, <4 x i64>* %241, align 32
  %336 = bitcast <4 x i64> %252 to <16 x i16>
  %337 = bitcast <4 x i64> %257 to <16 x i16>
  %338 = shufflevector <16 x i16> %336, <16 x i16> %337, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %339 = shufflevector <16 x i16> %336, <16 x i16> %337, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %340 = bitcast <4 x i64> %262 to <16 x i16>
  %341 = bitcast <4 x i64> %267 to <16 x i16>
  %342 = shufflevector <16 x i16> %340, <16 x i16> %341, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %343 = shufflevector <16 x i16> %340, <16 x i16> %341, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %344 = bitcast <4 x i64> %272 to <16 x i16>
  %345 = bitcast <4 x i64> %277 to <16 x i16>
  %346 = shufflevector <16 x i16> %344, <16 x i16> %345, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %347 = shufflevector <16 x i16> %344, <16 x i16> %345, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %348 = bitcast <4 x i64> %282 to <16 x i16>
  %349 = bitcast <4 x i64> %287 to <16 x i16>
  %350 = shufflevector <16 x i16> %348, <16 x i16> %349, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %351 = shufflevector <16 x i16> %348, <16 x i16> %349, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %352 = bitcast <16 x i16> %338 to <8 x i32>
  %353 = bitcast <16 x i16> %342 to <8 x i32>
  %354 = shufflevector <8 x i32> %352, <8 x i32> %353, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %355 = shufflevector <8 x i32> %352, <8 x i32> %353, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %356 = bitcast <16 x i16> %346 to <8 x i32>
  %357 = bitcast <16 x i16> %350 to <8 x i32>
  %358 = shufflevector <8 x i32> %356, <8 x i32> %357, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %359 = shufflevector <8 x i32> %356, <8 x i32> %357, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %360 = bitcast <16 x i16> %339 to <8 x i32>
  %361 = bitcast <16 x i16> %343 to <8 x i32>
  %362 = shufflevector <8 x i32> %360, <8 x i32> %361, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %363 = shufflevector <8 x i32> %360, <8 x i32> %361, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %364 = bitcast <16 x i16> %347 to <8 x i32>
  %365 = bitcast <16 x i16> %351 to <8 x i32>
  %366 = shufflevector <8 x i32> %364, <8 x i32> %365, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %367 = shufflevector <8 x i32> %364, <8 x i32> %365, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %368 = bitcast <8 x i32> %354 to <4 x i64>
  %369 = bitcast <8 x i32> %358 to <4 x i64>
  %370 = shufflevector <4 x i64> %368, <4 x i64> %369, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %370, <4 x i64>* %195, align 32
  %371 = shufflevector <4 x i64> %368, <4 x i64> %369, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %371, <4 x i64>* %202, align 32
  %372 = bitcast <8 x i32> %362 to <4 x i64>
  %373 = bitcast <8 x i32> %366 to <4 x i64>
  %374 = shufflevector <4 x i64> %372, <4 x i64> %373, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %374, <4 x i64>* %223, align 32
  %375 = shufflevector <4 x i64> %372, <4 x i64> %373, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %375, <4 x i64>* %230, align 32
  %376 = bitcast <8 x i32> %355 to <4 x i64>
  %377 = bitcast <8 x i32> %359 to <4 x i64>
  %378 = shufflevector <4 x i64> %376, <4 x i64> %377, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %378, <4 x i64>* %209, align 32
  %379 = shufflevector <4 x i64> %376, <4 x i64> %377, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %379, <4 x i64>* %216, align 32
  %380 = bitcast <8 x i32> %363 to <4 x i64>
  %381 = bitcast <8 x i32> %367 to <4 x i64>
  %382 = shufflevector <4 x i64> %380, <4 x i64> %381, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %382, <4 x i64>* %237, align 32
  %383 = shufflevector <4 x i64> %380, <4 x i64> %381, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %383, <4 x i64>* %244, align 32
  %384 = add nuw nsw i64 %159, 1
  %385 = icmp eq i64 %384, %131
  br i1 %385, label %156, label %158

386:                                              ; preds = %157, %386
  %387 = phi i64 [ %408, %386 ], [ 0, %157 ]
  %388 = phi i64 [ %409, %386 ], [ %135, %157 ]
  %389 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %10, i64 0, i64 %387
  %390 = bitcast <4 x i64>* %389 to <16 x i16>*
  %391 = load <16 x i16>, <16 x i16>* %390, align 32
  %392 = call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %391, <16 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  store <16 x i16> %392, <16 x i16>* %390, align 32
  %393 = or i64 %387, 1
  %394 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %10, i64 0, i64 %393
  %395 = bitcast <4 x i64>* %394 to <16 x i16>*
  %396 = load <16 x i16>, <16 x i16>* %395, align 32
  %397 = call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %396, <16 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  store <16 x i16> %397, <16 x i16>* %395, align 32
  %398 = or i64 %387, 2
  %399 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %10, i64 0, i64 %398
  %400 = bitcast <4 x i64>* %399 to <16 x i16>*
  %401 = load <16 x i16>, <16 x i16>* %400, align 32
  %402 = call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %401, <16 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  store <16 x i16> %402, <16 x i16>* %400, align 32
  %403 = or i64 %387, 3
  %404 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %10, i64 0, i64 %403
  %405 = bitcast <4 x i64>* %404 to <16 x i16>*
  %406 = load <16 x i16>, <16 x i16>* %405, align 32
  %407 = call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %406, <16 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  store <16 x i16> %407, <16 x i16>* %405, align 32
  %408 = add nuw nsw i64 %387, 4
  %409 = add i64 %388, -4
  %410 = icmp eq i64 %409, 0
  br i1 %410, label %411, label %386

411:                                              ; preds = %386, %157
  %412 = phi i64 [ 0, %157 ], [ %408, %386 ]
  br i1 %136, label %423, label %413

413:                                              ; preds = %411, %413
  %414 = phi i64 [ %420, %413 ], [ %412, %411 ]
  %415 = phi i64 [ %421, %413 ], [ %133, %411 ]
  %416 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %10, i64 0, i64 %414
  %417 = bitcast <4 x i64>* %416 to <16 x i16>*
  %418 = load <16 x i16>, <16 x i16>* %417, align 32
  %419 = call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %418, <16 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  store <16 x i16> %419, <16 x i16>* %417, align 32
  %420 = add nuw nsw i64 %414, 1
  %421 = add i64 %415, -1
  %422 = icmp eq i64 %421, 0
  br i1 %422, label %423, label %413, !llvm.loop !2

423:                                              ; preds = %411, %413, %156
  call void %88(<4 x i64>* nonnull %122, <4 x i64>* nonnull %122, i8 signext %46) #9
  br label %428

424:                                              ; preds = %428
  %425 = getelementptr inbounds [1024 x <4 x i64>], [1024 x <4 x i64>]* %9, i64 0, i64 %153
  br i1 %123, label %427, label %426

426:                                              ; preds = %424
  br i1 %126, label %850, label %436

427:                                              ; preds = %424
  br i1 %126, label %850, label %636

428:                                              ; preds = %428, %423
  %429 = phi i64 [ 0, %423 ], [ %434, %428 ]
  %430 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %10, i64 0, i64 %429
  %431 = bitcast <4 x i64>* %430 to <16 x i16>*
  %432 = load <16 x i16>, <16 x i16>* %431, align 32
  %433 = call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %432, <16 x i16> %113) #9
  store <16 x i16> %433, <16 x i16>* %431, align 32
  %434 = add nuw nsw i64 %429, 1
  %435 = icmp slt i64 %434, %127
  br i1 %435, label %428, label %424

436:                                              ; preds = %426, %436
  %437 = phi i64 [ %634, %436 ], [ 0, %426 ]
  %438 = shl nsw i64 %437, 4
  %439 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %10, i64 0, i64 %438
  %440 = load <4 x i64>, <4 x i64>* %439, align 32
  %441 = getelementptr inbounds <4 x i64>, <4 x i64>* %439, i64 1
  %442 = load <4 x i64>, <4 x i64>* %441, align 32
  %443 = getelementptr inbounds <4 x i64>, <4 x i64>* %439, i64 2
  %444 = load <4 x i64>, <4 x i64>* %443, align 32
  %445 = getelementptr inbounds <4 x i64>, <4 x i64>* %439, i64 3
  %446 = load <4 x i64>, <4 x i64>* %445, align 32
  %447 = getelementptr inbounds <4 x i64>, <4 x i64>* %439, i64 4
  %448 = load <4 x i64>, <4 x i64>* %447, align 32
  %449 = getelementptr inbounds <4 x i64>, <4 x i64>* %439, i64 5
  %450 = load <4 x i64>, <4 x i64>* %449, align 32
  %451 = getelementptr inbounds <4 x i64>, <4 x i64>* %439, i64 6
  %452 = load <4 x i64>, <4 x i64>* %451, align 32
  %453 = getelementptr inbounds <4 x i64>, <4 x i64>* %439, i64 7
  %454 = load <4 x i64>, <4 x i64>* %453, align 32
  %455 = getelementptr inbounds <4 x i64>, <4 x i64>* %439, i64 8
  %456 = load <4 x i64>, <4 x i64>* %455, align 32
  %457 = getelementptr inbounds <4 x i64>, <4 x i64>* %439, i64 9
  %458 = load <4 x i64>, <4 x i64>* %457, align 32
  %459 = getelementptr inbounds <4 x i64>, <4 x i64>* %439, i64 10
  %460 = load <4 x i64>, <4 x i64>* %459, align 32
  %461 = getelementptr inbounds <4 x i64>, <4 x i64>* %439, i64 11
  %462 = load <4 x i64>, <4 x i64>* %461, align 32
  %463 = getelementptr inbounds <4 x i64>, <4 x i64>* %439, i64 12
  %464 = load <4 x i64>, <4 x i64>* %463, align 32
  %465 = getelementptr inbounds <4 x i64>, <4 x i64>* %439, i64 13
  %466 = load <4 x i64>, <4 x i64>* %465, align 32
  %467 = getelementptr inbounds <4 x i64>, <4 x i64>* %439, i64 14
  %468 = load <4 x i64>, <4 x i64>* %467, align 32
  %469 = getelementptr inbounds <4 x i64>, <4 x i64>* %439, i64 15
  %470 = load <4 x i64>, <4 x i64>* %469, align 32
  %471 = xor i64 %437, -1
  %472 = add nsw i64 %471, %128
  %473 = mul nsw i64 %472, %129
  %474 = getelementptr inbounds <4 x i64>, <4 x i64>* %425, i64 %473
  %475 = shufflevector <4 x i64> %470, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %476 = shufflevector <4 x i64> %454, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %477 = shufflevector <2 x i64> %475, <2 x i64> %476, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %478 = shufflevector <4 x i64> %468, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %479 = shufflevector <4 x i64> %452, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %480 = shufflevector <2 x i64> %478, <2 x i64> %479, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %481 = shufflevector <4 x i64> %466, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %482 = shufflevector <4 x i64> %450, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %483 = shufflevector <2 x i64> %481, <2 x i64> %482, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %484 = shufflevector <4 x i64> %464, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %485 = shufflevector <4 x i64> %448, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %486 = shufflevector <2 x i64> %484, <2 x i64> %485, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %487 = shufflevector <4 x i64> %462, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %488 = shufflevector <4 x i64> %446, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %489 = shufflevector <2 x i64> %487, <2 x i64> %488, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %490 = shufflevector <4 x i64> %460, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %491 = shufflevector <4 x i64> %444, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %492 = shufflevector <2 x i64> %490, <2 x i64> %491, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %493 = shufflevector <4 x i64> %458, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %494 = shufflevector <4 x i64> %442, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %495 = shufflevector <2 x i64> %493, <2 x i64> %494, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %496 = shufflevector <4 x i64> %456, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %497 = shufflevector <4 x i64> %440, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %498 = shufflevector <2 x i64> %496, <2 x i64> %497, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %499 = shufflevector <4 x i64> %470, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %500 = shufflevector <4 x i64> %454, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %501 = shufflevector <2 x i64> %499, <2 x i64> %500, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %502 = shufflevector <4 x i64> %468, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %503 = shufflevector <4 x i64> %452, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %504 = shufflevector <2 x i64> %502, <2 x i64> %503, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %505 = shufflevector <4 x i64> %466, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %506 = shufflevector <4 x i64> %450, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %507 = shufflevector <2 x i64> %505, <2 x i64> %506, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %508 = shufflevector <4 x i64> %464, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %509 = shufflevector <4 x i64> %448, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %510 = shufflevector <2 x i64> %508, <2 x i64> %509, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %511 = shufflevector <4 x i64> %462, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %512 = shufflevector <4 x i64> %446, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %513 = shufflevector <2 x i64> %511, <2 x i64> %512, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %514 = shufflevector <4 x i64> %460, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %515 = shufflevector <4 x i64> %444, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %516 = shufflevector <2 x i64> %514, <2 x i64> %515, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %517 = shufflevector <4 x i64> %458, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %518 = shufflevector <4 x i64> %442, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %519 = shufflevector <2 x i64> %517, <2 x i64> %518, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %520 = shufflevector <4 x i64> %456, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %521 = shufflevector <4 x i64> %440, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %522 = shufflevector <2 x i64> %520, <2 x i64> %521, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %523 = bitcast <4 x i64> %477 to <16 x i16>
  %524 = bitcast <4 x i64> %480 to <16 x i16>
  %525 = shufflevector <16 x i16> %523, <16 x i16> %524, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %526 = shufflevector <16 x i16> %523, <16 x i16> %524, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %527 = bitcast <4 x i64> %483 to <16 x i16>
  %528 = bitcast <4 x i64> %486 to <16 x i16>
  %529 = shufflevector <16 x i16> %527, <16 x i16> %528, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %530 = shufflevector <16 x i16> %527, <16 x i16> %528, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %531 = bitcast <4 x i64> %489 to <16 x i16>
  %532 = bitcast <4 x i64> %492 to <16 x i16>
  %533 = shufflevector <16 x i16> %531, <16 x i16> %532, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %534 = shufflevector <16 x i16> %531, <16 x i16> %532, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %535 = bitcast <4 x i64> %495 to <16 x i16>
  %536 = bitcast <4 x i64> %498 to <16 x i16>
  %537 = shufflevector <16 x i16> %535, <16 x i16> %536, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %538 = shufflevector <16 x i16> %535, <16 x i16> %536, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %539 = bitcast <16 x i16> %525 to <8 x i32>
  %540 = bitcast <16 x i16> %529 to <8 x i32>
  %541 = shufflevector <8 x i32> %539, <8 x i32> %540, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %542 = shufflevector <8 x i32> %539, <8 x i32> %540, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %543 = bitcast <16 x i16> %533 to <8 x i32>
  %544 = bitcast <16 x i16> %537 to <8 x i32>
  %545 = shufflevector <8 x i32> %543, <8 x i32> %544, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %546 = shufflevector <8 x i32> %543, <8 x i32> %544, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %547 = bitcast <16 x i16> %526 to <8 x i32>
  %548 = bitcast <16 x i16> %530 to <8 x i32>
  %549 = shufflevector <8 x i32> %547, <8 x i32> %548, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %550 = shufflevector <8 x i32> %547, <8 x i32> %548, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %551 = bitcast <16 x i16> %534 to <8 x i32>
  %552 = bitcast <16 x i16> %538 to <8 x i32>
  %553 = shufflevector <8 x i32> %551, <8 x i32> %552, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %554 = shufflevector <8 x i32> %551, <8 x i32> %552, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %555 = bitcast <8 x i32> %541 to <4 x i64>
  %556 = bitcast <8 x i32> %545 to <4 x i64>
  %557 = shufflevector <4 x i64> %555, <4 x i64> %556, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %557, <4 x i64>* %474, align 32
  %558 = shufflevector <4 x i64> %555, <4 x i64> %556, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %559 = getelementptr inbounds <4 x i64>, <4 x i64>* %474, i64 1
  store <4 x i64> %558, <4 x i64>* %559, align 32
  %560 = bitcast <8 x i32> %549 to <4 x i64>
  %561 = bitcast <8 x i32> %553 to <4 x i64>
  %562 = shufflevector <4 x i64> %560, <4 x i64> %561, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %563 = getelementptr inbounds <4 x i64>, <4 x i64>* %474, i64 4
  store <4 x i64> %562, <4 x i64>* %563, align 32
  %564 = shufflevector <4 x i64> %560, <4 x i64> %561, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %565 = getelementptr inbounds <4 x i64>, <4 x i64>* %474, i64 5
  store <4 x i64> %564, <4 x i64>* %565, align 32
  %566 = bitcast <8 x i32> %542 to <4 x i64>
  %567 = bitcast <8 x i32> %546 to <4 x i64>
  %568 = shufflevector <4 x i64> %566, <4 x i64> %567, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %569 = getelementptr inbounds <4 x i64>, <4 x i64>* %474, i64 2
  store <4 x i64> %568, <4 x i64>* %569, align 32
  %570 = shufflevector <4 x i64> %566, <4 x i64> %567, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %571 = getelementptr inbounds <4 x i64>, <4 x i64>* %474, i64 3
  store <4 x i64> %570, <4 x i64>* %571, align 32
  %572 = bitcast <8 x i32> %550 to <4 x i64>
  %573 = bitcast <8 x i32> %554 to <4 x i64>
  %574 = shufflevector <4 x i64> %572, <4 x i64> %573, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %575 = getelementptr inbounds <4 x i64>, <4 x i64>* %474, i64 6
  store <4 x i64> %574, <4 x i64>* %575, align 32
  %576 = shufflevector <4 x i64> %572, <4 x i64> %573, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %577 = getelementptr inbounds <4 x i64>, <4 x i64>* %474, i64 7
  store <4 x i64> %576, <4 x i64>* %577, align 32
  %578 = getelementptr inbounds <4 x i64>, <4 x i64>* %474, i64 8
  %579 = bitcast <4 x i64> %501 to <16 x i16>
  %580 = bitcast <4 x i64> %504 to <16 x i16>
  %581 = shufflevector <16 x i16> %579, <16 x i16> %580, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %582 = shufflevector <16 x i16> %579, <16 x i16> %580, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %583 = bitcast <4 x i64> %507 to <16 x i16>
  %584 = bitcast <4 x i64> %510 to <16 x i16>
  %585 = shufflevector <16 x i16> %583, <16 x i16> %584, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %586 = shufflevector <16 x i16> %583, <16 x i16> %584, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %587 = bitcast <4 x i64> %513 to <16 x i16>
  %588 = bitcast <4 x i64> %516 to <16 x i16>
  %589 = shufflevector <16 x i16> %587, <16 x i16> %588, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %590 = shufflevector <16 x i16> %587, <16 x i16> %588, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %591 = bitcast <4 x i64> %519 to <16 x i16>
  %592 = bitcast <4 x i64> %522 to <16 x i16>
  %593 = shufflevector <16 x i16> %591, <16 x i16> %592, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %594 = shufflevector <16 x i16> %591, <16 x i16> %592, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %595 = bitcast <16 x i16> %581 to <8 x i32>
  %596 = bitcast <16 x i16> %585 to <8 x i32>
  %597 = shufflevector <8 x i32> %595, <8 x i32> %596, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %598 = shufflevector <8 x i32> %595, <8 x i32> %596, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %599 = bitcast <16 x i16> %589 to <8 x i32>
  %600 = bitcast <16 x i16> %593 to <8 x i32>
  %601 = shufflevector <8 x i32> %599, <8 x i32> %600, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %602 = shufflevector <8 x i32> %599, <8 x i32> %600, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %603 = bitcast <16 x i16> %582 to <8 x i32>
  %604 = bitcast <16 x i16> %586 to <8 x i32>
  %605 = shufflevector <8 x i32> %603, <8 x i32> %604, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %606 = shufflevector <8 x i32> %603, <8 x i32> %604, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %607 = bitcast <16 x i16> %590 to <8 x i32>
  %608 = bitcast <16 x i16> %594 to <8 x i32>
  %609 = shufflevector <8 x i32> %607, <8 x i32> %608, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %610 = shufflevector <8 x i32> %607, <8 x i32> %608, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %611 = bitcast <8 x i32> %597 to <4 x i64>
  %612 = bitcast <8 x i32> %601 to <4 x i64>
  %613 = shufflevector <4 x i64> %611, <4 x i64> %612, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %613, <4 x i64>* %578, align 32
  %614 = shufflevector <4 x i64> %611, <4 x i64> %612, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %615 = getelementptr inbounds <4 x i64>, <4 x i64>* %474, i64 9
  store <4 x i64> %614, <4 x i64>* %615, align 32
  %616 = bitcast <8 x i32> %605 to <4 x i64>
  %617 = bitcast <8 x i32> %609 to <4 x i64>
  %618 = shufflevector <4 x i64> %616, <4 x i64> %617, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %619 = getelementptr inbounds <4 x i64>, <4 x i64>* %474, i64 12
  store <4 x i64> %618, <4 x i64>* %619, align 32
  %620 = shufflevector <4 x i64> %616, <4 x i64> %617, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %621 = getelementptr inbounds <4 x i64>, <4 x i64>* %474, i64 13
  store <4 x i64> %620, <4 x i64>* %621, align 32
  %622 = bitcast <8 x i32> %598 to <4 x i64>
  %623 = bitcast <8 x i32> %602 to <4 x i64>
  %624 = shufflevector <4 x i64> %622, <4 x i64> %623, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %625 = getelementptr inbounds <4 x i64>, <4 x i64>* %474, i64 10
  store <4 x i64> %624, <4 x i64>* %625, align 32
  %626 = shufflevector <4 x i64> %622, <4 x i64> %623, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %627 = getelementptr inbounds <4 x i64>, <4 x i64>* %474, i64 11
  store <4 x i64> %626, <4 x i64>* %627, align 32
  %628 = bitcast <8 x i32> %606 to <4 x i64>
  %629 = bitcast <8 x i32> %610 to <4 x i64>
  %630 = shufflevector <4 x i64> %628, <4 x i64> %629, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %631 = getelementptr inbounds <4 x i64>, <4 x i64>* %474, i64 14
  store <4 x i64> %630, <4 x i64>* %631, align 32
  %632 = shufflevector <4 x i64> %628, <4 x i64> %629, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %633 = getelementptr inbounds <4 x i64>, <4 x i64>* %474, i64 15
  store <4 x i64> %632, <4 x i64>* %633, align 32
  %634 = add nuw nsw i64 %437, 1
  %635 = icmp slt i64 %634, %128
  br i1 %635, label %436, label %850

636:                                              ; preds = %427, %636
  %637 = phi i64 [ %848, %636 ], [ 0, %427 ]
  %638 = shl nsw i64 %637, 4
  %639 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %10, i64 0, i64 %638
  %640 = mul nsw i64 %637, %129
  %641 = getelementptr inbounds <4 x i64>, <4 x i64>* %425, i64 %640
  %642 = bitcast <4 x i64>* %639 to <2 x i64>*
  %643 = load <2 x i64>, <2 x i64>* %642, align 32
  %644 = getelementptr inbounds <4 x i64>, <4 x i64>* %639, i64 8
  %645 = bitcast <4 x i64>* %644 to <2 x i64>*
  %646 = load <2 x i64>, <2 x i64>* %645, align 32
  %647 = shufflevector <2 x i64> %643, <2 x i64> %646, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %648 = getelementptr inbounds <4 x i64>, <4 x i64>* %639, i64 1
  %649 = bitcast <4 x i64>* %648 to <2 x i64>*
  %650 = load <2 x i64>, <2 x i64>* %649, align 32
  %651 = getelementptr inbounds <4 x i64>, <4 x i64>* %639, i64 9
  %652 = bitcast <4 x i64>* %651 to <2 x i64>*
  %653 = load <2 x i64>, <2 x i64>* %652, align 32
  %654 = shufflevector <2 x i64> %650, <2 x i64> %653, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %655 = getelementptr inbounds <4 x i64>, <4 x i64>* %639, i64 2
  %656 = bitcast <4 x i64>* %655 to <2 x i64>*
  %657 = load <2 x i64>, <2 x i64>* %656, align 32
  %658 = getelementptr inbounds <4 x i64>, <4 x i64>* %639, i64 10
  %659 = bitcast <4 x i64>* %658 to <2 x i64>*
  %660 = load <2 x i64>, <2 x i64>* %659, align 32
  %661 = shufflevector <2 x i64> %657, <2 x i64> %660, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %662 = getelementptr inbounds <4 x i64>, <4 x i64>* %639, i64 3
  %663 = bitcast <4 x i64>* %662 to <2 x i64>*
  %664 = load <2 x i64>, <2 x i64>* %663, align 32
  %665 = getelementptr inbounds <4 x i64>, <4 x i64>* %639, i64 11
  %666 = bitcast <4 x i64>* %665 to <2 x i64>*
  %667 = load <2 x i64>, <2 x i64>* %666, align 32
  %668 = shufflevector <2 x i64> %664, <2 x i64> %667, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %669 = getelementptr inbounds <4 x i64>, <4 x i64>* %639, i64 4
  %670 = bitcast <4 x i64>* %669 to <2 x i64>*
  %671 = load <2 x i64>, <2 x i64>* %670, align 32
  %672 = getelementptr inbounds <4 x i64>, <4 x i64>* %639, i64 12
  %673 = bitcast <4 x i64>* %672 to <2 x i64>*
  %674 = load <2 x i64>, <2 x i64>* %673, align 32
  %675 = shufflevector <2 x i64> %671, <2 x i64> %674, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %676 = getelementptr inbounds <4 x i64>, <4 x i64>* %639, i64 5
  %677 = bitcast <4 x i64>* %676 to <2 x i64>*
  %678 = load <2 x i64>, <2 x i64>* %677, align 32
  %679 = getelementptr inbounds <4 x i64>, <4 x i64>* %639, i64 13
  %680 = bitcast <4 x i64>* %679 to <2 x i64>*
  %681 = load <2 x i64>, <2 x i64>* %680, align 32
  %682 = shufflevector <2 x i64> %678, <2 x i64> %681, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %683 = getelementptr inbounds <4 x i64>, <4 x i64>* %639, i64 6
  %684 = bitcast <4 x i64>* %683 to <2 x i64>*
  %685 = load <2 x i64>, <2 x i64>* %684, align 32
  %686 = getelementptr inbounds <4 x i64>, <4 x i64>* %639, i64 14
  %687 = bitcast <4 x i64>* %686 to <2 x i64>*
  %688 = load <2 x i64>, <2 x i64>* %687, align 32
  %689 = shufflevector <2 x i64> %685, <2 x i64> %688, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %690 = getelementptr inbounds <4 x i64>, <4 x i64>* %639, i64 7
  %691 = bitcast <4 x i64>* %690 to <2 x i64>*
  %692 = load <2 x i64>, <2 x i64>* %691, align 32
  %693 = getelementptr inbounds <4 x i64>, <4 x i64>* %639, i64 15
  %694 = bitcast <4 x i64>* %693 to <2 x i64>*
  %695 = load <2 x i64>, <2 x i64>* %694, align 32
  %696 = shufflevector <2 x i64> %692, <2 x i64> %695, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %697 = getelementptr inbounds <2 x i64>, <2 x i64>* %642, i64 1
  %698 = load <2 x i64>, <2 x i64>* %697, align 16
  %699 = getelementptr inbounds <2 x i64>, <2 x i64>* %645, i64 1
  %700 = load <2 x i64>, <2 x i64>* %699, align 16
  %701 = shufflevector <2 x i64> %698, <2 x i64> %700, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %702 = getelementptr inbounds <2 x i64>, <2 x i64>* %649, i64 1
  %703 = load <2 x i64>, <2 x i64>* %702, align 16
  %704 = getelementptr inbounds <2 x i64>, <2 x i64>* %652, i64 1
  %705 = load <2 x i64>, <2 x i64>* %704, align 16
  %706 = shufflevector <2 x i64> %703, <2 x i64> %705, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %707 = getelementptr inbounds <2 x i64>, <2 x i64>* %656, i64 1
  %708 = load <2 x i64>, <2 x i64>* %707, align 16
  %709 = getelementptr inbounds <2 x i64>, <2 x i64>* %659, i64 1
  %710 = load <2 x i64>, <2 x i64>* %709, align 16
  %711 = shufflevector <2 x i64> %708, <2 x i64> %710, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %712 = getelementptr inbounds <2 x i64>, <2 x i64>* %663, i64 1
  %713 = load <2 x i64>, <2 x i64>* %712, align 16
  %714 = getelementptr inbounds <2 x i64>, <2 x i64>* %666, i64 1
  %715 = load <2 x i64>, <2 x i64>* %714, align 16
  %716 = shufflevector <2 x i64> %713, <2 x i64> %715, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %717 = getelementptr inbounds <2 x i64>, <2 x i64>* %670, i64 1
  %718 = load <2 x i64>, <2 x i64>* %717, align 16
  %719 = getelementptr inbounds <2 x i64>, <2 x i64>* %673, i64 1
  %720 = load <2 x i64>, <2 x i64>* %719, align 16
  %721 = shufflevector <2 x i64> %718, <2 x i64> %720, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %722 = getelementptr inbounds <2 x i64>, <2 x i64>* %677, i64 1
  %723 = load <2 x i64>, <2 x i64>* %722, align 16
  %724 = getelementptr inbounds <2 x i64>, <2 x i64>* %680, i64 1
  %725 = load <2 x i64>, <2 x i64>* %724, align 16
  %726 = shufflevector <2 x i64> %723, <2 x i64> %725, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %727 = getelementptr inbounds <2 x i64>, <2 x i64>* %684, i64 1
  %728 = load <2 x i64>, <2 x i64>* %727, align 16
  %729 = getelementptr inbounds <2 x i64>, <2 x i64>* %687, i64 1
  %730 = load <2 x i64>, <2 x i64>* %729, align 16
  %731 = shufflevector <2 x i64> %728, <2 x i64> %730, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %732 = getelementptr inbounds <2 x i64>, <2 x i64>* %691, i64 1
  %733 = load <2 x i64>, <2 x i64>* %732, align 16
  %734 = getelementptr inbounds <2 x i64>, <2 x i64>* %694, i64 1
  %735 = load <2 x i64>, <2 x i64>* %734, align 16
  %736 = shufflevector <2 x i64> %733, <2 x i64> %735, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %737 = bitcast <4 x i64> %647 to <16 x i16>
  %738 = bitcast <4 x i64> %654 to <16 x i16>
  %739 = shufflevector <16 x i16> %737, <16 x i16> %738, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %740 = shufflevector <16 x i16> %737, <16 x i16> %738, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %741 = bitcast <4 x i64> %661 to <16 x i16>
  %742 = bitcast <4 x i64> %668 to <16 x i16>
  %743 = shufflevector <16 x i16> %741, <16 x i16> %742, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %744 = shufflevector <16 x i16> %741, <16 x i16> %742, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %745 = bitcast <4 x i64> %675 to <16 x i16>
  %746 = bitcast <4 x i64> %682 to <16 x i16>
  %747 = shufflevector <16 x i16> %745, <16 x i16> %746, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %748 = shufflevector <16 x i16> %745, <16 x i16> %746, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %749 = bitcast <4 x i64> %689 to <16 x i16>
  %750 = bitcast <4 x i64> %696 to <16 x i16>
  %751 = shufflevector <16 x i16> %749, <16 x i16> %750, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %752 = shufflevector <16 x i16> %749, <16 x i16> %750, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %753 = bitcast <16 x i16> %739 to <8 x i32>
  %754 = bitcast <16 x i16> %743 to <8 x i32>
  %755 = shufflevector <8 x i32> %753, <8 x i32> %754, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %756 = shufflevector <8 x i32> %753, <8 x i32> %754, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %757 = bitcast <16 x i16> %747 to <8 x i32>
  %758 = bitcast <16 x i16> %751 to <8 x i32>
  %759 = shufflevector <8 x i32> %757, <8 x i32> %758, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %760 = shufflevector <8 x i32> %757, <8 x i32> %758, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %761 = bitcast <16 x i16> %740 to <8 x i32>
  %762 = bitcast <16 x i16> %744 to <8 x i32>
  %763 = shufflevector <8 x i32> %761, <8 x i32> %762, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %764 = shufflevector <8 x i32> %761, <8 x i32> %762, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %765 = bitcast <16 x i16> %748 to <8 x i32>
  %766 = bitcast <16 x i16> %752 to <8 x i32>
  %767 = shufflevector <8 x i32> %765, <8 x i32> %766, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %768 = shufflevector <8 x i32> %765, <8 x i32> %766, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %769 = bitcast <8 x i32> %755 to <4 x i64>
  %770 = bitcast <8 x i32> %759 to <4 x i64>
  %771 = shufflevector <4 x i64> %769, <4 x i64> %770, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %771, <4 x i64>* %641, align 32
  %772 = shufflevector <4 x i64> %769, <4 x i64> %770, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %773 = getelementptr inbounds <4 x i64>, <4 x i64>* %641, i64 1
  store <4 x i64> %772, <4 x i64>* %773, align 32
  %774 = bitcast <8 x i32> %763 to <4 x i64>
  %775 = bitcast <8 x i32> %767 to <4 x i64>
  %776 = shufflevector <4 x i64> %774, <4 x i64> %775, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %777 = getelementptr inbounds <4 x i64>, <4 x i64>* %641, i64 4
  store <4 x i64> %776, <4 x i64>* %777, align 32
  %778 = shufflevector <4 x i64> %774, <4 x i64> %775, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %779 = getelementptr inbounds <4 x i64>, <4 x i64>* %641, i64 5
  store <4 x i64> %778, <4 x i64>* %779, align 32
  %780 = bitcast <8 x i32> %756 to <4 x i64>
  %781 = bitcast <8 x i32> %760 to <4 x i64>
  %782 = shufflevector <4 x i64> %780, <4 x i64> %781, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %783 = getelementptr inbounds <4 x i64>, <4 x i64>* %641, i64 2
  store <4 x i64> %782, <4 x i64>* %783, align 32
  %784 = shufflevector <4 x i64> %780, <4 x i64> %781, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %785 = getelementptr inbounds <4 x i64>, <4 x i64>* %641, i64 3
  store <4 x i64> %784, <4 x i64>* %785, align 32
  %786 = bitcast <8 x i32> %764 to <4 x i64>
  %787 = bitcast <8 x i32> %768 to <4 x i64>
  %788 = shufflevector <4 x i64> %786, <4 x i64> %787, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %789 = getelementptr inbounds <4 x i64>, <4 x i64>* %641, i64 6
  store <4 x i64> %788, <4 x i64>* %789, align 32
  %790 = shufflevector <4 x i64> %786, <4 x i64> %787, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %791 = getelementptr inbounds <4 x i64>, <4 x i64>* %641, i64 7
  store <4 x i64> %790, <4 x i64>* %791, align 32
  %792 = getelementptr inbounds <4 x i64>, <4 x i64>* %641, i64 8
  %793 = bitcast <4 x i64> %701 to <16 x i16>
  %794 = bitcast <4 x i64> %706 to <16 x i16>
  %795 = shufflevector <16 x i16> %793, <16 x i16> %794, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %796 = shufflevector <16 x i16> %793, <16 x i16> %794, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %797 = bitcast <4 x i64> %711 to <16 x i16>
  %798 = bitcast <4 x i64> %716 to <16 x i16>
  %799 = shufflevector <16 x i16> %797, <16 x i16> %798, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %800 = shufflevector <16 x i16> %797, <16 x i16> %798, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %801 = bitcast <4 x i64> %721 to <16 x i16>
  %802 = bitcast <4 x i64> %726 to <16 x i16>
  %803 = shufflevector <16 x i16> %801, <16 x i16> %802, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %804 = shufflevector <16 x i16> %801, <16 x i16> %802, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %805 = bitcast <4 x i64> %731 to <16 x i16>
  %806 = bitcast <4 x i64> %736 to <16 x i16>
  %807 = shufflevector <16 x i16> %805, <16 x i16> %806, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %808 = shufflevector <16 x i16> %805, <16 x i16> %806, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %809 = bitcast <16 x i16> %795 to <8 x i32>
  %810 = bitcast <16 x i16> %799 to <8 x i32>
  %811 = shufflevector <8 x i32> %809, <8 x i32> %810, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %812 = shufflevector <8 x i32> %809, <8 x i32> %810, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %813 = bitcast <16 x i16> %803 to <8 x i32>
  %814 = bitcast <16 x i16> %807 to <8 x i32>
  %815 = shufflevector <8 x i32> %813, <8 x i32> %814, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %816 = shufflevector <8 x i32> %813, <8 x i32> %814, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %817 = bitcast <16 x i16> %796 to <8 x i32>
  %818 = bitcast <16 x i16> %800 to <8 x i32>
  %819 = shufflevector <8 x i32> %817, <8 x i32> %818, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %820 = shufflevector <8 x i32> %817, <8 x i32> %818, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %821 = bitcast <16 x i16> %804 to <8 x i32>
  %822 = bitcast <16 x i16> %808 to <8 x i32>
  %823 = shufflevector <8 x i32> %821, <8 x i32> %822, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %824 = shufflevector <8 x i32> %821, <8 x i32> %822, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %825 = bitcast <8 x i32> %811 to <4 x i64>
  %826 = bitcast <8 x i32> %815 to <4 x i64>
  %827 = shufflevector <4 x i64> %825, <4 x i64> %826, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %827, <4 x i64>* %792, align 32
  %828 = shufflevector <4 x i64> %825, <4 x i64> %826, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %829 = getelementptr inbounds <4 x i64>, <4 x i64>* %641, i64 9
  store <4 x i64> %828, <4 x i64>* %829, align 32
  %830 = bitcast <8 x i32> %819 to <4 x i64>
  %831 = bitcast <8 x i32> %823 to <4 x i64>
  %832 = shufflevector <4 x i64> %830, <4 x i64> %831, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %833 = getelementptr inbounds <4 x i64>, <4 x i64>* %641, i64 12
  store <4 x i64> %832, <4 x i64>* %833, align 32
  %834 = shufflevector <4 x i64> %830, <4 x i64> %831, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %835 = getelementptr inbounds <4 x i64>, <4 x i64>* %641, i64 13
  store <4 x i64> %834, <4 x i64>* %835, align 32
  %836 = bitcast <8 x i32> %812 to <4 x i64>
  %837 = bitcast <8 x i32> %816 to <4 x i64>
  %838 = shufflevector <4 x i64> %836, <4 x i64> %837, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %839 = getelementptr inbounds <4 x i64>, <4 x i64>* %641, i64 10
  store <4 x i64> %838, <4 x i64>* %839, align 32
  %840 = shufflevector <4 x i64> %836, <4 x i64> %837, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %841 = getelementptr inbounds <4 x i64>, <4 x i64>* %641, i64 11
  store <4 x i64> %840, <4 x i64>* %841, align 32
  %842 = bitcast <8 x i32> %820 to <4 x i64>
  %843 = bitcast <8 x i32> %824 to <4 x i64>
  %844 = shufflevector <4 x i64> %842, <4 x i64> %843, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %845 = getelementptr inbounds <4 x i64>, <4 x i64>* %641, i64 14
  store <4 x i64> %844, <4 x i64>* %845, align 32
  %846 = shufflevector <4 x i64> %842, <4 x i64> %843, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %847 = getelementptr inbounds <4 x i64>, <4 x i64>* %641, i64 15
  store <4 x i64> %846, <4 x i64>* %847, align 32
  %848 = add nuw nsw i64 %637, 1
  %849 = icmp slt i64 %848, %128
  br i1 %849, label %636, label %850

850:                                              ; preds = %436, %636, %427, %426
  call void @llvm.lifetime.end.p0i8(i64 2048, i8* nonnull %119) #9
  %851 = add nuw nsw i64 %152, 1
  %852 = icmp slt i64 %851, %130
  br i1 %852, label %151, label %137

853:                                              ; preds = %868
  %854 = icmp ne i32 %105, 0
  %855 = select i1 %854, i64 -1, i64 1
  %856 = add nsw i32 %50, -1
  %857 = select i1 %854, i32 %856, i32 0
  %858 = sext i32 %2 to i64
  %859 = sext i32 %857 to i64
  %860 = zext i32 %50 to i64
  %861 = and i64 %860, 1
  %862 = sub nsw i64 %860, %861
  %863 = icmp eq i64 %861, 0
  br label %879

864:                                              ; preds = %868, %148
  %865 = phi i64 [ 0, %148 ], [ %869, %868 ]
  %866 = mul nsw i64 %865, %149
  %867 = getelementptr inbounds [1024 x <4 x i64>], [1024 x <4 x i64>]* %9, i64 0, i64 %866
  call void %94(<4 x i64>* %867, <4 x i64>* %867, i8 signext %44) #9
  br label %871

868:                                              ; preds = %871
  %869 = add nuw nsw i64 %865, 1
  %870 = icmp slt i64 %869, %150
  br i1 %870, label %864, label %853

871:                                              ; preds = %871, %864
  %872 = phi i64 [ 0, %864 ], [ %877, %871 ]
  %873 = getelementptr inbounds <4 x i64>, <4 x i64>* %867, i64 %872
  %874 = bitcast <4 x i64>* %873 to <16 x i16>*
  %875 = load <16 x i16>, <16 x i16>* %874, align 32
  %876 = call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %875, <16 x i16> %146) #9
  store <16 x i16> %876, <16 x i16>* %874, align 32
  %877 = add nuw nsw i64 %872, 1
  %878 = icmp slt i64 %877, %149
  br i1 %878, label %871, label %868

879:                                              ; preds = %936, %853
  %880 = phi i64 [ 0, %853 ], [ %937, %936 ]
  %881 = mul nsw i64 %880, %149
  %882 = getelementptr inbounds [1024 x <4 x i64>], [1024 x <4 x i64>]* %9, i64 0, i64 %881
  %883 = shl nsw i64 %880, 4
  %884 = getelementptr inbounds i8, i8* %1, i64 %883
  br label %885

885:                                              ; preds = %885, %879
  %886 = phi i64 [ %859, %879 ], [ %918, %885 ]
  %887 = phi i64 [ 0, %879 ], [ %917, %885 ]
  %888 = phi i64 [ %862, %879 ], [ %919, %885 ]
  %889 = getelementptr inbounds <4 x i64>, <4 x i64>* %882, i64 %886
  %890 = bitcast <4 x i64>* %889 to <16 x i16>*
  %891 = load <16 x i16>, <16 x i16>* %890, align 32
  %892 = mul nsw i64 %887, %858
  %893 = getelementptr inbounds i8, i8* %884, i64 %892
  %894 = bitcast i8* %893 to <2 x i64>*
  %895 = bitcast i8* %893 to <16 x i8>*
  %896 = load <16 x i8>, <16 x i8>* %895, align 1
  %897 = zext <16 x i8> %896 to <16 x i16>
  %898 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %897, <16 x i16> %891) #9
  %899 = call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %898, <16 x i16> undef) #9
  %900 = bitcast <32 x i8> %899 to <4 x i64>
  %901 = shufflevector <4 x i64> %900, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %901, <2 x i64>* %894, align 1
  %902 = or i64 %887, 1
  %903 = add i64 %886, %855
  %904 = getelementptr inbounds <4 x i64>, <4 x i64>* %882, i64 %903
  %905 = bitcast <4 x i64>* %904 to <16 x i16>*
  %906 = load <16 x i16>, <16 x i16>* %905, align 32
  %907 = mul nsw i64 %902, %858
  %908 = getelementptr inbounds i8, i8* %884, i64 %907
  %909 = bitcast i8* %908 to <2 x i64>*
  %910 = bitcast i8* %908 to <16 x i8>*
  %911 = load <16 x i8>, <16 x i8>* %910, align 1
  %912 = zext <16 x i8> %911 to <16 x i16>
  %913 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %912, <16 x i16> %906) #9
  %914 = call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %913, <16 x i16> undef) #9
  %915 = bitcast <32 x i8> %914 to <4 x i64>
  %916 = shufflevector <4 x i64> %915, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %916, <2 x i64>* %909, align 1
  %917 = add nuw nsw i64 %887, 2
  %918 = add i64 %903, %855
  %919 = add i64 %888, -2
  %920 = icmp eq i64 %919, 0
  br i1 %920, label %921, label %885

921:                                              ; preds = %885
  br i1 %863, label %936, label %922

922:                                              ; preds = %921
  %923 = getelementptr inbounds <4 x i64>, <4 x i64>* %882, i64 %918
  %924 = bitcast <4 x i64>* %923 to <16 x i16>*
  %925 = load <16 x i16>, <16 x i16>* %924, align 32
  %926 = mul nsw i64 %917, %858
  %927 = getelementptr inbounds i8, i8* %884, i64 %926
  %928 = bitcast i8* %927 to <2 x i64>*
  %929 = bitcast i8* %927 to <16 x i8>*
  %930 = load <16 x i8>, <16 x i8>* %929, align 1
  %931 = zext <16 x i8> %930 to <16 x i16>
  %932 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %931, <16 x i16> %925) #9
  %933 = call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %932, <16 x i16> undef) #9
  %934 = bitcast <32 x i8> %933 to <4 x i64>
  %935 = shufflevector <4 x i64> %934, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %935, <2 x i64>* %928, align 1
  br label %936

936:                                              ; preds = %921, %922
  %937 = add nuw nsw i64 %880, 1
  %938 = icmp slt i64 %937, %150
  br i1 %938, label %879, label %939

939:                                              ; preds = %936, %137
  call void @llvm.lifetime.end.p0i8(i64 32768, i8* nonnull %14) #9
  br label %2237

940:                                              ; preds = %12
  %941 = zext i8 %4 to i64
  %942 = getelementptr inbounds [19 x i8*], [19 x i8*]* @av1_inv_txfm_shift_ls, i64 0, i64 %941
  %943 = load i8*, i8** %942, align 8
  %944 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_wide_log2, i64 0, i64 %941
  %945 = load i32, i32* %944, align 4
  %946 = add nsw i32 %945, -2
  %947 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_high_log2, i64 0, i64 %941
  %948 = load i32, i32* %947, align 4
  %949 = add nsw i32 %948, -2
  %950 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_wide, i64 0, i64 %941
  %951 = load i32, i32* %950, align 4
  %952 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_high, i64 0, i64 %941
  %953 = load i32, i32* %952, align 4
  %954 = icmp slt i32 %951, 32
  %955 = select i1 %954, i32 %951, i32 32
  %956 = icmp slt i32 %953, 32
  %957 = select i1 %956, i32 %953, i32 32
  %958 = icmp eq i32 %951, %953
  br i1 %958, label %974, label %959

959:                                              ; preds = %940
  %960 = icmp sgt i32 %951, %953
  br i1 %960, label %961, label %967

961:                                              ; preds = %959
  %962 = shl nsw i32 %953, 1
  %963 = icmp eq i32 %962, %951
  br i1 %963, label %974, label %964

964:                                              ; preds = %961
  %965 = shl nsw i32 %953, 2
  %966 = icmp eq i32 %965, %951
  br i1 %966, label %974, label %973

967:                                              ; preds = %959
  %968 = shl nsw i32 %951, 1
  %969 = icmp eq i32 %968, %953
  br i1 %969, label %974, label %970

970:                                              ; preds = %967
  %971 = shl nsw i32 %951, 2
  %972 = icmp eq i32 %971, %953
  br i1 %972, label %974, label %973

973:                                              ; preds = %970, %964
  br label %974

974:                                              ; preds = %973, %970, %967, %964, %961, %940
  %975 = phi i32 [ 0, %973 ], [ 0, %940 ], [ 1, %961 ], [ 2, %964 ], [ -1, %967 ], [ -2, %970 ]
  %976 = bitcast [32 x <4 x i64>]* %8 to i8*
  call void @llvm.lifetime.start.p0i8(i64 1024, i8* nonnull %976) #9
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %976, i8 -86, i64 1024, i1 false) #9
  %977 = sext i32 %946 to i64
  %978 = getelementptr inbounds [5 x i32], [5 x i32]* @NewSqrt2list, i64 0, i64 %977
  %979 = load i32, i32* %978, align 4
  %980 = trunc i32 %979 to i16
  %981 = insertelement <16 x i16> undef, i16 %980, i32 0
  %982 = shufflevector <16 x i16> %981, <16 x i16> undef, <16 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %983 = sext i32 %955 to i64
  %984 = zext i32 %957 to i64
  %985 = getelementptr inbounds i8, i8* %943, i64 1
  %986 = sext i32 %949 to i64
  %987 = getelementptr inbounds [5 x i32], [5 x i32]* @NewSqrt2list, i64 0, i64 %986
  %988 = load i32, i32* %987, align 4
  %989 = trunc i32 %988 to i16
  %990 = insertelement <16 x i16> undef, i16 %989, i32 0
  %991 = shufflevector <16 x i16> %990, <16 x i16> undef, <16 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %992 = shufflevector <16 x i16> %991, <16 x i16> <i16 2048, i16 2048, i16 2048, i16 2048, i16 undef, i16 undef, i16 undef, i16 undef, i16 2048, i16 2048, i16 2048, i16 2048, i16 undef, i16 undef, i16 undef, i16 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %993 = sext i32 %2 to i64
  br label %994

994:                                              ; preds = %1096, %974
  %995 = phi i64 [ 0, %974 ], [ %1097, %1096 ]
  %996 = getelementptr inbounds i32, i32* %0, i64 %995
  %997 = load i8, i8* %943, align 1
  %998 = sext i8 %997 to i32
  %999 = sub nsw i32 12, %998
  %1000 = sub nsw i32 11, %998
  %1001 = shl i32 1, %1000
  %1002 = trunc i32 %1001 to i16
  %1003 = add i16 %1002, 2048
  %1004 = insertelement <16 x i16> undef, i16 %1003, i32 0
  %1005 = shufflevector <16 x i16> %1004, <16 x i16> undef, <16 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %1006 = shufflevector <16 x i16> %982, <16 x i16> %1005, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  switch i32 %975, label %1008 [
    i32 -1, label %1007
    i32 1, label %1007
  ]

1007:                                             ; preds = %994, %994
  br label %1033

1008:                                             ; preds = %994, %1008
  %1009 = phi i64 [ %1031, %1008 ], [ 0, %994 ]
  %1010 = phi i32* [ %1020, %1008 ], [ %996, %994 ]
  %1011 = bitcast i32* %1010 to i8*
  %1012 = tail call <32 x i8> @llvm.x86.avx.ldu.dq.256(i8* %1011) #9
  %1013 = getelementptr inbounds i32, i32* %1010, i64 8
  %1014 = bitcast i32* %1013 to <8 x i32>*
  %1015 = load <8 x i32>, <8 x i32>* %1014, align 32
  %1016 = bitcast <32 x i8> %1012 to <8 x i32>
  %1017 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1016, <8 x i32> %1015) #9
  %1018 = bitcast <16 x i16> %1017 to <4 x i64>
  %1019 = shufflevector <4 x i64> %1018, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %1020 = getelementptr inbounds i32, i32* %1010, i64 %983
  %1021 = bitcast <4 x i64> %1019 to <16 x i16>
  %1022 = shufflevector <16 x i16> %1021, <16 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1023 = shufflevector <16 x i16> %1021, <16 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1024 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1022, <16 x i16> %1006) #9
  %1025 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1023, <16 x i16> %1006) #9
  %1026 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1024, i32 %999) #9
  %1027 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1025, i32 %999) #9
  %1028 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1026, <8 x i32> %1027) #9
  %1029 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %8, i64 0, i64 %1009
  %1030 = bitcast <4 x i64>* %1029 to <16 x i16>*
  store <16 x i16> %1028, <16 x i16>* %1030, align 32
  %1031 = add nuw nsw i64 %1009, 1
  %1032 = icmp eq i64 %1031, %984
  br i1 %1032, label %1059, label %1008

1033:                                             ; preds = %1007, %1033
  %1034 = phi i64 [ %1057, %1033 ], [ 0, %1007 ]
  %1035 = phi i32* [ %1047, %1033 ], [ %996, %1007 ]
  %1036 = bitcast i32* %1035 to i8*
  %1037 = tail call <32 x i8> @llvm.x86.avx.ldu.dq.256(i8* %1036) #9
  %1038 = getelementptr inbounds i32, i32* %1035, i64 8
  %1039 = bitcast i32* %1038 to <8 x i32>*
  %1040 = load <8 x i32>, <8 x i32>* %1039, align 32
  %1041 = bitcast <32 x i8> %1037 to <8 x i32>
  %1042 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1041, <8 x i32> %1040) #9
  %1043 = bitcast <16 x i16> %1042 to <4 x i64>
  %1044 = shufflevector <4 x i64> %1043, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %1045 = bitcast <4 x i64> %1044 to <16 x i16>
  %1046 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %1045, <16 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  %1047 = getelementptr inbounds i32, i32* %1035, i64 %983
  %1048 = shufflevector <16 x i16> %1046, <16 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1049 = shufflevector <16 x i16> %1046, <16 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1050 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1048, <16 x i16> %1006) #9
  %1051 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1049, <16 x i16> %1006) #9
  %1052 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1050, i32 %999) #9
  %1053 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1051, i32 %999) #9
  %1054 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1052, <8 x i32> %1053) #9
  %1055 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %8, i64 0, i64 %1034
  %1056 = bitcast <4 x i64>* %1055 to <16 x i16>*
  store <16 x i16> %1054, <16 x i16>* %1056, align 32
  %1057 = add nuw nsw i64 %1034, 1
  %1058 = icmp eq i64 %1057, %984
  br i1 %1058, label %1059, label %1033

1059:                                             ; preds = %1033, %1008
  %1060 = load i8, i8* %985, align 1
  %1061 = sext i8 %1060 to i32
  %1062 = sub nsw i32 0, %1061
  %1063 = xor i32 %1061, -1
  %1064 = shl i32 1, %1063
  %1065 = insertelement <8 x i32> undef, i32 %1064, i32 0
  %1066 = shufflevector <8 x i32> %1065, <8 x i32> undef, <8 x i32> zeroinitializer
  %1067 = getelementptr inbounds i8, i8* %1, i64 %995
  br label %1068

1068:                                             ; preds = %1068, %1059
  %1069 = phi i64 [ 0, %1059 ], [ %1094, %1068 ]
  %1070 = phi i8* [ %1067, %1059 ], [ %1093, %1068 ]
  %1071 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %8, i64 0, i64 %1069
  %1072 = bitcast <4 x i64>* %1071 to <16 x i16>*
  %1073 = load <16 x i16>, <16 x i16>* %1072, align 32
  %1074 = shufflevector <16 x i16> %1073, <16 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1075 = shufflevector <16 x i16> %1073, <16 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1076 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1074, <16 x i16> %992) #9
  %1077 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1075, <16 x i16> %992) #9
  %1078 = ashr <8 x i32> %1076, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %1079 = ashr <8 x i32> %1077, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %1080 = add <8 x i32> %1078, %1066
  %1081 = add <8 x i32> %1079, %1066
  %1082 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1080, i32 %1062) #9
  %1083 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1081, i32 %1062) #9
  %1084 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1082, <8 x i32> %1083) #9
  %1085 = bitcast i8* %1070 to <2 x i64>*
  %1086 = bitcast i8* %1070 to <16 x i8>*
  %1087 = load <16 x i8>, <16 x i8>* %1086, align 1
  %1088 = zext <16 x i8> %1087 to <16 x i16>
  %1089 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1088, <16 x i16> %1084) #9
  %1090 = tail call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %1089, <16 x i16> undef) #9
  %1091 = bitcast <32 x i8> %1090 to <4 x i64>
  %1092 = shufflevector <4 x i64> %1091, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1092, <2 x i64>* %1085, align 1
  %1093 = getelementptr inbounds i8, i8* %1070, i64 %993
  %1094 = add nuw nsw i64 %1069, 1
  %1095 = icmp eq i64 %1094, %984
  br i1 %1095, label %1096, label %1068

1096:                                             ; preds = %1068
  %1097 = add nuw nsw i64 %995, 16
  %1098 = icmp slt i64 %1097, %983
  br i1 %1098, label %994, label %1099

1099:                                             ; preds = %1096
  call void @llvm.lifetime.end.p0i8(i64 1024, i8* nonnull %976) #9
  br label %2237

1100:                                             ; preds = %12, %12, %12
  %1101 = add nsw i32 %5, -1
  %1102 = zext i8 %4 to i64
  %1103 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_wide, i64 0, i64 %1102
  %1104 = load i32, i32* %1103, align 4
  %1105 = icmp slt i32 %1104, 32
  %1106 = select i1 %1105, i32 %1104, i32 32
  %1107 = icmp sgt i32 %1106, %5
  br i1 %1107, label %1110, label %1108

1108:                                             ; preds = %1100
  %1109 = add nsw i32 %1106, -1
  br label %1114

1110:                                             ; preds = %1100
  %1111 = sext i32 %1101 to i64
  %1112 = getelementptr inbounds [32 x i32], [32 x i32]* @eob_fill, i64 0, i64 %1111
  %1113 = load i32, i32* %1112, align 4
  br label %1114

1114:                                             ; preds = %1110, %1108
  %1115 = phi i32 [ %1109, %1108 ], [ %1113, %1110 ]
  %1116 = sdiv i32 %1101, %1106
  %1117 = sext i32 %1116 to i64
  %1118 = getelementptr inbounds [32 x i32], [32 x i32]* @eob_fill, i64 0, i64 %1117
  %1119 = load i32, i32* %1118, align 4
  %1120 = getelementptr inbounds [19 x i8*], [19 x i8*]* @av1_inv_txfm_shift_ls, i64 0, i64 %1102
  %1121 = load i8*, i8** %1120, align 8
  %1122 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_wide_log2, i64 0, i64 %1102
  %1123 = load i32, i32* %1122, align 4
  %1124 = add nsw i32 %1123, -2
  %1125 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_high_log2, i64 0, i64 %1102
  %1126 = load i32, i32* %1125, align 4
  %1127 = add nsw i32 %1126, -2
  %1128 = sext i32 %1124 to i64
  %1129 = sext i32 %1127 to i64
  %1130 = getelementptr inbounds [5 x [5 x i8]], [5 x [5 x i8]]* @av1_inv_cos_bit_col, i64 0, i64 %1128, i64 %1129
  %1131 = load i8, i8* %1130, align 1
  %1132 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_high, i64 0, i64 %1102
  %1133 = load i32, i32* %1132, align 4
  %1134 = add nsw i32 %1115, 16
  %1135 = ashr i32 %1134, 4
  %1136 = icmp eq i32 %1104, %1133
  br i1 %1136, label %1152, label %1137

1137:                                             ; preds = %1114
  %1138 = icmp sgt i32 %1104, %1133
  br i1 %1138, label %1139, label %1145

1139:                                             ; preds = %1137
  %1140 = shl nsw i32 %1133, 1
  %1141 = icmp eq i32 %1140, %1104
  br i1 %1141, label %1152, label %1142

1142:                                             ; preds = %1139
  %1143 = shl nsw i32 %1133, 2
  %1144 = icmp eq i32 %1143, %1104
  br i1 %1144, label %1152, label %1151

1145:                                             ; preds = %1137
  %1146 = shl nsw i32 %1104, 1
  %1147 = icmp eq i32 %1146, %1133
  br i1 %1147, label %1152, label %1148

1148:                                             ; preds = %1145
  %1149 = shl nsw i32 %1104, 2
  %1150 = icmp eq i32 %1149, %1133
  br i1 %1150, label %1152, label %1151

1151:                                             ; preds = %1148, %1142
  br label %1152

1152:                                             ; preds = %1151, %1148, %1145, %1142, %1139, %1114
  %1153 = phi i32 [ 0, %1151 ], [ 0, %1114 ], [ 1, %1139 ], [ 2, %1142 ], [ -1, %1145 ], [ -2, %1148 ]
  %1154 = sext i32 %1119 to i64
  %1155 = getelementptr inbounds [32 x i32], [32 x i32]* @lowbd_txfm_all_1d_zeros_idx, i64 0, i64 %1154
  %1156 = load i32, i32* %1155, align 4
  %1157 = zext i8 %3 to i64
  %1158 = getelementptr inbounds [16 x i8], [16 x i8]* @vitx_1d_tab, i64 0, i64 %1157
  %1159 = load i8, i8* %1158, align 1
  %1160 = zext i8 %1159 to i64
  %1161 = sext i32 %1156 to i64
  %1162 = getelementptr inbounds [5 x [3 x [4 x void (<4 x i64>*, <4 x i64>*, i8)*]]], [5 x [3 x [4 x void (<4 x i64>*, <4 x i64>*, i8)*]]]* @lowbd_txfm_all_1d_zeros_w16_arr, i64 0, i64 %1129, i64 %1160, i64 %1161
  %1163 = load void (<4 x i64>*, <4 x i64>*, i8)*, void (<4 x i64>*, <4 x i64>*, i8)** %1162, align 8
  %1164 = add nsw i8 %3, -4
  %1165 = lshr i8 %1164, 1
  %1166 = shl i8 %1164, 7
  %1167 = or i8 %1165, %1166
  %1168 = icmp ult i8 %1167, 6
  br i1 %1168, label %1169, label %1173

1169:                                             ; preds = %1152
  %1170 = sext i8 %1167 to i64
  %1171 = getelementptr inbounds [6 x i32], [6 x i32]* @switch.table.av1_lowbd_inv_txfm2d_add_avx2.2, i64 0, i64 %1170
  %1172 = load i32, i32* %1171, align 4
  br label %1173

1173:                                             ; preds = %1169, %1152
  %1174 = phi i32 [ 0, %1152 ], [ %1172, %1169 ]
  %1175 = icmp sgt i32 %1134, 15
  br i1 %1175, label %1176, label %2237

1176:                                             ; preds = %1173
  %1177 = bitcast [64 x <4 x i64>]* %10 to i8*
  %1178 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %10, i64 0, i64 0
  %1179 = add nsw i32 %1119, 1
  %1180 = getelementptr inbounds [5 x i32], [5 x i32]* @NewSqrt2list, i64 0, i64 %1128
  %1181 = load i32, i32* %1180, align 4
  %1182 = trunc i32 %1181 to i16
  %1183 = insertelement <16 x i16> undef, i16 %1182, i32 0
  %1184 = shufflevector <16 x i16> %1183, <16 x i16> undef, <16 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %1185 = sext i32 %1106 to i64
  %1186 = zext i32 %1179 to i64
  %1187 = getelementptr inbounds i8, i8* %1121, i64 1
  %1188 = icmp ne i32 %1174, 0
  %1189 = add nsw i32 %1133, -1
  %1190 = select i1 %1188, i32 %1189, i32 0
  %1191 = select i1 %1188, i64 -1, i64 1
  %1192 = sext i32 %2 to i64
  %1193 = sext i32 %1133 to i64
  %1194 = sext i32 %1190 to i64
  %1195 = sext i32 %1135 to i64
  br label %1196

1196:                                             ; preds = %1271, %1176
  %1197 = phi i64 [ 0, %1176 ], [ %1272, %1271 ]
  call void @llvm.lifetime.start.p0i8(i64 2048, i8* nonnull %1177) #9
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %1177, i8 -86, i64 2048, i1 false) #9
  %1198 = shl i64 %1197, 4
  %1199 = getelementptr inbounds i32, i32* %0, i64 %1198
  %1200 = load i8, i8* %1121, align 1
  %1201 = sext i8 %1200 to i32
  %1202 = sub nsw i32 12, %1201
  %1203 = sub nsw i32 11, %1201
  %1204 = shl i32 1, %1203
  %1205 = trunc i32 %1204 to i16
  %1206 = add i16 %1205, 2048
  %1207 = insertelement <16 x i16> undef, i16 %1206, i32 0
  %1208 = shufflevector <16 x i16> %1207, <16 x i16> undef, <16 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %1209 = shufflevector <16 x i16> %1184, <16 x i16> %1208, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  switch i32 %1153, label %1211 [
    i32 -1, label %1210
    i32 1, label %1210
  ]

1210:                                             ; preds = %1196, %1196
  br label %1236

1211:                                             ; preds = %1196, %1211
  %1212 = phi i64 [ %1234, %1211 ], [ 0, %1196 ]
  %1213 = phi i32* [ %1223, %1211 ], [ %1199, %1196 ]
  %1214 = bitcast i32* %1213 to i8*
  %1215 = call <32 x i8> @llvm.x86.avx.ldu.dq.256(i8* %1214) #9
  %1216 = getelementptr inbounds i32, i32* %1213, i64 8
  %1217 = bitcast i32* %1216 to <8 x i32>*
  %1218 = load <8 x i32>, <8 x i32>* %1217, align 32
  %1219 = bitcast <32 x i8> %1215 to <8 x i32>
  %1220 = call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1219, <8 x i32> %1218) #9
  %1221 = bitcast <16 x i16> %1220 to <4 x i64>
  %1222 = shufflevector <4 x i64> %1221, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %1223 = getelementptr inbounds i32, i32* %1213, i64 %1185
  %1224 = bitcast <4 x i64> %1222 to <16 x i16>
  %1225 = shufflevector <16 x i16> %1224, <16 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1226 = shufflevector <16 x i16> %1224, <16 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1227 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1225, <16 x i16> %1209) #9
  %1228 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1226, <16 x i16> %1209) #9
  %1229 = call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1227, i32 %1202) #9
  %1230 = call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1228, i32 %1202) #9
  %1231 = call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1229, <8 x i32> %1230) #9
  %1232 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %10, i64 0, i64 %1212
  %1233 = bitcast <4 x i64>* %1232 to <16 x i16>*
  store <16 x i16> %1231, <16 x i16>* %1233, align 32
  %1234 = add nuw nsw i64 %1212, 1
  %1235 = icmp eq i64 %1234, %1186
  br i1 %1235, label %1262, label %1211

1236:                                             ; preds = %1210, %1236
  %1237 = phi i64 [ %1260, %1236 ], [ 0, %1210 ]
  %1238 = phi i32* [ %1250, %1236 ], [ %1199, %1210 ]
  %1239 = bitcast i32* %1238 to i8*
  %1240 = call <32 x i8> @llvm.x86.avx.ldu.dq.256(i8* %1239) #9
  %1241 = getelementptr inbounds i32, i32* %1238, i64 8
  %1242 = bitcast i32* %1241 to <8 x i32>*
  %1243 = load <8 x i32>, <8 x i32>* %1242, align 32
  %1244 = bitcast <32 x i8> %1240 to <8 x i32>
  %1245 = call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1244, <8 x i32> %1243) #9
  %1246 = bitcast <16 x i16> %1245 to <4 x i64>
  %1247 = shufflevector <4 x i64> %1246, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %1248 = bitcast <4 x i64> %1247 to <16 x i16>
  %1249 = call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %1248, <16 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  %1250 = getelementptr inbounds i32, i32* %1238, i64 %1185
  %1251 = shufflevector <16 x i16> %1249, <16 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1252 = shufflevector <16 x i16> %1249, <16 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1253 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1251, <16 x i16> %1209) #9
  %1254 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1252, <16 x i16> %1209) #9
  %1255 = call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1253, i32 %1202) #9
  %1256 = call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1254, i32 %1202) #9
  %1257 = call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1255, <8 x i32> %1256) #9
  %1258 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %10, i64 0, i64 %1237
  %1259 = bitcast <4 x i64>* %1258 to <16 x i16>*
  store <16 x i16> %1257, <16 x i16>* %1259, align 32
  %1260 = add nuw nsw i64 %1237, 1
  %1261 = icmp eq i64 %1260, %1186
  br i1 %1261, label %1262, label %1236

1262:                                             ; preds = %1236, %1211
  call void %1163(<4 x i64>* nonnull %1178, <4 x i64>* nonnull %1178, i8 signext %1131) #9
  %1263 = load i8, i8* %1187, align 1
  %1264 = sext i8 %1263 to i32
  %1265 = add nsw i32 %1264, 15
  %1266 = shl i32 1, %1265
  %1267 = trunc i32 %1266 to i16
  %1268 = insertelement <16 x i16> undef, i16 %1267, i32 0
  %1269 = shufflevector <16 x i16> %1268, <16 x i16> undef, <16 x i32> zeroinitializer
  %1270 = getelementptr inbounds i8, i8* %1, i64 %1198
  br label %1274

1271:                                             ; preds = %1274
  call void @llvm.lifetime.end.p0i8(i64 2048, i8* nonnull %1177) #9
  %1272 = add nuw nsw i64 %1197, 1
  %1273 = icmp slt i64 %1272, %1195
  br i1 %1273, label %1196, label %2237

1274:                                             ; preds = %1274, %1262
  %1275 = phi i64 [ %1194, %1262 ], [ %1292, %1274 ]
  %1276 = phi i64 [ 0, %1262 ], [ %1291, %1274 ]
  %1277 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %10, i64 0, i64 %1275
  %1278 = bitcast <4 x i64>* %1277 to <16 x i16>*
  %1279 = load <16 x i16>, <16 x i16>* %1278, align 32
  %1280 = call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %1279, <16 x i16> %1269) #9
  %1281 = mul nsw i64 %1276, %1192
  %1282 = getelementptr inbounds i8, i8* %1270, i64 %1281
  %1283 = bitcast i8* %1282 to <2 x i64>*
  %1284 = bitcast i8* %1282 to <16 x i8>*
  %1285 = load <16 x i8>, <16 x i8>* %1284, align 1
  %1286 = zext <16 x i8> %1285 to <16 x i16>
  %1287 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1286, <16 x i16> %1280) #9
  %1288 = call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %1287, <16 x i16> undef) #9
  %1289 = bitcast <32 x i8> %1288 to <4 x i64>
  %1290 = shufflevector <4 x i64> %1289, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %1290, <2 x i64>* %1283, align 1
  %1291 = add nuw nsw i64 %1276, 1
  %1292 = add i64 %1275, %1191
  %1293 = icmp slt i64 %1291, %1193
  br i1 %1293, label %1274, label %1271

1294:                                             ; preds = %12, %12, %12
  %1295 = bitcast [64 x <4 x i64>]* %10 to i8*
  call void @llvm.lifetime.start.p0i8(i64 2048, i8* nonnull %1295) #9
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %1295, i8 -86, i64 2048, i1 false) #9
  %1296 = add nsw i32 %5, -1
  %1297 = zext i8 %4 to i64
  %1298 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_high, i64 0, i64 %1297
  %1299 = load i32, i32* %1298, align 4
  %1300 = icmp slt i32 %1299, 32
  %1301 = select i1 %1300, i32 %1299, i32 32
  %1302 = sdiv i32 %1296, %1301
  %1303 = icmp sgt i32 %1301, %5
  br i1 %1303, label %1306, label %1304

1304:                                             ; preds = %1294
  %1305 = add nsw i32 %1301, -1
  br label %1310

1306:                                             ; preds = %1294
  %1307 = sext i32 %1296 to i64
  %1308 = getelementptr inbounds [32 x i32], [32 x i32]* @eob_fill, i64 0, i64 %1307
  %1309 = load i32, i32* %1308, align 4
  br label %1310

1310:                                             ; preds = %1306, %1304
  %1311 = phi i32 [ %1305, %1304 ], [ %1309, %1306 ]
  %1312 = getelementptr inbounds [19 x i8*], [19 x i8*]* @av1_inv_txfm_shift_ls, i64 0, i64 %1297
  %1313 = load i8*, i8** %1312, align 8
  %1314 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_wide_log2, i64 0, i64 %1297
  %1315 = load i32, i32* %1314, align 4
  %1316 = add nsw i32 %1315, -2
  %1317 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_high_log2, i64 0, i64 %1297
  %1318 = load i32, i32* %1317, align 4
  %1319 = add nsw i32 %1318, -2
  %1320 = sext i32 %1316 to i64
  %1321 = sext i32 %1319 to i64
  %1322 = getelementptr inbounds [5 x [5 x i8]], [5 x [5 x i8]]* @av1_inv_cos_bit_row, i64 0, i64 %1320, i64 %1321
  %1323 = load i8, i8* %1322, align 1
  %1324 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_wide, i64 0, i64 %1297
  %1325 = load i32, i32* %1324, align 4
  %1326 = ashr i32 %1325, 4
  %1327 = add nsw i32 %1311, 16
  %1328 = ashr i32 %1327, 4
  %1329 = icmp slt i32 %1325, 32
  %1330 = select i1 %1329, i32 %1325, i32 32
  %1331 = icmp eq i32 %1325, %1299
  br i1 %1331, label %1347, label %1332

1332:                                             ; preds = %1310
  %1333 = icmp sgt i32 %1325, %1299
  br i1 %1333, label %1334, label %1340

1334:                                             ; preds = %1332
  %1335 = shl nsw i32 %1299, 1
  %1336 = icmp eq i32 %1335, %1325
  br i1 %1336, label %1347, label %1337

1337:                                             ; preds = %1334
  %1338 = shl nsw i32 %1299, 2
  %1339 = icmp eq i32 %1338, %1325
  br i1 %1339, label %1347, label %1346

1340:                                             ; preds = %1332
  %1341 = shl nsw i32 %1325, 1
  %1342 = icmp eq i32 %1341, %1299
  br i1 %1342, label %1347, label %1343

1343:                                             ; preds = %1340
  %1344 = shl nsw i32 %1325, 2
  %1345 = icmp eq i32 %1344, %1299
  br i1 %1345, label %1347, label %1346

1346:                                             ; preds = %1343, %1337
  br label %1347

1347:                                             ; preds = %1346, %1343, %1340, %1337, %1334, %1310
  %1348 = phi i32 [ 0, %1346 ], [ 0, %1310 ], [ 1, %1334 ], [ 2, %1337 ], [ -1, %1340 ], [ -2, %1343 ]
  %1349 = sext i32 %1302 to i64
  %1350 = getelementptr inbounds [32 x i32], [32 x i32]* @lowbd_txfm_all_1d_zeros_idx, i64 0, i64 %1349
  %1351 = load i32, i32* %1350, align 4
  %1352 = zext i8 %3 to i64
  %1353 = getelementptr inbounds [16 x i8], [16 x i8]* @hitx_1d_tab, i64 0, i64 %1352
  %1354 = load i8, i8* %1353, align 1
  %1355 = zext i8 %1354 to i64
  %1356 = sext i32 %1351 to i64
  %1357 = getelementptr inbounds [5 x [3 x [4 x void (<4 x i64>*, <4 x i64>*, i8)*]]], [5 x [3 x [4 x void (<4 x i64>*, <4 x i64>*, i8)*]]]* @lowbd_txfm_all_1d_zeros_w16_arr, i64 0, i64 %1320, i64 %1355, i64 %1356
  %1358 = load void (<4 x i64>*, <4 x i64>*, i8)*, void (<4 x i64>*, <4 x i64>*, i8)** %1357, align 8
  %1359 = icmp sgt i32 %1327, 15
  br i1 %1359, label %1360, label %2235

1360:                                             ; preds = %1347
  %1361 = icmp eq i8 %3, 15
  %1362 = bitcast [64 x <4 x i64>]* %7 to i8*
  %1363 = shl i32 %1330, 4
  %1364 = icmp slt i32 %1326, 4
  %1365 = select i1 %1364, i32 %1326, i32 4
  %1366 = lshr i64 483100, %1297
  %1367 = and i64 %1366, 1
  %1368 = icmp eq i64 %1367, 0
  %1369 = sext i32 %1330 to i64
  %1370 = zext i32 %1330 to i64
  %1371 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 0
  %1372 = zext i32 %1325 to i64
  %1373 = shl i32 %2, 4
  %1374 = getelementptr inbounds i8, i8* %1313, i64 1
  %1375 = getelementptr inbounds [5 x i32], [5 x i32]* @NewSqrt2list, i64 0, i64 %1321
  %1376 = sext i32 %2 to i64
  %1377 = sext i32 %1365 to i64
  %1378 = sext i32 %1326 to i64
  %1379 = sext i32 %1328 to i64
  %1380 = add nsw i64 %1370, -1
  %1381 = add nsw i64 %1372, -1
  %1382 = and i64 %1370, 3
  %1383 = icmp ult i64 %1380, 3
  %1384 = sub nsw i64 %1370, %1382
  %1385 = icmp eq i64 %1382, 0
  %1386 = and i64 %1372, 3
  %1387 = icmp ult i64 %1381, 3
  %1388 = sub nsw i64 %1372, %1386
  %1389 = icmp eq i64 %1386, 0
  %1390 = and i64 %1372, 3
  %1391 = icmp ult i64 %1381, 3
  %1392 = sub nsw i64 %1372, %1390
  %1393 = icmp eq i64 %1390, 0
  br label %1394

1394:                                             ; preds = %2189, %1360
  %1395 = phi i64 [ 0, %1360 ], [ %2190, %2189 ]
  call void @llvm.lifetime.start.p0i8(i64 2048, i8* nonnull %1362) #9
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %1362, i8 -86, i64 2048, i1 false) #9
  %1396 = trunc i64 %1395 to i32
  %1397 = mul i32 %1363, %1396
  %1398 = sext i32 %1397 to i64
  %1399 = getelementptr inbounds i32, i32* %0, i64 %1398
  br i1 %1368, label %1400, label %1402

1400:                                             ; preds = %1437, %1394
  switch i32 %1348, label %1668 [
    i32 -1, label %1401
    i32 1, label %1401
  ]

1401:                                             ; preds = %1400, %1400
  br i1 %1383, label %1656, label %1631

1402:                                             ; preds = %1394, %1437
  %1403 = phi i64 [ %1629, %1437 ], [ 0, %1394 ]
  %1404 = shl i64 %1403, 4
  %1405 = and i64 %1404, 4294967280
  %1406 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %1405
  %1407 = getelementptr inbounds i32, i32* %1399, i64 %1405
  br label %1408

1408:                                             ; preds = %1408, %1402
  %1409 = phi i64 [ 0, %1402 ], [ %1435, %1408 ]
  %1410 = mul nsw i64 %1409, %1369
  %1411 = getelementptr inbounds i32, i32* %1407, i64 %1410
  %1412 = bitcast i32* %1411 to i8*
  %1413 = call <32 x i8> @llvm.x86.avx.ldu.dq.256(i8* %1412) #9
  %1414 = getelementptr inbounds i32, i32* %1411, i64 8
  %1415 = bitcast i32* %1414 to <8 x i32>*
  %1416 = load <8 x i32>, <8 x i32>* %1415, align 32
  %1417 = bitcast <32 x i8> %1413 to <8 x i32>
  %1418 = call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1417, <8 x i32> %1416) #9
  %1419 = bitcast <16 x i16> %1418 to <4 x i64>
  %1420 = shufflevector <4 x i64> %1419, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %1421 = getelementptr inbounds <4 x i64>, <4 x i64>* %1406, i64 %1409
  store <4 x i64> %1420, <4 x i64>* %1421, align 32
  %1422 = or i64 %1409, 1
  %1423 = mul nsw i64 %1422, %1369
  %1424 = getelementptr inbounds i32, i32* %1407, i64 %1423
  %1425 = bitcast i32* %1424 to i8*
  %1426 = call <32 x i8> @llvm.x86.avx.ldu.dq.256(i8* %1425) #9
  %1427 = getelementptr inbounds i32, i32* %1424, i64 8
  %1428 = bitcast i32* %1427 to <8 x i32>*
  %1429 = load <8 x i32>, <8 x i32>* %1428, align 32
  %1430 = bitcast <32 x i8> %1426 to <8 x i32>
  %1431 = call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1430, <8 x i32> %1429) #9
  %1432 = bitcast <16 x i16> %1431 to <4 x i64>
  %1433 = shufflevector <4 x i64> %1432, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %1434 = getelementptr inbounds <4 x i64>, <4 x i64>* %1406, i64 %1422
  store <4 x i64> %1433, <4 x i64>* %1434, align 32
  %1435 = add nuw nsw i64 %1409, 2
  %1436 = icmp eq i64 %1435, 16
  br i1 %1436, label %1437, label %1408

1437:                                             ; preds = %1408
  %1438 = bitcast <4 x i64>* %1406 to <2 x i64>*
  %1439 = load <2 x i64>, <2 x i64>* %1438, align 32
  %1440 = getelementptr inbounds <4 x i64>, <4 x i64>* %1406, i64 8
  %1441 = bitcast <4 x i64>* %1440 to <2 x i64>*
  %1442 = load <2 x i64>, <2 x i64>* %1441, align 32
  %1443 = shufflevector <2 x i64> %1439, <2 x i64> %1442, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1444 = getelementptr inbounds <4 x i64>, <4 x i64>* %1406, i64 1
  %1445 = bitcast <4 x i64>* %1444 to <2 x i64>*
  %1446 = load <2 x i64>, <2 x i64>* %1445, align 32
  %1447 = getelementptr inbounds <4 x i64>, <4 x i64>* %1406, i64 9
  %1448 = bitcast <4 x i64>* %1447 to <2 x i64>*
  %1449 = load <2 x i64>, <2 x i64>* %1448, align 32
  %1450 = shufflevector <2 x i64> %1446, <2 x i64> %1449, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1451 = getelementptr inbounds <4 x i64>, <4 x i64>* %1406, i64 2
  %1452 = bitcast <4 x i64>* %1451 to <2 x i64>*
  %1453 = load <2 x i64>, <2 x i64>* %1452, align 32
  %1454 = getelementptr inbounds <4 x i64>, <4 x i64>* %1406, i64 10
  %1455 = bitcast <4 x i64>* %1454 to <2 x i64>*
  %1456 = load <2 x i64>, <2 x i64>* %1455, align 32
  %1457 = shufflevector <2 x i64> %1453, <2 x i64> %1456, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1458 = getelementptr inbounds <4 x i64>, <4 x i64>* %1406, i64 3
  %1459 = bitcast <4 x i64>* %1458 to <2 x i64>*
  %1460 = load <2 x i64>, <2 x i64>* %1459, align 32
  %1461 = getelementptr inbounds <4 x i64>, <4 x i64>* %1406, i64 11
  %1462 = bitcast <4 x i64>* %1461 to <2 x i64>*
  %1463 = load <2 x i64>, <2 x i64>* %1462, align 32
  %1464 = shufflevector <2 x i64> %1460, <2 x i64> %1463, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1465 = getelementptr inbounds <4 x i64>, <4 x i64>* %1406, i64 4
  %1466 = bitcast <4 x i64>* %1465 to <2 x i64>*
  %1467 = load <2 x i64>, <2 x i64>* %1466, align 32
  %1468 = getelementptr inbounds <4 x i64>, <4 x i64>* %1406, i64 12
  %1469 = bitcast <4 x i64>* %1468 to <2 x i64>*
  %1470 = load <2 x i64>, <2 x i64>* %1469, align 32
  %1471 = shufflevector <2 x i64> %1467, <2 x i64> %1470, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1472 = getelementptr inbounds <4 x i64>, <4 x i64>* %1406, i64 5
  %1473 = bitcast <4 x i64>* %1472 to <2 x i64>*
  %1474 = load <2 x i64>, <2 x i64>* %1473, align 32
  %1475 = getelementptr inbounds <4 x i64>, <4 x i64>* %1406, i64 13
  %1476 = bitcast <4 x i64>* %1475 to <2 x i64>*
  %1477 = load <2 x i64>, <2 x i64>* %1476, align 32
  %1478 = shufflevector <2 x i64> %1474, <2 x i64> %1477, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1479 = getelementptr inbounds <4 x i64>, <4 x i64>* %1406, i64 6
  %1480 = bitcast <4 x i64>* %1479 to <2 x i64>*
  %1481 = load <2 x i64>, <2 x i64>* %1480, align 32
  %1482 = getelementptr inbounds <4 x i64>, <4 x i64>* %1406, i64 14
  %1483 = bitcast <4 x i64>* %1482 to <2 x i64>*
  %1484 = load <2 x i64>, <2 x i64>* %1483, align 32
  %1485 = shufflevector <2 x i64> %1481, <2 x i64> %1484, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1486 = getelementptr inbounds <4 x i64>, <4 x i64>* %1406, i64 7
  %1487 = bitcast <4 x i64>* %1486 to <2 x i64>*
  %1488 = load <2 x i64>, <2 x i64>* %1487, align 32
  %1489 = getelementptr inbounds <4 x i64>, <4 x i64>* %1406, i64 15
  %1490 = bitcast <4 x i64>* %1489 to <2 x i64>*
  %1491 = load <2 x i64>, <2 x i64>* %1490, align 32
  %1492 = shufflevector <2 x i64> %1488, <2 x i64> %1491, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1493 = getelementptr inbounds <2 x i64>, <2 x i64>* %1438, i64 1
  %1494 = load <2 x i64>, <2 x i64>* %1493, align 16
  %1495 = getelementptr inbounds <2 x i64>, <2 x i64>* %1441, i64 1
  %1496 = load <2 x i64>, <2 x i64>* %1495, align 16
  %1497 = shufflevector <2 x i64> %1494, <2 x i64> %1496, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1498 = getelementptr inbounds <2 x i64>, <2 x i64>* %1445, i64 1
  %1499 = load <2 x i64>, <2 x i64>* %1498, align 16
  %1500 = getelementptr inbounds <2 x i64>, <2 x i64>* %1448, i64 1
  %1501 = load <2 x i64>, <2 x i64>* %1500, align 16
  %1502 = shufflevector <2 x i64> %1499, <2 x i64> %1501, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1503 = getelementptr inbounds <2 x i64>, <2 x i64>* %1452, i64 1
  %1504 = load <2 x i64>, <2 x i64>* %1503, align 16
  %1505 = getelementptr inbounds <2 x i64>, <2 x i64>* %1455, i64 1
  %1506 = load <2 x i64>, <2 x i64>* %1505, align 16
  %1507 = shufflevector <2 x i64> %1504, <2 x i64> %1506, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1508 = getelementptr inbounds <2 x i64>, <2 x i64>* %1459, i64 1
  %1509 = load <2 x i64>, <2 x i64>* %1508, align 16
  %1510 = getelementptr inbounds <2 x i64>, <2 x i64>* %1462, i64 1
  %1511 = load <2 x i64>, <2 x i64>* %1510, align 16
  %1512 = shufflevector <2 x i64> %1509, <2 x i64> %1511, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1513 = getelementptr inbounds <2 x i64>, <2 x i64>* %1466, i64 1
  %1514 = load <2 x i64>, <2 x i64>* %1513, align 16
  %1515 = getelementptr inbounds <2 x i64>, <2 x i64>* %1469, i64 1
  %1516 = load <2 x i64>, <2 x i64>* %1515, align 16
  %1517 = shufflevector <2 x i64> %1514, <2 x i64> %1516, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1518 = getelementptr inbounds <2 x i64>, <2 x i64>* %1473, i64 1
  %1519 = load <2 x i64>, <2 x i64>* %1518, align 16
  %1520 = getelementptr inbounds <2 x i64>, <2 x i64>* %1476, i64 1
  %1521 = load <2 x i64>, <2 x i64>* %1520, align 16
  %1522 = shufflevector <2 x i64> %1519, <2 x i64> %1521, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1523 = getelementptr inbounds <2 x i64>, <2 x i64>* %1480, i64 1
  %1524 = load <2 x i64>, <2 x i64>* %1523, align 16
  %1525 = getelementptr inbounds <2 x i64>, <2 x i64>* %1483, i64 1
  %1526 = load <2 x i64>, <2 x i64>* %1525, align 16
  %1527 = shufflevector <2 x i64> %1524, <2 x i64> %1526, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1528 = getelementptr inbounds <2 x i64>, <2 x i64>* %1487, i64 1
  %1529 = load <2 x i64>, <2 x i64>* %1528, align 16
  %1530 = getelementptr inbounds <2 x i64>, <2 x i64>* %1490, i64 1
  %1531 = load <2 x i64>, <2 x i64>* %1530, align 16
  %1532 = shufflevector <2 x i64> %1529, <2 x i64> %1531, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1533 = bitcast <4 x i64> %1443 to <16 x i16>
  %1534 = bitcast <4 x i64> %1450 to <16 x i16>
  %1535 = shufflevector <16 x i16> %1533, <16 x i16> %1534, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1536 = shufflevector <16 x i16> %1533, <16 x i16> %1534, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1537 = bitcast <4 x i64> %1457 to <16 x i16>
  %1538 = bitcast <4 x i64> %1464 to <16 x i16>
  %1539 = shufflevector <16 x i16> %1537, <16 x i16> %1538, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1540 = shufflevector <16 x i16> %1537, <16 x i16> %1538, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1541 = bitcast <4 x i64> %1471 to <16 x i16>
  %1542 = bitcast <4 x i64> %1478 to <16 x i16>
  %1543 = shufflevector <16 x i16> %1541, <16 x i16> %1542, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1544 = shufflevector <16 x i16> %1541, <16 x i16> %1542, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1545 = bitcast <4 x i64> %1485 to <16 x i16>
  %1546 = bitcast <4 x i64> %1492 to <16 x i16>
  %1547 = shufflevector <16 x i16> %1545, <16 x i16> %1546, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1548 = shufflevector <16 x i16> %1545, <16 x i16> %1546, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1549 = bitcast <16 x i16> %1535 to <8 x i32>
  %1550 = bitcast <16 x i16> %1539 to <8 x i32>
  %1551 = shufflevector <8 x i32> %1549, <8 x i32> %1550, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1552 = shufflevector <8 x i32> %1549, <8 x i32> %1550, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1553 = bitcast <16 x i16> %1543 to <8 x i32>
  %1554 = bitcast <16 x i16> %1547 to <8 x i32>
  %1555 = shufflevector <8 x i32> %1553, <8 x i32> %1554, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1556 = shufflevector <8 x i32> %1553, <8 x i32> %1554, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1557 = bitcast <16 x i16> %1536 to <8 x i32>
  %1558 = bitcast <16 x i16> %1540 to <8 x i32>
  %1559 = shufflevector <8 x i32> %1557, <8 x i32> %1558, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1560 = shufflevector <8 x i32> %1557, <8 x i32> %1558, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1561 = bitcast <16 x i16> %1544 to <8 x i32>
  %1562 = bitcast <16 x i16> %1548 to <8 x i32>
  %1563 = shufflevector <8 x i32> %1561, <8 x i32> %1562, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1564 = shufflevector <8 x i32> %1561, <8 x i32> %1562, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1565 = bitcast <8 x i32> %1551 to <4 x i64>
  %1566 = bitcast <8 x i32> %1555 to <4 x i64>
  %1567 = shufflevector <4 x i64> %1565, <4 x i64> %1566, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1567, <4 x i64>* %1406, align 32
  %1568 = shufflevector <4 x i64> %1565, <4 x i64> %1566, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1568, <4 x i64>* %1444, align 32
  %1569 = bitcast <8 x i32> %1559 to <4 x i64>
  %1570 = bitcast <8 x i32> %1563 to <4 x i64>
  %1571 = shufflevector <4 x i64> %1569, <4 x i64> %1570, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1571, <4 x i64>* %1465, align 32
  %1572 = shufflevector <4 x i64> %1569, <4 x i64> %1570, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1572, <4 x i64>* %1472, align 32
  %1573 = bitcast <8 x i32> %1552 to <4 x i64>
  %1574 = bitcast <8 x i32> %1556 to <4 x i64>
  %1575 = shufflevector <4 x i64> %1573, <4 x i64> %1574, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1575, <4 x i64>* %1451, align 32
  %1576 = shufflevector <4 x i64> %1573, <4 x i64> %1574, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1576, <4 x i64>* %1458, align 32
  %1577 = bitcast <8 x i32> %1560 to <4 x i64>
  %1578 = bitcast <8 x i32> %1564 to <4 x i64>
  %1579 = shufflevector <4 x i64> %1577, <4 x i64> %1578, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1579, <4 x i64>* %1479, align 32
  %1580 = shufflevector <4 x i64> %1577, <4 x i64> %1578, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1580, <4 x i64>* %1486, align 32
  %1581 = bitcast <4 x i64> %1497 to <16 x i16>
  %1582 = bitcast <4 x i64> %1502 to <16 x i16>
  %1583 = shufflevector <16 x i16> %1581, <16 x i16> %1582, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1584 = shufflevector <16 x i16> %1581, <16 x i16> %1582, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1585 = bitcast <4 x i64> %1507 to <16 x i16>
  %1586 = bitcast <4 x i64> %1512 to <16 x i16>
  %1587 = shufflevector <16 x i16> %1585, <16 x i16> %1586, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1588 = shufflevector <16 x i16> %1585, <16 x i16> %1586, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1589 = bitcast <4 x i64> %1517 to <16 x i16>
  %1590 = bitcast <4 x i64> %1522 to <16 x i16>
  %1591 = shufflevector <16 x i16> %1589, <16 x i16> %1590, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1592 = shufflevector <16 x i16> %1589, <16 x i16> %1590, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1593 = bitcast <4 x i64> %1527 to <16 x i16>
  %1594 = bitcast <4 x i64> %1532 to <16 x i16>
  %1595 = shufflevector <16 x i16> %1593, <16 x i16> %1594, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1596 = shufflevector <16 x i16> %1593, <16 x i16> %1594, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1597 = bitcast <16 x i16> %1583 to <8 x i32>
  %1598 = bitcast <16 x i16> %1587 to <8 x i32>
  %1599 = shufflevector <8 x i32> %1597, <8 x i32> %1598, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1600 = shufflevector <8 x i32> %1597, <8 x i32> %1598, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1601 = bitcast <16 x i16> %1591 to <8 x i32>
  %1602 = bitcast <16 x i16> %1595 to <8 x i32>
  %1603 = shufflevector <8 x i32> %1601, <8 x i32> %1602, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1604 = shufflevector <8 x i32> %1601, <8 x i32> %1602, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1605 = bitcast <16 x i16> %1584 to <8 x i32>
  %1606 = bitcast <16 x i16> %1588 to <8 x i32>
  %1607 = shufflevector <8 x i32> %1605, <8 x i32> %1606, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1608 = shufflevector <8 x i32> %1605, <8 x i32> %1606, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1609 = bitcast <16 x i16> %1592 to <8 x i32>
  %1610 = bitcast <16 x i16> %1596 to <8 x i32>
  %1611 = shufflevector <8 x i32> %1609, <8 x i32> %1610, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1612 = shufflevector <8 x i32> %1609, <8 x i32> %1610, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1613 = bitcast <8 x i32> %1599 to <4 x i64>
  %1614 = bitcast <8 x i32> %1603 to <4 x i64>
  %1615 = shufflevector <4 x i64> %1613, <4 x i64> %1614, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1615, <4 x i64>* %1440, align 32
  %1616 = shufflevector <4 x i64> %1613, <4 x i64> %1614, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1616, <4 x i64>* %1447, align 32
  %1617 = bitcast <8 x i32> %1607 to <4 x i64>
  %1618 = bitcast <8 x i32> %1611 to <4 x i64>
  %1619 = shufflevector <4 x i64> %1617, <4 x i64> %1618, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1619, <4 x i64>* %1468, align 32
  %1620 = shufflevector <4 x i64> %1617, <4 x i64> %1618, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1620, <4 x i64>* %1475, align 32
  %1621 = bitcast <8 x i32> %1600 to <4 x i64>
  %1622 = bitcast <8 x i32> %1604 to <4 x i64>
  %1623 = shufflevector <4 x i64> %1621, <4 x i64> %1622, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1623, <4 x i64>* %1454, align 32
  %1624 = shufflevector <4 x i64> %1621, <4 x i64> %1622, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1624, <4 x i64>* %1461, align 32
  %1625 = bitcast <8 x i32> %1608 to <4 x i64>
  %1626 = bitcast <8 x i32> %1612 to <4 x i64>
  %1627 = shufflevector <4 x i64> %1625, <4 x i64> %1626, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1627, <4 x i64>* %1482, align 32
  %1628 = shufflevector <4 x i64> %1625, <4 x i64> %1626, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  store <4 x i64> %1628, <4 x i64>* %1489, align 32
  %1629 = add nuw nsw i64 %1403, 1
  %1630 = icmp slt i64 %1629, %1377
  br i1 %1630, label %1402, label %1400

1631:                                             ; preds = %1401, %1631
  %1632 = phi i64 [ %1653, %1631 ], [ 0, %1401 ]
  %1633 = phi i64 [ %1654, %1631 ], [ %1384, %1401 ]
  %1634 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %1632
  %1635 = bitcast <4 x i64>* %1634 to <16 x i16>*
  %1636 = load <16 x i16>, <16 x i16>* %1635, align 32
  %1637 = call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %1636, <16 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  store <16 x i16> %1637, <16 x i16>* %1635, align 32
  %1638 = or i64 %1632, 1
  %1639 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %1638
  %1640 = bitcast <4 x i64>* %1639 to <16 x i16>*
  %1641 = load <16 x i16>, <16 x i16>* %1640, align 32
  %1642 = call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %1641, <16 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  store <16 x i16> %1642, <16 x i16>* %1640, align 32
  %1643 = or i64 %1632, 2
  %1644 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %1643
  %1645 = bitcast <4 x i64>* %1644 to <16 x i16>*
  %1646 = load <16 x i16>, <16 x i16>* %1645, align 32
  %1647 = call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %1646, <16 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  store <16 x i16> %1647, <16 x i16>* %1645, align 32
  %1648 = or i64 %1632, 3
  %1649 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %1648
  %1650 = bitcast <4 x i64>* %1649 to <16 x i16>*
  %1651 = load <16 x i16>, <16 x i16>* %1650, align 32
  %1652 = call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %1651, <16 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  store <16 x i16> %1652, <16 x i16>* %1650, align 32
  %1653 = add nuw nsw i64 %1632, 4
  %1654 = add i64 %1633, -4
  %1655 = icmp eq i64 %1654, 0
  br i1 %1655, label %1656, label %1631

1656:                                             ; preds = %1631, %1401
  %1657 = phi i64 [ 0, %1401 ], [ %1653, %1631 ]
  br i1 %1385, label %1668, label %1658

1658:                                             ; preds = %1656, %1658
  %1659 = phi i64 [ %1665, %1658 ], [ %1657, %1656 ]
  %1660 = phi i64 [ %1666, %1658 ], [ %1382, %1656 ]
  %1661 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %1659
  %1662 = bitcast <4 x i64>* %1661 to <16 x i16>*
  %1663 = load <16 x i16>, <16 x i16>* %1662, align 32
  %1664 = call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %1663, <16 x i16> <i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168, i16 23168>) #9
  store <16 x i16> %1664, <16 x i16>* %1662, align 32
  %1665 = add nuw nsw i64 %1659, 1
  %1666 = add i64 %1660, -1
  %1667 = icmp eq i64 %1666, 0
  br i1 %1667, label %1668, label %1658, !llvm.loop !4

1668:                                             ; preds = %1656, %1658, %1400
  call void %1358(<4 x i64>* nonnull %1371, <4 x i64>* nonnull %1371, i8 signext %1323) #9
  %1669 = load i8, i8* %1313, align 1
  %1670 = sext i8 %1669 to i32
  %1671 = icmp slt i8 %1669, 0
  br i1 %1671, label %1672, label %1708

1672:                                             ; preds = %1668
  %1673 = sub nsw i32 0, %1670
  %1674 = xor i32 %1670, -1
  %1675 = shl i32 1, %1674
  %1676 = trunc i32 %1675 to i16
  %1677 = insertelement <16 x i16> undef, i16 %1676, i32 0
  %1678 = shufflevector <16 x i16> %1677, <16 x i16> undef, <16 x i32> zeroinitializer
  br i1 %1391, label %1736, label %1679

1679:                                             ; preds = %1672, %1679
  %1680 = phi i64 [ %1705, %1679 ], [ 0, %1672 ]
  %1681 = phi i64 [ %1706, %1679 ], [ %1392, %1672 ]
  %1682 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %1680
  %1683 = bitcast <4 x i64>* %1682 to <16 x i16>*
  %1684 = load <16 x i16>, <16 x i16>* %1683, align 32
  %1685 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1684, <16 x i16> %1678) #9
  %1686 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %1685, i32 %1673) #9
  store <16 x i16> %1686, <16 x i16>* %1683, align 32
  %1687 = or i64 %1680, 1
  %1688 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %1687
  %1689 = bitcast <4 x i64>* %1688 to <16 x i16>*
  %1690 = load <16 x i16>, <16 x i16>* %1689, align 32
  %1691 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1690, <16 x i16> %1678) #9
  %1692 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %1691, i32 %1673) #9
  store <16 x i16> %1692, <16 x i16>* %1689, align 32
  %1693 = or i64 %1680, 2
  %1694 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %1693
  %1695 = bitcast <4 x i64>* %1694 to <16 x i16>*
  %1696 = load <16 x i16>, <16 x i16>* %1695, align 32
  %1697 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1696, <16 x i16> %1678) #9
  %1698 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %1697, i32 %1673) #9
  store <16 x i16> %1698, <16 x i16>* %1695, align 32
  %1699 = or i64 %1680, 3
  %1700 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %1699
  %1701 = bitcast <4 x i64>* %1700 to <16 x i16>*
  %1702 = load <16 x i16>, <16 x i16>* %1701, align 32
  %1703 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1702, <16 x i16> %1678) #9
  %1704 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %1703, i32 %1673) #9
  store <16 x i16> %1704, <16 x i16>* %1701, align 32
  %1705 = add nuw nsw i64 %1680, 4
  %1706 = add i64 %1681, -4
  %1707 = icmp eq i64 %1706, 0
  br i1 %1707, label %1736, label %1679

1708:                                             ; preds = %1668
  %1709 = icmp eq i8 %1669, 0
  br i1 %1709, label %1761, label %1710

1710:                                             ; preds = %1708
  br i1 %1387, label %1749, label %1711

1711:                                             ; preds = %1710, %1711
  %1712 = phi i64 [ %1733, %1711 ], [ 0, %1710 ]
  %1713 = phi i64 [ %1734, %1711 ], [ %1388, %1710 ]
  %1714 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %1712
  %1715 = bitcast <4 x i64>* %1714 to <16 x i16>*
  %1716 = load <16 x i16>, <16 x i16>* %1715, align 32
  %1717 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %1716, i32 %1670) #9
  store <16 x i16> %1717, <16 x i16>* %1715, align 32
  %1718 = or i64 %1712, 1
  %1719 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %1718
  %1720 = bitcast <4 x i64>* %1719 to <16 x i16>*
  %1721 = load <16 x i16>, <16 x i16>* %1720, align 32
  %1722 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %1721, i32 %1670) #9
  store <16 x i16> %1722, <16 x i16>* %1720, align 32
  %1723 = or i64 %1712, 2
  %1724 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %1723
  %1725 = bitcast <4 x i64>* %1724 to <16 x i16>*
  %1726 = load <16 x i16>, <16 x i16>* %1725, align 32
  %1727 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %1726, i32 %1670) #9
  store <16 x i16> %1727, <16 x i16>* %1725, align 32
  %1728 = or i64 %1712, 3
  %1729 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %1728
  %1730 = bitcast <4 x i64>* %1729 to <16 x i16>*
  %1731 = load <16 x i16>, <16 x i16>* %1730, align 32
  %1732 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %1731, i32 %1670) #9
  store <16 x i16> %1732, <16 x i16>* %1730, align 32
  %1733 = add nuw nsw i64 %1712, 4
  %1734 = add i64 %1713, -4
  %1735 = icmp eq i64 %1734, 0
  br i1 %1735, label %1749, label %1711

1736:                                             ; preds = %1679, %1672
  %1737 = phi i64 [ 0, %1672 ], [ %1705, %1679 ]
  br i1 %1393, label %1761, label %1738

1738:                                             ; preds = %1736, %1738
  %1739 = phi i64 [ %1746, %1738 ], [ %1737, %1736 ]
  %1740 = phi i64 [ %1747, %1738 ], [ %1390, %1736 ]
  %1741 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %1739
  %1742 = bitcast <4 x i64>* %1741 to <16 x i16>*
  %1743 = load <16 x i16>, <16 x i16>* %1742, align 32
  %1744 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1743, <16 x i16> %1678) #9
  %1745 = call <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16> %1744, i32 %1673) #9
  store <16 x i16> %1745, <16 x i16>* %1742, align 32
  %1746 = add nuw nsw i64 %1739, 1
  %1747 = add i64 %1740, -1
  %1748 = icmp eq i64 %1747, 0
  br i1 %1748, label %1761, label %1738, !llvm.loop !5

1749:                                             ; preds = %1711, %1710
  %1750 = phi i64 [ 0, %1710 ], [ %1733, %1711 ]
  br i1 %1389, label %1761, label %1751

1751:                                             ; preds = %1749, %1751
  %1752 = phi i64 [ %1758, %1751 ], [ %1750, %1749 ]
  %1753 = phi i64 [ %1759, %1751 ], [ %1386, %1749 ]
  %1754 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %1752
  %1755 = bitcast <4 x i64>* %1754 to <16 x i16>*
  %1756 = load <16 x i16>, <16 x i16>* %1755, align 32
  %1757 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> %1756, i32 %1670) #9
  store <16 x i16> %1757, <16 x i16>* %1755, align 32
  %1758 = add nuw nsw i64 %1752, 1
  %1759 = add i64 %1753, -1
  %1760 = icmp eq i64 %1759, 0
  br i1 %1760, label %1761, label %1751, !llvm.loop !6

1761:                                             ; preds = %1749, %1751, %1736, %1738, %1708
  br i1 %1361, label %1762, label %1763

1762:                                             ; preds = %1761
  br i1 %1368, label %2189, label %1764

1763:                                             ; preds = %1761
  br i1 %1368, label %2189, label %1966

1764:                                             ; preds = %1762, %1764
  %1765 = phi i64 [ %1964, %1764 ], [ 0, %1762 ]
  %1766 = shl nsw i64 %1765, 4
  %1767 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %1766
  %1768 = load <4 x i64>, <4 x i64>* %1767, align 32
  %1769 = getelementptr inbounds <4 x i64>, <4 x i64>* %1767, i64 1
  %1770 = load <4 x i64>, <4 x i64>* %1769, align 32
  %1771 = getelementptr inbounds <4 x i64>, <4 x i64>* %1767, i64 2
  %1772 = load <4 x i64>, <4 x i64>* %1771, align 32
  %1773 = getelementptr inbounds <4 x i64>, <4 x i64>* %1767, i64 3
  %1774 = load <4 x i64>, <4 x i64>* %1773, align 32
  %1775 = getelementptr inbounds <4 x i64>, <4 x i64>* %1767, i64 4
  %1776 = load <4 x i64>, <4 x i64>* %1775, align 32
  %1777 = getelementptr inbounds <4 x i64>, <4 x i64>* %1767, i64 5
  %1778 = load <4 x i64>, <4 x i64>* %1777, align 32
  %1779 = getelementptr inbounds <4 x i64>, <4 x i64>* %1767, i64 6
  %1780 = load <4 x i64>, <4 x i64>* %1779, align 32
  %1781 = getelementptr inbounds <4 x i64>, <4 x i64>* %1767, i64 7
  %1782 = load <4 x i64>, <4 x i64>* %1781, align 32
  %1783 = getelementptr inbounds <4 x i64>, <4 x i64>* %1767, i64 8
  %1784 = load <4 x i64>, <4 x i64>* %1783, align 32
  %1785 = getelementptr inbounds <4 x i64>, <4 x i64>* %1767, i64 9
  %1786 = load <4 x i64>, <4 x i64>* %1785, align 32
  %1787 = getelementptr inbounds <4 x i64>, <4 x i64>* %1767, i64 10
  %1788 = load <4 x i64>, <4 x i64>* %1787, align 32
  %1789 = getelementptr inbounds <4 x i64>, <4 x i64>* %1767, i64 11
  %1790 = load <4 x i64>, <4 x i64>* %1789, align 32
  %1791 = getelementptr inbounds <4 x i64>, <4 x i64>* %1767, i64 12
  %1792 = load <4 x i64>, <4 x i64>* %1791, align 32
  %1793 = getelementptr inbounds <4 x i64>, <4 x i64>* %1767, i64 13
  %1794 = load <4 x i64>, <4 x i64>* %1793, align 32
  %1795 = getelementptr inbounds <4 x i64>, <4 x i64>* %1767, i64 14
  %1796 = load <4 x i64>, <4 x i64>* %1795, align 32
  %1797 = getelementptr inbounds <4 x i64>, <4 x i64>* %1767, i64 15
  %1798 = load <4 x i64>, <4 x i64>* %1797, align 32
  %1799 = trunc i64 %1765 to i32
  %1800 = xor i32 %1799, -1
  %1801 = add i32 %1326, %1800
  %1802 = shl nsw i32 %1801, 4
  %1803 = sext i32 %1802 to i64
  %1804 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %10, i64 0, i64 %1803
  %1805 = shufflevector <4 x i64> %1798, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %1806 = shufflevector <4 x i64> %1782, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %1807 = shufflevector <2 x i64> %1805, <2 x i64> %1806, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1808 = shufflevector <4 x i64> %1796, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %1809 = shufflevector <4 x i64> %1780, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %1810 = shufflevector <2 x i64> %1808, <2 x i64> %1809, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1811 = shufflevector <4 x i64> %1794, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %1812 = shufflevector <4 x i64> %1778, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %1813 = shufflevector <2 x i64> %1811, <2 x i64> %1812, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1814 = shufflevector <4 x i64> %1792, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %1815 = shufflevector <4 x i64> %1776, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %1816 = shufflevector <2 x i64> %1814, <2 x i64> %1815, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1817 = shufflevector <4 x i64> %1790, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %1818 = shufflevector <4 x i64> %1774, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %1819 = shufflevector <2 x i64> %1817, <2 x i64> %1818, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1820 = shufflevector <4 x i64> %1788, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %1821 = shufflevector <4 x i64> %1772, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %1822 = shufflevector <2 x i64> %1820, <2 x i64> %1821, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1823 = shufflevector <4 x i64> %1786, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %1824 = shufflevector <4 x i64> %1770, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %1825 = shufflevector <2 x i64> %1823, <2 x i64> %1824, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1826 = shufflevector <4 x i64> %1784, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %1827 = shufflevector <4 x i64> %1768, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %1828 = shufflevector <2 x i64> %1826, <2 x i64> %1827, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1829 = shufflevector <4 x i64> %1798, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %1830 = shufflevector <4 x i64> %1782, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %1831 = shufflevector <2 x i64> %1829, <2 x i64> %1830, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1832 = shufflevector <4 x i64> %1796, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %1833 = shufflevector <4 x i64> %1780, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %1834 = shufflevector <2 x i64> %1832, <2 x i64> %1833, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1835 = shufflevector <4 x i64> %1794, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %1836 = shufflevector <4 x i64> %1778, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %1837 = shufflevector <2 x i64> %1835, <2 x i64> %1836, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1838 = shufflevector <4 x i64> %1792, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %1839 = shufflevector <4 x i64> %1776, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %1840 = shufflevector <2 x i64> %1838, <2 x i64> %1839, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1841 = shufflevector <4 x i64> %1790, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %1842 = shufflevector <4 x i64> %1774, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %1843 = shufflevector <2 x i64> %1841, <2 x i64> %1842, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1844 = shufflevector <4 x i64> %1788, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %1845 = shufflevector <4 x i64> %1772, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %1846 = shufflevector <2 x i64> %1844, <2 x i64> %1845, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1847 = shufflevector <4 x i64> %1786, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %1848 = shufflevector <4 x i64> %1770, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %1849 = shufflevector <2 x i64> %1847, <2 x i64> %1848, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1850 = shufflevector <4 x i64> %1784, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %1851 = shufflevector <4 x i64> %1768, <4 x i64> undef, <2 x i32> <i32 2, i32 3>
  %1852 = shufflevector <2 x i64> %1850, <2 x i64> %1851, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1853 = bitcast <4 x i64> %1807 to <16 x i16>
  %1854 = bitcast <4 x i64> %1810 to <16 x i16>
  %1855 = shufflevector <16 x i16> %1853, <16 x i16> %1854, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1856 = shufflevector <16 x i16> %1853, <16 x i16> %1854, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1857 = bitcast <4 x i64> %1813 to <16 x i16>
  %1858 = bitcast <4 x i64> %1816 to <16 x i16>
  %1859 = shufflevector <16 x i16> %1857, <16 x i16> %1858, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1860 = shufflevector <16 x i16> %1857, <16 x i16> %1858, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1861 = bitcast <4 x i64> %1819 to <16 x i16>
  %1862 = bitcast <4 x i64> %1822 to <16 x i16>
  %1863 = shufflevector <16 x i16> %1861, <16 x i16> %1862, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1864 = shufflevector <16 x i16> %1861, <16 x i16> %1862, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1865 = bitcast <4 x i64> %1825 to <16 x i16>
  %1866 = bitcast <4 x i64> %1828 to <16 x i16>
  %1867 = shufflevector <16 x i16> %1865, <16 x i16> %1866, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1868 = shufflevector <16 x i16> %1865, <16 x i16> %1866, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1869 = bitcast <16 x i16> %1855 to <8 x i32>
  %1870 = bitcast <16 x i16> %1859 to <8 x i32>
  %1871 = shufflevector <8 x i32> %1869, <8 x i32> %1870, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1872 = shufflevector <8 x i32> %1869, <8 x i32> %1870, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1873 = bitcast <16 x i16> %1863 to <8 x i32>
  %1874 = bitcast <16 x i16> %1867 to <8 x i32>
  %1875 = shufflevector <8 x i32> %1873, <8 x i32> %1874, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1876 = shufflevector <8 x i32> %1873, <8 x i32> %1874, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1877 = bitcast <16 x i16> %1856 to <8 x i32>
  %1878 = bitcast <16 x i16> %1860 to <8 x i32>
  %1879 = shufflevector <8 x i32> %1877, <8 x i32> %1878, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1880 = shufflevector <8 x i32> %1877, <8 x i32> %1878, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1881 = bitcast <16 x i16> %1864 to <8 x i32>
  %1882 = bitcast <16 x i16> %1868 to <8 x i32>
  %1883 = shufflevector <8 x i32> %1881, <8 x i32> %1882, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1884 = shufflevector <8 x i32> %1881, <8 x i32> %1882, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1885 = bitcast <8 x i32> %1871 to <4 x i64>
  %1886 = bitcast <8 x i32> %1875 to <4 x i64>
  %1887 = shufflevector <4 x i64> %1885, <4 x i64> %1886, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1887, <4 x i64>* %1804, align 32
  %1888 = shufflevector <4 x i64> %1885, <4 x i64> %1886, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %1889 = getelementptr inbounds <4 x i64>, <4 x i64>* %1804, i64 1
  store <4 x i64> %1888, <4 x i64>* %1889, align 32
  %1890 = bitcast <8 x i32> %1879 to <4 x i64>
  %1891 = bitcast <8 x i32> %1883 to <4 x i64>
  %1892 = shufflevector <4 x i64> %1890, <4 x i64> %1891, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %1893 = getelementptr inbounds <4 x i64>, <4 x i64>* %1804, i64 4
  store <4 x i64> %1892, <4 x i64>* %1893, align 32
  %1894 = shufflevector <4 x i64> %1890, <4 x i64> %1891, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %1895 = getelementptr inbounds <4 x i64>, <4 x i64>* %1804, i64 5
  store <4 x i64> %1894, <4 x i64>* %1895, align 32
  %1896 = bitcast <8 x i32> %1872 to <4 x i64>
  %1897 = bitcast <8 x i32> %1876 to <4 x i64>
  %1898 = shufflevector <4 x i64> %1896, <4 x i64> %1897, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %1899 = getelementptr inbounds <4 x i64>, <4 x i64>* %1804, i64 2
  store <4 x i64> %1898, <4 x i64>* %1899, align 32
  %1900 = shufflevector <4 x i64> %1896, <4 x i64> %1897, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %1901 = getelementptr inbounds <4 x i64>, <4 x i64>* %1804, i64 3
  store <4 x i64> %1900, <4 x i64>* %1901, align 32
  %1902 = bitcast <8 x i32> %1880 to <4 x i64>
  %1903 = bitcast <8 x i32> %1884 to <4 x i64>
  %1904 = shufflevector <4 x i64> %1902, <4 x i64> %1903, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %1905 = getelementptr inbounds <4 x i64>, <4 x i64>* %1804, i64 6
  store <4 x i64> %1904, <4 x i64>* %1905, align 32
  %1906 = shufflevector <4 x i64> %1902, <4 x i64> %1903, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %1907 = getelementptr inbounds <4 x i64>, <4 x i64>* %1804, i64 7
  store <4 x i64> %1906, <4 x i64>* %1907, align 32
  %1908 = getelementptr inbounds <4 x i64>, <4 x i64>* %1804, i64 8
  %1909 = bitcast <4 x i64> %1831 to <16 x i16>
  %1910 = bitcast <4 x i64> %1834 to <16 x i16>
  %1911 = shufflevector <16 x i16> %1909, <16 x i16> %1910, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1912 = shufflevector <16 x i16> %1909, <16 x i16> %1910, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1913 = bitcast <4 x i64> %1837 to <16 x i16>
  %1914 = bitcast <4 x i64> %1840 to <16 x i16>
  %1915 = shufflevector <16 x i16> %1913, <16 x i16> %1914, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1916 = shufflevector <16 x i16> %1913, <16 x i16> %1914, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1917 = bitcast <4 x i64> %1843 to <16 x i16>
  %1918 = bitcast <4 x i64> %1846 to <16 x i16>
  %1919 = shufflevector <16 x i16> %1917, <16 x i16> %1918, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1920 = shufflevector <16 x i16> %1917, <16 x i16> %1918, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1921 = bitcast <4 x i64> %1849 to <16 x i16>
  %1922 = bitcast <4 x i64> %1852 to <16 x i16>
  %1923 = shufflevector <16 x i16> %1921, <16 x i16> %1922, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1924 = shufflevector <16 x i16> %1921, <16 x i16> %1922, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1925 = bitcast <16 x i16> %1911 to <8 x i32>
  %1926 = bitcast <16 x i16> %1915 to <8 x i32>
  %1927 = shufflevector <8 x i32> %1925, <8 x i32> %1926, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1928 = shufflevector <8 x i32> %1925, <8 x i32> %1926, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1929 = bitcast <16 x i16> %1919 to <8 x i32>
  %1930 = bitcast <16 x i16> %1923 to <8 x i32>
  %1931 = shufflevector <8 x i32> %1929, <8 x i32> %1930, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1932 = shufflevector <8 x i32> %1929, <8 x i32> %1930, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1933 = bitcast <16 x i16> %1912 to <8 x i32>
  %1934 = bitcast <16 x i16> %1916 to <8 x i32>
  %1935 = shufflevector <8 x i32> %1933, <8 x i32> %1934, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1936 = shufflevector <8 x i32> %1933, <8 x i32> %1934, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1937 = bitcast <16 x i16> %1920 to <8 x i32>
  %1938 = bitcast <16 x i16> %1924 to <8 x i32>
  %1939 = shufflevector <8 x i32> %1937, <8 x i32> %1938, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %1940 = shufflevector <8 x i32> %1937, <8 x i32> %1938, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %1941 = bitcast <8 x i32> %1927 to <4 x i64>
  %1942 = bitcast <8 x i32> %1931 to <4 x i64>
  %1943 = shufflevector <4 x i64> %1941, <4 x i64> %1942, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %1943, <4 x i64>* %1908, align 32
  %1944 = shufflevector <4 x i64> %1941, <4 x i64> %1942, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %1945 = getelementptr inbounds <4 x i64>, <4 x i64>* %1804, i64 9
  store <4 x i64> %1944, <4 x i64>* %1945, align 32
  %1946 = bitcast <8 x i32> %1935 to <4 x i64>
  %1947 = bitcast <8 x i32> %1939 to <4 x i64>
  %1948 = shufflevector <4 x i64> %1946, <4 x i64> %1947, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %1949 = getelementptr inbounds <4 x i64>, <4 x i64>* %1804, i64 12
  store <4 x i64> %1948, <4 x i64>* %1949, align 32
  %1950 = shufflevector <4 x i64> %1946, <4 x i64> %1947, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %1951 = getelementptr inbounds <4 x i64>, <4 x i64>* %1804, i64 13
  store <4 x i64> %1950, <4 x i64>* %1951, align 32
  %1952 = bitcast <8 x i32> %1928 to <4 x i64>
  %1953 = bitcast <8 x i32> %1932 to <4 x i64>
  %1954 = shufflevector <4 x i64> %1952, <4 x i64> %1953, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %1955 = getelementptr inbounds <4 x i64>, <4 x i64>* %1804, i64 10
  store <4 x i64> %1954, <4 x i64>* %1955, align 32
  %1956 = shufflevector <4 x i64> %1952, <4 x i64> %1953, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %1957 = getelementptr inbounds <4 x i64>, <4 x i64>* %1804, i64 11
  store <4 x i64> %1956, <4 x i64>* %1957, align 32
  %1958 = bitcast <8 x i32> %1936 to <4 x i64>
  %1959 = bitcast <8 x i32> %1940 to <4 x i64>
  %1960 = shufflevector <4 x i64> %1958, <4 x i64> %1959, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %1961 = getelementptr inbounds <4 x i64>, <4 x i64>* %1804, i64 14
  store <4 x i64> %1960, <4 x i64>* %1961, align 32
  %1962 = shufflevector <4 x i64> %1958, <4 x i64> %1959, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %1963 = getelementptr inbounds <4 x i64>, <4 x i64>* %1804, i64 15
  store <4 x i64> %1962, <4 x i64>* %1963, align 32
  %1964 = add nuw nsw i64 %1765, 1
  %1965 = icmp slt i64 %1964, %1378
  br i1 %1965, label %1764, label %2179

1966:                                             ; preds = %1763, %1966
  %1967 = phi i64 [ %2177, %1966 ], [ 0, %1763 ]
  %1968 = shl nsw i64 %1967, 4
  %1969 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %1968
  %1970 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %10, i64 0, i64 %1968
  %1971 = bitcast <4 x i64>* %1969 to <2 x i64>*
  %1972 = load <2 x i64>, <2 x i64>* %1971, align 32
  %1973 = getelementptr inbounds <4 x i64>, <4 x i64>* %1969, i64 8
  %1974 = bitcast <4 x i64>* %1973 to <2 x i64>*
  %1975 = load <2 x i64>, <2 x i64>* %1974, align 32
  %1976 = shufflevector <2 x i64> %1972, <2 x i64> %1975, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1977 = getelementptr inbounds <4 x i64>, <4 x i64>* %1969, i64 1
  %1978 = bitcast <4 x i64>* %1977 to <2 x i64>*
  %1979 = load <2 x i64>, <2 x i64>* %1978, align 32
  %1980 = getelementptr inbounds <4 x i64>, <4 x i64>* %1969, i64 9
  %1981 = bitcast <4 x i64>* %1980 to <2 x i64>*
  %1982 = load <2 x i64>, <2 x i64>* %1981, align 32
  %1983 = shufflevector <2 x i64> %1979, <2 x i64> %1982, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1984 = getelementptr inbounds <4 x i64>, <4 x i64>* %1969, i64 2
  %1985 = bitcast <4 x i64>* %1984 to <2 x i64>*
  %1986 = load <2 x i64>, <2 x i64>* %1985, align 32
  %1987 = getelementptr inbounds <4 x i64>, <4 x i64>* %1969, i64 10
  %1988 = bitcast <4 x i64>* %1987 to <2 x i64>*
  %1989 = load <2 x i64>, <2 x i64>* %1988, align 32
  %1990 = shufflevector <2 x i64> %1986, <2 x i64> %1989, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1991 = getelementptr inbounds <4 x i64>, <4 x i64>* %1969, i64 3
  %1992 = bitcast <4 x i64>* %1991 to <2 x i64>*
  %1993 = load <2 x i64>, <2 x i64>* %1992, align 32
  %1994 = getelementptr inbounds <4 x i64>, <4 x i64>* %1969, i64 11
  %1995 = bitcast <4 x i64>* %1994 to <2 x i64>*
  %1996 = load <2 x i64>, <2 x i64>* %1995, align 32
  %1997 = shufflevector <2 x i64> %1993, <2 x i64> %1996, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %1998 = getelementptr inbounds <4 x i64>, <4 x i64>* %1969, i64 4
  %1999 = bitcast <4 x i64>* %1998 to <2 x i64>*
  %2000 = load <2 x i64>, <2 x i64>* %1999, align 32
  %2001 = getelementptr inbounds <4 x i64>, <4 x i64>* %1969, i64 12
  %2002 = bitcast <4 x i64>* %2001 to <2 x i64>*
  %2003 = load <2 x i64>, <2 x i64>* %2002, align 32
  %2004 = shufflevector <2 x i64> %2000, <2 x i64> %2003, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2005 = getelementptr inbounds <4 x i64>, <4 x i64>* %1969, i64 5
  %2006 = bitcast <4 x i64>* %2005 to <2 x i64>*
  %2007 = load <2 x i64>, <2 x i64>* %2006, align 32
  %2008 = getelementptr inbounds <4 x i64>, <4 x i64>* %1969, i64 13
  %2009 = bitcast <4 x i64>* %2008 to <2 x i64>*
  %2010 = load <2 x i64>, <2 x i64>* %2009, align 32
  %2011 = shufflevector <2 x i64> %2007, <2 x i64> %2010, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2012 = getelementptr inbounds <4 x i64>, <4 x i64>* %1969, i64 6
  %2013 = bitcast <4 x i64>* %2012 to <2 x i64>*
  %2014 = load <2 x i64>, <2 x i64>* %2013, align 32
  %2015 = getelementptr inbounds <4 x i64>, <4 x i64>* %1969, i64 14
  %2016 = bitcast <4 x i64>* %2015 to <2 x i64>*
  %2017 = load <2 x i64>, <2 x i64>* %2016, align 32
  %2018 = shufflevector <2 x i64> %2014, <2 x i64> %2017, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2019 = getelementptr inbounds <4 x i64>, <4 x i64>* %1969, i64 7
  %2020 = bitcast <4 x i64>* %2019 to <2 x i64>*
  %2021 = load <2 x i64>, <2 x i64>* %2020, align 32
  %2022 = getelementptr inbounds <4 x i64>, <4 x i64>* %1969, i64 15
  %2023 = bitcast <4 x i64>* %2022 to <2 x i64>*
  %2024 = load <2 x i64>, <2 x i64>* %2023, align 32
  %2025 = shufflevector <2 x i64> %2021, <2 x i64> %2024, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2026 = getelementptr inbounds <2 x i64>, <2 x i64>* %1971, i64 1
  %2027 = load <2 x i64>, <2 x i64>* %2026, align 16
  %2028 = getelementptr inbounds <2 x i64>, <2 x i64>* %1974, i64 1
  %2029 = load <2 x i64>, <2 x i64>* %2028, align 16
  %2030 = shufflevector <2 x i64> %2027, <2 x i64> %2029, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2031 = getelementptr inbounds <2 x i64>, <2 x i64>* %1978, i64 1
  %2032 = load <2 x i64>, <2 x i64>* %2031, align 16
  %2033 = getelementptr inbounds <2 x i64>, <2 x i64>* %1981, i64 1
  %2034 = load <2 x i64>, <2 x i64>* %2033, align 16
  %2035 = shufflevector <2 x i64> %2032, <2 x i64> %2034, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2036 = getelementptr inbounds <2 x i64>, <2 x i64>* %1985, i64 1
  %2037 = load <2 x i64>, <2 x i64>* %2036, align 16
  %2038 = getelementptr inbounds <2 x i64>, <2 x i64>* %1988, i64 1
  %2039 = load <2 x i64>, <2 x i64>* %2038, align 16
  %2040 = shufflevector <2 x i64> %2037, <2 x i64> %2039, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2041 = getelementptr inbounds <2 x i64>, <2 x i64>* %1992, i64 1
  %2042 = load <2 x i64>, <2 x i64>* %2041, align 16
  %2043 = getelementptr inbounds <2 x i64>, <2 x i64>* %1995, i64 1
  %2044 = load <2 x i64>, <2 x i64>* %2043, align 16
  %2045 = shufflevector <2 x i64> %2042, <2 x i64> %2044, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2046 = getelementptr inbounds <2 x i64>, <2 x i64>* %1999, i64 1
  %2047 = load <2 x i64>, <2 x i64>* %2046, align 16
  %2048 = getelementptr inbounds <2 x i64>, <2 x i64>* %2002, i64 1
  %2049 = load <2 x i64>, <2 x i64>* %2048, align 16
  %2050 = shufflevector <2 x i64> %2047, <2 x i64> %2049, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2051 = getelementptr inbounds <2 x i64>, <2 x i64>* %2006, i64 1
  %2052 = load <2 x i64>, <2 x i64>* %2051, align 16
  %2053 = getelementptr inbounds <2 x i64>, <2 x i64>* %2009, i64 1
  %2054 = load <2 x i64>, <2 x i64>* %2053, align 16
  %2055 = shufflevector <2 x i64> %2052, <2 x i64> %2054, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2056 = getelementptr inbounds <2 x i64>, <2 x i64>* %2013, i64 1
  %2057 = load <2 x i64>, <2 x i64>* %2056, align 16
  %2058 = getelementptr inbounds <2 x i64>, <2 x i64>* %2016, i64 1
  %2059 = load <2 x i64>, <2 x i64>* %2058, align 16
  %2060 = shufflevector <2 x i64> %2057, <2 x i64> %2059, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2061 = getelementptr inbounds <2 x i64>, <2 x i64>* %2020, i64 1
  %2062 = load <2 x i64>, <2 x i64>* %2061, align 16
  %2063 = getelementptr inbounds <2 x i64>, <2 x i64>* %2023, i64 1
  %2064 = load <2 x i64>, <2 x i64>* %2063, align 16
  %2065 = shufflevector <2 x i64> %2062, <2 x i64> %2064, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2066 = bitcast <4 x i64> %1976 to <16 x i16>
  %2067 = bitcast <4 x i64> %1983 to <16 x i16>
  %2068 = shufflevector <16 x i16> %2066, <16 x i16> %2067, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %2069 = shufflevector <16 x i16> %2066, <16 x i16> %2067, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %2070 = bitcast <4 x i64> %1990 to <16 x i16>
  %2071 = bitcast <4 x i64> %1997 to <16 x i16>
  %2072 = shufflevector <16 x i16> %2070, <16 x i16> %2071, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %2073 = shufflevector <16 x i16> %2070, <16 x i16> %2071, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %2074 = bitcast <4 x i64> %2004 to <16 x i16>
  %2075 = bitcast <4 x i64> %2011 to <16 x i16>
  %2076 = shufflevector <16 x i16> %2074, <16 x i16> %2075, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %2077 = shufflevector <16 x i16> %2074, <16 x i16> %2075, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %2078 = bitcast <4 x i64> %2018 to <16 x i16>
  %2079 = bitcast <4 x i64> %2025 to <16 x i16>
  %2080 = shufflevector <16 x i16> %2078, <16 x i16> %2079, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %2081 = shufflevector <16 x i16> %2078, <16 x i16> %2079, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %2082 = bitcast <16 x i16> %2068 to <8 x i32>
  %2083 = bitcast <16 x i16> %2072 to <8 x i32>
  %2084 = shufflevector <8 x i32> %2082, <8 x i32> %2083, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %2085 = shufflevector <8 x i32> %2082, <8 x i32> %2083, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %2086 = bitcast <16 x i16> %2076 to <8 x i32>
  %2087 = bitcast <16 x i16> %2080 to <8 x i32>
  %2088 = shufflevector <8 x i32> %2086, <8 x i32> %2087, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %2089 = shufflevector <8 x i32> %2086, <8 x i32> %2087, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %2090 = bitcast <16 x i16> %2069 to <8 x i32>
  %2091 = bitcast <16 x i16> %2073 to <8 x i32>
  %2092 = shufflevector <8 x i32> %2090, <8 x i32> %2091, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %2093 = shufflevector <8 x i32> %2090, <8 x i32> %2091, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %2094 = bitcast <16 x i16> %2077 to <8 x i32>
  %2095 = bitcast <16 x i16> %2081 to <8 x i32>
  %2096 = shufflevector <8 x i32> %2094, <8 x i32> %2095, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %2097 = shufflevector <8 x i32> %2094, <8 x i32> %2095, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %2098 = bitcast <8 x i32> %2084 to <4 x i64>
  %2099 = bitcast <8 x i32> %2088 to <4 x i64>
  %2100 = shufflevector <4 x i64> %2098, <4 x i64> %2099, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %2100, <4 x i64>* %1970, align 32
  %2101 = shufflevector <4 x i64> %2098, <4 x i64> %2099, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %2102 = getelementptr inbounds <4 x i64>, <4 x i64>* %1970, i64 1
  store <4 x i64> %2101, <4 x i64>* %2102, align 32
  %2103 = bitcast <8 x i32> %2092 to <4 x i64>
  %2104 = bitcast <8 x i32> %2096 to <4 x i64>
  %2105 = shufflevector <4 x i64> %2103, <4 x i64> %2104, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %2106 = getelementptr inbounds <4 x i64>, <4 x i64>* %1970, i64 4
  store <4 x i64> %2105, <4 x i64>* %2106, align 32
  %2107 = shufflevector <4 x i64> %2103, <4 x i64> %2104, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %2108 = getelementptr inbounds <4 x i64>, <4 x i64>* %1970, i64 5
  store <4 x i64> %2107, <4 x i64>* %2108, align 32
  %2109 = bitcast <8 x i32> %2085 to <4 x i64>
  %2110 = bitcast <8 x i32> %2089 to <4 x i64>
  %2111 = shufflevector <4 x i64> %2109, <4 x i64> %2110, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %2112 = getelementptr inbounds <4 x i64>, <4 x i64>* %1970, i64 2
  store <4 x i64> %2111, <4 x i64>* %2112, align 32
  %2113 = shufflevector <4 x i64> %2109, <4 x i64> %2110, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %2114 = getelementptr inbounds <4 x i64>, <4 x i64>* %1970, i64 3
  store <4 x i64> %2113, <4 x i64>* %2114, align 32
  %2115 = bitcast <8 x i32> %2093 to <4 x i64>
  %2116 = bitcast <8 x i32> %2097 to <4 x i64>
  %2117 = shufflevector <4 x i64> %2115, <4 x i64> %2116, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %2118 = getelementptr inbounds <4 x i64>, <4 x i64>* %1970, i64 6
  store <4 x i64> %2117, <4 x i64>* %2118, align 32
  %2119 = shufflevector <4 x i64> %2115, <4 x i64> %2116, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %2120 = getelementptr inbounds <4 x i64>, <4 x i64>* %1970, i64 7
  store <4 x i64> %2119, <4 x i64>* %2120, align 32
  %2121 = getelementptr inbounds <4 x i64>, <4 x i64>* %1970, i64 8
  %2122 = bitcast <4 x i64> %2030 to <16 x i16>
  %2123 = bitcast <4 x i64> %2035 to <16 x i16>
  %2124 = shufflevector <16 x i16> %2122, <16 x i16> %2123, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %2125 = shufflevector <16 x i16> %2122, <16 x i16> %2123, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %2126 = bitcast <4 x i64> %2040 to <16 x i16>
  %2127 = bitcast <4 x i64> %2045 to <16 x i16>
  %2128 = shufflevector <16 x i16> %2126, <16 x i16> %2127, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %2129 = shufflevector <16 x i16> %2126, <16 x i16> %2127, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %2130 = bitcast <4 x i64> %2050 to <16 x i16>
  %2131 = bitcast <4 x i64> %2055 to <16 x i16>
  %2132 = shufflevector <16 x i16> %2130, <16 x i16> %2131, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %2133 = shufflevector <16 x i16> %2130, <16 x i16> %2131, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %2134 = bitcast <4 x i64> %2060 to <16 x i16>
  %2135 = bitcast <4 x i64> %2065 to <16 x i16>
  %2136 = shufflevector <16 x i16> %2134, <16 x i16> %2135, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %2137 = shufflevector <16 x i16> %2134, <16 x i16> %2135, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %2138 = bitcast <16 x i16> %2124 to <8 x i32>
  %2139 = bitcast <16 x i16> %2128 to <8 x i32>
  %2140 = shufflevector <8 x i32> %2138, <8 x i32> %2139, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %2141 = shufflevector <8 x i32> %2138, <8 x i32> %2139, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %2142 = bitcast <16 x i16> %2132 to <8 x i32>
  %2143 = bitcast <16 x i16> %2136 to <8 x i32>
  %2144 = shufflevector <8 x i32> %2142, <8 x i32> %2143, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %2145 = shufflevector <8 x i32> %2142, <8 x i32> %2143, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %2146 = bitcast <16 x i16> %2125 to <8 x i32>
  %2147 = bitcast <16 x i16> %2129 to <8 x i32>
  %2148 = shufflevector <8 x i32> %2146, <8 x i32> %2147, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %2149 = shufflevector <8 x i32> %2146, <8 x i32> %2147, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %2150 = bitcast <16 x i16> %2133 to <8 x i32>
  %2151 = bitcast <16 x i16> %2137 to <8 x i32>
  %2152 = shufflevector <8 x i32> %2150, <8 x i32> %2151, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %2153 = shufflevector <8 x i32> %2150, <8 x i32> %2151, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %2154 = bitcast <8 x i32> %2140 to <4 x i64>
  %2155 = bitcast <8 x i32> %2144 to <4 x i64>
  %2156 = shufflevector <4 x i64> %2154, <4 x i64> %2155, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  store <4 x i64> %2156, <4 x i64>* %2121, align 32
  %2157 = shufflevector <4 x i64> %2154, <4 x i64> %2155, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %2158 = getelementptr inbounds <4 x i64>, <4 x i64>* %1970, i64 9
  store <4 x i64> %2157, <4 x i64>* %2158, align 32
  %2159 = bitcast <8 x i32> %2148 to <4 x i64>
  %2160 = bitcast <8 x i32> %2152 to <4 x i64>
  %2161 = shufflevector <4 x i64> %2159, <4 x i64> %2160, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %2162 = getelementptr inbounds <4 x i64>, <4 x i64>* %1970, i64 12
  store <4 x i64> %2161, <4 x i64>* %2162, align 32
  %2163 = shufflevector <4 x i64> %2159, <4 x i64> %2160, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %2164 = getelementptr inbounds <4 x i64>, <4 x i64>* %1970, i64 13
  store <4 x i64> %2163, <4 x i64>* %2164, align 32
  %2165 = bitcast <8 x i32> %2141 to <4 x i64>
  %2166 = bitcast <8 x i32> %2145 to <4 x i64>
  %2167 = shufflevector <4 x i64> %2165, <4 x i64> %2166, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %2168 = getelementptr inbounds <4 x i64>, <4 x i64>* %1970, i64 10
  store <4 x i64> %2167, <4 x i64>* %2168, align 32
  %2169 = shufflevector <4 x i64> %2165, <4 x i64> %2166, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %2170 = getelementptr inbounds <4 x i64>, <4 x i64>* %1970, i64 11
  store <4 x i64> %2169, <4 x i64>* %2170, align 32
  %2171 = bitcast <8 x i32> %2149 to <4 x i64>
  %2172 = bitcast <8 x i32> %2153 to <4 x i64>
  %2173 = shufflevector <4 x i64> %2171, <4 x i64> %2172, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %2174 = getelementptr inbounds <4 x i64>, <4 x i64>* %1970, i64 14
  store <4 x i64> %2173, <4 x i64>* %2174, align 32
  %2175 = shufflevector <4 x i64> %2171, <4 x i64> %2172, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %2176 = getelementptr inbounds <4 x i64>, <4 x i64>* %1970, i64 15
  store <4 x i64> %2175, <4 x i64>* %2176, align 32
  %2177 = add nuw nsw i64 %1967, 1
  %2178 = icmp slt i64 %2177, %1378
  br i1 %2178, label %1966, label %2179

2179:                                             ; preds = %1966, %1764
  br i1 %1368, label %2189, label %2180

2180:                                             ; preds = %2179
  %2181 = mul i32 %1373, %1396
  %2182 = sext i32 %2181 to i64
  %2183 = getelementptr inbounds i8, i8* %1, i64 %2182
  %2184 = load i32, i32* %1375, align 4
  %2185 = trunc i32 %2184 to i16
  %2186 = insertelement <16 x i16> undef, i16 %2185, i32 0
  %2187 = shufflevector <16 x i16> %2186, <16 x i16> undef, <16 x i32> <i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 0, i32 0, i32 0, i32 undef, i32 undef, i32 undef, i32 undef>
  %2188 = shufflevector <16 x i16> %2187, <16 x i16> <i16 2048, i16 2048, i16 2048, i16 2048, i16 undef, i16 undef, i16 undef, i16 undef, i16 2048, i16 2048, i16 2048, i16 2048, i16 undef, i16 undef, i16 undef, i16 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  br label %2192

2189:                                             ; preds = %2232, %2179, %1763, %1762
  call void @llvm.lifetime.end.p0i8(i64 2048, i8* nonnull %1362) #9
  %2190 = add nuw nsw i64 %1395, 1
  %2191 = icmp slt i64 %2190, %1379
  br i1 %2191, label %1394, label %2235

2192:                                             ; preds = %2232, %2180
  %2193 = phi i64 [ 0, %2180 ], [ %2233, %2232 ]
  %2194 = shl nsw i64 %2193, 4
  %2195 = getelementptr inbounds i8, i8* %2183, i64 %2194
  %2196 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %10, i64 0, i64 %2194
  %2197 = load i8, i8* %1374, align 1
  %2198 = sext i8 %2197 to i32
  %2199 = sub nsw i32 0, %2198
  %2200 = xor i32 %2198, -1
  %2201 = shl i32 1, %2200
  %2202 = insertelement <8 x i32> undef, i32 %2201, i32 0
  %2203 = shufflevector <8 x i32> %2202, <8 x i32> undef, <8 x i32> zeroinitializer
  br label %2204

2204:                                             ; preds = %2204, %2192
  %2205 = phi i64 [ 0, %2192 ], [ %2230, %2204 ]
  %2206 = phi i8* [ %2195, %2192 ], [ %2229, %2204 ]
  %2207 = getelementptr inbounds <4 x i64>, <4 x i64>* %2196, i64 %2205
  %2208 = bitcast <4 x i64>* %2207 to <16 x i16>*
  %2209 = load <16 x i16>, <16 x i16>* %2208, align 32
  %2210 = shufflevector <16 x i16> %2209, <16 x i16> <i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %2211 = shufflevector <16 x i16> %2209, <16 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1, i16 undef, i16 undef, i16 undef, i16 undef, i16 1, i16 1, i16 1, i16 1>, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %2212 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2210, <16 x i16> %2188) #9
  %2213 = call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %2211, <16 x i16> %2188) #9
  %2214 = ashr <8 x i32> %2212, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %2215 = ashr <8 x i32> %2213, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  %2216 = add <8 x i32> %2214, %2203
  %2217 = add <8 x i32> %2215, %2203
  %2218 = call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2216, i32 %2199) #9
  %2219 = call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2217, i32 %2199) #9
  %2220 = call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %2218, <8 x i32> %2219) #9
  %2221 = bitcast i8* %2206 to <2 x i64>*
  %2222 = bitcast i8* %2206 to <16 x i8>*
  %2223 = load <16 x i8>, <16 x i8>* %2222, align 1
  %2224 = zext <16 x i8> %2223 to <16 x i16>
  %2225 = call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %2224, <16 x i16> %2220) #9
  %2226 = call <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16> %2225, <16 x i16> undef) #9
  %2227 = bitcast <32 x i8> %2226 to <4 x i64>
  %2228 = shufflevector <4 x i64> %2227, <4 x i64> undef, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %2228, <2 x i64>* %2221, align 1
  %2229 = getelementptr inbounds i8, i8* %2206, i64 %1376
  %2230 = add nuw nsw i64 %2205, 1
  %2231 = icmp eq i64 %2230, 16
  br i1 %2231, label %2232, label %2204

2232:                                             ; preds = %2204
  %2233 = add nuw nsw i64 %2193, 1
  %2234 = icmp slt i64 %2233, %1378
  br i1 %2234, label %2192, label %2189

2235:                                             ; preds = %2189, %1347
  call void @llvm.lifetime.end.p0i8(i64 2048, i8* nonnull %1295) #9
  br label %2237

2236:                                             ; preds = %12
  tail call void @av1_lowbd_inv_txfm2d_add_ssse3(i32* %0, i8* %1, i32 %2, i8 zeroext %3, i8 zeroext %4, i32 %5) #9
  br label %2237

2237:                                             ; preds = %1271, %2236, %2235, %1173, %1099, %939, %11
  ret void
}

declare void @av1_lowbd_inv_txfm2d_add_ssse3(i32*, i8*, i32, i8 zeroext, i8 zeroext, i32) local_unnamed_addr #1

; Function Attrs: nounwind ssp uwtable
define hidden void @av1_inv_txfm_add_avx2(i32*, i8*, i32, %struct.txfm_param*) local_unnamed_addr #2 {
  %5 = getelementptr inbounds %struct.txfm_param, %struct.txfm_param* %3, i64 0, i32 2
  %6 = load i32, i32* %5, align 4
  %7 = icmp eq i32 %6, 0
  br i1 %7, label %8, label %15

8:                                                ; preds = %4
  %9 = getelementptr inbounds %struct.txfm_param, %struct.txfm_param* %3, i64 0, i32 0
  %10 = load i8, i8* %9, align 4
  %11 = getelementptr inbounds %struct.txfm_param, %struct.txfm_param* %3, i64 0, i32 1
  %12 = load i8, i8* %11, align 1
  %13 = getelementptr inbounds %struct.txfm_param, %struct.txfm_param* %3, i64 0, i32 6
  %14 = load i32, i32* %13, align 4
  tail call void @av1_lowbd_inv_txfm2d_add_avx2(i32* %0, i8* %1, i32 %2, i8 zeroext %10, i8 zeroext %12, i32 %14)
  br label %16

15:                                               ; preds = %4
  tail call void @av1_inv_txfm_add_c(i32* %0, i8* %1, i32 %2, %struct.txfm_param* %3) #9
  br label %16

16:                                               ; preds = %15, %8
  ret void
}

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #3

declare void @av1_inv_txfm_add_c(i32*, i8*, i32, %struct.txfm_param*) local_unnamed_addr #1

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #3

; Function Attrs: argmemonly nounwind
declare void @llvm.memset.p0i8.i64(i8* nocapture writeonly, i8, i64, i1 immarg) #3

; Function Attrs: nofree nounwind ssp uwtable
define internal void @idct16_low1_avx2(<4 x i64>* nocapture readonly, <4 x i64>* nocapture, i8 signext) #4 {
  %4 = bitcast <4 x i64>* %0 to <16 x i16>*
  %5 = load <16 x i16>, <16 x i16>* %4, align 32
  %6 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %7 = trunc i32 %6 to i16
  %8 = shl i16 %7, 3
  %9 = insertelement <16 x i16> undef, i16 %8, i32 0
  %10 = shufflevector <16 x i16> %9, <16 x i16> undef, <16 x i32> zeroinitializer
  %11 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %5, <16 x i16> %10) #9
  %12 = bitcast <4 x i64>* %1 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %12, align 32
  %13 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %14 = bitcast <4 x i64>* %13 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %14, align 32
  %15 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %16 = bitcast <4 x i64>* %15 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %16, align 32
  %17 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %18 = bitcast <4 x i64>* %17 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %18, align 32
  %19 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %20 = bitcast <4 x i64>* %19 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %20, align 32
  %21 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %22 = bitcast <4 x i64>* %21 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %22, align 32
  %23 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %24 = bitcast <4 x i64>* %23 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %24, align 32
  %25 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %26 = bitcast <4 x i64>* %25 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %26, align 32
  %27 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %28 = bitcast <4 x i64>* %27 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %28, align 32
  %29 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %30 = bitcast <4 x i64>* %29 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %30, align 32
  %31 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %32 = bitcast <4 x i64>* %31 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %32, align 32
  %33 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %34 = bitcast <4 x i64>* %33 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %34, align 32
  %35 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %36 = bitcast <4 x i64>* %35 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %36, align 32
  %37 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %38 = bitcast <4 x i64>* %37 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %38, align 32
  %39 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %40 = bitcast <4 x i64>* %39 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %40, align 32
  %41 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 15
  %42 = bitcast <4 x i64>* %41 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %42, align 32
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @idct16_low8_avx2(<4 x i64>* nocapture readonly, <4 x i64>* nocapture, i8 signext) #0 {
  %4 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 16), align 16
  %5 = trunc i32 %4 to i16
  %6 = sub i32 0, %4
  %7 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 48), align 16
  %8 = trunc i32 %7 to i16
  %9 = and i32 %6, 65535
  %10 = and i32 %7, 65535
  %11 = shl nuw i32 %10, 16
  %12 = or i32 %11, %9
  %13 = insertelement <8 x i32> undef, i32 %12, i32 0
  %14 = shufflevector <8 x i32> %13, <8 x i32> undef, <8 x i32> zeroinitializer
  %15 = shl i32 %4, 16
  %16 = or i32 %10, %15
  %17 = insertelement <8 x i32> undef, i32 %16, i32 0
  %18 = shufflevector <8 x i32> %17, <8 x i32> undef, <8 x i32> zeroinitializer
  %19 = sub i32 0, %7
  %20 = and i32 %19, 65535
  %21 = shl nuw i32 %9, 16
  %22 = or i32 %21, %20
  %23 = insertelement <8 x i32> undef, i32 %22, i32 0
  %24 = shufflevector <8 x i32> %23, <8 x i32> undef, <8 x i32> zeroinitializer
  %25 = bitcast <4 x i64>* %0 to <16 x i16>*
  %26 = load <16 x i16>, <16 x i16>* %25, align 32
  %27 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 4
  %28 = bitcast <4 x i64>* %27 to <16 x i16>*
  %29 = load <16 x i16>, <16 x i16>* %28, align 32
  %30 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 2
  %31 = bitcast <4 x i64>* %30 to <16 x i16>*
  %32 = load <16 x i16>, <16 x i16>* %31, align 32
  %33 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 6
  %34 = bitcast <4 x i64>* %33 to <16 x i16>*
  %35 = load <16 x i16>, <16 x i16>* %34, align 32
  %36 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 1
  %37 = bitcast <4 x i64>* %36 to <16 x i16>*
  %38 = load <16 x i16>, <16 x i16>* %37, align 32
  %39 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 5
  %40 = bitcast <4 x i64>* %39 to <16 x i16>*
  %41 = load <16 x i16>, <16 x i16>* %40, align 32
  %42 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 3
  %43 = bitcast <4 x i64>* %42 to <16 x i16>*
  %44 = load <16 x i16>, <16 x i16>* %43, align 32
  %45 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 7
  %46 = bitcast <4 x i64>* %45 to <16 x i16>*
  %47 = load <16 x i16>, <16 x i16>* %46, align 32
  %48 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 60), align 16
  %49 = trunc i32 %48 to i16
  %50 = shl i16 %49, 3
  %51 = insertelement <16 x i16> undef, i16 %50, i32 0
  %52 = shufflevector <16 x i16> %51, <16 x i16> undef, <16 x i32> zeroinitializer
  %53 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 4), align 16
  %54 = trunc i32 %53 to i16
  %55 = shl i16 %54, 3
  %56 = insertelement <16 x i16> undef, i16 %55, i32 0
  %57 = shufflevector <16 x i16> %56, <16 x i16> undef, <16 x i32> zeroinitializer
  %58 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %38, <16 x i16> %52) #9
  %59 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %38, <16 x i16> %57) #9
  %60 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 36), align 16
  %61 = trunc i32 %60 to i16
  %62 = shl i16 %61, 3
  %63 = sub i16 0, %62
  %64 = insertelement <16 x i16> undef, i16 %63, i32 0
  %65 = shufflevector <16 x i16> %64, <16 x i16> undef, <16 x i32> zeroinitializer
  %66 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 28), align 16
  %67 = trunc i32 %66 to i16
  %68 = shl i16 %67, 3
  %69 = insertelement <16 x i16> undef, i16 %68, i32 0
  %70 = shufflevector <16 x i16> %69, <16 x i16> undef, <16 x i32> zeroinitializer
  %71 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %47, <16 x i16> %65) #9
  %72 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %47, <16 x i16> %70) #9
  %73 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 44), align 16
  %74 = trunc i32 %73 to i16
  %75 = shl i16 %74, 3
  %76 = insertelement <16 x i16> undef, i16 %75, i32 0
  %77 = shufflevector <16 x i16> %76, <16 x i16> undef, <16 x i32> zeroinitializer
  %78 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 20), align 16
  %79 = trunc i32 %78 to i16
  %80 = shl i16 %79, 3
  %81 = insertelement <16 x i16> undef, i16 %80, i32 0
  %82 = shufflevector <16 x i16> %81, <16 x i16> undef, <16 x i32> zeroinitializer
  %83 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %41, <16 x i16> %77) #9
  %84 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %41, <16 x i16> %82) #9
  %85 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 52), align 16
  %86 = trunc i32 %85 to i16
  %87 = shl i16 %86, 3
  %88 = sub i16 0, %87
  %89 = insertelement <16 x i16> undef, i16 %88, i32 0
  %90 = shufflevector <16 x i16> %89, <16 x i16> undef, <16 x i32> zeroinitializer
  %91 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 12), align 16
  %92 = trunc i32 %91 to i16
  %93 = shl i16 %92, 3
  %94 = insertelement <16 x i16> undef, i16 %93, i32 0
  %95 = shufflevector <16 x i16> %94, <16 x i16> undef, <16 x i32> zeroinitializer
  %96 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %44, <16 x i16> %90) #9
  %97 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %44, <16 x i16> %95) #9
  %98 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 56), align 16
  %99 = trunc i32 %98 to i16
  %100 = shl i16 %99, 3
  %101 = insertelement <16 x i16> undef, i16 %100, i32 0
  %102 = shufflevector <16 x i16> %101, <16 x i16> undef, <16 x i32> zeroinitializer
  %103 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 8), align 16
  %104 = trunc i32 %103 to i16
  %105 = shl i16 %104, 3
  %106 = insertelement <16 x i16> undef, i16 %105, i32 0
  %107 = shufflevector <16 x i16> %106, <16 x i16> undef, <16 x i32> zeroinitializer
  %108 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %32, <16 x i16> %102) #9
  %109 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %32, <16 x i16> %107) #9
  %110 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 40), align 16
  %111 = trunc i32 %110 to i16
  %112 = shl i16 %111, 3
  %113 = sub i16 0, %112
  %114 = insertelement <16 x i16> undef, i16 %113, i32 0
  %115 = shufflevector <16 x i16> %114, <16 x i16> undef, <16 x i32> zeroinitializer
  %116 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 24), align 16
  %117 = trunc i32 %116 to i16
  %118 = shl i16 %117, 3
  %119 = insertelement <16 x i16> undef, i16 %118, i32 0
  %120 = shufflevector <16 x i16> %119, <16 x i16> undef, <16 x i32> zeroinitializer
  %121 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %35, <16 x i16> %115) #9
  %122 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %35, <16 x i16> %120) #9
  %123 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %58, <16 x i16> %71) #9
  %124 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %58, <16 x i16> %71) #9
  %125 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %96, <16 x i16> %83) #9
  %126 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %96, <16 x i16> %83) #9
  %127 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %97, <16 x i16> %84) #9
  %128 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %97, <16 x i16> %84) #9
  %129 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %59, <16 x i16> %72) #9
  %130 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %59, <16 x i16> %72) #9
  %131 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %132 = trunc i32 %131 to i16
  %133 = shl i16 %132, 3
  %134 = insertelement <16 x i16> undef, i16 %133, i32 0
  %135 = shufflevector <16 x i16> %134, <16 x i16> undef, <16 x i32> zeroinitializer
  %136 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %26, <16 x i16> %135) #9
  %137 = shl i16 %8, 3
  %138 = insertelement <16 x i16> undef, i16 %137, i32 0
  %139 = shufflevector <16 x i16> %138, <16 x i16> undef, <16 x i32> zeroinitializer
  %140 = shl i16 %5, 3
  %141 = insertelement <16 x i16> undef, i16 %140, i32 0
  %142 = shufflevector <16 x i16> %141, <16 x i16> undef, <16 x i32> zeroinitializer
  %143 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %29, <16 x i16> %139) #9
  %144 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %29, <16 x i16> %142) #9
  %145 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %108, <16 x i16> %121) #9
  %146 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %108, <16 x i16> %121) #9
  %147 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %109, <16 x i16> %122) #9
  %148 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %109, <16 x i16> %122) #9
  %149 = sext i8 %2 to i32
  %150 = shufflevector <16 x i16> %124, <16 x i16> %130, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %151 = shufflevector <16 x i16> %124, <16 x i16> %130, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %152 = bitcast <8 x i32> %14 to <16 x i16>
  %153 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %150, <16 x i16> %152) #9
  %154 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %151, <16 x i16> %152) #9
  %155 = bitcast <8 x i32> %18 to <16 x i16>
  %156 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %150, <16 x i16> %155) #9
  %157 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %151, <16 x i16> %155) #9
  %158 = add <8 x i32> %153, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %159 = add <8 x i32> %154, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %160 = add <8 x i32> %156, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %161 = add <8 x i32> %157, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %162 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %158, i32 %149) #9
  %163 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %159, i32 %149) #9
  %164 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %160, i32 %149) #9
  %165 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %161, i32 %149) #9
  %166 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %162, <8 x i32> %163) #9
  %167 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %164, <8 x i32> %165) #9
  %168 = shufflevector <16 x i16> %126, <16 x i16> %128, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %169 = shufflevector <16 x i16> %126, <16 x i16> %128, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %170 = bitcast <8 x i32> %24 to <16 x i16>
  %171 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %168, <16 x i16> %170) #9
  %172 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %169, <16 x i16> %170) #9
  %173 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %168, <16 x i16> %152) #9
  %174 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %169, <16 x i16> %152) #9
  %175 = add <8 x i32> %171, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %176 = add <8 x i32> %172, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %177 = add <8 x i32> %173, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %178 = add <8 x i32> %174, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %179 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %175, i32 %149) #9
  %180 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %176, i32 %149) #9
  %181 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %177, i32 %149) #9
  %182 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %178, i32 %149) #9
  %183 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %179, <8 x i32> %180) #9
  %184 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %181, <8 x i32> %182) #9
  %185 = sub i32 0, %131
  %186 = and i32 %185, 65535
  %187 = and i32 %131, 65535
  %188 = shl nuw i32 %187, 16
  %189 = or i32 %188, %186
  %190 = insertelement <8 x i32> undef, i32 %189, i32 0
  %191 = shufflevector <8 x i32> %190, <8 x i32> undef, <8 x i32> zeroinitializer
  %192 = or i32 %188, %187
  %193 = insertelement <8 x i32> undef, i32 %192, i32 0
  %194 = shufflevector <8 x i32> %193, <8 x i32> undef, <8 x i32> zeroinitializer
  %195 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %136, <16 x i16> %144) #9
  %196 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %136, <16 x i16> %144) #9
  %197 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %136, <16 x i16> %143) #9
  %198 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %136, <16 x i16> %143) #9
  %199 = shufflevector <16 x i16> %146, <16 x i16> %148, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %200 = shufflevector <16 x i16> %146, <16 x i16> %148, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %201 = bitcast <8 x i32> %191 to <16 x i16>
  %202 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %199, <16 x i16> %201) #9
  %203 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %200, <16 x i16> %201) #9
  %204 = bitcast <8 x i32> %194 to <16 x i16>
  %205 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %199, <16 x i16> %204) #9
  %206 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %200, <16 x i16> %204) #9
  %207 = add <8 x i32> %202, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %208 = add <8 x i32> %203, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %209 = add <8 x i32> %205, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %210 = add <8 x i32> %206, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %211 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %207, i32 %149) #9
  %212 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %208, i32 %149) #9
  %213 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %209, i32 %149) #9
  %214 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %210, i32 %149) #9
  %215 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %211, <8 x i32> %212) #9
  %216 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %213, <8 x i32> %214) #9
  %217 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %123, <16 x i16> %125) #9
  %218 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %123, <16 x i16> %125) #9
  %219 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %166, <16 x i16> %183) #9
  %220 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %166, <16 x i16> %183) #9
  %221 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %129, <16 x i16> %127) #9
  %222 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %129, <16 x i16> %127) #9
  %223 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %167, <16 x i16> %184) #9
  %224 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %167, <16 x i16> %184) #9
  %225 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %195, <16 x i16> %147) #9
  %226 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %195, <16 x i16> %147) #9
  %227 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %197, <16 x i16> %216) #9
  %228 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %197, <16 x i16> %216) #9
  %229 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %198, <16 x i16> %215) #9
  %230 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %198, <16 x i16> %215) #9
  %231 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %196, <16 x i16> %145) #9
  %232 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %196, <16 x i16> %145) #9
  %233 = shufflevector <16 x i16> %220, <16 x i16> %224, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %234 = shufflevector <16 x i16> %220, <16 x i16> %224, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %235 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %233, <16 x i16> %201) #9
  %236 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %234, <16 x i16> %201) #9
  %237 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %233, <16 x i16> %204) #9
  %238 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %234, <16 x i16> %204) #9
  %239 = add <8 x i32> %235, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %240 = add <8 x i32> %236, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %241 = add <8 x i32> %237, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %242 = add <8 x i32> %238, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %243 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %239, i32 %149) #9
  %244 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %240, i32 %149) #9
  %245 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %241, i32 %149) #9
  %246 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %242, i32 %149) #9
  %247 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %243, <8 x i32> %244) #9
  %248 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %245, <8 x i32> %246) #9
  %249 = shufflevector <16 x i16> %218, <16 x i16> %222, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %250 = shufflevector <16 x i16> %218, <16 x i16> %222, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %251 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %249, <16 x i16> %201) #9
  %252 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %250, <16 x i16> %201) #9
  %253 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %249, <16 x i16> %204) #9
  %254 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %250, <16 x i16> %204) #9
  %255 = add <8 x i32> %251, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %256 = add <8 x i32> %252, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %257 = add <8 x i32> %253, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %258 = add <8 x i32> %254, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %259 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %255, i32 %149) #9
  %260 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %256, i32 %149) #9
  %261 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %257, i32 %149) #9
  %262 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %258, i32 %149) #9
  %263 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %259, <8 x i32> %260) #9
  %264 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %261, <8 x i32> %262) #9
  %265 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 15
  %266 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %225, <16 x i16> %221) #9
  %267 = bitcast <4 x i64>* %1 to <16 x i16>*
  store <16 x i16> %266, <16 x i16>* %267, align 32
  %268 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %225, <16 x i16> %221) #9
  %269 = bitcast <4 x i64>* %265 to <16 x i16>*
  store <16 x i16> %268, <16 x i16>* %269, align 32
  %270 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %271 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %272 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %227, <16 x i16> %223) #9
  %273 = bitcast <4 x i64>* %270 to <16 x i16>*
  store <16 x i16> %272, <16 x i16>* %273, align 32
  %274 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %227, <16 x i16> %223) #9
  %275 = bitcast <4 x i64>* %271 to <16 x i16>*
  store <16 x i16> %274, <16 x i16>* %275, align 32
  %276 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %277 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %278 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %229, <16 x i16> %248) #9
  %279 = bitcast <4 x i64>* %276 to <16 x i16>*
  store <16 x i16> %278, <16 x i16>* %279, align 32
  %280 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %229, <16 x i16> %248) #9
  %281 = bitcast <4 x i64>* %277 to <16 x i16>*
  store <16 x i16> %280, <16 x i16>* %281, align 32
  %282 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %283 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %284 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %231, <16 x i16> %264) #9
  %285 = bitcast <4 x i64>* %282 to <16 x i16>*
  store <16 x i16> %284, <16 x i16>* %285, align 32
  %286 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %231, <16 x i16> %264) #9
  %287 = bitcast <4 x i64>* %283 to <16 x i16>*
  store <16 x i16> %286, <16 x i16>* %287, align 32
  %288 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %289 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %290 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %232, <16 x i16> %263) #9
  %291 = bitcast <4 x i64>* %288 to <16 x i16>*
  store <16 x i16> %290, <16 x i16>* %291, align 32
  %292 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %232, <16 x i16> %263) #9
  %293 = bitcast <4 x i64>* %289 to <16 x i16>*
  store <16 x i16> %292, <16 x i16>* %293, align 32
  %294 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %295 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %296 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %230, <16 x i16> %247) #9
  %297 = bitcast <4 x i64>* %294 to <16 x i16>*
  store <16 x i16> %296, <16 x i16>* %297, align 32
  %298 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %230, <16 x i16> %247) #9
  %299 = bitcast <4 x i64>* %295 to <16 x i16>*
  store <16 x i16> %298, <16 x i16>* %299, align 32
  %300 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %301 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %302 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %228, <16 x i16> %219) #9
  %303 = bitcast <4 x i64>* %300 to <16 x i16>*
  store <16 x i16> %302, <16 x i16>* %303, align 32
  %304 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %228, <16 x i16> %219) #9
  %305 = bitcast <4 x i64>* %301 to <16 x i16>*
  store <16 x i16> %304, <16 x i16>* %305, align 32
  %306 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %307 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %308 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %226, <16 x i16> %217) #9
  %309 = bitcast <4 x i64>* %306 to <16 x i16>*
  store <16 x i16> %308, <16 x i16>* %309, align 32
  %310 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %226, <16 x i16> %217) #9
  %311 = bitcast <4 x i64>* %307 to <16 x i16>*
  store <16 x i16> %310, <16 x i16>* %311, align 32
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @idct16_avx2(<4 x i64>* nocapture readonly, <4 x i64>* nocapture, i8 signext) #0 {
  %4 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 60), align 16
  %5 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 4), align 16
  %6 = and i32 %4, 65535
  %7 = shl i32 %5, 16
  %8 = sub i32 0, %7
  %9 = or i32 %6, %8
  %10 = insertelement <8 x i32> undef, i32 %9, i32 0
  %11 = shufflevector <8 x i32> %10, <8 x i32> undef, <8 x i32> zeroinitializer
  %12 = and i32 %5, 65535
  %13 = shl nuw i32 %6, 16
  %14 = or i32 %13, %12
  %15 = insertelement <8 x i32> undef, i32 %14, i32 0
  %16 = shufflevector <8 x i32> %15, <8 x i32> undef, <8 x i32> zeroinitializer
  %17 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 28), align 16
  %18 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 36), align 16
  %19 = and i32 %17, 65535
  %20 = shl i32 %18, 16
  %21 = sub i32 0, %20
  %22 = or i32 %19, %21
  %23 = insertelement <8 x i32> undef, i32 %22, i32 0
  %24 = shufflevector <8 x i32> %23, <8 x i32> undef, <8 x i32> zeroinitializer
  %25 = and i32 %18, 65535
  %26 = shl nuw i32 %19, 16
  %27 = or i32 %26, %25
  %28 = insertelement <8 x i32> undef, i32 %27, i32 0
  %29 = shufflevector <8 x i32> %28, <8 x i32> undef, <8 x i32> zeroinitializer
  %30 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 44), align 16
  %31 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 20), align 16
  %32 = and i32 %30, 65535
  %33 = shl i32 %31, 16
  %34 = sub i32 0, %33
  %35 = or i32 %32, %34
  %36 = insertelement <8 x i32> undef, i32 %35, i32 0
  %37 = shufflevector <8 x i32> %36, <8 x i32> undef, <8 x i32> zeroinitializer
  %38 = and i32 %31, 65535
  %39 = shl nuw i32 %32, 16
  %40 = or i32 %39, %38
  %41 = insertelement <8 x i32> undef, i32 %40, i32 0
  %42 = shufflevector <8 x i32> %41, <8 x i32> undef, <8 x i32> zeroinitializer
  %43 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 12), align 16
  %44 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 52), align 16
  %45 = and i32 %43, 65535
  %46 = shl i32 %44, 16
  %47 = sub i32 0, %46
  %48 = or i32 %45, %47
  %49 = insertelement <8 x i32> undef, i32 %48, i32 0
  %50 = shufflevector <8 x i32> %49, <8 x i32> undef, <8 x i32> zeroinitializer
  %51 = and i32 %44, 65535
  %52 = shl nuw i32 %45, 16
  %53 = or i32 %52, %51
  %54 = insertelement <8 x i32> undef, i32 %53, i32 0
  %55 = shufflevector <8 x i32> %54, <8 x i32> undef, <8 x i32> zeroinitializer
  %56 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 56), align 16
  %57 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 8), align 16
  %58 = and i32 %56, 65535
  %59 = shl i32 %57, 16
  %60 = sub i32 0, %59
  %61 = or i32 %58, %60
  %62 = insertelement <8 x i32> undef, i32 %61, i32 0
  %63 = shufflevector <8 x i32> %62, <8 x i32> undef, <8 x i32> zeroinitializer
  %64 = and i32 %57, 65535
  %65 = shl nuw i32 %58, 16
  %66 = or i32 %65, %64
  %67 = insertelement <8 x i32> undef, i32 %66, i32 0
  %68 = shufflevector <8 x i32> %67, <8 x i32> undef, <8 x i32> zeroinitializer
  %69 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 24), align 16
  %70 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 40), align 16
  %71 = and i32 %69, 65535
  %72 = shl i32 %70, 16
  %73 = sub i32 0, %72
  %74 = or i32 %71, %73
  %75 = insertelement <8 x i32> undef, i32 %74, i32 0
  %76 = shufflevector <8 x i32> %75, <8 x i32> undef, <8 x i32> zeroinitializer
  %77 = and i32 %70, 65535
  %78 = shl nuw i32 %71, 16
  %79 = or i32 %78, %77
  %80 = insertelement <8 x i32> undef, i32 %79, i32 0
  %81 = shufflevector <8 x i32> %80, <8 x i32> undef, <8 x i32> zeroinitializer
  %82 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %83 = and i32 %82, 65535
  %84 = shl nuw i32 %83, 16
  %85 = or i32 %84, %83
  %86 = insertelement <8 x i32> undef, i32 %85, i32 0
  %87 = shufflevector <8 x i32> %86, <8 x i32> undef, <8 x i32> zeroinitializer
  %88 = shl i32 %82, 16
  %89 = sub i32 0, %88
  %90 = or i32 %83, %89
  %91 = insertelement <8 x i32> undef, i32 %90, i32 0
  %92 = shufflevector <8 x i32> %91, <8 x i32> undef, <8 x i32> zeroinitializer
  %93 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 48), align 16
  %94 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 16), align 16
  %95 = sub i32 0, %94
  %96 = and i32 %93, 65535
  %97 = and i32 %95, 65535
  %98 = shl nuw i32 %97, 16
  %99 = or i32 %98, %96
  %100 = insertelement <8 x i32> undef, i32 %99, i32 0
  %101 = shufflevector <8 x i32> %100, <8 x i32> undef, <8 x i32> zeroinitializer
  %102 = and i32 %94, 65535
  %103 = shl nuw i32 %96, 16
  %104 = or i32 %103, %102
  %105 = insertelement <8 x i32> undef, i32 %104, i32 0
  %106 = shufflevector <8 x i32> %105, <8 x i32> undef, <8 x i32> zeroinitializer
  %107 = or i32 %103, %97
  %108 = insertelement <8 x i32> undef, i32 %107, i32 0
  %109 = shufflevector <8 x i32> %108, <8 x i32> undef, <8 x i32> zeroinitializer
  %110 = shl nuw i32 %102, 16
  %111 = or i32 %110, %96
  %112 = insertelement <8 x i32> undef, i32 %111, i32 0
  %113 = shufflevector <8 x i32> %112, <8 x i32> undef, <8 x i32> zeroinitializer
  %114 = sub i32 0, %93
  %115 = and i32 %114, 65535
  %116 = or i32 %98, %115
  %117 = insertelement <8 x i32> undef, i32 %116, i32 0
  %118 = shufflevector <8 x i32> %117, <8 x i32> undef, <8 x i32> zeroinitializer
  %119 = bitcast <4 x i64>* %0 to <16 x i16>*
  %120 = load <16 x i16>, <16 x i16>* %119, align 32
  %121 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 8
  %122 = bitcast <4 x i64>* %121 to <16 x i16>*
  %123 = load <16 x i16>, <16 x i16>* %122, align 32
  %124 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 4
  %125 = bitcast <4 x i64>* %124 to <16 x i16>*
  %126 = load <16 x i16>, <16 x i16>* %125, align 32
  %127 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 12
  %128 = bitcast <4 x i64>* %127 to <16 x i16>*
  %129 = load <16 x i16>, <16 x i16>* %128, align 32
  %130 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 2
  %131 = bitcast <4 x i64>* %130 to <16 x i16>*
  %132 = load <16 x i16>, <16 x i16>* %131, align 32
  %133 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 10
  %134 = bitcast <4 x i64>* %133 to <16 x i16>*
  %135 = load <16 x i16>, <16 x i16>* %134, align 32
  %136 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 6
  %137 = bitcast <4 x i64>* %136 to <16 x i16>*
  %138 = load <16 x i16>, <16 x i16>* %137, align 32
  %139 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 14
  %140 = bitcast <4 x i64>* %139 to <16 x i16>*
  %141 = load <16 x i16>, <16 x i16>* %140, align 32
  %142 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 1
  %143 = bitcast <4 x i64>* %142 to <16 x i16>*
  %144 = load <16 x i16>, <16 x i16>* %143, align 32
  %145 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 9
  %146 = bitcast <4 x i64>* %145 to <16 x i16>*
  %147 = load <16 x i16>, <16 x i16>* %146, align 32
  %148 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 5
  %149 = bitcast <4 x i64>* %148 to <16 x i16>*
  %150 = load <16 x i16>, <16 x i16>* %149, align 32
  %151 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 13
  %152 = bitcast <4 x i64>* %151 to <16 x i16>*
  %153 = load <16 x i16>, <16 x i16>* %152, align 32
  %154 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 3
  %155 = bitcast <4 x i64>* %154 to <16 x i16>*
  %156 = load <16 x i16>, <16 x i16>* %155, align 32
  %157 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 11
  %158 = bitcast <4 x i64>* %157 to <16 x i16>*
  %159 = load <16 x i16>, <16 x i16>* %158, align 32
  %160 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 7
  %161 = bitcast <4 x i64>* %160 to <16 x i16>*
  %162 = load <16 x i16>, <16 x i16>* %161, align 32
  %163 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 15
  %164 = bitcast <4 x i64>* %163 to <16 x i16>*
  %165 = load <16 x i16>, <16 x i16>* %164, align 32
  %166 = sext i8 %2 to i32
  %167 = shufflevector <16 x i16> %144, <16 x i16> %165, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %168 = shufflevector <16 x i16> %144, <16 x i16> %165, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %169 = bitcast <8 x i32> %11 to <16 x i16>
  %170 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %167, <16 x i16> %169) #9
  %171 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %168, <16 x i16> %169) #9
  %172 = bitcast <8 x i32> %16 to <16 x i16>
  %173 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %167, <16 x i16> %172) #9
  %174 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %168, <16 x i16> %172) #9
  %175 = add <8 x i32> %170, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %176 = add <8 x i32> %171, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %177 = add <8 x i32> %173, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %178 = add <8 x i32> %174, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %179 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %175, i32 %166) #9
  %180 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %176, i32 %166) #9
  %181 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %177, i32 %166) #9
  %182 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %178, i32 %166) #9
  %183 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %179, <8 x i32> %180) #9
  %184 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %181, <8 x i32> %182) #9
  %185 = shufflevector <16 x i16> %147, <16 x i16> %162, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %186 = shufflevector <16 x i16> %147, <16 x i16> %162, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %187 = bitcast <8 x i32> %24 to <16 x i16>
  %188 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %185, <16 x i16> %187) #9
  %189 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %186, <16 x i16> %187) #9
  %190 = bitcast <8 x i32> %29 to <16 x i16>
  %191 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %185, <16 x i16> %190) #9
  %192 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %186, <16 x i16> %190) #9
  %193 = add <8 x i32> %188, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %194 = add <8 x i32> %189, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %195 = add <8 x i32> %191, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %196 = add <8 x i32> %192, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %197 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %193, i32 %166) #9
  %198 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %194, i32 %166) #9
  %199 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %195, i32 %166) #9
  %200 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %196, i32 %166) #9
  %201 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %197, <8 x i32> %198) #9
  %202 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %199, <8 x i32> %200) #9
  %203 = shufflevector <16 x i16> %150, <16 x i16> %159, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %204 = shufflevector <16 x i16> %150, <16 x i16> %159, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %205 = bitcast <8 x i32> %37 to <16 x i16>
  %206 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %203, <16 x i16> %205) #9
  %207 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %204, <16 x i16> %205) #9
  %208 = bitcast <8 x i32> %42 to <16 x i16>
  %209 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %203, <16 x i16> %208) #9
  %210 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %204, <16 x i16> %208) #9
  %211 = add <8 x i32> %206, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %212 = add <8 x i32> %207, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %213 = add <8 x i32> %209, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %214 = add <8 x i32> %210, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %215 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %211, i32 %166) #9
  %216 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %212, i32 %166) #9
  %217 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %213, i32 %166) #9
  %218 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %214, i32 %166) #9
  %219 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %215, <8 x i32> %216) #9
  %220 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %217, <8 x i32> %218) #9
  %221 = shufflevector <16 x i16> %153, <16 x i16> %156, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %222 = shufflevector <16 x i16> %153, <16 x i16> %156, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %223 = bitcast <8 x i32> %50 to <16 x i16>
  %224 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %221, <16 x i16> %223) #9
  %225 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %222, <16 x i16> %223) #9
  %226 = bitcast <8 x i32> %55 to <16 x i16>
  %227 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %221, <16 x i16> %226) #9
  %228 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %222, <16 x i16> %226) #9
  %229 = add <8 x i32> %224, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %230 = add <8 x i32> %225, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %231 = add <8 x i32> %227, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %232 = add <8 x i32> %228, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %233 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %229, i32 %166) #9
  %234 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %230, i32 %166) #9
  %235 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %231, i32 %166) #9
  %236 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %232, i32 %166) #9
  %237 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %233, <8 x i32> %234) #9
  %238 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %235, <8 x i32> %236) #9
  %239 = shufflevector <16 x i16> %132, <16 x i16> %141, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %240 = shufflevector <16 x i16> %132, <16 x i16> %141, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %241 = bitcast <8 x i32> %63 to <16 x i16>
  %242 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %239, <16 x i16> %241) #9
  %243 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %240, <16 x i16> %241) #9
  %244 = bitcast <8 x i32> %68 to <16 x i16>
  %245 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %239, <16 x i16> %244) #9
  %246 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %240, <16 x i16> %244) #9
  %247 = add <8 x i32> %242, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %248 = add <8 x i32> %243, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %249 = add <8 x i32> %245, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %250 = add <8 x i32> %246, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %251 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %247, i32 %166) #9
  %252 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %248, i32 %166) #9
  %253 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %249, i32 %166) #9
  %254 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %250, i32 %166) #9
  %255 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %251, <8 x i32> %252) #9
  %256 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %253, <8 x i32> %254) #9
  %257 = shufflevector <16 x i16> %135, <16 x i16> %138, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %258 = shufflevector <16 x i16> %135, <16 x i16> %138, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %259 = bitcast <8 x i32> %76 to <16 x i16>
  %260 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %257, <16 x i16> %259) #9
  %261 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %258, <16 x i16> %259) #9
  %262 = bitcast <8 x i32> %81 to <16 x i16>
  %263 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %257, <16 x i16> %262) #9
  %264 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %258, <16 x i16> %262) #9
  %265 = add <8 x i32> %260, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %266 = add <8 x i32> %261, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %267 = add <8 x i32> %263, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %268 = add <8 x i32> %264, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %269 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %265, i32 %166) #9
  %270 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %266, i32 %166) #9
  %271 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %267, i32 %166) #9
  %272 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %268, i32 %166) #9
  %273 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %269, <8 x i32> %270) #9
  %274 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %271, <8 x i32> %272) #9
  %275 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %183, <16 x i16> %201) #9
  %276 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %183, <16 x i16> %201) #9
  %277 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %237, <16 x i16> %219) #9
  %278 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %237, <16 x i16> %219) #9
  %279 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %238, <16 x i16> %220) #9
  %280 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %238, <16 x i16> %220) #9
  %281 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %184, <16 x i16> %202) #9
  %282 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %184, <16 x i16> %202) #9
  %283 = shufflevector <16 x i16> %120, <16 x i16> %123, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %284 = shufflevector <16 x i16> %120, <16 x i16> %123, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %285 = bitcast <8 x i32> %87 to <16 x i16>
  %286 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %283, <16 x i16> %285) #9
  %287 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %284, <16 x i16> %285) #9
  %288 = bitcast <8 x i32> %92 to <16 x i16>
  %289 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %283, <16 x i16> %288) #9
  %290 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %284, <16 x i16> %288) #9
  %291 = add <8 x i32> %286, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %292 = add <8 x i32> %287, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %293 = add <8 x i32> %289, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %294 = add <8 x i32> %290, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %295 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %291, i32 %166) #9
  %296 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %292, i32 %166) #9
  %297 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %293, i32 %166) #9
  %298 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %294, i32 %166) #9
  %299 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %295, <8 x i32> %296) #9
  %300 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %297, <8 x i32> %298) #9
  %301 = shufflevector <16 x i16> %126, <16 x i16> %129, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %302 = shufflevector <16 x i16> %126, <16 x i16> %129, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %303 = bitcast <8 x i32> %101 to <16 x i16>
  %304 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %301, <16 x i16> %303) #9
  %305 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %302, <16 x i16> %303) #9
  %306 = bitcast <8 x i32> %106 to <16 x i16>
  %307 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %301, <16 x i16> %306) #9
  %308 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %302, <16 x i16> %306) #9
  %309 = add <8 x i32> %304, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %310 = add <8 x i32> %305, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %311 = add <8 x i32> %307, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %312 = add <8 x i32> %308, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %313 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %309, i32 %166) #9
  %314 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %310, i32 %166) #9
  %315 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %311, i32 %166) #9
  %316 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %312, i32 %166) #9
  %317 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %313, <8 x i32> %314) #9
  %318 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %315, <8 x i32> %316) #9
  %319 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %255, <16 x i16> %273) #9
  %320 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %255, <16 x i16> %273) #9
  %321 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %256, <16 x i16> %274) #9
  %322 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %256, <16 x i16> %274) #9
  %323 = shufflevector <16 x i16> %276, <16 x i16> %282, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %324 = shufflevector <16 x i16> %276, <16 x i16> %282, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %325 = bitcast <8 x i32> %109 to <16 x i16>
  %326 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %323, <16 x i16> %325) #9
  %327 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %324, <16 x i16> %325) #9
  %328 = bitcast <8 x i32> %113 to <16 x i16>
  %329 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %323, <16 x i16> %328) #9
  %330 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %324, <16 x i16> %328) #9
  %331 = add <8 x i32> %326, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %332 = add <8 x i32> %327, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %333 = add <8 x i32> %329, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %334 = add <8 x i32> %330, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %335 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %331, i32 %166) #9
  %336 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %332, i32 %166) #9
  %337 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %333, i32 %166) #9
  %338 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %334, i32 %166) #9
  %339 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %335, <8 x i32> %336) #9
  %340 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %337, <8 x i32> %338) #9
  %341 = shufflevector <16 x i16> %278, <16 x i16> %280, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %342 = shufflevector <16 x i16> %278, <16 x i16> %280, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %343 = bitcast <8 x i32> %118 to <16 x i16>
  %344 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %341, <16 x i16> %343) #9
  %345 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %342, <16 x i16> %343) #9
  %346 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %341, <16 x i16> %325) #9
  %347 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %342, <16 x i16> %325) #9
  %348 = add <8 x i32> %344, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %349 = add <8 x i32> %345, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %350 = add <8 x i32> %346, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %351 = add <8 x i32> %347, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %352 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %348, i32 %166) #9
  %353 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %349, i32 %166) #9
  %354 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %350, i32 %166) #9
  %355 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %351, i32 %166) #9
  %356 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %352, <8 x i32> %353) #9
  %357 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %354, <8 x i32> %355) #9
  %358 = sub i32 0, %82
  %359 = and i32 %358, 65535
  %360 = or i32 %84, %359
  %361 = insertelement <8 x i32> undef, i32 %360, i32 0
  %362 = shufflevector <8 x i32> %361, <8 x i32> undef, <8 x i32> zeroinitializer
  %363 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %299, <16 x i16> %318) #9
  %364 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %299, <16 x i16> %318) #9
  %365 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %300, <16 x i16> %317) #9
  %366 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %300, <16 x i16> %317) #9
  %367 = shufflevector <16 x i16> %320, <16 x i16> %322, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %368 = shufflevector <16 x i16> %320, <16 x i16> %322, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %369 = bitcast <8 x i32> %362 to <16 x i16>
  %370 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %367, <16 x i16> %369) #9
  %371 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %368, <16 x i16> %369) #9
  %372 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %367, <16 x i16> %285) #9
  %373 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %368, <16 x i16> %285) #9
  %374 = add <8 x i32> %370, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %375 = add <8 x i32> %371, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %376 = add <8 x i32> %372, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %377 = add <8 x i32> %373, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %378 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %374, i32 %166) #9
  %379 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %375, i32 %166) #9
  %380 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %376, i32 %166) #9
  %381 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %377, i32 %166) #9
  %382 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %378, <8 x i32> %379) #9
  %383 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %380, <8 x i32> %381) #9
  %384 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %275, <16 x i16> %277) #9
  %385 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %275, <16 x i16> %277) #9
  %386 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %339, <16 x i16> %356) #9
  %387 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %339, <16 x i16> %356) #9
  %388 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %281, <16 x i16> %279) #9
  %389 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %281, <16 x i16> %279) #9
  %390 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %340, <16 x i16> %357) #9
  %391 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %340, <16 x i16> %357) #9
  %392 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %363, <16 x i16> %321) #9
  %393 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %363, <16 x i16> %321) #9
  %394 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %365, <16 x i16> %383) #9
  %395 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %365, <16 x i16> %383) #9
  %396 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %366, <16 x i16> %382) #9
  %397 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %366, <16 x i16> %382) #9
  %398 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %364, <16 x i16> %319) #9
  %399 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %364, <16 x i16> %319) #9
  %400 = shufflevector <16 x i16> %387, <16 x i16> %391, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %401 = shufflevector <16 x i16> %387, <16 x i16> %391, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %402 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %400, <16 x i16> %369) #9
  %403 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %401, <16 x i16> %369) #9
  %404 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %400, <16 x i16> %285) #9
  %405 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %401, <16 x i16> %285) #9
  %406 = add <8 x i32> %402, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %407 = add <8 x i32> %403, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %408 = add <8 x i32> %404, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %409 = add <8 x i32> %405, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %410 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %406, i32 %166) #9
  %411 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %407, i32 %166) #9
  %412 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %408, i32 %166) #9
  %413 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %409, i32 %166) #9
  %414 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %410, <8 x i32> %411) #9
  %415 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %412, <8 x i32> %413) #9
  %416 = shufflevector <16 x i16> %385, <16 x i16> %389, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %417 = shufflevector <16 x i16> %385, <16 x i16> %389, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %418 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %416, <16 x i16> %369) #9
  %419 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %417, <16 x i16> %369) #9
  %420 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %416, <16 x i16> %285) #9
  %421 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %417, <16 x i16> %285) #9
  %422 = add <8 x i32> %418, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %423 = add <8 x i32> %419, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %424 = add <8 x i32> %420, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %425 = add <8 x i32> %421, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %426 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %422, i32 %166) #9
  %427 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %423, i32 %166) #9
  %428 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %424, i32 %166) #9
  %429 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %425, i32 %166) #9
  %430 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %426, <8 x i32> %427) #9
  %431 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %428, <8 x i32> %429) #9
  %432 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 15
  %433 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %392, <16 x i16> %388) #9
  %434 = bitcast <4 x i64>* %1 to <16 x i16>*
  store <16 x i16> %433, <16 x i16>* %434, align 32
  %435 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %392, <16 x i16> %388) #9
  %436 = bitcast <4 x i64>* %432 to <16 x i16>*
  store <16 x i16> %435, <16 x i16>* %436, align 32
  %437 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %438 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %439 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %394, <16 x i16> %390) #9
  %440 = bitcast <4 x i64>* %437 to <16 x i16>*
  store <16 x i16> %439, <16 x i16>* %440, align 32
  %441 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %394, <16 x i16> %390) #9
  %442 = bitcast <4 x i64>* %438 to <16 x i16>*
  store <16 x i16> %441, <16 x i16>* %442, align 32
  %443 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %444 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %445 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %396, <16 x i16> %415) #9
  %446 = bitcast <4 x i64>* %443 to <16 x i16>*
  store <16 x i16> %445, <16 x i16>* %446, align 32
  %447 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %396, <16 x i16> %415) #9
  %448 = bitcast <4 x i64>* %444 to <16 x i16>*
  store <16 x i16> %447, <16 x i16>* %448, align 32
  %449 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %450 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %451 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %398, <16 x i16> %431) #9
  %452 = bitcast <4 x i64>* %449 to <16 x i16>*
  store <16 x i16> %451, <16 x i16>* %452, align 32
  %453 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %398, <16 x i16> %431) #9
  %454 = bitcast <4 x i64>* %450 to <16 x i16>*
  store <16 x i16> %453, <16 x i16>* %454, align 32
  %455 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %456 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %457 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %399, <16 x i16> %430) #9
  %458 = bitcast <4 x i64>* %455 to <16 x i16>*
  store <16 x i16> %457, <16 x i16>* %458, align 32
  %459 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %399, <16 x i16> %430) #9
  %460 = bitcast <4 x i64>* %456 to <16 x i16>*
  store <16 x i16> %459, <16 x i16>* %460, align 32
  %461 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %462 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %463 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %397, <16 x i16> %414) #9
  %464 = bitcast <4 x i64>* %461 to <16 x i16>*
  store <16 x i16> %463, <16 x i16>* %464, align 32
  %465 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %397, <16 x i16> %414) #9
  %466 = bitcast <4 x i64>* %462 to <16 x i16>*
  store <16 x i16> %465, <16 x i16>* %466, align 32
  %467 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %468 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %469 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %395, <16 x i16> %386) #9
  %470 = bitcast <4 x i64>* %467 to <16 x i16>*
  store <16 x i16> %469, <16 x i16>* %470, align 32
  %471 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %395, <16 x i16> %386) #9
  %472 = bitcast <4 x i64>* %468 to <16 x i16>*
  store <16 x i16> %471, <16 x i16>* %472, align 32
  %473 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %474 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %475 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %393, <16 x i16> %384) #9
  %476 = bitcast <4 x i64>* %473 to <16 x i16>*
  store <16 x i16> %475, <16 x i16>* %476, align 32
  %477 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %393, <16 x i16> %384) #9
  %478 = bitcast <4 x i64>* %474 to <16 x i16>*
  store <16 x i16> %477, <16 x i16>* %478, align 32
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @iadst16_low1_avx2(<4 x i64>* nocapture readonly, <4 x i64>* nocapture, i8 signext) #0 {
  %4 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 8), align 16
  %5 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 56), align 16
  %6 = and i32 %4, 65535
  %7 = and i32 %5, 65535
  %8 = shl nuw i32 %7, 16
  %9 = or i32 %8, %6
  %10 = insertelement <8 x i32> undef, i32 %9, i32 0
  %11 = shufflevector <8 x i32> %10, <8 x i32> undef, <8 x i32> zeroinitializer
  %12 = shl i32 %4, 16
  %13 = sub i32 0, %12
  %14 = or i32 %7, %13
  %15 = insertelement <8 x i32> undef, i32 %14, i32 0
  %16 = shufflevector <8 x i32> %15, <8 x i32> undef, <8 x i32> zeroinitializer
  %17 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 16), align 16
  %18 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 48), align 16
  %19 = and i32 %17, 65535
  %20 = and i32 %18, 65535
  %21 = shl nuw i32 %20, 16
  %22 = or i32 %21, %19
  %23 = insertelement <8 x i32> undef, i32 %22, i32 0
  %24 = shufflevector <8 x i32> %23, <8 x i32> undef, <8 x i32> zeroinitializer
  %25 = shl i32 %17, 16
  %26 = sub i32 0, %25
  %27 = or i32 %20, %26
  %28 = insertelement <8 x i32> undef, i32 %27, i32 0
  %29 = shufflevector <8 x i32> %28, <8 x i32> undef, <8 x i32> zeroinitializer
  %30 = bitcast <4 x i64>* %0 to <16 x i16>*
  %31 = load <16 x i16>, <16 x i16>* %30, align 32
  %32 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 62), align 8
  %33 = trunc i32 %32 to i16
  %34 = shl i16 %33, 3
  %35 = insertelement <16 x i16> undef, i16 %34, i32 0
  %36 = shufflevector <16 x i16> %35, <16 x i16> undef, <16 x i32> zeroinitializer
  %37 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 2), align 8
  %38 = trunc i32 %37 to i16
  %39 = shl i16 %38, 3
  %40 = sub i16 0, %39
  %41 = insertelement <16 x i16> undef, i16 %40, i32 0
  %42 = shufflevector <16 x i16> %41, <16 x i16> undef, <16 x i32> zeroinitializer
  %43 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %31, <16 x i16> %36) #9
  %44 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %31, <16 x i16> %42) #9
  %45 = sext i8 %2 to i32
  %46 = shufflevector <16 x i16> %43, <16 x i16> %44, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %47 = shufflevector <16 x i16> %43, <16 x i16> %44, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %48 = bitcast <8 x i32> %11 to <16 x i16>
  %49 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %46, <16 x i16> %48) #9
  %50 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %47, <16 x i16> %48) #9
  %51 = bitcast <8 x i32> %16 to <16 x i16>
  %52 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %46, <16 x i16> %51) #9
  %53 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %47, <16 x i16> %51) #9
  %54 = add <8 x i32> %49, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %55 = add <8 x i32> %50, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %56 = add <8 x i32> %52, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %57 = add <8 x i32> %53, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %58 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %54, i32 %45) #9
  %59 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %55, i32 %45) #9
  %60 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %56, i32 %45) #9
  %61 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %57, i32 %45) #9
  %62 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %58, <8 x i32> %59) #9
  %63 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %60, <8 x i32> %61) #9
  %64 = bitcast <8 x i32> %24 to <16 x i16>
  %65 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %46, <16 x i16> %64) #9
  %66 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %47, <16 x i16> %64) #9
  %67 = bitcast <8 x i32> %29 to <16 x i16>
  %68 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %46, <16 x i16> %67) #9
  %69 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %47, <16 x i16> %67) #9
  %70 = add <8 x i32> %65, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %71 = add <8 x i32> %66, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %72 = add <8 x i32> %68, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %73 = add <8 x i32> %69, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %74 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %70, i32 %45) #9
  %75 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %71, i32 %45) #9
  %76 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %72, i32 %45) #9
  %77 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %73, i32 %45) #9
  %78 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %74, <8 x i32> %75) #9
  %79 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %76, <8 x i32> %77) #9
  %80 = shufflevector <16 x i16> %62, <16 x i16> %63, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %81 = shufflevector <16 x i16> %62, <16 x i16> %63, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %82 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %80, <16 x i16> %64) #9
  %83 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %81, <16 x i16> %64) #9
  %84 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %80, <16 x i16> %67) #9
  %85 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %81, <16 x i16> %67) #9
  %86 = add <8 x i32> %82, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %87 = add <8 x i32> %83, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %88 = add <8 x i32> %84, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %89 = add <8 x i32> %85, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %90 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %86, i32 %45) #9
  %91 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %87, i32 %45) #9
  %92 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %88, i32 %45) #9
  %93 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %89, i32 %45) #9
  %94 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %90, <8 x i32> %91) #9
  %95 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %92, <8 x i32> %93) #9
  %96 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %97 = and i32 %96, 65535
  %98 = shl nuw i32 %97, 16
  %99 = or i32 %98, %97
  %100 = insertelement <8 x i32> undef, i32 %99, i32 0
  %101 = shufflevector <8 x i32> %100, <8 x i32> undef, <8 x i32> zeroinitializer
  %102 = shl i32 %96, 16
  %103 = sub i32 0, %102
  %104 = or i32 %97, %103
  %105 = insertelement <8 x i32> undef, i32 %104, i32 0
  %106 = shufflevector <8 x i32> %105, <8 x i32> undef, <8 x i32> zeroinitializer
  %107 = bitcast <8 x i32> %101 to <16 x i16>
  %108 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %46, <16 x i16> %107) #9
  %109 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %47, <16 x i16> %107) #9
  %110 = bitcast <8 x i32> %106 to <16 x i16>
  %111 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %46, <16 x i16> %110) #9
  %112 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %47, <16 x i16> %110) #9
  %113 = add <8 x i32> %108, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %114 = add <8 x i32> %109, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %115 = add <8 x i32> %111, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %116 = add <8 x i32> %112, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %117 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %113, i32 %45) #9
  %118 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %114, i32 %45) #9
  %119 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %115, i32 %45) #9
  %120 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %116, i32 %45) #9
  %121 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %117, <8 x i32> %118) #9
  %122 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %119, <8 x i32> %120) #9
  %123 = shufflevector <16 x i16> %78, <16 x i16> %79, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %124 = shufflevector <16 x i16> %78, <16 x i16> %79, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %125 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %123, <16 x i16> %107) #9
  %126 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %124, <16 x i16> %107) #9
  %127 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %123, <16 x i16> %110) #9
  %128 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %124, <16 x i16> %110) #9
  %129 = add <8 x i32> %125, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %130 = add <8 x i32> %126, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %131 = add <8 x i32> %127, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %132 = add <8 x i32> %128, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %133 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %129, i32 %45) #9
  %134 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %130, i32 %45) #9
  %135 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %131, i32 %45) #9
  %136 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %132, i32 %45) #9
  %137 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %133, <8 x i32> %134) #9
  %138 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %135, <8 x i32> %136) #9
  %139 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %80, <16 x i16> %107) #9
  %140 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %81, <16 x i16> %107) #9
  %141 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %80, <16 x i16> %110) #9
  %142 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %81, <16 x i16> %110) #9
  %143 = add <8 x i32> %139, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %144 = add <8 x i32> %140, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %145 = add <8 x i32> %141, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %146 = add <8 x i32> %142, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %147 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %143, i32 %45) #9
  %148 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %144, i32 %45) #9
  %149 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %145, i32 %45) #9
  %150 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %146, i32 %45) #9
  %151 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %147, <8 x i32> %148) #9
  %152 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %149, <8 x i32> %150) #9
  %153 = shufflevector <16 x i16> %94, <16 x i16> %95, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %154 = shufflevector <16 x i16> %94, <16 x i16> %95, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %155 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %153, <16 x i16> %107) #9
  %156 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %154, <16 x i16> %107) #9
  %157 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %153, <16 x i16> %110) #9
  %158 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %154, <16 x i16> %110) #9
  %159 = add <8 x i32> %155, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %160 = add <8 x i32> %156, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %161 = add <8 x i32> %157, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %162 = add <8 x i32> %158, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %163 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %159, i32 %45) #9
  %164 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %160, i32 %45) #9
  %165 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %161, i32 %45) #9
  %166 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %162, i32 %45) #9
  %167 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %163, <8 x i32> %164) #9
  %168 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %165, <8 x i32> %166) #9
  %169 = bitcast <4 x i64>* %1 to <16 x i16>*
  store <16 x i16> %43, <16 x i16>* %169, align 32
  %170 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %62) #9
  %171 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %172 = bitcast <4 x i64>* %171 to <16 x i16>*
  store <16 x i16> %170, <16 x i16>* %172, align 32
  %173 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %174 = bitcast <4 x i64>* %173 to <16 x i16>*
  store <16 x i16> %94, <16 x i16>* %174, align 32
  %175 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %78) #9
  %176 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %177 = bitcast <4 x i64>* %176 to <16 x i16>*
  store <16 x i16> %175, <16 x i16>* %177, align 32
  %178 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %179 = bitcast <4 x i64>* %178 to <16 x i16>*
  store <16 x i16> %137, <16 x i16>* %179, align 32
  %180 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %167) #9
  %181 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %182 = bitcast <4 x i64>* %181 to <16 x i16>*
  store <16 x i16> %180, <16 x i16>* %182, align 32
  %183 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %184 = bitcast <4 x i64>* %183 to <16 x i16>*
  store <16 x i16> %151, <16 x i16>* %184, align 32
  %185 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %121) #9
  %186 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %187 = bitcast <4 x i64>* %186 to <16 x i16>*
  store <16 x i16> %185, <16 x i16>* %187, align 32
  %188 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %189 = bitcast <4 x i64>* %188 to <16 x i16>*
  store <16 x i16> %122, <16 x i16>* %189, align 32
  %190 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %152) #9
  %191 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %192 = bitcast <4 x i64>* %191 to <16 x i16>*
  store <16 x i16> %190, <16 x i16>* %192, align 32
  %193 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %194 = bitcast <4 x i64>* %193 to <16 x i16>*
  store <16 x i16> %168, <16 x i16>* %194, align 32
  %195 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %138) #9
  %196 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %197 = bitcast <4 x i64>* %196 to <16 x i16>*
  store <16 x i16> %195, <16 x i16>* %197, align 32
  %198 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %199 = bitcast <4 x i64>* %198 to <16 x i16>*
  store <16 x i16> %79, <16 x i16>* %199, align 32
  %200 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %95) #9
  %201 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %202 = bitcast <4 x i64>* %201 to <16 x i16>*
  store <16 x i16> %200, <16 x i16>* %202, align 32
  %203 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %204 = bitcast <4 x i64>* %203 to <16 x i16>*
  store <16 x i16> %63, <16 x i16>* %204, align 32
  %205 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %44) #9
  %206 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 15
  %207 = bitcast <4 x i64>* %206 to <16 x i16>*
  store <16 x i16> %205, <16 x i16>* %207, align 32
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @iadst16_low8_avx2(<4 x i64>* nocapture readonly, <4 x i64>* nocapture, i8 signext) #0 {
  %4 = bitcast <4 x i64>* %0 to <16 x i16>*
  %5 = load <16 x i16>, <16 x i16>* %4, align 32
  %6 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 2
  %7 = bitcast <4 x i64>* %6 to <16 x i16>*
  %8 = load <16 x i16>, <16 x i16>* %7, align 32
  %9 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 4
  %10 = bitcast <4 x i64>* %9 to <16 x i16>*
  %11 = load <16 x i16>, <16 x i16>* %10, align 32
  %12 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 6
  %13 = bitcast <4 x i64>* %12 to <16 x i16>*
  %14 = load <16 x i16>, <16 x i16>* %13, align 32
  %15 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 7
  %16 = bitcast <4 x i64>* %15 to <16 x i16>*
  %17 = load <16 x i16>, <16 x i16>* %16, align 32
  %18 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 5
  %19 = bitcast <4 x i64>* %18 to <16 x i16>*
  %20 = load <16 x i16>, <16 x i16>* %19, align 32
  %21 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 3
  %22 = bitcast <4 x i64>* %21 to <16 x i16>*
  %23 = load <16 x i16>, <16 x i16>* %22, align 32
  %24 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 1
  %25 = bitcast <4 x i64>* %24 to <16 x i16>*
  %26 = load <16 x i16>, <16 x i16>* %25, align 32
  %27 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 62), align 8
  %28 = trunc i32 %27 to i16
  %29 = shl i16 %28, 3
  %30 = insertelement <16 x i16> undef, i16 %29, i32 0
  %31 = shufflevector <16 x i16> %30, <16 x i16> undef, <16 x i32> zeroinitializer
  %32 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 2), align 8
  %33 = trunc i32 %32 to i16
  %34 = shl i16 %33, 3
  %35 = sub i16 0, %34
  %36 = insertelement <16 x i16> undef, i16 %35, i32 0
  %37 = shufflevector <16 x i16> %36, <16 x i16> undef, <16 x i32> zeroinitializer
  %38 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %5, <16 x i16> %31) #9
  %39 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %5, <16 x i16> %37) #9
  %40 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 54), align 8
  %41 = trunc i32 %40 to i16
  %42 = shl i16 %41, 3
  %43 = insertelement <16 x i16> undef, i16 %42, i32 0
  %44 = shufflevector <16 x i16> %43, <16 x i16> undef, <16 x i32> zeroinitializer
  %45 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 10), align 8
  %46 = trunc i32 %45 to i16
  %47 = shl i16 %46, 3
  %48 = sub i16 0, %47
  %49 = insertelement <16 x i16> undef, i16 %48, i32 0
  %50 = shufflevector <16 x i16> %49, <16 x i16> undef, <16 x i32> zeroinitializer
  %51 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %8, <16 x i16> %44) #9
  %52 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %8, <16 x i16> %50) #9
  %53 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 46), align 8
  %54 = trunc i32 %53 to i16
  %55 = shl i16 %54, 3
  %56 = insertelement <16 x i16> undef, i16 %55, i32 0
  %57 = shufflevector <16 x i16> %56, <16 x i16> undef, <16 x i32> zeroinitializer
  %58 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 18), align 8
  %59 = trunc i32 %58 to i16
  %60 = shl i16 %59, 3
  %61 = sub i16 0, %60
  %62 = insertelement <16 x i16> undef, i16 %61, i32 0
  %63 = shufflevector <16 x i16> %62, <16 x i16> undef, <16 x i32> zeroinitializer
  %64 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %11, <16 x i16> %57) #9
  %65 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %11, <16 x i16> %63) #9
  %66 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 38), align 8
  %67 = trunc i32 %66 to i16
  %68 = shl i16 %67, 3
  %69 = insertelement <16 x i16> undef, i16 %68, i32 0
  %70 = shufflevector <16 x i16> %69, <16 x i16> undef, <16 x i32> zeroinitializer
  %71 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 26), align 8
  %72 = trunc i32 %71 to i16
  %73 = shl i16 %72, 3
  %74 = sub i16 0, %73
  %75 = insertelement <16 x i16> undef, i16 %74, i32 0
  %76 = shufflevector <16 x i16> %75, <16 x i16> undef, <16 x i32> zeroinitializer
  %77 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %14, <16 x i16> %70) #9
  %78 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %14, <16 x i16> %76) #9
  %79 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 34), align 8
  %80 = trunc i32 %79 to i16
  %81 = shl i16 %80, 3
  %82 = insertelement <16 x i16> undef, i16 %81, i32 0
  %83 = shufflevector <16 x i16> %82, <16 x i16> undef, <16 x i32> zeroinitializer
  %84 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 30), align 8
  %85 = trunc i32 %84 to i16
  %86 = shl i16 %85, 3
  %87 = insertelement <16 x i16> undef, i16 %86, i32 0
  %88 = shufflevector <16 x i16> %87, <16 x i16> undef, <16 x i32> zeroinitializer
  %89 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %17, <16 x i16> %83) #9
  %90 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %17, <16 x i16> %88) #9
  %91 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 42), align 8
  %92 = trunc i32 %91 to i16
  %93 = shl i16 %92, 3
  %94 = insertelement <16 x i16> undef, i16 %93, i32 0
  %95 = shufflevector <16 x i16> %94, <16 x i16> undef, <16 x i32> zeroinitializer
  %96 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 22), align 8
  %97 = trunc i32 %96 to i16
  %98 = shl i16 %97, 3
  %99 = insertelement <16 x i16> undef, i16 %98, i32 0
  %100 = shufflevector <16 x i16> %99, <16 x i16> undef, <16 x i32> zeroinitializer
  %101 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %20, <16 x i16> %95) #9
  %102 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %20, <16 x i16> %100) #9
  %103 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 50), align 8
  %104 = trunc i32 %103 to i16
  %105 = shl i16 %104, 3
  %106 = insertelement <16 x i16> undef, i16 %105, i32 0
  %107 = shufflevector <16 x i16> %106, <16 x i16> undef, <16 x i32> zeroinitializer
  %108 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 14), align 8
  %109 = trunc i32 %108 to i16
  %110 = shl i16 %109, 3
  %111 = insertelement <16 x i16> undef, i16 %110, i32 0
  %112 = shufflevector <16 x i16> %111, <16 x i16> undef, <16 x i32> zeroinitializer
  %113 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %23, <16 x i16> %107) #9
  %114 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %23, <16 x i16> %112) #9
  %115 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 58), align 8
  %116 = trunc i32 %115 to i16
  %117 = shl i16 %116, 3
  %118 = insertelement <16 x i16> undef, i16 %117, i32 0
  %119 = shufflevector <16 x i16> %118, <16 x i16> undef, <16 x i32> zeroinitializer
  %120 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 6), align 8
  %121 = trunc i32 %120 to i16
  %122 = shl i16 %121, 3
  %123 = insertelement <16 x i16> undef, i16 %122, i32 0
  %124 = shufflevector <16 x i16> %123, <16 x i16> undef, <16 x i32> zeroinitializer
  %125 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %26, <16 x i16> %119) #9
  %126 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %26, <16 x i16> %124) #9
  %127 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %38, <16 x i16> %89) #9
  %128 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %38, <16 x i16> %89) #9
  %129 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %39, <16 x i16> %90) #9
  %130 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %39, <16 x i16> %90) #9
  %131 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %51, <16 x i16> %101) #9
  %132 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %51, <16 x i16> %101) #9
  %133 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %52, <16 x i16> %102) #9
  %134 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %52, <16 x i16> %102) #9
  %135 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %64, <16 x i16> %113) #9
  %136 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %64, <16 x i16> %113) #9
  %137 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %65, <16 x i16> %114) #9
  %138 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %65, <16 x i16> %114) #9
  %139 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %77, <16 x i16> %125) #9
  %140 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %77, <16 x i16> %125) #9
  %141 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %78, <16 x i16> %126) #9
  %142 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %78, <16 x i16> %126) #9
  %143 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 8), align 16
  %144 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 56), align 16
  %145 = and i32 %143, 65535
  %146 = and i32 %144, 65535
  %147 = shl nuw i32 %146, 16
  %148 = or i32 %147, %145
  %149 = insertelement <8 x i32> undef, i32 %148, i32 0
  %150 = shufflevector <8 x i32> %149, <8 x i32> undef, <8 x i32> zeroinitializer
  %151 = shl i32 %143, 16
  %152 = sub i32 0, %151
  %153 = or i32 %146, %152
  %154 = insertelement <8 x i32> undef, i32 %153, i32 0
  %155 = shufflevector <8 x i32> %154, <8 x i32> undef, <8 x i32> zeroinitializer
  %156 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 40), align 16
  %157 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 24), align 16
  %158 = and i32 %156, 65535
  %159 = and i32 %157, 65535
  %160 = shl nuw i32 %159, 16
  %161 = or i32 %160, %158
  %162 = insertelement <8 x i32> undef, i32 %161, i32 0
  %163 = shufflevector <8 x i32> %162, <8 x i32> undef, <8 x i32> zeroinitializer
  %164 = shl i32 %156, 16
  %165 = sub i32 0, %164
  %166 = or i32 %159, %165
  %167 = insertelement <8 x i32> undef, i32 %166, i32 0
  %168 = shufflevector <8 x i32> %167, <8 x i32> undef, <8 x i32> zeroinitializer
  %169 = sub i32 0, %144
  %170 = and i32 %169, 65535
  %171 = shl nuw i32 %145, 16
  %172 = or i32 %171, %170
  %173 = insertelement <8 x i32> undef, i32 %172, i32 0
  %174 = shufflevector <8 x i32> %173, <8 x i32> undef, <8 x i32> zeroinitializer
  %175 = sub i32 0, %157
  %176 = and i32 %175, 65535
  %177 = shl nuw i32 %158, 16
  %178 = or i32 %177, %176
  %179 = insertelement <8 x i32> undef, i32 %178, i32 0
  %180 = shufflevector <8 x i32> %179, <8 x i32> undef, <8 x i32> zeroinitializer
  %181 = sext i8 %2 to i32
  %182 = shufflevector <16 x i16> %128, <16 x i16> %130, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %183 = shufflevector <16 x i16> %128, <16 x i16> %130, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %184 = bitcast <8 x i32> %150 to <16 x i16>
  %185 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %182, <16 x i16> %184) #9
  %186 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %183, <16 x i16> %184) #9
  %187 = bitcast <8 x i32> %155 to <16 x i16>
  %188 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %182, <16 x i16> %187) #9
  %189 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %183, <16 x i16> %187) #9
  %190 = add <8 x i32> %185, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %191 = add <8 x i32> %186, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %192 = add <8 x i32> %188, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %193 = add <8 x i32> %189, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %194 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %190, i32 %181) #9
  %195 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %191, i32 %181) #9
  %196 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %192, i32 %181) #9
  %197 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %193, i32 %181) #9
  %198 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %194, <8 x i32> %195) #9
  %199 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %196, <8 x i32> %197) #9
  %200 = shufflevector <16 x i16> %132, <16 x i16> %134, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %201 = shufflevector <16 x i16> %132, <16 x i16> %134, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %202 = bitcast <8 x i32> %163 to <16 x i16>
  %203 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %200, <16 x i16> %202) #9
  %204 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %201, <16 x i16> %202) #9
  %205 = bitcast <8 x i32> %168 to <16 x i16>
  %206 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %200, <16 x i16> %205) #9
  %207 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %201, <16 x i16> %205) #9
  %208 = add <8 x i32> %203, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %209 = add <8 x i32> %204, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %210 = add <8 x i32> %206, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %211 = add <8 x i32> %207, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %212 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %208, i32 %181) #9
  %213 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %209, i32 %181) #9
  %214 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %210, i32 %181) #9
  %215 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %211, i32 %181) #9
  %216 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %212, <8 x i32> %213) #9
  %217 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %214, <8 x i32> %215) #9
  %218 = shufflevector <16 x i16> %136, <16 x i16> %138, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %219 = shufflevector <16 x i16> %136, <16 x i16> %138, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %220 = bitcast <8 x i32> %174 to <16 x i16>
  %221 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %218, <16 x i16> %220) #9
  %222 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %219, <16 x i16> %220) #9
  %223 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %218, <16 x i16> %184) #9
  %224 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %219, <16 x i16> %184) #9
  %225 = add <8 x i32> %221, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %226 = add <8 x i32> %222, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %227 = add <8 x i32> %223, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %228 = add <8 x i32> %224, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %229 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %225, i32 %181) #9
  %230 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %226, i32 %181) #9
  %231 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %227, i32 %181) #9
  %232 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %228, i32 %181) #9
  %233 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %229, <8 x i32> %230) #9
  %234 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %231, <8 x i32> %232) #9
  %235 = shufflevector <16 x i16> %140, <16 x i16> %142, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %236 = shufflevector <16 x i16> %140, <16 x i16> %142, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %237 = bitcast <8 x i32> %180 to <16 x i16>
  %238 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %235, <16 x i16> %237) #9
  %239 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %236, <16 x i16> %237) #9
  %240 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %235, <16 x i16> %202) #9
  %241 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %236, <16 x i16> %202) #9
  %242 = add <8 x i32> %238, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %243 = add <8 x i32> %239, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %244 = add <8 x i32> %240, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %245 = add <8 x i32> %241, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %246 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %242, i32 %181) #9
  %247 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %243, i32 %181) #9
  %248 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %244, i32 %181) #9
  %249 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %245, i32 %181) #9
  %250 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %246, <8 x i32> %247) #9
  %251 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %248, <8 x i32> %249) #9
  %252 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %127, <16 x i16> %135) #9
  %253 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %127, <16 x i16> %135) #9
  %254 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %129, <16 x i16> %137) #9
  %255 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %129, <16 x i16> %137) #9
  %256 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %131, <16 x i16> %139) #9
  %257 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %131, <16 x i16> %139) #9
  %258 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %133, <16 x i16> %141) #9
  %259 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %133, <16 x i16> %141) #9
  %260 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %198, <16 x i16> %233) #9
  %261 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %198, <16 x i16> %233) #9
  %262 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %199, <16 x i16> %234) #9
  %263 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %199, <16 x i16> %234) #9
  %264 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %216, <16 x i16> %250) #9
  %265 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %216, <16 x i16> %250) #9
  %266 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %217, <16 x i16> %251) #9
  %267 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %217, <16 x i16> %251) #9
  %268 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 16), align 16
  %269 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 48), align 16
  %270 = and i32 %268, 65535
  %271 = and i32 %269, 65535
  %272 = shl nuw i32 %271, 16
  %273 = or i32 %272, %270
  %274 = insertelement <8 x i32> undef, i32 %273, i32 0
  %275 = shufflevector <8 x i32> %274, <8 x i32> undef, <8 x i32> zeroinitializer
  %276 = shl i32 %268, 16
  %277 = sub i32 0, %276
  %278 = or i32 %271, %277
  %279 = insertelement <8 x i32> undef, i32 %278, i32 0
  %280 = shufflevector <8 x i32> %279, <8 x i32> undef, <8 x i32> zeroinitializer
  %281 = sub i32 0, %269
  %282 = and i32 %281, 65535
  %283 = shl nuw i32 %270, 16
  %284 = or i32 %283, %282
  %285 = insertelement <8 x i32> undef, i32 %284, i32 0
  %286 = shufflevector <8 x i32> %285, <8 x i32> undef, <8 x i32> zeroinitializer
  %287 = shufflevector <16 x i16> %253, <16 x i16> %255, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %288 = shufflevector <16 x i16> %253, <16 x i16> %255, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %289 = bitcast <8 x i32> %275 to <16 x i16>
  %290 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %287, <16 x i16> %289) #9
  %291 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %288, <16 x i16> %289) #9
  %292 = bitcast <8 x i32> %280 to <16 x i16>
  %293 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %287, <16 x i16> %292) #9
  %294 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %288, <16 x i16> %292) #9
  %295 = add <8 x i32> %290, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %296 = add <8 x i32> %291, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %297 = add <8 x i32> %293, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %298 = add <8 x i32> %294, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %299 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %295, i32 %181) #9
  %300 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %296, i32 %181) #9
  %301 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %297, i32 %181) #9
  %302 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %298, i32 %181) #9
  %303 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %299, <8 x i32> %300) #9
  %304 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %301, <8 x i32> %302) #9
  %305 = shufflevector <16 x i16> %257, <16 x i16> %259, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %306 = shufflevector <16 x i16> %257, <16 x i16> %259, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %307 = bitcast <8 x i32> %286 to <16 x i16>
  %308 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %305, <16 x i16> %307) #9
  %309 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %306, <16 x i16> %307) #9
  %310 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %305, <16 x i16> %289) #9
  %311 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %306, <16 x i16> %289) #9
  %312 = add <8 x i32> %308, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %313 = add <8 x i32> %309, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %314 = add <8 x i32> %310, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %315 = add <8 x i32> %311, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %316 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %312, i32 %181) #9
  %317 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %313, i32 %181) #9
  %318 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %314, i32 %181) #9
  %319 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %315, i32 %181) #9
  %320 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %316, <8 x i32> %317) #9
  %321 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %318, <8 x i32> %319) #9
  %322 = shufflevector <16 x i16> %261, <16 x i16> %263, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %323 = shufflevector <16 x i16> %261, <16 x i16> %263, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %324 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %322, <16 x i16> %289) #9
  %325 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %323, <16 x i16> %289) #9
  %326 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %322, <16 x i16> %292) #9
  %327 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %323, <16 x i16> %292) #9
  %328 = add <8 x i32> %324, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %329 = add <8 x i32> %325, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %330 = add <8 x i32> %326, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %331 = add <8 x i32> %327, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %332 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %328, i32 %181) #9
  %333 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %329, i32 %181) #9
  %334 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %330, i32 %181) #9
  %335 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %331, i32 %181) #9
  %336 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %332, <8 x i32> %333) #9
  %337 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %334, <8 x i32> %335) #9
  %338 = shufflevector <16 x i16> %265, <16 x i16> %267, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %339 = shufflevector <16 x i16> %265, <16 x i16> %267, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %340 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %338, <16 x i16> %307) #9
  %341 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %339, <16 x i16> %307) #9
  %342 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %338, <16 x i16> %289) #9
  %343 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %339, <16 x i16> %289) #9
  %344 = add <8 x i32> %340, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %345 = add <8 x i32> %341, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %346 = add <8 x i32> %342, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %347 = add <8 x i32> %343, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %348 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %344, i32 %181) #9
  %349 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %345, i32 %181) #9
  %350 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %346, i32 %181) #9
  %351 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %347, i32 %181) #9
  %352 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %348, <8 x i32> %349) #9
  %353 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %350, <8 x i32> %351) #9
  %354 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %252, <16 x i16> %256) #9
  %355 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %252, <16 x i16> %256) #9
  %356 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %254, <16 x i16> %258) #9
  %357 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %254, <16 x i16> %258) #9
  %358 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %303, <16 x i16> %320) #9
  %359 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %303, <16 x i16> %320) #9
  %360 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %304, <16 x i16> %321) #9
  %361 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %304, <16 x i16> %321) #9
  %362 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %260, <16 x i16> %264) #9
  %363 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %260, <16 x i16> %264) #9
  %364 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %262, <16 x i16> %266) #9
  %365 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %262, <16 x i16> %266) #9
  %366 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %336, <16 x i16> %352) #9
  %367 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %336, <16 x i16> %352) #9
  %368 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %337, <16 x i16> %353) #9
  %369 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %337, <16 x i16> %353) #9
  %370 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %371 = and i32 %370, 65535
  %372 = shl nuw i32 %371, 16
  %373 = or i32 %372, %371
  %374 = insertelement <8 x i32> undef, i32 %373, i32 0
  %375 = shufflevector <8 x i32> %374, <8 x i32> undef, <8 x i32> zeroinitializer
  %376 = shl i32 %370, 16
  %377 = sub i32 0, %376
  %378 = or i32 %371, %377
  %379 = insertelement <8 x i32> undef, i32 %378, i32 0
  %380 = shufflevector <8 x i32> %379, <8 x i32> undef, <8 x i32> zeroinitializer
  %381 = shufflevector <16 x i16> %355, <16 x i16> %357, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %382 = shufflevector <16 x i16> %355, <16 x i16> %357, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %383 = bitcast <8 x i32> %375 to <16 x i16>
  %384 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %381, <16 x i16> %383) #9
  %385 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %382, <16 x i16> %383) #9
  %386 = bitcast <8 x i32> %380 to <16 x i16>
  %387 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %381, <16 x i16> %386) #9
  %388 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %382, <16 x i16> %386) #9
  %389 = add <8 x i32> %384, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %390 = add <8 x i32> %385, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %391 = add <8 x i32> %387, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %392 = add <8 x i32> %388, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %393 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %389, i32 %181) #9
  %394 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %390, i32 %181) #9
  %395 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %391, i32 %181) #9
  %396 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %392, i32 %181) #9
  %397 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %393, <8 x i32> %394) #9
  %398 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %395, <8 x i32> %396) #9
  %399 = shufflevector <16 x i16> %359, <16 x i16> %361, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %400 = shufflevector <16 x i16> %359, <16 x i16> %361, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %401 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %399, <16 x i16> %383) #9
  %402 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %400, <16 x i16> %383) #9
  %403 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %399, <16 x i16> %386) #9
  %404 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %400, <16 x i16> %386) #9
  %405 = add <8 x i32> %401, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %406 = add <8 x i32> %402, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %407 = add <8 x i32> %403, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %408 = add <8 x i32> %404, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %409 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %405, i32 %181) #9
  %410 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %406, i32 %181) #9
  %411 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %407, i32 %181) #9
  %412 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %408, i32 %181) #9
  %413 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %409, <8 x i32> %410) #9
  %414 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %411, <8 x i32> %412) #9
  %415 = shufflevector <16 x i16> %363, <16 x i16> %365, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %416 = shufflevector <16 x i16> %363, <16 x i16> %365, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %417 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %415, <16 x i16> %383) #9
  %418 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %416, <16 x i16> %383) #9
  %419 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %415, <16 x i16> %386) #9
  %420 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %416, <16 x i16> %386) #9
  %421 = add <8 x i32> %417, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %422 = add <8 x i32> %418, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %423 = add <8 x i32> %419, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %424 = add <8 x i32> %420, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %425 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %421, i32 %181) #9
  %426 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %422, i32 %181) #9
  %427 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %423, i32 %181) #9
  %428 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %424, i32 %181) #9
  %429 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %425, <8 x i32> %426) #9
  %430 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %427, <8 x i32> %428) #9
  %431 = shufflevector <16 x i16> %367, <16 x i16> %369, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %432 = shufflevector <16 x i16> %367, <16 x i16> %369, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %433 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %431, <16 x i16> %383) #9
  %434 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %432, <16 x i16> %383) #9
  %435 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %431, <16 x i16> %386) #9
  %436 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %432, <16 x i16> %386) #9
  %437 = add <8 x i32> %433, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %438 = add <8 x i32> %434, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %439 = add <8 x i32> %435, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %440 = add <8 x i32> %436, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %441 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %437, i32 %181) #9
  %442 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %438, i32 %181) #9
  %443 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %439, i32 %181) #9
  %444 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %440, i32 %181) #9
  %445 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %441, <8 x i32> %442) #9
  %446 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %443, <8 x i32> %444) #9
  %447 = bitcast <4 x i64>* %1 to <16 x i16>*
  store <16 x i16> %354, <16 x i16>* %447, align 32
  %448 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %362) #9
  %449 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %450 = bitcast <4 x i64>* %449 to <16 x i16>*
  store <16 x i16> %448, <16 x i16>* %450, align 32
  %451 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %452 = bitcast <4 x i64>* %451 to <16 x i16>*
  store <16 x i16> %366, <16 x i16>* %452, align 32
  %453 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %358) #9
  %454 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %455 = bitcast <4 x i64>* %454 to <16 x i16>*
  store <16 x i16> %453, <16 x i16>* %455, align 32
  %456 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %457 = bitcast <4 x i64>* %456 to <16 x i16>*
  store <16 x i16> %413, <16 x i16>* %457, align 32
  %458 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %445) #9
  %459 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %460 = bitcast <4 x i64>* %459 to <16 x i16>*
  store <16 x i16> %458, <16 x i16>* %460, align 32
  %461 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %462 = bitcast <4 x i64>* %461 to <16 x i16>*
  store <16 x i16> %429, <16 x i16>* %462, align 32
  %463 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %397) #9
  %464 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %465 = bitcast <4 x i64>* %464 to <16 x i16>*
  store <16 x i16> %463, <16 x i16>* %465, align 32
  %466 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %467 = bitcast <4 x i64>* %466 to <16 x i16>*
  store <16 x i16> %398, <16 x i16>* %467, align 32
  %468 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %430) #9
  %469 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %470 = bitcast <4 x i64>* %469 to <16 x i16>*
  store <16 x i16> %468, <16 x i16>* %470, align 32
  %471 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %472 = bitcast <4 x i64>* %471 to <16 x i16>*
  store <16 x i16> %446, <16 x i16>* %472, align 32
  %473 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %414) #9
  %474 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %475 = bitcast <4 x i64>* %474 to <16 x i16>*
  store <16 x i16> %473, <16 x i16>* %475, align 32
  %476 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %477 = bitcast <4 x i64>* %476 to <16 x i16>*
  store <16 x i16> %360, <16 x i16>* %477, align 32
  %478 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %368) #9
  %479 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %480 = bitcast <4 x i64>* %479 to <16 x i16>*
  store <16 x i16> %478, <16 x i16>* %480, align 32
  %481 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %482 = bitcast <4 x i64>* %481 to <16 x i16>*
  store <16 x i16> %364, <16 x i16>* %482, align 32
  %483 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %356) #9
  %484 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 15
  %485 = bitcast <4 x i64>* %484 to <16 x i16>*
  store <16 x i16> %483, <16 x i16>* %485, align 32
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @iadst16_avx2(<4 x i64>* nocapture readonly, <4 x i64>* nocapture, i8 signext) #0 {
  %4 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 2), align 8
  %5 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 62), align 8
  %6 = and i32 %4, 65535
  %7 = and i32 %5, 65535
  %8 = shl nuw i32 %7, 16
  %9 = or i32 %8, %6
  %10 = insertelement <8 x i32> undef, i32 %9, i32 0
  %11 = shufflevector <8 x i32> %10, <8 x i32> undef, <8 x i32> zeroinitializer
  %12 = shl i32 %4, 16
  %13 = sub i32 0, %12
  %14 = or i32 %7, %13
  %15 = insertelement <8 x i32> undef, i32 %14, i32 0
  %16 = shufflevector <8 x i32> %15, <8 x i32> undef, <8 x i32> zeroinitializer
  %17 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 10), align 8
  %18 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 54), align 8
  %19 = and i32 %17, 65535
  %20 = and i32 %18, 65535
  %21 = shl nuw i32 %20, 16
  %22 = or i32 %21, %19
  %23 = insertelement <8 x i32> undef, i32 %22, i32 0
  %24 = shufflevector <8 x i32> %23, <8 x i32> undef, <8 x i32> zeroinitializer
  %25 = shl i32 %17, 16
  %26 = sub i32 0, %25
  %27 = or i32 %20, %26
  %28 = insertelement <8 x i32> undef, i32 %27, i32 0
  %29 = shufflevector <8 x i32> %28, <8 x i32> undef, <8 x i32> zeroinitializer
  %30 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 18), align 8
  %31 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 46), align 8
  %32 = and i32 %30, 65535
  %33 = and i32 %31, 65535
  %34 = shl nuw i32 %33, 16
  %35 = or i32 %34, %32
  %36 = insertelement <8 x i32> undef, i32 %35, i32 0
  %37 = shufflevector <8 x i32> %36, <8 x i32> undef, <8 x i32> zeroinitializer
  %38 = shl i32 %30, 16
  %39 = sub i32 0, %38
  %40 = or i32 %33, %39
  %41 = insertelement <8 x i32> undef, i32 %40, i32 0
  %42 = shufflevector <8 x i32> %41, <8 x i32> undef, <8 x i32> zeroinitializer
  %43 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 26), align 8
  %44 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 38), align 8
  %45 = and i32 %43, 65535
  %46 = and i32 %44, 65535
  %47 = shl nuw i32 %46, 16
  %48 = or i32 %47, %45
  %49 = insertelement <8 x i32> undef, i32 %48, i32 0
  %50 = shufflevector <8 x i32> %49, <8 x i32> undef, <8 x i32> zeroinitializer
  %51 = shl i32 %43, 16
  %52 = sub i32 0, %51
  %53 = or i32 %46, %52
  %54 = insertelement <8 x i32> undef, i32 %53, i32 0
  %55 = shufflevector <8 x i32> %54, <8 x i32> undef, <8 x i32> zeroinitializer
  %56 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 34), align 8
  %57 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 30), align 8
  %58 = and i32 %56, 65535
  %59 = and i32 %57, 65535
  %60 = shl nuw i32 %59, 16
  %61 = or i32 %60, %58
  %62 = insertelement <8 x i32> undef, i32 %61, i32 0
  %63 = shufflevector <8 x i32> %62, <8 x i32> undef, <8 x i32> zeroinitializer
  %64 = shl i32 %56, 16
  %65 = sub i32 0, %64
  %66 = or i32 %59, %65
  %67 = insertelement <8 x i32> undef, i32 %66, i32 0
  %68 = shufflevector <8 x i32> %67, <8 x i32> undef, <8 x i32> zeroinitializer
  %69 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 42), align 8
  %70 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 22), align 8
  %71 = and i32 %69, 65535
  %72 = and i32 %70, 65535
  %73 = shl nuw i32 %72, 16
  %74 = or i32 %73, %71
  %75 = insertelement <8 x i32> undef, i32 %74, i32 0
  %76 = shufflevector <8 x i32> %75, <8 x i32> undef, <8 x i32> zeroinitializer
  %77 = shl i32 %69, 16
  %78 = sub i32 0, %77
  %79 = or i32 %72, %78
  %80 = insertelement <8 x i32> undef, i32 %79, i32 0
  %81 = shufflevector <8 x i32> %80, <8 x i32> undef, <8 x i32> zeroinitializer
  %82 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 50), align 8
  %83 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 14), align 8
  %84 = and i32 %82, 65535
  %85 = and i32 %83, 65535
  %86 = shl nuw i32 %85, 16
  %87 = or i32 %86, %84
  %88 = insertelement <8 x i32> undef, i32 %87, i32 0
  %89 = shufflevector <8 x i32> %88, <8 x i32> undef, <8 x i32> zeroinitializer
  %90 = shl i32 %82, 16
  %91 = sub i32 0, %90
  %92 = or i32 %85, %91
  %93 = insertelement <8 x i32> undef, i32 %92, i32 0
  %94 = shufflevector <8 x i32> %93, <8 x i32> undef, <8 x i32> zeroinitializer
  %95 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 58), align 8
  %96 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 6), align 8
  %97 = and i32 %95, 65535
  %98 = and i32 %96, 65535
  %99 = shl nuw i32 %98, 16
  %100 = or i32 %99, %97
  %101 = insertelement <8 x i32> undef, i32 %100, i32 0
  %102 = shufflevector <8 x i32> %101, <8 x i32> undef, <8 x i32> zeroinitializer
  %103 = shl i32 %95, 16
  %104 = sub i32 0, %103
  %105 = or i32 %98, %104
  %106 = insertelement <8 x i32> undef, i32 %105, i32 0
  %107 = shufflevector <8 x i32> %106, <8 x i32> undef, <8 x i32> zeroinitializer
  %108 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 15
  %109 = bitcast <4 x i64>* %108 to <16 x i16>*
  %110 = load <16 x i16>, <16 x i16>* %109, align 32
  %111 = bitcast <4 x i64>* %0 to <16 x i16>*
  %112 = load <16 x i16>, <16 x i16>* %111, align 32
  %113 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 13
  %114 = bitcast <4 x i64>* %113 to <16 x i16>*
  %115 = load <16 x i16>, <16 x i16>* %114, align 32
  %116 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 2
  %117 = bitcast <4 x i64>* %116 to <16 x i16>*
  %118 = load <16 x i16>, <16 x i16>* %117, align 32
  %119 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 11
  %120 = bitcast <4 x i64>* %119 to <16 x i16>*
  %121 = load <16 x i16>, <16 x i16>* %120, align 32
  %122 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 4
  %123 = bitcast <4 x i64>* %122 to <16 x i16>*
  %124 = load <16 x i16>, <16 x i16>* %123, align 32
  %125 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 9
  %126 = bitcast <4 x i64>* %125 to <16 x i16>*
  %127 = load <16 x i16>, <16 x i16>* %126, align 32
  %128 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 6
  %129 = bitcast <4 x i64>* %128 to <16 x i16>*
  %130 = load <16 x i16>, <16 x i16>* %129, align 32
  %131 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 7
  %132 = bitcast <4 x i64>* %131 to <16 x i16>*
  %133 = load <16 x i16>, <16 x i16>* %132, align 32
  %134 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 8
  %135 = bitcast <4 x i64>* %134 to <16 x i16>*
  %136 = load <16 x i16>, <16 x i16>* %135, align 32
  %137 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 5
  %138 = bitcast <4 x i64>* %137 to <16 x i16>*
  %139 = load <16 x i16>, <16 x i16>* %138, align 32
  %140 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 10
  %141 = bitcast <4 x i64>* %140 to <16 x i16>*
  %142 = load <16 x i16>, <16 x i16>* %141, align 32
  %143 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 3
  %144 = bitcast <4 x i64>* %143 to <16 x i16>*
  %145 = load <16 x i16>, <16 x i16>* %144, align 32
  %146 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 12
  %147 = bitcast <4 x i64>* %146 to <16 x i16>*
  %148 = load <16 x i16>, <16 x i16>* %147, align 32
  %149 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 1
  %150 = bitcast <4 x i64>* %149 to <16 x i16>*
  %151 = load <16 x i16>, <16 x i16>* %150, align 32
  %152 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 14
  %153 = bitcast <4 x i64>* %152 to <16 x i16>*
  %154 = load <16 x i16>, <16 x i16>* %153, align 32
  %155 = sext i8 %2 to i32
  %156 = shufflevector <16 x i16> %110, <16 x i16> %112, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %157 = shufflevector <16 x i16> %110, <16 x i16> %112, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %158 = bitcast <8 x i32> %11 to <16 x i16>
  %159 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %156, <16 x i16> %158) #9
  %160 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %157, <16 x i16> %158) #9
  %161 = bitcast <8 x i32> %16 to <16 x i16>
  %162 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %156, <16 x i16> %161) #9
  %163 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %157, <16 x i16> %161) #9
  %164 = add <8 x i32> %159, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %165 = add <8 x i32> %160, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %166 = add <8 x i32> %162, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %167 = add <8 x i32> %163, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %168 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %164, i32 %155) #9
  %169 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %165, i32 %155) #9
  %170 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %166, i32 %155) #9
  %171 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %167, i32 %155) #9
  %172 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %168, <8 x i32> %169) #9
  %173 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %170, <8 x i32> %171) #9
  %174 = shufflevector <16 x i16> %115, <16 x i16> %118, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %175 = shufflevector <16 x i16> %115, <16 x i16> %118, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %176 = bitcast <8 x i32> %24 to <16 x i16>
  %177 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %174, <16 x i16> %176) #9
  %178 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %175, <16 x i16> %176) #9
  %179 = bitcast <8 x i32> %29 to <16 x i16>
  %180 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %174, <16 x i16> %179) #9
  %181 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %175, <16 x i16> %179) #9
  %182 = add <8 x i32> %177, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %183 = add <8 x i32> %178, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %184 = add <8 x i32> %180, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %185 = add <8 x i32> %181, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %186 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %182, i32 %155) #9
  %187 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %183, i32 %155) #9
  %188 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %184, i32 %155) #9
  %189 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %185, i32 %155) #9
  %190 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %186, <8 x i32> %187) #9
  %191 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %188, <8 x i32> %189) #9
  %192 = shufflevector <16 x i16> %121, <16 x i16> %124, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %193 = shufflevector <16 x i16> %121, <16 x i16> %124, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %194 = bitcast <8 x i32> %37 to <16 x i16>
  %195 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %192, <16 x i16> %194) #9
  %196 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %193, <16 x i16> %194) #9
  %197 = bitcast <8 x i32> %42 to <16 x i16>
  %198 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %192, <16 x i16> %197) #9
  %199 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %193, <16 x i16> %197) #9
  %200 = add <8 x i32> %195, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %201 = add <8 x i32> %196, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %202 = add <8 x i32> %198, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %203 = add <8 x i32> %199, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %204 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %200, i32 %155) #9
  %205 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %201, i32 %155) #9
  %206 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %202, i32 %155) #9
  %207 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %203, i32 %155) #9
  %208 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %204, <8 x i32> %205) #9
  %209 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %206, <8 x i32> %207) #9
  %210 = shufflevector <16 x i16> %127, <16 x i16> %130, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %211 = shufflevector <16 x i16> %127, <16 x i16> %130, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %212 = bitcast <8 x i32> %50 to <16 x i16>
  %213 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %210, <16 x i16> %212) #9
  %214 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %211, <16 x i16> %212) #9
  %215 = bitcast <8 x i32> %55 to <16 x i16>
  %216 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %210, <16 x i16> %215) #9
  %217 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %211, <16 x i16> %215) #9
  %218 = add <8 x i32> %213, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %219 = add <8 x i32> %214, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %220 = add <8 x i32> %216, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %221 = add <8 x i32> %217, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %222 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %218, i32 %155) #9
  %223 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %219, i32 %155) #9
  %224 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %220, i32 %155) #9
  %225 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %221, i32 %155) #9
  %226 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %222, <8 x i32> %223) #9
  %227 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %224, <8 x i32> %225) #9
  %228 = shufflevector <16 x i16> %133, <16 x i16> %136, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %229 = shufflevector <16 x i16> %133, <16 x i16> %136, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %230 = bitcast <8 x i32> %63 to <16 x i16>
  %231 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %228, <16 x i16> %230) #9
  %232 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %229, <16 x i16> %230) #9
  %233 = bitcast <8 x i32> %68 to <16 x i16>
  %234 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %228, <16 x i16> %233) #9
  %235 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %229, <16 x i16> %233) #9
  %236 = add <8 x i32> %231, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %237 = add <8 x i32> %232, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %238 = add <8 x i32> %234, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %239 = add <8 x i32> %235, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %240 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %236, i32 %155) #9
  %241 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %237, i32 %155) #9
  %242 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %238, i32 %155) #9
  %243 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %239, i32 %155) #9
  %244 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %240, <8 x i32> %241) #9
  %245 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %242, <8 x i32> %243) #9
  %246 = shufflevector <16 x i16> %139, <16 x i16> %142, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %247 = shufflevector <16 x i16> %139, <16 x i16> %142, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %248 = bitcast <8 x i32> %76 to <16 x i16>
  %249 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %246, <16 x i16> %248) #9
  %250 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %247, <16 x i16> %248) #9
  %251 = bitcast <8 x i32> %81 to <16 x i16>
  %252 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %246, <16 x i16> %251) #9
  %253 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %247, <16 x i16> %251) #9
  %254 = add <8 x i32> %249, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %255 = add <8 x i32> %250, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %256 = add <8 x i32> %252, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %257 = add <8 x i32> %253, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %258 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %254, i32 %155) #9
  %259 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %255, i32 %155) #9
  %260 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %256, i32 %155) #9
  %261 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %257, i32 %155) #9
  %262 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %258, <8 x i32> %259) #9
  %263 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %260, <8 x i32> %261) #9
  %264 = shufflevector <16 x i16> %145, <16 x i16> %148, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %265 = shufflevector <16 x i16> %145, <16 x i16> %148, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %266 = bitcast <8 x i32> %89 to <16 x i16>
  %267 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %264, <16 x i16> %266) #9
  %268 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %265, <16 x i16> %266) #9
  %269 = bitcast <8 x i32> %94 to <16 x i16>
  %270 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %264, <16 x i16> %269) #9
  %271 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %265, <16 x i16> %269) #9
  %272 = add <8 x i32> %267, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %273 = add <8 x i32> %268, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %274 = add <8 x i32> %270, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %275 = add <8 x i32> %271, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %276 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %272, i32 %155) #9
  %277 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %273, i32 %155) #9
  %278 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %274, i32 %155) #9
  %279 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %275, i32 %155) #9
  %280 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %276, <8 x i32> %277) #9
  %281 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %278, <8 x i32> %279) #9
  %282 = shufflevector <16 x i16> %151, <16 x i16> %154, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %283 = shufflevector <16 x i16> %151, <16 x i16> %154, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %284 = bitcast <8 x i32> %102 to <16 x i16>
  %285 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %282, <16 x i16> %284) #9
  %286 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %283, <16 x i16> %284) #9
  %287 = bitcast <8 x i32> %107 to <16 x i16>
  %288 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %282, <16 x i16> %287) #9
  %289 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %283, <16 x i16> %287) #9
  %290 = add <8 x i32> %285, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %291 = add <8 x i32> %286, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %292 = add <8 x i32> %288, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %293 = add <8 x i32> %289, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %294 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %290, i32 %155) #9
  %295 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %291, i32 %155) #9
  %296 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %292, i32 %155) #9
  %297 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %293, i32 %155) #9
  %298 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %294, <8 x i32> %295) #9
  %299 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %296, <8 x i32> %297) #9
  %300 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %172, <16 x i16> %244) #9
  %301 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %172, <16 x i16> %244) #9
  %302 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %173, <16 x i16> %245) #9
  %303 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %173, <16 x i16> %245) #9
  %304 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %190, <16 x i16> %262) #9
  %305 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %190, <16 x i16> %262) #9
  %306 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %191, <16 x i16> %263) #9
  %307 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %191, <16 x i16> %263) #9
  %308 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %208, <16 x i16> %280) #9
  %309 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %208, <16 x i16> %280) #9
  %310 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %209, <16 x i16> %281) #9
  %311 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %209, <16 x i16> %281) #9
  %312 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %226, <16 x i16> %298) #9
  %313 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %226, <16 x i16> %298) #9
  %314 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %227, <16 x i16> %299) #9
  %315 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %227, <16 x i16> %299) #9
  %316 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 8), align 16
  %317 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 56), align 16
  %318 = and i32 %316, 65535
  %319 = and i32 %317, 65535
  %320 = shl nuw i32 %319, 16
  %321 = or i32 %320, %318
  %322 = insertelement <8 x i32> undef, i32 %321, i32 0
  %323 = shufflevector <8 x i32> %322, <8 x i32> undef, <8 x i32> zeroinitializer
  %324 = shl i32 %316, 16
  %325 = sub i32 0, %324
  %326 = or i32 %319, %325
  %327 = insertelement <8 x i32> undef, i32 %326, i32 0
  %328 = shufflevector <8 x i32> %327, <8 x i32> undef, <8 x i32> zeroinitializer
  %329 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 40), align 16
  %330 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 24), align 16
  %331 = and i32 %329, 65535
  %332 = and i32 %330, 65535
  %333 = shl nuw i32 %332, 16
  %334 = or i32 %333, %331
  %335 = insertelement <8 x i32> undef, i32 %334, i32 0
  %336 = shufflevector <8 x i32> %335, <8 x i32> undef, <8 x i32> zeroinitializer
  %337 = shl i32 %329, 16
  %338 = sub i32 0, %337
  %339 = or i32 %332, %338
  %340 = insertelement <8 x i32> undef, i32 %339, i32 0
  %341 = shufflevector <8 x i32> %340, <8 x i32> undef, <8 x i32> zeroinitializer
  %342 = sub i32 0, %317
  %343 = and i32 %342, 65535
  %344 = shl nuw i32 %318, 16
  %345 = or i32 %344, %343
  %346 = insertelement <8 x i32> undef, i32 %345, i32 0
  %347 = shufflevector <8 x i32> %346, <8 x i32> undef, <8 x i32> zeroinitializer
  %348 = sub i32 0, %330
  %349 = and i32 %348, 65535
  %350 = shl nuw i32 %331, 16
  %351 = or i32 %350, %349
  %352 = insertelement <8 x i32> undef, i32 %351, i32 0
  %353 = shufflevector <8 x i32> %352, <8 x i32> undef, <8 x i32> zeroinitializer
  %354 = shufflevector <16 x i16> %301, <16 x i16> %303, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %355 = shufflevector <16 x i16> %301, <16 x i16> %303, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %356 = bitcast <8 x i32> %323 to <16 x i16>
  %357 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %354, <16 x i16> %356) #9
  %358 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %355, <16 x i16> %356) #9
  %359 = bitcast <8 x i32> %328 to <16 x i16>
  %360 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %354, <16 x i16> %359) #9
  %361 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %355, <16 x i16> %359) #9
  %362 = add <8 x i32> %357, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %363 = add <8 x i32> %358, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %364 = add <8 x i32> %360, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %365 = add <8 x i32> %361, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %366 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %362, i32 %155) #9
  %367 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %363, i32 %155) #9
  %368 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %364, i32 %155) #9
  %369 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %365, i32 %155) #9
  %370 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %366, <8 x i32> %367) #9
  %371 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %368, <8 x i32> %369) #9
  %372 = shufflevector <16 x i16> %305, <16 x i16> %307, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %373 = shufflevector <16 x i16> %305, <16 x i16> %307, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %374 = bitcast <8 x i32> %336 to <16 x i16>
  %375 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %372, <16 x i16> %374) #9
  %376 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %373, <16 x i16> %374) #9
  %377 = bitcast <8 x i32> %341 to <16 x i16>
  %378 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %372, <16 x i16> %377) #9
  %379 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %373, <16 x i16> %377) #9
  %380 = add <8 x i32> %375, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %381 = add <8 x i32> %376, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %382 = add <8 x i32> %378, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %383 = add <8 x i32> %379, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %384 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %380, i32 %155) #9
  %385 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %381, i32 %155) #9
  %386 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %382, i32 %155) #9
  %387 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %383, i32 %155) #9
  %388 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %384, <8 x i32> %385) #9
  %389 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %386, <8 x i32> %387) #9
  %390 = shufflevector <16 x i16> %309, <16 x i16> %311, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %391 = shufflevector <16 x i16> %309, <16 x i16> %311, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %392 = bitcast <8 x i32> %347 to <16 x i16>
  %393 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %390, <16 x i16> %392) #9
  %394 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %391, <16 x i16> %392) #9
  %395 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %390, <16 x i16> %356) #9
  %396 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %391, <16 x i16> %356) #9
  %397 = add <8 x i32> %393, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %398 = add <8 x i32> %394, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %399 = add <8 x i32> %395, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %400 = add <8 x i32> %396, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %401 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %397, i32 %155) #9
  %402 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %398, i32 %155) #9
  %403 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %399, i32 %155) #9
  %404 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %400, i32 %155) #9
  %405 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %401, <8 x i32> %402) #9
  %406 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %403, <8 x i32> %404) #9
  %407 = shufflevector <16 x i16> %313, <16 x i16> %315, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %408 = shufflevector <16 x i16> %313, <16 x i16> %315, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %409 = bitcast <8 x i32> %353 to <16 x i16>
  %410 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %407, <16 x i16> %409) #9
  %411 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %408, <16 x i16> %409) #9
  %412 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %407, <16 x i16> %374) #9
  %413 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %408, <16 x i16> %374) #9
  %414 = add <8 x i32> %410, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %415 = add <8 x i32> %411, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %416 = add <8 x i32> %412, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %417 = add <8 x i32> %413, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %418 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %414, i32 %155) #9
  %419 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %415, i32 %155) #9
  %420 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %416, i32 %155) #9
  %421 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %417, i32 %155) #9
  %422 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %418, <8 x i32> %419) #9
  %423 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %420, <8 x i32> %421) #9
  %424 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %300, <16 x i16> %308) #9
  %425 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %300, <16 x i16> %308) #9
  %426 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %302, <16 x i16> %310) #9
  %427 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %302, <16 x i16> %310) #9
  %428 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %304, <16 x i16> %312) #9
  %429 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %304, <16 x i16> %312) #9
  %430 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %306, <16 x i16> %314) #9
  %431 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %306, <16 x i16> %314) #9
  %432 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %370, <16 x i16> %405) #9
  %433 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %370, <16 x i16> %405) #9
  %434 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %371, <16 x i16> %406) #9
  %435 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %371, <16 x i16> %406) #9
  %436 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %388, <16 x i16> %422) #9
  %437 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %388, <16 x i16> %422) #9
  %438 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %389, <16 x i16> %423) #9
  %439 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %389, <16 x i16> %423) #9
  %440 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 16), align 16
  %441 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 48), align 16
  %442 = and i32 %440, 65535
  %443 = and i32 %441, 65535
  %444 = shl nuw i32 %443, 16
  %445 = or i32 %444, %442
  %446 = insertelement <8 x i32> undef, i32 %445, i32 0
  %447 = shufflevector <8 x i32> %446, <8 x i32> undef, <8 x i32> zeroinitializer
  %448 = shl i32 %440, 16
  %449 = sub i32 0, %448
  %450 = or i32 %443, %449
  %451 = insertelement <8 x i32> undef, i32 %450, i32 0
  %452 = shufflevector <8 x i32> %451, <8 x i32> undef, <8 x i32> zeroinitializer
  %453 = sub i32 0, %441
  %454 = and i32 %453, 65535
  %455 = shl nuw i32 %442, 16
  %456 = or i32 %455, %454
  %457 = insertelement <8 x i32> undef, i32 %456, i32 0
  %458 = shufflevector <8 x i32> %457, <8 x i32> undef, <8 x i32> zeroinitializer
  %459 = shufflevector <16 x i16> %425, <16 x i16> %427, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %460 = shufflevector <16 x i16> %425, <16 x i16> %427, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %461 = bitcast <8 x i32> %447 to <16 x i16>
  %462 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %459, <16 x i16> %461) #9
  %463 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %460, <16 x i16> %461) #9
  %464 = bitcast <8 x i32> %452 to <16 x i16>
  %465 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %459, <16 x i16> %464) #9
  %466 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %460, <16 x i16> %464) #9
  %467 = add <8 x i32> %462, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %468 = add <8 x i32> %463, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %469 = add <8 x i32> %465, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %470 = add <8 x i32> %466, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %471 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %467, i32 %155) #9
  %472 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %468, i32 %155) #9
  %473 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %469, i32 %155) #9
  %474 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %470, i32 %155) #9
  %475 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %471, <8 x i32> %472) #9
  %476 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %473, <8 x i32> %474) #9
  %477 = shufflevector <16 x i16> %429, <16 x i16> %431, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %478 = shufflevector <16 x i16> %429, <16 x i16> %431, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %479 = bitcast <8 x i32> %458 to <16 x i16>
  %480 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %477, <16 x i16> %479) #9
  %481 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %478, <16 x i16> %479) #9
  %482 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %477, <16 x i16> %461) #9
  %483 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %478, <16 x i16> %461) #9
  %484 = add <8 x i32> %480, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %485 = add <8 x i32> %481, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %486 = add <8 x i32> %482, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %487 = add <8 x i32> %483, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %488 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %484, i32 %155) #9
  %489 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %485, i32 %155) #9
  %490 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %486, i32 %155) #9
  %491 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %487, i32 %155) #9
  %492 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %488, <8 x i32> %489) #9
  %493 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %490, <8 x i32> %491) #9
  %494 = shufflevector <16 x i16> %433, <16 x i16> %435, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %495 = shufflevector <16 x i16> %433, <16 x i16> %435, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %496 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %494, <16 x i16> %461) #9
  %497 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %495, <16 x i16> %461) #9
  %498 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %494, <16 x i16> %464) #9
  %499 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %495, <16 x i16> %464) #9
  %500 = add <8 x i32> %496, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %501 = add <8 x i32> %497, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %502 = add <8 x i32> %498, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %503 = add <8 x i32> %499, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %504 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %500, i32 %155) #9
  %505 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %501, i32 %155) #9
  %506 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %502, i32 %155) #9
  %507 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %503, i32 %155) #9
  %508 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %504, <8 x i32> %505) #9
  %509 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %506, <8 x i32> %507) #9
  %510 = shufflevector <16 x i16> %437, <16 x i16> %439, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %511 = shufflevector <16 x i16> %437, <16 x i16> %439, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %512 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %510, <16 x i16> %479) #9
  %513 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %511, <16 x i16> %479) #9
  %514 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %510, <16 x i16> %461) #9
  %515 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %511, <16 x i16> %461) #9
  %516 = add <8 x i32> %512, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %517 = add <8 x i32> %513, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %518 = add <8 x i32> %514, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %519 = add <8 x i32> %515, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %520 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %516, i32 %155) #9
  %521 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %517, i32 %155) #9
  %522 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %518, i32 %155) #9
  %523 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %519, i32 %155) #9
  %524 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %520, <8 x i32> %521) #9
  %525 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %522, <8 x i32> %523) #9
  %526 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %424, <16 x i16> %428) #9
  %527 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %424, <16 x i16> %428) #9
  %528 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %426, <16 x i16> %430) #9
  %529 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %426, <16 x i16> %430) #9
  %530 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %475, <16 x i16> %492) #9
  %531 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %475, <16 x i16> %492) #9
  %532 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %476, <16 x i16> %493) #9
  %533 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %476, <16 x i16> %493) #9
  %534 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %432, <16 x i16> %436) #9
  %535 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %432, <16 x i16> %436) #9
  %536 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %434, <16 x i16> %438) #9
  %537 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %434, <16 x i16> %438) #9
  %538 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %508, <16 x i16> %524) #9
  %539 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %508, <16 x i16> %524) #9
  %540 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %509, <16 x i16> %525) #9
  %541 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %509, <16 x i16> %525) #9
  %542 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %543 = and i32 %542, 65535
  %544 = shl nuw i32 %543, 16
  %545 = or i32 %544, %543
  %546 = insertelement <8 x i32> undef, i32 %545, i32 0
  %547 = shufflevector <8 x i32> %546, <8 x i32> undef, <8 x i32> zeroinitializer
  %548 = shl i32 %542, 16
  %549 = sub i32 0, %548
  %550 = or i32 %543, %549
  %551 = insertelement <8 x i32> undef, i32 %550, i32 0
  %552 = shufflevector <8 x i32> %551, <8 x i32> undef, <8 x i32> zeroinitializer
  %553 = shufflevector <16 x i16> %527, <16 x i16> %529, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %554 = shufflevector <16 x i16> %527, <16 x i16> %529, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %555 = bitcast <8 x i32> %547 to <16 x i16>
  %556 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %553, <16 x i16> %555) #9
  %557 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %554, <16 x i16> %555) #9
  %558 = bitcast <8 x i32> %552 to <16 x i16>
  %559 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %553, <16 x i16> %558) #9
  %560 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %554, <16 x i16> %558) #9
  %561 = add <8 x i32> %556, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %562 = add <8 x i32> %557, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %563 = add <8 x i32> %559, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %564 = add <8 x i32> %560, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %565 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %561, i32 %155) #9
  %566 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %562, i32 %155) #9
  %567 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %563, i32 %155) #9
  %568 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %564, i32 %155) #9
  %569 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %565, <8 x i32> %566) #9
  %570 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %567, <8 x i32> %568) #9
  %571 = shufflevector <16 x i16> %531, <16 x i16> %533, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %572 = shufflevector <16 x i16> %531, <16 x i16> %533, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %573 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %571, <16 x i16> %555) #9
  %574 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %572, <16 x i16> %555) #9
  %575 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %571, <16 x i16> %558) #9
  %576 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %572, <16 x i16> %558) #9
  %577 = add <8 x i32> %573, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %578 = add <8 x i32> %574, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %579 = add <8 x i32> %575, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %580 = add <8 x i32> %576, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %581 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %577, i32 %155) #9
  %582 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %578, i32 %155) #9
  %583 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %579, i32 %155) #9
  %584 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %580, i32 %155) #9
  %585 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %581, <8 x i32> %582) #9
  %586 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %583, <8 x i32> %584) #9
  %587 = shufflevector <16 x i16> %535, <16 x i16> %537, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %588 = shufflevector <16 x i16> %535, <16 x i16> %537, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %589 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %587, <16 x i16> %555) #9
  %590 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %588, <16 x i16> %555) #9
  %591 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %587, <16 x i16> %558) #9
  %592 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %588, <16 x i16> %558) #9
  %593 = add <8 x i32> %589, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %594 = add <8 x i32> %590, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %595 = add <8 x i32> %591, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %596 = add <8 x i32> %592, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %597 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %593, i32 %155) #9
  %598 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %594, i32 %155) #9
  %599 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %595, i32 %155) #9
  %600 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %596, i32 %155) #9
  %601 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %597, <8 x i32> %598) #9
  %602 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %599, <8 x i32> %600) #9
  %603 = shufflevector <16 x i16> %539, <16 x i16> %541, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %604 = shufflevector <16 x i16> %539, <16 x i16> %541, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %605 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %603, <16 x i16> %555) #9
  %606 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %604, <16 x i16> %555) #9
  %607 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %603, <16 x i16> %558) #9
  %608 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %604, <16 x i16> %558) #9
  %609 = add <8 x i32> %605, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %610 = add <8 x i32> %606, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %611 = add <8 x i32> %607, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %612 = add <8 x i32> %608, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %613 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %609, i32 %155) #9
  %614 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %610, i32 %155) #9
  %615 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %611, i32 %155) #9
  %616 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %612, i32 %155) #9
  %617 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %613, <8 x i32> %614) #9
  %618 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %615, <8 x i32> %616) #9
  %619 = bitcast <4 x i64>* %1 to <16 x i16>*
  store <16 x i16> %526, <16 x i16>* %619, align 32
  %620 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %534) #9
  %621 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %622 = bitcast <4 x i64>* %621 to <16 x i16>*
  store <16 x i16> %620, <16 x i16>* %622, align 32
  %623 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %624 = bitcast <4 x i64>* %623 to <16 x i16>*
  store <16 x i16> %538, <16 x i16>* %624, align 32
  %625 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %530) #9
  %626 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %627 = bitcast <4 x i64>* %626 to <16 x i16>*
  store <16 x i16> %625, <16 x i16>* %627, align 32
  %628 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %629 = bitcast <4 x i64>* %628 to <16 x i16>*
  store <16 x i16> %585, <16 x i16>* %629, align 32
  %630 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %617) #9
  %631 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %632 = bitcast <4 x i64>* %631 to <16 x i16>*
  store <16 x i16> %630, <16 x i16>* %632, align 32
  %633 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %634 = bitcast <4 x i64>* %633 to <16 x i16>*
  store <16 x i16> %601, <16 x i16>* %634, align 32
  %635 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %569) #9
  %636 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %637 = bitcast <4 x i64>* %636 to <16 x i16>*
  store <16 x i16> %635, <16 x i16>* %637, align 32
  %638 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %639 = bitcast <4 x i64>* %638 to <16 x i16>*
  store <16 x i16> %570, <16 x i16>* %639, align 32
  %640 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %602) #9
  %641 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %642 = bitcast <4 x i64>* %641 to <16 x i16>*
  store <16 x i16> %640, <16 x i16>* %642, align 32
  %643 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %644 = bitcast <4 x i64>* %643 to <16 x i16>*
  store <16 x i16> %618, <16 x i16>* %644, align 32
  %645 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %586) #9
  %646 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %647 = bitcast <4 x i64>* %646 to <16 x i16>*
  store <16 x i16> %645, <16 x i16>* %647, align 32
  %648 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %649 = bitcast <4 x i64>* %648 to <16 x i16>*
  store <16 x i16> %532, <16 x i16>* %649, align 32
  %650 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %540) #9
  %651 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %652 = bitcast <4 x i64>* %651 to <16 x i16>*
  store <16 x i16> %650, <16 x i16>* %652, align 32
  %653 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %654 = bitcast <4 x i64>* %653 to <16 x i16>*
  store <16 x i16> %536, <16 x i16>* %654, align 32
  %655 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> zeroinitializer, <16 x i16> %528) #9
  %656 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 15
  %657 = bitcast <4 x i64>* %656 to <16 x i16>*
  store <16 x i16> %655, <16 x i16>* %657, align 32
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define internal void @idct32_low1_avx2(<4 x i64>* nocapture readonly, <4 x i64>*, i8 signext) #4 {
  %4 = bitcast <4 x i64>* %0 to <16 x i16>*
  %5 = load <16 x i16>, <16 x i16>* %4, align 32
  %6 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %7 = trunc i32 %6 to i16
  %8 = shl i16 %7, 3
  %9 = insertelement <16 x i16> undef, i16 %8, i32 0
  %10 = shufflevector <16 x i16> %9, <16 x i16> undef, <16 x i32> zeroinitializer
  %11 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %5, <16 x i16> %10) #9
  %12 = bitcast <4 x i64>* %1 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %12, align 32
  %13 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 31
  %14 = bitcast <4 x i64>* %13 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %14, align 32
  %15 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %16 = bitcast <4 x i64>* %15 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %16, align 32
  %17 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 30
  %18 = bitcast <4 x i64>* %17 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %18, align 32
  %19 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %20 = bitcast <4 x i64>* %19 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %20, align 32
  %21 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 29
  %22 = bitcast <4 x i64>* %21 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %22, align 32
  %23 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %24 = bitcast <4 x i64>* %23 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %24, align 32
  %25 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 28
  %26 = bitcast <4 x i64>* %25 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %26, align 32
  %27 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %28 = bitcast <4 x i64>* %27 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %28, align 32
  %29 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 27
  %30 = bitcast <4 x i64>* %29 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %30, align 32
  %31 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %32 = bitcast <4 x i64>* %31 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %32, align 32
  %33 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 26
  %34 = bitcast <4 x i64>* %33 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %34, align 32
  %35 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %36 = bitcast <4 x i64>* %35 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %36, align 32
  %37 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 25
  %38 = bitcast <4 x i64>* %37 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %38, align 32
  %39 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %40 = bitcast <4 x i64>* %39 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %40, align 32
  %41 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 24
  %42 = bitcast <4 x i64>* %41 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %42, align 32
  %43 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %44 = bitcast <4 x i64>* %43 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %44, align 32
  %45 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 23
  %46 = bitcast <4 x i64>* %45 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %46, align 32
  %47 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %48 = bitcast <4 x i64>* %47 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %48, align 32
  %49 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 22
  %50 = bitcast <4 x i64>* %49 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %50, align 32
  %51 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %52 = bitcast <4 x i64>* %51 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %52, align 32
  %53 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 21
  %54 = bitcast <4 x i64>* %53 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %54, align 32
  %55 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %56 = bitcast <4 x i64>* %55 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %56, align 32
  %57 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 20
  %58 = bitcast <4 x i64>* %57 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %58, align 32
  %59 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %60 = bitcast <4 x i64>* %59 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %60, align 32
  %61 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 19
  %62 = bitcast <4 x i64>* %61 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %62, align 32
  %63 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %64 = bitcast <4 x i64>* %63 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %64, align 32
  %65 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 18
  %66 = bitcast <4 x i64>* %65 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %66, align 32
  %67 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %68 = bitcast <4 x i64>* %67 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %68, align 32
  %69 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 17
  %70 = bitcast <4 x i64>* %69 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %70, align 32
  %71 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 15
  %72 = bitcast <4 x i64>* %71 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %72, align 32
  %73 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 16
  %74 = bitcast <4 x i64>* %73 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %74, align 32
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @idct32_low8_avx2(<4 x i64>* nocapture readonly, <4 x i64>*, i8 signext) #0 {
  %4 = bitcast <4 x i64>* %0 to <16 x i16>*
  %5 = load <16 x i16>, <16 x i16>* %4, align 32
  %6 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 4
  %7 = bitcast <4 x i64>* %6 to <16 x i16>*
  %8 = load <16 x i16>, <16 x i16>* %7, align 32
  %9 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 2
  %10 = bitcast <4 x i64>* %9 to <16 x i16>*
  %11 = load <16 x i16>, <16 x i16>* %10, align 32
  %12 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 6
  %13 = bitcast <4 x i64>* %12 to <16 x i16>*
  %14 = load <16 x i16>, <16 x i16>* %13, align 32
  %15 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 1
  %16 = bitcast <4 x i64>* %15 to <16 x i16>*
  %17 = load <16 x i16>, <16 x i16>* %16, align 32
  %18 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 5
  %19 = bitcast <4 x i64>* %18 to <16 x i16>*
  %20 = load <16 x i16>, <16 x i16>* %19, align 32
  %21 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 3
  %22 = bitcast <4 x i64>* %21 to <16 x i16>*
  %23 = load <16 x i16>, <16 x i16>* %22, align 32
  %24 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 7
  %25 = bitcast <4 x i64>* %24 to <16 x i16>*
  %26 = load <16 x i16>, <16 x i16>* %25, align 32
  %27 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 62), align 8
  %28 = trunc i32 %27 to i16
  %29 = shl i16 %28, 3
  %30 = insertelement <16 x i16> undef, i16 %29, i32 0
  %31 = shufflevector <16 x i16> %30, <16 x i16> undef, <16 x i32> zeroinitializer
  %32 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 2), align 8
  %33 = trunc i32 %32 to i16
  %34 = shl i16 %33, 3
  %35 = insertelement <16 x i16> undef, i16 %34, i32 0
  %36 = shufflevector <16 x i16> %35, <16 x i16> undef, <16 x i32> zeroinitializer
  %37 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %17, <16 x i16> %31) #9
  %38 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %17, <16 x i16> %36) #9
  %39 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 50), align 8
  %40 = trunc i32 %39 to i16
  %41 = shl i16 %40, 3
  %42 = sub i16 0, %41
  %43 = insertelement <16 x i16> undef, i16 %42, i32 0
  %44 = shufflevector <16 x i16> %43, <16 x i16> undef, <16 x i32> zeroinitializer
  %45 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 14), align 8
  %46 = trunc i32 %45 to i16
  %47 = shl i16 %46, 3
  %48 = insertelement <16 x i16> undef, i16 %47, i32 0
  %49 = shufflevector <16 x i16> %48, <16 x i16> undef, <16 x i32> zeroinitializer
  %50 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %26, <16 x i16> %44) #9
  %51 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %26, <16 x i16> %49) #9
  %52 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 54), align 8
  %53 = trunc i32 %52 to i16
  %54 = shl i16 %53, 3
  %55 = insertelement <16 x i16> undef, i16 %54, i32 0
  %56 = shufflevector <16 x i16> %55, <16 x i16> undef, <16 x i32> zeroinitializer
  %57 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 10), align 8
  %58 = trunc i32 %57 to i16
  %59 = shl i16 %58, 3
  %60 = insertelement <16 x i16> undef, i16 %59, i32 0
  %61 = shufflevector <16 x i16> %60, <16 x i16> undef, <16 x i32> zeroinitializer
  %62 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %20, <16 x i16> %56) #9
  %63 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %20, <16 x i16> %61) #9
  %64 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 58), align 8
  %65 = trunc i32 %64 to i16
  %66 = shl i16 %65, 3
  %67 = sub i16 0, %66
  %68 = insertelement <16 x i16> undef, i16 %67, i32 0
  %69 = shufflevector <16 x i16> %68, <16 x i16> undef, <16 x i32> zeroinitializer
  %70 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 6), align 8
  %71 = trunc i32 %70 to i16
  %72 = shl i16 %71, 3
  %73 = insertelement <16 x i16> undef, i16 %72, i32 0
  %74 = shufflevector <16 x i16> %73, <16 x i16> undef, <16 x i32> zeroinitializer
  %75 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %23, <16 x i16> %69) #9
  %76 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %23, <16 x i16> %74) #9
  %77 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 60), align 16
  %78 = trunc i32 %77 to i16
  %79 = shl i16 %78, 3
  %80 = insertelement <16 x i16> undef, i16 %79, i32 0
  %81 = shufflevector <16 x i16> %80, <16 x i16> undef, <16 x i32> zeroinitializer
  %82 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 4), align 16
  %83 = trunc i32 %82 to i16
  %84 = shl i16 %83, 3
  %85 = insertelement <16 x i16> undef, i16 %84, i32 0
  %86 = shufflevector <16 x i16> %85, <16 x i16> undef, <16 x i32> zeroinitializer
  %87 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %11, <16 x i16> %81) #9
  %88 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %11, <16 x i16> %86) #9
  %89 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 52), align 16
  %90 = trunc i32 %89 to i16
  %91 = shl i16 %90, 3
  %92 = sub i16 0, %91
  %93 = insertelement <16 x i16> undef, i16 %92, i32 0
  %94 = shufflevector <16 x i16> %93, <16 x i16> undef, <16 x i32> zeroinitializer
  %95 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 12), align 16
  %96 = trunc i32 %95 to i16
  %97 = shl i16 %96, 3
  %98 = insertelement <16 x i16> undef, i16 %97, i32 0
  %99 = shufflevector <16 x i16> %98, <16 x i16> undef, <16 x i32> zeroinitializer
  %100 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %14, <16 x i16> %94) #9
  %101 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %14, <16 x i16> %99) #9
  %102 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 56), align 16
  %103 = trunc i32 %102 to i16
  %104 = shl i16 %103, 3
  %105 = insertelement <16 x i16> undef, i16 %104, i32 0
  %106 = shufflevector <16 x i16> %105, <16 x i16> undef, <16 x i32> zeroinitializer
  %107 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 8), align 16
  %108 = trunc i32 %107 to i16
  %109 = shl i16 %108, 3
  %110 = insertelement <16 x i16> undef, i16 %109, i32 0
  %111 = shufflevector <16 x i16> %110, <16 x i16> undef, <16 x i32> zeroinitializer
  %112 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %8, <16 x i16> %106) #9
  %113 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %8, <16 x i16> %111) #9
  %114 = sub i32 0, %107
  %115 = and i32 %114, 65535
  %116 = and i32 %102, 65535
  %117 = shl nuw i32 %116, 16
  %118 = or i32 %117, %115
  %119 = insertelement <8 x i32> undef, i32 %118, i32 0
  %120 = shufflevector <8 x i32> %119, <8 x i32> undef, <8 x i32> zeroinitializer
  %121 = shl i32 %107, 16
  %122 = or i32 %121, %116
  %123 = insertelement <8 x i32> undef, i32 %122, i32 0
  %124 = shufflevector <8 x i32> %123, <8 x i32> undef, <8 x i32> zeroinitializer
  %125 = sub i32 0, %102
  %126 = and i32 %125, 65535
  %127 = shl nuw i32 %115, 16
  %128 = or i32 %127, %126
  %129 = insertelement <8 x i32> undef, i32 %128, i32 0
  %130 = shufflevector <8 x i32> %129, <8 x i32> undef, <8 x i32> zeroinitializer
  %131 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 40), align 16
  %132 = sub i32 0, %131
  %133 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 24), align 16
  %134 = and i32 %132, 65535
  %135 = and i32 %133, 65535
  %136 = shl nuw i32 %135, 16
  %137 = or i32 %136, %134
  %138 = insertelement <8 x i32> undef, i32 %137, i32 0
  %139 = shufflevector <8 x i32> %138, <8 x i32> undef, <8 x i32> zeroinitializer
  %140 = shl i32 %131, 16
  %141 = or i32 %135, %140
  %142 = insertelement <8 x i32> undef, i32 %141, i32 0
  %143 = shufflevector <8 x i32> %142, <8 x i32> undef, <8 x i32> zeroinitializer
  %144 = sub i32 0, %133
  %145 = and i32 %144, 65535
  %146 = shl nuw i32 %134, 16
  %147 = or i32 %146, %145
  %148 = insertelement <8 x i32> undef, i32 %147, i32 0
  %149 = shufflevector <8 x i32> %148, <8 x i32> undef, <8 x i32> zeroinitializer
  %150 = sext i8 %2 to i32
  %151 = shufflevector <16 x i16> %37, <16 x i16> %38, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %152 = shufflevector <16 x i16> %37, <16 x i16> %38, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %153 = bitcast <8 x i32> %120 to <16 x i16>
  %154 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %151, <16 x i16> %153) #9
  %155 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %152, <16 x i16> %153) #9
  %156 = bitcast <8 x i32> %124 to <16 x i16>
  %157 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %151, <16 x i16> %156) #9
  %158 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %152, <16 x i16> %156) #9
  %159 = add <8 x i32> %154, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %160 = add <8 x i32> %155, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %161 = add <8 x i32> %157, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %162 = add <8 x i32> %158, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %163 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %159, i32 %150) #9
  %164 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %160, i32 %150) #9
  %165 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %161, i32 %150) #9
  %166 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %162, i32 %150) #9
  %167 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %163, <8 x i32> %164) #9
  %168 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %165, <8 x i32> %166) #9
  %169 = shufflevector <16 x i16> %50, <16 x i16> %51, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %170 = shufflevector <16 x i16> %50, <16 x i16> %51, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %171 = bitcast <8 x i32> %130 to <16 x i16>
  %172 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %169, <16 x i16> %171) #9
  %173 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %170, <16 x i16> %171) #9
  %174 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %169, <16 x i16> %153) #9
  %175 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %170, <16 x i16> %153) #9
  %176 = add <8 x i32> %172, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %177 = add <8 x i32> %173, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %178 = add <8 x i32> %174, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %179 = add <8 x i32> %175, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %180 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %176, i32 %150) #9
  %181 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %177, i32 %150) #9
  %182 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %178, i32 %150) #9
  %183 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %179, i32 %150) #9
  %184 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %180, <8 x i32> %181) #9
  %185 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %182, <8 x i32> %183) #9
  %186 = shufflevector <16 x i16> %62, <16 x i16> %63, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %187 = shufflevector <16 x i16> %62, <16 x i16> %63, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %188 = bitcast <8 x i32> %139 to <16 x i16>
  %189 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %186, <16 x i16> %188) #9
  %190 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %187, <16 x i16> %188) #9
  %191 = bitcast <8 x i32> %143 to <16 x i16>
  %192 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %186, <16 x i16> %191) #9
  %193 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %187, <16 x i16> %191) #9
  %194 = add <8 x i32> %189, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %195 = add <8 x i32> %190, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %196 = add <8 x i32> %192, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %197 = add <8 x i32> %193, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %198 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %194, i32 %150) #9
  %199 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %195, i32 %150) #9
  %200 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %196, i32 %150) #9
  %201 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %197, i32 %150) #9
  %202 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %198, <8 x i32> %199) #9
  %203 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %200, <8 x i32> %201) #9
  %204 = shufflevector <16 x i16> %75, <16 x i16> %76, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %205 = shufflevector <16 x i16> %75, <16 x i16> %76, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %206 = bitcast <8 x i32> %149 to <16 x i16>
  %207 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %204, <16 x i16> %206) #9
  %208 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %205, <16 x i16> %206) #9
  %209 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %204, <16 x i16> %188) #9
  %210 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %205, <16 x i16> %188) #9
  %211 = add <8 x i32> %207, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %212 = add <8 x i32> %208, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %213 = add <8 x i32> %209, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %214 = add <8 x i32> %210, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %215 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %211, i32 %150) #9
  %216 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %212, i32 %150) #9
  %217 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %213, i32 %150) #9
  %218 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %214, i32 %150) #9
  %219 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %215, <8 x i32> %216) #9
  %220 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %217, <8 x i32> %218) #9
  %221 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %222 = trunc i32 %221 to i16
  %223 = shl i16 %222, 3
  %224 = insertelement <16 x i16> undef, i16 %223, i32 0
  %225 = shufflevector <16 x i16> %224, <16 x i16> undef, <16 x i32> zeroinitializer
  %226 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %5, <16 x i16> %225) #9
  %227 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 16), align 16
  %228 = sub i32 0, %227
  %229 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 48), align 16
  %230 = and i32 %228, 65535
  %231 = and i32 %229, 65535
  %232 = shl nuw i32 %231, 16
  %233 = or i32 %232, %230
  %234 = insertelement <8 x i32> undef, i32 %233, i32 0
  %235 = shufflevector <8 x i32> %234, <8 x i32> undef, <8 x i32> zeroinitializer
  %236 = shl i32 %227, 16
  %237 = or i32 %231, %236
  %238 = insertelement <8 x i32> undef, i32 %237, i32 0
  %239 = shufflevector <8 x i32> %238, <8 x i32> undef, <8 x i32> zeroinitializer
  %240 = sub i32 0, %229
  %241 = and i32 %240, 65535
  %242 = shl nuw i32 %230, 16
  %243 = or i32 %242, %241
  %244 = insertelement <8 x i32> undef, i32 %243, i32 0
  %245 = shufflevector <8 x i32> %244, <8 x i32> undef, <8 x i32> zeroinitializer
  %246 = shufflevector <16 x i16> %87, <16 x i16> %88, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %247 = shufflevector <16 x i16> %87, <16 x i16> %88, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %248 = bitcast <8 x i32> %235 to <16 x i16>
  %249 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %246, <16 x i16> %248) #9
  %250 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %247, <16 x i16> %248) #9
  %251 = bitcast <8 x i32> %239 to <16 x i16>
  %252 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %246, <16 x i16> %251) #9
  %253 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %247, <16 x i16> %251) #9
  %254 = add <8 x i32> %249, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %255 = add <8 x i32> %250, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %256 = add <8 x i32> %252, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %257 = add <8 x i32> %253, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %258 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %254, i32 %150) #9
  %259 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %255, i32 %150) #9
  %260 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %256, i32 %150) #9
  %261 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %257, i32 %150) #9
  %262 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %258, <8 x i32> %259) #9
  %263 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %260, <8 x i32> %261) #9
  %264 = shufflevector <16 x i16> %100, <16 x i16> %101, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %265 = shufflevector <16 x i16> %100, <16 x i16> %101, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %266 = bitcast <8 x i32> %245 to <16 x i16>
  %267 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %264, <16 x i16> %266) #9
  %268 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %265, <16 x i16> %266) #9
  %269 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %264, <16 x i16> %248) #9
  %270 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %265, <16 x i16> %248) #9
  %271 = add <8 x i32> %267, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %272 = add <8 x i32> %268, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %273 = add <8 x i32> %269, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %274 = add <8 x i32> %270, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %275 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %271, i32 %150) #9
  %276 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %272, i32 %150) #9
  %277 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %273, i32 %150) #9
  %278 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %274, i32 %150) #9
  %279 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %275, <8 x i32> %276) #9
  %280 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %277, <8 x i32> %278) #9
  %281 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %37, <16 x i16> %50) #9
  %282 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %37, <16 x i16> %50) #9
  %283 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %167, <16 x i16> %184) #9
  %284 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %167, <16 x i16> %184) #9
  %285 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %75, <16 x i16> %62) #9
  %286 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %75, <16 x i16> %62) #9
  %287 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %219, <16 x i16> %202) #9
  %288 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %219, <16 x i16> %202) #9
  %289 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %76, <16 x i16> %63) #9
  %290 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %76, <16 x i16> %63) #9
  %291 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %220, <16 x i16> %203) #9
  %292 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %220, <16 x i16> %203) #9
  %293 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %38, <16 x i16> %51) #9
  %294 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %38, <16 x i16> %51) #9
  %295 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %168, <16 x i16> %185) #9
  %296 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %168, <16 x i16> %185) #9
  %297 = sub i32 0, %221
  %298 = and i32 %297, 65535
  %299 = and i32 %221, 65535
  %300 = shl nuw i32 %299, 16
  %301 = or i32 %300, %298
  %302 = insertelement <8 x i32> undef, i32 %301, i32 0
  %303 = shufflevector <8 x i32> %302, <8 x i32> undef, <8 x i32> zeroinitializer
  %304 = or i32 %300, %299
  %305 = insertelement <8 x i32> undef, i32 %304, i32 0
  %306 = shufflevector <8 x i32> %305, <8 x i32> undef, <8 x i32> zeroinitializer
  %307 = shufflevector <16 x i16> %112, <16 x i16> %113, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %308 = shufflevector <16 x i16> %112, <16 x i16> %113, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %309 = bitcast <8 x i32> %303 to <16 x i16>
  %310 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %307, <16 x i16> %309) #9
  %311 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %308, <16 x i16> %309) #9
  %312 = bitcast <8 x i32> %306 to <16 x i16>
  %313 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %307, <16 x i16> %312) #9
  %314 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %308, <16 x i16> %312) #9
  %315 = add <8 x i32> %310, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %316 = add <8 x i32> %311, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %317 = add <8 x i32> %313, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %318 = add <8 x i32> %314, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %319 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %315, i32 %150) #9
  %320 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %316, i32 %150) #9
  %321 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %317, i32 %150) #9
  %322 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %318, i32 %150) #9
  %323 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %319, <8 x i32> %320) #9
  %324 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %321, <8 x i32> %322) #9
  %325 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %87, <16 x i16> %100) #9
  %326 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %87, <16 x i16> %100) #9
  %327 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %262, <16 x i16> %279) #9
  %328 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %262, <16 x i16> %279) #9
  %329 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %88, <16 x i16> %101) #9
  %330 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %88, <16 x i16> %101) #9
  %331 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %263, <16 x i16> %280) #9
  %332 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %263, <16 x i16> %280) #9
  %333 = shufflevector <16 x i16> %284, <16 x i16> %296, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %334 = shufflevector <16 x i16> %284, <16 x i16> %296, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %335 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %333, <16 x i16> %248) #9
  %336 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %334, <16 x i16> %248) #9
  %337 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %333, <16 x i16> %251) #9
  %338 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %334, <16 x i16> %251) #9
  %339 = add <8 x i32> %335, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %340 = add <8 x i32> %336, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %341 = add <8 x i32> %337, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %342 = add <8 x i32> %338, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %343 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %339, i32 %150) #9
  %344 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %340, i32 %150) #9
  %345 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %341, i32 %150) #9
  %346 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %342, i32 %150) #9
  %347 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %343, <8 x i32> %344) #9
  %348 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %345, <8 x i32> %346) #9
  %349 = shufflevector <16 x i16> %282, <16 x i16> %294, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %350 = shufflevector <16 x i16> %282, <16 x i16> %294, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %351 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %349, <16 x i16> %248) #9
  %352 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %350, <16 x i16> %248) #9
  %353 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %349, <16 x i16> %251) #9
  %354 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %350, <16 x i16> %251) #9
  %355 = add <8 x i32> %351, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %356 = add <8 x i32> %352, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %357 = add <8 x i32> %353, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %358 = add <8 x i32> %354, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %359 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %355, i32 %150) #9
  %360 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %356, i32 %150) #9
  %361 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %357, i32 %150) #9
  %362 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %358, i32 %150) #9
  %363 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %359, <8 x i32> %360) #9
  %364 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %361, <8 x i32> %362) #9
  %365 = shufflevector <16 x i16> %286, <16 x i16> %290, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %366 = shufflevector <16 x i16> %286, <16 x i16> %290, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %367 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %365, <16 x i16> %266) #9
  %368 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %366, <16 x i16> %266) #9
  %369 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %365, <16 x i16> %248) #9
  %370 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %366, <16 x i16> %248) #9
  %371 = add <8 x i32> %367, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %372 = add <8 x i32> %368, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %373 = add <8 x i32> %369, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %374 = add <8 x i32> %370, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %375 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %371, i32 %150) #9
  %376 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %372, i32 %150) #9
  %377 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %373, i32 %150) #9
  %378 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %374, i32 %150) #9
  %379 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %375, <8 x i32> %376) #9
  %380 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %377, <8 x i32> %378) #9
  %381 = shufflevector <16 x i16> %288, <16 x i16> %292, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %382 = shufflevector <16 x i16> %288, <16 x i16> %292, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %383 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %381, <16 x i16> %266) #9
  %384 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %382, <16 x i16> %266) #9
  %385 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %381, <16 x i16> %248) #9
  %386 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %382, <16 x i16> %248) #9
  %387 = add <8 x i32> %383, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %388 = add <8 x i32> %384, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %389 = add <8 x i32> %385, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %390 = add <8 x i32> %386, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %391 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %387, i32 %150) #9
  %392 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %388, i32 %150) #9
  %393 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %389, i32 %150) #9
  %394 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %390, i32 %150) #9
  %395 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %391, <8 x i32> %392) #9
  %396 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %393, <8 x i32> %394) #9
  %397 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %226, <16 x i16> %113) #9
  %398 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %226, <16 x i16> %113) #9
  %399 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %226, <16 x i16> %324) #9
  %400 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %226, <16 x i16> %324) #9
  %401 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %226, <16 x i16> %323) #9
  %402 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %226, <16 x i16> %323) #9
  %403 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %226, <16 x i16> %112) #9
  %404 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %226, <16 x i16> %112) #9
  %405 = shufflevector <16 x i16> %328, <16 x i16> %332, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %406 = shufflevector <16 x i16> %328, <16 x i16> %332, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %407 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %405, <16 x i16> %309) #9
  %408 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %406, <16 x i16> %309) #9
  %409 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %405, <16 x i16> %312) #9
  %410 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %406, <16 x i16> %312) #9
  %411 = add <8 x i32> %407, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %412 = add <8 x i32> %408, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %413 = add <8 x i32> %409, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %414 = add <8 x i32> %410, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %415 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %411, i32 %150) #9
  %416 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %412, i32 %150) #9
  %417 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %413, i32 %150) #9
  %418 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %414, i32 %150) #9
  %419 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %415, <8 x i32> %416) #9
  %420 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %417, <8 x i32> %418) #9
  %421 = shufflevector <16 x i16> %326, <16 x i16> %330, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %422 = shufflevector <16 x i16> %326, <16 x i16> %330, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %423 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %421, <16 x i16> %309) #9
  %424 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %422, <16 x i16> %309) #9
  %425 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %421, <16 x i16> %312) #9
  %426 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %422, <16 x i16> %312) #9
  %427 = add <8 x i32> %423, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %428 = add <8 x i32> %424, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %429 = add <8 x i32> %425, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %430 = add <8 x i32> %426, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %431 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %427, i32 %150) #9
  %432 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %428, i32 %150) #9
  %433 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %429, i32 %150) #9
  %434 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %430, i32 %150) #9
  %435 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %431, <8 x i32> %432) #9
  %436 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %433, <8 x i32> %434) #9
  %437 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %281, <16 x i16> %285) #9
  %438 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %281, <16 x i16> %285) #9
  %439 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %283, <16 x i16> %287) #9
  %440 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %283, <16 x i16> %287) #9
  %441 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %347, <16 x i16> %395) #9
  %442 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %347, <16 x i16> %395) #9
  %443 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %363, <16 x i16> %379) #9
  %444 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %363, <16 x i16> %379) #9
  %445 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %293, <16 x i16> %289) #9
  %446 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %293, <16 x i16> %289) #9
  %447 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %295, <16 x i16> %291) #9
  %448 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %295, <16 x i16> %291) #9
  %449 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %348, <16 x i16> %396) #9
  %450 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %348, <16 x i16> %396) #9
  %451 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %364, <16 x i16> %380) #9
  %452 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %364, <16 x i16> %380) #9
  %453 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %397, <16 x i16> %329) #9
  %454 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %397, <16 x i16> %329) #9
  %455 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %399, <16 x i16> %331) #9
  %456 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %399, <16 x i16> %331) #9
  %457 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %401, <16 x i16> %420) #9
  %458 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %401, <16 x i16> %420) #9
  %459 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %403, <16 x i16> %436) #9
  %460 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %403, <16 x i16> %436) #9
  %461 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %404, <16 x i16> %435) #9
  %462 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %404, <16 x i16> %435) #9
  %463 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %402, <16 x i16> %419) #9
  %464 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %402, <16 x i16> %419) #9
  %465 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %400, <16 x i16> %327) #9
  %466 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %400, <16 x i16> %327) #9
  %467 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %398, <16 x i16> %325) #9
  %468 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %398, <16 x i16> %325) #9
  %469 = shufflevector <16 x i16> %444, <16 x i16> %452, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %470 = shufflevector <16 x i16> %444, <16 x i16> %452, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %471 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %469, <16 x i16> %309) #9
  %472 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %470, <16 x i16> %309) #9
  %473 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %469, <16 x i16> %312) #9
  %474 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %470, <16 x i16> %312) #9
  %475 = add <8 x i32> %471, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %476 = add <8 x i32> %472, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %477 = add <8 x i32> %473, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %478 = add <8 x i32> %474, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %479 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %475, i32 %150) #9
  %480 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %476, i32 %150) #9
  %481 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %477, i32 %150) #9
  %482 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %478, i32 %150) #9
  %483 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %479, <8 x i32> %480) #9
  %484 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %481, <8 x i32> %482) #9
  %485 = shufflevector <16 x i16> %442, <16 x i16> %450, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %486 = shufflevector <16 x i16> %442, <16 x i16> %450, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %487 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %485, <16 x i16> %309) #9
  %488 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %486, <16 x i16> %309) #9
  %489 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %485, <16 x i16> %312) #9
  %490 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %486, <16 x i16> %312) #9
  %491 = add <8 x i32> %487, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %492 = add <8 x i32> %488, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %493 = add <8 x i32> %489, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %494 = add <8 x i32> %490, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %495 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %491, i32 %150) #9
  %496 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %492, i32 %150) #9
  %497 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %493, i32 %150) #9
  %498 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %494, i32 %150) #9
  %499 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %495, <8 x i32> %496) #9
  %500 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %497, <8 x i32> %498) #9
  %501 = shufflevector <16 x i16> %440, <16 x i16> %448, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %502 = shufflevector <16 x i16> %440, <16 x i16> %448, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %503 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %501, <16 x i16> %309) #9
  %504 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %502, <16 x i16> %309) #9
  %505 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %501, <16 x i16> %312) #9
  %506 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %502, <16 x i16> %312) #9
  %507 = add <8 x i32> %503, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %508 = add <8 x i32> %504, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %509 = add <8 x i32> %505, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %510 = add <8 x i32> %506, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %511 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %507, i32 %150) #9
  %512 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %508, i32 %150) #9
  %513 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %509, i32 %150) #9
  %514 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %510, i32 %150) #9
  %515 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %511, <8 x i32> %512) #9
  %516 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %513, <8 x i32> %514) #9
  %517 = shufflevector <16 x i16> %438, <16 x i16> %446, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %518 = shufflevector <16 x i16> %438, <16 x i16> %446, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %519 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %517, <16 x i16> %309) #9
  %520 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %518, <16 x i16> %309) #9
  %521 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %517, <16 x i16> %312) #9
  %522 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %518, <16 x i16> %312) #9
  %523 = add <8 x i32> %519, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %524 = add <8 x i32> %520, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %525 = add <8 x i32> %521, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %526 = add <8 x i32> %522, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %527 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %523, i32 %150) #9
  %528 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %524, i32 %150) #9
  %529 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %525, i32 %150) #9
  %530 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %526, i32 %150) #9
  %531 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %527, <8 x i32> %528) #9
  %532 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %529, <8 x i32> %530) #9
  %533 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 31
  %534 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %453, <16 x i16> %445) #9
  %535 = bitcast <4 x i64>* %1 to <16 x i16>*
  store <16 x i16> %534, <16 x i16>* %535, align 32
  %536 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %453, <16 x i16> %445) #9
  %537 = bitcast <4 x i64>* %533 to <16 x i16>*
  store <16 x i16> %536, <16 x i16>* %537, align 32
  %538 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %539 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 30
  %540 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %455, <16 x i16> %447) #9
  %541 = bitcast <4 x i64>* %538 to <16 x i16>*
  store <16 x i16> %540, <16 x i16>* %541, align 32
  %542 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %455, <16 x i16> %447) #9
  %543 = bitcast <4 x i64>* %539 to <16 x i16>*
  store <16 x i16> %542, <16 x i16>* %543, align 32
  %544 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %545 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 29
  %546 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %457, <16 x i16> %449) #9
  %547 = bitcast <4 x i64>* %544 to <16 x i16>*
  store <16 x i16> %546, <16 x i16>* %547, align 32
  %548 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %457, <16 x i16> %449) #9
  %549 = bitcast <4 x i64>* %545 to <16 x i16>*
  store <16 x i16> %548, <16 x i16>* %549, align 32
  %550 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %551 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 28
  %552 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %459, <16 x i16> %451) #9
  %553 = bitcast <4 x i64>* %550 to <16 x i16>*
  store <16 x i16> %552, <16 x i16>* %553, align 32
  %554 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %459, <16 x i16> %451) #9
  %555 = bitcast <4 x i64>* %551 to <16 x i16>*
  store <16 x i16> %554, <16 x i16>* %555, align 32
  %556 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %557 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 27
  %558 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %461, <16 x i16> %484) #9
  %559 = bitcast <4 x i64>* %556 to <16 x i16>*
  store <16 x i16> %558, <16 x i16>* %559, align 32
  %560 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %461, <16 x i16> %484) #9
  %561 = bitcast <4 x i64>* %557 to <16 x i16>*
  store <16 x i16> %560, <16 x i16>* %561, align 32
  %562 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %563 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 26
  %564 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %463, <16 x i16> %500) #9
  %565 = bitcast <4 x i64>* %562 to <16 x i16>*
  store <16 x i16> %564, <16 x i16>* %565, align 32
  %566 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %463, <16 x i16> %500) #9
  %567 = bitcast <4 x i64>* %563 to <16 x i16>*
  store <16 x i16> %566, <16 x i16>* %567, align 32
  %568 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %569 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 25
  %570 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %465, <16 x i16> %516) #9
  %571 = bitcast <4 x i64>* %568 to <16 x i16>*
  store <16 x i16> %570, <16 x i16>* %571, align 32
  %572 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %465, <16 x i16> %516) #9
  %573 = bitcast <4 x i64>* %569 to <16 x i16>*
  store <16 x i16> %572, <16 x i16>* %573, align 32
  %574 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %575 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 24
  %576 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %467, <16 x i16> %532) #9
  %577 = bitcast <4 x i64>* %574 to <16 x i16>*
  store <16 x i16> %576, <16 x i16>* %577, align 32
  %578 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %467, <16 x i16> %532) #9
  %579 = bitcast <4 x i64>* %575 to <16 x i16>*
  store <16 x i16> %578, <16 x i16>* %579, align 32
  %580 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %581 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 23
  %582 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %468, <16 x i16> %531) #9
  %583 = bitcast <4 x i64>* %580 to <16 x i16>*
  store <16 x i16> %582, <16 x i16>* %583, align 32
  %584 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %468, <16 x i16> %531) #9
  %585 = bitcast <4 x i64>* %581 to <16 x i16>*
  store <16 x i16> %584, <16 x i16>* %585, align 32
  %586 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %587 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 22
  %588 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %466, <16 x i16> %515) #9
  %589 = bitcast <4 x i64>* %586 to <16 x i16>*
  store <16 x i16> %588, <16 x i16>* %589, align 32
  %590 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %466, <16 x i16> %515) #9
  %591 = bitcast <4 x i64>* %587 to <16 x i16>*
  store <16 x i16> %590, <16 x i16>* %591, align 32
  %592 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %593 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 21
  %594 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %464, <16 x i16> %499) #9
  %595 = bitcast <4 x i64>* %592 to <16 x i16>*
  store <16 x i16> %594, <16 x i16>* %595, align 32
  %596 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %464, <16 x i16> %499) #9
  %597 = bitcast <4 x i64>* %593 to <16 x i16>*
  store <16 x i16> %596, <16 x i16>* %597, align 32
  %598 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %599 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 20
  %600 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %462, <16 x i16> %483) #9
  %601 = bitcast <4 x i64>* %598 to <16 x i16>*
  store <16 x i16> %600, <16 x i16>* %601, align 32
  %602 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %462, <16 x i16> %483) #9
  %603 = bitcast <4 x i64>* %599 to <16 x i16>*
  store <16 x i16> %602, <16 x i16>* %603, align 32
  %604 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %605 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 19
  %606 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %460, <16 x i16> %443) #9
  %607 = bitcast <4 x i64>* %604 to <16 x i16>*
  store <16 x i16> %606, <16 x i16>* %607, align 32
  %608 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %460, <16 x i16> %443) #9
  %609 = bitcast <4 x i64>* %605 to <16 x i16>*
  store <16 x i16> %608, <16 x i16>* %609, align 32
  %610 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %611 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 18
  %612 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %458, <16 x i16> %441) #9
  %613 = bitcast <4 x i64>* %610 to <16 x i16>*
  store <16 x i16> %612, <16 x i16>* %613, align 32
  %614 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %458, <16 x i16> %441) #9
  %615 = bitcast <4 x i64>* %611 to <16 x i16>*
  store <16 x i16> %614, <16 x i16>* %615, align 32
  %616 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %617 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 17
  %618 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %456, <16 x i16> %439) #9
  %619 = bitcast <4 x i64>* %616 to <16 x i16>*
  store <16 x i16> %618, <16 x i16>* %619, align 32
  %620 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %456, <16 x i16> %439) #9
  %621 = bitcast <4 x i64>* %617 to <16 x i16>*
  store <16 x i16> %620, <16 x i16>* %621, align 32
  %622 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 15
  %623 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 16
  %624 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %454, <16 x i16> %437) #9
  %625 = bitcast <4 x i64>* %622 to <16 x i16>*
  store <16 x i16> %624, <16 x i16>* %625, align 32
  %626 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %454, <16 x i16> %437) #9
  %627 = bitcast <4 x i64>* %623 to <16 x i16>*
  store <16 x i16> %626, <16 x i16>* %627, align 32
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @idct32_low16_avx2(<4 x i64>* nocapture readonly, <4 x i64>*, i8 signext) #0 {
  %4 = bitcast <4 x i64>* %0 to <16 x i16>*
  %5 = load <16 x i16>, <16 x i16>* %4, align 32
  %6 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 8
  %7 = bitcast <4 x i64>* %6 to <16 x i16>*
  %8 = load <16 x i16>, <16 x i16>* %7, align 32
  %9 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 4
  %10 = bitcast <4 x i64>* %9 to <16 x i16>*
  %11 = load <16 x i16>, <16 x i16>* %10, align 32
  %12 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 12
  %13 = bitcast <4 x i64>* %12 to <16 x i16>*
  %14 = load <16 x i16>, <16 x i16>* %13, align 32
  %15 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 2
  %16 = bitcast <4 x i64>* %15 to <16 x i16>*
  %17 = load <16 x i16>, <16 x i16>* %16, align 32
  %18 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 10
  %19 = bitcast <4 x i64>* %18 to <16 x i16>*
  %20 = load <16 x i16>, <16 x i16>* %19, align 32
  %21 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 6
  %22 = bitcast <4 x i64>* %21 to <16 x i16>*
  %23 = load <16 x i16>, <16 x i16>* %22, align 32
  %24 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 14
  %25 = bitcast <4 x i64>* %24 to <16 x i16>*
  %26 = load <16 x i16>, <16 x i16>* %25, align 32
  %27 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 1
  %28 = bitcast <4 x i64>* %27 to <16 x i16>*
  %29 = load <16 x i16>, <16 x i16>* %28, align 32
  %30 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 9
  %31 = bitcast <4 x i64>* %30 to <16 x i16>*
  %32 = load <16 x i16>, <16 x i16>* %31, align 32
  %33 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 5
  %34 = bitcast <4 x i64>* %33 to <16 x i16>*
  %35 = load <16 x i16>, <16 x i16>* %34, align 32
  %36 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 13
  %37 = bitcast <4 x i64>* %36 to <16 x i16>*
  %38 = load <16 x i16>, <16 x i16>* %37, align 32
  %39 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 3
  %40 = bitcast <4 x i64>* %39 to <16 x i16>*
  %41 = load <16 x i16>, <16 x i16>* %40, align 32
  %42 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 11
  %43 = bitcast <4 x i64>* %42 to <16 x i16>*
  %44 = load <16 x i16>, <16 x i16>* %43, align 32
  %45 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 7
  %46 = bitcast <4 x i64>* %45 to <16 x i16>*
  %47 = load <16 x i16>, <16 x i16>* %46, align 32
  %48 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 15
  %49 = bitcast <4 x i64>* %48 to <16 x i16>*
  %50 = load <16 x i16>, <16 x i16>* %49, align 32
  %51 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 62), align 8
  %52 = trunc i32 %51 to i16
  %53 = shl i16 %52, 3
  %54 = insertelement <16 x i16> undef, i16 %53, i32 0
  %55 = shufflevector <16 x i16> %54, <16 x i16> undef, <16 x i32> zeroinitializer
  %56 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 2), align 8
  %57 = trunc i32 %56 to i16
  %58 = shl i16 %57, 3
  %59 = insertelement <16 x i16> undef, i16 %58, i32 0
  %60 = shufflevector <16 x i16> %59, <16 x i16> undef, <16 x i32> zeroinitializer
  %61 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %29, <16 x i16> %55) #9
  %62 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %29, <16 x i16> %60) #9
  %63 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 34), align 8
  %64 = trunc i32 %63 to i16
  %65 = shl i16 %64, 3
  %66 = sub i16 0, %65
  %67 = insertelement <16 x i16> undef, i16 %66, i32 0
  %68 = shufflevector <16 x i16> %67, <16 x i16> undef, <16 x i32> zeroinitializer
  %69 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 30), align 8
  %70 = trunc i32 %69 to i16
  %71 = shl i16 %70, 3
  %72 = insertelement <16 x i16> undef, i16 %71, i32 0
  %73 = shufflevector <16 x i16> %72, <16 x i16> undef, <16 x i32> zeroinitializer
  %74 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %50, <16 x i16> %68) #9
  %75 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %50, <16 x i16> %73) #9
  %76 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 46), align 8
  %77 = trunc i32 %76 to i16
  %78 = shl i16 %77, 3
  %79 = insertelement <16 x i16> undef, i16 %78, i32 0
  %80 = shufflevector <16 x i16> %79, <16 x i16> undef, <16 x i32> zeroinitializer
  %81 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 18), align 8
  %82 = trunc i32 %81 to i16
  %83 = shl i16 %82, 3
  %84 = insertelement <16 x i16> undef, i16 %83, i32 0
  %85 = shufflevector <16 x i16> %84, <16 x i16> undef, <16 x i32> zeroinitializer
  %86 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %32, <16 x i16> %80) #9
  %87 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %32, <16 x i16> %85) #9
  %88 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 50), align 8
  %89 = trunc i32 %88 to i16
  %90 = shl i16 %89, 3
  %91 = sub i16 0, %90
  %92 = insertelement <16 x i16> undef, i16 %91, i32 0
  %93 = shufflevector <16 x i16> %92, <16 x i16> undef, <16 x i32> zeroinitializer
  %94 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 14), align 8
  %95 = trunc i32 %94 to i16
  %96 = shl i16 %95, 3
  %97 = insertelement <16 x i16> undef, i16 %96, i32 0
  %98 = shufflevector <16 x i16> %97, <16 x i16> undef, <16 x i32> zeroinitializer
  %99 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %47, <16 x i16> %93) #9
  %100 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %47, <16 x i16> %98) #9
  %101 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 54), align 8
  %102 = trunc i32 %101 to i16
  %103 = shl i16 %102, 3
  %104 = insertelement <16 x i16> undef, i16 %103, i32 0
  %105 = shufflevector <16 x i16> %104, <16 x i16> undef, <16 x i32> zeroinitializer
  %106 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 10), align 8
  %107 = trunc i32 %106 to i16
  %108 = shl i16 %107, 3
  %109 = insertelement <16 x i16> undef, i16 %108, i32 0
  %110 = shufflevector <16 x i16> %109, <16 x i16> undef, <16 x i32> zeroinitializer
  %111 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %35, <16 x i16> %105) #9
  %112 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %35, <16 x i16> %110) #9
  %113 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 42), align 8
  %114 = trunc i32 %113 to i16
  %115 = shl i16 %114, 3
  %116 = sub i16 0, %115
  %117 = insertelement <16 x i16> undef, i16 %116, i32 0
  %118 = shufflevector <16 x i16> %117, <16 x i16> undef, <16 x i32> zeroinitializer
  %119 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 22), align 8
  %120 = trunc i32 %119 to i16
  %121 = shl i16 %120, 3
  %122 = insertelement <16 x i16> undef, i16 %121, i32 0
  %123 = shufflevector <16 x i16> %122, <16 x i16> undef, <16 x i32> zeroinitializer
  %124 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %44, <16 x i16> %118) #9
  %125 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %44, <16 x i16> %123) #9
  %126 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 38), align 8
  %127 = trunc i32 %126 to i16
  %128 = shl i16 %127, 3
  %129 = insertelement <16 x i16> undef, i16 %128, i32 0
  %130 = shufflevector <16 x i16> %129, <16 x i16> undef, <16 x i32> zeroinitializer
  %131 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 26), align 8
  %132 = trunc i32 %131 to i16
  %133 = shl i16 %132, 3
  %134 = insertelement <16 x i16> undef, i16 %133, i32 0
  %135 = shufflevector <16 x i16> %134, <16 x i16> undef, <16 x i32> zeroinitializer
  %136 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %38, <16 x i16> %130) #9
  %137 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %38, <16 x i16> %135) #9
  %138 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 58), align 8
  %139 = trunc i32 %138 to i16
  %140 = shl i16 %139, 3
  %141 = sub i16 0, %140
  %142 = insertelement <16 x i16> undef, i16 %141, i32 0
  %143 = shufflevector <16 x i16> %142, <16 x i16> undef, <16 x i32> zeroinitializer
  %144 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 6), align 8
  %145 = trunc i32 %144 to i16
  %146 = shl i16 %145, 3
  %147 = insertelement <16 x i16> undef, i16 %146, i32 0
  %148 = shufflevector <16 x i16> %147, <16 x i16> undef, <16 x i32> zeroinitializer
  %149 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %41, <16 x i16> %143) #9
  %150 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %41, <16 x i16> %148) #9
  %151 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 60), align 16
  %152 = trunc i32 %151 to i16
  %153 = shl i16 %152, 3
  %154 = insertelement <16 x i16> undef, i16 %153, i32 0
  %155 = shufflevector <16 x i16> %154, <16 x i16> undef, <16 x i32> zeroinitializer
  %156 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 4), align 16
  %157 = trunc i32 %156 to i16
  %158 = shl i16 %157, 3
  %159 = insertelement <16 x i16> undef, i16 %158, i32 0
  %160 = shufflevector <16 x i16> %159, <16 x i16> undef, <16 x i32> zeroinitializer
  %161 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %17, <16 x i16> %155) #9
  %162 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %17, <16 x i16> %160) #9
  %163 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 36), align 16
  %164 = trunc i32 %163 to i16
  %165 = shl i16 %164, 3
  %166 = sub i16 0, %165
  %167 = insertelement <16 x i16> undef, i16 %166, i32 0
  %168 = shufflevector <16 x i16> %167, <16 x i16> undef, <16 x i32> zeroinitializer
  %169 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 28), align 16
  %170 = trunc i32 %169 to i16
  %171 = shl i16 %170, 3
  %172 = insertelement <16 x i16> undef, i16 %171, i32 0
  %173 = shufflevector <16 x i16> %172, <16 x i16> undef, <16 x i32> zeroinitializer
  %174 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %26, <16 x i16> %168) #9
  %175 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %26, <16 x i16> %173) #9
  %176 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 44), align 16
  %177 = trunc i32 %176 to i16
  %178 = shl i16 %177, 3
  %179 = insertelement <16 x i16> undef, i16 %178, i32 0
  %180 = shufflevector <16 x i16> %179, <16 x i16> undef, <16 x i32> zeroinitializer
  %181 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 20), align 16
  %182 = trunc i32 %181 to i16
  %183 = shl i16 %182, 3
  %184 = insertelement <16 x i16> undef, i16 %183, i32 0
  %185 = shufflevector <16 x i16> %184, <16 x i16> undef, <16 x i32> zeroinitializer
  %186 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %20, <16 x i16> %180) #9
  %187 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %20, <16 x i16> %185) #9
  %188 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 52), align 16
  %189 = trunc i32 %188 to i16
  %190 = shl i16 %189, 3
  %191 = sub i16 0, %190
  %192 = insertelement <16 x i16> undef, i16 %191, i32 0
  %193 = shufflevector <16 x i16> %192, <16 x i16> undef, <16 x i32> zeroinitializer
  %194 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 12), align 16
  %195 = trunc i32 %194 to i16
  %196 = shl i16 %195, 3
  %197 = insertelement <16 x i16> undef, i16 %196, i32 0
  %198 = shufflevector <16 x i16> %197, <16 x i16> undef, <16 x i32> zeroinitializer
  %199 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %23, <16 x i16> %193) #9
  %200 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %23, <16 x i16> %198) #9
  %201 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %61, <16 x i16> %74) #9
  %202 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %61, <16 x i16> %74) #9
  %203 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %99, <16 x i16> %86) #9
  %204 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %99, <16 x i16> %86) #9
  %205 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %111, <16 x i16> %124) #9
  %206 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %111, <16 x i16> %124) #9
  %207 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %149, <16 x i16> %136) #9
  %208 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %149, <16 x i16> %136) #9
  %209 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %150, <16 x i16> %137) #9
  %210 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %150, <16 x i16> %137) #9
  %211 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %112, <16 x i16> %125) #9
  %212 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %112, <16 x i16> %125) #9
  %213 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %100, <16 x i16> %87) #9
  %214 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %100, <16 x i16> %87) #9
  %215 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %62, <16 x i16> %75) #9
  %216 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %62, <16 x i16> %75) #9
  %217 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 56), align 16
  %218 = trunc i32 %217 to i16
  %219 = shl i16 %218, 3
  %220 = insertelement <16 x i16> undef, i16 %219, i32 0
  %221 = shufflevector <16 x i16> %220, <16 x i16> undef, <16 x i32> zeroinitializer
  %222 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 8), align 16
  %223 = trunc i32 %222 to i16
  %224 = shl i16 %223, 3
  %225 = insertelement <16 x i16> undef, i16 %224, i32 0
  %226 = shufflevector <16 x i16> %225, <16 x i16> undef, <16 x i32> zeroinitializer
  %227 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %11, <16 x i16> %221) #9
  %228 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %11, <16 x i16> %226) #9
  %229 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 40), align 16
  %230 = trunc i32 %229 to i16
  %231 = shl i16 %230, 3
  %232 = sub i16 0, %231
  %233 = insertelement <16 x i16> undef, i16 %232, i32 0
  %234 = shufflevector <16 x i16> %233, <16 x i16> undef, <16 x i32> zeroinitializer
  %235 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 24), align 16
  %236 = trunc i32 %235 to i16
  %237 = shl i16 %236, 3
  %238 = insertelement <16 x i16> undef, i16 %237, i32 0
  %239 = shufflevector <16 x i16> %238, <16 x i16> undef, <16 x i32> zeroinitializer
  %240 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %14, <16 x i16> %234) #9
  %241 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %14, <16 x i16> %239) #9
  %242 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %161, <16 x i16> %174) #9
  %243 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %161, <16 x i16> %174) #9
  %244 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %199, <16 x i16> %186) #9
  %245 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %199, <16 x i16> %186) #9
  %246 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %200, <16 x i16> %187) #9
  %247 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %200, <16 x i16> %187) #9
  %248 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %162, <16 x i16> %175) #9
  %249 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %162, <16 x i16> %175) #9
  %250 = sub i32 0, %222
  %251 = and i32 %250, 65535
  %252 = and i32 %217, 65535
  %253 = shl nuw i32 %252, 16
  %254 = or i32 %253, %251
  %255 = insertelement <8 x i32> undef, i32 %254, i32 0
  %256 = shufflevector <8 x i32> %255, <8 x i32> undef, <8 x i32> zeroinitializer
  %257 = shl i32 %222, 16
  %258 = or i32 %257, %252
  %259 = insertelement <8 x i32> undef, i32 %258, i32 0
  %260 = shufflevector <8 x i32> %259, <8 x i32> undef, <8 x i32> zeroinitializer
  %261 = sub i32 0, %217
  %262 = and i32 %261, 65535
  %263 = shl nuw i32 %251, 16
  %264 = or i32 %263, %262
  %265 = insertelement <8 x i32> undef, i32 %264, i32 0
  %266 = shufflevector <8 x i32> %265, <8 x i32> undef, <8 x i32> zeroinitializer
  %267 = sub i32 0, %229
  %268 = and i32 %267, 65535
  %269 = and i32 %235, 65535
  %270 = shl nuw i32 %269, 16
  %271 = or i32 %270, %268
  %272 = insertelement <8 x i32> undef, i32 %271, i32 0
  %273 = shufflevector <8 x i32> %272, <8 x i32> undef, <8 x i32> zeroinitializer
  %274 = shl i32 %229, 16
  %275 = or i32 %269, %274
  %276 = insertelement <8 x i32> undef, i32 %275, i32 0
  %277 = shufflevector <8 x i32> %276, <8 x i32> undef, <8 x i32> zeroinitializer
  %278 = sub i32 0, %235
  %279 = and i32 %278, 65535
  %280 = shl nuw i32 %268, 16
  %281 = or i32 %280, %279
  %282 = insertelement <8 x i32> undef, i32 %281, i32 0
  %283 = shufflevector <8 x i32> %282, <8 x i32> undef, <8 x i32> zeroinitializer
  %284 = sext i8 %2 to i32
  %285 = shufflevector <16 x i16> %202, <16 x i16> %216, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %286 = shufflevector <16 x i16> %202, <16 x i16> %216, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %287 = bitcast <8 x i32> %256 to <16 x i16>
  %288 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %285, <16 x i16> %287) #9
  %289 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %286, <16 x i16> %287) #9
  %290 = bitcast <8 x i32> %260 to <16 x i16>
  %291 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %285, <16 x i16> %290) #9
  %292 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %286, <16 x i16> %290) #9
  %293 = add <8 x i32> %288, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %294 = add <8 x i32> %289, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %295 = add <8 x i32> %291, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %296 = add <8 x i32> %292, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %297 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %293, i32 %284) #9
  %298 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %294, i32 %284) #9
  %299 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %295, i32 %284) #9
  %300 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %296, i32 %284) #9
  %301 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %297, <8 x i32> %298) #9
  %302 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %299, <8 x i32> %300) #9
  %303 = shufflevector <16 x i16> %204, <16 x i16> %214, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %304 = shufflevector <16 x i16> %204, <16 x i16> %214, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %305 = bitcast <8 x i32> %266 to <16 x i16>
  %306 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %303, <16 x i16> %305) #9
  %307 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %304, <16 x i16> %305) #9
  %308 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %303, <16 x i16> %287) #9
  %309 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %304, <16 x i16> %287) #9
  %310 = add <8 x i32> %306, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %311 = add <8 x i32> %307, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %312 = add <8 x i32> %308, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %313 = add <8 x i32> %309, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %314 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %310, i32 %284) #9
  %315 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %311, i32 %284) #9
  %316 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %312, i32 %284) #9
  %317 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %313, i32 %284) #9
  %318 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %314, <8 x i32> %315) #9
  %319 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %316, <8 x i32> %317) #9
  %320 = shufflevector <16 x i16> %206, <16 x i16> %212, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %321 = shufflevector <16 x i16> %206, <16 x i16> %212, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %322 = bitcast <8 x i32> %273 to <16 x i16>
  %323 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %320, <16 x i16> %322) #9
  %324 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %321, <16 x i16> %322) #9
  %325 = bitcast <8 x i32> %277 to <16 x i16>
  %326 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %320, <16 x i16> %325) #9
  %327 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %321, <16 x i16> %325) #9
  %328 = add <8 x i32> %323, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %329 = add <8 x i32> %324, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %330 = add <8 x i32> %326, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %331 = add <8 x i32> %327, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %332 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %328, i32 %284) #9
  %333 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %329, i32 %284) #9
  %334 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %330, i32 %284) #9
  %335 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %331, i32 %284) #9
  %336 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %332, <8 x i32> %333) #9
  %337 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %334, <8 x i32> %335) #9
  %338 = shufflevector <16 x i16> %208, <16 x i16> %210, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %339 = shufflevector <16 x i16> %208, <16 x i16> %210, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %340 = bitcast <8 x i32> %283 to <16 x i16>
  %341 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %338, <16 x i16> %340) #9
  %342 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %339, <16 x i16> %340) #9
  %343 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %338, <16 x i16> %322) #9
  %344 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %339, <16 x i16> %322) #9
  %345 = add <8 x i32> %341, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %346 = add <8 x i32> %342, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %347 = add <8 x i32> %343, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %348 = add <8 x i32> %344, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %349 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %345, i32 %284) #9
  %350 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %346, i32 %284) #9
  %351 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %347, i32 %284) #9
  %352 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %348, i32 %284) #9
  %353 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %349, <8 x i32> %350) #9
  %354 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %351, <8 x i32> %352) #9
  %355 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %356 = trunc i32 %355 to i16
  %357 = shl i16 %356, 3
  %358 = insertelement <16 x i16> undef, i16 %357, i32 0
  %359 = shufflevector <16 x i16> %358, <16 x i16> undef, <16 x i32> zeroinitializer
  %360 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %5, <16 x i16> %359) #9
  %361 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 48), align 16
  %362 = trunc i32 %361 to i16
  %363 = shl i16 %362, 3
  %364 = insertelement <16 x i16> undef, i16 %363, i32 0
  %365 = shufflevector <16 x i16> %364, <16 x i16> undef, <16 x i32> zeroinitializer
  %366 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 16), align 16
  %367 = trunc i32 %366 to i16
  %368 = shl i16 %367, 3
  %369 = insertelement <16 x i16> undef, i16 %368, i32 0
  %370 = shufflevector <16 x i16> %369, <16 x i16> undef, <16 x i32> zeroinitializer
  %371 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %8, <16 x i16> %365) #9
  %372 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %8, <16 x i16> %370) #9
  %373 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %227, <16 x i16> %240) #9
  %374 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %227, <16 x i16> %240) #9
  %375 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %228, <16 x i16> %241) #9
  %376 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %228, <16 x i16> %241) #9
  %377 = sub i32 0, %366
  %378 = and i32 %377, 65535
  %379 = and i32 %361, 65535
  %380 = shl nuw i32 %379, 16
  %381 = or i32 %380, %378
  %382 = insertelement <8 x i32> undef, i32 %381, i32 0
  %383 = shufflevector <8 x i32> %382, <8 x i32> undef, <8 x i32> zeroinitializer
  %384 = shl i32 %366, 16
  %385 = or i32 %384, %379
  %386 = insertelement <8 x i32> undef, i32 %385, i32 0
  %387 = shufflevector <8 x i32> %386, <8 x i32> undef, <8 x i32> zeroinitializer
  %388 = sub i32 0, %361
  %389 = and i32 %388, 65535
  %390 = shl nuw i32 %378, 16
  %391 = or i32 %390, %389
  %392 = insertelement <8 x i32> undef, i32 %391, i32 0
  %393 = shufflevector <8 x i32> %392, <8 x i32> undef, <8 x i32> zeroinitializer
  %394 = shufflevector <16 x i16> %243, <16 x i16> %249, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %395 = shufflevector <16 x i16> %243, <16 x i16> %249, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %396 = bitcast <8 x i32> %383 to <16 x i16>
  %397 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %394, <16 x i16> %396) #9
  %398 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %395, <16 x i16> %396) #9
  %399 = bitcast <8 x i32> %387 to <16 x i16>
  %400 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %394, <16 x i16> %399) #9
  %401 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %395, <16 x i16> %399) #9
  %402 = add <8 x i32> %397, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %403 = add <8 x i32> %398, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %404 = add <8 x i32> %400, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %405 = add <8 x i32> %401, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %406 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %402, i32 %284) #9
  %407 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %403, i32 %284) #9
  %408 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %404, i32 %284) #9
  %409 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %405, i32 %284) #9
  %410 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %406, <8 x i32> %407) #9
  %411 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %408, <8 x i32> %409) #9
  %412 = shufflevector <16 x i16> %245, <16 x i16> %247, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %413 = shufflevector <16 x i16> %245, <16 x i16> %247, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %414 = bitcast <8 x i32> %393 to <16 x i16>
  %415 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %412, <16 x i16> %414) #9
  %416 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %413, <16 x i16> %414) #9
  %417 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %412, <16 x i16> %396) #9
  %418 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %413, <16 x i16> %396) #9
  %419 = add <8 x i32> %415, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %420 = add <8 x i32> %416, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %421 = add <8 x i32> %417, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %422 = add <8 x i32> %418, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %423 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %419, i32 %284) #9
  %424 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %420, i32 %284) #9
  %425 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %421, i32 %284) #9
  %426 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %422, i32 %284) #9
  %427 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %423, <8 x i32> %424) #9
  %428 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %425, <8 x i32> %426) #9
  %429 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %201, <16 x i16> %203) #9
  %430 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %201, <16 x i16> %203) #9
  %431 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %301, <16 x i16> %318) #9
  %432 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %301, <16 x i16> %318) #9
  %433 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %207, <16 x i16> %205) #9
  %434 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %207, <16 x i16> %205) #9
  %435 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %353, <16 x i16> %336) #9
  %436 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %353, <16 x i16> %336) #9
  %437 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %209, <16 x i16> %211) #9
  %438 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %209, <16 x i16> %211) #9
  %439 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %354, <16 x i16> %337) #9
  %440 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %354, <16 x i16> %337) #9
  %441 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %215, <16 x i16> %213) #9
  %442 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %215, <16 x i16> %213) #9
  %443 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %302, <16 x i16> %319) #9
  %444 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %302, <16 x i16> %319) #9
  %445 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %360, <16 x i16> %372) #9
  %446 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %360, <16 x i16> %372) #9
  %447 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %360, <16 x i16> %371) #9
  %448 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %360, <16 x i16> %371) #9
  %449 = sub i32 0, %355
  %450 = and i32 %449, 65535
  %451 = and i32 %355, 65535
  %452 = shl nuw i32 %451, 16
  %453 = or i32 %452, %450
  %454 = insertelement <8 x i32> undef, i32 %453, i32 0
  %455 = shufflevector <8 x i32> %454, <8 x i32> undef, <8 x i32> zeroinitializer
  %456 = or i32 %452, %451
  %457 = insertelement <8 x i32> undef, i32 %456, i32 0
  %458 = shufflevector <8 x i32> %457, <8 x i32> undef, <8 x i32> zeroinitializer
  %459 = shufflevector <16 x i16> %374, <16 x i16> %376, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %460 = shufflevector <16 x i16> %374, <16 x i16> %376, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %461 = bitcast <8 x i32> %455 to <16 x i16>
  %462 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %459, <16 x i16> %461) #9
  %463 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %460, <16 x i16> %461) #9
  %464 = bitcast <8 x i32> %458 to <16 x i16>
  %465 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %459, <16 x i16> %464) #9
  %466 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %460, <16 x i16> %464) #9
  %467 = add <8 x i32> %462, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %468 = add <8 x i32> %463, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %469 = add <8 x i32> %465, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %470 = add <8 x i32> %466, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %471 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %467, i32 %284) #9
  %472 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %468, i32 %284) #9
  %473 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %469, i32 %284) #9
  %474 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %470, i32 %284) #9
  %475 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %471, <8 x i32> %472) #9
  %476 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %473, <8 x i32> %474) #9
  %477 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %242, <16 x i16> %244) #9
  %478 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %242, <16 x i16> %244) #9
  %479 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %410, <16 x i16> %427) #9
  %480 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %410, <16 x i16> %427) #9
  %481 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %248, <16 x i16> %246) #9
  %482 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %248, <16 x i16> %246) #9
  %483 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %411, <16 x i16> %428) #9
  %484 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %411, <16 x i16> %428) #9
  %485 = shufflevector <16 x i16> %432, <16 x i16> %444, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %486 = shufflevector <16 x i16> %432, <16 x i16> %444, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %487 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %485, <16 x i16> %396) #9
  %488 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %486, <16 x i16> %396) #9
  %489 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %485, <16 x i16> %399) #9
  %490 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %486, <16 x i16> %399) #9
  %491 = add <8 x i32> %487, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %492 = add <8 x i32> %488, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %493 = add <8 x i32> %489, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %494 = add <8 x i32> %490, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %495 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %491, i32 %284) #9
  %496 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %492, i32 %284) #9
  %497 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %493, i32 %284) #9
  %498 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %494, i32 %284) #9
  %499 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %495, <8 x i32> %496) #9
  %500 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %497, <8 x i32> %498) #9
  %501 = shufflevector <16 x i16> %430, <16 x i16> %442, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %502 = shufflevector <16 x i16> %430, <16 x i16> %442, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %503 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %501, <16 x i16> %396) #9
  %504 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %502, <16 x i16> %396) #9
  %505 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %501, <16 x i16> %399) #9
  %506 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %502, <16 x i16> %399) #9
  %507 = add <8 x i32> %503, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %508 = add <8 x i32> %504, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %509 = add <8 x i32> %505, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %510 = add <8 x i32> %506, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %511 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %507, i32 %284) #9
  %512 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %508, i32 %284) #9
  %513 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %509, i32 %284) #9
  %514 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %510, i32 %284) #9
  %515 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %511, <8 x i32> %512) #9
  %516 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %513, <8 x i32> %514) #9
  %517 = shufflevector <16 x i16> %434, <16 x i16> %438, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %518 = shufflevector <16 x i16> %434, <16 x i16> %438, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %519 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %517, <16 x i16> %414) #9
  %520 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %518, <16 x i16> %414) #9
  %521 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %517, <16 x i16> %396) #9
  %522 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %518, <16 x i16> %396) #9
  %523 = add <8 x i32> %519, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %524 = add <8 x i32> %520, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %525 = add <8 x i32> %521, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %526 = add <8 x i32> %522, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %527 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %523, i32 %284) #9
  %528 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %524, i32 %284) #9
  %529 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %525, i32 %284) #9
  %530 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %526, i32 %284) #9
  %531 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %527, <8 x i32> %528) #9
  %532 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %529, <8 x i32> %530) #9
  %533 = shufflevector <16 x i16> %436, <16 x i16> %440, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %534 = shufflevector <16 x i16> %436, <16 x i16> %440, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %535 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %533, <16 x i16> %414) #9
  %536 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %534, <16 x i16> %414) #9
  %537 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %533, <16 x i16> %396) #9
  %538 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %534, <16 x i16> %396) #9
  %539 = add <8 x i32> %535, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %540 = add <8 x i32> %536, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %541 = add <8 x i32> %537, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %542 = add <8 x i32> %538, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %543 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %539, i32 %284) #9
  %544 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %540, i32 %284) #9
  %545 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %541, i32 %284) #9
  %546 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %542, i32 %284) #9
  %547 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %543, <8 x i32> %544) #9
  %548 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %545, <8 x i32> %546) #9
  %549 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %445, <16 x i16> %375) #9
  %550 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %445, <16 x i16> %375) #9
  %551 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %447, <16 x i16> %476) #9
  %552 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %447, <16 x i16> %476) #9
  %553 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %448, <16 x i16> %475) #9
  %554 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %448, <16 x i16> %475) #9
  %555 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %446, <16 x i16> %373) #9
  %556 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %446, <16 x i16> %373) #9
  %557 = shufflevector <16 x i16> %480, <16 x i16> %484, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %558 = shufflevector <16 x i16> %480, <16 x i16> %484, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %559 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %557, <16 x i16> %461) #9
  %560 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %558, <16 x i16> %461) #9
  %561 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %557, <16 x i16> %464) #9
  %562 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %558, <16 x i16> %464) #9
  %563 = add <8 x i32> %559, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %564 = add <8 x i32> %560, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %565 = add <8 x i32> %561, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %566 = add <8 x i32> %562, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %567 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %563, i32 %284) #9
  %568 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %564, i32 %284) #9
  %569 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %565, i32 %284) #9
  %570 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %566, i32 %284) #9
  %571 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %567, <8 x i32> %568) #9
  %572 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %569, <8 x i32> %570) #9
  %573 = shufflevector <16 x i16> %478, <16 x i16> %482, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %574 = shufflevector <16 x i16> %478, <16 x i16> %482, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %575 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %573, <16 x i16> %461) #9
  %576 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %574, <16 x i16> %461) #9
  %577 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %573, <16 x i16> %464) #9
  %578 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %574, <16 x i16> %464) #9
  %579 = add <8 x i32> %575, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %580 = add <8 x i32> %576, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %581 = add <8 x i32> %577, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %582 = add <8 x i32> %578, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %583 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %579, i32 %284) #9
  %584 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %580, i32 %284) #9
  %585 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %581, i32 %284) #9
  %586 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %582, i32 %284) #9
  %587 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %583, <8 x i32> %584) #9
  %588 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %585, <8 x i32> %586) #9
  %589 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %429, <16 x i16> %433) #9
  %590 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %429, <16 x i16> %433) #9
  %591 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %431, <16 x i16> %435) #9
  %592 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %431, <16 x i16> %435) #9
  %593 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %499, <16 x i16> %547) #9
  %594 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %499, <16 x i16> %547) #9
  %595 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %515, <16 x i16> %531) #9
  %596 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %515, <16 x i16> %531) #9
  %597 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %441, <16 x i16> %437) #9
  %598 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %441, <16 x i16> %437) #9
  %599 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %443, <16 x i16> %439) #9
  %600 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %443, <16 x i16> %439) #9
  %601 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %500, <16 x i16> %548) #9
  %602 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %500, <16 x i16> %548) #9
  %603 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %516, <16 x i16> %532) #9
  %604 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %516, <16 x i16> %532) #9
  %605 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %549, <16 x i16> %481) #9
  %606 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %549, <16 x i16> %481) #9
  %607 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %551, <16 x i16> %483) #9
  %608 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %551, <16 x i16> %483) #9
  %609 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %553, <16 x i16> %572) #9
  %610 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %553, <16 x i16> %572) #9
  %611 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %555, <16 x i16> %588) #9
  %612 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %555, <16 x i16> %588) #9
  %613 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %556, <16 x i16> %587) #9
  %614 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %556, <16 x i16> %587) #9
  %615 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %554, <16 x i16> %571) #9
  %616 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %554, <16 x i16> %571) #9
  %617 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %552, <16 x i16> %479) #9
  %618 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %552, <16 x i16> %479) #9
  %619 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %550, <16 x i16> %477) #9
  %620 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %550, <16 x i16> %477) #9
  %621 = shufflevector <16 x i16> %596, <16 x i16> %604, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %622 = shufflevector <16 x i16> %596, <16 x i16> %604, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %623 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %621, <16 x i16> %461) #9
  %624 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %622, <16 x i16> %461) #9
  %625 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %621, <16 x i16> %464) #9
  %626 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %622, <16 x i16> %464) #9
  %627 = add <8 x i32> %623, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %628 = add <8 x i32> %624, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %629 = add <8 x i32> %625, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %630 = add <8 x i32> %626, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %631 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %627, i32 %284) #9
  %632 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %628, i32 %284) #9
  %633 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %629, i32 %284) #9
  %634 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %630, i32 %284) #9
  %635 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %631, <8 x i32> %632) #9
  %636 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %633, <8 x i32> %634) #9
  %637 = shufflevector <16 x i16> %594, <16 x i16> %602, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %638 = shufflevector <16 x i16> %594, <16 x i16> %602, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %639 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %637, <16 x i16> %461) #9
  %640 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %638, <16 x i16> %461) #9
  %641 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %637, <16 x i16> %464) #9
  %642 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %638, <16 x i16> %464) #9
  %643 = add <8 x i32> %639, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %644 = add <8 x i32> %640, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %645 = add <8 x i32> %641, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %646 = add <8 x i32> %642, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %647 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %643, i32 %284) #9
  %648 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %644, i32 %284) #9
  %649 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %645, i32 %284) #9
  %650 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %646, i32 %284) #9
  %651 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %647, <8 x i32> %648) #9
  %652 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %649, <8 x i32> %650) #9
  %653 = shufflevector <16 x i16> %592, <16 x i16> %600, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %654 = shufflevector <16 x i16> %592, <16 x i16> %600, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %655 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %653, <16 x i16> %461) #9
  %656 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %654, <16 x i16> %461) #9
  %657 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %653, <16 x i16> %464) #9
  %658 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %654, <16 x i16> %464) #9
  %659 = add <8 x i32> %655, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %660 = add <8 x i32> %656, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %661 = add <8 x i32> %657, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %662 = add <8 x i32> %658, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %663 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %659, i32 %284) #9
  %664 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %660, i32 %284) #9
  %665 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %661, i32 %284) #9
  %666 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %662, i32 %284) #9
  %667 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %663, <8 x i32> %664) #9
  %668 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %665, <8 x i32> %666) #9
  %669 = shufflevector <16 x i16> %590, <16 x i16> %598, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %670 = shufflevector <16 x i16> %590, <16 x i16> %598, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %671 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %669, <16 x i16> %461) #9
  %672 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %670, <16 x i16> %461) #9
  %673 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %669, <16 x i16> %464) #9
  %674 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %670, <16 x i16> %464) #9
  %675 = add <8 x i32> %671, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %676 = add <8 x i32> %672, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %677 = add <8 x i32> %673, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %678 = add <8 x i32> %674, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %679 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %675, i32 %284) #9
  %680 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %676, i32 %284) #9
  %681 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %677, i32 %284) #9
  %682 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %678, i32 %284) #9
  %683 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %679, <8 x i32> %680) #9
  %684 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %681, <8 x i32> %682) #9
  %685 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 31
  %686 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %605, <16 x i16> %597) #9
  %687 = bitcast <4 x i64>* %1 to <16 x i16>*
  store <16 x i16> %686, <16 x i16>* %687, align 32
  %688 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %605, <16 x i16> %597) #9
  %689 = bitcast <4 x i64>* %685 to <16 x i16>*
  store <16 x i16> %688, <16 x i16>* %689, align 32
  %690 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %691 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 30
  %692 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %607, <16 x i16> %599) #9
  %693 = bitcast <4 x i64>* %690 to <16 x i16>*
  store <16 x i16> %692, <16 x i16>* %693, align 32
  %694 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %607, <16 x i16> %599) #9
  %695 = bitcast <4 x i64>* %691 to <16 x i16>*
  store <16 x i16> %694, <16 x i16>* %695, align 32
  %696 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %697 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 29
  %698 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %609, <16 x i16> %601) #9
  %699 = bitcast <4 x i64>* %696 to <16 x i16>*
  store <16 x i16> %698, <16 x i16>* %699, align 32
  %700 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %609, <16 x i16> %601) #9
  %701 = bitcast <4 x i64>* %697 to <16 x i16>*
  store <16 x i16> %700, <16 x i16>* %701, align 32
  %702 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %703 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 28
  %704 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %611, <16 x i16> %603) #9
  %705 = bitcast <4 x i64>* %702 to <16 x i16>*
  store <16 x i16> %704, <16 x i16>* %705, align 32
  %706 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %611, <16 x i16> %603) #9
  %707 = bitcast <4 x i64>* %703 to <16 x i16>*
  store <16 x i16> %706, <16 x i16>* %707, align 32
  %708 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %709 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 27
  %710 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %613, <16 x i16> %636) #9
  %711 = bitcast <4 x i64>* %708 to <16 x i16>*
  store <16 x i16> %710, <16 x i16>* %711, align 32
  %712 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %613, <16 x i16> %636) #9
  %713 = bitcast <4 x i64>* %709 to <16 x i16>*
  store <16 x i16> %712, <16 x i16>* %713, align 32
  %714 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %715 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 26
  %716 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %615, <16 x i16> %652) #9
  %717 = bitcast <4 x i64>* %714 to <16 x i16>*
  store <16 x i16> %716, <16 x i16>* %717, align 32
  %718 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %615, <16 x i16> %652) #9
  %719 = bitcast <4 x i64>* %715 to <16 x i16>*
  store <16 x i16> %718, <16 x i16>* %719, align 32
  %720 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %721 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 25
  %722 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %617, <16 x i16> %668) #9
  %723 = bitcast <4 x i64>* %720 to <16 x i16>*
  store <16 x i16> %722, <16 x i16>* %723, align 32
  %724 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %617, <16 x i16> %668) #9
  %725 = bitcast <4 x i64>* %721 to <16 x i16>*
  store <16 x i16> %724, <16 x i16>* %725, align 32
  %726 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %727 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 24
  %728 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %619, <16 x i16> %684) #9
  %729 = bitcast <4 x i64>* %726 to <16 x i16>*
  store <16 x i16> %728, <16 x i16>* %729, align 32
  %730 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %619, <16 x i16> %684) #9
  %731 = bitcast <4 x i64>* %727 to <16 x i16>*
  store <16 x i16> %730, <16 x i16>* %731, align 32
  %732 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %733 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 23
  %734 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %620, <16 x i16> %683) #9
  %735 = bitcast <4 x i64>* %732 to <16 x i16>*
  store <16 x i16> %734, <16 x i16>* %735, align 32
  %736 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %620, <16 x i16> %683) #9
  %737 = bitcast <4 x i64>* %733 to <16 x i16>*
  store <16 x i16> %736, <16 x i16>* %737, align 32
  %738 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %739 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 22
  %740 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %618, <16 x i16> %667) #9
  %741 = bitcast <4 x i64>* %738 to <16 x i16>*
  store <16 x i16> %740, <16 x i16>* %741, align 32
  %742 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %618, <16 x i16> %667) #9
  %743 = bitcast <4 x i64>* %739 to <16 x i16>*
  store <16 x i16> %742, <16 x i16>* %743, align 32
  %744 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %745 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 21
  %746 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %616, <16 x i16> %651) #9
  %747 = bitcast <4 x i64>* %744 to <16 x i16>*
  store <16 x i16> %746, <16 x i16>* %747, align 32
  %748 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %616, <16 x i16> %651) #9
  %749 = bitcast <4 x i64>* %745 to <16 x i16>*
  store <16 x i16> %748, <16 x i16>* %749, align 32
  %750 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %751 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 20
  %752 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %614, <16 x i16> %635) #9
  %753 = bitcast <4 x i64>* %750 to <16 x i16>*
  store <16 x i16> %752, <16 x i16>* %753, align 32
  %754 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %614, <16 x i16> %635) #9
  %755 = bitcast <4 x i64>* %751 to <16 x i16>*
  store <16 x i16> %754, <16 x i16>* %755, align 32
  %756 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %757 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 19
  %758 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %612, <16 x i16> %595) #9
  %759 = bitcast <4 x i64>* %756 to <16 x i16>*
  store <16 x i16> %758, <16 x i16>* %759, align 32
  %760 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %612, <16 x i16> %595) #9
  %761 = bitcast <4 x i64>* %757 to <16 x i16>*
  store <16 x i16> %760, <16 x i16>* %761, align 32
  %762 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %763 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 18
  %764 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %610, <16 x i16> %593) #9
  %765 = bitcast <4 x i64>* %762 to <16 x i16>*
  store <16 x i16> %764, <16 x i16>* %765, align 32
  %766 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %610, <16 x i16> %593) #9
  %767 = bitcast <4 x i64>* %763 to <16 x i16>*
  store <16 x i16> %766, <16 x i16>* %767, align 32
  %768 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %769 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 17
  %770 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %608, <16 x i16> %591) #9
  %771 = bitcast <4 x i64>* %768 to <16 x i16>*
  store <16 x i16> %770, <16 x i16>* %771, align 32
  %772 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %608, <16 x i16> %591) #9
  %773 = bitcast <4 x i64>* %769 to <16 x i16>*
  store <16 x i16> %772, <16 x i16>* %773, align 32
  %774 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 15
  %775 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 16
  %776 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %606, <16 x i16> %589) #9
  %777 = bitcast <4 x i64>* %774 to <16 x i16>*
  store <16 x i16> %776, <16 x i16>* %777, align 32
  %778 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %606, <16 x i16> %589) #9
  %779 = bitcast <4 x i64>* %775 to <16 x i16>*
  store <16 x i16> %778, <16 x i16>* %779, align 32
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @idct32_avx2(<4 x i64>* readonly, <4 x i64>*, i8 signext) #0 {
  %4 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 62), align 8
  %5 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 2), align 8
  %6 = and i32 %4, 65535
  %7 = shl i32 %5, 16
  %8 = sub i32 0, %7
  %9 = or i32 %6, %8
  %10 = insertelement <8 x i32> undef, i32 %9, i32 0
  %11 = shufflevector <8 x i32> %10, <8 x i32> undef, <8 x i32> zeroinitializer
  %12 = and i32 %5, 65535
  %13 = shl nuw i32 %6, 16
  %14 = or i32 %13, %12
  %15 = insertelement <8 x i32> undef, i32 %14, i32 0
  %16 = shufflevector <8 x i32> %15, <8 x i32> undef, <8 x i32> zeroinitializer
  %17 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 30), align 8
  %18 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 34), align 8
  %19 = and i32 %17, 65535
  %20 = shl i32 %18, 16
  %21 = sub i32 0, %20
  %22 = or i32 %19, %21
  %23 = insertelement <8 x i32> undef, i32 %22, i32 0
  %24 = shufflevector <8 x i32> %23, <8 x i32> undef, <8 x i32> zeroinitializer
  %25 = and i32 %18, 65535
  %26 = shl nuw i32 %19, 16
  %27 = or i32 %26, %25
  %28 = insertelement <8 x i32> undef, i32 %27, i32 0
  %29 = shufflevector <8 x i32> %28, <8 x i32> undef, <8 x i32> zeroinitializer
  %30 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 46), align 8
  %31 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 18), align 8
  %32 = and i32 %30, 65535
  %33 = shl i32 %31, 16
  %34 = sub i32 0, %33
  %35 = or i32 %32, %34
  %36 = insertelement <8 x i32> undef, i32 %35, i32 0
  %37 = shufflevector <8 x i32> %36, <8 x i32> undef, <8 x i32> zeroinitializer
  %38 = and i32 %31, 65535
  %39 = shl nuw i32 %32, 16
  %40 = or i32 %39, %38
  %41 = insertelement <8 x i32> undef, i32 %40, i32 0
  %42 = shufflevector <8 x i32> %41, <8 x i32> undef, <8 x i32> zeroinitializer
  %43 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 14), align 8
  %44 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 50), align 8
  %45 = and i32 %43, 65535
  %46 = shl i32 %44, 16
  %47 = sub i32 0, %46
  %48 = or i32 %45, %47
  %49 = insertelement <8 x i32> undef, i32 %48, i32 0
  %50 = shufflevector <8 x i32> %49, <8 x i32> undef, <8 x i32> zeroinitializer
  %51 = and i32 %44, 65535
  %52 = shl nuw i32 %45, 16
  %53 = or i32 %52, %51
  %54 = insertelement <8 x i32> undef, i32 %53, i32 0
  %55 = shufflevector <8 x i32> %54, <8 x i32> undef, <8 x i32> zeroinitializer
  %56 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 54), align 8
  %57 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 10), align 8
  %58 = and i32 %56, 65535
  %59 = shl i32 %57, 16
  %60 = sub i32 0, %59
  %61 = or i32 %58, %60
  %62 = insertelement <8 x i32> undef, i32 %61, i32 0
  %63 = shufflevector <8 x i32> %62, <8 x i32> undef, <8 x i32> zeroinitializer
  %64 = and i32 %57, 65535
  %65 = shl nuw i32 %58, 16
  %66 = or i32 %65, %64
  %67 = insertelement <8 x i32> undef, i32 %66, i32 0
  %68 = shufflevector <8 x i32> %67, <8 x i32> undef, <8 x i32> zeroinitializer
  %69 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 22), align 8
  %70 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 42), align 8
  %71 = and i32 %69, 65535
  %72 = shl i32 %70, 16
  %73 = sub i32 0, %72
  %74 = or i32 %71, %73
  %75 = insertelement <8 x i32> undef, i32 %74, i32 0
  %76 = shufflevector <8 x i32> %75, <8 x i32> undef, <8 x i32> zeroinitializer
  %77 = and i32 %70, 65535
  %78 = shl nuw i32 %71, 16
  %79 = or i32 %78, %77
  %80 = insertelement <8 x i32> undef, i32 %79, i32 0
  %81 = shufflevector <8 x i32> %80, <8 x i32> undef, <8 x i32> zeroinitializer
  %82 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 38), align 8
  %83 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 26), align 8
  %84 = and i32 %82, 65535
  %85 = shl i32 %83, 16
  %86 = sub i32 0, %85
  %87 = or i32 %84, %86
  %88 = insertelement <8 x i32> undef, i32 %87, i32 0
  %89 = shufflevector <8 x i32> %88, <8 x i32> undef, <8 x i32> zeroinitializer
  %90 = and i32 %83, 65535
  %91 = shl nuw i32 %84, 16
  %92 = or i32 %91, %90
  %93 = insertelement <8 x i32> undef, i32 %92, i32 0
  %94 = shufflevector <8 x i32> %93, <8 x i32> undef, <8 x i32> zeroinitializer
  %95 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 6), align 8
  %96 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 58), align 8
  %97 = and i32 %95, 65535
  %98 = shl i32 %96, 16
  %99 = sub i32 0, %98
  %100 = or i32 %97, %99
  %101 = insertelement <8 x i32> undef, i32 %100, i32 0
  %102 = shufflevector <8 x i32> %101, <8 x i32> undef, <8 x i32> zeroinitializer
  %103 = and i32 %96, 65535
  %104 = shl nuw i32 %97, 16
  %105 = or i32 %104, %103
  %106 = insertelement <8 x i32> undef, i32 %105, i32 0
  %107 = shufflevector <8 x i32> %106, <8 x i32> undef, <8 x i32> zeroinitializer
  %108 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 60), align 16
  %109 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 4), align 16
  %110 = and i32 %108, 65535
  %111 = shl i32 %109, 16
  %112 = sub i32 0, %111
  %113 = or i32 %110, %112
  %114 = insertelement <8 x i32> undef, i32 %113, i32 0
  %115 = shufflevector <8 x i32> %114, <8 x i32> undef, <8 x i32> zeroinitializer
  %116 = and i32 %109, 65535
  %117 = shl nuw i32 %110, 16
  %118 = or i32 %117, %116
  %119 = insertelement <8 x i32> undef, i32 %118, i32 0
  %120 = shufflevector <8 x i32> %119, <8 x i32> undef, <8 x i32> zeroinitializer
  %121 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 28), align 16
  %122 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 36), align 16
  %123 = and i32 %121, 65535
  %124 = shl i32 %122, 16
  %125 = sub i32 0, %124
  %126 = or i32 %123, %125
  %127 = insertelement <8 x i32> undef, i32 %126, i32 0
  %128 = shufflevector <8 x i32> %127, <8 x i32> undef, <8 x i32> zeroinitializer
  %129 = and i32 %122, 65535
  %130 = shl nuw i32 %123, 16
  %131 = or i32 %130, %129
  %132 = insertelement <8 x i32> undef, i32 %131, i32 0
  %133 = shufflevector <8 x i32> %132, <8 x i32> undef, <8 x i32> zeroinitializer
  %134 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 44), align 16
  %135 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 20), align 16
  %136 = and i32 %134, 65535
  %137 = shl i32 %135, 16
  %138 = sub i32 0, %137
  %139 = or i32 %136, %138
  %140 = insertelement <8 x i32> undef, i32 %139, i32 0
  %141 = shufflevector <8 x i32> %140, <8 x i32> undef, <8 x i32> zeroinitializer
  %142 = and i32 %135, 65535
  %143 = shl nuw i32 %136, 16
  %144 = or i32 %143, %142
  %145 = insertelement <8 x i32> undef, i32 %144, i32 0
  %146 = shufflevector <8 x i32> %145, <8 x i32> undef, <8 x i32> zeroinitializer
  %147 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 12), align 16
  %148 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 52), align 16
  %149 = and i32 %147, 65535
  %150 = shl i32 %148, 16
  %151 = sub i32 0, %150
  %152 = or i32 %149, %151
  %153 = insertelement <8 x i32> undef, i32 %152, i32 0
  %154 = shufflevector <8 x i32> %153, <8 x i32> undef, <8 x i32> zeroinitializer
  %155 = and i32 %148, 65535
  %156 = shl nuw i32 %149, 16
  %157 = or i32 %156, %155
  %158 = insertelement <8 x i32> undef, i32 %157, i32 0
  %159 = shufflevector <8 x i32> %158, <8 x i32> undef, <8 x i32> zeroinitializer
  %160 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 56), align 16
  %161 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 8), align 16
  %162 = and i32 %160, 65535
  %163 = shl i32 %161, 16
  %164 = sub i32 0, %163
  %165 = or i32 %162, %164
  %166 = insertelement <8 x i32> undef, i32 %165, i32 0
  %167 = shufflevector <8 x i32> %166, <8 x i32> undef, <8 x i32> zeroinitializer
  %168 = and i32 %161, 65535
  %169 = shl nuw i32 %162, 16
  %170 = or i32 %169, %168
  %171 = insertelement <8 x i32> undef, i32 %170, i32 0
  %172 = shufflevector <8 x i32> %171, <8 x i32> undef, <8 x i32> zeroinitializer
  %173 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 24), align 16
  %174 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 40), align 16
  %175 = and i32 %173, 65535
  %176 = shl i32 %174, 16
  %177 = sub i32 0, %176
  %178 = or i32 %175, %177
  %179 = insertelement <8 x i32> undef, i32 %178, i32 0
  %180 = shufflevector <8 x i32> %179, <8 x i32> undef, <8 x i32> zeroinitializer
  %181 = and i32 %174, 65535
  %182 = shl nuw i32 %175, 16
  %183 = or i32 %182, %181
  %184 = insertelement <8 x i32> undef, i32 %183, i32 0
  %185 = shufflevector <8 x i32> %184, <8 x i32> undef, <8 x i32> zeroinitializer
  %186 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %187 = and i32 %186, 65535
  %188 = shl nuw i32 %187, 16
  %189 = or i32 %188, %187
  %190 = insertelement <8 x i32> undef, i32 %189, i32 0
  %191 = shufflevector <8 x i32> %190, <8 x i32> undef, <8 x i32> zeroinitializer
  %192 = shl i32 %186, 16
  %193 = sub i32 0, %192
  %194 = or i32 %187, %193
  %195 = insertelement <8 x i32> undef, i32 %194, i32 0
  %196 = shufflevector <8 x i32> %195, <8 x i32> undef, <8 x i32> zeroinitializer
  %197 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 48), align 16
  %198 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 16), align 16
  %199 = and i32 %197, 65535
  %200 = shl i32 %198, 16
  %201 = sub i32 0, %200
  %202 = or i32 %199, %201
  %203 = insertelement <8 x i32> undef, i32 %202, i32 0
  %204 = shufflevector <8 x i32> %203, <8 x i32> undef, <8 x i32> zeroinitializer
  %205 = and i32 %198, 65535
  %206 = shl nuw i32 %199, 16
  %207 = or i32 %206, %205
  %208 = insertelement <8 x i32> undef, i32 %207, i32 0
  %209 = shufflevector <8 x i32> %208, <8 x i32> undef, <8 x i32> zeroinitializer
  %210 = bitcast <4 x i64>* %0 to <16 x i16>*
  %211 = load <16 x i16>, <16 x i16>* %210, align 32
  %212 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 16
  %213 = bitcast <4 x i64>* %212 to <16 x i16>*
  %214 = load <16 x i16>, <16 x i16>* %213, align 32
  %215 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 8
  %216 = bitcast <4 x i64>* %215 to <16 x i16>*
  %217 = load <16 x i16>, <16 x i16>* %216, align 32
  %218 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 24
  %219 = bitcast <4 x i64>* %218 to <16 x i16>*
  %220 = load <16 x i16>, <16 x i16>* %219, align 32
  %221 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 4
  %222 = bitcast <4 x i64>* %221 to <16 x i16>*
  %223 = load <16 x i16>, <16 x i16>* %222, align 32
  %224 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 20
  %225 = bitcast <4 x i64>* %224 to <16 x i16>*
  %226 = load <16 x i16>, <16 x i16>* %225, align 32
  %227 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 12
  %228 = bitcast <4 x i64>* %227 to <16 x i16>*
  %229 = load <16 x i16>, <16 x i16>* %228, align 32
  %230 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 28
  %231 = bitcast <4 x i64>* %230 to <16 x i16>*
  %232 = load <16 x i16>, <16 x i16>* %231, align 32
  %233 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 2
  %234 = bitcast <4 x i64>* %233 to <16 x i16>*
  %235 = load <16 x i16>, <16 x i16>* %234, align 32
  %236 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 18
  %237 = bitcast <4 x i64>* %236 to <16 x i16>*
  %238 = load <16 x i16>, <16 x i16>* %237, align 32
  %239 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 10
  %240 = bitcast <4 x i64>* %239 to <16 x i16>*
  %241 = load <16 x i16>, <16 x i16>* %240, align 32
  %242 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 26
  %243 = bitcast <4 x i64>* %242 to <16 x i16>*
  %244 = load <16 x i16>, <16 x i16>* %243, align 32
  %245 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 6
  %246 = bitcast <4 x i64>* %245 to <16 x i16>*
  %247 = load <16 x i16>, <16 x i16>* %246, align 32
  %248 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 22
  %249 = bitcast <4 x i64>* %248 to <16 x i16>*
  %250 = load <16 x i16>, <16 x i16>* %249, align 32
  %251 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 14
  %252 = bitcast <4 x i64>* %251 to <16 x i16>*
  %253 = load <16 x i16>, <16 x i16>* %252, align 32
  %254 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 30
  %255 = bitcast <4 x i64>* %254 to <16 x i16>*
  %256 = load <16 x i16>, <16 x i16>* %255, align 32
  %257 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 1
  %258 = bitcast <4 x i64>* %257 to <16 x i16>*
  %259 = load <16 x i16>, <16 x i16>* %258, align 32
  %260 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 17
  %261 = bitcast <4 x i64>* %260 to <16 x i16>*
  %262 = load <16 x i16>, <16 x i16>* %261, align 32
  %263 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 9
  %264 = bitcast <4 x i64>* %263 to <16 x i16>*
  %265 = load <16 x i16>, <16 x i16>* %264, align 32
  %266 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 25
  %267 = bitcast <4 x i64>* %266 to <16 x i16>*
  %268 = load <16 x i16>, <16 x i16>* %267, align 32
  %269 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 5
  %270 = bitcast <4 x i64>* %269 to <16 x i16>*
  %271 = load <16 x i16>, <16 x i16>* %270, align 32
  %272 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 21
  %273 = bitcast <4 x i64>* %272 to <16 x i16>*
  %274 = load <16 x i16>, <16 x i16>* %273, align 32
  %275 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 13
  %276 = bitcast <4 x i64>* %275 to <16 x i16>*
  %277 = load <16 x i16>, <16 x i16>* %276, align 32
  %278 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 29
  %279 = bitcast <4 x i64>* %278 to <16 x i16>*
  %280 = load <16 x i16>, <16 x i16>* %279, align 32
  %281 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 3
  %282 = bitcast <4 x i64>* %281 to <16 x i16>*
  %283 = load <16 x i16>, <16 x i16>* %282, align 32
  %284 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 19
  %285 = bitcast <4 x i64>* %284 to <16 x i16>*
  %286 = load <16 x i16>, <16 x i16>* %285, align 32
  %287 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 11
  %288 = bitcast <4 x i64>* %287 to <16 x i16>*
  %289 = load <16 x i16>, <16 x i16>* %288, align 32
  %290 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 27
  %291 = bitcast <4 x i64>* %290 to <16 x i16>*
  %292 = load <16 x i16>, <16 x i16>* %291, align 32
  %293 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 7
  %294 = bitcast <4 x i64>* %293 to <16 x i16>*
  %295 = load <16 x i16>, <16 x i16>* %294, align 32
  %296 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 23
  %297 = bitcast <4 x i64>* %296 to <16 x i16>*
  %298 = load <16 x i16>, <16 x i16>* %297, align 32
  %299 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 15
  %300 = bitcast <4 x i64>* %299 to <16 x i16>*
  %301 = load <16 x i16>, <16 x i16>* %300, align 32
  %302 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 31
  %303 = bitcast <4 x i64>* %302 to <16 x i16>*
  %304 = load <16 x i16>, <16 x i16>* %303, align 32
  %305 = sext i8 %2 to i32
  %306 = shufflevector <16 x i16> %259, <16 x i16> %304, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %307 = shufflevector <16 x i16> %259, <16 x i16> %304, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %308 = bitcast <8 x i32> %11 to <16 x i16>
  %309 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %306, <16 x i16> %308) #9
  %310 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %307, <16 x i16> %308) #9
  %311 = bitcast <8 x i32> %16 to <16 x i16>
  %312 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %306, <16 x i16> %311) #9
  %313 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %307, <16 x i16> %311) #9
  %314 = add <8 x i32> %309, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %315 = add <8 x i32> %310, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %316 = add <8 x i32> %312, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %317 = add <8 x i32> %313, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %318 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %314, i32 %305) #9
  %319 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %315, i32 %305) #9
  %320 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %316, i32 %305) #9
  %321 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %317, i32 %305) #9
  %322 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %318, <8 x i32> %319) #9
  %323 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %320, <8 x i32> %321) #9
  %324 = shufflevector <16 x i16> %262, <16 x i16> %301, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %325 = shufflevector <16 x i16> %262, <16 x i16> %301, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %326 = bitcast <8 x i32> %24 to <16 x i16>
  %327 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %324, <16 x i16> %326) #9
  %328 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %325, <16 x i16> %326) #9
  %329 = bitcast <8 x i32> %29 to <16 x i16>
  %330 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %324, <16 x i16> %329) #9
  %331 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %325, <16 x i16> %329) #9
  %332 = add <8 x i32> %327, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %333 = add <8 x i32> %328, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %334 = add <8 x i32> %330, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %335 = add <8 x i32> %331, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %336 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %332, i32 %305) #9
  %337 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %333, i32 %305) #9
  %338 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %334, i32 %305) #9
  %339 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %335, i32 %305) #9
  %340 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %336, <8 x i32> %337) #9
  %341 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %338, <8 x i32> %339) #9
  %342 = shufflevector <16 x i16> %265, <16 x i16> %298, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %343 = shufflevector <16 x i16> %265, <16 x i16> %298, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %344 = bitcast <8 x i32> %37 to <16 x i16>
  %345 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %342, <16 x i16> %344) #9
  %346 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %343, <16 x i16> %344) #9
  %347 = bitcast <8 x i32> %42 to <16 x i16>
  %348 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %342, <16 x i16> %347) #9
  %349 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %343, <16 x i16> %347) #9
  %350 = add <8 x i32> %345, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %351 = add <8 x i32> %346, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %352 = add <8 x i32> %348, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %353 = add <8 x i32> %349, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %354 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %350, i32 %305) #9
  %355 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %351, i32 %305) #9
  %356 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %352, i32 %305) #9
  %357 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %353, i32 %305) #9
  %358 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %354, <8 x i32> %355) #9
  %359 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %356, <8 x i32> %357) #9
  %360 = shufflevector <16 x i16> %268, <16 x i16> %295, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %361 = shufflevector <16 x i16> %268, <16 x i16> %295, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %362 = bitcast <8 x i32> %50 to <16 x i16>
  %363 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %360, <16 x i16> %362) #9
  %364 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %361, <16 x i16> %362) #9
  %365 = bitcast <8 x i32> %55 to <16 x i16>
  %366 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %360, <16 x i16> %365) #9
  %367 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %361, <16 x i16> %365) #9
  %368 = add <8 x i32> %363, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %369 = add <8 x i32> %364, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %370 = add <8 x i32> %366, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %371 = add <8 x i32> %367, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %372 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %368, i32 %305) #9
  %373 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %369, i32 %305) #9
  %374 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %370, i32 %305) #9
  %375 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %371, i32 %305) #9
  %376 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %372, <8 x i32> %373) #9
  %377 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %374, <8 x i32> %375) #9
  %378 = shufflevector <16 x i16> %271, <16 x i16> %292, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %379 = shufflevector <16 x i16> %271, <16 x i16> %292, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %380 = bitcast <8 x i32> %63 to <16 x i16>
  %381 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %378, <16 x i16> %380) #9
  %382 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %379, <16 x i16> %380) #9
  %383 = bitcast <8 x i32> %68 to <16 x i16>
  %384 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %378, <16 x i16> %383) #9
  %385 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %379, <16 x i16> %383) #9
  %386 = add <8 x i32> %381, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %387 = add <8 x i32> %382, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %388 = add <8 x i32> %384, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %389 = add <8 x i32> %385, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %390 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %386, i32 %305) #9
  %391 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %387, i32 %305) #9
  %392 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %388, i32 %305) #9
  %393 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %389, i32 %305) #9
  %394 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %390, <8 x i32> %391) #9
  %395 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %392, <8 x i32> %393) #9
  %396 = shufflevector <16 x i16> %274, <16 x i16> %289, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %397 = shufflevector <16 x i16> %274, <16 x i16> %289, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %398 = bitcast <8 x i32> %76 to <16 x i16>
  %399 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %396, <16 x i16> %398) #9
  %400 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %397, <16 x i16> %398) #9
  %401 = bitcast <8 x i32> %81 to <16 x i16>
  %402 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %396, <16 x i16> %401) #9
  %403 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %397, <16 x i16> %401) #9
  %404 = add <8 x i32> %399, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %405 = add <8 x i32> %400, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %406 = add <8 x i32> %402, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %407 = add <8 x i32> %403, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %408 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %404, i32 %305) #9
  %409 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %405, i32 %305) #9
  %410 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %406, i32 %305) #9
  %411 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %407, i32 %305) #9
  %412 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %408, <8 x i32> %409) #9
  %413 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %410, <8 x i32> %411) #9
  %414 = shufflevector <16 x i16> %277, <16 x i16> %286, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %415 = shufflevector <16 x i16> %277, <16 x i16> %286, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %416 = bitcast <8 x i32> %89 to <16 x i16>
  %417 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %414, <16 x i16> %416) #9
  %418 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %415, <16 x i16> %416) #9
  %419 = bitcast <8 x i32> %94 to <16 x i16>
  %420 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %414, <16 x i16> %419) #9
  %421 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %415, <16 x i16> %419) #9
  %422 = add <8 x i32> %417, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %423 = add <8 x i32> %418, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %424 = add <8 x i32> %420, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %425 = add <8 x i32> %421, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %426 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %422, i32 %305) #9
  %427 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %423, i32 %305) #9
  %428 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %424, i32 %305) #9
  %429 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %425, i32 %305) #9
  %430 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %426, <8 x i32> %427) #9
  %431 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %428, <8 x i32> %429) #9
  %432 = shufflevector <16 x i16> %280, <16 x i16> %283, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %433 = shufflevector <16 x i16> %280, <16 x i16> %283, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %434 = bitcast <8 x i32> %102 to <16 x i16>
  %435 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %432, <16 x i16> %434) #9
  %436 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %433, <16 x i16> %434) #9
  %437 = bitcast <8 x i32> %107 to <16 x i16>
  %438 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %432, <16 x i16> %437) #9
  %439 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %433, <16 x i16> %437) #9
  %440 = add <8 x i32> %435, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %441 = add <8 x i32> %436, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %442 = add <8 x i32> %438, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %443 = add <8 x i32> %439, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %444 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %440, i32 %305) #9
  %445 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %441, i32 %305) #9
  %446 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %442, i32 %305) #9
  %447 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %443, i32 %305) #9
  %448 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %444, <8 x i32> %445) #9
  %449 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %446, <8 x i32> %447) #9
  %450 = shufflevector <16 x i16> %235, <16 x i16> %256, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %451 = shufflevector <16 x i16> %235, <16 x i16> %256, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %452 = bitcast <8 x i32> %115 to <16 x i16>
  %453 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %450, <16 x i16> %452) #9
  %454 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %451, <16 x i16> %452) #9
  %455 = bitcast <8 x i32> %120 to <16 x i16>
  %456 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %450, <16 x i16> %455) #9
  %457 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %451, <16 x i16> %455) #9
  %458 = add <8 x i32> %453, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %459 = add <8 x i32> %454, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %460 = add <8 x i32> %456, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %461 = add <8 x i32> %457, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %462 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %458, i32 %305) #9
  %463 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %459, i32 %305) #9
  %464 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %460, i32 %305) #9
  %465 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %461, i32 %305) #9
  %466 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %462, <8 x i32> %463) #9
  %467 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %464, <8 x i32> %465) #9
  %468 = shufflevector <16 x i16> %238, <16 x i16> %253, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %469 = shufflevector <16 x i16> %238, <16 x i16> %253, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %470 = bitcast <8 x i32> %128 to <16 x i16>
  %471 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %468, <16 x i16> %470) #9
  %472 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %469, <16 x i16> %470) #9
  %473 = bitcast <8 x i32> %133 to <16 x i16>
  %474 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %468, <16 x i16> %473) #9
  %475 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %469, <16 x i16> %473) #9
  %476 = add <8 x i32> %471, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %477 = add <8 x i32> %472, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %478 = add <8 x i32> %474, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %479 = add <8 x i32> %475, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %480 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %476, i32 %305) #9
  %481 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %477, i32 %305) #9
  %482 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %478, i32 %305) #9
  %483 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %479, i32 %305) #9
  %484 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %480, <8 x i32> %481) #9
  %485 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %482, <8 x i32> %483) #9
  %486 = shufflevector <16 x i16> %241, <16 x i16> %250, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %487 = shufflevector <16 x i16> %241, <16 x i16> %250, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %488 = bitcast <8 x i32> %141 to <16 x i16>
  %489 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %486, <16 x i16> %488) #9
  %490 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %487, <16 x i16> %488) #9
  %491 = bitcast <8 x i32> %146 to <16 x i16>
  %492 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %486, <16 x i16> %491) #9
  %493 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %487, <16 x i16> %491) #9
  %494 = add <8 x i32> %489, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %495 = add <8 x i32> %490, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %496 = add <8 x i32> %492, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %497 = add <8 x i32> %493, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %498 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %494, i32 %305) #9
  %499 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %495, i32 %305) #9
  %500 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %496, i32 %305) #9
  %501 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %497, i32 %305) #9
  %502 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %498, <8 x i32> %499) #9
  %503 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %500, <8 x i32> %501) #9
  %504 = shufflevector <16 x i16> %244, <16 x i16> %247, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %505 = shufflevector <16 x i16> %244, <16 x i16> %247, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %506 = bitcast <8 x i32> %154 to <16 x i16>
  %507 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %504, <16 x i16> %506) #9
  %508 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %505, <16 x i16> %506) #9
  %509 = bitcast <8 x i32> %159 to <16 x i16>
  %510 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %504, <16 x i16> %509) #9
  %511 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %505, <16 x i16> %509) #9
  %512 = add <8 x i32> %507, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %513 = add <8 x i32> %508, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %514 = add <8 x i32> %510, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %515 = add <8 x i32> %511, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %516 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %512, i32 %305) #9
  %517 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %513, i32 %305) #9
  %518 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %514, i32 %305) #9
  %519 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %515, i32 %305) #9
  %520 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %516, <8 x i32> %517) #9
  %521 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %518, <8 x i32> %519) #9
  %522 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %322, <16 x i16> %340) #9
  %523 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %322, <16 x i16> %340) #9
  %524 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %376, <16 x i16> %358) #9
  %525 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %376, <16 x i16> %358) #9
  %526 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %394, <16 x i16> %412) #9
  %527 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %394, <16 x i16> %412) #9
  %528 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %448, <16 x i16> %430) #9
  %529 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %448, <16 x i16> %430) #9
  %530 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %449, <16 x i16> %431) #9
  %531 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %449, <16 x i16> %431) #9
  %532 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %395, <16 x i16> %413) #9
  %533 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %395, <16 x i16> %413) #9
  %534 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %377, <16 x i16> %359) #9
  %535 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %377, <16 x i16> %359) #9
  %536 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %323, <16 x i16> %341) #9
  %537 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %323, <16 x i16> %341) #9
  %538 = shufflevector <16 x i16> %223, <16 x i16> %232, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %539 = shufflevector <16 x i16> %223, <16 x i16> %232, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %540 = bitcast <8 x i32> %167 to <16 x i16>
  %541 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %538, <16 x i16> %540) #9
  %542 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %539, <16 x i16> %540) #9
  %543 = bitcast <8 x i32> %172 to <16 x i16>
  %544 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %538, <16 x i16> %543) #9
  %545 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %539, <16 x i16> %543) #9
  %546 = add <8 x i32> %541, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %547 = add <8 x i32> %542, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %548 = add <8 x i32> %544, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %549 = add <8 x i32> %545, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %550 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %546, i32 %305) #9
  %551 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %547, i32 %305) #9
  %552 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %548, i32 %305) #9
  %553 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %549, i32 %305) #9
  %554 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %550, <8 x i32> %551) #9
  %555 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %552, <8 x i32> %553) #9
  %556 = shufflevector <16 x i16> %226, <16 x i16> %229, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %557 = shufflevector <16 x i16> %226, <16 x i16> %229, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %558 = bitcast <8 x i32> %180 to <16 x i16>
  %559 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %556, <16 x i16> %558) #9
  %560 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %557, <16 x i16> %558) #9
  %561 = bitcast <8 x i32> %185 to <16 x i16>
  %562 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %556, <16 x i16> %561) #9
  %563 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %557, <16 x i16> %561) #9
  %564 = add <8 x i32> %559, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %565 = add <8 x i32> %560, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %566 = add <8 x i32> %562, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %567 = add <8 x i32> %563, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %568 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %564, i32 %305) #9
  %569 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %565, i32 %305) #9
  %570 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %566, i32 %305) #9
  %571 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %567, i32 %305) #9
  %572 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %568, <8 x i32> %569) #9
  %573 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %570, <8 x i32> %571) #9
  %574 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %466, <16 x i16> %484) #9
  %575 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %466, <16 x i16> %484) #9
  %576 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %520, <16 x i16> %502) #9
  %577 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %520, <16 x i16> %502) #9
  %578 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %521, <16 x i16> %503) #9
  %579 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %521, <16 x i16> %503) #9
  %580 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %467, <16 x i16> %485) #9
  %581 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %467, <16 x i16> %485) #9
  %582 = sub i32 0, %161
  %583 = and i32 %582, 65535
  %584 = or i32 %169, %583
  %585 = insertelement <8 x i32> undef, i32 %584, i32 0
  %586 = shufflevector <8 x i32> %585, <8 x i32> undef, <8 x i32> zeroinitializer
  %587 = or i32 %163, %162
  %588 = insertelement <8 x i32> undef, i32 %587, i32 0
  %589 = shufflevector <8 x i32> %588, <8 x i32> undef, <8 x i32> zeroinitializer
  %590 = sub i32 0, %160
  %591 = and i32 %590, 65535
  %592 = shl nuw i32 %583, 16
  %593 = or i32 %592, %591
  %594 = insertelement <8 x i32> undef, i32 %593, i32 0
  %595 = shufflevector <8 x i32> %594, <8 x i32> undef, <8 x i32> zeroinitializer
  %596 = sub i32 0, %174
  %597 = and i32 %596, 65535
  %598 = or i32 %182, %597
  %599 = insertelement <8 x i32> undef, i32 %598, i32 0
  %600 = shufflevector <8 x i32> %599, <8 x i32> undef, <8 x i32> zeroinitializer
  %601 = or i32 %176, %175
  %602 = insertelement <8 x i32> undef, i32 %601, i32 0
  %603 = shufflevector <8 x i32> %602, <8 x i32> undef, <8 x i32> zeroinitializer
  %604 = sub i32 0, %173
  %605 = and i32 %604, 65535
  %606 = shl nuw i32 %597, 16
  %607 = or i32 %606, %605
  %608 = insertelement <8 x i32> undef, i32 %607, i32 0
  %609 = shufflevector <8 x i32> %608, <8 x i32> undef, <8 x i32> zeroinitializer
  %610 = shufflevector <16 x i16> %523, <16 x i16> %537, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %611 = shufflevector <16 x i16> %523, <16 x i16> %537, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %612 = bitcast <8 x i32> %586 to <16 x i16>
  %613 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %610, <16 x i16> %612) #9
  %614 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %611, <16 x i16> %612) #9
  %615 = bitcast <8 x i32> %589 to <16 x i16>
  %616 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %610, <16 x i16> %615) #9
  %617 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %611, <16 x i16> %615) #9
  %618 = add <8 x i32> %613, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %619 = add <8 x i32> %614, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %620 = add <8 x i32> %616, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %621 = add <8 x i32> %617, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %622 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %618, i32 %305) #9
  %623 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %619, i32 %305) #9
  %624 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %620, i32 %305) #9
  %625 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %621, i32 %305) #9
  %626 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %622, <8 x i32> %623) #9
  %627 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %624, <8 x i32> %625) #9
  %628 = shufflevector <16 x i16> %525, <16 x i16> %535, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %629 = shufflevector <16 x i16> %525, <16 x i16> %535, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %630 = bitcast <8 x i32> %595 to <16 x i16>
  %631 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %628, <16 x i16> %630) #9
  %632 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %629, <16 x i16> %630) #9
  %633 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %628, <16 x i16> %612) #9
  %634 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %629, <16 x i16> %612) #9
  %635 = add <8 x i32> %631, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %636 = add <8 x i32> %632, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %637 = add <8 x i32> %633, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %638 = add <8 x i32> %634, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %639 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %635, i32 %305) #9
  %640 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %636, i32 %305) #9
  %641 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %637, i32 %305) #9
  %642 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %638, i32 %305) #9
  %643 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %639, <8 x i32> %640) #9
  %644 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %641, <8 x i32> %642) #9
  %645 = shufflevector <16 x i16> %527, <16 x i16> %533, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %646 = shufflevector <16 x i16> %527, <16 x i16> %533, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %647 = bitcast <8 x i32> %600 to <16 x i16>
  %648 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %645, <16 x i16> %647) #9
  %649 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %646, <16 x i16> %647) #9
  %650 = bitcast <8 x i32> %603 to <16 x i16>
  %651 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %645, <16 x i16> %650) #9
  %652 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %646, <16 x i16> %650) #9
  %653 = add <8 x i32> %648, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %654 = add <8 x i32> %649, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %655 = add <8 x i32> %651, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %656 = add <8 x i32> %652, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %657 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %653, i32 %305) #9
  %658 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %654, i32 %305) #9
  %659 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %655, i32 %305) #9
  %660 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %656, i32 %305) #9
  %661 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %657, <8 x i32> %658) #9
  %662 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %659, <8 x i32> %660) #9
  %663 = shufflevector <16 x i16> %529, <16 x i16> %531, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %664 = shufflevector <16 x i16> %529, <16 x i16> %531, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %665 = bitcast <8 x i32> %609 to <16 x i16>
  %666 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %663, <16 x i16> %665) #9
  %667 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %664, <16 x i16> %665) #9
  %668 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %663, <16 x i16> %647) #9
  %669 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %664, <16 x i16> %647) #9
  %670 = add <8 x i32> %666, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %671 = add <8 x i32> %667, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %672 = add <8 x i32> %668, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %673 = add <8 x i32> %669, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %674 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %670, i32 %305) #9
  %675 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %671, i32 %305) #9
  %676 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %672, i32 %305) #9
  %677 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %673, i32 %305) #9
  %678 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %674, <8 x i32> %675) #9
  %679 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %676, <8 x i32> %677) #9
  %680 = shufflevector <16 x i16> %211, <16 x i16> %214, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %681 = shufflevector <16 x i16> %211, <16 x i16> %214, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %682 = bitcast <8 x i32> %191 to <16 x i16>
  %683 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %680, <16 x i16> %682) #9
  %684 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %681, <16 x i16> %682) #9
  %685 = bitcast <8 x i32> %196 to <16 x i16>
  %686 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %680, <16 x i16> %685) #9
  %687 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %681, <16 x i16> %685) #9
  %688 = add <8 x i32> %683, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %689 = add <8 x i32> %684, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %690 = add <8 x i32> %686, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %691 = add <8 x i32> %687, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %692 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %688, i32 %305) #9
  %693 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %689, i32 %305) #9
  %694 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %690, i32 %305) #9
  %695 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %691, i32 %305) #9
  %696 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %692, <8 x i32> %693) #9
  %697 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %694, <8 x i32> %695) #9
  %698 = shufflevector <16 x i16> %217, <16 x i16> %220, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %699 = shufflevector <16 x i16> %217, <16 x i16> %220, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %700 = bitcast <8 x i32> %204 to <16 x i16>
  %701 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %698, <16 x i16> %700) #9
  %702 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %699, <16 x i16> %700) #9
  %703 = bitcast <8 x i32> %209 to <16 x i16>
  %704 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %698, <16 x i16> %703) #9
  %705 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %699, <16 x i16> %703) #9
  %706 = add <8 x i32> %701, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %707 = add <8 x i32> %702, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %708 = add <8 x i32> %704, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %709 = add <8 x i32> %705, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %710 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %706, i32 %305) #9
  %711 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %707, i32 %305) #9
  %712 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %708, i32 %305) #9
  %713 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %709, i32 %305) #9
  %714 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %710, <8 x i32> %711) #9
  %715 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %712, <8 x i32> %713) #9
  %716 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %554, <16 x i16> %572) #9
  %717 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %554, <16 x i16> %572) #9
  %718 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %555, <16 x i16> %573) #9
  %719 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %555, <16 x i16> %573) #9
  %720 = sub i32 0, %198
  %721 = and i32 %720, 65535
  %722 = or i32 %206, %721
  %723 = insertelement <8 x i32> undef, i32 %722, i32 0
  %724 = shufflevector <8 x i32> %723, <8 x i32> undef, <8 x i32> zeroinitializer
  %725 = or i32 %200, %199
  %726 = insertelement <8 x i32> undef, i32 %725, i32 0
  %727 = shufflevector <8 x i32> %726, <8 x i32> undef, <8 x i32> zeroinitializer
  %728 = sub i32 0, %197
  %729 = and i32 %728, 65535
  %730 = shl nuw i32 %721, 16
  %731 = or i32 %730, %729
  %732 = insertelement <8 x i32> undef, i32 %731, i32 0
  %733 = shufflevector <8 x i32> %732, <8 x i32> undef, <8 x i32> zeroinitializer
  %734 = shufflevector <16 x i16> %575, <16 x i16> %581, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %735 = shufflevector <16 x i16> %575, <16 x i16> %581, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %736 = bitcast <8 x i32> %724 to <16 x i16>
  %737 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %734, <16 x i16> %736) #9
  %738 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %735, <16 x i16> %736) #9
  %739 = bitcast <8 x i32> %727 to <16 x i16>
  %740 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %734, <16 x i16> %739) #9
  %741 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %735, <16 x i16> %739) #9
  %742 = add <8 x i32> %737, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %743 = add <8 x i32> %738, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %744 = add <8 x i32> %740, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %745 = add <8 x i32> %741, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %746 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %742, i32 %305) #9
  %747 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %743, i32 %305) #9
  %748 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %744, i32 %305) #9
  %749 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %745, i32 %305) #9
  %750 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %746, <8 x i32> %747) #9
  %751 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %748, <8 x i32> %749) #9
  %752 = shufflevector <16 x i16> %577, <16 x i16> %579, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %753 = shufflevector <16 x i16> %577, <16 x i16> %579, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %754 = bitcast <8 x i32> %733 to <16 x i16>
  %755 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %752, <16 x i16> %754) #9
  %756 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %753, <16 x i16> %754) #9
  %757 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %752, <16 x i16> %736) #9
  %758 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %753, <16 x i16> %736) #9
  %759 = add <8 x i32> %755, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %760 = add <8 x i32> %756, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %761 = add <8 x i32> %757, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %762 = add <8 x i32> %758, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %763 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %759, i32 %305) #9
  %764 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %760, i32 %305) #9
  %765 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %761, i32 %305) #9
  %766 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %762, i32 %305) #9
  %767 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %763, <8 x i32> %764) #9
  %768 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %765, <8 x i32> %766) #9
  %769 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %522, <16 x i16> %524) #9
  %770 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %522, <16 x i16> %524) #9
  %771 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %626, <16 x i16> %643) #9
  %772 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %626, <16 x i16> %643) #9
  %773 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %528, <16 x i16> %526) #9
  %774 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %528, <16 x i16> %526) #9
  %775 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %678, <16 x i16> %661) #9
  %776 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %678, <16 x i16> %661) #9
  %777 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %530, <16 x i16> %532) #9
  %778 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %530, <16 x i16> %532) #9
  %779 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %679, <16 x i16> %662) #9
  %780 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %679, <16 x i16> %662) #9
  %781 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %536, <16 x i16> %534) #9
  %782 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %536, <16 x i16> %534) #9
  %783 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %627, <16 x i16> %644) #9
  %784 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %627, <16 x i16> %644) #9
  %785 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %696, <16 x i16> %715) #9
  %786 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %696, <16 x i16> %715) #9
  %787 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %697, <16 x i16> %714) #9
  %788 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %697, <16 x i16> %714) #9
  %789 = sub i32 0, %186
  %790 = and i32 %789, 65535
  %791 = or i32 %188, %790
  %792 = insertelement <8 x i32> undef, i32 %791, i32 0
  %793 = shufflevector <8 x i32> %792, <8 x i32> undef, <8 x i32> zeroinitializer
  %794 = shufflevector <16 x i16> %717, <16 x i16> %719, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %795 = shufflevector <16 x i16> %717, <16 x i16> %719, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %796 = bitcast <8 x i32> %793 to <16 x i16>
  %797 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %794, <16 x i16> %796) #9
  %798 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %795, <16 x i16> %796) #9
  %799 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %794, <16 x i16> %682) #9
  %800 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %795, <16 x i16> %682) #9
  %801 = add <8 x i32> %797, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %802 = add <8 x i32> %798, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %803 = add <8 x i32> %799, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %804 = add <8 x i32> %800, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %805 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %801, i32 %305) #9
  %806 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %802, i32 %305) #9
  %807 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %803, i32 %305) #9
  %808 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %804, i32 %305) #9
  %809 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %805, <8 x i32> %806) #9
  %810 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %807, <8 x i32> %808) #9
  %811 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %574, <16 x i16> %576) #9
  %812 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %574, <16 x i16> %576) #9
  %813 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %750, <16 x i16> %767) #9
  %814 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %750, <16 x i16> %767) #9
  %815 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %580, <16 x i16> %578) #9
  %816 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %580, <16 x i16> %578) #9
  %817 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %751, <16 x i16> %768) #9
  %818 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %751, <16 x i16> %768) #9
  %819 = shufflevector <16 x i16> %772, <16 x i16> %784, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %820 = shufflevector <16 x i16> %772, <16 x i16> %784, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %821 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %819, <16 x i16> %736) #9
  %822 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %820, <16 x i16> %736) #9
  %823 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %819, <16 x i16> %739) #9
  %824 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %820, <16 x i16> %739) #9
  %825 = add <8 x i32> %821, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %826 = add <8 x i32> %822, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %827 = add <8 x i32> %823, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %828 = add <8 x i32> %824, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %829 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %825, i32 %305) #9
  %830 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %826, i32 %305) #9
  %831 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %827, i32 %305) #9
  %832 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %828, i32 %305) #9
  %833 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %829, <8 x i32> %830) #9
  %834 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %831, <8 x i32> %832) #9
  %835 = shufflevector <16 x i16> %770, <16 x i16> %782, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %836 = shufflevector <16 x i16> %770, <16 x i16> %782, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %837 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %835, <16 x i16> %736) #9
  %838 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %836, <16 x i16> %736) #9
  %839 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %835, <16 x i16> %739) #9
  %840 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %836, <16 x i16> %739) #9
  %841 = add <8 x i32> %837, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %842 = add <8 x i32> %838, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %843 = add <8 x i32> %839, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %844 = add <8 x i32> %840, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %845 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %841, i32 %305) #9
  %846 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %842, i32 %305) #9
  %847 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %843, i32 %305) #9
  %848 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %844, i32 %305) #9
  %849 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %845, <8 x i32> %846) #9
  %850 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %847, <8 x i32> %848) #9
  %851 = shufflevector <16 x i16> %774, <16 x i16> %778, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %852 = shufflevector <16 x i16> %774, <16 x i16> %778, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %853 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %851, <16 x i16> %754) #9
  %854 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %852, <16 x i16> %754) #9
  %855 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %851, <16 x i16> %736) #9
  %856 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %852, <16 x i16> %736) #9
  %857 = add <8 x i32> %853, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %858 = add <8 x i32> %854, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %859 = add <8 x i32> %855, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %860 = add <8 x i32> %856, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %861 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %857, i32 %305) #9
  %862 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %858, i32 %305) #9
  %863 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %859, i32 %305) #9
  %864 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %860, i32 %305) #9
  %865 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %861, <8 x i32> %862) #9
  %866 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %863, <8 x i32> %864) #9
  %867 = shufflevector <16 x i16> %776, <16 x i16> %780, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %868 = shufflevector <16 x i16> %776, <16 x i16> %780, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %869 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %867, <16 x i16> %754) #9
  %870 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %868, <16 x i16> %754) #9
  %871 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %867, <16 x i16> %736) #9
  %872 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %868, <16 x i16> %736) #9
  %873 = add <8 x i32> %869, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %874 = add <8 x i32> %870, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %875 = add <8 x i32> %871, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %876 = add <8 x i32> %872, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %877 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %873, i32 %305) #9
  %878 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %874, i32 %305) #9
  %879 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %875, i32 %305) #9
  %880 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %876, i32 %305) #9
  %881 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %877, <8 x i32> %878) #9
  %882 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %879, <8 x i32> %880) #9
  %883 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %785, <16 x i16> %718) #9
  %884 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %785, <16 x i16> %718) #9
  %885 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %787, <16 x i16> %810) #9
  %886 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %787, <16 x i16> %810) #9
  %887 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %788, <16 x i16> %809) #9
  %888 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %788, <16 x i16> %809) #9
  %889 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %786, <16 x i16> %716) #9
  %890 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %786, <16 x i16> %716) #9
  %891 = shufflevector <16 x i16> %814, <16 x i16> %818, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %892 = shufflevector <16 x i16> %814, <16 x i16> %818, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %893 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %891, <16 x i16> %796) #9
  %894 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %892, <16 x i16> %796) #9
  %895 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %891, <16 x i16> %682) #9
  %896 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %892, <16 x i16> %682) #9
  %897 = add <8 x i32> %893, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %898 = add <8 x i32> %894, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %899 = add <8 x i32> %895, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %900 = add <8 x i32> %896, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %901 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %897, i32 %305) #9
  %902 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %898, i32 %305) #9
  %903 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %899, i32 %305) #9
  %904 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %900, i32 %305) #9
  %905 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %901, <8 x i32> %902) #9
  %906 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %903, <8 x i32> %904) #9
  %907 = shufflevector <16 x i16> %812, <16 x i16> %816, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %908 = shufflevector <16 x i16> %812, <16 x i16> %816, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %909 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %907, <16 x i16> %796) #9
  %910 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %908, <16 x i16> %796) #9
  %911 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %907, <16 x i16> %682) #9
  %912 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %908, <16 x i16> %682) #9
  %913 = add <8 x i32> %909, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %914 = add <8 x i32> %910, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %915 = add <8 x i32> %911, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %916 = add <8 x i32> %912, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %917 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %913, i32 %305) #9
  %918 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %914, i32 %305) #9
  %919 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %915, i32 %305) #9
  %920 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %916, i32 %305) #9
  %921 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %917, <8 x i32> %918) #9
  %922 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %919, <8 x i32> %920) #9
  %923 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %769, <16 x i16> %773) #9
  %924 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %769, <16 x i16> %773) #9
  %925 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %771, <16 x i16> %775) #9
  %926 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %771, <16 x i16> %775) #9
  %927 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %833, <16 x i16> %881) #9
  %928 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %833, <16 x i16> %881) #9
  %929 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %849, <16 x i16> %865) #9
  %930 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %849, <16 x i16> %865) #9
  %931 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %781, <16 x i16> %777) #9
  %932 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %781, <16 x i16> %777) #9
  %933 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %783, <16 x i16> %779) #9
  %934 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %783, <16 x i16> %779) #9
  %935 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %834, <16 x i16> %882) #9
  %936 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %834, <16 x i16> %882) #9
  %937 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %850, <16 x i16> %866) #9
  %938 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %850, <16 x i16> %866) #9
  %939 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %883, <16 x i16> %815) #9
  %940 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %883, <16 x i16> %815) #9
  %941 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %885, <16 x i16> %817) #9
  %942 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %885, <16 x i16> %817) #9
  %943 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %887, <16 x i16> %906) #9
  %944 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %887, <16 x i16> %906) #9
  %945 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %889, <16 x i16> %922) #9
  %946 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %889, <16 x i16> %922) #9
  %947 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %890, <16 x i16> %921) #9
  %948 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %890, <16 x i16> %921) #9
  %949 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %888, <16 x i16> %905) #9
  %950 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %888, <16 x i16> %905) #9
  %951 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %886, <16 x i16> %813) #9
  %952 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %886, <16 x i16> %813) #9
  %953 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %884, <16 x i16> %811) #9
  %954 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %884, <16 x i16> %811) #9
  %955 = shufflevector <16 x i16> %930, <16 x i16> %938, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %956 = shufflevector <16 x i16> %930, <16 x i16> %938, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %957 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %955, <16 x i16> %796) #9
  %958 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %956, <16 x i16> %796) #9
  %959 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %955, <16 x i16> %682) #9
  %960 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %956, <16 x i16> %682) #9
  %961 = add <8 x i32> %957, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %962 = add <8 x i32> %958, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %963 = add <8 x i32> %959, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %964 = add <8 x i32> %960, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %965 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %961, i32 %305) #9
  %966 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %962, i32 %305) #9
  %967 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %963, i32 %305) #9
  %968 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %964, i32 %305) #9
  %969 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %965, <8 x i32> %966) #9
  %970 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %967, <8 x i32> %968) #9
  %971 = shufflevector <16 x i16> %928, <16 x i16> %936, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %972 = shufflevector <16 x i16> %928, <16 x i16> %936, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %973 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %971, <16 x i16> %796) #9
  %974 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %972, <16 x i16> %796) #9
  %975 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %971, <16 x i16> %682) #9
  %976 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %972, <16 x i16> %682) #9
  %977 = add <8 x i32> %973, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %978 = add <8 x i32> %974, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %979 = add <8 x i32> %975, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %980 = add <8 x i32> %976, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %981 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %977, i32 %305) #9
  %982 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %978, i32 %305) #9
  %983 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %979, i32 %305) #9
  %984 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %980, i32 %305) #9
  %985 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %981, <8 x i32> %982) #9
  %986 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %983, <8 x i32> %984) #9
  %987 = shufflevector <16 x i16> %926, <16 x i16> %934, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %988 = shufflevector <16 x i16> %926, <16 x i16> %934, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %989 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %987, <16 x i16> %796) #9
  %990 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %988, <16 x i16> %796) #9
  %991 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %987, <16 x i16> %682) #9
  %992 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %988, <16 x i16> %682) #9
  %993 = add <8 x i32> %989, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %994 = add <8 x i32> %990, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %995 = add <8 x i32> %991, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %996 = add <8 x i32> %992, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %997 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %993, i32 %305) #9
  %998 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %994, i32 %305) #9
  %999 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %995, i32 %305) #9
  %1000 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %996, i32 %305) #9
  %1001 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %997, <8 x i32> %998) #9
  %1002 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %999, <8 x i32> %1000) #9
  %1003 = shufflevector <16 x i16> %924, <16 x i16> %932, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1004 = shufflevector <16 x i16> %924, <16 x i16> %932, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1005 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1003, <16 x i16> %796) #9
  %1006 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1004, <16 x i16> %796) #9
  %1007 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1003, <16 x i16> %682) #9
  %1008 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1004, <16 x i16> %682) #9
  %1009 = add <8 x i32> %1005, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1010 = add <8 x i32> %1006, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1011 = add <8 x i32> %1007, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1012 = add <8 x i32> %1008, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1013 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1009, i32 %305) #9
  %1014 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1010, i32 %305) #9
  %1015 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1011, i32 %305) #9
  %1016 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1012, i32 %305) #9
  %1017 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1013, <8 x i32> %1014) #9
  %1018 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1015, <8 x i32> %1016) #9
  %1019 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 31
  %1020 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %939, <16 x i16> %931) #9
  %1021 = bitcast <4 x i64>* %1 to <16 x i16>*
  store <16 x i16> %1020, <16 x i16>* %1021, align 32
  %1022 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %939, <16 x i16> %931) #9
  %1023 = bitcast <4 x i64>* %1019 to <16 x i16>*
  store <16 x i16> %1022, <16 x i16>* %1023, align 32
  %1024 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %1025 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 30
  %1026 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %941, <16 x i16> %933) #9
  %1027 = bitcast <4 x i64>* %1024 to <16 x i16>*
  store <16 x i16> %1026, <16 x i16>* %1027, align 32
  %1028 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %941, <16 x i16> %933) #9
  %1029 = bitcast <4 x i64>* %1025 to <16 x i16>*
  store <16 x i16> %1028, <16 x i16>* %1029, align 32
  %1030 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %1031 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 29
  %1032 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %943, <16 x i16> %935) #9
  %1033 = bitcast <4 x i64>* %1030 to <16 x i16>*
  store <16 x i16> %1032, <16 x i16>* %1033, align 32
  %1034 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %943, <16 x i16> %935) #9
  %1035 = bitcast <4 x i64>* %1031 to <16 x i16>*
  store <16 x i16> %1034, <16 x i16>* %1035, align 32
  %1036 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %1037 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 28
  %1038 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %945, <16 x i16> %937) #9
  %1039 = bitcast <4 x i64>* %1036 to <16 x i16>*
  store <16 x i16> %1038, <16 x i16>* %1039, align 32
  %1040 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %945, <16 x i16> %937) #9
  %1041 = bitcast <4 x i64>* %1037 to <16 x i16>*
  store <16 x i16> %1040, <16 x i16>* %1041, align 32
  %1042 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %1043 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 27
  %1044 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %947, <16 x i16> %970) #9
  %1045 = bitcast <4 x i64>* %1042 to <16 x i16>*
  store <16 x i16> %1044, <16 x i16>* %1045, align 32
  %1046 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %947, <16 x i16> %970) #9
  %1047 = bitcast <4 x i64>* %1043 to <16 x i16>*
  store <16 x i16> %1046, <16 x i16>* %1047, align 32
  %1048 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %1049 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 26
  %1050 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %949, <16 x i16> %986) #9
  %1051 = bitcast <4 x i64>* %1048 to <16 x i16>*
  store <16 x i16> %1050, <16 x i16>* %1051, align 32
  %1052 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %949, <16 x i16> %986) #9
  %1053 = bitcast <4 x i64>* %1049 to <16 x i16>*
  store <16 x i16> %1052, <16 x i16>* %1053, align 32
  %1054 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %1055 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 25
  %1056 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %951, <16 x i16> %1002) #9
  %1057 = bitcast <4 x i64>* %1054 to <16 x i16>*
  store <16 x i16> %1056, <16 x i16>* %1057, align 32
  %1058 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %951, <16 x i16> %1002) #9
  %1059 = bitcast <4 x i64>* %1055 to <16 x i16>*
  store <16 x i16> %1058, <16 x i16>* %1059, align 32
  %1060 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %1061 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 24
  %1062 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %953, <16 x i16> %1018) #9
  %1063 = bitcast <4 x i64>* %1060 to <16 x i16>*
  store <16 x i16> %1062, <16 x i16>* %1063, align 32
  %1064 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %953, <16 x i16> %1018) #9
  %1065 = bitcast <4 x i64>* %1061 to <16 x i16>*
  store <16 x i16> %1064, <16 x i16>* %1065, align 32
  %1066 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %1067 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 23
  %1068 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %954, <16 x i16> %1017) #9
  %1069 = bitcast <4 x i64>* %1066 to <16 x i16>*
  store <16 x i16> %1068, <16 x i16>* %1069, align 32
  %1070 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %954, <16 x i16> %1017) #9
  %1071 = bitcast <4 x i64>* %1067 to <16 x i16>*
  store <16 x i16> %1070, <16 x i16>* %1071, align 32
  %1072 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %1073 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 22
  %1074 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %952, <16 x i16> %1001) #9
  %1075 = bitcast <4 x i64>* %1072 to <16 x i16>*
  store <16 x i16> %1074, <16 x i16>* %1075, align 32
  %1076 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %952, <16 x i16> %1001) #9
  %1077 = bitcast <4 x i64>* %1073 to <16 x i16>*
  store <16 x i16> %1076, <16 x i16>* %1077, align 32
  %1078 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %1079 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 21
  %1080 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %950, <16 x i16> %985) #9
  %1081 = bitcast <4 x i64>* %1078 to <16 x i16>*
  store <16 x i16> %1080, <16 x i16>* %1081, align 32
  %1082 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %950, <16 x i16> %985) #9
  %1083 = bitcast <4 x i64>* %1079 to <16 x i16>*
  store <16 x i16> %1082, <16 x i16>* %1083, align 32
  %1084 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %1085 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 20
  %1086 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %948, <16 x i16> %969) #9
  %1087 = bitcast <4 x i64>* %1084 to <16 x i16>*
  store <16 x i16> %1086, <16 x i16>* %1087, align 32
  %1088 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %948, <16 x i16> %969) #9
  %1089 = bitcast <4 x i64>* %1085 to <16 x i16>*
  store <16 x i16> %1088, <16 x i16>* %1089, align 32
  %1090 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %1091 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 19
  %1092 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %946, <16 x i16> %929) #9
  %1093 = bitcast <4 x i64>* %1090 to <16 x i16>*
  store <16 x i16> %1092, <16 x i16>* %1093, align 32
  %1094 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %946, <16 x i16> %929) #9
  %1095 = bitcast <4 x i64>* %1091 to <16 x i16>*
  store <16 x i16> %1094, <16 x i16>* %1095, align 32
  %1096 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %1097 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 18
  %1098 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %944, <16 x i16> %927) #9
  %1099 = bitcast <4 x i64>* %1096 to <16 x i16>*
  store <16 x i16> %1098, <16 x i16>* %1099, align 32
  %1100 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %944, <16 x i16> %927) #9
  %1101 = bitcast <4 x i64>* %1097 to <16 x i16>*
  store <16 x i16> %1100, <16 x i16>* %1101, align 32
  %1102 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %1103 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 17
  %1104 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %942, <16 x i16> %925) #9
  %1105 = bitcast <4 x i64>* %1102 to <16 x i16>*
  store <16 x i16> %1104, <16 x i16>* %1105, align 32
  %1106 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %942, <16 x i16> %925) #9
  %1107 = bitcast <4 x i64>* %1103 to <16 x i16>*
  store <16 x i16> %1106, <16 x i16>* %1107, align 32
  %1108 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 15
  %1109 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 16
  %1110 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %940, <16 x i16> %923) #9
  %1111 = bitcast <4 x i64>* %1108 to <16 x i16>*
  store <16 x i16> %1110, <16 x i16>* %1111, align 32
  %1112 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %940, <16 x i16> %923) #9
  %1113 = bitcast <4 x i64>* %1109 to <16 x i16>*
  store <16 x i16> %1112, <16 x i16>* %1113, align 32
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define internal void @idct64_low1_avx2(<4 x i64>* nocapture readonly, <4 x i64>*, i8 signext) #4 {
  %4 = bitcast <4 x i64>* %0 to <16 x i16>*
  %5 = load <16 x i16>, <16 x i16>* %4, align 32
  %6 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %7 = trunc i32 %6 to i16
  %8 = shl i16 %7, 3
  %9 = insertelement <16 x i16> undef, i16 %8, i32 0
  %10 = shufflevector <16 x i16> %9, <16 x i16> undef, <16 x i32> zeroinitializer
  %11 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %5, <16 x i16> %10) #9
  %12 = bitcast <4 x i64>* %1 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %12, align 32
  %13 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 63
  %14 = bitcast <4 x i64>* %13 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %14, align 32
  %15 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %16 = bitcast <4 x i64>* %15 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %16, align 32
  %17 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 62
  %18 = bitcast <4 x i64>* %17 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %18, align 32
  %19 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %20 = bitcast <4 x i64>* %19 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %20, align 32
  %21 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 61
  %22 = bitcast <4 x i64>* %21 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %22, align 32
  %23 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %24 = bitcast <4 x i64>* %23 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %24, align 32
  %25 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 60
  %26 = bitcast <4 x i64>* %25 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %26, align 32
  %27 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %28 = bitcast <4 x i64>* %27 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %28, align 32
  %29 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 59
  %30 = bitcast <4 x i64>* %29 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %30, align 32
  %31 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %32 = bitcast <4 x i64>* %31 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %32, align 32
  %33 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 58
  %34 = bitcast <4 x i64>* %33 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %34, align 32
  %35 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %36 = bitcast <4 x i64>* %35 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %36, align 32
  %37 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 57
  %38 = bitcast <4 x i64>* %37 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %38, align 32
  %39 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %40 = bitcast <4 x i64>* %39 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %40, align 32
  %41 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 56
  %42 = bitcast <4 x i64>* %41 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %42, align 32
  %43 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %44 = bitcast <4 x i64>* %43 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %44, align 32
  %45 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 55
  %46 = bitcast <4 x i64>* %45 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %46, align 32
  %47 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %48 = bitcast <4 x i64>* %47 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %48, align 32
  %49 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 54
  %50 = bitcast <4 x i64>* %49 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %50, align 32
  %51 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %52 = bitcast <4 x i64>* %51 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %52, align 32
  %53 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 53
  %54 = bitcast <4 x i64>* %53 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %54, align 32
  %55 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %56 = bitcast <4 x i64>* %55 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %56, align 32
  %57 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 52
  %58 = bitcast <4 x i64>* %57 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %58, align 32
  %59 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %60 = bitcast <4 x i64>* %59 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %60, align 32
  %61 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 51
  %62 = bitcast <4 x i64>* %61 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %62, align 32
  %63 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %64 = bitcast <4 x i64>* %63 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %64, align 32
  %65 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 50
  %66 = bitcast <4 x i64>* %65 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %66, align 32
  %67 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %68 = bitcast <4 x i64>* %67 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %68, align 32
  %69 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 49
  %70 = bitcast <4 x i64>* %69 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %70, align 32
  %71 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 15
  %72 = bitcast <4 x i64>* %71 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %72, align 32
  %73 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 48
  %74 = bitcast <4 x i64>* %73 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %74, align 32
  %75 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 16
  %76 = bitcast <4 x i64>* %75 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %76, align 32
  %77 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 47
  %78 = bitcast <4 x i64>* %77 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %78, align 32
  %79 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 17
  %80 = bitcast <4 x i64>* %79 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %80, align 32
  %81 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 46
  %82 = bitcast <4 x i64>* %81 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %82, align 32
  %83 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 18
  %84 = bitcast <4 x i64>* %83 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %84, align 32
  %85 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 45
  %86 = bitcast <4 x i64>* %85 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %86, align 32
  %87 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 19
  %88 = bitcast <4 x i64>* %87 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %88, align 32
  %89 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 44
  %90 = bitcast <4 x i64>* %89 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %90, align 32
  %91 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 20
  %92 = bitcast <4 x i64>* %91 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %92, align 32
  %93 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 43
  %94 = bitcast <4 x i64>* %93 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %94, align 32
  %95 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 21
  %96 = bitcast <4 x i64>* %95 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %96, align 32
  %97 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 42
  %98 = bitcast <4 x i64>* %97 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %98, align 32
  %99 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 22
  %100 = bitcast <4 x i64>* %99 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %100, align 32
  %101 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 41
  %102 = bitcast <4 x i64>* %101 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %102, align 32
  %103 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 23
  %104 = bitcast <4 x i64>* %103 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %104, align 32
  %105 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 40
  %106 = bitcast <4 x i64>* %105 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %106, align 32
  %107 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 24
  %108 = bitcast <4 x i64>* %107 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %108, align 32
  %109 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 39
  %110 = bitcast <4 x i64>* %109 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %110, align 32
  %111 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 25
  %112 = bitcast <4 x i64>* %111 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %112, align 32
  %113 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 38
  %114 = bitcast <4 x i64>* %113 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %114, align 32
  %115 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 26
  %116 = bitcast <4 x i64>* %115 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %116, align 32
  %117 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 37
  %118 = bitcast <4 x i64>* %117 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %118, align 32
  %119 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 27
  %120 = bitcast <4 x i64>* %119 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %120, align 32
  %121 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 36
  %122 = bitcast <4 x i64>* %121 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %122, align 32
  %123 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 28
  %124 = bitcast <4 x i64>* %123 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %124, align 32
  %125 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 35
  %126 = bitcast <4 x i64>* %125 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %126, align 32
  %127 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 29
  %128 = bitcast <4 x i64>* %127 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %128, align 32
  %129 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 34
  %130 = bitcast <4 x i64>* %129 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %130, align 32
  %131 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 30
  %132 = bitcast <4 x i64>* %131 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %132, align 32
  %133 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 33
  %134 = bitcast <4 x i64>* %133 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %134, align 32
  %135 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 31
  %136 = bitcast <4 x i64>* %135 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %136, align 32
  %137 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 32
  %138 = bitcast <4 x i64>* %137 to <16 x i16>*
  store <16 x i16> %11, <16 x i16>* %138, align 32
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @idct64_low8_avx2(<4 x i64>* nocapture readonly, <4 x i64>*, i8 signext) #0 {
  %4 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 4), align 16
  %5 = trunc i32 %4 to i16
  %6 = sub i32 0, %4
  %7 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 60), align 16
  %8 = trunc i32 %7 to i16
  %9 = and i32 %6, 65535
  %10 = and i32 %7, 65535
  %11 = shl nuw i32 %10, 16
  %12 = or i32 %11, %9
  %13 = insertelement <8 x i32> undef, i32 %12, i32 0
  %14 = shufflevector <8 x i32> %13, <8 x i32> undef, <8 x i32> zeroinitializer
  %15 = shl i32 %4, 16
  %16 = or i32 %10, %15
  %17 = insertelement <8 x i32> undef, i32 %16, i32 0
  %18 = shufflevector <8 x i32> %17, <8 x i32> undef, <8 x i32> zeroinitializer
  %19 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 36), align 16
  %20 = sub i32 0, %19
  %21 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 28), align 16
  %22 = and i32 %20, 65535
  %23 = shl i32 %21, 16
  %24 = or i32 %23, %22
  %25 = insertelement <8 x i32> undef, i32 %24, i32 0
  %26 = shufflevector <8 x i32> %25, <8 x i32> undef, <8 x i32> zeroinitializer
  %27 = sub i32 0, %21
  %28 = and i32 %27, 65535
  %29 = shl nuw i32 %22, 16
  %30 = or i32 %29, %28
  %31 = insertelement <8 x i32> undef, i32 %30, i32 0
  %32 = shufflevector <8 x i32> %31, <8 x i32> undef, <8 x i32> zeroinitializer
  %33 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 20), align 16
  %34 = sub i32 0, %33
  %35 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 44), align 16
  %36 = and i32 %34, 65535
  %37 = and i32 %35, 65535
  %38 = shl nuw i32 %37, 16
  %39 = or i32 %38, %36
  %40 = insertelement <8 x i32> undef, i32 %39, i32 0
  %41 = shufflevector <8 x i32> %40, <8 x i32> undef, <8 x i32> zeroinitializer
  %42 = shl i32 %33, 16
  %43 = or i32 %37, %42
  %44 = insertelement <8 x i32> undef, i32 %43, i32 0
  %45 = shufflevector <8 x i32> %44, <8 x i32> undef, <8 x i32> zeroinitializer
  %46 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 52), align 16
  %47 = sub i32 0, %46
  %48 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 12), align 16
  %49 = and i32 %47, 65535
  %50 = shl i32 %48, 16
  %51 = or i32 %50, %49
  %52 = insertelement <8 x i32> undef, i32 %51, i32 0
  %53 = shufflevector <8 x i32> %52, <8 x i32> undef, <8 x i32> zeroinitializer
  %54 = sub i32 0, %48
  %55 = and i32 %54, 65535
  %56 = shl nuw i32 %49, 16
  %57 = or i32 %56, %55
  %58 = insertelement <8 x i32> undef, i32 %57, i32 0
  %59 = shufflevector <8 x i32> %58, <8 x i32> undef, <8 x i32> zeroinitializer
  %60 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 8), align 16
  %61 = sub i32 0, %60
  %62 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 56), align 16
  %63 = and i32 %61, 65535
  %64 = and i32 %62, 65535
  %65 = shl nuw i32 %64, 16
  %66 = or i32 %65, %63
  %67 = insertelement <8 x i32> undef, i32 %66, i32 0
  %68 = shufflevector <8 x i32> %67, <8 x i32> undef, <8 x i32> zeroinitializer
  %69 = shl i32 %60, 16
  %70 = or i32 %64, %69
  %71 = insertelement <8 x i32> undef, i32 %70, i32 0
  %72 = shufflevector <8 x i32> %71, <8 x i32> undef, <8 x i32> zeroinitializer
  %73 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 40), align 16
  %74 = sub i32 0, %73
  %75 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 24), align 16
  %76 = and i32 %74, 65535
  %77 = shl i32 %75, 16
  %78 = or i32 %77, %76
  %79 = insertelement <8 x i32> undef, i32 %78, i32 0
  %80 = shufflevector <8 x i32> %79, <8 x i32> undef, <8 x i32> zeroinitializer
  %81 = sub i32 0, %75
  %82 = and i32 %81, 65535
  %83 = shl nuw i32 %76, 16
  %84 = or i32 %83, %82
  %85 = insertelement <8 x i32> undef, i32 %84, i32 0
  %86 = shufflevector <8 x i32> %85, <8 x i32> undef, <8 x i32> zeroinitializer
  %87 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %88 = trunc i32 %87 to i16
  %89 = and i32 %87, 65535
  %90 = shl nuw i32 %89, 16
  %91 = or i32 %90, %89
  %92 = insertelement <8 x i32> undef, i32 %91, i32 0
  %93 = shufflevector <8 x i32> %92, <8 x i32> undef, <8 x i32> zeroinitializer
  %94 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 16), align 16
  %95 = sub i32 0, %94
  %96 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 48), align 16
  %97 = and i32 %95, 65535
  %98 = and i32 %96, 65535
  %99 = shl nuw i32 %98, 16
  %100 = or i32 %99, %97
  %101 = insertelement <8 x i32> undef, i32 %100, i32 0
  %102 = shufflevector <8 x i32> %101, <8 x i32> undef, <8 x i32> zeroinitializer
  %103 = shl i32 %94, 16
  %104 = or i32 %98, %103
  %105 = insertelement <8 x i32> undef, i32 %104, i32 0
  %106 = shufflevector <8 x i32> %105, <8 x i32> undef, <8 x i32> zeroinitializer
  %107 = sub i32 0, %87
  %108 = and i32 %107, 65535
  %109 = or i32 %90, %108
  %110 = insertelement <8 x i32> undef, i32 %109, i32 0
  %111 = shufflevector <8 x i32> %110, <8 x i32> undef, <8 x i32> zeroinitializer
  %112 = bitcast <4 x i64>* %0 to <16 x i16>*
  %113 = load <16 x i16>, <16 x i16>* %112, align 32
  %114 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 4
  %115 = bitcast <4 x i64>* %114 to <16 x i16>*
  %116 = load <16 x i16>, <16 x i16>* %115, align 32
  %117 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 2
  %118 = bitcast <4 x i64>* %117 to <16 x i16>*
  %119 = load <16 x i16>, <16 x i16>* %118, align 32
  %120 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 6
  %121 = bitcast <4 x i64>* %120 to <16 x i16>*
  %122 = load <16 x i16>, <16 x i16>* %121, align 32
  %123 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 1
  %124 = bitcast <4 x i64>* %123 to <16 x i16>*
  %125 = load <16 x i16>, <16 x i16>* %124, align 32
  %126 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 5
  %127 = bitcast <4 x i64>* %126 to <16 x i16>*
  %128 = load <16 x i16>, <16 x i16>* %127, align 32
  %129 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 3
  %130 = bitcast <4 x i64>* %129 to <16 x i16>*
  %131 = load <16 x i16>, <16 x i16>* %130, align 32
  %132 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 7
  %133 = bitcast <4 x i64>* %132 to <16 x i16>*
  %134 = load <16 x i16>, <16 x i16>* %133, align 32
  %135 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 63), align 4
  %136 = trunc i32 %135 to i16
  %137 = shl i16 %136, 3
  %138 = insertelement <16 x i16> undef, i16 %137, i32 0
  %139 = shufflevector <16 x i16> %138, <16 x i16> undef, <16 x i32> zeroinitializer
  %140 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 1), align 4
  %141 = trunc i32 %140 to i16
  %142 = shl i16 %141, 3
  %143 = insertelement <16 x i16> undef, i16 %142, i32 0
  %144 = shufflevector <16 x i16> %143, <16 x i16> undef, <16 x i32> zeroinitializer
  %145 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %125, <16 x i16> %139) #9
  %146 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %125, <16 x i16> %144) #9
  %147 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 57), align 4
  %148 = trunc i32 %147 to i16
  %149 = shl i16 %148, 3
  %150 = sub i16 0, %149
  %151 = insertelement <16 x i16> undef, i16 %150, i32 0
  %152 = shufflevector <16 x i16> %151, <16 x i16> undef, <16 x i32> zeroinitializer
  %153 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 7), align 4
  %154 = trunc i32 %153 to i16
  %155 = shl i16 %154, 3
  %156 = insertelement <16 x i16> undef, i16 %155, i32 0
  %157 = shufflevector <16 x i16> %156, <16 x i16> undef, <16 x i32> zeroinitializer
  %158 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %134, <16 x i16> %152) #9
  %159 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %134, <16 x i16> %157) #9
  %160 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 59), align 4
  %161 = trunc i32 %160 to i16
  %162 = shl i16 %161, 3
  %163 = insertelement <16 x i16> undef, i16 %162, i32 0
  %164 = shufflevector <16 x i16> %163, <16 x i16> undef, <16 x i32> zeroinitializer
  %165 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 5), align 4
  %166 = trunc i32 %165 to i16
  %167 = shl i16 %166, 3
  %168 = insertelement <16 x i16> undef, i16 %167, i32 0
  %169 = shufflevector <16 x i16> %168, <16 x i16> undef, <16 x i32> zeroinitializer
  %170 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %128, <16 x i16> %164) #9
  %171 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %128, <16 x i16> %169) #9
  %172 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 61), align 4
  %173 = trunc i32 %172 to i16
  %174 = shl i16 %173, 3
  %175 = sub i16 0, %174
  %176 = insertelement <16 x i16> undef, i16 %175, i32 0
  %177 = shufflevector <16 x i16> %176, <16 x i16> undef, <16 x i32> zeroinitializer
  %178 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 3), align 4
  %179 = trunc i32 %178 to i16
  %180 = shl i16 %179, 3
  %181 = insertelement <16 x i16> undef, i16 %180, i32 0
  %182 = shufflevector <16 x i16> %181, <16 x i16> undef, <16 x i32> zeroinitializer
  %183 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %131, <16 x i16> %177) #9
  %184 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %131, <16 x i16> %182) #9
  %185 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 62), align 8
  %186 = trunc i32 %185 to i16
  %187 = shl i16 %186, 3
  %188 = insertelement <16 x i16> undef, i16 %187, i32 0
  %189 = shufflevector <16 x i16> %188, <16 x i16> undef, <16 x i32> zeroinitializer
  %190 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 2), align 8
  %191 = trunc i32 %190 to i16
  %192 = shl i16 %191, 3
  %193 = insertelement <16 x i16> undef, i16 %192, i32 0
  %194 = shufflevector <16 x i16> %193, <16 x i16> undef, <16 x i32> zeroinitializer
  %195 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %119, <16 x i16> %189) #9
  %196 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %119, <16 x i16> %194) #9
  %197 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 58), align 8
  %198 = trunc i32 %197 to i16
  %199 = shl i16 %198, 3
  %200 = sub i16 0, %199
  %201 = insertelement <16 x i16> undef, i16 %200, i32 0
  %202 = shufflevector <16 x i16> %201, <16 x i16> undef, <16 x i32> zeroinitializer
  %203 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 6), align 8
  %204 = trunc i32 %203 to i16
  %205 = shl i16 %204, 3
  %206 = insertelement <16 x i16> undef, i16 %205, i32 0
  %207 = shufflevector <16 x i16> %206, <16 x i16> undef, <16 x i32> zeroinitializer
  %208 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %122, <16 x i16> %202) #9
  %209 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %122, <16 x i16> %207) #9
  %210 = shl i16 %8, 3
  %211 = insertelement <16 x i16> undef, i16 %210, i32 0
  %212 = shufflevector <16 x i16> %211, <16 x i16> undef, <16 x i32> zeroinitializer
  %213 = shl i16 %5, 3
  %214 = insertelement <16 x i16> undef, i16 %213, i32 0
  %215 = shufflevector <16 x i16> %214, <16 x i16> undef, <16 x i32> zeroinitializer
  %216 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %116, <16 x i16> %212) #9
  %217 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %116, <16 x i16> %215) #9
  %218 = sext i8 %2 to i32
  %219 = shufflevector <16 x i16> %145, <16 x i16> %146, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %220 = shufflevector <16 x i16> %145, <16 x i16> %146, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %221 = bitcast <8 x i32> %14 to <16 x i16>
  %222 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %219, <16 x i16> %221) #9
  %223 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %220, <16 x i16> %221) #9
  %224 = bitcast <8 x i32> %18 to <16 x i16>
  %225 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %219, <16 x i16> %224) #9
  %226 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %220, <16 x i16> %224) #9
  %227 = add <8 x i32> %222, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %228 = add <8 x i32> %223, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %229 = add <8 x i32> %225, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %230 = add <8 x i32> %226, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %231 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %227, i32 %218) #9
  %232 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %228, i32 %218) #9
  %233 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %229, i32 %218) #9
  %234 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %230, i32 %218) #9
  %235 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %231, <8 x i32> %232) #9
  %236 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %233, <8 x i32> %234) #9
  %237 = shufflevector <16 x i16> %158, <16 x i16> %159, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %238 = shufflevector <16 x i16> %158, <16 x i16> %159, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %239 = bitcast <8 x i32> %32 to <16 x i16>
  %240 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %237, <16 x i16> %239) #9
  %241 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %238, <16 x i16> %239) #9
  %242 = bitcast <8 x i32> %26 to <16 x i16>
  %243 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %237, <16 x i16> %242) #9
  %244 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %238, <16 x i16> %242) #9
  %245 = add <8 x i32> %240, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %246 = add <8 x i32> %241, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %247 = add <8 x i32> %243, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %248 = add <8 x i32> %244, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %249 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %245, i32 %218) #9
  %250 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %246, i32 %218) #9
  %251 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %247, i32 %218) #9
  %252 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %248, i32 %218) #9
  %253 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %249, <8 x i32> %250) #9
  %254 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %251, <8 x i32> %252) #9
  %255 = shufflevector <16 x i16> %170, <16 x i16> %171, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %256 = shufflevector <16 x i16> %170, <16 x i16> %171, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %257 = bitcast <8 x i32> %41 to <16 x i16>
  %258 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %255, <16 x i16> %257) #9
  %259 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %256, <16 x i16> %257) #9
  %260 = bitcast <8 x i32> %45 to <16 x i16>
  %261 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %255, <16 x i16> %260) #9
  %262 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %256, <16 x i16> %260) #9
  %263 = add <8 x i32> %258, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %264 = add <8 x i32> %259, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %265 = add <8 x i32> %261, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %266 = add <8 x i32> %262, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %267 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %263, i32 %218) #9
  %268 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %264, i32 %218) #9
  %269 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %265, i32 %218) #9
  %270 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %266, i32 %218) #9
  %271 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %267, <8 x i32> %268) #9
  %272 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %269, <8 x i32> %270) #9
  %273 = shufflevector <16 x i16> %183, <16 x i16> %184, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %274 = shufflevector <16 x i16> %183, <16 x i16> %184, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %275 = bitcast <8 x i32> %59 to <16 x i16>
  %276 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %273, <16 x i16> %275) #9
  %277 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %274, <16 x i16> %275) #9
  %278 = bitcast <8 x i32> %53 to <16 x i16>
  %279 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %273, <16 x i16> %278) #9
  %280 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %274, <16 x i16> %278) #9
  %281 = add <8 x i32> %276, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %282 = add <8 x i32> %277, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %283 = add <8 x i32> %279, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %284 = add <8 x i32> %280, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %285 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %281, i32 %218) #9
  %286 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %282, i32 %218) #9
  %287 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %283, i32 %218) #9
  %288 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %284, i32 %218) #9
  %289 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %285, <8 x i32> %286) #9
  %290 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %287, <8 x i32> %288) #9
  %291 = shufflevector <16 x i16> %195, <16 x i16> %196, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %292 = shufflevector <16 x i16> %195, <16 x i16> %196, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %293 = bitcast <8 x i32> %68 to <16 x i16>
  %294 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %291, <16 x i16> %293) #9
  %295 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %292, <16 x i16> %293) #9
  %296 = bitcast <8 x i32> %72 to <16 x i16>
  %297 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %291, <16 x i16> %296) #9
  %298 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %292, <16 x i16> %296) #9
  %299 = add <8 x i32> %294, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %300 = add <8 x i32> %295, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %301 = add <8 x i32> %297, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %302 = add <8 x i32> %298, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %303 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %299, i32 %218) #9
  %304 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %300, i32 %218) #9
  %305 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %301, i32 %218) #9
  %306 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %302, i32 %218) #9
  %307 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %303, <8 x i32> %304) #9
  %308 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %305, <8 x i32> %306) #9
  %309 = shufflevector <16 x i16> %208, <16 x i16> %209, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %310 = shufflevector <16 x i16> %208, <16 x i16> %209, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %311 = bitcast <8 x i32> %86 to <16 x i16>
  %312 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %309, <16 x i16> %311) #9
  %313 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %310, <16 x i16> %311) #9
  %314 = bitcast <8 x i32> %80 to <16 x i16>
  %315 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %309, <16 x i16> %314) #9
  %316 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %310, <16 x i16> %314) #9
  %317 = add <8 x i32> %312, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %318 = add <8 x i32> %313, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %319 = add <8 x i32> %315, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %320 = add <8 x i32> %316, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %321 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %317, i32 %218) #9
  %322 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %318, i32 %218) #9
  %323 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %319, i32 %218) #9
  %324 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %320, i32 %218) #9
  %325 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %321, <8 x i32> %322) #9
  %326 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %323, <8 x i32> %324) #9
  %327 = shl i16 %88, 3
  %328 = insertelement <16 x i16> undef, i16 %327, i32 0
  %329 = shufflevector <16 x i16> %328, <16 x i16> undef, <16 x i32> zeroinitializer
  %330 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %113, <16 x i16> %329) #9
  %331 = shufflevector <16 x i16> %216, <16 x i16> %217, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %332 = shufflevector <16 x i16> %216, <16 x i16> %217, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %333 = bitcast <8 x i32> %102 to <16 x i16>
  %334 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %331, <16 x i16> %333) #9
  %335 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %332, <16 x i16> %333) #9
  %336 = bitcast <8 x i32> %106 to <16 x i16>
  %337 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %331, <16 x i16> %336) #9
  %338 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %332, <16 x i16> %336) #9
  %339 = add <8 x i32> %334, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %340 = add <8 x i32> %335, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %341 = add <8 x i32> %337, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %342 = add <8 x i32> %338, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %343 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %339, i32 %218) #9
  %344 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %340, i32 %218) #9
  %345 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %341, i32 %218) #9
  %346 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %342, i32 %218) #9
  %347 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %343, <8 x i32> %344) #9
  %348 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %345, <8 x i32> %346) #9
  %349 = sub i32 0, %62
  %350 = and i32 %349, 65535
  %351 = shl nuw i32 %63, 16
  %352 = or i32 %351, %350
  %353 = insertelement <8 x i32> undef, i32 %352, i32 0
  %354 = shufflevector <8 x i32> %353, <8 x i32> undef, <8 x i32> zeroinitializer
  %355 = and i32 %75, 65535
  %356 = shl nuw i32 %355, 16
  %357 = or i32 %356, %76
  %358 = insertelement <8 x i32> undef, i32 %357, i32 0
  %359 = shufflevector <8 x i32> %358, <8 x i32> undef, <8 x i32> zeroinitializer
  %360 = shl i32 %73, 16
  %361 = or i32 %355, %360
  %362 = insertelement <8 x i32> undef, i32 %361, i32 0
  %363 = shufflevector <8 x i32> %362, <8 x i32> undef, <8 x i32> zeroinitializer
  %364 = shufflevector <16 x i16> %235, <16 x i16> %236, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %365 = shufflevector <16 x i16> %235, <16 x i16> %236, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %366 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %364, <16 x i16> %293) #9
  %367 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %365, <16 x i16> %293) #9
  %368 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %364, <16 x i16> %296) #9
  %369 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %365, <16 x i16> %296) #9
  %370 = add <8 x i32> %366, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %371 = add <8 x i32> %367, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %372 = add <8 x i32> %368, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %373 = add <8 x i32> %369, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %374 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %370, i32 %218) #9
  %375 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %371, i32 %218) #9
  %376 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %372, i32 %218) #9
  %377 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %373, i32 %218) #9
  %378 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %374, <8 x i32> %375) #9
  %379 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %376, <8 x i32> %377) #9
  %380 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %219, <16 x i16> %293) #9
  %381 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %220, <16 x i16> %293) #9
  %382 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %219, <16 x i16> %296) #9
  %383 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %220, <16 x i16> %296) #9
  %384 = add <8 x i32> %380, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %385 = add <8 x i32> %381, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %386 = add <8 x i32> %382, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %387 = add <8 x i32> %383, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %388 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %384, i32 %218) #9
  %389 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %385, i32 %218) #9
  %390 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %386, i32 %218) #9
  %391 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %387, i32 %218) #9
  %392 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %388, <8 x i32> %389) #9
  %393 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %390, <8 x i32> %391) #9
  %394 = bitcast <8 x i32> %354 to <16 x i16>
  %395 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %237, <16 x i16> %394) #9
  %396 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %238, <16 x i16> %394) #9
  %397 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %237, <16 x i16> %293) #9
  %398 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %238, <16 x i16> %293) #9
  %399 = add <8 x i32> %395, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %400 = add <8 x i32> %396, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %401 = add <8 x i32> %397, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %402 = add <8 x i32> %398, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %403 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %399, i32 %218) #9
  %404 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %400, i32 %218) #9
  %405 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %401, i32 %218) #9
  %406 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %402, i32 %218) #9
  %407 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %403, <8 x i32> %404) #9
  %408 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %405, <8 x i32> %406) #9
  %409 = shufflevector <16 x i16> %253, <16 x i16> %254, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %410 = shufflevector <16 x i16> %253, <16 x i16> %254, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %411 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %409, <16 x i16> %394) #9
  %412 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %410, <16 x i16> %394) #9
  %413 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %409, <16 x i16> %293) #9
  %414 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %410, <16 x i16> %293) #9
  %415 = add <8 x i32> %411, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %416 = add <8 x i32> %412, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %417 = add <8 x i32> %413, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %418 = add <8 x i32> %414, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %419 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %415, i32 %218) #9
  %420 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %416, i32 %218) #9
  %421 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %417, i32 %218) #9
  %422 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %418, i32 %218) #9
  %423 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %419, <8 x i32> %420) #9
  %424 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %421, <8 x i32> %422) #9
  %425 = shufflevector <16 x i16> %271, <16 x i16> %272, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %426 = shufflevector <16 x i16> %271, <16 x i16> %272, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %427 = bitcast <8 x i32> %359 to <16 x i16>
  %428 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %425, <16 x i16> %427) #9
  %429 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %426, <16 x i16> %427) #9
  %430 = bitcast <8 x i32> %363 to <16 x i16>
  %431 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %425, <16 x i16> %430) #9
  %432 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %426, <16 x i16> %430) #9
  %433 = add <8 x i32> %428, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %434 = add <8 x i32> %429, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %435 = add <8 x i32> %431, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %436 = add <8 x i32> %432, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %437 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %433, i32 %218) #9
  %438 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %434, i32 %218) #9
  %439 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %435, i32 %218) #9
  %440 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %436, i32 %218) #9
  %441 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %437, <8 x i32> %438) #9
  %442 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %439, <8 x i32> %440) #9
  %443 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %255, <16 x i16> %427) #9
  %444 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %256, <16 x i16> %427) #9
  %445 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %255, <16 x i16> %430) #9
  %446 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %256, <16 x i16> %430) #9
  %447 = add <8 x i32> %443, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %448 = add <8 x i32> %444, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %449 = add <8 x i32> %445, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %450 = add <8 x i32> %446, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %451 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %447, i32 %218) #9
  %452 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %448, i32 %218) #9
  %453 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %449, i32 %218) #9
  %454 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %450, i32 %218) #9
  %455 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %451, <8 x i32> %452) #9
  %456 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %453, <8 x i32> %454) #9
  %457 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %273, <16 x i16> %311) #9
  %458 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %274, <16 x i16> %311) #9
  %459 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %273, <16 x i16> %427) #9
  %460 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %274, <16 x i16> %427) #9
  %461 = add <8 x i32> %457, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %462 = add <8 x i32> %458, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %463 = add <8 x i32> %459, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %464 = add <8 x i32> %460, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %465 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %461, i32 %218) #9
  %466 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %462, i32 %218) #9
  %467 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %463, i32 %218) #9
  %468 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %464, i32 %218) #9
  %469 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %465, <8 x i32> %466) #9
  %470 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %467, <8 x i32> %468) #9
  %471 = shufflevector <16 x i16> %289, <16 x i16> %290, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %472 = shufflevector <16 x i16> %289, <16 x i16> %290, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %473 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %471, <16 x i16> %311) #9
  %474 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %472, <16 x i16> %311) #9
  %475 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %471, <16 x i16> %427) #9
  %476 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %472, <16 x i16> %427) #9
  %477 = add <8 x i32> %473, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %478 = add <8 x i32> %474, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %479 = add <8 x i32> %475, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %480 = add <8 x i32> %476, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %481 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %477, i32 %218) #9
  %482 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %478, i32 %218) #9
  %483 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %479, i32 %218) #9
  %484 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %480, i32 %218) #9
  %485 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %481, <8 x i32> %482) #9
  %486 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %483, <8 x i32> %484) #9
  %487 = sub i32 0, %96
  %488 = and i32 %487, 65535
  %489 = shl nuw i32 %97, 16
  %490 = or i32 %489, %488
  %491 = insertelement <8 x i32> undef, i32 %490, i32 0
  %492 = shufflevector <8 x i32> %491, <8 x i32> undef, <8 x i32> zeroinitializer
  %493 = shufflevector <16 x i16> %307, <16 x i16> %308, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %494 = shufflevector <16 x i16> %307, <16 x i16> %308, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %495 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %493, <16 x i16> %333) #9
  %496 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %494, <16 x i16> %333) #9
  %497 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %493, <16 x i16> %336) #9
  %498 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %494, <16 x i16> %336) #9
  %499 = add <8 x i32> %495, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %500 = add <8 x i32> %496, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %501 = add <8 x i32> %497, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %502 = add <8 x i32> %498, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %503 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %499, i32 %218) #9
  %504 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %500, i32 %218) #9
  %505 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %501, i32 %218) #9
  %506 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %502, i32 %218) #9
  %507 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %503, <8 x i32> %504) #9
  %508 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %505, <8 x i32> %506) #9
  %509 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %291, <16 x i16> %333) #9
  %510 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %292, <16 x i16> %333) #9
  %511 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %291, <16 x i16> %336) #9
  %512 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %292, <16 x i16> %336) #9
  %513 = add <8 x i32> %509, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %514 = add <8 x i32> %510, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %515 = add <8 x i32> %511, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %516 = add <8 x i32> %512, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %517 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %513, i32 %218) #9
  %518 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %514, i32 %218) #9
  %519 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %515, i32 %218) #9
  %520 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %516, i32 %218) #9
  %521 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %517, <8 x i32> %518) #9
  %522 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %519, <8 x i32> %520) #9
  %523 = bitcast <8 x i32> %492 to <16 x i16>
  %524 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %309, <16 x i16> %523) #9
  %525 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %310, <16 x i16> %523) #9
  %526 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %309, <16 x i16> %333) #9
  %527 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %310, <16 x i16> %333) #9
  %528 = add <8 x i32> %524, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %529 = add <8 x i32> %525, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %530 = add <8 x i32> %526, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %531 = add <8 x i32> %527, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %532 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %528, i32 %218) #9
  %533 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %529, i32 %218) #9
  %534 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %530, i32 %218) #9
  %535 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %531, i32 %218) #9
  %536 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %532, <8 x i32> %533) #9
  %537 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %534, <8 x i32> %535) #9
  %538 = shufflevector <16 x i16> %325, <16 x i16> %326, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %539 = shufflevector <16 x i16> %325, <16 x i16> %326, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %540 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %538, <16 x i16> %523) #9
  %541 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %539, <16 x i16> %523) #9
  %542 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %538, <16 x i16> %333) #9
  %543 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %539, <16 x i16> %333) #9
  %544 = add <8 x i32> %540, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %545 = add <8 x i32> %541, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %546 = add <8 x i32> %542, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %547 = add <8 x i32> %543, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %548 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %544, i32 %218) #9
  %549 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %545, i32 %218) #9
  %550 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %546, i32 %218) #9
  %551 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %547, i32 %218) #9
  %552 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %548, <8 x i32> %549) #9
  %553 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %550, <8 x i32> %551) #9
  %554 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %145, <16 x i16> %158) #9
  %555 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %145, <16 x i16> %158) #9
  %556 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %235, <16 x i16> %253) #9
  %557 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %235, <16 x i16> %253) #9
  %558 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %378, <16 x i16> %423) #9
  %559 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %378, <16 x i16> %423) #9
  %560 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %392, <16 x i16> %407) #9
  %561 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %392, <16 x i16> %407) #9
  %562 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %183, <16 x i16> %170) #9
  %563 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %183, <16 x i16> %170) #9
  %564 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %289, <16 x i16> %271) #9
  %565 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %289, <16 x i16> %271) #9
  %566 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %485, <16 x i16> %441) #9
  %567 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %485, <16 x i16> %441) #9
  %568 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %469, <16 x i16> %455) #9
  %569 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %469, <16 x i16> %455) #9
  %570 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %184, <16 x i16> %171) #9
  %571 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %184, <16 x i16> %171) #9
  %572 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %290, <16 x i16> %272) #9
  %573 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %290, <16 x i16> %272) #9
  %574 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %486, <16 x i16> %442) #9
  %575 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %486, <16 x i16> %442) #9
  %576 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %470, <16 x i16> %456) #9
  %577 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %470, <16 x i16> %456) #9
  %578 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %146, <16 x i16> %159) #9
  %579 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %146, <16 x i16> %159) #9
  %580 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %236, <16 x i16> %254) #9
  %581 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %236, <16 x i16> %254) #9
  %582 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %379, <16 x i16> %424) #9
  %583 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %379, <16 x i16> %424) #9
  %584 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %393, <16 x i16> %408) #9
  %585 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %393, <16 x i16> %408) #9
  %586 = shufflevector <16 x i16> %347, <16 x i16> %348, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %587 = shufflevector <16 x i16> %347, <16 x i16> %348, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %588 = bitcast <8 x i32> %111 to <16 x i16>
  %589 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %586, <16 x i16> %588) #9
  %590 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %587, <16 x i16> %588) #9
  %591 = bitcast <8 x i32> %93 to <16 x i16>
  %592 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %586, <16 x i16> %591) #9
  %593 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %587, <16 x i16> %591) #9
  %594 = add <8 x i32> %589, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %595 = add <8 x i32> %590, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %596 = add <8 x i32> %592, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %597 = add <8 x i32> %593, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %598 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %594, i32 %218) #9
  %599 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %595, i32 %218) #9
  %600 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %596, i32 %218) #9
  %601 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %597, i32 %218) #9
  %602 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %598, <8 x i32> %599) #9
  %603 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %600, <8 x i32> %601) #9
  %604 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %331, <16 x i16> %588) #9
  %605 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %332, <16 x i16> %588) #9
  %606 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %331, <16 x i16> %591) #9
  %607 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %332, <16 x i16> %591) #9
  %608 = add <8 x i32> %604, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %609 = add <8 x i32> %605, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %610 = add <8 x i32> %606, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %611 = add <8 x i32> %607, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %612 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %608, i32 %218) #9
  %613 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %609, i32 %218) #9
  %614 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %610, i32 %218) #9
  %615 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %611, i32 %218) #9
  %616 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %612, <8 x i32> %613) #9
  %617 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %614, <8 x i32> %615) #9
  %618 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %195, <16 x i16> %208) #9
  %619 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %195, <16 x i16> %208) #9
  %620 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %307, <16 x i16> %325) #9
  %621 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %307, <16 x i16> %325) #9
  %622 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %507, <16 x i16> %552) #9
  %623 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %507, <16 x i16> %552) #9
  %624 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %521, <16 x i16> %536) #9
  %625 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %521, <16 x i16> %536) #9
  %626 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %196, <16 x i16> %209) #9
  %627 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %196, <16 x i16> %209) #9
  %628 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %308, <16 x i16> %326) #9
  %629 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %308, <16 x i16> %326) #9
  %630 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %508, <16 x i16> %553) #9
  %631 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %508, <16 x i16> %553) #9
  %632 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %522, <16 x i16> %537) #9
  %633 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %522, <16 x i16> %537) #9
  %634 = shufflevector <16 x i16> %561, <16 x i16> %585, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %635 = shufflevector <16 x i16> %561, <16 x i16> %585, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %636 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %634, <16 x i16> %333) #9
  %637 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %635, <16 x i16> %333) #9
  %638 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %634, <16 x i16> %336) #9
  %639 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %635, <16 x i16> %336) #9
  %640 = add <8 x i32> %636, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %641 = add <8 x i32> %637, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %642 = add <8 x i32> %638, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %643 = add <8 x i32> %639, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %644 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %640, i32 %218) #9
  %645 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %641, i32 %218) #9
  %646 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %642, i32 %218) #9
  %647 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %643, i32 %218) #9
  %648 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %644, <8 x i32> %645) #9
  %649 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %646, <8 x i32> %647) #9
  %650 = shufflevector <16 x i16> %559, <16 x i16> %583, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %651 = shufflevector <16 x i16> %559, <16 x i16> %583, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %652 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %650, <16 x i16> %333) #9
  %653 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %651, <16 x i16> %333) #9
  %654 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %650, <16 x i16> %336) #9
  %655 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %651, <16 x i16> %336) #9
  %656 = add <8 x i32> %652, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %657 = add <8 x i32> %653, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %658 = add <8 x i32> %654, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %659 = add <8 x i32> %655, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %660 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %656, i32 %218) #9
  %661 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %657, i32 %218) #9
  %662 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %658, i32 %218) #9
  %663 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %659, i32 %218) #9
  %664 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %660, <8 x i32> %661) #9
  %665 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %662, <8 x i32> %663) #9
  %666 = shufflevector <16 x i16> %557, <16 x i16> %581, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %667 = shufflevector <16 x i16> %557, <16 x i16> %581, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %668 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %666, <16 x i16> %333) #9
  %669 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %667, <16 x i16> %333) #9
  %670 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %666, <16 x i16> %336) #9
  %671 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %667, <16 x i16> %336) #9
  %672 = add <8 x i32> %668, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %673 = add <8 x i32> %669, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %674 = add <8 x i32> %670, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %675 = add <8 x i32> %671, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %676 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %672, i32 %218) #9
  %677 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %673, i32 %218) #9
  %678 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %674, i32 %218) #9
  %679 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %675, i32 %218) #9
  %680 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %676, <8 x i32> %677) #9
  %681 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %678, <8 x i32> %679) #9
  %682 = shufflevector <16 x i16> %555, <16 x i16> %579, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %683 = shufflevector <16 x i16> %555, <16 x i16> %579, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %684 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %682, <16 x i16> %333) #9
  %685 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %683, <16 x i16> %333) #9
  %686 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %682, <16 x i16> %336) #9
  %687 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %683, <16 x i16> %336) #9
  %688 = add <8 x i32> %684, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %689 = add <8 x i32> %685, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %690 = add <8 x i32> %686, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %691 = add <8 x i32> %687, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %692 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %688, i32 %218) #9
  %693 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %689, i32 %218) #9
  %694 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %690, i32 %218) #9
  %695 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %691, i32 %218) #9
  %696 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %692, <8 x i32> %693) #9
  %697 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %694, <8 x i32> %695) #9
  %698 = shufflevector <16 x i16> %563, <16 x i16> %571, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %699 = shufflevector <16 x i16> %563, <16 x i16> %571, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %700 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %698, <16 x i16> %523) #9
  %701 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %699, <16 x i16> %523) #9
  %702 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %698, <16 x i16> %333) #9
  %703 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %699, <16 x i16> %333) #9
  %704 = add <8 x i32> %700, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %705 = add <8 x i32> %701, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %706 = add <8 x i32> %702, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %707 = add <8 x i32> %703, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %708 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %704, i32 %218) #9
  %709 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %705, i32 %218) #9
  %710 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %706, i32 %218) #9
  %711 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %707, i32 %218) #9
  %712 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %708, <8 x i32> %709) #9
  %713 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %710, <8 x i32> %711) #9
  %714 = shufflevector <16 x i16> %565, <16 x i16> %573, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %715 = shufflevector <16 x i16> %565, <16 x i16> %573, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %716 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %714, <16 x i16> %523) #9
  %717 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %715, <16 x i16> %523) #9
  %718 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %714, <16 x i16> %333) #9
  %719 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %715, <16 x i16> %333) #9
  %720 = add <8 x i32> %716, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %721 = add <8 x i32> %717, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %722 = add <8 x i32> %718, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %723 = add <8 x i32> %719, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %724 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %720, i32 %218) #9
  %725 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %721, i32 %218) #9
  %726 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %722, i32 %218) #9
  %727 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %723, i32 %218) #9
  %728 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %724, <8 x i32> %725) #9
  %729 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %726, <8 x i32> %727) #9
  %730 = shufflevector <16 x i16> %567, <16 x i16> %575, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %731 = shufflevector <16 x i16> %567, <16 x i16> %575, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %732 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %730, <16 x i16> %523) #9
  %733 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %731, <16 x i16> %523) #9
  %734 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %730, <16 x i16> %333) #9
  %735 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %731, <16 x i16> %333) #9
  %736 = add <8 x i32> %732, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %737 = add <8 x i32> %733, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %738 = add <8 x i32> %734, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %739 = add <8 x i32> %735, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %740 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %736, i32 %218) #9
  %741 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %737, i32 %218) #9
  %742 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %738, i32 %218) #9
  %743 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %739, i32 %218) #9
  %744 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %740, <8 x i32> %741) #9
  %745 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %742, <8 x i32> %743) #9
  %746 = shufflevector <16 x i16> %569, <16 x i16> %577, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %747 = shufflevector <16 x i16> %569, <16 x i16> %577, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %748 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %746, <16 x i16> %523) #9
  %749 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %747, <16 x i16> %523) #9
  %750 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %746, <16 x i16> %333) #9
  %751 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %747, <16 x i16> %333) #9
  %752 = add <8 x i32> %748, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %753 = add <8 x i32> %749, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %754 = add <8 x i32> %750, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %755 = add <8 x i32> %751, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %756 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %752, i32 %218) #9
  %757 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %753, i32 %218) #9
  %758 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %754, i32 %218) #9
  %759 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %755, i32 %218) #9
  %760 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %756, <8 x i32> %757) #9
  %761 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %758, <8 x i32> %759) #9
  %762 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %330, <16 x i16> %217) #9
  %763 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %330, <16 x i16> %217) #9
  %764 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %330, <16 x i16> %348) #9
  %765 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %330, <16 x i16> %348) #9
  %766 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %330, <16 x i16> %603) #9
  %767 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %330, <16 x i16> %603) #9
  %768 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %330, <16 x i16> %617) #9
  %769 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %330, <16 x i16> %617) #9
  %770 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %330, <16 x i16> %616) #9
  %771 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %330, <16 x i16> %616) #9
  %772 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %330, <16 x i16> %602) #9
  %773 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %330, <16 x i16> %602) #9
  %774 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %330, <16 x i16> %347) #9
  %775 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %330, <16 x i16> %347) #9
  %776 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %330, <16 x i16> %216) #9
  %777 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %330, <16 x i16> %216) #9
  %778 = shufflevector <16 x i16> %625, <16 x i16> %633, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %779 = shufflevector <16 x i16> %625, <16 x i16> %633, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %780 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %778, <16 x i16> %588) #9
  %781 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %779, <16 x i16> %588) #9
  %782 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %778, <16 x i16> %591) #9
  %783 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %779, <16 x i16> %591) #9
  %784 = add <8 x i32> %780, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %785 = add <8 x i32> %781, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %786 = add <8 x i32> %782, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %787 = add <8 x i32> %783, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %788 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %784, i32 %218) #9
  %789 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %785, i32 %218) #9
  %790 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %786, i32 %218) #9
  %791 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %787, i32 %218) #9
  %792 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %788, <8 x i32> %789) #9
  %793 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %790, <8 x i32> %791) #9
  %794 = shufflevector <16 x i16> %623, <16 x i16> %631, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %795 = shufflevector <16 x i16> %623, <16 x i16> %631, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %796 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %794, <16 x i16> %588) #9
  %797 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %795, <16 x i16> %588) #9
  %798 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %794, <16 x i16> %591) #9
  %799 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %795, <16 x i16> %591) #9
  %800 = add <8 x i32> %796, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %801 = add <8 x i32> %797, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %802 = add <8 x i32> %798, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %803 = add <8 x i32> %799, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %804 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %800, i32 %218) #9
  %805 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %801, i32 %218) #9
  %806 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %802, i32 %218) #9
  %807 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %803, i32 %218) #9
  %808 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %804, <8 x i32> %805) #9
  %809 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %806, <8 x i32> %807) #9
  %810 = shufflevector <16 x i16> %621, <16 x i16> %629, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %811 = shufflevector <16 x i16> %621, <16 x i16> %629, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %812 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %810, <16 x i16> %588) #9
  %813 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %811, <16 x i16> %588) #9
  %814 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %810, <16 x i16> %591) #9
  %815 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %811, <16 x i16> %591) #9
  %816 = add <8 x i32> %812, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %817 = add <8 x i32> %813, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %818 = add <8 x i32> %814, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %819 = add <8 x i32> %815, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %820 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %816, i32 %218) #9
  %821 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %817, i32 %218) #9
  %822 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %818, i32 %218) #9
  %823 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %819, i32 %218) #9
  %824 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %820, <8 x i32> %821) #9
  %825 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %822, <8 x i32> %823) #9
  %826 = shufflevector <16 x i16> %619, <16 x i16> %627, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %827 = shufflevector <16 x i16> %619, <16 x i16> %627, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %828 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %826, <16 x i16> %588) #9
  %829 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %827, <16 x i16> %588) #9
  %830 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %826, <16 x i16> %591) #9
  %831 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %827, <16 x i16> %591) #9
  %832 = add <8 x i32> %828, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %833 = add <8 x i32> %829, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %834 = add <8 x i32> %830, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %835 = add <8 x i32> %831, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %836 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %832, i32 %218) #9
  %837 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %833, i32 %218) #9
  %838 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %834, i32 %218) #9
  %839 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %835, i32 %218) #9
  %840 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %836, <8 x i32> %837) #9
  %841 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %838, <8 x i32> %839) #9
  %842 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %554, <16 x i16> %562) #9
  %843 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %554, <16 x i16> %562) #9
  %844 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %556, <16 x i16> %564) #9
  %845 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %556, <16 x i16> %564) #9
  %846 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %558, <16 x i16> %566) #9
  %847 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %558, <16 x i16> %566) #9
  %848 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %560, <16 x i16> %568) #9
  %849 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %560, <16 x i16> %568) #9
  %850 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %648, <16 x i16> %760) #9
  %851 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %648, <16 x i16> %760) #9
  %852 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %664, <16 x i16> %744) #9
  %853 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %664, <16 x i16> %744) #9
  %854 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %680, <16 x i16> %728) #9
  %855 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %680, <16 x i16> %728) #9
  %856 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %696, <16 x i16> %712) #9
  %857 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %696, <16 x i16> %712) #9
  %858 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %578, <16 x i16> %570) #9
  %859 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %578, <16 x i16> %570) #9
  %860 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %580, <16 x i16> %572) #9
  %861 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %580, <16 x i16> %572) #9
  %862 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %582, <16 x i16> %574) #9
  %863 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %582, <16 x i16> %574) #9
  %864 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %584, <16 x i16> %576) #9
  %865 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %584, <16 x i16> %576) #9
  %866 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %649, <16 x i16> %761) #9
  %867 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %649, <16 x i16> %761) #9
  %868 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %665, <16 x i16> %745) #9
  %869 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %665, <16 x i16> %745) #9
  %870 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %681, <16 x i16> %729) #9
  %871 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %681, <16 x i16> %729) #9
  %872 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %697, <16 x i16> %713) #9
  %873 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %697, <16 x i16> %713) #9
  %874 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %762, <16 x i16> %626) #9
  %875 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %762, <16 x i16> %626) #9
  %876 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %764, <16 x i16> %628) #9
  %877 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %764, <16 x i16> %628) #9
  %878 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %766, <16 x i16> %630) #9
  %879 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %766, <16 x i16> %630) #9
  %880 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %768, <16 x i16> %632) #9
  %881 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %768, <16 x i16> %632) #9
  %882 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %770, <16 x i16> %793) #9
  %883 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %770, <16 x i16> %793) #9
  %884 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %772, <16 x i16> %809) #9
  %885 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %772, <16 x i16> %809) #9
  %886 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %774, <16 x i16> %825) #9
  %887 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %774, <16 x i16> %825) #9
  %888 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %776, <16 x i16> %841) #9
  %889 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %776, <16 x i16> %841) #9
  %890 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %777, <16 x i16> %840) #9
  %891 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %777, <16 x i16> %840) #9
  %892 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %775, <16 x i16> %824) #9
  %893 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %775, <16 x i16> %824) #9
  %894 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %773, <16 x i16> %808) #9
  %895 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %773, <16 x i16> %808) #9
  %896 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %771, <16 x i16> %792) #9
  %897 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %771, <16 x i16> %792) #9
  %898 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %769, <16 x i16> %624) #9
  %899 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %769, <16 x i16> %624) #9
  %900 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %767, <16 x i16> %622) #9
  %901 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %767, <16 x i16> %622) #9
  %902 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %765, <16 x i16> %620) #9
  %903 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %765, <16 x i16> %620) #9
  %904 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %763, <16 x i16> %618) #9
  %905 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %763, <16 x i16> %618) #9
  %906 = shufflevector <16 x i16> %857, <16 x i16> %873, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %907 = shufflevector <16 x i16> %857, <16 x i16> %873, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %908 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %906, <16 x i16> %588) #9
  %909 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %907, <16 x i16> %588) #9
  %910 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %906, <16 x i16> %591) #9
  %911 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %907, <16 x i16> %591) #9
  %912 = add <8 x i32> %908, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %913 = add <8 x i32> %909, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %914 = add <8 x i32> %910, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %915 = add <8 x i32> %911, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %916 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %912, i32 %218) #9
  %917 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %913, i32 %218) #9
  %918 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %914, i32 %218) #9
  %919 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %915, i32 %218) #9
  %920 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %916, <8 x i32> %917) #9
  %921 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %918, <8 x i32> %919) #9
  %922 = shufflevector <16 x i16> %855, <16 x i16> %871, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %923 = shufflevector <16 x i16> %855, <16 x i16> %871, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %924 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %922, <16 x i16> %588) #9
  %925 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %923, <16 x i16> %588) #9
  %926 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %922, <16 x i16> %591) #9
  %927 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %923, <16 x i16> %591) #9
  %928 = add <8 x i32> %924, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %929 = add <8 x i32> %925, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %930 = add <8 x i32> %926, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %931 = add <8 x i32> %927, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %932 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %928, i32 %218) #9
  %933 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %929, i32 %218) #9
  %934 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %930, i32 %218) #9
  %935 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %931, i32 %218) #9
  %936 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %932, <8 x i32> %933) #9
  %937 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %934, <8 x i32> %935) #9
  %938 = shufflevector <16 x i16> %853, <16 x i16> %869, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %939 = shufflevector <16 x i16> %853, <16 x i16> %869, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %940 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %938, <16 x i16> %588) #9
  %941 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %939, <16 x i16> %588) #9
  %942 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %938, <16 x i16> %591) #9
  %943 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %939, <16 x i16> %591) #9
  %944 = add <8 x i32> %940, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %945 = add <8 x i32> %941, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %946 = add <8 x i32> %942, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %947 = add <8 x i32> %943, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %948 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %944, i32 %218) #9
  %949 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %945, i32 %218) #9
  %950 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %946, i32 %218) #9
  %951 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %947, i32 %218) #9
  %952 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %948, <8 x i32> %949) #9
  %953 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %950, <8 x i32> %951) #9
  %954 = shufflevector <16 x i16> %851, <16 x i16> %867, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %955 = shufflevector <16 x i16> %851, <16 x i16> %867, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %956 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %954, <16 x i16> %588) #9
  %957 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %955, <16 x i16> %588) #9
  %958 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %954, <16 x i16> %591) #9
  %959 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %955, <16 x i16> %591) #9
  %960 = add <8 x i32> %956, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %961 = add <8 x i32> %957, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %962 = add <8 x i32> %958, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %963 = add <8 x i32> %959, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %964 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %960, i32 %218) #9
  %965 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %961, i32 %218) #9
  %966 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %962, i32 %218) #9
  %967 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %963, i32 %218) #9
  %968 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %964, <8 x i32> %965) #9
  %969 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %966, <8 x i32> %967) #9
  %970 = shufflevector <16 x i16> %849, <16 x i16> %865, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %971 = shufflevector <16 x i16> %849, <16 x i16> %865, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %972 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %970, <16 x i16> %588) #9
  %973 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %971, <16 x i16> %588) #9
  %974 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %970, <16 x i16> %591) #9
  %975 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %971, <16 x i16> %591) #9
  %976 = add <8 x i32> %972, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %977 = add <8 x i32> %973, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %978 = add <8 x i32> %974, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %979 = add <8 x i32> %975, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %980 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %976, i32 %218) #9
  %981 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %977, i32 %218) #9
  %982 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %978, i32 %218) #9
  %983 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %979, i32 %218) #9
  %984 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %980, <8 x i32> %981) #9
  %985 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %982, <8 x i32> %983) #9
  %986 = shufflevector <16 x i16> %847, <16 x i16> %863, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %987 = shufflevector <16 x i16> %847, <16 x i16> %863, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %988 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %986, <16 x i16> %588) #9
  %989 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %987, <16 x i16> %588) #9
  %990 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %986, <16 x i16> %591) #9
  %991 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %987, <16 x i16> %591) #9
  %992 = add <8 x i32> %988, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %993 = add <8 x i32> %989, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %994 = add <8 x i32> %990, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %995 = add <8 x i32> %991, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %996 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %992, i32 %218) #9
  %997 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %993, i32 %218) #9
  %998 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %994, i32 %218) #9
  %999 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %995, i32 %218) #9
  %1000 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %996, <8 x i32> %997) #9
  %1001 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %998, <8 x i32> %999) #9
  %1002 = shufflevector <16 x i16> %845, <16 x i16> %861, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1003 = shufflevector <16 x i16> %845, <16 x i16> %861, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1004 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1002, <16 x i16> %588) #9
  %1005 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1003, <16 x i16> %588) #9
  %1006 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1002, <16 x i16> %591) #9
  %1007 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1003, <16 x i16> %591) #9
  %1008 = add <8 x i32> %1004, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1009 = add <8 x i32> %1005, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1010 = add <8 x i32> %1006, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1011 = add <8 x i32> %1007, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1012 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1008, i32 %218) #9
  %1013 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1009, i32 %218) #9
  %1014 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1010, i32 %218) #9
  %1015 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1011, i32 %218) #9
  %1016 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1012, <8 x i32> %1013) #9
  %1017 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1014, <8 x i32> %1015) #9
  %1018 = shufflevector <16 x i16> %843, <16 x i16> %859, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1019 = shufflevector <16 x i16> %843, <16 x i16> %859, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1020 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1018, <16 x i16> %588) #9
  %1021 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1019, <16 x i16> %588) #9
  %1022 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1018, <16 x i16> %591) #9
  %1023 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1019, <16 x i16> %591) #9
  %1024 = add <8 x i32> %1020, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1025 = add <8 x i32> %1021, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1026 = add <8 x i32> %1022, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1027 = add <8 x i32> %1023, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1028 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1024, i32 %218) #9
  %1029 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1025, i32 %218) #9
  %1030 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1026, i32 %218) #9
  %1031 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1027, i32 %218) #9
  %1032 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1028, <8 x i32> %1029) #9
  %1033 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1030, <8 x i32> %1031) #9
  %1034 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 63
  %1035 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %874, <16 x i16> %858) #9
  %1036 = bitcast <4 x i64>* %1 to <16 x i16>*
  store <16 x i16> %1035, <16 x i16>* %1036, align 32
  %1037 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %874, <16 x i16> %858) #9
  %1038 = bitcast <4 x i64>* %1034 to <16 x i16>*
  store <16 x i16> %1037, <16 x i16>* %1038, align 32
  %1039 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %1040 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 62
  %1041 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %876, <16 x i16> %860) #9
  %1042 = bitcast <4 x i64>* %1039 to <16 x i16>*
  store <16 x i16> %1041, <16 x i16>* %1042, align 32
  %1043 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %876, <16 x i16> %860) #9
  %1044 = bitcast <4 x i64>* %1040 to <16 x i16>*
  store <16 x i16> %1043, <16 x i16>* %1044, align 32
  %1045 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %1046 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 61
  %1047 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %878, <16 x i16> %862) #9
  %1048 = bitcast <4 x i64>* %1045 to <16 x i16>*
  store <16 x i16> %1047, <16 x i16>* %1048, align 32
  %1049 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %878, <16 x i16> %862) #9
  %1050 = bitcast <4 x i64>* %1046 to <16 x i16>*
  store <16 x i16> %1049, <16 x i16>* %1050, align 32
  %1051 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %1052 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 60
  %1053 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %880, <16 x i16> %864) #9
  %1054 = bitcast <4 x i64>* %1051 to <16 x i16>*
  store <16 x i16> %1053, <16 x i16>* %1054, align 32
  %1055 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %880, <16 x i16> %864) #9
  %1056 = bitcast <4 x i64>* %1052 to <16 x i16>*
  store <16 x i16> %1055, <16 x i16>* %1056, align 32
  %1057 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %1058 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 59
  %1059 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %882, <16 x i16> %866) #9
  %1060 = bitcast <4 x i64>* %1057 to <16 x i16>*
  store <16 x i16> %1059, <16 x i16>* %1060, align 32
  %1061 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %882, <16 x i16> %866) #9
  %1062 = bitcast <4 x i64>* %1058 to <16 x i16>*
  store <16 x i16> %1061, <16 x i16>* %1062, align 32
  %1063 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %1064 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 58
  %1065 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %884, <16 x i16> %868) #9
  %1066 = bitcast <4 x i64>* %1063 to <16 x i16>*
  store <16 x i16> %1065, <16 x i16>* %1066, align 32
  %1067 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %884, <16 x i16> %868) #9
  %1068 = bitcast <4 x i64>* %1064 to <16 x i16>*
  store <16 x i16> %1067, <16 x i16>* %1068, align 32
  %1069 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %1070 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 57
  %1071 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %886, <16 x i16> %870) #9
  %1072 = bitcast <4 x i64>* %1069 to <16 x i16>*
  store <16 x i16> %1071, <16 x i16>* %1072, align 32
  %1073 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %886, <16 x i16> %870) #9
  %1074 = bitcast <4 x i64>* %1070 to <16 x i16>*
  store <16 x i16> %1073, <16 x i16>* %1074, align 32
  %1075 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %1076 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 56
  %1077 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %888, <16 x i16> %872) #9
  %1078 = bitcast <4 x i64>* %1075 to <16 x i16>*
  store <16 x i16> %1077, <16 x i16>* %1078, align 32
  %1079 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %888, <16 x i16> %872) #9
  %1080 = bitcast <4 x i64>* %1076 to <16 x i16>*
  store <16 x i16> %1079, <16 x i16>* %1080, align 32
  %1081 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %1082 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 55
  %1083 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %890, <16 x i16> %921) #9
  %1084 = bitcast <4 x i64>* %1081 to <16 x i16>*
  store <16 x i16> %1083, <16 x i16>* %1084, align 32
  %1085 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %890, <16 x i16> %921) #9
  %1086 = bitcast <4 x i64>* %1082 to <16 x i16>*
  store <16 x i16> %1085, <16 x i16>* %1086, align 32
  %1087 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %1088 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 54
  %1089 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %892, <16 x i16> %937) #9
  %1090 = bitcast <4 x i64>* %1087 to <16 x i16>*
  store <16 x i16> %1089, <16 x i16>* %1090, align 32
  %1091 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %892, <16 x i16> %937) #9
  %1092 = bitcast <4 x i64>* %1088 to <16 x i16>*
  store <16 x i16> %1091, <16 x i16>* %1092, align 32
  %1093 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %1094 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 53
  %1095 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %894, <16 x i16> %953) #9
  %1096 = bitcast <4 x i64>* %1093 to <16 x i16>*
  store <16 x i16> %1095, <16 x i16>* %1096, align 32
  %1097 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %894, <16 x i16> %953) #9
  %1098 = bitcast <4 x i64>* %1094 to <16 x i16>*
  store <16 x i16> %1097, <16 x i16>* %1098, align 32
  %1099 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %1100 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 52
  %1101 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %896, <16 x i16> %969) #9
  %1102 = bitcast <4 x i64>* %1099 to <16 x i16>*
  store <16 x i16> %1101, <16 x i16>* %1102, align 32
  %1103 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %896, <16 x i16> %969) #9
  %1104 = bitcast <4 x i64>* %1100 to <16 x i16>*
  store <16 x i16> %1103, <16 x i16>* %1104, align 32
  %1105 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %1106 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 51
  %1107 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %898, <16 x i16> %985) #9
  %1108 = bitcast <4 x i64>* %1105 to <16 x i16>*
  store <16 x i16> %1107, <16 x i16>* %1108, align 32
  %1109 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %898, <16 x i16> %985) #9
  %1110 = bitcast <4 x i64>* %1106 to <16 x i16>*
  store <16 x i16> %1109, <16 x i16>* %1110, align 32
  %1111 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %1112 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 50
  %1113 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %900, <16 x i16> %1001) #9
  %1114 = bitcast <4 x i64>* %1111 to <16 x i16>*
  store <16 x i16> %1113, <16 x i16>* %1114, align 32
  %1115 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %900, <16 x i16> %1001) #9
  %1116 = bitcast <4 x i64>* %1112 to <16 x i16>*
  store <16 x i16> %1115, <16 x i16>* %1116, align 32
  %1117 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %1118 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 49
  %1119 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %902, <16 x i16> %1017) #9
  %1120 = bitcast <4 x i64>* %1117 to <16 x i16>*
  store <16 x i16> %1119, <16 x i16>* %1120, align 32
  %1121 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %902, <16 x i16> %1017) #9
  %1122 = bitcast <4 x i64>* %1118 to <16 x i16>*
  store <16 x i16> %1121, <16 x i16>* %1122, align 32
  %1123 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 15
  %1124 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 48
  %1125 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %904, <16 x i16> %1033) #9
  %1126 = bitcast <4 x i64>* %1123 to <16 x i16>*
  store <16 x i16> %1125, <16 x i16>* %1126, align 32
  %1127 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %904, <16 x i16> %1033) #9
  %1128 = bitcast <4 x i64>* %1124 to <16 x i16>*
  store <16 x i16> %1127, <16 x i16>* %1128, align 32
  %1129 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 16
  %1130 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 47
  %1131 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %905, <16 x i16> %1032) #9
  %1132 = bitcast <4 x i64>* %1129 to <16 x i16>*
  store <16 x i16> %1131, <16 x i16>* %1132, align 32
  %1133 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %905, <16 x i16> %1032) #9
  %1134 = bitcast <4 x i64>* %1130 to <16 x i16>*
  store <16 x i16> %1133, <16 x i16>* %1134, align 32
  %1135 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 17
  %1136 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 46
  %1137 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %903, <16 x i16> %1016) #9
  %1138 = bitcast <4 x i64>* %1135 to <16 x i16>*
  store <16 x i16> %1137, <16 x i16>* %1138, align 32
  %1139 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %903, <16 x i16> %1016) #9
  %1140 = bitcast <4 x i64>* %1136 to <16 x i16>*
  store <16 x i16> %1139, <16 x i16>* %1140, align 32
  %1141 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 18
  %1142 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 45
  %1143 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %901, <16 x i16> %1000) #9
  %1144 = bitcast <4 x i64>* %1141 to <16 x i16>*
  store <16 x i16> %1143, <16 x i16>* %1144, align 32
  %1145 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %901, <16 x i16> %1000) #9
  %1146 = bitcast <4 x i64>* %1142 to <16 x i16>*
  store <16 x i16> %1145, <16 x i16>* %1146, align 32
  %1147 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 19
  %1148 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 44
  %1149 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %899, <16 x i16> %984) #9
  %1150 = bitcast <4 x i64>* %1147 to <16 x i16>*
  store <16 x i16> %1149, <16 x i16>* %1150, align 32
  %1151 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %899, <16 x i16> %984) #9
  %1152 = bitcast <4 x i64>* %1148 to <16 x i16>*
  store <16 x i16> %1151, <16 x i16>* %1152, align 32
  %1153 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 20
  %1154 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 43
  %1155 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %897, <16 x i16> %968) #9
  %1156 = bitcast <4 x i64>* %1153 to <16 x i16>*
  store <16 x i16> %1155, <16 x i16>* %1156, align 32
  %1157 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %897, <16 x i16> %968) #9
  %1158 = bitcast <4 x i64>* %1154 to <16 x i16>*
  store <16 x i16> %1157, <16 x i16>* %1158, align 32
  %1159 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 21
  %1160 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 42
  %1161 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %895, <16 x i16> %952) #9
  %1162 = bitcast <4 x i64>* %1159 to <16 x i16>*
  store <16 x i16> %1161, <16 x i16>* %1162, align 32
  %1163 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %895, <16 x i16> %952) #9
  %1164 = bitcast <4 x i64>* %1160 to <16 x i16>*
  store <16 x i16> %1163, <16 x i16>* %1164, align 32
  %1165 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 22
  %1166 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 41
  %1167 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %893, <16 x i16> %936) #9
  %1168 = bitcast <4 x i64>* %1165 to <16 x i16>*
  store <16 x i16> %1167, <16 x i16>* %1168, align 32
  %1169 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %893, <16 x i16> %936) #9
  %1170 = bitcast <4 x i64>* %1166 to <16 x i16>*
  store <16 x i16> %1169, <16 x i16>* %1170, align 32
  %1171 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 23
  %1172 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 40
  %1173 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %891, <16 x i16> %920) #9
  %1174 = bitcast <4 x i64>* %1171 to <16 x i16>*
  store <16 x i16> %1173, <16 x i16>* %1174, align 32
  %1175 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %891, <16 x i16> %920) #9
  %1176 = bitcast <4 x i64>* %1172 to <16 x i16>*
  store <16 x i16> %1175, <16 x i16>* %1176, align 32
  %1177 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 24
  %1178 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 39
  %1179 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %889, <16 x i16> %856) #9
  %1180 = bitcast <4 x i64>* %1177 to <16 x i16>*
  store <16 x i16> %1179, <16 x i16>* %1180, align 32
  %1181 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %889, <16 x i16> %856) #9
  %1182 = bitcast <4 x i64>* %1178 to <16 x i16>*
  store <16 x i16> %1181, <16 x i16>* %1182, align 32
  %1183 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 25
  %1184 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 38
  %1185 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %887, <16 x i16> %854) #9
  %1186 = bitcast <4 x i64>* %1183 to <16 x i16>*
  store <16 x i16> %1185, <16 x i16>* %1186, align 32
  %1187 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %887, <16 x i16> %854) #9
  %1188 = bitcast <4 x i64>* %1184 to <16 x i16>*
  store <16 x i16> %1187, <16 x i16>* %1188, align 32
  %1189 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 26
  %1190 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 37
  %1191 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %885, <16 x i16> %852) #9
  %1192 = bitcast <4 x i64>* %1189 to <16 x i16>*
  store <16 x i16> %1191, <16 x i16>* %1192, align 32
  %1193 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %885, <16 x i16> %852) #9
  %1194 = bitcast <4 x i64>* %1190 to <16 x i16>*
  store <16 x i16> %1193, <16 x i16>* %1194, align 32
  %1195 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 27
  %1196 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 36
  %1197 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %883, <16 x i16> %850) #9
  %1198 = bitcast <4 x i64>* %1195 to <16 x i16>*
  store <16 x i16> %1197, <16 x i16>* %1198, align 32
  %1199 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %883, <16 x i16> %850) #9
  %1200 = bitcast <4 x i64>* %1196 to <16 x i16>*
  store <16 x i16> %1199, <16 x i16>* %1200, align 32
  %1201 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 28
  %1202 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 35
  %1203 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %881, <16 x i16> %848) #9
  %1204 = bitcast <4 x i64>* %1201 to <16 x i16>*
  store <16 x i16> %1203, <16 x i16>* %1204, align 32
  %1205 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %881, <16 x i16> %848) #9
  %1206 = bitcast <4 x i64>* %1202 to <16 x i16>*
  store <16 x i16> %1205, <16 x i16>* %1206, align 32
  %1207 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 29
  %1208 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 34
  %1209 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %879, <16 x i16> %846) #9
  %1210 = bitcast <4 x i64>* %1207 to <16 x i16>*
  store <16 x i16> %1209, <16 x i16>* %1210, align 32
  %1211 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %879, <16 x i16> %846) #9
  %1212 = bitcast <4 x i64>* %1208 to <16 x i16>*
  store <16 x i16> %1211, <16 x i16>* %1212, align 32
  %1213 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 30
  %1214 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 33
  %1215 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %877, <16 x i16> %844) #9
  %1216 = bitcast <4 x i64>* %1213 to <16 x i16>*
  store <16 x i16> %1215, <16 x i16>* %1216, align 32
  %1217 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %877, <16 x i16> %844) #9
  %1218 = bitcast <4 x i64>* %1214 to <16 x i16>*
  store <16 x i16> %1217, <16 x i16>* %1218, align 32
  %1219 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 31
  %1220 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 32
  %1221 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %875, <16 x i16> %842) #9
  %1222 = bitcast <4 x i64>* %1219 to <16 x i16>*
  store <16 x i16> %1221, <16 x i16>* %1222, align 32
  %1223 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %875, <16 x i16> %842) #9
  %1224 = bitcast <4 x i64>* %1220 to <16 x i16>*
  store <16 x i16> %1223, <16 x i16>* %1224, align 32
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @idct64_low16_avx2(<4 x i64>* nocapture readonly, <4 x i64>*, i8 signext) #0 {
  %4 = alloca [64 x <4 x i64>], align 32
  %5 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %6 = trunc i32 %5 to i16
  %7 = and i32 %5, 65535
  %8 = shl nuw i32 %7, 16
  %9 = or i32 %8, %7
  %10 = insertelement <8 x i32> undef, i32 %9, i32 0
  %11 = shufflevector <8 x i32> %10, <8 x i32> undef, <8 x i32> zeroinitializer
  %12 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 16), align 16
  %13 = sub i32 0, %12
  %14 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 48), align 16
  %15 = and i32 %13, 65535
  %16 = and i32 %14, 65535
  %17 = shl nuw i32 %16, 16
  %18 = or i32 %17, %15
  %19 = insertelement <8 x i32> undef, i32 %18, i32 0
  %20 = shufflevector <8 x i32> %19, <8 x i32> undef, <8 x i32> zeroinitializer
  %21 = shl i32 %12, 16
  %22 = or i32 %16, %21
  %23 = insertelement <8 x i32> undef, i32 %22, i32 0
  %24 = shufflevector <8 x i32> %23, <8 x i32> undef, <8 x i32> zeroinitializer
  %25 = sub i32 0, %14
  %26 = and i32 %25, 65535
  %27 = shl nuw i32 %15, 16
  %28 = or i32 %27, %26
  %29 = insertelement <8 x i32> undef, i32 %28, i32 0
  %30 = shufflevector <8 x i32> %29, <8 x i32> undef, <8 x i32> zeroinitializer
  %31 = sub i32 0, %5
  %32 = and i32 %31, 65535
  %33 = or i32 %8, %32
  %34 = insertelement <8 x i32> undef, i32 %33, i32 0
  %35 = shufflevector <8 x i32> %34, <8 x i32> undef, <8 x i32> zeroinitializer
  %36 = bitcast [64 x <4 x i64>]* %4 to i8*
  call void @llvm.lifetime.start.p0i8(i64 2048, i8* nonnull %36) #9
  %37 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 1
  %38 = bitcast <4 x i64>* %37 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %38, i8 -86, i64 1984, i1 false)
  %39 = load <4 x i64>, <4 x i64>* %0, align 32
  %40 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 0
  store <4 x i64> %39, <4 x i64>* %40, align 32
  %41 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 8
  %42 = load <4 x i64>, <4 x i64>* %41, align 32
  %43 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 4
  store <4 x i64> %42, <4 x i64>* %43, align 32
  %44 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 4
  %45 = load <4 x i64>, <4 x i64>* %44, align 32
  %46 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 8
  store <4 x i64> %45, <4 x i64>* %46, align 32
  %47 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 12
  %48 = load <4 x i64>, <4 x i64>* %47, align 32
  %49 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 12
  store <4 x i64> %48, <4 x i64>* %49, align 32
  %50 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 2
  %51 = load <4 x i64>, <4 x i64>* %50, align 32
  %52 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 16
  store <4 x i64> %51, <4 x i64>* %52, align 32
  %53 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 10
  %54 = load <4 x i64>, <4 x i64>* %53, align 32
  %55 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 20
  store <4 x i64> %54, <4 x i64>* %55, align 32
  %56 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 6
  %57 = load <4 x i64>, <4 x i64>* %56, align 32
  %58 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 24
  store <4 x i64> %57, <4 x i64>* %58, align 32
  %59 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 14
  %60 = load <4 x i64>, <4 x i64>* %59, align 32
  %61 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 28
  store <4 x i64> %60, <4 x i64>* %61, align 32
  %62 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 1
  %63 = bitcast <4 x i64>* %62 to <16 x i16>*
  %64 = load <16 x i16>, <16 x i16>* %63, align 32
  %65 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 32
  %66 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 9
  %67 = bitcast <4 x i64>* %66 to <16 x i16>*
  %68 = load <16 x i16>, <16 x i16>* %67, align 32
  %69 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 36
  %70 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 5
  %71 = load <4 x i64>, <4 x i64>* %70, align 32
  %72 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 40
  store <4 x i64> %71, <4 x i64>* %72, align 32
  %73 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 13
  %74 = load <4 x i64>, <4 x i64>* %73, align 32
  %75 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 44
  store <4 x i64> %74, <4 x i64>* %75, align 32
  %76 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 3
  %77 = load <4 x i64>, <4 x i64>* %76, align 32
  %78 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 48
  store <4 x i64> %77, <4 x i64>* %78, align 32
  %79 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 11
  %80 = load <4 x i64>, <4 x i64>* %79, align 32
  %81 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 52
  store <4 x i64> %80, <4 x i64>* %81, align 32
  %82 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 7
  %83 = bitcast <4 x i64>* %82 to <16 x i16>*
  %84 = load <16 x i16>, <16 x i16>* %83, align 32
  %85 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 56
  %86 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 15
  %87 = bitcast <4 x i64>* %86 to <16 x i16>*
  %88 = load <16 x i16>, <16 x i16>* %87, align 32
  %89 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 60
  %90 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 63), align 4
  %91 = trunc i32 %90 to i16
  %92 = shl i16 %91, 3
  %93 = insertelement <16 x i16> undef, i16 %92, i32 0
  %94 = shufflevector <16 x i16> %93, <16 x i16> undef, <16 x i32> zeroinitializer
  %95 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 1), align 4
  %96 = trunc i32 %95 to i16
  %97 = shl i16 %96, 3
  %98 = insertelement <16 x i16> undef, i16 %97, i32 0
  %99 = shufflevector <16 x i16> %98, <16 x i16> undef, <16 x i32> zeroinitializer
  %100 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %64, <16 x i16> %94) #9
  %101 = bitcast <4 x i64>* %65 to <16 x i16>*
  store <16 x i16> %100, <16 x i16>* %101, align 32
  %102 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %64, <16 x i16> %99) #9
  %103 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 63
  %104 = bitcast <4 x i64>* %103 to <16 x i16>*
  store <16 x i16> %102, <16 x i16>* %104, align 32
  %105 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 49), align 4
  %106 = trunc i32 %105 to i16
  %107 = shl i16 %106, 3
  %108 = sub i16 0, %107
  %109 = insertelement <16 x i16> undef, i16 %108, i32 0
  %110 = shufflevector <16 x i16> %109, <16 x i16> undef, <16 x i32> zeroinitializer
  %111 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 15), align 4
  %112 = trunc i32 %111 to i16
  %113 = shl i16 %112, 3
  %114 = insertelement <16 x i16> undef, i16 %113, i32 0
  %115 = shufflevector <16 x i16> %114, <16 x i16> undef, <16 x i32> zeroinitializer
  %116 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %88, <16 x i16> %110) #9
  %117 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 35
  %118 = bitcast <4 x i64>* %117 to <16 x i16>*
  store <16 x i16> %116, <16 x i16>* %118, align 32
  %119 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %88, <16 x i16> %115) #9
  %120 = bitcast <4 x i64>* %89 to <16 x i16>*
  store <16 x i16> %119, <16 x i16>* %120, align 32
  %121 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 55), align 4
  %122 = trunc i32 %121 to i16
  %123 = shl i16 %122, 3
  %124 = insertelement <16 x i16> undef, i16 %123, i32 0
  %125 = shufflevector <16 x i16> %124, <16 x i16> undef, <16 x i32> zeroinitializer
  %126 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 9), align 4
  %127 = trunc i32 %126 to i16
  %128 = shl i16 %127, 3
  %129 = insertelement <16 x i16> undef, i16 %128, i32 0
  %130 = shufflevector <16 x i16> %129, <16 x i16> undef, <16 x i32> zeroinitializer
  %131 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %68, <16 x i16> %125) #9
  %132 = bitcast <4 x i64>* %69 to <16 x i16>*
  store <16 x i16> %131, <16 x i16>* %132, align 32
  %133 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %68, <16 x i16> %130) #9
  %134 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 59
  %135 = bitcast <4 x i64>* %134 to <16 x i16>*
  store <16 x i16> %133, <16 x i16>* %135, align 32
  %136 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 57), align 4
  %137 = trunc i32 %136 to i16
  %138 = shl i16 %137, 3
  %139 = sub i16 0, %138
  %140 = insertelement <16 x i16> undef, i16 %139, i32 0
  %141 = shufflevector <16 x i16> %140, <16 x i16> undef, <16 x i32> zeroinitializer
  %142 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 7), align 4
  %143 = trunc i32 %142 to i16
  %144 = shl i16 %143, 3
  %145 = insertelement <16 x i16> undef, i16 %144, i32 0
  %146 = shufflevector <16 x i16> %145, <16 x i16> undef, <16 x i32> zeroinitializer
  %147 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %84, <16 x i16> %141) #9
  %148 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 39
  %149 = bitcast <4 x i64>* %148 to <16 x i16>*
  store <16 x i16> %147, <16 x i16>* %149, align 32
  %150 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %84, <16 x i16> %146) #9
  %151 = bitcast <4 x i64>* %85 to <16 x i16>*
  store <16 x i16> %150, <16 x i16>* %151, align 32
  %152 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 59), align 4
  %153 = trunc i32 %152 to i16
  %154 = shl i16 %153, 3
  %155 = insertelement <16 x i16> undef, i16 %154, i32 0
  %156 = shufflevector <16 x i16> %155, <16 x i16> undef, <16 x i32> zeroinitializer
  %157 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 5), align 4
  %158 = trunc i32 %157 to i16
  %159 = shl i16 %158, 3
  %160 = insertelement <16 x i16> undef, i16 %159, i32 0
  %161 = shufflevector <16 x i16> %160, <16 x i16> undef, <16 x i32> zeroinitializer
  %162 = bitcast <4 x i64> %71 to <16 x i16>
  %163 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %162, <16 x i16> %156) #9
  %164 = bitcast <4 x i64>* %72 to <16 x i16>*
  store <16 x i16> %163, <16 x i16>* %164, align 32
  %165 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %162, <16 x i16> %161) #9
  %166 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 55
  %167 = bitcast <4 x i64>* %166 to <16 x i16>*
  store <16 x i16> %165, <16 x i16>* %167, align 32
  %168 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 53), align 4
  %169 = trunc i32 %168 to i16
  %170 = shl i16 %169, 3
  %171 = sub i16 0, %170
  %172 = insertelement <16 x i16> undef, i16 %171, i32 0
  %173 = shufflevector <16 x i16> %172, <16 x i16> undef, <16 x i32> zeroinitializer
  %174 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 11), align 4
  %175 = trunc i32 %174 to i16
  %176 = shl i16 %175, 3
  %177 = insertelement <16 x i16> undef, i16 %176, i32 0
  %178 = shufflevector <16 x i16> %177, <16 x i16> undef, <16 x i32> zeroinitializer
  %179 = bitcast <4 x i64> %80 to <16 x i16>
  %180 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %179, <16 x i16> %173) #9
  %181 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 43
  %182 = bitcast <4 x i64>* %181 to <16 x i16>*
  store <16 x i16> %180, <16 x i16>* %182, align 32
  %183 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %179, <16 x i16> %178) #9
  %184 = bitcast <4 x i64>* %81 to <16 x i16>*
  store <16 x i16> %183, <16 x i16>* %184, align 32
  %185 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 51), align 4
  %186 = trunc i32 %185 to i16
  %187 = shl i16 %186, 3
  %188 = insertelement <16 x i16> undef, i16 %187, i32 0
  %189 = shufflevector <16 x i16> %188, <16 x i16> undef, <16 x i32> zeroinitializer
  %190 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 13), align 4
  %191 = trunc i32 %190 to i16
  %192 = shl i16 %191, 3
  %193 = insertelement <16 x i16> undef, i16 %192, i32 0
  %194 = shufflevector <16 x i16> %193, <16 x i16> undef, <16 x i32> zeroinitializer
  %195 = bitcast <4 x i64> %74 to <16 x i16>
  %196 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %195, <16 x i16> %189) #9
  %197 = bitcast <4 x i64>* %75 to <16 x i16>*
  store <16 x i16> %196, <16 x i16>* %197, align 32
  %198 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %195, <16 x i16> %194) #9
  %199 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 51
  %200 = bitcast <4 x i64>* %199 to <16 x i16>*
  store <16 x i16> %198, <16 x i16>* %200, align 32
  %201 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 61), align 4
  %202 = trunc i32 %201 to i16
  %203 = shl i16 %202, 3
  %204 = sub i16 0, %203
  %205 = insertelement <16 x i16> undef, i16 %204, i32 0
  %206 = shufflevector <16 x i16> %205, <16 x i16> undef, <16 x i32> zeroinitializer
  %207 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 3), align 4
  %208 = trunc i32 %207 to i16
  %209 = shl i16 %208, 3
  %210 = insertelement <16 x i16> undef, i16 %209, i32 0
  %211 = shufflevector <16 x i16> %210, <16 x i16> undef, <16 x i32> zeroinitializer
  %212 = bitcast <4 x i64> %77 to <16 x i16>
  %213 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %212, <16 x i16> %206) #9
  %214 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 47
  %215 = bitcast <4 x i64>* %214 to <16 x i16>*
  store <16 x i16> %213, <16 x i16>* %215, align 32
  %216 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %212, <16 x i16> %211) #9
  %217 = bitcast <4 x i64>* %78 to <16 x i16>*
  store <16 x i16> %216, <16 x i16>* %217, align 32
  %218 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 62), align 8
  %219 = trunc i32 %218 to i16
  %220 = shl i16 %219, 3
  %221 = insertelement <16 x i16> undef, i16 %220, i32 0
  %222 = shufflevector <16 x i16> %221, <16 x i16> undef, <16 x i32> zeroinitializer
  %223 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 2), align 8
  %224 = trunc i32 %223 to i16
  %225 = shl i16 %224, 3
  %226 = insertelement <16 x i16> undef, i16 %225, i32 0
  %227 = shufflevector <16 x i16> %226, <16 x i16> undef, <16 x i32> zeroinitializer
  %228 = bitcast <4 x i64> %51 to <16 x i16>
  %229 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %228, <16 x i16> %222) #9
  %230 = bitcast <4 x i64>* %52 to <16 x i16>*
  store <16 x i16> %229, <16 x i16>* %230, align 32
  %231 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %228, <16 x i16> %227) #9
  %232 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 31
  %233 = bitcast <4 x i64>* %232 to <16 x i16>*
  store <16 x i16> %231, <16 x i16>* %233, align 32
  %234 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 50), align 8
  %235 = trunc i32 %234 to i16
  %236 = shl i16 %235, 3
  %237 = sub i16 0, %236
  %238 = insertelement <16 x i16> undef, i16 %237, i32 0
  %239 = shufflevector <16 x i16> %238, <16 x i16> undef, <16 x i32> zeroinitializer
  %240 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 14), align 8
  %241 = trunc i32 %240 to i16
  %242 = shl i16 %241, 3
  %243 = insertelement <16 x i16> undef, i16 %242, i32 0
  %244 = shufflevector <16 x i16> %243, <16 x i16> undef, <16 x i32> zeroinitializer
  %245 = bitcast <4 x i64> %60 to <16 x i16>
  %246 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %245, <16 x i16> %239) #9
  %247 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 19
  %248 = bitcast <4 x i64>* %247 to <16 x i16>*
  store <16 x i16> %246, <16 x i16>* %248, align 32
  %249 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %245, <16 x i16> %244) #9
  %250 = bitcast <4 x i64>* %61 to <16 x i16>*
  store <16 x i16> %249, <16 x i16>* %250, align 32
  %251 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 54), align 8
  %252 = trunc i32 %251 to i16
  %253 = shl i16 %252, 3
  %254 = insertelement <16 x i16> undef, i16 %253, i32 0
  %255 = shufflevector <16 x i16> %254, <16 x i16> undef, <16 x i32> zeroinitializer
  %256 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 10), align 8
  %257 = trunc i32 %256 to i16
  %258 = shl i16 %257, 3
  %259 = insertelement <16 x i16> undef, i16 %258, i32 0
  %260 = shufflevector <16 x i16> %259, <16 x i16> undef, <16 x i32> zeroinitializer
  %261 = bitcast <4 x i64> %54 to <16 x i16>
  %262 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %261, <16 x i16> %255) #9
  %263 = bitcast <4 x i64>* %55 to <16 x i16>*
  store <16 x i16> %262, <16 x i16>* %263, align 32
  %264 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %261, <16 x i16> %260) #9
  %265 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 27
  %266 = bitcast <4 x i64>* %265 to <16 x i16>*
  store <16 x i16> %264, <16 x i16>* %266, align 32
  %267 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 58), align 8
  %268 = trunc i32 %267 to i16
  %269 = shl i16 %268, 3
  %270 = sub i16 0, %269
  %271 = insertelement <16 x i16> undef, i16 %270, i32 0
  %272 = shufflevector <16 x i16> %271, <16 x i16> undef, <16 x i32> zeroinitializer
  %273 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 6), align 8
  %274 = trunc i32 %273 to i16
  %275 = shl i16 %274, 3
  %276 = insertelement <16 x i16> undef, i16 %275, i32 0
  %277 = shufflevector <16 x i16> %276, <16 x i16> undef, <16 x i32> zeroinitializer
  %278 = bitcast <4 x i64> %57 to <16 x i16>
  %279 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %278, <16 x i16> %272) #9
  %280 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 23
  %281 = bitcast <4 x i64>* %280 to <16 x i16>*
  store <16 x i16> %279, <16 x i16>* %281, align 32
  %282 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %278, <16 x i16> %277) #9
  %283 = bitcast <4 x i64>* %58 to <16 x i16>*
  store <16 x i16> %282, <16 x i16>* %283, align 32
  %284 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 33
  %285 = bitcast <4 x i64>* %284 to <16 x i16>*
  store <16 x i16> %100, <16 x i16>* %285, align 32
  %286 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 34
  %287 = bitcast <4 x i64>* %286 to <16 x i16>*
  store <16 x i16> %116, <16 x i16>* %287, align 32
  %288 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 37
  %289 = bitcast <4 x i64>* %288 to <16 x i16>*
  store <16 x i16> %131, <16 x i16>* %289, align 32
  %290 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 38
  %291 = bitcast <4 x i64>* %290 to <16 x i16>*
  store <16 x i16> %147, <16 x i16>* %291, align 32
  %292 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 41
  %293 = bitcast <4 x i64>* %292 to <16 x i16>*
  store <16 x i16> %163, <16 x i16>* %293, align 32
  %294 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 42
  %295 = bitcast <4 x i64>* %294 to <16 x i16>*
  store <16 x i16> %180, <16 x i16>* %295, align 32
  %296 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 45
  %297 = bitcast <4 x i64>* %296 to <16 x i16>*
  store <16 x i16> %196, <16 x i16>* %297, align 32
  %298 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 46
  %299 = bitcast <4 x i64>* %298 to <16 x i16>*
  store <16 x i16> %213, <16 x i16>* %299, align 32
  %300 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 49
  %301 = bitcast <4 x i64>* %300 to <16 x i16>*
  store <16 x i16> %216, <16 x i16>* %301, align 32
  %302 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 50
  %303 = bitcast <4 x i64>* %302 to <16 x i16>*
  store <16 x i16> %198, <16 x i16>* %303, align 32
  %304 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 53
  %305 = bitcast <4 x i64>* %304 to <16 x i16>*
  store <16 x i16> %183, <16 x i16>* %305, align 32
  %306 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 54
  %307 = bitcast <4 x i64>* %306 to <16 x i16>*
  store <16 x i16> %165, <16 x i16>* %307, align 32
  %308 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 57
  %309 = bitcast <4 x i64>* %308 to <16 x i16>*
  store <16 x i16> %150, <16 x i16>* %309, align 32
  %310 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 58
  %311 = bitcast <4 x i64>* %310 to <16 x i16>*
  store <16 x i16> %133, <16 x i16>* %311, align 32
  %312 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 61
  %313 = bitcast <4 x i64>* %312 to <16 x i16>*
  store <16 x i16> %119, <16 x i16>* %313, align 32
  %314 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 62
  %315 = bitcast <4 x i64>* %314 to <16 x i16>*
  store <16 x i16> %102, <16 x i16>* %315, align 32
  %316 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 60), align 16
  %317 = trunc i32 %316 to i16
  %318 = shl i16 %317, 3
  %319 = insertelement <16 x i16> undef, i16 %318, i32 0
  %320 = shufflevector <16 x i16> %319, <16 x i16> undef, <16 x i32> zeroinitializer
  %321 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 4), align 16
  %322 = trunc i32 %321 to i16
  %323 = shl i16 %322, 3
  %324 = insertelement <16 x i16> undef, i16 %323, i32 0
  %325 = shufflevector <16 x i16> %324, <16 x i16> undef, <16 x i32> zeroinitializer
  %326 = bitcast <4 x i64> %45 to <16 x i16>
  %327 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %326, <16 x i16> %320) #9
  %328 = bitcast <4 x i64>* %46 to <16 x i16>*
  store <16 x i16> %327, <16 x i16>* %328, align 32
  %329 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %326, <16 x i16> %325) #9
  %330 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 15
  %331 = bitcast <4 x i64>* %330 to <16 x i16>*
  store <16 x i16> %329, <16 x i16>* %331, align 32
  %332 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 52), align 16
  %333 = trunc i32 %332 to i16
  %334 = shl i16 %333, 3
  %335 = sub i16 0, %334
  %336 = insertelement <16 x i16> undef, i16 %335, i32 0
  %337 = shufflevector <16 x i16> %336, <16 x i16> undef, <16 x i32> zeroinitializer
  %338 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 12), align 16
  %339 = trunc i32 %338 to i16
  %340 = shl i16 %339, 3
  %341 = insertelement <16 x i16> undef, i16 %340, i32 0
  %342 = shufflevector <16 x i16> %341, <16 x i16> undef, <16 x i32> zeroinitializer
  %343 = bitcast <4 x i64> %48 to <16 x i16>
  %344 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %343, <16 x i16> %337) #9
  %345 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 11
  %346 = bitcast <4 x i64>* %345 to <16 x i16>*
  store <16 x i16> %344, <16 x i16>* %346, align 32
  %347 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %343, <16 x i16> %342) #9
  %348 = bitcast <4 x i64>* %49 to <16 x i16>*
  store <16 x i16> %347, <16 x i16>* %348, align 32
  %349 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 17
  %350 = bitcast <4 x i64>* %349 to <16 x i16>*
  store <16 x i16> %229, <16 x i16>* %350, align 32
  %351 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 18
  %352 = bitcast <4 x i64>* %351 to <16 x i16>*
  store <16 x i16> %246, <16 x i16>* %352, align 32
  %353 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 21
  %354 = bitcast <4 x i64>* %353 to <16 x i16>*
  store <16 x i16> %262, <16 x i16>* %354, align 32
  %355 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 22
  %356 = bitcast <4 x i64>* %355 to <16 x i16>*
  store <16 x i16> %279, <16 x i16>* %356, align 32
  %357 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 25
  %358 = bitcast <4 x i64>* %357 to <16 x i16>*
  store <16 x i16> %282, <16 x i16>* %358, align 32
  %359 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 26
  %360 = bitcast <4 x i64>* %359 to <16 x i16>*
  store <16 x i16> %264, <16 x i16>* %360, align 32
  %361 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 29
  %362 = bitcast <4 x i64>* %361 to <16 x i16>*
  store <16 x i16> %249, <16 x i16>* %362, align 32
  %363 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 30
  %364 = bitcast <4 x i64>* %363 to <16 x i16>*
  store <16 x i16> %231, <16 x i16>* %364, align 32
  call fastcc void @idct64_stage4_high32_avx2(<4 x i64>* nonnull %40, <4 x i64> <i64 8796093024256, i64 8796093024256, i64 8796093024256, i64 8796093024256>, i8 signext %2)
  %365 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 56), align 16
  %366 = trunc i32 %365 to i16
  %367 = shl i16 %366, 3
  %368 = insertelement <16 x i16> undef, i16 %367, i32 0
  %369 = shufflevector <16 x i16> %368, <16 x i16> undef, <16 x i32> zeroinitializer
  %370 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 8), align 16
  %371 = trunc i32 %370 to i16
  %372 = shl i16 %371, 3
  %373 = insertelement <16 x i16> undef, i16 %372, i32 0
  %374 = shufflevector <16 x i16> %373, <16 x i16> undef, <16 x i32> zeroinitializer
  %375 = bitcast <4 x i64>* %43 to <16 x i16>*
  %376 = load <16 x i16>, <16 x i16>* %375, align 32
  %377 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %376, <16 x i16> %369) #9
  store <16 x i16> %377, <16 x i16>* %375, align 32
  %378 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %376, <16 x i16> %374) #9
  %379 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 7
  %380 = bitcast <4 x i64>* %379 to <16 x i16>*
  store <16 x i16> %378, <16 x i16>* %380, align 32
  %381 = load <4 x i64>, <4 x i64>* %46, align 32
  %382 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 9
  store <4 x i64> %381, <4 x i64>* %382, align 32
  %383 = load <4 x i64>, <4 x i64>* %345, align 32
  %384 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 10
  store <4 x i64> %383, <4 x i64>* %384, align 32
  %385 = load <4 x i64>, <4 x i64>* %49, align 32
  %386 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 13
  store <4 x i64> %385, <4 x i64>* %386, align 32
  %387 = load <4 x i64>, <4 x i64>* %330, align 32
  %388 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 14
  store <4 x i64> %387, <4 x i64>* %388, align 32
  %389 = sub i32 0, %370
  %390 = and i32 %389, 65535
  %391 = and i32 %365, 65535
  %392 = shl nuw i32 %391, 16
  %393 = or i32 %392, %390
  %394 = insertelement <8 x i32> undef, i32 %393, i32 0
  %395 = shufflevector <8 x i32> %394, <8 x i32> undef, <8 x i32> zeroinitializer
  %396 = shl i32 %370, 16
  %397 = or i32 %396, %391
  %398 = insertelement <8 x i32> undef, i32 %397, i32 0
  %399 = shufflevector <8 x i32> %398, <8 x i32> undef, <8 x i32> zeroinitializer
  %400 = sub i32 0, %365
  %401 = and i32 %400, 65535
  %402 = shl nuw i32 %390, 16
  %403 = or i32 %402, %401
  %404 = insertelement <8 x i32> undef, i32 %403, i32 0
  %405 = shufflevector <8 x i32> %404, <8 x i32> undef, <8 x i32> zeroinitializer
  %406 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 40), align 16
  %407 = sub i32 0, %406
  %408 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 24), align 16
  %409 = and i32 %407, 65535
  %410 = and i32 %408, 65535
  %411 = shl nuw i32 %410, 16
  %412 = or i32 %411, %409
  %413 = insertelement <8 x i32> undef, i32 %412, i32 0
  %414 = shufflevector <8 x i32> %413, <8 x i32> undef, <8 x i32> zeroinitializer
  %415 = shl i32 %406, 16
  %416 = or i32 %410, %415
  %417 = insertelement <8 x i32> undef, i32 %416, i32 0
  %418 = shufflevector <8 x i32> %417, <8 x i32> undef, <8 x i32> zeroinitializer
  %419 = sub i32 0, %408
  %420 = and i32 %419, 65535
  %421 = shl nuw i32 %409, 16
  %422 = or i32 %421, %420
  %423 = insertelement <8 x i32> undef, i32 %422, i32 0
  %424 = shufflevector <8 x i32> %423, <8 x i32> undef, <8 x i32> zeroinitializer
  %425 = sext i8 %2 to i32
  %426 = load <16 x i16>, <16 x i16>* %350, align 32
  %427 = load <16 x i16>, <16 x i16>* %364, align 32
  %428 = shufflevector <16 x i16> %426, <16 x i16> %427, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %429 = shufflevector <16 x i16> %426, <16 x i16> %427, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %430 = bitcast <8 x i32> %395 to <16 x i16>
  %431 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %428, <16 x i16> %430) #9
  %432 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %429, <16 x i16> %430) #9
  %433 = bitcast <8 x i32> %399 to <16 x i16>
  %434 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %428, <16 x i16> %433) #9
  %435 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %429, <16 x i16> %433) #9
  %436 = add <8 x i32> %431, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %437 = add <8 x i32> %432, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %438 = add <8 x i32> %434, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %439 = add <8 x i32> %435, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %440 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %436, i32 %425) #9
  %441 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %437, i32 %425) #9
  %442 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %438, i32 %425) #9
  %443 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %439, i32 %425) #9
  %444 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %440, <8 x i32> %441) #9
  store <16 x i16> %444, <16 x i16>* %350, align 32
  %445 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %442, <8 x i32> %443) #9
  store <16 x i16> %445, <16 x i16>* %364, align 32
  %446 = load <16 x i16>, <16 x i16>* %352, align 32
  %447 = load <16 x i16>, <16 x i16>* %362, align 32
  %448 = shufflevector <16 x i16> %446, <16 x i16> %447, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %449 = shufflevector <16 x i16> %446, <16 x i16> %447, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %450 = bitcast <8 x i32> %405 to <16 x i16>
  %451 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %448, <16 x i16> %450) #9
  %452 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %449, <16 x i16> %450) #9
  %453 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %448, <16 x i16> %430) #9
  %454 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %449, <16 x i16> %430) #9
  %455 = add <8 x i32> %451, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %456 = add <8 x i32> %452, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %457 = add <8 x i32> %453, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %458 = add <8 x i32> %454, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %459 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %455, i32 %425) #9
  %460 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %456, i32 %425) #9
  %461 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %457, i32 %425) #9
  %462 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %458, i32 %425) #9
  %463 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %459, <8 x i32> %460) #9
  store <16 x i16> %463, <16 x i16>* %352, align 32
  %464 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %461, <8 x i32> %462) #9
  store <16 x i16> %464, <16 x i16>* %362, align 32
  %465 = load <16 x i16>, <16 x i16>* %354, align 32
  %466 = load <16 x i16>, <16 x i16>* %360, align 32
  %467 = shufflevector <16 x i16> %465, <16 x i16> %466, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %468 = shufflevector <16 x i16> %465, <16 x i16> %466, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %469 = bitcast <8 x i32> %414 to <16 x i16>
  %470 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %467, <16 x i16> %469) #9
  %471 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %468, <16 x i16> %469) #9
  %472 = bitcast <8 x i32> %418 to <16 x i16>
  %473 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %467, <16 x i16> %472) #9
  %474 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %468, <16 x i16> %472) #9
  %475 = add <8 x i32> %470, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %476 = add <8 x i32> %471, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %477 = add <8 x i32> %473, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %478 = add <8 x i32> %474, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %479 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %475, i32 %425) #9
  %480 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %476, i32 %425) #9
  %481 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %477, i32 %425) #9
  %482 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %478, i32 %425) #9
  %483 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %479, <8 x i32> %480) #9
  store <16 x i16> %483, <16 x i16>* %354, align 32
  %484 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %481, <8 x i32> %482) #9
  store <16 x i16> %484, <16 x i16>* %360, align 32
  %485 = load <16 x i16>, <16 x i16>* %356, align 32
  %486 = load <16 x i16>, <16 x i16>* %358, align 32
  %487 = shufflevector <16 x i16> %485, <16 x i16> %486, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %488 = shufflevector <16 x i16> %485, <16 x i16> %486, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %489 = bitcast <8 x i32> %424 to <16 x i16>
  %490 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %487, <16 x i16> %489) #9
  %491 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %488, <16 x i16> %489) #9
  %492 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %487, <16 x i16> %469) #9
  %493 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %488, <16 x i16> %469) #9
  %494 = add <8 x i32> %490, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %495 = add <8 x i32> %491, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %496 = add <8 x i32> %492, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %497 = add <8 x i32> %493, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %498 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %494, i32 %425) #9
  %499 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %495, i32 %425) #9
  %500 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %496, i32 %425) #9
  %501 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %497, i32 %425) #9
  %502 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %498, <8 x i32> %499) #9
  store <16 x i16> %502, <16 x i16>* %356, align 32
  %503 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %500, <8 x i32> %501) #9
  store <16 x i16> %503, <16 x i16>* %358, align 32
  %504 = load <16 x i16>, <16 x i16>* %101, align 32
  %505 = load <16 x i16>, <16 x i16>* %118, align 32
  %506 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %504, <16 x i16> %505) #9
  store <16 x i16> %506, <16 x i16>* %101, align 32
  %507 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %504, <16 x i16> %505) #9
  store <16 x i16> %507, <16 x i16>* %118, align 32
  %508 = load <16 x i16>, <16 x i16>* %285, align 32
  %509 = load <16 x i16>, <16 x i16>* %287, align 32
  %510 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %508, <16 x i16> %509) #9
  store <16 x i16> %510, <16 x i16>* %285, align 32
  %511 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %508, <16 x i16> %509) #9
  store <16 x i16> %511, <16 x i16>* %287, align 32
  %512 = load <16 x i16>, <16 x i16>* %149, align 32
  %513 = load <16 x i16>, <16 x i16>* %132, align 32
  %514 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %512, <16 x i16> %513) #9
  store <16 x i16> %514, <16 x i16>* %149, align 32
  %515 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %512, <16 x i16> %513) #9
  store <16 x i16> %515, <16 x i16>* %132, align 32
  %516 = load <16 x i16>, <16 x i16>* %291, align 32
  %517 = load <16 x i16>, <16 x i16>* %289, align 32
  %518 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %516, <16 x i16> %517) #9
  store <16 x i16> %518, <16 x i16>* %291, align 32
  %519 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %516, <16 x i16> %517) #9
  store <16 x i16> %519, <16 x i16>* %289, align 32
  %520 = load <16 x i16>, <16 x i16>* %164, align 32
  %521 = load <16 x i16>, <16 x i16>* %182, align 32
  %522 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %520, <16 x i16> %521) #9
  store <16 x i16> %522, <16 x i16>* %164, align 32
  %523 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %520, <16 x i16> %521) #9
  store <16 x i16> %523, <16 x i16>* %182, align 32
  %524 = load <16 x i16>, <16 x i16>* %293, align 32
  %525 = load <16 x i16>, <16 x i16>* %295, align 32
  %526 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %524, <16 x i16> %525) #9
  store <16 x i16> %526, <16 x i16>* %293, align 32
  %527 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %524, <16 x i16> %525) #9
  store <16 x i16> %527, <16 x i16>* %295, align 32
  %528 = load <16 x i16>, <16 x i16>* %215, align 32
  %529 = load <16 x i16>, <16 x i16>* %197, align 32
  %530 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %528, <16 x i16> %529) #9
  store <16 x i16> %530, <16 x i16>* %215, align 32
  %531 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %528, <16 x i16> %529) #9
  store <16 x i16> %531, <16 x i16>* %197, align 32
  %532 = load <16 x i16>, <16 x i16>* %299, align 32
  %533 = load <16 x i16>, <16 x i16>* %297, align 32
  %534 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %532, <16 x i16> %533) #9
  store <16 x i16> %534, <16 x i16>* %299, align 32
  %535 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %532, <16 x i16> %533) #9
  store <16 x i16> %535, <16 x i16>* %297, align 32
  %536 = load <16 x i16>, <16 x i16>* %217, align 32
  %537 = load <16 x i16>, <16 x i16>* %200, align 32
  %538 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %536, <16 x i16> %537) #9
  store <16 x i16> %538, <16 x i16>* %217, align 32
  %539 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %536, <16 x i16> %537) #9
  store <16 x i16> %539, <16 x i16>* %200, align 32
  %540 = load <16 x i16>, <16 x i16>* %301, align 32
  %541 = load <16 x i16>, <16 x i16>* %303, align 32
  %542 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %540, <16 x i16> %541) #9
  store <16 x i16> %542, <16 x i16>* %301, align 32
  %543 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %540, <16 x i16> %541) #9
  store <16 x i16> %543, <16 x i16>* %303, align 32
  %544 = load <16 x i16>, <16 x i16>* %167, align 32
  %545 = load <16 x i16>, <16 x i16>* %184, align 32
  %546 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %544, <16 x i16> %545) #9
  store <16 x i16> %546, <16 x i16>* %167, align 32
  %547 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %544, <16 x i16> %545) #9
  store <16 x i16> %547, <16 x i16>* %184, align 32
  %548 = load <16 x i16>, <16 x i16>* %307, align 32
  %549 = load <16 x i16>, <16 x i16>* %305, align 32
  %550 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %548, <16 x i16> %549) #9
  store <16 x i16> %550, <16 x i16>* %307, align 32
  %551 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %548, <16 x i16> %549) #9
  store <16 x i16> %551, <16 x i16>* %305, align 32
  %552 = load <16 x i16>, <16 x i16>* %151, align 32
  %553 = load <16 x i16>, <16 x i16>* %135, align 32
  %554 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %552, <16 x i16> %553) #9
  store <16 x i16> %554, <16 x i16>* %151, align 32
  %555 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %552, <16 x i16> %553) #9
  store <16 x i16> %555, <16 x i16>* %135, align 32
  %556 = load <16 x i16>, <16 x i16>* %309, align 32
  %557 = load <16 x i16>, <16 x i16>* %311, align 32
  %558 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %556, <16 x i16> %557) #9
  store <16 x i16> %558, <16 x i16>* %309, align 32
  %559 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %556, <16 x i16> %557) #9
  store <16 x i16> %559, <16 x i16>* %311, align 32
  %560 = load <16 x i16>, <16 x i16>* %104, align 32
  %561 = load <16 x i16>, <16 x i16>* %120, align 32
  %562 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %560, <16 x i16> %561) #9
  store <16 x i16> %562, <16 x i16>* %104, align 32
  %563 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %560, <16 x i16> %561) #9
  store <16 x i16> %563, <16 x i16>* %120, align 32
  %564 = load <16 x i16>, <16 x i16>* %315, align 32
  %565 = load <16 x i16>, <16 x i16>* %313, align 32
  %566 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %564, <16 x i16> %565) #9
  store <16 x i16> %566, <16 x i16>* %315, align 32
  %567 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %564, <16 x i16> %565) #9
  store <16 x i16> %567, <16 x i16>* %313, align 32
  %568 = shl i16 %6, 3
  %569 = insertelement <16 x i16> undef, i16 %568, i32 0
  %570 = shufflevector <16 x i16> %569, <16 x i16> undef, <16 x i32> zeroinitializer
  %571 = bitcast [64 x <4 x i64>]* %4 to <16 x i16>*
  %572 = load <16 x i16>, <16 x i16>* %571, align 32
  %573 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %572, <16 x i16> %570) #9
  store <16 x i16> %573, <16 x i16>* %571, align 32
  %574 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 1
  %575 = bitcast <4 x i64>* %574 to <16 x i16>*
  store <16 x i16> %573, <16 x i16>* %575, align 32
  %576 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 5
  %577 = bitcast <4 x i64>* %576 to <16 x i16>*
  store <16 x i16> %377, <16 x i16>* %577, align 32
  %578 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 6
  %579 = bitcast <4 x i64>* %578 to <16 x i16>*
  store <16 x i16> %378, <16 x i16>* %579, align 32
  %580 = bitcast <4 x i64>* %382 to <16 x i16>*
  %581 = load <16 x i16>, <16 x i16>* %580, align 32
  %582 = bitcast <4 x i64>* %388 to <16 x i16>*
  %583 = load <16 x i16>, <16 x i16>* %582, align 32
  %584 = shufflevector <16 x i16> %581, <16 x i16> %583, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %585 = shufflevector <16 x i16> %581, <16 x i16> %583, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %586 = bitcast <8 x i32> %20 to <16 x i16>
  %587 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %584, <16 x i16> %586) #9
  %588 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %585, <16 x i16> %586) #9
  %589 = bitcast <8 x i32> %24 to <16 x i16>
  %590 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %584, <16 x i16> %589) #9
  %591 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %585, <16 x i16> %589) #9
  %592 = add <8 x i32> %587, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %593 = add <8 x i32> %588, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %594 = add <8 x i32> %590, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %595 = add <8 x i32> %591, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %596 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %592, i32 %425) #9
  %597 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %593, i32 %425) #9
  %598 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %594, i32 %425) #9
  %599 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %595, i32 %425) #9
  %600 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %596, <8 x i32> %597) #9
  store <16 x i16> %600, <16 x i16>* %580, align 32
  %601 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %598, <8 x i32> %599) #9
  store <16 x i16> %601, <16 x i16>* %582, align 32
  %602 = bitcast <4 x i64>* %384 to <16 x i16>*
  %603 = load <16 x i16>, <16 x i16>* %602, align 32
  %604 = bitcast <4 x i64>* %386 to <16 x i16>*
  %605 = load <16 x i16>, <16 x i16>* %604, align 32
  %606 = shufflevector <16 x i16> %603, <16 x i16> %605, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %607 = shufflevector <16 x i16> %603, <16 x i16> %605, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %608 = bitcast <8 x i32> %30 to <16 x i16>
  %609 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %606, <16 x i16> %608) #9
  %610 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %607, <16 x i16> %608) #9
  %611 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %606, <16 x i16> %586) #9
  %612 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %607, <16 x i16> %586) #9
  %613 = add <8 x i32> %609, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %614 = add <8 x i32> %610, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %615 = add <8 x i32> %611, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %616 = add <8 x i32> %612, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %617 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %613, i32 %425) #9
  %618 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %614, i32 %425) #9
  %619 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %615, i32 %425) #9
  %620 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %616, i32 %425) #9
  %621 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %617, <8 x i32> %618) #9
  store <16 x i16> %621, <16 x i16>* %602, align 32
  %622 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %619, <8 x i32> %620) #9
  store <16 x i16> %622, <16 x i16>* %604, align 32
  %623 = load <16 x i16>, <16 x i16>* %230, align 32
  %624 = load <16 x i16>, <16 x i16>* %248, align 32
  %625 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %623, <16 x i16> %624) #9
  store <16 x i16> %625, <16 x i16>* %230, align 32
  %626 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %623, <16 x i16> %624) #9
  store <16 x i16> %626, <16 x i16>* %248, align 32
  %627 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %444, <16 x i16> %463) #9
  store <16 x i16> %627, <16 x i16>* %350, align 32
  %628 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %444, <16 x i16> %463) #9
  store <16 x i16> %628, <16 x i16>* %352, align 32
  %629 = load <16 x i16>, <16 x i16>* %281, align 32
  %630 = load <16 x i16>, <16 x i16>* %263, align 32
  %631 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %629, <16 x i16> %630) #9
  store <16 x i16> %631, <16 x i16>* %281, align 32
  %632 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %629, <16 x i16> %630) #9
  store <16 x i16> %632, <16 x i16>* %263, align 32
  %633 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %502, <16 x i16> %483) #9
  store <16 x i16> %633, <16 x i16>* %356, align 32
  %634 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %502, <16 x i16> %483) #9
  store <16 x i16> %634, <16 x i16>* %354, align 32
  %635 = load <16 x i16>, <16 x i16>* %283, align 32
  %636 = load <16 x i16>, <16 x i16>* %266, align 32
  %637 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %635, <16 x i16> %636) #9
  store <16 x i16> %637, <16 x i16>* %283, align 32
  %638 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %635, <16 x i16> %636) #9
  store <16 x i16> %638, <16 x i16>* %266, align 32
  %639 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %503, <16 x i16> %484) #9
  store <16 x i16> %639, <16 x i16>* %358, align 32
  %640 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %503, <16 x i16> %484) #9
  store <16 x i16> %640, <16 x i16>* %360, align 32
  %641 = load <16 x i16>, <16 x i16>* %233, align 32
  %642 = load <16 x i16>, <16 x i16>* %250, align 32
  %643 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %641, <16 x i16> %642) #9
  store <16 x i16> %643, <16 x i16>* %233, align 32
  %644 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %641, <16 x i16> %642) #9
  store <16 x i16> %644, <16 x i16>* %250, align 32
  %645 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %445, <16 x i16> %464) #9
  store <16 x i16> %645, <16 x i16>* %364, align 32
  %646 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %445, <16 x i16> %464) #9
  store <16 x i16> %646, <16 x i16>* %362, align 32
  %647 = shufflevector <16 x i16> %511, <16 x i16> %567, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %648 = shufflevector <16 x i16> %511, <16 x i16> %567, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %649 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %647, <16 x i16> %430) #9
  %650 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %648, <16 x i16> %430) #9
  %651 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %647, <16 x i16> %433) #9
  %652 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %648, <16 x i16> %433) #9
  %653 = add <8 x i32> %649, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %654 = add <8 x i32> %650, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %655 = add <8 x i32> %651, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %656 = add <8 x i32> %652, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %657 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %653, i32 %425) #9
  %658 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %654, i32 %425) #9
  %659 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %655, i32 %425) #9
  %660 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %656, i32 %425) #9
  %661 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %657, <8 x i32> %658) #9
  store <16 x i16> %661, <16 x i16>* %287, align 32
  %662 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %659, <8 x i32> %660) #9
  store <16 x i16> %662, <16 x i16>* %313, align 32
  %663 = shufflevector <16 x i16> %507, <16 x i16> %563, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %664 = shufflevector <16 x i16> %507, <16 x i16> %563, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %665 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %663, <16 x i16> %430) #9
  %666 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %664, <16 x i16> %430) #9
  %667 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %663, <16 x i16> %433) #9
  %668 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %664, <16 x i16> %433) #9
  %669 = add <8 x i32> %665, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %670 = add <8 x i32> %666, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %671 = add <8 x i32> %667, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %672 = add <8 x i32> %668, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %673 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %669, i32 %425) #9
  %674 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %670, i32 %425) #9
  %675 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %671, i32 %425) #9
  %676 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %672, i32 %425) #9
  %677 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %673, <8 x i32> %674) #9
  store <16 x i16> %677, <16 x i16>* %118, align 32
  %678 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %675, <8 x i32> %676) #9
  store <16 x i16> %678, <16 x i16>* %120, align 32
  %679 = shufflevector <16 x i16> %515, <16 x i16> %555, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %680 = shufflevector <16 x i16> %515, <16 x i16> %555, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %681 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %679, <16 x i16> %450) #9
  %682 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %680, <16 x i16> %450) #9
  %683 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %679, <16 x i16> %430) #9
  %684 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %680, <16 x i16> %430) #9
  %685 = add <8 x i32> %681, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %686 = add <8 x i32> %682, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %687 = add <8 x i32> %683, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %688 = add <8 x i32> %684, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %689 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %685, i32 %425) #9
  %690 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %686, i32 %425) #9
  %691 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %687, i32 %425) #9
  %692 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %688, i32 %425) #9
  %693 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %689, <8 x i32> %690) #9
  store <16 x i16> %693, <16 x i16>* %132, align 32
  %694 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %691, <8 x i32> %692) #9
  store <16 x i16> %694, <16 x i16>* %135, align 32
  %695 = shufflevector <16 x i16> %519, <16 x i16> %559, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %696 = shufflevector <16 x i16> %519, <16 x i16> %559, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %697 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %695, <16 x i16> %450) #9
  %698 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %696, <16 x i16> %450) #9
  %699 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %695, <16 x i16> %430) #9
  %700 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %696, <16 x i16> %430) #9
  %701 = add <8 x i32> %697, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %702 = add <8 x i32> %698, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %703 = add <8 x i32> %699, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %704 = add <8 x i32> %700, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %705 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %701, i32 %425) #9
  %706 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %702, i32 %425) #9
  %707 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %703, i32 %425) #9
  %708 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %704, i32 %425) #9
  %709 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %705, <8 x i32> %706) #9
  store <16 x i16> %709, <16 x i16>* %289, align 32
  %710 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %707, <8 x i32> %708) #9
  store <16 x i16> %710, <16 x i16>* %311, align 32
  %711 = shufflevector <16 x i16> %527, <16 x i16> %551, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %712 = shufflevector <16 x i16> %527, <16 x i16> %551, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %713 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %711, <16 x i16> %469) #9
  %714 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %712, <16 x i16> %469) #9
  %715 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %711, <16 x i16> %472) #9
  %716 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %712, <16 x i16> %472) #9
  %717 = add <8 x i32> %713, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %718 = add <8 x i32> %714, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %719 = add <8 x i32> %715, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %720 = add <8 x i32> %716, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %721 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %717, i32 %425) #9
  %722 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %718, i32 %425) #9
  %723 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %719, i32 %425) #9
  %724 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %720, i32 %425) #9
  %725 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %721, <8 x i32> %722) #9
  store <16 x i16> %725, <16 x i16>* %295, align 32
  %726 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %723, <8 x i32> %724) #9
  store <16 x i16> %726, <16 x i16>* %305, align 32
  %727 = shufflevector <16 x i16> %523, <16 x i16> %547, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %728 = shufflevector <16 x i16> %523, <16 x i16> %547, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %729 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %727, <16 x i16> %469) #9
  %730 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %728, <16 x i16> %469) #9
  %731 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %727, <16 x i16> %472) #9
  %732 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %728, <16 x i16> %472) #9
  %733 = add <8 x i32> %729, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %734 = add <8 x i32> %730, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %735 = add <8 x i32> %731, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %736 = add <8 x i32> %732, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %737 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %733, i32 %425) #9
  %738 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %734, i32 %425) #9
  %739 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %735, i32 %425) #9
  %740 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %736, i32 %425) #9
  %741 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %737, <8 x i32> %738) #9
  store <16 x i16> %741, <16 x i16>* %182, align 32
  %742 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %739, <8 x i32> %740) #9
  store <16 x i16> %742, <16 x i16>* %184, align 32
  %743 = shufflevector <16 x i16> %531, <16 x i16> %539, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %744 = shufflevector <16 x i16> %531, <16 x i16> %539, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %745 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %743, <16 x i16> %489) #9
  %746 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %744, <16 x i16> %489) #9
  %747 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %743, <16 x i16> %469) #9
  %748 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %744, <16 x i16> %469) #9
  %749 = add <8 x i32> %745, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %750 = add <8 x i32> %746, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %751 = add <8 x i32> %747, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %752 = add <8 x i32> %748, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %753 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %749, i32 %425) #9
  %754 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %750, i32 %425) #9
  %755 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %751, i32 %425) #9
  %756 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %752, i32 %425) #9
  %757 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %753, <8 x i32> %754) #9
  store <16 x i16> %757, <16 x i16>* %197, align 32
  %758 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %755, <8 x i32> %756) #9
  store <16 x i16> %758, <16 x i16>* %200, align 32
  %759 = shufflevector <16 x i16> %535, <16 x i16> %543, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %760 = shufflevector <16 x i16> %535, <16 x i16> %543, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %761 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %759, <16 x i16> %489) #9
  %762 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %760, <16 x i16> %489) #9
  %763 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %759, <16 x i16> %469) #9
  %764 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %760, <16 x i16> %469) #9
  %765 = add <8 x i32> %761, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %766 = add <8 x i32> %762, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %767 = add <8 x i32> %763, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %768 = add <8 x i32> %764, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %769 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %765, i32 %425) #9
  %770 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %766, i32 %425) #9
  %771 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %767, i32 %425) #9
  %772 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %768, i32 %425) #9
  %773 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %769, <8 x i32> %770) #9
  store <16 x i16> %773, <16 x i16>* %297, align 32
  %774 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %771, <8 x i32> %772) #9
  store <16 x i16> %774, <16 x i16>* %303, align 32
  %775 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 3
  %776 = bitcast <4 x i64>* %775 to <16 x i16>*
  store <16 x i16> %573, <16 x i16>* %776, align 32
  %777 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 2
  %778 = bitcast <4 x i64>* %777 to <16 x i16>*
  store <16 x i16> %573, <16 x i16>* %778, align 32
  %779 = load <16 x i16>, <16 x i16>* %577, align 32
  %780 = load <16 x i16>, <16 x i16>* %579, align 32
  %781 = shufflevector <16 x i16> %779, <16 x i16> %780, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %782 = shufflevector <16 x i16> %779, <16 x i16> %780, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %783 = bitcast <8 x i32> %35 to <16 x i16>
  %784 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %781, <16 x i16> %783) #9
  %785 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %782, <16 x i16> %783) #9
  %786 = bitcast <8 x i32> %11 to <16 x i16>
  %787 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %781, <16 x i16> %786) #9
  %788 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %782, <16 x i16> %786) #9
  %789 = add <8 x i32> %784, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %790 = add <8 x i32> %785, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %791 = add <8 x i32> %787, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %792 = add <8 x i32> %788, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %793 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %789, i32 %425) #9
  %794 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %790, i32 %425) #9
  %795 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %791, i32 %425) #9
  %796 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %792, i32 %425) #9
  %797 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %793, <8 x i32> %794) #9
  store <16 x i16> %797, <16 x i16>* %577, align 32
  %798 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %795, <8 x i32> %796) #9
  store <16 x i16> %798, <16 x i16>* %579, align 32
  %799 = load <16 x i16>, <16 x i16>* %328, align 32
  %800 = load <16 x i16>, <16 x i16>* %346, align 32
  %801 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %799, <16 x i16> %800) #9
  store <16 x i16> %801, <16 x i16>* %328, align 32
  %802 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %799, <16 x i16> %800) #9
  store <16 x i16> %802, <16 x i16>* %346, align 32
  %803 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %600, <16 x i16> %621) #9
  store <16 x i16> %803, <16 x i16>* %580, align 32
  %804 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %600, <16 x i16> %621) #9
  store <16 x i16> %804, <16 x i16>* %602, align 32
  %805 = load <16 x i16>, <16 x i16>* %331, align 32
  %806 = load <16 x i16>, <16 x i16>* %348, align 32
  %807 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %805, <16 x i16> %806) #9
  store <16 x i16> %807, <16 x i16>* %331, align 32
  %808 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %805, <16 x i16> %806) #9
  store <16 x i16> %808, <16 x i16>* %348, align 32
  %809 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %601, <16 x i16> %622) #9
  store <16 x i16> %809, <16 x i16>* %582, align 32
  %810 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %601, <16 x i16> %622) #9
  store <16 x i16> %810, <16 x i16>* %604, align 32
  %811 = shufflevector <16 x i16> %628, <16 x i16> %646, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %812 = shufflevector <16 x i16> %628, <16 x i16> %646, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %813 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %811, <16 x i16> %586) #9
  %814 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %812, <16 x i16> %586) #9
  %815 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %811, <16 x i16> %589) #9
  %816 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %812, <16 x i16> %589) #9
  %817 = add <8 x i32> %813, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %818 = add <8 x i32> %814, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %819 = add <8 x i32> %815, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %820 = add <8 x i32> %816, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %821 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %817, i32 %425) #9
  %822 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %818, i32 %425) #9
  %823 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %819, i32 %425) #9
  %824 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %820, i32 %425) #9
  %825 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %821, <8 x i32> %822) #9
  store <16 x i16> %825, <16 x i16>* %352, align 32
  %826 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %823, <8 x i32> %824) #9
  store <16 x i16> %826, <16 x i16>* %362, align 32
  %827 = shufflevector <16 x i16> %626, <16 x i16> %644, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %828 = shufflevector <16 x i16> %626, <16 x i16> %644, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %829 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %827, <16 x i16> %586) #9
  %830 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %828, <16 x i16> %586) #9
  %831 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %827, <16 x i16> %589) #9
  %832 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %828, <16 x i16> %589) #9
  %833 = add <8 x i32> %829, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %834 = add <8 x i32> %830, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %835 = add <8 x i32> %831, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %836 = add <8 x i32> %832, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %837 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %833, i32 %425) #9
  %838 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %834, i32 %425) #9
  %839 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %835, i32 %425) #9
  %840 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %836, i32 %425) #9
  %841 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %837, <8 x i32> %838) #9
  store <16 x i16> %841, <16 x i16>* %248, align 32
  %842 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %839, <8 x i32> %840) #9
  store <16 x i16> %842, <16 x i16>* %250, align 32
  %843 = shufflevector <16 x i16> %632, <16 x i16> %638, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %844 = shufflevector <16 x i16> %632, <16 x i16> %638, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %845 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %843, <16 x i16> %608) #9
  %846 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %844, <16 x i16> %608) #9
  %847 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %843, <16 x i16> %586) #9
  %848 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %844, <16 x i16> %586) #9
  %849 = add <8 x i32> %845, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %850 = add <8 x i32> %846, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %851 = add <8 x i32> %847, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %852 = add <8 x i32> %848, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %853 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %849, i32 %425) #9
  %854 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %850, i32 %425) #9
  %855 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %851, i32 %425) #9
  %856 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %852, i32 %425) #9
  %857 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %853, <8 x i32> %854) #9
  store <16 x i16> %857, <16 x i16>* %263, align 32
  %858 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %855, <8 x i32> %856) #9
  store <16 x i16> %858, <16 x i16>* %266, align 32
  %859 = shufflevector <16 x i16> %634, <16 x i16> %640, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %860 = shufflevector <16 x i16> %634, <16 x i16> %640, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %861 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %859, <16 x i16> %608) #9
  %862 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %860, <16 x i16> %608) #9
  %863 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %859, <16 x i16> %586) #9
  %864 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %860, <16 x i16> %586) #9
  %865 = add <8 x i32> %861, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %866 = add <8 x i32> %862, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %867 = add <8 x i32> %863, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %868 = add <8 x i32> %864, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %869 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %865, i32 %425) #9
  %870 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %866, i32 %425) #9
  %871 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %867, i32 %425) #9
  %872 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %868, i32 %425) #9
  %873 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %869, <8 x i32> %870) #9
  store <16 x i16> %873, <16 x i16>* %354, align 32
  %874 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %871, <8 x i32> %872) #9
  store <16 x i16> %874, <16 x i16>* %360, align 32
  %875 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %506, <16 x i16> %514) #9
  store <16 x i16> %875, <16 x i16>* %101, align 32
  %876 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %506, <16 x i16> %514) #9
  store <16 x i16> %876, <16 x i16>* %149, align 32
  %877 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %510, <16 x i16> %518) #9
  store <16 x i16> %877, <16 x i16>* %285, align 32
  %878 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %510, <16 x i16> %518) #9
  store <16 x i16> %878, <16 x i16>* %291, align 32
  %879 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %661, <16 x i16> %709) #9
  store <16 x i16> %879, <16 x i16>* %287, align 32
  %880 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %661, <16 x i16> %709) #9
  store <16 x i16> %880, <16 x i16>* %289, align 32
  %881 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %677, <16 x i16> %693) #9
  store <16 x i16> %881, <16 x i16>* %118, align 32
  %882 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %677, <16 x i16> %693) #9
  store <16 x i16> %882, <16 x i16>* %132, align 32
  %883 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %530, <16 x i16> %522) #9
  store <16 x i16> %883, <16 x i16>* %215, align 32
  %884 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %530, <16 x i16> %522) #9
  store <16 x i16> %884, <16 x i16>* %164, align 32
  %885 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %534, <16 x i16> %526) #9
  store <16 x i16> %885, <16 x i16>* %299, align 32
  %886 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %534, <16 x i16> %526) #9
  store <16 x i16> %886, <16 x i16>* %293, align 32
  %887 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %773, <16 x i16> %725) #9
  store <16 x i16> %887, <16 x i16>* %297, align 32
  %888 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %773, <16 x i16> %725) #9
  store <16 x i16> %888, <16 x i16>* %295, align 32
  %889 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %757, <16 x i16> %741) #9
  store <16 x i16> %889, <16 x i16>* %197, align 32
  %890 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %757, <16 x i16> %741) #9
  store <16 x i16> %890, <16 x i16>* %182, align 32
  %891 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %538, <16 x i16> %546) #9
  store <16 x i16> %891, <16 x i16>* %217, align 32
  %892 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %538, <16 x i16> %546) #9
  store <16 x i16> %892, <16 x i16>* %167, align 32
  %893 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %542, <16 x i16> %550) #9
  store <16 x i16> %893, <16 x i16>* %301, align 32
  %894 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %542, <16 x i16> %550) #9
  store <16 x i16> %894, <16 x i16>* %307, align 32
  %895 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %774, <16 x i16> %726) #9
  store <16 x i16> %895, <16 x i16>* %303, align 32
  %896 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %774, <16 x i16> %726) #9
  store <16 x i16> %896, <16 x i16>* %305, align 32
  %897 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %758, <16 x i16> %742) #9
  store <16 x i16> %897, <16 x i16>* %200, align 32
  %898 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %758, <16 x i16> %742) #9
  store <16 x i16> %898, <16 x i16>* %184, align 32
  %899 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %562, <16 x i16> %554) #9
  store <16 x i16> %899, <16 x i16>* %104, align 32
  %900 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %562, <16 x i16> %554) #9
  store <16 x i16> %900, <16 x i16>* %151, align 32
  %901 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %566, <16 x i16> %558) #9
  store <16 x i16> %901, <16 x i16>* %315, align 32
  %902 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %566, <16 x i16> %558) #9
  store <16 x i16> %902, <16 x i16>* %309, align 32
  %903 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %662, <16 x i16> %710) #9
  store <16 x i16> %903, <16 x i16>* %313, align 32
  %904 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %662, <16 x i16> %710) #9
  store <16 x i16> %904, <16 x i16>* %311, align 32
  %905 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %678, <16 x i16> %694) #9
  store <16 x i16> %905, <16 x i16>* %120, align 32
  %906 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %678, <16 x i16> %694) #9
  store <16 x i16> %906, <16 x i16>* %135, align 32
  %907 = load <16 x i16>, <16 x i16>* %571, align 32
  %908 = load <16 x i16>, <16 x i16>* %380, align 32
  %909 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %907, <16 x i16> %908) #9
  store <16 x i16> %909, <16 x i16>* %571, align 32
  %910 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %907, <16 x i16> %908) #9
  store <16 x i16> %910, <16 x i16>* %380, align 32
  %911 = load <16 x i16>, <16 x i16>* %575, align 32
  %912 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %911, <16 x i16> %798) #9
  store <16 x i16> %912, <16 x i16>* %575, align 32
  %913 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %911, <16 x i16> %798) #9
  store <16 x i16> %913, <16 x i16>* %579, align 32
  %914 = load <16 x i16>, <16 x i16>* %778, align 32
  %915 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %914, <16 x i16> %797) #9
  store <16 x i16> %915, <16 x i16>* %778, align 32
  %916 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %914, <16 x i16> %797) #9
  store <16 x i16> %916, <16 x i16>* %577, align 32
  %917 = load <16 x i16>, <16 x i16>* %776, align 32
  %918 = load <16 x i16>, <16 x i16>* %375, align 32
  %919 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %917, <16 x i16> %918) #9
  store <16 x i16> %919, <16 x i16>* %776, align 32
  %920 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %917, <16 x i16> %918) #9
  store <16 x i16> %920, <16 x i16>* %375, align 32
  %921 = shufflevector <16 x i16> %804, <16 x i16> %810, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %922 = shufflevector <16 x i16> %804, <16 x i16> %810, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %923 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %921, <16 x i16> %783) #9
  %924 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %922, <16 x i16> %783) #9
  %925 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %921, <16 x i16> %786) #9
  %926 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %922, <16 x i16> %786) #9
  %927 = add <8 x i32> %923, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %928 = add <8 x i32> %924, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %929 = add <8 x i32> %925, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %930 = add <8 x i32> %926, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %931 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %927, i32 %425) #9
  %932 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %928, i32 %425) #9
  %933 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %929, i32 %425) #9
  %934 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %930, i32 %425) #9
  %935 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %931, <8 x i32> %932) #9
  store <16 x i16> %935, <16 x i16>* %602, align 32
  %936 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %933, <8 x i32> %934) #9
  store <16 x i16> %936, <16 x i16>* %604, align 32
  %937 = shufflevector <16 x i16> %802, <16 x i16> %808, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %938 = shufflevector <16 x i16> %802, <16 x i16> %808, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %939 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %937, <16 x i16> %783) #9
  %940 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %938, <16 x i16> %783) #9
  %941 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %937, <16 x i16> %786) #9
  %942 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %938, <16 x i16> %786) #9
  %943 = add <8 x i32> %939, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %944 = add <8 x i32> %940, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %945 = add <8 x i32> %941, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %946 = add <8 x i32> %942, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %947 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %943, i32 %425) #9
  %948 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %944, i32 %425) #9
  %949 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %945, i32 %425) #9
  %950 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %946, i32 %425) #9
  %951 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %947, <8 x i32> %948) #9
  store <16 x i16> %951, <16 x i16>* %346, align 32
  %952 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %949, <8 x i32> %950) #9
  store <16 x i16> %952, <16 x i16>* %348, align 32
  %953 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %625, <16 x i16> %631) #9
  store <16 x i16> %953, <16 x i16>* %230, align 32
  %954 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %625, <16 x i16> %631) #9
  store <16 x i16> %954, <16 x i16>* %281, align 32
  %955 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %627, <16 x i16> %633) #9
  store <16 x i16> %955, <16 x i16>* %350, align 32
  %956 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %627, <16 x i16> %633) #9
  store <16 x i16> %956, <16 x i16>* %356, align 32
  %957 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %825, <16 x i16> %873) #9
  store <16 x i16> %957, <16 x i16>* %352, align 32
  %958 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %825, <16 x i16> %873) #9
  store <16 x i16> %958, <16 x i16>* %354, align 32
  %959 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %841, <16 x i16> %857) #9
  store <16 x i16> %959, <16 x i16>* %248, align 32
  %960 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %841, <16 x i16> %857) #9
  store <16 x i16> %960, <16 x i16>* %263, align 32
  %961 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %643, <16 x i16> %637) #9
  store <16 x i16> %961, <16 x i16>* %233, align 32
  %962 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %643, <16 x i16> %637) #9
  store <16 x i16> %962, <16 x i16>* %283, align 32
  %963 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %645, <16 x i16> %639) #9
  store <16 x i16> %963, <16 x i16>* %364, align 32
  %964 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %645, <16 x i16> %639) #9
  store <16 x i16> %964, <16 x i16>* %358, align 32
  %965 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %826, <16 x i16> %874) #9
  store <16 x i16> %965, <16 x i16>* %362, align 32
  %966 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %826, <16 x i16> %874) #9
  store <16 x i16> %966, <16 x i16>* %360, align 32
  %967 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %842, <16 x i16> %858) #9
  store <16 x i16> %967, <16 x i16>* %250, align 32
  %968 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %842, <16 x i16> %858) #9
  store <16 x i16> %968, <16 x i16>* %266, align 32
  %969 = shufflevector <16 x i16> %882, <16 x i16> %906, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %970 = shufflevector <16 x i16> %882, <16 x i16> %906, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %971 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %969, <16 x i16> %586) #9
  %972 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %970, <16 x i16> %586) #9
  %973 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %969, <16 x i16> %589) #9
  %974 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %970, <16 x i16> %589) #9
  %975 = add <8 x i32> %971, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %976 = add <8 x i32> %972, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %977 = add <8 x i32> %973, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %978 = add <8 x i32> %974, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %979 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %975, i32 %425) #9
  %980 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %976, i32 %425) #9
  %981 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %977, i32 %425) #9
  %982 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %978, i32 %425) #9
  %983 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %979, <8 x i32> %980) #9
  store <16 x i16> %983, <16 x i16>* %132, align 32
  %984 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %981, <8 x i32> %982) #9
  store <16 x i16> %984, <16 x i16>* %135, align 32
  %985 = shufflevector <16 x i16> %880, <16 x i16> %904, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %986 = shufflevector <16 x i16> %880, <16 x i16> %904, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %987 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %985, <16 x i16> %586) #9
  %988 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %986, <16 x i16> %586) #9
  %989 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %985, <16 x i16> %589) #9
  %990 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %986, <16 x i16> %589) #9
  %991 = add <8 x i32> %987, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %992 = add <8 x i32> %988, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %993 = add <8 x i32> %989, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %994 = add <8 x i32> %990, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %995 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %991, i32 %425) #9
  %996 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %992, i32 %425) #9
  %997 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %993, i32 %425) #9
  %998 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %994, i32 %425) #9
  %999 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %995, <8 x i32> %996) #9
  store <16 x i16> %999, <16 x i16>* %289, align 32
  %1000 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %997, <8 x i32> %998) #9
  store <16 x i16> %1000, <16 x i16>* %311, align 32
  %1001 = shufflevector <16 x i16> %878, <16 x i16> %902, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1002 = shufflevector <16 x i16> %878, <16 x i16> %902, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1003 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1001, <16 x i16> %586) #9
  %1004 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1002, <16 x i16> %586) #9
  %1005 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1001, <16 x i16> %589) #9
  %1006 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1002, <16 x i16> %589) #9
  %1007 = add <8 x i32> %1003, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1008 = add <8 x i32> %1004, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1009 = add <8 x i32> %1005, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1010 = add <8 x i32> %1006, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1011 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1007, i32 %425) #9
  %1012 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1008, i32 %425) #9
  %1013 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1009, i32 %425) #9
  %1014 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1010, i32 %425) #9
  %1015 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1011, <8 x i32> %1012) #9
  store <16 x i16> %1015, <16 x i16>* %291, align 32
  %1016 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1013, <8 x i32> %1014) #9
  store <16 x i16> %1016, <16 x i16>* %309, align 32
  %1017 = shufflevector <16 x i16> %876, <16 x i16> %900, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1018 = shufflevector <16 x i16> %876, <16 x i16> %900, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1019 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1017, <16 x i16> %586) #9
  %1020 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1018, <16 x i16> %586) #9
  %1021 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1017, <16 x i16> %589) #9
  %1022 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1018, <16 x i16> %589) #9
  %1023 = add <8 x i32> %1019, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1024 = add <8 x i32> %1020, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1025 = add <8 x i32> %1021, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1026 = add <8 x i32> %1022, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1027 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1023, i32 %425) #9
  %1028 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1024, i32 %425) #9
  %1029 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1025, i32 %425) #9
  %1030 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1026, i32 %425) #9
  %1031 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1027, <8 x i32> %1028) #9
  store <16 x i16> %1031, <16 x i16>* %149, align 32
  %1032 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1029, <8 x i32> %1030) #9
  store <16 x i16> %1032, <16 x i16>* %151, align 32
  %1033 = shufflevector <16 x i16> %884, <16 x i16> %892, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1034 = shufflevector <16 x i16> %884, <16 x i16> %892, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1035 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1033, <16 x i16> %608) #9
  %1036 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1034, <16 x i16> %608) #9
  %1037 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1033, <16 x i16> %586) #9
  %1038 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1034, <16 x i16> %586) #9
  %1039 = add <8 x i32> %1035, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1040 = add <8 x i32> %1036, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1041 = add <8 x i32> %1037, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1042 = add <8 x i32> %1038, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1043 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1039, i32 %425) #9
  %1044 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1040, i32 %425) #9
  %1045 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1041, i32 %425) #9
  %1046 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1042, i32 %425) #9
  %1047 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1043, <8 x i32> %1044) #9
  store <16 x i16> %1047, <16 x i16>* %164, align 32
  %1048 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1045, <8 x i32> %1046) #9
  store <16 x i16> %1048, <16 x i16>* %167, align 32
  %1049 = shufflevector <16 x i16> %886, <16 x i16> %894, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1050 = shufflevector <16 x i16> %886, <16 x i16> %894, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1051 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1049, <16 x i16> %608) #9
  %1052 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1050, <16 x i16> %608) #9
  %1053 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1049, <16 x i16> %586) #9
  %1054 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1050, <16 x i16> %586) #9
  %1055 = add <8 x i32> %1051, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1056 = add <8 x i32> %1052, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1057 = add <8 x i32> %1053, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1058 = add <8 x i32> %1054, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1059 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1055, i32 %425) #9
  %1060 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1056, i32 %425) #9
  %1061 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1057, i32 %425) #9
  %1062 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1058, i32 %425) #9
  %1063 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1059, <8 x i32> %1060) #9
  store <16 x i16> %1063, <16 x i16>* %293, align 32
  %1064 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1061, <8 x i32> %1062) #9
  store <16 x i16> %1064, <16 x i16>* %307, align 32
  %1065 = shufflevector <16 x i16> %888, <16 x i16> %896, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1066 = shufflevector <16 x i16> %888, <16 x i16> %896, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1067 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1065, <16 x i16> %608) #9
  %1068 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1066, <16 x i16> %608) #9
  %1069 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1065, <16 x i16> %586) #9
  %1070 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1066, <16 x i16> %586) #9
  %1071 = add <8 x i32> %1067, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1072 = add <8 x i32> %1068, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1073 = add <8 x i32> %1069, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1074 = add <8 x i32> %1070, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1075 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1071, i32 %425) #9
  %1076 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1072, i32 %425) #9
  %1077 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1073, i32 %425) #9
  %1078 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1074, i32 %425) #9
  %1079 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1075, <8 x i32> %1076) #9
  store <16 x i16> %1079, <16 x i16>* %295, align 32
  %1080 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1077, <8 x i32> %1078) #9
  store <16 x i16> %1080, <16 x i16>* %305, align 32
  %1081 = shufflevector <16 x i16> %890, <16 x i16> %898, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1082 = shufflevector <16 x i16> %890, <16 x i16> %898, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1083 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1081, <16 x i16> %608) #9
  %1084 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1082, <16 x i16> %608) #9
  %1085 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1081, <16 x i16> %586) #9
  %1086 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1082, <16 x i16> %586) #9
  %1087 = add <8 x i32> %1083, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1088 = add <8 x i32> %1084, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1089 = add <8 x i32> %1085, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1090 = add <8 x i32> %1086, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1091 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1087, i32 %425) #9
  %1092 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1088, i32 %425) #9
  %1093 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1089, i32 %425) #9
  %1094 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1090, i32 %425) #9
  %1095 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1091, <8 x i32> %1092) #9
  store <16 x i16> %1095, <16 x i16>* %182, align 32
  %1096 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1093, <8 x i32> %1094) #9
  store <16 x i16> %1096, <16 x i16>* %184, align 32
  %1097 = load <16 x i16>, <16 x i16>* %331, align 32
  %1098 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %909, <16 x i16> %1097) #9
  store <16 x i16> %1098, <16 x i16>* %571, align 32
  %1099 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %909, <16 x i16> %1097) #9
  store <16 x i16> %1099, <16 x i16>* %331, align 32
  %1100 = load <16 x i16>, <16 x i16>* %575, align 32
  %1101 = load <16 x i16>, <16 x i16>* %582, align 32
  %1102 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1100, <16 x i16> %1101) #9
  store <16 x i16> %1102, <16 x i16>* %575, align 32
  %1103 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1100, <16 x i16> %1101) #9
  store <16 x i16> %1103, <16 x i16>* %582, align 32
  %1104 = load <16 x i16>, <16 x i16>* %778, align 32
  %1105 = load <16 x i16>, <16 x i16>* %604, align 32
  %1106 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1104, <16 x i16> %1105) #9
  store <16 x i16> %1106, <16 x i16>* %778, align 32
  %1107 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1104, <16 x i16> %1105) #9
  store <16 x i16> %1107, <16 x i16>* %604, align 32
  %1108 = load <16 x i16>, <16 x i16>* %776, align 32
  %1109 = load <16 x i16>, <16 x i16>* %348, align 32
  %1110 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1108, <16 x i16> %1109) #9
  store <16 x i16> %1110, <16 x i16>* %776, align 32
  %1111 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1108, <16 x i16> %1109) #9
  store <16 x i16> %1111, <16 x i16>* %348, align 32
  %1112 = load <16 x i16>, <16 x i16>* %375, align 32
  %1113 = load <16 x i16>, <16 x i16>* %346, align 32
  %1114 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1112, <16 x i16> %1113) #9
  store <16 x i16> %1114, <16 x i16>* %375, align 32
  %1115 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1112, <16 x i16> %1113) #9
  store <16 x i16> %1115, <16 x i16>* %346, align 32
  %1116 = load <16 x i16>, <16 x i16>* %577, align 32
  %1117 = load <16 x i16>, <16 x i16>* %602, align 32
  %1118 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1116, <16 x i16> %1117) #9
  store <16 x i16> %1118, <16 x i16>* %577, align 32
  %1119 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1116, <16 x i16> %1117) #9
  store <16 x i16> %1119, <16 x i16>* %602, align 32
  %1120 = load <16 x i16>, <16 x i16>* %579, align 32
  %1121 = load <16 x i16>, <16 x i16>* %580, align 32
  %1122 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1120, <16 x i16> %1121) #9
  store <16 x i16> %1122, <16 x i16>* %579, align 32
  %1123 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1120, <16 x i16> %1121) #9
  store <16 x i16> %1123, <16 x i16>* %580, align 32
  %1124 = load <16 x i16>, <16 x i16>* %380, align 32
  %1125 = load <16 x i16>, <16 x i16>* %328, align 32
  %1126 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1124, <16 x i16> %1125) #9
  store <16 x i16> %1126, <16 x i16>* %380, align 32
  %1127 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1124, <16 x i16> %1125) #9
  store <16 x i16> %1127, <16 x i16>* %328, align 32
  %1128 = shufflevector <16 x i16> %960, <16 x i16> %968, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1129 = shufflevector <16 x i16> %960, <16 x i16> %968, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1130 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1128, <16 x i16> %783) #9
  %1131 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1129, <16 x i16> %783) #9
  %1132 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1128, <16 x i16> %786) #9
  %1133 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1129, <16 x i16> %786) #9
  %1134 = add <8 x i32> %1130, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1135 = add <8 x i32> %1131, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1136 = add <8 x i32> %1132, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1137 = add <8 x i32> %1133, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1138 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1134, i32 %425) #9
  %1139 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1135, i32 %425) #9
  %1140 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1136, i32 %425) #9
  %1141 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1137, i32 %425) #9
  %1142 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1138, <8 x i32> %1139) #9
  store <16 x i16> %1142, <16 x i16>* %263, align 32
  %1143 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1140, <8 x i32> %1141) #9
  store <16 x i16> %1143, <16 x i16>* %266, align 32
  %1144 = shufflevector <16 x i16> %958, <16 x i16> %966, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1145 = shufflevector <16 x i16> %958, <16 x i16> %966, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1146 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1144, <16 x i16> %783) #9
  %1147 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1145, <16 x i16> %783) #9
  %1148 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1144, <16 x i16> %786) #9
  %1149 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1145, <16 x i16> %786) #9
  %1150 = add <8 x i32> %1146, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1151 = add <8 x i32> %1147, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1152 = add <8 x i32> %1148, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1153 = add <8 x i32> %1149, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1154 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1150, i32 %425) #9
  %1155 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1151, i32 %425) #9
  %1156 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1152, i32 %425) #9
  %1157 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1153, i32 %425) #9
  %1158 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1154, <8 x i32> %1155) #9
  store <16 x i16> %1158, <16 x i16>* %354, align 32
  %1159 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1156, <8 x i32> %1157) #9
  store <16 x i16> %1159, <16 x i16>* %360, align 32
  %1160 = shufflevector <16 x i16> %956, <16 x i16> %964, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1161 = shufflevector <16 x i16> %956, <16 x i16> %964, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1162 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1160, <16 x i16> %783) #9
  %1163 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1161, <16 x i16> %783) #9
  %1164 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1160, <16 x i16> %786) #9
  %1165 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1161, <16 x i16> %786) #9
  %1166 = add <8 x i32> %1162, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1167 = add <8 x i32> %1163, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1168 = add <8 x i32> %1164, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1169 = add <8 x i32> %1165, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1170 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1166, i32 %425) #9
  %1171 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1167, i32 %425) #9
  %1172 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1168, i32 %425) #9
  %1173 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1169, i32 %425) #9
  %1174 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1170, <8 x i32> %1171) #9
  store <16 x i16> %1174, <16 x i16>* %356, align 32
  %1175 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1172, <8 x i32> %1173) #9
  store <16 x i16> %1175, <16 x i16>* %358, align 32
  %1176 = shufflevector <16 x i16> %954, <16 x i16> %962, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1177 = shufflevector <16 x i16> %954, <16 x i16> %962, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1178 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1176, <16 x i16> %783) #9
  %1179 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1177, <16 x i16> %783) #9
  %1180 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1176, <16 x i16> %786) #9
  %1181 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1177, <16 x i16> %786) #9
  %1182 = add <8 x i32> %1178, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1183 = add <8 x i32> %1179, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1184 = add <8 x i32> %1180, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1185 = add <8 x i32> %1181, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1186 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1182, i32 %425) #9
  %1187 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1183, i32 %425) #9
  %1188 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1184, i32 %425) #9
  %1189 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1185, i32 %425) #9
  %1190 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1186, <8 x i32> %1187) #9
  store <16 x i16> %1190, <16 x i16>* %281, align 32
  %1191 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1188, <8 x i32> %1189) #9
  %1192 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %875, <16 x i16> %883) #9
  store <16 x i16> %1192, <16 x i16>* %101, align 32
  %1193 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %875, <16 x i16> %883) #9
  store <16 x i16> %1193, <16 x i16>* %215, align 32
  %1194 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %877, <16 x i16> %885) #9
  store <16 x i16> %1194, <16 x i16>* %285, align 32
  %1195 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %877, <16 x i16> %885) #9
  store <16 x i16> %1195, <16 x i16>* %299, align 32
  %1196 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %879, <16 x i16> %887) #9
  store <16 x i16> %1196, <16 x i16>* %287, align 32
  %1197 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %879, <16 x i16> %887) #9
  store <16 x i16> %1197, <16 x i16>* %297, align 32
  %1198 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %881, <16 x i16> %889) #9
  store <16 x i16> %1198, <16 x i16>* %118, align 32
  %1199 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %881, <16 x i16> %889) #9
  store <16 x i16> %1199, <16 x i16>* %197, align 32
  %1200 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %983, <16 x i16> %1095) #9
  store <16 x i16> %1200, <16 x i16>* %132, align 32
  %1201 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %983, <16 x i16> %1095) #9
  store <16 x i16> %1201, <16 x i16>* %182, align 32
  %1202 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %999, <16 x i16> %1079) #9
  store <16 x i16> %1202, <16 x i16>* %289, align 32
  %1203 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %999, <16 x i16> %1079) #9
  store <16 x i16> %1203, <16 x i16>* %295, align 32
  %1204 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1015, <16 x i16> %1063) #9
  store <16 x i16> %1204, <16 x i16>* %291, align 32
  %1205 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1015, <16 x i16> %1063) #9
  store <16 x i16> %1205, <16 x i16>* %293, align 32
  %1206 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1031, <16 x i16> %1047) #9
  store <16 x i16> %1206, <16 x i16>* %149, align 32
  %1207 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1031, <16 x i16> %1047) #9
  store <16 x i16> %1207, <16 x i16>* %164, align 32
  %1208 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %899, <16 x i16> %891) #9
  store <16 x i16> %1208, <16 x i16>* %104, align 32
  %1209 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %899, <16 x i16> %891) #9
  store <16 x i16> %1209, <16 x i16>* %217, align 32
  %1210 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %901, <16 x i16> %893) #9
  store <16 x i16> %1210, <16 x i16>* %315, align 32
  %1211 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %901, <16 x i16> %893) #9
  store <16 x i16> %1211, <16 x i16>* %301, align 32
  %1212 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %903, <16 x i16> %895) #9
  store <16 x i16> %1212, <16 x i16>* %313, align 32
  %1213 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %903, <16 x i16> %895) #9
  store <16 x i16> %1213, <16 x i16>* %303, align 32
  %1214 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %905, <16 x i16> %897) #9
  store <16 x i16> %1214, <16 x i16>* %120, align 32
  %1215 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %905, <16 x i16> %897) #9
  store <16 x i16> %1215, <16 x i16>* %200, align 32
  %1216 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %984, <16 x i16> %1096) #9
  store <16 x i16> %1216, <16 x i16>* %135, align 32
  %1217 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %984, <16 x i16> %1096) #9
  store <16 x i16> %1217, <16 x i16>* %184, align 32
  %1218 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1000, <16 x i16> %1080) #9
  store <16 x i16> %1218, <16 x i16>* %311, align 32
  %1219 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1000, <16 x i16> %1080) #9
  store <16 x i16> %1219, <16 x i16>* %305, align 32
  %1220 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1016, <16 x i16> %1064) #9
  store <16 x i16> %1220, <16 x i16>* %309, align 32
  %1221 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1016, <16 x i16> %1064) #9
  store <16 x i16> %1221, <16 x i16>* %307, align 32
  %1222 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1032, <16 x i16> %1048) #9
  store <16 x i16> %1222, <16 x i16>* %151, align 32
  %1223 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1032, <16 x i16> %1048) #9
  %1224 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1098, <16 x i16> %961) #9
  store <16 x i16> %1224, <16 x i16>* %571, align 32
  %1225 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1098, <16 x i16> %961) #9
  store <16 x i16> %1225, <16 x i16>* %233, align 32
  %1226 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1102, <16 x i16> %963) #9
  store <16 x i16> %1226, <16 x i16>* %575, align 32
  %1227 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1102, <16 x i16> %963) #9
  store <16 x i16> %1227, <16 x i16>* %364, align 32
  %1228 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1106, <16 x i16> %965) #9
  store <16 x i16> %1228, <16 x i16>* %778, align 32
  %1229 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1106, <16 x i16> %965) #9
  store <16 x i16> %1229, <16 x i16>* %362, align 32
  %1230 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1110, <16 x i16> %967) #9
  store <16 x i16> %1230, <16 x i16>* %776, align 32
  %1231 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1110, <16 x i16> %967) #9
  store <16 x i16> %1231, <16 x i16>* %250, align 32
  %1232 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1114, <16 x i16> %1143) #9
  store <16 x i16> %1232, <16 x i16>* %375, align 32
  %1233 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1114, <16 x i16> %1143) #9
  store <16 x i16> %1233, <16 x i16>* %266, align 32
  %1234 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1118, <16 x i16> %1159) #9
  store <16 x i16> %1234, <16 x i16>* %577, align 32
  %1235 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1118, <16 x i16> %1159) #9
  store <16 x i16> %1235, <16 x i16>* %360, align 32
  %1236 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1122, <16 x i16> %1175) #9
  store <16 x i16> %1236, <16 x i16>* %579, align 32
  %1237 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1122, <16 x i16> %1175) #9
  store <16 x i16> %1237, <16 x i16>* %358, align 32
  %1238 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1126, <16 x i16> %1191) #9
  store <16 x i16> %1238, <16 x i16>* %380, align 32
  %1239 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1126, <16 x i16> %1191) #9
  store <16 x i16> %1239, <16 x i16>* %283, align 32
  %1240 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1127, <16 x i16> %1190) #9
  store <16 x i16> %1240, <16 x i16>* %328, align 32
  %1241 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1127, <16 x i16> %1190) #9
  store <16 x i16> %1241, <16 x i16>* %281, align 32
  %1242 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1123, <16 x i16> %1174) #9
  store <16 x i16> %1242, <16 x i16>* %580, align 32
  %1243 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1123, <16 x i16> %1174) #9
  store <16 x i16> %1243, <16 x i16>* %356, align 32
  %1244 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1119, <16 x i16> %1158) #9
  store <16 x i16> %1244, <16 x i16>* %602, align 32
  %1245 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1119, <16 x i16> %1158) #9
  store <16 x i16> %1245, <16 x i16>* %354, align 32
  %1246 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1115, <16 x i16> %1142) #9
  store <16 x i16> %1246, <16 x i16>* %346, align 32
  %1247 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1115, <16 x i16> %1142) #9
  store <16 x i16> %1247, <16 x i16>* %263, align 32
  %1248 = load <16 x i16>, <16 x i16>* %248, align 32
  %1249 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1111, <16 x i16> %1248) #9
  store <16 x i16> %1249, <16 x i16>* %348, align 32
  %1250 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1111, <16 x i16> %1248) #9
  store <16 x i16> %1250, <16 x i16>* %248, align 32
  %1251 = load <16 x i16>, <16 x i16>* %352, align 32
  %1252 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1107, <16 x i16> %1251) #9
  store <16 x i16> %1252, <16 x i16>* %604, align 32
  %1253 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1107, <16 x i16> %1251) #9
  store <16 x i16> %1253, <16 x i16>* %352, align 32
  %1254 = load <16 x i16>, <16 x i16>* %350, align 32
  %1255 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1103, <16 x i16> %1254) #9
  store <16 x i16> %1255, <16 x i16>* %582, align 32
  %1256 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1103, <16 x i16> %1254) #9
  store <16 x i16> %1256, <16 x i16>* %350, align 32
  %1257 = load <16 x i16>, <16 x i16>* %230, align 32
  %1258 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1099, <16 x i16> %1257) #9
  store <16 x i16> %1258, <16 x i16>* %331, align 32
  %1259 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1099, <16 x i16> %1257) #9
  store <16 x i16> %1259, <16 x i16>* %230, align 32
  %1260 = shufflevector <16 x i16> %1207, <16 x i16> %1223, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1261 = shufflevector <16 x i16> %1207, <16 x i16> %1223, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1262 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1260, <16 x i16> %783) #9
  %1263 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1261, <16 x i16> %783) #9
  %1264 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1260, <16 x i16> %786) #9
  %1265 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1261, <16 x i16> %786) #9
  %1266 = add <8 x i32> %1262, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1267 = add <8 x i32> %1263, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1268 = add <8 x i32> %1264, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1269 = add <8 x i32> %1265, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1270 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1266, i32 %425) #9
  %1271 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1267, i32 %425) #9
  %1272 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1268, i32 %425) #9
  %1273 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1269, i32 %425) #9
  %1274 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1270, <8 x i32> %1271) #9
  store <16 x i16> %1274, <16 x i16>* %164, align 32
  %1275 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1272, <8 x i32> %1273) #9
  store <16 x i16> %1275, <16 x i16>* %167, align 32
  %1276 = shufflevector <16 x i16> %1205, <16 x i16> %1221, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1277 = shufflevector <16 x i16> %1205, <16 x i16> %1221, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1278 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1276, <16 x i16> %783) #9
  %1279 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1277, <16 x i16> %783) #9
  %1280 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1276, <16 x i16> %786) #9
  %1281 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1277, <16 x i16> %786) #9
  %1282 = add <8 x i32> %1278, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1283 = add <8 x i32> %1279, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1284 = add <8 x i32> %1280, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1285 = add <8 x i32> %1281, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1286 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1282, i32 %425) #9
  %1287 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1283, i32 %425) #9
  %1288 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1284, i32 %425) #9
  %1289 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1285, i32 %425) #9
  %1290 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1286, <8 x i32> %1287) #9
  store <16 x i16> %1290, <16 x i16>* %293, align 32
  %1291 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1288, <8 x i32> %1289) #9
  store <16 x i16> %1291, <16 x i16>* %307, align 32
  %1292 = shufflevector <16 x i16> %1203, <16 x i16> %1219, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1293 = shufflevector <16 x i16> %1203, <16 x i16> %1219, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1294 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1292, <16 x i16> %783) #9
  %1295 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1293, <16 x i16> %783) #9
  %1296 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1292, <16 x i16> %786) #9
  %1297 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1293, <16 x i16> %786) #9
  %1298 = add <8 x i32> %1294, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1299 = add <8 x i32> %1295, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1300 = add <8 x i32> %1296, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1301 = add <8 x i32> %1297, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1302 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1298, i32 %425) #9
  %1303 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1299, i32 %425) #9
  %1304 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1300, i32 %425) #9
  %1305 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1301, i32 %425) #9
  %1306 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1302, <8 x i32> %1303) #9
  store <16 x i16> %1306, <16 x i16>* %295, align 32
  %1307 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1304, <8 x i32> %1305) #9
  store <16 x i16> %1307, <16 x i16>* %305, align 32
  %1308 = shufflevector <16 x i16> %1201, <16 x i16> %1217, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1309 = shufflevector <16 x i16> %1201, <16 x i16> %1217, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1310 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1308, <16 x i16> %783) #9
  %1311 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1309, <16 x i16> %783) #9
  %1312 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1308, <16 x i16> %786) #9
  %1313 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1309, <16 x i16> %786) #9
  %1314 = add <8 x i32> %1310, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1315 = add <8 x i32> %1311, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1316 = add <8 x i32> %1312, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1317 = add <8 x i32> %1313, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1318 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1314, i32 %425) #9
  %1319 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1315, i32 %425) #9
  %1320 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1316, i32 %425) #9
  %1321 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1317, i32 %425) #9
  %1322 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1318, <8 x i32> %1319) #9
  store <16 x i16> %1322, <16 x i16>* %182, align 32
  %1323 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1320, <8 x i32> %1321) #9
  store <16 x i16> %1323, <16 x i16>* %184, align 32
  %1324 = shufflevector <16 x i16> %1199, <16 x i16> %1215, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1325 = shufflevector <16 x i16> %1199, <16 x i16> %1215, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1326 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1324, <16 x i16> %783) #9
  %1327 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1325, <16 x i16> %783) #9
  %1328 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1324, <16 x i16> %786) #9
  %1329 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1325, <16 x i16> %786) #9
  %1330 = add <8 x i32> %1326, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1331 = add <8 x i32> %1327, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1332 = add <8 x i32> %1328, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1333 = add <8 x i32> %1329, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1334 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1330, i32 %425) #9
  %1335 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1331, i32 %425) #9
  %1336 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1332, i32 %425) #9
  %1337 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1333, i32 %425) #9
  %1338 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1334, <8 x i32> %1335) #9
  store <16 x i16> %1338, <16 x i16>* %197, align 32
  %1339 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1336, <8 x i32> %1337) #9
  store <16 x i16> %1339, <16 x i16>* %200, align 32
  %1340 = shufflevector <16 x i16> %1197, <16 x i16> %1213, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1341 = shufflevector <16 x i16> %1197, <16 x i16> %1213, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1342 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1340, <16 x i16> %783) #9
  %1343 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1341, <16 x i16> %783) #9
  %1344 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1340, <16 x i16> %786) #9
  %1345 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1341, <16 x i16> %786) #9
  %1346 = add <8 x i32> %1342, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1347 = add <8 x i32> %1343, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1348 = add <8 x i32> %1344, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1349 = add <8 x i32> %1345, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1350 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1346, i32 %425) #9
  %1351 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1347, i32 %425) #9
  %1352 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1348, i32 %425) #9
  %1353 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1349, i32 %425) #9
  %1354 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1350, <8 x i32> %1351) #9
  store <16 x i16> %1354, <16 x i16>* %297, align 32
  %1355 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1352, <8 x i32> %1353) #9
  store <16 x i16> %1355, <16 x i16>* %303, align 32
  %1356 = shufflevector <16 x i16> %1195, <16 x i16> %1211, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1357 = shufflevector <16 x i16> %1195, <16 x i16> %1211, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1358 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1356, <16 x i16> %783) #9
  %1359 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1357, <16 x i16> %783) #9
  %1360 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1356, <16 x i16> %786) #9
  %1361 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1357, <16 x i16> %786) #9
  %1362 = add <8 x i32> %1358, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1363 = add <8 x i32> %1359, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1364 = add <8 x i32> %1360, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1365 = add <8 x i32> %1361, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1366 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1362, i32 %425) #9
  %1367 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1363, i32 %425) #9
  %1368 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1364, i32 %425) #9
  %1369 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1365, i32 %425) #9
  %1370 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1366, <8 x i32> %1367) #9
  store <16 x i16> %1370, <16 x i16>* %299, align 32
  %1371 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1368, <8 x i32> %1369) #9
  store <16 x i16> %1371, <16 x i16>* %301, align 32
  %1372 = shufflevector <16 x i16> %1193, <16 x i16> %1209, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1373 = shufflevector <16 x i16> %1193, <16 x i16> %1209, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1374 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1372, <16 x i16> %783) #9
  %1375 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1373, <16 x i16> %783) #9
  %1376 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1372, <16 x i16> %786) #9
  %1377 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1373, <16 x i16> %786) #9
  %1378 = add <8 x i32> %1374, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1379 = add <8 x i32> %1375, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1380 = add <8 x i32> %1376, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1381 = add <8 x i32> %1377, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1382 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1378, i32 %425) #9
  %1383 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1379, i32 %425) #9
  %1384 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1380, i32 %425) #9
  %1385 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1381, i32 %425) #9
  %1386 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1382, <8 x i32> %1383) #9
  store <16 x i16> %1386, <16 x i16>* %215, align 32
  %1387 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1384, <8 x i32> %1385) #9
  store <16 x i16> %1387, <16 x i16>* %217, align 32
  %1388 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 63
  %1389 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1224, <16 x i16> %1208) #9
  %1390 = bitcast <4 x i64>* %1 to <16 x i16>*
  store <16 x i16> %1389, <16 x i16>* %1390, align 32
  %1391 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1224, <16 x i16> %1208) #9
  %1392 = bitcast <4 x i64>* %1388 to <16 x i16>*
  store <16 x i16> %1391, <16 x i16>* %1392, align 32
  %1393 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %1394 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 62
  %1395 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1226, <16 x i16> %1210) #9
  %1396 = bitcast <4 x i64>* %1393 to <16 x i16>*
  store <16 x i16> %1395, <16 x i16>* %1396, align 32
  %1397 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1226, <16 x i16> %1210) #9
  %1398 = bitcast <4 x i64>* %1394 to <16 x i16>*
  store <16 x i16> %1397, <16 x i16>* %1398, align 32
  %1399 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %1400 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 61
  %1401 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1228, <16 x i16> %1212) #9
  %1402 = bitcast <4 x i64>* %1399 to <16 x i16>*
  store <16 x i16> %1401, <16 x i16>* %1402, align 32
  %1403 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1228, <16 x i16> %1212) #9
  %1404 = bitcast <4 x i64>* %1400 to <16 x i16>*
  store <16 x i16> %1403, <16 x i16>* %1404, align 32
  %1405 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %1406 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 60
  %1407 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1230, <16 x i16> %1214) #9
  %1408 = bitcast <4 x i64>* %1405 to <16 x i16>*
  store <16 x i16> %1407, <16 x i16>* %1408, align 32
  %1409 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1230, <16 x i16> %1214) #9
  %1410 = bitcast <4 x i64>* %1406 to <16 x i16>*
  store <16 x i16> %1409, <16 x i16>* %1410, align 32
  %1411 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %1412 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 59
  %1413 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1232, <16 x i16> %1216) #9
  %1414 = bitcast <4 x i64>* %1411 to <16 x i16>*
  store <16 x i16> %1413, <16 x i16>* %1414, align 32
  %1415 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1232, <16 x i16> %1216) #9
  %1416 = bitcast <4 x i64>* %1412 to <16 x i16>*
  store <16 x i16> %1415, <16 x i16>* %1416, align 32
  %1417 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %1418 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 58
  %1419 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1234, <16 x i16> %1218) #9
  %1420 = bitcast <4 x i64>* %1417 to <16 x i16>*
  store <16 x i16> %1419, <16 x i16>* %1420, align 32
  %1421 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1234, <16 x i16> %1218) #9
  %1422 = bitcast <4 x i64>* %1418 to <16 x i16>*
  store <16 x i16> %1421, <16 x i16>* %1422, align 32
  %1423 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %1424 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 57
  %1425 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1236, <16 x i16> %1220) #9
  %1426 = bitcast <4 x i64>* %1423 to <16 x i16>*
  store <16 x i16> %1425, <16 x i16>* %1426, align 32
  %1427 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1236, <16 x i16> %1220) #9
  %1428 = bitcast <4 x i64>* %1424 to <16 x i16>*
  store <16 x i16> %1427, <16 x i16>* %1428, align 32
  %1429 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %1430 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 56
  %1431 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1238, <16 x i16> %1222) #9
  %1432 = bitcast <4 x i64>* %1429 to <16 x i16>*
  store <16 x i16> %1431, <16 x i16>* %1432, align 32
  %1433 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1238, <16 x i16> %1222) #9
  %1434 = bitcast <4 x i64>* %1430 to <16 x i16>*
  store <16 x i16> %1433, <16 x i16>* %1434, align 32
  %1435 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %1436 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 55
  %1437 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1240, <16 x i16> %1275) #9
  %1438 = bitcast <4 x i64>* %1435 to <16 x i16>*
  store <16 x i16> %1437, <16 x i16>* %1438, align 32
  %1439 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1240, <16 x i16> %1275) #9
  %1440 = bitcast <4 x i64>* %1436 to <16 x i16>*
  store <16 x i16> %1439, <16 x i16>* %1440, align 32
  %1441 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %1442 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 54
  %1443 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1242, <16 x i16> %1291) #9
  %1444 = bitcast <4 x i64>* %1441 to <16 x i16>*
  store <16 x i16> %1443, <16 x i16>* %1444, align 32
  %1445 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1242, <16 x i16> %1291) #9
  %1446 = bitcast <4 x i64>* %1442 to <16 x i16>*
  store <16 x i16> %1445, <16 x i16>* %1446, align 32
  %1447 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %1448 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 53
  %1449 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1244, <16 x i16> %1307) #9
  %1450 = bitcast <4 x i64>* %1447 to <16 x i16>*
  store <16 x i16> %1449, <16 x i16>* %1450, align 32
  %1451 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1244, <16 x i16> %1307) #9
  %1452 = bitcast <4 x i64>* %1448 to <16 x i16>*
  store <16 x i16> %1451, <16 x i16>* %1452, align 32
  %1453 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %1454 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 52
  %1455 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1246, <16 x i16> %1323) #9
  %1456 = bitcast <4 x i64>* %1453 to <16 x i16>*
  store <16 x i16> %1455, <16 x i16>* %1456, align 32
  %1457 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1246, <16 x i16> %1323) #9
  %1458 = bitcast <4 x i64>* %1454 to <16 x i16>*
  store <16 x i16> %1457, <16 x i16>* %1458, align 32
  %1459 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %1460 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 51
  %1461 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1249, <16 x i16> %1339) #9
  %1462 = bitcast <4 x i64>* %1459 to <16 x i16>*
  store <16 x i16> %1461, <16 x i16>* %1462, align 32
  %1463 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1249, <16 x i16> %1339) #9
  %1464 = bitcast <4 x i64>* %1460 to <16 x i16>*
  store <16 x i16> %1463, <16 x i16>* %1464, align 32
  %1465 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %1466 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 50
  %1467 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1252, <16 x i16> %1355) #9
  %1468 = bitcast <4 x i64>* %1465 to <16 x i16>*
  store <16 x i16> %1467, <16 x i16>* %1468, align 32
  %1469 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1252, <16 x i16> %1355) #9
  %1470 = bitcast <4 x i64>* %1466 to <16 x i16>*
  store <16 x i16> %1469, <16 x i16>* %1470, align 32
  %1471 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %1472 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 49
  %1473 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1255, <16 x i16> %1371) #9
  %1474 = bitcast <4 x i64>* %1471 to <16 x i16>*
  store <16 x i16> %1473, <16 x i16>* %1474, align 32
  %1475 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1255, <16 x i16> %1371) #9
  %1476 = bitcast <4 x i64>* %1472 to <16 x i16>*
  store <16 x i16> %1475, <16 x i16>* %1476, align 32
  %1477 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 15
  %1478 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 48
  %1479 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1258, <16 x i16> %1387) #9
  %1480 = bitcast <4 x i64>* %1477 to <16 x i16>*
  store <16 x i16> %1479, <16 x i16>* %1480, align 32
  %1481 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1258, <16 x i16> %1387) #9
  %1482 = bitcast <4 x i64>* %1478 to <16 x i16>*
  store <16 x i16> %1481, <16 x i16>* %1482, align 32
  %1483 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 16
  %1484 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 47
  %1485 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1259, <16 x i16> %1386) #9
  %1486 = bitcast <4 x i64>* %1483 to <16 x i16>*
  store <16 x i16> %1485, <16 x i16>* %1486, align 32
  %1487 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1259, <16 x i16> %1386) #9
  %1488 = bitcast <4 x i64>* %1484 to <16 x i16>*
  store <16 x i16> %1487, <16 x i16>* %1488, align 32
  %1489 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 17
  %1490 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 46
  %1491 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1256, <16 x i16> %1370) #9
  %1492 = bitcast <4 x i64>* %1489 to <16 x i16>*
  store <16 x i16> %1491, <16 x i16>* %1492, align 32
  %1493 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1256, <16 x i16> %1370) #9
  %1494 = bitcast <4 x i64>* %1490 to <16 x i16>*
  store <16 x i16> %1493, <16 x i16>* %1494, align 32
  %1495 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 18
  %1496 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 45
  %1497 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1253, <16 x i16> %1354) #9
  %1498 = bitcast <4 x i64>* %1495 to <16 x i16>*
  store <16 x i16> %1497, <16 x i16>* %1498, align 32
  %1499 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1253, <16 x i16> %1354) #9
  %1500 = bitcast <4 x i64>* %1496 to <16 x i16>*
  store <16 x i16> %1499, <16 x i16>* %1500, align 32
  %1501 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 19
  %1502 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 44
  %1503 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1250, <16 x i16> %1338) #9
  %1504 = bitcast <4 x i64>* %1501 to <16 x i16>*
  store <16 x i16> %1503, <16 x i16>* %1504, align 32
  %1505 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1250, <16 x i16> %1338) #9
  %1506 = bitcast <4 x i64>* %1502 to <16 x i16>*
  store <16 x i16> %1505, <16 x i16>* %1506, align 32
  %1507 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 20
  %1508 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 43
  %1509 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1247, <16 x i16> %1322) #9
  %1510 = bitcast <4 x i64>* %1507 to <16 x i16>*
  store <16 x i16> %1509, <16 x i16>* %1510, align 32
  %1511 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1247, <16 x i16> %1322) #9
  %1512 = bitcast <4 x i64>* %1508 to <16 x i16>*
  store <16 x i16> %1511, <16 x i16>* %1512, align 32
  %1513 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 21
  %1514 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 42
  %1515 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1245, <16 x i16> %1306) #9
  %1516 = bitcast <4 x i64>* %1513 to <16 x i16>*
  store <16 x i16> %1515, <16 x i16>* %1516, align 32
  %1517 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1245, <16 x i16> %1306) #9
  %1518 = bitcast <4 x i64>* %1514 to <16 x i16>*
  store <16 x i16> %1517, <16 x i16>* %1518, align 32
  %1519 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 22
  %1520 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 41
  %1521 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1243, <16 x i16> %1290) #9
  %1522 = bitcast <4 x i64>* %1519 to <16 x i16>*
  store <16 x i16> %1521, <16 x i16>* %1522, align 32
  %1523 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1243, <16 x i16> %1290) #9
  %1524 = bitcast <4 x i64>* %1520 to <16 x i16>*
  store <16 x i16> %1523, <16 x i16>* %1524, align 32
  %1525 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 23
  %1526 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 40
  %1527 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1241, <16 x i16> %1274) #9
  %1528 = bitcast <4 x i64>* %1525 to <16 x i16>*
  store <16 x i16> %1527, <16 x i16>* %1528, align 32
  %1529 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1241, <16 x i16> %1274) #9
  %1530 = bitcast <4 x i64>* %1526 to <16 x i16>*
  store <16 x i16> %1529, <16 x i16>* %1530, align 32
  %1531 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 24
  %1532 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 39
  %1533 = load <16 x i16>, <16 x i16>* %149, align 32
  %1534 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1239, <16 x i16> %1533) #9
  %1535 = bitcast <4 x i64>* %1531 to <16 x i16>*
  store <16 x i16> %1534, <16 x i16>* %1535, align 32
  %1536 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1239, <16 x i16> %1533) #9
  %1537 = bitcast <4 x i64>* %1532 to <16 x i16>*
  store <16 x i16> %1536, <16 x i16>* %1537, align 32
  %1538 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 25
  %1539 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 38
  %1540 = load <16 x i16>, <16 x i16>* %291, align 32
  %1541 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1237, <16 x i16> %1540) #9
  %1542 = bitcast <4 x i64>* %1538 to <16 x i16>*
  store <16 x i16> %1541, <16 x i16>* %1542, align 32
  %1543 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1237, <16 x i16> %1540) #9
  %1544 = bitcast <4 x i64>* %1539 to <16 x i16>*
  store <16 x i16> %1543, <16 x i16>* %1544, align 32
  %1545 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 26
  %1546 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 37
  %1547 = load <16 x i16>, <16 x i16>* %289, align 32
  %1548 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1235, <16 x i16> %1547) #9
  %1549 = bitcast <4 x i64>* %1545 to <16 x i16>*
  store <16 x i16> %1548, <16 x i16>* %1549, align 32
  %1550 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1235, <16 x i16> %1547) #9
  %1551 = bitcast <4 x i64>* %1546 to <16 x i16>*
  store <16 x i16> %1550, <16 x i16>* %1551, align 32
  %1552 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 27
  %1553 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 36
  %1554 = load <16 x i16>, <16 x i16>* %132, align 32
  %1555 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1233, <16 x i16> %1554) #9
  %1556 = bitcast <4 x i64>* %1552 to <16 x i16>*
  store <16 x i16> %1555, <16 x i16>* %1556, align 32
  %1557 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1233, <16 x i16> %1554) #9
  %1558 = bitcast <4 x i64>* %1553 to <16 x i16>*
  store <16 x i16> %1557, <16 x i16>* %1558, align 32
  %1559 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 28
  %1560 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 35
  %1561 = load <16 x i16>, <16 x i16>* %118, align 32
  %1562 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1231, <16 x i16> %1561) #9
  %1563 = bitcast <4 x i64>* %1559 to <16 x i16>*
  store <16 x i16> %1562, <16 x i16>* %1563, align 32
  %1564 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1231, <16 x i16> %1561) #9
  %1565 = bitcast <4 x i64>* %1560 to <16 x i16>*
  store <16 x i16> %1564, <16 x i16>* %1565, align 32
  %1566 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 29
  %1567 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 34
  %1568 = load <16 x i16>, <16 x i16>* %362, align 32
  %1569 = load <16 x i16>, <16 x i16>* %287, align 32
  %1570 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1568, <16 x i16> %1569) #9
  %1571 = bitcast <4 x i64>* %1566 to <16 x i16>*
  store <16 x i16> %1570, <16 x i16>* %1571, align 32
  %1572 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1568, <16 x i16> %1569) #9
  %1573 = bitcast <4 x i64>* %1567 to <16 x i16>*
  store <16 x i16> %1572, <16 x i16>* %1573, align 32
  %1574 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 30
  %1575 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 33
  %1576 = load <16 x i16>, <16 x i16>* %364, align 32
  %1577 = load <16 x i16>, <16 x i16>* %285, align 32
  %1578 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1576, <16 x i16> %1577) #9
  %1579 = bitcast <4 x i64>* %1574 to <16 x i16>*
  store <16 x i16> %1578, <16 x i16>* %1579, align 32
  %1580 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1576, <16 x i16> %1577) #9
  %1581 = bitcast <4 x i64>* %1575 to <16 x i16>*
  store <16 x i16> %1580, <16 x i16>* %1581, align 32
  %1582 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 31
  %1583 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 32
  %1584 = load <16 x i16>, <16 x i16>* %233, align 32
  %1585 = load <16 x i16>, <16 x i16>* %101, align 32
  %1586 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1584, <16 x i16> %1585) #9
  %1587 = bitcast <4 x i64>* %1582 to <16 x i16>*
  store <16 x i16> %1586, <16 x i16>* %1587, align 32
  %1588 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1584, <16 x i16> %1585) #9
  %1589 = bitcast <4 x i64>* %1583 to <16 x i16>*
  store <16 x i16> %1588, <16 x i16>* %1589, align 32
  call void @llvm.lifetime.end.p0i8(i64 2048, i8* nonnull %36) #9
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @idct64_low32_avx2(<4 x i64>* readonly, <4 x i64>*, i8 signext) #0 {
  %4 = alloca [64 x <4 x i64>], align 32
  %5 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 32), align 16
  %6 = trunc i32 %5 to i16
  %7 = and i32 %5, 65535
  %8 = shl nuw i32 %7, 16
  %9 = or i32 %8, %7
  %10 = insertelement <8 x i32> undef, i32 %9, i32 0
  %11 = shufflevector <8 x i32> %10, <8 x i32> undef, <8 x i32> zeroinitializer
  %12 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 16), align 16
  %13 = trunc i32 %12 to i16
  %14 = sub i32 0, %12
  %15 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 48), align 16
  %16 = trunc i32 %15 to i16
  %17 = and i32 %14, 65535
  %18 = and i32 %15, 65535
  %19 = shl nuw i32 %18, 16
  %20 = or i32 %19, %17
  %21 = insertelement <8 x i32> undef, i32 %20, i32 0
  %22 = shufflevector <8 x i32> %21, <8 x i32> undef, <8 x i32> zeroinitializer
  %23 = shl i32 %12, 16
  %24 = or i32 %18, %23
  %25 = insertelement <8 x i32> undef, i32 %24, i32 0
  %26 = shufflevector <8 x i32> %25, <8 x i32> undef, <8 x i32> zeroinitializer
  %27 = sub i32 0, %15
  %28 = and i32 %27, 65535
  %29 = shl nuw i32 %17, 16
  %30 = or i32 %29, %28
  %31 = insertelement <8 x i32> undef, i32 %30, i32 0
  %32 = shufflevector <8 x i32> %31, <8 x i32> undef, <8 x i32> zeroinitializer
  %33 = sub i32 0, %5
  %34 = and i32 %33, 65535
  %35 = or i32 %8, %34
  %36 = insertelement <8 x i32> undef, i32 %35, i32 0
  %37 = shufflevector <8 x i32> %36, <8 x i32> undef, <8 x i32> zeroinitializer
  %38 = bitcast [64 x <4 x i64>]* %4 to i8*
  call void @llvm.lifetime.start.p0i8(i64 2048, i8* nonnull %38) #9
  %39 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 1
  %40 = bitcast <4 x i64>* %39 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %40, i8 -86, i64 2016, i1 false)
  %41 = load <4 x i64>, <4 x i64>* %0, align 32
  %42 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 0
  store <4 x i64> %41, <4 x i64>* %42, align 32
  %43 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 16
  %44 = load <4 x i64>, <4 x i64>* %43, align 32
  %45 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 2
  store <4 x i64> %44, <4 x i64>* %45, align 32
  %46 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 8
  %47 = load <4 x i64>, <4 x i64>* %46, align 32
  %48 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 4
  store <4 x i64> %47, <4 x i64>* %48, align 32
  %49 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 24
  %50 = load <4 x i64>, <4 x i64>* %49, align 32
  %51 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 6
  store <4 x i64> %50, <4 x i64>* %51, align 32
  %52 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 4
  %53 = load <4 x i64>, <4 x i64>* %52, align 32
  %54 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 8
  store <4 x i64> %53, <4 x i64>* %54, align 32
  %55 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 20
  %56 = load <4 x i64>, <4 x i64>* %55, align 32
  %57 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 10
  store <4 x i64> %56, <4 x i64>* %57, align 32
  %58 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 12
  %59 = load <4 x i64>, <4 x i64>* %58, align 32
  %60 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 12
  store <4 x i64> %59, <4 x i64>* %60, align 32
  %61 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 28
  %62 = load <4 x i64>, <4 x i64>* %61, align 32
  %63 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 14
  store <4 x i64> %62, <4 x i64>* %63, align 32
  %64 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 2
  %65 = load <4 x i64>, <4 x i64>* %64, align 32
  %66 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 16
  store <4 x i64> %65, <4 x i64>* %66, align 32
  %67 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 18
  %68 = load <4 x i64>, <4 x i64>* %67, align 32
  %69 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 18
  store <4 x i64> %68, <4 x i64>* %69, align 32
  %70 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 10
  %71 = load <4 x i64>, <4 x i64>* %70, align 32
  %72 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 20
  store <4 x i64> %71, <4 x i64>* %72, align 32
  %73 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 26
  %74 = load <4 x i64>, <4 x i64>* %73, align 32
  %75 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 22
  store <4 x i64> %74, <4 x i64>* %75, align 32
  %76 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 6
  %77 = load <4 x i64>, <4 x i64>* %76, align 32
  %78 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 24
  store <4 x i64> %77, <4 x i64>* %78, align 32
  %79 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 22
  %80 = load <4 x i64>, <4 x i64>* %79, align 32
  %81 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 26
  store <4 x i64> %80, <4 x i64>* %81, align 32
  %82 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 14
  %83 = load <4 x i64>, <4 x i64>* %82, align 32
  %84 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 28
  store <4 x i64> %83, <4 x i64>* %84, align 32
  %85 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 30
  %86 = load <4 x i64>, <4 x i64>* %85, align 32
  %87 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 30
  store <4 x i64> %86, <4 x i64>* %87, align 32
  %88 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 1
  %89 = bitcast <4 x i64>* %88 to <16 x i16>*
  %90 = load <16 x i16>, <16 x i16>* %89, align 32
  %91 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 32
  %92 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 17
  %93 = load <4 x i64>, <4 x i64>* %92, align 32
  %94 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 34
  store <4 x i64> %93, <4 x i64>* %94, align 32
  %95 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 9
  %96 = load <4 x i64>, <4 x i64>* %95, align 32
  %97 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 36
  store <4 x i64> %96, <4 x i64>* %97, align 32
  %98 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 25
  %99 = load <4 x i64>, <4 x i64>* %98, align 32
  %100 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 38
  store <4 x i64> %99, <4 x i64>* %100, align 32
  %101 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 5
  %102 = load <4 x i64>, <4 x i64>* %101, align 32
  %103 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 40
  store <4 x i64> %102, <4 x i64>* %103, align 32
  %104 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 21
  %105 = load <4 x i64>, <4 x i64>* %104, align 32
  %106 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 42
  store <4 x i64> %105, <4 x i64>* %106, align 32
  %107 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 13
  %108 = load <4 x i64>, <4 x i64>* %107, align 32
  %109 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 44
  store <4 x i64> %108, <4 x i64>* %109, align 32
  %110 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 29
  %111 = load <4 x i64>, <4 x i64>* %110, align 32
  %112 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 46
  store <4 x i64> %111, <4 x i64>* %112, align 32
  %113 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 3
  %114 = load <4 x i64>, <4 x i64>* %113, align 32
  %115 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 48
  store <4 x i64> %114, <4 x i64>* %115, align 32
  %116 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 19
  %117 = load <4 x i64>, <4 x i64>* %116, align 32
  %118 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 50
  store <4 x i64> %117, <4 x i64>* %118, align 32
  %119 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 11
  %120 = load <4 x i64>, <4 x i64>* %119, align 32
  %121 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 52
  store <4 x i64> %120, <4 x i64>* %121, align 32
  %122 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 27
  %123 = load <4 x i64>, <4 x i64>* %122, align 32
  %124 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 54
  store <4 x i64> %123, <4 x i64>* %124, align 32
  %125 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 7
  %126 = load <4 x i64>, <4 x i64>* %125, align 32
  %127 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 56
  store <4 x i64> %126, <4 x i64>* %127, align 32
  %128 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 23
  %129 = load <4 x i64>, <4 x i64>* %128, align 32
  %130 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 58
  store <4 x i64> %129, <4 x i64>* %130, align 32
  %131 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 15
  %132 = bitcast <4 x i64>* %131 to <16 x i16>*
  %133 = load <16 x i16>, <16 x i16>* %132, align 32
  %134 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 60
  %135 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 31
  %136 = bitcast <4 x i64>* %135 to <16 x i16>*
  %137 = load <16 x i16>, <16 x i16>* %136, align 32
  %138 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 62
  %139 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 63), align 4
  %140 = trunc i32 %139 to i16
  %141 = shl i16 %140, 3
  %142 = insertelement <16 x i16> undef, i16 %141, i32 0
  %143 = shufflevector <16 x i16> %142, <16 x i16> undef, <16 x i32> zeroinitializer
  %144 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 1), align 4
  %145 = trunc i32 %144 to i16
  %146 = shl i16 %145, 3
  %147 = insertelement <16 x i16> undef, i16 %146, i32 0
  %148 = shufflevector <16 x i16> %147, <16 x i16> undef, <16 x i32> zeroinitializer
  %149 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %90, <16 x i16> %143) #9
  %150 = bitcast <4 x i64>* %91 to <16 x i16>*
  store <16 x i16> %149, <16 x i16>* %150, align 32
  %151 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %90, <16 x i16> %148) #9
  %152 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 63
  %153 = bitcast <4 x i64>* %152 to <16 x i16>*
  store <16 x i16> %151, <16 x i16>* %153, align 32
  %154 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 33), align 4
  %155 = trunc i32 %154 to i16
  %156 = shl i16 %155, 3
  %157 = sub i16 0, %156
  %158 = insertelement <16 x i16> undef, i16 %157, i32 0
  %159 = shufflevector <16 x i16> %158, <16 x i16> undef, <16 x i32> zeroinitializer
  %160 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 31), align 4
  %161 = trunc i32 %160 to i16
  %162 = shl i16 %161, 3
  %163 = insertelement <16 x i16> undef, i16 %162, i32 0
  %164 = shufflevector <16 x i16> %163, <16 x i16> undef, <16 x i32> zeroinitializer
  %165 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %137, <16 x i16> %159) #9
  %166 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 33
  %167 = bitcast <4 x i64>* %166 to <16 x i16>*
  store <16 x i16> %165, <16 x i16>* %167, align 32
  %168 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %137, <16 x i16> %164) #9
  %169 = bitcast <4 x i64>* %138 to <16 x i16>*
  store <16 x i16> %168, <16 x i16>* %169, align 32
  %170 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 47), align 4
  %171 = trunc i32 %170 to i16
  %172 = shl i16 %171, 3
  %173 = insertelement <16 x i16> undef, i16 %172, i32 0
  %174 = shufflevector <16 x i16> %173, <16 x i16> undef, <16 x i32> zeroinitializer
  %175 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 17), align 4
  %176 = trunc i32 %175 to i16
  %177 = shl i16 %176, 3
  %178 = insertelement <16 x i16> undef, i16 %177, i32 0
  %179 = shufflevector <16 x i16> %178, <16 x i16> undef, <16 x i32> zeroinitializer
  %180 = bitcast <4 x i64> %93 to <16 x i16>
  %181 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %180, <16 x i16> %174) #9
  %182 = bitcast <4 x i64>* %94 to <16 x i16>*
  store <16 x i16> %181, <16 x i16>* %182, align 32
  %183 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %180, <16 x i16> %179) #9
  %184 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 61
  %185 = bitcast <4 x i64>* %184 to <16 x i16>*
  store <16 x i16> %183, <16 x i16>* %185, align 32
  %186 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 49), align 4
  %187 = trunc i32 %186 to i16
  %188 = shl i16 %187, 3
  %189 = sub i16 0, %188
  %190 = insertelement <16 x i16> undef, i16 %189, i32 0
  %191 = shufflevector <16 x i16> %190, <16 x i16> undef, <16 x i32> zeroinitializer
  %192 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 15), align 4
  %193 = trunc i32 %192 to i16
  %194 = shl i16 %193, 3
  %195 = insertelement <16 x i16> undef, i16 %194, i32 0
  %196 = shufflevector <16 x i16> %195, <16 x i16> undef, <16 x i32> zeroinitializer
  %197 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %133, <16 x i16> %191) #9
  %198 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 35
  %199 = bitcast <4 x i64>* %198 to <16 x i16>*
  store <16 x i16> %197, <16 x i16>* %199, align 32
  %200 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %133, <16 x i16> %196) #9
  %201 = bitcast <4 x i64>* %134 to <16 x i16>*
  store <16 x i16> %200, <16 x i16>* %201, align 32
  %202 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 55), align 4
  %203 = trunc i32 %202 to i16
  %204 = shl i16 %203, 3
  %205 = insertelement <16 x i16> undef, i16 %204, i32 0
  %206 = shufflevector <16 x i16> %205, <16 x i16> undef, <16 x i32> zeroinitializer
  %207 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 9), align 4
  %208 = trunc i32 %207 to i16
  %209 = shl i16 %208, 3
  %210 = insertelement <16 x i16> undef, i16 %209, i32 0
  %211 = shufflevector <16 x i16> %210, <16 x i16> undef, <16 x i32> zeroinitializer
  %212 = bitcast <4 x i64> %96 to <16 x i16>
  %213 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %212, <16 x i16> %206) #9
  %214 = bitcast <4 x i64>* %97 to <16 x i16>*
  store <16 x i16> %213, <16 x i16>* %214, align 32
  %215 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %212, <16 x i16> %211) #9
  %216 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 59
  %217 = bitcast <4 x i64>* %216 to <16 x i16>*
  store <16 x i16> %215, <16 x i16>* %217, align 32
  %218 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 41), align 4
  %219 = trunc i32 %218 to i16
  %220 = shl i16 %219, 3
  %221 = sub i16 0, %220
  %222 = insertelement <16 x i16> undef, i16 %221, i32 0
  %223 = shufflevector <16 x i16> %222, <16 x i16> undef, <16 x i32> zeroinitializer
  %224 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 23), align 4
  %225 = trunc i32 %224 to i16
  %226 = shl i16 %225, 3
  %227 = insertelement <16 x i16> undef, i16 %226, i32 0
  %228 = shufflevector <16 x i16> %227, <16 x i16> undef, <16 x i32> zeroinitializer
  %229 = bitcast <4 x i64> %129 to <16 x i16>
  %230 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %229, <16 x i16> %223) #9
  %231 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 37
  %232 = bitcast <4 x i64>* %231 to <16 x i16>*
  store <16 x i16> %230, <16 x i16>* %232, align 32
  %233 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %229, <16 x i16> %228) #9
  %234 = bitcast <4 x i64>* %130 to <16 x i16>*
  store <16 x i16> %233, <16 x i16>* %234, align 32
  %235 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 39), align 4
  %236 = trunc i32 %235 to i16
  %237 = shl i16 %236, 3
  %238 = insertelement <16 x i16> undef, i16 %237, i32 0
  %239 = shufflevector <16 x i16> %238, <16 x i16> undef, <16 x i32> zeroinitializer
  %240 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 25), align 4
  %241 = trunc i32 %240 to i16
  %242 = shl i16 %241, 3
  %243 = insertelement <16 x i16> undef, i16 %242, i32 0
  %244 = shufflevector <16 x i16> %243, <16 x i16> undef, <16 x i32> zeroinitializer
  %245 = bitcast <4 x i64> %99 to <16 x i16>
  %246 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %245, <16 x i16> %239) #9
  %247 = bitcast <4 x i64>* %100 to <16 x i16>*
  store <16 x i16> %246, <16 x i16>* %247, align 32
  %248 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %245, <16 x i16> %244) #9
  %249 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 57
  %250 = bitcast <4 x i64>* %249 to <16 x i16>*
  store <16 x i16> %248, <16 x i16>* %250, align 32
  %251 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 57), align 4
  %252 = trunc i32 %251 to i16
  %253 = shl i16 %252, 3
  %254 = sub i16 0, %253
  %255 = insertelement <16 x i16> undef, i16 %254, i32 0
  %256 = shufflevector <16 x i16> %255, <16 x i16> undef, <16 x i32> zeroinitializer
  %257 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 7), align 4
  %258 = trunc i32 %257 to i16
  %259 = shl i16 %258, 3
  %260 = insertelement <16 x i16> undef, i16 %259, i32 0
  %261 = shufflevector <16 x i16> %260, <16 x i16> undef, <16 x i32> zeroinitializer
  %262 = bitcast <4 x i64> %126 to <16 x i16>
  %263 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %262, <16 x i16> %256) #9
  %264 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 39
  %265 = bitcast <4 x i64>* %264 to <16 x i16>*
  store <16 x i16> %263, <16 x i16>* %265, align 32
  %266 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %262, <16 x i16> %261) #9
  %267 = bitcast <4 x i64>* %127 to <16 x i16>*
  store <16 x i16> %266, <16 x i16>* %267, align 32
  %268 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 59), align 4
  %269 = trunc i32 %268 to i16
  %270 = shl i16 %269, 3
  %271 = insertelement <16 x i16> undef, i16 %270, i32 0
  %272 = shufflevector <16 x i16> %271, <16 x i16> undef, <16 x i32> zeroinitializer
  %273 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 5), align 4
  %274 = trunc i32 %273 to i16
  %275 = shl i16 %274, 3
  %276 = insertelement <16 x i16> undef, i16 %275, i32 0
  %277 = shufflevector <16 x i16> %276, <16 x i16> undef, <16 x i32> zeroinitializer
  %278 = bitcast <4 x i64> %102 to <16 x i16>
  %279 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %278, <16 x i16> %272) #9
  %280 = bitcast <4 x i64>* %103 to <16 x i16>*
  store <16 x i16> %279, <16 x i16>* %280, align 32
  %281 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %278, <16 x i16> %277) #9
  %282 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 55
  %283 = bitcast <4 x i64>* %282 to <16 x i16>*
  store <16 x i16> %281, <16 x i16>* %283, align 32
  %284 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 37), align 4
  %285 = trunc i32 %284 to i16
  %286 = shl i16 %285, 3
  %287 = sub i16 0, %286
  %288 = insertelement <16 x i16> undef, i16 %287, i32 0
  %289 = shufflevector <16 x i16> %288, <16 x i16> undef, <16 x i32> zeroinitializer
  %290 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 27), align 4
  %291 = trunc i32 %290 to i16
  %292 = shl i16 %291, 3
  %293 = insertelement <16 x i16> undef, i16 %292, i32 0
  %294 = shufflevector <16 x i16> %293, <16 x i16> undef, <16 x i32> zeroinitializer
  %295 = bitcast <4 x i64> %123 to <16 x i16>
  %296 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %295, <16 x i16> %289) #9
  %297 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 41
  %298 = bitcast <4 x i64>* %297 to <16 x i16>*
  store <16 x i16> %296, <16 x i16>* %298, align 32
  %299 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %295, <16 x i16> %294) #9
  %300 = bitcast <4 x i64>* %124 to <16 x i16>*
  store <16 x i16> %299, <16 x i16>* %300, align 32
  %301 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 43), align 4
  %302 = trunc i32 %301 to i16
  %303 = shl i16 %302, 3
  %304 = insertelement <16 x i16> undef, i16 %303, i32 0
  %305 = shufflevector <16 x i16> %304, <16 x i16> undef, <16 x i32> zeroinitializer
  %306 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 21), align 4
  %307 = trunc i32 %306 to i16
  %308 = shl i16 %307, 3
  %309 = insertelement <16 x i16> undef, i16 %308, i32 0
  %310 = shufflevector <16 x i16> %309, <16 x i16> undef, <16 x i32> zeroinitializer
  %311 = bitcast <4 x i64> %105 to <16 x i16>
  %312 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %311, <16 x i16> %305) #9
  %313 = bitcast <4 x i64>* %106 to <16 x i16>*
  store <16 x i16> %312, <16 x i16>* %313, align 32
  %314 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %311, <16 x i16> %310) #9
  %315 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 53
  %316 = bitcast <4 x i64>* %315 to <16 x i16>*
  store <16 x i16> %314, <16 x i16>* %316, align 32
  %317 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 53), align 4
  %318 = trunc i32 %317 to i16
  %319 = shl i16 %318, 3
  %320 = sub i16 0, %319
  %321 = insertelement <16 x i16> undef, i16 %320, i32 0
  %322 = shufflevector <16 x i16> %321, <16 x i16> undef, <16 x i32> zeroinitializer
  %323 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 11), align 4
  %324 = trunc i32 %323 to i16
  %325 = shl i16 %324, 3
  %326 = insertelement <16 x i16> undef, i16 %325, i32 0
  %327 = shufflevector <16 x i16> %326, <16 x i16> undef, <16 x i32> zeroinitializer
  %328 = bitcast <4 x i64> %120 to <16 x i16>
  %329 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %328, <16 x i16> %322) #9
  %330 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 43
  %331 = bitcast <4 x i64>* %330 to <16 x i16>*
  store <16 x i16> %329, <16 x i16>* %331, align 32
  %332 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %328, <16 x i16> %327) #9
  %333 = bitcast <4 x i64>* %121 to <16 x i16>*
  store <16 x i16> %332, <16 x i16>* %333, align 32
  %334 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 51), align 4
  %335 = trunc i32 %334 to i16
  %336 = shl i16 %335, 3
  %337 = insertelement <16 x i16> undef, i16 %336, i32 0
  %338 = shufflevector <16 x i16> %337, <16 x i16> undef, <16 x i32> zeroinitializer
  %339 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 13), align 4
  %340 = trunc i32 %339 to i16
  %341 = shl i16 %340, 3
  %342 = insertelement <16 x i16> undef, i16 %341, i32 0
  %343 = shufflevector <16 x i16> %342, <16 x i16> undef, <16 x i32> zeroinitializer
  %344 = bitcast <4 x i64> %108 to <16 x i16>
  %345 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %344, <16 x i16> %338) #9
  %346 = bitcast <4 x i64>* %109 to <16 x i16>*
  store <16 x i16> %345, <16 x i16>* %346, align 32
  %347 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %344, <16 x i16> %343) #9
  %348 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 51
  %349 = bitcast <4 x i64>* %348 to <16 x i16>*
  store <16 x i16> %347, <16 x i16>* %349, align 32
  %350 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 45), align 4
  %351 = trunc i32 %350 to i16
  %352 = shl i16 %351, 3
  %353 = sub i16 0, %352
  %354 = insertelement <16 x i16> undef, i16 %353, i32 0
  %355 = shufflevector <16 x i16> %354, <16 x i16> undef, <16 x i32> zeroinitializer
  %356 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 19), align 4
  %357 = trunc i32 %356 to i16
  %358 = shl i16 %357, 3
  %359 = insertelement <16 x i16> undef, i16 %358, i32 0
  %360 = shufflevector <16 x i16> %359, <16 x i16> undef, <16 x i32> zeroinitializer
  %361 = bitcast <4 x i64> %117 to <16 x i16>
  %362 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %361, <16 x i16> %355) #9
  %363 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 45
  %364 = bitcast <4 x i64>* %363 to <16 x i16>*
  store <16 x i16> %362, <16 x i16>* %364, align 32
  %365 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %361, <16 x i16> %360) #9
  %366 = bitcast <4 x i64>* %118 to <16 x i16>*
  store <16 x i16> %365, <16 x i16>* %366, align 32
  %367 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 35), align 4
  %368 = trunc i32 %367 to i16
  %369 = shl i16 %368, 3
  %370 = insertelement <16 x i16> undef, i16 %369, i32 0
  %371 = shufflevector <16 x i16> %370, <16 x i16> undef, <16 x i32> zeroinitializer
  %372 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 29), align 4
  %373 = trunc i32 %372 to i16
  %374 = shl i16 %373, 3
  %375 = insertelement <16 x i16> undef, i16 %374, i32 0
  %376 = shufflevector <16 x i16> %375, <16 x i16> undef, <16 x i32> zeroinitializer
  %377 = bitcast <4 x i64> %111 to <16 x i16>
  %378 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %377, <16 x i16> %371) #9
  %379 = bitcast <4 x i64>* %112 to <16 x i16>*
  store <16 x i16> %378, <16 x i16>* %379, align 32
  %380 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %377, <16 x i16> %376) #9
  %381 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 49
  %382 = bitcast <4 x i64>* %381 to <16 x i16>*
  store <16 x i16> %380, <16 x i16>* %382, align 32
  %383 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 61), align 4
  %384 = trunc i32 %383 to i16
  %385 = shl i16 %384, 3
  %386 = sub i16 0, %385
  %387 = insertelement <16 x i16> undef, i16 %386, i32 0
  %388 = shufflevector <16 x i16> %387, <16 x i16> undef, <16 x i32> zeroinitializer
  %389 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 3), align 4
  %390 = trunc i32 %389 to i16
  %391 = shl i16 %390, 3
  %392 = insertelement <16 x i16> undef, i16 %391, i32 0
  %393 = shufflevector <16 x i16> %392, <16 x i16> undef, <16 x i32> zeroinitializer
  %394 = bitcast <4 x i64> %114 to <16 x i16>
  %395 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %394, <16 x i16> %388) #9
  %396 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 47
  %397 = bitcast <4 x i64>* %396 to <16 x i16>*
  store <16 x i16> %395, <16 x i16>* %397, align 32
  %398 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %394, <16 x i16> %393) #9
  %399 = bitcast <4 x i64>* %115 to <16 x i16>*
  store <16 x i16> %398, <16 x i16>* %399, align 32
  %400 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 62), align 8
  %401 = trunc i32 %400 to i16
  %402 = shl i16 %401, 3
  %403 = insertelement <16 x i16> undef, i16 %402, i32 0
  %404 = shufflevector <16 x i16> %403, <16 x i16> undef, <16 x i32> zeroinitializer
  %405 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 2), align 8
  %406 = trunc i32 %405 to i16
  %407 = shl i16 %406, 3
  %408 = insertelement <16 x i16> undef, i16 %407, i32 0
  %409 = shufflevector <16 x i16> %408, <16 x i16> undef, <16 x i32> zeroinitializer
  %410 = bitcast <4 x i64> %65 to <16 x i16>
  %411 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %410, <16 x i16> %404) #9
  %412 = bitcast <4 x i64>* %66 to <16 x i16>*
  store <16 x i16> %411, <16 x i16>* %412, align 32
  %413 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %410, <16 x i16> %409) #9
  %414 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 31
  %415 = bitcast <4 x i64>* %414 to <16 x i16>*
  store <16 x i16> %413, <16 x i16>* %415, align 32
  %416 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 34), align 8
  %417 = trunc i32 %416 to i16
  %418 = shl i16 %417, 3
  %419 = sub i16 0, %418
  %420 = insertelement <16 x i16> undef, i16 %419, i32 0
  %421 = shufflevector <16 x i16> %420, <16 x i16> undef, <16 x i32> zeroinitializer
  %422 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 30), align 8
  %423 = trunc i32 %422 to i16
  %424 = shl i16 %423, 3
  %425 = insertelement <16 x i16> undef, i16 %424, i32 0
  %426 = shufflevector <16 x i16> %425, <16 x i16> undef, <16 x i32> zeroinitializer
  %427 = bitcast <4 x i64> %86 to <16 x i16>
  %428 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %427, <16 x i16> %421) #9
  %429 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 17
  %430 = bitcast <4 x i64>* %429 to <16 x i16>*
  store <16 x i16> %428, <16 x i16>* %430, align 32
  %431 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %427, <16 x i16> %426) #9
  %432 = bitcast <4 x i64>* %87 to <16 x i16>*
  store <16 x i16> %431, <16 x i16>* %432, align 32
  %433 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 46), align 8
  %434 = trunc i32 %433 to i16
  %435 = shl i16 %434, 3
  %436 = insertelement <16 x i16> undef, i16 %435, i32 0
  %437 = shufflevector <16 x i16> %436, <16 x i16> undef, <16 x i32> zeroinitializer
  %438 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 18), align 8
  %439 = trunc i32 %438 to i16
  %440 = shl i16 %439, 3
  %441 = insertelement <16 x i16> undef, i16 %440, i32 0
  %442 = shufflevector <16 x i16> %441, <16 x i16> undef, <16 x i32> zeroinitializer
  %443 = bitcast <4 x i64> %68 to <16 x i16>
  %444 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %443, <16 x i16> %437) #9
  %445 = bitcast <4 x i64>* %69 to <16 x i16>*
  store <16 x i16> %444, <16 x i16>* %445, align 32
  %446 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %443, <16 x i16> %442) #9
  %447 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 29
  %448 = bitcast <4 x i64>* %447 to <16 x i16>*
  store <16 x i16> %446, <16 x i16>* %448, align 32
  %449 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 50), align 8
  %450 = trunc i32 %449 to i16
  %451 = shl i16 %450, 3
  %452 = sub i16 0, %451
  %453 = insertelement <16 x i16> undef, i16 %452, i32 0
  %454 = shufflevector <16 x i16> %453, <16 x i16> undef, <16 x i32> zeroinitializer
  %455 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 14), align 8
  %456 = trunc i32 %455 to i16
  %457 = shl i16 %456, 3
  %458 = insertelement <16 x i16> undef, i16 %457, i32 0
  %459 = shufflevector <16 x i16> %458, <16 x i16> undef, <16 x i32> zeroinitializer
  %460 = bitcast <4 x i64> %83 to <16 x i16>
  %461 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %460, <16 x i16> %454) #9
  %462 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 19
  %463 = bitcast <4 x i64>* %462 to <16 x i16>*
  store <16 x i16> %461, <16 x i16>* %463, align 32
  %464 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %460, <16 x i16> %459) #9
  %465 = bitcast <4 x i64>* %84 to <16 x i16>*
  store <16 x i16> %464, <16 x i16>* %465, align 32
  %466 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 54), align 8
  %467 = trunc i32 %466 to i16
  %468 = shl i16 %467, 3
  %469 = insertelement <16 x i16> undef, i16 %468, i32 0
  %470 = shufflevector <16 x i16> %469, <16 x i16> undef, <16 x i32> zeroinitializer
  %471 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 10), align 8
  %472 = trunc i32 %471 to i16
  %473 = shl i16 %472, 3
  %474 = insertelement <16 x i16> undef, i16 %473, i32 0
  %475 = shufflevector <16 x i16> %474, <16 x i16> undef, <16 x i32> zeroinitializer
  %476 = bitcast <4 x i64> %71 to <16 x i16>
  %477 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %476, <16 x i16> %470) #9
  %478 = bitcast <4 x i64>* %72 to <16 x i16>*
  store <16 x i16> %477, <16 x i16>* %478, align 32
  %479 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %476, <16 x i16> %475) #9
  %480 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 27
  %481 = bitcast <4 x i64>* %480 to <16 x i16>*
  store <16 x i16> %479, <16 x i16>* %481, align 32
  %482 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 42), align 8
  %483 = trunc i32 %482 to i16
  %484 = shl i16 %483, 3
  %485 = sub i16 0, %484
  %486 = insertelement <16 x i16> undef, i16 %485, i32 0
  %487 = shufflevector <16 x i16> %486, <16 x i16> undef, <16 x i32> zeroinitializer
  %488 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 22), align 8
  %489 = trunc i32 %488 to i16
  %490 = shl i16 %489, 3
  %491 = insertelement <16 x i16> undef, i16 %490, i32 0
  %492 = shufflevector <16 x i16> %491, <16 x i16> undef, <16 x i32> zeroinitializer
  %493 = bitcast <4 x i64> %80 to <16 x i16>
  %494 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %493, <16 x i16> %487) #9
  %495 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 21
  %496 = bitcast <4 x i64>* %495 to <16 x i16>*
  store <16 x i16> %494, <16 x i16>* %496, align 32
  %497 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %493, <16 x i16> %492) #9
  %498 = bitcast <4 x i64>* %81 to <16 x i16>*
  store <16 x i16> %497, <16 x i16>* %498, align 32
  %499 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 38), align 8
  %500 = trunc i32 %499 to i16
  %501 = shl i16 %500, 3
  %502 = insertelement <16 x i16> undef, i16 %501, i32 0
  %503 = shufflevector <16 x i16> %502, <16 x i16> undef, <16 x i32> zeroinitializer
  %504 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 26), align 8
  %505 = trunc i32 %504 to i16
  %506 = shl i16 %505, 3
  %507 = insertelement <16 x i16> undef, i16 %506, i32 0
  %508 = shufflevector <16 x i16> %507, <16 x i16> undef, <16 x i32> zeroinitializer
  %509 = bitcast <4 x i64> %74 to <16 x i16>
  %510 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %509, <16 x i16> %503) #9
  %511 = bitcast <4 x i64>* %75 to <16 x i16>*
  store <16 x i16> %510, <16 x i16>* %511, align 32
  %512 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %509, <16 x i16> %508) #9
  %513 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 25
  %514 = bitcast <4 x i64>* %513 to <16 x i16>*
  store <16 x i16> %512, <16 x i16>* %514, align 32
  %515 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 58), align 8
  %516 = trunc i32 %515 to i16
  %517 = shl i16 %516, 3
  %518 = sub i16 0, %517
  %519 = insertelement <16 x i16> undef, i16 %518, i32 0
  %520 = shufflevector <16 x i16> %519, <16 x i16> undef, <16 x i32> zeroinitializer
  %521 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 6), align 8
  %522 = trunc i32 %521 to i16
  %523 = shl i16 %522, 3
  %524 = insertelement <16 x i16> undef, i16 %523, i32 0
  %525 = shufflevector <16 x i16> %524, <16 x i16> undef, <16 x i32> zeroinitializer
  %526 = bitcast <4 x i64> %77 to <16 x i16>
  %527 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %526, <16 x i16> %520) #9
  %528 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 23
  %529 = bitcast <4 x i64>* %528 to <16 x i16>*
  store <16 x i16> %527, <16 x i16>* %529, align 32
  %530 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %526, <16 x i16> %525) #9
  %531 = bitcast <4 x i64>* %78 to <16 x i16>*
  store <16 x i16> %530, <16 x i16>* %531, align 32
  %532 = load <16 x i16>, <16 x i16>* %150, align 32
  %533 = load <16 x i16>, <16 x i16>* %167, align 32
  %534 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %532, <16 x i16> %533) #9
  store <16 x i16> %534, <16 x i16>* %150, align 32
  %535 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %532, <16 x i16> %533) #9
  store <16 x i16> %535, <16 x i16>* %167, align 32
  %536 = load <16 x i16>, <16 x i16>* %199, align 32
  %537 = load <16 x i16>, <16 x i16>* %182, align 32
  %538 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %536, <16 x i16> %537) #9
  store <16 x i16> %538, <16 x i16>* %199, align 32
  %539 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %536, <16 x i16> %537) #9
  store <16 x i16> %539, <16 x i16>* %182, align 32
  %540 = load <16 x i16>, <16 x i16>* %214, align 32
  %541 = load <16 x i16>, <16 x i16>* %232, align 32
  %542 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %540, <16 x i16> %541) #9
  store <16 x i16> %542, <16 x i16>* %214, align 32
  %543 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %540, <16 x i16> %541) #9
  store <16 x i16> %543, <16 x i16>* %232, align 32
  %544 = load <16 x i16>, <16 x i16>* %265, align 32
  %545 = load <16 x i16>, <16 x i16>* %247, align 32
  %546 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %544, <16 x i16> %545) #9
  store <16 x i16> %546, <16 x i16>* %265, align 32
  %547 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %544, <16 x i16> %545) #9
  store <16 x i16> %547, <16 x i16>* %247, align 32
  %548 = load <16 x i16>, <16 x i16>* %280, align 32
  %549 = load <16 x i16>, <16 x i16>* %298, align 32
  %550 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %548, <16 x i16> %549) #9
  store <16 x i16> %550, <16 x i16>* %280, align 32
  %551 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %548, <16 x i16> %549) #9
  store <16 x i16> %551, <16 x i16>* %298, align 32
  %552 = load <16 x i16>, <16 x i16>* %331, align 32
  %553 = load <16 x i16>, <16 x i16>* %313, align 32
  %554 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %552, <16 x i16> %553) #9
  store <16 x i16> %554, <16 x i16>* %331, align 32
  %555 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %552, <16 x i16> %553) #9
  store <16 x i16> %555, <16 x i16>* %313, align 32
  %556 = load <16 x i16>, <16 x i16>* %346, align 32
  %557 = load <16 x i16>, <16 x i16>* %364, align 32
  %558 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %556, <16 x i16> %557) #9
  store <16 x i16> %558, <16 x i16>* %346, align 32
  %559 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %556, <16 x i16> %557) #9
  store <16 x i16> %559, <16 x i16>* %364, align 32
  %560 = load <16 x i16>, <16 x i16>* %397, align 32
  %561 = load <16 x i16>, <16 x i16>* %379, align 32
  %562 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %560, <16 x i16> %561) #9
  store <16 x i16> %562, <16 x i16>* %397, align 32
  %563 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %560, <16 x i16> %561) #9
  store <16 x i16> %563, <16 x i16>* %379, align 32
  %564 = load <16 x i16>, <16 x i16>* %399, align 32
  %565 = load <16 x i16>, <16 x i16>* %382, align 32
  %566 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %564, <16 x i16> %565) #9
  store <16 x i16> %566, <16 x i16>* %399, align 32
  %567 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %564, <16 x i16> %565) #9
  store <16 x i16> %567, <16 x i16>* %382, align 32
  %568 = load <16 x i16>, <16 x i16>* %349, align 32
  %569 = load <16 x i16>, <16 x i16>* %366, align 32
  %570 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %568, <16 x i16> %569) #9
  store <16 x i16> %570, <16 x i16>* %349, align 32
  %571 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %568, <16 x i16> %569) #9
  store <16 x i16> %571, <16 x i16>* %366, align 32
  %572 = load <16 x i16>, <16 x i16>* %333, align 32
  %573 = load <16 x i16>, <16 x i16>* %316, align 32
  %574 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %572, <16 x i16> %573) #9
  store <16 x i16> %574, <16 x i16>* %333, align 32
  %575 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %572, <16 x i16> %573) #9
  store <16 x i16> %575, <16 x i16>* %316, align 32
  %576 = load <16 x i16>, <16 x i16>* %283, align 32
  %577 = load <16 x i16>, <16 x i16>* %300, align 32
  %578 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %576, <16 x i16> %577) #9
  store <16 x i16> %578, <16 x i16>* %283, align 32
  %579 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %576, <16 x i16> %577) #9
  store <16 x i16> %579, <16 x i16>* %300, align 32
  %580 = load <16 x i16>, <16 x i16>* %267, align 32
  %581 = load <16 x i16>, <16 x i16>* %250, align 32
  %582 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %580, <16 x i16> %581) #9
  store <16 x i16> %582, <16 x i16>* %267, align 32
  %583 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %580, <16 x i16> %581) #9
  store <16 x i16> %583, <16 x i16>* %250, align 32
  %584 = load <16 x i16>, <16 x i16>* %217, align 32
  %585 = load <16 x i16>, <16 x i16>* %234, align 32
  %586 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %584, <16 x i16> %585) #9
  store <16 x i16> %586, <16 x i16>* %217, align 32
  %587 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %584, <16 x i16> %585) #9
  store <16 x i16> %587, <16 x i16>* %234, align 32
  %588 = load <16 x i16>, <16 x i16>* %201, align 32
  %589 = load <16 x i16>, <16 x i16>* %185, align 32
  %590 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %588, <16 x i16> %589) #9
  store <16 x i16> %590, <16 x i16>* %201, align 32
  %591 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %588, <16 x i16> %589) #9
  store <16 x i16> %591, <16 x i16>* %185, align 32
  %592 = load <16 x i16>, <16 x i16>* %153, align 32
  %593 = load <16 x i16>, <16 x i16>* %169, align 32
  %594 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %592, <16 x i16> %593) #9
  store <16 x i16> %594, <16 x i16>* %153, align 32
  %595 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %592, <16 x i16> %593) #9
  store <16 x i16> %595, <16 x i16>* %169, align 32
  %596 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 60), align 16
  %597 = trunc i32 %596 to i16
  %598 = shl i16 %597, 3
  %599 = insertelement <16 x i16> undef, i16 %598, i32 0
  %600 = shufflevector <16 x i16> %599, <16 x i16> undef, <16 x i32> zeroinitializer
  %601 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 4), align 16
  %602 = trunc i32 %601 to i16
  %603 = shl i16 %602, 3
  %604 = insertelement <16 x i16> undef, i16 %603, i32 0
  %605 = shufflevector <16 x i16> %604, <16 x i16> undef, <16 x i32> zeroinitializer
  %606 = bitcast <4 x i64>* %54 to <16 x i16>*
  %607 = load <16 x i16>, <16 x i16>* %606, align 32
  %608 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %607, <16 x i16> %600) #9
  store <16 x i16> %608, <16 x i16>* %606, align 32
  %609 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %607, <16 x i16> %605) #9
  %610 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 15
  %611 = bitcast <4 x i64>* %610 to <16 x i16>*
  store <16 x i16> %609, <16 x i16>* %611, align 32
  %612 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 36), align 16
  %613 = trunc i32 %612 to i16
  %614 = shl i16 %613, 3
  %615 = sub i16 0, %614
  %616 = insertelement <16 x i16> undef, i16 %615, i32 0
  %617 = shufflevector <16 x i16> %616, <16 x i16> undef, <16 x i32> zeroinitializer
  %618 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 28), align 16
  %619 = trunc i32 %618 to i16
  %620 = shl i16 %619, 3
  %621 = insertelement <16 x i16> undef, i16 %620, i32 0
  %622 = shufflevector <16 x i16> %621, <16 x i16> undef, <16 x i32> zeroinitializer
  %623 = bitcast <4 x i64>* %63 to <16 x i16>*
  %624 = load <16 x i16>, <16 x i16>* %623, align 32
  %625 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %624, <16 x i16> %617) #9
  %626 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 9
  %627 = bitcast <4 x i64>* %626 to <16 x i16>*
  store <16 x i16> %625, <16 x i16>* %627, align 32
  %628 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %624, <16 x i16> %622) #9
  store <16 x i16> %628, <16 x i16>* %623, align 32
  %629 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 44), align 16
  %630 = trunc i32 %629 to i16
  %631 = shl i16 %630, 3
  %632 = insertelement <16 x i16> undef, i16 %631, i32 0
  %633 = shufflevector <16 x i16> %632, <16 x i16> undef, <16 x i32> zeroinitializer
  %634 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 20), align 16
  %635 = trunc i32 %634 to i16
  %636 = shl i16 %635, 3
  %637 = insertelement <16 x i16> undef, i16 %636, i32 0
  %638 = shufflevector <16 x i16> %637, <16 x i16> undef, <16 x i32> zeroinitializer
  %639 = bitcast <4 x i64>* %57 to <16 x i16>*
  %640 = load <16 x i16>, <16 x i16>* %639, align 32
  %641 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %640, <16 x i16> %633) #9
  store <16 x i16> %641, <16 x i16>* %639, align 32
  %642 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %640, <16 x i16> %638) #9
  %643 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 13
  %644 = bitcast <4 x i64>* %643 to <16 x i16>*
  store <16 x i16> %642, <16 x i16>* %644, align 32
  %645 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 52), align 16
  %646 = trunc i32 %645 to i16
  %647 = shl i16 %646, 3
  %648 = sub i16 0, %647
  %649 = insertelement <16 x i16> undef, i16 %648, i32 0
  %650 = shufflevector <16 x i16> %649, <16 x i16> undef, <16 x i32> zeroinitializer
  %651 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 12), align 16
  %652 = trunc i32 %651 to i16
  %653 = shl i16 %652, 3
  %654 = insertelement <16 x i16> undef, i16 %653, i32 0
  %655 = shufflevector <16 x i16> %654, <16 x i16> undef, <16 x i32> zeroinitializer
  %656 = bitcast <4 x i64>* %60 to <16 x i16>*
  %657 = load <16 x i16>, <16 x i16>* %656, align 32
  %658 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %657, <16 x i16> %650) #9
  %659 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 11
  %660 = bitcast <4 x i64>* %659 to <16 x i16>*
  store <16 x i16> %658, <16 x i16>* %660, align 32
  %661 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %657, <16 x i16> %655) #9
  store <16 x i16> %661, <16 x i16>* %656, align 32
  %662 = load <16 x i16>, <16 x i16>* %412, align 32
  %663 = load <16 x i16>, <16 x i16>* %430, align 32
  %664 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %662, <16 x i16> %663) #9
  store <16 x i16> %664, <16 x i16>* %412, align 32
  %665 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %662, <16 x i16> %663) #9
  store <16 x i16> %665, <16 x i16>* %430, align 32
  %666 = load <16 x i16>, <16 x i16>* %463, align 32
  %667 = load <16 x i16>, <16 x i16>* %445, align 32
  %668 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %666, <16 x i16> %667) #9
  store <16 x i16> %668, <16 x i16>* %463, align 32
  %669 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %666, <16 x i16> %667) #9
  store <16 x i16> %669, <16 x i16>* %445, align 32
  %670 = load <16 x i16>, <16 x i16>* %478, align 32
  %671 = load <16 x i16>, <16 x i16>* %496, align 32
  %672 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %670, <16 x i16> %671) #9
  store <16 x i16> %672, <16 x i16>* %478, align 32
  %673 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %670, <16 x i16> %671) #9
  store <16 x i16> %673, <16 x i16>* %496, align 32
  %674 = load <16 x i16>, <16 x i16>* %529, align 32
  %675 = load <16 x i16>, <16 x i16>* %511, align 32
  %676 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %674, <16 x i16> %675) #9
  store <16 x i16> %676, <16 x i16>* %529, align 32
  %677 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %674, <16 x i16> %675) #9
  store <16 x i16> %677, <16 x i16>* %511, align 32
  %678 = load <16 x i16>, <16 x i16>* %531, align 32
  %679 = load <16 x i16>, <16 x i16>* %514, align 32
  %680 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %678, <16 x i16> %679) #9
  store <16 x i16> %680, <16 x i16>* %531, align 32
  %681 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %678, <16 x i16> %679) #9
  store <16 x i16> %681, <16 x i16>* %514, align 32
  %682 = load <16 x i16>, <16 x i16>* %481, align 32
  %683 = load <16 x i16>, <16 x i16>* %498, align 32
  %684 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %682, <16 x i16> %683) #9
  store <16 x i16> %684, <16 x i16>* %481, align 32
  %685 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %682, <16 x i16> %683) #9
  store <16 x i16> %685, <16 x i16>* %498, align 32
  %686 = load <16 x i16>, <16 x i16>* %465, align 32
  %687 = load <16 x i16>, <16 x i16>* %448, align 32
  %688 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %686, <16 x i16> %687) #9
  store <16 x i16> %688, <16 x i16>* %465, align 32
  %689 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %686, <16 x i16> %687) #9
  store <16 x i16> %689, <16 x i16>* %448, align 32
  %690 = load <16 x i16>, <16 x i16>* %415, align 32
  %691 = load <16 x i16>, <16 x i16>* %432, align 32
  %692 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %690, <16 x i16> %691) #9
  store <16 x i16> %692, <16 x i16>* %415, align 32
  %693 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %690, <16 x i16> %691) #9
  store <16 x i16> %693, <16 x i16>* %432, align 32
  call fastcc void @idct64_stage4_high32_avx2(<4 x i64>* nonnull %42, <4 x i64> <i64 8796093024256, i64 8796093024256, i64 8796093024256, i64 8796093024256>, i8 signext %2)
  %694 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 56), align 16
  %695 = trunc i32 %694 to i16
  %696 = shl i16 %695, 3
  %697 = insertelement <16 x i16> undef, i16 %696, i32 0
  %698 = shufflevector <16 x i16> %697, <16 x i16> undef, <16 x i32> zeroinitializer
  %699 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 8), align 16
  %700 = trunc i32 %699 to i16
  %701 = shl i16 %700, 3
  %702 = insertelement <16 x i16> undef, i16 %701, i32 0
  %703 = shufflevector <16 x i16> %702, <16 x i16> undef, <16 x i32> zeroinitializer
  %704 = bitcast <4 x i64>* %48 to <16 x i16>*
  %705 = load <16 x i16>, <16 x i16>* %704, align 32
  %706 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %705, <16 x i16> %698) #9
  store <16 x i16> %706, <16 x i16>* %704, align 32
  %707 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %705, <16 x i16> %703) #9
  %708 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 7
  %709 = bitcast <4 x i64>* %708 to <16 x i16>*
  store <16 x i16> %707, <16 x i16>* %709, align 32
  %710 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 40), align 16
  %711 = trunc i32 %710 to i16
  %712 = shl i16 %711, 3
  %713 = sub i16 0, %712
  %714 = insertelement <16 x i16> undef, i16 %713, i32 0
  %715 = shufflevector <16 x i16> %714, <16 x i16> undef, <16 x i32> zeroinitializer
  %716 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 24), align 16
  %717 = trunc i32 %716 to i16
  %718 = shl i16 %717, 3
  %719 = insertelement <16 x i16> undef, i16 %718, i32 0
  %720 = shufflevector <16 x i16> %719, <16 x i16> undef, <16 x i32> zeroinitializer
  %721 = bitcast <4 x i64>* %51 to <16 x i16>*
  %722 = load <16 x i16>, <16 x i16>* %721, align 32
  %723 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %722, <16 x i16> %715) #9
  %724 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 5
  %725 = bitcast <4 x i64>* %724 to <16 x i16>*
  store <16 x i16> %723, <16 x i16>* %725, align 32
  %726 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %722, <16 x i16> %720) #9
  store <16 x i16> %726, <16 x i16>* %721, align 32
  %727 = load <16 x i16>, <16 x i16>* %606, align 32
  %728 = load <16 x i16>, <16 x i16>* %627, align 32
  %729 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %727, <16 x i16> %728) #9
  store <16 x i16> %729, <16 x i16>* %606, align 32
  %730 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %727, <16 x i16> %728) #9
  store <16 x i16> %730, <16 x i16>* %627, align 32
  %731 = load <16 x i16>, <16 x i16>* %660, align 32
  %732 = load <16 x i16>, <16 x i16>* %639, align 32
  %733 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %731, <16 x i16> %732) #9
  store <16 x i16> %733, <16 x i16>* %660, align 32
  %734 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %731, <16 x i16> %732) #9
  store <16 x i16> %734, <16 x i16>* %639, align 32
  %735 = load <16 x i16>, <16 x i16>* %656, align 32
  %736 = load <16 x i16>, <16 x i16>* %644, align 32
  %737 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %735, <16 x i16> %736) #9
  store <16 x i16> %737, <16 x i16>* %656, align 32
  %738 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %735, <16 x i16> %736) #9
  store <16 x i16> %738, <16 x i16>* %644, align 32
  %739 = load <16 x i16>, <16 x i16>* %611, align 32
  %740 = load <16 x i16>, <16 x i16>* %623, align 32
  %741 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %739, <16 x i16> %740) #9
  store <16 x i16> %741, <16 x i16>* %611, align 32
  %742 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %739, <16 x i16> %740) #9
  store <16 x i16> %742, <16 x i16>* %623, align 32
  %743 = sub i32 0, %699
  %744 = and i32 %743, 65535
  %745 = and i32 %694, 65535
  %746 = shl nuw i32 %745, 16
  %747 = or i32 %746, %744
  %748 = insertelement <8 x i32> undef, i32 %747, i32 0
  %749 = shufflevector <8 x i32> %748, <8 x i32> undef, <8 x i32> zeroinitializer
  %750 = shl i32 %699, 16
  %751 = or i32 %750, %745
  %752 = insertelement <8 x i32> undef, i32 %751, i32 0
  %753 = shufflevector <8 x i32> %752, <8 x i32> undef, <8 x i32> zeroinitializer
  %754 = sub i32 0, %694
  %755 = and i32 %754, 65535
  %756 = shl nuw i32 %744, 16
  %757 = or i32 %756, %755
  %758 = insertelement <8 x i32> undef, i32 %757, i32 0
  %759 = shufflevector <8 x i32> %758, <8 x i32> undef, <8 x i32> zeroinitializer
  %760 = sub i32 0, %710
  %761 = and i32 %760, 65535
  %762 = and i32 %716, 65535
  %763 = shl nuw i32 %762, 16
  %764 = or i32 %763, %761
  %765 = insertelement <8 x i32> undef, i32 %764, i32 0
  %766 = shufflevector <8 x i32> %765, <8 x i32> undef, <8 x i32> zeroinitializer
  %767 = shl i32 %710, 16
  %768 = or i32 %762, %767
  %769 = insertelement <8 x i32> undef, i32 %768, i32 0
  %770 = shufflevector <8 x i32> %769, <8 x i32> undef, <8 x i32> zeroinitializer
  %771 = sub i32 0, %716
  %772 = and i32 %771, 65535
  %773 = shl nuw i32 %761, 16
  %774 = or i32 %773, %772
  %775 = insertelement <8 x i32> undef, i32 %774, i32 0
  %776 = shufflevector <8 x i32> %775, <8 x i32> undef, <8 x i32> zeroinitializer
  %777 = sext i8 %2 to i32
  %778 = load <16 x i16>, <16 x i16>* %430, align 32
  %779 = load <16 x i16>, <16 x i16>* %432, align 32
  %780 = shufflevector <16 x i16> %778, <16 x i16> %779, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %781 = shufflevector <16 x i16> %778, <16 x i16> %779, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %782 = bitcast <8 x i32> %749 to <16 x i16>
  %783 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %780, <16 x i16> %782) #9
  %784 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %781, <16 x i16> %782) #9
  %785 = bitcast <8 x i32> %753 to <16 x i16>
  %786 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %780, <16 x i16> %785) #9
  %787 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %781, <16 x i16> %785) #9
  %788 = add <8 x i32> %783, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %789 = add <8 x i32> %784, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %790 = add <8 x i32> %786, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %791 = add <8 x i32> %787, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %792 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %788, i32 %777) #9
  %793 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %789, i32 %777) #9
  %794 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %790, i32 %777) #9
  %795 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %791, i32 %777) #9
  %796 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %792, <8 x i32> %793) #9
  store <16 x i16> %796, <16 x i16>* %430, align 32
  %797 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %794, <8 x i32> %795) #9
  store <16 x i16> %797, <16 x i16>* %432, align 32
  %798 = load <16 x i16>, <16 x i16>* %445, align 32
  %799 = load <16 x i16>, <16 x i16>* %448, align 32
  %800 = shufflevector <16 x i16> %798, <16 x i16> %799, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %801 = shufflevector <16 x i16> %798, <16 x i16> %799, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %802 = bitcast <8 x i32> %759 to <16 x i16>
  %803 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %800, <16 x i16> %802) #9
  %804 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %801, <16 x i16> %802) #9
  %805 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %800, <16 x i16> %782) #9
  %806 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %801, <16 x i16> %782) #9
  %807 = add <8 x i32> %803, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %808 = add <8 x i32> %804, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %809 = add <8 x i32> %805, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %810 = add <8 x i32> %806, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %811 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %807, i32 %777) #9
  %812 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %808, i32 %777) #9
  %813 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %809, i32 %777) #9
  %814 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %810, i32 %777) #9
  %815 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %811, <8 x i32> %812) #9
  store <16 x i16> %815, <16 x i16>* %445, align 32
  %816 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %813, <8 x i32> %814) #9
  store <16 x i16> %816, <16 x i16>* %448, align 32
  %817 = load <16 x i16>, <16 x i16>* %496, align 32
  %818 = load <16 x i16>, <16 x i16>* %498, align 32
  %819 = shufflevector <16 x i16> %817, <16 x i16> %818, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %820 = shufflevector <16 x i16> %817, <16 x i16> %818, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %821 = bitcast <8 x i32> %766 to <16 x i16>
  %822 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %819, <16 x i16> %821) #9
  %823 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %820, <16 x i16> %821) #9
  %824 = bitcast <8 x i32> %770 to <16 x i16>
  %825 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %819, <16 x i16> %824) #9
  %826 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %820, <16 x i16> %824) #9
  %827 = add <8 x i32> %822, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %828 = add <8 x i32> %823, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %829 = add <8 x i32> %825, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %830 = add <8 x i32> %826, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %831 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %827, i32 %777) #9
  %832 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %828, i32 %777) #9
  %833 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %829, i32 %777) #9
  %834 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %830, i32 %777) #9
  %835 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %831, <8 x i32> %832) #9
  store <16 x i16> %835, <16 x i16>* %496, align 32
  %836 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %833, <8 x i32> %834) #9
  store <16 x i16> %836, <16 x i16>* %498, align 32
  %837 = load <16 x i16>, <16 x i16>* %511, align 32
  %838 = load <16 x i16>, <16 x i16>* %514, align 32
  %839 = shufflevector <16 x i16> %837, <16 x i16> %838, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %840 = shufflevector <16 x i16> %837, <16 x i16> %838, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %841 = bitcast <8 x i32> %776 to <16 x i16>
  %842 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %839, <16 x i16> %841) #9
  %843 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %840, <16 x i16> %841) #9
  %844 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %839, <16 x i16> %821) #9
  %845 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %840, <16 x i16> %821) #9
  %846 = add <8 x i32> %842, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %847 = add <8 x i32> %843, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %848 = add <8 x i32> %844, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %849 = add <8 x i32> %845, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %850 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %846, i32 %777) #9
  %851 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %847, i32 %777) #9
  %852 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %848, i32 %777) #9
  %853 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %849, i32 %777) #9
  %854 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %850, <8 x i32> %851) #9
  store <16 x i16> %854, <16 x i16>* %511, align 32
  %855 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %852, <8 x i32> %853) #9
  store <16 x i16> %855, <16 x i16>* %514, align 32
  %856 = load <16 x i16>, <16 x i16>* %150, align 32
  %857 = load <16 x i16>, <16 x i16>* %199, align 32
  %858 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %856, <16 x i16> %857) #9
  store <16 x i16> %858, <16 x i16>* %150, align 32
  %859 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %856, <16 x i16> %857) #9
  store <16 x i16> %859, <16 x i16>* %199, align 32
  %860 = load <16 x i16>, <16 x i16>* %167, align 32
  %861 = load <16 x i16>, <16 x i16>* %182, align 32
  %862 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %860, <16 x i16> %861) #9
  store <16 x i16> %862, <16 x i16>* %167, align 32
  %863 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %860, <16 x i16> %861) #9
  store <16 x i16> %863, <16 x i16>* %182, align 32
  %864 = load <16 x i16>, <16 x i16>* %265, align 32
  %865 = load <16 x i16>, <16 x i16>* %214, align 32
  %866 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %864, <16 x i16> %865) #9
  store <16 x i16> %866, <16 x i16>* %265, align 32
  %867 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %864, <16 x i16> %865) #9
  store <16 x i16> %867, <16 x i16>* %214, align 32
  %868 = load <16 x i16>, <16 x i16>* %247, align 32
  %869 = load <16 x i16>, <16 x i16>* %232, align 32
  %870 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %868, <16 x i16> %869) #9
  store <16 x i16> %870, <16 x i16>* %247, align 32
  %871 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %868, <16 x i16> %869) #9
  store <16 x i16> %871, <16 x i16>* %232, align 32
  %872 = load <16 x i16>, <16 x i16>* %280, align 32
  %873 = load <16 x i16>, <16 x i16>* %331, align 32
  %874 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %872, <16 x i16> %873) #9
  store <16 x i16> %874, <16 x i16>* %280, align 32
  %875 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %872, <16 x i16> %873) #9
  store <16 x i16> %875, <16 x i16>* %331, align 32
  %876 = load <16 x i16>, <16 x i16>* %298, align 32
  %877 = load <16 x i16>, <16 x i16>* %313, align 32
  %878 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %876, <16 x i16> %877) #9
  store <16 x i16> %878, <16 x i16>* %298, align 32
  %879 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %876, <16 x i16> %877) #9
  store <16 x i16> %879, <16 x i16>* %313, align 32
  %880 = load <16 x i16>, <16 x i16>* %397, align 32
  %881 = load <16 x i16>, <16 x i16>* %346, align 32
  %882 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %880, <16 x i16> %881) #9
  store <16 x i16> %882, <16 x i16>* %397, align 32
  %883 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %880, <16 x i16> %881) #9
  store <16 x i16> %883, <16 x i16>* %346, align 32
  %884 = load <16 x i16>, <16 x i16>* %379, align 32
  %885 = load <16 x i16>, <16 x i16>* %364, align 32
  %886 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %884, <16 x i16> %885) #9
  store <16 x i16> %886, <16 x i16>* %379, align 32
  %887 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %884, <16 x i16> %885) #9
  store <16 x i16> %887, <16 x i16>* %364, align 32
  %888 = load <16 x i16>, <16 x i16>* %399, align 32
  %889 = load <16 x i16>, <16 x i16>* %349, align 32
  %890 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %888, <16 x i16> %889) #9
  store <16 x i16> %890, <16 x i16>* %399, align 32
  %891 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %888, <16 x i16> %889) #9
  store <16 x i16> %891, <16 x i16>* %349, align 32
  %892 = load <16 x i16>, <16 x i16>* %382, align 32
  %893 = load <16 x i16>, <16 x i16>* %366, align 32
  %894 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %892, <16 x i16> %893) #9
  store <16 x i16> %894, <16 x i16>* %382, align 32
  %895 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %892, <16 x i16> %893) #9
  store <16 x i16> %895, <16 x i16>* %366, align 32
  %896 = load <16 x i16>, <16 x i16>* %283, align 32
  %897 = load <16 x i16>, <16 x i16>* %333, align 32
  %898 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %896, <16 x i16> %897) #9
  store <16 x i16> %898, <16 x i16>* %283, align 32
  %899 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %896, <16 x i16> %897) #9
  store <16 x i16> %899, <16 x i16>* %333, align 32
  %900 = load <16 x i16>, <16 x i16>* %300, align 32
  %901 = load <16 x i16>, <16 x i16>* %316, align 32
  %902 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %900, <16 x i16> %901) #9
  store <16 x i16> %902, <16 x i16>* %300, align 32
  %903 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %900, <16 x i16> %901) #9
  store <16 x i16> %903, <16 x i16>* %316, align 32
  %904 = load <16 x i16>, <16 x i16>* %267, align 32
  %905 = load <16 x i16>, <16 x i16>* %217, align 32
  %906 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %904, <16 x i16> %905) #9
  store <16 x i16> %906, <16 x i16>* %267, align 32
  %907 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %904, <16 x i16> %905) #9
  store <16 x i16> %907, <16 x i16>* %217, align 32
  %908 = load <16 x i16>, <16 x i16>* %250, align 32
  %909 = load <16 x i16>, <16 x i16>* %234, align 32
  %910 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %908, <16 x i16> %909) #9
  store <16 x i16> %910, <16 x i16>* %250, align 32
  %911 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %908, <16 x i16> %909) #9
  store <16 x i16> %911, <16 x i16>* %234, align 32
  %912 = load <16 x i16>, <16 x i16>* %153, align 32
  %913 = load <16 x i16>, <16 x i16>* %201, align 32
  %914 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %912, <16 x i16> %913) #9
  store <16 x i16> %914, <16 x i16>* %153, align 32
  %915 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %912, <16 x i16> %913) #9
  store <16 x i16> %915, <16 x i16>* %201, align 32
  %916 = load <16 x i16>, <16 x i16>* %169, align 32
  %917 = load <16 x i16>, <16 x i16>* %185, align 32
  %918 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %916, <16 x i16> %917) #9
  store <16 x i16> %918, <16 x i16>* %169, align 32
  %919 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %916, <16 x i16> %917) #9
  store <16 x i16> %919, <16 x i16>* %185, align 32
  %920 = shl i16 %6, 3
  %921 = insertelement <16 x i16> undef, i16 %920, i32 0
  %922 = shufflevector <16 x i16> %921, <16 x i16> undef, <16 x i32> zeroinitializer
  %923 = bitcast [64 x <4 x i64>]* %4 to <16 x i16>*
  %924 = load <16 x i16>, <16 x i16>* %923, align 32
  %925 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %924, <16 x i16> %922) #9
  store <16 x i16> %925, <16 x i16>* %923, align 32
  %926 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 1
  %927 = bitcast <4 x i64>* %926 to <16 x i16>*
  store <16 x i16> %925, <16 x i16>* %927, align 32
  %928 = shl i16 %16, 3
  %929 = insertelement <16 x i16> undef, i16 %928, i32 0
  %930 = shufflevector <16 x i16> %929, <16 x i16> undef, <16 x i32> zeroinitializer
  %931 = shl i16 %13, 3
  %932 = insertelement <16 x i16> undef, i16 %931, i32 0
  %933 = shufflevector <16 x i16> %932, <16 x i16> undef, <16 x i32> zeroinitializer
  %934 = bitcast <4 x i64>* %45 to <16 x i16>*
  %935 = load <16 x i16>, <16 x i16>* %934, align 32
  %936 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %935, <16 x i16> %930) #9
  store <16 x i16> %936, <16 x i16>* %934, align 32
  %937 = tail call <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16> %935, <16 x i16> %933) #9
  %938 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %4, i64 0, i64 3
  %939 = bitcast <4 x i64>* %938 to <16 x i16>*
  store <16 x i16> %937, <16 x i16>* %939, align 32
  %940 = load <16 x i16>, <16 x i16>* %704, align 32
  %941 = load <16 x i16>, <16 x i16>* %725, align 32
  %942 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %940, <16 x i16> %941) #9
  store <16 x i16> %942, <16 x i16>* %704, align 32
  %943 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %940, <16 x i16> %941) #9
  store <16 x i16> %943, <16 x i16>* %725, align 32
  %944 = load <16 x i16>, <16 x i16>* %709, align 32
  %945 = load <16 x i16>, <16 x i16>* %721, align 32
  %946 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %944, <16 x i16> %945) #9
  store <16 x i16> %946, <16 x i16>* %709, align 32
  %947 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %944, <16 x i16> %945) #9
  store <16 x i16> %947, <16 x i16>* %721, align 32
  %948 = shufflevector <16 x i16> %730, <16 x i16> %742, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %949 = shufflevector <16 x i16> %730, <16 x i16> %742, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %950 = bitcast <8 x i32> %22 to <16 x i16>
  %951 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %948, <16 x i16> %950) #9
  %952 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %949, <16 x i16> %950) #9
  %953 = bitcast <8 x i32> %26 to <16 x i16>
  %954 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %948, <16 x i16> %953) #9
  %955 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %949, <16 x i16> %953) #9
  %956 = add <8 x i32> %951, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %957 = add <8 x i32> %952, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %958 = add <8 x i32> %954, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %959 = add <8 x i32> %955, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %960 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %956, i32 %777) #9
  %961 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %957, i32 %777) #9
  %962 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %958, i32 %777) #9
  %963 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %959, i32 %777) #9
  %964 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %960, <8 x i32> %961) #9
  store <16 x i16> %964, <16 x i16>* %627, align 32
  %965 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %962, <8 x i32> %963) #9
  store <16 x i16> %965, <16 x i16>* %623, align 32
  %966 = shufflevector <16 x i16> %734, <16 x i16> %738, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %967 = shufflevector <16 x i16> %734, <16 x i16> %738, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %968 = bitcast <8 x i32> %32 to <16 x i16>
  %969 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %966, <16 x i16> %968) #9
  %970 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %967, <16 x i16> %968) #9
  %971 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %966, <16 x i16> %950) #9
  %972 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %967, <16 x i16> %950) #9
  %973 = add <8 x i32> %969, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %974 = add <8 x i32> %970, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %975 = add <8 x i32> %971, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %976 = add <8 x i32> %972, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %977 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %973, i32 %777) #9
  %978 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %974, i32 %777) #9
  %979 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %975, i32 %777) #9
  %980 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %976, i32 %777) #9
  %981 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %977, <8 x i32> %978) #9
  store <16 x i16> %981, <16 x i16>* %639, align 32
  %982 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %979, <8 x i32> %980) #9
  store <16 x i16> %982, <16 x i16>* %644, align 32
  %983 = load <16 x i16>, <16 x i16>* %412, align 32
  %984 = load <16 x i16>, <16 x i16>* %463, align 32
  %985 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %983, <16 x i16> %984) #9
  store <16 x i16> %985, <16 x i16>* %412, align 32
  %986 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %983, <16 x i16> %984) #9
  store <16 x i16> %986, <16 x i16>* %463, align 32
  %987 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %796, <16 x i16> %815) #9
  store <16 x i16> %987, <16 x i16>* %430, align 32
  %988 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %796, <16 x i16> %815) #9
  store <16 x i16> %988, <16 x i16>* %445, align 32
  %989 = load <16 x i16>, <16 x i16>* %529, align 32
  %990 = load <16 x i16>, <16 x i16>* %478, align 32
  %991 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %989, <16 x i16> %990) #9
  store <16 x i16> %991, <16 x i16>* %529, align 32
  %992 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %989, <16 x i16> %990) #9
  store <16 x i16> %992, <16 x i16>* %478, align 32
  %993 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %854, <16 x i16> %835) #9
  store <16 x i16> %993, <16 x i16>* %511, align 32
  %994 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %854, <16 x i16> %835) #9
  store <16 x i16> %994, <16 x i16>* %496, align 32
  %995 = load <16 x i16>, <16 x i16>* %531, align 32
  %996 = load <16 x i16>, <16 x i16>* %481, align 32
  %997 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %995, <16 x i16> %996) #9
  store <16 x i16> %997, <16 x i16>* %531, align 32
  %998 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %995, <16 x i16> %996) #9
  store <16 x i16> %998, <16 x i16>* %481, align 32
  %999 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %855, <16 x i16> %836) #9
  store <16 x i16> %999, <16 x i16>* %514, align 32
  %1000 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %855, <16 x i16> %836) #9
  store <16 x i16> %1000, <16 x i16>* %498, align 32
  %1001 = load <16 x i16>, <16 x i16>* %415, align 32
  %1002 = load <16 x i16>, <16 x i16>* %465, align 32
  %1003 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1001, <16 x i16> %1002) #9
  store <16 x i16> %1003, <16 x i16>* %415, align 32
  %1004 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1001, <16 x i16> %1002) #9
  store <16 x i16> %1004, <16 x i16>* %465, align 32
  %1005 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %797, <16 x i16> %816) #9
  store <16 x i16> %1005, <16 x i16>* %432, align 32
  %1006 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %797, <16 x i16> %816) #9
  store <16 x i16> %1006, <16 x i16>* %448, align 32
  %1007 = shufflevector <16 x i16> %863, <16 x i16> %919, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1008 = shufflevector <16 x i16> %863, <16 x i16> %919, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1009 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1007, <16 x i16> %782) #9
  %1010 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1008, <16 x i16> %782) #9
  %1011 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1007, <16 x i16> %785) #9
  %1012 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1008, <16 x i16> %785) #9
  %1013 = add <8 x i32> %1009, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1014 = add <8 x i32> %1010, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1015 = add <8 x i32> %1011, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1016 = add <8 x i32> %1012, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1017 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1013, i32 %777) #9
  %1018 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1014, i32 %777) #9
  %1019 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1015, i32 %777) #9
  %1020 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1016, i32 %777) #9
  %1021 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1017, <8 x i32> %1018) #9
  store <16 x i16> %1021, <16 x i16>* %182, align 32
  %1022 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1019, <8 x i32> %1020) #9
  store <16 x i16> %1022, <16 x i16>* %185, align 32
  %1023 = shufflevector <16 x i16> %859, <16 x i16> %915, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1024 = shufflevector <16 x i16> %859, <16 x i16> %915, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1025 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1023, <16 x i16> %782) #9
  %1026 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1024, <16 x i16> %782) #9
  %1027 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1023, <16 x i16> %785) #9
  %1028 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1024, <16 x i16> %785) #9
  %1029 = add <8 x i32> %1025, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1030 = add <8 x i32> %1026, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1031 = add <8 x i32> %1027, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1032 = add <8 x i32> %1028, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1033 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1029, i32 %777) #9
  %1034 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1030, i32 %777) #9
  %1035 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1031, i32 %777) #9
  %1036 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1032, i32 %777) #9
  %1037 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1033, <8 x i32> %1034) #9
  store <16 x i16> %1037, <16 x i16>* %199, align 32
  %1038 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1035, <8 x i32> %1036) #9
  store <16 x i16> %1038, <16 x i16>* %201, align 32
  %1039 = shufflevector <16 x i16> %867, <16 x i16> %907, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1040 = shufflevector <16 x i16> %867, <16 x i16> %907, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1041 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1039, <16 x i16> %802) #9
  %1042 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1040, <16 x i16> %802) #9
  %1043 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1039, <16 x i16> %782) #9
  %1044 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1040, <16 x i16> %782) #9
  %1045 = add <8 x i32> %1041, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1046 = add <8 x i32> %1042, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1047 = add <8 x i32> %1043, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1048 = add <8 x i32> %1044, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1049 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1045, i32 %777) #9
  %1050 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1046, i32 %777) #9
  %1051 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1047, i32 %777) #9
  %1052 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1048, i32 %777) #9
  %1053 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1049, <8 x i32> %1050) #9
  store <16 x i16> %1053, <16 x i16>* %214, align 32
  %1054 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1051, <8 x i32> %1052) #9
  store <16 x i16> %1054, <16 x i16>* %217, align 32
  %1055 = shufflevector <16 x i16> %871, <16 x i16> %911, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1056 = shufflevector <16 x i16> %871, <16 x i16> %911, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1057 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1055, <16 x i16> %802) #9
  %1058 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1056, <16 x i16> %802) #9
  %1059 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1055, <16 x i16> %782) #9
  %1060 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1056, <16 x i16> %782) #9
  %1061 = add <8 x i32> %1057, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1062 = add <8 x i32> %1058, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1063 = add <8 x i32> %1059, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1064 = add <8 x i32> %1060, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1065 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1061, i32 %777) #9
  %1066 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1062, i32 %777) #9
  %1067 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1063, i32 %777) #9
  %1068 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1064, i32 %777) #9
  %1069 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1065, <8 x i32> %1066) #9
  store <16 x i16> %1069, <16 x i16>* %232, align 32
  %1070 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1067, <8 x i32> %1068) #9
  store <16 x i16> %1070, <16 x i16>* %234, align 32
  %1071 = shufflevector <16 x i16> %879, <16 x i16> %903, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1072 = shufflevector <16 x i16> %879, <16 x i16> %903, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1073 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1071, <16 x i16> %821) #9
  %1074 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1072, <16 x i16> %821) #9
  %1075 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1071, <16 x i16> %824) #9
  %1076 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1072, <16 x i16> %824) #9
  %1077 = add <8 x i32> %1073, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1078 = add <8 x i32> %1074, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1079 = add <8 x i32> %1075, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1080 = add <8 x i32> %1076, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1081 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1077, i32 %777) #9
  %1082 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1078, i32 %777) #9
  %1083 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1079, i32 %777) #9
  %1084 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1080, i32 %777) #9
  %1085 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1081, <8 x i32> %1082) #9
  store <16 x i16> %1085, <16 x i16>* %313, align 32
  %1086 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1083, <8 x i32> %1084) #9
  store <16 x i16> %1086, <16 x i16>* %316, align 32
  %1087 = shufflevector <16 x i16> %875, <16 x i16> %899, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1088 = shufflevector <16 x i16> %875, <16 x i16> %899, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1089 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1087, <16 x i16> %821) #9
  %1090 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1088, <16 x i16> %821) #9
  %1091 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1087, <16 x i16> %824) #9
  %1092 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1088, <16 x i16> %824) #9
  %1093 = add <8 x i32> %1089, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1094 = add <8 x i32> %1090, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1095 = add <8 x i32> %1091, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1096 = add <8 x i32> %1092, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1097 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1093, i32 %777) #9
  %1098 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1094, i32 %777) #9
  %1099 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1095, i32 %777) #9
  %1100 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1096, i32 %777) #9
  %1101 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1097, <8 x i32> %1098) #9
  store <16 x i16> %1101, <16 x i16>* %331, align 32
  %1102 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1099, <8 x i32> %1100) #9
  store <16 x i16> %1102, <16 x i16>* %333, align 32
  %1103 = shufflevector <16 x i16> %883, <16 x i16> %891, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1104 = shufflevector <16 x i16> %883, <16 x i16> %891, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1105 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1103, <16 x i16> %841) #9
  %1106 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1104, <16 x i16> %841) #9
  %1107 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1103, <16 x i16> %821) #9
  %1108 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1104, <16 x i16> %821) #9
  %1109 = add <8 x i32> %1105, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1110 = add <8 x i32> %1106, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1111 = add <8 x i32> %1107, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1112 = add <8 x i32> %1108, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1113 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1109, i32 %777) #9
  %1114 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1110, i32 %777) #9
  %1115 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1111, i32 %777) #9
  %1116 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1112, i32 %777) #9
  %1117 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1113, <8 x i32> %1114) #9
  store <16 x i16> %1117, <16 x i16>* %346, align 32
  %1118 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1115, <8 x i32> %1116) #9
  store <16 x i16> %1118, <16 x i16>* %349, align 32
  %1119 = shufflevector <16 x i16> %887, <16 x i16> %895, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1120 = shufflevector <16 x i16> %887, <16 x i16> %895, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1121 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1119, <16 x i16> %841) #9
  %1122 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1120, <16 x i16> %841) #9
  %1123 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1119, <16 x i16> %821) #9
  %1124 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1120, <16 x i16> %821) #9
  %1125 = add <8 x i32> %1121, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1126 = add <8 x i32> %1122, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1127 = add <8 x i32> %1123, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1128 = add <8 x i32> %1124, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1129 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1125, i32 %777) #9
  %1130 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1126, i32 %777) #9
  %1131 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1127, i32 %777) #9
  %1132 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1128, i32 %777) #9
  %1133 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1129, <8 x i32> %1130) #9
  store <16 x i16> %1133, <16 x i16>* %364, align 32
  %1134 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1131, <8 x i32> %1132) #9
  store <16 x i16> %1134, <16 x i16>* %366, align 32
  %1135 = load <16 x i16>, <16 x i16>* %923, align 32
  %1136 = load <16 x i16>, <16 x i16>* %939, align 32
  %1137 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1135, <16 x i16> %1136) #9
  store <16 x i16> %1137, <16 x i16>* %923, align 32
  %1138 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1135, <16 x i16> %1136) #9
  store <16 x i16> %1138, <16 x i16>* %939, align 32
  %1139 = load <16 x i16>, <16 x i16>* %927, align 32
  %1140 = load <16 x i16>, <16 x i16>* %934, align 32
  %1141 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1139, <16 x i16> %1140) #9
  store <16 x i16> %1141, <16 x i16>* %927, align 32
  %1142 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1139, <16 x i16> %1140) #9
  store <16 x i16> %1142, <16 x i16>* %934, align 32
  %1143 = shufflevector <16 x i16> %943, <16 x i16> %947, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1144 = shufflevector <16 x i16> %943, <16 x i16> %947, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1145 = bitcast <8 x i32> %37 to <16 x i16>
  %1146 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1143, <16 x i16> %1145) #9
  %1147 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1144, <16 x i16> %1145) #9
  %1148 = bitcast <8 x i32> %11 to <16 x i16>
  %1149 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1143, <16 x i16> %1148) #9
  %1150 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1144, <16 x i16> %1148) #9
  %1151 = add <8 x i32> %1146, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1152 = add <8 x i32> %1147, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1153 = add <8 x i32> %1149, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1154 = add <8 x i32> %1150, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1155 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1151, i32 %777) #9
  %1156 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1152, i32 %777) #9
  %1157 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1153, i32 %777) #9
  %1158 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1154, i32 %777) #9
  %1159 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1155, <8 x i32> %1156) #9
  store <16 x i16> %1159, <16 x i16>* %725, align 32
  %1160 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1157, <8 x i32> %1158) #9
  store <16 x i16> %1160, <16 x i16>* %721, align 32
  %1161 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %729, <16 x i16> %733) #9
  store <16 x i16> %1161, <16 x i16>* %606, align 32
  %1162 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %729, <16 x i16> %733) #9
  store <16 x i16> %1162, <16 x i16>* %660, align 32
  %1163 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %964, <16 x i16> %981) #9
  store <16 x i16> %1163, <16 x i16>* %627, align 32
  %1164 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %964, <16 x i16> %981) #9
  store <16 x i16> %1164, <16 x i16>* %639, align 32
  %1165 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %741, <16 x i16> %737) #9
  store <16 x i16> %1165, <16 x i16>* %611, align 32
  %1166 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %741, <16 x i16> %737) #9
  store <16 x i16> %1166, <16 x i16>* %656, align 32
  %1167 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %965, <16 x i16> %982) #9
  store <16 x i16> %1167, <16 x i16>* %623, align 32
  %1168 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %965, <16 x i16> %982) #9
  store <16 x i16> %1168, <16 x i16>* %644, align 32
  %1169 = shufflevector <16 x i16> %988, <16 x i16> %1006, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1170 = shufflevector <16 x i16> %988, <16 x i16> %1006, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1171 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1169, <16 x i16> %950) #9
  %1172 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1170, <16 x i16> %950) #9
  %1173 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1169, <16 x i16> %953) #9
  %1174 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1170, <16 x i16> %953) #9
  %1175 = add <8 x i32> %1171, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1176 = add <8 x i32> %1172, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1177 = add <8 x i32> %1173, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1178 = add <8 x i32> %1174, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1179 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1175, i32 %777) #9
  %1180 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1176, i32 %777) #9
  %1181 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1177, i32 %777) #9
  %1182 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1178, i32 %777) #9
  %1183 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1179, <8 x i32> %1180) #9
  store <16 x i16> %1183, <16 x i16>* %445, align 32
  %1184 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1181, <8 x i32> %1182) #9
  store <16 x i16> %1184, <16 x i16>* %448, align 32
  %1185 = shufflevector <16 x i16> %986, <16 x i16> %1004, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1186 = shufflevector <16 x i16> %986, <16 x i16> %1004, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1187 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1185, <16 x i16> %950) #9
  %1188 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1186, <16 x i16> %950) #9
  %1189 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1185, <16 x i16> %953) #9
  %1190 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1186, <16 x i16> %953) #9
  %1191 = add <8 x i32> %1187, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1192 = add <8 x i32> %1188, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1193 = add <8 x i32> %1189, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1194 = add <8 x i32> %1190, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1195 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1191, i32 %777) #9
  %1196 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1192, i32 %777) #9
  %1197 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1193, i32 %777) #9
  %1198 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1194, i32 %777) #9
  %1199 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1195, <8 x i32> %1196) #9
  store <16 x i16> %1199, <16 x i16>* %463, align 32
  %1200 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1197, <8 x i32> %1198) #9
  store <16 x i16> %1200, <16 x i16>* %465, align 32
  %1201 = shufflevector <16 x i16> %992, <16 x i16> %998, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1202 = shufflevector <16 x i16> %992, <16 x i16> %998, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1203 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1201, <16 x i16> %968) #9
  %1204 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1202, <16 x i16> %968) #9
  %1205 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1201, <16 x i16> %950) #9
  %1206 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1202, <16 x i16> %950) #9
  %1207 = add <8 x i32> %1203, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1208 = add <8 x i32> %1204, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1209 = add <8 x i32> %1205, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1210 = add <8 x i32> %1206, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1211 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1207, i32 %777) #9
  %1212 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1208, i32 %777) #9
  %1213 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1209, i32 %777) #9
  %1214 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1210, i32 %777) #9
  %1215 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1211, <8 x i32> %1212) #9
  store <16 x i16> %1215, <16 x i16>* %478, align 32
  %1216 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1213, <8 x i32> %1214) #9
  store <16 x i16> %1216, <16 x i16>* %481, align 32
  %1217 = shufflevector <16 x i16> %994, <16 x i16> %1000, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1218 = shufflevector <16 x i16> %994, <16 x i16> %1000, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1219 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1217, <16 x i16> %968) #9
  %1220 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1218, <16 x i16> %968) #9
  %1221 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1217, <16 x i16> %950) #9
  %1222 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1218, <16 x i16> %950) #9
  %1223 = add <8 x i32> %1219, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1224 = add <8 x i32> %1220, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1225 = add <8 x i32> %1221, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1226 = add <8 x i32> %1222, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1227 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1223, i32 %777) #9
  %1228 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1224, i32 %777) #9
  %1229 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1225, i32 %777) #9
  %1230 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1226, i32 %777) #9
  %1231 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1227, <8 x i32> %1228) #9
  store <16 x i16> %1231, <16 x i16>* %496, align 32
  %1232 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1229, <8 x i32> %1230) #9
  store <16 x i16> %1232, <16 x i16>* %498, align 32
  %1233 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %858, <16 x i16> %866) #9
  store <16 x i16> %1233, <16 x i16>* %150, align 32
  %1234 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %858, <16 x i16> %866) #9
  store <16 x i16> %1234, <16 x i16>* %265, align 32
  %1235 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %862, <16 x i16> %870) #9
  store <16 x i16> %1235, <16 x i16>* %167, align 32
  %1236 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %862, <16 x i16> %870) #9
  store <16 x i16> %1236, <16 x i16>* %247, align 32
  %1237 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1021, <16 x i16> %1069) #9
  store <16 x i16> %1237, <16 x i16>* %182, align 32
  %1238 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1021, <16 x i16> %1069) #9
  store <16 x i16> %1238, <16 x i16>* %232, align 32
  %1239 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1037, <16 x i16> %1053) #9
  store <16 x i16> %1239, <16 x i16>* %199, align 32
  %1240 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1037, <16 x i16> %1053) #9
  store <16 x i16> %1240, <16 x i16>* %214, align 32
  %1241 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %882, <16 x i16> %874) #9
  store <16 x i16> %1241, <16 x i16>* %397, align 32
  %1242 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %882, <16 x i16> %874) #9
  store <16 x i16> %1242, <16 x i16>* %280, align 32
  %1243 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %886, <16 x i16> %878) #9
  store <16 x i16> %1243, <16 x i16>* %379, align 32
  %1244 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %886, <16 x i16> %878) #9
  store <16 x i16> %1244, <16 x i16>* %298, align 32
  %1245 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1133, <16 x i16> %1085) #9
  store <16 x i16> %1245, <16 x i16>* %364, align 32
  %1246 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1133, <16 x i16> %1085) #9
  store <16 x i16> %1246, <16 x i16>* %313, align 32
  %1247 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1117, <16 x i16> %1101) #9
  store <16 x i16> %1247, <16 x i16>* %346, align 32
  %1248 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1117, <16 x i16> %1101) #9
  store <16 x i16> %1248, <16 x i16>* %331, align 32
  %1249 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %890, <16 x i16> %898) #9
  store <16 x i16> %1249, <16 x i16>* %399, align 32
  %1250 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %890, <16 x i16> %898) #9
  store <16 x i16> %1250, <16 x i16>* %283, align 32
  %1251 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %894, <16 x i16> %902) #9
  store <16 x i16> %1251, <16 x i16>* %382, align 32
  %1252 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %894, <16 x i16> %902) #9
  store <16 x i16> %1252, <16 x i16>* %300, align 32
  %1253 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1134, <16 x i16> %1086) #9
  store <16 x i16> %1253, <16 x i16>* %366, align 32
  %1254 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1134, <16 x i16> %1086) #9
  store <16 x i16> %1254, <16 x i16>* %316, align 32
  %1255 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1118, <16 x i16> %1102) #9
  store <16 x i16> %1255, <16 x i16>* %349, align 32
  %1256 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1118, <16 x i16> %1102) #9
  store <16 x i16> %1256, <16 x i16>* %333, align 32
  %1257 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %914, <16 x i16> %906) #9
  store <16 x i16> %1257, <16 x i16>* %153, align 32
  %1258 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %914, <16 x i16> %906) #9
  store <16 x i16> %1258, <16 x i16>* %267, align 32
  %1259 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %918, <16 x i16> %910) #9
  store <16 x i16> %1259, <16 x i16>* %169, align 32
  %1260 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %918, <16 x i16> %910) #9
  store <16 x i16> %1260, <16 x i16>* %250, align 32
  %1261 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1022, <16 x i16> %1070) #9
  store <16 x i16> %1261, <16 x i16>* %185, align 32
  %1262 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1022, <16 x i16> %1070) #9
  store <16 x i16> %1262, <16 x i16>* %234, align 32
  %1263 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1038, <16 x i16> %1054) #9
  store <16 x i16> %1263, <16 x i16>* %201, align 32
  %1264 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1038, <16 x i16> %1054) #9
  store <16 x i16> %1264, <16 x i16>* %217, align 32
  %1265 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1137, <16 x i16> %946) #9
  store <16 x i16> %1265, <16 x i16>* %923, align 32
  %1266 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1137, <16 x i16> %946) #9
  store <16 x i16> %1266, <16 x i16>* %709, align 32
  %1267 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1141, <16 x i16> %1160) #9
  store <16 x i16> %1267, <16 x i16>* %927, align 32
  %1268 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1141, <16 x i16> %1160) #9
  store <16 x i16> %1268, <16 x i16>* %721, align 32
  %1269 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1142, <16 x i16> %1159) #9
  store <16 x i16> %1269, <16 x i16>* %934, align 32
  %1270 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1142, <16 x i16> %1159) #9
  store <16 x i16> %1270, <16 x i16>* %725, align 32
  %1271 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1138, <16 x i16> %942) #9
  store <16 x i16> %1271, <16 x i16>* %939, align 32
  %1272 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1138, <16 x i16> %942) #9
  store <16 x i16> %1272, <16 x i16>* %704, align 32
  %1273 = shufflevector <16 x i16> %1164, <16 x i16> %1168, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1274 = shufflevector <16 x i16> %1164, <16 x i16> %1168, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1275 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1273, <16 x i16> %1145) #9
  %1276 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1274, <16 x i16> %1145) #9
  %1277 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1273, <16 x i16> %1148) #9
  %1278 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1274, <16 x i16> %1148) #9
  %1279 = add <8 x i32> %1275, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1280 = add <8 x i32> %1276, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1281 = add <8 x i32> %1277, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1282 = add <8 x i32> %1278, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1283 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1279, i32 %777) #9
  %1284 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1280, i32 %777) #9
  %1285 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1281, i32 %777) #9
  %1286 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1282, i32 %777) #9
  %1287 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1283, <8 x i32> %1284) #9
  store <16 x i16> %1287, <16 x i16>* %639, align 32
  %1288 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1285, <8 x i32> %1286) #9
  store <16 x i16> %1288, <16 x i16>* %644, align 32
  %1289 = shufflevector <16 x i16> %1162, <16 x i16> %1166, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1290 = shufflevector <16 x i16> %1162, <16 x i16> %1166, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1291 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1289, <16 x i16> %1145) #9
  %1292 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1290, <16 x i16> %1145) #9
  %1293 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1289, <16 x i16> %1148) #9
  %1294 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1290, <16 x i16> %1148) #9
  %1295 = add <8 x i32> %1291, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1296 = add <8 x i32> %1292, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1297 = add <8 x i32> %1293, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1298 = add <8 x i32> %1294, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1299 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1295, i32 %777) #9
  %1300 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1296, i32 %777) #9
  %1301 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1297, i32 %777) #9
  %1302 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1298, i32 %777) #9
  %1303 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1299, <8 x i32> %1300) #9
  store <16 x i16> %1303, <16 x i16>* %660, align 32
  %1304 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1301, <8 x i32> %1302) #9
  store <16 x i16> %1304, <16 x i16>* %656, align 32
  %1305 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %985, <16 x i16> %991) #9
  store <16 x i16> %1305, <16 x i16>* %412, align 32
  %1306 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %985, <16 x i16> %991) #9
  store <16 x i16> %1306, <16 x i16>* %529, align 32
  %1307 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %987, <16 x i16> %993) #9
  store <16 x i16> %1307, <16 x i16>* %430, align 32
  %1308 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %987, <16 x i16> %993) #9
  store <16 x i16> %1308, <16 x i16>* %511, align 32
  %1309 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1183, <16 x i16> %1231) #9
  store <16 x i16> %1309, <16 x i16>* %445, align 32
  %1310 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1183, <16 x i16> %1231) #9
  store <16 x i16> %1310, <16 x i16>* %496, align 32
  %1311 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1199, <16 x i16> %1215) #9
  store <16 x i16> %1311, <16 x i16>* %463, align 32
  %1312 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1199, <16 x i16> %1215) #9
  store <16 x i16> %1312, <16 x i16>* %478, align 32
  %1313 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1003, <16 x i16> %997) #9
  store <16 x i16> %1313, <16 x i16>* %415, align 32
  %1314 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1003, <16 x i16> %997) #9
  store <16 x i16> %1314, <16 x i16>* %531, align 32
  %1315 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1005, <16 x i16> %999) #9
  store <16 x i16> %1315, <16 x i16>* %432, align 32
  %1316 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1005, <16 x i16> %999) #9
  store <16 x i16> %1316, <16 x i16>* %514, align 32
  %1317 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1184, <16 x i16> %1232) #9
  store <16 x i16> %1317, <16 x i16>* %448, align 32
  %1318 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1184, <16 x i16> %1232) #9
  store <16 x i16> %1318, <16 x i16>* %498, align 32
  %1319 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1200, <16 x i16> %1216) #9
  store <16 x i16> %1319, <16 x i16>* %465, align 32
  %1320 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1200, <16 x i16> %1216) #9
  store <16 x i16> %1320, <16 x i16>* %481, align 32
  %1321 = shufflevector <16 x i16> %1240, <16 x i16> %1264, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1322 = shufflevector <16 x i16> %1240, <16 x i16> %1264, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1323 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1321, <16 x i16> %950) #9
  %1324 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1322, <16 x i16> %950) #9
  %1325 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1321, <16 x i16> %953) #9
  %1326 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1322, <16 x i16> %953) #9
  %1327 = add <8 x i32> %1323, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1328 = add <8 x i32> %1324, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1329 = add <8 x i32> %1325, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1330 = add <8 x i32> %1326, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1331 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1327, i32 %777) #9
  %1332 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1328, i32 %777) #9
  %1333 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1329, i32 %777) #9
  %1334 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1330, i32 %777) #9
  %1335 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1331, <8 x i32> %1332) #9
  store <16 x i16> %1335, <16 x i16>* %214, align 32
  %1336 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1333, <8 x i32> %1334) #9
  store <16 x i16> %1336, <16 x i16>* %217, align 32
  %1337 = shufflevector <16 x i16> %1238, <16 x i16> %1262, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1338 = shufflevector <16 x i16> %1238, <16 x i16> %1262, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1339 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1337, <16 x i16> %950) #9
  %1340 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1338, <16 x i16> %950) #9
  %1341 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1337, <16 x i16> %953) #9
  %1342 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1338, <16 x i16> %953) #9
  %1343 = add <8 x i32> %1339, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1344 = add <8 x i32> %1340, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1345 = add <8 x i32> %1341, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1346 = add <8 x i32> %1342, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1347 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1343, i32 %777) #9
  %1348 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1344, i32 %777) #9
  %1349 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1345, i32 %777) #9
  %1350 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1346, i32 %777) #9
  %1351 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1347, <8 x i32> %1348) #9
  store <16 x i16> %1351, <16 x i16>* %232, align 32
  %1352 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1349, <8 x i32> %1350) #9
  store <16 x i16> %1352, <16 x i16>* %234, align 32
  %1353 = shufflevector <16 x i16> %1236, <16 x i16> %1260, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1354 = shufflevector <16 x i16> %1236, <16 x i16> %1260, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1355 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1353, <16 x i16> %950) #9
  %1356 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1354, <16 x i16> %950) #9
  %1357 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1353, <16 x i16> %953) #9
  %1358 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1354, <16 x i16> %953) #9
  %1359 = add <8 x i32> %1355, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1360 = add <8 x i32> %1356, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1361 = add <8 x i32> %1357, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1362 = add <8 x i32> %1358, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1363 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1359, i32 %777) #9
  %1364 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1360, i32 %777) #9
  %1365 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1361, i32 %777) #9
  %1366 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1362, i32 %777) #9
  %1367 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1363, <8 x i32> %1364) #9
  store <16 x i16> %1367, <16 x i16>* %247, align 32
  %1368 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1365, <8 x i32> %1366) #9
  store <16 x i16> %1368, <16 x i16>* %250, align 32
  %1369 = shufflevector <16 x i16> %1234, <16 x i16> %1258, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1370 = shufflevector <16 x i16> %1234, <16 x i16> %1258, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1371 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1369, <16 x i16> %950) #9
  %1372 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1370, <16 x i16> %950) #9
  %1373 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1369, <16 x i16> %953) #9
  %1374 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1370, <16 x i16> %953) #9
  %1375 = add <8 x i32> %1371, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1376 = add <8 x i32> %1372, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1377 = add <8 x i32> %1373, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1378 = add <8 x i32> %1374, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1379 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1375, i32 %777) #9
  %1380 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1376, i32 %777) #9
  %1381 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1377, i32 %777) #9
  %1382 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1378, i32 %777) #9
  %1383 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1379, <8 x i32> %1380) #9
  store <16 x i16> %1383, <16 x i16>* %265, align 32
  %1384 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1381, <8 x i32> %1382) #9
  store <16 x i16> %1384, <16 x i16>* %267, align 32
  %1385 = shufflevector <16 x i16> %1242, <16 x i16> %1250, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1386 = shufflevector <16 x i16> %1242, <16 x i16> %1250, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1387 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1385, <16 x i16> %968) #9
  %1388 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1386, <16 x i16> %968) #9
  %1389 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1385, <16 x i16> %950) #9
  %1390 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1386, <16 x i16> %950) #9
  %1391 = add <8 x i32> %1387, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1392 = add <8 x i32> %1388, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1393 = add <8 x i32> %1389, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1394 = add <8 x i32> %1390, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1395 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1391, i32 %777) #9
  %1396 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1392, i32 %777) #9
  %1397 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1393, i32 %777) #9
  %1398 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1394, i32 %777) #9
  %1399 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1395, <8 x i32> %1396) #9
  store <16 x i16> %1399, <16 x i16>* %280, align 32
  %1400 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1397, <8 x i32> %1398) #9
  store <16 x i16> %1400, <16 x i16>* %283, align 32
  %1401 = shufflevector <16 x i16> %1244, <16 x i16> %1252, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1402 = shufflevector <16 x i16> %1244, <16 x i16> %1252, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1403 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1401, <16 x i16> %968) #9
  %1404 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1402, <16 x i16> %968) #9
  %1405 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1401, <16 x i16> %950) #9
  %1406 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1402, <16 x i16> %950) #9
  %1407 = add <8 x i32> %1403, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1408 = add <8 x i32> %1404, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1409 = add <8 x i32> %1405, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1410 = add <8 x i32> %1406, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1411 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1407, i32 %777) #9
  %1412 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1408, i32 %777) #9
  %1413 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1409, i32 %777) #9
  %1414 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1410, i32 %777) #9
  %1415 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1411, <8 x i32> %1412) #9
  store <16 x i16> %1415, <16 x i16>* %298, align 32
  %1416 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1413, <8 x i32> %1414) #9
  store <16 x i16> %1416, <16 x i16>* %300, align 32
  %1417 = shufflevector <16 x i16> %1246, <16 x i16> %1254, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1418 = shufflevector <16 x i16> %1246, <16 x i16> %1254, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1419 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1417, <16 x i16> %968) #9
  %1420 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1418, <16 x i16> %968) #9
  %1421 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1417, <16 x i16> %950) #9
  %1422 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1418, <16 x i16> %950) #9
  %1423 = add <8 x i32> %1419, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1424 = add <8 x i32> %1420, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1425 = add <8 x i32> %1421, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1426 = add <8 x i32> %1422, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1427 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1423, i32 %777) #9
  %1428 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1424, i32 %777) #9
  %1429 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1425, i32 %777) #9
  %1430 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1426, i32 %777) #9
  %1431 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1427, <8 x i32> %1428) #9
  store <16 x i16> %1431, <16 x i16>* %313, align 32
  %1432 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1429, <8 x i32> %1430) #9
  store <16 x i16> %1432, <16 x i16>* %316, align 32
  %1433 = shufflevector <16 x i16> %1248, <16 x i16> %1256, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1434 = shufflevector <16 x i16> %1248, <16 x i16> %1256, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1435 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1433, <16 x i16> %968) #9
  %1436 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1434, <16 x i16> %968) #9
  %1437 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1433, <16 x i16> %950) #9
  %1438 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1434, <16 x i16> %950) #9
  %1439 = add <8 x i32> %1435, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1440 = add <8 x i32> %1436, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1441 = add <8 x i32> %1437, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1442 = add <8 x i32> %1438, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1443 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1439, i32 %777) #9
  %1444 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1440, i32 %777) #9
  %1445 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1441, i32 %777) #9
  %1446 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1442, i32 %777) #9
  %1447 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1443, <8 x i32> %1444) #9
  store <16 x i16> %1447, <16 x i16>* %331, align 32
  %1448 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1445, <8 x i32> %1446) #9
  store <16 x i16> %1448, <16 x i16>* %333, align 32
  %1449 = load <16 x i16>, <16 x i16>* %611, align 32
  %1450 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1265, <16 x i16> %1449) #9
  store <16 x i16> %1450, <16 x i16>* %923, align 32
  %1451 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1265, <16 x i16> %1449) #9
  store <16 x i16> %1451, <16 x i16>* %611, align 32
  %1452 = load <16 x i16>, <16 x i16>* %927, align 32
  %1453 = load <16 x i16>, <16 x i16>* %623, align 32
  %1454 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1452, <16 x i16> %1453) #9
  store <16 x i16> %1454, <16 x i16>* %927, align 32
  %1455 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1452, <16 x i16> %1453) #9
  store <16 x i16> %1455, <16 x i16>* %623, align 32
  %1456 = load <16 x i16>, <16 x i16>* %934, align 32
  %1457 = load <16 x i16>, <16 x i16>* %644, align 32
  %1458 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1456, <16 x i16> %1457) #9
  store <16 x i16> %1458, <16 x i16>* %934, align 32
  %1459 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1456, <16 x i16> %1457) #9
  store <16 x i16> %1459, <16 x i16>* %644, align 32
  %1460 = load <16 x i16>, <16 x i16>* %939, align 32
  %1461 = load <16 x i16>, <16 x i16>* %656, align 32
  %1462 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1460, <16 x i16> %1461) #9
  store <16 x i16> %1462, <16 x i16>* %939, align 32
  %1463 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1460, <16 x i16> %1461) #9
  store <16 x i16> %1463, <16 x i16>* %656, align 32
  %1464 = load <16 x i16>, <16 x i16>* %704, align 32
  %1465 = load <16 x i16>, <16 x i16>* %660, align 32
  %1466 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1464, <16 x i16> %1465) #9
  store <16 x i16> %1466, <16 x i16>* %704, align 32
  %1467 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1464, <16 x i16> %1465) #9
  store <16 x i16> %1467, <16 x i16>* %660, align 32
  %1468 = load <16 x i16>, <16 x i16>* %725, align 32
  %1469 = load <16 x i16>, <16 x i16>* %639, align 32
  %1470 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1468, <16 x i16> %1469) #9
  store <16 x i16> %1470, <16 x i16>* %725, align 32
  %1471 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1468, <16 x i16> %1469) #9
  store <16 x i16> %1471, <16 x i16>* %639, align 32
  %1472 = load <16 x i16>, <16 x i16>* %721, align 32
  %1473 = load <16 x i16>, <16 x i16>* %627, align 32
  %1474 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1472, <16 x i16> %1473) #9
  store <16 x i16> %1474, <16 x i16>* %721, align 32
  %1475 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1472, <16 x i16> %1473) #9
  store <16 x i16> %1475, <16 x i16>* %627, align 32
  %1476 = load <16 x i16>, <16 x i16>* %709, align 32
  %1477 = load <16 x i16>, <16 x i16>* %606, align 32
  %1478 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1476, <16 x i16> %1477) #9
  store <16 x i16> %1478, <16 x i16>* %709, align 32
  %1479 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1476, <16 x i16> %1477) #9
  store <16 x i16> %1479, <16 x i16>* %606, align 32
  %1480 = shufflevector <16 x i16> %1312, <16 x i16> %1320, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1481 = shufflevector <16 x i16> %1312, <16 x i16> %1320, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1482 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1480, <16 x i16> %1145) #9
  %1483 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1481, <16 x i16> %1145) #9
  %1484 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1480, <16 x i16> %1148) #9
  %1485 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1481, <16 x i16> %1148) #9
  %1486 = add <8 x i32> %1482, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1487 = add <8 x i32> %1483, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1488 = add <8 x i32> %1484, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1489 = add <8 x i32> %1485, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1490 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1486, i32 %777) #9
  %1491 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1487, i32 %777) #9
  %1492 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1488, i32 %777) #9
  %1493 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1489, i32 %777) #9
  %1494 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1490, <8 x i32> %1491) #9
  store <16 x i16> %1494, <16 x i16>* %478, align 32
  %1495 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1492, <8 x i32> %1493) #9
  store <16 x i16> %1495, <16 x i16>* %481, align 32
  %1496 = shufflevector <16 x i16> %1310, <16 x i16> %1318, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1497 = shufflevector <16 x i16> %1310, <16 x i16> %1318, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1498 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1496, <16 x i16> %1145) #9
  %1499 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1497, <16 x i16> %1145) #9
  %1500 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1496, <16 x i16> %1148) #9
  %1501 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1497, <16 x i16> %1148) #9
  %1502 = add <8 x i32> %1498, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1503 = add <8 x i32> %1499, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1504 = add <8 x i32> %1500, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1505 = add <8 x i32> %1501, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1506 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1502, i32 %777) #9
  %1507 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1503, i32 %777) #9
  %1508 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1504, i32 %777) #9
  %1509 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1505, i32 %777) #9
  %1510 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1506, <8 x i32> %1507) #9
  store <16 x i16> %1510, <16 x i16>* %496, align 32
  %1511 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1508, <8 x i32> %1509) #9
  store <16 x i16> %1511, <16 x i16>* %498, align 32
  %1512 = shufflevector <16 x i16> %1308, <16 x i16> %1316, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1513 = shufflevector <16 x i16> %1308, <16 x i16> %1316, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1514 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1512, <16 x i16> %1145) #9
  %1515 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1513, <16 x i16> %1145) #9
  %1516 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1512, <16 x i16> %1148) #9
  %1517 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1513, <16 x i16> %1148) #9
  %1518 = add <8 x i32> %1514, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1519 = add <8 x i32> %1515, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1520 = add <8 x i32> %1516, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1521 = add <8 x i32> %1517, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1522 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1518, i32 %777) #9
  %1523 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1519, i32 %777) #9
  %1524 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1520, i32 %777) #9
  %1525 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1521, i32 %777) #9
  %1526 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1522, <8 x i32> %1523) #9
  store <16 x i16> %1526, <16 x i16>* %511, align 32
  %1527 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1524, <8 x i32> %1525) #9
  store <16 x i16> %1527, <16 x i16>* %514, align 32
  %1528 = shufflevector <16 x i16> %1306, <16 x i16> %1314, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1529 = shufflevector <16 x i16> %1306, <16 x i16> %1314, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1530 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1528, <16 x i16> %1145) #9
  %1531 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1529, <16 x i16> %1145) #9
  %1532 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1528, <16 x i16> %1148) #9
  %1533 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1529, <16 x i16> %1148) #9
  %1534 = add <8 x i32> %1530, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1535 = add <8 x i32> %1531, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1536 = add <8 x i32> %1532, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1537 = add <8 x i32> %1533, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1538 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1534, i32 %777) #9
  %1539 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1535, i32 %777) #9
  %1540 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1536, i32 %777) #9
  %1541 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1537, i32 %777) #9
  %1542 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1538, <8 x i32> %1539) #9
  store <16 x i16> %1542, <16 x i16>* %529, align 32
  %1543 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1540, <8 x i32> %1541) #9
  %1544 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1233, <16 x i16> %1241) #9
  store <16 x i16> %1544, <16 x i16>* %150, align 32
  %1545 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1233, <16 x i16> %1241) #9
  store <16 x i16> %1545, <16 x i16>* %397, align 32
  %1546 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1235, <16 x i16> %1243) #9
  store <16 x i16> %1546, <16 x i16>* %167, align 32
  %1547 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1235, <16 x i16> %1243) #9
  store <16 x i16> %1547, <16 x i16>* %379, align 32
  %1548 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1237, <16 x i16> %1245) #9
  store <16 x i16> %1548, <16 x i16>* %182, align 32
  %1549 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1237, <16 x i16> %1245) #9
  store <16 x i16> %1549, <16 x i16>* %364, align 32
  %1550 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1239, <16 x i16> %1247) #9
  store <16 x i16> %1550, <16 x i16>* %199, align 32
  %1551 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1239, <16 x i16> %1247) #9
  store <16 x i16> %1551, <16 x i16>* %346, align 32
  %1552 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1335, <16 x i16> %1447) #9
  store <16 x i16> %1552, <16 x i16>* %214, align 32
  %1553 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1335, <16 x i16> %1447) #9
  store <16 x i16> %1553, <16 x i16>* %331, align 32
  %1554 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1351, <16 x i16> %1431) #9
  store <16 x i16> %1554, <16 x i16>* %232, align 32
  %1555 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1351, <16 x i16> %1431) #9
  store <16 x i16> %1555, <16 x i16>* %313, align 32
  %1556 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1367, <16 x i16> %1415) #9
  store <16 x i16> %1556, <16 x i16>* %247, align 32
  %1557 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1367, <16 x i16> %1415) #9
  store <16 x i16> %1557, <16 x i16>* %298, align 32
  %1558 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1383, <16 x i16> %1399) #9
  store <16 x i16> %1558, <16 x i16>* %265, align 32
  %1559 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1383, <16 x i16> %1399) #9
  store <16 x i16> %1559, <16 x i16>* %280, align 32
  %1560 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1257, <16 x i16> %1249) #9
  store <16 x i16> %1560, <16 x i16>* %153, align 32
  %1561 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1257, <16 x i16> %1249) #9
  store <16 x i16> %1561, <16 x i16>* %399, align 32
  %1562 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1259, <16 x i16> %1251) #9
  store <16 x i16> %1562, <16 x i16>* %169, align 32
  %1563 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1259, <16 x i16> %1251) #9
  store <16 x i16> %1563, <16 x i16>* %382, align 32
  %1564 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1261, <16 x i16> %1253) #9
  store <16 x i16> %1564, <16 x i16>* %185, align 32
  %1565 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1261, <16 x i16> %1253) #9
  store <16 x i16> %1565, <16 x i16>* %366, align 32
  %1566 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1263, <16 x i16> %1255) #9
  store <16 x i16> %1566, <16 x i16>* %201, align 32
  %1567 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1263, <16 x i16> %1255) #9
  store <16 x i16> %1567, <16 x i16>* %349, align 32
  %1568 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1336, <16 x i16> %1448) #9
  store <16 x i16> %1568, <16 x i16>* %217, align 32
  %1569 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1336, <16 x i16> %1448) #9
  store <16 x i16> %1569, <16 x i16>* %333, align 32
  %1570 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1352, <16 x i16> %1432) #9
  store <16 x i16> %1570, <16 x i16>* %234, align 32
  %1571 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1352, <16 x i16> %1432) #9
  store <16 x i16> %1571, <16 x i16>* %316, align 32
  %1572 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1368, <16 x i16> %1416) #9
  store <16 x i16> %1572, <16 x i16>* %250, align 32
  %1573 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1368, <16 x i16> %1416) #9
  store <16 x i16> %1573, <16 x i16>* %300, align 32
  %1574 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1384, <16 x i16> %1400) #9
  store <16 x i16> %1574, <16 x i16>* %267, align 32
  %1575 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1384, <16 x i16> %1400) #9
  %1576 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1450, <16 x i16> %1313) #9
  store <16 x i16> %1576, <16 x i16>* %923, align 32
  %1577 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1450, <16 x i16> %1313) #9
  store <16 x i16> %1577, <16 x i16>* %415, align 32
  %1578 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1454, <16 x i16> %1315) #9
  store <16 x i16> %1578, <16 x i16>* %927, align 32
  %1579 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1454, <16 x i16> %1315) #9
  store <16 x i16> %1579, <16 x i16>* %432, align 32
  %1580 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1458, <16 x i16> %1317) #9
  store <16 x i16> %1580, <16 x i16>* %934, align 32
  %1581 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1458, <16 x i16> %1317) #9
  store <16 x i16> %1581, <16 x i16>* %448, align 32
  %1582 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1462, <16 x i16> %1319) #9
  store <16 x i16> %1582, <16 x i16>* %939, align 32
  %1583 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1462, <16 x i16> %1319) #9
  store <16 x i16> %1583, <16 x i16>* %465, align 32
  %1584 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1466, <16 x i16> %1495) #9
  store <16 x i16> %1584, <16 x i16>* %704, align 32
  %1585 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1466, <16 x i16> %1495) #9
  store <16 x i16> %1585, <16 x i16>* %481, align 32
  %1586 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1470, <16 x i16> %1511) #9
  store <16 x i16> %1586, <16 x i16>* %725, align 32
  %1587 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1470, <16 x i16> %1511) #9
  store <16 x i16> %1587, <16 x i16>* %498, align 32
  %1588 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1474, <16 x i16> %1527) #9
  store <16 x i16> %1588, <16 x i16>* %721, align 32
  %1589 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1474, <16 x i16> %1527) #9
  store <16 x i16> %1589, <16 x i16>* %514, align 32
  %1590 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1478, <16 x i16> %1543) #9
  store <16 x i16> %1590, <16 x i16>* %709, align 32
  %1591 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1478, <16 x i16> %1543) #9
  store <16 x i16> %1591, <16 x i16>* %531, align 32
  %1592 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1479, <16 x i16> %1542) #9
  store <16 x i16> %1592, <16 x i16>* %606, align 32
  %1593 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1479, <16 x i16> %1542) #9
  store <16 x i16> %1593, <16 x i16>* %529, align 32
  %1594 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1475, <16 x i16> %1526) #9
  store <16 x i16> %1594, <16 x i16>* %627, align 32
  %1595 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1475, <16 x i16> %1526) #9
  store <16 x i16> %1595, <16 x i16>* %511, align 32
  %1596 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1471, <16 x i16> %1510) #9
  store <16 x i16> %1596, <16 x i16>* %639, align 32
  %1597 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1471, <16 x i16> %1510) #9
  store <16 x i16> %1597, <16 x i16>* %496, align 32
  %1598 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1467, <16 x i16> %1494) #9
  store <16 x i16> %1598, <16 x i16>* %660, align 32
  %1599 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1467, <16 x i16> %1494) #9
  store <16 x i16> %1599, <16 x i16>* %478, align 32
  %1600 = load <16 x i16>, <16 x i16>* %463, align 32
  %1601 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1463, <16 x i16> %1600) #9
  store <16 x i16> %1601, <16 x i16>* %656, align 32
  %1602 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1463, <16 x i16> %1600) #9
  store <16 x i16> %1602, <16 x i16>* %463, align 32
  %1603 = load <16 x i16>, <16 x i16>* %445, align 32
  %1604 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1459, <16 x i16> %1603) #9
  store <16 x i16> %1604, <16 x i16>* %644, align 32
  %1605 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1459, <16 x i16> %1603) #9
  store <16 x i16> %1605, <16 x i16>* %445, align 32
  %1606 = load <16 x i16>, <16 x i16>* %430, align 32
  %1607 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1455, <16 x i16> %1606) #9
  store <16 x i16> %1607, <16 x i16>* %623, align 32
  %1608 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1455, <16 x i16> %1606) #9
  store <16 x i16> %1608, <16 x i16>* %430, align 32
  %1609 = load <16 x i16>, <16 x i16>* %412, align 32
  %1610 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1451, <16 x i16> %1609) #9
  store <16 x i16> %1610, <16 x i16>* %611, align 32
  %1611 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1451, <16 x i16> %1609) #9
  store <16 x i16> %1611, <16 x i16>* %412, align 32
  %1612 = shufflevector <16 x i16> %1559, <16 x i16> %1575, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1613 = shufflevector <16 x i16> %1559, <16 x i16> %1575, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1614 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1612, <16 x i16> %1145) #9
  %1615 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1613, <16 x i16> %1145) #9
  %1616 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1612, <16 x i16> %1148) #9
  %1617 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1613, <16 x i16> %1148) #9
  %1618 = add <8 x i32> %1614, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1619 = add <8 x i32> %1615, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1620 = add <8 x i32> %1616, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1621 = add <8 x i32> %1617, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1622 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1618, i32 %777) #9
  %1623 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1619, i32 %777) #9
  %1624 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1620, i32 %777) #9
  %1625 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1621, i32 %777) #9
  %1626 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1622, <8 x i32> %1623) #9
  store <16 x i16> %1626, <16 x i16>* %280, align 32
  %1627 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1624, <8 x i32> %1625) #9
  store <16 x i16> %1627, <16 x i16>* %283, align 32
  %1628 = shufflevector <16 x i16> %1557, <16 x i16> %1573, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1629 = shufflevector <16 x i16> %1557, <16 x i16> %1573, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1630 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1628, <16 x i16> %1145) #9
  %1631 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1629, <16 x i16> %1145) #9
  %1632 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1628, <16 x i16> %1148) #9
  %1633 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1629, <16 x i16> %1148) #9
  %1634 = add <8 x i32> %1630, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1635 = add <8 x i32> %1631, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1636 = add <8 x i32> %1632, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1637 = add <8 x i32> %1633, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1638 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1634, i32 %777) #9
  %1639 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1635, i32 %777) #9
  %1640 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1636, i32 %777) #9
  %1641 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1637, i32 %777) #9
  %1642 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1638, <8 x i32> %1639) #9
  store <16 x i16> %1642, <16 x i16>* %298, align 32
  %1643 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1640, <8 x i32> %1641) #9
  store <16 x i16> %1643, <16 x i16>* %300, align 32
  %1644 = shufflevector <16 x i16> %1555, <16 x i16> %1571, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1645 = shufflevector <16 x i16> %1555, <16 x i16> %1571, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1646 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1644, <16 x i16> %1145) #9
  %1647 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1645, <16 x i16> %1145) #9
  %1648 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1644, <16 x i16> %1148) #9
  %1649 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1645, <16 x i16> %1148) #9
  %1650 = add <8 x i32> %1646, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1651 = add <8 x i32> %1647, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1652 = add <8 x i32> %1648, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1653 = add <8 x i32> %1649, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1654 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1650, i32 %777) #9
  %1655 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1651, i32 %777) #9
  %1656 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1652, i32 %777) #9
  %1657 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1653, i32 %777) #9
  %1658 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1654, <8 x i32> %1655) #9
  store <16 x i16> %1658, <16 x i16>* %313, align 32
  %1659 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1656, <8 x i32> %1657) #9
  store <16 x i16> %1659, <16 x i16>* %316, align 32
  %1660 = shufflevector <16 x i16> %1553, <16 x i16> %1569, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1661 = shufflevector <16 x i16> %1553, <16 x i16> %1569, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1662 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1660, <16 x i16> %1145) #9
  %1663 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1661, <16 x i16> %1145) #9
  %1664 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1660, <16 x i16> %1148) #9
  %1665 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1661, <16 x i16> %1148) #9
  %1666 = add <8 x i32> %1662, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1667 = add <8 x i32> %1663, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1668 = add <8 x i32> %1664, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1669 = add <8 x i32> %1665, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1670 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1666, i32 %777) #9
  %1671 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1667, i32 %777) #9
  %1672 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1668, i32 %777) #9
  %1673 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1669, i32 %777) #9
  %1674 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1670, <8 x i32> %1671) #9
  store <16 x i16> %1674, <16 x i16>* %331, align 32
  %1675 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1672, <8 x i32> %1673) #9
  store <16 x i16> %1675, <16 x i16>* %333, align 32
  %1676 = shufflevector <16 x i16> %1551, <16 x i16> %1567, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1677 = shufflevector <16 x i16> %1551, <16 x i16> %1567, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1678 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1676, <16 x i16> %1145) #9
  %1679 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1677, <16 x i16> %1145) #9
  %1680 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1676, <16 x i16> %1148) #9
  %1681 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1677, <16 x i16> %1148) #9
  %1682 = add <8 x i32> %1678, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1683 = add <8 x i32> %1679, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1684 = add <8 x i32> %1680, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1685 = add <8 x i32> %1681, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1686 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1682, i32 %777) #9
  %1687 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1683, i32 %777) #9
  %1688 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1684, i32 %777) #9
  %1689 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1685, i32 %777) #9
  %1690 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1686, <8 x i32> %1687) #9
  store <16 x i16> %1690, <16 x i16>* %346, align 32
  %1691 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1688, <8 x i32> %1689) #9
  store <16 x i16> %1691, <16 x i16>* %349, align 32
  %1692 = shufflevector <16 x i16> %1549, <16 x i16> %1565, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1693 = shufflevector <16 x i16> %1549, <16 x i16> %1565, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1694 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1692, <16 x i16> %1145) #9
  %1695 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1693, <16 x i16> %1145) #9
  %1696 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1692, <16 x i16> %1148) #9
  %1697 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1693, <16 x i16> %1148) #9
  %1698 = add <8 x i32> %1694, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1699 = add <8 x i32> %1695, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1700 = add <8 x i32> %1696, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1701 = add <8 x i32> %1697, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1702 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1698, i32 %777) #9
  %1703 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1699, i32 %777) #9
  %1704 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1700, i32 %777) #9
  %1705 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1701, i32 %777) #9
  %1706 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1702, <8 x i32> %1703) #9
  store <16 x i16> %1706, <16 x i16>* %364, align 32
  %1707 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1704, <8 x i32> %1705) #9
  store <16 x i16> %1707, <16 x i16>* %366, align 32
  %1708 = shufflevector <16 x i16> %1547, <16 x i16> %1563, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1709 = shufflevector <16 x i16> %1547, <16 x i16> %1563, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1710 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1708, <16 x i16> %1145) #9
  %1711 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1709, <16 x i16> %1145) #9
  %1712 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1708, <16 x i16> %1148) #9
  %1713 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1709, <16 x i16> %1148) #9
  %1714 = add <8 x i32> %1710, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1715 = add <8 x i32> %1711, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1716 = add <8 x i32> %1712, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1717 = add <8 x i32> %1713, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1718 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1714, i32 %777) #9
  %1719 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1715, i32 %777) #9
  %1720 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1716, i32 %777) #9
  %1721 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1717, i32 %777) #9
  %1722 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1718, <8 x i32> %1719) #9
  store <16 x i16> %1722, <16 x i16>* %379, align 32
  %1723 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1720, <8 x i32> %1721) #9
  store <16 x i16> %1723, <16 x i16>* %382, align 32
  %1724 = shufflevector <16 x i16> %1545, <16 x i16> %1561, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %1725 = shufflevector <16 x i16> %1545, <16 x i16> %1561, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %1726 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1724, <16 x i16> %1145) #9
  %1727 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1725, <16 x i16> %1145) #9
  %1728 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1724, <16 x i16> %1148) #9
  %1729 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %1725, <16 x i16> %1148) #9
  %1730 = add <8 x i32> %1726, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1731 = add <8 x i32> %1727, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1732 = add <8 x i32> %1728, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1733 = add <8 x i32> %1729, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %1734 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1730, i32 %777) #9
  %1735 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1731, i32 %777) #9
  %1736 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1732, i32 %777) #9
  %1737 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1733, i32 %777) #9
  %1738 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1734, <8 x i32> %1735) #9
  store <16 x i16> %1738, <16 x i16>* %397, align 32
  %1739 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %1736, <8 x i32> %1737) #9
  store <16 x i16> %1739, <16 x i16>* %399, align 32
  %1740 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 63
  %1741 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1576, <16 x i16> %1560) #9
  %1742 = bitcast <4 x i64>* %1 to <16 x i16>*
  store <16 x i16> %1741, <16 x i16>* %1742, align 32
  %1743 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1576, <16 x i16> %1560) #9
  %1744 = bitcast <4 x i64>* %1740 to <16 x i16>*
  store <16 x i16> %1743, <16 x i16>* %1744, align 32
  %1745 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %1746 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 62
  %1747 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1578, <16 x i16> %1562) #9
  %1748 = bitcast <4 x i64>* %1745 to <16 x i16>*
  store <16 x i16> %1747, <16 x i16>* %1748, align 32
  %1749 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1578, <16 x i16> %1562) #9
  %1750 = bitcast <4 x i64>* %1746 to <16 x i16>*
  store <16 x i16> %1749, <16 x i16>* %1750, align 32
  %1751 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %1752 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 61
  %1753 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1580, <16 x i16> %1564) #9
  %1754 = bitcast <4 x i64>* %1751 to <16 x i16>*
  store <16 x i16> %1753, <16 x i16>* %1754, align 32
  %1755 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1580, <16 x i16> %1564) #9
  %1756 = bitcast <4 x i64>* %1752 to <16 x i16>*
  store <16 x i16> %1755, <16 x i16>* %1756, align 32
  %1757 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %1758 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 60
  %1759 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1582, <16 x i16> %1566) #9
  %1760 = bitcast <4 x i64>* %1757 to <16 x i16>*
  store <16 x i16> %1759, <16 x i16>* %1760, align 32
  %1761 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1582, <16 x i16> %1566) #9
  %1762 = bitcast <4 x i64>* %1758 to <16 x i16>*
  store <16 x i16> %1761, <16 x i16>* %1762, align 32
  %1763 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %1764 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 59
  %1765 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1584, <16 x i16> %1568) #9
  %1766 = bitcast <4 x i64>* %1763 to <16 x i16>*
  store <16 x i16> %1765, <16 x i16>* %1766, align 32
  %1767 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1584, <16 x i16> %1568) #9
  %1768 = bitcast <4 x i64>* %1764 to <16 x i16>*
  store <16 x i16> %1767, <16 x i16>* %1768, align 32
  %1769 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %1770 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 58
  %1771 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1586, <16 x i16> %1570) #9
  %1772 = bitcast <4 x i64>* %1769 to <16 x i16>*
  store <16 x i16> %1771, <16 x i16>* %1772, align 32
  %1773 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1586, <16 x i16> %1570) #9
  %1774 = bitcast <4 x i64>* %1770 to <16 x i16>*
  store <16 x i16> %1773, <16 x i16>* %1774, align 32
  %1775 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %1776 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 57
  %1777 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1588, <16 x i16> %1572) #9
  %1778 = bitcast <4 x i64>* %1775 to <16 x i16>*
  store <16 x i16> %1777, <16 x i16>* %1778, align 32
  %1779 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1588, <16 x i16> %1572) #9
  %1780 = bitcast <4 x i64>* %1776 to <16 x i16>*
  store <16 x i16> %1779, <16 x i16>* %1780, align 32
  %1781 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %1782 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 56
  %1783 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1590, <16 x i16> %1574) #9
  %1784 = bitcast <4 x i64>* %1781 to <16 x i16>*
  store <16 x i16> %1783, <16 x i16>* %1784, align 32
  %1785 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1590, <16 x i16> %1574) #9
  %1786 = bitcast <4 x i64>* %1782 to <16 x i16>*
  store <16 x i16> %1785, <16 x i16>* %1786, align 32
  %1787 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %1788 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 55
  %1789 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1592, <16 x i16> %1627) #9
  %1790 = bitcast <4 x i64>* %1787 to <16 x i16>*
  store <16 x i16> %1789, <16 x i16>* %1790, align 32
  %1791 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1592, <16 x i16> %1627) #9
  %1792 = bitcast <4 x i64>* %1788 to <16 x i16>*
  store <16 x i16> %1791, <16 x i16>* %1792, align 32
  %1793 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %1794 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 54
  %1795 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1594, <16 x i16> %1643) #9
  %1796 = bitcast <4 x i64>* %1793 to <16 x i16>*
  store <16 x i16> %1795, <16 x i16>* %1796, align 32
  %1797 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1594, <16 x i16> %1643) #9
  %1798 = bitcast <4 x i64>* %1794 to <16 x i16>*
  store <16 x i16> %1797, <16 x i16>* %1798, align 32
  %1799 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %1800 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 53
  %1801 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1596, <16 x i16> %1659) #9
  %1802 = bitcast <4 x i64>* %1799 to <16 x i16>*
  store <16 x i16> %1801, <16 x i16>* %1802, align 32
  %1803 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1596, <16 x i16> %1659) #9
  %1804 = bitcast <4 x i64>* %1800 to <16 x i16>*
  store <16 x i16> %1803, <16 x i16>* %1804, align 32
  %1805 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %1806 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 52
  %1807 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1598, <16 x i16> %1675) #9
  %1808 = bitcast <4 x i64>* %1805 to <16 x i16>*
  store <16 x i16> %1807, <16 x i16>* %1808, align 32
  %1809 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1598, <16 x i16> %1675) #9
  %1810 = bitcast <4 x i64>* %1806 to <16 x i16>*
  store <16 x i16> %1809, <16 x i16>* %1810, align 32
  %1811 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %1812 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 51
  %1813 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1601, <16 x i16> %1691) #9
  %1814 = bitcast <4 x i64>* %1811 to <16 x i16>*
  store <16 x i16> %1813, <16 x i16>* %1814, align 32
  %1815 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1601, <16 x i16> %1691) #9
  %1816 = bitcast <4 x i64>* %1812 to <16 x i16>*
  store <16 x i16> %1815, <16 x i16>* %1816, align 32
  %1817 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %1818 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 50
  %1819 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1604, <16 x i16> %1707) #9
  %1820 = bitcast <4 x i64>* %1817 to <16 x i16>*
  store <16 x i16> %1819, <16 x i16>* %1820, align 32
  %1821 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1604, <16 x i16> %1707) #9
  %1822 = bitcast <4 x i64>* %1818 to <16 x i16>*
  store <16 x i16> %1821, <16 x i16>* %1822, align 32
  %1823 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %1824 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 49
  %1825 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1607, <16 x i16> %1723) #9
  %1826 = bitcast <4 x i64>* %1823 to <16 x i16>*
  store <16 x i16> %1825, <16 x i16>* %1826, align 32
  %1827 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1607, <16 x i16> %1723) #9
  %1828 = bitcast <4 x i64>* %1824 to <16 x i16>*
  store <16 x i16> %1827, <16 x i16>* %1828, align 32
  %1829 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 15
  %1830 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 48
  %1831 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1610, <16 x i16> %1739) #9
  %1832 = bitcast <4 x i64>* %1829 to <16 x i16>*
  store <16 x i16> %1831, <16 x i16>* %1832, align 32
  %1833 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1610, <16 x i16> %1739) #9
  %1834 = bitcast <4 x i64>* %1830 to <16 x i16>*
  store <16 x i16> %1833, <16 x i16>* %1834, align 32
  %1835 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 16
  %1836 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 47
  %1837 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1611, <16 x i16> %1738) #9
  %1838 = bitcast <4 x i64>* %1835 to <16 x i16>*
  store <16 x i16> %1837, <16 x i16>* %1838, align 32
  %1839 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1611, <16 x i16> %1738) #9
  %1840 = bitcast <4 x i64>* %1836 to <16 x i16>*
  store <16 x i16> %1839, <16 x i16>* %1840, align 32
  %1841 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 17
  %1842 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 46
  %1843 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1608, <16 x i16> %1722) #9
  %1844 = bitcast <4 x i64>* %1841 to <16 x i16>*
  store <16 x i16> %1843, <16 x i16>* %1844, align 32
  %1845 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1608, <16 x i16> %1722) #9
  %1846 = bitcast <4 x i64>* %1842 to <16 x i16>*
  store <16 x i16> %1845, <16 x i16>* %1846, align 32
  %1847 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 18
  %1848 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 45
  %1849 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1605, <16 x i16> %1706) #9
  %1850 = bitcast <4 x i64>* %1847 to <16 x i16>*
  store <16 x i16> %1849, <16 x i16>* %1850, align 32
  %1851 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1605, <16 x i16> %1706) #9
  %1852 = bitcast <4 x i64>* %1848 to <16 x i16>*
  store <16 x i16> %1851, <16 x i16>* %1852, align 32
  %1853 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 19
  %1854 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 44
  %1855 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1602, <16 x i16> %1690) #9
  %1856 = bitcast <4 x i64>* %1853 to <16 x i16>*
  store <16 x i16> %1855, <16 x i16>* %1856, align 32
  %1857 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1602, <16 x i16> %1690) #9
  %1858 = bitcast <4 x i64>* %1854 to <16 x i16>*
  store <16 x i16> %1857, <16 x i16>* %1858, align 32
  %1859 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 20
  %1860 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 43
  %1861 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1599, <16 x i16> %1674) #9
  %1862 = bitcast <4 x i64>* %1859 to <16 x i16>*
  store <16 x i16> %1861, <16 x i16>* %1862, align 32
  %1863 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1599, <16 x i16> %1674) #9
  %1864 = bitcast <4 x i64>* %1860 to <16 x i16>*
  store <16 x i16> %1863, <16 x i16>* %1864, align 32
  %1865 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 21
  %1866 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 42
  %1867 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1597, <16 x i16> %1658) #9
  %1868 = bitcast <4 x i64>* %1865 to <16 x i16>*
  store <16 x i16> %1867, <16 x i16>* %1868, align 32
  %1869 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1597, <16 x i16> %1658) #9
  %1870 = bitcast <4 x i64>* %1866 to <16 x i16>*
  store <16 x i16> %1869, <16 x i16>* %1870, align 32
  %1871 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 22
  %1872 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 41
  %1873 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1595, <16 x i16> %1642) #9
  %1874 = bitcast <4 x i64>* %1871 to <16 x i16>*
  store <16 x i16> %1873, <16 x i16>* %1874, align 32
  %1875 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1595, <16 x i16> %1642) #9
  %1876 = bitcast <4 x i64>* %1872 to <16 x i16>*
  store <16 x i16> %1875, <16 x i16>* %1876, align 32
  %1877 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 23
  %1878 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 40
  %1879 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1593, <16 x i16> %1626) #9
  %1880 = bitcast <4 x i64>* %1877 to <16 x i16>*
  store <16 x i16> %1879, <16 x i16>* %1880, align 32
  %1881 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1593, <16 x i16> %1626) #9
  %1882 = bitcast <4 x i64>* %1878 to <16 x i16>*
  store <16 x i16> %1881, <16 x i16>* %1882, align 32
  %1883 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 24
  %1884 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 39
  %1885 = load <16 x i16>, <16 x i16>* %265, align 32
  %1886 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1591, <16 x i16> %1885) #9
  %1887 = bitcast <4 x i64>* %1883 to <16 x i16>*
  store <16 x i16> %1886, <16 x i16>* %1887, align 32
  %1888 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1591, <16 x i16> %1885) #9
  %1889 = bitcast <4 x i64>* %1884 to <16 x i16>*
  store <16 x i16> %1888, <16 x i16>* %1889, align 32
  %1890 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 25
  %1891 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 38
  %1892 = load <16 x i16>, <16 x i16>* %247, align 32
  %1893 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1589, <16 x i16> %1892) #9
  %1894 = bitcast <4 x i64>* %1890 to <16 x i16>*
  store <16 x i16> %1893, <16 x i16>* %1894, align 32
  %1895 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1589, <16 x i16> %1892) #9
  %1896 = bitcast <4 x i64>* %1891 to <16 x i16>*
  store <16 x i16> %1895, <16 x i16>* %1896, align 32
  %1897 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 26
  %1898 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 37
  %1899 = load <16 x i16>, <16 x i16>* %232, align 32
  %1900 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1587, <16 x i16> %1899) #9
  %1901 = bitcast <4 x i64>* %1897 to <16 x i16>*
  store <16 x i16> %1900, <16 x i16>* %1901, align 32
  %1902 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1587, <16 x i16> %1899) #9
  %1903 = bitcast <4 x i64>* %1898 to <16 x i16>*
  store <16 x i16> %1902, <16 x i16>* %1903, align 32
  %1904 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 27
  %1905 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 36
  %1906 = load <16 x i16>, <16 x i16>* %214, align 32
  %1907 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1585, <16 x i16> %1906) #9
  %1908 = bitcast <4 x i64>* %1904 to <16 x i16>*
  store <16 x i16> %1907, <16 x i16>* %1908, align 32
  %1909 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1585, <16 x i16> %1906) #9
  %1910 = bitcast <4 x i64>* %1905 to <16 x i16>*
  store <16 x i16> %1909, <16 x i16>* %1910, align 32
  %1911 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 28
  %1912 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 35
  %1913 = load <16 x i16>, <16 x i16>* %199, align 32
  %1914 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1583, <16 x i16> %1913) #9
  %1915 = bitcast <4 x i64>* %1911 to <16 x i16>*
  store <16 x i16> %1914, <16 x i16>* %1915, align 32
  %1916 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1583, <16 x i16> %1913) #9
  %1917 = bitcast <4 x i64>* %1912 to <16 x i16>*
  store <16 x i16> %1916, <16 x i16>* %1917, align 32
  %1918 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 29
  %1919 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 34
  %1920 = load <16 x i16>, <16 x i16>* %448, align 32
  %1921 = load <16 x i16>, <16 x i16>* %182, align 32
  %1922 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1920, <16 x i16> %1921) #9
  %1923 = bitcast <4 x i64>* %1918 to <16 x i16>*
  store <16 x i16> %1922, <16 x i16>* %1923, align 32
  %1924 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1920, <16 x i16> %1921) #9
  %1925 = bitcast <4 x i64>* %1919 to <16 x i16>*
  store <16 x i16> %1924, <16 x i16>* %1925, align 32
  %1926 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 30
  %1927 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 33
  %1928 = load <16 x i16>, <16 x i16>* %432, align 32
  %1929 = load <16 x i16>, <16 x i16>* %167, align 32
  %1930 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1928, <16 x i16> %1929) #9
  %1931 = bitcast <4 x i64>* %1926 to <16 x i16>*
  store <16 x i16> %1930, <16 x i16>* %1931, align 32
  %1932 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1928, <16 x i16> %1929) #9
  %1933 = bitcast <4 x i64>* %1927 to <16 x i16>*
  store <16 x i16> %1932, <16 x i16>* %1933, align 32
  %1934 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 31
  %1935 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 32
  %1936 = load <16 x i16>, <16 x i16>* %415, align 32
  %1937 = load <16 x i16>, <16 x i16>* %150, align 32
  %1938 = tail call <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16> %1936, <16 x i16> %1937) #9
  %1939 = bitcast <4 x i64>* %1934 to <16 x i16>*
  store <16 x i16> %1938, <16 x i16>* %1939, align 32
  %1940 = tail call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %1936, <16 x i16> %1937) #9
  %1941 = bitcast <4 x i64>* %1935 to <16 x i16>*
  store <16 x i16> %1940, <16 x i16>* %1941, align 32
  call void @llvm.lifetime.end.p0i8(i64 2048, i8* nonnull %38) #9
  ret void
}

; Function Attrs: nounwind readnone speculatable
declare <16 x i16> @llvm.sadd.sat.v16i16(<16 x i16>, <16 x i16>) #5

; Function Attrs: nounwind readnone speculatable
declare <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16>, <16 x i16>) #5

; Function Attrs: nounwind readnone
declare <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16>, <16 x i16>) #6

; Function Attrs: nounwind readnone
declare <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32>, i32) #6

; Function Attrs: nounwind readnone
declare <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32>, <8 x i32>) #6

; Function Attrs: inlinehint nofree nounwind ssp uwtable
define internal fastcc void @idct64_stage4_high32_avx2(<4 x i64>* nocapture, <4 x i64>, i8 signext) unnamed_addr #7 {
  %4 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 4), align 16
  %5 = sub i32 0, %4
  %6 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 60), align 16
  %7 = and i32 %5, 65535
  %8 = and i32 %6, 65535
  %9 = shl nuw i32 %8, 16
  %10 = or i32 %9, %7
  %11 = insertelement <8 x i32> undef, i32 %10, i32 0
  %12 = shufflevector <8 x i32> %11, <8 x i32> undef, <8 x i32> zeroinitializer
  %13 = shl i32 %4, 16
  %14 = or i32 %8, %13
  %15 = insertelement <8 x i32> undef, i32 %14, i32 0
  %16 = shufflevector <8 x i32> %15, <8 x i32> undef, <8 x i32> zeroinitializer
  %17 = sub i32 0, %6
  %18 = and i32 %17, 65535
  %19 = shl nuw i32 %7, 16
  %20 = or i32 %19, %18
  %21 = insertelement <8 x i32> undef, i32 %20, i32 0
  %22 = shufflevector <8 x i32> %21, <8 x i32> undef, <8 x i32> zeroinitializer
  %23 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 36), align 16
  %24 = sub i32 0, %23
  %25 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 28), align 16
  %26 = and i32 %24, 65535
  %27 = and i32 %25, 65535
  %28 = shl nuw i32 %27, 16
  %29 = or i32 %28, %26
  %30 = insertelement <8 x i32> undef, i32 %29, i32 0
  %31 = shufflevector <8 x i32> %30, <8 x i32> undef, <8 x i32> zeroinitializer
  %32 = shl i32 %23, 16
  %33 = or i32 %27, %32
  %34 = insertelement <8 x i32> undef, i32 %33, i32 0
  %35 = shufflevector <8 x i32> %34, <8 x i32> undef, <8 x i32> zeroinitializer
  %36 = sub i32 0, %25
  %37 = and i32 %36, 65535
  %38 = shl nuw i32 %26, 16
  %39 = or i32 %38, %37
  %40 = insertelement <8 x i32> undef, i32 %39, i32 0
  %41 = shufflevector <8 x i32> %40, <8 x i32> undef, <8 x i32> zeroinitializer
  %42 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 20), align 16
  %43 = sub i32 0, %42
  %44 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 44), align 16
  %45 = and i32 %43, 65535
  %46 = and i32 %44, 65535
  %47 = shl nuw i32 %46, 16
  %48 = or i32 %47, %45
  %49 = insertelement <8 x i32> undef, i32 %48, i32 0
  %50 = shufflevector <8 x i32> %49, <8 x i32> undef, <8 x i32> zeroinitializer
  %51 = shl i32 %42, 16
  %52 = or i32 %46, %51
  %53 = insertelement <8 x i32> undef, i32 %52, i32 0
  %54 = shufflevector <8 x i32> %53, <8 x i32> undef, <8 x i32> zeroinitializer
  %55 = sub i32 0, %44
  %56 = and i32 %55, 65535
  %57 = shl nuw i32 %45, 16
  %58 = or i32 %57, %56
  %59 = insertelement <8 x i32> undef, i32 %58, i32 0
  %60 = shufflevector <8 x i32> %59, <8 x i32> undef, <8 x i32> zeroinitializer
  %61 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 52), align 16
  %62 = sub i32 0, %61
  %63 = load i32, i32* getelementptr inbounds ([7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 2, i64 12), align 16
  %64 = and i32 %62, 65535
  %65 = and i32 %63, 65535
  %66 = shl nuw i32 %65, 16
  %67 = or i32 %66, %64
  %68 = insertelement <8 x i32> undef, i32 %67, i32 0
  %69 = shufflevector <8 x i32> %68, <8 x i32> undef, <8 x i32> zeroinitializer
  %70 = shl i32 %61, 16
  %71 = or i32 %65, %70
  %72 = insertelement <8 x i32> undef, i32 %71, i32 0
  %73 = shufflevector <8 x i32> %72, <8 x i32> undef, <8 x i32> zeroinitializer
  %74 = sub i32 0, %63
  %75 = and i32 %74, 65535
  %76 = shl nuw i32 %64, 16
  %77 = or i32 %76, %75
  %78 = insertelement <8 x i32> undef, i32 %77, i32 0
  %79 = shufflevector <8 x i32> %78, <8 x i32> undef, <8 x i32> zeroinitializer
  %80 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 33
  %81 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 62
  %82 = sext i8 %2 to i32
  %83 = bitcast <4 x i64>* %80 to <16 x i16>*
  %84 = load <16 x i16>, <16 x i16>* %83, align 32
  %85 = bitcast <4 x i64>* %81 to <16 x i16>*
  %86 = load <16 x i16>, <16 x i16>* %85, align 32
  %87 = shufflevector <16 x i16> %84, <16 x i16> %86, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %88 = shufflevector <16 x i16> %84, <16 x i16> %86, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %89 = bitcast <8 x i32> %12 to <16 x i16>
  %90 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %87, <16 x i16> %89) #9
  %91 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %88, <16 x i16> %89) #9
  %92 = bitcast <8 x i32> %16 to <16 x i16>
  %93 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %87, <16 x i16> %92) #9
  %94 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %88, <16 x i16> %92) #9
  %95 = bitcast <4 x i64> %1 to <8 x i32>
  %96 = add <8 x i32> %90, %95
  %97 = add <8 x i32> %91, %95
  %98 = add <8 x i32> %93, %95
  %99 = add <8 x i32> %94, %95
  %100 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %96, i32 %82) #9
  %101 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %97, i32 %82) #9
  %102 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %98, i32 %82) #9
  %103 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %99, i32 %82) #9
  %104 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %100, <8 x i32> %101) #9
  store <16 x i16> %104, <16 x i16>* %83, align 32
  %105 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %102, <8 x i32> %103) #9
  store <16 x i16> %105, <16 x i16>* %85, align 32
  %106 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 34
  %107 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 61
  %108 = bitcast <4 x i64>* %106 to <16 x i16>*
  %109 = load <16 x i16>, <16 x i16>* %108, align 32
  %110 = bitcast <4 x i64>* %107 to <16 x i16>*
  %111 = load <16 x i16>, <16 x i16>* %110, align 32
  %112 = shufflevector <16 x i16> %109, <16 x i16> %111, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %113 = shufflevector <16 x i16> %109, <16 x i16> %111, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %114 = bitcast <8 x i32> %22 to <16 x i16>
  %115 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %112, <16 x i16> %114) #9
  %116 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %113, <16 x i16> %114) #9
  %117 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %112, <16 x i16> %89) #9
  %118 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %113, <16 x i16> %89) #9
  %119 = add <8 x i32> %115, %95
  %120 = add <8 x i32> %116, %95
  %121 = add <8 x i32> %117, %95
  %122 = add <8 x i32> %118, %95
  %123 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %119, i32 %82) #9
  %124 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %120, i32 %82) #9
  %125 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %121, i32 %82) #9
  %126 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %122, i32 %82) #9
  %127 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %123, <8 x i32> %124) #9
  store <16 x i16> %127, <16 x i16>* %108, align 32
  %128 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %125, <8 x i32> %126) #9
  store <16 x i16> %128, <16 x i16>* %110, align 32
  %129 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 37
  %130 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 58
  %131 = bitcast <4 x i64>* %129 to <16 x i16>*
  %132 = load <16 x i16>, <16 x i16>* %131, align 32
  %133 = bitcast <4 x i64>* %130 to <16 x i16>*
  %134 = load <16 x i16>, <16 x i16>* %133, align 32
  %135 = shufflevector <16 x i16> %132, <16 x i16> %134, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %136 = shufflevector <16 x i16> %132, <16 x i16> %134, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %137 = bitcast <8 x i32> %31 to <16 x i16>
  %138 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %135, <16 x i16> %137) #9
  %139 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %136, <16 x i16> %137) #9
  %140 = bitcast <8 x i32> %35 to <16 x i16>
  %141 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %135, <16 x i16> %140) #9
  %142 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %136, <16 x i16> %140) #9
  %143 = add <8 x i32> %138, %95
  %144 = add <8 x i32> %139, %95
  %145 = add <8 x i32> %141, %95
  %146 = add <8 x i32> %142, %95
  %147 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %143, i32 %82) #9
  %148 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %144, i32 %82) #9
  %149 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %145, i32 %82) #9
  %150 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %146, i32 %82) #9
  %151 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %147, <8 x i32> %148) #9
  store <16 x i16> %151, <16 x i16>* %131, align 32
  %152 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %149, <8 x i32> %150) #9
  store <16 x i16> %152, <16 x i16>* %133, align 32
  %153 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 38
  %154 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 57
  %155 = bitcast <4 x i64>* %153 to <16 x i16>*
  %156 = load <16 x i16>, <16 x i16>* %155, align 32
  %157 = bitcast <4 x i64>* %154 to <16 x i16>*
  %158 = load <16 x i16>, <16 x i16>* %157, align 32
  %159 = shufflevector <16 x i16> %156, <16 x i16> %158, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %160 = shufflevector <16 x i16> %156, <16 x i16> %158, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %161 = bitcast <8 x i32> %41 to <16 x i16>
  %162 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %159, <16 x i16> %161) #9
  %163 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %160, <16 x i16> %161) #9
  %164 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %159, <16 x i16> %137) #9
  %165 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %160, <16 x i16> %137) #9
  %166 = add <8 x i32> %162, %95
  %167 = add <8 x i32> %163, %95
  %168 = add <8 x i32> %164, %95
  %169 = add <8 x i32> %165, %95
  %170 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %166, i32 %82) #9
  %171 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %167, i32 %82) #9
  %172 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %168, i32 %82) #9
  %173 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %169, i32 %82) #9
  %174 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %170, <8 x i32> %171) #9
  store <16 x i16> %174, <16 x i16>* %155, align 32
  %175 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %172, <8 x i32> %173) #9
  store <16 x i16> %175, <16 x i16>* %157, align 32
  %176 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 41
  %177 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 54
  %178 = bitcast <4 x i64>* %176 to <16 x i16>*
  %179 = load <16 x i16>, <16 x i16>* %178, align 32
  %180 = bitcast <4 x i64>* %177 to <16 x i16>*
  %181 = load <16 x i16>, <16 x i16>* %180, align 32
  %182 = shufflevector <16 x i16> %179, <16 x i16> %181, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %183 = shufflevector <16 x i16> %179, <16 x i16> %181, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %184 = bitcast <8 x i32> %50 to <16 x i16>
  %185 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %182, <16 x i16> %184) #9
  %186 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %183, <16 x i16> %184) #9
  %187 = bitcast <8 x i32> %54 to <16 x i16>
  %188 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %182, <16 x i16> %187) #9
  %189 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %183, <16 x i16> %187) #9
  %190 = add <8 x i32> %185, %95
  %191 = add <8 x i32> %186, %95
  %192 = add <8 x i32> %188, %95
  %193 = add <8 x i32> %189, %95
  %194 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %190, i32 %82) #9
  %195 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %191, i32 %82) #9
  %196 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %192, i32 %82) #9
  %197 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %193, i32 %82) #9
  %198 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %194, <8 x i32> %195) #9
  store <16 x i16> %198, <16 x i16>* %178, align 32
  %199 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %196, <8 x i32> %197) #9
  store <16 x i16> %199, <16 x i16>* %180, align 32
  %200 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 42
  %201 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 53
  %202 = bitcast <4 x i64>* %200 to <16 x i16>*
  %203 = load <16 x i16>, <16 x i16>* %202, align 32
  %204 = bitcast <4 x i64>* %201 to <16 x i16>*
  %205 = load <16 x i16>, <16 x i16>* %204, align 32
  %206 = shufflevector <16 x i16> %203, <16 x i16> %205, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %207 = shufflevector <16 x i16> %203, <16 x i16> %205, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %208 = bitcast <8 x i32> %60 to <16 x i16>
  %209 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %206, <16 x i16> %208) #9
  %210 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %207, <16 x i16> %208) #9
  %211 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %206, <16 x i16> %184) #9
  %212 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %207, <16 x i16> %184) #9
  %213 = add <8 x i32> %209, %95
  %214 = add <8 x i32> %210, %95
  %215 = add <8 x i32> %211, %95
  %216 = add <8 x i32> %212, %95
  %217 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %213, i32 %82) #9
  %218 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %214, i32 %82) #9
  %219 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %215, i32 %82) #9
  %220 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %216, i32 %82) #9
  %221 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %217, <8 x i32> %218) #9
  store <16 x i16> %221, <16 x i16>* %202, align 32
  %222 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %219, <8 x i32> %220) #9
  store <16 x i16> %222, <16 x i16>* %204, align 32
  %223 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 45
  %224 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 50
  %225 = bitcast <4 x i64>* %223 to <16 x i16>*
  %226 = load <16 x i16>, <16 x i16>* %225, align 32
  %227 = bitcast <4 x i64>* %224 to <16 x i16>*
  %228 = load <16 x i16>, <16 x i16>* %227, align 32
  %229 = shufflevector <16 x i16> %226, <16 x i16> %228, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %230 = shufflevector <16 x i16> %226, <16 x i16> %228, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %231 = bitcast <8 x i32> %69 to <16 x i16>
  %232 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %229, <16 x i16> %231) #9
  %233 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %230, <16 x i16> %231) #9
  %234 = bitcast <8 x i32> %73 to <16 x i16>
  %235 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %229, <16 x i16> %234) #9
  %236 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %230, <16 x i16> %234) #9
  %237 = add <8 x i32> %232, %95
  %238 = add <8 x i32> %233, %95
  %239 = add <8 x i32> %235, %95
  %240 = add <8 x i32> %236, %95
  %241 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %237, i32 %82) #9
  %242 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %238, i32 %82) #9
  %243 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %239, i32 %82) #9
  %244 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %240, i32 %82) #9
  %245 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %241, <8 x i32> %242) #9
  store <16 x i16> %245, <16 x i16>* %225, align 32
  %246 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %243, <8 x i32> %244) #9
  store <16 x i16> %246, <16 x i16>* %227, align 32
  %247 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 46
  %248 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 49
  %249 = bitcast <4 x i64>* %247 to <16 x i16>*
  %250 = load <16 x i16>, <16 x i16>* %249, align 32
  %251 = bitcast <4 x i64>* %248 to <16 x i16>*
  %252 = load <16 x i16>, <16 x i16>* %251, align 32
  %253 = shufflevector <16 x i16> %250, <16 x i16> %252, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27>
  %254 = shufflevector <16 x i16> %250, <16 x i16> %252, <16 x i32> <i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %255 = bitcast <8 x i32> %79 to <16 x i16>
  %256 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %253, <16 x i16> %255) #9
  %257 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %254, <16 x i16> %255) #9
  %258 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %253, <16 x i16> %231) #9
  %259 = tail call <8 x i32> @llvm.x86.avx2.pmadd.wd(<16 x i16> %254, <16 x i16> %231) #9
  %260 = add <8 x i32> %256, %95
  %261 = add <8 x i32> %257, %95
  %262 = add <8 x i32> %258, %95
  %263 = add <8 x i32> %259, %95
  %264 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %260, i32 %82) #9
  %265 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %261, i32 %82) #9
  %266 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %262, i32 %82) #9
  %267 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %263, i32 %82) #9
  %268 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %264, <8 x i32> %265) #9
  store <16 x i16> %268, <16 x i16>* %249, align 32
  %269 = tail call <16 x i16> @llvm.x86.avx2.packssdw(<8 x i32> %266, <8 x i32> %267) #9
  store <16 x i16> %269, <16 x i16>* %251, align 32
  ret void
}

; Function Attrs: nounwind readonly
declare <32 x i8> @llvm.x86.avx.ldu.dq.256(i8*) #8

; Function Attrs: nounwind readnone
declare <16 x i16> @llvm.x86.avx2.pmul.hr.sw(<16 x i16>, <16 x i16>) #6

; Function Attrs: nounwind readnone
declare <32 x i8> @llvm.x86.avx2.packuswb(<16 x i16>, <16 x i16>) #6

; Function Attrs: nounwind readnone
declare <16 x i16> @llvm.x86.avx2.psrai.w(<16 x i16>, i32) #6

; Function Attrs: nounwind readnone
declare <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16>, i32) #6

attributes #0 = { nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="256" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+avx,+avx2,+cx8,+fxsr,+mmx,+popcnt,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+avx,+avx2,+cx8,+fxsr,+mmx,+popcnt,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #2 = { nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+avx,+avx2,+cx8,+fxsr,+mmx,+popcnt,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #3 = { argmemonly nounwind }
attributes #4 = { nofree nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="256" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+avx,+avx2,+cx8,+fxsr,+mmx,+popcnt,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #5 = { nounwind readnone speculatable }
attributes #6 = { nounwind readnone }
attributes #7 = { inlinehint nofree nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="256" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+avx,+avx2,+cx8,+fxsr,+mmx,+popcnt,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #8 = { nounwind readonly }
attributes #9 = { nounwind }

!llvm.module.flags = !{!0, !1}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 2}
!2 = distinct !{!2, !3}
!3 = !{!"llvm.loop.unroll.disable"}
!4 = distinct !{!4, !3}
!5 = distinct !{!5, !3}
!6 = distinct !{!6, !3}
