; ModuleID = '../../third_party/libaom/source/libaom/av1/common/x86/highbd_inv_txfm_avx2.c'
source_filename = "../../third_party/libaom/source/libaom/av1/common/x86/highbd_inv_txfm_avx2.c"
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

module asm ".symver exp, exp@GLIBC_2.2.5"
module asm ".symver exp2, exp2@GLIBC_2.2.5"
module asm ".symver exp2f, exp2f@GLIBC_2.2.5"
module asm ".symver expf, expf@GLIBC_2.2.5"
module asm ".symver lgamma, lgamma@GLIBC_2.2.5"
module asm ".symver lgammaf, lgammaf@GLIBC_2.2.5"
module asm ".symver lgammal, lgammal@GLIBC_2.2.5"
module asm ".symver log, log@GLIBC_2.2.5"
module asm ".symver log2, log2@GLIBC_2.2.5"
module asm ".symver log2f, log2f@GLIBC_2.2.5"
module asm ".symver logf, logf@GLIBC_2.2.5"
module asm ".symver pow, pow@GLIBC_2.2.5"
module asm ".symver powf, powf@GLIBC_2.2.5"

%struct.txfm_param = type { i8, i8, i32, i32, i32, i8, i32 }

@av1_inv_txfm_shift_ls = external local_unnamed_addr global [19 x i8*], align 16
@tx_size_wide = internal unnamed_addr constant [19 x i32] [i32 4, i32 8, i32 16, i32 32, i32 64, i32 4, i32 8, i32 8, i32 16, i32 16, i32 32, i32 32, i32 64, i32 4, i32 16, i32 8, i32 32, i32 16, i32 64], align 16
@tx_size_high = internal unnamed_addr constant [19 x i32] [i32 4, i32 8, i32 16, i32 32, i32 64, i32 8, i32 4, i32 16, i32 8, i32 32, i32 16, i32 64, i32 32, i32 16, i32 4, i32 32, i32 8, i32 64, i32 16], align 16
@lowbd_txfm_all_1d_zeros_idx = internal unnamed_addr constant [32 x i32] [i32 0, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 2, i32 2, i32 2, i32 2, i32 2, i32 2, i32 2, i32 2, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3], align 16
@highbd_txfm_all_1d_zeros_w8_arr = internal unnamed_addr constant [5 x [3 x [4 x void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)*]]] [[3 x [4 x void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)*]] zeroinitializer, [3 x [4 x void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)*]] [[4 x void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)*] [void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)* @idct8x8_low1_avx2, void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)* @idct8x8_avx2, void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)* null, void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)* null], [4 x void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)*] [void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)* @iadst8x8_low1_avx2, void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)* @iadst8x8_avx2, void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)* null, void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)* null], [4 x void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)*] zeroinitializer], [3 x [4 x void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)*]] [[4 x void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)*] [void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)* @idct16_low1_avx2, void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)* @idct16_low8_avx2, void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)* @idct16_avx2, void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)* null], [4 x void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)*] [void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)* @iadst16_low1_avx2, void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)* @iadst16_low8_avx2, void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)* @iadst16_avx2, void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)* null], [4 x void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)*] zeroinitializer], [3 x [4 x void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)*]] [[4 x void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)*] [void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)* @idct32_low1_avx2, void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)* @idct32_low8_avx2, void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)* @idct32_low16_avx2, void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)* @idct32_avx2], [4 x void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)*] zeroinitializer, [4 x void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)*] zeroinitializer], [3 x [4 x void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)*]] [[4 x void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)*] [void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)* @idct64_low1_avx2, void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)* @idct64_low8_avx2, void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)* @idct64_low16_avx2, void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)* @idct64_avx2], [4 x void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)*] zeroinitializer, [4 x void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)*] zeroinitializer]], align 16
@hitx_1d_tab = internal unnamed_addr constant [16 x i8] c"\00\00\01\01\00\01\01\01\01\02\02\00\02\01\02\01", align 16
@vitx_1d_tab = internal unnamed_addr constant [16 x i8] c"\00\01\00\01\01\00\01\01\01\02\00\02\01\02\01\02", align 16
@av1_inv_cos_bit_row = external local_unnamed_addr constant [5 x [5 x i8]], align 16
@av1_inv_cos_bit_col = external local_unnamed_addr constant [5 x [5 x i8]], align 16
@tx_size_wide_log2_eob = internal unnamed_addr constant [19 x i32] [i32 2, i32 3, i32 4, i32 5, i32 5, i32 2, i32 3, i32 3, i32 4, i32 4, i32 5, i32 5, i32 5, i32 2, i32 4, i32 3, i32 5, i32 4, i32 5], align 16
@av1_eob_to_eobxy_default = internal unnamed_addr constant [19 x i16*] [i16* null, i16* getelementptr inbounds ([8 x i16], [8 x i16]* @av1_eob_to_eobxy_8x8_default, i32 0, i32 0), i16* getelementptr inbounds ([16 x i16], [16 x i16]* @av1_eob_to_eobxy_16x16_default, i32 0, i32 0), i16* getelementptr inbounds ([32 x i16], [32 x i16]* @av1_eob_to_eobxy_32x32_default, i32 0, i32 0), i16* getelementptr inbounds ([32 x i16], [32 x i16]* @av1_eob_to_eobxy_32x32_default, i32 0, i32 0), i16* null, i16* null, i16* getelementptr inbounds ([16 x i16], [16 x i16]* @av1_eob_to_eobxy_8x16_default, i32 0, i32 0), i16* getelementptr inbounds ([8 x i16], [8 x i16]* @av1_eob_to_eobxy_16x8_default, i32 0, i32 0), i16* getelementptr inbounds ([32 x i16], [32 x i16]* @av1_eob_to_eobxy_16x32_default, i32 0, i32 0), i16* getelementptr inbounds ([16 x i16], [16 x i16]* @av1_eob_to_eobxy_32x16_default, i32 0, i32 0), i16* getelementptr inbounds ([32 x i16], [32 x i16]* @av1_eob_to_eobxy_32x32_default, i32 0, i32 0), i16* getelementptr inbounds ([32 x i16], [32 x i16]* @av1_eob_to_eobxy_32x32_default, i32 0, i32 0), i16* null, i16* null, i16* getelementptr inbounds ([32 x i16], [32 x i16]* @av1_eob_to_eobxy_8x32_default, i32 0, i32 0), i16* getelementptr inbounds ([8 x i16], [8 x i16]* @av1_eob_to_eobxy_32x8_default, i32 0, i32 0), i16* getelementptr inbounds ([32 x i16], [32 x i16]* @av1_eob_to_eobxy_16x32_default, i32 0, i32 0), i16* getelementptr inbounds ([16 x i16], [16 x i16]* @av1_eob_to_eobxy_32x16_default, i32 0, i32 0)], align 16
@av1_eob_to_eobxy_8x8_default = internal constant [8 x i16] [i16 1799, i16 1799, i16 1799, i16 1799, i16 1799, i16 1799, i16 1799, i16 1799], align 16
@av1_eob_to_eobxy_16x16_default = internal constant [16 x i16] [i16 1799, i16 1799, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855], align 16
@av1_eob_to_eobxy_32x32_default = internal constant [32 x i16] [i16 1799, i16 3855, i16 3855, i16 3855, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967, i16 7967], align 16
@av1_eob_to_eobxy_8x16_default = internal constant [16 x i16] [i16 1799, i16 1799, i16 1799, i16 1799, i16 1799, i16 3847, i16 3847, i16 3847, i16 3847, i16 3847, i16 3847, i16 3847, i16 3847, i16 3847, i16 3847, i16 3847], align 16
@av1_eob_to_eobxy_16x8_default = internal constant [8 x i16] [i16 1799, i16 1799, i16 1807, i16 1807, i16 1807, i16 1807, i16 1807, i16 1807], align 16
@av1_eob_to_eobxy_16x32_default = internal constant [32 x i16] [i16 1799, i16 1799, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 3855, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951, i16 7951], align 16
@av1_eob_to_eobxy_32x16_default = internal constant [16 x i16] [i16 1799, i16 3855, i16 3855, i16 3855, i16 3871, i16 3871, i16 3871, i16 3871, i16 3871, i16 3871, i16 3871, i16 3871, i16 3871, i16 3871, i16 3871, i16 3871], align 16
@av1_eob_to_eobxy_8x32_default = internal constant [32 x i16] [i16 1799, i16 1799, i16 1799, i16 1799, i16 1799, i16 3847, i16 3847, i16 3847, i16 3847, i16 3847, i16 3847, i16 3847, i16 3847, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943, i16 7943], align 16
@av1_eob_to_eobxy_32x8_default = internal constant [8 x i16] [i16 1799, i16 1807, i16 1807, i16 1823, i16 1823, i16 1823, i16 1823, i16 1823], align 16
@tx_size_wide_log2 = internal unnamed_addr constant [19 x i32] [i32 2, i32 3, i32 4, i32 5, i32 6, i32 2, i32 3, i32 3, i32 4, i32 4, i32 5, i32 5, i32 6, i32 2, i32 4, i32 3, i32 5, i32 4, i32 6], align 16
@tx_size_high_log2 = internal unnamed_addr constant [19 x i32] [i32 2, i32 3, i32 4, i32 5, i32 6, i32 3, i32 2, i32 4, i32 3, i32 5, i32 4, i32 6, i32 5, i32 4, i32 2, i32 5, i32 3, i32 6, i32 4], align 16
@av1_cospi_arr_data = external local_unnamed_addr constant [7 x [64 x i32]], align 16
@switch.table.av1_highbd_inv_txfm2d_add_universe_avx2 = private unnamed_addr constant [5 x i32] [i32 1, i32 0, i32 1, i32 0, i32 1], align 4
@switch.table.av1_highbd_inv_txfm2d_add_universe_avx2.1 = private unnamed_addr constant [5 x i32] [i32 0, i32 1, i32 1, i32 1, i32 0], align 4

; Function Attrs: nounwind ssp uwtable
define hidden void @av1_highbd_inv_txfm2d_add_universe_avx2(i32*, i8*, i32, i8 zeroext, i8 zeroext, i32, i32) local_unnamed_addr #0 {
  %8 = alloca [512 x <4 x i64>], align 32
  %9 = alloca [64 x <4 x i64>], align 32
  switch i8 %3, label %665 [
    i8 0, label %10
    i8 1, label %10
    i8 2, label %10
    i8 3, label %10
    i8 4, label %10
    i8 5, label %10
    i8 6, label %10
    i8 7, label %10
    i8 8, label %10
    i8 9, label %664
    i8 11, label %664
    i8 13, label %664
    i8 15, label %664
    i8 10, label %664
    i8 12, label %664
    i8 14, label %664
  ]

10:                                               ; preds = %7, %7, %7, %7, %7, %7, %7, %7, %7
  %11 = ptrtoint i8* %1 to i64
  %12 = shl i64 %11, 1
  %13 = inttoptr i64 %12 to i16*
  %14 = bitcast [512 x <4 x i64>]* %8 to i8*
  call void @llvm.lifetime.start.p0i8(i64 16384, i8* nonnull %14) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %14, i8 -86, i64 16384, i1 false) #8
  %15 = icmp eq i32 %5, 1
  %16 = zext i8 %4 to i64
  br i1 %15, label %30, label %17

17:                                               ; preds = %10
  %18 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_wide_log2_eob, i64 0, i64 %16
  %19 = load i32, i32* %18, align 4
  %20 = add nsw i32 %5, -1
  %21 = ashr i32 %20, %19
  %22 = getelementptr inbounds [19 x i16*], [19 x i16*]* @av1_eob_to_eobxy_default, i64 0, i64 %16
  %23 = load i16*, i16** %22, align 8
  %24 = sext i32 %21 to i64
  %25 = getelementptr inbounds i16, i16* %23, i64 %24
  %26 = load i16, i16* %25, align 2
  %27 = sext i16 %26 to i32
  %28 = and i32 %27, 255
  %29 = ashr i32 %27, 8
  br label %30

30:                                               ; preds = %17, %10
  %31 = phi i32 [ %28, %17 ], [ 0, %10 ]
  %32 = phi i32 [ %29, %17 ], [ 0, %10 ]
  %33 = getelementptr inbounds [19 x i8*], [19 x i8*]* @av1_inv_txfm_shift_ls, i64 0, i64 %16
  %34 = load i8*, i8** %33, align 8
  %35 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_wide_log2, i64 0, i64 %16
  %36 = load i32, i32* %35, align 4
  %37 = add nsw i32 %36, -2
  %38 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_high_log2, i64 0, i64 %16
  %39 = load i32, i32* %38, align 4
  %40 = add nsw i32 %39, -2
  %41 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_wide, i64 0, i64 %16
  %42 = load i32, i32* %41, align 4
  %43 = getelementptr inbounds [19 x i32], [19 x i32]* @tx_size_high, i64 0, i64 %16
  %44 = load i32, i32* %43, align 4
  %45 = ashr i32 %42, 3
  %46 = add nuw nsw i32 %31, 8
  %47 = lshr i32 %46, 3
  %48 = add nsw i32 %32, 8
  %49 = ashr i32 %48, 3
  %50 = icmp slt i32 %42, 32
  %51 = select i1 %50, i32 %42, i32 32
  %52 = icmp eq i32 %42, %44
  br i1 %52, label %68, label %53

53:                                               ; preds = %30
  %54 = icmp sgt i32 %42, %44
  br i1 %54, label %55, label %61

55:                                               ; preds = %53
  %56 = shl nsw i32 %44, 1
  %57 = icmp eq i32 %56, %42
  br i1 %57, label %68, label %58

58:                                               ; preds = %55
  %59 = shl nsw i32 %44, 2
  %60 = icmp eq i32 %59, %42
  br i1 %60, label %68, label %67

61:                                               ; preds = %53
  %62 = shl nsw i32 %42, 1
  %63 = icmp eq i32 %62, %44
  br i1 %63, label %68, label %64

64:                                               ; preds = %61
  %65 = shl nsw i32 %42, 2
  %66 = icmp eq i32 %65, %44
  br i1 %66, label %68, label %67

67:                                               ; preds = %64, %58
  br label %68

68:                                               ; preds = %67, %64, %61, %58, %55, %30
  %69 = phi i32 [ 0, %67 ], [ 0, %30 ], [ 1, %55 ], [ 2, %58 ], [ -1, %61 ], [ -2, %64 ]
  %70 = zext i32 %31 to i64
  %71 = getelementptr inbounds [32 x i32], [32 x i32]* @lowbd_txfm_all_1d_zeros_idx, i64 0, i64 %70
  %72 = load i32, i32* %71, align 4
  %73 = sext i32 %32 to i64
  %74 = getelementptr inbounds [32 x i32], [32 x i32]* @lowbd_txfm_all_1d_zeros_idx, i64 0, i64 %73
  %75 = load i32, i32* %74, align 4
  %76 = sext i32 %37 to i64
  %77 = zext i8 %3 to i64
  %78 = getelementptr inbounds [16 x i8], [16 x i8]* @hitx_1d_tab, i64 0, i64 %77
  %79 = load i8, i8* %78, align 1
  %80 = zext i8 %79 to i64
  %81 = sext i32 %72 to i64
  %82 = getelementptr inbounds [5 x [3 x [4 x void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)*]]], [5 x [3 x [4 x void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)*]]]* @highbd_txfm_all_1d_zeros_w8_arr, i64 0, i64 %76, i64 %80, i64 %81
  %83 = load void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)*, void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)** %82, align 8
  %84 = sext i32 %40 to i64
  %85 = getelementptr inbounds [16 x i8], [16 x i8]* @vitx_1d_tab, i64 0, i64 %77
  %86 = load i8, i8* %85, align 1
  %87 = zext i8 %86 to i64
  %88 = sext i32 %75 to i64
  %89 = getelementptr inbounds [5 x [3 x [4 x void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)*]]], [5 x [3 x [4 x void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)*]]]* @highbd_txfm_all_1d_zeros_w8_arr, i64 0, i64 %84, i64 %87, i64 %88
  %90 = load void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)*, void (<4 x i64>*, <4 x i64>*, i32, i32, i32, i32)** %89, align 8
  %91 = add i8 %3, -4
  %92 = icmp ult i8 %91, 5
  br i1 %92, label %93, label %100

93:                                               ; preds = %68
  %94 = sext i8 %91 to i64
  %95 = getelementptr inbounds [5 x i32], [5 x i32]* @switch.table.av1_highbd_inv_txfm2d_add_universe_avx2, i64 0, i64 %94
  %96 = load i32, i32* %95, align 4
  %97 = sext i8 %91 to i64
  %98 = getelementptr inbounds [5 x i32], [5 x i32]* @switch.table.av1_highbd_inv_txfm2d_add_universe_avx2.1, i64 0, i64 %97
  %99 = load i32, i32* %98, align 4
  br label %100

100:                                              ; preds = %93, %68
  %101 = phi i32 [ 0, %68 ], [ %96, %93 ]
  %102 = phi i32 [ 0, %68 ], [ %99, %93 ]
  %103 = icmp sgt i32 %48, 7
  br i1 %103, label %107, label %104

104:                                              ; preds = %100
  %105 = lshr i64 516062, %16
  %106 = and i64 %105, 1
  br label %132

107:                                              ; preds = %100
  %108 = bitcast [64 x <4 x i64>]* %9 to i8*
  %109 = shl i32 %51, 3
  %110 = sext i32 %51 to i64
  %111 = shl nsw i64 %110, 1
  %112 = mul nsw i64 %110, 3
  %113 = shl nsw i64 %110, 2
  %114 = mul nsw i64 %110, 5
  %115 = mul nsw i64 %110, 6
  %116 = mul nsw i64 %110, 7
  %117 = and i32 %46, 504
  %118 = icmp eq i32 %117, 0
  %119 = zext i32 %117 to i64
  %120 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %9, i64 0, i64 0
  %121 = getelementptr inbounds [5 x [5 x i8]], [5 x [5 x i8]]* @av1_inv_cos_bit_row, i64 0, i64 %76, i64 %84
  %122 = load i8, i8* %121, align 1
  %123 = sext i8 %122 to i32
  %124 = icmp eq i32 %102, 0
  %125 = lshr i64 516062, %16
  %126 = and i64 %125, 1
  %127 = icmp eq i64 %126, 0
  %128 = sext i32 %45 to i64
  %129 = sext i32 %44 to i64
  %130 = sext i32 %49 to i64
  %131 = zext i32 %47 to i64
  br label %152

132:                                              ; preds = %444, %104
  %133 = phi i64 [ %106, %104 ], [ %126, %444 ]
  %134 = icmp eq i64 %133, 0
  br i1 %134, label %447, label %135

135:                                              ; preds = %132
  %136 = getelementptr inbounds [5 x [5 x i8]], [5 x [5 x i8]]* @av1_inv_cos_bit_col, i64 0, i64 %76, i64 %84
  %137 = load i8, i8* %136, align 1
  %138 = sext i8 %137 to i32
  %139 = getelementptr inbounds i8, i8* %34, i64 1
  %140 = zext i32 %44 to i64
  %141 = sext i32 %44 to i64
  %142 = sext i32 %45 to i64
  %143 = add nsw i64 %140, -1
  %144 = and i64 %140, 3
  %145 = icmp ult i64 %143, 3
  %146 = sub nsw i64 %140, %144
  %147 = icmp eq i64 %144, 0
  %148 = and i64 %140, 3
  %149 = icmp ult i64 %143, 3
  %150 = sub nsw i64 %140, %148
  %151 = icmp eq i64 %148, 0
  br label %465

152:                                              ; preds = %444, %107
  %153 = phi i64 [ 0, %107 ], [ %445, %444 ]
  call void @llvm.lifetime.start.p0i8(i64 2048, i8* nonnull %108) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %108, i8 -86, i64 2048, i1 false) #8
  %154 = trunc i64 %153 to i32
  %155 = mul i32 %109, %154
  %156 = sext i32 %155 to i64
  %157 = getelementptr inbounds i32, i32* %0, i64 %156
  br label %159

158:                                              ; preds = %159
  switch i32 %69, label %262 [
    i32 -1, label %244
    i32 1, label %244
  ]

159:                                              ; preds = %159, %152
  %160 = phi i64 [ 0, %152 ], [ %242, %159 ]
  %161 = shl nsw i64 %160, 3
  %162 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %9, i64 0, i64 %161
  %163 = getelementptr inbounds i32, i32* %157, i64 %161
  %164 = bitcast i32* %163 to <8 x i32>*
  %165 = load <8 x i32>, <8 x i32>* %164, align 1
  %166 = getelementptr inbounds i32, i32* %163, i64 %110
  %167 = bitcast i32* %166 to <8 x i32>*
  %168 = load <8 x i32>, <8 x i32>* %167, align 1
  %169 = getelementptr inbounds <4 x i64>, <4 x i64>* %162, i64 1
  %170 = getelementptr inbounds i32, i32* %163, i64 %111
  %171 = bitcast i32* %170 to <8 x i32>*
  %172 = load <8 x i32>, <8 x i32>* %171, align 1
  %173 = getelementptr inbounds <4 x i64>, <4 x i64>* %162, i64 2
  %174 = getelementptr inbounds i32, i32* %163, i64 %112
  %175 = bitcast i32* %174 to <8 x i32>*
  %176 = load <8 x i32>, <8 x i32>* %175, align 1
  %177 = getelementptr inbounds <4 x i64>, <4 x i64>* %162, i64 3
  %178 = getelementptr inbounds i32, i32* %163, i64 %113
  %179 = bitcast i32* %178 to <8 x i32>*
  %180 = load <8 x i32>, <8 x i32>* %179, align 1
  %181 = getelementptr inbounds <4 x i64>, <4 x i64>* %162, i64 4
  %182 = getelementptr inbounds i32, i32* %163, i64 %114
  %183 = bitcast i32* %182 to <8 x i32>*
  %184 = load <8 x i32>, <8 x i32>* %183, align 1
  %185 = getelementptr inbounds <4 x i64>, <4 x i64>* %162, i64 5
  %186 = getelementptr inbounds i32, i32* %163, i64 %115
  %187 = bitcast i32* %186 to <8 x i32>*
  %188 = load <8 x i32>, <8 x i32>* %187, align 1
  %189 = getelementptr inbounds <4 x i64>, <4 x i64>* %162, i64 6
  %190 = getelementptr inbounds i32, i32* %163, i64 %116
  %191 = bitcast i32* %190 to <8 x i32>*
  %192 = load <8 x i32>, <8 x i32>* %191, align 1
  %193 = getelementptr inbounds <4 x i64>, <4 x i64>* %162, i64 7
  %194 = bitcast <4 x i64>* %162 to <8 x i32>*
  %195 = bitcast <4 x i64>* %169 to <8 x i32>*
  %196 = shufflevector <8 x i32> %165, <8 x i32> %168, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %197 = bitcast <8 x i32> %196 to <4 x i64>
  %198 = shufflevector <8 x i32> %165, <8 x i32> %168, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %199 = bitcast <8 x i32> %198 to <4 x i64>
  %200 = bitcast <4 x i64>* %173 to <8 x i32>*
  %201 = bitcast <4 x i64>* %177 to <8 x i32>*
  %202 = shufflevector <8 x i32> %172, <8 x i32> %176, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %203 = bitcast <8 x i32> %202 to <4 x i64>
  %204 = shufflevector <8 x i32> %172, <8 x i32> %176, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %205 = bitcast <8 x i32> %204 to <4 x i64>
  %206 = bitcast <4 x i64>* %181 to <8 x i32>*
  %207 = bitcast <4 x i64>* %185 to <8 x i32>*
  %208 = shufflevector <8 x i32> %180, <8 x i32> %184, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %209 = bitcast <8 x i32> %208 to <4 x i64>
  %210 = shufflevector <8 x i32> %180, <8 x i32> %184, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %211 = bitcast <8 x i32> %210 to <4 x i64>
  %212 = bitcast <4 x i64>* %189 to <8 x i32>*
  %213 = bitcast <4 x i64>* %193 to <8 x i32>*
  %214 = shufflevector <8 x i32> %188, <8 x i32> %192, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %215 = bitcast <8 x i32> %214 to <4 x i64>
  %216 = shufflevector <8 x i32> %188, <8 x i32> %192, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %217 = bitcast <8 x i32> %216 to <4 x i64>
  %218 = shufflevector <4 x i64> %197, <4 x i64> %203, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %219 = shufflevector <4 x i64> %209, <4 x i64> %215, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %220 = bitcast <4 x i64> %218 to <8 x i32>
  %221 = bitcast <4 x i64> %219 to <8 x i32>
  %222 = shufflevector <8 x i32> %220, <8 x i32> %221, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  store <8 x i32> %222, <8 x i32>* %194, align 32
  %223 = shufflevector <8 x i32> %220, <8 x i32> %221, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
  store <8 x i32> %223, <8 x i32>* %206, align 32
  %224 = shufflevector <4 x i64> %197, <4 x i64> %203, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %225 = shufflevector <4 x i64> %209, <4 x i64> %215, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %226 = bitcast <4 x i64> %224 to <8 x i32>
  %227 = bitcast <4 x i64> %225 to <8 x i32>
  %228 = shufflevector <8 x i32> %226, <8 x i32> %227, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  store <8 x i32> %228, <8 x i32>* %195, align 32
  %229 = shufflevector <8 x i32> %226, <8 x i32> %227, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
  store <8 x i32> %229, <8 x i32>* %207, align 32
  %230 = shufflevector <4 x i64> %199, <4 x i64> %205, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %231 = shufflevector <4 x i64> %211, <4 x i64> %217, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %232 = bitcast <4 x i64> %230 to <8 x i32>
  %233 = bitcast <4 x i64> %231 to <8 x i32>
  %234 = shufflevector <8 x i32> %232, <8 x i32> %233, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  store <8 x i32> %234, <8 x i32>* %200, align 32
  %235 = shufflevector <8 x i32> %232, <8 x i32> %233, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
  store <8 x i32> %235, <8 x i32>* %212, align 32
  %236 = shufflevector <4 x i64> %199, <4 x i64> %205, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %237 = shufflevector <4 x i64> %211, <4 x i64> %217, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %238 = bitcast <4 x i64> %236 to <8 x i32>
  %239 = bitcast <4 x i64> %237 to <8 x i32>
  %240 = shufflevector <8 x i32> %238, <8 x i32> %239, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  store <8 x i32> %240, <8 x i32>* %201, align 32
  %241 = shufflevector <8 x i32> %238, <8 x i32> %239, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
  store <8 x i32> %241, <8 x i32>* %213, align 32
  %242 = add nuw nsw i64 %160, 1
  %243 = icmp eq i64 %242, %131
  br i1 %243, label %158, label %159

244:                                              ; preds = %158, %158
  br i1 %118, label %262, label %245

245:                                              ; preds = %244, %245
  %246 = phi i64 [ %260, %245 ], [ 0, %244 ]
  %247 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %9, i64 0, i64 %246
  %248 = bitcast <4 x i64>* %247 to <8 x i32>*
  %249 = load <8 x i32>, <8 x i32>* %248, align 32
  %250 = mul <8 x i32> %249, <i32 2896, i32 2896, i32 2896, i32 2896, i32 2896, i32 2896, i32 2896, i32 2896>
  %251 = add <8 x i32> %250, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %252 = ashr <8 x i32> %251, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  store <8 x i32> %252, <8 x i32>* %248, align 32
  %253 = or i64 %246, 1
  %254 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %9, i64 0, i64 %253
  %255 = bitcast <4 x i64>* %254 to <8 x i32>*
  %256 = load <8 x i32>, <8 x i32>* %255, align 32
  %257 = mul <8 x i32> %256, <i32 2896, i32 2896, i32 2896, i32 2896, i32 2896, i32 2896, i32 2896, i32 2896>
  %258 = add <8 x i32> %257, <i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048, i32 2048>
  %259 = ashr <8 x i32> %258, <i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12, i32 12>
  store <8 x i32> %259, <8 x i32>* %255, align 32
  %260 = add nuw nsw i64 %246, 2
  %261 = icmp eq i64 %260, %119
  br i1 %261, label %262, label %245

262:                                              ; preds = %245, %244, %158
  %263 = load i8, i8* %34, align 1
  %264 = sext i8 %263 to i32
  %265 = sub nsw i32 0, %264
  call void %83(<4 x i64>* nonnull %120, <4 x i64>* nonnull %120, i32 %123, i32 0, i32 %6, i32 %265) #8
  %266 = shl nsw i64 %153, 3
  %267 = getelementptr inbounds [512 x <4 x i64>], [512 x <4 x i64>]* %8, i64 0, i64 %266
  br i1 %124, label %269, label %268

268:                                              ; preds = %262
  br i1 %127, label %444, label %270

269:                                              ; preds = %262
  br i1 %127, label %444, label %358

270:                                              ; preds = %268, %270
  %271 = phi i64 [ %356, %270 ], [ 0, %268 ]
  %272 = shl nsw i64 %271, 3
  %273 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %9, i64 0, i64 %272
  %274 = xor i64 %271, -1
  %275 = add nsw i64 %274, %128
  %276 = mul nsw i64 %275, %129
  %277 = getelementptr inbounds <4 x i64>, <4 x i64>* %267, i64 %276
  %278 = getelementptr inbounds <4 x i64>, <4 x i64>* %273, i64 7
  %279 = bitcast <4 x i64>* %278 to <8 x i32>*
  %280 = load <8 x i32>, <8 x i32>* %279, align 32
  %281 = getelementptr inbounds <4 x i64>, <4 x i64>* %273, i64 6
  %282 = bitcast <4 x i64>* %281 to <8 x i32>*
  %283 = load <8 x i32>, <8 x i32>* %282, align 32
  %284 = shufflevector <8 x i32> %280, <8 x i32> %283, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %285 = bitcast <8 x i32> %284 to <4 x i64>
  %286 = shufflevector <8 x i32> %280, <8 x i32> %283, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %287 = bitcast <8 x i32> %286 to <4 x i64>
  %288 = getelementptr inbounds <4 x i64>, <4 x i64>* %273, i64 5
  %289 = bitcast <4 x i64>* %288 to <8 x i32>*
  %290 = load <8 x i32>, <8 x i32>* %289, align 32
  %291 = getelementptr inbounds <4 x i64>, <4 x i64>* %273, i64 4
  %292 = bitcast <4 x i64>* %291 to <8 x i32>*
  %293 = load <8 x i32>, <8 x i32>* %292, align 32
  %294 = shufflevector <8 x i32> %290, <8 x i32> %293, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %295 = bitcast <8 x i32> %294 to <4 x i64>
  %296 = shufflevector <8 x i32> %290, <8 x i32> %293, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %297 = bitcast <8 x i32> %296 to <4 x i64>
  %298 = getelementptr inbounds <4 x i64>, <4 x i64>* %273, i64 3
  %299 = bitcast <4 x i64>* %298 to <8 x i32>*
  %300 = load <8 x i32>, <8 x i32>* %299, align 32
  %301 = getelementptr inbounds <4 x i64>, <4 x i64>* %273, i64 2
  %302 = bitcast <4 x i64>* %301 to <8 x i32>*
  %303 = load <8 x i32>, <8 x i32>* %302, align 32
  %304 = shufflevector <8 x i32> %300, <8 x i32> %303, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %305 = bitcast <8 x i32> %304 to <4 x i64>
  %306 = shufflevector <8 x i32> %300, <8 x i32> %303, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %307 = bitcast <8 x i32> %306 to <4 x i64>
  %308 = getelementptr inbounds <4 x i64>, <4 x i64>* %273, i64 1
  %309 = bitcast <4 x i64>* %308 to <8 x i32>*
  %310 = load <8 x i32>, <8 x i32>* %309, align 32
  %311 = bitcast <4 x i64>* %273 to <8 x i32>*
  %312 = load <8 x i32>, <8 x i32>* %311, align 32
  %313 = shufflevector <8 x i32> %310, <8 x i32> %312, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %314 = bitcast <8 x i32> %313 to <4 x i64>
  %315 = shufflevector <8 x i32> %310, <8 x i32> %312, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %316 = bitcast <8 x i32> %315 to <4 x i64>
  %317 = shufflevector <4 x i64> %285, <4 x i64> %295, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %318 = shufflevector <4 x i64> %305, <4 x i64> %314, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %319 = bitcast <4 x i64> %317 to <8 x i32>
  %320 = bitcast <4 x i64> %318 to <8 x i32>
  %321 = shufflevector <8 x i32> %319, <8 x i32> %320, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %322 = bitcast <4 x i64>* %277 to <8 x i32>*
  store <8 x i32> %321, <8 x i32>* %322, align 32
  %323 = shufflevector <8 x i32> %319, <8 x i32> %320, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
  %324 = getelementptr inbounds <4 x i64>, <4 x i64>* %277, i64 4
  %325 = bitcast <4 x i64>* %324 to <8 x i32>*
  store <8 x i32> %323, <8 x i32>* %325, align 32
  %326 = shufflevector <4 x i64> %285, <4 x i64> %295, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %327 = shufflevector <4 x i64> %305, <4 x i64> %314, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %328 = bitcast <4 x i64> %326 to <8 x i32>
  %329 = bitcast <4 x i64> %327 to <8 x i32>
  %330 = shufflevector <8 x i32> %328, <8 x i32> %329, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %331 = getelementptr inbounds <4 x i64>, <4 x i64>* %277, i64 1
  %332 = bitcast <4 x i64>* %331 to <8 x i32>*
  store <8 x i32> %330, <8 x i32>* %332, align 32
  %333 = shufflevector <8 x i32> %328, <8 x i32> %329, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
  %334 = getelementptr inbounds <4 x i64>, <4 x i64>* %277, i64 5
  %335 = bitcast <4 x i64>* %334 to <8 x i32>*
  store <8 x i32> %333, <8 x i32>* %335, align 32
  %336 = shufflevector <4 x i64> %287, <4 x i64> %297, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %337 = shufflevector <4 x i64> %307, <4 x i64> %316, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %338 = bitcast <4 x i64> %336 to <8 x i32>
  %339 = bitcast <4 x i64> %337 to <8 x i32>
  %340 = shufflevector <8 x i32> %338, <8 x i32> %339, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %341 = getelementptr inbounds <4 x i64>, <4 x i64>* %277, i64 2
  %342 = bitcast <4 x i64>* %341 to <8 x i32>*
  store <8 x i32> %340, <8 x i32>* %342, align 32
  %343 = shufflevector <8 x i32> %338, <8 x i32> %339, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
  %344 = getelementptr inbounds <4 x i64>, <4 x i64>* %277, i64 6
  %345 = bitcast <4 x i64>* %344 to <8 x i32>*
  store <8 x i32> %343, <8 x i32>* %345, align 32
  %346 = shufflevector <4 x i64> %287, <4 x i64> %297, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %347 = shufflevector <4 x i64> %307, <4 x i64> %316, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %348 = bitcast <4 x i64> %346 to <8 x i32>
  %349 = bitcast <4 x i64> %347 to <8 x i32>
  %350 = shufflevector <8 x i32> %348, <8 x i32> %349, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %351 = getelementptr inbounds <4 x i64>, <4 x i64>* %277, i64 3
  %352 = bitcast <4 x i64>* %351 to <8 x i32>*
  store <8 x i32> %350, <8 x i32>* %352, align 32
  %353 = shufflevector <8 x i32> %348, <8 x i32> %349, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
  %354 = getelementptr inbounds <4 x i64>, <4 x i64>* %277, i64 7
  %355 = bitcast <4 x i64>* %354 to <8 x i32>*
  store <8 x i32> %353, <8 x i32>* %355, align 32
  %356 = add nuw nsw i64 %271, 1
  %357 = icmp slt i64 %356, %128
  br i1 %357, label %270, label %444

358:                                              ; preds = %269, %358
  %359 = phi i64 [ %442, %358 ], [ 0, %269 ]
  %360 = shl nsw i64 %359, 3
  %361 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %9, i64 0, i64 %360
  %362 = mul nsw i64 %359, %129
  %363 = getelementptr inbounds <4 x i64>, <4 x i64>* %267, i64 %362
  %364 = bitcast <4 x i64>* %361 to <8 x i32>*
  %365 = load <8 x i32>, <8 x i32>* %364, align 32
  %366 = getelementptr inbounds <4 x i64>, <4 x i64>* %361, i64 1
  %367 = bitcast <4 x i64>* %366 to <8 x i32>*
  %368 = load <8 x i32>, <8 x i32>* %367, align 32
  %369 = shufflevector <8 x i32> %365, <8 x i32> %368, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %370 = bitcast <8 x i32> %369 to <4 x i64>
  %371 = shufflevector <8 x i32> %365, <8 x i32> %368, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %372 = bitcast <8 x i32> %371 to <4 x i64>
  %373 = getelementptr inbounds <4 x i64>, <4 x i64>* %361, i64 2
  %374 = bitcast <4 x i64>* %373 to <8 x i32>*
  %375 = load <8 x i32>, <8 x i32>* %374, align 32
  %376 = getelementptr inbounds <4 x i64>, <4 x i64>* %361, i64 3
  %377 = bitcast <4 x i64>* %376 to <8 x i32>*
  %378 = load <8 x i32>, <8 x i32>* %377, align 32
  %379 = shufflevector <8 x i32> %375, <8 x i32> %378, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %380 = bitcast <8 x i32> %379 to <4 x i64>
  %381 = shufflevector <8 x i32> %375, <8 x i32> %378, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %382 = bitcast <8 x i32> %381 to <4 x i64>
  %383 = getelementptr inbounds <4 x i64>, <4 x i64>* %361, i64 4
  %384 = bitcast <4 x i64>* %383 to <8 x i32>*
  %385 = load <8 x i32>, <8 x i32>* %384, align 32
  %386 = getelementptr inbounds <4 x i64>, <4 x i64>* %361, i64 5
  %387 = bitcast <4 x i64>* %386 to <8 x i32>*
  %388 = load <8 x i32>, <8 x i32>* %387, align 32
  %389 = shufflevector <8 x i32> %385, <8 x i32> %388, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %390 = bitcast <8 x i32> %389 to <4 x i64>
  %391 = shufflevector <8 x i32> %385, <8 x i32> %388, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %392 = bitcast <8 x i32> %391 to <4 x i64>
  %393 = getelementptr inbounds <4 x i64>, <4 x i64>* %361, i64 6
  %394 = bitcast <4 x i64>* %393 to <8 x i32>*
  %395 = load <8 x i32>, <8 x i32>* %394, align 32
  %396 = getelementptr inbounds <4 x i64>, <4 x i64>* %361, i64 7
  %397 = bitcast <4 x i64>* %396 to <8 x i32>*
  %398 = load <8 x i32>, <8 x i32>* %397, align 32
  %399 = shufflevector <8 x i32> %395, <8 x i32> %398, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
  %400 = bitcast <8 x i32> %399 to <4 x i64>
  %401 = shufflevector <8 x i32> %395, <8 x i32> %398, <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
  %402 = bitcast <8 x i32> %401 to <4 x i64>
  %403 = shufflevector <4 x i64> %370, <4 x i64> %380, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %404 = shufflevector <4 x i64> %390, <4 x i64> %400, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %405 = bitcast <4 x i64> %403 to <8 x i32>
  %406 = bitcast <4 x i64> %404 to <8 x i32>
  %407 = shufflevector <8 x i32> %405, <8 x i32> %406, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %408 = bitcast <4 x i64>* %363 to <8 x i32>*
  store <8 x i32> %407, <8 x i32>* %408, align 32
  %409 = shufflevector <8 x i32> %405, <8 x i32> %406, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
  %410 = getelementptr inbounds <4 x i64>, <4 x i64>* %363, i64 4
  %411 = bitcast <4 x i64>* %410 to <8 x i32>*
  store <8 x i32> %409, <8 x i32>* %411, align 32
  %412 = shufflevector <4 x i64> %370, <4 x i64> %380, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %413 = shufflevector <4 x i64> %390, <4 x i64> %400, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %414 = bitcast <4 x i64> %412 to <8 x i32>
  %415 = bitcast <4 x i64> %413 to <8 x i32>
  %416 = shufflevector <8 x i32> %414, <8 x i32> %415, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %417 = getelementptr inbounds <4 x i64>, <4 x i64>* %363, i64 1
  %418 = bitcast <4 x i64>* %417 to <8 x i32>*
  store <8 x i32> %416, <8 x i32>* %418, align 32
  %419 = shufflevector <8 x i32> %414, <8 x i32> %415, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
  %420 = getelementptr inbounds <4 x i64>, <4 x i64>* %363, i64 5
  %421 = bitcast <4 x i64>* %420 to <8 x i32>*
  store <8 x i32> %419, <8 x i32>* %421, align 32
  %422 = shufflevector <4 x i64> %372, <4 x i64> %382, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %423 = shufflevector <4 x i64> %392, <4 x i64> %402, <4 x i32> <i32 0, i32 4, i32 2, i32 6>
  %424 = bitcast <4 x i64> %422 to <8 x i32>
  %425 = bitcast <4 x i64> %423 to <8 x i32>
  %426 = shufflevector <8 x i32> %424, <8 x i32> %425, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %427 = getelementptr inbounds <4 x i64>, <4 x i64>* %363, i64 2
  %428 = bitcast <4 x i64>* %427 to <8 x i32>*
  store <8 x i32> %426, <8 x i32>* %428, align 32
  %429 = shufflevector <8 x i32> %424, <8 x i32> %425, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
  %430 = getelementptr inbounds <4 x i64>, <4 x i64>* %363, i64 6
  %431 = bitcast <4 x i64>* %430 to <8 x i32>*
  store <8 x i32> %429, <8 x i32>* %431, align 32
  %432 = shufflevector <4 x i64> %372, <4 x i64> %382, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %433 = shufflevector <4 x i64> %392, <4 x i64> %402, <4 x i32> <i32 1, i32 5, i32 3, i32 7>
  %434 = bitcast <4 x i64> %432 to <8 x i32>
  %435 = bitcast <4 x i64> %433 to <8 x i32>
  %436 = shufflevector <8 x i32> %434, <8 x i32> %435, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
  %437 = getelementptr inbounds <4 x i64>, <4 x i64>* %363, i64 3
  %438 = bitcast <4 x i64>* %437 to <8 x i32>*
  store <8 x i32> %436, <8 x i32>* %438, align 32
  %439 = shufflevector <8 x i32> %434, <8 x i32> %435, <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
  %440 = getelementptr inbounds <4 x i64>, <4 x i64>* %363, i64 7
  %441 = bitcast <4 x i64>* %440 to <8 x i32>*
  store <8 x i32> %439, <8 x i32>* %441, align 32
  %442 = add nuw nsw i64 %359, 1
  %443 = icmp slt i64 %442, %128
  br i1 %443, label %358, label %444

444:                                              ; preds = %270, %358, %269, %268
  call void @llvm.lifetime.end.p0i8(i64 2048, i8* nonnull %108) #8
  %445 = add nuw nsw i64 %153, 1
  %446 = icmp slt i64 %445, %130
  br i1 %446, label %152, label %132

447:                                              ; preds = %558, %132
  %448 = lshr i64 483100, %16
  %449 = and i64 %448, 1
  %450 = icmp eq i64 %449, 0
  br i1 %450, label %615, label %451

451:                                              ; preds = %447
  %452 = ashr i32 %42, 4
  %453 = shl i32 %44, 1
  %454 = icmp ne i32 %101, 0
  %455 = select i1 %454, i64 -1, i64 1
  %456 = add nsw i32 %44, -1
  %457 = select i1 %454, i32 %456, i32 0
  %458 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>, i32 %6) #8
  %459 = add <16 x i16> %458, <i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1>
  %460 = sext i32 %457 to i64
  %461 = sext i32 %44 to i64
  %462 = sext i32 %2 to i64
  %463 = zext i32 %44 to i64
  %464 = sext i32 %452 to i64
  br label %561

465:                                              ; preds = %558, %135
  %466 = phi i64 [ 0, %135 ], [ %559, %558 ]
  %467 = mul nsw i64 %466, %141
  %468 = getelementptr inbounds [512 x <4 x i64>], [512 x <4 x i64>]* %8, i64 0, i64 %467
  call void %90(<4 x i64>* %468, <4 x i64>* %468, i32 %138, i32 1, i32 %6, i32 0) #8
  %469 = load i8, i8* %139, align 1
  %470 = sext i8 %469 to i32
  %471 = sub nsw i32 0, %470
  %472 = icmp slt i8 %469, 0
  br i1 %472, label %474, label %473

473:                                              ; preds = %465
  br i1 %145, label %546, label %508

474:                                              ; preds = %465
  %475 = xor i32 %470, -1
  %476 = shl i32 1, %475
  %477 = insertelement <8 x i32> undef, i32 %476, i32 0
  %478 = shufflevector <8 x i32> %477, <8 x i32> undef, <8 x i32> zeroinitializer
  br i1 %149, label %533, label %479

479:                                              ; preds = %474, %479
  %480 = phi i64 [ %505, %479 ], [ 0, %474 ]
  %481 = phi i64 [ %506, %479 ], [ %150, %474 ]
  %482 = getelementptr inbounds <4 x i64>, <4 x i64>* %468, i64 %480
  %483 = bitcast <4 x i64>* %482 to <8 x i32>*
  %484 = load <8 x i32>, <8 x i32>* %483, align 32
  %485 = add <8 x i32> %484, %478
  %486 = call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %485, i32 %471) #8
  store <8 x i32> %486, <8 x i32>* %483, align 32
  %487 = or i64 %480, 1
  %488 = getelementptr inbounds <4 x i64>, <4 x i64>* %468, i64 %487
  %489 = bitcast <4 x i64>* %488 to <8 x i32>*
  %490 = load <8 x i32>, <8 x i32>* %489, align 32
  %491 = add <8 x i32> %490, %478
  %492 = call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %491, i32 %471) #8
  store <8 x i32> %492, <8 x i32>* %489, align 32
  %493 = or i64 %480, 2
  %494 = getelementptr inbounds <4 x i64>, <4 x i64>* %468, i64 %493
  %495 = bitcast <4 x i64>* %494 to <8 x i32>*
  %496 = load <8 x i32>, <8 x i32>* %495, align 32
  %497 = add <8 x i32> %496, %478
  %498 = call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %497, i32 %471) #8
  store <8 x i32> %498, <8 x i32>* %495, align 32
  %499 = or i64 %480, 3
  %500 = getelementptr inbounds <4 x i64>, <4 x i64>* %468, i64 %499
  %501 = bitcast <4 x i64>* %500 to <8 x i32>*
  %502 = load <8 x i32>, <8 x i32>* %501, align 32
  %503 = add <8 x i32> %502, %478
  %504 = call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %503, i32 %471) #8
  store <8 x i32> %504, <8 x i32>* %501, align 32
  %505 = add nuw nsw i64 %480, 4
  %506 = add i64 %481, -4
  %507 = icmp eq i64 %506, 0
  br i1 %507, label %533, label %479

508:                                              ; preds = %473, %508
  %509 = phi i64 [ %530, %508 ], [ 0, %473 ]
  %510 = phi i64 [ %531, %508 ], [ %146, %473 ]
  %511 = getelementptr inbounds <4 x i64>, <4 x i64>* %468, i64 %509
  %512 = bitcast <4 x i64>* %511 to <8 x i32>*
  %513 = load <8 x i32>, <8 x i32>* %512, align 32
  %514 = call <8 x i32> @llvm.x86.avx2.pslli.d(<8 x i32> %513, i32 %470) #8
  store <8 x i32> %514, <8 x i32>* %512, align 32
  %515 = or i64 %509, 1
  %516 = getelementptr inbounds <4 x i64>, <4 x i64>* %468, i64 %515
  %517 = bitcast <4 x i64>* %516 to <8 x i32>*
  %518 = load <8 x i32>, <8 x i32>* %517, align 32
  %519 = call <8 x i32> @llvm.x86.avx2.pslli.d(<8 x i32> %518, i32 %470) #8
  store <8 x i32> %519, <8 x i32>* %517, align 32
  %520 = or i64 %509, 2
  %521 = getelementptr inbounds <4 x i64>, <4 x i64>* %468, i64 %520
  %522 = bitcast <4 x i64>* %521 to <8 x i32>*
  %523 = load <8 x i32>, <8 x i32>* %522, align 32
  %524 = call <8 x i32> @llvm.x86.avx2.pslli.d(<8 x i32> %523, i32 %470) #8
  store <8 x i32> %524, <8 x i32>* %522, align 32
  %525 = or i64 %509, 3
  %526 = getelementptr inbounds <4 x i64>, <4 x i64>* %468, i64 %525
  %527 = bitcast <4 x i64>* %526 to <8 x i32>*
  %528 = load <8 x i32>, <8 x i32>* %527, align 32
  %529 = call <8 x i32> @llvm.x86.avx2.pslli.d(<8 x i32> %528, i32 %470) #8
  store <8 x i32> %529, <8 x i32>* %527, align 32
  %530 = add nuw nsw i64 %509, 4
  %531 = add i64 %510, -4
  %532 = icmp eq i64 %531, 0
  br i1 %532, label %546, label %508

533:                                              ; preds = %479, %474
  %534 = phi i64 [ 0, %474 ], [ %505, %479 ]
  br i1 %151, label %558, label %535

535:                                              ; preds = %533, %535
  %536 = phi i64 [ %543, %535 ], [ %534, %533 ]
  %537 = phi i64 [ %544, %535 ], [ %148, %533 ]
  %538 = getelementptr inbounds <4 x i64>, <4 x i64>* %468, i64 %536
  %539 = bitcast <4 x i64>* %538 to <8 x i32>*
  %540 = load <8 x i32>, <8 x i32>* %539, align 32
  %541 = add <8 x i32> %540, %478
  %542 = call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %541, i32 %471) #8
  store <8 x i32> %542, <8 x i32>* %539, align 32
  %543 = add nuw nsw i64 %536, 1
  %544 = add i64 %537, -1
  %545 = icmp eq i64 %544, 0
  br i1 %545, label %558, label %535, !llvm.loop !2

546:                                              ; preds = %508, %473
  %547 = phi i64 [ 0, %473 ], [ %530, %508 ]
  br i1 %147, label %558, label %548

548:                                              ; preds = %546, %548
  %549 = phi i64 [ %555, %548 ], [ %547, %546 ]
  %550 = phi i64 [ %556, %548 ], [ %144, %546 ]
  %551 = getelementptr inbounds <4 x i64>, <4 x i64>* %468, i64 %549
  %552 = bitcast <4 x i64>* %551 to <8 x i32>*
  %553 = load <8 x i32>, <8 x i32>* %552, align 32
  %554 = call <8 x i32> @llvm.x86.avx2.pslli.d(<8 x i32> %553, i32 %470) #8
  store <8 x i32> %554, <8 x i32>* %552, align 32
  %555 = add nuw nsw i64 %549, 1
  %556 = add i64 %550, -1
  %557 = icmp eq i64 %556, 0
  br i1 %557, label %558, label %548, !llvm.loop !4

558:                                              ; preds = %546, %548, %533, %535
  %559 = add nuw nsw i64 %466, 1
  %560 = icmp slt i64 %559, %142
  br i1 %560, label %465, label %447

561:                                              ; preds = %612, %451
  %562 = phi i64 [ 0, %451 ], [ %613, %612 ]
  %563 = trunc i64 %562 to i32
  %564 = mul i32 %453, %563
  %565 = sext i32 %564 to i64
  %566 = getelementptr inbounds [512 x <4 x i64>], [512 x <4 x i64>]* %8, i64 0, i64 %565
  %567 = shl nsw i64 %562, 4
  %568 = getelementptr inbounds i16, i16* %13, i64 %567
  br label %569

569:                                              ; preds = %569, %561
  %570 = phi i64 [ 0, %561 ], [ %609, %569 ]
  %571 = phi i64 [ %460, %561 ], [ %610, %569 ]
  %572 = mul nsw i64 %570, %462
  %573 = getelementptr inbounds i16, i16* %568, i64 %572
  %574 = bitcast i16* %573 to <4 x i64>*
  %575 = load <4 x i64>, <4 x i64>* %574, align 2
  %576 = getelementptr inbounds <4 x i64>, <4 x i64>* %566, i64 %571
  %577 = bitcast <4 x i64>* %576 to <8 x i32>*
  %578 = load <8 x i32>, <8 x i32>* %577, align 32
  %579 = add nsw i64 %571, %461
  %580 = getelementptr inbounds <4 x i64>, <4 x i64>* %566, i64 %579
  %581 = bitcast <4 x i64>* %580 to <8 x i32>*
  %582 = load <8 x i32>, <8 x i32>* %581, align 32
  %583 = shufflevector <4 x i64> %575, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  %584 = bitcast <2 x i64> %583 to <8 x i16>
  %585 = sext <8 x i16> %584 to <8 x i32>
  %586 = bitcast <4 x i64> %575 to <8 x i32>
  %587 = shufflevector <8 x i32> %586, <8 x i32> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %588 = bitcast <4 x i32> %587 to <8 x i16>
  %589 = sext <8 x i16> %588 to <8 x i32>
  %590 = add <8 x i32> %578, %585
  %591 = add <8 x i32> %582, %589
  %592 = call <16 x i16> @llvm.x86.avx2.packusdw(<8 x i32> %590, <8 x i32> %591) #8
  %593 = bitcast <16 x i16> %592 to <4 x i64>
  %594 = shufflevector <4 x i64> %593, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %595 = bitcast <4 x i64> %594 to <16 x i16>
  %596 = icmp slt <16 x i16> %459, %595
  %597 = sext <16 x i1> %596 to <16 x i16>
  %598 = bitcast <16 x i16> %597 to <4 x i64>
  %599 = xor <4 x i64> %598, <i64 -1, i64 -1, i64 -1, i64 -1>
  %600 = and <4 x i64> %594, %599
  %601 = and <16 x i16> %459, %597
  %602 = bitcast <16 x i16> %601 to <4 x i64>
  %603 = or <4 x i64> %600, %602
  %604 = bitcast <4 x i64> %603 to <16 x i16>
  %605 = icmp sgt <16 x i16> %604, zeroinitializer
  %606 = sext <16 x i1> %605 to <16 x i16>
  %607 = bitcast <16 x i16> %606 to <4 x i64>
  %608 = and <4 x i64> %603, %607
  store <4 x i64> %608, <4 x i64>* %574, align 2
  %609 = add nuw nsw i64 %570, 1
  %610 = add i64 %571, %455
  %611 = icmp eq i64 %609, %463
  br i1 %611, label %612, label %569

612:                                              ; preds = %569
  %613 = add nuw nsw i64 %562, 1
  %614 = icmp slt i64 %613, %464
  br i1 %614, label %561, label %663

615:                                              ; preds = %447
  %616 = lshr i64 32962, %16
  %617 = and i64 %616, 1
  %618 = icmp eq i64 %617, 0
  br i1 %618, label %663, label %619

619:                                              ; preds = %615
  %620 = icmp ne i32 %101, 0
  %621 = select i1 %620, i64 -1, i64 1
  %622 = add nsw i32 %44, -1
  %623 = select i1 %620, i32 %622, i32 0
  %624 = call <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>, i32 %6) #8
  %625 = add <16 x i16> %624, <i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1>
  %626 = sext i32 %623 to i64
  %627 = sext i32 %2 to i64
  %628 = zext i32 %44 to i64
  br label %629

629:                                              ; preds = %629, %619
  %630 = phi i64 [ 0, %619 ], [ %660, %629 ]
  %631 = phi i64 [ %626, %619 ], [ %661, %629 ]
  %632 = mul nsw i64 %630, %627
  %633 = getelementptr inbounds i16, i16* %13, i64 %632
  %634 = bitcast i16* %633 to <2 x i64>*
  %635 = bitcast i16* %633 to <8 x i16>*
  %636 = load <8 x i16>, <8 x i16>* %635, align 2
  %637 = sext <8 x i16> %636 to <8 x i32>
  %638 = getelementptr inbounds [512 x <4 x i64>], [512 x <4 x i64>]* %8, i64 0, i64 %631
  %639 = bitcast <4 x i64>* %638 to <8 x i32>*
  %640 = load <8 x i32>, <8 x i32>* %639, align 32
  %641 = add <8 x i32> %640, %637
  %642 = call <16 x i16> @llvm.x86.avx2.packusdw(<8 x i32> %641, <8 x i32> %641) #8
  %643 = bitcast <16 x i16> %642 to <4 x i64>
  %644 = shufflevector <4 x i64> %643, <4 x i64> undef, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  %645 = bitcast <4 x i64> %644 to <16 x i16>
  %646 = icmp slt <16 x i16> %625, %645
  %647 = sext <16 x i1> %646 to <16 x i16>
  %648 = bitcast <16 x i16> %647 to <4 x i64>
  %649 = xor <4 x i64> %648, <i64 -1, i64 -1, i64 -1, i64 -1>
  %650 = and <4 x i64> %644, %649
  %651 = and <16 x i16> %625, %647
  %652 = bitcast <16 x i16> %651 to <4 x i64>
  %653 = or <4 x i64> %650, %652
  %654 = bitcast <4 x i64> %653 to <16 x i16>
  %655 = icmp sgt <16 x i16> %654, zeroinitializer
  %656 = sext <16 x i1> %655 to <16 x i16>
  %657 = bitcast <16 x i16> %656 to <4 x i64>
  %658 = and <4 x i64> %653, %657
  %659 = shufflevector <4 x i64> %658, <4 x i64> undef, <2 x i32> <i32 0, i32 1>
  store <2 x i64> %659, <2 x i64>* %634, align 2
  %660 = add nuw nsw i64 %630, 1
  %661 = add i64 %631, %621
  %662 = icmp eq i64 %660, %628
  br i1 %662, label %663, label %629

663:                                              ; preds = %612, %629, %615
  call void @llvm.lifetime.end.p0i8(i64 16384, i8* nonnull %14) #8
  br label %665

664:                                              ; preds = %7, %7, %7, %7, %7, %7, %7
  tail call void @av1_highbd_inv_txfm2d_add_universe_sse4_1(i32* %0, i8* %1, i32 %2, i8 zeroext %3, i8 zeroext %4, i32 %5, i32 %6) #8
  br label %665

665:                                              ; preds = %7, %664, %663
  ret void
}

declare void @av1_highbd_inv_txfm2d_add_universe_sse4_1(i32*, i8*, i32, i8 zeroext, i8 zeroext, i32, i32) local_unnamed_addr #1

; Function Attrs: nounwind ssp uwtable
define hidden void @av1_highbd_inv_txfm_add_avx2(i32*, i8*, i32, %struct.txfm_param*) local_unnamed_addr #2 {
  %5 = getelementptr inbounds %struct.txfm_param, %struct.txfm_param* %3, i64 0, i32 1
  %6 = load i8, i8* %5, align 1
  switch i8 %6, label %12 [
    i8 5, label %7
    i8 6, label %8
    i8 0, label %9
    i8 14, label %10
    i8 13, label %11
  ]

7:                                                ; preds = %4
  tail call void @av1_highbd_inv_txfm_add_4x8_sse4_1(i32* %0, i8* %1, i32 %2, %struct.txfm_param* %3) #8
  br label %19

8:                                                ; preds = %4
  tail call void @av1_highbd_inv_txfm_add_8x4_sse4_1(i32* %0, i8* %1, i32 %2, %struct.txfm_param* %3) #8
  br label %19

9:                                                ; preds = %4
  tail call void @av1_highbd_inv_txfm_add_4x4_sse4_1(i32* %0, i8* %1, i32 %2, %struct.txfm_param* %3) #8
  br label %19

10:                                               ; preds = %4
  tail call void @av1_highbd_inv_txfm_add_16x4_sse4_1(i32* %0, i8* %1, i32 %2, %struct.txfm_param* %3) #8
  br label %19

11:                                               ; preds = %4
  tail call void @av1_highbd_inv_txfm_add_4x16_sse4_1(i32* %0, i8* %1, i32 %2, %struct.txfm_param* %3) #8
  br label %19

12:                                               ; preds = %4
  %13 = getelementptr inbounds %struct.txfm_param, %struct.txfm_param* %3, i64 0, i32 0
  %14 = load i8, i8* %13, align 4
  %15 = getelementptr inbounds %struct.txfm_param, %struct.txfm_param* %3, i64 0, i32 6
  %16 = load i32, i32* %15, align 4
  %17 = getelementptr inbounds %struct.txfm_param, %struct.txfm_param* %3, i64 0, i32 3
  %18 = load i32, i32* %17, align 4
  tail call void @av1_highbd_inv_txfm2d_add_universe_avx2(i32* %0, i8* %1, i32 %2, i8 zeroext %14, i8 zeroext %6, i32 %16, i32 %18)
  br label %19

19:                                               ; preds = %12, %11, %10, %9, %8, %7
  ret void
}

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #3

declare void @av1_highbd_inv_txfm_add_4x8_sse4_1(i32*, i8*, i32, %struct.txfm_param*) local_unnamed_addr #1

declare void @av1_highbd_inv_txfm_add_8x4_sse4_1(i32*, i8*, i32, %struct.txfm_param*) local_unnamed_addr #1

declare void @av1_highbd_inv_txfm_add_4x4_sse4_1(i32*, i8*, i32, %struct.txfm_param*) local_unnamed_addr #1

declare void @av1_highbd_inv_txfm_add_16x4_sse4_1(i32*, i8*, i32, %struct.txfm_param*) local_unnamed_addr #1

declare void @av1_highbd_inv_txfm_add_4x16_sse4_1(i32*, i8*, i32, %struct.txfm_param*) local_unnamed_addr #1

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #3

; Function Attrs: argmemonly nounwind
declare void @llvm.memset.p0i8.i64(i8* nocapture writeonly, i8, i64, i1 immarg) #3

; Function Attrs: nofree nounwind ssp uwtable
define internal void @idct8x8_low1_avx2(<4 x i64>* nocapture readonly, <4 x i64>* nocapture, i32, i32, i32, i32) #4 {
  %7 = add nsw i32 %2, -10
  %8 = sext i32 %7 to i64
  %9 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 32
  %10 = load i32, i32* %9, align 16
  %11 = insertelement <8 x i32> undef, i32 %10, i32 0
  %12 = shufflevector <8 x i32> %11, <8 x i32> undef, <8 x i32> zeroinitializer
  %13 = add nsw i32 %2, -1
  %14 = shl i32 1, %13
  %15 = insertelement <8 x i32> undef, i32 %14, i32 0
  %16 = shufflevector <8 x i32> %15, <8 x i32> undef, <8 x i32> zeroinitializer
  %17 = icmp ne i32 %3, 0
  %18 = select i1 %17, i32 6, i32 8
  %19 = add nsw i32 %18, %4
  %20 = icmp slt i32 %19, 16
  %21 = add i32 %19, -1
  %22 = shl i32 1, %21
  %23 = select i1 %20, i32 32768, i32 %22
  %24 = sub nsw i32 0, %23
  %25 = insertelement <8 x i32> undef, i32 %24, i32 0
  %26 = shufflevector <8 x i32> %25, <8 x i32> undef, <8 x i32> zeroinitializer
  %27 = add nsw i32 %23, -1
  %28 = insertelement <8 x i32> undef, i32 %27, i32 0
  %29 = shufflevector <8 x i32> %28, <8 x i32> undef, <8 x i32> zeroinitializer
  %30 = bitcast <4 x i64>* %0 to <8 x i32>*
  %31 = load <8 x i32>, <8 x i32>* %30, align 32
  %32 = mul <8 x i32> %12, %31
  %33 = add <8 x i32> %32, %16
  %34 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %33, i32 %2) #8
  br i1 %17, label %52, label %35

35:                                               ; preds = %6
  %36 = icmp sgt i32 %4, 10
  %37 = select i1 %36, i32 %4, i32 10
  %38 = shl i32 1, %5
  %39 = ashr i32 %38, 1
  %40 = insertelement <8 x i32> undef, i32 %39, i32 0
  %41 = shufflevector <8 x i32> %40, <8 x i32> undef, <8 x i32> zeroinitializer
  %42 = shl i32 32, %37
  %43 = sub nsw i32 0, %42
  %44 = insertelement <8 x i32> undef, i32 %43, i32 0
  %45 = shufflevector <8 x i32> %44, <8 x i32> undef, <8 x i32> zeroinitializer
  %46 = add nsw i32 %42, -1
  %47 = insertelement <8 x i32> undef, i32 %46, i32 0
  %48 = shufflevector <8 x i32> %47, <8 x i32> undef, <8 x i32> zeroinitializer
  %49 = add <8 x i32> %34, %41
  %50 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %5, i32 0
  %51 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %49, <4 x i32> %50) #8
  br label %52

52:                                               ; preds = %35, %6
  %53 = phi <8 x i32> [ %26, %6 ], [ %45, %35 ]
  %54 = phi <8 x i32> [ %29, %6 ], [ %48, %35 ]
  %55 = phi <8 x i32> [ %34, %6 ], [ %51, %35 ]
  %56 = icmp sgt <8 x i32> %55, %53
  %57 = select <8 x i1> %56, <8 x i32> %55, <8 x i32> %53
  %58 = icmp slt <8 x i32> %57, %54
  %59 = select <8 x i1> %58, <8 x i32> %57, <8 x i32> %54
  %60 = bitcast <4 x i64>* %1 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %60, align 32
  %61 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %62 = bitcast <4 x i64>* %61 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %62, align 32
  %63 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %64 = bitcast <4 x i64>* %63 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %64, align 32
  %65 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %66 = bitcast <4 x i64>* %65 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %66, align 32
  %67 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %68 = bitcast <4 x i64>* %67 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %68, align 32
  %69 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %70 = bitcast <4 x i64>* %69 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %70, align 32
  %71 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %72 = bitcast <4 x i64>* %71 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %72, align 32
  %73 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %74 = bitcast <4 x i64>* %73 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %74, align 32
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @idct8x8_avx2(<4 x i64>* nocapture readonly, <4 x i64>* nocapture, i32, i32, i32, i32) #0 {
  %7 = add nsw i32 %2, -10
  %8 = sext i32 %7 to i64
  %9 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 56
  %10 = load i32, i32* %9, align 16
  %11 = insertelement <8 x i32> undef, i32 %10, i32 0
  %12 = shufflevector <8 x i32> %11, <8 x i32> undef, <8 x i32> zeroinitializer
  %13 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 8
  %14 = load i32, i32* %13, align 16
  %15 = sub nsw i32 0, %14
  %16 = insertelement <8 x i32> undef, i32 %15, i32 0
  %17 = shufflevector <8 x i32> %16, <8 x i32> undef, <8 x i32> zeroinitializer
  %18 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 24
  %19 = load i32, i32* %18, align 16
  %20 = insertelement <8 x i32> undef, i32 %19, i32 0
  %21 = shufflevector <8 x i32> %20, <8 x i32> undef, <8 x i32> zeroinitializer
  %22 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 40
  %23 = load i32, i32* %22, align 16
  %24 = sub nsw i32 0, %23
  %25 = insertelement <8 x i32> undef, i32 %24, i32 0
  %26 = shufflevector <8 x i32> %25, <8 x i32> undef, <8 x i32> zeroinitializer
  %27 = insertelement <8 x i32> undef, i32 %23, i32 0
  %28 = shufflevector <8 x i32> %27, <8 x i32> undef, <8 x i32> zeroinitializer
  %29 = insertelement <8 x i32> undef, i32 %14, i32 0
  %30 = shufflevector <8 x i32> %29, <8 x i32> undef, <8 x i32> zeroinitializer
  %31 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 32
  %32 = load i32, i32* %31, align 16
  %33 = insertelement <8 x i32> undef, i32 %32, i32 0
  %34 = shufflevector <8 x i32> %33, <8 x i32> undef, <8 x i32> zeroinitializer
  %35 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 48
  %36 = load i32, i32* %35, align 16
  %37 = insertelement <8 x i32> undef, i32 %36, i32 0
  %38 = shufflevector <8 x i32> %37, <8 x i32> undef, <8 x i32> zeroinitializer
  %39 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 16
  %40 = load i32, i32* %39, align 16
  %41 = sub nsw i32 0, %40
  %42 = insertelement <8 x i32> undef, i32 %41, i32 0
  %43 = shufflevector <8 x i32> %42, <8 x i32> undef, <8 x i32> zeroinitializer
  %44 = insertelement <8 x i32> undef, i32 %40, i32 0
  %45 = shufflevector <8 x i32> %44, <8 x i32> undef, <8 x i32> zeroinitializer
  %46 = add nsw i32 %2, -1
  %47 = shl i32 1, %46
  %48 = insertelement <8 x i32> undef, i32 %47, i32 0
  %49 = shufflevector <8 x i32> %48, <8 x i32> undef, <8 x i32> zeroinitializer
  %50 = icmp ne i32 %3, 0
  %51 = select i1 %50, i32 6, i32 8
  %52 = add nsw i32 %51, %4
  %53 = icmp slt i32 %52, 16
  %54 = add i32 %52, -1
  %55 = shl i32 1, %54
  %56 = select i1 %53, i32 32768, i32 %55
  %57 = sub nsw i32 0, %56
  %58 = insertelement <8 x i32> undef, i32 %57, i32 0
  %59 = shufflevector <8 x i32> %58, <8 x i32> undef, <8 x i32> zeroinitializer
  %60 = add nsw i32 %56, -1
  %61 = insertelement <8 x i32> undef, i32 %60, i32 0
  %62 = shufflevector <8 x i32> %61, <8 x i32> undef, <8 x i32> zeroinitializer
  %63 = bitcast <4 x i64>* %0 to <8 x i32>*
  %64 = load <8 x i32>, <8 x i32>* %63, align 32
  %65 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 4
  %66 = bitcast <4 x i64>* %65 to <8 x i32>*
  %67 = load <8 x i32>, <8 x i32>* %66, align 32
  %68 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 2
  %69 = bitcast <4 x i64>* %68 to <8 x i32>*
  %70 = load <8 x i32>, <8 x i32>* %69, align 32
  %71 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 6
  %72 = bitcast <4 x i64>* %71 to <8 x i32>*
  %73 = load <8 x i32>, <8 x i32>* %72, align 32
  %74 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 1
  %75 = bitcast <4 x i64>* %74 to <8 x i32>*
  %76 = load <8 x i32>, <8 x i32>* %75, align 32
  %77 = mul <8 x i32> %76, %12
  %78 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 7
  %79 = bitcast <4 x i64>* %78 to <8 x i32>*
  %80 = load <8 x i32>, <8 x i32>* %79, align 32
  %81 = mul <8 x i32> %80, %17
  %82 = add <8 x i32> %77, %49
  %83 = add <8 x i32> %82, %81
  %84 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %83, i32 %2) #8
  %85 = mul <8 x i32> %76, %30
  %86 = mul <8 x i32> %80, %12
  %87 = add <8 x i32> %85, %49
  %88 = add <8 x i32> %87, %86
  %89 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %88, i32 %2) #8
  %90 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 5
  %91 = bitcast <4 x i64>* %90 to <8 x i32>*
  %92 = load <8 x i32>, <8 x i32>* %91, align 32
  %93 = mul <8 x i32> %92, %21
  %94 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 3
  %95 = bitcast <4 x i64>* %94 to <8 x i32>*
  %96 = load <8 x i32>, <8 x i32>* %95, align 32
  %97 = mul <8 x i32> %96, %26
  %98 = add <8 x i32> %93, %49
  %99 = add <8 x i32> %98, %97
  %100 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %99, i32 %2) #8
  %101 = mul <8 x i32> %92, %28
  %102 = mul <8 x i32> %96, %21
  %103 = add <8 x i32> %101, %49
  %104 = add <8 x i32> %103, %102
  %105 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %104, i32 %2) #8
  %106 = mul <8 x i32> %64, %34
  %107 = mul <8 x i32> %67, %34
  %108 = add <8 x i32> %106, %49
  %109 = add <8 x i32> %108, %107
  %110 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %109, i32 %2) #8
  %111 = sub <8 x i32> %108, %107
  %112 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %111, i32 %2) #8
  %113 = mul <8 x i32> %70, %38
  %114 = mul <8 x i32> %73, %43
  %115 = add <8 x i32> %113, %49
  %116 = add <8 x i32> %115, %114
  %117 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %116, i32 %2) #8
  %118 = mul <8 x i32> %70, %45
  %119 = mul <8 x i32> %73, %38
  %120 = add <8 x i32> %118, %49
  %121 = add <8 x i32> %120, %119
  %122 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %121, i32 %2) #8
  %123 = add <8 x i32> %100, %84
  %124 = sub <8 x i32> %84, %100
  %125 = icmp sgt <8 x i32> %123, %59
  %126 = select <8 x i1> %125, <8 x i32> %123, <8 x i32> %59
  %127 = icmp slt <8 x i32> %126, %62
  %128 = select <8 x i1> %127, <8 x i32> %126, <8 x i32> %62
  %129 = icmp sgt <8 x i32> %124, %59
  %130 = select <8 x i1> %129, <8 x i32> %124, <8 x i32> %59
  %131 = icmp slt <8 x i32> %130, %62
  %132 = select <8 x i1> %131, <8 x i32> %130, <8 x i32> %62
  %133 = add <8 x i32> %105, %89
  %134 = sub <8 x i32> %89, %105
  %135 = icmp sgt <8 x i32> %133, %59
  %136 = select <8 x i1> %135, <8 x i32> %133, <8 x i32> %59
  %137 = icmp slt <8 x i32> %136, %62
  %138 = select <8 x i1> %137, <8 x i32> %136, <8 x i32> %62
  %139 = icmp sgt <8 x i32> %134, %59
  %140 = select <8 x i1> %139, <8 x i32> %134, <8 x i32> %59
  %141 = icmp slt <8 x i32> %140, %62
  %142 = select <8 x i1> %141, <8 x i32> %140, <8 x i32> %62
  %143 = add <8 x i32> %122, %110
  %144 = sub <8 x i32> %110, %122
  %145 = icmp sgt <8 x i32> %143, %59
  %146 = select <8 x i1> %145, <8 x i32> %143, <8 x i32> %59
  %147 = icmp slt <8 x i32> %146, %62
  %148 = select <8 x i1> %147, <8 x i32> %146, <8 x i32> %62
  %149 = icmp sgt <8 x i32> %144, %59
  %150 = select <8 x i1> %149, <8 x i32> %144, <8 x i32> %59
  %151 = icmp slt <8 x i32> %150, %62
  %152 = select <8 x i1> %151, <8 x i32> %150, <8 x i32> %62
  %153 = add <8 x i32> %117, %112
  %154 = sub <8 x i32> %112, %117
  %155 = icmp sgt <8 x i32> %153, %59
  %156 = select <8 x i1> %155, <8 x i32> %153, <8 x i32> %59
  %157 = icmp slt <8 x i32> %156, %62
  %158 = select <8 x i1> %157, <8 x i32> %156, <8 x i32> %62
  %159 = icmp sgt <8 x i32> %154, %59
  %160 = select <8 x i1> %159, <8 x i32> %154, <8 x i32> %59
  %161 = icmp slt <8 x i32> %160, %62
  %162 = select <8 x i1> %161, <8 x i32> %160, <8 x i32> %62
  %163 = mul <8 x i32> %132, %34
  %164 = mul <8 x i32> %142, %34
  %165 = add <8 x i32> %163, %49
  %166 = add <8 x i32> %165, %164
  %167 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %166, i32 %2) #8
  %168 = sub <8 x i32> %49, %163
  %169 = add <8 x i32> %168, %164
  %170 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %169, i32 %2) #8
  %171 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %172 = add <8 x i32> %148, %138
  %173 = sub <8 x i32> %148, %138
  %174 = icmp sgt <8 x i32> %172, %59
  %175 = select <8 x i1> %174, <8 x i32> %172, <8 x i32> %59
  %176 = icmp slt <8 x i32> %175, %62
  %177 = select <8 x i1> %176, <8 x i32> %175, <8 x i32> %62
  %178 = icmp sgt <8 x i32> %173, %59
  %179 = select <8 x i1> %178, <8 x i32> %173, <8 x i32> %59
  %180 = icmp slt <8 x i32> %179, %62
  %181 = select <8 x i1> %180, <8 x i32> %179, <8 x i32> %62
  %182 = bitcast <4 x i64>* %1 to <8 x i32>*
  store <8 x i32> %177, <8 x i32>* %182, align 32
  %183 = bitcast <4 x i64>* %171 to <8 x i32>*
  store <8 x i32> %181, <8 x i32>* %183, align 32
  %184 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %185 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %186 = add <8 x i32> %158, %167
  %187 = sub <8 x i32> %158, %167
  %188 = icmp sgt <8 x i32> %186, %59
  %189 = select <8 x i1> %188, <8 x i32> %186, <8 x i32> %59
  %190 = icmp slt <8 x i32> %189, %62
  %191 = select <8 x i1> %190, <8 x i32> %189, <8 x i32> %62
  %192 = icmp sgt <8 x i32> %187, %59
  %193 = select <8 x i1> %192, <8 x i32> %187, <8 x i32> %59
  %194 = icmp slt <8 x i32> %193, %62
  %195 = select <8 x i1> %194, <8 x i32> %193, <8 x i32> %62
  %196 = bitcast <4 x i64>* %184 to <8 x i32>*
  store <8 x i32> %191, <8 x i32>* %196, align 32
  %197 = bitcast <4 x i64>* %185 to <8 x i32>*
  store <8 x i32> %195, <8 x i32>* %197, align 32
  %198 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %199 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %200 = add <8 x i32> %162, %170
  %201 = sub <8 x i32> %162, %170
  %202 = icmp sgt <8 x i32> %200, %59
  %203 = select <8 x i1> %202, <8 x i32> %200, <8 x i32> %59
  %204 = icmp slt <8 x i32> %203, %62
  %205 = select <8 x i1> %204, <8 x i32> %203, <8 x i32> %62
  %206 = icmp sgt <8 x i32> %201, %59
  %207 = select <8 x i1> %206, <8 x i32> %201, <8 x i32> %59
  %208 = icmp slt <8 x i32> %207, %62
  %209 = select <8 x i1> %208, <8 x i32> %207, <8 x i32> %62
  %210 = bitcast <4 x i64>* %198 to <8 x i32>*
  store <8 x i32> %205, <8 x i32>* %210, align 32
  %211 = bitcast <4 x i64>* %199 to <8 x i32>*
  store <8 x i32> %209, <8 x i32>* %211, align 32
  %212 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %213 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %214 = add <8 x i32> %152, %128
  %215 = sub <8 x i32> %152, %128
  %216 = icmp sgt <8 x i32> %214, %59
  %217 = select <8 x i1> %216, <8 x i32> %214, <8 x i32> %59
  %218 = icmp slt <8 x i32> %217, %62
  %219 = select <8 x i1> %218, <8 x i32> %217, <8 x i32> %62
  %220 = icmp sgt <8 x i32> %215, %59
  %221 = select <8 x i1> %220, <8 x i32> %215, <8 x i32> %59
  %222 = icmp slt <8 x i32> %221, %62
  %223 = select <8 x i1> %222, <8 x i32> %221, <8 x i32> %62
  %224 = bitcast <4 x i64>* %212 to <8 x i32>*
  store <8 x i32> %219, <8 x i32>* %224, align 32
  %225 = bitcast <4 x i64>* %213 to <8 x i32>*
  store <8 x i32> %223, <8 x i32>* %225, align 32
  br i1 %50, label %299, label %226

226:                                              ; preds = %6
  %227 = icmp sgt i32 %4, 10
  %228 = select i1 %227, i32 %4, i32 10
  %229 = shl i32 32, %228
  %230 = sub nsw i32 0, %229
  %231 = insertelement <8 x i32> undef, i32 %230, i32 0
  %232 = shufflevector <8 x i32> %231, <8 x i32> undef, <8 x i32> zeroinitializer
  %233 = add nsw i32 %229, -1
  %234 = insertelement <8 x i32> undef, i32 %233, i32 0
  %235 = shufflevector <8 x i32> %234, <8 x i32> undef, <8 x i32> zeroinitializer
  %236 = icmp eq i32 %5, 0
  br i1 %236, label %258, label %237

237:                                              ; preds = %226
  %238 = add nsw i32 %5, -1
  %239 = shl i32 1, %238
  %240 = insertelement <8 x i32> undef, i32 %239, i32 0
  %241 = shufflevector <8 x i32> %240, <8 x i32> undef, <8 x i32> zeroinitializer
  %242 = add <8 x i32> %177, %241
  %243 = add <8 x i32> %191, %241
  %244 = add <8 x i32> %205, %241
  %245 = add <8 x i32> %219, %241
  %246 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %242, i32 %5) #8
  store <8 x i32> %246, <8 x i32>* %182, align 32
  %247 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %243, i32 %5) #8
  store <8 x i32> %247, <8 x i32>* %196, align 32
  %248 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %244, i32 %5) #8
  store <8 x i32> %248, <8 x i32>* %210, align 32
  %249 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %245, i32 %5) #8
  store <8 x i32> %249, <8 x i32>* %224, align 32
  %250 = add <8 x i32> %223, %241
  %251 = add <8 x i32> %209, %241
  %252 = add <8 x i32> %195, %241
  %253 = add <8 x i32> %181, %241
  %254 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %250, i32 %5) #8
  store <8 x i32> %254, <8 x i32>* %225, align 32
  %255 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %251, i32 %5) #8
  store <8 x i32> %255, <8 x i32>* %211, align 32
  %256 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %252, i32 %5) #8
  store <8 x i32> %256, <8 x i32>* %197, align 32
  %257 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %253, i32 %5) #8
  store <8 x i32> %257, <8 x i32>* %183, align 32
  br label %258

258:                                              ; preds = %226, %237
  %259 = phi <8 x i32> [ %181, %226 ], [ %257, %237 ]
  %260 = phi <8 x i32> [ %195, %226 ], [ %256, %237 ]
  %261 = phi <8 x i32> [ %209, %226 ], [ %255, %237 ]
  %262 = phi <8 x i32> [ %223, %226 ], [ %254, %237 ]
  %263 = phi <8 x i32> [ %219, %226 ], [ %249, %237 ]
  %264 = phi <8 x i32> [ %205, %226 ], [ %248, %237 ]
  %265 = phi <8 x i32> [ %191, %226 ], [ %247, %237 ]
  %266 = phi <8 x i32> [ %177, %226 ], [ %246, %237 ]
  %267 = icmp sgt <8 x i32> %266, %232
  %268 = select <8 x i1> %267, <8 x i32> %266, <8 x i32> %232
  %269 = icmp slt <8 x i32> %268, %235
  %270 = select <8 x i1> %269, <8 x i32> %268, <8 x i32> %235
  store <8 x i32> %270, <8 x i32>* %182, align 32
  %271 = icmp sgt <8 x i32> %265, %232
  %272 = select <8 x i1> %271, <8 x i32> %265, <8 x i32> %232
  %273 = icmp slt <8 x i32> %272, %235
  %274 = select <8 x i1> %273, <8 x i32> %272, <8 x i32> %235
  store <8 x i32> %274, <8 x i32>* %196, align 32
  %275 = icmp sgt <8 x i32> %264, %232
  %276 = select <8 x i1> %275, <8 x i32> %264, <8 x i32> %232
  %277 = icmp slt <8 x i32> %276, %235
  %278 = select <8 x i1> %277, <8 x i32> %276, <8 x i32> %235
  store <8 x i32> %278, <8 x i32>* %210, align 32
  %279 = icmp sgt <8 x i32> %263, %232
  %280 = select <8 x i1> %279, <8 x i32> %263, <8 x i32> %232
  %281 = icmp slt <8 x i32> %280, %235
  %282 = select <8 x i1> %281, <8 x i32> %280, <8 x i32> %235
  store <8 x i32> %282, <8 x i32>* %224, align 32
  %283 = icmp sgt <8 x i32> %262, %232
  %284 = select <8 x i1> %283, <8 x i32> %262, <8 x i32> %232
  %285 = icmp slt <8 x i32> %284, %235
  %286 = select <8 x i1> %285, <8 x i32> %284, <8 x i32> %235
  store <8 x i32> %286, <8 x i32>* %225, align 32
  %287 = icmp sgt <8 x i32> %261, %232
  %288 = select <8 x i1> %287, <8 x i32> %261, <8 x i32> %232
  %289 = icmp slt <8 x i32> %288, %235
  %290 = select <8 x i1> %289, <8 x i32> %288, <8 x i32> %235
  store <8 x i32> %290, <8 x i32>* %211, align 32
  %291 = icmp sgt <8 x i32> %260, %232
  %292 = select <8 x i1> %291, <8 x i32> %260, <8 x i32> %232
  %293 = icmp slt <8 x i32> %292, %235
  %294 = select <8 x i1> %293, <8 x i32> %292, <8 x i32> %235
  store <8 x i32> %294, <8 x i32>* %197, align 32
  %295 = icmp sgt <8 x i32> %259, %232
  %296 = select <8 x i1> %295, <8 x i32> %259, <8 x i32> %232
  %297 = icmp slt <8 x i32> %296, %235
  %298 = select <8 x i1> %297, <8 x i32> %296, <8 x i32> %235
  store <8 x i32> %298, <8 x i32>* %183, align 32
  br label %299

299:                                              ; preds = %258, %6
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @iadst8x8_low1_avx2(<4 x i64>* nocapture readonly, <4 x i64>* nocapture, i32, i32, i32, i32) #0 {
  %7 = add nsw i32 %2, -10
  %8 = sext i32 %7 to i64
  %9 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 4
  %10 = load i32, i32* %9, align 16
  %11 = insertelement <8 x i32> undef, i32 %10, i32 0
  %12 = shufflevector <8 x i32> %11, <8 x i32> undef, <8 x i32> zeroinitializer
  %13 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 60
  %14 = load i32, i32* %13, align 16
  %15 = insertelement <8 x i32> undef, i32 %14, i32 0
  %16 = shufflevector <8 x i32> %15, <8 x i32> undef, <8 x i32> zeroinitializer
  %17 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 16
  %18 = load i32, i32* %17, align 16
  %19 = insertelement <8 x i32> undef, i32 %18, i32 0
  %20 = shufflevector <8 x i32> %19, <8 x i32> undef, <8 x i32> zeroinitializer
  %21 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 48
  %22 = load i32, i32* %21, align 16
  %23 = insertelement <8 x i32> undef, i32 %22, i32 0
  %24 = shufflevector <8 x i32> %23, <8 x i32> undef, <8 x i32> zeroinitializer
  %25 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 32
  %26 = load i32, i32* %25, align 16
  %27 = insertelement <8 x i32> undef, i32 %26, i32 0
  %28 = shufflevector <8 x i32> %27, <8 x i32> undef, <8 x i32> zeroinitializer
  %29 = add nsw i32 %2, -1
  %30 = shl i32 1, %29
  %31 = insertelement <8 x i32> undef, i32 %30, i32 0
  %32 = shufflevector <8 x i32> %31, <8 x i32> undef, <8 x i32> zeroinitializer
  %33 = bitcast <4 x i64>* %0 to <8 x i32>*
  %34 = load <8 x i32>, <8 x i32>* %33, align 32
  %35 = mul <8 x i32> %34, %16
  %36 = add <8 x i32> %35, %32
  %37 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %36, i32 %2) #8
  %38 = mul <8 x i32> %34, %12
  %39 = sub <8 x i32> %32, %38
  %40 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %39, i32 %2) #8
  %41 = mul <8 x i32> %37, %20
  %42 = mul <8 x i32> %40, %24
  %43 = add <8 x i32> %41, %32
  %44 = add <8 x i32> %43, %42
  %45 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %44, i32 %2) #8
  %46 = mul <8 x i32> %37, %24
  %47 = mul <8 x i32> %20, %40
  %48 = add <8 x i32> %46, %32
  %49 = sub <8 x i32> %48, %47
  %50 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %49, i32 %2) #8
  %51 = mul <8 x i32> %28, %37
  %52 = mul <8 x i32> %40, %28
  %53 = add <8 x i32> %51, %32
  %54 = add <8 x i32> %53, %52
  %55 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %54, i32 %2) #8
  %56 = sub <8 x i32> %53, %52
  %57 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %56, i32 %2) #8
  %58 = mul <8 x i32> %45, %28
  %59 = mul <8 x i32> %50, %28
  %60 = add <8 x i32> %58, %32
  %61 = add <8 x i32> %60, %59
  %62 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %61, i32 %2) #8
  %63 = sub <8 x i32> %60, %59
  %64 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %63, i32 %2) #8
  %65 = icmp eq i32 %3, 0
  br i1 %65, label %84, label %66

66:                                               ; preds = %6
  %67 = bitcast <4 x i64>* %1 to <8 x i32>*
  store <8 x i32> %37, <8 x i32>* %67, align 32
  %68 = sub <8 x i32> zeroinitializer, %45
  %69 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %70 = bitcast <4 x i64>* %69 to <8 x i32>*
  store <8 x i32> %68, <8 x i32>* %70, align 32
  %71 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %72 = bitcast <4 x i64>* %71 to <8 x i32>*
  store <8 x i32> %62, <8 x i32>* %72, align 32
  %73 = sub <8 x i32> zeroinitializer, %55
  %74 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %75 = bitcast <4 x i64>* %74 to <8 x i32>*
  store <8 x i32> %73, <8 x i32>* %75, align 32
  %76 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %77 = bitcast <4 x i64>* %76 to <8 x i32>*
  store <8 x i32> %57, <8 x i32>* %77, align 32
  %78 = sub <8 x i32> zeroinitializer, %64
  %79 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %80 = bitcast <4 x i64>* %79 to <8 x i32>*
  store <8 x i32> %78, <8 x i32>* %80, align 32
  %81 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %82 = bitcast <4 x i64>* %81 to <8 x i32>*
  store <8 x i32> %50, <8 x i32>* %82, align 32
  %83 = sub <8 x i32> zeroinitializer, %40
  br label %160

84:                                               ; preds = %6
  %85 = icmp sgt i32 %4, 10
  %86 = select i1 %85, i32 %4, i32 10
  %87 = shl i32 32, %86
  %88 = sub nsw i32 0, %87
  %89 = insertelement <8 x i32> undef, i32 %88, i32 0
  %90 = shufflevector <8 x i32> %89, <8 x i32> undef, <8 x i32> zeroinitializer
  %91 = add nsw i32 %87, -1
  %92 = insertelement <8 x i32> undef, i32 %91, i32 0
  %93 = shufflevector <8 x i32> %92, <8 x i32> undef, <8 x i32> zeroinitializer
  %94 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %95 = shl i32 1, %5
  %96 = ashr i32 %95, 1
  %97 = insertelement <8 x i32> undef, i32 %96, i32 0
  %98 = shufflevector <8 x i32> %97, <8 x i32> undef, <8 x i32> zeroinitializer
  %99 = add <8 x i32> %37, %98
  %100 = sub <8 x i32> %98, %45
  %101 = insertelement <4 x i32> <i32 undef, i32 0, i32 0, i32 0>, i32 %5, i32 0
  %102 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %99, <4 x i32> %101) #8
  %103 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %100, <4 x i32> %101) #8
  %104 = icmp sgt <8 x i32> %102, %90
  %105 = select <8 x i1> %104, <8 x i32> %102, <8 x i32> %90
  %106 = icmp slt <8 x i32> %105, %93
  %107 = select <8 x i1> %106, <8 x i32> %105, <8 x i32> %93
  %108 = icmp sgt <8 x i32> %103, %90
  %109 = select <8 x i1> %108, <8 x i32> %103, <8 x i32> %90
  %110 = icmp slt <8 x i32> %109, %93
  %111 = select <8 x i1> %110, <8 x i32> %109, <8 x i32> %93
  %112 = bitcast <4 x i64>* %1 to <8 x i32>*
  store <8 x i32> %107, <8 x i32>* %112, align 32
  %113 = bitcast <4 x i64>* %94 to <8 x i32>*
  store <8 x i32> %111, <8 x i32>* %113, align 32
  %114 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %115 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %116 = add <8 x i32> %62, %98
  %117 = sub <8 x i32> %98, %55
  %118 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %116, <4 x i32> %101) #8
  %119 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %117, <4 x i32> %101) #8
  %120 = icmp sgt <8 x i32> %118, %90
  %121 = select <8 x i1> %120, <8 x i32> %118, <8 x i32> %90
  %122 = icmp slt <8 x i32> %121, %93
  %123 = select <8 x i1> %122, <8 x i32> %121, <8 x i32> %93
  %124 = icmp sgt <8 x i32> %119, %90
  %125 = select <8 x i1> %124, <8 x i32> %119, <8 x i32> %90
  %126 = icmp slt <8 x i32> %125, %93
  %127 = select <8 x i1> %126, <8 x i32> %125, <8 x i32> %93
  %128 = bitcast <4 x i64>* %114 to <8 x i32>*
  store <8 x i32> %123, <8 x i32>* %128, align 32
  %129 = bitcast <4 x i64>* %115 to <8 x i32>*
  store <8 x i32> %127, <8 x i32>* %129, align 32
  %130 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %131 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %132 = add <8 x i32> %57, %98
  %133 = sub <8 x i32> %98, %64
  %134 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %132, <4 x i32> %101) #8
  %135 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %133, <4 x i32> %101) #8
  %136 = icmp sgt <8 x i32> %134, %90
  %137 = select <8 x i1> %136, <8 x i32> %134, <8 x i32> %90
  %138 = icmp slt <8 x i32> %137, %93
  %139 = select <8 x i1> %138, <8 x i32> %137, <8 x i32> %93
  %140 = icmp sgt <8 x i32> %135, %90
  %141 = select <8 x i1> %140, <8 x i32> %135, <8 x i32> %90
  %142 = icmp slt <8 x i32> %141, %93
  %143 = select <8 x i1> %142, <8 x i32> %141, <8 x i32> %93
  %144 = bitcast <4 x i64>* %130 to <8 x i32>*
  store <8 x i32> %139, <8 x i32>* %144, align 32
  %145 = bitcast <4 x i64>* %131 to <8 x i32>*
  store <8 x i32> %143, <8 x i32>* %145, align 32
  %146 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %147 = add <8 x i32> %50, %98
  %148 = sub <8 x i32> %98, %40
  %149 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %147, <4 x i32> %101) #8
  %150 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %148, <4 x i32> %101) #8
  %151 = icmp sgt <8 x i32> %149, %90
  %152 = select <8 x i1> %151, <8 x i32> %149, <8 x i32> %90
  %153 = icmp slt <8 x i32> %152, %93
  %154 = select <8 x i1> %153, <8 x i32> %152, <8 x i32> %93
  %155 = icmp sgt <8 x i32> %150, %90
  %156 = select <8 x i1> %155, <8 x i32> %150, <8 x i32> %90
  %157 = icmp slt <8 x i32> %156, %93
  %158 = select <8 x i1> %157, <8 x i32> %156, <8 x i32> %93
  %159 = bitcast <4 x i64>* %146 to <8 x i32>*
  store <8 x i32> %154, <8 x i32>* %159, align 32
  br label %160

160:                                              ; preds = %84, %66
  %161 = phi <8 x i32> [ %158, %84 ], [ %83, %66 ]
  %162 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %163 = bitcast <4 x i64>* %162 to <8 x i32>*
  store <8 x i32> %161, <8 x i32>* %163, align 32
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @iadst8x8_avx2(<4 x i64>* nocapture readonly, <4 x i64>* nocapture, i32, i32, i32, i32) #0 {
  %7 = add nsw i32 %2, -10
  %8 = sext i32 %7 to i64
  %9 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 4
  %10 = load i32, i32* %9, align 16
  %11 = insertelement <8 x i32> undef, i32 %10, i32 0
  %12 = shufflevector <8 x i32> %11, <8 x i32> undef, <8 x i32> zeroinitializer
  %13 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 60
  %14 = load i32, i32* %13, align 16
  %15 = insertelement <8 x i32> undef, i32 %14, i32 0
  %16 = shufflevector <8 x i32> %15, <8 x i32> undef, <8 x i32> zeroinitializer
  %17 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 20
  %18 = load i32, i32* %17, align 16
  %19 = insertelement <8 x i32> undef, i32 %18, i32 0
  %20 = shufflevector <8 x i32> %19, <8 x i32> undef, <8 x i32> zeroinitializer
  %21 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 44
  %22 = load i32, i32* %21, align 16
  %23 = insertelement <8 x i32> undef, i32 %22, i32 0
  %24 = shufflevector <8 x i32> %23, <8 x i32> undef, <8 x i32> zeroinitializer
  %25 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 36
  %26 = load i32, i32* %25, align 16
  %27 = insertelement <8 x i32> undef, i32 %26, i32 0
  %28 = shufflevector <8 x i32> %27, <8 x i32> undef, <8 x i32> zeroinitializer
  %29 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 28
  %30 = load i32, i32* %29, align 16
  %31 = insertelement <8 x i32> undef, i32 %30, i32 0
  %32 = shufflevector <8 x i32> %31, <8 x i32> undef, <8 x i32> zeroinitializer
  %33 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 52
  %34 = load i32, i32* %33, align 16
  %35 = insertelement <8 x i32> undef, i32 %34, i32 0
  %36 = shufflevector <8 x i32> %35, <8 x i32> undef, <8 x i32> zeroinitializer
  %37 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 12
  %38 = load i32, i32* %37, align 16
  %39 = insertelement <8 x i32> undef, i32 %38, i32 0
  %40 = shufflevector <8 x i32> %39, <8 x i32> undef, <8 x i32> zeroinitializer
  %41 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 16
  %42 = load i32, i32* %41, align 16
  %43 = insertelement <8 x i32> undef, i32 %42, i32 0
  %44 = shufflevector <8 x i32> %43, <8 x i32> undef, <8 x i32> zeroinitializer
  %45 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 48
  %46 = load i32, i32* %45, align 16
  %47 = insertelement <8 x i32> undef, i32 %46, i32 0
  %48 = shufflevector <8 x i32> %47, <8 x i32> undef, <8 x i32> zeroinitializer
  %49 = sub nsw i32 0, %46
  %50 = insertelement <8 x i32> undef, i32 %49, i32 0
  %51 = shufflevector <8 x i32> %50, <8 x i32> undef, <8 x i32> zeroinitializer
  %52 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 32
  %53 = load i32, i32* %52, align 16
  %54 = insertelement <8 x i32> undef, i32 %53, i32 0
  %55 = shufflevector <8 x i32> %54, <8 x i32> undef, <8 x i32> zeroinitializer
  %56 = add nsw i32 %2, -1
  %57 = shl i32 1, %56
  %58 = insertelement <8 x i32> undef, i32 %57, i32 0
  %59 = shufflevector <8 x i32> %58, <8 x i32> undef, <8 x i32> zeroinitializer
  %60 = icmp ne i32 %3, 0
  %61 = select i1 %60, i32 6, i32 8
  %62 = add nsw i32 %61, %4
  %63 = icmp slt i32 %62, 16
  %64 = add i32 %62, -1
  %65 = shl i32 1, %64
  %66 = select i1 %63, i32 32768, i32 %65
  %67 = sub nsw i32 0, %66
  %68 = insertelement <8 x i32> undef, i32 %67, i32 0
  %69 = shufflevector <8 x i32> %68, <8 x i32> undef, <8 x i32> zeroinitializer
  %70 = add nsw i32 %66, -1
  %71 = insertelement <8 x i32> undef, i32 %70, i32 0
  %72 = shufflevector <8 x i32> %71, <8 x i32> undef, <8 x i32> zeroinitializer
  %73 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 7
  %74 = bitcast <4 x i64>* %73 to <8 x i32>*
  %75 = load <8 x i32>, <8 x i32>* %74, align 32
  %76 = mul <8 x i32> %75, %12
  %77 = bitcast <4 x i64>* %0 to <8 x i32>*
  %78 = load <8 x i32>, <8 x i32>* %77, align 32
  %79 = mul <8 x i32> %78, %16
  %80 = add <8 x i32> %76, %59
  %81 = add <8 x i32> %80, %79
  %82 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %81, i32 %2) #8
  %83 = mul <8 x i32> %75, %16
  %84 = mul <8 x i32> %12, %78
  %85 = add <8 x i32> %83, %59
  %86 = sub <8 x i32> %85, %84
  %87 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %86, i32 %2) #8
  %88 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 5
  %89 = bitcast <4 x i64>* %88 to <8 x i32>*
  %90 = load <8 x i32>, <8 x i32>* %89, align 32
  %91 = mul <8 x i32> %90, %20
  %92 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 2
  %93 = bitcast <4 x i64>* %92 to <8 x i32>*
  %94 = load <8 x i32>, <8 x i32>* %93, align 32
  %95 = mul <8 x i32> %94, %24
  %96 = add <8 x i32> %91, %59
  %97 = add <8 x i32> %96, %95
  %98 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %97, i32 %2) #8
  %99 = mul <8 x i32> %90, %24
  %100 = mul <8 x i32> %20, %94
  %101 = add <8 x i32> %99, %59
  %102 = sub <8 x i32> %101, %100
  %103 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %102, i32 %2) #8
  %104 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 3
  %105 = bitcast <4 x i64>* %104 to <8 x i32>*
  %106 = load <8 x i32>, <8 x i32>* %105, align 32
  %107 = mul <8 x i32> %106, %28
  %108 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 4
  %109 = bitcast <4 x i64>* %108 to <8 x i32>*
  %110 = load <8 x i32>, <8 x i32>* %109, align 32
  %111 = mul <8 x i32> %110, %32
  %112 = add <8 x i32> %107, %59
  %113 = add <8 x i32> %112, %111
  %114 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %113, i32 %2) #8
  %115 = mul <8 x i32> %106, %32
  %116 = mul <8 x i32> %28, %110
  %117 = add <8 x i32> %115, %59
  %118 = sub <8 x i32> %117, %116
  %119 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %118, i32 %2) #8
  %120 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 1
  %121 = bitcast <4 x i64>* %120 to <8 x i32>*
  %122 = load <8 x i32>, <8 x i32>* %121, align 32
  %123 = mul <8 x i32> %122, %36
  %124 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 6
  %125 = bitcast <4 x i64>* %124 to <8 x i32>*
  %126 = load <8 x i32>, <8 x i32>* %125, align 32
  %127 = mul <8 x i32> %126, %40
  %128 = add <8 x i32> %123, %59
  %129 = add <8 x i32> %128, %127
  %130 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %129, i32 %2) #8
  %131 = mul <8 x i32> %122, %40
  %132 = mul <8 x i32> %36, %126
  %133 = add <8 x i32> %131, %59
  %134 = sub <8 x i32> %133, %132
  %135 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %134, i32 %2) #8
  %136 = add <8 x i32> %114, %82
  %137 = sub <8 x i32> %82, %114
  %138 = icmp sgt <8 x i32> %136, %69
  %139 = select <8 x i1> %138, <8 x i32> %136, <8 x i32> %69
  %140 = icmp slt <8 x i32> %139, %72
  %141 = select <8 x i1> %140, <8 x i32> %139, <8 x i32> %72
  %142 = icmp sgt <8 x i32> %137, %69
  %143 = select <8 x i1> %142, <8 x i32> %137, <8 x i32> %69
  %144 = icmp slt <8 x i32> %143, %72
  %145 = select <8 x i1> %144, <8 x i32> %143, <8 x i32> %72
  %146 = add <8 x i32> %119, %87
  %147 = sub <8 x i32> %87, %119
  %148 = icmp sgt <8 x i32> %146, %69
  %149 = select <8 x i1> %148, <8 x i32> %146, <8 x i32> %69
  %150 = icmp slt <8 x i32> %149, %72
  %151 = select <8 x i1> %150, <8 x i32> %149, <8 x i32> %72
  %152 = icmp sgt <8 x i32> %147, %69
  %153 = select <8 x i1> %152, <8 x i32> %147, <8 x i32> %69
  %154 = icmp slt <8 x i32> %153, %72
  %155 = select <8 x i1> %154, <8 x i32> %153, <8 x i32> %72
  %156 = add <8 x i32> %130, %98
  %157 = sub <8 x i32> %98, %130
  %158 = icmp sgt <8 x i32> %156, %69
  %159 = select <8 x i1> %158, <8 x i32> %156, <8 x i32> %69
  %160 = icmp slt <8 x i32> %159, %72
  %161 = select <8 x i1> %160, <8 x i32> %159, <8 x i32> %72
  %162 = icmp sgt <8 x i32> %157, %69
  %163 = select <8 x i1> %162, <8 x i32> %157, <8 x i32> %69
  %164 = icmp slt <8 x i32> %163, %72
  %165 = select <8 x i1> %164, <8 x i32> %163, <8 x i32> %72
  %166 = add <8 x i32> %135, %103
  %167 = sub <8 x i32> %103, %135
  %168 = icmp sgt <8 x i32> %166, %69
  %169 = select <8 x i1> %168, <8 x i32> %166, <8 x i32> %69
  %170 = icmp slt <8 x i32> %169, %72
  %171 = select <8 x i1> %170, <8 x i32> %169, <8 x i32> %72
  %172 = icmp sgt <8 x i32> %167, %69
  %173 = select <8 x i1> %172, <8 x i32> %167, <8 x i32> %69
  %174 = icmp slt <8 x i32> %173, %72
  %175 = select <8 x i1> %174, <8 x i32> %173, <8 x i32> %72
  %176 = mul <8 x i32> %145, %44
  %177 = mul <8 x i32> %155, %48
  %178 = add <8 x i32> %176, %59
  %179 = add <8 x i32> %178, %177
  %180 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %179, i32 %2) #8
  %181 = mul <8 x i32> %145, %48
  %182 = mul <8 x i32> %44, %155
  %183 = add <8 x i32> %181, %59
  %184 = sub <8 x i32> %183, %182
  %185 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %184, i32 %2) #8
  %186 = mul <8 x i32> %165, %51
  %187 = mul <8 x i32> %175, %44
  %188 = add <8 x i32> %186, %59
  %189 = add <8 x i32> %188, %187
  %190 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %189, i32 %2) #8
  %191 = mul <8 x i32> %165, %44
  %192 = mul <8 x i32> %51, %175
  %193 = add <8 x i32> %191, %59
  %194 = sub <8 x i32> %193, %192
  %195 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %194, i32 %2) #8
  %196 = add <8 x i32> %161, %141
  %197 = sub <8 x i32> %141, %161
  %198 = icmp sgt <8 x i32> %196, %69
  %199 = select <8 x i1> %198, <8 x i32> %196, <8 x i32> %69
  %200 = icmp slt <8 x i32> %199, %72
  %201 = select <8 x i1> %200, <8 x i32> %199, <8 x i32> %72
  %202 = icmp sgt <8 x i32> %197, %69
  %203 = select <8 x i1> %202, <8 x i32> %197, <8 x i32> %69
  %204 = icmp slt <8 x i32> %203, %72
  %205 = select <8 x i1> %204, <8 x i32> %203, <8 x i32> %72
  %206 = add <8 x i32> %171, %151
  %207 = sub <8 x i32> %151, %171
  %208 = icmp sgt <8 x i32> %206, %69
  %209 = select <8 x i1> %208, <8 x i32> %206, <8 x i32> %69
  %210 = icmp slt <8 x i32> %209, %72
  %211 = select <8 x i1> %210, <8 x i32> %209, <8 x i32> %72
  %212 = icmp sgt <8 x i32> %207, %69
  %213 = select <8 x i1> %212, <8 x i32> %207, <8 x i32> %69
  %214 = icmp slt <8 x i32> %213, %72
  %215 = select <8 x i1> %214, <8 x i32> %213, <8 x i32> %72
  %216 = add <8 x i32> %190, %180
  %217 = sub <8 x i32> %180, %190
  %218 = icmp sgt <8 x i32> %216, %69
  %219 = select <8 x i1> %218, <8 x i32> %216, <8 x i32> %69
  %220 = icmp slt <8 x i32> %219, %72
  %221 = select <8 x i1> %220, <8 x i32> %219, <8 x i32> %72
  %222 = icmp sgt <8 x i32> %217, %69
  %223 = select <8 x i1> %222, <8 x i32> %217, <8 x i32> %69
  %224 = icmp slt <8 x i32> %223, %72
  %225 = select <8 x i1> %224, <8 x i32> %223, <8 x i32> %72
  %226 = add <8 x i32> %195, %185
  %227 = sub <8 x i32> %185, %195
  %228 = icmp sgt <8 x i32> %226, %69
  %229 = select <8 x i1> %228, <8 x i32> %226, <8 x i32> %69
  %230 = icmp slt <8 x i32> %229, %72
  %231 = select <8 x i1> %230, <8 x i32> %229, <8 x i32> %72
  %232 = icmp sgt <8 x i32> %227, %69
  %233 = select <8 x i1> %232, <8 x i32> %227, <8 x i32> %69
  %234 = icmp slt <8 x i32> %233, %72
  %235 = select <8 x i1> %234, <8 x i32> %233, <8 x i32> %72
  %236 = mul <8 x i32> %205, %55
  %237 = mul <8 x i32> %215, %55
  %238 = add <8 x i32> %236, %59
  %239 = add <8 x i32> %238, %237
  %240 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %239, i32 %2) #8
  %241 = sub <8 x i32> %238, %237
  %242 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %241, i32 %2) #8
  %243 = mul <8 x i32> %225, %55
  %244 = mul <8 x i32> %235, %55
  %245 = add <8 x i32> %243, %59
  %246 = add <8 x i32> %245, %244
  %247 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %246, i32 %2) #8
  %248 = sub <8 x i32> %245, %244
  %249 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %248, i32 %2) #8
  br i1 %60, label %250, label %268

250:                                              ; preds = %6
  %251 = bitcast <4 x i64>* %1 to <8 x i32>*
  store <8 x i32> %201, <8 x i32>* %251, align 32
  %252 = sub <8 x i32> zeroinitializer, %221
  %253 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %254 = bitcast <4 x i64>* %253 to <8 x i32>*
  store <8 x i32> %252, <8 x i32>* %254, align 32
  %255 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %256 = bitcast <4 x i64>* %255 to <8 x i32>*
  store <8 x i32> %247, <8 x i32>* %256, align 32
  %257 = sub <8 x i32> zeroinitializer, %240
  %258 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %259 = bitcast <4 x i64>* %258 to <8 x i32>*
  store <8 x i32> %257, <8 x i32>* %259, align 32
  %260 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %261 = bitcast <4 x i64>* %260 to <8 x i32>*
  store <8 x i32> %242, <8 x i32>* %261, align 32
  %262 = sub <8 x i32> zeroinitializer, %249
  %263 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %264 = bitcast <4 x i64>* %263 to <8 x i32>*
  store <8 x i32> %262, <8 x i32>* %264, align 32
  %265 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %266 = bitcast <4 x i64>* %265 to <8 x i32>*
  store <8 x i32> %231, <8 x i32>* %266, align 32
  %267 = sub <8 x i32> zeroinitializer, %211
  br label %344

268:                                              ; preds = %6
  %269 = icmp sgt i32 %4, 10
  %270 = select i1 %269, i32 %4, i32 10
  %271 = shl i32 32, %270
  %272 = sub nsw i32 0, %271
  %273 = insertelement <8 x i32> undef, i32 %272, i32 0
  %274 = shufflevector <8 x i32> %273, <8 x i32> undef, <8 x i32> zeroinitializer
  %275 = add nsw i32 %271, -1
  %276 = insertelement <8 x i32> undef, i32 %275, i32 0
  %277 = shufflevector <8 x i32> %276, <8 x i32> undef, <8 x i32> zeroinitializer
  %278 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %279 = shl i32 1, %5
  %280 = ashr i32 %279, 1
  %281 = insertelement <8 x i32> undef, i32 %280, i32 0
  %282 = shufflevector <8 x i32> %281, <8 x i32> undef, <8 x i32> zeroinitializer
  %283 = add <8 x i32> %201, %282
  %284 = sub <8 x i32> %282, %221
  %285 = insertelement <4 x i32> <i32 undef, i32 0, i32 0, i32 0>, i32 %5, i32 0
  %286 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %283, <4 x i32> %285) #8
  %287 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %284, <4 x i32> %285) #8
  %288 = icmp sgt <8 x i32> %286, %274
  %289 = select <8 x i1> %288, <8 x i32> %286, <8 x i32> %274
  %290 = icmp slt <8 x i32> %289, %277
  %291 = select <8 x i1> %290, <8 x i32> %289, <8 x i32> %277
  %292 = icmp sgt <8 x i32> %287, %274
  %293 = select <8 x i1> %292, <8 x i32> %287, <8 x i32> %274
  %294 = icmp slt <8 x i32> %293, %277
  %295 = select <8 x i1> %294, <8 x i32> %293, <8 x i32> %277
  %296 = bitcast <4 x i64>* %1 to <8 x i32>*
  store <8 x i32> %291, <8 x i32>* %296, align 32
  %297 = bitcast <4 x i64>* %278 to <8 x i32>*
  store <8 x i32> %295, <8 x i32>* %297, align 32
  %298 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %299 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %300 = add <8 x i32> %247, %282
  %301 = sub <8 x i32> %282, %240
  %302 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %300, <4 x i32> %285) #8
  %303 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %301, <4 x i32> %285) #8
  %304 = icmp sgt <8 x i32> %302, %274
  %305 = select <8 x i1> %304, <8 x i32> %302, <8 x i32> %274
  %306 = icmp slt <8 x i32> %305, %277
  %307 = select <8 x i1> %306, <8 x i32> %305, <8 x i32> %277
  %308 = icmp sgt <8 x i32> %303, %274
  %309 = select <8 x i1> %308, <8 x i32> %303, <8 x i32> %274
  %310 = icmp slt <8 x i32> %309, %277
  %311 = select <8 x i1> %310, <8 x i32> %309, <8 x i32> %277
  %312 = bitcast <4 x i64>* %298 to <8 x i32>*
  store <8 x i32> %307, <8 x i32>* %312, align 32
  %313 = bitcast <4 x i64>* %299 to <8 x i32>*
  store <8 x i32> %311, <8 x i32>* %313, align 32
  %314 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %315 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %316 = add <8 x i32> %242, %282
  %317 = sub <8 x i32> %282, %249
  %318 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %316, <4 x i32> %285) #8
  %319 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %317, <4 x i32> %285) #8
  %320 = icmp sgt <8 x i32> %318, %274
  %321 = select <8 x i1> %320, <8 x i32> %318, <8 x i32> %274
  %322 = icmp slt <8 x i32> %321, %277
  %323 = select <8 x i1> %322, <8 x i32> %321, <8 x i32> %277
  %324 = icmp sgt <8 x i32> %319, %274
  %325 = select <8 x i1> %324, <8 x i32> %319, <8 x i32> %274
  %326 = icmp slt <8 x i32> %325, %277
  %327 = select <8 x i1> %326, <8 x i32> %325, <8 x i32> %277
  %328 = bitcast <4 x i64>* %314 to <8 x i32>*
  store <8 x i32> %323, <8 x i32>* %328, align 32
  %329 = bitcast <4 x i64>* %315 to <8 x i32>*
  store <8 x i32> %327, <8 x i32>* %329, align 32
  %330 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %331 = add <8 x i32> %231, %282
  %332 = sub <8 x i32> %282, %211
  %333 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %331, <4 x i32> %285) #8
  %334 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %332, <4 x i32> %285) #8
  %335 = icmp sgt <8 x i32> %333, %274
  %336 = select <8 x i1> %335, <8 x i32> %333, <8 x i32> %274
  %337 = icmp slt <8 x i32> %336, %277
  %338 = select <8 x i1> %337, <8 x i32> %336, <8 x i32> %277
  %339 = icmp sgt <8 x i32> %334, %274
  %340 = select <8 x i1> %339, <8 x i32> %334, <8 x i32> %274
  %341 = icmp slt <8 x i32> %340, %277
  %342 = select <8 x i1> %341, <8 x i32> %340, <8 x i32> %277
  %343 = bitcast <4 x i64>* %330 to <8 x i32>*
  store <8 x i32> %338, <8 x i32>* %343, align 32
  br label %344

344:                                              ; preds = %268, %250
  %345 = phi <8 x i32> [ %342, %268 ], [ %267, %250 ]
  %346 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %347 = bitcast <4 x i64>* %346 to <8 x i32>*
  store <8 x i32> %345, <8 x i32>* %347, align 32
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define internal void @idct16_low1_avx2(<4 x i64>*, <4 x i64>* nocapture, i32, i32, i32, i32) #4 {
  %7 = add nsw i32 %2, -10
  %8 = sext i32 %7 to i64
  %9 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 32
  %10 = load i32, i32* %9, align 16
  %11 = insertelement <8 x i32> undef, i32 %10, i32 0
  %12 = shufflevector <8 x i32> %11, <8 x i32> undef, <8 x i32> zeroinitializer
  %13 = add nsw i32 %2, -1
  %14 = shl i32 1, %13
  %15 = insertelement <8 x i32> undef, i32 %14, i32 0
  %16 = shufflevector <8 x i32> %15, <8 x i32> undef, <8 x i32> zeroinitializer
  %17 = icmp ne i32 %3, 0
  %18 = select i1 %17, i32 6, i32 8
  %19 = add nsw i32 %18, %4
  %20 = icmp slt i32 %19, 16
  %21 = add i32 %19, -1
  %22 = shl i32 1, %21
  %23 = select i1 %20, i32 32768, i32 %22
  %24 = sub nsw i32 0, %23
  %25 = insertelement <8 x i32> undef, i32 %24, i32 0
  %26 = shufflevector <8 x i32> %25, <8 x i32> undef, <8 x i32> zeroinitializer
  %27 = add nsw i32 %23, -1
  %28 = insertelement <8 x i32> undef, i32 %27, i32 0
  %29 = shufflevector <8 x i32> %28, <8 x i32> undef, <8 x i32> zeroinitializer
  %30 = bitcast <4 x i64>* %0 to <8 x i32>*
  %31 = load <8 x i32>, <8 x i32>* %30, align 32
  %32 = mul <8 x i32> %12, %31
  %33 = add <8 x i32> %32, %16
  %34 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %33, i32 %2) #8
  br i1 %17, label %52, label %35

35:                                               ; preds = %6
  %36 = icmp sgt i32 %4, 10
  %37 = select i1 %36, i32 %4, i32 10
  %38 = shl i32 32, %37
  %39 = sub nsw i32 0, %38
  %40 = insertelement <8 x i32> undef, i32 %39, i32 0
  %41 = shufflevector <8 x i32> %40, <8 x i32> undef, <8 x i32> zeroinitializer
  %42 = add nsw i32 %38, -1
  %43 = insertelement <8 x i32> undef, i32 %42, i32 0
  %44 = shufflevector <8 x i32> %43, <8 x i32> undef, <8 x i32> zeroinitializer
  %45 = shl i32 1, %5
  %46 = ashr i32 %45, 1
  %47 = insertelement <8 x i32> undef, i32 %46, i32 0
  %48 = shufflevector <8 x i32> %47, <8 x i32> undef, <8 x i32> zeroinitializer
  %49 = add <8 x i32> %34, %48
  %50 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %5, i32 0
  %51 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %49, <4 x i32> %50) #8
  br label %52

52:                                               ; preds = %35, %6
  %53 = phi <8 x i32> [ %51, %35 ], [ %34, %6 ]
  %54 = phi <8 x i32> [ %41, %35 ], [ %26, %6 ]
  %55 = phi <8 x i32> [ %44, %35 ], [ %29, %6 ]
  %56 = icmp sgt <8 x i32> %53, %54
  %57 = select <8 x i1> %56, <8 x i32> %53, <8 x i32> %54
  %58 = icmp slt <8 x i32> %57, %55
  %59 = select <8 x i1> %58, <8 x i32> %57, <8 x i32> %55
  store <8 x i32> %59, <8 x i32>* %30, align 32
  %60 = bitcast <4 x i64>* %1 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %60, align 32
  %61 = load <4 x i64>, <4 x i64>* %0, align 32
  %62 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  store <4 x i64> %61, <4 x i64>* %62, align 32
  %63 = load <4 x i64>, <4 x i64>* %0, align 32
  %64 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  store <4 x i64> %63, <4 x i64>* %64, align 32
  %65 = load <4 x i64>, <4 x i64>* %0, align 32
  %66 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  store <4 x i64> %65, <4 x i64>* %66, align 32
  %67 = load <4 x i64>, <4 x i64>* %0, align 32
  %68 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  store <4 x i64> %67, <4 x i64>* %68, align 32
  %69 = load <4 x i64>, <4 x i64>* %0, align 32
  %70 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  store <4 x i64> %69, <4 x i64>* %70, align 32
  %71 = load <4 x i64>, <4 x i64>* %0, align 32
  %72 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  store <4 x i64> %71, <4 x i64>* %72, align 32
  %73 = load <4 x i64>, <4 x i64>* %0, align 32
  %74 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  store <4 x i64> %73, <4 x i64>* %74, align 32
  %75 = load <4 x i64>, <4 x i64>* %0, align 32
  %76 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  store <4 x i64> %75, <4 x i64>* %76, align 32
  %77 = load <4 x i64>, <4 x i64>* %0, align 32
  %78 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  store <4 x i64> %77, <4 x i64>* %78, align 32
  %79 = load <4 x i64>, <4 x i64>* %0, align 32
  %80 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  store <4 x i64> %79, <4 x i64>* %80, align 32
  %81 = load <4 x i64>, <4 x i64>* %0, align 32
  %82 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  store <4 x i64> %81, <4 x i64>* %82, align 32
  %83 = load <4 x i64>, <4 x i64>* %0, align 32
  %84 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  store <4 x i64> %83, <4 x i64>* %84, align 32
  %85 = load <4 x i64>, <4 x i64>* %0, align 32
  %86 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  store <4 x i64> %85, <4 x i64>* %86, align 32
  %87 = load <4 x i64>, <4 x i64>* %0, align 32
  %88 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  store <4 x i64> %87, <4 x i64>* %88, align 32
  %89 = load <4 x i64>, <4 x i64>* %0, align 32
  %90 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 15
  store <4 x i64> %89, <4 x i64>* %90, align 32
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @idct16_low8_avx2(<4 x i64>* nocapture readonly, <4 x i64>*, i32, i32, i32, i32) #0 {
  %7 = add nsw i32 %2, -10
  %8 = sext i32 %7 to i64
  %9 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 60
  %10 = load i32, i32* %9, align 16
  %11 = insertelement <8 x i32> undef, i32 %10, i32 0
  %12 = shufflevector <8 x i32> %11, <8 x i32> undef, <8 x i32> zeroinitializer
  %13 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 28
  %14 = load i32, i32* %13, align 16
  %15 = insertelement <8 x i32> undef, i32 %14, i32 0
  %16 = shufflevector <8 x i32> %15, <8 x i32> undef, <8 x i32> zeroinitializer
  %17 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 44
  %18 = load i32, i32* %17, align 16
  %19 = insertelement <8 x i32> undef, i32 %18, i32 0
  %20 = shufflevector <8 x i32> %19, <8 x i32> undef, <8 x i32> zeroinitializer
  %21 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 20
  %22 = load i32, i32* %21, align 16
  %23 = insertelement <8 x i32> undef, i32 %22, i32 0
  %24 = shufflevector <8 x i32> %23, <8 x i32> undef, <8 x i32> zeroinitializer
  %25 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 12
  %26 = load i32, i32* %25, align 16
  %27 = insertelement <8 x i32> undef, i32 %26, i32 0
  %28 = shufflevector <8 x i32> %27, <8 x i32> undef, <8 x i32> zeroinitializer
  %29 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 4
  %30 = load i32, i32* %29, align 16
  %31 = insertelement <8 x i32> undef, i32 %30, i32 0
  %32 = shufflevector <8 x i32> %31, <8 x i32> undef, <8 x i32> zeroinitializer
  %33 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 56
  %34 = load i32, i32* %33, align 16
  %35 = insertelement <8 x i32> undef, i32 %34, i32 0
  %36 = shufflevector <8 x i32> %35, <8 x i32> undef, <8 x i32> zeroinitializer
  %37 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 24
  %38 = load i32, i32* %37, align 16
  %39 = insertelement <8 x i32> undef, i32 %38, i32 0
  %40 = shufflevector <8 x i32> %39, <8 x i32> undef, <8 x i32> zeroinitializer
  %41 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 40
  %42 = load i32, i32* %41, align 16
  %43 = sub nsw i32 0, %42
  %44 = insertelement <8 x i32> undef, i32 %43, i32 0
  %45 = shufflevector <8 x i32> %44, <8 x i32> undef, <8 x i32> zeroinitializer
  %46 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 8
  %47 = load i32, i32* %46, align 16
  %48 = insertelement <8 x i32> undef, i32 %47, i32 0
  %49 = shufflevector <8 x i32> %48, <8 x i32> undef, <8 x i32> zeroinitializer
  %50 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 32
  %51 = load i32, i32* %50, align 16
  %52 = insertelement <8 x i32> undef, i32 %51, i32 0
  %53 = shufflevector <8 x i32> %52, <8 x i32> undef, <8 x i32> zeroinitializer
  %54 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 48
  %55 = load i32, i32* %54, align 16
  %56 = insertelement <8 x i32> undef, i32 %55, i32 0
  %57 = shufflevector <8 x i32> %56, <8 x i32> undef, <8 x i32> zeroinitializer
  %58 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 16
  %59 = load i32, i32* %58, align 16
  %60 = insertelement <8 x i32> undef, i32 %59, i32 0
  %61 = shufflevector <8 x i32> %60, <8 x i32> undef, <8 x i32> zeroinitializer
  %62 = sub nsw i32 0, %59
  %63 = insertelement <8 x i32> undef, i32 %62, i32 0
  %64 = shufflevector <8 x i32> %63, <8 x i32> undef, <8 x i32> zeroinitializer
  %65 = sub nsw i32 0, %55
  %66 = insertelement <8 x i32> undef, i32 %65, i32 0
  %67 = shufflevector <8 x i32> %66, <8 x i32> undef, <8 x i32> zeroinitializer
  %68 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 36
  %69 = load i32, i32* %68, align 16
  %70 = sub nsw i32 0, %69
  %71 = insertelement <8 x i32> undef, i32 %70, i32 0
  %72 = shufflevector <8 x i32> %71, <8 x i32> undef, <8 x i32> zeroinitializer
  %73 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 52
  %74 = load i32, i32* %73, align 16
  %75 = sub nsw i32 0, %74
  %76 = insertelement <8 x i32> undef, i32 %75, i32 0
  %77 = shufflevector <8 x i32> %76, <8 x i32> undef, <8 x i32> zeroinitializer
  %78 = add nsw i32 %2, -1
  %79 = shl i32 1, %78
  %80 = insertelement <8 x i32> undef, i32 %79, i32 0
  %81 = shufflevector <8 x i32> %80, <8 x i32> undef, <8 x i32> zeroinitializer
  %82 = icmp ne i32 %3, 0
  %83 = select i1 %82, i32 6, i32 8
  %84 = add nsw i32 %83, %4
  %85 = icmp slt i32 %84, 16
  %86 = add i32 %84, -1
  %87 = shl i32 1, %86
  %88 = select i1 %85, i32 32768, i32 %87
  %89 = sub nsw i32 0, %88
  %90 = insertelement <8 x i32> undef, i32 %89, i32 0
  %91 = shufflevector <8 x i32> %90, <8 x i32> undef, <8 x i32> zeroinitializer
  %92 = add nsw i32 %88, -1
  %93 = insertelement <8 x i32> undef, i32 %92, i32 0
  %94 = shufflevector <8 x i32> %93, <8 x i32> undef, <8 x i32> zeroinitializer
  %95 = bitcast <4 x i64>* %0 to <8 x i32>*
  %96 = load <8 x i32>, <8 x i32>* %95, align 32
  %97 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 4
  %98 = bitcast <4 x i64>* %97 to <8 x i32>*
  %99 = load <8 x i32>, <8 x i32>* %98, align 32
  %100 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 2
  %101 = bitcast <4 x i64>* %100 to <8 x i32>*
  %102 = load <8 x i32>, <8 x i32>* %101, align 32
  %103 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 6
  %104 = bitcast <4 x i64>* %103 to <8 x i32>*
  %105 = load <8 x i32>, <8 x i32>* %104, align 32
  %106 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 1
  %107 = bitcast <4 x i64>* %106 to <8 x i32>*
  %108 = load <8 x i32>, <8 x i32>* %107, align 32
  %109 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 5
  %110 = bitcast <4 x i64>* %109 to <8 x i32>*
  %111 = load <8 x i32>, <8 x i32>* %110, align 32
  %112 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 3
  %113 = bitcast <4 x i64>* %112 to <8 x i32>*
  %114 = load <8 x i32>, <8 x i32>* %113, align 32
  %115 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 7
  %116 = bitcast <4 x i64>* %115 to <8 x i32>*
  %117 = load <8 x i32>, <8 x i32>* %116, align 32
  %118 = mul <8 x i32> %108, %32
  %119 = add <8 x i32> %118, %81
  %120 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %119, i32 %2) #8
  %121 = mul <8 x i32> %108, %12
  %122 = add <8 x i32> %121, %81
  %123 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %122, i32 %2) #8
  %124 = mul <8 x i32> %117, %72
  %125 = add <8 x i32> %124, %81
  %126 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %125, i32 %2) #8
  %127 = mul <8 x i32> %117, %16
  %128 = add <8 x i32> %127, %81
  %129 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %128, i32 %2) #8
  %130 = mul <8 x i32> %111, %24
  %131 = add <8 x i32> %130, %81
  %132 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %131, i32 %2) #8
  %133 = mul <8 x i32> %111, %20
  %134 = add <8 x i32> %133, %81
  %135 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %134, i32 %2) #8
  %136 = mul <8 x i32> %114, %77
  %137 = add <8 x i32> %136, %81
  %138 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %137, i32 %2) #8
  %139 = mul <8 x i32> %114, %28
  %140 = add <8 x i32> %139, %81
  %141 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %140, i32 %2) #8
  %142 = mul <8 x i32> %102, %49
  %143 = add <8 x i32> %142, %81
  %144 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %143, i32 %2) #8
  %145 = mul <8 x i32> %102, %36
  %146 = add <8 x i32> %145, %81
  %147 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %146, i32 %2) #8
  %148 = mul <8 x i32> %105, %45
  %149 = add <8 x i32> %148, %81
  %150 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %149, i32 %2) #8
  %151 = mul <8 x i32> %105, %40
  %152 = add <8 x i32> %151, %81
  %153 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %152, i32 %2) #8
  %154 = add <8 x i32> %126, %123
  %155 = sub <8 x i32> %123, %126
  %156 = icmp sgt <8 x i32> %154, %91
  %157 = select <8 x i1> %156, <8 x i32> %154, <8 x i32> %91
  %158 = icmp slt <8 x i32> %157, %94
  %159 = select <8 x i1> %158, <8 x i32> %157, <8 x i32> %94
  %160 = icmp sgt <8 x i32> %155, %91
  %161 = select <8 x i1> %160, <8 x i32> %155, <8 x i32> %91
  %162 = icmp slt <8 x i32> %161, %94
  %163 = select <8 x i1> %162, <8 x i32> %161, <8 x i32> %94
  %164 = add <8 x i32> %138, %135
  %165 = sub <8 x i32> %138, %135
  %166 = icmp sgt <8 x i32> %164, %91
  %167 = select <8 x i1> %166, <8 x i32> %164, <8 x i32> %91
  %168 = icmp slt <8 x i32> %167, %94
  %169 = select <8 x i1> %168, <8 x i32> %167, <8 x i32> %94
  %170 = icmp sgt <8 x i32> %165, %91
  %171 = select <8 x i1> %170, <8 x i32> %165, <8 x i32> %91
  %172 = icmp slt <8 x i32> %171, %94
  %173 = select <8 x i1> %172, <8 x i32> %171, <8 x i32> %94
  %174 = add <8 x i32> %141, %132
  %175 = sub <8 x i32> %141, %132
  %176 = icmp sgt <8 x i32> %174, %91
  %177 = select <8 x i1> %176, <8 x i32> %174, <8 x i32> %91
  %178 = icmp slt <8 x i32> %177, %94
  %179 = select <8 x i1> %178, <8 x i32> %177, <8 x i32> %94
  %180 = icmp sgt <8 x i32> %175, %91
  %181 = select <8 x i1> %180, <8 x i32> %175, <8 x i32> %91
  %182 = icmp slt <8 x i32> %181, %94
  %183 = select <8 x i1> %182, <8 x i32> %181, <8 x i32> %94
  %184 = add <8 x i32> %129, %120
  %185 = sub <8 x i32> %120, %129
  %186 = icmp sgt <8 x i32> %184, %91
  %187 = select <8 x i1> %186, <8 x i32> %184, <8 x i32> %91
  %188 = icmp slt <8 x i32> %187, %94
  %189 = select <8 x i1> %188, <8 x i32> %187, <8 x i32> %94
  %190 = icmp sgt <8 x i32> %185, %91
  %191 = select <8 x i1> %190, <8 x i32> %185, <8 x i32> %91
  %192 = icmp slt <8 x i32> %191, %94
  %193 = select <8 x i1> %192, <8 x i32> %191, <8 x i32> %94
  %194 = mul <8 x i32> %96, %53
  %195 = add <8 x i32> %194, %81
  %196 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %195, i32 %2) #8
  %197 = mul <8 x i32> %99, %61
  %198 = add <8 x i32> %197, %81
  %199 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %198, i32 %2) #8
  %200 = mul <8 x i32> %99, %57
  %201 = add <8 x i32> %200, %81
  %202 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %201, i32 %2) #8
  %203 = add <8 x i32> %150, %147
  %204 = sub <8 x i32> %147, %150
  %205 = icmp sgt <8 x i32> %203, %91
  %206 = select <8 x i1> %205, <8 x i32> %203, <8 x i32> %91
  %207 = icmp slt <8 x i32> %206, %94
  %208 = select <8 x i1> %207, <8 x i32> %206, <8 x i32> %94
  %209 = icmp sgt <8 x i32> %204, %91
  %210 = select <8 x i1> %209, <8 x i32> %204, <8 x i32> %91
  %211 = icmp slt <8 x i32> %210, %94
  %212 = select <8 x i1> %211, <8 x i32> %210, <8 x i32> %94
  %213 = add <8 x i32> %153, %144
  %214 = sub <8 x i32> %144, %153
  %215 = icmp sgt <8 x i32> %213, %91
  %216 = select <8 x i1> %215, <8 x i32> %213, <8 x i32> %91
  %217 = icmp slt <8 x i32> %216, %94
  %218 = select <8 x i1> %217, <8 x i32> %216, <8 x i32> %94
  %219 = icmp sgt <8 x i32> %214, %91
  %220 = select <8 x i1> %219, <8 x i32> %214, <8 x i32> %91
  %221 = icmp slt <8 x i32> %220, %94
  %222 = select <8 x i1> %221, <8 x i32> %220, <8 x i32> %94
  %223 = mul <8 x i32> %163, %64
  %224 = mul <8 x i32> %193, %57
  %225 = add <8 x i32> %223, %81
  %226 = add <8 x i32> %225, %224
  %227 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %226, i32 %2) #8
  %228 = mul <8 x i32> %163, %57
  %229 = mul <8 x i32> %193, %61
  %230 = add <8 x i32> %228, %81
  %231 = add <8 x i32> %230, %229
  %232 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %231, i32 %2) #8
  %233 = mul <8 x i32> %173, %67
  %234 = mul <8 x i32> %183, %64
  %235 = add <8 x i32> %233, %81
  %236 = add <8 x i32> %235, %234
  %237 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %236, i32 %2) #8
  %238 = mul <8 x i32> %173, %64
  %239 = mul <8 x i32> %183, %57
  %240 = add <8 x i32> %238, %81
  %241 = add <8 x i32> %240, %239
  %242 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %241, i32 %2) #8
  %243 = add <8 x i32> %199, %196
  %244 = sub <8 x i32> %196, %199
  %245 = icmp sgt <8 x i32> %243, %91
  %246 = select <8 x i1> %245, <8 x i32> %243, <8 x i32> %91
  %247 = icmp slt <8 x i32> %246, %94
  %248 = select <8 x i1> %247, <8 x i32> %246, <8 x i32> %94
  %249 = icmp sgt <8 x i32> %244, %91
  %250 = select <8 x i1> %249, <8 x i32> %244, <8 x i32> %91
  %251 = icmp slt <8 x i32> %250, %94
  %252 = select <8 x i1> %251, <8 x i32> %250, <8 x i32> %94
  %253 = add <8 x i32> %202, %196
  %254 = sub <8 x i32> %196, %202
  %255 = icmp sgt <8 x i32> %253, %91
  %256 = select <8 x i1> %255, <8 x i32> %253, <8 x i32> %91
  %257 = icmp slt <8 x i32> %256, %94
  %258 = select <8 x i1> %257, <8 x i32> %256, <8 x i32> %94
  %259 = icmp sgt <8 x i32> %254, %91
  %260 = select <8 x i1> %259, <8 x i32> %254, <8 x i32> %91
  %261 = icmp slt <8 x i32> %260, %94
  %262 = select <8 x i1> %261, <8 x i32> %260, <8 x i32> %94
  %263 = mul <8 x i32> %212, %53
  %264 = mul <8 x i32> %222, %53
  %265 = sub <8 x i32> %81, %263
  %266 = add <8 x i32> %265, %264
  %267 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %266, i32 %2) #8
  %268 = add <8 x i32> %263, %81
  %269 = add <8 x i32> %268, %264
  %270 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %269, i32 %2) #8
  %271 = add <8 x i32> %169, %159
  %272 = sub <8 x i32> %159, %169
  %273 = icmp sgt <8 x i32> %271, %91
  %274 = select <8 x i1> %273, <8 x i32> %271, <8 x i32> %91
  %275 = icmp slt <8 x i32> %274, %94
  %276 = select <8 x i1> %275, <8 x i32> %274, <8 x i32> %94
  %277 = icmp sgt <8 x i32> %272, %91
  %278 = select <8 x i1> %277, <8 x i32> %272, <8 x i32> %91
  %279 = icmp slt <8 x i32> %278, %94
  %280 = select <8 x i1> %279, <8 x i32> %278, <8 x i32> %94
  %281 = add <8 x i32> %237, %227
  %282 = sub <8 x i32> %227, %237
  %283 = icmp sgt <8 x i32> %281, %91
  %284 = select <8 x i1> %283, <8 x i32> %281, <8 x i32> %91
  %285 = icmp slt <8 x i32> %284, %94
  %286 = select <8 x i1> %285, <8 x i32> %284, <8 x i32> %94
  %287 = icmp sgt <8 x i32> %282, %91
  %288 = select <8 x i1> %287, <8 x i32> %282, <8 x i32> %91
  %289 = icmp slt <8 x i32> %288, %94
  %290 = select <8 x i1> %289, <8 x i32> %288, <8 x i32> %94
  %291 = add <8 x i32> %179, %189
  %292 = sub <8 x i32> %189, %179
  %293 = icmp sgt <8 x i32> %291, %91
  %294 = select <8 x i1> %293, <8 x i32> %291, <8 x i32> %91
  %295 = icmp slt <8 x i32> %294, %94
  %296 = select <8 x i1> %295, <8 x i32> %294, <8 x i32> %94
  %297 = icmp sgt <8 x i32> %292, %91
  %298 = select <8 x i1> %297, <8 x i32> %292, <8 x i32> %91
  %299 = icmp slt <8 x i32> %298, %94
  %300 = select <8 x i1> %299, <8 x i32> %298, <8 x i32> %94
  %301 = add <8 x i32> %242, %232
  %302 = sub <8 x i32> %232, %242
  %303 = icmp sgt <8 x i32> %301, %91
  %304 = select <8 x i1> %303, <8 x i32> %301, <8 x i32> %91
  %305 = icmp slt <8 x i32> %304, %94
  %306 = select <8 x i1> %305, <8 x i32> %304, <8 x i32> %94
  %307 = icmp sgt <8 x i32> %302, %91
  %308 = select <8 x i1> %307, <8 x i32> %302, <8 x i32> %91
  %309 = icmp slt <8 x i32> %308, %94
  %310 = select <8 x i1> %309, <8 x i32> %308, <8 x i32> %94
  %311 = add <8 x i32> %248, %218
  %312 = sub <8 x i32> %248, %218
  %313 = icmp sgt <8 x i32> %311, %91
  %314 = select <8 x i1> %313, <8 x i32> %311, <8 x i32> %91
  %315 = icmp slt <8 x i32> %314, %94
  %316 = select <8 x i1> %315, <8 x i32> %314, <8 x i32> %94
  %317 = icmp sgt <8 x i32> %312, %91
  %318 = select <8 x i1> %317, <8 x i32> %312, <8 x i32> %91
  %319 = icmp slt <8 x i32> %318, %94
  %320 = select <8 x i1> %319, <8 x i32> %318, <8 x i32> %94
  %321 = add <8 x i32> %270, %258
  %322 = sub <8 x i32> %258, %270
  %323 = icmp sgt <8 x i32> %321, %91
  %324 = select <8 x i1> %323, <8 x i32> %321, <8 x i32> %91
  %325 = icmp slt <8 x i32> %324, %94
  %326 = select <8 x i1> %325, <8 x i32> %324, <8 x i32> %94
  %327 = icmp sgt <8 x i32> %322, %91
  %328 = select <8 x i1> %327, <8 x i32> %322, <8 x i32> %91
  %329 = icmp slt <8 x i32> %328, %94
  %330 = select <8 x i1> %329, <8 x i32> %328, <8 x i32> %94
  %331 = add <8 x i32> %267, %262
  %332 = sub <8 x i32> %262, %267
  %333 = icmp sgt <8 x i32> %331, %91
  %334 = select <8 x i1> %333, <8 x i32> %331, <8 x i32> %91
  %335 = icmp slt <8 x i32> %334, %94
  %336 = select <8 x i1> %335, <8 x i32> %334, <8 x i32> %94
  %337 = icmp sgt <8 x i32> %332, %91
  %338 = select <8 x i1> %337, <8 x i32> %332, <8 x i32> %91
  %339 = icmp slt <8 x i32> %338, %94
  %340 = select <8 x i1> %339, <8 x i32> %338, <8 x i32> %94
  %341 = add <8 x i32> %252, %208
  %342 = sub <8 x i32> %252, %208
  %343 = icmp sgt <8 x i32> %341, %91
  %344 = select <8 x i1> %343, <8 x i32> %341, <8 x i32> %91
  %345 = icmp slt <8 x i32> %344, %94
  %346 = select <8 x i1> %345, <8 x i32> %344, <8 x i32> %94
  %347 = icmp sgt <8 x i32> %342, %91
  %348 = select <8 x i1> %347, <8 x i32> %342, <8 x i32> %91
  %349 = icmp slt <8 x i32> %348, %94
  %350 = select <8 x i1> %349, <8 x i32> %348, <8 x i32> %94
  %351 = mul <8 x i32> %290, %53
  %352 = mul <8 x i32> %310, %53
  %353 = sub <8 x i32> %81, %351
  %354 = add <8 x i32> %353, %352
  %355 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %354, i32 %2) #8
  %356 = add <8 x i32> %351, %81
  %357 = add <8 x i32> %356, %352
  %358 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %357, i32 %2) #8
  %359 = mul <8 x i32> %280, %53
  %360 = mul <8 x i32> %300, %53
  %361 = sub <8 x i32> %81, %359
  %362 = add <8 x i32> %361, %360
  %363 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %362, i32 %2) #8
  %364 = add <8 x i32> %359, %81
  %365 = add <8 x i32> %364, %360
  %366 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %365, i32 %2) #8
  %367 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 15
  %368 = add <8 x i32> %316, %296
  %369 = sub <8 x i32> %316, %296
  %370 = icmp sgt <8 x i32> %368, %91
  %371 = select <8 x i1> %370, <8 x i32> %368, <8 x i32> %91
  %372 = icmp slt <8 x i32> %371, %94
  %373 = select <8 x i1> %372, <8 x i32> %371, <8 x i32> %94
  %374 = icmp sgt <8 x i32> %369, %91
  %375 = select <8 x i1> %374, <8 x i32> %369, <8 x i32> %91
  %376 = icmp slt <8 x i32> %375, %94
  %377 = select <8 x i1> %376, <8 x i32> %375, <8 x i32> %94
  %378 = bitcast <4 x i64>* %1 to <8 x i32>*
  store <8 x i32> %373, <8 x i32>* %378, align 32
  %379 = bitcast <4 x i64>* %367 to <8 x i32>*
  store <8 x i32> %377, <8 x i32>* %379, align 32
  %380 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %381 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %382 = add <8 x i32> %326, %306
  %383 = sub <8 x i32> %326, %306
  %384 = icmp sgt <8 x i32> %382, %91
  %385 = select <8 x i1> %384, <8 x i32> %382, <8 x i32> %91
  %386 = icmp slt <8 x i32> %385, %94
  %387 = select <8 x i1> %386, <8 x i32> %385, <8 x i32> %94
  %388 = icmp sgt <8 x i32> %383, %91
  %389 = select <8 x i1> %388, <8 x i32> %383, <8 x i32> %91
  %390 = icmp slt <8 x i32> %389, %94
  %391 = select <8 x i1> %390, <8 x i32> %389, <8 x i32> %94
  %392 = bitcast <4 x i64>* %380 to <8 x i32>*
  store <8 x i32> %387, <8 x i32>* %392, align 32
  %393 = bitcast <4 x i64>* %381 to <8 x i32>*
  store <8 x i32> %391, <8 x i32>* %393, align 32
  %394 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %395 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %396 = add <8 x i32> %336, %358
  %397 = sub <8 x i32> %336, %358
  %398 = icmp sgt <8 x i32> %396, %91
  %399 = select <8 x i1> %398, <8 x i32> %396, <8 x i32> %91
  %400 = icmp slt <8 x i32> %399, %94
  %401 = select <8 x i1> %400, <8 x i32> %399, <8 x i32> %94
  %402 = icmp sgt <8 x i32> %397, %91
  %403 = select <8 x i1> %402, <8 x i32> %397, <8 x i32> %91
  %404 = icmp slt <8 x i32> %403, %94
  %405 = select <8 x i1> %404, <8 x i32> %403, <8 x i32> %94
  %406 = bitcast <4 x i64>* %394 to <8 x i32>*
  store <8 x i32> %401, <8 x i32>* %406, align 32
  %407 = bitcast <4 x i64>* %395 to <8 x i32>*
  store <8 x i32> %405, <8 x i32>* %407, align 32
  %408 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %409 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %410 = add <8 x i32> %366, %346
  %411 = sub <8 x i32> %346, %366
  %412 = icmp sgt <8 x i32> %410, %91
  %413 = select <8 x i1> %412, <8 x i32> %410, <8 x i32> %91
  %414 = icmp slt <8 x i32> %413, %94
  %415 = select <8 x i1> %414, <8 x i32> %413, <8 x i32> %94
  %416 = icmp sgt <8 x i32> %411, %91
  %417 = select <8 x i1> %416, <8 x i32> %411, <8 x i32> %91
  %418 = icmp slt <8 x i32> %417, %94
  %419 = select <8 x i1> %418, <8 x i32> %417, <8 x i32> %94
  %420 = bitcast <4 x i64>* %408 to <8 x i32>*
  store <8 x i32> %415, <8 x i32>* %420, align 32
  %421 = bitcast <4 x i64>* %409 to <8 x i32>*
  store <8 x i32> %419, <8 x i32>* %421, align 32
  %422 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %423 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %424 = add <8 x i32> %363, %350
  %425 = sub <8 x i32> %350, %363
  %426 = icmp sgt <8 x i32> %424, %91
  %427 = select <8 x i1> %426, <8 x i32> %424, <8 x i32> %91
  %428 = icmp slt <8 x i32> %427, %94
  %429 = select <8 x i1> %428, <8 x i32> %427, <8 x i32> %94
  %430 = icmp sgt <8 x i32> %425, %91
  %431 = select <8 x i1> %430, <8 x i32> %425, <8 x i32> %91
  %432 = icmp slt <8 x i32> %431, %94
  %433 = select <8 x i1> %432, <8 x i32> %431, <8 x i32> %94
  %434 = bitcast <4 x i64>* %422 to <8 x i32>*
  store <8 x i32> %429, <8 x i32>* %434, align 32
  %435 = bitcast <4 x i64>* %423 to <8 x i32>*
  store <8 x i32> %433, <8 x i32>* %435, align 32
  %436 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %437 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %438 = add <8 x i32> %340, %355
  %439 = sub <8 x i32> %340, %355
  %440 = icmp sgt <8 x i32> %438, %91
  %441 = select <8 x i1> %440, <8 x i32> %438, <8 x i32> %91
  %442 = icmp slt <8 x i32> %441, %94
  %443 = select <8 x i1> %442, <8 x i32> %441, <8 x i32> %94
  %444 = icmp sgt <8 x i32> %439, %91
  %445 = select <8 x i1> %444, <8 x i32> %439, <8 x i32> %91
  %446 = icmp slt <8 x i32> %445, %94
  %447 = select <8 x i1> %446, <8 x i32> %445, <8 x i32> %94
  %448 = bitcast <4 x i64>* %436 to <8 x i32>*
  store <8 x i32> %443, <8 x i32>* %448, align 32
  %449 = bitcast <4 x i64>* %437 to <8 x i32>*
  store <8 x i32> %447, <8 x i32>* %449, align 32
  %450 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %451 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %452 = add <8 x i32> %330, %286
  %453 = sub <8 x i32> %330, %286
  %454 = icmp sgt <8 x i32> %452, %91
  %455 = select <8 x i1> %454, <8 x i32> %452, <8 x i32> %91
  %456 = icmp slt <8 x i32> %455, %94
  %457 = select <8 x i1> %456, <8 x i32> %455, <8 x i32> %94
  %458 = icmp sgt <8 x i32> %453, %91
  %459 = select <8 x i1> %458, <8 x i32> %453, <8 x i32> %91
  %460 = icmp slt <8 x i32> %459, %94
  %461 = select <8 x i1> %460, <8 x i32> %459, <8 x i32> %94
  %462 = bitcast <4 x i64>* %450 to <8 x i32>*
  store <8 x i32> %457, <8 x i32>* %462, align 32
  %463 = bitcast <4 x i64>* %451 to <8 x i32>*
  store <8 x i32> %461, <8 x i32>* %463, align 32
  %464 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %465 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %466 = add <8 x i32> %320, %276
  %467 = sub <8 x i32> %320, %276
  %468 = icmp sgt <8 x i32> %466, %91
  %469 = select <8 x i1> %468, <8 x i32> %466, <8 x i32> %91
  %470 = icmp slt <8 x i32> %469, %94
  %471 = select <8 x i1> %470, <8 x i32> %469, <8 x i32> %94
  %472 = icmp sgt <8 x i32> %467, %91
  %473 = select <8 x i1> %472, <8 x i32> %467, <8 x i32> %91
  %474 = icmp slt <8 x i32> %473, %94
  %475 = select <8 x i1> %474, <8 x i32> %473, <8 x i32> %94
  %476 = bitcast <4 x i64>* %464 to <8 x i32>*
  store <8 x i32> %471, <8 x i32>* %476, align 32
  %477 = bitcast <4 x i64>* %465 to <8 x i32>*
  store <8 x i32> %475, <8 x i32>* %477, align 32
  br i1 %82, label %611, label %478

478:                                              ; preds = %6
  %479 = icmp sgt i32 %4, 10
  %480 = select i1 %479, i32 %4, i32 10
  %481 = shl i32 32, %480
  %482 = sub nsw i32 0, %481
  %483 = insertelement <8 x i32> undef, i32 %482, i32 0
  %484 = shufflevector <8 x i32> %483, <8 x i32> undef, <8 x i32> zeroinitializer
  %485 = add nsw i32 %481, -1
  %486 = insertelement <8 x i32> undef, i32 %485, i32 0
  %487 = shufflevector <8 x i32> %486, <8 x i32> undef, <8 x i32> zeroinitializer
  %488 = icmp eq i32 %5, 0
  br i1 %488, label %489, label %493

489:                                              ; preds = %478
  %490 = load <8 x i32>, <8 x i32>* %378, align 32
  %491 = load <8 x i32>, <8 x i32>* %392, align 32
  %492 = load <8 x i32>, <8 x i32>* %379, align 32
  br label %530

493:                                              ; preds = %478
  %494 = add nsw i32 %5, -1
  %495 = shl i32 1, %494
  %496 = insertelement <8 x i32> undef, i32 %495, i32 0
  %497 = shufflevector <8 x i32> %496, <8 x i32> undef, <8 x i32> zeroinitializer
  %498 = add <8 x i32> %373, %497
  %499 = add <8 x i32> %387, %497
  %500 = add <8 x i32> %401, %497
  %501 = add <8 x i32> %415, %497
  %502 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %498, i32 %5) #8
  store <8 x i32> %502, <8 x i32>* %378, align 32
  %503 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %499, i32 %5) #8
  store <8 x i32> %503, <8 x i32>* %392, align 32
  %504 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %500, i32 %5) #8
  store <8 x i32> %504, <8 x i32>* %406, align 32
  %505 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %501, i32 %5) #8
  store <8 x i32> %505, <8 x i32>* %420, align 32
  %506 = add <8 x i32> %429, %497
  %507 = add <8 x i32> %443, %497
  %508 = add <8 x i32> %457, %497
  %509 = add <8 x i32> %471, %497
  %510 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %506, i32 %5) #8
  store <8 x i32> %510, <8 x i32>* %434, align 32
  %511 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %507, i32 %5) #8
  store <8 x i32> %511, <8 x i32>* %448, align 32
  %512 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %508, i32 %5) #8
  store <8 x i32> %512, <8 x i32>* %462, align 32
  %513 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %509, i32 %5) #8
  store <8 x i32> %513, <8 x i32>* %476, align 32
  %514 = add <8 x i32> %475, %497
  %515 = add <8 x i32> %461, %497
  %516 = add <8 x i32> %447, %497
  %517 = add <8 x i32> %433, %497
  %518 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %514, i32 %5) #8
  store <8 x i32> %518, <8 x i32>* %477, align 32
  %519 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %515, i32 %5) #8
  store <8 x i32> %519, <8 x i32>* %463, align 32
  %520 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %516, i32 %5) #8
  store <8 x i32> %520, <8 x i32>* %449, align 32
  %521 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %517, i32 %5) #8
  store <8 x i32> %521, <8 x i32>* %435, align 32
  %522 = add <8 x i32> %419, %497
  %523 = add <8 x i32> %405, %497
  %524 = add <8 x i32> %391, %497
  %525 = add <8 x i32> %377, %497
  %526 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %522, i32 %5) #8
  store <8 x i32> %526, <8 x i32>* %421, align 32
  %527 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %523, i32 %5) #8
  store <8 x i32> %527, <8 x i32>* %407, align 32
  %528 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %524, i32 %5) #8
  store <8 x i32> %528, <8 x i32>* %393, align 32
  %529 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %525, i32 %5) #8
  store <8 x i32> %529, <8 x i32>* %379, align 32
  br label %530

530:                                              ; preds = %489, %493
  %531 = phi <8 x i32> [ %492, %489 ], [ %529, %493 ]
  %532 = phi <8 x i32> [ %391, %489 ], [ %528, %493 ]
  %533 = phi <8 x i32> [ %405, %489 ], [ %527, %493 ]
  %534 = phi <8 x i32> [ %419, %489 ], [ %526, %493 ]
  %535 = phi <8 x i32> [ %433, %489 ], [ %521, %493 ]
  %536 = phi <8 x i32> [ %447, %489 ], [ %520, %493 ]
  %537 = phi <8 x i32> [ %461, %489 ], [ %519, %493 ]
  %538 = phi <8 x i32> [ %475, %489 ], [ %518, %493 ]
  %539 = phi <8 x i32> [ %471, %489 ], [ %513, %493 ]
  %540 = phi <8 x i32> [ %457, %489 ], [ %512, %493 ]
  %541 = phi <8 x i32> [ %443, %489 ], [ %511, %493 ]
  %542 = phi <8 x i32> [ %429, %489 ], [ %510, %493 ]
  %543 = phi <8 x i32> [ %415, %489 ], [ %505, %493 ]
  %544 = phi <8 x i32> [ %401, %489 ], [ %504, %493 ]
  %545 = phi <8 x i32> [ %491, %489 ], [ %503, %493 ]
  %546 = phi <8 x i32> [ %490, %489 ], [ %502, %493 ]
  %547 = icmp sgt <8 x i32> %546, %484
  %548 = select <8 x i1> %547, <8 x i32> %546, <8 x i32> %484
  %549 = icmp slt <8 x i32> %548, %487
  %550 = select <8 x i1> %549, <8 x i32> %548, <8 x i32> %487
  store <8 x i32> %550, <8 x i32>* %378, align 32
  %551 = icmp sgt <8 x i32> %545, %484
  %552 = select <8 x i1> %551, <8 x i32> %545, <8 x i32> %484
  %553 = icmp slt <8 x i32> %552, %487
  %554 = select <8 x i1> %553, <8 x i32> %552, <8 x i32> %487
  store <8 x i32> %554, <8 x i32>* %392, align 32
  %555 = icmp sgt <8 x i32> %544, %484
  %556 = select <8 x i1> %555, <8 x i32> %544, <8 x i32> %484
  %557 = icmp slt <8 x i32> %556, %487
  %558 = select <8 x i1> %557, <8 x i32> %556, <8 x i32> %487
  store <8 x i32> %558, <8 x i32>* %406, align 32
  %559 = icmp sgt <8 x i32> %543, %484
  %560 = select <8 x i1> %559, <8 x i32> %543, <8 x i32> %484
  %561 = icmp slt <8 x i32> %560, %487
  %562 = select <8 x i1> %561, <8 x i32> %560, <8 x i32> %487
  store <8 x i32> %562, <8 x i32>* %420, align 32
  %563 = icmp sgt <8 x i32> %542, %484
  %564 = select <8 x i1> %563, <8 x i32> %542, <8 x i32> %484
  %565 = icmp slt <8 x i32> %564, %487
  %566 = select <8 x i1> %565, <8 x i32> %564, <8 x i32> %487
  store <8 x i32> %566, <8 x i32>* %434, align 32
  %567 = icmp sgt <8 x i32> %541, %484
  %568 = select <8 x i1> %567, <8 x i32> %541, <8 x i32> %484
  %569 = icmp slt <8 x i32> %568, %487
  %570 = select <8 x i1> %569, <8 x i32> %568, <8 x i32> %487
  store <8 x i32> %570, <8 x i32>* %448, align 32
  %571 = icmp sgt <8 x i32> %540, %484
  %572 = select <8 x i1> %571, <8 x i32> %540, <8 x i32> %484
  %573 = icmp slt <8 x i32> %572, %487
  %574 = select <8 x i1> %573, <8 x i32> %572, <8 x i32> %487
  store <8 x i32> %574, <8 x i32>* %462, align 32
  %575 = icmp sgt <8 x i32> %539, %484
  %576 = select <8 x i1> %575, <8 x i32> %539, <8 x i32> %484
  %577 = icmp slt <8 x i32> %576, %487
  %578 = select <8 x i1> %577, <8 x i32> %576, <8 x i32> %487
  store <8 x i32> %578, <8 x i32>* %476, align 32
  %579 = icmp sgt <8 x i32> %538, %484
  %580 = select <8 x i1> %579, <8 x i32> %538, <8 x i32> %484
  %581 = icmp slt <8 x i32> %580, %487
  %582 = select <8 x i1> %581, <8 x i32> %580, <8 x i32> %487
  store <8 x i32> %582, <8 x i32>* %477, align 32
  %583 = icmp sgt <8 x i32> %537, %484
  %584 = select <8 x i1> %583, <8 x i32> %537, <8 x i32> %484
  %585 = icmp slt <8 x i32> %584, %487
  %586 = select <8 x i1> %585, <8 x i32> %584, <8 x i32> %487
  store <8 x i32> %586, <8 x i32>* %463, align 32
  %587 = icmp sgt <8 x i32> %536, %484
  %588 = select <8 x i1> %587, <8 x i32> %536, <8 x i32> %484
  %589 = icmp slt <8 x i32> %588, %487
  %590 = select <8 x i1> %589, <8 x i32> %588, <8 x i32> %487
  store <8 x i32> %590, <8 x i32>* %449, align 32
  %591 = icmp sgt <8 x i32> %535, %484
  %592 = select <8 x i1> %591, <8 x i32> %535, <8 x i32> %484
  %593 = icmp slt <8 x i32> %592, %487
  %594 = select <8 x i1> %593, <8 x i32> %592, <8 x i32> %487
  store <8 x i32> %594, <8 x i32>* %435, align 32
  %595 = icmp sgt <8 x i32> %534, %484
  %596 = select <8 x i1> %595, <8 x i32> %534, <8 x i32> %484
  %597 = icmp slt <8 x i32> %596, %487
  %598 = select <8 x i1> %597, <8 x i32> %596, <8 x i32> %487
  store <8 x i32> %598, <8 x i32>* %421, align 32
  %599 = icmp sgt <8 x i32> %533, %484
  %600 = select <8 x i1> %599, <8 x i32> %533, <8 x i32> %484
  %601 = icmp slt <8 x i32> %600, %487
  %602 = select <8 x i1> %601, <8 x i32> %600, <8 x i32> %487
  store <8 x i32> %602, <8 x i32>* %407, align 32
  %603 = icmp sgt <8 x i32> %532, %484
  %604 = select <8 x i1> %603, <8 x i32> %532, <8 x i32> %484
  %605 = icmp slt <8 x i32> %604, %487
  %606 = select <8 x i1> %605, <8 x i32> %604, <8 x i32> %487
  store <8 x i32> %606, <8 x i32>* %393, align 32
  %607 = icmp sgt <8 x i32> %531, %484
  %608 = select <8 x i1> %607, <8 x i32> %531, <8 x i32> %484
  %609 = icmp slt <8 x i32> %608, %487
  %610 = select <8 x i1> %609, <8 x i32> %608, <8 x i32> %487
  store <8 x i32> %610, <8 x i32>* %379, align 32
  br label %611

611:                                              ; preds = %530, %6
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @idct16_avx2(<4 x i64>* nocapture readonly, <4 x i64>*, i32, i32, i32, i32) #0 {
  %7 = add nsw i32 %2, -10
  %8 = sext i32 %7 to i64
  %9 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 60
  %10 = load i32, i32* %9, align 16
  %11 = insertelement <8 x i32> undef, i32 %10, i32 0
  %12 = shufflevector <8 x i32> %11, <8 x i32> undef, <8 x i32> zeroinitializer
  %13 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 4
  %14 = load i32, i32* %13, align 16
  %15 = sub nsw i32 0, %14
  %16 = insertelement <8 x i32> undef, i32 %15, i32 0
  %17 = shufflevector <8 x i32> %16, <8 x i32> undef, <8 x i32> zeroinitializer
  %18 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 28
  %19 = load i32, i32* %18, align 16
  %20 = insertelement <8 x i32> undef, i32 %19, i32 0
  %21 = shufflevector <8 x i32> %20, <8 x i32> undef, <8 x i32> zeroinitializer
  %22 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 36
  %23 = load i32, i32* %22, align 16
  %24 = sub nsw i32 0, %23
  %25 = insertelement <8 x i32> undef, i32 %24, i32 0
  %26 = shufflevector <8 x i32> %25, <8 x i32> undef, <8 x i32> zeroinitializer
  %27 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 44
  %28 = load i32, i32* %27, align 16
  %29 = insertelement <8 x i32> undef, i32 %28, i32 0
  %30 = shufflevector <8 x i32> %29, <8 x i32> undef, <8 x i32> zeroinitializer
  %31 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 20
  %32 = load i32, i32* %31, align 16
  %33 = insertelement <8 x i32> undef, i32 %32, i32 0
  %34 = shufflevector <8 x i32> %33, <8 x i32> undef, <8 x i32> zeroinitializer
  %35 = sub nsw i32 0, %32
  %36 = insertelement <8 x i32> undef, i32 %35, i32 0
  %37 = shufflevector <8 x i32> %36, <8 x i32> undef, <8 x i32> zeroinitializer
  %38 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 12
  %39 = load i32, i32* %38, align 16
  %40 = insertelement <8 x i32> undef, i32 %39, i32 0
  %41 = shufflevector <8 x i32> %40, <8 x i32> undef, <8 x i32> zeroinitializer
  %42 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 52
  %43 = load i32, i32* %42, align 16
  %44 = sub nsw i32 0, %43
  %45 = insertelement <8 x i32> undef, i32 %44, i32 0
  %46 = shufflevector <8 x i32> %45, <8 x i32> undef, <8 x i32> zeroinitializer
  %47 = insertelement <8 x i32> undef, i32 %43, i32 0
  %48 = shufflevector <8 x i32> %47, <8 x i32> undef, <8 x i32> zeroinitializer
  %49 = insertelement <8 x i32> undef, i32 %23, i32 0
  %50 = shufflevector <8 x i32> %49, <8 x i32> undef, <8 x i32> zeroinitializer
  %51 = insertelement <8 x i32> undef, i32 %14, i32 0
  %52 = shufflevector <8 x i32> %51, <8 x i32> undef, <8 x i32> zeroinitializer
  %53 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 56
  %54 = load i32, i32* %53, align 16
  %55 = insertelement <8 x i32> undef, i32 %54, i32 0
  %56 = shufflevector <8 x i32> %55, <8 x i32> undef, <8 x i32> zeroinitializer
  %57 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 8
  %58 = load i32, i32* %57, align 16
  %59 = sub nsw i32 0, %58
  %60 = insertelement <8 x i32> undef, i32 %59, i32 0
  %61 = shufflevector <8 x i32> %60, <8 x i32> undef, <8 x i32> zeroinitializer
  %62 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 24
  %63 = load i32, i32* %62, align 16
  %64 = insertelement <8 x i32> undef, i32 %63, i32 0
  %65 = shufflevector <8 x i32> %64, <8 x i32> undef, <8 x i32> zeroinitializer
  %66 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 40
  %67 = load i32, i32* %66, align 16
  %68 = sub nsw i32 0, %67
  %69 = insertelement <8 x i32> undef, i32 %68, i32 0
  %70 = shufflevector <8 x i32> %69, <8 x i32> undef, <8 x i32> zeroinitializer
  %71 = insertelement <8 x i32> undef, i32 %67, i32 0
  %72 = shufflevector <8 x i32> %71, <8 x i32> undef, <8 x i32> zeroinitializer
  %73 = insertelement <8 x i32> undef, i32 %58, i32 0
  %74 = shufflevector <8 x i32> %73, <8 x i32> undef, <8 x i32> zeroinitializer
  %75 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 32
  %76 = load i32, i32* %75, align 16
  %77 = insertelement <8 x i32> undef, i32 %76, i32 0
  %78 = shufflevector <8 x i32> %77, <8 x i32> undef, <8 x i32> zeroinitializer
  %79 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 48
  %80 = load i32, i32* %79, align 16
  %81 = insertelement <8 x i32> undef, i32 %80, i32 0
  %82 = shufflevector <8 x i32> %81, <8 x i32> undef, <8 x i32> zeroinitializer
  %83 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 16
  %84 = load i32, i32* %83, align 16
  %85 = insertelement <8 x i32> undef, i32 %84, i32 0
  %86 = shufflevector <8 x i32> %85, <8 x i32> undef, <8 x i32> zeroinitializer
  %87 = sub nsw i32 0, %84
  %88 = insertelement <8 x i32> undef, i32 %87, i32 0
  %89 = shufflevector <8 x i32> %88, <8 x i32> undef, <8 x i32> zeroinitializer
  %90 = sub nsw i32 0, %80
  %91 = insertelement <8 x i32> undef, i32 %90, i32 0
  %92 = shufflevector <8 x i32> %91, <8 x i32> undef, <8 x i32> zeroinitializer
  %93 = add nsw i32 %2, -1
  %94 = shl i32 1, %93
  %95 = insertelement <8 x i32> undef, i32 %94, i32 0
  %96 = shufflevector <8 x i32> %95, <8 x i32> undef, <8 x i32> zeroinitializer
  %97 = icmp ne i32 %3, 0
  %98 = select i1 %97, i32 6, i32 8
  %99 = add nsw i32 %98, %4
  %100 = icmp slt i32 %99, 16
  %101 = add i32 %99, -1
  %102 = shl i32 1, %101
  %103 = select i1 %100, i32 32768, i32 %102
  %104 = sub nsw i32 0, %103
  %105 = insertelement <8 x i32> undef, i32 %104, i32 0
  %106 = shufflevector <8 x i32> %105, <8 x i32> undef, <8 x i32> zeroinitializer
  %107 = add nsw i32 %103, -1
  %108 = insertelement <8 x i32> undef, i32 %107, i32 0
  %109 = shufflevector <8 x i32> %108, <8 x i32> undef, <8 x i32> zeroinitializer
  %110 = bitcast <4 x i64>* %0 to <8 x i32>*
  %111 = load <8 x i32>, <8 x i32>* %110, align 32
  %112 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 8
  %113 = bitcast <4 x i64>* %112 to <8 x i32>*
  %114 = load <8 x i32>, <8 x i32>* %113, align 32
  %115 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 4
  %116 = bitcast <4 x i64>* %115 to <8 x i32>*
  %117 = load <8 x i32>, <8 x i32>* %116, align 32
  %118 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 12
  %119 = bitcast <4 x i64>* %118 to <8 x i32>*
  %120 = load <8 x i32>, <8 x i32>* %119, align 32
  %121 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 2
  %122 = bitcast <4 x i64>* %121 to <8 x i32>*
  %123 = load <8 x i32>, <8 x i32>* %122, align 32
  %124 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 10
  %125 = bitcast <4 x i64>* %124 to <8 x i32>*
  %126 = load <8 x i32>, <8 x i32>* %125, align 32
  %127 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 6
  %128 = bitcast <4 x i64>* %127 to <8 x i32>*
  %129 = load <8 x i32>, <8 x i32>* %128, align 32
  %130 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 14
  %131 = bitcast <4 x i64>* %130 to <8 x i32>*
  %132 = load <8 x i32>, <8 x i32>* %131, align 32
  %133 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 1
  %134 = bitcast <4 x i64>* %133 to <8 x i32>*
  %135 = load <8 x i32>, <8 x i32>* %134, align 32
  %136 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 9
  %137 = bitcast <4 x i64>* %136 to <8 x i32>*
  %138 = load <8 x i32>, <8 x i32>* %137, align 32
  %139 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 5
  %140 = bitcast <4 x i64>* %139 to <8 x i32>*
  %141 = load <8 x i32>, <8 x i32>* %140, align 32
  %142 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 13
  %143 = bitcast <4 x i64>* %142 to <8 x i32>*
  %144 = load <8 x i32>, <8 x i32>* %143, align 32
  %145 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 3
  %146 = bitcast <4 x i64>* %145 to <8 x i32>*
  %147 = load <8 x i32>, <8 x i32>* %146, align 32
  %148 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 11
  %149 = bitcast <4 x i64>* %148 to <8 x i32>*
  %150 = load <8 x i32>, <8 x i32>* %149, align 32
  %151 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 7
  %152 = bitcast <4 x i64>* %151 to <8 x i32>*
  %153 = load <8 x i32>, <8 x i32>* %152, align 32
  %154 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 15
  %155 = bitcast <4 x i64>* %154 to <8 x i32>*
  %156 = load <8 x i32>, <8 x i32>* %155, align 32
  %157 = mul <8 x i32> %135, %12
  %158 = mul <8 x i32> %156, %17
  %159 = add <8 x i32> %157, %96
  %160 = add <8 x i32> %159, %158
  %161 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %160, i32 %2) #8
  %162 = mul <8 x i32> %138, %21
  %163 = mul <8 x i32> %153, %26
  %164 = add <8 x i32> %162, %96
  %165 = add <8 x i32> %164, %163
  %166 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %165, i32 %2) #8
  %167 = mul <8 x i32> %141, %30
  %168 = mul <8 x i32> %150, %37
  %169 = add <8 x i32> %167, %96
  %170 = add <8 x i32> %169, %168
  %171 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %170, i32 %2) #8
  %172 = mul <8 x i32> %144, %41
  %173 = mul <8 x i32> %147, %46
  %174 = add <8 x i32> %172, %96
  %175 = add <8 x i32> %174, %173
  %176 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %175, i32 %2) #8
  %177 = mul <8 x i32> %144, %48
  %178 = mul <8 x i32> %147, %41
  %179 = add <8 x i32> %177, %96
  %180 = add <8 x i32> %179, %178
  %181 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %180, i32 %2) #8
  %182 = mul <8 x i32> %141, %34
  %183 = mul <8 x i32> %150, %30
  %184 = add <8 x i32> %182, %96
  %185 = add <8 x i32> %184, %183
  %186 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %185, i32 %2) #8
  %187 = mul <8 x i32> %138, %50
  %188 = mul <8 x i32> %153, %21
  %189 = add <8 x i32> %187, %96
  %190 = add <8 x i32> %189, %188
  %191 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %190, i32 %2) #8
  %192 = mul <8 x i32> %135, %52
  %193 = mul <8 x i32> %156, %12
  %194 = add <8 x i32> %192, %96
  %195 = add <8 x i32> %194, %193
  %196 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %195, i32 %2) #8
  %197 = mul <8 x i32> %123, %56
  %198 = mul <8 x i32> %132, %61
  %199 = add <8 x i32> %197, %96
  %200 = add <8 x i32> %199, %198
  %201 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %200, i32 %2) #8
  %202 = mul <8 x i32> %126, %65
  %203 = mul <8 x i32> %129, %70
  %204 = add <8 x i32> %202, %96
  %205 = add <8 x i32> %204, %203
  %206 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %205, i32 %2) #8
  %207 = mul <8 x i32> %126, %72
  %208 = mul <8 x i32> %129, %65
  %209 = add <8 x i32> %207, %96
  %210 = add <8 x i32> %209, %208
  %211 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %210, i32 %2) #8
  %212 = mul <8 x i32> %123, %74
  %213 = mul <8 x i32> %132, %56
  %214 = add <8 x i32> %212, %96
  %215 = add <8 x i32> %214, %213
  %216 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %215, i32 %2) #8
  %217 = add <8 x i32> %166, %161
  %218 = sub <8 x i32> %161, %166
  %219 = icmp sgt <8 x i32> %217, %106
  %220 = select <8 x i1> %219, <8 x i32> %217, <8 x i32> %106
  %221 = icmp slt <8 x i32> %220, %109
  %222 = select <8 x i1> %221, <8 x i32> %220, <8 x i32> %109
  %223 = icmp sgt <8 x i32> %218, %106
  %224 = select <8 x i1> %223, <8 x i32> %218, <8 x i32> %106
  %225 = icmp slt <8 x i32> %224, %109
  %226 = select <8 x i1> %225, <8 x i32> %224, <8 x i32> %109
  %227 = add <8 x i32> %176, %171
  %228 = sub <8 x i32> %176, %171
  %229 = icmp sgt <8 x i32> %227, %106
  %230 = select <8 x i1> %229, <8 x i32> %227, <8 x i32> %106
  %231 = icmp slt <8 x i32> %230, %109
  %232 = select <8 x i1> %231, <8 x i32> %230, <8 x i32> %109
  %233 = icmp sgt <8 x i32> %228, %106
  %234 = select <8 x i1> %233, <8 x i32> %228, <8 x i32> %106
  %235 = icmp slt <8 x i32> %234, %109
  %236 = select <8 x i1> %235, <8 x i32> %234, <8 x i32> %109
  %237 = add <8 x i32> %186, %181
  %238 = sub <8 x i32> %181, %186
  %239 = icmp sgt <8 x i32> %237, %106
  %240 = select <8 x i1> %239, <8 x i32> %237, <8 x i32> %106
  %241 = icmp slt <8 x i32> %240, %109
  %242 = select <8 x i1> %241, <8 x i32> %240, <8 x i32> %109
  %243 = icmp sgt <8 x i32> %238, %106
  %244 = select <8 x i1> %243, <8 x i32> %238, <8 x i32> %106
  %245 = icmp slt <8 x i32> %244, %109
  %246 = select <8 x i1> %245, <8 x i32> %244, <8 x i32> %109
  %247 = add <8 x i32> %196, %191
  %248 = sub <8 x i32> %196, %191
  %249 = icmp sgt <8 x i32> %247, %106
  %250 = select <8 x i1> %249, <8 x i32> %247, <8 x i32> %106
  %251 = icmp slt <8 x i32> %250, %109
  %252 = select <8 x i1> %251, <8 x i32> %250, <8 x i32> %109
  %253 = icmp sgt <8 x i32> %248, %106
  %254 = select <8 x i1> %253, <8 x i32> %248, <8 x i32> %106
  %255 = icmp slt <8 x i32> %254, %109
  %256 = select <8 x i1> %255, <8 x i32> %254, <8 x i32> %109
  %257 = mul <8 x i32> %111, %78
  %258 = mul <8 x i32> %114, %78
  %259 = add <8 x i32> %257, %96
  %260 = add <8 x i32> %259, %258
  %261 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %260, i32 %2) #8
  %262 = sub <8 x i32> %259, %258
  %263 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %262, i32 %2) #8
  %264 = mul <8 x i32> %117, %82
  %265 = mul <8 x i32> %120, %89
  %266 = add <8 x i32> %264, %96
  %267 = add <8 x i32> %266, %265
  %268 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %267, i32 %2) #8
  %269 = mul <8 x i32> %117, %86
  %270 = mul <8 x i32> %120, %82
  %271 = add <8 x i32> %269, %96
  %272 = add <8 x i32> %271, %270
  %273 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %272, i32 %2) #8
  %274 = add <8 x i32> %206, %201
  %275 = sub <8 x i32> %201, %206
  %276 = icmp sgt <8 x i32> %274, %106
  %277 = select <8 x i1> %276, <8 x i32> %274, <8 x i32> %106
  %278 = icmp slt <8 x i32> %277, %109
  %279 = select <8 x i1> %278, <8 x i32> %277, <8 x i32> %109
  %280 = icmp sgt <8 x i32> %275, %106
  %281 = select <8 x i1> %280, <8 x i32> %275, <8 x i32> %106
  %282 = icmp slt <8 x i32> %281, %109
  %283 = select <8 x i1> %282, <8 x i32> %281, <8 x i32> %109
  %284 = add <8 x i32> %216, %211
  %285 = sub <8 x i32> %216, %211
  %286 = icmp sgt <8 x i32> %284, %106
  %287 = select <8 x i1> %286, <8 x i32> %284, <8 x i32> %106
  %288 = icmp slt <8 x i32> %287, %109
  %289 = select <8 x i1> %288, <8 x i32> %287, <8 x i32> %109
  %290 = icmp sgt <8 x i32> %285, %106
  %291 = select <8 x i1> %290, <8 x i32> %285, <8 x i32> %106
  %292 = icmp slt <8 x i32> %291, %109
  %293 = select <8 x i1> %292, <8 x i32> %291, <8 x i32> %109
  %294 = mul <8 x i32> %226, %89
  %295 = mul <8 x i32> %256, %82
  %296 = add <8 x i32> %294, %96
  %297 = add <8 x i32> %296, %295
  %298 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %297, i32 %2) #8
  %299 = mul <8 x i32> %236, %92
  %300 = mul <8 x i32> %246, %89
  %301 = add <8 x i32> %299, %96
  %302 = add <8 x i32> %301, %300
  %303 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %302, i32 %2) #8
  %304 = mul <8 x i32> %236, %89
  %305 = mul <8 x i32> %246, %82
  %306 = add <8 x i32> %304, %96
  %307 = add <8 x i32> %306, %305
  %308 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %307, i32 %2) #8
  %309 = mul <8 x i32> %226, %82
  %310 = mul <8 x i32> %256, %86
  %311 = add <8 x i32> %309, %96
  %312 = add <8 x i32> %311, %310
  %313 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %312, i32 %2) #8
  %314 = add <8 x i32> %273, %261
  %315 = sub <8 x i32> %261, %273
  %316 = icmp sgt <8 x i32> %314, %106
  %317 = select <8 x i1> %316, <8 x i32> %314, <8 x i32> %106
  %318 = icmp slt <8 x i32> %317, %109
  %319 = select <8 x i1> %318, <8 x i32> %317, <8 x i32> %109
  %320 = icmp sgt <8 x i32> %315, %106
  %321 = select <8 x i1> %320, <8 x i32> %315, <8 x i32> %106
  %322 = icmp slt <8 x i32> %321, %109
  %323 = select <8 x i1> %322, <8 x i32> %321, <8 x i32> %109
  %324 = add <8 x i32> %268, %263
  %325 = sub <8 x i32> %263, %268
  %326 = icmp sgt <8 x i32> %324, %106
  %327 = select <8 x i1> %326, <8 x i32> %324, <8 x i32> %106
  %328 = icmp slt <8 x i32> %327, %109
  %329 = select <8 x i1> %328, <8 x i32> %327, <8 x i32> %109
  %330 = icmp sgt <8 x i32> %325, %106
  %331 = select <8 x i1> %330, <8 x i32> %325, <8 x i32> %106
  %332 = icmp slt <8 x i32> %331, %109
  %333 = select <8 x i1> %332, <8 x i32> %331, <8 x i32> %109
  %334 = mul <8 x i32> %283, %78
  %335 = mul <8 x i32> %293, %78
  %336 = sub <8 x i32> %96, %334
  %337 = add <8 x i32> %336, %335
  %338 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %337, i32 %2) #8
  %339 = add <8 x i32> %334, %96
  %340 = add <8 x i32> %339, %335
  %341 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %340, i32 %2) #8
  %342 = add <8 x i32> %232, %222
  %343 = sub <8 x i32> %222, %232
  %344 = icmp sgt <8 x i32> %342, %106
  %345 = select <8 x i1> %344, <8 x i32> %342, <8 x i32> %106
  %346 = icmp slt <8 x i32> %345, %109
  %347 = select <8 x i1> %346, <8 x i32> %345, <8 x i32> %109
  %348 = icmp sgt <8 x i32> %343, %106
  %349 = select <8 x i1> %348, <8 x i32> %343, <8 x i32> %106
  %350 = icmp slt <8 x i32> %349, %109
  %351 = select <8 x i1> %350, <8 x i32> %349, <8 x i32> %109
  %352 = add <8 x i32> %303, %298
  %353 = sub <8 x i32> %298, %303
  %354 = icmp sgt <8 x i32> %352, %106
  %355 = select <8 x i1> %354, <8 x i32> %352, <8 x i32> %106
  %356 = icmp slt <8 x i32> %355, %109
  %357 = select <8 x i1> %356, <8 x i32> %355, <8 x i32> %109
  %358 = icmp sgt <8 x i32> %353, %106
  %359 = select <8 x i1> %358, <8 x i32> %353, <8 x i32> %106
  %360 = icmp slt <8 x i32> %359, %109
  %361 = select <8 x i1> %360, <8 x i32> %359, <8 x i32> %109
  %362 = add <8 x i32> %252, %242
  %363 = sub <8 x i32> %252, %242
  %364 = icmp sgt <8 x i32> %362, %106
  %365 = select <8 x i1> %364, <8 x i32> %362, <8 x i32> %106
  %366 = icmp slt <8 x i32> %365, %109
  %367 = select <8 x i1> %366, <8 x i32> %365, <8 x i32> %109
  %368 = icmp sgt <8 x i32> %363, %106
  %369 = select <8 x i1> %368, <8 x i32> %363, <8 x i32> %106
  %370 = icmp slt <8 x i32> %369, %109
  %371 = select <8 x i1> %370, <8 x i32> %369, <8 x i32> %109
  %372 = add <8 x i32> %313, %308
  %373 = sub <8 x i32> %313, %308
  %374 = icmp sgt <8 x i32> %372, %106
  %375 = select <8 x i1> %374, <8 x i32> %372, <8 x i32> %106
  %376 = icmp slt <8 x i32> %375, %109
  %377 = select <8 x i1> %376, <8 x i32> %375, <8 x i32> %109
  %378 = icmp sgt <8 x i32> %373, %106
  %379 = select <8 x i1> %378, <8 x i32> %373, <8 x i32> %106
  %380 = icmp slt <8 x i32> %379, %109
  %381 = select <8 x i1> %380, <8 x i32> %379, <8 x i32> %109
  %382 = add <8 x i32> %319, %289
  %383 = sub <8 x i32> %319, %289
  %384 = icmp sgt <8 x i32> %382, %106
  %385 = select <8 x i1> %384, <8 x i32> %382, <8 x i32> %106
  %386 = icmp slt <8 x i32> %385, %109
  %387 = select <8 x i1> %386, <8 x i32> %385, <8 x i32> %109
  %388 = icmp sgt <8 x i32> %383, %106
  %389 = select <8 x i1> %388, <8 x i32> %383, <8 x i32> %106
  %390 = icmp slt <8 x i32> %389, %109
  %391 = select <8 x i1> %390, <8 x i32> %389, <8 x i32> %109
  %392 = add <8 x i32> %341, %329
  %393 = sub <8 x i32> %329, %341
  %394 = icmp sgt <8 x i32> %392, %106
  %395 = select <8 x i1> %394, <8 x i32> %392, <8 x i32> %106
  %396 = icmp slt <8 x i32> %395, %109
  %397 = select <8 x i1> %396, <8 x i32> %395, <8 x i32> %109
  %398 = icmp sgt <8 x i32> %393, %106
  %399 = select <8 x i1> %398, <8 x i32> %393, <8 x i32> %106
  %400 = icmp slt <8 x i32> %399, %109
  %401 = select <8 x i1> %400, <8 x i32> %399, <8 x i32> %109
  %402 = add <8 x i32> %338, %333
  %403 = sub <8 x i32> %333, %338
  %404 = icmp sgt <8 x i32> %402, %106
  %405 = select <8 x i1> %404, <8 x i32> %402, <8 x i32> %106
  %406 = icmp slt <8 x i32> %405, %109
  %407 = select <8 x i1> %406, <8 x i32> %405, <8 x i32> %109
  %408 = icmp sgt <8 x i32> %403, %106
  %409 = select <8 x i1> %408, <8 x i32> %403, <8 x i32> %106
  %410 = icmp slt <8 x i32> %409, %109
  %411 = select <8 x i1> %410, <8 x i32> %409, <8 x i32> %109
  %412 = add <8 x i32> %323, %279
  %413 = sub <8 x i32> %323, %279
  %414 = icmp sgt <8 x i32> %412, %106
  %415 = select <8 x i1> %414, <8 x i32> %412, <8 x i32> %106
  %416 = icmp slt <8 x i32> %415, %109
  %417 = select <8 x i1> %416, <8 x i32> %415, <8 x i32> %109
  %418 = icmp sgt <8 x i32> %413, %106
  %419 = select <8 x i1> %418, <8 x i32> %413, <8 x i32> %106
  %420 = icmp slt <8 x i32> %419, %109
  %421 = select <8 x i1> %420, <8 x i32> %419, <8 x i32> %109
  %422 = mul <8 x i32> %361, %78
  %423 = mul <8 x i32> %381, %78
  %424 = sub <8 x i32> %96, %422
  %425 = add <8 x i32> %424, %423
  %426 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %425, i32 %2) #8
  %427 = add <8 x i32> %422, %96
  %428 = add <8 x i32> %427, %423
  %429 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %428, i32 %2) #8
  %430 = mul <8 x i32> %351, %78
  %431 = mul <8 x i32> %371, %78
  %432 = sub <8 x i32> %96, %430
  %433 = add <8 x i32> %432, %431
  %434 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %433, i32 %2) #8
  %435 = add <8 x i32> %430, %96
  %436 = add <8 x i32> %435, %431
  %437 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %436, i32 %2) #8
  %438 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 15
  %439 = add <8 x i32> %387, %367
  %440 = sub <8 x i32> %387, %367
  %441 = icmp sgt <8 x i32> %439, %106
  %442 = select <8 x i1> %441, <8 x i32> %439, <8 x i32> %106
  %443 = icmp slt <8 x i32> %442, %109
  %444 = select <8 x i1> %443, <8 x i32> %442, <8 x i32> %109
  %445 = icmp sgt <8 x i32> %440, %106
  %446 = select <8 x i1> %445, <8 x i32> %440, <8 x i32> %106
  %447 = icmp slt <8 x i32> %446, %109
  %448 = select <8 x i1> %447, <8 x i32> %446, <8 x i32> %109
  %449 = bitcast <4 x i64>* %1 to <8 x i32>*
  store <8 x i32> %444, <8 x i32>* %449, align 32
  %450 = bitcast <4 x i64>* %438 to <8 x i32>*
  store <8 x i32> %448, <8 x i32>* %450, align 32
  %451 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %452 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %453 = add <8 x i32> %397, %377
  %454 = sub <8 x i32> %397, %377
  %455 = icmp sgt <8 x i32> %453, %106
  %456 = select <8 x i1> %455, <8 x i32> %453, <8 x i32> %106
  %457 = icmp slt <8 x i32> %456, %109
  %458 = select <8 x i1> %457, <8 x i32> %456, <8 x i32> %109
  %459 = icmp sgt <8 x i32> %454, %106
  %460 = select <8 x i1> %459, <8 x i32> %454, <8 x i32> %106
  %461 = icmp slt <8 x i32> %460, %109
  %462 = select <8 x i1> %461, <8 x i32> %460, <8 x i32> %109
  %463 = bitcast <4 x i64>* %451 to <8 x i32>*
  store <8 x i32> %458, <8 x i32>* %463, align 32
  %464 = bitcast <4 x i64>* %452 to <8 x i32>*
  store <8 x i32> %462, <8 x i32>* %464, align 32
  %465 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %466 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %467 = add <8 x i32> %407, %429
  %468 = sub <8 x i32> %407, %429
  %469 = icmp sgt <8 x i32> %467, %106
  %470 = select <8 x i1> %469, <8 x i32> %467, <8 x i32> %106
  %471 = icmp slt <8 x i32> %470, %109
  %472 = select <8 x i1> %471, <8 x i32> %470, <8 x i32> %109
  %473 = icmp sgt <8 x i32> %468, %106
  %474 = select <8 x i1> %473, <8 x i32> %468, <8 x i32> %106
  %475 = icmp slt <8 x i32> %474, %109
  %476 = select <8 x i1> %475, <8 x i32> %474, <8 x i32> %109
  %477 = bitcast <4 x i64>* %465 to <8 x i32>*
  store <8 x i32> %472, <8 x i32>* %477, align 32
  %478 = bitcast <4 x i64>* %466 to <8 x i32>*
  store <8 x i32> %476, <8 x i32>* %478, align 32
  %479 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %480 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %481 = add <8 x i32> %437, %417
  %482 = sub <8 x i32> %417, %437
  %483 = icmp sgt <8 x i32> %481, %106
  %484 = select <8 x i1> %483, <8 x i32> %481, <8 x i32> %106
  %485 = icmp slt <8 x i32> %484, %109
  %486 = select <8 x i1> %485, <8 x i32> %484, <8 x i32> %109
  %487 = icmp sgt <8 x i32> %482, %106
  %488 = select <8 x i1> %487, <8 x i32> %482, <8 x i32> %106
  %489 = icmp slt <8 x i32> %488, %109
  %490 = select <8 x i1> %489, <8 x i32> %488, <8 x i32> %109
  %491 = bitcast <4 x i64>* %479 to <8 x i32>*
  store <8 x i32> %486, <8 x i32>* %491, align 32
  %492 = bitcast <4 x i64>* %480 to <8 x i32>*
  store <8 x i32> %490, <8 x i32>* %492, align 32
  %493 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %494 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %495 = add <8 x i32> %421, %434
  %496 = sub <8 x i32> %421, %434
  %497 = icmp sgt <8 x i32> %495, %106
  %498 = select <8 x i1> %497, <8 x i32> %495, <8 x i32> %106
  %499 = icmp slt <8 x i32> %498, %109
  %500 = select <8 x i1> %499, <8 x i32> %498, <8 x i32> %109
  %501 = icmp sgt <8 x i32> %496, %106
  %502 = select <8 x i1> %501, <8 x i32> %496, <8 x i32> %106
  %503 = icmp slt <8 x i32> %502, %109
  %504 = select <8 x i1> %503, <8 x i32> %502, <8 x i32> %109
  %505 = bitcast <4 x i64>* %493 to <8 x i32>*
  store <8 x i32> %500, <8 x i32>* %505, align 32
  %506 = bitcast <4 x i64>* %494 to <8 x i32>*
  store <8 x i32> %504, <8 x i32>* %506, align 32
  %507 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %508 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %509 = add <8 x i32> %411, %426
  %510 = sub <8 x i32> %411, %426
  %511 = icmp sgt <8 x i32> %509, %106
  %512 = select <8 x i1> %511, <8 x i32> %509, <8 x i32> %106
  %513 = icmp slt <8 x i32> %512, %109
  %514 = select <8 x i1> %513, <8 x i32> %512, <8 x i32> %109
  %515 = icmp sgt <8 x i32> %510, %106
  %516 = select <8 x i1> %515, <8 x i32> %510, <8 x i32> %106
  %517 = icmp slt <8 x i32> %516, %109
  %518 = select <8 x i1> %517, <8 x i32> %516, <8 x i32> %109
  %519 = bitcast <4 x i64>* %507 to <8 x i32>*
  store <8 x i32> %514, <8 x i32>* %519, align 32
  %520 = bitcast <4 x i64>* %508 to <8 x i32>*
  store <8 x i32> %518, <8 x i32>* %520, align 32
  %521 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %522 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %523 = add <8 x i32> %401, %357
  %524 = sub <8 x i32> %401, %357
  %525 = icmp sgt <8 x i32> %523, %106
  %526 = select <8 x i1> %525, <8 x i32> %523, <8 x i32> %106
  %527 = icmp slt <8 x i32> %526, %109
  %528 = select <8 x i1> %527, <8 x i32> %526, <8 x i32> %109
  %529 = icmp sgt <8 x i32> %524, %106
  %530 = select <8 x i1> %529, <8 x i32> %524, <8 x i32> %106
  %531 = icmp slt <8 x i32> %530, %109
  %532 = select <8 x i1> %531, <8 x i32> %530, <8 x i32> %109
  %533 = bitcast <4 x i64>* %521 to <8 x i32>*
  store <8 x i32> %528, <8 x i32>* %533, align 32
  %534 = bitcast <4 x i64>* %522 to <8 x i32>*
  store <8 x i32> %532, <8 x i32>* %534, align 32
  %535 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %536 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %537 = add <8 x i32> %391, %347
  %538 = sub <8 x i32> %391, %347
  %539 = icmp sgt <8 x i32> %537, %106
  %540 = select <8 x i1> %539, <8 x i32> %537, <8 x i32> %106
  %541 = icmp slt <8 x i32> %540, %109
  %542 = select <8 x i1> %541, <8 x i32> %540, <8 x i32> %109
  %543 = icmp sgt <8 x i32> %538, %106
  %544 = select <8 x i1> %543, <8 x i32> %538, <8 x i32> %106
  %545 = icmp slt <8 x i32> %544, %109
  %546 = select <8 x i1> %545, <8 x i32> %544, <8 x i32> %109
  %547 = bitcast <4 x i64>* %535 to <8 x i32>*
  store <8 x i32> %542, <8 x i32>* %547, align 32
  %548 = bitcast <4 x i64>* %536 to <8 x i32>*
  store <8 x i32> %546, <8 x i32>* %548, align 32
  br i1 %97, label %682, label %549

549:                                              ; preds = %6
  %550 = icmp sgt i32 %4, 10
  %551 = select i1 %550, i32 %4, i32 10
  %552 = shl i32 32, %551
  %553 = sub nsw i32 0, %552
  %554 = insertelement <8 x i32> undef, i32 %553, i32 0
  %555 = shufflevector <8 x i32> %554, <8 x i32> undef, <8 x i32> zeroinitializer
  %556 = add nsw i32 %552, -1
  %557 = insertelement <8 x i32> undef, i32 %556, i32 0
  %558 = shufflevector <8 x i32> %557, <8 x i32> undef, <8 x i32> zeroinitializer
  %559 = icmp eq i32 %5, 0
  br i1 %559, label %560, label %564

560:                                              ; preds = %549
  %561 = load <8 x i32>, <8 x i32>* %449, align 32
  %562 = load <8 x i32>, <8 x i32>* %463, align 32
  %563 = load <8 x i32>, <8 x i32>* %450, align 32
  br label %601

564:                                              ; preds = %549
  %565 = add nsw i32 %5, -1
  %566 = shl i32 1, %565
  %567 = insertelement <8 x i32> undef, i32 %566, i32 0
  %568 = shufflevector <8 x i32> %567, <8 x i32> undef, <8 x i32> zeroinitializer
  %569 = add <8 x i32> %444, %568
  %570 = add <8 x i32> %458, %568
  %571 = add <8 x i32> %472, %568
  %572 = add <8 x i32> %486, %568
  %573 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %569, i32 %5) #8
  store <8 x i32> %573, <8 x i32>* %449, align 32
  %574 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %570, i32 %5) #8
  store <8 x i32> %574, <8 x i32>* %463, align 32
  %575 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %571, i32 %5) #8
  store <8 x i32> %575, <8 x i32>* %477, align 32
  %576 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %572, i32 %5) #8
  store <8 x i32> %576, <8 x i32>* %491, align 32
  %577 = add <8 x i32> %500, %568
  %578 = add <8 x i32> %514, %568
  %579 = add <8 x i32> %528, %568
  %580 = add <8 x i32> %542, %568
  %581 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %577, i32 %5) #8
  store <8 x i32> %581, <8 x i32>* %505, align 32
  %582 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %578, i32 %5) #8
  store <8 x i32> %582, <8 x i32>* %519, align 32
  %583 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %579, i32 %5) #8
  store <8 x i32> %583, <8 x i32>* %533, align 32
  %584 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %580, i32 %5) #8
  store <8 x i32> %584, <8 x i32>* %547, align 32
  %585 = add <8 x i32> %546, %568
  %586 = add <8 x i32> %532, %568
  %587 = add <8 x i32> %518, %568
  %588 = add <8 x i32> %504, %568
  %589 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %585, i32 %5) #8
  store <8 x i32> %589, <8 x i32>* %548, align 32
  %590 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %586, i32 %5) #8
  store <8 x i32> %590, <8 x i32>* %534, align 32
  %591 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %587, i32 %5) #8
  store <8 x i32> %591, <8 x i32>* %520, align 32
  %592 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %588, i32 %5) #8
  store <8 x i32> %592, <8 x i32>* %506, align 32
  %593 = add <8 x i32> %490, %568
  %594 = add <8 x i32> %476, %568
  %595 = add <8 x i32> %462, %568
  %596 = add <8 x i32> %448, %568
  %597 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %593, i32 %5) #8
  store <8 x i32> %597, <8 x i32>* %492, align 32
  %598 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %594, i32 %5) #8
  store <8 x i32> %598, <8 x i32>* %478, align 32
  %599 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %595, i32 %5) #8
  store <8 x i32> %599, <8 x i32>* %464, align 32
  %600 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %596, i32 %5) #8
  store <8 x i32> %600, <8 x i32>* %450, align 32
  br label %601

601:                                              ; preds = %560, %564
  %602 = phi <8 x i32> [ %563, %560 ], [ %600, %564 ]
  %603 = phi <8 x i32> [ %462, %560 ], [ %599, %564 ]
  %604 = phi <8 x i32> [ %476, %560 ], [ %598, %564 ]
  %605 = phi <8 x i32> [ %490, %560 ], [ %597, %564 ]
  %606 = phi <8 x i32> [ %504, %560 ], [ %592, %564 ]
  %607 = phi <8 x i32> [ %518, %560 ], [ %591, %564 ]
  %608 = phi <8 x i32> [ %532, %560 ], [ %590, %564 ]
  %609 = phi <8 x i32> [ %546, %560 ], [ %589, %564 ]
  %610 = phi <8 x i32> [ %542, %560 ], [ %584, %564 ]
  %611 = phi <8 x i32> [ %528, %560 ], [ %583, %564 ]
  %612 = phi <8 x i32> [ %514, %560 ], [ %582, %564 ]
  %613 = phi <8 x i32> [ %500, %560 ], [ %581, %564 ]
  %614 = phi <8 x i32> [ %486, %560 ], [ %576, %564 ]
  %615 = phi <8 x i32> [ %472, %560 ], [ %575, %564 ]
  %616 = phi <8 x i32> [ %562, %560 ], [ %574, %564 ]
  %617 = phi <8 x i32> [ %561, %560 ], [ %573, %564 ]
  %618 = icmp sgt <8 x i32> %617, %555
  %619 = select <8 x i1> %618, <8 x i32> %617, <8 x i32> %555
  %620 = icmp slt <8 x i32> %619, %558
  %621 = select <8 x i1> %620, <8 x i32> %619, <8 x i32> %558
  store <8 x i32> %621, <8 x i32>* %449, align 32
  %622 = icmp sgt <8 x i32> %616, %555
  %623 = select <8 x i1> %622, <8 x i32> %616, <8 x i32> %555
  %624 = icmp slt <8 x i32> %623, %558
  %625 = select <8 x i1> %624, <8 x i32> %623, <8 x i32> %558
  store <8 x i32> %625, <8 x i32>* %463, align 32
  %626 = icmp sgt <8 x i32> %615, %555
  %627 = select <8 x i1> %626, <8 x i32> %615, <8 x i32> %555
  %628 = icmp slt <8 x i32> %627, %558
  %629 = select <8 x i1> %628, <8 x i32> %627, <8 x i32> %558
  store <8 x i32> %629, <8 x i32>* %477, align 32
  %630 = icmp sgt <8 x i32> %614, %555
  %631 = select <8 x i1> %630, <8 x i32> %614, <8 x i32> %555
  %632 = icmp slt <8 x i32> %631, %558
  %633 = select <8 x i1> %632, <8 x i32> %631, <8 x i32> %558
  store <8 x i32> %633, <8 x i32>* %491, align 32
  %634 = icmp sgt <8 x i32> %613, %555
  %635 = select <8 x i1> %634, <8 x i32> %613, <8 x i32> %555
  %636 = icmp slt <8 x i32> %635, %558
  %637 = select <8 x i1> %636, <8 x i32> %635, <8 x i32> %558
  store <8 x i32> %637, <8 x i32>* %505, align 32
  %638 = icmp sgt <8 x i32> %612, %555
  %639 = select <8 x i1> %638, <8 x i32> %612, <8 x i32> %555
  %640 = icmp slt <8 x i32> %639, %558
  %641 = select <8 x i1> %640, <8 x i32> %639, <8 x i32> %558
  store <8 x i32> %641, <8 x i32>* %519, align 32
  %642 = icmp sgt <8 x i32> %611, %555
  %643 = select <8 x i1> %642, <8 x i32> %611, <8 x i32> %555
  %644 = icmp slt <8 x i32> %643, %558
  %645 = select <8 x i1> %644, <8 x i32> %643, <8 x i32> %558
  store <8 x i32> %645, <8 x i32>* %533, align 32
  %646 = icmp sgt <8 x i32> %610, %555
  %647 = select <8 x i1> %646, <8 x i32> %610, <8 x i32> %555
  %648 = icmp slt <8 x i32> %647, %558
  %649 = select <8 x i1> %648, <8 x i32> %647, <8 x i32> %558
  store <8 x i32> %649, <8 x i32>* %547, align 32
  %650 = icmp sgt <8 x i32> %609, %555
  %651 = select <8 x i1> %650, <8 x i32> %609, <8 x i32> %555
  %652 = icmp slt <8 x i32> %651, %558
  %653 = select <8 x i1> %652, <8 x i32> %651, <8 x i32> %558
  store <8 x i32> %653, <8 x i32>* %548, align 32
  %654 = icmp sgt <8 x i32> %608, %555
  %655 = select <8 x i1> %654, <8 x i32> %608, <8 x i32> %555
  %656 = icmp slt <8 x i32> %655, %558
  %657 = select <8 x i1> %656, <8 x i32> %655, <8 x i32> %558
  store <8 x i32> %657, <8 x i32>* %534, align 32
  %658 = icmp sgt <8 x i32> %607, %555
  %659 = select <8 x i1> %658, <8 x i32> %607, <8 x i32> %555
  %660 = icmp slt <8 x i32> %659, %558
  %661 = select <8 x i1> %660, <8 x i32> %659, <8 x i32> %558
  store <8 x i32> %661, <8 x i32>* %520, align 32
  %662 = icmp sgt <8 x i32> %606, %555
  %663 = select <8 x i1> %662, <8 x i32> %606, <8 x i32> %555
  %664 = icmp slt <8 x i32> %663, %558
  %665 = select <8 x i1> %664, <8 x i32> %663, <8 x i32> %558
  store <8 x i32> %665, <8 x i32>* %506, align 32
  %666 = icmp sgt <8 x i32> %605, %555
  %667 = select <8 x i1> %666, <8 x i32> %605, <8 x i32> %555
  %668 = icmp slt <8 x i32> %667, %558
  %669 = select <8 x i1> %668, <8 x i32> %667, <8 x i32> %558
  store <8 x i32> %669, <8 x i32>* %492, align 32
  %670 = icmp sgt <8 x i32> %604, %555
  %671 = select <8 x i1> %670, <8 x i32> %604, <8 x i32> %555
  %672 = icmp slt <8 x i32> %671, %558
  %673 = select <8 x i1> %672, <8 x i32> %671, <8 x i32> %558
  store <8 x i32> %673, <8 x i32>* %478, align 32
  %674 = icmp sgt <8 x i32> %603, %555
  %675 = select <8 x i1> %674, <8 x i32> %603, <8 x i32> %555
  %676 = icmp slt <8 x i32> %675, %558
  %677 = select <8 x i1> %676, <8 x i32> %675, <8 x i32> %558
  store <8 x i32> %677, <8 x i32>* %464, align 32
  %678 = icmp sgt <8 x i32> %602, %555
  %679 = select <8 x i1> %678, <8 x i32> %602, <8 x i32> %555
  %680 = icmp slt <8 x i32> %679, %558
  %681 = select <8 x i1> %680, <8 x i32> %679, <8 x i32> %558
  store <8 x i32> %681, <8 x i32>* %450, align 32
  br label %682

682:                                              ; preds = %601, %6
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @iadst16_low1_avx2(<4 x i64>* nocapture readonly, <4 x i64>*, i32, i32, i32, i32) #0 {
  %7 = add nsw i32 %2, -10
  %8 = sext i32 %7 to i64
  %9 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 2
  %10 = load i32, i32* %9, align 8
  %11 = insertelement <8 x i32> undef, i32 %10, i32 0
  %12 = shufflevector <8 x i32> %11, <8 x i32> undef, <8 x i32> zeroinitializer
  %13 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 62
  %14 = load i32, i32* %13, align 8
  %15 = insertelement <8 x i32> undef, i32 %14, i32 0
  %16 = shufflevector <8 x i32> %15, <8 x i32> undef, <8 x i32> zeroinitializer
  %17 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 8
  %18 = load i32, i32* %17, align 16
  %19 = insertelement <8 x i32> undef, i32 %18, i32 0
  %20 = shufflevector <8 x i32> %19, <8 x i32> undef, <8 x i32> zeroinitializer
  %21 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 56
  %22 = load i32, i32* %21, align 16
  %23 = insertelement <8 x i32> undef, i32 %22, i32 0
  %24 = shufflevector <8 x i32> %23, <8 x i32> undef, <8 x i32> zeroinitializer
  %25 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 48
  %26 = load i32, i32* %25, align 16
  %27 = insertelement <8 x i32> undef, i32 %26, i32 0
  %28 = shufflevector <8 x i32> %27, <8 x i32> undef, <8 x i32> zeroinitializer
  %29 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 16
  %30 = load i32, i32* %29, align 16
  %31 = insertelement <8 x i32> undef, i32 %30, i32 0
  %32 = shufflevector <8 x i32> %31, <8 x i32> undef, <8 x i32> zeroinitializer
  %33 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 32
  %34 = load i32, i32* %33, align 16
  %35 = insertelement <8 x i32> undef, i32 %34, i32 0
  %36 = shufflevector <8 x i32> %35, <8 x i32> undef, <8 x i32> zeroinitializer
  %37 = add nsw i32 %2, -1
  %38 = shl i32 1, %37
  %39 = insertelement <8 x i32> undef, i32 %38, i32 0
  %40 = shufflevector <8 x i32> %39, <8 x i32> undef, <8 x i32> zeroinitializer
  %41 = bitcast <4 x i64>* %0 to <8 x i32>*
  %42 = load <8 x i32>, <8 x i32>* %41, align 32
  %43 = mul <8 x i32> %42, %16
  %44 = add <8 x i32> %43, %40
  %45 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %44, i32 %2) #8
  %46 = mul <8 x i32> %42, %12
  %47 = sub <8 x i32> %40, %46
  %48 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %47, i32 %2) #8
  %49 = mul <8 x i32> %45, %20
  %50 = mul <8 x i32> %48, %24
  %51 = add <8 x i32> %49, %40
  %52 = add <8 x i32> %51, %50
  %53 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %52, i32 %2) #8
  %54 = mul <8 x i32> %45, %24
  %55 = mul <8 x i32> %20, %48
  %56 = add <8 x i32> %54, %40
  %57 = sub <8 x i32> %56, %55
  %58 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %57, i32 %2) #8
  %59 = mul <8 x i32> %45, %32
  %60 = mul <8 x i32> %48, %28
  %61 = add <8 x i32> %59, %40
  %62 = add <8 x i32> %61, %60
  %63 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %62, i32 %2) #8
  %64 = mul <8 x i32> %45, %28
  %65 = mul <8 x i32> %32, %48
  %66 = add <8 x i32> %64, %40
  %67 = sub <8 x i32> %66, %65
  %68 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %67, i32 %2) #8
  %69 = mul <8 x i32> %53, %32
  %70 = mul <8 x i32> %58, %28
  %71 = add <8 x i32> %69, %40
  %72 = add <8 x i32> %71, %70
  %73 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %72, i32 %2) #8
  %74 = mul <8 x i32> %53, %28
  %75 = mul <8 x i32> %32, %58
  %76 = add <8 x i32> %74, %40
  %77 = sub <8 x i32> %76, %75
  %78 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %77, i32 %2) #8
  %79 = mul <8 x i32> %36, %45
  %80 = mul <8 x i32> %48, %36
  %81 = add <8 x i32> %79, %40
  %82 = add <8 x i32> %81, %80
  %83 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %82, i32 %2) #8
  %84 = sub <8 x i32> %81, %80
  %85 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %84, i32 %2) #8
  %86 = mul <8 x i32> %63, %36
  %87 = mul <8 x i32> %68, %36
  %88 = add <8 x i32> %86, %40
  %89 = add <8 x i32> %88, %87
  %90 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %89, i32 %2) #8
  %91 = sub <8 x i32> %88, %87
  %92 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %91, i32 %2) #8
  %93 = mul <8 x i32> %53, %36
  %94 = mul <8 x i32> %58, %36
  %95 = add <8 x i32> %93, %40
  %96 = add <8 x i32> %95, %94
  %97 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %96, i32 %2) #8
  %98 = sub <8 x i32> %95, %94
  %99 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %98, i32 %2) #8
  %100 = mul <8 x i32> %73, %36
  %101 = mul <8 x i32> %78, %36
  %102 = add <8 x i32> %100, %40
  %103 = add <8 x i32> %102, %101
  %104 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %103, i32 %2) #8
  %105 = sub <8 x i32> %102, %101
  %106 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %105, i32 %2) #8
  %107 = icmp eq i32 %3, 0
  br i1 %107, label %146, label %108

108:                                              ; preds = %6
  %109 = bitcast <4 x i64>* %1 to <8 x i32>*
  store <8 x i32> %45, <8 x i32>* %109, align 32
  %110 = sub <8 x i32> zeroinitializer, %53
  %111 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %112 = bitcast <4 x i64>* %111 to <8 x i32>*
  store <8 x i32> %110, <8 x i32>* %112, align 32
  %113 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %114 = bitcast <4 x i64>* %113 to <8 x i32>*
  store <8 x i32> %73, <8 x i32>* %114, align 32
  %115 = sub <8 x i32> zeroinitializer, %63
  %116 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %117 = bitcast <4 x i64>* %116 to <8 x i32>*
  store <8 x i32> %115, <8 x i32>* %117, align 32
  %118 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %119 = bitcast <4 x i64>* %118 to <8 x i32>*
  store <8 x i32> %90, <8 x i32>* %119, align 32
  %120 = sub <8 x i32> zeroinitializer, %104
  %121 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %122 = bitcast <4 x i64>* %121 to <8 x i32>*
  store <8 x i32> %120, <8 x i32>* %122, align 32
  %123 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %124 = bitcast <4 x i64>* %123 to <8 x i32>*
  store <8 x i32> %97, <8 x i32>* %124, align 32
  %125 = sub <8 x i32> zeroinitializer, %83
  %126 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %127 = bitcast <4 x i64>* %126 to <8 x i32>*
  store <8 x i32> %125, <8 x i32>* %127, align 32
  %128 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %129 = bitcast <4 x i64>* %128 to <8 x i32>*
  store <8 x i32> %85, <8 x i32>* %129, align 32
  %130 = sub <8 x i32> zeroinitializer, %99
  %131 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %132 = bitcast <4 x i64>* %131 to <8 x i32>*
  store <8 x i32> %130, <8 x i32>* %132, align 32
  %133 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %134 = bitcast <4 x i64>* %133 to <8 x i32>*
  store <8 x i32> %106, <8 x i32>* %134, align 32
  %135 = sub <8 x i32> zeroinitializer, %92
  %136 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %137 = bitcast <4 x i64>* %136 to <8 x i32>*
  store <8 x i32> %135, <8 x i32>* %137, align 32
  %138 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %139 = bitcast <4 x i64>* %138 to <8 x i32>*
  store <8 x i32> %68, <8 x i32>* %139, align 32
  %140 = sub <8 x i32> zeroinitializer, %78
  %141 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %142 = bitcast <4 x i64>* %141 to <8 x i32>*
  store <8 x i32> %140, <8 x i32>* %142, align 32
  %143 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %144 = bitcast <4 x i64>* %143 to <8 x i32>*
  store <8 x i32> %58, <8 x i32>* %144, align 32
  %145 = sub <8 x i32> zeroinitializer, %48
  br label %286

146:                                              ; preds = %6
  %147 = icmp sgt i32 %4, 10
  %148 = select i1 %147, i32 %4, i32 10
  %149 = shl i32 32, %148
  %150 = sub nsw i32 0, %149
  %151 = insertelement <8 x i32> undef, i32 %150, i32 0
  %152 = shufflevector <8 x i32> %151, <8 x i32> undef, <8 x i32> zeroinitializer
  %153 = add nsw i32 %149, -1
  %154 = insertelement <8 x i32> undef, i32 %153, i32 0
  %155 = shufflevector <8 x i32> %154, <8 x i32> undef, <8 x i32> zeroinitializer
  %156 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %157 = shl i32 1, %5
  %158 = ashr i32 %157, 1
  %159 = insertelement <8 x i32> undef, i32 %158, i32 0
  %160 = shufflevector <8 x i32> %159, <8 x i32> undef, <8 x i32> zeroinitializer
  %161 = add <8 x i32> %45, %160
  %162 = sub <8 x i32> %160, %53
  %163 = insertelement <4 x i32> <i32 undef, i32 0, i32 0, i32 0>, i32 %5, i32 0
  %164 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %161, <4 x i32> %163) #8
  %165 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %162, <4 x i32> %163) #8
  %166 = icmp sgt <8 x i32> %164, %152
  %167 = select <8 x i1> %166, <8 x i32> %164, <8 x i32> %152
  %168 = icmp slt <8 x i32> %167, %155
  %169 = select <8 x i1> %168, <8 x i32> %167, <8 x i32> %155
  %170 = icmp sgt <8 x i32> %165, %152
  %171 = select <8 x i1> %170, <8 x i32> %165, <8 x i32> %152
  %172 = icmp slt <8 x i32> %171, %155
  %173 = select <8 x i1> %172, <8 x i32> %171, <8 x i32> %155
  %174 = bitcast <4 x i64>* %1 to <8 x i32>*
  store <8 x i32> %169, <8 x i32>* %174, align 32
  %175 = bitcast <4 x i64>* %156 to <8 x i32>*
  store <8 x i32> %173, <8 x i32>* %175, align 32
  %176 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %177 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %178 = add <8 x i32> %73, %160
  %179 = sub <8 x i32> %160, %63
  %180 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %178, <4 x i32> %163) #8
  %181 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %179, <4 x i32> %163) #8
  %182 = icmp sgt <8 x i32> %180, %152
  %183 = select <8 x i1> %182, <8 x i32> %180, <8 x i32> %152
  %184 = icmp slt <8 x i32> %183, %155
  %185 = select <8 x i1> %184, <8 x i32> %183, <8 x i32> %155
  %186 = icmp sgt <8 x i32> %181, %152
  %187 = select <8 x i1> %186, <8 x i32> %181, <8 x i32> %152
  %188 = icmp slt <8 x i32> %187, %155
  %189 = select <8 x i1> %188, <8 x i32> %187, <8 x i32> %155
  %190 = bitcast <4 x i64>* %176 to <8 x i32>*
  store <8 x i32> %185, <8 x i32>* %190, align 32
  %191 = bitcast <4 x i64>* %177 to <8 x i32>*
  store <8 x i32> %189, <8 x i32>* %191, align 32
  %192 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %193 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %194 = add <8 x i32> %90, %160
  %195 = sub <8 x i32> %160, %104
  %196 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %194, <4 x i32> %163) #8
  %197 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %195, <4 x i32> %163) #8
  %198 = icmp sgt <8 x i32> %196, %152
  %199 = select <8 x i1> %198, <8 x i32> %196, <8 x i32> %152
  %200 = icmp slt <8 x i32> %199, %155
  %201 = select <8 x i1> %200, <8 x i32> %199, <8 x i32> %155
  %202 = icmp sgt <8 x i32> %197, %152
  %203 = select <8 x i1> %202, <8 x i32> %197, <8 x i32> %152
  %204 = icmp slt <8 x i32> %203, %155
  %205 = select <8 x i1> %204, <8 x i32> %203, <8 x i32> %155
  %206 = bitcast <4 x i64>* %192 to <8 x i32>*
  store <8 x i32> %201, <8 x i32>* %206, align 32
  %207 = bitcast <4 x i64>* %193 to <8 x i32>*
  store <8 x i32> %205, <8 x i32>* %207, align 32
  %208 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %209 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %210 = add <8 x i32> %97, %160
  %211 = sub <8 x i32> %160, %83
  %212 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %210, <4 x i32> %163) #8
  %213 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %211, <4 x i32> %163) #8
  %214 = icmp sgt <8 x i32> %212, %152
  %215 = select <8 x i1> %214, <8 x i32> %212, <8 x i32> %152
  %216 = icmp slt <8 x i32> %215, %155
  %217 = select <8 x i1> %216, <8 x i32> %215, <8 x i32> %155
  %218 = icmp sgt <8 x i32> %213, %152
  %219 = select <8 x i1> %218, <8 x i32> %213, <8 x i32> %152
  %220 = icmp slt <8 x i32> %219, %155
  %221 = select <8 x i1> %220, <8 x i32> %219, <8 x i32> %155
  %222 = bitcast <4 x i64>* %208 to <8 x i32>*
  store <8 x i32> %217, <8 x i32>* %222, align 32
  %223 = bitcast <4 x i64>* %209 to <8 x i32>*
  store <8 x i32> %221, <8 x i32>* %223, align 32
  %224 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %225 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %226 = add <8 x i32> %85, %160
  %227 = sub <8 x i32> %160, %99
  %228 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %226, <4 x i32> %163) #8
  %229 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %227, <4 x i32> %163) #8
  %230 = icmp sgt <8 x i32> %228, %152
  %231 = select <8 x i1> %230, <8 x i32> %228, <8 x i32> %152
  %232 = icmp slt <8 x i32> %231, %155
  %233 = select <8 x i1> %232, <8 x i32> %231, <8 x i32> %155
  %234 = icmp sgt <8 x i32> %229, %152
  %235 = select <8 x i1> %234, <8 x i32> %229, <8 x i32> %152
  %236 = icmp slt <8 x i32> %235, %155
  %237 = select <8 x i1> %236, <8 x i32> %235, <8 x i32> %155
  %238 = bitcast <4 x i64>* %224 to <8 x i32>*
  store <8 x i32> %233, <8 x i32>* %238, align 32
  %239 = bitcast <4 x i64>* %225 to <8 x i32>*
  store <8 x i32> %237, <8 x i32>* %239, align 32
  %240 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %241 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %242 = add <8 x i32> %106, %160
  %243 = sub <8 x i32> %160, %92
  %244 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %242, <4 x i32> %163) #8
  %245 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %243, <4 x i32> %163) #8
  %246 = icmp sgt <8 x i32> %244, %152
  %247 = select <8 x i1> %246, <8 x i32> %244, <8 x i32> %152
  %248 = icmp slt <8 x i32> %247, %155
  %249 = select <8 x i1> %248, <8 x i32> %247, <8 x i32> %155
  %250 = icmp sgt <8 x i32> %245, %152
  %251 = select <8 x i1> %250, <8 x i32> %245, <8 x i32> %152
  %252 = icmp slt <8 x i32> %251, %155
  %253 = select <8 x i1> %252, <8 x i32> %251, <8 x i32> %155
  %254 = bitcast <4 x i64>* %240 to <8 x i32>*
  store <8 x i32> %249, <8 x i32>* %254, align 32
  %255 = bitcast <4 x i64>* %241 to <8 x i32>*
  store <8 x i32> %253, <8 x i32>* %255, align 32
  %256 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %257 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %258 = add <8 x i32> %68, %160
  %259 = sub <8 x i32> %160, %78
  %260 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %258, <4 x i32> %163) #8
  %261 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %259, <4 x i32> %163) #8
  %262 = icmp sgt <8 x i32> %260, %152
  %263 = select <8 x i1> %262, <8 x i32> %260, <8 x i32> %152
  %264 = icmp slt <8 x i32> %263, %155
  %265 = select <8 x i1> %264, <8 x i32> %263, <8 x i32> %155
  %266 = icmp sgt <8 x i32> %261, %152
  %267 = select <8 x i1> %266, <8 x i32> %261, <8 x i32> %152
  %268 = icmp slt <8 x i32> %267, %155
  %269 = select <8 x i1> %268, <8 x i32> %267, <8 x i32> %155
  %270 = bitcast <4 x i64>* %256 to <8 x i32>*
  store <8 x i32> %265, <8 x i32>* %270, align 32
  %271 = bitcast <4 x i64>* %257 to <8 x i32>*
  store <8 x i32> %269, <8 x i32>* %271, align 32
  %272 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %273 = add <8 x i32> %58, %160
  %274 = sub <8 x i32> %160, %48
  %275 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %273, <4 x i32> %163) #8
  %276 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %274, <4 x i32> %163) #8
  %277 = icmp sgt <8 x i32> %275, %152
  %278 = select <8 x i1> %277, <8 x i32> %275, <8 x i32> %152
  %279 = icmp slt <8 x i32> %278, %155
  %280 = select <8 x i1> %279, <8 x i32> %278, <8 x i32> %155
  %281 = icmp sgt <8 x i32> %276, %152
  %282 = select <8 x i1> %281, <8 x i32> %276, <8 x i32> %152
  %283 = icmp slt <8 x i32> %282, %155
  %284 = select <8 x i1> %283, <8 x i32> %282, <8 x i32> %155
  %285 = bitcast <4 x i64>* %272 to <8 x i32>*
  store <8 x i32> %280, <8 x i32>* %285, align 32
  br label %286

286:                                              ; preds = %146, %108
  %287 = phi <8 x i32> [ %284, %146 ], [ %145, %108 ]
  %288 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 15
  %289 = bitcast <4 x i64>* %288 to <8 x i32>*
  store <8 x i32> %287, <8 x i32>* %289, align 32
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @iadst16_low8_avx2(<4 x i64>* nocapture readonly, <4 x i64>*, i32, i32, i32, i32) #0 {
  %7 = add nsw i32 %2, -10
  %8 = sext i32 %7 to i64
  %9 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 2
  %10 = load i32, i32* %9, align 8
  %11 = insertelement <8 x i32> undef, i32 %10, i32 0
  %12 = shufflevector <8 x i32> %11, <8 x i32> undef, <8 x i32> zeroinitializer
  %13 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 62
  %14 = load i32, i32* %13, align 8
  %15 = insertelement <8 x i32> undef, i32 %14, i32 0
  %16 = shufflevector <8 x i32> %15, <8 x i32> undef, <8 x i32> zeroinitializer
  %17 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 10
  %18 = load i32, i32* %17, align 8
  %19 = insertelement <8 x i32> undef, i32 %18, i32 0
  %20 = shufflevector <8 x i32> %19, <8 x i32> undef, <8 x i32> zeroinitializer
  %21 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 54
  %22 = load i32, i32* %21, align 8
  %23 = insertelement <8 x i32> undef, i32 %22, i32 0
  %24 = shufflevector <8 x i32> %23, <8 x i32> undef, <8 x i32> zeroinitializer
  %25 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 18
  %26 = load i32, i32* %25, align 8
  %27 = insertelement <8 x i32> undef, i32 %26, i32 0
  %28 = shufflevector <8 x i32> %27, <8 x i32> undef, <8 x i32> zeroinitializer
  %29 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 46
  %30 = load i32, i32* %29, align 8
  %31 = insertelement <8 x i32> undef, i32 %30, i32 0
  %32 = shufflevector <8 x i32> %31, <8 x i32> undef, <8 x i32> zeroinitializer
  %33 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 26
  %34 = load i32, i32* %33, align 8
  %35 = insertelement <8 x i32> undef, i32 %34, i32 0
  %36 = shufflevector <8 x i32> %35, <8 x i32> undef, <8 x i32> zeroinitializer
  %37 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 38
  %38 = load i32, i32* %37, align 8
  %39 = insertelement <8 x i32> undef, i32 %38, i32 0
  %40 = shufflevector <8 x i32> %39, <8 x i32> undef, <8 x i32> zeroinitializer
  %41 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 34
  %42 = load i32, i32* %41, align 8
  %43 = insertelement <8 x i32> undef, i32 %42, i32 0
  %44 = shufflevector <8 x i32> %43, <8 x i32> undef, <8 x i32> zeroinitializer
  %45 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 30
  %46 = load i32, i32* %45, align 8
  %47 = insertelement <8 x i32> undef, i32 %46, i32 0
  %48 = shufflevector <8 x i32> %47, <8 x i32> undef, <8 x i32> zeroinitializer
  %49 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 42
  %50 = load i32, i32* %49, align 8
  %51 = insertelement <8 x i32> undef, i32 %50, i32 0
  %52 = shufflevector <8 x i32> %51, <8 x i32> undef, <8 x i32> zeroinitializer
  %53 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 22
  %54 = load i32, i32* %53, align 8
  %55 = insertelement <8 x i32> undef, i32 %54, i32 0
  %56 = shufflevector <8 x i32> %55, <8 x i32> undef, <8 x i32> zeroinitializer
  %57 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 50
  %58 = load i32, i32* %57, align 8
  %59 = insertelement <8 x i32> undef, i32 %58, i32 0
  %60 = shufflevector <8 x i32> %59, <8 x i32> undef, <8 x i32> zeroinitializer
  %61 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 14
  %62 = load i32, i32* %61, align 8
  %63 = insertelement <8 x i32> undef, i32 %62, i32 0
  %64 = shufflevector <8 x i32> %63, <8 x i32> undef, <8 x i32> zeroinitializer
  %65 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 58
  %66 = load i32, i32* %65, align 8
  %67 = insertelement <8 x i32> undef, i32 %66, i32 0
  %68 = shufflevector <8 x i32> %67, <8 x i32> undef, <8 x i32> zeroinitializer
  %69 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 6
  %70 = load i32, i32* %69, align 8
  %71 = insertelement <8 x i32> undef, i32 %70, i32 0
  %72 = shufflevector <8 x i32> %71, <8 x i32> undef, <8 x i32> zeroinitializer
  %73 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 8
  %74 = load i32, i32* %73, align 16
  %75 = insertelement <8 x i32> undef, i32 %74, i32 0
  %76 = shufflevector <8 x i32> %75, <8 x i32> undef, <8 x i32> zeroinitializer
  %77 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 56
  %78 = load i32, i32* %77, align 16
  %79 = insertelement <8 x i32> undef, i32 %78, i32 0
  %80 = shufflevector <8 x i32> %79, <8 x i32> undef, <8 x i32> zeroinitializer
  %81 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 40
  %82 = load i32, i32* %81, align 16
  %83 = insertelement <8 x i32> undef, i32 %82, i32 0
  %84 = shufflevector <8 x i32> %83, <8 x i32> undef, <8 x i32> zeroinitializer
  %85 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 24
  %86 = load i32, i32* %85, align 16
  %87 = insertelement <8 x i32> undef, i32 %86, i32 0
  %88 = shufflevector <8 x i32> %87, <8 x i32> undef, <8 x i32> zeroinitializer
  %89 = sub nsw i32 0, %78
  %90 = insertelement <8 x i32> undef, i32 %89, i32 0
  %91 = shufflevector <8 x i32> %90, <8 x i32> undef, <8 x i32> zeroinitializer
  %92 = sub nsw i32 0, %86
  %93 = insertelement <8 x i32> undef, i32 %92, i32 0
  %94 = shufflevector <8 x i32> %93, <8 x i32> undef, <8 x i32> zeroinitializer
  %95 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 48
  %96 = load i32, i32* %95, align 16
  %97 = insertelement <8 x i32> undef, i32 %96, i32 0
  %98 = shufflevector <8 x i32> %97, <8 x i32> undef, <8 x i32> zeroinitializer
  %99 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 16
  %100 = load i32, i32* %99, align 16
  %101 = insertelement <8 x i32> undef, i32 %100, i32 0
  %102 = shufflevector <8 x i32> %101, <8 x i32> undef, <8 x i32> zeroinitializer
  %103 = sub nsw i32 0, %96
  %104 = insertelement <8 x i32> undef, i32 %103, i32 0
  %105 = shufflevector <8 x i32> %104, <8 x i32> undef, <8 x i32> zeroinitializer
  %106 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 32
  %107 = load i32, i32* %106, align 16
  %108 = insertelement <8 x i32> undef, i32 %107, i32 0
  %109 = shufflevector <8 x i32> %108, <8 x i32> undef, <8 x i32> zeroinitializer
  %110 = add nsw i32 %2, -1
  %111 = shl i32 1, %110
  %112 = insertelement <8 x i32> undef, i32 %111, i32 0
  %113 = shufflevector <8 x i32> %112, <8 x i32> undef, <8 x i32> zeroinitializer
  %114 = icmp ne i32 %3, 0
  %115 = select i1 %114, i32 6, i32 8
  %116 = add nsw i32 %115, %4
  %117 = icmp slt i32 %116, 16
  %118 = add i32 %116, -1
  %119 = shl i32 1, %118
  %120 = select i1 %117, i32 32768, i32 %119
  %121 = sub nsw i32 0, %120
  %122 = insertelement <8 x i32> undef, i32 %121, i32 0
  %123 = shufflevector <8 x i32> %122, <8 x i32> undef, <8 x i32> zeroinitializer
  %124 = add nsw i32 %120, -1
  %125 = insertelement <8 x i32> undef, i32 %124, i32 0
  %126 = shufflevector <8 x i32> %125, <8 x i32> undef, <8 x i32> zeroinitializer
  %127 = bitcast <4 x i64>* %0 to <8 x i32>*
  %128 = load <8 x i32>, <8 x i32>* %127, align 32
  %129 = mul <8 x i32> %128, %16
  %130 = add <8 x i32> %129, %113
  %131 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %130, i32 %2) #8
  %132 = mul <8 x i32> %128, %12
  %133 = sub <8 x i32> %113, %132
  %134 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %133, i32 %2) #8
  %135 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 2
  %136 = bitcast <4 x i64>* %135 to <8 x i32>*
  %137 = load <8 x i32>, <8 x i32>* %136, align 32
  %138 = mul <8 x i32> %137, %24
  %139 = add <8 x i32> %138, %113
  %140 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %139, i32 %2) #8
  %141 = mul <8 x i32> %137, %20
  %142 = sub <8 x i32> %113, %141
  %143 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %142, i32 %2) #8
  %144 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 4
  %145 = bitcast <4 x i64>* %144 to <8 x i32>*
  %146 = load <8 x i32>, <8 x i32>* %145, align 32
  %147 = mul <8 x i32> %146, %32
  %148 = add <8 x i32> %147, %113
  %149 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %148, i32 %2) #8
  %150 = mul <8 x i32> %146, %28
  %151 = sub <8 x i32> %113, %150
  %152 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %151, i32 %2) #8
  %153 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 6
  %154 = bitcast <4 x i64>* %153 to <8 x i32>*
  %155 = load <8 x i32>, <8 x i32>* %154, align 32
  %156 = mul <8 x i32> %155, %40
  %157 = add <8 x i32> %156, %113
  %158 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %157, i32 %2) #8
  %159 = mul <8 x i32> %155, %36
  %160 = sub <8 x i32> %113, %159
  %161 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %160, i32 %2) #8
  %162 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 7
  %163 = bitcast <4 x i64>* %162 to <8 x i32>*
  %164 = load <8 x i32>, <8 x i32>* %163, align 32
  %165 = mul <8 x i32> %164, %44
  %166 = add <8 x i32> %165, %113
  %167 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %166, i32 %2) #8
  %168 = mul <8 x i32> %164, %48
  %169 = add <8 x i32> %168, %113
  %170 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %169, i32 %2) #8
  %171 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 5
  %172 = bitcast <4 x i64>* %171 to <8 x i32>*
  %173 = load <8 x i32>, <8 x i32>* %172, align 32
  %174 = mul <8 x i32> %173, %52
  %175 = add <8 x i32> %174, %113
  %176 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %175, i32 %2) #8
  %177 = mul <8 x i32> %173, %56
  %178 = add <8 x i32> %177, %113
  %179 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %178, i32 %2) #8
  %180 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 3
  %181 = bitcast <4 x i64>* %180 to <8 x i32>*
  %182 = load <8 x i32>, <8 x i32>* %181, align 32
  %183 = mul <8 x i32> %182, %60
  %184 = add <8 x i32> %183, %113
  %185 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %184, i32 %2) #8
  %186 = mul <8 x i32> %182, %64
  %187 = add <8 x i32> %186, %113
  %188 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %187, i32 %2) #8
  %189 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 1
  %190 = bitcast <4 x i64>* %189 to <8 x i32>*
  %191 = load <8 x i32>, <8 x i32>* %190, align 32
  %192 = mul <8 x i32> %191, %68
  %193 = add <8 x i32> %192, %113
  %194 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %193, i32 %2) #8
  %195 = mul <8 x i32> %191, %72
  %196 = add <8 x i32> %195, %113
  %197 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %196, i32 %2) #8
  %198 = add <8 x i32> %167, %131
  %199 = sub <8 x i32> %131, %167
  %200 = icmp sgt <8 x i32> %198, %123
  %201 = select <8 x i1> %200, <8 x i32> %198, <8 x i32> %123
  %202 = icmp slt <8 x i32> %201, %126
  %203 = select <8 x i1> %202, <8 x i32> %201, <8 x i32> %126
  %204 = icmp sgt <8 x i32> %199, %123
  %205 = select <8 x i1> %204, <8 x i32> %199, <8 x i32> %123
  %206 = icmp slt <8 x i32> %205, %126
  %207 = select <8 x i1> %206, <8 x i32> %205, <8 x i32> %126
  %208 = add <8 x i32> %170, %134
  %209 = sub <8 x i32> %134, %170
  %210 = icmp sgt <8 x i32> %208, %123
  %211 = select <8 x i1> %210, <8 x i32> %208, <8 x i32> %123
  %212 = icmp slt <8 x i32> %211, %126
  %213 = select <8 x i1> %212, <8 x i32> %211, <8 x i32> %126
  %214 = icmp sgt <8 x i32> %209, %123
  %215 = select <8 x i1> %214, <8 x i32> %209, <8 x i32> %123
  %216 = icmp slt <8 x i32> %215, %126
  %217 = select <8 x i1> %216, <8 x i32> %215, <8 x i32> %126
  %218 = add <8 x i32> %176, %140
  %219 = sub <8 x i32> %140, %176
  %220 = icmp sgt <8 x i32> %218, %123
  %221 = select <8 x i1> %220, <8 x i32> %218, <8 x i32> %123
  %222 = icmp slt <8 x i32> %221, %126
  %223 = select <8 x i1> %222, <8 x i32> %221, <8 x i32> %126
  %224 = icmp sgt <8 x i32> %219, %123
  %225 = select <8 x i1> %224, <8 x i32> %219, <8 x i32> %123
  %226 = icmp slt <8 x i32> %225, %126
  %227 = select <8 x i1> %226, <8 x i32> %225, <8 x i32> %126
  %228 = add <8 x i32> %179, %143
  %229 = sub <8 x i32> %143, %179
  %230 = icmp sgt <8 x i32> %228, %123
  %231 = select <8 x i1> %230, <8 x i32> %228, <8 x i32> %123
  %232 = icmp slt <8 x i32> %231, %126
  %233 = select <8 x i1> %232, <8 x i32> %231, <8 x i32> %126
  %234 = icmp sgt <8 x i32> %229, %123
  %235 = select <8 x i1> %234, <8 x i32> %229, <8 x i32> %123
  %236 = icmp slt <8 x i32> %235, %126
  %237 = select <8 x i1> %236, <8 x i32> %235, <8 x i32> %126
  %238 = add <8 x i32> %185, %149
  %239 = sub <8 x i32> %149, %185
  %240 = icmp sgt <8 x i32> %238, %123
  %241 = select <8 x i1> %240, <8 x i32> %238, <8 x i32> %123
  %242 = icmp slt <8 x i32> %241, %126
  %243 = select <8 x i1> %242, <8 x i32> %241, <8 x i32> %126
  %244 = icmp sgt <8 x i32> %239, %123
  %245 = select <8 x i1> %244, <8 x i32> %239, <8 x i32> %123
  %246 = icmp slt <8 x i32> %245, %126
  %247 = select <8 x i1> %246, <8 x i32> %245, <8 x i32> %126
  %248 = add <8 x i32> %188, %152
  %249 = sub <8 x i32> %152, %188
  %250 = icmp sgt <8 x i32> %248, %123
  %251 = select <8 x i1> %250, <8 x i32> %248, <8 x i32> %123
  %252 = icmp slt <8 x i32> %251, %126
  %253 = select <8 x i1> %252, <8 x i32> %251, <8 x i32> %126
  %254 = icmp sgt <8 x i32> %249, %123
  %255 = select <8 x i1> %254, <8 x i32> %249, <8 x i32> %123
  %256 = icmp slt <8 x i32> %255, %126
  %257 = select <8 x i1> %256, <8 x i32> %255, <8 x i32> %126
  %258 = add <8 x i32> %194, %158
  %259 = sub <8 x i32> %158, %194
  %260 = icmp sgt <8 x i32> %258, %123
  %261 = select <8 x i1> %260, <8 x i32> %258, <8 x i32> %123
  %262 = icmp slt <8 x i32> %261, %126
  %263 = select <8 x i1> %262, <8 x i32> %261, <8 x i32> %126
  %264 = icmp sgt <8 x i32> %259, %123
  %265 = select <8 x i1> %264, <8 x i32> %259, <8 x i32> %123
  %266 = icmp slt <8 x i32> %265, %126
  %267 = select <8 x i1> %266, <8 x i32> %265, <8 x i32> %126
  %268 = add <8 x i32> %197, %161
  %269 = sub <8 x i32> %161, %197
  %270 = icmp sgt <8 x i32> %268, %123
  %271 = select <8 x i1> %270, <8 x i32> %268, <8 x i32> %123
  %272 = icmp slt <8 x i32> %271, %126
  %273 = select <8 x i1> %272, <8 x i32> %271, <8 x i32> %126
  %274 = icmp sgt <8 x i32> %269, %123
  %275 = select <8 x i1> %274, <8 x i32> %269, <8 x i32> %123
  %276 = icmp slt <8 x i32> %275, %126
  %277 = select <8 x i1> %276, <8 x i32> %275, <8 x i32> %126
  %278 = mul <8 x i32> %207, %80
  %279 = mul <8 x i32> %217, %80
  %280 = mul <8 x i32> %207, %76
  %281 = add <8 x i32> %280, %113
  %282 = add <8 x i32> %281, %279
  %283 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %282, i32 %2) #8
  %284 = mul <8 x i32> %76, %217
  %285 = add <8 x i32> %278, %113
  %286 = sub <8 x i32> %285, %284
  %287 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %286, i32 %2) #8
  %288 = mul <8 x i32> %237, %88
  %289 = mul <8 x i32> %227, %88
  %290 = mul <8 x i32> %227, %84
  %291 = add <8 x i32> %290, %113
  %292 = add <8 x i32> %291, %288
  %293 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %292, i32 %2) #8
  %294 = mul <8 x i32> %84, %237
  %295 = add <8 x i32> %289, %113
  %296 = sub <8 x i32> %295, %294
  %297 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %296, i32 %2) #8
  %298 = mul <8 x i32> %257, %76
  %299 = mul <8 x i32> %247, %76
  %300 = mul <8 x i32> %247, %91
  %301 = add <8 x i32> %300, %113
  %302 = add <8 x i32> %301, %298
  %303 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %302, i32 %2) #8
  %304 = mul <8 x i32> %91, %257
  %305 = add <8 x i32> %299, %113
  %306 = sub <8 x i32> %305, %304
  %307 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %306, i32 %2) #8
  %308 = mul <8 x i32> %277, %84
  %309 = mul <8 x i32> %267, %84
  %310 = mul <8 x i32> %267, %94
  %311 = add <8 x i32> %310, %113
  %312 = add <8 x i32> %311, %308
  %313 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %312, i32 %2) #8
  %314 = mul <8 x i32> %94, %277
  %315 = add <8 x i32> %309, %113
  %316 = sub <8 x i32> %315, %314
  %317 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %316, i32 %2) #8
  %318 = add <8 x i32> %243, %203
  %319 = sub <8 x i32> %203, %243
  %320 = icmp sgt <8 x i32> %318, %123
  %321 = select <8 x i1> %320, <8 x i32> %318, <8 x i32> %123
  %322 = icmp slt <8 x i32> %321, %126
  %323 = select <8 x i1> %322, <8 x i32> %321, <8 x i32> %126
  %324 = icmp sgt <8 x i32> %319, %123
  %325 = select <8 x i1> %324, <8 x i32> %319, <8 x i32> %123
  %326 = icmp slt <8 x i32> %325, %126
  %327 = select <8 x i1> %326, <8 x i32> %325, <8 x i32> %126
  %328 = add <8 x i32> %253, %213
  %329 = sub <8 x i32> %213, %253
  %330 = icmp sgt <8 x i32> %328, %123
  %331 = select <8 x i1> %330, <8 x i32> %328, <8 x i32> %123
  %332 = icmp slt <8 x i32> %331, %126
  %333 = select <8 x i1> %332, <8 x i32> %331, <8 x i32> %126
  %334 = icmp sgt <8 x i32> %329, %123
  %335 = select <8 x i1> %334, <8 x i32> %329, <8 x i32> %123
  %336 = icmp slt <8 x i32> %335, %126
  %337 = select <8 x i1> %336, <8 x i32> %335, <8 x i32> %126
  %338 = add <8 x i32> %263, %223
  %339 = sub <8 x i32> %223, %263
  %340 = icmp sgt <8 x i32> %338, %123
  %341 = select <8 x i1> %340, <8 x i32> %338, <8 x i32> %123
  %342 = icmp slt <8 x i32> %341, %126
  %343 = select <8 x i1> %342, <8 x i32> %341, <8 x i32> %126
  %344 = icmp sgt <8 x i32> %339, %123
  %345 = select <8 x i1> %344, <8 x i32> %339, <8 x i32> %123
  %346 = icmp slt <8 x i32> %345, %126
  %347 = select <8 x i1> %346, <8 x i32> %345, <8 x i32> %126
  %348 = add <8 x i32> %273, %233
  %349 = sub <8 x i32> %233, %273
  %350 = icmp sgt <8 x i32> %348, %123
  %351 = select <8 x i1> %350, <8 x i32> %348, <8 x i32> %123
  %352 = icmp slt <8 x i32> %351, %126
  %353 = select <8 x i1> %352, <8 x i32> %351, <8 x i32> %126
  %354 = icmp sgt <8 x i32> %349, %123
  %355 = select <8 x i1> %354, <8 x i32> %349, <8 x i32> %123
  %356 = icmp slt <8 x i32> %355, %126
  %357 = select <8 x i1> %356, <8 x i32> %355, <8 x i32> %126
  %358 = add <8 x i32> %303, %283
  %359 = sub <8 x i32> %283, %303
  %360 = icmp sgt <8 x i32> %358, %123
  %361 = select <8 x i1> %360, <8 x i32> %358, <8 x i32> %123
  %362 = icmp slt <8 x i32> %361, %126
  %363 = select <8 x i1> %362, <8 x i32> %361, <8 x i32> %126
  %364 = icmp sgt <8 x i32> %359, %123
  %365 = select <8 x i1> %364, <8 x i32> %359, <8 x i32> %123
  %366 = icmp slt <8 x i32> %365, %126
  %367 = select <8 x i1> %366, <8 x i32> %365, <8 x i32> %126
  %368 = add <8 x i32> %307, %287
  %369 = sub <8 x i32> %287, %307
  %370 = icmp sgt <8 x i32> %368, %123
  %371 = select <8 x i1> %370, <8 x i32> %368, <8 x i32> %123
  %372 = icmp slt <8 x i32> %371, %126
  %373 = select <8 x i1> %372, <8 x i32> %371, <8 x i32> %126
  %374 = icmp sgt <8 x i32> %369, %123
  %375 = select <8 x i1> %374, <8 x i32> %369, <8 x i32> %123
  %376 = icmp slt <8 x i32> %375, %126
  %377 = select <8 x i1> %376, <8 x i32> %375, <8 x i32> %126
  %378 = add <8 x i32> %313, %293
  %379 = sub <8 x i32> %293, %313
  %380 = icmp sgt <8 x i32> %378, %123
  %381 = select <8 x i1> %380, <8 x i32> %378, <8 x i32> %123
  %382 = icmp slt <8 x i32> %381, %126
  %383 = select <8 x i1> %382, <8 x i32> %381, <8 x i32> %126
  %384 = icmp sgt <8 x i32> %379, %123
  %385 = select <8 x i1> %384, <8 x i32> %379, <8 x i32> %123
  %386 = icmp slt <8 x i32> %385, %126
  %387 = select <8 x i1> %386, <8 x i32> %385, <8 x i32> %126
  %388 = add <8 x i32> %317, %297
  %389 = sub <8 x i32> %297, %317
  %390 = icmp sgt <8 x i32> %388, %123
  %391 = select <8 x i1> %390, <8 x i32> %388, <8 x i32> %123
  %392 = icmp slt <8 x i32> %391, %126
  %393 = select <8 x i1> %392, <8 x i32> %391, <8 x i32> %126
  %394 = icmp sgt <8 x i32> %389, %123
  %395 = select <8 x i1> %394, <8 x i32> %389, <8 x i32> %123
  %396 = icmp slt <8 x i32> %395, %126
  %397 = select <8 x i1> %396, <8 x i32> %395, <8 x i32> %126
  %398 = mul <8 x i32> %337, %98
  %399 = mul <8 x i32> %327, %98
  %400 = mul <8 x i32> %327, %102
  %401 = add <8 x i32> %400, %113
  %402 = add <8 x i32> %401, %398
  %403 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %402, i32 %2) #8
  %404 = mul <8 x i32> %102, %337
  %405 = add <8 x i32> %399, %113
  %406 = sub <8 x i32> %405, %404
  %407 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %406, i32 %2) #8
  %408 = mul <8 x i32> %357, %102
  %409 = mul <8 x i32> %347, %102
  %410 = mul <8 x i32> %347, %105
  %411 = add <8 x i32> %410, %113
  %412 = add <8 x i32> %411, %408
  %413 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %412, i32 %2) #8
  %414 = mul <8 x i32> %105, %357
  %415 = add <8 x i32> %409, %113
  %416 = sub <8 x i32> %415, %414
  %417 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %416, i32 %2) #8
  %418 = mul <8 x i32> %377, %98
  %419 = mul <8 x i32> %367, %98
  %420 = mul <8 x i32> %367, %102
  %421 = add <8 x i32> %420, %113
  %422 = add <8 x i32> %421, %418
  %423 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %422, i32 %2) #8
  %424 = mul <8 x i32> %102, %377
  %425 = add <8 x i32> %419, %113
  %426 = sub <8 x i32> %425, %424
  %427 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %426, i32 %2) #8
  %428 = mul <8 x i32> %397, %102
  %429 = mul <8 x i32> %387, %102
  %430 = mul <8 x i32> %387, %105
  %431 = add <8 x i32> %430, %113
  %432 = add <8 x i32> %431, %428
  %433 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %432, i32 %2) #8
  %434 = mul <8 x i32> %105, %397
  %435 = add <8 x i32> %429, %113
  %436 = sub <8 x i32> %435, %434
  %437 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %436, i32 %2) #8
  %438 = add <8 x i32> %343, %323
  %439 = sub <8 x i32> %323, %343
  %440 = icmp sgt <8 x i32> %438, %123
  %441 = select <8 x i1> %440, <8 x i32> %438, <8 x i32> %123
  %442 = icmp slt <8 x i32> %441, %126
  %443 = select <8 x i1> %442, <8 x i32> %441, <8 x i32> %126
  %444 = icmp sgt <8 x i32> %439, %123
  %445 = select <8 x i1> %444, <8 x i32> %439, <8 x i32> %123
  %446 = icmp slt <8 x i32> %445, %126
  %447 = select <8 x i1> %446, <8 x i32> %445, <8 x i32> %126
  %448 = add <8 x i32> %353, %333
  %449 = sub <8 x i32> %333, %353
  %450 = icmp sgt <8 x i32> %448, %123
  %451 = select <8 x i1> %450, <8 x i32> %448, <8 x i32> %123
  %452 = icmp slt <8 x i32> %451, %126
  %453 = select <8 x i1> %452, <8 x i32> %451, <8 x i32> %126
  %454 = icmp sgt <8 x i32> %449, %123
  %455 = select <8 x i1> %454, <8 x i32> %449, <8 x i32> %123
  %456 = icmp slt <8 x i32> %455, %126
  %457 = select <8 x i1> %456, <8 x i32> %455, <8 x i32> %126
  %458 = add <8 x i32> %413, %403
  %459 = sub <8 x i32> %403, %413
  %460 = icmp sgt <8 x i32> %458, %123
  %461 = select <8 x i1> %460, <8 x i32> %458, <8 x i32> %123
  %462 = icmp slt <8 x i32> %461, %126
  %463 = select <8 x i1> %462, <8 x i32> %461, <8 x i32> %126
  %464 = icmp sgt <8 x i32> %459, %123
  %465 = select <8 x i1> %464, <8 x i32> %459, <8 x i32> %123
  %466 = icmp slt <8 x i32> %465, %126
  %467 = select <8 x i1> %466, <8 x i32> %465, <8 x i32> %126
  %468 = add <8 x i32> %417, %407
  %469 = sub <8 x i32> %407, %417
  %470 = icmp sgt <8 x i32> %468, %123
  %471 = select <8 x i1> %470, <8 x i32> %468, <8 x i32> %123
  %472 = icmp slt <8 x i32> %471, %126
  %473 = select <8 x i1> %472, <8 x i32> %471, <8 x i32> %126
  %474 = icmp sgt <8 x i32> %469, %123
  %475 = select <8 x i1> %474, <8 x i32> %469, <8 x i32> %123
  %476 = icmp slt <8 x i32> %475, %126
  %477 = select <8 x i1> %476, <8 x i32> %475, <8 x i32> %126
  %478 = add <8 x i32> %383, %363
  %479 = sub <8 x i32> %363, %383
  %480 = icmp sgt <8 x i32> %478, %123
  %481 = select <8 x i1> %480, <8 x i32> %478, <8 x i32> %123
  %482 = icmp slt <8 x i32> %481, %126
  %483 = select <8 x i1> %482, <8 x i32> %481, <8 x i32> %126
  %484 = icmp sgt <8 x i32> %479, %123
  %485 = select <8 x i1> %484, <8 x i32> %479, <8 x i32> %123
  %486 = icmp slt <8 x i32> %485, %126
  %487 = select <8 x i1> %486, <8 x i32> %485, <8 x i32> %126
  %488 = add <8 x i32> %393, %373
  %489 = sub <8 x i32> %373, %393
  %490 = icmp sgt <8 x i32> %488, %123
  %491 = select <8 x i1> %490, <8 x i32> %488, <8 x i32> %123
  %492 = icmp slt <8 x i32> %491, %126
  %493 = select <8 x i1> %492, <8 x i32> %491, <8 x i32> %126
  %494 = icmp sgt <8 x i32> %489, %123
  %495 = select <8 x i1> %494, <8 x i32> %489, <8 x i32> %123
  %496 = icmp slt <8 x i32> %495, %126
  %497 = select <8 x i1> %496, <8 x i32> %495, <8 x i32> %126
  %498 = add <8 x i32> %433, %423
  %499 = sub <8 x i32> %423, %433
  %500 = icmp sgt <8 x i32> %498, %123
  %501 = select <8 x i1> %500, <8 x i32> %498, <8 x i32> %123
  %502 = icmp slt <8 x i32> %501, %126
  %503 = select <8 x i1> %502, <8 x i32> %501, <8 x i32> %126
  %504 = icmp sgt <8 x i32> %499, %123
  %505 = select <8 x i1> %504, <8 x i32> %499, <8 x i32> %123
  %506 = icmp slt <8 x i32> %505, %126
  %507 = select <8 x i1> %506, <8 x i32> %505, <8 x i32> %126
  %508 = add <8 x i32> %437, %427
  %509 = sub <8 x i32> %427, %437
  %510 = icmp sgt <8 x i32> %508, %123
  %511 = select <8 x i1> %510, <8 x i32> %508, <8 x i32> %123
  %512 = icmp slt <8 x i32> %511, %126
  %513 = select <8 x i1> %512, <8 x i32> %511, <8 x i32> %126
  %514 = icmp sgt <8 x i32> %509, %123
  %515 = select <8 x i1> %514, <8 x i32> %509, <8 x i32> %123
  %516 = icmp slt <8 x i32> %515, %126
  %517 = select <8 x i1> %516, <8 x i32> %515, <8 x i32> %126
  %518 = mul <8 x i32> %447, %109
  %519 = mul <8 x i32> %457, %109
  %520 = add <8 x i32> %518, %113
  %521 = add <8 x i32> %520, %519
  %522 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %521, i32 %2) #8
  %523 = sub <8 x i32> %520, %519
  %524 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %523, i32 %2) #8
  %525 = mul <8 x i32> %467, %109
  %526 = mul <8 x i32> %477, %109
  %527 = add <8 x i32> %525, %113
  %528 = add <8 x i32> %527, %526
  %529 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %528, i32 %2) #8
  %530 = sub <8 x i32> %527, %526
  %531 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %530, i32 %2) #8
  %532 = mul <8 x i32> %487, %109
  %533 = mul <8 x i32> %497, %109
  %534 = add <8 x i32> %532, %113
  %535 = add <8 x i32> %534, %533
  %536 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %535, i32 %2) #8
  %537 = sub <8 x i32> %534, %533
  %538 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %537, i32 %2) #8
  %539 = mul <8 x i32> %507, %109
  %540 = mul <8 x i32> %517, %109
  %541 = add <8 x i32> %539, %113
  %542 = add <8 x i32> %541, %540
  %543 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %542, i32 %2) #8
  %544 = sub <8 x i32> %541, %540
  %545 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %544, i32 %2) #8
  br i1 %114, label %546, label %584

546:                                              ; preds = %6
  %547 = bitcast <4 x i64>* %1 to <8 x i32>*
  store <8 x i32> %443, <8 x i32>* %547, align 32
  %548 = sub <8 x i32> zeroinitializer, %483
  %549 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %550 = bitcast <4 x i64>* %549 to <8 x i32>*
  store <8 x i32> %548, <8 x i32>* %550, align 32
  %551 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %552 = bitcast <4 x i64>* %551 to <8 x i32>*
  store <8 x i32> %503, <8 x i32>* %552, align 32
  %553 = sub <8 x i32> zeroinitializer, %463
  %554 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %555 = bitcast <4 x i64>* %554 to <8 x i32>*
  store <8 x i32> %553, <8 x i32>* %555, align 32
  %556 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %557 = bitcast <4 x i64>* %556 to <8 x i32>*
  store <8 x i32> %529, <8 x i32>* %557, align 32
  %558 = sub <8 x i32> zeroinitializer, %543
  %559 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %560 = bitcast <4 x i64>* %559 to <8 x i32>*
  store <8 x i32> %558, <8 x i32>* %560, align 32
  %561 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %562 = bitcast <4 x i64>* %561 to <8 x i32>*
  store <8 x i32> %536, <8 x i32>* %562, align 32
  %563 = sub <8 x i32> zeroinitializer, %522
  %564 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %565 = bitcast <4 x i64>* %564 to <8 x i32>*
  store <8 x i32> %563, <8 x i32>* %565, align 32
  %566 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %567 = bitcast <4 x i64>* %566 to <8 x i32>*
  store <8 x i32> %524, <8 x i32>* %567, align 32
  %568 = sub <8 x i32> zeroinitializer, %538
  %569 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %570 = bitcast <4 x i64>* %569 to <8 x i32>*
  store <8 x i32> %568, <8 x i32>* %570, align 32
  %571 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %572 = bitcast <4 x i64>* %571 to <8 x i32>*
  store <8 x i32> %545, <8 x i32>* %572, align 32
  %573 = sub <8 x i32> zeroinitializer, %531
  %574 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %575 = bitcast <4 x i64>* %574 to <8 x i32>*
  store <8 x i32> %573, <8 x i32>* %575, align 32
  %576 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %577 = bitcast <4 x i64>* %576 to <8 x i32>*
  store <8 x i32> %473, <8 x i32>* %577, align 32
  %578 = sub <8 x i32> zeroinitializer, %513
  %579 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %580 = bitcast <4 x i64>* %579 to <8 x i32>*
  store <8 x i32> %578, <8 x i32>* %580, align 32
  %581 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %582 = bitcast <4 x i64>* %581 to <8 x i32>*
  store <8 x i32> %493, <8 x i32>* %582, align 32
  %583 = sub <8 x i32> zeroinitializer, %453
  br label %724

584:                                              ; preds = %6
  %585 = icmp sgt i32 %4, 10
  %586 = select i1 %585, i32 %4, i32 10
  %587 = shl i32 32, %586
  %588 = sub nsw i32 0, %587
  %589 = insertelement <8 x i32> undef, i32 %588, i32 0
  %590 = shufflevector <8 x i32> %589, <8 x i32> undef, <8 x i32> zeroinitializer
  %591 = add nsw i32 %587, -1
  %592 = insertelement <8 x i32> undef, i32 %591, i32 0
  %593 = shufflevector <8 x i32> %592, <8 x i32> undef, <8 x i32> zeroinitializer
  %594 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %595 = shl i32 1, %5
  %596 = ashr i32 %595, 1
  %597 = insertelement <8 x i32> undef, i32 %596, i32 0
  %598 = shufflevector <8 x i32> %597, <8 x i32> undef, <8 x i32> zeroinitializer
  %599 = add <8 x i32> %443, %598
  %600 = sub <8 x i32> %598, %483
  %601 = insertelement <4 x i32> <i32 undef, i32 0, i32 0, i32 0>, i32 %5, i32 0
  %602 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %599, <4 x i32> %601) #8
  %603 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %600, <4 x i32> %601) #8
  %604 = icmp sgt <8 x i32> %602, %590
  %605 = select <8 x i1> %604, <8 x i32> %602, <8 x i32> %590
  %606 = icmp slt <8 x i32> %605, %593
  %607 = select <8 x i1> %606, <8 x i32> %605, <8 x i32> %593
  %608 = icmp sgt <8 x i32> %603, %590
  %609 = select <8 x i1> %608, <8 x i32> %603, <8 x i32> %590
  %610 = icmp slt <8 x i32> %609, %593
  %611 = select <8 x i1> %610, <8 x i32> %609, <8 x i32> %593
  %612 = bitcast <4 x i64>* %1 to <8 x i32>*
  store <8 x i32> %607, <8 x i32>* %612, align 32
  %613 = bitcast <4 x i64>* %594 to <8 x i32>*
  store <8 x i32> %611, <8 x i32>* %613, align 32
  %614 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %615 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %616 = add <8 x i32> %503, %598
  %617 = sub <8 x i32> %598, %463
  %618 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %616, <4 x i32> %601) #8
  %619 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %617, <4 x i32> %601) #8
  %620 = icmp sgt <8 x i32> %618, %590
  %621 = select <8 x i1> %620, <8 x i32> %618, <8 x i32> %590
  %622 = icmp slt <8 x i32> %621, %593
  %623 = select <8 x i1> %622, <8 x i32> %621, <8 x i32> %593
  %624 = icmp sgt <8 x i32> %619, %590
  %625 = select <8 x i1> %624, <8 x i32> %619, <8 x i32> %590
  %626 = icmp slt <8 x i32> %625, %593
  %627 = select <8 x i1> %626, <8 x i32> %625, <8 x i32> %593
  %628 = bitcast <4 x i64>* %614 to <8 x i32>*
  store <8 x i32> %623, <8 x i32>* %628, align 32
  %629 = bitcast <4 x i64>* %615 to <8 x i32>*
  store <8 x i32> %627, <8 x i32>* %629, align 32
  %630 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %631 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %632 = add <8 x i32> %529, %598
  %633 = sub <8 x i32> %598, %543
  %634 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %632, <4 x i32> %601) #8
  %635 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %633, <4 x i32> %601) #8
  %636 = icmp sgt <8 x i32> %634, %590
  %637 = select <8 x i1> %636, <8 x i32> %634, <8 x i32> %590
  %638 = icmp slt <8 x i32> %637, %593
  %639 = select <8 x i1> %638, <8 x i32> %637, <8 x i32> %593
  %640 = icmp sgt <8 x i32> %635, %590
  %641 = select <8 x i1> %640, <8 x i32> %635, <8 x i32> %590
  %642 = icmp slt <8 x i32> %641, %593
  %643 = select <8 x i1> %642, <8 x i32> %641, <8 x i32> %593
  %644 = bitcast <4 x i64>* %630 to <8 x i32>*
  store <8 x i32> %639, <8 x i32>* %644, align 32
  %645 = bitcast <4 x i64>* %631 to <8 x i32>*
  store <8 x i32> %643, <8 x i32>* %645, align 32
  %646 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %647 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %648 = add <8 x i32> %536, %598
  %649 = sub <8 x i32> %598, %522
  %650 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %648, <4 x i32> %601) #8
  %651 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %649, <4 x i32> %601) #8
  %652 = icmp sgt <8 x i32> %650, %590
  %653 = select <8 x i1> %652, <8 x i32> %650, <8 x i32> %590
  %654 = icmp slt <8 x i32> %653, %593
  %655 = select <8 x i1> %654, <8 x i32> %653, <8 x i32> %593
  %656 = icmp sgt <8 x i32> %651, %590
  %657 = select <8 x i1> %656, <8 x i32> %651, <8 x i32> %590
  %658 = icmp slt <8 x i32> %657, %593
  %659 = select <8 x i1> %658, <8 x i32> %657, <8 x i32> %593
  %660 = bitcast <4 x i64>* %646 to <8 x i32>*
  store <8 x i32> %655, <8 x i32>* %660, align 32
  %661 = bitcast <4 x i64>* %647 to <8 x i32>*
  store <8 x i32> %659, <8 x i32>* %661, align 32
  %662 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %663 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %664 = add <8 x i32> %524, %598
  %665 = sub <8 x i32> %598, %538
  %666 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %664, <4 x i32> %601) #8
  %667 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %665, <4 x i32> %601) #8
  %668 = icmp sgt <8 x i32> %666, %590
  %669 = select <8 x i1> %668, <8 x i32> %666, <8 x i32> %590
  %670 = icmp slt <8 x i32> %669, %593
  %671 = select <8 x i1> %670, <8 x i32> %669, <8 x i32> %593
  %672 = icmp sgt <8 x i32> %667, %590
  %673 = select <8 x i1> %672, <8 x i32> %667, <8 x i32> %590
  %674 = icmp slt <8 x i32> %673, %593
  %675 = select <8 x i1> %674, <8 x i32> %673, <8 x i32> %593
  %676 = bitcast <4 x i64>* %662 to <8 x i32>*
  store <8 x i32> %671, <8 x i32>* %676, align 32
  %677 = bitcast <4 x i64>* %663 to <8 x i32>*
  store <8 x i32> %675, <8 x i32>* %677, align 32
  %678 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %679 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %680 = add <8 x i32> %545, %598
  %681 = sub <8 x i32> %598, %531
  %682 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %680, <4 x i32> %601) #8
  %683 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %681, <4 x i32> %601) #8
  %684 = icmp sgt <8 x i32> %682, %590
  %685 = select <8 x i1> %684, <8 x i32> %682, <8 x i32> %590
  %686 = icmp slt <8 x i32> %685, %593
  %687 = select <8 x i1> %686, <8 x i32> %685, <8 x i32> %593
  %688 = icmp sgt <8 x i32> %683, %590
  %689 = select <8 x i1> %688, <8 x i32> %683, <8 x i32> %590
  %690 = icmp slt <8 x i32> %689, %593
  %691 = select <8 x i1> %690, <8 x i32> %689, <8 x i32> %593
  %692 = bitcast <4 x i64>* %678 to <8 x i32>*
  store <8 x i32> %687, <8 x i32>* %692, align 32
  %693 = bitcast <4 x i64>* %679 to <8 x i32>*
  store <8 x i32> %691, <8 x i32>* %693, align 32
  %694 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %695 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %696 = add <8 x i32> %473, %598
  %697 = sub <8 x i32> %598, %513
  %698 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %696, <4 x i32> %601) #8
  %699 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %697, <4 x i32> %601) #8
  %700 = icmp sgt <8 x i32> %698, %590
  %701 = select <8 x i1> %700, <8 x i32> %698, <8 x i32> %590
  %702 = icmp slt <8 x i32> %701, %593
  %703 = select <8 x i1> %702, <8 x i32> %701, <8 x i32> %593
  %704 = icmp sgt <8 x i32> %699, %590
  %705 = select <8 x i1> %704, <8 x i32> %699, <8 x i32> %590
  %706 = icmp slt <8 x i32> %705, %593
  %707 = select <8 x i1> %706, <8 x i32> %705, <8 x i32> %593
  %708 = bitcast <4 x i64>* %694 to <8 x i32>*
  store <8 x i32> %703, <8 x i32>* %708, align 32
  %709 = bitcast <4 x i64>* %695 to <8 x i32>*
  store <8 x i32> %707, <8 x i32>* %709, align 32
  %710 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %711 = add <8 x i32> %493, %598
  %712 = sub <8 x i32> %598, %453
  %713 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %711, <4 x i32> %601) #8
  %714 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %712, <4 x i32> %601) #8
  %715 = icmp sgt <8 x i32> %713, %590
  %716 = select <8 x i1> %715, <8 x i32> %713, <8 x i32> %590
  %717 = icmp slt <8 x i32> %716, %593
  %718 = select <8 x i1> %717, <8 x i32> %716, <8 x i32> %593
  %719 = icmp sgt <8 x i32> %714, %590
  %720 = select <8 x i1> %719, <8 x i32> %714, <8 x i32> %590
  %721 = icmp slt <8 x i32> %720, %593
  %722 = select <8 x i1> %721, <8 x i32> %720, <8 x i32> %593
  %723 = bitcast <4 x i64>* %710 to <8 x i32>*
  store <8 x i32> %718, <8 x i32>* %723, align 32
  br label %724

724:                                              ; preds = %584, %546
  %725 = phi <8 x i32> [ %722, %584 ], [ %583, %546 ]
  %726 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 15
  %727 = bitcast <4 x i64>* %726 to <8 x i32>*
  store <8 x i32> %725, <8 x i32>* %727, align 32
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @iadst16_avx2(<4 x i64>* nocapture readonly, <4 x i64>*, i32, i32, i32, i32) #0 {
  %7 = add nsw i32 %2, -10
  %8 = sext i32 %7 to i64
  %9 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 2
  %10 = load i32, i32* %9, align 8
  %11 = insertelement <8 x i32> undef, i32 %10, i32 0
  %12 = shufflevector <8 x i32> %11, <8 x i32> undef, <8 x i32> zeroinitializer
  %13 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 62
  %14 = load i32, i32* %13, align 8
  %15 = insertelement <8 x i32> undef, i32 %14, i32 0
  %16 = shufflevector <8 x i32> %15, <8 x i32> undef, <8 x i32> zeroinitializer
  %17 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 10
  %18 = load i32, i32* %17, align 8
  %19 = insertelement <8 x i32> undef, i32 %18, i32 0
  %20 = shufflevector <8 x i32> %19, <8 x i32> undef, <8 x i32> zeroinitializer
  %21 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 54
  %22 = load i32, i32* %21, align 8
  %23 = insertelement <8 x i32> undef, i32 %22, i32 0
  %24 = shufflevector <8 x i32> %23, <8 x i32> undef, <8 x i32> zeroinitializer
  %25 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 18
  %26 = load i32, i32* %25, align 8
  %27 = insertelement <8 x i32> undef, i32 %26, i32 0
  %28 = shufflevector <8 x i32> %27, <8 x i32> undef, <8 x i32> zeroinitializer
  %29 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 46
  %30 = load i32, i32* %29, align 8
  %31 = insertelement <8 x i32> undef, i32 %30, i32 0
  %32 = shufflevector <8 x i32> %31, <8 x i32> undef, <8 x i32> zeroinitializer
  %33 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 26
  %34 = load i32, i32* %33, align 8
  %35 = insertelement <8 x i32> undef, i32 %34, i32 0
  %36 = shufflevector <8 x i32> %35, <8 x i32> undef, <8 x i32> zeroinitializer
  %37 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 38
  %38 = load i32, i32* %37, align 8
  %39 = insertelement <8 x i32> undef, i32 %38, i32 0
  %40 = shufflevector <8 x i32> %39, <8 x i32> undef, <8 x i32> zeroinitializer
  %41 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 34
  %42 = load i32, i32* %41, align 8
  %43 = insertelement <8 x i32> undef, i32 %42, i32 0
  %44 = shufflevector <8 x i32> %43, <8 x i32> undef, <8 x i32> zeroinitializer
  %45 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 30
  %46 = load i32, i32* %45, align 8
  %47 = insertelement <8 x i32> undef, i32 %46, i32 0
  %48 = shufflevector <8 x i32> %47, <8 x i32> undef, <8 x i32> zeroinitializer
  %49 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 42
  %50 = load i32, i32* %49, align 8
  %51 = insertelement <8 x i32> undef, i32 %50, i32 0
  %52 = shufflevector <8 x i32> %51, <8 x i32> undef, <8 x i32> zeroinitializer
  %53 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 22
  %54 = load i32, i32* %53, align 8
  %55 = insertelement <8 x i32> undef, i32 %54, i32 0
  %56 = shufflevector <8 x i32> %55, <8 x i32> undef, <8 x i32> zeroinitializer
  %57 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 50
  %58 = load i32, i32* %57, align 8
  %59 = insertelement <8 x i32> undef, i32 %58, i32 0
  %60 = shufflevector <8 x i32> %59, <8 x i32> undef, <8 x i32> zeroinitializer
  %61 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 14
  %62 = load i32, i32* %61, align 8
  %63 = insertelement <8 x i32> undef, i32 %62, i32 0
  %64 = shufflevector <8 x i32> %63, <8 x i32> undef, <8 x i32> zeroinitializer
  %65 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 58
  %66 = load i32, i32* %65, align 8
  %67 = insertelement <8 x i32> undef, i32 %66, i32 0
  %68 = shufflevector <8 x i32> %67, <8 x i32> undef, <8 x i32> zeroinitializer
  %69 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 6
  %70 = load i32, i32* %69, align 8
  %71 = insertelement <8 x i32> undef, i32 %70, i32 0
  %72 = shufflevector <8 x i32> %71, <8 x i32> undef, <8 x i32> zeroinitializer
  %73 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 8
  %74 = load i32, i32* %73, align 16
  %75 = insertelement <8 x i32> undef, i32 %74, i32 0
  %76 = shufflevector <8 x i32> %75, <8 x i32> undef, <8 x i32> zeroinitializer
  %77 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 56
  %78 = load i32, i32* %77, align 16
  %79 = insertelement <8 x i32> undef, i32 %78, i32 0
  %80 = shufflevector <8 x i32> %79, <8 x i32> undef, <8 x i32> zeroinitializer
  %81 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 40
  %82 = load i32, i32* %81, align 16
  %83 = insertelement <8 x i32> undef, i32 %82, i32 0
  %84 = shufflevector <8 x i32> %83, <8 x i32> undef, <8 x i32> zeroinitializer
  %85 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 24
  %86 = load i32, i32* %85, align 16
  %87 = insertelement <8 x i32> undef, i32 %86, i32 0
  %88 = shufflevector <8 x i32> %87, <8 x i32> undef, <8 x i32> zeroinitializer
  %89 = sub nsw i32 0, %78
  %90 = insertelement <8 x i32> undef, i32 %89, i32 0
  %91 = shufflevector <8 x i32> %90, <8 x i32> undef, <8 x i32> zeroinitializer
  %92 = sub nsw i32 0, %86
  %93 = insertelement <8 x i32> undef, i32 %92, i32 0
  %94 = shufflevector <8 x i32> %93, <8 x i32> undef, <8 x i32> zeroinitializer
  %95 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 48
  %96 = load i32, i32* %95, align 16
  %97 = insertelement <8 x i32> undef, i32 %96, i32 0
  %98 = shufflevector <8 x i32> %97, <8 x i32> undef, <8 x i32> zeroinitializer
  %99 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 16
  %100 = load i32, i32* %99, align 16
  %101 = insertelement <8 x i32> undef, i32 %100, i32 0
  %102 = shufflevector <8 x i32> %101, <8 x i32> undef, <8 x i32> zeroinitializer
  %103 = sub nsw i32 0, %96
  %104 = insertelement <8 x i32> undef, i32 %103, i32 0
  %105 = shufflevector <8 x i32> %104, <8 x i32> undef, <8 x i32> zeroinitializer
  %106 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 32
  %107 = load i32, i32* %106, align 16
  %108 = insertelement <8 x i32> undef, i32 %107, i32 0
  %109 = shufflevector <8 x i32> %108, <8 x i32> undef, <8 x i32> zeroinitializer
  %110 = add nsw i32 %2, -1
  %111 = shl i32 1, %110
  %112 = insertelement <8 x i32> undef, i32 %111, i32 0
  %113 = shufflevector <8 x i32> %112, <8 x i32> undef, <8 x i32> zeroinitializer
  %114 = icmp ne i32 %3, 0
  %115 = select i1 %114, i32 6, i32 8
  %116 = add nsw i32 %115, %4
  %117 = icmp slt i32 %116, 16
  %118 = add i32 %116, -1
  %119 = shl i32 1, %118
  %120 = select i1 %117, i32 32768, i32 %119
  %121 = sub nsw i32 0, %120
  %122 = insertelement <8 x i32> undef, i32 %121, i32 0
  %123 = shufflevector <8 x i32> %122, <8 x i32> undef, <8 x i32> zeroinitializer
  %124 = add nsw i32 %120, -1
  %125 = insertelement <8 x i32> undef, i32 %124, i32 0
  %126 = shufflevector <8 x i32> %125, <8 x i32> undef, <8 x i32> zeroinitializer
  %127 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 15
  %128 = bitcast <4 x i64>* %127 to <8 x i32>*
  %129 = load <8 x i32>, <8 x i32>* %128, align 32
  %130 = mul <8 x i32> %129, %12
  %131 = bitcast <4 x i64>* %0 to <8 x i32>*
  %132 = load <8 x i32>, <8 x i32>* %131, align 32
  %133 = mul <8 x i32> %132, %16
  %134 = add <8 x i32> %130, %113
  %135 = add <8 x i32> %134, %133
  %136 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %135, i32 %2) #8
  %137 = mul <8 x i32> %129, %16
  %138 = mul <8 x i32> %12, %132
  %139 = add <8 x i32> %137, %113
  %140 = sub <8 x i32> %139, %138
  %141 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %140, i32 %2) #8
  %142 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 13
  %143 = bitcast <4 x i64>* %142 to <8 x i32>*
  %144 = load <8 x i32>, <8 x i32>* %143, align 32
  %145 = mul <8 x i32> %144, %20
  %146 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 2
  %147 = bitcast <4 x i64>* %146 to <8 x i32>*
  %148 = load <8 x i32>, <8 x i32>* %147, align 32
  %149 = mul <8 x i32> %148, %24
  %150 = add <8 x i32> %145, %113
  %151 = add <8 x i32> %150, %149
  %152 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %151, i32 %2) #8
  %153 = mul <8 x i32> %144, %24
  %154 = mul <8 x i32> %20, %148
  %155 = add <8 x i32> %153, %113
  %156 = sub <8 x i32> %155, %154
  %157 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %156, i32 %2) #8
  %158 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 11
  %159 = bitcast <4 x i64>* %158 to <8 x i32>*
  %160 = load <8 x i32>, <8 x i32>* %159, align 32
  %161 = mul <8 x i32> %160, %28
  %162 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 4
  %163 = bitcast <4 x i64>* %162 to <8 x i32>*
  %164 = load <8 x i32>, <8 x i32>* %163, align 32
  %165 = mul <8 x i32> %164, %32
  %166 = add <8 x i32> %161, %113
  %167 = add <8 x i32> %166, %165
  %168 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %167, i32 %2) #8
  %169 = mul <8 x i32> %160, %32
  %170 = mul <8 x i32> %28, %164
  %171 = add <8 x i32> %169, %113
  %172 = sub <8 x i32> %171, %170
  %173 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %172, i32 %2) #8
  %174 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 9
  %175 = bitcast <4 x i64>* %174 to <8 x i32>*
  %176 = load <8 x i32>, <8 x i32>* %175, align 32
  %177 = mul <8 x i32> %176, %36
  %178 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 6
  %179 = bitcast <4 x i64>* %178 to <8 x i32>*
  %180 = load <8 x i32>, <8 x i32>* %179, align 32
  %181 = mul <8 x i32> %180, %40
  %182 = add <8 x i32> %177, %113
  %183 = add <8 x i32> %182, %181
  %184 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %183, i32 %2) #8
  %185 = mul <8 x i32> %176, %40
  %186 = mul <8 x i32> %36, %180
  %187 = add <8 x i32> %185, %113
  %188 = sub <8 x i32> %187, %186
  %189 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %188, i32 %2) #8
  %190 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 7
  %191 = bitcast <4 x i64>* %190 to <8 x i32>*
  %192 = load <8 x i32>, <8 x i32>* %191, align 32
  %193 = mul <8 x i32> %192, %44
  %194 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 8
  %195 = bitcast <4 x i64>* %194 to <8 x i32>*
  %196 = load <8 x i32>, <8 x i32>* %195, align 32
  %197 = mul <8 x i32> %196, %48
  %198 = add <8 x i32> %193, %113
  %199 = add <8 x i32> %198, %197
  %200 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %199, i32 %2) #8
  %201 = mul <8 x i32> %192, %48
  %202 = mul <8 x i32> %44, %196
  %203 = add <8 x i32> %201, %113
  %204 = sub <8 x i32> %203, %202
  %205 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %204, i32 %2) #8
  %206 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 5
  %207 = bitcast <4 x i64>* %206 to <8 x i32>*
  %208 = load <8 x i32>, <8 x i32>* %207, align 32
  %209 = mul <8 x i32> %208, %52
  %210 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 10
  %211 = bitcast <4 x i64>* %210 to <8 x i32>*
  %212 = load <8 x i32>, <8 x i32>* %211, align 32
  %213 = mul <8 x i32> %212, %56
  %214 = add <8 x i32> %209, %113
  %215 = add <8 x i32> %214, %213
  %216 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %215, i32 %2) #8
  %217 = mul <8 x i32> %208, %56
  %218 = mul <8 x i32> %52, %212
  %219 = add <8 x i32> %217, %113
  %220 = sub <8 x i32> %219, %218
  %221 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %220, i32 %2) #8
  %222 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 3
  %223 = bitcast <4 x i64>* %222 to <8 x i32>*
  %224 = load <8 x i32>, <8 x i32>* %223, align 32
  %225 = mul <8 x i32> %224, %60
  %226 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 12
  %227 = bitcast <4 x i64>* %226 to <8 x i32>*
  %228 = load <8 x i32>, <8 x i32>* %227, align 32
  %229 = mul <8 x i32> %228, %64
  %230 = add <8 x i32> %225, %113
  %231 = add <8 x i32> %230, %229
  %232 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %231, i32 %2) #8
  %233 = mul <8 x i32> %224, %64
  %234 = mul <8 x i32> %60, %228
  %235 = add <8 x i32> %233, %113
  %236 = sub <8 x i32> %235, %234
  %237 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %236, i32 %2) #8
  %238 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 1
  %239 = bitcast <4 x i64>* %238 to <8 x i32>*
  %240 = load <8 x i32>, <8 x i32>* %239, align 32
  %241 = mul <8 x i32> %240, %68
  %242 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 14
  %243 = bitcast <4 x i64>* %242 to <8 x i32>*
  %244 = load <8 x i32>, <8 x i32>* %243, align 32
  %245 = mul <8 x i32> %244, %72
  %246 = add <8 x i32> %241, %113
  %247 = add <8 x i32> %246, %245
  %248 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %247, i32 %2) #8
  %249 = mul <8 x i32> %240, %72
  %250 = mul <8 x i32> %68, %244
  %251 = add <8 x i32> %249, %113
  %252 = sub <8 x i32> %251, %250
  %253 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %252, i32 %2) #8
  %254 = add <8 x i32> %200, %136
  %255 = sub <8 x i32> %136, %200
  %256 = icmp sgt <8 x i32> %254, %123
  %257 = select <8 x i1> %256, <8 x i32> %254, <8 x i32> %123
  %258 = icmp slt <8 x i32> %257, %126
  %259 = select <8 x i1> %258, <8 x i32> %257, <8 x i32> %126
  %260 = icmp sgt <8 x i32> %255, %123
  %261 = select <8 x i1> %260, <8 x i32> %255, <8 x i32> %123
  %262 = icmp slt <8 x i32> %261, %126
  %263 = select <8 x i1> %262, <8 x i32> %261, <8 x i32> %126
  %264 = add <8 x i32> %205, %141
  %265 = sub <8 x i32> %141, %205
  %266 = icmp sgt <8 x i32> %264, %123
  %267 = select <8 x i1> %266, <8 x i32> %264, <8 x i32> %123
  %268 = icmp slt <8 x i32> %267, %126
  %269 = select <8 x i1> %268, <8 x i32> %267, <8 x i32> %126
  %270 = icmp sgt <8 x i32> %265, %123
  %271 = select <8 x i1> %270, <8 x i32> %265, <8 x i32> %123
  %272 = icmp slt <8 x i32> %271, %126
  %273 = select <8 x i1> %272, <8 x i32> %271, <8 x i32> %126
  %274 = add <8 x i32> %216, %152
  %275 = sub <8 x i32> %152, %216
  %276 = icmp sgt <8 x i32> %274, %123
  %277 = select <8 x i1> %276, <8 x i32> %274, <8 x i32> %123
  %278 = icmp slt <8 x i32> %277, %126
  %279 = select <8 x i1> %278, <8 x i32> %277, <8 x i32> %126
  %280 = icmp sgt <8 x i32> %275, %123
  %281 = select <8 x i1> %280, <8 x i32> %275, <8 x i32> %123
  %282 = icmp slt <8 x i32> %281, %126
  %283 = select <8 x i1> %282, <8 x i32> %281, <8 x i32> %126
  %284 = add <8 x i32> %221, %157
  %285 = sub <8 x i32> %157, %221
  %286 = icmp sgt <8 x i32> %284, %123
  %287 = select <8 x i1> %286, <8 x i32> %284, <8 x i32> %123
  %288 = icmp slt <8 x i32> %287, %126
  %289 = select <8 x i1> %288, <8 x i32> %287, <8 x i32> %126
  %290 = icmp sgt <8 x i32> %285, %123
  %291 = select <8 x i1> %290, <8 x i32> %285, <8 x i32> %123
  %292 = icmp slt <8 x i32> %291, %126
  %293 = select <8 x i1> %292, <8 x i32> %291, <8 x i32> %126
  %294 = add <8 x i32> %232, %168
  %295 = sub <8 x i32> %168, %232
  %296 = icmp sgt <8 x i32> %294, %123
  %297 = select <8 x i1> %296, <8 x i32> %294, <8 x i32> %123
  %298 = icmp slt <8 x i32> %297, %126
  %299 = select <8 x i1> %298, <8 x i32> %297, <8 x i32> %126
  %300 = icmp sgt <8 x i32> %295, %123
  %301 = select <8 x i1> %300, <8 x i32> %295, <8 x i32> %123
  %302 = icmp slt <8 x i32> %301, %126
  %303 = select <8 x i1> %302, <8 x i32> %301, <8 x i32> %126
  %304 = add <8 x i32> %237, %173
  %305 = sub <8 x i32> %173, %237
  %306 = icmp sgt <8 x i32> %304, %123
  %307 = select <8 x i1> %306, <8 x i32> %304, <8 x i32> %123
  %308 = icmp slt <8 x i32> %307, %126
  %309 = select <8 x i1> %308, <8 x i32> %307, <8 x i32> %126
  %310 = icmp sgt <8 x i32> %305, %123
  %311 = select <8 x i1> %310, <8 x i32> %305, <8 x i32> %123
  %312 = icmp slt <8 x i32> %311, %126
  %313 = select <8 x i1> %312, <8 x i32> %311, <8 x i32> %126
  %314 = add <8 x i32> %248, %184
  %315 = sub <8 x i32> %184, %248
  %316 = icmp sgt <8 x i32> %314, %123
  %317 = select <8 x i1> %316, <8 x i32> %314, <8 x i32> %123
  %318 = icmp slt <8 x i32> %317, %126
  %319 = select <8 x i1> %318, <8 x i32> %317, <8 x i32> %126
  %320 = icmp sgt <8 x i32> %315, %123
  %321 = select <8 x i1> %320, <8 x i32> %315, <8 x i32> %123
  %322 = icmp slt <8 x i32> %321, %126
  %323 = select <8 x i1> %322, <8 x i32> %321, <8 x i32> %126
  %324 = add <8 x i32> %253, %189
  %325 = sub <8 x i32> %189, %253
  %326 = icmp sgt <8 x i32> %324, %123
  %327 = select <8 x i1> %326, <8 x i32> %324, <8 x i32> %123
  %328 = icmp slt <8 x i32> %327, %126
  %329 = select <8 x i1> %328, <8 x i32> %327, <8 x i32> %126
  %330 = icmp sgt <8 x i32> %325, %123
  %331 = select <8 x i1> %330, <8 x i32> %325, <8 x i32> %123
  %332 = icmp slt <8 x i32> %331, %126
  %333 = select <8 x i1> %332, <8 x i32> %331, <8 x i32> %126
  %334 = mul <8 x i32> %263, %76
  %335 = mul <8 x i32> %273, %80
  %336 = add <8 x i32> %334, %113
  %337 = add <8 x i32> %336, %335
  %338 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %337, i32 %2) #8
  %339 = mul <8 x i32> %263, %80
  %340 = mul <8 x i32> %76, %273
  %341 = add <8 x i32> %339, %113
  %342 = sub <8 x i32> %341, %340
  %343 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %342, i32 %2) #8
  %344 = mul <8 x i32> %283, %84
  %345 = mul <8 x i32> %293, %88
  %346 = add <8 x i32> %344, %113
  %347 = add <8 x i32> %346, %345
  %348 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %347, i32 %2) #8
  %349 = mul <8 x i32> %283, %88
  %350 = mul <8 x i32> %84, %293
  %351 = add <8 x i32> %349, %113
  %352 = sub <8 x i32> %351, %350
  %353 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %352, i32 %2) #8
  %354 = mul <8 x i32> %303, %91
  %355 = mul <8 x i32> %313, %76
  %356 = add <8 x i32> %354, %113
  %357 = add <8 x i32> %356, %355
  %358 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %357, i32 %2) #8
  %359 = mul <8 x i32> %303, %76
  %360 = mul <8 x i32> %91, %313
  %361 = add <8 x i32> %359, %113
  %362 = sub <8 x i32> %361, %360
  %363 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %362, i32 %2) #8
  %364 = mul <8 x i32> %323, %94
  %365 = mul <8 x i32> %333, %84
  %366 = add <8 x i32> %364, %113
  %367 = add <8 x i32> %366, %365
  %368 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %367, i32 %2) #8
  %369 = mul <8 x i32> %323, %84
  %370 = mul <8 x i32> %94, %333
  %371 = add <8 x i32> %369, %113
  %372 = sub <8 x i32> %371, %370
  %373 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %372, i32 %2) #8
  %374 = add <8 x i32> %299, %259
  %375 = sub <8 x i32> %259, %299
  %376 = icmp sgt <8 x i32> %374, %123
  %377 = select <8 x i1> %376, <8 x i32> %374, <8 x i32> %123
  %378 = icmp slt <8 x i32> %377, %126
  %379 = select <8 x i1> %378, <8 x i32> %377, <8 x i32> %126
  %380 = icmp sgt <8 x i32> %375, %123
  %381 = select <8 x i1> %380, <8 x i32> %375, <8 x i32> %123
  %382 = icmp slt <8 x i32> %381, %126
  %383 = select <8 x i1> %382, <8 x i32> %381, <8 x i32> %126
  %384 = add <8 x i32> %309, %269
  %385 = sub <8 x i32> %269, %309
  %386 = icmp sgt <8 x i32> %384, %123
  %387 = select <8 x i1> %386, <8 x i32> %384, <8 x i32> %123
  %388 = icmp slt <8 x i32> %387, %126
  %389 = select <8 x i1> %388, <8 x i32> %387, <8 x i32> %126
  %390 = icmp sgt <8 x i32> %385, %123
  %391 = select <8 x i1> %390, <8 x i32> %385, <8 x i32> %123
  %392 = icmp slt <8 x i32> %391, %126
  %393 = select <8 x i1> %392, <8 x i32> %391, <8 x i32> %126
  %394 = add <8 x i32> %319, %279
  %395 = sub <8 x i32> %279, %319
  %396 = icmp sgt <8 x i32> %394, %123
  %397 = select <8 x i1> %396, <8 x i32> %394, <8 x i32> %123
  %398 = icmp slt <8 x i32> %397, %126
  %399 = select <8 x i1> %398, <8 x i32> %397, <8 x i32> %126
  %400 = icmp sgt <8 x i32> %395, %123
  %401 = select <8 x i1> %400, <8 x i32> %395, <8 x i32> %123
  %402 = icmp slt <8 x i32> %401, %126
  %403 = select <8 x i1> %402, <8 x i32> %401, <8 x i32> %126
  %404 = add <8 x i32> %329, %289
  %405 = sub <8 x i32> %289, %329
  %406 = icmp sgt <8 x i32> %404, %123
  %407 = select <8 x i1> %406, <8 x i32> %404, <8 x i32> %123
  %408 = icmp slt <8 x i32> %407, %126
  %409 = select <8 x i1> %408, <8 x i32> %407, <8 x i32> %126
  %410 = icmp sgt <8 x i32> %405, %123
  %411 = select <8 x i1> %410, <8 x i32> %405, <8 x i32> %123
  %412 = icmp slt <8 x i32> %411, %126
  %413 = select <8 x i1> %412, <8 x i32> %411, <8 x i32> %126
  %414 = add <8 x i32> %358, %338
  %415 = sub <8 x i32> %338, %358
  %416 = icmp sgt <8 x i32> %414, %123
  %417 = select <8 x i1> %416, <8 x i32> %414, <8 x i32> %123
  %418 = icmp slt <8 x i32> %417, %126
  %419 = select <8 x i1> %418, <8 x i32> %417, <8 x i32> %126
  %420 = icmp sgt <8 x i32> %415, %123
  %421 = select <8 x i1> %420, <8 x i32> %415, <8 x i32> %123
  %422 = icmp slt <8 x i32> %421, %126
  %423 = select <8 x i1> %422, <8 x i32> %421, <8 x i32> %126
  %424 = add <8 x i32> %363, %343
  %425 = sub <8 x i32> %343, %363
  %426 = icmp sgt <8 x i32> %424, %123
  %427 = select <8 x i1> %426, <8 x i32> %424, <8 x i32> %123
  %428 = icmp slt <8 x i32> %427, %126
  %429 = select <8 x i1> %428, <8 x i32> %427, <8 x i32> %126
  %430 = icmp sgt <8 x i32> %425, %123
  %431 = select <8 x i1> %430, <8 x i32> %425, <8 x i32> %123
  %432 = icmp slt <8 x i32> %431, %126
  %433 = select <8 x i1> %432, <8 x i32> %431, <8 x i32> %126
  %434 = add <8 x i32> %368, %348
  %435 = sub <8 x i32> %348, %368
  %436 = icmp sgt <8 x i32> %434, %123
  %437 = select <8 x i1> %436, <8 x i32> %434, <8 x i32> %123
  %438 = icmp slt <8 x i32> %437, %126
  %439 = select <8 x i1> %438, <8 x i32> %437, <8 x i32> %126
  %440 = icmp sgt <8 x i32> %435, %123
  %441 = select <8 x i1> %440, <8 x i32> %435, <8 x i32> %123
  %442 = icmp slt <8 x i32> %441, %126
  %443 = select <8 x i1> %442, <8 x i32> %441, <8 x i32> %126
  %444 = add <8 x i32> %373, %353
  %445 = sub <8 x i32> %353, %373
  %446 = icmp sgt <8 x i32> %444, %123
  %447 = select <8 x i1> %446, <8 x i32> %444, <8 x i32> %123
  %448 = icmp slt <8 x i32> %447, %126
  %449 = select <8 x i1> %448, <8 x i32> %447, <8 x i32> %126
  %450 = icmp sgt <8 x i32> %445, %123
  %451 = select <8 x i1> %450, <8 x i32> %445, <8 x i32> %123
  %452 = icmp slt <8 x i32> %451, %126
  %453 = select <8 x i1> %452, <8 x i32> %451, <8 x i32> %126
  %454 = mul <8 x i32> %383, %102
  %455 = mul <8 x i32> %393, %98
  %456 = add <8 x i32> %454, %113
  %457 = add <8 x i32> %456, %455
  %458 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %457, i32 %2) #8
  %459 = mul <8 x i32> %383, %98
  %460 = mul <8 x i32> %102, %393
  %461 = add <8 x i32> %459, %113
  %462 = sub <8 x i32> %461, %460
  %463 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %462, i32 %2) #8
  %464 = mul <8 x i32> %403, %105
  %465 = mul <8 x i32> %413, %102
  %466 = add <8 x i32> %464, %113
  %467 = add <8 x i32> %466, %465
  %468 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %467, i32 %2) #8
  %469 = mul <8 x i32> %403, %102
  %470 = mul <8 x i32> %105, %413
  %471 = add <8 x i32> %469, %113
  %472 = sub <8 x i32> %471, %470
  %473 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %472, i32 %2) #8
  %474 = mul <8 x i32> %423, %102
  %475 = mul <8 x i32> %433, %98
  %476 = add <8 x i32> %474, %113
  %477 = add <8 x i32> %476, %475
  %478 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %477, i32 %2) #8
  %479 = mul <8 x i32> %423, %98
  %480 = mul <8 x i32> %102, %433
  %481 = add <8 x i32> %479, %113
  %482 = sub <8 x i32> %481, %480
  %483 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %482, i32 %2) #8
  %484 = mul <8 x i32> %443, %105
  %485 = mul <8 x i32> %453, %102
  %486 = add <8 x i32> %484, %113
  %487 = add <8 x i32> %486, %485
  %488 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %487, i32 %2) #8
  %489 = mul <8 x i32> %443, %102
  %490 = mul <8 x i32> %105, %453
  %491 = add <8 x i32> %489, %113
  %492 = sub <8 x i32> %491, %490
  %493 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %492, i32 %2) #8
  %494 = add <8 x i32> %399, %379
  %495 = sub <8 x i32> %379, %399
  %496 = icmp sgt <8 x i32> %494, %123
  %497 = select <8 x i1> %496, <8 x i32> %494, <8 x i32> %123
  %498 = icmp slt <8 x i32> %497, %126
  %499 = select <8 x i1> %498, <8 x i32> %497, <8 x i32> %126
  %500 = icmp sgt <8 x i32> %495, %123
  %501 = select <8 x i1> %500, <8 x i32> %495, <8 x i32> %123
  %502 = icmp slt <8 x i32> %501, %126
  %503 = select <8 x i1> %502, <8 x i32> %501, <8 x i32> %126
  %504 = add <8 x i32> %409, %389
  %505 = sub <8 x i32> %389, %409
  %506 = icmp sgt <8 x i32> %504, %123
  %507 = select <8 x i1> %506, <8 x i32> %504, <8 x i32> %123
  %508 = icmp slt <8 x i32> %507, %126
  %509 = select <8 x i1> %508, <8 x i32> %507, <8 x i32> %126
  %510 = icmp sgt <8 x i32> %505, %123
  %511 = select <8 x i1> %510, <8 x i32> %505, <8 x i32> %123
  %512 = icmp slt <8 x i32> %511, %126
  %513 = select <8 x i1> %512, <8 x i32> %511, <8 x i32> %126
  %514 = add <8 x i32> %468, %458
  %515 = sub <8 x i32> %458, %468
  %516 = icmp sgt <8 x i32> %514, %123
  %517 = select <8 x i1> %516, <8 x i32> %514, <8 x i32> %123
  %518 = icmp slt <8 x i32> %517, %126
  %519 = select <8 x i1> %518, <8 x i32> %517, <8 x i32> %126
  %520 = icmp sgt <8 x i32> %515, %123
  %521 = select <8 x i1> %520, <8 x i32> %515, <8 x i32> %123
  %522 = icmp slt <8 x i32> %521, %126
  %523 = select <8 x i1> %522, <8 x i32> %521, <8 x i32> %126
  %524 = add <8 x i32> %473, %463
  %525 = sub <8 x i32> %463, %473
  %526 = icmp sgt <8 x i32> %524, %123
  %527 = select <8 x i1> %526, <8 x i32> %524, <8 x i32> %123
  %528 = icmp slt <8 x i32> %527, %126
  %529 = select <8 x i1> %528, <8 x i32> %527, <8 x i32> %126
  %530 = icmp sgt <8 x i32> %525, %123
  %531 = select <8 x i1> %530, <8 x i32> %525, <8 x i32> %123
  %532 = icmp slt <8 x i32> %531, %126
  %533 = select <8 x i1> %532, <8 x i32> %531, <8 x i32> %126
  %534 = add <8 x i32> %439, %419
  %535 = sub <8 x i32> %419, %439
  %536 = icmp sgt <8 x i32> %534, %123
  %537 = select <8 x i1> %536, <8 x i32> %534, <8 x i32> %123
  %538 = icmp slt <8 x i32> %537, %126
  %539 = select <8 x i1> %538, <8 x i32> %537, <8 x i32> %126
  %540 = icmp sgt <8 x i32> %535, %123
  %541 = select <8 x i1> %540, <8 x i32> %535, <8 x i32> %123
  %542 = icmp slt <8 x i32> %541, %126
  %543 = select <8 x i1> %542, <8 x i32> %541, <8 x i32> %126
  %544 = add <8 x i32> %449, %429
  %545 = sub <8 x i32> %429, %449
  %546 = icmp sgt <8 x i32> %544, %123
  %547 = select <8 x i1> %546, <8 x i32> %544, <8 x i32> %123
  %548 = icmp slt <8 x i32> %547, %126
  %549 = select <8 x i1> %548, <8 x i32> %547, <8 x i32> %126
  %550 = icmp sgt <8 x i32> %545, %123
  %551 = select <8 x i1> %550, <8 x i32> %545, <8 x i32> %123
  %552 = icmp slt <8 x i32> %551, %126
  %553 = select <8 x i1> %552, <8 x i32> %551, <8 x i32> %126
  %554 = add <8 x i32> %488, %478
  %555 = sub <8 x i32> %478, %488
  %556 = icmp sgt <8 x i32> %554, %123
  %557 = select <8 x i1> %556, <8 x i32> %554, <8 x i32> %123
  %558 = icmp slt <8 x i32> %557, %126
  %559 = select <8 x i1> %558, <8 x i32> %557, <8 x i32> %126
  %560 = icmp sgt <8 x i32> %555, %123
  %561 = select <8 x i1> %560, <8 x i32> %555, <8 x i32> %123
  %562 = icmp slt <8 x i32> %561, %126
  %563 = select <8 x i1> %562, <8 x i32> %561, <8 x i32> %126
  %564 = add <8 x i32> %493, %483
  %565 = sub <8 x i32> %483, %493
  %566 = icmp sgt <8 x i32> %564, %123
  %567 = select <8 x i1> %566, <8 x i32> %564, <8 x i32> %123
  %568 = icmp slt <8 x i32> %567, %126
  %569 = select <8 x i1> %568, <8 x i32> %567, <8 x i32> %126
  %570 = icmp sgt <8 x i32> %565, %123
  %571 = select <8 x i1> %570, <8 x i32> %565, <8 x i32> %123
  %572 = icmp slt <8 x i32> %571, %126
  %573 = select <8 x i1> %572, <8 x i32> %571, <8 x i32> %126
  %574 = mul <8 x i32> %503, %109
  %575 = mul <8 x i32> %513, %109
  %576 = add <8 x i32> %574, %113
  %577 = add <8 x i32> %576, %575
  %578 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %577, i32 %2) #8
  %579 = sub <8 x i32> %576, %575
  %580 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %579, i32 %2) #8
  %581 = mul <8 x i32> %523, %109
  %582 = mul <8 x i32> %533, %109
  %583 = add <8 x i32> %581, %113
  %584 = add <8 x i32> %583, %582
  %585 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %584, i32 %2) #8
  %586 = sub <8 x i32> %583, %582
  %587 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %586, i32 %2) #8
  %588 = mul <8 x i32> %543, %109
  %589 = mul <8 x i32> %553, %109
  %590 = add <8 x i32> %588, %113
  %591 = add <8 x i32> %590, %589
  %592 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %591, i32 %2) #8
  %593 = sub <8 x i32> %590, %589
  %594 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %593, i32 %2) #8
  %595 = mul <8 x i32> %563, %109
  %596 = mul <8 x i32> %573, %109
  %597 = add <8 x i32> %595, %113
  %598 = add <8 x i32> %597, %596
  %599 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %598, i32 %2) #8
  %600 = sub <8 x i32> %597, %596
  %601 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %600, i32 %2) #8
  br i1 %114, label %602, label %640

602:                                              ; preds = %6
  %603 = bitcast <4 x i64>* %1 to <8 x i32>*
  store <8 x i32> %499, <8 x i32>* %603, align 32
  %604 = sub <8 x i32> zeroinitializer, %539
  %605 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %606 = bitcast <4 x i64>* %605 to <8 x i32>*
  store <8 x i32> %604, <8 x i32>* %606, align 32
  %607 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %608 = bitcast <4 x i64>* %607 to <8 x i32>*
  store <8 x i32> %559, <8 x i32>* %608, align 32
  %609 = sub <8 x i32> zeroinitializer, %519
  %610 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %611 = bitcast <4 x i64>* %610 to <8 x i32>*
  store <8 x i32> %609, <8 x i32>* %611, align 32
  %612 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %613 = bitcast <4 x i64>* %612 to <8 x i32>*
  store <8 x i32> %585, <8 x i32>* %613, align 32
  %614 = sub <8 x i32> zeroinitializer, %599
  %615 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %616 = bitcast <4 x i64>* %615 to <8 x i32>*
  store <8 x i32> %614, <8 x i32>* %616, align 32
  %617 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %618 = bitcast <4 x i64>* %617 to <8 x i32>*
  store <8 x i32> %592, <8 x i32>* %618, align 32
  %619 = sub <8 x i32> zeroinitializer, %578
  %620 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %621 = bitcast <4 x i64>* %620 to <8 x i32>*
  store <8 x i32> %619, <8 x i32>* %621, align 32
  %622 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %623 = bitcast <4 x i64>* %622 to <8 x i32>*
  store <8 x i32> %580, <8 x i32>* %623, align 32
  %624 = sub <8 x i32> zeroinitializer, %594
  %625 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %626 = bitcast <4 x i64>* %625 to <8 x i32>*
  store <8 x i32> %624, <8 x i32>* %626, align 32
  %627 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %628 = bitcast <4 x i64>* %627 to <8 x i32>*
  store <8 x i32> %601, <8 x i32>* %628, align 32
  %629 = sub <8 x i32> zeroinitializer, %587
  %630 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %631 = bitcast <4 x i64>* %630 to <8 x i32>*
  store <8 x i32> %629, <8 x i32>* %631, align 32
  %632 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %633 = bitcast <4 x i64>* %632 to <8 x i32>*
  store <8 x i32> %529, <8 x i32>* %633, align 32
  %634 = sub <8 x i32> zeroinitializer, %569
  %635 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %636 = bitcast <4 x i64>* %635 to <8 x i32>*
  store <8 x i32> %634, <8 x i32>* %636, align 32
  %637 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %638 = bitcast <4 x i64>* %637 to <8 x i32>*
  store <8 x i32> %549, <8 x i32>* %638, align 32
  %639 = sub <8 x i32> zeroinitializer, %509
  br label %780

640:                                              ; preds = %6
  %641 = icmp sgt i32 %4, 10
  %642 = select i1 %641, i32 %4, i32 10
  %643 = shl i32 32, %642
  %644 = sub nsw i32 0, %643
  %645 = insertelement <8 x i32> undef, i32 %644, i32 0
  %646 = shufflevector <8 x i32> %645, <8 x i32> undef, <8 x i32> zeroinitializer
  %647 = add nsw i32 %643, -1
  %648 = insertelement <8 x i32> undef, i32 %647, i32 0
  %649 = shufflevector <8 x i32> %648, <8 x i32> undef, <8 x i32> zeroinitializer
  %650 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %651 = shl i32 1, %5
  %652 = ashr i32 %651, 1
  %653 = insertelement <8 x i32> undef, i32 %652, i32 0
  %654 = shufflevector <8 x i32> %653, <8 x i32> undef, <8 x i32> zeroinitializer
  %655 = add <8 x i32> %499, %654
  %656 = sub <8 x i32> %654, %539
  %657 = insertelement <4 x i32> <i32 undef, i32 0, i32 0, i32 0>, i32 %5, i32 0
  %658 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %655, <4 x i32> %657) #8
  %659 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %656, <4 x i32> %657) #8
  %660 = icmp sgt <8 x i32> %658, %646
  %661 = select <8 x i1> %660, <8 x i32> %658, <8 x i32> %646
  %662 = icmp slt <8 x i32> %661, %649
  %663 = select <8 x i1> %662, <8 x i32> %661, <8 x i32> %649
  %664 = icmp sgt <8 x i32> %659, %646
  %665 = select <8 x i1> %664, <8 x i32> %659, <8 x i32> %646
  %666 = icmp slt <8 x i32> %665, %649
  %667 = select <8 x i1> %666, <8 x i32> %665, <8 x i32> %649
  %668 = bitcast <4 x i64>* %1 to <8 x i32>*
  store <8 x i32> %663, <8 x i32>* %668, align 32
  %669 = bitcast <4 x i64>* %650 to <8 x i32>*
  store <8 x i32> %667, <8 x i32>* %669, align 32
  %670 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %671 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %672 = add <8 x i32> %559, %654
  %673 = sub <8 x i32> %654, %519
  %674 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %672, <4 x i32> %657) #8
  %675 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %673, <4 x i32> %657) #8
  %676 = icmp sgt <8 x i32> %674, %646
  %677 = select <8 x i1> %676, <8 x i32> %674, <8 x i32> %646
  %678 = icmp slt <8 x i32> %677, %649
  %679 = select <8 x i1> %678, <8 x i32> %677, <8 x i32> %649
  %680 = icmp sgt <8 x i32> %675, %646
  %681 = select <8 x i1> %680, <8 x i32> %675, <8 x i32> %646
  %682 = icmp slt <8 x i32> %681, %649
  %683 = select <8 x i1> %682, <8 x i32> %681, <8 x i32> %649
  %684 = bitcast <4 x i64>* %670 to <8 x i32>*
  store <8 x i32> %679, <8 x i32>* %684, align 32
  %685 = bitcast <4 x i64>* %671 to <8 x i32>*
  store <8 x i32> %683, <8 x i32>* %685, align 32
  %686 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %687 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %688 = add <8 x i32> %585, %654
  %689 = sub <8 x i32> %654, %599
  %690 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %688, <4 x i32> %657) #8
  %691 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %689, <4 x i32> %657) #8
  %692 = icmp sgt <8 x i32> %690, %646
  %693 = select <8 x i1> %692, <8 x i32> %690, <8 x i32> %646
  %694 = icmp slt <8 x i32> %693, %649
  %695 = select <8 x i1> %694, <8 x i32> %693, <8 x i32> %649
  %696 = icmp sgt <8 x i32> %691, %646
  %697 = select <8 x i1> %696, <8 x i32> %691, <8 x i32> %646
  %698 = icmp slt <8 x i32> %697, %649
  %699 = select <8 x i1> %698, <8 x i32> %697, <8 x i32> %649
  %700 = bitcast <4 x i64>* %686 to <8 x i32>*
  store <8 x i32> %695, <8 x i32>* %700, align 32
  %701 = bitcast <4 x i64>* %687 to <8 x i32>*
  store <8 x i32> %699, <8 x i32>* %701, align 32
  %702 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %703 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %704 = add <8 x i32> %592, %654
  %705 = sub <8 x i32> %654, %578
  %706 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %704, <4 x i32> %657) #8
  %707 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %705, <4 x i32> %657) #8
  %708 = icmp sgt <8 x i32> %706, %646
  %709 = select <8 x i1> %708, <8 x i32> %706, <8 x i32> %646
  %710 = icmp slt <8 x i32> %709, %649
  %711 = select <8 x i1> %710, <8 x i32> %709, <8 x i32> %649
  %712 = icmp sgt <8 x i32> %707, %646
  %713 = select <8 x i1> %712, <8 x i32> %707, <8 x i32> %646
  %714 = icmp slt <8 x i32> %713, %649
  %715 = select <8 x i1> %714, <8 x i32> %713, <8 x i32> %649
  %716 = bitcast <4 x i64>* %702 to <8 x i32>*
  store <8 x i32> %711, <8 x i32>* %716, align 32
  %717 = bitcast <4 x i64>* %703 to <8 x i32>*
  store <8 x i32> %715, <8 x i32>* %717, align 32
  %718 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %719 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %720 = add <8 x i32> %580, %654
  %721 = sub <8 x i32> %654, %594
  %722 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %720, <4 x i32> %657) #8
  %723 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %721, <4 x i32> %657) #8
  %724 = icmp sgt <8 x i32> %722, %646
  %725 = select <8 x i1> %724, <8 x i32> %722, <8 x i32> %646
  %726 = icmp slt <8 x i32> %725, %649
  %727 = select <8 x i1> %726, <8 x i32> %725, <8 x i32> %649
  %728 = icmp sgt <8 x i32> %723, %646
  %729 = select <8 x i1> %728, <8 x i32> %723, <8 x i32> %646
  %730 = icmp slt <8 x i32> %729, %649
  %731 = select <8 x i1> %730, <8 x i32> %729, <8 x i32> %649
  %732 = bitcast <4 x i64>* %718 to <8 x i32>*
  store <8 x i32> %727, <8 x i32>* %732, align 32
  %733 = bitcast <4 x i64>* %719 to <8 x i32>*
  store <8 x i32> %731, <8 x i32>* %733, align 32
  %734 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %735 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %736 = add <8 x i32> %601, %654
  %737 = sub <8 x i32> %654, %587
  %738 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %736, <4 x i32> %657) #8
  %739 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %737, <4 x i32> %657) #8
  %740 = icmp sgt <8 x i32> %738, %646
  %741 = select <8 x i1> %740, <8 x i32> %738, <8 x i32> %646
  %742 = icmp slt <8 x i32> %741, %649
  %743 = select <8 x i1> %742, <8 x i32> %741, <8 x i32> %649
  %744 = icmp sgt <8 x i32> %739, %646
  %745 = select <8 x i1> %744, <8 x i32> %739, <8 x i32> %646
  %746 = icmp slt <8 x i32> %745, %649
  %747 = select <8 x i1> %746, <8 x i32> %745, <8 x i32> %649
  %748 = bitcast <4 x i64>* %734 to <8 x i32>*
  store <8 x i32> %743, <8 x i32>* %748, align 32
  %749 = bitcast <4 x i64>* %735 to <8 x i32>*
  store <8 x i32> %747, <8 x i32>* %749, align 32
  %750 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %751 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %752 = add <8 x i32> %529, %654
  %753 = sub <8 x i32> %654, %569
  %754 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %752, <4 x i32> %657) #8
  %755 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %753, <4 x i32> %657) #8
  %756 = icmp sgt <8 x i32> %754, %646
  %757 = select <8 x i1> %756, <8 x i32> %754, <8 x i32> %646
  %758 = icmp slt <8 x i32> %757, %649
  %759 = select <8 x i1> %758, <8 x i32> %757, <8 x i32> %649
  %760 = icmp sgt <8 x i32> %755, %646
  %761 = select <8 x i1> %760, <8 x i32> %755, <8 x i32> %646
  %762 = icmp slt <8 x i32> %761, %649
  %763 = select <8 x i1> %762, <8 x i32> %761, <8 x i32> %649
  %764 = bitcast <4 x i64>* %750 to <8 x i32>*
  store <8 x i32> %759, <8 x i32>* %764, align 32
  %765 = bitcast <4 x i64>* %751 to <8 x i32>*
  store <8 x i32> %763, <8 x i32>* %765, align 32
  %766 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %767 = add <8 x i32> %549, %654
  %768 = sub <8 x i32> %654, %509
  %769 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %767, <4 x i32> %657) #8
  %770 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %768, <4 x i32> %657) #8
  %771 = icmp sgt <8 x i32> %769, %646
  %772 = select <8 x i1> %771, <8 x i32> %769, <8 x i32> %646
  %773 = icmp slt <8 x i32> %772, %649
  %774 = select <8 x i1> %773, <8 x i32> %772, <8 x i32> %649
  %775 = icmp sgt <8 x i32> %770, %646
  %776 = select <8 x i1> %775, <8 x i32> %770, <8 x i32> %646
  %777 = icmp slt <8 x i32> %776, %649
  %778 = select <8 x i1> %777, <8 x i32> %776, <8 x i32> %649
  %779 = bitcast <4 x i64>* %766 to <8 x i32>*
  store <8 x i32> %774, <8 x i32>* %779, align 32
  br label %780

780:                                              ; preds = %640, %602
  %781 = phi <8 x i32> [ %778, %640 ], [ %639, %602 ]
  %782 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 15
  %783 = bitcast <4 x i64>* %782 to <8 x i32>*
  store <8 x i32> %781, <8 x i32>* %783, align 32
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define internal void @idct32_low1_avx2(<4 x i64>* nocapture readonly, <4 x i64>*, i32, i32, i32, i32) #4 {
  %7 = add nsw i32 %2, -10
  %8 = sext i32 %7 to i64
  %9 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 32
  %10 = load i32, i32* %9, align 16
  %11 = insertelement <8 x i32> undef, i32 %10, i32 0
  %12 = shufflevector <8 x i32> %11, <8 x i32> undef, <8 x i32> zeroinitializer
  %13 = add nsw i32 %2, -1
  %14 = shl i32 1, %13
  %15 = insertelement <8 x i32> undef, i32 %14, i32 0
  %16 = shufflevector <8 x i32> %15, <8 x i32> undef, <8 x i32> zeroinitializer
  %17 = icmp ne i32 %3, 0
  %18 = select i1 %17, i32 6, i32 8
  %19 = add nsw i32 %18, %4
  %20 = icmp slt i32 %19, 16
  %21 = add i32 %19, -1
  %22 = shl i32 1, %21
  %23 = select i1 %20, i32 32768, i32 %22
  %24 = sub nsw i32 0, %23
  %25 = insertelement <8 x i32> undef, i32 %24, i32 0
  %26 = shufflevector <8 x i32> %25, <8 x i32> undef, <8 x i32> zeroinitializer
  %27 = add nsw i32 %23, -1
  %28 = insertelement <8 x i32> undef, i32 %27, i32 0
  %29 = shufflevector <8 x i32> %28, <8 x i32> undef, <8 x i32> zeroinitializer
  %30 = bitcast <4 x i64>* %0 to <8 x i32>*
  %31 = load <8 x i32>, <8 x i32>* %30, align 32
  %32 = mul <8 x i32> %12, %31
  %33 = add <8 x i32> %32, %16
  %34 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %33, i32 %2) #8
  br i1 %17, label %52, label %35

35:                                               ; preds = %6
  %36 = icmp sgt i32 %4, 10
  %37 = select i1 %36, i32 %4, i32 10
  %38 = shl i32 1, %5
  %39 = ashr i32 %38, 1
  %40 = insertelement <8 x i32> undef, i32 %39, i32 0
  %41 = shufflevector <8 x i32> %40, <8 x i32> undef, <8 x i32> zeroinitializer
  %42 = shl i32 32, %37
  %43 = sub nsw i32 0, %42
  %44 = insertelement <8 x i32> undef, i32 %43, i32 0
  %45 = shufflevector <8 x i32> %44, <8 x i32> undef, <8 x i32> zeroinitializer
  %46 = add nsw i32 %42, -1
  %47 = insertelement <8 x i32> undef, i32 %46, i32 0
  %48 = shufflevector <8 x i32> %47, <8 x i32> undef, <8 x i32> zeroinitializer
  %49 = add <8 x i32> %34, %41
  %50 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %5, i32 0
  %51 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %49, <4 x i32> %50) #8
  br label %52

52:                                               ; preds = %35, %6
  %53 = phi <8 x i32> [ %26, %6 ], [ %45, %35 ]
  %54 = phi <8 x i32> [ %29, %6 ], [ %48, %35 ]
  %55 = phi <8 x i32> [ %34, %6 ], [ %51, %35 ]
  %56 = icmp sgt <8 x i32> %55, %53
  %57 = select <8 x i1> %56, <8 x i32> %55, <8 x i32> %53
  %58 = icmp slt <8 x i32> %57, %54
  %59 = select <8 x i1> %58, <8 x i32> %57, <8 x i32> %54
  %60 = bitcast <4 x i64>* %1 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %60, align 32
  %61 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %62 = bitcast <4 x i64>* %61 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %62, align 32
  %63 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %64 = bitcast <4 x i64>* %63 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %64, align 32
  %65 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %66 = bitcast <4 x i64>* %65 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %66, align 32
  %67 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %68 = bitcast <4 x i64>* %67 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %68, align 32
  %69 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %70 = bitcast <4 x i64>* %69 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %70, align 32
  %71 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %72 = bitcast <4 x i64>* %71 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %72, align 32
  %73 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %74 = bitcast <4 x i64>* %73 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %74, align 32
  %75 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %76 = bitcast <4 x i64>* %75 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %76, align 32
  %77 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %78 = bitcast <4 x i64>* %77 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %78, align 32
  %79 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %80 = bitcast <4 x i64>* %79 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %80, align 32
  %81 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %82 = bitcast <4 x i64>* %81 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %82, align 32
  %83 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %84 = bitcast <4 x i64>* %83 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %84, align 32
  %85 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %86 = bitcast <4 x i64>* %85 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %86, align 32
  %87 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %88 = bitcast <4 x i64>* %87 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %88, align 32
  %89 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 15
  %90 = bitcast <4 x i64>* %89 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %90, align 32
  %91 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 16
  %92 = bitcast <4 x i64>* %91 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %92, align 32
  %93 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 17
  %94 = bitcast <4 x i64>* %93 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %94, align 32
  %95 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 18
  %96 = bitcast <4 x i64>* %95 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %96, align 32
  %97 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 19
  %98 = bitcast <4 x i64>* %97 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %98, align 32
  %99 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 20
  %100 = bitcast <4 x i64>* %99 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %100, align 32
  %101 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 21
  %102 = bitcast <4 x i64>* %101 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %102, align 32
  %103 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 22
  %104 = bitcast <4 x i64>* %103 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %104, align 32
  %105 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 23
  %106 = bitcast <4 x i64>* %105 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %106, align 32
  %107 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 24
  %108 = bitcast <4 x i64>* %107 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %108, align 32
  %109 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 25
  %110 = bitcast <4 x i64>* %109 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %110, align 32
  %111 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 26
  %112 = bitcast <4 x i64>* %111 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %112, align 32
  %113 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 27
  %114 = bitcast <4 x i64>* %113 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %114, align 32
  %115 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 28
  %116 = bitcast <4 x i64>* %115 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %116, align 32
  %117 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 29
  %118 = bitcast <4 x i64>* %117 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %118, align 32
  %119 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 30
  %120 = bitcast <4 x i64>* %119 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %120, align 32
  %121 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 31
  %122 = bitcast <4 x i64>* %121 to <8 x i32>*
  store <8 x i32> %59, <8 x i32>* %122, align 32
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @idct32_low8_avx2(<4 x i64>* nocapture readonly, <4 x i64>*, i32, i32, i32, i32) #0 {
  %7 = alloca <4 x i64>, align 32
  %8 = alloca <4 x i64>, align 32
  %9 = alloca [32 x <4 x i64>], align 32
  %10 = add nsw i32 %2, -10
  %11 = sext i32 %10 to i64
  %12 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 62
  %13 = load i32, i32* %12, align 8
  %14 = insertelement <8 x i32> undef, i32 %13, i32 0
  %15 = shufflevector <8 x i32> %14, <8 x i32> undef, <8 x i32> zeroinitializer
  %16 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 14
  %17 = load i32, i32* %16, align 8
  %18 = insertelement <8 x i32> undef, i32 %17, i32 0
  %19 = shufflevector <8 x i32> %18, <8 x i32> undef, <8 x i32> zeroinitializer
  %20 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 54
  %21 = load i32, i32* %20, align 8
  %22 = insertelement <8 x i32> undef, i32 %21, i32 0
  %23 = shufflevector <8 x i32> %22, <8 x i32> undef, <8 x i32> zeroinitializer
  %24 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 6
  %25 = load i32, i32* %24, align 8
  %26 = insertelement <8 x i32> undef, i32 %25, i32 0
  %27 = shufflevector <8 x i32> %26, <8 x i32> undef, <8 x i32> zeroinitializer
  %28 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 10
  %29 = load i32, i32* %28, align 8
  %30 = insertelement <8 x i32> undef, i32 %29, i32 0
  %31 = shufflevector <8 x i32> %30, <8 x i32> undef, <8 x i32> zeroinitializer
  %32 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 2
  %33 = load i32, i32* %32, align 8
  %34 = insertelement <8 x i32> undef, i32 %33, i32 0
  %35 = shufflevector <8 x i32> %34, <8 x i32> undef, <8 x i32> zeroinitializer
  %36 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 58
  %37 = load i32, i32* %36, align 8
  %38 = sub nsw i32 0, %37
  %39 = insertelement <8 x i32> undef, i32 %38, i32 0
  %40 = shufflevector <8 x i32> %39, <8 x i32> undef, <8 x i32> zeroinitializer
  %41 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 50
  %42 = load i32, i32* %41, align 8
  %43 = sub nsw i32 0, %42
  %44 = insertelement <8 x i32> undef, i32 %43, i32 0
  %45 = shufflevector <8 x i32> %44, <8 x i32> undef, <8 x i32> zeroinitializer
  %46 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 60
  %47 = load i32, i32* %46, align 16
  %48 = insertelement <8 x i32> undef, i32 %47, i32 0
  %49 = shufflevector <8 x i32> %48, <8 x i32> undef, <8 x i32> zeroinitializer
  %50 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 12
  %51 = load i32, i32* %50, align 16
  %52 = insertelement <8 x i32> undef, i32 %51, i32 0
  %53 = shufflevector <8 x i32> %52, <8 x i32> undef, <8 x i32> zeroinitializer
  %54 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 4
  %55 = load i32, i32* %54, align 16
  %56 = insertelement <8 x i32> undef, i32 %55, i32 0
  %57 = shufflevector <8 x i32> %56, <8 x i32> undef, <8 x i32> zeroinitializer
  %58 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 52
  %59 = load i32, i32* %58, align 16
  %60 = sub nsw i32 0, %59
  %61 = insertelement <8 x i32> undef, i32 %60, i32 0
  %62 = shufflevector <8 x i32> %61, <8 x i32> undef, <8 x i32> zeroinitializer
  %63 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 56
  %64 = load i32, i32* %63, align 16
  %65 = insertelement <8 x i32> undef, i32 %64, i32 0
  %66 = shufflevector <8 x i32> %65, <8 x i32> undef, <8 x i32> zeroinitializer
  %67 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 24
  %68 = load i32, i32* %67, align 16
  %69 = insertelement <8 x i32> undef, i32 %68, i32 0
  %70 = shufflevector <8 x i32> %69, <8 x i32> undef, <8 x i32> zeroinitializer
  %71 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 40
  %72 = load i32, i32* %71, align 16
  %73 = insertelement <8 x i32> undef, i32 %72, i32 0
  %74 = shufflevector <8 x i32> %73, <8 x i32> undef, <8 x i32> zeroinitializer
  %75 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 8
  %76 = load i32, i32* %75, align 16
  %77 = insertelement <8 x i32> undef, i32 %76, i32 0
  %78 = shufflevector <8 x i32> %77, <8 x i32> undef, <8 x i32> zeroinitializer
  %79 = sub nsw i32 0, %72
  %80 = insertelement <8 x i32> undef, i32 %79, i32 0
  %81 = shufflevector <8 x i32> %80, <8 x i32> undef, <8 x i32> zeroinitializer
  %82 = sub nsw i32 0, %76
  %83 = insertelement <8 x i32> undef, i32 %82, i32 0
  %84 = shufflevector <8 x i32> %83, <8 x i32> undef, <8 x i32> zeroinitializer
  %85 = sub nsw i32 0, %64
  %86 = insertelement <8 x i32> undef, i32 %85, i32 0
  %87 = shufflevector <8 x i32> %86, <8 x i32> undef, <8 x i32> zeroinitializer
  %88 = sub nsw i32 0, %68
  %89 = insertelement <8 x i32> undef, i32 %88, i32 0
  %90 = shufflevector <8 x i32> %89, <8 x i32> undef, <8 x i32> zeroinitializer
  %91 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 32
  %92 = load i32, i32* %91, align 16
  %93 = insertelement <8 x i32> undef, i32 %92, i32 0
  %94 = shufflevector <8 x i32> %93, <8 x i32> undef, <8 x i32> zeroinitializer
  %95 = sub nsw i32 0, %92
  %96 = insertelement <8 x i32> undef, i32 %95, i32 0
  %97 = shufflevector <8 x i32> %96, <8 x i32> undef, <8 x i32> zeroinitializer
  %98 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 48
  %99 = load i32, i32* %98, align 16
  %100 = insertelement <8 x i32> undef, i32 %99, i32 0
  %101 = shufflevector <8 x i32> %100, <8 x i32> undef, <8 x i32> zeroinitializer
  %102 = sub nsw i32 0, %99
  %103 = insertelement <8 x i32> undef, i32 %102, i32 0
  %104 = shufflevector <8 x i32> %103, <8 x i32> undef, <8 x i32> zeroinitializer
  %105 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 16
  %106 = load i32, i32* %105, align 16
  %107 = insertelement <8 x i32> undef, i32 %106, i32 0
  %108 = shufflevector <8 x i32> %107, <8 x i32> undef, <8 x i32> zeroinitializer
  %109 = sub nsw i32 0, %106
  %110 = insertelement <8 x i32> undef, i32 %109, i32 0
  %111 = shufflevector <8 x i32> %110, <8 x i32> undef, <8 x i32> zeroinitializer
  %112 = add nsw i32 %2, -1
  %113 = shl i32 1, %112
  %114 = insertelement <8 x i32> undef, i32 %113, i32 0
  %115 = shufflevector <8 x i32> %114, <8 x i32> undef, <8 x i32> zeroinitializer
  %116 = icmp eq i32 %3, 0
  %117 = select i1 %116, i32 8, i32 6
  %118 = add nsw i32 %117, %4
  %119 = icmp slt i32 %118, 16
  %120 = add i32 %118, -1
  %121 = bitcast <4 x i64>* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %121) #8
  %122 = shl i32 1, %120
  %123 = select i1 %119, i32 32768, i32 %122
  %124 = sub nsw i32 0, %123
  %125 = insertelement <8 x i32> undef, i32 %124, i32 0
  %126 = shufflevector <8 x i32> %125, <8 x i32> undef, <8 x i32> zeroinitializer
  %127 = bitcast <4 x i64>* %7 to <8 x i32>*
  store <8 x i32> %126, <8 x i32>* %127, align 32
  %128 = bitcast <4 x i64>* %8 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %128) #8
  %129 = add nsw i32 %123, -1
  %130 = insertelement <8 x i32> undef, i32 %129, i32 0
  %131 = shufflevector <8 x i32> %130, <8 x i32> undef, <8 x i32> zeroinitializer
  %132 = bitcast <4 x i64>* %8 to <8 x i32>*
  store <8 x i32> %131, <8 x i32>* %132, align 32
  %133 = bitcast [32 x <4 x i64>]* %9 to i8*
  call void @llvm.lifetime.start.p0i8(i64 1024, i8* nonnull %133) #8
  %134 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 1
  %135 = bitcast <4 x i64>* %134 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %135, i8 -86, i64 960, i1 false)
  %136 = load <4 x i64>, <4 x i64>* %0, align 32
  %137 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 0
  store <4 x i64> %136, <4 x i64>* %137, align 32
  %138 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 4
  %139 = load <4 x i64>, <4 x i64>* %138, align 32
  %140 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 4
  store <4 x i64> %139, <4 x i64>* %140, align 32
  %141 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 2
  %142 = bitcast <4 x i64>* %141 to <8 x i32>*
  %143 = load <8 x i32>, <8 x i32>* %142, align 32
  %144 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 8
  %145 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 6
  %146 = bitcast <4 x i64>* %145 to <8 x i32>*
  %147 = load <8 x i32>, <8 x i32>* %146, align 32
  %148 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 12
  %149 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 1
  %150 = bitcast <4 x i64>* %149 to <8 x i32>*
  %151 = load <8 x i32>, <8 x i32>* %150, align 32
  %152 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 16
  %153 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 5
  %154 = bitcast <4 x i64>* %153 to <8 x i32>*
  %155 = load <8 x i32>, <8 x i32>* %154, align 32
  %156 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 20
  %157 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 3
  %158 = bitcast <4 x i64>* %157 to <8 x i32>*
  %159 = load <8 x i32>, <8 x i32>* %158, align 32
  %160 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 24
  %161 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 7
  %162 = bitcast <4 x i64>* %161 to <8 x i32>*
  %163 = load <8 x i32>, <8 x i32>* %162, align 32
  %164 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 28
  %165 = bitcast <4 x i64>* %152 to <8 x i32>*
  %166 = mul <8 x i32> %35, %151
  %167 = add <8 x i32> %166, %115
  %168 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %167, i32 %2) #8
  %169 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 31
  %170 = bitcast <4 x i64>* %169 to <8 x i32>*
  store <8 x i32> %168, <8 x i32>* %170, align 32
  %171 = mul <8 x i32> %15, %151
  %172 = add <8 x i32> %171, %115
  %173 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %172, i32 %2) #8
  store <8 x i32> %173, <8 x i32>* %165, align 32
  %174 = bitcast <4 x i64>* %164 to <8 x i32>*
  %175 = mul <8 x i32> %45, %163
  %176 = add <8 x i32> %175, %115
  %177 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %176, i32 %2) #8
  %178 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 19
  %179 = bitcast <4 x i64>* %178 to <8 x i32>*
  store <8 x i32> %177, <8 x i32>* %179, align 32
  %180 = mul <8 x i32> %19, %163
  %181 = add <8 x i32> %180, %115
  %182 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %181, i32 %2) #8
  store <8 x i32> %182, <8 x i32>* %174, align 32
  %183 = bitcast <4 x i64>* %156 to <8 x i32>*
  %184 = mul <8 x i32> %31, %155
  %185 = add <8 x i32> %184, %115
  %186 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %185, i32 %2) #8
  %187 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 27
  %188 = bitcast <4 x i64>* %187 to <8 x i32>*
  store <8 x i32> %186, <8 x i32>* %188, align 32
  %189 = mul <8 x i32> %23, %155
  %190 = add <8 x i32> %189, %115
  %191 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %190, i32 %2) #8
  store <8 x i32> %191, <8 x i32>* %183, align 32
  %192 = bitcast <4 x i64>* %160 to <8 x i32>*
  %193 = mul <8 x i32> %40, %159
  %194 = add <8 x i32> %193, %115
  %195 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %194, i32 %2) #8
  %196 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 23
  %197 = bitcast <4 x i64>* %196 to <8 x i32>*
  store <8 x i32> %195, <8 x i32>* %197, align 32
  %198 = mul <8 x i32> %27, %159
  %199 = add <8 x i32> %198, %115
  %200 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %199, i32 %2) #8
  store <8 x i32> %200, <8 x i32>* %192, align 32
  %201 = bitcast <4 x i64>* %144 to <8 x i32>*
  %202 = mul <8 x i32> %57, %143
  %203 = add <8 x i32> %202, %115
  %204 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %203, i32 %2) #8
  %205 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 15
  %206 = bitcast <4 x i64>* %205 to <8 x i32>*
  store <8 x i32> %204, <8 x i32>* %206, align 32
  %207 = mul <8 x i32> %49, %143
  %208 = add <8 x i32> %207, %115
  %209 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %208, i32 %2) #8
  store <8 x i32> %209, <8 x i32>* %201, align 32
  %210 = bitcast <4 x i64>* %148 to <8 x i32>*
  %211 = mul <8 x i32> %62, %147
  %212 = add <8 x i32> %211, %115
  %213 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %212, i32 %2) #8
  %214 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 11
  %215 = bitcast <4 x i64>* %214 to <8 x i32>*
  store <8 x i32> %213, <8 x i32>* %215, align 32
  %216 = mul <8 x i32> %53, %147
  %217 = add <8 x i32> %216, %115
  %218 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %217, i32 %2) #8
  store <8 x i32> %218, <8 x i32>* %210, align 32
  %219 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 17
  %220 = bitcast <4 x i64>* %219 to <8 x i32>*
  %221 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 18
  %222 = bitcast <4 x i64>* %221 to <8 x i32>*
  %223 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 21
  %224 = bitcast <4 x i64>* %223 to <8 x i32>*
  %225 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 22
  %226 = bitcast <4 x i64>* %225 to <8 x i32>*
  %227 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 25
  %228 = bitcast <4 x i64>* %227 to <8 x i32>*
  %229 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 26
  %230 = bitcast <4 x i64>* %229 to <8 x i32>*
  %231 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 29
  %232 = bitcast <4 x i64>* %231 to <8 x i32>*
  %233 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 30
  %234 = bitcast <4 x i64>* %233 to <8 x i32>*
  %235 = bitcast <4 x i64>* %140 to <8 x i32>*
  %236 = load <8 x i32>, <8 x i32>* %235, align 32
  %237 = mul <8 x i32> %236, %78
  %238 = add <8 x i32> %237, %115
  %239 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %238, i32 %2) #8
  %240 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 7
  %241 = bitcast <4 x i64>* %240 to <8 x i32>*
  store <8 x i32> %239, <8 x i32>* %241, align 32
  %242 = mul <8 x i32> %236, %66
  %243 = add <8 x i32> %242, %115
  %244 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %243, i32 %2) #8
  store <8 x i32> %244, <8 x i32>* %235, align 32
  %245 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 9
  %246 = bitcast <4 x i64>* %245 to <8 x i32>*
  %247 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 10
  %248 = bitcast <4 x i64>* %247 to <8 x i32>*
  %249 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 13
  %250 = bitcast <4 x i64>* %249 to <8 x i32>*
  %251 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 14
  %252 = bitcast <4 x i64>* %251 to <8 x i32>*
  %253 = mul <8 x i32> %173, %84
  %254 = mul <8 x i32> %168, %66
  %255 = add <8 x i32> %253, %115
  %256 = add <8 x i32> %255, %254
  %257 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %256, i32 %2) #8
  %258 = mul <8 x i32> %173, %66
  %259 = mul <8 x i32> %168, %78
  %260 = add <8 x i32> %258, %115
  %261 = add <8 x i32> %260, %259
  %262 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %261, i32 %2) #8
  store <8 x i32> %262, <8 x i32>* %234, align 32
  store <8 x i32> %257, <8 x i32>* %220, align 32
  %263 = mul <8 x i32> %177, %87
  %264 = mul <8 x i32> %182, %84
  %265 = add <8 x i32> %263, %115
  %266 = add <8 x i32> %265, %264
  %267 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %266, i32 %2) #8
  %268 = mul <8 x i32> %177, %84
  %269 = mul <8 x i32> %182, %66
  %270 = add <8 x i32> %268, %115
  %271 = add <8 x i32> %270, %269
  %272 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %271, i32 %2) #8
  store <8 x i32> %272, <8 x i32>* %232, align 32
  %273 = mul <8 x i32> %191, %81
  %274 = mul <8 x i32> %186, %70
  %275 = add <8 x i32> %273, %115
  %276 = add <8 x i32> %275, %274
  %277 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %276, i32 %2) #8
  %278 = mul <8 x i32> %191, %70
  %279 = mul <8 x i32> %186, %74
  %280 = add <8 x i32> %278, %115
  %281 = add <8 x i32> %280, %279
  %282 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %281, i32 %2) #8
  store <8 x i32> %282, <8 x i32>* %230, align 32
  store <8 x i32> %277, <8 x i32>* %224, align 32
  %283 = mul <8 x i32> %195, %90
  %284 = mul <8 x i32> %200, %81
  %285 = add <8 x i32> %283, %115
  %286 = add <8 x i32> %285, %284
  %287 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %286, i32 %2) #8
  %288 = mul <8 x i32> %195, %81
  %289 = mul <8 x i32> %200, %70
  %290 = add <8 x i32> %288, %115
  %291 = add <8 x i32> %290, %289
  %292 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %291, i32 %2) #8
  store <8 x i32> %292, <8 x i32>* %228, align 32
  %293 = bitcast [32 x <4 x i64>]* %9 to <8 x i32>*
  %294 = load <8 x i32>, <8 x i32>* %293, align 32
  %295 = mul <8 x i32> %294, %94
  %296 = add <8 x i32> %295, %115
  %297 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %296, i32 %2) #8
  store <8 x i32> %297, <8 x i32>* %293, align 32
  %298 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 1
  %299 = bitcast <4 x i64>* %298 to <8 x i32>*
  store <8 x i32> %297, <8 x i32>* %299, align 32
  %300 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 5
  %301 = bitcast <4 x i64>* %300 to <8 x i32>*
  store <8 x i32> %244, <8 x i32>* %301, align 32
  %302 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 6
  %303 = bitcast <4 x i64>* %302 to <8 x i32>*
  store <8 x i32> %239, <8 x i32>* %303, align 32
  %304 = mul <8 x i32> %209, %111
  %305 = mul <8 x i32> %204, %101
  %306 = add <8 x i32> %304, %115
  %307 = add <8 x i32> %306, %305
  %308 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %307, i32 %2) #8
  %309 = mul <8 x i32> %209, %101
  %310 = mul <8 x i32> %204, %108
  %311 = add <8 x i32> %309, %115
  %312 = add <8 x i32> %311, %310
  %313 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %312, i32 %2) #8
  store <8 x i32> %313, <8 x i32>* %252, align 32
  store <8 x i32> %308, <8 x i32>* %246, align 32
  %314 = mul <8 x i32> %213, %104
  %315 = mul <8 x i32> %218, %111
  %316 = add <8 x i32> %314, %115
  %317 = add <8 x i32> %316, %315
  %318 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %317, i32 %2) #8
  %319 = mul <8 x i32> %213, %111
  %320 = mul <8 x i32> %218, %101
  %321 = add <8 x i32> %319, %115
  %322 = add <8 x i32> %321, %320
  %323 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %322, i32 %2) #8
  store <8 x i32> %323, <8 x i32>* %250, align 32
  store <8 x i32> %318, <8 x i32>* %248, align 32
  %324 = load <8 x i32>, <8 x i32>* %165, align 32
  %325 = load <8 x i32>, <8 x i32>* %179, align 32
  %326 = add <8 x i32> %325, %324
  %327 = sub <8 x i32> %324, %325
  %328 = load <8 x i32>, <8 x i32>* %127, align 32
  %329 = icmp sgt <8 x i32> %326, %328
  %330 = select <8 x i1> %329, <8 x i32> %326, <8 x i32> %328
  %331 = load <8 x i32>, <8 x i32>* %132, align 32
  %332 = icmp slt <8 x i32> %330, %331
  %333 = select <8 x i1> %332, <8 x i32> %330, <8 x i32> %331
  %334 = icmp sgt <8 x i32> %327, %328
  %335 = select <8 x i1> %334, <8 x i32> %327, <8 x i32> %328
  %336 = icmp slt <8 x i32> %335, %331
  %337 = select <8 x i1> %336, <8 x i32> %335, <8 x i32> %331
  store <8 x i32> %333, <8 x i32>* %165, align 32
  store <8 x i32> %337, <8 x i32>* %179, align 32
  %338 = add <8 x i32> %267, %257
  %339 = sub <8 x i32> %257, %267
  %340 = icmp sgt <8 x i32> %338, %328
  %341 = select <8 x i1> %340, <8 x i32> %338, <8 x i32> %328
  %342 = icmp slt <8 x i32> %341, %331
  %343 = select <8 x i1> %342, <8 x i32> %341, <8 x i32> %331
  %344 = icmp sgt <8 x i32> %339, %328
  %345 = select <8 x i1> %344, <8 x i32> %339, <8 x i32> %328
  %346 = icmp slt <8 x i32> %345, %331
  %347 = select <8 x i1> %346, <8 x i32> %345, <8 x i32> %331
  store <8 x i32> %343, <8 x i32>* %220, align 32
  store <8 x i32> %347, <8 x i32>* %222, align 32
  %348 = load <8 x i32>, <8 x i32>* %197, align 32
  %349 = load <8 x i32>, <8 x i32>* %183, align 32
  %350 = add <8 x i32> %349, %348
  %351 = sub <8 x i32> %348, %349
  %352 = icmp sgt <8 x i32> %350, %328
  %353 = select <8 x i1> %352, <8 x i32> %350, <8 x i32> %328
  %354 = icmp slt <8 x i32> %353, %331
  %355 = select <8 x i1> %354, <8 x i32> %353, <8 x i32> %331
  %356 = icmp sgt <8 x i32> %351, %328
  %357 = select <8 x i1> %356, <8 x i32> %351, <8 x i32> %328
  %358 = icmp slt <8 x i32> %357, %331
  %359 = select <8 x i1> %358, <8 x i32> %357, <8 x i32> %331
  store <8 x i32> %355, <8 x i32>* %197, align 32
  store <8 x i32> %359, <8 x i32>* %183, align 32
  %360 = add <8 x i32> %287, %277
  %361 = sub <8 x i32> %287, %277
  %362 = icmp sgt <8 x i32> %360, %328
  %363 = select <8 x i1> %362, <8 x i32> %360, <8 x i32> %328
  %364 = icmp slt <8 x i32> %363, %331
  %365 = select <8 x i1> %364, <8 x i32> %363, <8 x i32> %331
  %366 = icmp sgt <8 x i32> %361, %328
  %367 = select <8 x i1> %366, <8 x i32> %361, <8 x i32> %328
  %368 = icmp slt <8 x i32> %367, %331
  %369 = select <8 x i1> %368, <8 x i32> %367, <8 x i32> %331
  store <8 x i32> %365, <8 x i32>* %226, align 32
  store <8 x i32> %369, <8 x i32>* %224, align 32
  %370 = load <8 x i32>, <8 x i32>* %192, align 32
  %371 = load <8 x i32>, <8 x i32>* %188, align 32
  %372 = add <8 x i32> %371, %370
  %373 = sub <8 x i32> %370, %371
  %374 = icmp sgt <8 x i32> %372, %328
  %375 = select <8 x i1> %374, <8 x i32> %372, <8 x i32> %328
  %376 = icmp slt <8 x i32> %375, %331
  %377 = select <8 x i1> %376, <8 x i32> %375, <8 x i32> %331
  %378 = icmp sgt <8 x i32> %373, %328
  %379 = select <8 x i1> %378, <8 x i32> %373, <8 x i32> %328
  %380 = icmp slt <8 x i32> %379, %331
  %381 = select <8 x i1> %380, <8 x i32> %379, <8 x i32> %331
  store <8 x i32> %377, <8 x i32>* %192, align 32
  store <8 x i32> %381, <8 x i32>* %188, align 32
  %382 = add <8 x i32> %292, %282
  %383 = sub <8 x i32> %292, %282
  %384 = icmp sgt <8 x i32> %382, %328
  %385 = select <8 x i1> %384, <8 x i32> %382, <8 x i32> %328
  %386 = icmp slt <8 x i32> %385, %331
  %387 = select <8 x i1> %386, <8 x i32> %385, <8 x i32> %331
  %388 = icmp sgt <8 x i32> %383, %328
  %389 = select <8 x i1> %388, <8 x i32> %383, <8 x i32> %328
  %390 = icmp slt <8 x i32> %389, %331
  %391 = select <8 x i1> %390, <8 x i32> %389, <8 x i32> %331
  store <8 x i32> %387, <8 x i32>* %228, align 32
  store <8 x i32> %391, <8 x i32>* %230, align 32
  %392 = load <8 x i32>, <8 x i32>* %170, align 32
  %393 = load <8 x i32>, <8 x i32>* %174, align 32
  %394 = add <8 x i32> %393, %392
  %395 = sub <8 x i32> %392, %393
  %396 = icmp sgt <8 x i32> %394, %328
  %397 = select <8 x i1> %396, <8 x i32> %394, <8 x i32> %328
  %398 = icmp slt <8 x i32> %397, %331
  %399 = select <8 x i1> %398, <8 x i32> %397, <8 x i32> %331
  %400 = icmp sgt <8 x i32> %395, %328
  %401 = select <8 x i1> %400, <8 x i32> %395, <8 x i32> %328
  %402 = icmp slt <8 x i32> %401, %331
  %403 = select <8 x i1> %402, <8 x i32> %401, <8 x i32> %331
  store <8 x i32> %399, <8 x i32>* %170, align 32
  store <8 x i32> %403, <8 x i32>* %174, align 32
  %404 = add <8 x i32> %272, %262
  %405 = sub <8 x i32> %262, %272
  %406 = icmp sgt <8 x i32> %404, %328
  %407 = select <8 x i1> %406, <8 x i32> %404, <8 x i32> %328
  %408 = icmp slt <8 x i32> %407, %331
  %409 = select <8 x i1> %408, <8 x i32> %407, <8 x i32> %331
  %410 = icmp sgt <8 x i32> %405, %328
  %411 = select <8 x i1> %410, <8 x i32> %405, <8 x i32> %328
  %412 = icmp slt <8 x i32> %411, %331
  %413 = select <8 x i1> %412, <8 x i32> %411, <8 x i32> %331
  store <8 x i32> %409, <8 x i32>* %234, align 32
  %414 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 3
  %415 = bitcast <4 x i64>* %414 to <8 x i32>*
  store <8 x i32> %297, <8 x i32>* %415, align 32
  %416 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 2
  %417 = bitcast <4 x i64>* %416 to <8 x i32>*
  store <8 x i32> %297, <8 x i32>* %417, align 32
  %418 = load <8 x i32>, <8 x i32>* %301, align 32
  %419 = mul <8 x i32> %418, %97
  %420 = load <8 x i32>, <8 x i32>* %303, align 32
  %421 = mul <8 x i32> %420, %94
  %422 = add <8 x i32> %421, %115
  %423 = add <8 x i32> %422, %419
  %424 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %423, i32 %2) #8
  %425 = mul <8 x i32> %418, %94
  %426 = add <8 x i32> %422, %425
  %427 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %426, i32 %2) #8
  store <8 x i32> %427, <8 x i32>* %303, align 32
  store <8 x i32> %424, <8 x i32>* %301, align 32
  %428 = load <8 x i32>, <8 x i32>* %201, align 32
  %429 = load <8 x i32>, <8 x i32>* %215, align 32
  %430 = add <8 x i32> %429, %428
  %431 = sub <8 x i32> %428, %429
  %432 = icmp sgt <8 x i32> %430, %328
  %433 = select <8 x i1> %432, <8 x i32> %430, <8 x i32> %328
  %434 = icmp slt <8 x i32> %433, %331
  %435 = select <8 x i1> %434, <8 x i32> %433, <8 x i32> %331
  %436 = icmp sgt <8 x i32> %431, %328
  %437 = select <8 x i1> %436, <8 x i32> %431, <8 x i32> %328
  %438 = icmp slt <8 x i32> %437, %331
  %439 = select <8 x i1> %438, <8 x i32> %437, <8 x i32> %331
  store <8 x i32> %435, <8 x i32>* %201, align 32
  store <8 x i32> %439, <8 x i32>* %215, align 32
  %440 = add <8 x i32> %318, %308
  %441 = sub <8 x i32> %308, %318
  %442 = icmp sgt <8 x i32> %440, %328
  %443 = select <8 x i1> %442, <8 x i32> %440, <8 x i32> %328
  %444 = icmp slt <8 x i32> %443, %331
  %445 = select <8 x i1> %444, <8 x i32> %443, <8 x i32> %331
  %446 = icmp sgt <8 x i32> %441, %328
  %447 = select <8 x i1> %446, <8 x i32> %441, <8 x i32> %328
  %448 = icmp slt <8 x i32> %447, %331
  %449 = select <8 x i1> %448, <8 x i32> %447, <8 x i32> %331
  store <8 x i32> %445, <8 x i32>* %246, align 32
  store <8 x i32> %449, <8 x i32>* %248, align 32
  %450 = load <8 x i32>, <8 x i32>* %206, align 32
  %451 = load <8 x i32>, <8 x i32>* %210, align 32
  %452 = add <8 x i32> %451, %450
  %453 = sub <8 x i32> %450, %451
  %454 = icmp sgt <8 x i32> %452, %328
  %455 = select <8 x i1> %454, <8 x i32> %452, <8 x i32> %328
  %456 = icmp slt <8 x i32> %455, %331
  %457 = select <8 x i1> %456, <8 x i32> %455, <8 x i32> %331
  %458 = icmp sgt <8 x i32> %453, %328
  %459 = select <8 x i1> %458, <8 x i32> %453, <8 x i32> %328
  %460 = icmp slt <8 x i32> %459, %331
  %461 = select <8 x i1> %460, <8 x i32> %459, <8 x i32> %331
  store <8 x i32> %457, <8 x i32>* %206, align 32
  store <8 x i32> %461, <8 x i32>* %210, align 32
  %462 = add <8 x i32> %323, %313
  %463 = sub <8 x i32> %313, %323
  %464 = icmp sgt <8 x i32> %462, %328
  %465 = select <8 x i1> %464, <8 x i32> %462, <8 x i32> %328
  %466 = icmp slt <8 x i32> %465, %331
  %467 = select <8 x i1> %466, <8 x i32> %465, <8 x i32> %331
  %468 = icmp sgt <8 x i32> %463, %328
  %469 = select <8 x i1> %468, <8 x i32> %463, <8 x i32> %328
  %470 = icmp slt <8 x i32> %469, %331
  %471 = select <8 x i1> %470, <8 x i32> %469, <8 x i32> %331
  store <8 x i32> %467, <8 x i32>* %252, align 32
  store <8 x i32> %471, <8 x i32>* %250, align 32
  %472 = mul <8 x i32> %347, %111
  %473 = mul <8 x i32> %413, %101
  %474 = add <8 x i32> %472, %115
  %475 = add <8 x i32> %474, %473
  %476 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %475, i32 %2) #8
  %477 = mul <8 x i32> %347, %101
  %478 = mul <8 x i32> %413, %108
  %479 = add <8 x i32> %477, %115
  %480 = add <8 x i32> %479, %478
  %481 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %480, i32 %2) #8
  store <8 x i32> %481, <8 x i32>* %232, align 32
  store <8 x i32> %476, <8 x i32>* %222, align 32
  %482 = mul <8 x i32> %337, %111
  %483 = mul <8 x i32> %403, %101
  %484 = add <8 x i32> %482, %115
  %485 = add <8 x i32> %484, %483
  %486 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %485, i32 %2) #8
  %487 = mul <8 x i32> %337, %101
  %488 = mul <8 x i32> %403, %108
  %489 = add <8 x i32> %487, %115
  %490 = add <8 x i32> %489, %488
  %491 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %490, i32 %2) #8
  store <8 x i32> %491, <8 x i32>* %174, align 32
  store <8 x i32> %486, <8 x i32>* %179, align 32
  %492 = mul <8 x i32> %359, %104
  %493 = mul <8 x i32> %381, %111
  %494 = add <8 x i32> %492, %115
  %495 = add <8 x i32> %494, %493
  %496 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %495, i32 %2) #8
  %497 = mul <8 x i32> %359, %111
  %498 = mul <8 x i32> %381, %101
  %499 = add <8 x i32> %497, %115
  %500 = add <8 x i32> %499, %498
  %501 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %500, i32 %2) #8
  store <8 x i32> %501, <8 x i32>* %188, align 32
  store <8 x i32> %496, <8 x i32>* %183, align 32
  %502 = mul <8 x i32> %369, %104
  %503 = mul <8 x i32> %391, %111
  %504 = add <8 x i32> %502, %115
  %505 = add <8 x i32> %504, %503
  %506 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %505, i32 %2) #8
  %507 = mul <8 x i32> %369, %111
  %508 = mul <8 x i32> %391, %101
  %509 = add <8 x i32> %507, %115
  %510 = add <8 x i32> %509, %508
  %511 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %510, i32 %2) #8
  store <8 x i32> %511, <8 x i32>* %230, align 32
  store <8 x i32> %506, <8 x i32>* %224, align 32
  %512 = load <8 x i32>, <8 x i32>* %293, align 32
  %513 = load <8 x i32>, <8 x i32>* %241, align 32
  %514 = add <8 x i32> %513, %512
  %515 = sub <8 x i32> %512, %513
  %516 = icmp sgt <8 x i32> %514, %328
  %517 = select <8 x i1> %516, <8 x i32> %514, <8 x i32> %328
  %518 = icmp slt <8 x i32> %517, %331
  %519 = select <8 x i1> %518, <8 x i32> %517, <8 x i32> %331
  %520 = icmp sgt <8 x i32> %515, %328
  %521 = select <8 x i1> %520, <8 x i32> %515, <8 x i32> %328
  %522 = icmp slt <8 x i32> %521, %331
  %523 = select <8 x i1> %522, <8 x i32> %521, <8 x i32> %331
  store <8 x i32> %519, <8 x i32>* %293, align 32
  store <8 x i32> %523, <8 x i32>* %241, align 32
  %524 = load <8 x i32>, <8 x i32>* %299, align 32
  %525 = add <8 x i32> %524, %427
  %526 = sub <8 x i32> %524, %427
  %527 = icmp sgt <8 x i32> %525, %328
  %528 = select <8 x i1> %527, <8 x i32> %525, <8 x i32> %328
  %529 = icmp slt <8 x i32> %528, %331
  %530 = select <8 x i1> %529, <8 x i32> %528, <8 x i32> %331
  %531 = icmp sgt <8 x i32> %526, %328
  %532 = select <8 x i1> %531, <8 x i32> %526, <8 x i32> %328
  %533 = icmp slt <8 x i32> %532, %331
  %534 = select <8 x i1> %533, <8 x i32> %532, <8 x i32> %331
  store <8 x i32> %530, <8 x i32>* %299, align 32
  store <8 x i32> %534, <8 x i32>* %303, align 32
  %535 = load <8 x i32>, <8 x i32>* %417, align 32
  %536 = add <8 x i32> %535, %424
  %537 = sub <8 x i32> %535, %424
  %538 = icmp sgt <8 x i32> %536, %328
  %539 = select <8 x i1> %538, <8 x i32> %536, <8 x i32> %328
  %540 = icmp slt <8 x i32> %539, %331
  %541 = select <8 x i1> %540, <8 x i32> %539, <8 x i32> %331
  %542 = icmp sgt <8 x i32> %537, %328
  %543 = select <8 x i1> %542, <8 x i32> %537, <8 x i32> %328
  %544 = icmp slt <8 x i32> %543, %331
  %545 = select <8 x i1> %544, <8 x i32> %543, <8 x i32> %331
  store <8 x i32> %541, <8 x i32>* %417, align 32
  store <8 x i32> %545, <8 x i32>* %301, align 32
  %546 = load <8 x i32>, <8 x i32>* %415, align 32
  %547 = load <8 x i32>, <8 x i32>* %235, align 32
  %548 = add <8 x i32> %547, %546
  %549 = sub <8 x i32> %546, %547
  %550 = icmp sgt <8 x i32> %548, %328
  %551 = select <8 x i1> %550, <8 x i32> %548, <8 x i32> %328
  %552 = icmp slt <8 x i32> %551, %331
  %553 = select <8 x i1> %552, <8 x i32> %551, <8 x i32> %331
  %554 = icmp sgt <8 x i32> %549, %328
  %555 = select <8 x i1> %554, <8 x i32> %549, <8 x i32> %328
  %556 = icmp slt <8 x i32> %555, %331
  %557 = select <8 x i1> %556, <8 x i32> %555, <8 x i32> %331
  store <8 x i32> %553, <8 x i32>* %415, align 32
  store <8 x i32> %557, <8 x i32>* %235, align 32
  %558 = mul <8 x i32> %449, %97
  %559 = mul <8 x i32> %471, %94
  %560 = add <8 x i32> %559, %115
  %561 = add <8 x i32> %560, %558
  %562 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %561, i32 %2) #8
  %563 = mul <8 x i32> %449, %94
  %564 = add <8 x i32> %560, %563
  %565 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %564, i32 %2) #8
  store <8 x i32> %565, <8 x i32>* %250, align 32
  store <8 x i32> %562, <8 x i32>* %248, align 32
  %566 = mul <8 x i32> %439, %97
  %567 = mul <8 x i32> %461, %94
  %568 = add <8 x i32> %567, %115
  %569 = add <8 x i32> %568, %566
  %570 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %569, i32 %2) #8
  %571 = mul <8 x i32> %439, %94
  %572 = add <8 x i32> %568, %571
  %573 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %572, i32 %2) #8
  store <8 x i32> %573, <8 x i32>* %210, align 32
  store <8 x i32> %570, <8 x i32>* %215, align 32
  %574 = add <8 x i32> %355, %333
  %575 = sub <8 x i32> %333, %355
  %576 = icmp sgt <8 x i32> %574, %328
  %577 = select <8 x i1> %576, <8 x i32> %574, <8 x i32> %328
  %578 = icmp slt <8 x i32> %577, %331
  %579 = select <8 x i1> %578, <8 x i32> %577, <8 x i32> %331
  %580 = icmp sgt <8 x i32> %575, %328
  %581 = select <8 x i1> %580, <8 x i32> %575, <8 x i32> %328
  %582 = icmp slt <8 x i32> %581, %331
  %583 = select <8 x i1> %582, <8 x i32> %581, <8 x i32> %331
  store <8 x i32> %579, <8 x i32>* %165, align 32
  store <8 x i32> %583, <8 x i32>* %197, align 32
  %584 = add <8 x i32> %365, %343
  %585 = sub <8 x i32> %343, %365
  %586 = icmp sgt <8 x i32> %584, %328
  %587 = select <8 x i1> %586, <8 x i32> %584, <8 x i32> %328
  %588 = icmp slt <8 x i32> %587, %331
  %589 = select <8 x i1> %588, <8 x i32> %587, <8 x i32> %331
  %590 = icmp sgt <8 x i32> %585, %328
  %591 = select <8 x i1> %590, <8 x i32> %585, <8 x i32> %328
  %592 = icmp slt <8 x i32> %591, %331
  %593 = select <8 x i1> %592, <8 x i32> %591, <8 x i32> %331
  store <8 x i32> %589, <8 x i32>* %220, align 32
  store <8 x i32> %593, <8 x i32>* %226, align 32
  %594 = add <8 x i32> %506, %476
  %595 = sub <8 x i32> %476, %506
  %596 = icmp sgt <8 x i32> %594, %328
  %597 = select <8 x i1> %596, <8 x i32> %594, <8 x i32> %328
  %598 = icmp slt <8 x i32> %597, %331
  %599 = select <8 x i1> %598, <8 x i32> %597, <8 x i32> %331
  %600 = icmp sgt <8 x i32> %595, %328
  %601 = select <8 x i1> %600, <8 x i32> %595, <8 x i32> %328
  %602 = icmp slt <8 x i32> %601, %331
  %603 = select <8 x i1> %602, <8 x i32> %601, <8 x i32> %331
  store <8 x i32> %599, <8 x i32>* %222, align 32
  store <8 x i32> %603, <8 x i32>* %224, align 32
  %604 = add <8 x i32> %496, %486
  %605 = sub <8 x i32> %486, %496
  %606 = icmp sgt <8 x i32> %604, %328
  %607 = select <8 x i1> %606, <8 x i32> %604, <8 x i32> %328
  %608 = icmp slt <8 x i32> %607, %331
  %609 = select <8 x i1> %608, <8 x i32> %607, <8 x i32> %331
  %610 = icmp sgt <8 x i32> %605, %328
  %611 = select <8 x i1> %610, <8 x i32> %605, <8 x i32> %328
  %612 = icmp slt <8 x i32> %611, %331
  %613 = select <8 x i1> %612, <8 x i32> %611, <8 x i32> %331
  store <8 x i32> %609, <8 x i32>* %179, align 32
  store <8 x i32> %613, <8 x i32>* %183, align 32
  %614 = add <8 x i32> %399, %377
  %615 = sub <8 x i32> %399, %377
  %616 = icmp sgt <8 x i32> %614, %328
  %617 = select <8 x i1> %616, <8 x i32> %614, <8 x i32> %328
  %618 = icmp slt <8 x i32> %617, %331
  %619 = select <8 x i1> %618, <8 x i32> %617, <8 x i32> %331
  %620 = icmp sgt <8 x i32> %615, %328
  %621 = select <8 x i1> %620, <8 x i32> %615, <8 x i32> %328
  %622 = icmp slt <8 x i32> %621, %331
  %623 = select <8 x i1> %622, <8 x i32> %621, <8 x i32> %331
  store <8 x i32> %619, <8 x i32>* %170, align 32
  store <8 x i32> %623, <8 x i32>* %192, align 32
  %624 = add <8 x i32> %387, %409
  %625 = sub <8 x i32> %409, %387
  %626 = icmp sgt <8 x i32> %624, %328
  %627 = select <8 x i1> %626, <8 x i32> %624, <8 x i32> %328
  %628 = icmp slt <8 x i32> %627, %331
  %629 = select <8 x i1> %628, <8 x i32> %627, <8 x i32> %331
  %630 = icmp sgt <8 x i32> %625, %328
  %631 = select <8 x i1> %630, <8 x i32> %625, <8 x i32> %328
  %632 = icmp slt <8 x i32> %631, %331
  %633 = select <8 x i1> %632, <8 x i32> %631, <8 x i32> %331
  store <8 x i32> %629, <8 x i32>* %234, align 32
  store <8 x i32> %633, <8 x i32>* %228, align 32
  %634 = add <8 x i32> %511, %481
  %635 = sub <8 x i32> %481, %511
  %636 = icmp sgt <8 x i32> %634, %328
  %637 = select <8 x i1> %636, <8 x i32> %634, <8 x i32> %328
  %638 = icmp slt <8 x i32> %637, %331
  %639 = select <8 x i1> %638, <8 x i32> %637, <8 x i32> %331
  %640 = icmp sgt <8 x i32> %635, %328
  %641 = select <8 x i1> %640, <8 x i32> %635, <8 x i32> %328
  %642 = icmp slt <8 x i32> %641, %331
  %643 = select <8 x i1> %642, <8 x i32> %641, <8 x i32> %331
  store <8 x i32> %639, <8 x i32>* %232, align 32
  store <8 x i32> %643, <8 x i32>* %230, align 32
  %644 = add <8 x i32> %501, %491
  %645 = sub <8 x i32> %491, %501
  %646 = icmp sgt <8 x i32> %644, %328
  %647 = select <8 x i1> %646, <8 x i32> %644, <8 x i32> %328
  %648 = icmp slt <8 x i32> %647, %331
  %649 = select <8 x i1> %648, <8 x i32> %647, <8 x i32> %331
  %650 = icmp sgt <8 x i32> %645, %328
  %651 = select <8 x i1> %650, <8 x i32> %645, <8 x i32> %328
  %652 = icmp slt <8 x i32> %651, %331
  %653 = select <8 x i1> %652, <8 x i32> %651, <8 x i32> %331
  store <8 x i32> %649, <8 x i32>* %174, align 32
  store <8 x i32> %653, <8 x i32>* %188, align 32
  %654 = add <8 x i32> %519, %457
  %655 = sub <8 x i32> %519, %457
  %656 = icmp sgt <8 x i32> %654, %328
  %657 = select <8 x i1> %656, <8 x i32> %654, <8 x i32> %328
  %658 = icmp slt <8 x i32> %657, %331
  %659 = select <8 x i1> %658, <8 x i32> %657, <8 x i32> %331
  %660 = icmp sgt <8 x i32> %655, %328
  %661 = select <8 x i1> %660, <8 x i32> %655, <8 x i32> %328
  %662 = icmp slt <8 x i32> %661, %331
  %663 = select <8 x i1> %662, <8 x i32> %661, <8 x i32> %331
  store <8 x i32> %659, <8 x i32>* %293, align 32
  store <8 x i32> %663, <8 x i32>* %206, align 32
  %664 = add <8 x i32> %530, %467
  %665 = sub <8 x i32> %530, %467
  %666 = icmp sgt <8 x i32> %664, %328
  %667 = select <8 x i1> %666, <8 x i32> %664, <8 x i32> %328
  %668 = icmp slt <8 x i32> %667, %331
  %669 = select <8 x i1> %668, <8 x i32> %667, <8 x i32> %331
  %670 = icmp sgt <8 x i32> %665, %328
  %671 = select <8 x i1> %670, <8 x i32> %665, <8 x i32> %328
  %672 = icmp slt <8 x i32> %671, %331
  %673 = select <8 x i1> %672, <8 x i32> %671, <8 x i32> %331
  store <8 x i32> %669, <8 x i32>* %299, align 32
  store <8 x i32> %673, <8 x i32>* %252, align 32
  %674 = add <8 x i32> %565, %541
  %675 = sub <8 x i32> %541, %565
  %676 = icmp sgt <8 x i32> %674, %328
  %677 = select <8 x i1> %676, <8 x i32> %674, <8 x i32> %328
  %678 = icmp slt <8 x i32> %677, %331
  %679 = select <8 x i1> %678, <8 x i32> %677, <8 x i32> %331
  %680 = icmp sgt <8 x i32> %675, %328
  %681 = select <8 x i1> %680, <8 x i32> %675, <8 x i32> %328
  %682 = icmp slt <8 x i32> %681, %331
  %683 = select <8 x i1> %682, <8 x i32> %681, <8 x i32> %331
  store <8 x i32> %679, <8 x i32>* %417, align 32
  store <8 x i32> %683, <8 x i32>* %250, align 32
  %684 = add <8 x i32> %573, %553
  %685 = sub <8 x i32> %553, %573
  %686 = icmp sgt <8 x i32> %684, %328
  %687 = select <8 x i1> %686, <8 x i32> %684, <8 x i32> %328
  %688 = icmp slt <8 x i32> %687, %331
  %689 = select <8 x i1> %688, <8 x i32> %687, <8 x i32> %331
  %690 = icmp sgt <8 x i32> %685, %328
  %691 = select <8 x i1> %690, <8 x i32> %685, <8 x i32> %328
  %692 = icmp slt <8 x i32> %691, %331
  %693 = select <8 x i1> %692, <8 x i32> %691, <8 x i32> %331
  store <8 x i32> %689, <8 x i32>* %415, align 32
  store <8 x i32> %693, <8 x i32>* %210, align 32
  %694 = add <8 x i32> %570, %557
  %695 = sub <8 x i32> %557, %570
  %696 = icmp sgt <8 x i32> %694, %328
  %697 = select <8 x i1> %696, <8 x i32> %694, <8 x i32> %328
  %698 = icmp slt <8 x i32> %697, %331
  %699 = select <8 x i1> %698, <8 x i32> %697, <8 x i32> %331
  %700 = icmp sgt <8 x i32> %695, %328
  %701 = select <8 x i1> %700, <8 x i32> %695, <8 x i32> %328
  %702 = icmp slt <8 x i32> %701, %331
  %703 = select <8 x i1> %702, <8 x i32> %701, <8 x i32> %331
  store <8 x i32> %699, <8 x i32>* %235, align 32
  store <8 x i32> %703, <8 x i32>* %215, align 32
  %704 = add <8 x i32> %562, %545
  %705 = sub <8 x i32> %545, %562
  %706 = icmp sgt <8 x i32> %704, %328
  %707 = select <8 x i1> %706, <8 x i32> %704, <8 x i32> %328
  %708 = icmp slt <8 x i32> %707, %331
  %709 = select <8 x i1> %708, <8 x i32> %707, <8 x i32> %331
  %710 = icmp sgt <8 x i32> %705, %328
  %711 = select <8 x i1> %710, <8 x i32> %705, <8 x i32> %328
  %712 = icmp slt <8 x i32> %711, %331
  %713 = select <8 x i1> %712, <8 x i32> %711, <8 x i32> %331
  store <8 x i32> %709, <8 x i32>* %301, align 32
  store <8 x i32> %713, <8 x i32>* %248, align 32
  %714 = add <8 x i32> %534, %445
  %715 = sub <8 x i32> %534, %445
  %716 = icmp sgt <8 x i32> %714, %328
  %717 = select <8 x i1> %716, <8 x i32> %714, <8 x i32> %328
  %718 = icmp slt <8 x i32> %717, %331
  %719 = select <8 x i1> %718, <8 x i32> %717, <8 x i32> %331
  %720 = icmp sgt <8 x i32> %715, %328
  %721 = select <8 x i1> %720, <8 x i32> %715, <8 x i32> %328
  %722 = icmp slt <8 x i32> %721, %331
  %723 = select <8 x i1> %722, <8 x i32> %721, <8 x i32> %331
  store <8 x i32> %719, <8 x i32>* %303, align 32
  store <8 x i32> %723, <8 x i32>* %246, align 32
  %724 = add <8 x i32> %523, %435
  %725 = sub <8 x i32> %523, %435
  %726 = icmp sgt <8 x i32> %724, %328
  %727 = select <8 x i1> %726, <8 x i32> %724, <8 x i32> %328
  %728 = icmp slt <8 x i32> %727, %331
  %729 = select <8 x i1> %728, <8 x i32> %727, <8 x i32> %331
  %730 = icmp sgt <8 x i32> %725, %328
  %731 = select <8 x i1> %730, <8 x i32> %725, <8 x i32> %328
  %732 = icmp slt <8 x i32> %731, %331
  %733 = select <8 x i1> %732, <8 x i32> %731, <8 x i32> %331
  store <8 x i32> %729, <8 x i32>* %241, align 32
  store <8 x i32> %733, <8 x i32>* %201, align 32
  %734 = mul <8 x i32> %613, %97
  %735 = mul <8 x i32> %653, %94
  %736 = add <8 x i32> %735, %115
  %737 = add <8 x i32> %736, %734
  %738 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %737, i32 %2) #8
  %739 = mul <8 x i32> %613, %94
  %740 = add <8 x i32> %736, %739
  %741 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %740, i32 %2) #8
  store <8 x i32> %741, <8 x i32>* %188, align 32
  store <8 x i32> %738, <8 x i32>* %183, align 32
  %742 = mul <8 x i32> %603, %97
  %743 = mul <8 x i32> %643, %94
  %744 = add <8 x i32> %743, %115
  %745 = add <8 x i32> %744, %742
  %746 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %745, i32 %2) #8
  %747 = mul <8 x i32> %603, %94
  %748 = add <8 x i32> %744, %747
  %749 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %748, i32 %2) #8
  store <8 x i32> %749, <8 x i32>* %230, align 32
  store <8 x i32> %746, <8 x i32>* %224, align 32
  %750 = mul <8 x i32> %593, %97
  %751 = mul <8 x i32> %633, %94
  %752 = add <8 x i32> %751, %115
  %753 = add <8 x i32> %752, %750
  %754 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %753, i32 %2) #8
  %755 = mul <8 x i32> %593, %94
  %756 = add <8 x i32> %752, %755
  %757 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %756, i32 %2) #8
  store <8 x i32> %757, <8 x i32>* %228, align 32
  store <8 x i32> %754, <8 x i32>* %226, align 32
  %758 = mul <8 x i32> %583, %97
  %759 = mul <8 x i32> %623, %94
  %760 = add <8 x i32> %759, %115
  %761 = add <8 x i32> %760, %758
  %762 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %761, i32 %2) #8
  %763 = mul <8 x i32> %583, %94
  %764 = add <8 x i32> %760, %763
  %765 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %764, i32 %2) #8
  store <8 x i32> %765, <8 x i32>* %192, align 32
  store <8 x i32> %762, <8 x i32>* %197, align 32
  call fastcc void @idct32_stage9_avx2(<4 x i64>* nonnull %137, <4 x i64>* %1, i32 %3, i32 %4, i32 %5, <4 x i64>* nonnull %7, <4 x i64>* nonnull %8)
  call void @llvm.lifetime.end.p0i8(i64 1024, i8* nonnull %133) #8
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %128) #8
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %121) #8
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @idct32_low16_avx2(<4 x i64>* nocapture readonly, <4 x i64>*, i32, i32, i32, i32) #0 {
  %7 = alloca <4 x i64>, align 32
  %8 = alloca <4 x i64>, align 32
  %9 = alloca [32 x <4 x i64>], align 32
  %10 = add nsw i32 %2, -10
  %11 = sext i32 %10 to i64
  %12 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 62
  %13 = load i32, i32* %12, align 8
  %14 = insertelement <8 x i32> undef, i32 %13, i32 0
  %15 = shufflevector <8 x i32> %14, <8 x i32> undef, <8 x i32> zeroinitializer
  %16 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 30
  %17 = load i32, i32* %16, align 8
  %18 = insertelement <8 x i32> undef, i32 %17, i32 0
  %19 = shufflevector <8 x i32> %18, <8 x i32> undef, <8 x i32> zeroinitializer
  %20 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 46
  %21 = load i32, i32* %20, align 8
  %22 = insertelement <8 x i32> undef, i32 %21, i32 0
  %23 = shufflevector <8 x i32> %22, <8 x i32> undef, <8 x i32> zeroinitializer
  %24 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 14
  %25 = load i32, i32* %24, align 8
  %26 = insertelement <8 x i32> undef, i32 %25, i32 0
  %27 = shufflevector <8 x i32> %26, <8 x i32> undef, <8 x i32> zeroinitializer
  %28 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 54
  %29 = load i32, i32* %28, align 8
  %30 = insertelement <8 x i32> undef, i32 %29, i32 0
  %31 = shufflevector <8 x i32> %30, <8 x i32> undef, <8 x i32> zeroinitializer
  %32 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 22
  %33 = load i32, i32* %32, align 8
  %34 = insertelement <8 x i32> undef, i32 %33, i32 0
  %35 = shufflevector <8 x i32> %34, <8 x i32> undef, <8 x i32> zeroinitializer
  %36 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 38
  %37 = load i32, i32* %36, align 8
  %38 = insertelement <8 x i32> undef, i32 %37, i32 0
  %39 = shufflevector <8 x i32> %38, <8 x i32> undef, <8 x i32> zeroinitializer
  %40 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 6
  %41 = load i32, i32* %40, align 8
  %42 = insertelement <8 x i32> undef, i32 %41, i32 0
  %43 = shufflevector <8 x i32> %42, <8 x i32> undef, <8 x i32> zeroinitializer
  %44 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 26
  %45 = load i32, i32* %44, align 8
  %46 = insertelement <8 x i32> undef, i32 %45, i32 0
  %47 = shufflevector <8 x i32> %46, <8 x i32> undef, <8 x i32> zeroinitializer
  %48 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 10
  %49 = load i32, i32* %48, align 8
  %50 = insertelement <8 x i32> undef, i32 %49, i32 0
  %51 = shufflevector <8 x i32> %50, <8 x i32> undef, <8 x i32> zeroinitializer
  %52 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 18
  %53 = load i32, i32* %52, align 8
  %54 = insertelement <8 x i32> undef, i32 %53, i32 0
  %55 = shufflevector <8 x i32> %54, <8 x i32> undef, <8 x i32> zeroinitializer
  %56 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 2
  %57 = load i32, i32* %56, align 8
  %58 = insertelement <8 x i32> undef, i32 %57, i32 0
  %59 = shufflevector <8 x i32> %58, <8 x i32> undef, <8 x i32> zeroinitializer
  %60 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 58
  %61 = load i32, i32* %60, align 8
  %62 = sub nsw i32 0, %61
  %63 = insertelement <8 x i32> undef, i32 %62, i32 0
  %64 = shufflevector <8 x i32> %63, <8 x i32> undef, <8 x i32> zeroinitializer
  %65 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 42
  %66 = load i32, i32* %65, align 8
  %67 = sub nsw i32 0, %66
  %68 = insertelement <8 x i32> undef, i32 %67, i32 0
  %69 = shufflevector <8 x i32> %68, <8 x i32> undef, <8 x i32> zeroinitializer
  %70 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 50
  %71 = load i32, i32* %70, align 8
  %72 = sub nsw i32 0, %71
  %73 = insertelement <8 x i32> undef, i32 %72, i32 0
  %74 = shufflevector <8 x i32> %73, <8 x i32> undef, <8 x i32> zeroinitializer
  %75 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 34
  %76 = load i32, i32* %75, align 8
  %77 = sub nsw i32 0, %76
  %78 = insertelement <8 x i32> undef, i32 %77, i32 0
  %79 = shufflevector <8 x i32> %78, <8 x i32> undef, <8 x i32> zeroinitializer
  %80 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 60
  %81 = load i32, i32* %80, align 16
  %82 = insertelement <8 x i32> undef, i32 %81, i32 0
  %83 = shufflevector <8 x i32> %82, <8 x i32> undef, <8 x i32> zeroinitializer
  %84 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 28
  %85 = load i32, i32* %84, align 16
  %86 = insertelement <8 x i32> undef, i32 %85, i32 0
  %87 = shufflevector <8 x i32> %86, <8 x i32> undef, <8 x i32> zeroinitializer
  %88 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 44
  %89 = load i32, i32* %88, align 16
  %90 = insertelement <8 x i32> undef, i32 %89, i32 0
  %91 = shufflevector <8 x i32> %90, <8 x i32> undef, <8 x i32> zeroinitializer
  %92 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 12
  %93 = load i32, i32* %92, align 16
  %94 = insertelement <8 x i32> undef, i32 %93, i32 0
  %95 = shufflevector <8 x i32> %94, <8 x i32> undef, <8 x i32> zeroinitializer
  %96 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 20
  %97 = load i32, i32* %96, align 16
  %98 = insertelement <8 x i32> undef, i32 %97, i32 0
  %99 = shufflevector <8 x i32> %98, <8 x i32> undef, <8 x i32> zeroinitializer
  %100 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 4
  %101 = load i32, i32* %100, align 16
  %102 = insertelement <8 x i32> undef, i32 %101, i32 0
  %103 = shufflevector <8 x i32> %102, <8 x i32> undef, <8 x i32> zeroinitializer
  %104 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 52
  %105 = load i32, i32* %104, align 16
  %106 = sub nsw i32 0, %105
  %107 = insertelement <8 x i32> undef, i32 %106, i32 0
  %108 = shufflevector <8 x i32> %107, <8 x i32> undef, <8 x i32> zeroinitializer
  %109 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 36
  %110 = load i32, i32* %109, align 16
  %111 = sub nsw i32 0, %110
  %112 = insertelement <8 x i32> undef, i32 %111, i32 0
  %113 = shufflevector <8 x i32> %112, <8 x i32> undef, <8 x i32> zeroinitializer
  %114 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 56
  %115 = load i32, i32* %114, align 16
  %116 = insertelement <8 x i32> undef, i32 %115, i32 0
  %117 = shufflevector <8 x i32> %116, <8 x i32> undef, <8 x i32> zeroinitializer
  %118 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 24
  %119 = load i32, i32* %118, align 16
  %120 = insertelement <8 x i32> undef, i32 %119, i32 0
  %121 = shufflevector <8 x i32> %120, <8 x i32> undef, <8 x i32> zeroinitializer
  %122 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 40
  %123 = load i32, i32* %122, align 16
  %124 = insertelement <8 x i32> undef, i32 %123, i32 0
  %125 = shufflevector <8 x i32> %124, <8 x i32> undef, <8 x i32> zeroinitializer
  %126 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 8
  %127 = load i32, i32* %126, align 16
  %128 = insertelement <8 x i32> undef, i32 %127, i32 0
  %129 = shufflevector <8 x i32> %128, <8 x i32> undef, <8 x i32> zeroinitializer
  %130 = sub nsw i32 0, %123
  %131 = insertelement <8 x i32> undef, i32 %130, i32 0
  %132 = shufflevector <8 x i32> %131, <8 x i32> undef, <8 x i32> zeroinitializer
  %133 = sub nsw i32 0, %127
  %134 = insertelement <8 x i32> undef, i32 %133, i32 0
  %135 = shufflevector <8 x i32> %134, <8 x i32> undef, <8 x i32> zeroinitializer
  %136 = sub nsw i32 0, %115
  %137 = insertelement <8 x i32> undef, i32 %136, i32 0
  %138 = shufflevector <8 x i32> %137, <8 x i32> undef, <8 x i32> zeroinitializer
  %139 = sub nsw i32 0, %119
  %140 = insertelement <8 x i32> undef, i32 %139, i32 0
  %141 = shufflevector <8 x i32> %140, <8 x i32> undef, <8 x i32> zeroinitializer
  %142 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 32
  %143 = load i32, i32* %142, align 16
  %144 = insertelement <8 x i32> undef, i32 %143, i32 0
  %145 = shufflevector <8 x i32> %144, <8 x i32> undef, <8 x i32> zeroinitializer
  %146 = sub nsw i32 0, %143
  %147 = insertelement <8 x i32> undef, i32 %146, i32 0
  %148 = shufflevector <8 x i32> %147, <8 x i32> undef, <8 x i32> zeroinitializer
  %149 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 48
  %150 = load i32, i32* %149, align 16
  %151 = insertelement <8 x i32> undef, i32 %150, i32 0
  %152 = shufflevector <8 x i32> %151, <8 x i32> undef, <8 x i32> zeroinitializer
  %153 = sub nsw i32 0, %150
  %154 = insertelement <8 x i32> undef, i32 %153, i32 0
  %155 = shufflevector <8 x i32> %154, <8 x i32> undef, <8 x i32> zeroinitializer
  %156 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %11, i64 16
  %157 = load i32, i32* %156, align 16
  %158 = insertelement <8 x i32> undef, i32 %157, i32 0
  %159 = shufflevector <8 x i32> %158, <8 x i32> undef, <8 x i32> zeroinitializer
  %160 = sub nsw i32 0, %157
  %161 = insertelement <8 x i32> undef, i32 %160, i32 0
  %162 = shufflevector <8 x i32> %161, <8 x i32> undef, <8 x i32> zeroinitializer
  %163 = add nsw i32 %2, -1
  %164 = shl i32 1, %163
  %165 = insertelement <8 x i32> undef, i32 %164, i32 0
  %166 = shufflevector <8 x i32> %165, <8 x i32> undef, <8 x i32> zeroinitializer
  %167 = icmp eq i32 %3, 0
  %168 = select i1 %167, i32 8, i32 6
  %169 = add nsw i32 %168, %4
  %170 = icmp slt i32 %169, 16
  %171 = add i32 %169, -1
  %172 = bitcast <4 x i64>* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %172) #8
  %173 = shl i32 1, %171
  %174 = select i1 %170, i32 32768, i32 %173
  %175 = sub nsw i32 0, %174
  %176 = insertelement <8 x i32> undef, i32 %175, i32 0
  %177 = shufflevector <8 x i32> %176, <8 x i32> undef, <8 x i32> zeroinitializer
  %178 = bitcast <4 x i64>* %7 to <8 x i32>*
  store <8 x i32> %177, <8 x i32>* %178, align 32
  %179 = bitcast <4 x i64>* %8 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %179) #8
  %180 = add nsw i32 %174, -1
  %181 = insertelement <8 x i32> undef, i32 %180, i32 0
  %182 = shufflevector <8 x i32> %181, <8 x i32> undef, <8 x i32> zeroinitializer
  %183 = bitcast <4 x i64>* %8 to <8 x i32>*
  store <8 x i32> %182, <8 x i32>* %183, align 32
  %184 = bitcast [32 x <4 x i64>]* %9 to i8*
  call void @llvm.lifetime.start.p0i8(i64 1024, i8* nonnull %184) #8
  %185 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 1
  %186 = bitcast <4 x i64>* %185 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %186, i8 -86, i64 864, i1 false)
  %187 = load <4 x i64>, <4 x i64>* %0, align 32
  %188 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 0
  store <4 x i64> %187, <4 x i64>* %188, align 32
  %189 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 8
  %190 = load <4 x i64>, <4 x i64>* %189, align 32
  %191 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 2
  store <4 x i64> %190, <4 x i64>* %191, align 32
  %192 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 4
  %193 = load <4 x i64>, <4 x i64>* %192, align 32
  %194 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 4
  store <4 x i64> %193, <4 x i64>* %194, align 32
  %195 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 12
  %196 = load <4 x i64>, <4 x i64>* %195, align 32
  %197 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 6
  store <4 x i64> %196, <4 x i64>* %197, align 32
  %198 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 2
  %199 = load <4 x i64>, <4 x i64>* %198, align 32
  %200 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 8
  store <4 x i64> %199, <4 x i64>* %200, align 32
  %201 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 10
  %202 = load <4 x i64>, <4 x i64>* %201, align 32
  %203 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 10
  store <4 x i64> %202, <4 x i64>* %203, align 32
  %204 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 6
  %205 = load <4 x i64>, <4 x i64>* %204, align 32
  %206 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 12
  store <4 x i64> %205, <4 x i64>* %206, align 32
  %207 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 14
  %208 = load <4 x i64>, <4 x i64>* %207, align 32
  %209 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 14
  store <4 x i64> %208, <4 x i64>* %209, align 32
  %210 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 1
  %211 = bitcast <4 x i64>* %210 to <8 x i32>*
  %212 = load <8 x i32>, <8 x i32>* %211, align 32
  %213 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 16
  %214 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 9
  %215 = bitcast <4 x i64>* %214 to <8 x i32>*
  %216 = load <8 x i32>, <8 x i32>* %215, align 32
  %217 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 18
  %218 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 5
  %219 = bitcast <4 x i64>* %218 to <8 x i32>*
  %220 = load <8 x i32>, <8 x i32>* %219, align 32
  %221 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 20
  %222 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 13
  %223 = bitcast <4 x i64>* %222 to <8 x i32>*
  %224 = load <8 x i32>, <8 x i32>* %223, align 32
  %225 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 22
  %226 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 3
  %227 = load <4 x i64>, <4 x i64>* %226, align 32
  %228 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 24
  store <4 x i64> %227, <4 x i64>* %228, align 32
  %229 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 11
  %230 = bitcast <4 x i64>* %229 to <8 x i32>*
  %231 = load <8 x i32>, <8 x i32>* %230, align 32
  %232 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 26
  %233 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 7
  %234 = bitcast <4 x i64>* %233 to <8 x i32>*
  %235 = load <8 x i32>, <8 x i32>* %234, align 32
  %236 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 28
  %237 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 15
  %238 = bitcast <4 x i64>* %237 to <8 x i32>*
  %239 = load <8 x i32>, <8 x i32>* %238, align 32
  %240 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 30
  %241 = bitcast <4 x i64>* %213 to <8 x i32>*
  %242 = mul <8 x i32> %59, %212
  %243 = add <8 x i32> %242, %166
  %244 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %243, i32 %2) #8
  %245 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 31
  %246 = bitcast <4 x i64>* %245 to <8 x i32>*
  store <8 x i32> %244, <8 x i32>* %246, align 32
  %247 = mul <8 x i32> %15, %212
  %248 = add <8 x i32> %247, %166
  %249 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %248, i32 %2) #8
  store <8 x i32> %249, <8 x i32>* %241, align 32
  %250 = bitcast <4 x i64>* %240 to <8 x i32>*
  %251 = mul <8 x i32> %79, %239
  %252 = add <8 x i32> %251, %166
  %253 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %252, i32 %2) #8
  %254 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 17
  %255 = bitcast <4 x i64>* %254 to <8 x i32>*
  store <8 x i32> %253, <8 x i32>* %255, align 32
  %256 = mul <8 x i32> %19, %239
  %257 = add <8 x i32> %256, %166
  %258 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %257, i32 %2) #8
  store <8 x i32> %258, <8 x i32>* %250, align 32
  %259 = bitcast <4 x i64>* %217 to <8 x i32>*
  %260 = mul <8 x i32> %55, %216
  %261 = add <8 x i32> %260, %166
  %262 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %261, i32 %2) #8
  %263 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 29
  %264 = bitcast <4 x i64>* %263 to <8 x i32>*
  store <8 x i32> %262, <8 x i32>* %264, align 32
  %265 = mul <8 x i32> %23, %216
  %266 = add <8 x i32> %265, %166
  %267 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %266, i32 %2) #8
  store <8 x i32> %267, <8 x i32>* %259, align 32
  %268 = bitcast <4 x i64>* %236 to <8 x i32>*
  %269 = mul <8 x i32> %74, %235
  %270 = add <8 x i32> %269, %166
  %271 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %270, i32 %2) #8
  %272 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 19
  %273 = bitcast <4 x i64>* %272 to <8 x i32>*
  store <8 x i32> %271, <8 x i32>* %273, align 32
  %274 = mul <8 x i32> %27, %235
  %275 = add <8 x i32> %274, %166
  %276 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %275, i32 %2) #8
  store <8 x i32> %276, <8 x i32>* %268, align 32
  %277 = bitcast <4 x i64>* %221 to <8 x i32>*
  %278 = mul <8 x i32> %51, %220
  %279 = add <8 x i32> %278, %166
  %280 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %279, i32 %2) #8
  %281 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 27
  %282 = bitcast <4 x i64>* %281 to <8 x i32>*
  store <8 x i32> %280, <8 x i32>* %282, align 32
  %283 = mul <8 x i32> %31, %220
  %284 = add <8 x i32> %283, %166
  %285 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %284, i32 %2) #8
  store <8 x i32> %285, <8 x i32>* %277, align 32
  %286 = bitcast <4 x i64>* %232 to <8 x i32>*
  %287 = mul <8 x i32> %69, %231
  %288 = add <8 x i32> %287, %166
  %289 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %288, i32 %2) #8
  %290 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 21
  %291 = bitcast <4 x i64>* %290 to <8 x i32>*
  store <8 x i32> %289, <8 x i32>* %291, align 32
  %292 = mul <8 x i32> %35, %231
  %293 = add <8 x i32> %292, %166
  %294 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %293, i32 %2) #8
  store <8 x i32> %294, <8 x i32>* %286, align 32
  %295 = bitcast <4 x i64>* %225 to <8 x i32>*
  %296 = mul <8 x i32> %47, %224
  %297 = add <8 x i32> %296, %166
  %298 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %297, i32 %2) #8
  %299 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 25
  %300 = bitcast <4 x i64>* %299 to <8 x i32>*
  store <8 x i32> %298, <8 x i32>* %300, align 32
  %301 = mul <8 x i32> %39, %224
  %302 = add <8 x i32> %301, %166
  %303 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %302, i32 %2) #8
  store <8 x i32> %303, <8 x i32>* %295, align 32
  %304 = bitcast <4 x i64>* %228 to <8 x i32>*
  %305 = load <8 x i32>, <8 x i32>* %304, align 32
  %306 = mul <8 x i32> %305, %64
  %307 = add <8 x i32> %306, %166
  %308 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %307, i32 %2) #8
  %309 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 23
  %310 = bitcast <4 x i64>* %309 to <8 x i32>*
  store <8 x i32> %308, <8 x i32>* %310, align 32
  %311 = mul <8 x i32> %305, %43
  %312 = add <8 x i32> %311, %166
  %313 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %312, i32 %2) #8
  store <8 x i32> %313, <8 x i32>* %304, align 32
  %314 = bitcast <4 x i64>* %200 to <8 x i32>*
  %315 = load <8 x i32>, <8 x i32>* %314, align 32
  %316 = mul <8 x i32> %315, %103
  %317 = add <8 x i32> %316, %166
  %318 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %317, i32 %2) #8
  %319 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 15
  %320 = bitcast <4 x i64>* %319 to <8 x i32>*
  store <8 x i32> %318, <8 x i32>* %320, align 32
  %321 = mul <8 x i32> %315, %83
  %322 = add <8 x i32> %321, %166
  %323 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %322, i32 %2) #8
  store <8 x i32> %323, <8 x i32>* %314, align 32
  %324 = bitcast <4 x i64>* %209 to <8 x i32>*
  %325 = load <8 x i32>, <8 x i32>* %324, align 32
  %326 = mul <8 x i32> %325, %113
  %327 = add <8 x i32> %326, %166
  %328 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %327, i32 %2) #8
  %329 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 9
  %330 = bitcast <4 x i64>* %329 to <8 x i32>*
  store <8 x i32> %328, <8 x i32>* %330, align 32
  %331 = mul <8 x i32> %325, %87
  %332 = add <8 x i32> %331, %166
  %333 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %332, i32 %2) #8
  store <8 x i32> %333, <8 x i32>* %324, align 32
  %334 = bitcast <4 x i64>* %203 to <8 x i32>*
  %335 = load <8 x i32>, <8 x i32>* %334, align 32
  %336 = mul <8 x i32> %335, %99
  %337 = add <8 x i32> %336, %166
  %338 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %337, i32 %2) #8
  %339 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 13
  %340 = bitcast <4 x i64>* %339 to <8 x i32>*
  store <8 x i32> %338, <8 x i32>* %340, align 32
  %341 = mul <8 x i32> %335, %91
  %342 = add <8 x i32> %341, %166
  %343 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %342, i32 %2) #8
  store <8 x i32> %343, <8 x i32>* %334, align 32
  %344 = bitcast <4 x i64>* %206 to <8 x i32>*
  %345 = load <8 x i32>, <8 x i32>* %344, align 32
  %346 = mul <8 x i32> %345, %108
  %347 = add <8 x i32> %346, %166
  %348 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %347, i32 %2) #8
  %349 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 11
  %350 = bitcast <4 x i64>* %349 to <8 x i32>*
  store <8 x i32> %348, <8 x i32>* %350, align 32
  %351 = mul <8 x i32> %345, %95
  %352 = add <8 x i32> %351, %166
  %353 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %352, i32 %2) #8
  store <8 x i32> %353, <8 x i32>* %344, align 32
  %354 = add <8 x i32> %253, %249
  %355 = sub <8 x i32> %249, %253
  %356 = load <8 x i32>, <8 x i32>* %178, align 32
  %357 = icmp sgt <8 x i32> %354, %356
  %358 = select <8 x i1> %357, <8 x i32> %354, <8 x i32> %356
  %359 = load <8 x i32>, <8 x i32>* %183, align 32
  %360 = icmp slt <8 x i32> %358, %359
  %361 = select <8 x i1> %360, <8 x i32> %358, <8 x i32> %359
  %362 = icmp sgt <8 x i32> %355, %356
  %363 = select <8 x i1> %362, <8 x i32> %355, <8 x i32> %356
  %364 = icmp slt <8 x i32> %363, %359
  %365 = select <8 x i1> %364, <8 x i32> %363, <8 x i32> %359
  store <8 x i32> %361, <8 x i32>* %241, align 32
  store <8 x i32> %365, <8 x i32>* %255, align 32
  %366 = add <8 x i32> %271, %267
  %367 = sub <8 x i32> %271, %267
  %368 = icmp sgt <8 x i32> %366, %356
  %369 = select <8 x i1> %368, <8 x i32> %366, <8 x i32> %356
  %370 = icmp slt <8 x i32> %369, %359
  %371 = select <8 x i1> %370, <8 x i32> %369, <8 x i32> %359
  %372 = icmp sgt <8 x i32> %367, %356
  %373 = select <8 x i1> %372, <8 x i32> %367, <8 x i32> %356
  %374 = icmp slt <8 x i32> %373, %359
  %375 = select <8 x i1> %374, <8 x i32> %373, <8 x i32> %359
  store <8 x i32> %371, <8 x i32>* %273, align 32
  store <8 x i32> %375, <8 x i32>* %259, align 32
  %376 = add <8 x i32> %289, %285
  %377 = sub <8 x i32> %285, %289
  %378 = icmp sgt <8 x i32> %376, %356
  %379 = select <8 x i1> %378, <8 x i32> %376, <8 x i32> %356
  %380 = icmp slt <8 x i32> %379, %359
  %381 = select <8 x i1> %380, <8 x i32> %379, <8 x i32> %359
  %382 = icmp sgt <8 x i32> %377, %356
  %383 = select <8 x i1> %382, <8 x i32> %377, <8 x i32> %356
  %384 = icmp slt <8 x i32> %383, %359
  %385 = select <8 x i1> %384, <8 x i32> %383, <8 x i32> %359
  store <8 x i32> %381, <8 x i32>* %277, align 32
  store <8 x i32> %385, <8 x i32>* %291, align 32
  %386 = add <8 x i32> %308, %303
  %387 = sub <8 x i32> %308, %303
  %388 = icmp sgt <8 x i32> %386, %356
  %389 = select <8 x i1> %388, <8 x i32> %386, <8 x i32> %356
  %390 = icmp slt <8 x i32> %389, %359
  %391 = select <8 x i1> %390, <8 x i32> %389, <8 x i32> %359
  %392 = icmp sgt <8 x i32> %387, %356
  %393 = select <8 x i1> %392, <8 x i32> %387, <8 x i32> %356
  %394 = icmp slt <8 x i32> %393, %359
  %395 = select <8 x i1> %394, <8 x i32> %393, <8 x i32> %359
  store <8 x i32> %391, <8 x i32>* %310, align 32
  store <8 x i32> %395, <8 x i32>* %295, align 32
  %396 = add <8 x i32> %313, %298
  %397 = sub <8 x i32> %313, %298
  %398 = icmp sgt <8 x i32> %396, %356
  %399 = select <8 x i1> %398, <8 x i32> %396, <8 x i32> %356
  %400 = icmp slt <8 x i32> %399, %359
  %401 = select <8 x i1> %400, <8 x i32> %399, <8 x i32> %359
  %402 = icmp sgt <8 x i32> %397, %356
  %403 = select <8 x i1> %402, <8 x i32> %397, <8 x i32> %356
  %404 = icmp slt <8 x i32> %403, %359
  %405 = select <8 x i1> %404, <8 x i32> %403, <8 x i32> %359
  store <8 x i32> %401, <8 x i32>* %304, align 32
  store <8 x i32> %405, <8 x i32>* %300, align 32
  %406 = add <8 x i32> %294, %280
  %407 = sub <8 x i32> %280, %294
  %408 = icmp sgt <8 x i32> %406, %356
  %409 = select <8 x i1> %408, <8 x i32> %406, <8 x i32> %356
  %410 = icmp slt <8 x i32> %409, %359
  %411 = select <8 x i1> %410, <8 x i32> %409, <8 x i32> %359
  %412 = icmp sgt <8 x i32> %407, %356
  %413 = select <8 x i1> %412, <8 x i32> %407, <8 x i32> %356
  %414 = icmp slt <8 x i32> %413, %359
  %415 = select <8 x i1> %414, <8 x i32> %413, <8 x i32> %359
  store <8 x i32> %411, <8 x i32>* %282, align 32
  store <8 x i32> %415, <8 x i32>* %286, align 32
  %416 = add <8 x i32> %276, %262
  %417 = sub <8 x i32> %276, %262
  %418 = icmp sgt <8 x i32> %416, %356
  %419 = select <8 x i1> %418, <8 x i32> %416, <8 x i32> %356
  %420 = icmp slt <8 x i32> %419, %359
  %421 = select <8 x i1> %420, <8 x i32> %419, <8 x i32> %359
  %422 = icmp sgt <8 x i32> %417, %356
  %423 = select <8 x i1> %422, <8 x i32> %417, <8 x i32> %356
  %424 = icmp slt <8 x i32> %423, %359
  %425 = select <8 x i1> %424, <8 x i32> %423, <8 x i32> %359
  store <8 x i32> %421, <8 x i32>* %268, align 32
  store <8 x i32> %425, <8 x i32>* %264, align 32
  %426 = add <8 x i32> %258, %244
  %427 = sub <8 x i32> %244, %258
  %428 = icmp sgt <8 x i32> %426, %356
  %429 = select <8 x i1> %428, <8 x i32> %426, <8 x i32> %356
  %430 = icmp slt <8 x i32> %429, %359
  %431 = select <8 x i1> %430, <8 x i32> %429, <8 x i32> %359
  %432 = icmp sgt <8 x i32> %427, %356
  %433 = select <8 x i1> %432, <8 x i32> %427, <8 x i32> %356
  %434 = icmp slt <8 x i32> %433, %359
  %435 = select <8 x i1> %434, <8 x i32> %433, <8 x i32> %359
  store <8 x i32> %431, <8 x i32>* %246, align 32
  %436 = bitcast <4 x i64>* %194 to <8 x i32>*
  %437 = load <8 x i32>, <8 x i32>* %436, align 32
  %438 = mul <8 x i32> %437, %129
  %439 = add <8 x i32> %438, %166
  %440 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %439, i32 %2) #8
  %441 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 7
  %442 = bitcast <4 x i64>* %441 to <8 x i32>*
  store <8 x i32> %440, <8 x i32>* %442, align 32
  %443 = mul <8 x i32> %437, %117
  %444 = add <8 x i32> %443, %166
  %445 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %444, i32 %2) #8
  store <8 x i32> %445, <8 x i32>* %436, align 32
  %446 = bitcast <4 x i64>* %197 to <8 x i32>*
  %447 = load <8 x i32>, <8 x i32>* %446, align 32
  %448 = mul <8 x i32> %447, %132
  %449 = add <8 x i32> %448, %166
  %450 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %449, i32 %2) #8
  %451 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 5
  %452 = bitcast <4 x i64>* %451 to <8 x i32>*
  store <8 x i32> %450, <8 x i32>* %452, align 32
  %453 = mul <8 x i32> %447, %121
  %454 = add <8 x i32> %453, %166
  %455 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %454, i32 %2) #8
  store <8 x i32> %455, <8 x i32>* %446, align 32
  %456 = add <8 x i32> %328, %323
  %457 = sub <8 x i32> %323, %328
  %458 = icmp sgt <8 x i32> %456, %356
  %459 = select <8 x i1> %458, <8 x i32> %456, <8 x i32> %356
  %460 = icmp slt <8 x i32> %459, %359
  %461 = select <8 x i1> %460, <8 x i32> %459, <8 x i32> %359
  %462 = icmp sgt <8 x i32> %457, %356
  %463 = select <8 x i1> %462, <8 x i32> %457, <8 x i32> %356
  %464 = icmp slt <8 x i32> %463, %359
  %465 = select <8 x i1> %464, <8 x i32> %463, <8 x i32> %359
  store <8 x i32> %461, <8 x i32>* %314, align 32
  store <8 x i32> %465, <8 x i32>* %330, align 32
  %466 = add <8 x i32> %348, %343
  %467 = sub <8 x i32> %348, %343
  %468 = icmp sgt <8 x i32> %466, %356
  %469 = select <8 x i1> %468, <8 x i32> %466, <8 x i32> %356
  %470 = icmp slt <8 x i32> %469, %359
  %471 = select <8 x i1> %470, <8 x i32> %469, <8 x i32> %359
  %472 = icmp sgt <8 x i32> %467, %356
  %473 = select <8 x i1> %472, <8 x i32> %467, <8 x i32> %356
  %474 = icmp slt <8 x i32> %473, %359
  %475 = select <8 x i1> %474, <8 x i32> %473, <8 x i32> %359
  store <8 x i32> %471, <8 x i32>* %350, align 32
  store <8 x i32> %475, <8 x i32>* %334, align 32
  %476 = add <8 x i32> %353, %338
  %477 = sub <8 x i32> %353, %338
  %478 = icmp sgt <8 x i32> %476, %356
  %479 = select <8 x i1> %478, <8 x i32> %476, <8 x i32> %356
  %480 = icmp slt <8 x i32> %479, %359
  %481 = select <8 x i1> %480, <8 x i32> %479, <8 x i32> %359
  %482 = icmp sgt <8 x i32> %477, %356
  %483 = select <8 x i1> %482, <8 x i32> %477, <8 x i32> %356
  %484 = icmp slt <8 x i32> %483, %359
  %485 = select <8 x i1> %484, <8 x i32> %483, <8 x i32> %359
  store <8 x i32> %481, <8 x i32>* %344, align 32
  store <8 x i32> %485, <8 x i32>* %340, align 32
  %486 = add <8 x i32> %333, %318
  %487 = sub <8 x i32> %318, %333
  %488 = icmp sgt <8 x i32> %486, %356
  %489 = select <8 x i1> %488, <8 x i32> %486, <8 x i32> %356
  %490 = icmp slt <8 x i32> %489, %359
  %491 = select <8 x i1> %490, <8 x i32> %489, <8 x i32> %359
  %492 = icmp sgt <8 x i32> %487, %356
  %493 = select <8 x i1> %492, <8 x i32> %487, <8 x i32> %356
  %494 = icmp slt <8 x i32> %493, %359
  %495 = select <8 x i1> %494, <8 x i32> %493, <8 x i32> %359
  store <8 x i32> %491, <8 x i32>* %320, align 32
  store <8 x i32> %495, <8 x i32>* %324, align 32
  %496 = load <8 x i32>, <8 x i32>* %255, align 32
  %497 = mul <8 x i32> %496, %135
  %498 = mul <8 x i32> %435, %117
  %499 = add <8 x i32> %497, %166
  %500 = add <8 x i32> %499, %498
  %501 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %500, i32 %2) #8
  %502 = mul <8 x i32> %496, %117
  %503 = mul <8 x i32> %435, %129
  %504 = add <8 x i32> %502, %166
  %505 = add <8 x i32> %504, %503
  %506 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %505, i32 %2) #8
  store <8 x i32> %506, <8 x i32>* %250, align 32
  store <8 x i32> %501, <8 x i32>* %255, align 32
  %507 = load <8 x i32>, <8 x i32>* %259, align 32
  %508 = mul <8 x i32> %507, %138
  %509 = load <8 x i32>, <8 x i32>* %264, align 32
  %510 = mul <8 x i32> %509, %135
  %511 = add <8 x i32> %508, %166
  %512 = add <8 x i32> %511, %510
  %513 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %512, i32 %2) #8
  %514 = mul <8 x i32> %507, %135
  %515 = mul <8 x i32> %509, %117
  %516 = add <8 x i32> %514, %166
  %517 = add <8 x i32> %516, %515
  %518 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %517, i32 %2) #8
  store <8 x i32> %518, <8 x i32>* %264, align 32
  store <8 x i32> %513, <8 x i32>* %259, align 32
  %519 = load <8 x i32>, <8 x i32>* %291, align 32
  %520 = mul <8 x i32> %519, %132
  %521 = load <8 x i32>, <8 x i32>* %286, align 32
  %522 = mul <8 x i32> %521, %121
  %523 = add <8 x i32> %520, %166
  %524 = add <8 x i32> %523, %522
  %525 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %524, i32 %2) #8
  %526 = mul <8 x i32> %519, %121
  %527 = mul <8 x i32> %521, %125
  %528 = add <8 x i32> %526, %166
  %529 = add <8 x i32> %528, %527
  %530 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %529, i32 %2) #8
  store <8 x i32> %530, <8 x i32>* %286, align 32
  store <8 x i32> %525, <8 x i32>* %291, align 32
  %531 = load <8 x i32>, <8 x i32>* %295, align 32
  %532 = mul <8 x i32> %531, %141
  %533 = load <8 x i32>, <8 x i32>* %300, align 32
  %534 = mul <8 x i32> %533, %132
  %535 = add <8 x i32> %532, %166
  %536 = add <8 x i32> %535, %534
  %537 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %536, i32 %2) #8
  %538 = mul <8 x i32> %531, %132
  %539 = mul <8 x i32> %533, %121
  %540 = add <8 x i32> %538, %166
  %541 = add <8 x i32> %540, %539
  %542 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %541, i32 %2) #8
  store <8 x i32> %542, <8 x i32>* %300, align 32
  store <8 x i32> %537, <8 x i32>* %295, align 32
  %543 = bitcast [32 x <4 x i64>]* %9 to <8 x i32>*
  %544 = load <8 x i32>, <8 x i32>* %543, align 32
  %545 = mul <8 x i32> %544, %145
  %546 = add <8 x i32> %545, %166
  %547 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %546, i32 %2) #8
  store <8 x i32> %547, <8 x i32>* %543, align 32
  %548 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 1
  %549 = bitcast <4 x i64>* %548 to <8 x i32>*
  store <8 x i32> %547, <8 x i32>* %549, align 32
  %550 = bitcast <4 x i64>* %191 to <8 x i32>*
  %551 = load <8 x i32>, <8 x i32>* %550, align 32
  %552 = mul <8 x i32> %551, %159
  %553 = add <8 x i32> %552, %166
  %554 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %553, i32 %2) #8
  %555 = getelementptr inbounds [32 x <4 x i64>], [32 x <4 x i64>]* %9, i64 0, i64 3
  %556 = bitcast <4 x i64>* %555 to <8 x i32>*
  store <8 x i32> %554, <8 x i32>* %556, align 32
  %557 = mul <8 x i32> %551, %152
  %558 = add <8 x i32> %557, %166
  %559 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %558, i32 %2) #8
  store <8 x i32> %559, <8 x i32>* %550, align 32
  %560 = add <8 x i32> %450, %445
  %561 = sub <8 x i32> %445, %450
  %562 = icmp sgt <8 x i32> %560, %356
  %563 = select <8 x i1> %562, <8 x i32> %560, <8 x i32> %356
  %564 = icmp slt <8 x i32> %563, %359
  %565 = select <8 x i1> %564, <8 x i32> %563, <8 x i32> %359
  %566 = icmp sgt <8 x i32> %561, %356
  %567 = select <8 x i1> %566, <8 x i32> %561, <8 x i32> %356
  %568 = icmp slt <8 x i32> %567, %359
  %569 = select <8 x i1> %568, <8 x i32> %567, <8 x i32> %359
  store <8 x i32> %565, <8 x i32>* %436, align 32
  store <8 x i32> %569, <8 x i32>* %452, align 32
  %570 = add <8 x i32> %455, %440
  %571 = sub <8 x i32> %440, %455
  %572 = icmp sgt <8 x i32> %570, %356
  %573 = select <8 x i1> %572, <8 x i32> %570, <8 x i32> %356
  %574 = icmp slt <8 x i32> %573, %359
  %575 = select <8 x i1> %574, <8 x i32> %573, <8 x i32> %359
  %576 = icmp sgt <8 x i32> %571, %356
  %577 = select <8 x i1> %576, <8 x i32> %571, <8 x i32> %356
  %578 = icmp slt <8 x i32> %577, %359
  %579 = select <8 x i1> %578, <8 x i32> %577, <8 x i32> %359
  store <8 x i32> %575, <8 x i32>* %442, align 32
  store <8 x i32> %579, <8 x i32>* %446, align 32
  %580 = load <8 x i32>, <8 x i32>* %330, align 32
  %581 = mul <8 x i32> %580, %162
  %582 = load <8 x i32>, <8 x i32>* %324, align 32
  %583 = mul <8 x i32> %582, %152
  %584 = add <8 x i32> %581, %166
  %585 = add <8 x i32> %584, %583
  %586 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %585, i32 %2) #8
  %587 = mul <8 x i32> %580, %152
  %588 = mul <8 x i32> %582, %159
  %589 = add <8 x i32> %587, %166
  %590 = add <8 x i32> %589, %588
  %591 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %590, i32 %2) #8
  store <8 x i32> %591, <8 x i32>* %324, align 32
  store <8 x i32> %586, <8 x i32>* %330, align 32
  %592 = load <8 x i32>, <8 x i32>* %334, align 32
  %593 = mul <8 x i32> %592, %155
  %594 = load <8 x i32>, <8 x i32>* %340, align 32
  %595 = mul <8 x i32> %594, %162
  %596 = add <8 x i32> %593, %166
  %597 = add <8 x i32> %596, %595
  %598 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %597, i32 %2) #8
  %599 = mul <8 x i32> %592, %162
  %600 = mul <8 x i32> %594, %152
  %601 = add <8 x i32> %599, %166
  %602 = add <8 x i32> %601, %600
  %603 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %602, i32 %2) #8
  store <8 x i32> %603, <8 x i32>* %340, align 32
  store <8 x i32> %598, <8 x i32>* %334, align 32
  %604 = load <8 x i32>, <8 x i32>* %241, align 32
  %605 = load <8 x i32>, <8 x i32>* %273, align 32
  %606 = add <8 x i32> %605, %604
  %607 = sub <8 x i32> %604, %605
  %608 = icmp sgt <8 x i32> %606, %356
  %609 = select <8 x i1> %608, <8 x i32> %606, <8 x i32> %356
  %610 = icmp slt <8 x i32> %609, %359
  %611 = select <8 x i1> %610, <8 x i32> %609, <8 x i32> %359
  %612 = icmp sgt <8 x i32> %607, %356
  %613 = select <8 x i1> %612, <8 x i32> %607, <8 x i32> %356
  %614 = icmp slt <8 x i32> %613, %359
  %615 = select <8 x i1> %614, <8 x i32> %613, <8 x i32> %359
  store <8 x i32> %611, <8 x i32>* %241, align 32
  store <8 x i32> %615, <8 x i32>* %273, align 32
  %616 = add <8 x i32> %513, %501
  %617 = sub <8 x i32> %501, %513
  %618 = icmp sgt <8 x i32> %616, %356
  %619 = select <8 x i1> %618, <8 x i32> %616, <8 x i32> %356
  %620 = icmp slt <8 x i32> %619, %359
  %621 = select <8 x i1> %620, <8 x i32> %619, <8 x i32> %359
  %622 = icmp sgt <8 x i32> %617, %356
  %623 = select <8 x i1> %622, <8 x i32> %617, <8 x i32> %356
  %624 = icmp slt <8 x i32> %623, %359
  %625 = select <8 x i1> %624, <8 x i32> %623, <8 x i32> %359
  store <8 x i32> %621, <8 x i32>* %255, align 32
  store <8 x i32> %625, <8 x i32>* %259, align 32
  %626 = load <8 x i32>, <8 x i32>* %310, align 32
  %627 = load <8 x i32>, <8 x i32>* %277, align 32
  %628 = add <8 x i32> %627, %626
  %629 = sub <8 x i32> %626, %627
  %630 = icmp sgt <8 x i32> %628, %356
  %631 = select <8 x i1> %630, <8 x i32> %628, <8 x i32> %356
  %632 = icmp slt <8 x i32> %631, %359
  %633 = select <8 x i1> %632, <8 x i32> %631, <8 x i32> %359
  %634 = icmp sgt <8 x i32> %629, %356
  %635 = select <8 x i1> %634, <8 x i32> %629, <8 x i32> %356
  %636 = icmp slt <8 x i32> %635, %359
  %637 = select <8 x i1> %636, <8 x i32> %635, <8 x i32> %359
  store <8 x i32> %633, <8 x i32>* %310, align 32
  store <8 x i32> %637, <8 x i32>* %277, align 32
  %638 = add <8 x i32> %537, %525
  %639 = sub <8 x i32> %537, %525
  %640 = icmp sgt <8 x i32> %638, %356
  %641 = select <8 x i1> %640, <8 x i32> %638, <8 x i32> %356
  %642 = icmp slt <8 x i32> %641, %359
  %643 = select <8 x i1> %642, <8 x i32> %641, <8 x i32> %359
  %644 = icmp sgt <8 x i32> %639, %356
  %645 = select <8 x i1> %644, <8 x i32> %639, <8 x i32> %356
  %646 = icmp slt <8 x i32> %645, %359
  %647 = select <8 x i1> %646, <8 x i32> %645, <8 x i32> %359
  store <8 x i32> %643, <8 x i32>* %295, align 32
  store <8 x i32> %647, <8 x i32>* %291, align 32
  %648 = load <8 x i32>, <8 x i32>* %304, align 32
  %649 = load <8 x i32>, <8 x i32>* %282, align 32
  %650 = add <8 x i32> %649, %648
  %651 = sub <8 x i32> %648, %649
  %652 = icmp sgt <8 x i32> %650, %356
  %653 = select <8 x i1> %652, <8 x i32> %650, <8 x i32> %356
  %654 = icmp slt <8 x i32> %653, %359
  %655 = select <8 x i1> %654, <8 x i32> %653, <8 x i32> %359
  %656 = icmp sgt <8 x i32> %651, %356
  %657 = select <8 x i1> %656, <8 x i32> %651, <8 x i32> %356
  %658 = icmp slt <8 x i32> %657, %359
  %659 = select <8 x i1> %658, <8 x i32> %657, <8 x i32> %359
  store <8 x i32> %655, <8 x i32>* %304, align 32
  store <8 x i32> %659, <8 x i32>* %282, align 32
  %660 = add <8 x i32> %542, %530
  %661 = sub <8 x i32> %542, %530
  %662 = icmp sgt <8 x i32> %660, %356
  %663 = select <8 x i1> %662, <8 x i32> %660, <8 x i32> %356
  %664 = icmp slt <8 x i32> %663, %359
  %665 = select <8 x i1> %664, <8 x i32> %663, <8 x i32> %359
  %666 = icmp sgt <8 x i32> %661, %356
  %667 = select <8 x i1> %666, <8 x i32> %661, <8 x i32> %356
  %668 = icmp slt <8 x i32> %667, %359
  %669 = select <8 x i1> %668, <8 x i32> %667, <8 x i32> %359
  store <8 x i32> %665, <8 x i32>* %300, align 32
  store <8 x i32> %669, <8 x i32>* %286, align 32
  %670 = load <8 x i32>, <8 x i32>* %246, align 32
  %671 = load <8 x i32>, <8 x i32>* %268, align 32
  %672 = add <8 x i32> %671, %670
  %673 = sub <8 x i32> %670, %671
  %674 = icmp sgt <8 x i32> %672, %356
  %675 = select <8 x i1> %674, <8 x i32> %672, <8 x i32> %356
  %676 = icmp slt <8 x i32> %675, %359
  %677 = select <8 x i1> %676, <8 x i32> %675, <8 x i32> %359
  %678 = icmp sgt <8 x i32> %673, %356
  %679 = select <8 x i1> %678, <8 x i32> %673, <8 x i32> %356
  %680 = icmp slt <8 x i32> %679, %359
  %681 = select <8 x i1> %680, <8 x i32> %679, <8 x i32> %359
  store <8 x i32> %677, <8 x i32>* %246, align 32
  store <8 x i32> %681, <8 x i32>* %268, align 32
  %682 = add <8 x i32> %518, %506
  %683 = sub <8 x i32> %506, %518
  %684 = icmp sgt <8 x i32> %682, %356
  %685 = select <8 x i1> %684, <8 x i32> %682, <8 x i32> %356
  %686 = icmp slt <8 x i32> %685, %359
  %687 = select <8 x i1> %686, <8 x i32> %685, <8 x i32> %359
  %688 = icmp sgt <8 x i32> %683, %356
  %689 = select <8 x i1> %688, <8 x i32> %683, <8 x i32> %356
  %690 = icmp slt <8 x i32> %689, %359
  %691 = select <8 x i1> %690, <8 x i32> %689, <8 x i32> %359
  store <8 x i32> %687, <8 x i32>* %250, align 32
  %692 = add <8 x i32> %554, %547
  %693 = sub <8 x i32> %547, %554
  %694 = icmp sgt <8 x i32> %692, %356
  %695 = select <8 x i1> %694, <8 x i32> %692, <8 x i32> %356
  %696 = icmp slt <8 x i32> %695, %359
  %697 = select <8 x i1> %696, <8 x i32> %695, <8 x i32> %359
  %698 = icmp sgt <8 x i32> %693, %356
  %699 = select <8 x i1> %698, <8 x i32> %693, <8 x i32> %356
  %700 = icmp slt <8 x i32> %699, %359
  %701 = select <8 x i1> %700, <8 x i32> %699, <8 x i32> %359
  store <8 x i32> %697, <8 x i32>* %543, align 32
  store <8 x i32> %701, <8 x i32>* %556, align 32
  %702 = add <8 x i32> %559, %547
  %703 = sub <8 x i32> %547, %559
  %704 = icmp sgt <8 x i32> %702, %356
  %705 = select <8 x i1> %704, <8 x i32> %702, <8 x i32> %356
  %706 = icmp slt <8 x i32> %705, %359
  %707 = select <8 x i1> %706, <8 x i32> %705, <8 x i32> %359
  %708 = icmp sgt <8 x i32> %703, %356
  %709 = select <8 x i1> %708, <8 x i32> %703, <8 x i32> %356
  %710 = icmp slt <8 x i32> %709, %359
  %711 = select <8 x i1> %710, <8 x i32> %709, <8 x i32> %359
  store <8 x i32> %707, <8 x i32>* %549, align 32
  store <8 x i32> %711, <8 x i32>* %550, align 32
  %712 = load <8 x i32>, <8 x i32>* %452, align 32
  %713 = mul <8 x i32> %712, %148
  %714 = load <8 x i32>, <8 x i32>* %446, align 32
  %715 = mul <8 x i32> %714, %145
  %716 = add <8 x i32> %715, %166
  %717 = add <8 x i32> %716, %713
  %718 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %717, i32 %2) #8
  %719 = mul <8 x i32> %712, %145
  %720 = add <8 x i32> %716, %719
  %721 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %720, i32 %2) #8
  store <8 x i32> %721, <8 x i32>* %446, align 32
  store <8 x i32> %718, <8 x i32>* %452, align 32
  %722 = load <8 x i32>, <8 x i32>* %314, align 32
  %723 = load <8 x i32>, <8 x i32>* %350, align 32
  %724 = add <8 x i32> %723, %722
  %725 = sub <8 x i32> %722, %723
  %726 = icmp sgt <8 x i32> %724, %356
  %727 = select <8 x i1> %726, <8 x i32> %724, <8 x i32> %356
  %728 = icmp slt <8 x i32> %727, %359
  %729 = select <8 x i1> %728, <8 x i32> %727, <8 x i32> %359
  %730 = icmp sgt <8 x i32> %725, %356
  %731 = select <8 x i1> %730, <8 x i32> %725, <8 x i32> %356
  %732 = icmp slt <8 x i32> %731, %359
  %733 = select <8 x i1> %732, <8 x i32> %731, <8 x i32> %359
  store <8 x i32> %729, <8 x i32>* %314, align 32
  store <8 x i32> %733, <8 x i32>* %350, align 32
  %734 = add <8 x i32> %598, %586
  %735 = sub <8 x i32> %586, %598
  %736 = icmp sgt <8 x i32> %734, %356
  %737 = select <8 x i1> %736, <8 x i32> %734, <8 x i32> %356
  %738 = icmp slt <8 x i32> %737, %359
  %739 = select <8 x i1> %738, <8 x i32> %737, <8 x i32> %359
  %740 = icmp sgt <8 x i32> %735, %356
  %741 = select <8 x i1> %740, <8 x i32> %735, <8 x i32> %356
  %742 = icmp slt <8 x i32> %741, %359
  %743 = select <8 x i1> %742, <8 x i32> %741, <8 x i32> %359
  store <8 x i32> %739, <8 x i32>* %330, align 32
  store <8 x i32> %743, <8 x i32>* %334, align 32
  %744 = load <8 x i32>, <8 x i32>* %320, align 32
  %745 = load <8 x i32>, <8 x i32>* %344, align 32
  %746 = add <8 x i32> %745, %744
  %747 = sub <8 x i32> %744, %745
  %748 = icmp sgt <8 x i32> %746, %356
  %749 = select <8 x i1> %748, <8 x i32> %746, <8 x i32> %356
  %750 = icmp slt <8 x i32> %749, %359
  %751 = select <8 x i1> %750, <8 x i32> %749, <8 x i32> %359
  %752 = icmp sgt <8 x i32> %747, %356
  %753 = select <8 x i1> %752, <8 x i32> %747, <8 x i32> %356
  %754 = icmp slt <8 x i32> %753, %359
  %755 = select <8 x i1> %754, <8 x i32> %753, <8 x i32> %359
  store <8 x i32> %751, <8 x i32>* %320, align 32
  store <8 x i32> %755, <8 x i32>* %344, align 32
  %756 = add <8 x i32> %603, %591
  %757 = sub <8 x i32> %591, %603
  %758 = icmp sgt <8 x i32> %756, %356
  %759 = select <8 x i1> %758, <8 x i32> %756, <8 x i32> %356
  %760 = icmp slt <8 x i32> %759, %359
  %761 = select <8 x i1> %760, <8 x i32> %759, <8 x i32> %359
  %762 = icmp sgt <8 x i32> %757, %356
  %763 = select <8 x i1> %762, <8 x i32> %757, <8 x i32> %356
  %764 = icmp slt <8 x i32> %763, %359
  %765 = select <8 x i1> %764, <8 x i32> %763, <8 x i32> %359
  store <8 x i32> %761, <8 x i32>* %324, align 32
  store <8 x i32> %765, <8 x i32>* %340, align 32
  %766 = mul <8 x i32> %625, %162
  %767 = mul <8 x i32> %691, %152
  %768 = add <8 x i32> %766, %166
  %769 = add <8 x i32> %768, %767
  %770 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %769, i32 %2) #8
  %771 = mul <8 x i32> %625, %152
  %772 = mul <8 x i32> %691, %159
  %773 = add <8 x i32> %771, %166
  %774 = add <8 x i32> %773, %772
  %775 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %774, i32 %2) #8
  store <8 x i32> %775, <8 x i32>* %264, align 32
  store <8 x i32> %770, <8 x i32>* %259, align 32
  %776 = mul <8 x i32> %615, %162
  %777 = mul <8 x i32> %681, %152
  %778 = add <8 x i32> %776, %166
  %779 = add <8 x i32> %778, %777
  %780 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %779, i32 %2) #8
  %781 = mul <8 x i32> %615, %152
  %782 = mul <8 x i32> %681, %159
  %783 = add <8 x i32> %781, %166
  %784 = add <8 x i32> %783, %782
  %785 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %784, i32 %2) #8
  store <8 x i32> %785, <8 x i32>* %268, align 32
  store <8 x i32> %780, <8 x i32>* %273, align 32
  %786 = mul <8 x i32> %637, %155
  %787 = mul <8 x i32> %659, %162
  %788 = add <8 x i32> %786, %166
  %789 = add <8 x i32> %788, %787
  %790 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %789, i32 %2) #8
  %791 = mul <8 x i32> %637, %162
  %792 = mul <8 x i32> %659, %152
  %793 = add <8 x i32> %791, %166
  %794 = add <8 x i32> %793, %792
  %795 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %794, i32 %2) #8
  store <8 x i32> %795, <8 x i32>* %282, align 32
  store <8 x i32> %790, <8 x i32>* %277, align 32
  %796 = mul <8 x i32> %647, %155
  %797 = mul <8 x i32> %669, %162
  %798 = add <8 x i32> %796, %166
  %799 = add <8 x i32> %798, %797
  %800 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %799, i32 %2) #8
  %801 = mul <8 x i32> %647, %162
  %802 = mul <8 x i32> %669, %152
  %803 = add <8 x i32> %801, %166
  %804 = add <8 x i32> %803, %802
  %805 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %804, i32 %2) #8
  store <8 x i32> %805, <8 x i32>* %286, align 32
  store <8 x i32> %800, <8 x i32>* %291, align 32
  %806 = load <8 x i32>, <8 x i32>* %442, align 32
  %807 = add <8 x i32> %806, %697
  %808 = sub <8 x i32> %697, %806
  %809 = icmp sgt <8 x i32> %807, %356
  %810 = select <8 x i1> %809, <8 x i32> %807, <8 x i32> %356
  %811 = icmp slt <8 x i32> %810, %359
  %812 = select <8 x i1> %811, <8 x i32> %810, <8 x i32> %359
  %813 = icmp sgt <8 x i32> %808, %356
  %814 = select <8 x i1> %813, <8 x i32> %808, <8 x i32> %356
  %815 = icmp slt <8 x i32> %814, %359
  %816 = select <8 x i1> %815, <8 x i32> %814, <8 x i32> %359
  store <8 x i32> %812, <8 x i32>* %543, align 32
  store <8 x i32> %816, <8 x i32>* %442, align 32
  %817 = load <8 x i32>, <8 x i32>* %549, align 32
  %818 = add <8 x i32> %817, %721
  %819 = sub <8 x i32> %817, %721
  %820 = icmp sgt <8 x i32> %818, %356
  %821 = select <8 x i1> %820, <8 x i32> %818, <8 x i32> %356
  %822 = icmp slt <8 x i32> %821, %359
  %823 = select <8 x i1> %822, <8 x i32> %821, <8 x i32> %359
  %824 = icmp sgt <8 x i32> %819, %356
  %825 = select <8 x i1> %824, <8 x i32> %819, <8 x i32> %356
  %826 = icmp slt <8 x i32> %825, %359
  %827 = select <8 x i1> %826, <8 x i32> %825, <8 x i32> %359
  store <8 x i32> %823, <8 x i32>* %549, align 32
  store <8 x i32> %827, <8 x i32>* %446, align 32
  %828 = load <8 x i32>, <8 x i32>* %550, align 32
  %829 = add <8 x i32> %828, %718
  %830 = sub <8 x i32> %828, %718
  %831 = icmp sgt <8 x i32> %829, %356
  %832 = select <8 x i1> %831, <8 x i32> %829, <8 x i32> %356
  %833 = icmp slt <8 x i32> %832, %359
  %834 = select <8 x i1> %833, <8 x i32> %832, <8 x i32> %359
  %835 = icmp sgt <8 x i32> %830, %356
  %836 = select <8 x i1> %835, <8 x i32> %830, <8 x i32> %356
  %837 = icmp slt <8 x i32> %836, %359
  %838 = select <8 x i1> %837, <8 x i32> %836, <8 x i32> %359
  store <8 x i32> %834, <8 x i32>* %550, align 32
  store <8 x i32> %838, <8 x i32>* %452, align 32
  %839 = load <8 x i32>, <8 x i32>* %556, align 32
  %840 = load <8 x i32>, <8 x i32>* %436, align 32
  %841 = add <8 x i32> %840, %839
  %842 = sub <8 x i32> %839, %840
  %843 = icmp sgt <8 x i32> %841, %356
  %844 = select <8 x i1> %843, <8 x i32> %841, <8 x i32> %356
  %845 = icmp slt <8 x i32> %844, %359
  %846 = select <8 x i1> %845, <8 x i32> %844, <8 x i32> %359
  %847 = icmp sgt <8 x i32> %842, %356
  %848 = select <8 x i1> %847, <8 x i32> %842, <8 x i32> %356
  %849 = icmp slt <8 x i32> %848, %359
  %850 = select <8 x i1> %849, <8 x i32> %848, <8 x i32> %359
  store <8 x i32> %846, <8 x i32>* %556, align 32
  store <8 x i32> %850, <8 x i32>* %436, align 32
  %851 = mul <8 x i32> %743, %148
  %852 = mul <8 x i32> %765, %145
  %853 = add <8 x i32> %852, %166
  %854 = add <8 x i32> %853, %851
  %855 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %854, i32 %2) #8
  %856 = mul <8 x i32> %743, %145
  %857 = add <8 x i32> %853, %856
  %858 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %857, i32 %2) #8
  store <8 x i32> %858, <8 x i32>* %340, align 32
  store <8 x i32> %855, <8 x i32>* %334, align 32
  %859 = mul <8 x i32> %733, %148
  %860 = mul <8 x i32> %755, %145
  %861 = add <8 x i32> %860, %166
  %862 = add <8 x i32> %861, %859
  %863 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %862, i32 %2) #8
  %864 = mul <8 x i32> %733, %145
  %865 = add <8 x i32> %861, %864
  %866 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %865, i32 %2) #8
  store <8 x i32> %866, <8 x i32>* %344, align 32
  store <8 x i32> %863, <8 x i32>* %350, align 32
  %867 = add <8 x i32> %633, %611
  %868 = sub <8 x i32> %611, %633
  %869 = icmp sgt <8 x i32> %867, %356
  %870 = select <8 x i1> %869, <8 x i32> %867, <8 x i32> %356
  %871 = icmp slt <8 x i32> %870, %359
  %872 = select <8 x i1> %871, <8 x i32> %870, <8 x i32> %359
  %873 = icmp sgt <8 x i32> %868, %356
  %874 = select <8 x i1> %873, <8 x i32> %868, <8 x i32> %356
  %875 = icmp slt <8 x i32> %874, %359
  %876 = select <8 x i1> %875, <8 x i32> %874, <8 x i32> %359
  store <8 x i32> %872, <8 x i32>* %241, align 32
  store <8 x i32> %876, <8 x i32>* %310, align 32
  %877 = add <8 x i32> %643, %621
  %878 = sub <8 x i32> %621, %643
  %879 = load <8 x i32>, <8 x i32>* %178, align 32
  %880 = icmp sgt <8 x i32> %877, %879
  %881 = select <8 x i1> %880, <8 x i32> %877, <8 x i32> %879
  %882 = load <8 x i32>, <8 x i32>* %183, align 32
  %883 = icmp slt <8 x i32> %881, %882
  %884 = select <8 x i1> %883, <8 x i32> %881, <8 x i32> %882
  %885 = icmp sgt <8 x i32> %878, %879
  %886 = select <8 x i1> %885, <8 x i32> %878, <8 x i32> %879
  %887 = icmp slt <8 x i32> %886, %882
  %888 = select <8 x i1> %887, <8 x i32> %886, <8 x i32> %882
  store <8 x i32> %884, <8 x i32>* %255, align 32
  store <8 x i32> %888, <8 x i32>* %295, align 32
  %889 = add <8 x i32> %800, %770
  %890 = sub <8 x i32> %770, %800
  %891 = icmp sgt <8 x i32> %889, %879
  %892 = select <8 x i1> %891, <8 x i32> %889, <8 x i32> %879
  %893 = icmp slt <8 x i32> %892, %882
  %894 = select <8 x i1> %893, <8 x i32> %892, <8 x i32> %882
  %895 = icmp sgt <8 x i32> %890, %879
  %896 = select <8 x i1> %895, <8 x i32> %890, <8 x i32> %879
  %897 = icmp slt <8 x i32> %896, %882
  %898 = select <8 x i1> %897, <8 x i32> %896, <8 x i32> %882
  store <8 x i32> %894, <8 x i32>* %259, align 32
  store <8 x i32> %898, <8 x i32>* %291, align 32
  %899 = add <8 x i32> %790, %780
  %900 = sub <8 x i32> %780, %790
  %901 = icmp sgt <8 x i32> %899, %879
  %902 = select <8 x i1> %901, <8 x i32> %899, <8 x i32> %879
  %903 = icmp slt <8 x i32> %902, %882
  %904 = select <8 x i1> %903, <8 x i32> %902, <8 x i32> %882
  %905 = icmp sgt <8 x i32> %900, %879
  %906 = select <8 x i1> %905, <8 x i32> %900, <8 x i32> %879
  %907 = icmp slt <8 x i32> %906, %882
  %908 = select <8 x i1> %907, <8 x i32> %906, <8 x i32> %882
  store <8 x i32> %904, <8 x i32>* %273, align 32
  store <8 x i32> %908, <8 x i32>* %277, align 32
  %909 = add <8 x i32> %677, %655
  %910 = sub <8 x i32> %677, %655
  %911 = icmp sgt <8 x i32> %909, %879
  %912 = select <8 x i1> %911, <8 x i32> %909, <8 x i32> %879
  %913 = icmp slt <8 x i32> %912, %882
  %914 = select <8 x i1> %913, <8 x i32> %912, <8 x i32> %882
  %915 = icmp sgt <8 x i32> %910, %879
  %916 = select <8 x i1> %915, <8 x i32> %910, <8 x i32> %879
  %917 = icmp slt <8 x i32> %916, %882
  %918 = select <8 x i1> %917, <8 x i32> %916, <8 x i32> %882
  store <8 x i32> %914, <8 x i32>* %246, align 32
  store <8 x i32> %918, <8 x i32>* %304, align 32
  %919 = add <8 x i32> %665, %687
  %920 = sub <8 x i32> %687, %665
  %921 = icmp sgt <8 x i32> %919, %879
  %922 = select <8 x i1> %921, <8 x i32> %919, <8 x i32> %879
  %923 = icmp slt <8 x i32> %922, %882
  %924 = select <8 x i1> %923, <8 x i32> %922, <8 x i32> %882
  %925 = icmp sgt <8 x i32> %920, %879
  %926 = select <8 x i1> %925, <8 x i32> %920, <8 x i32> %879
  %927 = icmp slt <8 x i32> %926, %882
  %928 = select <8 x i1> %927, <8 x i32> %926, <8 x i32> %882
  store <8 x i32> %924, <8 x i32>* %250, align 32
  store <8 x i32> %928, <8 x i32>* %300, align 32
  %929 = add <8 x i32> %805, %775
  %930 = sub <8 x i32> %775, %805
  %931 = icmp sgt <8 x i32> %929, %879
  %932 = select <8 x i1> %931, <8 x i32> %929, <8 x i32> %879
  %933 = icmp slt <8 x i32> %932, %882
  %934 = select <8 x i1> %933, <8 x i32> %932, <8 x i32> %882
  %935 = icmp sgt <8 x i32> %930, %879
  %936 = select <8 x i1> %935, <8 x i32> %930, <8 x i32> %879
  %937 = icmp slt <8 x i32> %936, %882
  %938 = select <8 x i1> %937, <8 x i32> %936, <8 x i32> %882
  store <8 x i32> %934, <8 x i32>* %264, align 32
  store <8 x i32> %938, <8 x i32>* %286, align 32
  %939 = add <8 x i32> %795, %785
  %940 = sub <8 x i32> %785, %795
  %941 = icmp sgt <8 x i32> %939, %879
  %942 = select <8 x i1> %941, <8 x i32> %939, <8 x i32> %879
  %943 = icmp slt <8 x i32> %942, %882
  %944 = select <8 x i1> %943, <8 x i32> %942, <8 x i32> %882
  %945 = icmp sgt <8 x i32> %940, %879
  %946 = select <8 x i1> %945, <8 x i32> %940, <8 x i32> %879
  %947 = icmp slt <8 x i32> %946, %882
  %948 = select <8 x i1> %947, <8 x i32> %946, <8 x i32> %882
  store <8 x i32> %944, <8 x i32>* %268, align 32
  store <8 x i32> %948, <8 x i32>* %282, align 32
  %949 = add <8 x i32> %812, %751
  %950 = sub <8 x i32> %812, %751
  %951 = icmp sgt <8 x i32> %949, %879
  %952 = select <8 x i1> %951, <8 x i32> %949, <8 x i32> %879
  %953 = icmp slt <8 x i32> %952, %882
  %954 = select <8 x i1> %953, <8 x i32> %952, <8 x i32> %882
  %955 = icmp sgt <8 x i32> %950, %879
  %956 = select <8 x i1> %955, <8 x i32> %950, <8 x i32> %879
  %957 = icmp slt <8 x i32> %956, %882
  %958 = select <8 x i1> %957, <8 x i32> %956, <8 x i32> %882
  store <8 x i32> %954, <8 x i32>* %543, align 32
  store <8 x i32> %958, <8 x i32>* %320, align 32
  %959 = add <8 x i32> %823, %761
  %960 = sub <8 x i32> %823, %761
  %961 = icmp sgt <8 x i32> %959, %879
  %962 = select <8 x i1> %961, <8 x i32> %959, <8 x i32> %879
  %963 = icmp slt <8 x i32> %962, %882
  %964 = select <8 x i1> %963, <8 x i32> %962, <8 x i32> %882
  %965 = icmp sgt <8 x i32> %960, %879
  %966 = select <8 x i1> %965, <8 x i32> %960, <8 x i32> %879
  %967 = icmp slt <8 x i32> %966, %882
  %968 = select <8 x i1> %967, <8 x i32> %966, <8 x i32> %882
  store <8 x i32> %964, <8 x i32>* %549, align 32
  store <8 x i32> %968, <8 x i32>* %324, align 32
  %969 = add <8 x i32> %858, %834
  %970 = sub <8 x i32> %834, %858
  %971 = icmp sgt <8 x i32> %969, %879
  %972 = select <8 x i1> %971, <8 x i32> %969, <8 x i32> %879
  %973 = icmp slt <8 x i32> %972, %882
  %974 = select <8 x i1> %973, <8 x i32> %972, <8 x i32> %882
  %975 = icmp sgt <8 x i32> %970, %879
  %976 = select <8 x i1> %975, <8 x i32> %970, <8 x i32> %879
  %977 = icmp slt <8 x i32> %976, %882
  %978 = select <8 x i1> %977, <8 x i32> %976, <8 x i32> %882
  store <8 x i32> %974, <8 x i32>* %550, align 32
  store <8 x i32> %978, <8 x i32>* %340, align 32
  %979 = add <8 x i32> %866, %846
  %980 = sub <8 x i32> %846, %866
  %981 = icmp sgt <8 x i32> %979, %879
  %982 = select <8 x i1> %981, <8 x i32> %979, <8 x i32> %879
  %983 = icmp slt <8 x i32> %982, %882
  %984 = select <8 x i1> %983, <8 x i32> %982, <8 x i32> %882
  %985 = icmp sgt <8 x i32> %980, %879
  %986 = select <8 x i1> %985, <8 x i32> %980, <8 x i32> %879
  %987 = icmp slt <8 x i32> %986, %882
  %988 = select <8 x i1> %987, <8 x i32> %986, <8 x i32> %882
  store <8 x i32> %984, <8 x i32>* %556, align 32
  store <8 x i32> %988, <8 x i32>* %344, align 32
  %989 = add <8 x i32> %863, %850
  %990 = sub <8 x i32> %850, %863
  %991 = icmp sgt <8 x i32> %989, %879
  %992 = select <8 x i1> %991, <8 x i32> %989, <8 x i32> %879
  %993 = icmp slt <8 x i32> %992, %882
  %994 = select <8 x i1> %993, <8 x i32> %992, <8 x i32> %882
  %995 = icmp sgt <8 x i32> %990, %879
  %996 = select <8 x i1> %995, <8 x i32> %990, <8 x i32> %879
  %997 = icmp slt <8 x i32> %996, %882
  %998 = select <8 x i1> %997, <8 x i32> %996, <8 x i32> %882
  store <8 x i32> %994, <8 x i32>* %436, align 32
  store <8 x i32> %998, <8 x i32>* %350, align 32
  %999 = add <8 x i32> %855, %838
  %1000 = sub <8 x i32> %838, %855
  %1001 = icmp sgt <8 x i32> %999, %879
  %1002 = select <8 x i1> %1001, <8 x i32> %999, <8 x i32> %879
  %1003 = icmp slt <8 x i32> %1002, %882
  %1004 = select <8 x i1> %1003, <8 x i32> %1002, <8 x i32> %882
  %1005 = icmp sgt <8 x i32> %1000, %879
  %1006 = select <8 x i1> %1005, <8 x i32> %1000, <8 x i32> %879
  %1007 = icmp slt <8 x i32> %1006, %882
  %1008 = select <8 x i1> %1007, <8 x i32> %1006, <8 x i32> %882
  store <8 x i32> %1004, <8 x i32>* %452, align 32
  store <8 x i32> %1008, <8 x i32>* %334, align 32
  %1009 = add <8 x i32> %827, %739
  %1010 = sub <8 x i32> %827, %739
  %1011 = icmp sgt <8 x i32> %1009, %879
  %1012 = select <8 x i1> %1011, <8 x i32> %1009, <8 x i32> %879
  %1013 = icmp slt <8 x i32> %1012, %882
  %1014 = select <8 x i1> %1013, <8 x i32> %1012, <8 x i32> %882
  %1015 = icmp sgt <8 x i32> %1010, %879
  %1016 = select <8 x i1> %1015, <8 x i32> %1010, <8 x i32> %879
  %1017 = icmp slt <8 x i32> %1016, %882
  %1018 = select <8 x i1> %1017, <8 x i32> %1016, <8 x i32> %882
  store <8 x i32> %1014, <8 x i32>* %446, align 32
  store <8 x i32> %1018, <8 x i32>* %330, align 32
  %1019 = add <8 x i32> %816, %729
  %1020 = sub <8 x i32> %816, %729
  %1021 = icmp sgt <8 x i32> %1019, %879
  %1022 = select <8 x i1> %1021, <8 x i32> %1019, <8 x i32> %879
  %1023 = icmp slt <8 x i32> %1022, %882
  %1024 = select <8 x i1> %1023, <8 x i32> %1022, <8 x i32> %882
  %1025 = icmp sgt <8 x i32> %1020, %879
  %1026 = select <8 x i1> %1025, <8 x i32> %1020, <8 x i32> %879
  %1027 = icmp slt <8 x i32> %1026, %882
  %1028 = select <8 x i1> %1027, <8 x i32> %1026, <8 x i32> %882
  store <8 x i32> %1024, <8 x i32>* %442, align 32
  store <8 x i32> %1028, <8 x i32>* %314, align 32
  %1029 = mul <8 x i32> %908, %148
  %1030 = mul <8 x i32> %948, %145
  %1031 = add <8 x i32> %1030, %166
  %1032 = add <8 x i32> %1031, %1029
  %1033 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1032, i32 %2) #8
  %1034 = mul <8 x i32> %908, %145
  %1035 = add <8 x i32> %1031, %1034
  %1036 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1035, i32 %2) #8
  store <8 x i32> %1036, <8 x i32>* %282, align 32
  store <8 x i32> %1033, <8 x i32>* %277, align 32
  %1037 = mul <8 x i32> %898, %148
  %1038 = mul <8 x i32> %938, %145
  %1039 = add <8 x i32> %1038, %166
  %1040 = add <8 x i32> %1039, %1037
  %1041 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1040, i32 %2) #8
  %1042 = mul <8 x i32> %898, %145
  %1043 = add <8 x i32> %1039, %1042
  %1044 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1043, i32 %2) #8
  store <8 x i32> %1044, <8 x i32>* %286, align 32
  store <8 x i32> %1041, <8 x i32>* %291, align 32
  %1045 = mul <8 x i32> %888, %148
  %1046 = mul <8 x i32> %928, %145
  %1047 = add <8 x i32> %1046, %166
  %1048 = add <8 x i32> %1047, %1045
  %1049 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1048, i32 %2) #8
  %1050 = mul <8 x i32> %888, %145
  %1051 = add <8 x i32> %1047, %1050
  %1052 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1051, i32 %2) #8
  store <8 x i32> %1052, <8 x i32>* %300, align 32
  store <8 x i32> %1049, <8 x i32>* %295, align 32
  %1053 = mul <8 x i32> %876, %148
  %1054 = mul <8 x i32> %918, %145
  %1055 = add <8 x i32> %1054, %166
  %1056 = add <8 x i32> %1055, %1053
  %1057 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1056, i32 %2) #8
  %1058 = mul <8 x i32> %876, %145
  %1059 = add <8 x i32> %1055, %1058
  %1060 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1059, i32 %2) #8
  store <8 x i32> %1060, <8 x i32>* %304, align 32
  store <8 x i32> %1057, <8 x i32>* %310, align 32
  call fastcc void @idct32_stage9_avx2(<4 x i64>* nonnull %188, <4 x i64>* %1, i32 %3, i32 %4, i32 %5, <4 x i64>* nonnull %7, <4 x i64>* nonnull %8)
  call void @llvm.lifetime.end.p0i8(i64 1024, i8* nonnull %184) #8
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %179) #8
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %172) #8
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @idct32_avx2(<4 x i64>* readonly, <4 x i64>*, i32, i32, i32, i32) #0 {
  %7 = add nsw i32 %2, -10
  %8 = sext i32 %7 to i64
  %9 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 62
  %10 = load i32, i32* %9, align 8
  %11 = insertelement <8 x i32> undef, i32 %10, i32 0
  %12 = shufflevector <8 x i32> %11, <8 x i32> undef, <8 x i32> zeroinitializer
  %13 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 30
  %14 = load i32, i32* %13, align 8
  %15 = insertelement <8 x i32> undef, i32 %14, i32 0
  %16 = shufflevector <8 x i32> %15, <8 x i32> undef, <8 x i32> zeroinitializer
  %17 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 46
  %18 = load i32, i32* %17, align 8
  %19 = insertelement <8 x i32> undef, i32 %18, i32 0
  %20 = shufflevector <8 x i32> %19, <8 x i32> undef, <8 x i32> zeroinitializer
  %21 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 14
  %22 = load i32, i32* %21, align 8
  %23 = insertelement <8 x i32> undef, i32 %22, i32 0
  %24 = shufflevector <8 x i32> %23, <8 x i32> undef, <8 x i32> zeroinitializer
  %25 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 54
  %26 = load i32, i32* %25, align 8
  %27 = insertelement <8 x i32> undef, i32 %26, i32 0
  %28 = shufflevector <8 x i32> %27, <8 x i32> undef, <8 x i32> zeroinitializer
  %29 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 22
  %30 = load i32, i32* %29, align 8
  %31 = insertelement <8 x i32> undef, i32 %30, i32 0
  %32 = shufflevector <8 x i32> %31, <8 x i32> undef, <8 x i32> zeroinitializer
  %33 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 38
  %34 = load i32, i32* %33, align 8
  %35 = insertelement <8 x i32> undef, i32 %34, i32 0
  %36 = shufflevector <8 x i32> %35, <8 x i32> undef, <8 x i32> zeroinitializer
  %37 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 6
  %38 = load i32, i32* %37, align 8
  %39 = insertelement <8 x i32> undef, i32 %38, i32 0
  %40 = shufflevector <8 x i32> %39, <8 x i32> undef, <8 x i32> zeroinitializer
  %41 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 58
  %42 = load i32, i32* %41, align 8
  %43 = insertelement <8 x i32> undef, i32 %42, i32 0
  %44 = shufflevector <8 x i32> %43, <8 x i32> undef, <8 x i32> zeroinitializer
  %45 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 26
  %46 = load i32, i32* %45, align 8
  %47 = insertelement <8 x i32> undef, i32 %46, i32 0
  %48 = shufflevector <8 x i32> %47, <8 x i32> undef, <8 x i32> zeroinitializer
  %49 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 42
  %50 = load i32, i32* %49, align 8
  %51 = insertelement <8 x i32> undef, i32 %50, i32 0
  %52 = shufflevector <8 x i32> %51, <8 x i32> undef, <8 x i32> zeroinitializer
  %53 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 10
  %54 = load i32, i32* %53, align 8
  %55 = insertelement <8 x i32> undef, i32 %54, i32 0
  %56 = shufflevector <8 x i32> %55, <8 x i32> undef, <8 x i32> zeroinitializer
  %57 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 50
  %58 = load i32, i32* %57, align 8
  %59 = insertelement <8 x i32> undef, i32 %58, i32 0
  %60 = shufflevector <8 x i32> %59, <8 x i32> undef, <8 x i32> zeroinitializer
  %61 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 18
  %62 = load i32, i32* %61, align 8
  %63 = insertelement <8 x i32> undef, i32 %62, i32 0
  %64 = shufflevector <8 x i32> %63, <8 x i32> undef, <8 x i32> zeroinitializer
  %65 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 34
  %66 = load i32, i32* %65, align 8
  %67 = insertelement <8 x i32> undef, i32 %66, i32 0
  %68 = shufflevector <8 x i32> %67, <8 x i32> undef, <8 x i32> zeroinitializer
  %69 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 2
  %70 = load i32, i32* %69, align 8
  %71 = insertelement <8 x i32> undef, i32 %70, i32 0
  %72 = shufflevector <8 x i32> %71, <8 x i32> undef, <8 x i32> zeroinitializer
  %73 = sub nsw i32 0, %42
  %74 = insertelement <8 x i32> undef, i32 %73, i32 0
  %75 = shufflevector <8 x i32> %74, <8 x i32> undef, <8 x i32> zeroinitializer
  %76 = sub nsw i32 0, %46
  %77 = insertelement <8 x i32> undef, i32 %76, i32 0
  %78 = shufflevector <8 x i32> %77, <8 x i32> undef, <8 x i32> zeroinitializer
  %79 = sub nsw i32 0, %50
  %80 = insertelement <8 x i32> undef, i32 %79, i32 0
  %81 = shufflevector <8 x i32> %80, <8 x i32> undef, <8 x i32> zeroinitializer
  %82 = sub nsw i32 0, %54
  %83 = insertelement <8 x i32> undef, i32 %82, i32 0
  %84 = shufflevector <8 x i32> %83, <8 x i32> undef, <8 x i32> zeroinitializer
  %85 = sub nsw i32 0, %58
  %86 = insertelement <8 x i32> undef, i32 %85, i32 0
  %87 = shufflevector <8 x i32> %86, <8 x i32> undef, <8 x i32> zeroinitializer
  %88 = sub nsw i32 0, %62
  %89 = insertelement <8 x i32> undef, i32 %88, i32 0
  %90 = shufflevector <8 x i32> %89, <8 x i32> undef, <8 x i32> zeroinitializer
  %91 = sub nsw i32 0, %66
  %92 = insertelement <8 x i32> undef, i32 %91, i32 0
  %93 = shufflevector <8 x i32> %92, <8 x i32> undef, <8 x i32> zeroinitializer
  %94 = sub nsw i32 0, %70
  %95 = insertelement <8 x i32> undef, i32 %94, i32 0
  %96 = shufflevector <8 x i32> %95, <8 x i32> undef, <8 x i32> zeroinitializer
  %97 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 60
  %98 = load i32, i32* %97, align 16
  %99 = insertelement <8 x i32> undef, i32 %98, i32 0
  %100 = shufflevector <8 x i32> %99, <8 x i32> undef, <8 x i32> zeroinitializer
  %101 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 28
  %102 = load i32, i32* %101, align 16
  %103 = insertelement <8 x i32> undef, i32 %102, i32 0
  %104 = shufflevector <8 x i32> %103, <8 x i32> undef, <8 x i32> zeroinitializer
  %105 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 44
  %106 = load i32, i32* %105, align 16
  %107 = insertelement <8 x i32> undef, i32 %106, i32 0
  %108 = shufflevector <8 x i32> %107, <8 x i32> undef, <8 x i32> zeroinitializer
  %109 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 12
  %110 = load i32, i32* %109, align 16
  %111 = insertelement <8 x i32> undef, i32 %110, i32 0
  %112 = shufflevector <8 x i32> %111, <8 x i32> undef, <8 x i32> zeroinitializer
  %113 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 52
  %114 = load i32, i32* %113, align 16
  %115 = insertelement <8 x i32> undef, i32 %114, i32 0
  %116 = shufflevector <8 x i32> %115, <8 x i32> undef, <8 x i32> zeroinitializer
  %117 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 20
  %118 = load i32, i32* %117, align 16
  %119 = insertelement <8 x i32> undef, i32 %118, i32 0
  %120 = shufflevector <8 x i32> %119, <8 x i32> undef, <8 x i32> zeroinitializer
  %121 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 36
  %122 = load i32, i32* %121, align 16
  %123 = insertelement <8 x i32> undef, i32 %122, i32 0
  %124 = shufflevector <8 x i32> %123, <8 x i32> undef, <8 x i32> zeroinitializer
  %125 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 4
  %126 = load i32, i32* %125, align 16
  %127 = insertelement <8 x i32> undef, i32 %126, i32 0
  %128 = shufflevector <8 x i32> %127, <8 x i32> undef, <8 x i32> zeroinitializer
  %129 = sub nsw i32 0, %114
  %130 = insertelement <8 x i32> undef, i32 %129, i32 0
  %131 = shufflevector <8 x i32> %130, <8 x i32> undef, <8 x i32> zeroinitializer
  %132 = sub nsw i32 0, %118
  %133 = insertelement <8 x i32> undef, i32 %132, i32 0
  %134 = shufflevector <8 x i32> %133, <8 x i32> undef, <8 x i32> zeroinitializer
  %135 = sub nsw i32 0, %122
  %136 = insertelement <8 x i32> undef, i32 %135, i32 0
  %137 = shufflevector <8 x i32> %136, <8 x i32> undef, <8 x i32> zeroinitializer
  %138 = sub nsw i32 0, %126
  %139 = insertelement <8 x i32> undef, i32 %138, i32 0
  %140 = shufflevector <8 x i32> %139, <8 x i32> undef, <8 x i32> zeroinitializer
  %141 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 56
  %142 = load i32, i32* %141, align 16
  %143 = insertelement <8 x i32> undef, i32 %142, i32 0
  %144 = shufflevector <8 x i32> %143, <8 x i32> undef, <8 x i32> zeroinitializer
  %145 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 24
  %146 = load i32, i32* %145, align 16
  %147 = insertelement <8 x i32> undef, i32 %146, i32 0
  %148 = shufflevector <8 x i32> %147, <8 x i32> undef, <8 x i32> zeroinitializer
  %149 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 40
  %150 = load i32, i32* %149, align 16
  %151 = insertelement <8 x i32> undef, i32 %150, i32 0
  %152 = shufflevector <8 x i32> %151, <8 x i32> undef, <8 x i32> zeroinitializer
  %153 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 8
  %154 = load i32, i32* %153, align 16
  %155 = insertelement <8 x i32> undef, i32 %154, i32 0
  %156 = shufflevector <8 x i32> %155, <8 x i32> undef, <8 x i32> zeroinitializer
  %157 = sub nsw i32 0, %150
  %158 = insertelement <8 x i32> undef, i32 %157, i32 0
  %159 = shufflevector <8 x i32> %158, <8 x i32> undef, <8 x i32> zeroinitializer
  %160 = sub nsw i32 0, %154
  %161 = insertelement <8 x i32> undef, i32 %160, i32 0
  %162 = shufflevector <8 x i32> %161, <8 x i32> undef, <8 x i32> zeroinitializer
  %163 = sub nsw i32 0, %142
  %164 = insertelement <8 x i32> undef, i32 %163, i32 0
  %165 = shufflevector <8 x i32> %164, <8 x i32> undef, <8 x i32> zeroinitializer
  %166 = sub nsw i32 0, %146
  %167 = insertelement <8 x i32> undef, i32 %166, i32 0
  %168 = shufflevector <8 x i32> %167, <8 x i32> undef, <8 x i32> zeroinitializer
  %169 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 32
  %170 = load i32, i32* %169, align 16
  %171 = insertelement <8 x i32> undef, i32 %170, i32 0
  %172 = shufflevector <8 x i32> %171, <8 x i32> undef, <8 x i32> zeroinitializer
  %173 = sub nsw i32 0, %170
  %174 = insertelement <8 x i32> undef, i32 %173, i32 0
  %175 = shufflevector <8 x i32> %174, <8 x i32> undef, <8 x i32> zeroinitializer
  %176 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 48
  %177 = load i32, i32* %176, align 16
  %178 = insertelement <8 x i32> undef, i32 %177, i32 0
  %179 = shufflevector <8 x i32> %178, <8 x i32> undef, <8 x i32> zeroinitializer
  %180 = sub nsw i32 0, %177
  %181 = insertelement <8 x i32> undef, i32 %180, i32 0
  %182 = shufflevector <8 x i32> %181, <8 x i32> undef, <8 x i32> zeroinitializer
  %183 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 16
  %184 = load i32, i32* %183, align 16
  %185 = insertelement <8 x i32> undef, i32 %184, i32 0
  %186 = shufflevector <8 x i32> %185, <8 x i32> undef, <8 x i32> zeroinitializer
  %187 = sub nsw i32 0, %184
  %188 = insertelement <8 x i32> undef, i32 %187, i32 0
  %189 = shufflevector <8 x i32> %188, <8 x i32> undef, <8 x i32> zeroinitializer
  %190 = add nsw i32 %2, -1
  %191 = shl i32 1, %190
  %192 = insertelement <8 x i32> undef, i32 %191, i32 0
  %193 = shufflevector <8 x i32> %192, <8 x i32> undef, <8 x i32> zeroinitializer
  %194 = icmp ne i32 %3, 0
  %195 = select i1 %194, i32 6, i32 8
  %196 = add nsw i32 %195, %4
  %197 = icmp slt i32 %196, 16
  %198 = add i32 %196, -1
  %199 = shl i32 1, %198
  %200 = select i1 %197, i32 32768, i32 %199
  %201 = sub nsw i32 0, %200
  %202 = insertelement <8 x i32> undef, i32 %201, i32 0
  %203 = shufflevector <8 x i32> %202, <8 x i32> undef, <8 x i32> zeroinitializer
  %204 = add nsw i32 %200, -1
  %205 = insertelement <8 x i32> undef, i32 %204, i32 0
  %206 = shufflevector <8 x i32> %205, <8 x i32> undef, <8 x i32> zeroinitializer
  %207 = bitcast <4 x i64>* %0 to <8 x i32>*
  %208 = load <8 x i32>, <8 x i32>* %207, align 32
  %209 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 16
  %210 = bitcast <4 x i64>* %209 to <8 x i32>*
  %211 = load <8 x i32>, <8 x i32>* %210, align 32
  %212 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 8
  %213 = bitcast <4 x i64>* %212 to <8 x i32>*
  %214 = load <8 x i32>, <8 x i32>* %213, align 32
  %215 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 24
  %216 = bitcast <4 x i64>* %215 to <8 x i32>*
  %217 = load <8 x i32>, <8 x i32>* %216, align 32
  %218 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 4
  %219 = bitcast <4 x i64>* %218 to <8 x i32>*
  %220 = load <8 x i32>, <8 x i32>* %219, align 32
  %221 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 20
  %222 = bitcast <4 x i64>* %221 to <8 x i32>*
  %223 = load <8 x i32>, <8 x i32>* %222, align 32
  %224 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 12
  %225 = bitcast <4 x i64>* %224 to <8 x i32>*
  %226 = load <8 x i32>, <8 x i32>* %225, align 32
  %227 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 28
  %228 = bitcast <4 x i64>* %227 to <8 x i32>*
  %229 = load <8 x i32>, <8 x i32>* %228, align 32
  %230 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 2
  %231 = bitcast <4 x i64>* %230 to <8 x i32>*
  %232 = load <8 x i32>, <8 x i32>* %231, align 32
  %233 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 18
  %234 = bitcast <4 x i64>* %233 to <8 x i32>*
  %235 = load <8 x i32>, <8 x i32>* %234, align 32
  %236 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 10
  %237 = bitcast <4 x i64>* %236 to <8 x i32>*
  %238 = load <8 x i32>, <8 x i32>* %237, align 32
  %239 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 26
  %240 = bitcast <4 x i64>* %239 to <8 x i32>*
  %241 = load <8 x i32>, <8 x i32>* %240, align 32
  %242 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 6
  %243 = bitcast <4 x i64>* %242 to <8 x i32>*
  %244 = load <8 x i32>, <8 x i32>* %243, align 32
  %245 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 22
  %246 = bitcast <4 x i64>* %245 to <8 x i32>*
  %247 = load <8 x i32>, <8 x i32>* %246, align 32
  %248 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 14
  %249 = bitcast <4 x i64>* %248 to <8 x i32>*
  %250 = load <8 x i32>, <8 x i32>* %249, align 32
  %251 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 30
  %252 = bitcast <4 x i64>* %251 to <8 x i32>*
  %253 = load <8 x i32>, <8 x i32>* %252, align 32
  %254 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 1
  %255 = bitcast <4 x i64>* %254 to <8 x i32>*
  %256 = load <8 x i32>, <8 x i32>* %255, align 32
  %257 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 17
  %258 = bitcast <4 x i64>* %257 to <8 x i32>*
  %259 = load <8 x i32>, <8 x i32>* %258, align 32
  %260 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 9
  %261 = bitcast <4 x i64>* %260 to <8 x i32>*
  %262 = load <8 x i32>, <8 x i32>* %261, align 32
  %263 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 25
  %264 = bitcast <4 x i64>* %263 to <8 x i32>*
  %265 = load <8 x i32>, <8 x i32>* %264, align 32
  %266 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 5
  %267 = bitcast <4 x i64>* %266 to <8 x i32>*
  %268 = load <8 x i32>, <8 x i32>* %267, align 32
  %269 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 21
  %270 = bitcast <4 x i64>* %269 to <8 x i32>*
  %271 = load <8 x i32>, <8 x i32>* %270, align 32
  %272 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 13
  %273 = bitcast <4 x i64>* %272 to <8 x i32>*
  %274 = load <8 x i32>, <8 x i32>* %273, align 32
  %275 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 29
  %276 = bitcast <4 x i64>* %275 to <8 x i32>*
  %277 = load <8 x i32>, <8 x i32>* %276, align 32
  %278 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 3
  %279 = bitcast <4 x i64>* %278 to <8 x i32>*
  %280 = load <8 x i32>, <8 x i32>* %279, align 32
  %281 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 19
  %282 = bitcast <4 x i64>* %281 to <8 x i32>*
  %283 = load <8 x i32>, <8 x i32>* %282, align 32
  %284 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 11
  %285 = bitcast <4 x i64>* %284 to <8 x i32>*
  %286 = load <8 x i32>, <8 x i32>* %285, align 32
  %287 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 27
  %288 = bitcast <4 x i64>* %287 to <8 x i32>*
  %289 = load <8 x i32>, <8 x i32>* %288, align 32
  %290 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 7
  %291 = bitcast <4 x i64>* %290 to <8 x i32>*
  %292 = load <8 x i32>, <8 x i32>* %291, align 32
  %293 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 23
  %294 = bitcast <4 x i64>* %293 to <8 x i32>*
  %295 = load <8 x i32>, <8 x i32>* %294, align 32
  %296 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 15
  %297 = bitcast <4 x i64>* %296 to <8 x i32>*
  %298 = load <8 x i32>, <8 x i32>* %297, align 32
  %299 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 31
  %300 = bitcast <4 x i64>* %299 to <8 x i32>*
  %301 = load <8 x i32>, <8 x i32>* %300, align 32
  %302 = mul <8 x i32> %256, %12
  %303 = mul <8 x i32> %301, %96
  %304 = add <8 x i32> %302, %193
  %305 = add <8 x i32> %304, %303
  %306 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %305, i32 %2) #8
  %307 = mul <8 x i32> %259, %16
  %308 = mul <8 x i32> %298, %93
  %309 = add <8 x i32> %307, %193
  %310 = add <8 x i32> %309, %308
  %311 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %310, i32 %2) #8
  %312 = mul <8 x i32> %262, %20
  %313 = mul <8 x i32> %295, %90
  %314 = add <8 x i32> %312, %193
  %315 = add <8 x i32> %314, %313
  %316 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %315, i32 %2) #8
  %317 = mul <8 x i32> %265, %24
  %318 = mul <8 x i32> %292, %87
  %319 = add <8 x i32> %317, %193
  %320 = add <8 x i32> %319, %318
  %321 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %320, i32 %2) #8
  %322 = mul <8 x i32> %268, %28
  %323 = mul <8 x i32> %289, %84
  %324 = add <8 x i32> %322, %193
  %325 = add <8 x i32> %324, %323
  %326 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %325, i32 %2) #8
  %327 = mul <8 x i32> %271, %32
  %328 = mul <8 x i32> %286, %81
  %329 = add <8 x i32> %327, %193
  %330 = add <8 x i32> %329, %328
  %331 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %330, i32 %2) #8
  %332 = mul <8 x i32> %274, %36
  %333 = mul <8 x i32> %283, %78
  %334 = add <8 x i32> %332, %193
  %335 = add <8 x i32> %334, %333
  %336 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %335, i32 %2) #8
  %337 = mul <8 x i32> %277, %40
  %338 = mul <8 x i32> %280, %75
  %339 = add <8 x i32> %337, %193
  %340 = add <8 x i32> %339, %338
  %341 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %340, i32 %2) #8
  %342 = mul <8 x i32> %277, %44
  %343 = mul <8 x i32> %280, %40
  %344 = add <8 x i32> %342, %193
  %345 = add <8 x i32> %344, %343
  %346 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %345, i32 %2) #8
  %347 = mul <8 x i32> %274, %48
  %348 = mul <8 x i32> %283, %36
  %349 = add <8 x i32> %347, %193
  %350 = add <8 x i32> %349, %348
  %351 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %350, i32 %2) #8
  %352 = mul <8 x i32> %271, %52
  %353 = mul <8 x i32> %286, %32
  %354 = add <8 x i32> %352, %193
  %355 = add <8 x i32> %354, %353
  %356 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %355, i32 %2) #8
  %357 = mul <8 x i32> %268, %56
  %358 = mul <8 x i32> %289, %28
  %359 = add <8 x i32> %357, %193
  %360 = add <8 x i32> %359, %358
  %361 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %360, i32 %2) #8
  %362 = mul <8 x i32> %265, %60
  %363 = mul <8 x i32> %292, %24
  %364 = add <8 x i32> %362, %193
  %365 = add <8 x i32> %364, %363
  %366 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %365, i32 %2) #8
  %367 = mul <8 x i32> %262, %64
  %368 = mul <8 x i32> %295, %20
  %369 = add <8 x i32> %367, %193
  %370 = add <8 x i32> %369, %368
  %371 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %370, i32 %2) #8
  %372 = mul <8 x i32> %259, %68
  %373 = mul <8 x i32> %298, %16
  %374 = add <8 x i32> %372, %193
  %375 = add <8 x i32> %374, %373
  %376 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %375, i32 %2) #8
  %377 = mul <8 x i32> %256, %72
  %378 = mul <8 x i32> %301, %12
  %379 = add <8 x i32> %377, %193
  %380 = add <8 x i32> %379, %378
  %381 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %380, i32 %2) #8
  %382 = mul <8 x i32> %232, %100
  %383 = mul <8 x i32> %253, %140
  %384 = add <8 x i32> %382, %193
  %385 = add <8 x i32> %384, %383
  %386 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %385, i32 %2) #8
  %387 = mul <8 x i32> %235, %104
  %388 = mul <8 x i32> %250, %137
  %389 = add <8 x i32> %387, %193
  %390 = add <8 x i32> %389, %388
  %391 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %390, i32 %2) #8
  %392 = mul <8 x i32> %238, %108
  %393 = mul <8 x i32> %247, %134
  %394 = add <8 x i32> %392, %193
  %395 = add <8 x i32> %394, %393
  %396 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %395, i32 %2) #8
  %397 = mul <8 x i32> %241, %112
  %398 = mul <8 x i32> %244, %131
  %399 = add <8 x i32> %397, %193
  %400 = add <8 x i32> %399, %398
  %401 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %400, i32 %2) #8
  %402 = mul <8 x i32> %241, %116
  %403 = mul <8 x i32> %244, %112
  %404 = add <8 x i32> %402, %193
  %405 = add <8 x i32> %404, %403
  %406 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %405, i32 %2) #8
  %407 = mul <8 x i32> %238, %120
  %408 = mul <8 x i32> %247, %108
  %409 = add <8 x i32> %407, %193
  %410 = add <8 x i32> %409, %408
  %411 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %410, i32 %2) #8
  %412 = mul <8 x i32> %235, %124
  %413 = mul <8 x i32> %250, %104
  %414 = add <8 x i32> %412, %193
  %415 = add <8 x i32> %414, %413
  %416 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %415, i32 %2) #8
  %417 = mul <8 x i32> %232, %128
  %418 = mul <8 x i32> %253, %100
  %419 = add <8 x i32> %417, %193
  %420 = add <8 x i32> %419, %418
  %421 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %420, i32 %2) #8
  %422 = add <8 x i32> %311, %306
  %423 = sub <8 x i32> %306, %311
  %424 = icmp sgt <8 x i32> %422, %203
  %425 = select <8 x i1> %424, <8 x i32> %422, <8 x i32> %203
  %426 = icmp slt <8 x i32> %425, %206
  %427 = select <8 x i1> %426, <8 x i32> %425, <8 x i32> %206
  %428 = icmp sgt <8 x i32> %423, %203
  %429 = select <8 x i1> %428, <8 x i32> %423, <8 x i32> %203
  %430 = icmp slt <8 x i32> %429, %206
  %431 = select <8 x i1> %430, <8 x i32> %429, <8 x i32> %206
  %432 = add <8 x i32> %321, %316
  %433 = sub <8 x i32> %321, %316
  %434 = icmp sgt <8 x i32> %432, %203
  %435 = select <8 x i1> %434, <8 x i32> %432, <8 x i32> %203
  %436 = icmp slt <8 x i32> %435, %206
  %437 = select <8 x i1> %436, <8 x i32> %435, <8 x i32> %206
  %438 = icmp sgt <8 x i32> %433, %203
  %439 = select <8 x i1> %438, <8 x i32> %433, <8 x i32> %203
  %440 = icmp slt <8 x i32> %439, %206
  %441 = select <8 x i1> %440, <8 x i32> %439, <8 x i32> %206
  %442 = add <8 x i32> %331, %326
  %443 = sub <8 x i32> %326, %331
  %444 = icmp sgt <8 x i32> %442, %203
  %445 = select <8 x i1> %444, <8 x i32> %442, <8 x i32> %203
  %446 = icmp slt <8 x i32> %445, %206
  %447 = select <8 x i1> %446, <8 x i32> %445, <8 x i32> %206
  %448 = icmp sgt <8 x i32> %443, %203
  %449 = select <8 x i1> %448, <8 x i32> %443, <8 x i32> %203
  %450 = icmp slt <8 x i32> %449, %206
  %451 = select <8 x i1> %450, <8 x i32> %449, <8 x i32> %206
  %452 = add <8 x i32> %341, %336
  %453 = sub <8 x i32> %341, %336
  %454 = icmp sgt <8 x i32> %452, %203
  %455 = select <8 x i1> %454, <8 x i32> %452, <8 x i32> %203
  %456 = icmp slt <8 x i32> %455, %206
  %457 = select <8 x i1> %456, <8 x i32> %455, <8 x i32> %206
  %458 = icmp sgt <8 x i32> %453, %203
  %459 = select <8 x i1> %458, <8 x i32> %453, <8 x i32> %203
  %460 = icmp slt <8 x i32> %459, %206
  %461 = select <8 x i1> %460, <8 x i32> %459, <8 x i32> %206
  %462 = add <8 x i32> %351, %346
  %463 = sub <8 x i32> %346, %351
  %464 = icmp sgt <8 x i32> %462, %203
  %465 = select <8 x i1> %464, <8 x i32> %462, <8 x i32> %203
  %466 = icmp slt <8 x i32> %465, %206
  %467 = select <8 x i1> %466, <8 x i32> %465, <8 x i32> %206
  %468 = icmp sgt <8 x i32> %463, %203
  %469 = select <8 x i1> %468, <8 x i32> %463, <8 x i32> %203
  %470 = icmp slt <8 x i32> %469, %206
  %471 = select <8 x i1> %470, <8 x i32> %469, <8 x i32> %206
  %472 = add <8 x i32> %361, %356
  %473 = sub <8 x i32> %361, %356
  %474 = icmp sgt <8 x i32> %472, %203
  %475 = select <8 x i1> %474, <8 x i32> %472, <8 x i32> %203
  %476 = icmp slt <8 x i32> %475, %206
  %477 = select <8 x i1> %476, <8 x i32> %475, <8 x i32> %206
  %478 = icmp sgt <8 x i32> %473, %203
  %479 = select <8 x i1> %478, <8 x i32> %473, <8 x i32> %203
  %480 = icmp slt <8 x i32> %479, %206
  %481 = select <8 x i1> %480, <8 x i32> %479, <8 x i32> %206
  %482 = add <8 x i32> %371, %366
  %483 = sub <8 x i32> %366, %371
  %484 = icmp sgt <8 x i32> %482, %203
  %485 = select <8 x i1> %484, <8 x i32> %482, <8 x i32> %203
  %486 = icmp slt <8 x i32> %485, %206
  %487 = select <8 x i1> %486, <8 x i32> %485, <8 x i32> %206
  %488 = icmp sgt <8 x i32> %483, %203
  %489 = select <8 x i1> %488, <8 x i32> %483, <8 x i32> %203
  %490 = icmp slt <8 x i32> %489, %206
  %491 = select <8 x i1> %490, <8 x i32> %489, <8 x i32> %206
  %492 = add <8 x i32> %381, %376
  %493 = sub <8 x i32> %381, %376
  %494 = icmp sgt <8 x i32> %492, %203
  %495 = select <8 x i1> %494, <8 x i32> %492, <8 x i32> %203
  %496 = icmp slt <8 x i32> %495, %206
  %497 = select <8 x i1> %496, <8 x i32> %495, <8 x i32> %206
  %498 = icmp sgt <8 x i32> %493, %203
  %499 = select <8 x i1> %498, <8 x i32> %493, <8 x i32> %203
  %500 = icmp slt <8 x i32> %499, %206
  %501 = select <8 x i1> %500, <8 x i32> %499, <8 x i32> %206
  %502 = mul <8 x i32> %220, %144
  %503 = mul <8 x i32> %229, %162
  %504 = add <8 x i32> %502, %193
  %505 = add <8 x i32> %504, %503
  %506 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %505, i32 %2) #8
  %507 = mul <8 x i32> %223, %148
  %508 = mul <8 x i32> %226, %159
  %509 = add <8 x i32> %507, %193
  %510 = add <8 x i32> %509, %508
  %511 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %510, i32 %2) #8
  %512 = mul <8 x i32> %223, %152
  %513 = mul <8 x i32> %226, %148
  %514 = add <8 x i32> %512, %193
  %515 = add <8 x i32> %514, %513
  %516 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %515, i32 %2) #8
  %517 = mul <8 x i32> %220, %156
  %518 = mul <8 x i32> %229, %144
  %519 = add <8 x i32> %517, %193
  %520 = add <8 x i32> %519, %518
  %521 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %520, i32 %2) #8
  %522 = add <8 x i32> %391, %386
  %523 = sub <8 x i32> %386, %391
  %524 = icmp sgt <8 x i32> %522, %203
  %525 = select <8 x i1> %524, <8 x i32> %522, <8 x i32> %203
  %526 = icmp slt <8 x i32> %525, %206
  %527 = select <8 x i1> %526, <8 x i32> %525, <8 x i32> %206
  %528 = icmp sgt <8 x i32> %523, %203
  %529 = select <8 x i1> %528, <8 x i32> %523, <8 x i32> %203
  %530 = icmp slt <8 x i32> %529, %206
  %531 = select <8 x i1> %530, <8 x i32> %529, <8 x i32> %206
  %532 = add <8 x i32> %401, %396
  %533 = sub <8 x i32> %401, %396
  %534 = icmp sgt <8 x i32> %532, %203
  %535 = select <8 x i1> %534, <8 x i32> %532, <8 x i32> %203
  %536 = icmp slt <8 x i32> %535, %206
  %537 = select <8 x i1> %536, <8 x i32> %535, <8 x i32> %206
  %538 = icmp sgt <8 x i32> %533, %203
  %539 = select <8 x i1> %538, <8 x i32> %533, <8 x i32> %203
  %540 = icmp slt <8 x i32> %539, %206
  %541 = select <8 x i1> %540, <8 x i32> %539, <8 x i32> %206
  %542 = add <8 x i32> %411, %406
  %543 = sub <8 x i32> %406, %411
  %544 = icmp sgt <8 x i32> %542, %203
  %545 = select <8 x i1> %544, <8 x i32> %542, <8 x i32> %203
  %546 = icmp slt <8 x i32> %545, %206
  %547 = select <8 x i1> %546, <8 x i32> %545, <8 x i32> %206
  %548 = icmp sgt <8 x i32> %543, %203
  %549 = select <8 x i1> %548, <8 x i32> %543, <8 x i32> %203
  %550 = icmp slt <8 x i32> %549, %206
  %551 = select <8 x i1> %550, <8 x i32> %549, <8 x i32> %206
  %552 = add <8 x i32> %421, %416
  %553 = sub <8 x i32> %421, %416
  %554 = icmp sgt <8 x i32> %552, %203
  %555 = select <8 x i1> %554, <8 x i32> %552, <8 x i32> %203
  %556 = icmp slt <8 x i32> %555, %206
  %557 = select <8 x i1> %556, <8 x i32> %555, <8 x i32> %206
  %558 = icmp sgt <8 x i32> %553, %203
  %559 = select <8 x i1> %558, <8 x i32> %553, <8 x i32> %203
  %560 = icmp slt <8 x i32> %559, %206
  %561 = select <8 x i1> %560, <8 x i32> %559, <8 x i32> %206
  %562 = mul <8 x i32> %431, %162
  %563 = mul <8 x i32> %501, %144
  %564 = add <8 x i32> %562, %193
  %565 = add <8 x i32> %564, %563
  %566 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %565, i32 %2) #8
  %567 = mul <8 x i32> %441, %165
  %568 = mul <8 x i32> %491, %162
  %569 = add <8 x i32> %567, %193
  %570 = add <8 x i32> %569, %568
  %571 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %570, i32 %2) #8
  %572 = mul <8 x i32> %451, %159
  %573 = mul <8 x i32> %481, %148
  %574 = add <8 x i32> %572, %193
  %575 = add <8 x i32> %574, %573
  %576 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %575, i32 %2) #8
  %577 = mul <8 x i32> %461, %168
  %578 = mul <8 x i32> %471, %159
  %579 = add <8 x i32> %577, %193
  %580 = add <8 x i32> %579, %578
  %581 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %580, i32 %2) #8
  %582 = mul <8 x i32> %461, %159
  %583 = mul <8 x i32> %471, %148
  %584 = add <8 x i32> %582, %193
  %585 = add <8 x i32> %584, %583
  %586 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %585, i32 %2) #8
  %587 = mul <8 x i32> %451, %148
  %588 = mul <8 x i32> %481, %152
  %589 = add <8 x i32> %587, %193
  %590 = add <8 x i32> %589, %588
  %591 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %590, i32 %2) #8
  %592 = mul <8 x i32> %441, %162
  %593 = mul <8 x i32> %491, %144
  %594 = add <8 x i32> %592, %193
  %595 = add <8 x i32> %594, %593
  %596 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %595, i32 %2) #8
  %597 = mul <8 x i32> %431, %144
  %598 = mul <8 x i32> %501, %156
  %599 = add <8 x i32> %597, %193
  %600 = add <8 x i32> %599, %598
  %601 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %600, i32 %2) #8
  %602 = mul <8 x i32> %208, %172
  %603 = mul <8 x i32> %211, %172
  %604 = add <8 x i32> %602, %193
  %605 = add <8 x i32> %604, %603
  %606 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %605, i32 %2) #8
  %607 = mul <8 x i32> %211, %175
  %608 = add <8 x i32> %604, %607
  %609 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %608, i32 %2) #8
  %610 = mul <8 x i32> %214, %179
  %611 = mul <8 x i32> %217, %189
  %612 = add <8 x i32> %610, %193
  %613 = add <8 x i32> %612, %611
  %614 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %613, i32 %2) #8
  %615 = mul <8 x i32> %214, %186
  %616 = mul <8 x i32> %217, %179
  %617 = add <8 x i32> %615, %193
  %618 = add <8 x i32> %617, %616
  %619 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %618, i32 %2) #8
  %620 = add <8 x i32> %511, %506
  %621 = sub <8 x i32> %506, %511
  %622 = icmp sgt <8 x i32> %620, %203
  %623 = select <8 x i1> %622, <8 x i32> %620, <8 x i32> %203
  %624 = icmp slt <8 x i32> %623, %206
  %625 = select <8 x i1> %624, <8 x i32> %623, <8 x i32> %206
  %626 = icmp sgt <8 x i32> %621, %203
  %627 = select <8 x i1> %626, <8 x i32> %621, <8 x i32> %203
  %628 = icmp slt <8 x i32> %627, %206
  %629 = select <8 x i1> %628, <8 x i32> %627, <8 x i32> %206
  %630 = add <8 x i32> %521, %516
  %631 = sub <8 x i32> %521, %516
  %632 = icmp sgt <8 x i32> %630, %203
  %633 = select <8 x i1> %632, <8 x i32> %630, <8 x i32> %203
  %634 = icmp slt <8 x i32> %633, %206
  %635 = select <8 x i1> %634, <8 x i32> %633, <8 x i32> %206
  %636 = icmp sgt <8 x i32> %631, %203
  %637 = select <8 x i1> %636, <8 x i32> %631, <8 x i32> %203
  %638 = icmp slt <8 x i32> %637, %206
  %639 = select <8 x i1> %638, <8 x i32> %637, <8 x i32> %206
  %640 = mul <8 x i32> %531, %189
  %641 = mul <8 x i32> %561, %179
  %642 = add <8 x i32> %640, %193
  %643 = add <8 x i32> %642, %641
  %644 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %643, i32 %2) #8
  %645 = mul <8 x i32> %541, %182
  %646 = mul <8 x i32> %551, %189
  %647 = add <8 x i32> %645, %193
  %648 = add <8 x i32> %647, %646
  %649 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %648, i32 %2) #8
  %650 = mul <8 x i32> %541, %189
  %651 = mul <8 x i32> %551, %179
  %652 = add <8 x i32> %650, %193
  %653 = add <8 x i32> %652, %651
  %654 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %653, i32 %2) #8
  %655 = mul <8 x i32> %531, %179
  %656 = mul <8 x i32> %561, %186
  %657 = add <8 x i32> %655, %193
  %658 = add <8 x i32> %657, %656
  %659 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %658, i32 %2) #8
  %660 = add <8 x i32> %437, %427
  %661 = sub <8 x i32> %427, %437
  %662 = icmp sgt <8 x i32> %660, %203
  %663 = select <8 x i1> %662, <8 x i32> %660, <8 x i32> %203
  %664 = icmp slt <8 x i32> %663, %206
  %665 = select <8 x i1> %664, <8 x i32> %663, <8 x i32> %206
  %666 = icmp sgt <8 x i32> %661, %203
  %667 = select <8 x i1> %666, <8 x i32> %661, <8 x i32> %203
  %668 = icmp slt <8 x i32> %667, %206
  %669 = select <8 x i1> %668, <8 x i32> %667, <8 x i32> %206
  %670 = add <8 x i32> %571, %566
  %671 = sub <8 x i32> %566, %571
  %672 = icmp sgt <8 x i32> %670, %203
  %673 = select <8 x i1> %672, <8 x i32> %670, <8 x i32> %203
  %674 = icmp slt <8 x i32> %673, %206
  %675 = select <8 x i1> %674, <8 x i32> %673, <8 x i32> %206
  %676 = icmp sgt <8 x i32> %671, %203
  %677 = select <8 x i1> %676, <8 x i32> %671, <8 x i32> %203
  %678 = icmp slt <8 x i32> %677, %206
  %679 = select <8 x i1> %678, <8 x i32> %677, <8 x i32> %206
  %680 = add <8 x i32> %457, %447
  %681 = sub <8 x i32> %457, %447
  %682 = icmp sgt <8 x i32> %680, %203
  %683 = select <8 x i1> %682, <8 x i32> %680, <8 x i32> %203
  %684 = icmp slt <8 x i32> %683, %206
  %685 = select <8 x i1> %684, <8 x i32> %683, <8 x i32> %206
  %686 = icmp sgt <8 x i32> %681, %203
  %687 = select <8 x i1> %686, <8 x i32> %681, <8 x i32> %203
  %688 = icmp slt <8 x i32> %687, %206
  %689 = select <8 x i1> %688, <8 x i32> %687, <8 x i32> %206
  %690 = add <8 x i32> %581, %576
  %691 = sub <8 x i32> %581, %576
  %692 = icmp sgt <8 x i32> %690, %203
  %693 = select <8 x i1> %692, <8 x i32> %690, <8 x i32> %203
  %694 = icmp slt <8 x i32> %693, %206
  %695 = select <8 x i1> %694, <8 x i32> %693, <8 x i32> %206
  %696 = icmp sgt <8 x i32> %691, %203
  %697 = select <8 x i1> %696, <8 x i32> %691, <8 x i32> %203
  %698 = icmp slt <8 x i32> %697, %206
  %699 = select <8 x i1> %698, <8 x i32> %697, <8 x i32> %206
  %700 = add <8 x i32> %477, %467
  %701 = sub <8 x i32> %467, %477
  %702 = icmp sgt <8 x i32> %700, %203
  %703 = select <8 x i1> %702, <8 x i32> %700, <8 x i32> %203
  %704 = icmp slt <8 x i32> %703, %206
  %705 = select <8 x i1> %704, <8 x i32> %703, <8 x i32> %206
  %706 = icmp sgt <8 x i32> %701, %203
  %707 = select <8 x i1> %706, <8 x i32> %701, <8 x i32> %203
  %708 = icmp slt <8 x i32> %707, %206
  %709 = select <8 x i1> %708, <8 x i32> %707, <8 x i32> %206
  %710 = add <8 x i32> %591, %586
  %711 = sub <8 x i32> %586, %591
  %712 = icmp sgt <8 x i32> %710, %203
  %713 = select <8 x i1> %712, <8 x i32> %710, <8 x i32> %203
  %714 = icmp slt <8 x i32> %713, %206
  %715 = select <8 x i1> %714, <8 x i32> %713, <8 x i32> %206
  %716 = icmp sgt <8 x i32> %711, %203
  %717 = select <8 x i1> %716, <8 x i32> %711, <8 x i32> %203
  %718 = icmp slt <8 x i32> %717, %206
  %719 = select <8 x i1> %718, <8 x i32> %717, <8 x i32> %206
  %720 = add <8 x i32> %497, %487
  %721 = sub <8 x i32> %497, %487
  %722 = icmp sgt <8 x i32> %720, %203
  %723 = select <8 x i1> %722, <8 x i32> %720, <8 x i32> %203
  %724 = icmp slt <8 x i32> %723, %206
  %725 = select <8 x i1> %724, <8 x i32> %723, <8 x i32> %206
  %726 = icmp sgt <8 x i32> %721, %203
  %727 = select <8 x i1> %726, <8 x i32> %721, <8 x i32> %203
  %728 = icmp slt <8 x i32> %727, %206
  %729 = select <8 x i1> %728, <8 x i32> %727, <8 x i32> %206
  %730 = add <8 x i32> %601, %596
  %731 = sub <8 x i32> %601, %596
  %732 = icmp sgt <8 x i32> %730, %203
  %733 = select <8 x i1> %732, <8 x i32> %730, <8 x i32> %203
  %734 = icmp slt <8 x i32> %733, %206
  %735 = select <8 x i1> %734, <8 x i32> %733, <8 x i32> %206
  %736 = icmp sgt <8 x i32> %731, %203
  %737 = select <8 x i1> %736, <8 x i32> %731, <8 x i32> %203
  %738 = icmp slt <8 x i32> %737, %206
  %739 = select <8 x i1> %738, <8 x i32> %737, <8 x i32> %206
  %740 = add <8 x i32> %619, %606
  %741 = sub <8 x i32> %606, %619
  %742 = icmp sgt <8 x i32> %740, %203
  %743 = select <8 x i1> %742, <8 x i32> %740, <8 x i32> %203
  %744 = icmp slt <8 x i32> %743, %206
  %745 = select <8 x i1> %744, <8 x i32> %743, <8 x i32> %206
  %746 = icmp sgt <8 x i32> %741, %203
  %747 = select <8 x i1> %746, <8 x i32> %741, <8 x i32> %203
  %748 = icmp slt <8 x i32> %747, %206
  %749 = select <8 x i1> %748, <8 x i32> %747, <8 x i32> %206
  %750 = add <8 x i32> %614, %609
  %751 = sub <8 x i32> %609, %614
  %752 = icmp sgt <8 x i32> %750, %203
  %753 = select <8 x i1> %752, <8 x i32> %750, <8 x i32> %203
  %754 = icmp slt <8 x i32> %753, %206
  %755 = select <8 x i1> %754, <8 x i32> %753, <8 x i32> %206
  %756 = icmp sgt <8 x i32> %751, %203
  %757 = select <8 x i1> %756, <8 x i32> %751, <8 x i32> %203
  %758 = icmp slt <8 x i32> %757, %206
  %759 = select <8 x i1> %758, <8 x i32> %757, <8 x i32> %206
  %760 = mul <8 x i32> %629, %175
  %761 = mul <8 x i32> %639, %172
  %762 = add <8 x i32> %761, %193
  %763 = add <8 x i32> %762, %760
  %764 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %763, i32 %2) #8
  %765 = mul <8 x i32> %629, %172
  %766 = add <8 x i32> %762, %765
  %767 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %766, i32 %2) #8
  %768 = add <8 x i32> %537, %527
  %769 = sub <8 x i32> %527, %537
  %770 = icmp sgt <8 x i32> %768, %203
  %771 = select <8 x i1> %770, <8 x i32> %768, <8 x i32> %203
  %772 = icmp slt <8 x i32> %771, %206
  %773 = select <8 x i1> %772, <8 x i32> %771, <8 x i32> %206
  %774 = icmp sgt <8 x i32> %769, %203
  %775 = select <8 x i1> %774, <8 x i32> %769, <8 x i32> %203
  %776 = icmp slt <8 x i32> %775, %206
  %777 = select <8 x i1> %776, <8 x i32> %775, <8 x i32> %206
  %778 = add <8 x i32> %649, %644
  %779 = sub <8 x i32> %644, %649
  %780 = icmp sgt <8 x i32> %778, %203
  %781 = select <8 x i1> %780, <8 x i32> %778, <8 x i32> %203
  %782 = icmp slt <8 x i32> %781, %206
  %783 = select <8 x i1> %782, <8 x i32> %781, <8 x i32> %206
  %784 = icmp sgt <8 x i32> %779, %203
  %785 = select <8 x i1> %784, <8 x i32> %779, <8 x i32> %203
  %786 = icmp slt <8 x i32> %785, %206
  %787 = select <8 x i1> %786, <8 x i32> %785, <8 x i32> %206
  %788 = add <8 x i32> %557, %547
  %789 = sub <8 x i32> %557, %547
  %790 = icmp sgt <8 x i32> %788, %203
  %791 = select <8 x i1> %790, <8 x i32> %788, <8 x i32> %203
  %792 = icmp slt <8 x i32> %791, %206
  %793 = select <8 x i1> %792, <8 x i32> %791, <8 x i32> %206
  %794 = icmp sgt <8 x i32> %789, %203
  %795 = select <8 x i1> %794, <8 x i32> %789, <8 x i32> %203
  %796 = icmp slt <8 x i32> %795, %206
  %797 = select <8 x i1> %796, <8 x i32> %795, <8 x i32> %206
  %798 = add <8 x i32> %659, %654
  %799 = sub <8 x i32> %659, %654
  %800 = icmp sgt <8 x i32> %798, %203
  %801 = select <8 x i1> %800, <8 x i32> %798, <8 x i32> %203
  %802 = icmp slt <8 x i32> %801, %206
  %803 = select <8 x i1> %802, <8 x i32> %801, <8 x i32> %206
  %804 = icmp sgt <8 x i32> %799, %203
  %805 = select <8 x i1> %804, <8 x i32> %799, <8 x i32> %203
  %806 = icmp slt <8 x i32> %805, %206
  %807 = select <8 x i1> %806, <8 x i32> %805, <8 x i32> %206
  %808 = mul <8 x i32> %679, %189
  %809 = mul <8 x i32> %739, %179
  %810 = add <8 x i32> %808, %193
  %811 = add <8 x i32> %810, %809
  %812 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %811, i32 %2) #8
  %813 = mul <8 x i32> %669, %189
  %814 = mul <8 x i32> %729, %179
  %815 = add <8 x i32> %813, %193
  %816 = add <8 x i32> %815, %814
  %817 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %816, i32 %2) #8
  %818 = mul <8 x i32> %689, %182
  %819 = mul <8 x i32> %709, %189
  %820 = add <8 x i32> %818, %193
  %821 = add <8 x i32> %820, %819
  %822 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %821, i32 %2) #8
  %823 = mul <8 x i32> %699, %182
  %824 = mul <8 x i32> %719, %189
  %825 = add <8 x i32> %823, %193
  %826 = add <8 x i32> %825, %824
  %827 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %826, i32 %2) #8
  %828 = mul <8 x i32> %699, %189
  %829 = mul <8 x i32> %719, %179
  %830 = add <8 x i32> %828, %193
  %831 = add <8 x i32> %830, %829
  %832 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %831, i32 %2) #8
  %833 = mul <8 x i32> %689, %189
  %834 = mul <8 x i32> %709, %179
  %835 = add <8 x i32> %833, %193
  %836 = add <8 x i32> %835, %834
  %837 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %836, i32 %2) #8
  %838 = mul <8 x i32> %669, %179
  %839 = mul <8 x i32> %729, %186
  %840 = add <8 x i32> %838, %193
  %841 = add <8 x i32> %840, %839
  %842 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %841, i32 %2) #8
  %843 = mul <8 x i32> %679, %179
  %844 = mul <8 x i32> %739, %186
  %845 = add <8 x i32> %843, %193
  %846 = add <8 x i32> %845, %844
  %847 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %846, i32 %2) #8
  %848 = add <8 x i32> %745, %635
  %849 = sub <8 x i32> %745, %635
  %850 = icmp sgt <8 x i32> %848, %203
  %851 = select <8 x i1> %850, <8 x i32> %848, <8 x i32> %203
  %852 = icmp slt <8 x i32> %851, %206
  %853 = select <8 x i1> %852, <8 x i32> %851, <8 x i32> %206
  %854 = icmp sgt <8 x i32> %849, %203
  %855 = select <8 x i1> %854, <8 x i32> %849, <8 x i32> %203
  %856 = icmp slt <8 x i32> %855, %206
  %857 = select <8 x i1> %856, <8 x i32> %855, <8 x i32> %206
  %858 = add <8 x i32> %767, %755
  %859 = sub <8 x i32> %755, %767
  %860 = icmp sgt <8 x i32> %858, %203
  %861 = select <8 x i1> %860, <8 x i32> %858, <8 x i32> %203
  %862 = icmp slt <8 x i32> %861, %206
  %863 = select <8 x i1> %862, <8 x i32> %861, <8 x i32> %206
  %864 = icmp sgt <8 x i32> %859, %203
  %865 = select <8 x i1> %864, <8 x i32> %859, <8 x i32> %203
  %866 = icmp slt <8 x i32> %865, %206
  %867 = select <8 x i1> %866, <8 x i32> %865, <8 x i32> %206
  %868 = add <8 x i32> %764, %759
  %869 = sub <8 x i32> %759, %764
  %870 = icmp sgt <8 x i32> %868, %203
  %871 = select <8 x i1> %870, <8 x i32> %868, <8 x i32> %203
  %872 = icmp slt <8 x i32> %871, %206
  %873 = select <8 x i1> %872, <8 x i32> %871, <8 x i32> %206
  %874 = icmp sgt <8 x i32> %869, %203
  %875 = select <8 x i1> %874, <8 x i32> %869, <8 x i32> %203
  %876 = icmp slt <8 x i32> %875, %206
  %877 = select <8 x i1> %876, <8 x i32> %875, <8 x i32> %206
  %878 = add <8 x i32> %749, %625
  %879 = sub <8 x i32> %749, %625
  %880 = icmp sgt <8 x i32> %878, %203
  %881 = select <8 x i1> %880, <8 x i32> %878, <8 x i32> %203
  %882 = icmp slt <8 x i32> %881, %206
  %883 = select <8 x i1> %882, <8 x i32> %881, <8 x i32> %206
  %884 = icmp sgt <8 x i32> %879, %203
  %885 = select <8 x i1> %884, <8 x i32> %879, <8 x i32> %203
  %886 = icmp slt <8 x i32> %885, %206
  %887 = select <8 x i1> %886, <8 x i32> %885, <8 x i32> %206
  %888 = mul <8 x i32> %787, %175
  %889 = mul <8 x i32> %807, %172
  %890 = add <8 x i32> %889, %193
  %891 = add <8 x i32> %890, %888
  %892 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %891, i32 %2) #8
  %893 = mul <8 x i32> %777, %175
  %894 = mul <8 x i32> %797, %172
  %895 = add <8 x i32> %894, %193
  %896 = add <8 x i32> %895, %893
  %897 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %896, i32 %2) #8
  %898 = mul <8 x i32> %777, %172
  %899 = add <8 x i32> %895, %898
  %900 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %899, i32 %2) #8
  %901 = mul <8 x i32> %787, %172
  %902 = add <8 x i32> %890, %901
  %903 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %902, i32 %2) #8
  %904 = add <8 x i32> %685, %665
  %905 = sub <8 x i32> %665, %685
  %906 = icmp sgt <8 x i32> %904, %203
  %907 = select <8 x i1> %906, <8 x i32> %904, <8 x i32> %203
  %908 = icmp slt <8 x i32> %907, %206
  %909 = select <8 x i1> %908, <8 x i32> %907, <8 x i32> %206
  %910 = icmp sgt <8 x i32> %905, %203
  %911 = select <8 x i1> %910, <8 x i32> %905, <8 x i32> %203
  %912 = icmp slt <8 x i32> %911, %206
  %913 = select <8 x i1> %912, <8 x i32> %911, <8 x i32> %206
  %914 = add <8 x i32> %695, %675
  %915 = sub <8 x i32> %675, %695
  %916 = icmp sgt <8 x i32> %914, %203
  %917 = select <8 x i1> %916, <8 x i32> %914, <8 x i32> %203
  %918 = icmp slt <8 x i32> %917, %206
  %919 = select <8 x i1> %918, <8 x i32> %917, <8 x i32> %206
  %920 = icmp sgt <8 x i32> %915, %203
  %921 = select <8 x i1> %920, <8 x i32> %915, <8 x i32> %203
  %922 = icmp slt <8 x i32> %921, %206
  %923 = select <8 x i1> %922, <8 x i32> %921, <8 x i32> %206
  %924 = add <8 x i32> %827, %812
  %925 = sub <8 x i32> %812, %827
  %926 = icmp sgt <8 x i32> %924, %203
  %927 = select <8 x i1> %926, <8 x i32> %924, <8 x i32> %203
  %928 = icmp slt <8 x i32> %927, %206
  %929 = select <8 x i1> %928, <8 x i32> %927, <8 x i32> %206
  %930 = icmp sgt <8 x i32> %925, %203
  %931 = select <8 x i1> %930, <8 x i32> %925, <8 x i32> %203
  %932 = icmp slt <8 x i32> %931, %206
  %933 = select <8 x i1> %932, <8 x i32> %931, <8 x i32> %206
  %934 = add <8 x i32> %822, %817
  %935 = sub <8 x i32> %817, %822
  %936 = icmp sgt <8 x i32> %934, %203
  %937 = select <8 x i1> %936, <8 x i32> %934, <8 x i32> %203
  %938 = icmp slt <8 x i32> %937, %206
  %939 = select <8 x i1> %938, <8 x i32> %937, <8 x i32> %206
  %940 = icmp sgt <8 x i32> %935, %203
  %941 = select <8 x i1> %940, <8 x i32> %935, <8 x i32> %203
  %942 = icmp slt <8 x i32> %941, %206
  %943 = select <8 x i1> %942, <8 x i32> %941, <8 x i32> %206
  %944 = add <8 x i32> %725, %705
  %945 = sub <8 x i32> %725, %705
  %946 = icmp sgt <8 x i32> %944, %203
  %947 = select <8 x i1> %946, <8 x i32> %944, <8 x i32> %203
  %948 = icmp slt <8 x i32> %947, %206
  %949 = select <8 x i1> %948, <8 x i32> %947, <8 x i32> %206
  %950 = icmp sgt <8 x i32> %945, %203
  %951 = select <8 x i1> %950, <8 x i32> %945, <8 x i32> %203
  %952 = icmp slt <8 x i32> %951, %206
  %953 = select <8 x i1> %952, <8 x i32> %951, <8 x i32> %206
  %954 = add <8 x i32> %735, %715
  %955 = sub <8 x i32> %735, %715
  %956 = icmp sgt <8 x i32> %954, %203
  %957 = select <8 x i1> %956, <8 x i32> %954, <8 x i32> %203
  %958 = icmp slt <8 x i32> %957, %206
  %959 = select <8 x i1> %958, <8 x i32> %957, <8 x i32> %206
  %960 = icmp sgt <8 x i32> %955, %203
  %961 = select <8 x i1> %960, <8 x i32> %955, <8 x i32> %203
  %962 = icmp slt <8 x i32> %961, %206
  %963 = select <8 x i1> %962, <8 x i32> %961, <8 x i32> %206
  %964 = add <8 x i32> %847, %832
  %965 = sub <8 x i32> %847, %832
  %966 = icmp sgt <8 x i32> %964, %203
  %967 = select <8 x i1> %966, <8 x i32> %964, <8 x i32> %203
  %968 = icmp slt <8 x i32> %967, %206
  %969 = select <8 x i1> %968, <8 x i32> %967, <8 x i32> %206
  %970 = icmp sgt <8 x i32> %965, %203
  %971 = select <8 x i1> %970, <8 x i32> %965, <8 x i32> %203
  %972 = icmp slt <8 x i32> %971, %206
  %973 = select <8 x i1> %972, <8 x i32> %971, <8 x i32> %206
  %974 = add <8 x i32> %842, %837
  %975 = sub <8 x i32> %842, %837
  %976 = icmp sgt <8 x i32> %974, %203
  %977 = select <8 x i1> %976, <8 x i32> %974, <8 x i32> %203
  %978 = icmp slt <8 x i32> %977, %206
  %979 = select <8 x i1> %978, <8 x i32> %977, <8 x i32> %206
  %980 = icmp sgt <8 x i32> %975, %203
  %981 = select <8 x i1> %980, <8 x i32> %975, <8 x i32> %203
  %982 = icmp slt <8 x i32> %981, %206
  %983 = select <8 x i1> %982, <8 x i32> %981, <8 x i32> %206
  %984 = add <8 x i32> %853, %793
  %985 = sub <8 x i32> %853, %793
  %986 = icmp sgt <8 x i32> %984, %203
  %987 = select <8 x i1> %986, <8 x i32> %984, <8 x i32> %203
  %988 = icmp slt <8 x i32> %987, %206
  %989 = select <8 x i1> %988, <8 x i32> %987, <8 x i32> %206
  %990 = icmp sgt <8 x i32> %985, %203
  %991 = select <8 x i1> %990, <8 x i32> %985, <8 x i32> %203
  %992 = icmp slt <8 x i32> %991, %206
  %993 = select <8 x i1> %992, <8 x i32> %991, <8 x i32> %206
  %994 = add <8 x i32> %863, %803
  %995 = sub <8 x i32> %863, %803
  %996 = icmp sgt <8 x i32> %994, %203
  %997 = select <8 x i1> %996, <8 x i32> %994, <8 x i32> %203
  %998 = icmp slt <8 x i32> %997, %206
  %999 = select <8 x i1> %998, <8 x i32> %997, <8 x i32> %206
  %1000 = icmp sgt <8 x i32> %995, %203
  %1001 = select <8 x i1> %1000, <8 x i32> %995, <8 x i32> %203
  %1002 = icmp slt <8 x i32> %1001, %206
  %1003 = select <8 x i1> %1002, <8 x i32> %1001, <8 x i32> %206
  %1004 = add <8 x i32> %903, %873
  %1005 = sub <8 x i32> %873, %903
  %1006 = icmp sgt <8 x i32> %1004, %203
  %1007 = select <8 x i1> %1006, <8 x i32> %1004, <8 x i32> %203
  %1008 = icmp slt <8 x i32> %1007, %206
  %1009 = select <8 x i1> %1008, <8 x i32> %1007, <8 x i32> %206
  %1010 = icmp sgt <8 x i32> %1005, %203
  %1011 = select <8 x i1> %1010, <8 x i32> %1005, <8 x i32> %203
  %1012 = icmp slt <8 x i32> %1011, %206
  %1013 = select <8 x i1> %1012, <8 x i32> %1011, <8 x i32> %206
  %1014 = add <8 x i32> %900, %883
  %1015 = sub <8 x i32> %883, %900
  %1016 = icmp sgt <8 x i32> %1014, %203
  %1017 = select <8 x i1> %1016, <8 x i32> %1014, <8 x i32> %203
  %1018 = icmp slt <8 x i32> %1017, %206
  %1019 = select <8 x i1> %1018, <8 x i32> %1017, <8 x i32> %206
  %1020 = icmp sgt <8 x i32> %1015, %203
  %1021 = select <8 x i1> %1020, <8 x i32> %1015, <8 x i32> %203
  %1022 = icmp slt <8 x i32> %1021, %206
  %1023 = select <8 x i1> %1022, <8 x i32> %1021, <8 x i32> %206
  %1024 = add <8 x i32> %897, %887
  %1025 = sub <8 x i32> %887, %897
  %1026 = icmp sgt <8 x i32> %1024, %203
  %1027 = select <8 x i1> %1026, <8 x i32> %1024, <8 x i32> %203
  %1028 = icmp slt <8 x i32> %1027, %206
  %1029 = select <8 x i1> %1028, <8 x i32> %1027, <8 x i32> %206
  %1030 = icmp sgt <8 x i32> %1025, %203
  %1031 = select <8 x i1> %1030, <8 x i32> %1025, <8 x i32> %203
  %1032 = icmp slt <8 x i32> %1031, %206
  %1033 = select <8 x i1> %1032, <8 x i32> %1031, <8 x i32> %206
  %1034 = add <8 x i32> %892, %877
  %1035 = sub <8 x i32> %877, %892
  %1036 = icmp sgt <8 x i32> %1034, %203
  %1037 = select <8 x i1> %1036, <8 x i32> %1034, <8 x i32> %203
  %1038 = icmp slt <8 x i32> %1037, %206
  %1039 = select <8 x i1> %1038, <8 x i32> %1037, <8 x i32> %206
  %1040 = icmp sgt <8 x i32> %1035, %203
  %1041 = select <8 x i1> %1040, <8 x i32> %1035, <8 x i32> %203
  %1042 = icmp slt <8 x i32> %1041, %206
  %1043 = select <8 x i1> %1042, <8 x i32> %1041, <8 x i32> %206
  %1044 = add <8 x i32> %867, %783
  %1045 = sub <8 x i32> %867, %783
  %1046 = icmp sgt <8 x i32> %1044, %203
  %1047 = select <8 x i1> %1046, <8 x i32> %1044, <8 x i32> %203
  %1048 = icmp slt <8 x i32> %1047, %206
  %1049 = select <8 x i1> %1048, <8 x i32> %1047, <8 x i32> %206
  %1050 = icmp sgt <8 x i32> %1045, %203
  %1051 = select <8 x i1> %1050, <8 x i32> %1045, <8 x i32> %203
  %1052 = icmp slt <8 x i32> %1051, %206
  %1053 = select <8 x i1> %1052, <8 x i32> %1051, <8 x i32> %206
  %1054 = add <8 x i32> %857, %773
  %1055 = sub <8 x i32> %857, %773
  %1056 = icmp sgt <8 x i32> %1054, %203
  %1057 = select <8 x i1> %1056, <8 x i32> %1054, <8 x i32> %203
  %1058 = icmp slt <8 x i32> %1057, %206
  %1059 = select <8 x i1> %1058, <8 x i32> %1057, <8 x i32> %206
  %1060 = icmp sgt <8 x i32> %1055, %203
  %1061 = select <8 x i1> %1060, <8 x i32> %1055, <8 x i32> %203
  %1062 = icmp slt <8 x i32> %1061, %206
  %1063 = select <8 x i1> %1062, <8 x i32> %1061, <8 x i32> %206
  %1064 = mul <8 x i32> %943, %175
  %1065 = mul <8 x i32> %983, %172
  %1066 = add <8 x i32> %1065, %193
  %1067 = add <8 x i32> %1066, %1064
  %1068 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1067, i32 %2) #8
  %1069 = mul <8 x i32> %933, %175
  %1070 = mul <8 x i32> %973, %172
  %1071 = add <8 x i32> %1070, %193
  %1072 = add <8 x i32> %1071, %1069
  %1073 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1072, i32 %2) #8
  %1074 = mul <8 x i32> %923, %175
  %1075 = mul <8 x i32> %963, %172
  %1076 = add <8 x i32> %1075, %193
  %1077 = add <8 x i32> %1076, %1074
  %1078 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1077, i32 %2) #8
  %1079 = mul <8 x i32> %913, %175
  %1080 = mul <8 x i32> %953, %172
  %1081 = add <8 x i32> %1080, %193
  %1082 = add <8 x i32> %1081, %1079
  %1083 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1082, i32 %2) #8
  %1084 = mul <8 x i32> %913, %172
  %1085 = add <8 x i32> %1081, %1084
  %1086 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1085, i32 %2) #8
  %1087 = mul <8 x i32> %923, %172
  %1088 = add <8 x i32> %1076, %1087
  %1089 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1088, i32 %2) #8
  %1090 = mul <8 x i32> %933, %172
  %1091 = add <8 x i32> %1071, %1090
  %1092 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1091, i32 %2) #8
  %1093 = mul <8 x i32> %943, %172
  %1094 = add <8 x i32> %1066, %1093
  %1095 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1094, i32 %2) #8
  %1096 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 31
  %1097 = add <8 x i32> %989, %949
  %1098 = sub <8 x i32> %989, %949
  %1099 = icmp sgt <8 x i32> %1097, %203
  %1100 = select <8 x i1> %1099, <8 x i32> %1097, <8 x i32> %203
  %1101 = icmp slt <8 x i32> %1100, %206
  %1102 = select <8 x i1> %1101, <8 x i32> %1100, <8 x i32> %206
  %1103 = icmp sgt <8 x i32> %1098, %203
  %1104 = select <8 x i1> %1103, <8 x i32> %1098, <8 x i32> %203
  %1105 = icmp slt <8 x i32> %1104, %206
  %1106 = select <8 x i1> %1105, <8 x i32> %1104, <8 x i32> %206
  %1107 = bitcast <4 x i64>* %1 to <8 x i32>*
  store <8 x i32> %1102, <8 x i32>* %1107, align 32
  %1108 = bitcast <4 x i64>* %1096 to <8 x i32>*
  store <8 x i32> %1106, <8 x i32>* %1108, align 32
  %1109 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %1110 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 30
  %1111 = add <8 x i32> %999, %959
  %1112 = sub <8 x i32> %999, %959
  %1113 = icmp sgt <8 x i32> %1111, %203
  %1114 = select <8 x i1> %1113, <8 x i32> %1111, <8 x i32> %203
  %1115 = icmp slt <8 x i32> %1114, %206
  %1116 = select <8 x i1> %1115, <8 x i32> %1114, <8 x i32> %206
  %1117 = icmp sgt <8 x i32> %1112, %203
  %1118 = select <8 x i1> %1117, <8 x i32> %1112, <8 x i32> %203
  %1119 = icmp slt <8 x i32> %1118, %206
  %1120 = select <8 x i1> %1119, <8 x i32> %1118, <8 x i32> %206
  %1121 = bitcast <4 x i64>* %1109 to <8 x i32>*
  store <8 x i32> %1116, <8 x i32>* %1121, align 32
  %1122 = bitcast <4 x i64>* %1110 to <8 x i32>*
  store <8 x i32> %1120, <8 x i32>* %1122, align 32
  %1123 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %1124 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 29
  %1125 = add <8 x i32> %1009, %969
  %1126 = sub <8 x i32> %1009, %969
  %1127 = icmp sgt <8 x i32> %1125, %203
  %1128 = select <8 x i1> %1127, <8 x i32> %1125, <8 x i32> %203
  %1129 = icmp slt <8 x i32> %1128, %206
  %1130 = select <8 x i1> %1129, <8 x i32> %1128, <8 x i32> %206
  %1131 = icmp sgt <8 x i32> %1126, %203
  %1132 = select <8 x i1> %1131, <8 x i32> %1126, <8 x i32> %203
  %1133 = icmp slt <8 x i32> %1132, %206
  %1134 = select <8 x i1> %1133, <8 x i32> %1132, <8 x i32> %206
  %1135 = bitcast <4 x i64>* %1123 to <8 x i32>*
  store <8 x i32> %1130, <8 x i32>* %1135, align 32
  %1136 = bitcast <4 x i64>* %1124 to <8 x i32>*
  store <8 x i32> %1134, <8 x i32>* %1136, align 32
  %1137 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %1138 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 28
  %1139 = add <8 x i32> %1019, %979
  %1140 = sub <8 x i32> %1019, %979
  %1141 = icmp sgt <8 x i32> %1139, %203
  %1142 = select <8 x i1> %1141, <8 x i32> %1139, <8 x i32> %203
  %1143 = icmp slt <8 x i32> %1142, %206
  %1144 = select <8 x i1> %1143, <8 x i32> %1142, <8 x i32> %206
  %1145 = icmp sgt <8 x i32> %1140, %203
  %1146 = select <8 x i1> %1145, <8 x i32> %1140, <8 x i32> %203
  %1147 = icmp slt <8 x i32> %1146, %206
  %1148 = select <8 x i1> %1147, <8 x i32> %1146, <8 x i32> %206
  %1149 = bitcast <4 x i64>* %1137 to <8 x i32>*
  store <8 x i32> %1144, <8 x i32>* %1149, align 32
  %1150 = bitcast <4 x i64>* %1138 to <8 x i32>*
  store <8 x i32> %1148, <8 x i32>* %1150, align 32
  %1151 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %1152 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 27
  %1153 = add <8 x i32> %1095, %1029
  %1154 = sub <8 x i32> %1029, %1095
  %1155 = icmp sgt <8 x i32> %1153, %203
  %1156 = select <8 x i1> %1155, <8 x i32> %1153, <8 x i32> %203
  %1157 = icmp slt <8 x i32> %1156, %206
  %1158 = select <8 x i1> %1157, <8 x i32> %1156, <8 x i32> %206
  %1159 = icmp sgt <8 x i32> %1154, %203
  %1160 = select <8 x i1> %1159, <8 x i32> %1154, <8 x i32> %203
  %1161 = icmp slt <8 x i32> %1160, %206
  %1162 = select <8 x i1> %1161, <8 x i32> %1160, <8 x i32> %206
  %1163 = bitcast <4 x i64>* %1151 to <8 x i32>*
  store <8 x i32> %1158, <8 x i32>* %1163, align 32
  %1164 = bitcast <4 x i64>* %1152 to <8 x i32>*
  store <8 x i32> %1162, <8 x i32>* %1164, align 32
  %1165 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %1166 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 26
  %1167 = add <8 x i32> %1092, %1039
  %1168 = sub <8 x i32> %1039, %1092
  %1169 = icmp sgt <8 x i32> %1167, %203
  %1170 = select <8 x i1> %1169, <8 x i32> %1167, <8 x i32> %203
  %1171 = icmp slt <8 x i32> %1170, %206
  %1172 = select <8 x i1> %1171, <8 x i32> %1170, <8 x i32> %206
  %1173 = icmp sgt <8 x i32> %1168, %203
  %1174 = select <8 x i1> %1173, <8 x i32> %1168, <8 x i32> %203
  %1175 = icmp slt <8 x i32> %1174, %206
  %1176 = select <8 x i1> %1175, <8 x i32> %1174, <8 x i32> %206
  %1177 = bitcast <4 x i64>* %1165 to <8 x i32>*
  store <8 x i32> %1172, <8 x i32>* %1177, align 32
  %1178 = bitcast <4 x i64>* %1166 to <8 x i32>*
  store <8 x i32> %1176, <8 x i32>* %1178, align 32
  %1179 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %1180 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 25
  %1181 = add <8 x i32> %1089, %1049
  %1182 = sub <8 x i32> %1049, %1089
  %1183 = icmp sgt <8 x i32> %1181, %203
  %1184 = select <8 x i1> %1183, <8 x i32> %1181, <8 x i32> %203
  %1185 = icmp slt <8 x i32> %1184, %206
  %1186 = select <8 x i1> %1185, <8 x i32> %1184, <8 x i32> %206
  %1187 = icmp sgt <8 x i32> %1182, %203
  %1188 = select <8 x i1> %1187, <8 x i32> %1182, <8 x i32> %203
  %1189 = icmp slt <8 x i32> %1188, %206
  %1190 = select <8 x i1> %1189, <8 x i32> %1188, <8 x i32> %206
  %1191 = bitcast <4 x i64>* %1179 to <8 x i32>*
  store <8 x i32> %1186, <8 x i32>* %1191, align 32
  %1192 = bitcast <4 x i64>* %1180 to <8 x i32>*
  store <8 x i32> %1190, <8 x i32>* %1192, align 32
  %1193 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %1194 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 24
  %1195 = add <8 x i32> %1086, %1059
  %1196 = sub <8 x i32> %1059, %1086
  %1197 = icmp sgt <8 x i32> %1195, %203
  %1198 = select <8 x i1> %1197, <8 x i32> %1195, <8 x i32> %203
  %1199 = icmp slt <8 x i32> %1198, %206
  %1200 = select <8 x i1> %1199, <8 x i32> %1198, <8 x i32> %206
  %1201 = icmp sgt <8 x i32> %1196, %203
  %1202 = select <8 x i1> %1201, <8 x i32> %1196, <8 x i32> %203
  %1203 = icmp slt <8 x i32> %1202, %206
  %1204 = select <8 x i1> %1203, <8 x i32> %1202, <8 x i32> %206
  %1205 = bitcast <4 x i64>* %1193 to <8 x i32>*
  store <8 x i32> %1200, <8 x i32>* %1205, align 32
  %1206 = bitcast <4 x i64>* %1194 to <8 x i32>*
  store <8 x i32> %1204, <8 x i32>* %1206, align 32
  %1207 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %1208 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 23
  %1209 = add <8 x i32> %1083, %1063
  %1210 = sub <8 x i32> %1063, %1083
  %1211 = icmp sgt <8 x i32> %1209, %203
  %1212 = select <8 x i1> %1211, <8 x i32> %1209, <8 x i32> %203
  %1213 = icmp slt <8 x i32> %1212, %206
  %1214 = select <8 x i1> %1213, <8 x i32> %1212, <8 x i32> %206
  %1215 = icmp sgt <8 x i32> %1210, %203
  %1216 = select <8 x i1> %1215, <8 x i32> %1210, <8 x i32> %203
  %1217 = icmp slt <8 x i32> %1216, %206
  %1218 = select <8 x i1> %1217, <8 x i32> %1216, <8 x i32> %206
  %1219 = bitcast <4 x i64>* %1207 to <8 x i32>*
  store <8 x i32> %1214, <8 x i32>* %1219, align 32
  %1220 = bitcast <4 x i64>* %1208 to <8 x i32>*
  store <8 x i32> %1218, <8 x i32>* %1220, align 32
  %1221 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %1222 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 22
  %1223 = add <8 x i32> %1078, %1053
  %1224 = sub <8 x i32> %1053, %1078
  %1225 = icmp sgt <8 x i32> %1223, %203
  %1226 = select <8 x i1> %1225, <8 x i32> %1223, <8 x i32> %203
  %1227 = icmp slt <8 x i32> %1226, %206
  %1228 = select <8 x i1> %1227, <8 x i32> %1226, <8 x i32> %206
  %1229 = icmp sgt <8 x i32> %1224, %203
  %1230 = select <8 x i1> %1229, <8 x i32> %1224, <8 x i32> %203
  %1231 = icmp slt <8 x i32> %1230, %206
  %1232 = select <8 x i1> %1231, <8 x i32> %1230, <8 x i32> %206
  %1233 = bitcast <4 x i64>* %1221 to <8 x i32>*
  store <8 x i32> %1228, <8 x i32>* %1233, align 32
  %1234 = bitcast <4 x i64>* %1222 to <8 x i32>*
  store <8 x i32> %1232, <8 x i32>* %1234, align 32
  %1235 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %1236 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 21
  %1237 = add <8 x i32> %1073, %1043
  %1238 = sub <8 x i32> %1043, %1073
  %1239 = icmp sgt <8 x i32> %1237, %203
  %1240 = select <8 x i1> %1239, <8 x i32> %1237, <8 x i32> %203
  %1241 = icmp slt <8 x i32> %1240, %206
  %1242 = select <8 x i1> %1241, <8 x i32> %1240, <8 x i32> %206
  %1243 = icmp sgt <8 x i32> %1238, %203
  %1244 = select <8 x i1> %1243, <8 x i32> %1238, <8 x i32> %203
  %1245 = icmp slt <8 x i32> %1244, %206
  %1246 = select <8 x i1> %1245, <8 x i32> %1244, <8 x i32> %206
  %1247 = bitcast <4 x i64>* %1235 to <8 x i32>*
  store <8 x i32> %1242, <8 x i32>* %1247, align 32
  %1248 = bitcast <4 x i64>* %1236 to <8 x i32>*
  store <8 x i32> %1246, <8 x i32>* %1248, align 32
  %1249 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %1250 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 20
  %1251 = add <8 x i32> %1033, %1068
  %1252 = sub <8 x i32> %1033, %1068
  %1253 = icmp sgt <8 x i32> %1251, %203
  %1254 = select <8 x i1> %1253, <8 x i32> %1251, <8 x i32> %203
  %1255 = icmp slt <8 x i32> %1254, %206
  %1256 = select <8 x i1> %1255, <8 x i32> %1254, <8 x i32> %206
  %1257 = icmp sgt <8 x i32> %1252, %203
  %1258 = select <8 x i1> %1257, <8 x i32> %1252, <8 x i32> %203
  %1259 = icmp slt <8 x i32> %1258, %206
  %1260 = select <8 x i1> %1259, <8 x i32> %1258, <8 x i32> %206
  %1261 = bitcast <4 x i64>* %1249 to <8 x i32>*
  store <8 x i32> %1256, <8 x i32>* %1261, align 32
  %1262 = bitcast <4 x i64>* %1250 to <8 x i32>*
  store <8 x i32> %1260, <8 x i32>* %1262, align 32
  %1263 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %1264 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 19
  %1265 = add <8 x i32> %1023, %939
  %1266 = sub <8 x i32> %1023, %939
  %1267 = icmp sgt <8 x i32> %1265, %203
  %1268 = select <8 x i1> %1267, <8 x i32> %1265, <8 x i32> %203
  %1269 = icmp slt <8 x i32> %1268, %206
  %1270 = select <8 x i1> %1269, <8 x i32> %1268, <8 x i32> %206
  %1271 = icmp sgt <8 x i32> %1266, %203
  %1272 = select <8 x i1> %1271, <8 x i32> %1266, <8 x i32> %203
  %1273 = icmp slt <8 x i32> %1272, %206
  %1274 = select <8 x i1> %1273, <8 x i32> %1272, <8 x i32> %206
  %1275 = bitcast <4 x i64>* %1263 to <8 x i32>*
  store <8 x i32> %1270, <8 x i32>* %1275, align 32
  %1276 = bitcast <4 x i64>* %1264 to <8 x i32>*
  store <8 x i32> %1274, <8 x i32>* %1276, align 32
  %1277 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %1278 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 18
  %1279 = add <8 x i32> %1013, %929
  %1280 = sub <8 x i32> %1013, %929
  %1281 = icmp sgt <8 x i32> %1279, %203
  %1282 = select <8 x i1> %1281, <8 x i32> %1279, <8 x i32> %203
  %1283 = icmp slt <8 x i32> %1282, %206
  %1284 = select <8 x i1> %1283, <8 x i32> %1282, <8 x i32> %206
  %1285 = icmp sgt <8 x i32> %1280, %203
  %1286 = select <8 x i1> %1285, <8 x i32> %1280, <8 x i32> %203
  %1287 = icmp slt <8 x i32> %1286, %206
  %1288 = select <8 x i1> %1287, <8 x i32> %1286, <8 x i32> %206
  %1289 = bitcast <4 x i64>* %1277 to <8 x i32>*
  store <8 x i32> %1284, <8 x i32>* %1289, align 32
  %1290 = bitcast <4 x i64>* %1278 to <8 x i32>*
  store <8 x i32> %1288, <8 x i32>* %1290, align 32
  %1291 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %1292 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 17
  %1293 = add <8 x i32> %1003, %919
  %1294 = sub <8 x i32> %1003, %919
  %1295 = icmp sgt <8 x i32> %1293, %203
  %1296 = select <8 x i1> %1295, <8 x i32> %1293, <8 x i32> %203
  %1297 = icmp slt <8 x i32> %1296, %206
  %1298 = select <8 x i1> %1297, <8 x i32> %1296, <8 x i32> %206
  %1299 = icmp sgt <8 x i32> %1294, %203
  %1300 = select <8 x i1> %1299, <8 x i32> %1294, <8 x i32> %203
  %1301 = icmp slt <8 x i32> %1300, %206
  %1302 = select <8 x i1> %1301, <8 x i32> %1300, <8 x i32> %206
  %1303 = bitcast <4 x i64>* %1291 to <8 x i32>*
  store <8 x i32> %1298, <8 x i32>* %1303, align 32
  %1304 = bitcast <4 x i64>* %1292 to <8 x i32>*
  store <8 x i32> %1302, <8 x i32>* %1304, align 32
  %1305 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 15
  %1306 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 16
  %1307 = add <8 x i32> %993, %909
  %1308 = sub <8 x i32> %993, %909
  %1309 = icmp sgt <8 x i32> %1307, %203
  %1310 = select <8 x i1> %1309, <8 x i32> %1307, <8 x i32> %203
  %1311 = icmp slt <8 x i32> %1310, %206
  %1312 = select <8 x i1> %1311, <8 x i32> %1310, <8 x i32> %206
  %1313 = icmp sgt <8 x i32> %1308, %203
  %1314 = select <8 x i1> %1313, <8 x i32> %1308, <8 x i32> %203
  %1315 = icmp slt <8 x i32> %1314, %206
  %1316 = select <8 x i1> %1315, <8 x i32> %1314, <8 x i32> %206
  %1317 = bitcast <4 x i64>* %1305 to <8 x i32>*
  store <8 x i32> %1312, <8 x i32>* %1317, align 32
  %1318 = bitcast <4 x i64>* %1306 to <8 x i32>*
  store <8 x i32> %1316, <8 x i32>* %1318, align 32
  br i1 %194, label %1444, label %1319

1319:                                             ; preds = %6
  %1320 = icmp sgt i32 %4, 10
  %1321 = select i1 %1320, i32 %4, i32 10
  %1322 = shl i32 32, %1321
  %1323 = sub nsw i32 0, %1322
  %1324 = insertelement <8 x i32> undef, i32 %1323, i32 0
  %1325 = shufflevector <8 x i32> %1324, <8 x i32> undef, <8 x i32> zeroinitializer
  %1326 = add nsw i32 %1322, -1
  %1327 = insertelement <8 x i32> undef, i32 %1326, i32 0
  %1328 = shufflevector <8 x i32> %1327, <8 x i32> undef, <8 x i32> zeroinitializer
  %1329 = icmp eq i32 %5, 0
  br i1 %1329, label %1408, label %1330

1330:                                             ; preds = %1319
  %1331 = add nsw i32 %5, -1
  %1332 = shl i32 1, %1331
  %1333 = insertelement <8 x i32> undef, i32 %1332, i32 0
  %1334 = shufflevector <8 x i32> %1333, <8 x i32> undef, <8 x i32> zeroinitializer
  %1335 = add <8 x i32> %1102, %1334
  %1336 = add <8 x i32> %1116, %1334
  %1337 = add <8 x i32> %1130, %1334
  %1338 = add <8 x i32> %1144, %1334
  %1339 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1335, i32 %5) #8
  store <8 x i32> %1339, <8 x i32>* %1107, align 32
  %1340 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1336, i32 %5) #8
  store <8 x i32> %1340, <8 x i32>* %1121, align 32
  %1341 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1337, i32 %5) #8
  store <8 x i32> %1341, <8 x i32>* %1135, align 32
  %1342 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1338, i32 %5) #8
  store <8 x i32> %1342, <8 x i32>* %1149, align 32
  %1343 = add <8 x i32> %1158, %1334
  %1344 = add <8 x i32> %1172, %1334
  %1345 = add <8 x i32> %1186, %1334
  %1346 = add <8 x i32> %1200, %1334
  %1347 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1343, i32 %5) #8
  store <8 x i32> %1347, <8 x i32>* %1163, align 32
  %1348 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1344, i32 %5) #8
  store <8 x i32> %1348, <8 x i32>* %1177, align 32
  %1349 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1345, i32 %5) #8
  store <8 x i32> %1349, <8 x i32>* %1191, align 32
  %1350 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1346, i32 %5) #8
  store <8 x i32> %1350, <8 x i32>* %1205, align 32
  %1351 = add <8 x i32> %1214, %1334
  %1352 = add <8 x i32> %1228, %1334
  %1353 = add <8 x i32> %1242, %1334
  %1354 = add <8 x i32> %1256, %1334
  %1355 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1351, i32 %5) #8
  store <8 x i32> %1355, <8 x i32>* %1219, align 32
  %1356 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1352, i32 %5) #8
  store <8 x i32> %1356, <8 x i32>* %1233, align 32
  %1357 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1353, i32 %5) #8
  store <8 x i32> %1357, <8 x i32>* %1247, align 32
  %1358 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1354, i32 %5) #8
  store <8 x i32> %1358, <8 x i32>* %1261, align 32
  %1359 = add <8 x i32> %1270, %1334
  %1360 = add <8 x i32> %1284, %1334
  %1361 = add <8 x i32> %1298, %1334
  %1362 = add <8 x i32> %1312, %1334
  %1363 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1359, i32 %5) #8
  store <8 x i32> %1363, <8 x i32>* %1275, align 32
  %1364 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1360, i32 %5) #8
  store <8 x i32> %1364, <8 x i32>* %1289, align 32
  %1365 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1361, i32 %5) #8
  store <8 x i32> %1365, <8 x i32>* %1303, align 32
  %1366 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1362, i32 %5) #8
  store <8 x i32> %1366, <8 x i32>* %1317, align 32
  %1367 = add <8 x i32> %1316, %1334
  %1368 = add <8 x i32> %1302, %1334
  %1369 = add <8 x i32> %1288, %1334
  %1370 = add <8 x i32> %1274, %1334
  %1371 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1367, i32 %5) #8
  store <8 x i32> %1371, <8 x i32>* %1318, align 32
  %1372 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1368, i32 %5) #8
  store <8 x i32> %1372, <8 x i32>* %1304, align 32
  %1373 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1369, i32 %5) #8
  store <8 x i32> %1373, <8 x i32>* %1290, align 32
  %1374 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1370, i32 %5) #8
  store <8 x i32> %1374, <8 x i32>* %1276, align 32
  %1375 = add <8 x i32> %1260, %1334
  %1376 = add <8 x i32> %1246, %1334
  %1377 = add <8 x i32> %1232, %1334
  %1378 = load <8 x i32>, <8 x i32>* %1220, align 32
  %1379 = add <8 x i32> %1378, %1334
  %1380 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1375, i32 %5) #8
  store <8 x i32> %1380, <8 x i32>* %1262, align 32
  %1381 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1376, i32 %5) #8
  store <8 x i32> %1381, <8 x i32>* %1248, align 32
  %1382 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1377, i32 %5) #8
  store <8 x i32> %1382, <8 x i32>* %1234, align 32
  %1383 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1379, i32 %5) #8
  store <8 x i32> %1383, <8 x i32>* %1220, align 32
  %1384 = load <8 x i32>, <8 x i32>* %1206, align 32
  %1385 = add <8 x i32> %1384, %1334
  %1386 = load <8 x i32>, <8 x i32>* %1192, align 32
  %1387 = add <8 x i32> %1386, %1334
  %1388 = load <8 x i32>, <8 x i32>* %1178, align 32
  %1389 = add <8 x i32> %1388, %1334
  %1390 = load <8 x i32>, <8 x i32>* %1164, align 32
  %1391 = add <8 x i32> %1390, %1334
  %1392 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1385, i32 %5) #8
  store <8 x i32> %1392, <8 x i32>* %1206, align 32
  %1393 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1387, i32 %5) #8
  store <8 x i32> %1393, <8 x i32>* %1192, align 32
  %1394 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1389, i32 %5) #8
  store <8 x i32> %1394, <8 x i32>* %1178, align 32
  %1395 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1391, i32 %5) #8
  store <8 x i32> %1395, <8 x i32>* %1164, align 32
  %1396 = load <8 x i32>, <8 x i32>* %1150, align 32
  %1397 = add <8 x i32> %1396, %1334
  %1398 = load <8 x i32>, <8 x i32>* %1136, align 32
  %1399 = add <8 x i32> %1398, %1334
  %1400 = load <8 x i32>, <8 x i32>* %1122, align 32
  %1401 = add <8 x i32> %1400, %1334
  %1402 = load <8 x i32>, <8 x i32>* %1108, align 32
  %1403 = add <8 x i32> %1402, %1334
  %1404 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1397, i32 %5) #8
  store <8 x i32> %1404, <8 x i32>* %1150, align 32
  %1405 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1399, i32 %5) #8
  store <8 x i32> %1405, <8 x i32>* %1136, align 32
  %1406 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1401, i32 %5) #8
  store <8 x i32> %1406, <8 x i32>* %1122, align 32
  %1407 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1403, i32 %5) #8
  store <8 x i32> %1407, <8 x i32>* %1108, align 32
  br label %1408

1408:                                             ; preds = %1319, %1330
  br label %1409

1409:                                             ; preds = %1408, %1409
  %1410 = phi i64 [ %1442, %1409 ], [ 0, %1408 ]
  %1411 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 %1410
  %1412 = bitcast <4 x i64>* %1411 to <8 x i32>*
  %1413 = load <8 x i32>, <8 x i32>* %1412, align 32
  %1414 = icmp sgt <8 x i32> %1413, %1325
  %1415 = select <8 x i1> %1414, <8 x i32> %1413, <8 x i32> %1325
  %1416 = icmp slt <8 x i32> %1415, %1328
  %1417 = select <8 x i1> %1416, <8 x i32> %1415, <8 x i32> %1328
  store <8 x i32> %1417, <8 x i32>* %1412, align 32
  %1418 = or i64 %1410, 1
  %1419 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 %1418
  %1420 = bitcast <4 x i64>* %1419 to <8 x i32>*
  %1421 = load <8 x i32>, <8 x i32>* %1420, align 32
  %1422 = icmp sgt <8 x i32> %1421, %1325
  %1423 = select <8 x i1> %1422, <8 x i32> %1421, <8 x i32> %1325
  %1424 = icmp slt <8 x i32> %1423, %1328
  %1425 = select <8 x i1> %1424, <8 x i32> %1423, <8 x i32> %1328
  store <8 x i32> %1425, <8 x i32>* %1420, align 32
  %1426 = or i64 %1410, 2
  %1427 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 %1426
  %1428 = bitcast <4 x i64>* %1427 to <8 x i32>*
  %1429 = load <8 x i32>, <8 x i32>* %1428, align 32
  %1430 = icmp sgt <8 x i32> %1429, %1325
  %1431 = select <8 x i1> %1430, <8 x i32> %1429, <8 x i32> %1325
  %1432 = icmp slt <8 x i32> %1431, %1328
  %1433 = select <8 x i1> %1432, <8 x i32> %1431, <8 x i32> %1328
  store <8 x i32> %1433, <8 x i32>* %1428, align 32
  %1434 = or i64 %1410, 3
  %1435 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 %1434
  %1436 = bitcast <4 x i64>* %1435 to <8 x i32>*
  %1437 = load <8 x i32>, <8 x i32>* %1436, align 32
  %1438 = icmp sgt <8 x i32> %1437, %1325
  %1439 = select <8 x i1> %1438, <8 x i32> %1437, <8 x i32> %1325
  %1440 = icmp slt <8 x i32> %1439, %1328
  %1441 = select <8 x i1> %1440, <8 x i32> %1439, <8 x i32> %1328
  store <8 x i32> %1441, <8 x i32>* %1436, align 32
  %1442 = add nuw nsw i64 %1410, 4
  %1443 = icmp ult i64 %1442, 32
  br i1 %1443, label %1409, label %1444

1444:                                             ; preds = %1409, %6
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @idct64_low1_avx2(<4 x i64>* nocapture readonly, <4 x i64>*, i32, i32, i32, i32) #0 {
  %7 = add nsw i32 %2, -10
  %8 = sext i32 %7 to i64
  %9 = add nsw i32 %2, -1
  %10 = shl i32 1, %9
  %11 = insertelement <8 x i32> undef, i32 %10, i32 0
  %12 = shufflevector <8 x i32> %11, <8 x i32> undef, <8 x i32> zeroinitializer
  %13 = icmp ne i32 %3, 0
  %14 = select i1 %13, i32 6, i32 8
  %15 = add nsw i32 %14, %4
  %16 = icmp slt i32 %15, 16
  %17 = add i32 %15, -1
  %18 = shl i32 1, %17
  %19 = select i1 %16, i32 32768, i32 %18
  %20 = sub nsw i32 0, %19
  %21 = insertelement <8 x i32> undef, i32 %20, i32 0
  %22 = shufflevector <8 x i32> %21, <8 x i32> undef, <8 x i32> zeroinitializer
  %23 = add nsw i32 %19, -1
  %24 = insertelement <8 x i32> undef, i32 %23, i32 0
  %25 = shufflevector <8 x i32> %24, <8 x i32> undef, <8 x i32> zeroinitializer
  %26 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %8, i64 32
  %27 = load i32, i32* %26, align 16
  %28 = insertelement <8 x i32> undef, i32 %27, i32 0
  %29 = shufflevector <8 x i32> %28, <8 x i32> undef, <8 x i32> zeroinitializer
  %30 = bitcast <4 x i64>* %0 to <8 x i32>*
  %31 = load <8 x i32>, <8 x i32>* %30, align 32
  %32 = mul <8 x i32> %29, %31
  %33 = add <8 x i32> %32, %12
  %34 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %33, i32 %2) #8
  br i1 %13, label %54, label %35

35:                                               ; preds = %6
  %36 = icmp sgt i32 %4, 10
  %37 = select i1 %36, i32 %4, i32 10
  %38 = shl i32 32, %37
  %39 = sub nsw i32 0, %38
  %40 = insertelement <8 x i32> undef, i32 %39, i32 0
  %41 = shufflevector <8 x i32> %40, <8 x i32> undef, <8 x i32> zeroinitializer
  %42 = add nsw i32 %38, -1
  %43 = insertelement <8 x i32> undef, i32 %42, i32 0
  %44 = shufflevector <8 x i32> %43, <8 x i32> undef, <8 x i32> zeroinitializer
  %45 = icmp eq i32 %5, 0
  br i1 %45, label %54, label %46

46:                                               ; preds = %35
  %47 = shl i32 1, %5
  %48 = ashr i32 %47, 1
  %49 = insertelement <8 x i32> undef, i32 %48, i32 0
  %50 = shufflevector <8 x i32> %49, <8 x i32> undef, <8 x i32> zeroinitializer
  %51 = add <8 x i32> %34, %50
  %52 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %5, i32 0
  %53 = tail call <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32> %51, <4 x i32> %52) #8
  br label %54

54:                                               ; preds = %46, %35, %6
  %55 = phi <8 x i32> [ %22, %6 ], [ %41, %35 ], [ %41, %46 ]
  %56 = phi <8 x i32> [ %25, %6 ], [ %44, %35 ], [ %44, %46 ]
  %57 = phi <8 x i32> [ %34, %6 ], [ %34, %35 ], [ %53, %46 ]
  %58 = icmp sgt <8 x i32> %57, %55
  %59 = select <8 x i1> %58, <8 x i32> %57, <8 x i32> %55
  %60 = icmp slt <8 x i32> %59, %56
  %61 = select <8 x i1> %60, <8 x i32> %59, <8 x i32> %56
  %62 = bitcast <4 x i64>* %1 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %62, align 32
  %63 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %64 = bitcast <4 x i64>* %63 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %64, align 32
  %65 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %66 = bitcast <4 x i64>* %65 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %66, align 32
  %67 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %68 = bitcast <4 x i64>* %67 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %68, align 32
  %69 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %70 = bitcast <4 x i64>* %69 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %70, align 32
  %71 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %72 = bitcast <4 x i64>* %71 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %72, align 32
  %73 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %74 = bitcast <4 x i64>* %73 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %74, align 32
  %75 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %76 = bitcast <4 x i64>* %75 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %76, align 32
  %77 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %78 = bitcast <4 x i64>* %77 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %78, align 32
  %79 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %80 = bitcast <4 x i64>* %79 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %80, align 32
  %81 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %82 = bitcast <4 x i64>* %81 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %82, align 32
  %83 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %84 = bitcast <4 x i64>* %83 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %84, align 32
  %85 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %86 = bitcast <4 x i64>* %85 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %86, align 32
  %87 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %88 = bitcast <4 x i64>* %87 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %88, align 32
  %89 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %90 = bitcast <4 x i64>* %89 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %90, align 32
  %91 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 15
  %92 = bitcast <4 x i64>* %91 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %92, align 32
  %93 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 16
  %94 = bitcast <4 x i64>* %93 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %94, align 32
  %95 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 17
  %96 = bitcast <4 x i64>* %95 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %96, align 32
  %97 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 18
  %98 = bitcast <4 x i64>* %97 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %98, align 32
  %99 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 19
  %100 = bitcast <4 x i64>* %99 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %100, align 32
  %101 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 20
  %102 = bitcast <4 x i64>* %101 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %102, align 32
  %103 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 21
  %104 = bitcast <4 x i64>* %103 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %104, align 32
  %105 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 22
  %106 = bitcast <4 x i64>* %105 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %106, align 32
  %107 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 23
  %108 = bitcast <4 x i64>* %107 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %108, align 32
  %109 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 24
  %110 = bitcast <4 x i64>* %109 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %110, align 32
  %111 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 25
  %112 = bitcast <4 x i64>* %111 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %112, align 32
  %113 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 26
  %114 = bitcast <4 x i64>* %113 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %114, align 32
  %115 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 27
  %116 = bitcast <4 x i64>* %115 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %116, align 32
  %117 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 28
  %118 = bitcast <4 x i64>* %117 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %118, align 32
  %119 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 29
  %120 = bitcast <4 x i64>* %119 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %120, align 32
  %121 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 30
  %122 = bitcast <4 x i64>* %121 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %122, align 32
  %123 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 31
  %124 = bitcast <4 x i64>* %123 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %124, align 32
  %125 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 32
  %126 = bitcast <4 x i64>* %125 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %126, align 32
  %127 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 33
  %128 = bitcast <4 x i64>* %127 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %128, align 32
  %129 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 34
  %130 = bitcast <4 x i64>* %129 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %130, align 32
  %131 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 35
  %132 = bitcast <4 x i64>* %131 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %132, align 32
  %133 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 36
  %134 = bitcast <4 x i64>* %133 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %134, align 32
  %135 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 37
  %136 = bitcast <4 x i64>* %135 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %136, align 32
  %137 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 38
  %138 = bitcast <4 x i64>* %137 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %138, align 32
  %139 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 39
  %140 = bitcast <4 x i64>* %139 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %140, align 32
  %141 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 40
  %142 = bitcast <4 x i64>* %141 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %142, align 32
  %143 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 41
  %144 = bitcast <4 x i64>* %143 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %144, align 32
  %145 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 42
  %146 = bitcast <4 x i64>* %145 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %146, align 32
  %147 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 43
  %148 = bitcast <4 x i64>* %147 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %148, align 32
  %149 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 44
  %150 = bitcast <4 x i64>* %149 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %150, align 32
  %151 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 45
  %152 = bitcast <4 x i64>* %151 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %152, align 32
  %153 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 46
  %154 = bitcast <4 x i64>* %153 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %154, align 32
  %155 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 47
  %156 = bitcast <4 x i64>* %155 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %156, align 32
  %157 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 48
  %158 = bitcast <4 x i64>* %157 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %158, align 32
  %159 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 49
  %160 = bitcast <4 x i64>* %159 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %160, align 32
  %161 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 50
  %162 = bitcast <4 x i64>* %161 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %162, align 32
  %163 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 51
  %164 = bitcast <4 x i64>* %163 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %164, align 32
  %165 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 52
  %166 = bitcast <4 x i64>* %165 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %166, align 32
  %167 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 53
  %168 = bitcast <4 x i64>* %167 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %168, align 32
  %169 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 54
  %170 = bitcast <4 x i64>* %169 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %170, align 32
  %171 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 55
  %172 = bitcast <4 x i64>* %171 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %172, align 32
  %173 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 56
  %174 = bitcast <4 x i64>* %173 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %174, align 32
  %175 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 57
  %176 = bitcast <4 x i64>* %175 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %176, align 32
  %177 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 58
  %178 = bitcast <4 x i64>* %177 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %178, align 32
  %179 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 59
  %180 = bitcast <4 x i64>* %179 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %180, align 32
  %181 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 60
  %182 = bitcast <4 x i64>* %181 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %182, align 32
  %183 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 61
  %184 = bitcast <4 x i64>* %183 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %184, align 32
  %185 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 62
  %186 = bitcast <4 x i64>* %185 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %186, align 32
  %187 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 63
  %188 = bitcast <4 x i64>* %187 to <8 x i32>*
  store <8 x i32> %61, <8 x i32>* %188, align 32
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @idct64_low8_avx2(<4 x i64>* nocapture readonly, <4 x i64>*, i32, i32, i32, i32) #0 {
  %7 = alloca <4 x i64>, align 32
  %8 = alloca <4 x i64>, align 32
  %9 = alloca <4 x i64>, align 32
  %10 = alloca <4 x i64>, align 32
  %11 = alloca <4 x i64>, align 32
  %12 = alloca [64 x <4 x i64>], align 32
  %13 = add nsw i32 %2, -10
  %14 = sext i32 %13 to i64
  %15 = bitcast <4 x i64>* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %15) #8
  %16 = add nsw i32 %2, -1
  %17 = shl i32 1, %16
  %18 = insertelement <8 x i32> undef, i32 %17, i32 0
  %19 = shufflevector <8 x i32> %18, <8 x i32> undef, <8 x i32> zeroinitializer
  %20 = bitcast <4 x i64>* %7 to <8 x i32>*
  store <8 x i32> %19, <8 x i32>* %20, align 32
  %21 = icmp eq i32 %3, 0
  %22 = select i1 %21, i32 8, i32 6
  %23 = add nsw i32 %22, %4
  %24 = icmp slt i32 %23, 16
  %25 = add i32 %23, -1
  %26 = bitcast <4 x i64>* %8 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %26) #8
  %27 = shl i32 1, %25
  %28 = select i1 %24, i32 32768, i32 %27
  %29 = sub nsw i32 0, %28
  %30 = insertelement <8 x i32> undef, i32 %29, i32 0
  %31 = shufflevector <8 x i32> %30, <8 x i32> undef, <8 x i32> zeroinitializer
  %32 = bitcast <4 x i64>* %8 to <8 x i32>*
  store <8 x i32> %31, <8 x i32>* %32, align 32
  %33 = bitcast <4 x i64>* %9 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %33) #8
  %34 = add nsw i32 %28, -1
  %35 = insertelement <8 x i32> undef, i32 %34, i32 0
  %36 = shufflevector <8 x i32> %35, <8 x i32> undef, <8 x i32> zeroinitializer
  %37 = bitcast <4 x i64>* %9 to <8 x i32>*
  store <8 x i32> %36, <8 x i32>* %37, align 32
  %38 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 1
  %39 = load i32, i32* %38, align 4
  %40 = insertelement <8 x i32> undef, i32 %39, i32 0
  %41 = shufflevector <8 x i32> %40, <8 x i32> undef, <8 x i32> zeroinitializer
  %42 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 2
  %43 = load i32, i32* %42, align 8
  %44 = insertelement <8 x i32> undef, i32 %43, i32 0
  %45 = shufflevector <8 x i32> %44, <8 x i32> undef, <8 x i32> zeroinitializer
  %46 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 3
  %47 = load i32, i32* %46, align 4
  %48 = insertelement <8 x i32> undef, i32 %47, i32 0
  %49 = shufflevector <8 x i32> %48, <8 x i32> undef, <8 x i32> zeroinitializer
  %50 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 4
  %51 = load i32, i32* %50, align 16
  %52 = insertelement <8 x i32> undef, i32 %51, i32 0
  %53 = shufflevector <8 x i32> %52, <8 x i32> undef, <8 x i32> zeroinitializer
  %54 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 6
  %55 = load i32, i32* %54, align 8
  %56 = insertelement <8 x i32> undef, i32 %55, i32 0
  %57 = shufflevector <8 x i32> %56, <8 x i32> undef, <8 x i32> zeroinitializer
  %58 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 8
  %59 = load i32, i32* %58, align 16
  %60 = insertelement <8 x i32> undef, i32 %59, i32 0
  %61 = shufflevector <8 x i32> %60, <8 x i32> undef, <8 x i32> zeroinitializer
  %62 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 12
  %63 = load i32, i32* %62, align 16
  %64 = insertelement <8 x i32> undef, i32 %63, i32 0
  %65 = shufflevector <8 x i32> %64, <8 x i32> undef, <8 x i32> zeroinitializer
  %66 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 16
  %67 = load i32, i32* %66, align 16
  %68 = insertelement <8 x i32> undef, i32 %67, i32 0
  %69 = shufflevector <8 x i32> %68, <8 x i32> undef, <8 x i32> zeroinitializer
  %70 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 20
  %71 = load i32, i32* %70, align 16
  %72 = insertelement <8 x i32> undef, i32 %71, i32 0
  %73 = shufflevector <8 x i32> %72, <8 x i32> undef, <8 x i32> zeroinitializer
  %74 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 24
  %75 = load i32, i32* %74, align 16
  %76 = insertelement <8 x i32> undef, i32 %75, i32 0
  %77 = shufflevector <8 x i32> %76, <8 x i32> undef, <8 x i32> zeroinitializer
  %78 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 28
  %79 = load i32, i32* %78, align 16
  %80 = insertelement <8 x i32> undef, i32 %79, i32 0
  %81 = shufflevector <8 x i32> %80, <8 x i32> undef, <8 x i32> zeroinitializer
  %82 = bitcast <4 x i64>* %10 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %82) #8
  %83 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 32
  %84 = load i32, i32* %83, align 16
  %85 = insertelement <8 x i32> undef, i32 %84, i32 0
  %86 = shufflevector <8 x i32> %85, <8 x i32> undef, <8 x i32> zeroinitializer
  %87 = bitcast <4 x i64>* %10 to <8 x i32>*
  store <8 x i32> %86, <8 x i32>* %87, align 32
  %88 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 40
  %89 = load i32, i32* %88, align 16
  %90 = insertelement <8 x i32> undef, i32 %89, i32 0
  %91 = shufflevector <8 x i32> %90, <8 x i32> undef, <8 x i32> zeroinitializer
  %92 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 44
  %93 = load i32, i32* %92, align 16
  %94 = insertelement <8 x i32> undef, i32 %93, i32 0
  %95 = shufflevector <8 x i32> %94, <8 x i32> undef, <8 x i32> zeroinitializer
  %96 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 48
  %97 = load i32, i32* %96, align 16
  %98 = insertelement <8 x i32> undef, i32 %97, i32 0
  %99 = shufflevector <8 x i32> %98, <8 x i32> undef, <8 x i32> zeroinitializer
  %100 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 56
  %101 = load i32, i32* %100, align 16
  %102 = insertelement <8 x i32> undef, i32 %101, i32 0
  %103 = shufflevector <8 x i32> %102, <8 x i32> undef, <8 x i32> zeroinitializer
  %104 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 60
  %105 = load i32, i32* %104, align 16
  %106 = insertelement <8 x i32> undef, i32 %105, i32 0
  %107 = shufflevector <8 x i32> %106, <8 x i32> undef, <8 x i32> zeroinitializer
  %108 = sub nsw i32 0, %51
  %109 = insertelement <8 x i32> undef, i32 %108, i32 0
  %110 = shufflevector <8 x i32> %109, <8 x i32> undef, <8 x i32> zeroinitializer
  %111 = sub nsw i32 0, %59
  %112 = insertelement <8 x i32> undef, i32 %111, i32 0
  %113 = shufflevector <8 x i32> %112, <8 x i32> undef, <8 x i32> zeroinitializer
  %114 = sub nsw i32 0, %63
  %115 = insertelement <8 x i32> undef, i32 %114, i32 0
  %116 = shufflevector <8 x i32> %115, <8 x i32> undef, <8 x i32> zeroinitializer
  %117 = sub nsw i32 0, %67
  %118 = insertelement <8 x i32> undef, i32 %117, i32 0
  %119 = shufflevector <8 x i32> %118, <8 x i32> undef, <8 x i32> zeroinitializer
  %120 = sub nsw i32 0, %71
  %121 = insertelement <8 x i32> undef, i32 %120, i32 0
  %122 = shufflevector <8 x i32> %121, <8 x i32> undef, <8 x i32> zeroinitializer
  %123 = sub nsw i32 0, %75
  %124 = insertelement <8 x i32> undef, i32 %123, i32 0
  %125 = shufflevector <8 x i32> %124, <8 x i32> undef, <8 x i32> zeroinitializer
  %126 = sub nsw i32 0, %79
  %127 = insertelement <8 x i32> undef, i32 %126, i32 0
  %128 = shufflevector <8 x i32> %127, <8 x i32> undef, <8 x i32> zeroinitializer
  %129 = bitcast <4 x i64>* %11 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %129) #8
  %130 = sub nsw i32 0, %84
  %131 = insertelement <8 x i32> undef, i32 %130, i32 0
  %132 = shufflevector <8 x i32> %131, <8 x i32> undef, <8 x i32> zeroinitializer
  %133 = bitcast <4 x i64>* %11 to <8 x i32>*
  store <8 x i32> %132, <8 x i32>* %133, align 32
  %134 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 36
  %135 = load i32, i32* %134, align 16
  %136 = sub nsw i32 0, %135
  %137 = insertelement <8 x i32> undef, i32 %136, i32 0
  %138 = shufflevector <8 x i32> %137, <8 x i32> undef, <8 x i32> zeroinitializer
  %139 = sub nsw i32 0, %89
  %140 = insertelement <8 x i32> undef, i32 %139, i32 0
  %141 = shufflevector <8 x i32> %140, <8 x i32> undef, <8 x i32> zeroinitializer
  %142 = sub nsw i32 0, %97
  %143 = insertelement <8 x i32> undef, i32 %142, i32 0
  %144 = shufflevector <8 x i32> %143, <8 x i32> undef, <8 x i32> zeroinitializer
  %145 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 52
  %146 = load i32, i32* %145, align 16
  %147 = sub nsw i32 0, %146
  %148 = insertelement <8 x i32> undef, i32 %147, i32 0
  %149 = shufflevector <8 x i32> %148, <8 x i32> undef, <8 x i32> zeroinitializer
  %150 = sub nsw i32 0, %101
  %151 = insertelement <8 x i32> undef, i32 %150, i32 0
  %152 = shufflevector <8 x i32> %151, <8 x i32> undef, <8 x i32> zeroinitializer
  %153 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 63
  %154 = load i32, i32* %153, align 4
  %155 = insertelement <8 x i32> undef, i32 %154, i32 0
  %156 = shufflevector <8 x i32> %155, <8 x i32> undef, <8 x i32> zeroinitializer
  %157 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 57
  %158 = load i32, i32* %157, align 4
  %159 = sub nsw i32 0, %158
  %160 = insertelement <8 x i32> undef, i32 %159, i32 0
  %161 = shufflevector <8 x i32> %160, <8 x i32> undef, <8 x i32> zeroinitializer
  %162 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 7
  %163 = load i32, i32* %162, align 4
  %164 = insertelement <8 x i32> undef, i32 %163, i32 0
  %165 = shufflevector <8 x i32> %164, <8 x i32> undef, <8 x i32> zeroinitializer
  %166 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 5
  %167 = load i32, i32* %166, align 4
  %168 = insertelement <8 x i32> undef, i32 %167, i32 0
  %169 = shufflevector <8 x i32> %168, <8 x i32> undef, <8 x i32> zeroinitializer
  %170 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 59
  %171 = load i32, i32* %170, align 4
  %172 = insertelement <8 x i32> undef, i32 %171, i32 0
  %173 = shufflevector <8 x i32> %172, <8 x i32> undef, <8 x i32> zeroinitializer
  %174 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 61
  %175 = load i32, i32* %174, align 4
  %176 = sub nsw i32 0, %175
  %177 = insertelement <8 x i32> undef, i32 %176, i32 0
  %178 = shufflevector <8 x i32> %177, <8 x i32> undef, <8 x i32> zeroinitializer
  %179 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 58
  %180 = load i32, i32* %179, align 8
  %181 = sub nsw i32 0, %180
  %182 = insertelement <8 x i32> undef, i32 %181, i32 0
  %183 = shufflevector <8 x i32> %182, <8 x i32> undef, <8 x i32> zeroinitializer
  %184 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 62
  %185 = load i32, i32* %184, align 8
  %186 = insertelement <8 x i32> undef, i32 %185, i32 0
  %187 = shufflevector <8 x i32> %186, <8 x i32> undef, <8 x i32> zeroinitializer
  %188 = bitcast [64 x <4 x i64>]* %12 to i8*
  call void @llvm.lifetime.start.p0i8(i64 2048, i8* nonnull %188) #8
  %189 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 1
  %190 = bitcast <4 x i64>* %189 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %190, i8 -86, i64 1984, i1 false)
  %191 = load <4 x i64>, <4 x i64>* %0, align 32
  %192 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 0
  store <4 x i64> %191, <4 x i64>* %192, align 32
  %193 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 4
  %194 = load <4 x i64>, <4 x i64>* %193, align 32
  %195 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 8
  store <4 x i64> %194, <4 x i64>* %195, align 32
  %196 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 2
  %197 = bitcast <4 x i64>* %196 to <8 x i32>*
  %198 = load <8 x i32>, <8 x i32>* %197, align 32
  %199 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 16
  %200 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 6
  %201 = bitcast <4 x i64>* %200 to <8 x i32>*
  %202 = load <8 x i32>, <8 x i32>* %201, align 32
  %203 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 24
  %204 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 1
  %205 = bitcast <4 x i64>* %204 to <8 x i32>*
  %206 = load <8 x i32>, <8 x i32>* %205, align 32
  %207 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 32
  %208 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 5
  %209 = bitcast <4 x i64>* %208 to <8 x i32>*
  %210 = load <8 x i32>, <8 x i32>* %209, align 32
  %211 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 40
  %212 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 3
  %213 = bitcast <4 x i64>* %212 to <8 x i32>*
  %214 = load <8 x i32>, <8 x i32>* %213, align 32
  %215 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 48
  %216 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 7
  %217 = bitcast <4 x i64>* %216 to <8 x i32>*
  %218 = load <8 x i32>, <8 x i32>* %217, align 32
  %219 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 56
  %220 = bitcast <4 x i64>* %207 to <8 x i32>*
  %221 = mul <8 x i32> %41, %206
  %222 = load <8 x i32>, <8 x i32>* %20, align 32
  %223 = add <8 x i32> %222, %221
  %224 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %223, i32 %2) #8
  %225 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 63
  %226 = bitcast <4 x i64>* %225 to <8 x i32>*
  store <8 x i32> %224, <8 x i32>* %226, align 32
  %227 = mul <8 x i32> %156, %206
  %228 = add <8 x i32> %222, %227
  %229 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %228, i32 %2) #8
  store <8 x i32> %229, <8 x i32>* %220, align 32
  %230 = bitcast <4 x i64>* %219 to <8 x i32>*
  %231 = mul <8 x i32> %161, %218
  %232 = add <8 x i32> %231, %222
  %233 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %232, i32 %2) #8
  %234 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 39
  %235 = bitcast <4 x i64>* %234 to <8 x i32>*
  store <8 x i32> %233, <8 x i32>* %235, align 32
  %236 = mul <8 x i32> %165, %218
  %237 = add <8 x i32> %236, %222
  %238 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %237, i32 %2) #8
  store <8 x i32> %238, <8 x i32>* %230, align 32
  %239 = bitcast <4 x i64>* %211 to <8 x i32>*
  %240 = mul <8 x i32> %169, %210
  %241 = add <8 x i32> %240, %222
  %242 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %241, i32 %2) #8
  %243 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 55
  %244 = bitcast <4 x i64>* %243 to <8 x i32>*
  store <8 x i32> %242, <8 x i32>* %244, align 32
  %245 = mul <8 x i32> %173, %210
  %246 = add <8 x i32> %245, %222
  %247 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %246, i32 %2) #8
  store <8 x i32> %247, <8 x i32>* %239, align 32
  %248 = bitcast <4 x i64>* %215 to <8 x i32>*
  %249 = mul <8 x i32> %178, %214
  %250 = add <8 x i32> %249, %222
  %251 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %250, i32 %2) #8
  %252 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 47
  %253 = bitcast <4 x i64>* %252 to <8 x i32>*
  store <8 x i32> %251, <8 x i32>* %253, align 32
  %254 = mul <8 x i32> %49, %214
  %255 = add <8 x i32> %254, %222
  %256 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %255, i32 %2) #8
  store <8 x i32> %256, <8 x i32>* %248, align 32
  %257 = bitcast <4 x i64>* %199 to <8 x i32>*
  %258 = mul <8 x i32> %45, %198
  %259 = add <8 x i32> %258, %222
  %260 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %259, i32 %2) #8
  %261 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 31
  %262 = bitcast <4 x i64>* %261 to <8 x i32>*
  store <8 x i32> %260, <8 x i32>* %262, align 32
  %263 = mul <8 x i32> %187, %198
  %264 = add <8 x i32> %263, %222
  %265 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %264, i32 %2) #8
  store <8 x i32> %265, <8 x i32>* %257, align 32
  %266 = bitcast <4 x i64>* %203 to <8 x i32>*
  %267 = mul <8 x i32> %183, %202
  %268 = add <8 x i32> %267, %222
  %269 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %268, i32 %2) #8
  %270 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 23
  %271 = bitcast <4 x i64>* %270 to <8 x i32>*
  store <8 x i32> %269, <8 x i32>* %271, align 32
  %272 = mul <8 x i32> %57, %202
  %273 = add <8 x i32> %272, %222
  %274 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %273, i32 %2) #8
  store <8 x i32> %274, <8 x i32>* %266, align 32
  %275 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 33
  %276 = bitcast <4 x i64>* %275 to <8 x i32>*
  %277 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 38
  %278 = bitcast <4 x i64>* %277 to <8 x i32>*
  %279 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 41
  %280 = bitcast <4 x i64>* %279 to <8 x i32>*
  %281 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 46
  %282 = bitcast <4 x i64>* %281 to <8 x i32>*
  %283 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 49
  %284 = bitcast <4 x i64>* %283 to <8 x i32>*
  %285 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 54
  %286 = bitcast <4 x i64>* %285 to <8 x i32>*
  %287 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 57
  %288 = bitcast <4 x i64>* %287 to <8 x i32>*
  %289 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 62
  %290 = bitcast <4 x i64>* %289 to <8 x i32>*
  %291 = bitcast <4 x i64>* %195 to <8 x i32>*
  %292 = load <8 x i32>, <8 x i32>* %291, align 32
  %293 = mul <8 x i32> %292, %53
  %294 = add <8 x i32> %293, %222
  %295 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %294, i32 %2) #8
  %296 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 15
  %297 = bitcast <4 x i64>* %296 to <8 x i32>*
  store <8 x i32> %295, <8 x i32>* %297, align 32
  %298 = mul <8 x i32> %292, %107
  %299 = add <8 x i32> %298, %222
  %300 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %299, i32 %2) #8
  store <8 x i32> %300, <8 x i32>* %291, align 32
  %301 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 17
  %302 = bitcast <4 x i64>* %301 to <8 x i32>*
  %303 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 22
  %304 = bitcast <4 x i64>* %303 to <8 x i32>*
  %305 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 25
  %306 = bitcast <4 x i64>* %305 to <8 x i32>*
  %307 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 30
  %308 = bitcast <4 x i64>* %307 to <8 x i32>*
  %309 = mul <8 x i32> %229, %110
  %310 = mul <8 x i32> %224, %107
  %311 = add <8 x i32> %309, %222
  %312 = add <8 x i32> %311, %310
  %313 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %312, i32 %2) #8
  %314 = mul <8 x i32> %229, %107
  %315 = mul <8 x i32> %224, %53
  %316 = add <8 x i32> %314, %222
  %317 = add <8 x i32> %316, %315
  %318 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %317, i32 %2) #8
  store <8 x i32> %318, <8 x i32>* %290, align 32
  store <8 x i32> %313, <8 x i32>* %276, align 32
  %319 = mul <8 x i32> %233, %138
  %320 = mul <8 x i32> %238, %81
  %321 = add <8 x i32> %319, %222
  %322 = add <8 x i32> %321, %320
  %323 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %322, i32 %2) #8
  %324 = mul <8 x i32> %233, %128
  %325 = mul <8 x i32> %238, %138
  %326 = add <8 x i32> %324, %222
  %327 = add <8 x i32> %326, %325
  %328 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %327, i32 %2) #8
  store <8 x i32> %328, <8 x i32>* %278, align 32
  store <8 x i32> %323, <8 x i32>* %288, align 32
  %329 = mul <8 x i32> %247, %122
  %330 = mul <8 x i32> %242, %95
  %331 = add <8 x i32> %329, %222
  %332 = add <8 x i32> %331, %330
  %333 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %332, i32 %2) #8
  %334 = mul <8 x i32> %247, %95
  %335 = mul <8 x i32> %242, %73
  %336 = add <8 x i32> %334, %222
  %337 = add <8 x i32> %336, %335
  %338 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %337, i32 %2) #8
  store <8 x i32> %338, <8 x i32>* %286, align 32
  store <8 x i32> %333, <8 x i32>* %280, align 32
  %339 = mul <8 x i32> %251, %116
  %340 = mul <8 x i32> %256, %149
  %341 = add <8 x i32> %339, %222
  %342 = add <8 x i32> %341, %340
  %343 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %342, i32 %2) #8
  %344 = mul <8 x i32> %251, %149
  %345 = mul <8 x i32> %256, %65
  %346 = add <8 x i32> %344, %222
  %347 = add <8 x i32> %346, %345
  %348 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %347, i32 %2) #8
  store <8 x i32> %348, <8 x i32>* %284, align 32
  store <8 x i32> %343, <8 x i32>* %282, align 32
  %349 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 9
  %350 = bitcast <4 x i64>* %349 to <8 x i32>*
  %351 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 14
  %352 = bitcast <4 x i64>* %351 to <8 x i32>*
  %353 = mul <8 x i32> %265, %113
  %354 = mul <8 x i32> %260, %103
  %355 = add <8 x i32> %353, %222
  %356 = add <8 x i32> %355, %354
  %357 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %356, i32 %2) #8
  %358 = mul <8 x i32> %265, %103
  %359 = mul <8 x i32> %260, %61
  %360 = add <8 x i32> %358, %222
  %361 = add <8 x i32> %360, %359
  %362 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %361, i32 %2) #8
  store <8 x i32> %362, <8 x i32>* %308, align 32
  store <8 x i32> %357, <8 x i32>* %302, align 32
  %363 = mul <8 x i32> %269, %125
  %364 = mul <8 x i32> %274, %141
  %365 = add <8 x i32> %363, %222
  %366 = add <8 x i32> %365, %364
  %367 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %366, i32 %2) #8
  %368 = mul <8 x i32> %269, %141
  %369 = mul <8 x i32> %274, %77
  %370 = add <8 x i32> %368, %222
  %371 = add <8 x i32> %370, %369
  %372 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %371, i32 %2) #8
  store <8 x i32> %372, <8 x i32>* %306, align 32
  store <8 x i32> %367, <8 x i32>* %304, align 32
  %373 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 35
  %374 = bitcast <4 x i64>* %373 to <8 x i32>*
  store <8 x i32> %229, <8 x i32>* %374, align 32
  %375 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 34
  %376 = bitcast <4 x i64>* %375 to <8 x i32>*
  %377 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 36
  %378 = bitcast <4 x i64>* %377 to <8 x i32>*
  store <8 x i32> %233, <8 x i32>* %378, align 32
  %379 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 37
  %380 = bitcast <4 x i64>* %379 to <8 x i32>*
  store <8 x i32> %328, <8 x i32>* %380, align 32
  %381 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 43
  %382 = bitcast <4 x i64>* %381 to <8 x i32>*
  store <8 x i32> %247, <8 x i32>* %382, align 32
  %383 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 42
  %384 = bitcast <4 x i64>* %383 to <8 x i32>*
  store <8 x i32> %333, <8 x i32>* %384, align 32
  %385 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 44
  %386 = bitcast <4 x i64>* %385 to <8 x i32>*
  store <8 x i32> %251, <8 x i32>* %386, align 32
  %387 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 45
  %388 = bitcast <4 x i64>* %387 to <8 x i32>*
  store <8 x i32> %343, <8 x i32>* %388, align 32
  %389 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 51
  %390 = bitcast <4 x i64>* %389 to <8 x i32>*
  store <8 x i32> %256, <8 x i32>* %390, align 32
  %391 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 50
  %392 = bitcast <4 x i64>* %391 to <8 x i32>*
  store <8 x i32> %348, <8 x i32>* %392, align 32
  %393 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 52
  %394 = bitcast <4 x i64>* %393 to <8 x i32>*
  store <8 x i32> %242, <8 x i32>* %394, align 32
  %395 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 53
  %396 = bitcast <4 x i64>* %395 to <8 x i32>*
  store <8 x i32> %338, <8 x i32>* %396, align 32
  %397 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 59
  %398 = bitcast <4 x i64>* %397 to <8 x i32>*
  %399 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 58
  %400 = bitcast <4 x i64>* %399 to <8 x i32>*
  store <8 x i32> %323, <8 x i32>* %400, align 32
  %401 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 60
  %402 = bitcast <4 x i64>* %401 to <8 x i32>*
  %403 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 61
  %404 = bitcast <4 x i64>* %403 to <8 x i32>*
  %405 = load <8 x i32>, <8 x i32>* %87, align 32
  %406 = bitcast [64 x <4 x i64>]* %12 to <8 x i32>*
  %407 = load <8 x i32>, <8 x i32>* %406, align 32
  %408 = mul <8 x i32> %407, %405
  %409 = add <8 x i32> %408, %222
  %410 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %409, i32 %2) #8
  %411 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 1
  %412 = bitcast <4 x i64>* %411 to <8 x i32>*
  store <8 x i32> %410, <8 x i32>* %412, align 32
  store <8 x i32> %410, <8 x i32>* %406, align 32
  %413 = mul <8 x i32> %300, %119
  %414 = mul <8 x i32> %295, %99
  %415 = add <8 x i32> %413, %222
  %416 = add <8 x i32> %415, %414
  %417 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %416, i32 %2) #8
  %418 = mul <8 x i32> %300, %99
  %419 = mul <8 x i32> %295, %69
  %420 = add <8 x i32> %418, %222
  %421 = add <8 x i32> %420, %419
  %422 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %421, i32 %2) #8
  store <8 x i32> %422, <8 x i32>* %352, align 32
  store <8 x i32> %417, <8 x i32>* %350, align 32
  %423 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 19
  %424 = bitcast <4 x i64>* %423 to <8 x i32>*
  store <8 x i32> %265, <8 x i32>* %424, align 32
  %425 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 18
  %426 = bitcast <4 x i64>* %425 to <8 x i32>*
  store <8 x i32> %357, <8 x i32>* %426, align 32
  %427 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 20
  %428 = bitcast <4 x i64>* %427 to <8 x i32>*
  store <8 x i32> %269, <8 x i32>* %428, align 32
  %429 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 21
  %430 = bitcast <4 x i64>* %429 to <8 x i32>*
  store <8 x i32> %367, <8 x i32>* %430, align 32
  %431 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 27
  %432 = bitcast <4 x i64>* %431 to <8 x i32>*
  store <8 x i32> %274, <8 x i32>* %432, align 32
  %433 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 26
  %434 = bitcast <4 x i64>* %433 to <8 x i32>*
  store <8 x i32> %372, <8 x i32>* %434, align 32
  %435 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 28
  %436 = bitcast <4 x i64>* %435 to <8 x i32>*
  store <8 x i32> %260, <8 x i32>* %436, align 32
  %437 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 29
  %438 = bitcast <4 x i64>* %437 to <8 x i32>*
  store <8 x i32> %362, <8 x i32>* %438, align 32
  %439 = mul <8 x i32> %313, %113
  %440 = mul <8 x i32> %318, %103
  %441 = add <8 x i32> %439, %222
  %442 = add <8 x i32> %441, %440
  %443 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %442, i32 %2) #8
  %444 = mul <8 x i32> %313, %103
  %445 = mul <8 x i32> %318, %61
  %446 = add <8 x i32> %444, %222
  %447 = add <8 x i32> %446, %445
  %448 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %447, i32 %2) #8
  store <8 x i32> %448, <8 x i32>* %404, align 32
  store <8 x i32> %443, <8 x i32>* %376, align 32
  %449 = load <8 x i32>, <8 x i32>* %374, align 32
  %450 = mul <8 x i32> %449, %113
  %451 = mul <8 x i32> %224, %103
  %452 = add <8 x i32> %450, %222
  %453 = add <8 x i32> %452, %451
  %454 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %453, i32 %2) #8
  %455 = mul <8 x i32> %449, %103
  %456 = mul <8 x i32> %224, %61
  %457 = add <8 x i32> %455, %222
  %458 = add <8 x i32> %457, %456
  %459 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %458, i32 %2) #8
  store <8 x i32> %459, <8 x i32>* %402, align 32
  store <8 x i32> %454, <8 x i32>* %374, align 32
  %460 = load <8 x i32>, <8 x i32>* %378, align 32
  %461 = mul <8 x i32> %460, %152
  %462 = mul <8 x i32> %238, %113
  %463 = add <8 x i32> %461, %222
  %464 = add <8 x i32> %463, %462
  %465 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %464, i32 %2) #8
  %466 = mul <8 x i32> %460, %113
  %467 = mul <8 x i32> %238, %103
  %468 = add <8 x i32> %466, %222
  %469 = add <8 x i32> %468, %467
  %470 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %469, i32 %2) #8
  store <8 x i32> %470, <8 x i32>* %398, align 32
  store <8 x i32> %465, <8 x i32>* %378, align 32
  %471 = load <8 x i32>, <8 x i32>* %380, align 32
  %472 = mul <8 x i32> %471, %152
  %473 = mul <8 x i32> %323, %113
  %474 = add <8 x i32> %472, %222
  %475 = add <8 x i32> %474, %473
  %476 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %475, i32 %2) #8
  %477 = mul <8 x i32> %471, %113
  %478 = mul <8 x i32> %323, %103
  %479 = add <8 x i32> %477, %222
  %480 = add <8 x i32> %479, %478
  %481 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %480, i32 %2) #8
  store <8 x i32> %481, <8 x i32>* %400, align 32
  store <8 x i32> %476, <8 x i32>* %380, align 32
  %482 = load <8 x i32>, <8 x i32>* %384, align 32
  %483 = mul <8 x i32> %482, %141
  %484 = load <8 x i32>, <8 x i32>* %396, align 32
  %485 = mul <8 x i32> %484, %77
  %486 = add <8 x i32> %483, %222
  %487 = add <8 x i32> %486, %485
  %488 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %487, i32 %2) #8
  %489 = mul <8 x i32> %482, %77
  %490 = mul <8 x i32> %484, %91
  %491 = add <8 x i32> %489, %222
  %492 = add <8 x i32> %491, %490
  %493 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %492, i32 %2) #8
  store <8 x i32> %493, <8 x i32>* %396, align 32
  store <8 x i32> %488, <8 x i32>* %384, align 32
  %494 = load <8 x i32>, <8 x i32>* %382, align 32
  %495 = mul <8 x i32> %494, %141
  %496 = load <8 x i32>, <8 x i32>* %394, align 32
  %497 = mul <8 x i32> %496, %77
  %498 = add <8 x i32> %495, %222
  %499 = add <8 x i32> %498, %497
  %500 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %499, i32 %2) #8
  %501 = mul <8 x i32> %494, %77
  %502 = mul <8 x i32> %496, %91
  %503 = add <8 x i32> %501, %222
  %504 = add <8 x i32> %503, %502
  %505 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %504, i32 %2) #8
  store <8 x i32> %505, <8 x i32>* %394, align 32
  store <8 x i32> %500, <8 x i32>* %382, align 32
  %506 = load <8 x i32>, <8 x i32>* %386, align 32
  %507 = mul <8 x i32> %506, %125
  %508 = load <8 x i32>, <8 x i32>* %390, align 32
  %509 = mul <8 x i32> %508, %141
  %510 = add <8 x i32> %507, %222
  %511 = add <8 x i32> %510, %509
  %512 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %511, i32 %2) #8
  %513 = mul <8 x i32> %506, %141
  %514 = mul <8 x i32> %508, %77
  %515 = add <8 x i32> %513, %222
  %516 = add <8 x i32> %515, %514
  %517 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %516, i32 %2) #8
  store <8 x i32> %517, <8 x i32>* %390, align 32
  store <8 x i32> %512, <8 x i32>* %386, align 32
  %518 = load <8 x i32>, <8 x i32>* %388, align 32
  %519 = mul <8 x i32> %518, %125
  %520 = load <8 x i32>, <8 x i32>* %392, align 32
  %521 = mul <8 x i32> %520, %141
  %522 = add <8 x i32> %519, %222
  %523 = add <8 x i32> %522, %521
  %524 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %523, i32 %2) #8
  %525 = mul <8 x i32> %518, %141
  %526 = mul <8 x i32> %520, %77
  %527 = add <8 x i32> %525, %222
  %528 = add <8 x i32> %527, %526
  %529 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %528, i32 %2) #8
  store <8 x i32> %529, <8 x i32>* %392, align 32
  store <8 x i32> %524, <8 x i32>* %388, align 32
  %530 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 3
  %531 = bitcast <4 x i64>* %530 to <8 x i32>*
  store <8 x i32> %410, <8 x i32>* %531, align 32
  %532 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 2
  %533 = bitcast <4 x i64>* %532 to <8 x i32>*
  store <8 x i32> %410, <8 x i32>* %533, align 32
  %534 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 11
  %535 = bitcast <4 x i64>* %534 to <8 x i32>*
  store <8 x i32> %300, <8 x i32>* %535, align 32
  %536 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 10
  %537 = bitcast <4 x i64>* %536 to <8 x i32>*
  store <8 x i32> %417, <8 x i32>* %537, align 32
  %538 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 12
  %539 = bitcast <4 x i64>* %538 to <8 x i32>*
  store <8 x i32> %295, <8 x i32>* %539, align 32
  %540 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 13
  %541 = bitcast <4 x i64>* %540 to <8 x i32>*
  store <8 x i32> %422, <8 x i32>* %541, align 32
  %542 = load <8 x i32>, <8 x i32>* %426, align 32
  %543 = mul <8 x i32> %542, %119
  %544 = load <8 x i32>, <8 x i32>* %438, align 32
  %545 = mul <8 x i32> %544, %99
  %546 = add <8 x i32> %543, %222
  %547 = add <8 x i32> %546, %545
  %548 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %547, i32 %2) #8
  %549 = mul <8 x i32> %542, %99
  %550 = mul <8 x i32> %544, %69
  %551 = add <8 x i32> %549, %222
  %552 = add <8 x i32> %551, %550
  %553 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %552, i32 %2) #8
  store <8 x i32> %553, <8 x i32>* %438, align 32
  store <8 x i32> %548, <8 x i32>* %426, align 32
  %554 = load <8 x i32>, <8 x i32>* %424, align 32
  %555 = mul <8 x i32> %554, %119
  %556 = load <8 x i32>, <8 x i32>* %436, align 32
  %557 = mul <8 x i32> %556, %99
  %558 = add <8 x i32> %555, %222
  %559 = add <8 x i32> %558, %557
  %560 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %559, i32 %2) #8
  %561 = mul <8 x i32> %554, %99
  %562 = mul <8 x i32> %556, %69
  %563 = add <8 x i32> %561, %222
  %564 = add <8 x i32> %563, %562
  %565 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %564, i32 %2) #8
  store <8 x i32> %565, <8 x i32>* %436, align 32
  store <8 x i32> %560, <8 x i32>* %424, align 32
  %566 = load <8 x i32>, <8 x i32>* %428, align 32
  %567 = mul <8 x i32> %566, %144
  %568 = load <8 x i32>, <8 x i32>* %432, align 32
  %569 = mul <8 x i32> %568, %119
  %570 = add <8 x i32> %567, %222
  %571 = add <8 x i32> %570, %569
  %572 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %571, i32 %2) #8
  %573 = mul <8 x i32> %566, %119
  %574 = mul <8 x i32> %568, %99
  %575 = add <8 x i32> %573, %222
  %576 = add <8 x i32> %575, %574
  %577 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %576, i32 %2) #8
  store <8 x i32> %577, <8 x i32>* %432, align 32
  store <8 x i32> %572, <8 x i32>* %428, align 32
  %578 = load <8 x i32>, <8 x i32>* %430, align 32
  %579 = mul <8 x i32> %578, %144
  %580 = load <8 x i32>, <8 x i32>* %434, align 32
  %581 = mul <8 x i32> %580, %119
  %582 = add <8 x i32> %579, %222
  %583 = add <8 x i32> %582, %581
  %584 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %583, i32 %2) #8
  %585 = mul <8 x i32> %578, %119
  %586 = mul <8 x i32> %580, %99
  %587 = add <8 x i32> %585, %222
  %588 = add <8 x i32> %587, %586
  %589 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %588, i32 %2) #8
  store <8 x i32> %589, <8 x i32>* %434, align 32
  store <8 x i32> %584, <8 x i32>* %430, align 32
  %590 = load <8 x i32>, <8 x i32>* %32, align 32
  %591 = load <8 x i32>, <8 x i32>* %37, align 32
  br label %592

592:                                              ; preds = %6, %592
  %593 = phi i64 [ 32, %6 ], [ %752, %592 ]
  %594 = phi i32 [ 32, %6 ], [ %750, %592 ]
  %595 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %593
  %596 = bitcast <4 x i64>* %595 to <8 x i32>*
  %597 = load <8 x i32>, <8 x i32>* %596, align 32
  %598 = and i64 %593, 4294967280
  %599 = or i64 %598, 7
  %600 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %599
  %601 = bitcast <4 x i64>* %600 to <8 x i32>*
  %602 = load <8 x i32>, <8 x i32>* %601, align 32
  %603 = add <8 x i32> %602, %597
  %604 = sub <8 x i32> %597, %602
  %605 = icmp sgt <8 x i32> %603, %590
  %606 = select <8 x i1> %605, <8 x i32> %603, <8 x i32> %590
  %607 = icmp slt <8 x i32> %606, %591
  %608 = select <8 x i1> %607, <8 x i32> %606, <8 x i32> %591
  %609 = icmp sgt <8 x i32> %604, %590
  %610 = select <8 x i1> %609, <8 x i32> %604, <8 x i32> %590
  %611 = icmp slt <8 x i32> %610, %591
  %612 = select <8 x i1> %611, <8 x i32> %610, <8 x i32> %591
  store <8 x i32> %608, <8 x i32>* %596, align 32
  store <8 x i32> %612, <8 x i32>* %601, align 32
  %613 = and i64 %593, 4294967280
  %614 = or i64 %613, 15
  %615 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %614
  %616 = bitcast <4 x i64>* %615 to <8 x i32>*
  %617 = load <8 x i32>, <8 x i32>* %616, align 32
  %618 = and i64 %593, 4294967280
  %619 = or i64 %618, 8
  %620 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %619
  %621 = bitcast <4 x i64>* %620 to <8 x i32>*
  %622 = load <8 x i32>, <8 x i32>* %621, align 32
  %623 = add <8 x i32> %622, %617
  %624 = sub <8 x i32> %617, %622
  %625 = icmp sgt <8 x i32> %623, %590
  %626 = select <8 x i1> %625, <8 x i32> %623, <8 x i32> %590
  %627 = icmp slt <8 x i32> %626, %591
  %628 = select <8 x i1> %627, <8 x i32> %626, <8 x i32> %591
  %629 = icmp sgt <8 x i32> %624, %590
  %630 = select <8 x i1> %629, <8 x i32> %624, <8 x i32> %590
  %631 = icmp slt <8 x i32> %630, %591
  %632 = select <8 x i1> %631, <8 x i32> %630, <8 x i32> %591
  store <8 x i32> %628, <8 x i32>* %616, align 32
  store <8 x i32> %632, <8 x i32>* %621, align 32
  %633 = or i64 %593, 1
  %634 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %633
  %635 = bitcast <4 x i64>* %634 to <8 x i32>*
  %636 = load <8 x i32>, <8 x i32>* %635, align 32
  %637 = and i64 %593, 4294967280
  %638 = or i64 %637, 6
  %639 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %638
  %640 = bitcast <4 x i64>* %639 to <8 x i32>*
  %641 = load <8 x i32>, <8 x i32>* %640, align 32
  %642 = add <8 x i32> %641, %636
  %643 = sub <8 x i32> %636, %641
  %644 = icmp sgt <8 x i32> %642, %590
  %645 = select <8 x i1> %644, <8 x i32> %642, <8 x i32> %590
  %646 = icmp slt <8 x i32> %645, %591
  %647 = select <8 x i1> %646, <8 x i32> %645, <8 x i32> %591
  %648 = icmp sgt <8 x i32> %643, %590
  %649 = select <8 x i1> %648, <8 x i32> %643, <8 x i32> %590
  %650 = icmp slt <8 x i32> %649, %591
  %651 = select <8 x i1> %650, <8 x i32> %649, <8 x i32> %591
  store <8 x i32> %647, <8 x i32>* %635, align 32
  store <8 x i32> %651, <8 x i32>* %640, align 32
  %652 = and i64 %593, 4294967280
  %653 = or i64 %652, 14
  %654 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %653
  %655 = bitcast <4 x i64>* %654 to <8 x i32>*
  %656 = load <8 x i32>, <8 x i32>* %655, align 32
  %657 = and i64 %633, 4294967281
  %658 = or i64 %657, 8
  %659 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %658
  %660 = bitcast <4 x i64>* %659 to <8 x i32>*
  %661 = load <8 x i32>, <8 x i32>* %660, align 32
  %662 = add <8 x i32> %661, %656
  %663 = sub <8 x i32> %656, %661
  %664 = icmp sgt <8 x i32> %662, %590
  %665 = select <8 x i1> %664, <8 x i32> %662, <8 x i32> %590
  %666 = icmp slt <8 x i32> %665, %591
  %667 = select <8 x i1> %666, <8 x i32> %665, <8 x i32> %591
  %668 = icmp sgt <8 x i32> %663, %590
  %669 = select <8 x i1> %668, <8 x i32> %663, <8 x i32> %590
  %670 = icmp slt <8 x i32> %669, %591
  %671 = select <8 x i1> %670, <8 x i32> %669, <8 x i32> %591
  store <8 x i32> %667, <8 x i32>* %655, align 32
  store <8 x i32> %671, <8 x i32>* %660, align 32
  %672 = add nuw nsw i64 %633, 1
  %673 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %672
  %674 = bitcast <4 x i64>* %673 to <8 x i32>*
  %675 = load <8 x i32>, <8 x i32>* %674, align 32
  %676 = and i64 %672, 4294967280
  %677 = or i64 %676, 5
  %678 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %677
  %679 = bitcast <4 x i64>* %678 to <8 x i32>*
  %680 = load <8 x i32>, <8 x i32>* %679, align 32
  %681 = add <8 x i32> %680, %675
  %682 = sub <8 x i32> %675, %680
  %683 = icmp sgt <8 x i32> %681, %590
  %684 = select <8 x i1> %683, <8 x i32> %681, <8 x i32> %590
  %685 = icmp slt <8 x i32> %684, %591
  %686 = select <8 x i1> %685, <8 x i32> %684, <8 x i32> %591
  %687 = icmp sgt <8 x i32> %682, %590
  %688 = select <8 x i1> %687, <8 x i32> %682, <8 x i32> %590
  %689 = icmp slt <8 x i32> %688, %591
  %690 = select <8 x i1> %689, <8 x i32> %688, <8 x i32> %591
  store <8 x i32> %686, <8 x i32>* %674, align 32
  store <8 x i32> %690, <8 x i32>* %679, align 32
  %691 = and i64 %672, 4294967280
  %692 = or i64 %691, 13
  %693 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %692
  %694 = bitcast <4 x i64>* %693 to <8 x i32>*
  %695 = load <8 x i32>, <8 x i32>* %694, align 32
  %696 = and i64 %672, 4294967282
  %697 = or i64 %696, 8
  %698 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %697
  %699 = bitcast <4 x i64>* %698 to <8 x i32>*
  %700 = load <8 x i32>, <8 x i32>* %699, align 32
  %701 = add <8 x i32> %700, %695
  %702 = sub <8 x i32> %695, %700
  %703 = icmp sgt <8 x i32> %701, %590
  %704 = select <8 x i1> %703, <8 x i32> %701, <8 x i32> %590
  %705 = icmp slt <8 x i32> %704, %591
  %706 = select <8 x i1> %705, <8 x i32> %704, <8 x i32> %591
  %707 = icmp sgt <8 x i32> %702, %590
  %708 = select <8 x i1> %707, <8 x i32> %702, <8 x i32> %590
  %709 = icmp slt <8 x i32> %708, %591
  %710 = select <8 x i1> %709, <8 x i32> %708, <8 x i32> %591
  store <8 x i32> %706, <8 x i32>* %694, align 32
  store <8 x i32> %710, <8 x i32>* %699, align 32
  %711 = or i64 %593, 3
  %712 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %711
  %713 = bitcast <4 x i64>* %712 to <8 x i32>*
  %714 = load <8 x i32>, <8 x i32>* %713, align 32
  %715 = and i64 %593, 4294967280
  %716 = or i64 %715, 4
  %717 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %716
  %718 = bitcast <4 x i64>* %717 to <8 x i32>*
  %719 = load <8 x i32>, <8 x i32>* %718, align 32
  %720 = add <8 x i32> %719, %714
  %721 = sub <8 x i32> %714, %719
  %722 = icmp sgt <8 x i32> %720, %590
  %723 = select <8 x i1> %722, <8 x i32> %720, <8 x i32> %590
  %724 = icmp slt <8 x i32> %723, %591
  %725 = select <8 x i1> %724, <8 x i32> %723, <8 x i32> %591
  %726 = icmp sgt <8 x i32> %721, %590
  %727 = select <8 x i1> %726, <8 x i32> %721, <8 x i32> %590
  %728 = icmp slt <8 x i32> %727, %591
  %729 = select <8 x i1> %728, <8 x i32> %727, <8 x i32> %591
  store <8 x i32> %725, <8 x i32>* %713, align 32
  store <8 x i32> %729, <8 x i32>* %718, align 32
  %730 = and i64 %593, 4294967280
  %731 = or i64 %730, 12
  %732 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %731
  %733 = bitcast <4 x i64>* %732 to <8 x i32>*
  %734 = load <8 x i32>, <8 x i32>* %733, align 32
  %735 = and i64 %711, 4294967283
  %736 = or i64 %735, 8
  %737 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %736
  %738 = bitcast <4 x i64>* %737 to <8 x i32>*
  %739 = load <8 x i32>, <8 x i32>* %738, align 32
  %740 = add <8 x i32> %739, %734
  %741 = sub <8 x i32> %734, %739
  %742 = icmp sgt <8 x i32> %740, %590
  %743 = select <8 x i1> %742, <8 x i32> %740, <8 x i32> %590
  %744 = icmp slt <8 x i32> %743, %591
  %745 = select <8 x i1> %744, <8 x i32> %743, <8 x i32> %591
  %746 = icmp sgt <8 x i32> %741, %590
  %747 = select <8 x i1> %746, <8 x i32> %741, <8 x i32> %590
  %748 = icmp slt <8 x i32> %747, %591
  %749 = select <8 x i1> %748, <8 x i32> %747, <8 x i32> %591
  store <8 x i32> %745, <8 x i32>* %733, align 32
  store <8 x i32> %749, <8 x i32>* %738, align 32
  %750 = add nuw nsw i32 %594, 16
  %751 = icmp ult i32 %750, 64
  %752 = add nuw nsw i64 %593, 16
  br i1 %751, label %592, label %753

753:                                              ; preds = %592
  %754 = load <4 x i64>, <4 x i64>* %192, align 32
  %755 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 7
  store <4 x i64> %754, <4 x i64>* %755, align 32
  %756 = load <4 x i64>, <4 x i64>* %411, align 32
  %757 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 6
  store <4 x i64> %756, <4 x i64>* %757, align 32
  %758 = load <4 x i64>, <4 x i64>* %532, align 32
  %759 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 5
  store <4 x i64> %758, <4 x i64>* %759, align 32
  %760 = load <4 x i64>, <4 x i64>* %530, align 32
  %761 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 4
  store <4 x i64> %760, <4 x i64>* %761, align 32
  %762 = load <8 x i32>, <8 x i32>* %133, align 32
  %763 = load <8 x i32>, <8 x i32>* %537, align 32
  %764 = mul <8 x i32> %763, %762
  %765 = load <8 x i32>, <8 x i32>* %541, align 32
  %766 = mul <8 x i32> %765, %405
  %767 = load <8 x i32>, <8 x i32>* %20, align 32
  %768 = add <8 x i32> %767, %766
  %769 = add <8 x i32> %768, %764
  %770 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %769, i32 %2) #8
  %771 = mul <8 x i32> %763, %405
  %772 = add <8 x i32> %768, %771
  %773 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %772, i32 %2) #8
  store <8 x i32> %773, <8 x i32>* %541, align 32
  store <8 x i32> %770, <8 x i32>* %537, align 32
  %774 = load <8 x i32>, <8 x i32>* %535, align 32
  %775 = mul <8 x i32> %774, %762
  %776 = load <8 x i32>, <8 x i32>* %539, align 32
  %777 = mul <8 x i32> %776, %405
  %778 = add <8 x i32> %777, %767
  %779 = add <8 x i32> %778, %775
  %780 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %779, i32 %2) #8
  %781 = mul <8 x i32> %774, %405
  %782 = add <8 x i32> %778, %781
  %783 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %782, i32 %2) #8
  store <8 x i32> %783, <8 x i32>* %539, align 32
  store <8 x i32> %780, <8 x i32>* %535, align 32
  %784 = load <8 x i32>, <8 x i32>* %257, align 32
  %785 = load <8 x i32>, <8 x i32>* %271, align 32
  %786 = add <8 x i32> %785, %784
  %787 = sub <8 x i32> %784, %785
  %788 = load <8 x i32>, <8 x i32>* %32, align 32
  %789 = icmp sgt <8 x i32> %786, %788
  %790 = select <8 x i1> %789, <8 x i32> %786, <8 x i32> %788
  %791 = load <8 x i32>, <8 x i32>* %37, align 32
  %792 = icmp slt <8 x i32> %790, %791
  %793 = select <8 x i1> %792, <8 x i32> %790, <8 x i32> %791
  %794 = icmp sgt <8 x i32> %787, %788
  %795 = select <8 x i1> %794, <8 x i32> %787, <8 x i32> %788
  %796 = icmp slt <8 x i32> %795, %791
  %797 = select <8 x i1> %796, <8 x i32> %795, <8 x i32> %791
  store <8 x i32> %793, <8 x i32>* %257, align 32
  store <8 x i32> %797, <8 x i32>* %271, align 32
  %798 = load <8 x i32>, <8 x i32>* %262, align 32
  %799 = load <8 x i32>, <8 x i32>* %266, align 32
  %800 = add <8 x i32> %799, %798
  %801 = sub <8 x i32> %798, %799
  %802 = icmp sgt <8 x i32> %800, %788
  %803 = select <8 x i1> %802, <8 x i32> %800, <8 x i32> %788
  %804 = icmp slt <8 x i32> %803, %791
  %805 = select <8 x i1> %804, <8 x i32> %803, <8 x i32> %791
  %806 = icmp sgt <8 x i32> %801, %788
  %807 = select <8 x i1> %806, <8 x i32> %801, <8 x i32> %788
  %808 = icmp slt <8 x i32> %807, %791
  %809 = select <8 x i1> %808, <8 x i32> %807, <8 x i32> %791
  store <8 x i32> %805, <8 x i32>* %262, align 32
  store <8 x i32> %809, <8 x i32>* %266, align 32
  %810 = load <8 x i32>, <8 x i32>* %302, align 32
  %811 = load <8 x i32>, <8 x i32>* %304, align 32
  %812 = add <8 x i32> %811, %810
  %813 = sub <8 x i32> %810, %811
  %814 = icmp sgt <8 x i32> %812, %788
  %815 = select <8 x i1> %814, <8 x i32> %812, <8 x i32> %788
  %816 = icmp slt <8 x i32> %815, %791
  %817 = select <8 x i1> %816, <8 x i32> %815, <8 x i32> %791
  %818 = icmp sgt <8 x i32> %813, %788
  %819 = select <8 x i1> %818, <8 x i32> %813, <8 x i32> %788
  %820 = icmp slt <8 x i32> %819, %791
  %821 = select <8 x i1> %820, <8 x i32> %819, <8 x i32> %791
  store <8 x i32> %817, <8 x i32>* %302, align 32
  store <8 x i32> %821, <8 x i32>* %304, align 32
  %822 = load <8 x i32>, <8 x i32>* %308, align 32
  %823 = load <8 x i32>, <8 x i32>* %306, align 32
  %824 = add <8 x i32> %823, %822
  %825 = sub <8 x i32> %822, %823
  %826 = icmp sgt <8 x i32> %824, %788
  %827 = select <8 x i1> %826, <8 x i32> %824, <8 x i32> %788
  %828 = icmp slt <8 x i32> %827, %791
  %829 = select <8 x i1> %828, <8 x i32> %827, <8 x i32> %791
  %830 = icmp sgt <8 x i32> %825, %788
  %831 = select <8 x i1> %830, <8 x i32> %825, <8 x i32> %788
  %832 = icmp slt <8 x i32> %831, %791
  %833 = select <8 x i1> %832, <8 x i32> %831, <8 x i32> %791
  store <8 x i32> %829, <8 x i32>* %308, align 32
  store <8 x i32> %833, <8 x i32>* %306, align 32
  %834 = load <8 x i32>, <8 x i32>* %426, align 32
  %835 = load <8 x i32>, <8 x i32>* %430, align 32
  %836 = add <8 x i32> %835, %834
  %837 = sub <8 x i32> %834, %835
  %838 = icmp sgt <8 x i32> %836, %788
  %839 = select <8 x i1> %838, <8 x i32> %836, <8 x i32> %788
  %840 = icmp slt <8 x i32> %839, %791
  %841 = select <8 x i1> %840, <8 x i32> %839, <8 x i32> %791
  %842 = icmp sgt <8 x i32> %837, %788
  %843 = select <8 x i1> %842, <8 x i32> %837, <8 x i32> %788
  %844 = icmp slt <8 x i32> %843, %791
  %845 = select <8 x i1> %844, <8 x i32> %843, <8 x i32> %791
  store <8 x i32> %841, <8 x i32>* %426, align 32
  store <8 x i32> %845, <8 x i32>* %430, align 32
  %846 = load <8 x i32>, <8 x i32>* %438, align 32
  %847 = load <8 x i32>, <8 x i32>* %434, align 32
  %848 = add <8 x i32> %847, %846
  %849 = sub <8 x i32> %846, %847
  %850 = icmp sgt <8 x i32> %848, %788
  %851 = select <8 x i1> %850, <8 x i32> %848, <8 x i32> %788
  %852 = icmp slt <8 x i32> %851, %791
  %853 = select <8 x i1> %852, <8 x i32> %851, <8 x i32> %791
  %854 = icmp sgt <8 x i32> %849, %788
  %855 = select <8 x i1> %854, <8 x i32> %849, <8 x i32> %788
  %856 = icmp slt <8 x i32> %855, %791
  %857 = select <8 x i1> %856, <8 x i32> %855, <8 x i32> %791
  store <8 x i32> %853, <8 x i32>* %438, align 32
  store <8 x i32> %857, <8 x i32>* %434, align 32
  %858 = load <8 x i32>, <8 x i32>* %424, align 32
  %859 = load <8 x i32>, <8 x i32>* %428, align 32
  %860 = add <8 x i32> %859, %858
  %861 = sub <8 x i32> %858, %859
  %862 = icmp sgt <8 x i32> %860, %788
  %863 = select <8 x i1> %862, <8 x i32> %860, <8 x i32> %788
  %864 = icmp slt <8 x i32> %863, %791
  %865 = select <8 x i1> %864, <8 x i32> %863, <8 x i32> %791
  %866 = icmp sgt <8 x i32> %861, %788
  %867 = select <8 x i1> %866, <8 x i32> %861, <8 x i32> %788
  %868 = icmp slt <8 x i32> %867, %791
  %869 = select <8 x i1> %868, <8 x i32> %867, <8 x i32> %791
  store <8 x i32> %865, <8 x i32>* %424, align 32
  store <8 x i32> %869, <8 x i32>* %428, align 32
  %870 = load <8 x i32>, <8 x i32>* %436, align 32
  %871 = load <8 x i32>, <8 x i32>* %432, align 32
  %872 = add <8 x i32> %871, %870
  %873 = sub <8 x i32> %870, %871
  %874 = icmp sgt <8 x i32> %872, %788
  %875 = select <8 x i1> %874, <8 x i32> %872, <8 x i32> %788
  %876 = icmp slt <8 x i32> %875, %791
  %877 = select <8 x i1> %876, <8 x i32> %875, <8 x i32> %791
  %878 = icmp sgt <8 x i32> %873, %788
  %879 = select <8 x i1> %878, <8 x i32> %873, <8 x i32> %788
  %880 = icmp slt <8 x i32> %879, %791
  %881 = select <8 x i1> %880, <8 x i32> %879, <8 x i32> %791
  store <8 x i32> %877, <8 x i32>* %436, align 32
  store <8 x i32> %881, <8 x i32>* %432, align 32
  %882 = load <8 x i32>, <8 x i32>* %378, align 32
  %883 = mul <8 x i32> %882, %119
  %884 = load <8 x i32>, <8 x i32>* %398, align 32
  %885 = mul <8 x i32> %884, %99
  %886 = add <8 x i32> %883, %767
  %887 = add <8 x i32> %886, %885
  %888 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %887, i32 %2) #8
  %889 = load <8 x i32>, <8 x i32>* %380, align 32
  %890 = mul <8 x i32> %889, %119
  %891 = load <8 x i32>, <8 x i32>* %400, align 32
  %892 = mul <8 x i32> %891, %99
  %893 = add <8 x i32> %890, %767
  %894 = add <8 x i32> %893, %892
  %895 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %894, i32 %2) #8
  %896 = load <8 x i32>, <8 x i32>* %278, align 32
  %897 = mul <8 x i32> %896, %119
  %898 = load <8 x i32>, <8 x i32>* %288, align 32
  %899 = mul <8 x i32> %898, %99
  %900 = add <8 x i32> %897, %767
  %901 = add <8 x i32> %900, %899
  %902 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %901, i32 %2) #8
  %903 = load <8 x i32>, <8 x i32>* %235, align 32
  %904 = mul <8 x i32> %903, %119
  %905 = load <8 x i32>, <8 x i32>* %230, align 32
  %906 = mul <8 x i32> %905, %99
  %907 = add <8 x i32> %904, %767
  %908 = add <8 x i32> %907, %906
  %909 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %908, i32 %2) #8
  %910 = mul <8 x i32> %903, %99
  %911 = mul <8 x i32> %905, %69
  %912 = add <8 x i32> %910, %767
  %913 = add <8 x i32> %912, %911
  %914 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %913, i32 %2) #8
  store <8 x i32> %914, <8 x i32>* %230, align 32
  %915 = mul <8 x i32> %896, %99
  %916 = mul <8 x i32> %898, %69
  %917 = add <8 x i32> %915, %767
  %918 = add <8 x i32> %917, %916
  %919 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %918, i32 %2) #8
  store <8 x i32> %919, <8 x i32>* %288, align 32
  %920 = mul <8 x i32> %889, %99
  %921 = mul <8 x i32> %891, %69
  %922 = add <8 x i32> %920, %767
  %923 = add <8 x i32> %922, %921
  %924 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %923, i32 %2) #8
  store <8 x i32> %924, <8 x i32>* %400, align 32
  %925 = mul <8 x i32> %882, %99
  %926 = mul <8 x i32> %884, %69
  %927 = add <8 x i32> %925, %767
  %928 = add <8 x i32> %927, %926
  %929 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %928, i32 %2) #8
  store <8 x i32> %929, <8 x i32>* %398, align 32
  store <8 x i32> %888, <8 x i32>* %378, align 32
  store <8 x i32> %895, <8 x i32>* %380, align 32
  store <8 x i32> %902, <8 x i32>* %278, align 32
  store <8 x i32> %909, <8 x i32>* %235, align 32
  %930 = load <8 x i32>, <8 x i32>* %239, align 32
  %931 = mul <8 x i32> %930, %144
  %932 = load <8 x i32>, <8 x i32>* %244, align 32
  %933 = mul <8 x i32> %932, %119
  %934 = add <8 x i32> %931, %767
  %935 = add <8 x i32> %934, %933
  %936 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %935, i32 %2) #8
  %937 = load <8 x i32>, <8 x i32>* %280, align 32
  %938 = mul <8 x i32> %937, %144
  %939 = load <8 x i32>, <8 x i32>* %286, align 32
  %940 = mul <8 x i32> %939, %119
  %941 = add <8 x i32> %938, %767
  %942 = add <8 x i32> %941, %940
  %943 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %942, i32 %2) #8
  %944 = load <8 x i32>, <8 x i32>* %384, align 32
  %945 = mul <8 x i32> %944, %144
  %946 = load <8 x i32>, <8 x i32>* %396, align 32
  %947 = mul <8 x i32> %946, %119
  %948 = add <8 x i32> %945, %767
  %949 = add <8 x i32> %948, %947
  %950 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %949, i32 %2) #8
  %951 = load <8 x i32>, <8 x i32>* %382, align 32
  %952 = mul <8 x i32> %951, %144
  %953 = load <8 x i32>, <8 x i32>* %394, align 32
  %954 = mul <8 x i32> %953, %119
  %955 = add <8 x i32> %952, %767
  %956 = add <8 x i32> %955, %954
  %957 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %956, i32 %2) #8
  %958 = mul <8 x i32> %951, %119
  %959 = mul <8 x i32> %953, %99
  %960 = add <8 x i32> %958, %767
  %961 = add <8 x i32> %960, %959
  %962 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %961, i32 %2) #8
  store <8 x i32> %962, <8 x i32>* %394, align 32
  %963 = mul <8 x i32> %944, %119
  %964 = mul <8 x i32> %946, %99
  %965 = add <8 x i32> %963, %767
  %966 = add <8 x i32> %965, %964
  %967 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %966, i32 %2) #8
  store <8 x i32> %967, <8 x i32>* %396, align 32
  %968 = mul <8 x i32> %937, %119
  %969 = mul <8 x i32> %939, %99
  %970 = add <8 x i32> %968, %767
  %971 = add <8 x i32> %970, %969
  %972 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %971, i32 %2) #8
  store <8 x i32> %972, <8 x i32>* %286, align 32
  %973 = mul <8 x i32> %930, %119
  %974 = mul <8 x i32> %932, %99
  %975 = add <8 x i32> %973, %767
  %976 = add <8 x i32> %975, %974
  %977 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %976, i32 %2) #8
  store <8 x i32> %977, <8 x i32>* %244, align 32
  store <8 x i32> %936, <8 x i32>* %239, align 32
  store <8 x i32> %943, <8 x i32>* %280, align 32
  store <8 x i32> %950, <8 x i32>* %384, align 32
  store <8 x i32> %957, <8 x i32>* %382, align 32
  call fastcc void @idct64_stage9_avx2(<4 x i64>* nonnull %192, <4 x i64>* nonnull %11, <4 x i64>* nonnull %10, <4 x i64>* nonnull %8, <4 x i64>* nonnull %9, <4 x i64>* nonnull %7, i32 %2)
  br label %978

978:                                              ; preds = %978, %753
  %979 = phi i64 [ 0, %753 ], [ %997, %978 ]
  %980 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %979
  %981 = bitcast <4 x i64>* %980 to <8 x i32>*
  %982 = load <8 x i32>, <8 x i32>* %981, align 32
  %983 = sub nuw nsw i64 31, %979
  %984 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %983
  %985 = bitcast <4 x i64>* %984 to <8 x i32>*
  %986 = load <8 x i32>, <8 x i32>* %985, align 32
  %987 = add <8 x i32> %986, %982
  %988 = sub <8 x i32> %982, %986
  %989 = icmp sgt <8 x i32> %987, %788
  %990 = select <8 x i1> %989, <8 x i32> %987, <8 x i32> %788
  %991 = icmp slt <8 x i32> %990, %791
  %992 = select <8 x i1> %991, <8 x i32> %990, <8 x i32> %791
  %993 = icmp sgt <8 x i32> %988, %788
  %994 = select <8 x i1> %993, <8 x i32> %988, <8 x i32> %788
  %995 = icmp slt <8 x i32> %994, %791
  %996 = select <8 x i1> %995, <8 x i32> %994, <8 x i32> %791
  store <8 x i32> %992, <8 x i32>* %981, align 32
  store <8 x i32> %996, <8 x i32>* %985, align 32
  %997 = add nuw nsw i64 %979, 1
  %998 = icmp eq i64 %997, 16
  br i1 %998, label %999, label %978

999:                                              ; preds = %978
  %1000 = load <8 x i32>, <8 x i32>* %239, align 32
  %1001 = mul <8 x i32> %1000, %762
  %1002 = load <8 x i32>, <8 x i32>* %244, align 32
  %1003 = mul <8 x i32> %1002, %405
  %1004 = add <8 x i32> %1001, %767
  %1005 = add <8 x i32> %1004, %1003
  %1006 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1005, i32 %2) #8
  %1007 = load <8 x i32>, <8 x i32>* %280, align 32
  %1008 = mul <8 x i32> %1007, %762
  %1009 = load <8 x i32>, <8 x i32>* %286, align 32
  %1010 = mul <8 x i32> %1009, %405
  %1011 = add <8 x i32> %1008, %767
  %1012 = add <8 x i32> %1011, %1010
  %1013 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1012, i32 %2) #8
  %1014 = load <8 x i32>, <8 x i32>* %384, align 32
  %1015 = mul <8 x i32> %1014, %762
  %1016 = load <8 x i32>, <8 x i32>* %396, align 32
  %1017 = mul <8 x i32> %1016, %405
  %1018 = add <8 x i32> %1015, %767
  %1019 = add <8 x i32> %1018, %1017
  %1020 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1019, i32 %2) #8
  %1021 = load <8 x i32>, <8 x i32>* %382, align 32
  %1022 = mul <8 x i32> %1021, %762
  %1023 = load <8 x i32>, <8 x i32>* %394, align 32
  %1024 = mul <8 x i32> %1023, %405
  %1025 = add <8 x i32> %1024, %767
  %1026 = add <8 x i32> %1025, %1022
  %1027 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1026, i32 %2) #8
  %1028 = mul <8 x i32> %1021, %405
  %1029 = add <8 x i32> %1025, %1028
  %1030 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1029, i32 %2) #8
  store <8 x i32> %1030, <8 x i32>* %394, align 32
  %1031 = add <8 x i32> %1016, %1014
  %1032 = mul <8 x i32> %1031, %405
  %1033 = add <8 x i32> %1032, %767
  %1034 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1033, i32 %2) #8
  store <8 x i32> %1034, <8 x i32>* %396, align 32
  %1035 = add <8 x i32> %1009, %1007
  %1036 = mul <8 x i32> %1035, %405
  %1037 = add <8 x i32> %1036, %767
  %1038 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1037, i32 %2) #8
  store <8 x i32> %1038, <8 x i32>* %286, align 32
  %1039 = add <8 x i32> %1002, %1000
  %1040 = mul <8 x i32> %1039, %405
  %1041 = add <8 x i32> %1040, %767
  %1042 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1041, i32 %2) #8
  store <8 x i32> %1042, <8 x i32>* %244, align 32
  store <8 x i32> %1006, <8 x i32>* %239, align 32
  store <8 x i32> %1013, <8 x i32>* %280, align 32
  store <8 x i32> %1020, <8 x i32>* %384, align 32
  store <8 x i32> %1027, <8 x i32>* %382, align 32
  %1043 = load <8 x i32>, <8 x i32>* %386, align 32
  %1044 = mul <8 x i32> %1043, %762
  %1045 = load <8 x i32>, <8 x i32>* %390, align 32
  %1046 = mul <8 x i32> %1045, %405
  %1047 = add <8 x i32> %1044, %767
  %1048 = add <8 x i32> %1047, %1046
  %1049 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1048, i32 %2) #8
  %1050 = load <8 x i32>, <8 x i32>* %388, align 32
  %1051 = mul <8 x i32> %1050, %762
  %1052 = load <8 x i32>, <8 x i32>* %392, align 32
  %1053 = mul <8 x i32> %1052, %405
  %1054 = add <8 x i32> %1051, %767
  %1055 = add <8 x i32> %1054, %1053
  %1056 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1055, i32 %2) #8
  %1057 = load <8 x i32>, <8 x i32>* %282, align 32
  %1058 = mul <8 x i32> %1057, %762
  %1059 = load <8 x i32>, <8 x i32>* %284, align 32
  %1060 = mul <8 x i32> %1059, %405
  %1061 = add <8 x i32> %1058, %767
  %1062 = add <8 x i32> %1061, %1060
  %1063 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1062, i32 %2) #8
  %1064 = load <8 x i32>, <8 x i32>* %253, align 32
  %1065 = mul <8 x i32> %1064, %762
  %1066 = load <8 x i32>, <8 x i32>* %248, align 32
  %1067 = mul <8 x i32> %1066, %405
  %1068 = add <8 x i32> %1067, %767
  %1069 = add <8 x i32> %1068, %1065
  %1070 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1069, i32 %2) #8
  %1071 = mul <8 x i32> %1064, %405
  %1072 = add <8 x i32> %1068, %1071
  %1073 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1072, i32 %2) #8
  store <8 x i32> %1073, <8 x i32>* %248, align 32
  %1074 = load <8 x i32>, <8 x i32>* %87, align 32
  %1075 = add <8 x i32> %1059, %1057
  %1076 = mul <8 x i32> %1074, %1075
  %1077 = add <8 x i32> %1076, %767
  %1078 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1077, i32 %2) #8
  store <8 x i32> %1078, <8 x i32>* %284, align 32
  %1079 = add <8 x i32> %1052, %1050
  %1080 = mul <8 x i32> %1074, %1079
  %1081 = add <8 x i32> %1080, %767
  %1082 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1081, i32 %2) #8
  store <8 x i32> %1082, <8 x i32>* %392, align 32
  %1083 = add <8 x i32> %1045, %1043
  %1084 = mul <8 x i32> %1074, %1083
  %1085 = add <8 x i32> %1084, %767
  %1086 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1085, i32 %2) #8
  store <8 x i32> %1086, <8 x i32>* %390, align 32
  store <8 x i32> %1049, <8 x i32>* %386, align 32
  store <8 x i32> %1056, <8 x i32>* %388, align 32
  store <8 x i32> %1063, <8 x i32>* %282, align 32
  store <8 x i32> %1070, <8 x i32>* %253, align 32
  call fastcc void @idct64_stage11_avx2(<4 x i64>* nonnull %192, <4 x i64>* %1, i32 %3, i32 %4, i32 %5, <4 x i64>* nonnull %8, <4 x i64>* nonnull %9)
  call void @llvm.lifetime.end.p0i8(i64 2048, i8* nonnull %188) #8
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %129) #8
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %82) #8
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %33) #8
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %26) #8
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %15) #8
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @idct64_low16_avx2(<4 x i64>* nocapture readonly, <4 x i64>*, i32, i32, i32, i32) #0 {
  %7 = alloca <4 x i64>, align 32
  %8 = alloca <4 x i64>, align 32
  %9 = alloca <4 x i64>, align 32
  %10 = alloca <4 x i64>, align 32
  %11 = alloca <4 x i64>, align 32
  %12 = alloca [64 x <4 x i64>], align 32
  %13 = add nsw i32 %2, -10
  %14 = sext i32 %13 to i64
  %15 = bitcast <4 x i64>* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %15) #8
  %16 = add nsw i32 %2, -1
  %17 = shl i32 1, %16
  %18 = insertelement <8 x i32> undef, i32 %17, i32 0
  %19 = shufflevector <8 x i32> %18, <8 x i32> undef, <8 x i32> zeroinitializer
  %20 = bitcast <4 x i64>* %7 to <8 x i32>*
  store <8 x i32> %19, <8 x i32>* %20, align 32
  %21 = icmp eq i32 %3, 0
  %22 = select i1 %21, i32 8, i32 6
  %23 = add nsw i32 %22, %4
  %24 = icmp slt i32 %23, 16
  %25 = add i32 %23, -1
  %26 = bitcast <4 x i64>* %8 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %26) #8
  %27 = shl i32 1, %25
  %28 = select i1 %24, i32 32768, i32 %27
  %29 = sub nsw i32 0, %28
  %30 = insertelement <8 x i32> undef, i32 %29, i32 0
  %31 = shufflevector <8 x i32> %30, <8 x i32> undef, <8 x i32> zeroinitializer
  %32 = bitcast <4 x i64>* %8 to <8 x i32>*
  store <8 x i32> %31, <8 x i32>* %32, align 32
  %33 = bitcast <4 x i64>* %9 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %33) #8
  %34 = add nsw i32 %28, -1
  %35 = insertelement <8 x i32> undef, i32 %34, i32 0
  %36 = shufflevector <8 x i32> %35, <8 x i32> undef, <8 x i32> zeroinitializer
  %37 = bitcast <4 x i64>* %9 to <8 x i32>*
  store <8 x i32> %36, <8 x i32>* %37, align 32
  %38 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 1
  %39 = load i32, i32* %38, align 4
  %40 = insertelement <8 x i32> undef, i32 %39, i32 0
  %41 = shufflevector <8 x i32> %40, <8 x i32> undef, <8 x i32> zeroinitializer
  %42 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 2
  %43 = load i32, i32* %42, align 8
  %44 = insertelement <8 x i32> undef, i32 %43, i32 0
  %45 = shufflevector <8 x i32> %44, <8 x i32> undef, <8 x i32> zeroinitializer
  %46 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 3
  %47 = load i32, i32* %46, align 4
  %48 = insertelement <8 x i32> undef, i32 %47, i32 0
  %49 = shufflevector <8 x i32> %48, <8 x i32> undef, <8 x i32> zeroinitializer
  %50 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 4
  %51 = load i32, i32* %50, align 16
  %52 = insertelement <8 x i32> undef, i32 %51, i32 0
  %53 = shufflevector <8 x i32> %52, <8 x i32> undef, <8 x i32> zeroinitializer
  %54 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 5
  %55 = load i32, i32* %54, align 4
  %56 = insertelement <8 x i32> undef, i32 %55, i32 0
  %57 = shufflevector <8 x i32> %56, <8 x i32> undef, <8 x i32> zeroinitializer
  %58 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 6
  %59 = load i32, i32* %58, align 8
  %60 = insertelement <8 x i32> undef, i32 %59, i32 0
  %61 = shufflevector <8 x i32> %60, <8 x i32> undef, <8 x i32> zeroinitializer
  %62 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 7
  %63 = load i32, i32* %62, align 4
  %64 = insertelement <8 x i32> undef, i32 %63, i32 0
  %65 = shufflevector <8 x i32> %64, <8 x i32> undef, <8 x i32> zeroinitializer
  %66 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 8
  %67 = load i32, i32* %66, align 16
  %68 = insertelement <8 x i32> undef, i32 %67, i32 0
  %69 = shufflevector <8 x i32> %68, <8 x i32> undef, <8 x i32> zeroinitializer
  %70 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 9
  %71 = load i32, i32* %70, align 4
  %72 = insertelement <8 x i32> undef, i32 %71, i32 0
  %73 = shufflevector <8 x i32> %72, <8 x i32> undef, <8 x i32> zeroinitializer
  %74 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 10
  %75 = load i32, i32* %74, align 8
  %76 = insertelement <8 x i32> undef, i32 %75, i32 0
  %77 = shufflevector <8 x i32> %76, <8 x i32> undef, <8 x i32> zeroinitializer
  %78 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 11
  %79 = load i32, i32* %78, align 4
  %80 = insertelement <8 x i32> undef, i32 %79, i32 0
  %81 = shufflevector <8 x i32> %80, <8 x i32> undef, <8 x i32> zeroinitializer
  %82 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 12
  %83 = load i32, i32* %82, align 16
  %84 = insertelement <8 x i32> undef, i32 %83, i32 0
  %85 = shufflevector <8 x i32> %84, <8 x i32> undef, <8 x i32> zeroinitializer
  %86 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 13
  %87 = load i32, i32* %86, align 4
  %88 = insertelement <8 x i32> undef, i32 %87, i32 0
  %89 = shufflevector <8 x i32> %88, <8 x i32> undef, <8 x i32> zeroinitializer
  %90 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 14
  %91 = load i32, i32* %90, align 8
  %92 = insertelement <8 x i32> undef, i32 %91, i32 0
  %93 = shufflevector <8 x i32> %92, <8 x i32> undef, <8 x i32> zeroinitializer
  %94 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 15
  %95 = load i32, i32* %94, align 4
  %96 = insertelement <8 x i32> undef, i32 %95, i32 0
  %97 = shufflevector <8 x i32> %96, <8 x i32> undef, <8 x i32> zeroinitializer
  %98 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 16
  %99 = load i32, i32* %98, align 16
  %100 = insertelement <8 x i32> undef, i32 %99, i32 0
  %101 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 20
  %102 = load i32, i32* %101, align 16
  %103 = insertelement <8 x i32> undef, i32 %102, i32 0
  %104 = shufflevector <8 x i32> %103, <8 x i32> undef, <8 x i32> zeroinitializer
  %105 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 24
  %106 = load i32, i32* %105, align 16
  %107 = insertelement <8 x i32> undef, i32 %106, i32 0
  %108 = shufflevector <8 x i32> %107, <8 x i32> undef, <8 x i32> zeroinitializer
  %109 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 28
  %110 = load i32, i32* %109, align 16
  %111 = insertelement <8 x i32> undef, i32 %110, i32 0
  %112 = shufflevector <8 x i32> %111, <8 x i32> undef, <8 x i32> zeroinitializer
  %113 = bitcast <4 x i64>* %10 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %113) #8
  %114 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 32
  %115 = load i32, i32* %114, align 16
  %116 = insertelement <8 x i32> undef, i32 %115, i32 0
  %117 = shufflevector <8 x i32> %116, <8 x i32> undef, <8 x i32> zeroinitializer
  %118 = bitcast <4 x i64>* %10 to <8 x i32>*
  store <8 x i32> %117, <8 x i32>* %118, align 32
  %119 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 36
  %120 = load i32, i32* %119, align 16
  %121 = insertelement <8 x i32> undef, i32 %120, i32 0
  %122 = shufflevector <8 x i32> %121, <8 x i32> undef, <8 x i32> zeroinitializer
  %123 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 40
  %124 = load i32, i32* %123, align 16
  %125 = insertelement <8 x i32> undef, i32 %124, i32 0
  %126 = shufflevector <8 x i32> %125, <8 x i32> undef, <8 x i32> zeroinitializer
  %127 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 44
  %128 = load i32, i32* %127, align 16
  %129 = insertelement <8 x i32> undef, i32 %128, i32 0
  %130 = shufflevector <8 x i32> %129, <8 x i32> undef, <8 x i32> zeroinitializer
  %131 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 48
  %132 = load i32, i32* %131, align 16
  %133 = insertelement <8 x i32> undef, i32 %132, i32 0
  %134 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 51
  %135 = load i32, i32* %134, align 4
  %136 = insertelement <8 x i32> undef, i32 %135, i32 0
  %137 = shufflevector <8 x i32> %136, <8 x i32> undef, <8 x i32> zeroinitializer
  %138 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 52
  %139 = load i32, i32* %138, align 16
  %140 = insertelement <8 x i32> undef, i32 %139, i32 0
  %141 = shufflevector <8 x i32> %140, <8 x i32> undef, <8 x i32> zeroinitializer
  %142 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 54
  %143 = load i32, i32* %142, align 8
  %144 = insertelement <8 x i32> undef, i32 %143, i32 0
  %145 = shufflevector <8 x i32> %144, <8 x i32> undef, <8 x i32> zeroinitializer
  %146 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 55
  %147 = load i32, i32* %146, align 4
  %148 = insertelement <8 x i32> undef, i32 %147, i32 0
  %149 = shufflevector <8 x i32> %148, <8 x i32> undef, <8 x i32> zeroinitializer
  %150 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 56
  %151 = load i32, i32* %150, align 16
  %152 = insertelement <8 x i32> undef, i32 %151, i32 0
  %153 = shufflevector <8 x i32> %152, <8 x i32> undef, <8 x i32> zeroinitializer
  %154 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 59
  %155 = load i32, i32* %154, align 4
  %156 = insertelement <8 x i32> undef, i32 %155, i32 0
  %157 = shufflevector <8 x i32> %156, <8 x i32> undef, <8 x i32> zeroinitializer
  %158 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 60
  %159 = load i32, i32* %158, align 16
  %160 = insertelement <8 x i32> undef, i32 %159, i32 0
  %161 = shufflevector <8 x i32> %160, <8 x i32> undef, <8 x i32> zeroinitializer
  %162 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 62
  %163 = load i32, i32* %162, align 8
  %164 = insertelement <8 x i32> undef, i32 %163, i32 0
  %165 = shufflevector <8 x i32> %164, <8 x i32> undef, <8 x i32> zeroinitializer
  %166 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 63
  %167 = load i32, i32* %166, align 4
  %168 = insertelement <8 x i32> undef, i32 %167, i32 0
  %169 = shufflevector <8 x i32> %168, <8 x i32> undef, <8 x i32> zeroinitializer
  %170 = sub nsw i32 0, %51
  %171 = insertelement <8 x i32> undef, i32 %170, i32 0
  %172 = shufflevector <8 x i32> %171, <8 x i32> undef, <8 x i32> zeroinitializer
  %173 = sub nsw i32 0, %67
  %174 = insertelement <8 x i32> undef, i32 %173, i32 0
  %175 = shufflevector <8 x i32> %174, <8 x i32> undef, <8 x i32> zeroinitializer
  %176 = sub nsw i32 0, %83
  %177 = insertelement <8 x i32> undef, i32 %176, i32 0
  %178 = shufflevector <8 x i32> %177, <8 x i32> undef, <8 x i32> zeroinitializer
  %179 = sub nsw i32 0, %99
  %180 = insertelement <8 x i32> undef, i32 %179, i32 0
  %181 = sub nsw i32 0, %102
  %182 = insertelement <8 x i32> undef, i32 %181, i32 0
  %183 = shufflevector <8 x i32> %182, <8 x i32> undef, <8 x i32> zeroinitializer
  %184 = sub nsw i32 0, %106
  %185 = insertelement <8 x i32> undef, i32 %184, i32 0
  %186 = shufflevector <8 x i32> %185, <8 x i32> undef, <8 x i32> zeroinitializer
  %187 = sub nsw i32 0, %110
  %188 = insertelement <8 x i32> undef, i32 %187, i32 0
  %189 = shufflevector <8 x i32> %188, <8 x i32> undef, <8 x i32> zeroinitializer
  %190 = bitcast <4 x i64>* %11 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %190) #8
  %191 = sub nsw i32 0, %115
  %192 = insertelement <8 x i32> undef, i32 %191, i32 0
  %193 = shufflevector <8 x i32> %192, <8 x i32> undef, <8 x i32> zeroinitializer
  %194 = bitcast <4 x i64>* %11 to <8 x i32>*
  store <8 x i32> %193, <8 x i32>* %194, align 32
  %195 = sub nsw i32 0, %120
  %196 = insertelement <8 x i32> undef, i32 %195, i32 0
  %197 = shufflevector <8 x i32> %196, <8 x i32> undef, <8 x i32> zeroinitializer
  %198 = sub nsw i32 0, %124
  %199 = insertelement <8 x i32> undef, i32 %198, i32 0
  %200 = shufflevector <8 x i32> %199, <8 x i32> undef, <8 x i32> zeroinitializer
  %201 = sub nsw i32 0, %128
  %202 = insertelement <8 x i32> undef, i32 %201, i32 0
  %203 = shufflevector <8 x i32> %202, <8 x i32> undef, <8 x i32> zeroinitializer
  %204 = sub nsw i32 0, %132
  %205 = insertelement <8 x i32> undef, i32 %204, i32 0
  %206 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 49
  %207 = load i32, i32* %206, align 4
  %208 = sub nsw i32 0, %207
  %209 = insertelement <8 x i32> undef, i32 %208, i32 0
  %210 = shufflevector <8 x i32> %209, <8 x i32> undef, <8 x i32> zeroinitializer
  %211 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 50
  %212 = load i32, i32* %211, align 8
  %213 = sub nsw i32 0, %212
  %214 = insertelement <8 x i32> undef, i32 %213, i32 0
  %215 = shufflevector <8 x i32> %214, <8 x i32> undef, <8 x i32> zeroinitializer
  %216 = sub nsw i32 0, %139
  %217 = insertelement <8 x i32> undef, i32 %216, i32 0
  %218 = shufflevector <8 x i32> %217, <8 x i32> undef, <8 x i32> zeroinitializer
  %219 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 53
  %220 = load i32, i32* %219, align 4
  %221 = sub nsw i32 0, %220
  %222 = insertelement <8 x i32> undef, i32 %221, i32 0
  %223 = shufflevector <8 x i32> %222, <8 x i32> undef, <8 x i32> zeroinitializer
  %224 = sub nsw i32 0, %151
  %225 = insertelement <8 x i32> undef, i32 %224, i32 0
  %226 = shufflevector <8 x i32> %225, <8 x i32> undef, <8 x i32> zeroinitializer
  %227 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 57
  %228 = load i32, i32* %227, align 4
  %229 = sub nsw i32 0, %228
  %230 = insertelement <8 x i32> undef, i32 %229, i32 0
  %231 = shufflevector <8 x i32> %230, <8 x i32> undef, <8 x i32> zeroinitializer
  %232 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 58
  %233 = load i32, i32* %232, align 8
  %234 = sub nsw i32 0, %233
  %235 = insertelement <8 x i32> undef, i32 %234, i32 0
  %236 = shufflevector <8 x i32> %235, <8 x i32> undef, <8 x i32> zeroinitializer
  %237 = sub nsw i32 0, %159
  %238 = insertelement <8 x i32> undef, i32 %237, i32 0
  %239 = shufflevector <8 x i32> %238, <8 x i32> undef, <8 x i32> zeroinitializer
  %240 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %14, i64 61
  %241 = load i32, i32* %240, align 4
  %242 = sub nsw i32 0, %241
  %243 = insertelement <8 x i32> undef, i32 %242, i32 0
  %244 = shufflevector <8 x i32> %243, <8 x i32> undef, <8 x i32> zeroinitializer
  %245 = bitcast [64 x <4 x i64>]* %12 to i8*
  call void @llvm.lifetime.start.p0i8(i64 2048, i8* nonnull %245) #8
  %246 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 1
  %247 = bitcast <4 x i64>* %246 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %247, i8 -86, i64 1984, i1 false)
  %248 = load <4 x i64>, <4 x i64>* %0, align 32
  %249 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 0
  store <4 x i64> %248, <4 x i64>* %249, align 32
  %250 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 1
  %251 = bitcast <4 x i64>* %250 to <8 x i32>*
  %252 = load <8 x i32>, <8 x i32>* %251, align 32
  %253 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 32
  %254 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 9
  %255 = bitcast <4 x i64>* %254 to <8 x i32>*
  %256 = load <8 x i32>, <8 x i32>* %255, align 32
  %257 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 36
  %258 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 5
  %259 = load <4 x i64>, <4 x i64>* %258, align 32
  %260 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 40
  store <4 x i64> %259, <4 x i64>* %260, align 32
  %261 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 13
  %262 = load <4 x i64>, <4 x i64>* %261, align 32
  %263 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 44
  store <4 x i64> %262, <4 x i64>* %263, align 32
  %264 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 3
  %265 = load <4 x i64>, <4 x i64>* %264, align 32
  %266 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 48
  store <4 x i64> %265, <4 x i64>* %266, align 32
  %267 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 11
  %268 = load <4 x i64>, <4 x i64>* %267, align 32
  %269 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 52
  store <4 x i64> %268, <4 x i64>* %269, align 32
  %270 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 7
  %271 = bitcast <4 x i64>* %270 to <8 x i32>*
  %272 = load <8 x i32>, <8 x i32>* %271, align 32
  %273 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 56
  %274 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 15
  %275 = bitcast <4 x i64>* %274 to <8 x i32>*
  %276 = load <8 x i32>, <8 x i32>* %275, align 32
  %277 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 60
  %278 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 2
  %279 = load <4 x i64>, <4 x i64>* %278, align 32
  %280 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 16
  store <4 x i64> %279, <4 x i64>* %280, align 32
  %281 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 10
  %282 = load <4 x i64>, <4 x i64>* %281, align 32
  %283 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 20
  store <4 x i64> %282, <4 x i64>* %283, align 32
  %284 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 6
  %285 = load <4 x i64>, <4 x i64>* %284, align 32
  %286 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 24
  store <4 x i64> %285, <4 x i64>* %286, align 32
  %287 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 14
  %288 = load <4 x i64>, <4 x i64>* %287, align 32
  %289 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 28
  store <4 x i64> %288, <4 x i64>* %289, align 32
  %290 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 8
  %291 = load <4 x i64>, <4 x i64>* %290, align 32
  %292 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 4
  store <4 x i64> %291, <4 x i64>* %292, align 32
  %293 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 4
  %294 = load <4 x i64>, <4 x i64>* %293, align 32
  %295 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 8
  store <4 x i64> %294, <4 x i64>* %295, align 32
  %296 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 12
  %297 = load <4 x i64>, <4 x i64>* %296, align 32
  %298 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 12
  store <4 x i64> %297, <4 x i64>* %298, align 32
  %299 = bitcast <4 x i64>* %253 to <8 x i32>*
  %300 = mul <8 x i32> %41, %252
  %301 = load <8 x i32>, <8 x i32>* %20, align 32
  %302 = add <8 x i32> %301, %300
  %303 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %302, i32 %2) #8
  %304 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 63
  %305 = bitcast <4 x i64>* %304 to <8 x i32>*
  store <8 x i32> %303, <8 x i32>* %305, align 32
  %306 = mul <8 x i32> %169, %252
  %307 = add <8 x i32> %301, %306
  %308 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %307, i32 %2) #8
  store <8 x i32> %308, <8 x i32>* %299, align 32
  %309 = bitcast <4 x i64>* %277 to <8 x i32>*
  %310 = mul <8 x i32> %210, %276
  %311 = add <8 x i32> %310, %301
  %312 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %311, i32 %2) #8
  %313 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 35
  %314 = bitcast <4 x i64>* %313 to <8 x i32>*
  store <8 x i32> %312, <8 x i32>* %314, align 32
  %315 = mul <8 x i32> %97, %276
  %316 = add <8 x i32> %315, %301
  %317 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %316, i32 %2) #8
  store <8 x i32> %317, <8 x i32>* %309, align 32
  %318 = bitcast <4 x i64>* %257 to <8 x i32>*
  %319 = mul <8 x i32> %73, %256
  %320 = add <8 x i32> %319, %301
  %321 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %320, i32 %2) #8
  %322 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 59
  %323 = bitcast <4 x i64>* %322 to <8 x i32>*
  store <8 x i32> %321, <8 x i32>* %323, align 32
  %324 = mul <8 x i32> %149, %256
  %325 = add <8 x i32> %324, %301
  %326 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %325, i32 %2) #8
  store <8 x i32> %326, <8 x i32>* %318, align 32
  %327 = bitcast <4 x i64>* %273 to <8 x i32>*
  %328 = mul <8 x i32> %231, %272
  %329 = add <8 x i32> %328, %301
  %330 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %329, i32 %2) #8
  %331 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 39
  %332 = bitcast <4 x i64>* %331 to <8 x i32>*
  store <8 x i32> %330, <8 x i32>* %332, align 32
  %333 = mul <8 x i32> %65, %272
  %334 = add <8 x i32> %333, %301
  %335 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %334, i32 %2) #8
  store <8 x i32> %335, <8 x i32>* %327, align 32
  %336 = bitcast <4 x i64>* %260 to <8 x i32>*
  %337 = load <8 x i32>, <8 x i32>* %336, align 32
  %338 = mul <8 x i32> %337, %57
  %339 = add <8 x i32> %338, %301
  %340 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %339, i32 %2) #8
  %341 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 55
  %342 = bitcast <4 x i64>* %341 to <8 x i32>*
  store <8 x i32> %340, <8 x i32>* %342, align 32
  %343 = mul <8 x i32> %337, %157
  %344 = add <8 x i32> %343, %301
  %345 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %344, i32 %2) #8
  store <8 x i32> %345, <8 x i32>* %336, align 32
  %346 = bitcast <4 x i64>* %269 to <8 x i32>*
  %347 = load <8 x i32>, <8 x i32>* %346, align 32
  %348 = mul <8 x i32> %347, %223
  %349 = add <8 x i32> %348, %301
  %350 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %349, i32 %2) #8
  %351 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 43
  %352 = bitcast <4 x i64>* %351 to <8 x i32>*
  store <8 x i32> %350, <8 x i32>* %352, align 32
  %353 = mul <8 x i32> %347, %81
  %354 = add <8 x i32> %353, %301
  %355 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %354, i32 %2) #8
  store <8 x i32> %355, <8 x i32>* %346, align 32
  %356 = bitcast <4 x i64>* %266 to <8 x i32>*
  %357 = load <8 x i32>, <8 x i32>* %356, align 32
  %358 = mul <8 x i32> %357, %244
  %359 = add <8 x i32> %358, %301
  %360 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %359, i32 %2) #8
  %361 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 47
  %362 = bitcast <4 x i64>* %361 to <8 x i32>*
  store <8 x i32> %360, <8 x i32>* %362, align 32
  %363 = mul <8 x i32> %357, %49
  %364 = add <8 x i32> %363, %301
  %365 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %364, i32 %2) #8
  store <8 x i32> %365, <8 x i32>* %356, align 32
  %366 = bitcast <4 x i64>* %263 to <8 x i32>*
  %367 = load <8 x i32>, <8 x i32>* %366, align 32
  %368 = mul <8 x i32> %367, %89
  %369 = add <8 x i32> %368, %301
  %370 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %369, i32 %2) #8
  %371 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 51
  %372 = bitcast <4 x i64>* %371 to <8 x i32>*
  store <8 x i32> %370, <8 x i32>* %372, align 32
  %373 = mul <8 x i32> %367, %137
  %374 = add <8 x i32> %373, %301
  %375 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %374, i32 %2) #8
  store <8 x i32> %375, <8 x i32>* %366, align 32
  %376 = bitcast <4 x i64>* %280 to <8 x i32>*
  %377 = load <8 x i32>, <8 x i32>* %376, align 32
  %378 = mul <8 x i32> %377, %45
  %379 = add <8 x i32> %378, %301
  %380 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %379, i32 %2) #8
  %381 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 31
  %382 = bitcast <4 x i64>* %381 to <8 x i32>*
  store <8 x i32> %380, <8 x i32>* %382, align 32
  %383 = mul <8 x i32> %377, %165
  %384 = add <8 x i32> %383, %301
  %385 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %384, i32 %2) #8
  store <8 x i32> %385, <8 x i32>* %376, align 32
  %386 = bitcast <4 x i64>* %289 to <8 x i32>*
  %387 = load <8 x i32>, <8 x i32>* %386, align 32
  %388 = mul <8 x i32> %387, %215
  %389 = add <8 x i32> %388, %301
  %390 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %389, i32 %2) #8
  %391 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 19
  %392 = bitcast <4 x i64>* %391 to <8 x i32>*
  store <8 x i32> %390, <8 x i32>* %392, align 32
  %393 = mul <8 x i32> %387, %93
  %394 = add <8 x i32> %393, %301
  %395 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %394, i32 %2) #8
  store <8 x i32> %395, <8 x i32>* %386, align 32
  %396 = bitcast <4 x i64>* %283 to <8 x i32>*
  %397 = load <8 x i32>, <8 x i32>* %396, align 32
  %398 = mul <8 x i32> %397, %77
  %399 = add <8 x i32> %398, %301
  %400 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %399, i32 %2) #8
  %401 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 27
  %402 = bitcast <4 x i64>* %401 to <8 x i32>*
  store <8 x i32> %400, <8 x i32>* %402, align 32
  %403 = mul <8 x i32> %397, %145
  %404 = add <8 x i32> %403, %301
  %405 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %404, i32 %2) #8
  store <8 x i32> %405, <8 x i32>* %396, align 32
  %406 = bitcast <4 x i64>* %286 to <8 x i32>*
  %407 = load <8 x i32>, <8 x i32>* %406, align 32
  %408 = mul <8 x i32> %407, %236
  %409 = add <8 x i32> %408, %301
  %410 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %409, i32 %2) #8
  %411 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 23
  %412 = bitcast <4 x i64>* %411 to <8 x i32>*
  store <8 x i32> %410, <8 x i32>* %412, align 32
  %413 = mul <8 x i32> %407, %61
  %414 = add <8 x i32> %413, %301
  %415 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %414, i32 %2) #8
  store <8 x i32> %415, <8 x i32>* %406, align 32
  %416 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 33
  %417 = bitcast <4 x i64>* %416 to <8 x i32>*
  store <8 x i32> %308, <8 x i32>* %417, align 32
  %418 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 34
  %419 = bitcast <4 x i64>* %418 to <8 x i32>*
  store <8 x i32> %312, <8 x i32>* %419, align 32
  %420 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 37
  %421 = bitcast <4 x i64>* %420 to <8 x i32>*
  store <8 x i32> %326, <8 x i32>* %421, align 32
  %422 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 38
  %423 = bitcast <4 x i64>* %422 to <8 x i32>*
  store <8 x i32> %330, <8 x i32>* %423, align 32
  %424 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 41
  %425 = bitcast <4 x i64>* %424 to <8 x i32>*
  store <8 x i32> %345, <8 x i32>* %425, align 32
  %426 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 42
  %427 = bitcast <4 x i64>* %426 to <8 x i32>*
  store <8 x i32> %350, <8 x i32>* %427, align 32
  %428 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 45
  %429 = bitcast <4 x i64>* %428 to <8 x i32>*
  store <8 x i32> %375, <8 x i32>* %429, align 32
  %430 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 46
  %431 = bitcast <4 x i64>* %430 to <8 x i32>*
  store <8 x i32> %360, <8 x i32>* %431, align 32
  %432 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 49
  %433 = bitcast <4 x i64>* %432 to <8 x i32>*
  store <8 x i32> %365, <8 x i32>* %433, align 32
  %434 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 50
  %435 = bitcast <4 x i64>* %434 to <8 x i32>*
  store <8 x i32> %370, <8 x i32>* %435, align 32
  %436 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 53
  %437 = bitcast <4 x i64>* %436 to <8 x i32>*
  store <8 x i32> %355, <8 x i32>* %437, align 32
  %438 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 54
  %439 = bitcast <4 x i64>* %438 to <8 x i32>*
  store <8 x i32> %340, <8 x i32>* %439, align 32
  %440 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 57
  %441 = bitcast <4 x i64>* %440 to <8 x i32>*
  %442 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 58
  %443 = bitcast <4 x i64>* %442 to <8 x i32>*
  %444 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 61
  %445 = bitcast <4 x i64>* %444 to <8 x i32>*
  %446 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 62
  %447 = bitcast <4 x i64>* %446 to <8 x i32>*
  %448 = bitcast <4 x i64>* %295 to <8 x i32>*
  %449 = load <8 x i32>, <8 x i32>* %448, align 32
  %450 = mul <8 x i32> %449, %53
  %451 = add <8 x i32> %450, %301
  %452 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %451, i32 %2) #8
  %453 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 15
  %454 = bitcast <4 x i64>* %453 to <8 x i32>*
  store <8 x i32> %452, <8 x i32>* %454, align 32
  %455 = mul <8 x i32> %449, %161
  %456 = add <8 x i32> %455, %301
  %457 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %456, i32 %2) #8
  store <8 x i32> %457, <8 x i32>* %448, align 32
  %458 = bitcast <4 x i64>* %298 to <8 x i32>*
  %459 = load <8 x i32>, <8 x i32>* %458, align 32
  %460 = mul <8 x i32> %459, %218
  %461 = add <8 x i32> %460, %301
  %462 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %461, i32 %2) #8
  %463 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 11
  %464 = bitcast <4 x i64>* %463 to <8 x i32>*
  store <8 x i32> %462, <8 x i32>* %464, align 32
  %465 = mul <8 x i32> %459, %85
  %466 = add <8 x i32> %465, %301
  %467 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %466, i32 %2) #8
  store <8 x i32> %467, <8 x i32>* %458, align 32
  %468 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 17
  %469 = bitcast <4 x i64>* %468 to <8 x i32>*
  store <8 x i32> %385, <8 x i32>* %469, align 32
  %470 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 18
  %471 = bitcast <4 x i64>* %470 to <8 x i32>*
  store <8 x i32> %390, <8 x i32>* %471, align 32
  %472 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 21
  %473 = bitcast <4 x i64>* %472 to <8 x i32>*
  store <8 x i32> %405, <8 x i32>* %473, align 32
  %474 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 22
  %475 = bitcast <4 x i64>* %474 to <8 x i32>*
  store <8 x i32> %410, <8 x i32>* %475, align 32
  %476 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 25
  %477 = bitcast <4 x i64>* %476 to <8 x i32>*
  store <8 x i32> %415, <8 x i32>* %477, align 32
  %478 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 26
  %479 = bitcast <4 x i64>* %478 to <8 x i32>*
  store <8 x i32> %400, <8 x i32>* %479, align 32
  %480 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 29
  %481 = bitcast <4 x i64>* %480 to <8 x i32>*
  store <8 x i32> %395, <8 x i32>* %481, align 32
  %482 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 30
  %483 = bitcast <4 x i64>* %482 to <8 x i32>*
  store <8 x i32> %380, <8 x i32>* %483, align 32
  %484 = mul <8 x i32> %308, %172
  %485 = mul <8 x i32> %303, %161
  %486 = add <8 x i32> %484, %301
  %487 = add <8 x i32> %486, %485
  %488 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %487, i32 %2) #8
  %489 = mul <8 x i32> %312, %239
  %490 = mul <8 x i32> %317, %172
  %491 = add <8 x i32> %489, %301
  %492 = add <8 x i32> %491, %490
  %493 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %492, i32 %2) #8
  %494 = mul <8 x i32> %326, %197
  %495 = mul <8 x i32> %321, %112
  %496 = add <8 x i32> %494, %301
  %497 = add <8 x i32> %496, %495
  %498 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %497, i32 %2) #8
  %499 = load <8 x i32>, <8 x i32>* %423, align 32
  %500 = mul <8 x i32> %499, %189
  %501 = mul <8 x i32> %335, %197
  %502 = add <8 x i32> %500, %301
  %503 = add <8 x i32> %502, %501
  %504 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %503, i32 %2) #8
  %505 = mul <8 x i32> %499, %197
  %506 = mul <8 x i32> %335, %112
  %507 = add <8 x i32> %505, %301
  %508 = add <8 x i32> %507, %506
  %509 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %508, i32 %2) #8
  store <8 x i32> %509, <8 x i32>* %441, align 32
  %510 = mul <8 x i32> %326, %112
  %511 = mul <8 x i32> %321, %122
  %512 = add <8 x i32> %510, %301
  %513 = add <8 x i32> %512, %511
  %514 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %513, i32 %2) #8
  store <8 x i32> %514, <8 x i32>* %443, align 32
  %515 = mul <8 x i32> %312, %172
  %516 = mul <8 x i32> %317, %161
  %517 = add <8 x i32> %515, %301
  %518 = add <8 x i32> %517, %516
  %519 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %518, i32 %2) #8
  store <8 x i32> %519, <8 x i32>* %445, align 32
  %520 = mul <8 x i32> %308, %161
  %521 = mul <8 x i32> %303, %53
  %522 = add <8 x i32> %520, %301
  %523 = add <8 x i32> %522, %521
  %524 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %523, i32 %2) #8
  store <8 x i32> %524, <8 x i32>* %447, align 32
  store <8 x i32> %488, <8 x i32>* %417, align 32
  store <8 x i32> %493, <8 x i32>* %419, align 32
  store <8 x i32> %498, <8 x i32>* %421, align 32
  store <8 x i32> %504, <8 x i32>* %423, align 32
  %525 = load <8 x i32>, <8 x i32>* %425, align 32
  %526 = mul <8 x i32> %525, %183
  %527 = load <8 x i32>, <8 x i32>* %439, align 32
  %528 = mul <8 x i32> %527, %130
  %529 = add <8 x i32> %526, %301
  %530 = add <8 x i32> %529, %528
  %531 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %530, i32 %2) #8
  %532 = load <8 x i32>, <8 x i32>* %427, align 32
  %533 = mul <8 x i32> %532, %203
  %534 = load <8 x i32>, <8 x i32>* %437, align 32
  %535 = mul <8 x i32> %534, %183
  %536 = add <8 x i32> %533, %301
  %537 = add <8 x i32> %536, %535
  %538 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %537, i32 %2) #8
  %539 = load <8 x i32>, <8 x i32>* %429, align 32
  %540 = mul <8 x i32> %539, %218
  %541 = load <8 x i32>, <8 x i32>* %435, align 32
  %542 = mul <8 x i32> %541, %85
  %543 = add <8 x i32> %540, %301
  %544 = add <8 x i32> %543, %542
  %545 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %544, i32 %2) #8
  %546 = load <8 x i32>, <8 x i32>* %431, align 32
  %547 = mul <8 x i32> %546, %178
  %548 = load <8 x i32>, <8 x i32>* %433, align 32
  %549 = mul <8 x i32> %548, %218
  %550 = add <8 x i32> %547, %301
  %551 = add <8 x i32> %550, %549
  %552 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %551, i32 %2) #8
  %553 = mul <8 x i32> %546, %218
  %554 = mul <8 x i32> %548, %85
  %555 = add <8 x i32> %553, %301
  %556 = add <8 x i32> %555, %554
  %557 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %556, i32 %2) #8
  store <8 x i32> %557, <8 x i32>* %433, align 32
  %558 = mul <8 x i32> %539, %85
  %559 = mul <8 x i32> %541, %141
  %560 = add <8 x i32> %558, %301
  %561 = add <8 x i32> %560, %559
  %562 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %561, i32 %2) #8
  store <8 x i32> %562, <8 x i32>* %435, align 32
  %563 = mul <8 x i32> %532, %183
  %564 = mul <8 x i32> %534, %130
  %565 = add <8 x i32> %563, %301
  %566 = add <8 x i32> %565, %564
  %567 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %566, i32 %2) #8
  store <8 x i32> %567, <8 x i32>* %437, align 32
  %568 = mul <8 x i32> %525, %130
  %569 = mul <8 x i32> %527, %104
  %570 = add <8 x i32> %568, %301
  %571 = add <8 x i32> %570, %569
  %572 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %571, i32 %2) #8
  store <8 x i32> %572, <8 x i32>* %439, align 32
  store <8 x i32> %531, <8 x i32>* %425, align 32
  store <8 x i32> %538, <8 x i32>* %427, align 32
  store <8 x i32> %545, <8 x i32>* %429, align 32
  store <8 x i32> %552, <8 x i32>* %431, align 32
  %573 = bitcast <4 x i64>* %292 to <8 x i32>*
  %574 = load <8 x i32>, <8 x i32>* %573, align 32
  %575 = mul <8 x i32> %574, %69
  %576 = add <8 x i32> %575, %301
  %577 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %576, i32 %2) #8
  %578 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 7
  %579 = bitcast <4 x i64>* %578 to <8 x i32>*
  store <8 x i32> %577, <8 x i32>* %579, align 32
  %580 = mul <8 x i32> %574, %153
  %581 = add <8 x i32> %580, %301
  %582 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %581, i32 %2) #8
  store <8 x i32> %582, <8 x i32>* %573, align 32
  %583 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 9
  %584 = bitcast <4 x i64>* %583 to <8 x i32>*
  store <8 x i32> %457, <8 x i32>* %584, align 32
  %585 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 10
  %586 = bitcast <4 x i64>* %585 to <8 x i32>*
  store <8 x i32> %462, <8 x i32>* %586, align 32
  %587 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 13
  %588 = bitcast <4 x i64>* %587 to <8 x i32>*
  store <8 x i32> %467, <8 x i32>* %588, align 32
  %589 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 14
  %590 = bitcast <4 x i64>* %589 to <8 x i32>*
  store <8 x i32> %452, <8 x i32>* %590, align 32
  %591 = load <8 x i32>, <8 x i32>* %469, align 32
  %592 = mul <8 x i32> %591, %175
  %593 = load <8 x i32>, <8 x i32>* %483, align 32
  %594 = mul <8 x i32> %593, %153
  %595 = add <8 x i32> %592, %301
  %596 = add <8 x i32> %595, %594
  %597 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %596, i32 %2) #8
  %598 = load <8 x i32>, <8 x i32>* %471, align 32
  %599 = mul <8 x i32> %598, %226
  %600 = load <8 x i32>, <8 x i32>* %481, align 32
  %601 = mul <8 x i32> %600, %175
  %602 = add <8 x i32> %599, %301
  %603 = add <8 x i32> %602, %601
  %604 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %603, i32 %2) #8
  %605 = load <8 x i32>, <8 x i32>* %473, align 32
  %606 = mul <8 x i32> %605, %200
  %607 = load <8 x i32>, <8 x i32>* %479, align 32
  %608 = mul <8 x i32> %607, %108
  %609 = add <8 x i32> %606, %301
  %610 = add <8 x i32> %609, %608
  %611 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %610, i32 %2) #8
  %612 = load <8 x i32>, <8 x i32>* %475, align 32
  %613 = mul <8 x i32> %612, %186
  %614 = load <8 x i32>, <8 x i32>* %477, align 32
  %615 = mul <8 x i32> %614, %200
  %616 = add <8 x i32> %613, %301
  %617 = add <8 x i32> %616, %615
  %618 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %617, i32 %2) #8
  %619 = mul <8 x i32> %612, %200
  %620 = mul <8 x i32> %614, %108
  %621 = add <8 x i32> %619, %301
  %622 = add <8 x i32> %621, %620
  %623 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %622, i32 %2) #8
  store <8 x i32> %623, <8 x i32>* %477, align 32
  %624 = mul <8 x i32> %605, %108
  %625 = mul <8 x i32> %607, %126
  %626 = add <8 x i32> %624, %301
  %627 = add <8 x i32> %626, %625
  %628 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %627, i32 %2) #8
  store <8 x i32> %628, <8 x i32>* %479, align 32
  %629 = mul <8 x i32> %598, %175
  %630 = mul <8 x i32> %600, %153
  %631 = add <8 x i32> %629, %301
  %632 = add <8 x i32> %631, %630
  %633 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %632, i32 %2) #8
  store <8 x i32> %633, <8 x i32>* %481, align 32
  %634 = mul <8 x i32> %591, %153
  %635 = mul <8 x i32> %593, %69
  %636 = add <8 x i32> %634, %301
  %637 = add <8 x i32> %636, %635
  %638 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %637, i32 %2) #8
  store <8 x i32> %638, <8 x i32>* %483, align 32
  store <8 x i32> %597, <8 x i32>* %469, align 32
  store <8 x i32> %604, <8 x i32>* %471, align 32
  store <8 x i32> %611, <8 x i32>* %473, align 32
  store <8 x i32> %618, <8 x i32>* %475, align 32
  %639 = load <8 x i32>, <8 x i32>* %32, align 32
  %640 = load <8 x i32>, <8 x i32>* %37, align 32
  br label %641

641:                                              ; preds = %6, %641
  %642 = phi i64 [ 32, %6 ], [ %714, %641 ]
  %643 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %642
  %644 = bitcast <4 x i64>* %643 to <8 x i32>*
  %645 = load <8 x i32>, <8 x i32>* %644, align 32
  %646 = or i64 %642, 3
  %647 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %646
  %648 = bitcast <4 x i64>* %647 to <8 x i32>*
  %649 = load <8 x i32>, <8 x i32>* %648, align 32
  %650 = add <8 x i32> %649, %645
  %651 = sub <8 x i32> %645, %649
  %652 = icmp sgt <8 x i32> %650, %639
  %653 = select <8 x i1> %652, <8 x i32> %650, <8 x i32> %639
  %654 = icmp slt <8 x i32> %653, %640
  %655 = select <8 x i1> %654, <8 x i32> %653, <8 x i32> %640
  %656 = icmp sgt <8 x i32> %651, %639
  %657 = select <8 x i1> %656, <8 x i32> %651, <8 x i32> %639
  %658 = icmp slt <8 x i32> %657, %640
  %659 = select <8 x i1> %658, <8 x i32> %657, <8 x i32> %640
  store <8 x i32> %655, <8 x i32>* %644, align 32
  store <8 x i32> %659, <8 x i32>* %648, align 32
  %660 = or i64 %642, 1
  %661 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %660
  %662 = bitcast <4 x i64>* %661 to <8 x i32>*
  %663 = load <8 x i32>, <8 x i32>* %662, align 32
  %664 = or i64 %642, 2
  %665 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %664
  %666 = bitcast <4 x i64>* %665 to <8 x i32>*
  %667 = load <8 x i32>, <8 x i32>* %666, align 32
  %668 = add <8 x i32> %667, %663
  %669 = sub <8 x i32> %663, %667
  %670 = icmp sgt <8 x i32> %668, %639
  %671 = select <8 x i1> %670, <8 x i32> %668, <8 x i32> %639
  %672 = icmp slt <8 x i32> %671, %640
  %673 = select <8 x i1> %672, <8 x i32> %671, <8 x i32> %640
  %674 = icmp sgt <8 x i32> %669, %639
  %675 = select <8 x i1> %674, <8 x i32> %669, <8 x i32> %639
  %676 = icmp slt <8 x i32> %675, %640
  %677 = select <8 x i1> %676, <8 x i32> %675, <8 x i32> %640
  store <8 x i32> %673, <8 x i32>* %662, align 32
  store <8 x i32> %677, <8 x i32>* %666, align 32
  %678 = or i64 %642, 7
  %679 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %678
  %680 = bitcast <4 x i64>* %679 to <8 x i32>*
  %681 = load <8 x i32>, <8 x i32>* %680, align 32
  %682 = or i64 %642, 4
  %683 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %682
  %684 = bitcast <4 x i64>* %683 to <8 x i32>*
  %685 = load <8 x i32>, <8 x i32>* %684, align 32
  %686 = add <8 x i32> %685, %681
  %687 = sub <8 x i32> %681, %685
  %688 = icmp sgt <8 x i32> %686, %639
  %689 = select <8 x i1> %688, <8 x i32> %686, <8 x i32> %639
  %690 = icmp slt <8 x i32> %689, %640
  %691 = select <8 x i1> %690, <8 x i32> %689, <8 x i32> %640
  %692 = icmp sgt <8 x i32> %687, %639
  %693 = select <8 x i1> %692, <8 x i32> %687, <8 x i32> %639
  %694 = icmp slt <8 x i32> %693, %640
  %695 = select <8 x i1> %694, <8 x i32> %693, <8 x i32> %640
  store <8 x i32> %691, <8 x i32>* %680, align 32
  store <8 x i32> %695, <8 x i32>* %684, align 32
  %696 = or i64 %642, 6
  %697 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %696
  %698 = bitcast <4 x i64>* %697 to <8 x i32>*
  %699 = load <8 x i32>, <8 x i32>* %698, align 32
  %700 = or i64 %642, 5
  %701 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %700
  %702 = bitcast <4 x i64>* %701 to <8 x i32>*
  %703 = load <8 x i32>, <8 x i32>* %702, align 32
  %704 = add <8 x i32> %703, %699
  %705 = sub <8 x i32> %699, %703
  %706 = icmp sgt <8 x i32> %704, %639
  %707 = select <8 x i1> %706, <8 x i32> %704, <8 x i32> %639
  %708 = icmp slt <8 x i32> %707, %640
  %709 = select <8 x i1> %708, <8 x i32> %707, <8 x i32> %640
  %710 = icmp sgt <8 x i32> %705, %639
  %711 = select <8 x i1> %710, <8 x i32> %705, <8 x i32> %639
  %712 = icmp slt <8 x i32> %711, %640
  %713 = select <8 x i1> %712, <8 x i32> %711, <8 x i32> %640
  store <8 x i32> %709, <8 x i32>* %698, align 32
  store <8 x i32> %713, <8 x i32>* %702, align 32
  %714 = add nuw nsw i64 %642, 8
  %715 = icmp ult i64 %714, 64
  br i1 %715, label %641, label %716

716:                                              ; preds = %641
  %717 = shufflevector <8 x i32> %100, <8 x i32> undef, <8 x i32> zeroinitializer
  %718 = shufflevector <8 x i32> %133, <8 x i32> undef, <8 x i32> zeroinitializer
  %719 = shufflevector <8 x i32> %180, <8 x i32> undef, <8 x i32> zeroinitializer
  %720 = shufflevector <8 x i32> %205, <8 x i32> undef, <8 x i32> zeroinitializer
  %721 = load <8 x i32>, <8 x i32>* %118, align 32
  %722 = bitcast [64 x <4 x i64>]* %12 to <8 x i32>*
  %723 = load <8 x i32>, <8 x i32>* %722, align 32
  %724 = mul <8 x i32> %723, %721
  %725 = add <8 x i32> %724, %301
  %726 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %725, i32 %2) #8
  %727 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 1
  %728 = bitcast <4 x i64>* %727 to <8 x i32>*
  store <8 x i32> %726, <8 x i32>* %728, align 32
  store <8 x i32> %726, <8 x i32>* %722, align 32
  %729 = load <4 x i64>, <4 x i64>* %292, align 32
  %730 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 5
  store <4 x i64> %729, <4 x i64>* %730, align 32
  %731 = load <4 x i64>, <4 x i64>* %578, align 32
  %732 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 6
  store <4 x i64> %731, <4 x i64>* %732, align 32
  %733 = load <8 x i32>, <8 x i32>* %584, align 32
  %734 = mul <8 x i32> %733, %719
  %735 = load <8 x i32>, <8 x i32>* %590, align 32
  %736 = mul <8 x i32> %735, %718
  %737 = add <8 x i32> %734, %301
  %738 = add <8 x i32> %737, %736
  %739 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %738, i32 %2) #8
  %740 = mul <8 x i32> %733, %718
  %741 = mul <8 x i32> %735, %717
  %742 = add <8 x i32> %740, %301
  %743 = add <8 x i32> %742, %741
  %744 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %743, i32 %2) #8
  store <8 x i32> %744, <8 x i32>* %590, align 32
  store <8 x i32> %739, <8 x i32>* %584, align 32
  %745 = load <8 x i32>, <8 x i32>* %586, align 32
  %746 = mul <8 x i32> %745, %720
  %747 = load <8 x i32>, <8 x i32>* %588, align 32
  %748 = mul <8 x i32> %747, %719
  %749 = add <8 x i32> %746, %301
  %750 = add <8 x i32> %749, %748
  %751 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %750, i32 %2) #8
  %752 = mul <8 x i32> %745, %719
  %753 = mul <8 x i32> %747, %718
  %754 = add <8 x i32> %752, %301
  %755 = add <8 x i32> %754, %753
  %756 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %755, i32 %2) #8
  store <8 x i32> %756, <8 x i32>* %588, align 32
  store <8 x i32> %751, <8 x i32>* %586, align 32
  %757 = load <8 x i32>, <8 x i32>* %376, align 32
  %758 = load <8 x i32>, <8 x i32>* %392, align 32
  %759 = add <8 x i32> %758, %757
  %760 = sub <8 x i32> %757, %758
  %761 = icmp sgt <8 x i32> %759, %639
  %762 = select <8 x i1> %761, <8 x i32> %759, <8 x i32> %639
  %763 = icmp slt <8 x i32> %762, %640
  %764 = select <8 x i1> %763, <8 x i32> %762, <8 x i32> %640
  %765 = icmp sgt <8 x i32> %760, %639
  %766 = select <8 x i1> %765, <8 x i32> %760, <8 x i32> %639
  %767 = icmp slt <8 x i32> %766, %640
  %768 = select <8 x i1> %767, <8 x i32> %766, <8 x i32> %640
  store <8 x i32> %764, <8 x i32>* %376, align 32
  store <8 x i32> %768, <8 x i32>* %392, align 32
  %769 = load <8 x i32>, <8 x i32>* %469, align 32
  %770 = load <8 x i32>, <8 x i32>* %471, align 32
  %771 = add <8 x i32> %770, %769
  %772 = sub <8 x i32> %769, %770
  %773 = icmp sgt <8 x i32> %771, %639
  %774 = select <8 x i1> %773, <8 x i32> %771, <8 x i32> %639
  %775 = icmp slt <8 x i32> %774, %640
  %776 = select <8 x i1> %775, <8 x i32> %774, <8 x i32> %640
  %777 = icmp sgt <8 x i32> %772, %639
  %778 = select <8 x i1> %777, <8 x i32> %772, <8 x i32> %639
  %779 = icmp slt <8 x i32> %778, %640
  %780 = select <8 x i1> %779, <8 x i32> %778, <8 x i32> %640
  store <8 x i32> %776, <8 x i32>* %469, align 32
  store <8 x i32> %780, <8 x i32>* %471, align 32
  %781 = load <8 x i32>, <8 x i32>* %412, align 32
  %782 = load <8 x i32>, <8 x i32>* %396, align 32
  %783 = add <8 x i32> %782, %781
  %784 = sub <8 x i32> %781, %782
  %785 = icmp sgt <8 x i32> %783, %639
  %786 = select <8 x i1> %785, <8 x i32> %783, <8 x i32> %639
  %787 = icmp slt <8 x i32> %786, %640
  %788 = select <8 x i1> %787, <8 x i32> %786, <8 x i32> %640
  %789 = icmp sgt <8 x i32> %784, %639
  %790 = select <8 x i1> %789, <8 x i32> %784, <8 x i32> %639
  %791 = icmp slt <8 x i32> %790, %640
  %792 = select <8 x i1> %791, <8 x i32> %790, <8 x i32> %640
  store <8 x i32> %788, <8 x i32>* %412, align 32
  store <8 x i32> %792, <8 x i32>* %396, align 32
  %793 = load <8 x i32>, <8 x i32>* %475, align 32
  %794 = load <8 x i32>, <8 x i32>* %473, align 32
  %795 = add <8 x i32> %794, %793
  %796 = sub <8 x i32> %793, %794
  %797 = icmp sgt <8 x i32> %795, %639
  %798 = select <8 x i1> %797, <8 x i32> %795, <8 x i32> %639
  %799 = icmp slt <8 x i32> %798, %640
  %800 = select <8 x i1> %799, <8 x i32> %798, <8 x i32> %640
  %801 = icmp sgt <8 x i32> %796, %639
  %802 = select <8 x i1> %801, <8 x i32> %796, <8 x i32> %639
  %803 = icmp slt <8 x i32> %802, %640
  %804 = select <8 x i1> %803, <8 x i32> %802, <8 x i32> %640
  store <8 x i32> %800, <8 x i32>* %475, align 32
  store <8 x i32> %804, <8 x i32>* %473, align 32
  %805 = load <8 x i32>, <8 x i32>* %406, align 32
  %806 = load <8 x i32>, <8 x i32>* %402, align 32
  %807 = add <8 x i32> %806, %805
  %808 = sub <8 x i32> %805, %806
  %809 = icmp sgt <8 x i32> %807, %639
  %810 = select <8 x i1> %809, <8 x i32> %807, <8 x i32> %639
  %811 = icmp slt <8 x i32> %810, %640
  %812 = select <8 x i1> %811, <8 x i32> %810, <8 x i32> %640
  %813 = icmp sgt <8 x i32> %808, %639
  %814 = select <8 x i1> %813, <8 x i32> %808, <8 x i32> %639
  %815 = icmp slt <8 x i32> %814, %640
  %816 = select <8 x i1> %815, <8 x i32> %814, <8 x i32> %640
  store <8 x i32> %812, <8 x i32>* %406, align 32
  store <8 x i32> %816, <8 x i32>* %402, align 32
  %817 = load <8 x i32>, <8 x i32>* %477, align 32
  %818 = load <8 x i32>, <8 x i32>* %479, align 32
  %819 = add <8 x i32> %818, %817
  %820 = sub <8 x i32> %817, %818
  %821 = icmp sgt <8 x i32> %819, %639
  %822 = select <8 x i1> %821, <8 x i32> %819, <8 x i32> %639
  %823 = icmp slt <8 x i32> %822, %640
  %824 = select <8 x i1> %823, <8 x i32> %822, <8 x i32> %640
  %825 = icmp sgt <8 x i32> %820, %639
  %826 = select <8 x i1> %825, <8 x i32> %820, <8 x i32> %639
  %827 = icmp slt <8 x i32> %826, %640
  %828 = select <8 x i1> %827, <8 x i32> %826, <8 x i32> %640
  store <8 x i32> %824, <8 x i32>* %477, align 32
  store <8 x i32> %828, <8 x i32>* %479, align 32
  %829 = load <8 x i32>, <8 x i32>* %382, align 32
  %830 = load <8 x i32>, <8 x i32>* %386, align 32
  %831 = add <8 x i32> %830, %829
  %832 = sub <8 x i32> %829, %830
  %833 = icmp sgt <8 x i32> %831, %639
  %834 = select <8 x i1> %833, <8 x i32> %831, <8 x i32> %639
  %835 = icmp slt <8 x i32> %834, %640
  %836 = select <8 x i1> %835, <8 x i32> %834, <8 x i32> %640
  %837 = icmp sgt <8 x i32> %832, %639
  %838 = select <8 x i1> %837, <8 x i32> %832, <8 x i32> %639
  %839 = icmp slt <8 x i32> %838, %640
  %840 = select <8 x i1> %839, <8 x i32> %838, <8 x i32> %640
  store <8 x i32> %836, <8 x i32>* %382, align 32
  store <8 x i32> %840, <8 x i32>* %386, align 32
  %841 = load <8 x i32>, <8 x i32>* %483, align 32
  %842 = load <8 x i32>, <8 x i32>* %481, align 32
  %843 = add <8 x i32> %842, %841
  %844 = sub <8 x i32> %841, %842
  %845 = icmp sgt <8 x i32> %843, %639
  %846 = select <8 x i1> %845, <8 x i32> %843, <8 x i32> %639
  %847 = icmp slt <8 x i32> %846, %640
  %848 = select <8 x i1> %847, <8 x i32> %846, <8 x i32> %640
  %849 = icmp sgt <8 x i32> %844, %639
  %850 = select <8 x i1> %849, <8 x i32> %844, <8 x i32> %639
  %851 = icmp slt <8 x i32> %850, %640
  %852 = select <8 x i1> %851, <8 x i32> %850, <8 x i32> %640
  store <8 x i32> %848, <8 x i32>* %483, align 32
  store <8 x i32> %852, <8 x i32>* %481, align 32
  %853 = load <8 x i32>, <8 x i32>* %419, align 32
  %854 = mul <8 x i32> %853, %175
  %855 = load <8 x i32>, <8 x i32>* %445, align 32
  %856 = mul <8 x i32> %855, %153
  %857 = add <8 x i32> %856, %854
  %858 = load <8 x i32>, <8 x i32>* %20, align 32
  %859 = add <8 x i32> %857, %858
  %860 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %859, i32 %2) #8
  %861 = load <8 x i32>, <8 x i32>* %314, align 32
  %862 = mul <8 x i32> %861, %175
  %863 = load <8 x i32>, <8 x i32>* %309, align 32
  %864 = mul <8 x i32> %863, %153
  %865 = add <8 x i32> %862, %858
  %866 = add <8 x i32> %865, %864
  %867 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %866, i32 %2) #8
  %868 = load <8 x i32>, <8 x i32>* %318, align 32
  %869 = mul <8 x i32> %868, %226
  %870 = load <8 x i32>, <8 x i32>* %323, align 32
  %871 = mul <8 x i32> %870, %175
  %872 = add <8 x i32> %869, %858
  %873 = add <8 x i32> %872, %871
  %874 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %873, i32 %2) #8
  %875 = load <8 x i32>, <8 x i32>* %421, align 32
  %876 = mul <8 x i32> %875, %226
  %877 = load <8 x i32>, <8 x i32>* %443, align 32
  %878 = mul <8 x i32> %877, %175
  %879 = add <8 x i32> %876, %858
  %880 = add <8 x i32> %879, %878
  %881 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %880, i32 %2) #8
  %882 = mul <8 x i32> %875, %175
  %883 = mul <8 x i32> %877, %153
  %884 = add <8 x i32> %882, %858
  %885 = add <8 x i32> %884, %883
  %886 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %885, i32 %2) #8
  store <8 x i32> %886, <8 x i32>* %443, align 32
  %887 = mul <8 x i32> %868, %175
  %888 = mul <8 x i32> %870, %153
  %889 = add <8 x i32> %887, %858
  %890 = add <8 x i32> %889, %888
  %891 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %890, i32 %2) #8
  store <8 x i32> %891, <8 x i32>* %323, align 32
  %892 = mul <8 x i32> %861, %153
  %893 = mul <8 x i32> %863, %69
  %894 = add <8 x i32> %892, %858
  %895 = add <8 x i32> %894, %893
  %896 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %895, i32 %2) #8
  store <8 x i32> %896, <8 x i32>* %309, align 32
  %897 = mul <8 x i32> %853, %153
  %898 = mul <8 x i32> %855, %69
  %899 = add <8 x i32> %898, %897
  %900 = add <8 x i32> %899, %858
  %901 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %900, i32 %2) #8
  store <8 x i32> %901, <8 x i32>* %445, align 32
  store <8 x i32> %860, <8 x i32>* %419, align 32
  store <8 x i32> %867, <8 x i32>* %314, align 32
  store <8 x i32> %874, <8 x i32>* %318, align 32
  store <8 x i32> %881, <8 x i32>* %421, align 32
  %902 = load <8 x i32>, <8 x i32>* %427, align 32
  %903 = mul <8 x i32> %902, %200
  %904 = load <8 x i32>, <8 x i32>* %437, align 32
  %905 = mul <8 x i32> %904, %108
  %906 = add <8 x i32> %903, %858
  %907 = add <8 x i32> %906, %905
  %908 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %907, i32 %2) #8
  %909 = load <8 x i32>, <8 x i32>* %352, align 32
  %910 = mul <8 x i32> %909, %200
  %911 = load <8 x i32>, <8 x i32>* %346, align 32
  %912 = mul <8 x i32> %911, %108
  %913 = add <8 x i32> %910, %858
  %914 = add <8 x i32> %913, %912
  %915 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %914, i32 %2) #8
  %916 = load <8 x i32>, <8 x i32>* %366, align 32
  %917 = mul <8 x i32> %916, %186
  %918 = load <8 x i32>, <8 x i32>* %372, align 32
  %919 = mul <8 x i32> %918, %200
  %920 = add <8 x i32> %917, %858
  %921 = add <8 x i32> %920, %919
  %922 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %921, i32 %2) #8
  %923 = load <8 x i32>, <8 x i32>* %429, align 32
  %924 = mul <8 x i32> %923, %186
  %925 = load <8 x i32>, <8 x i32>* %435, align 32
  %926 = mul <8 x i32> %925, %200
  %927 = add <8 x i32> %924, %858
  %928 = add <8 x i32> %927, %926
  %929 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %928, i32 %2) #8
  %930 = mul <8 x i32> %923, %200
  %931 = mul <8 x i32> %925, %108
  %932 = add <8 x i32> %930, %858
  %933 = add <8 x i32> %932, %931
  %934 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %933, i32 %2) #8
  store <8 x i32> %934, <8 x i32>* %435, align 32
  %935 = mul <8 x i32> %916, %200
  %936 = mul <8 x i32> %918, %108
  %937 = add <8 x i32> %935, %858
  %938 = add <8 x i32> %937, %936
  %939 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %938, i32 %2) #8
  store <8 x i32> %939, <8 x i32>* %372, align 32
  %940 = mul <8 x i32> %909, %108
  %941 = mul <8 x i32> %911, %126
  %942 = add <8 x i32> %940, %858
  %943 = add <8 x i32> %942, %941
  %944 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %943, i32 %2) #8
  store <8 x i32> %944, <8 x i32>* %346, align 32
  %945 = mul <8 x i32> %902, %108
  %946 = mul <8 x i32> %904, %126
  %947 = add <8 x i32> %945, %858
  %948 = add <8 x i32> %947, %946
  %949 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %948, i32 %2) #8
  store <8 x i32> %949, <8 x i32>* %437, align 32
  store <8 x i32> %908, <8 x i32>* %427, align 32
  store <8 x i32> %915, <8 x i32>* %352, align 32
  store <8 x i32> %922, <8 x i32>* %366, align 32
  store <8 x i32> %929, <8 x i32>* %429, align 32
  %950 = load <4 x i64>, <4 x i64>* %249, align 32
  %951 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 3
  store <4 x i64> %950, <4 x i64>* %951, align 32
  %952 = load <4 x i64>, <4 x i64>* %727, align 32
  %953 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 2
  store <4 x i64> %952, <4 x i64>* %953, align 32
  %954 = load <8 x i32>, <8 x i32>* %194, align 32
  %955 = bitcast <4 x i64>* %730 to <8 x i32>*
  %956 = load <8 x i32>, <8 x i32>* %955, align 32
  %957 = mul <8 x i32> %956, %954
  %958 = bitcast <4 x i64>* %732 to <8 x i32>*
  %959 = load <8 x i32>, <8 x i32>* %958, align 32
  %960 = mul <8 x i32> %959, %721
  %961 = add <8 x i32> %960, %858
  %962 = add <8 x i32> %961, %957
  %963 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %962, i32 %2) #8
  %964 = mul <8 x i32> %956, %721
  %965 = add <8 x i32> %961, %964
  %966 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %965, i32 %2) #8
  store <8 x i32> %966, <8 x i32>* %958, align 32
  store <8 x i32> %963, <8 x i32>* %955, align 32
  %967 = load <8 x i32>, <8 x i32>* %448, align 32
  %968 = load <8 x i32>, <8 x i32>* %464, align 32
  %969 = add <8 x i32> %968, %967
  %970 = sub <8 x i32> %967, %968
  %971 = load <8 x i32>, <8 x i32>* %32, align 32
  %972 = icmp sgt <8 x i32> %969, %971
  %973 = select <8 x i1> %972, <8 x i32> %969, <8 x i32> %971
  %974 = load <8 x i32>, <8 x i32>* %37, align 32
  %975 = icmp slt <8 x i32> %973, %974
  %976 = select <8 x i1> %975, <8 x i32> %973, <8 x i32> %974
  %977 = icmp sgt <8 x i32> %970, %971
  %978 = select <8 x i1> %977, <8 x i32> %970, <8 x i32> %971
  %979 = icmp slt <8 x i32> %978, %974
  %980 = select <8 x i1> %979, <8 x i32> %978, <8 x i32> %974
  store <8 x i32> %976, <8 x i32>* %448, align 32
  store <8 x i32> %980, <8 x i32>* %464, align 32
  %981 = load <8 x i32>, <8 x i32>* %584, align 32
  %982 = load <8 x i32>, <8 x i32>* %586, align 32
  %983 = add <8 x i32> %982, %981
  %984 = sub <8 x i32> %981, %982
  %985 = icmp sgt <8 x i32> %983, %971
  %986 = select <8 x i1> %985, <8 x i32> %983, <8 x i32> %971
  %987 = icmp slt <8 x i32> %986, %974
  %988 = select <8 x i1> %987, <8 x i32> %986, <8 x i32> %974
  %989 = icmp sgt <8 x i32> %984, %971
  %990 = select <8 x i1> %989, <8 x i32> %984, <8 x i32> %971
  %991 = icmp slt <8 x i32> %990, %974
  %992 = select <8 x i1> %991, <8 x i32> %990, <8 x i32> %974
  store <8 x i32> %988, <8 x i32>* %584, align 32
  store <8 x i32> %992, <8 x i32>* %586, align 32
  %993 = load <8 x i32>, <8 x i32>* %454, align 32
  %994 = load <8 x i32>, <8 x i32>* %458, align 32
  %995 = add <8 x i32> %994, %993
  %996 = sub <8 x i32> %993, %994
  %997 = icmp sgt <8 x i32> %995, %971
  %998 = select <8 x i1> %997, <8 x i32> %995, <8 x i32> %971
  %999 = icmp slt <8 x i32> %998, %974
  %1000 = select <8 x i1> %999, <8 x i32> %998, <8 x i32> %974
  %1001 = icmp sgt <8 x i32> %996, %971
  %1002 = select <8 x i1> %1001, <8 x i32> %996, <8 x i32> %971
  %1003 = icmp slt <8 x i32> %1002, %974
  %1004 = select <8 x i1> %1003, <8 x i32> %1002, <8 x i32> %974
  store <8 x i32> %1000, <8 x i32>* %454, align 32
  store <8 x i32> %1004, <8 x i32>* %458, align 32
  %1005 = load <8 x i32>, <8 x i32>* %590, align 32
  %1006 = load <8 x i32>, <8 x i32>* %588, align 32
  %1007 = add <8 x i32> %1006, %1005
  %1008 = sub <8 x i32> %1005, %1006
  %1009 = icmp sgt <8 x i32> %1007, %971
  %1010 = select <8 x i1> %1009, <8 x i32> %1007, <8 x i32> %971
  %1011 = icmp slt <8 x i32> %1010, %974
  %1012 = select <8 x i1> %1011, <8 x i32> %1010, <8 x i32> %974
  %1013 = icmp sgt <8 x i32> %1008, %971
  %1014 = select <8 x i1> %1013, <8 x i32> %1008, <8 x i32> %971
  %1015 = icmp slt <8 x i32> %1014, %974
  %1016 = select <8 x i1> %1015, <8 x i32> %1014, <8 x i32> %974
  store <8 x i32> %1012, <8 x i32>* %590, align 32
  store <8 x i32> %1016, <8 x i32>* %588, align 32
  %1017 = load <8 x i32>, <8 x i32>* %471, align 32
  %1018 = mul <8 x i32> %1017, %719
  %1019 = load <8 x i32>, <8 x i32>* %481, align 32
  %1020 = mul <8 x i32> %1019, %718
  %1021 = add <8 x i32> %1018, %858
  %1022 = add <8 x i32> %1021, %1020
  %1023 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1022, i32 %2) #8
  %1024 = load <8 x i32>, <8 x i32>* %392, align 32
  %1025 = mul <8 x i32> %1024, %719
  %1026 = load <8 x i32>, <8 x i32>* %386, align 32
  %1027 = mul <8 x i32> %1026, %718
  %1028 = add <8 x i32> %1025, %858
  %1029 = add <8 x i32> %1028, %1027
  %1030 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1029, i32 %2) #8
  %1031 = load <8 x i32>, <8 x i32>* %396, align 32
  %1032 = mul <8 x i32> %1031, %720
  %1033 = load <8 x i32>, <8 x i32>* %402, align 32
  %1034 = mul <8 x i32> %1033, %719
  %1035 = add <8 x i32> %1032, %858
  %1036 = add <8 x i32> %1035, %1034
  %1037 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1036, i32 %2) #8
  %1038 = load <8 x i32>, <8 x i32>* %473, align 32
  %1039 = mul <8 x i32> %1038, %720
  %1040 = load <8 x i32>, <8 x i32>* %479, align 32
  %1041 = mul <8 x i32> %1040, %719
  %1042 = add <8 x i32> %1039, %858
  %1043 = add <8 x i32> %1042, %1041
  %1044 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1043, i32 %2) #8
  %1045 = mul <8 x i32> %1038, %719
  %1046 = mul <8 x i32> %1040, %718
  %1047 = add <8 x i32> %1045, %858
  %1048 = add <8 x i32> %1047, %1046
  %1049 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1048, i32 %2) #8
  store <8 x i32> %1049, <8 x i32>* %479, align 32
  %1050 = mul <8 x i32> %1031, %719
  %1051 = mul <8 x i32> %1033, %718
  %1052 = add <8 x i32> %1050, %858
  %1053 = add <8 x i32> %1052, %1051
  %1054 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1053, i32 %2) #8
  store <8 x i32> %1054, <8 x i32>* %402, align 32
  %1055 = mul <8 x i32> %1024, %718
  %1056 = mul <8 x i32> %1026, %717
  %1057 = add <8 x i32> %1055, %858
  %1058 = add <8 x i32> %1057, %1056
  %1059 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1058, i32 %2) #8
  store <8 x i32> %1059, <8 x i32>* %386, align 32
  %1060 = mul <8 x i32> %1017, %718
  %1061 = mul <8 x i32> %1019, %717
  %1062 = add <8 x i32> %1060, %858
  %1063 = add <8 x i32> %1062, %1061
  %1064 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1063, i32 %2) #8
  store <8 x i32> %1064, <8 x i32>* %481, align 32
  store <8 x i32> %1023, <8 x i32>* %471, align 32
  store <8 x i32> %1030, <8 x i32>* %392, align 32
  store <8 x i32> %1037, <8 x i32>* %396, align 32
  store <8 x i32> %1044, <8 x i32>* %473, align 32
  br label %1065

1065:                                             ; preds = %716, %1065
  %1066 = phi i64 [ 32, %716 ], [ %1225, %1065 ]
  %1067 = phi i32 [ 32, %716 ], [ %1223, %1065 ]
  %1068 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %1066
  %1069 = bitcast <4 x i64>* %1068 to <8 x i32>*
  %1070 = load <8 x i32>, <8 x i32>* %1069, align 32
  %1071 = and i64 %1066, 4294967280
  %1072 = or i64 %1071, 7
  %1073 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %1072
  %1074 = bitcast <4 x i64>* %1073 to <8 x i32>*
  %1075 = load <8 x i32>, <8 x i32>* %1074, align 32
  %1076 = add <8 x i32> %1075, %1070
  %1077 = sub <8 x i32> %1070, %1075
  %1078 = icmp sgt <8 x i32> %1076, %971
  %1079 = select <8 x i1> %1078, <8 x i32> %1076, <8 x i32> %971
  %1080 = icmp slt <8 x i32> %1079, %974
  %1081 = select <8 x i1> %1080, <8 x i32> %1079, <8 x i32> %974
  %1082 = icmp sgt <8 x i32> %1077, %971
  %1083 = select <8 x i1> %1082, <8 x i32> %1077, <8 x i32> %971
  %1084 = icmp slt <8 x i32> %1083, %974
  %1085 = select <8 x i1> %1084, <8 x i32> %1083, <8 x i32> %974
  store <8 x i32> %1081, <8 x i32>* %1069, align 32
  store <8 x i32> %1085, <8 x i32>* %1074, align 32
  %1086 = and i64 %1066, 4294967280
  %1087 = or i64 %1086, 15
  %1088 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %1087
  %1089 = bitcast <4 x i64>* %1088 to <8 x i32>*
  %1090 = load <8 x i32>, <8 x i32>* %1089, align 32
  %1091 = and i64 %1066, 4294967280
  %1092 = or i64 %1091, 8
  %1093 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %1092
  %1094 = bitcast <4 x i64>* %1093 to <8 x i32>*
  %1095 = load <8 x i32>, <8 x i32>* %1094, align 32
  %1096 = add <8 x i32> %1095, %1090
  %1097 = sub <8 x i32> %1090, %1095
  %1098 = icmp sgt <8 x i32> %1096, %971
  %1099 = select <8 x i1> %1098, <8 x i32> %1096, <8 x i32> %971
  %1100 = icmp slt <8 x i32> %1099, %974
  %1101 = select <8 x i1> %1100, <8 x i32> %1099, <8 x i32> %974
  %1102 = icmp sgt <8 x i32> %1097, %971
  %1103 = select <8 x i1> %1102, <8 x i32> %1097, <8 x i32> %971
  %1104 = icmp slt <8 x i32> %1103, %974
  %1105 = select <8 x i1> %1104, <8 x i32> %1103, <8 x i32> %974
  store <8 x i32> %1101, <8 x i32>* %1089, align 32
  store <8 x i32> %1105, <8 x i32>* %1094, align 32
  %1106 = or i64 %1066, 1
  %1107 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %1106
  %1108 = bitcast <4 x i64>* %1107 to <8 x i32>*
  %1109 = load <8 x i32>, <8 x i32>* %1108, align 32
  %1110 = and i64 %1066, 4294967280
  %1111 = or i64 %1110, 6
  %1112 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %1111
  %1113 = bitcast <4 x i64>* %1112 to <8 x i32>*
  %1114 = load <8 x i32>, <8 x i32>* %1113, align 32
  %1115 = add <8 x i32> %1114, %1109
  %1116 = sub <8 x i32> %1109, %1114
  %1117 = icmp sgt <8 x i32> %1115, %971
  %1118 = select <8 x i1> %1117, <8 x i32> %1115, <8 x i32> %971
  %1119 = icmp slt <8 x i32> %1118, %974
  %1120 = select <8 x i1> %1119, <8 x i32> %1118, <8 x i32> %974
  %1121 = icmp sgt <8 x i32> %1116, %971
  %1122 = select <8 x i1> %1121, <8 x i32> %1116, <8 x i32> %971
  %1123 = icmp slt <8 x i32> %1122, %974
  %1124 = select <8 x i1> %1123, <8 x i32> %1122, <8 x i32> %974
  store <8 x i32> %1120, <8 x i32>* %1108, align 32
  store <8 x i32> %1124, <8 x i32>* %1113, align 32
  %1125 = and i64 %1066, 4294967280
  %1126 = or i64 %1125, 14
  %1127 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %1126
  %1128 = bitcast <4 x i64>* %1127 to <8 x i32>*
  %1129 = load <8 x i32>, <8 x i32>* %1128, align 32
  %1130 = and i64 %1106, 4294967281
  %1131 = or i64 %1130, 8
  %1132 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %1131
  %1133 = bitcast <4 x i64>* %1132 to <8 x i32>*
  %1134 = load <8 x i32>, <8 x i32>* %1133, align 32
  %1135 = add <8 x i32> %1134, %1129
  %1136 = sub <8 x i32> %1129, %1134
  %1137 = icmp sgt <8 x i32> %1135, %971
  %1138 = select <8 x i1> %1137, <8 x i32> %1135, <8 x i32> %971
  %1139 = icmp slt <8 x i32> %1138, %974
  %1140 = select <8 x i1> %1139, <8 x i32> %1138, <8 x i32> %974
  %1141 = icmp sgt <8 x i32> %1136, %971
  %1142 = select <8 x i1> %1141, <8 x i32> %1136, <8 x i32> %971
  %1143 = icmp slt <8 x i32> %1142, %974
  %1144 = select <8 x i1> %1143, <8 x i32> %1142, <8 x i32> %974
  store <8 x i32> %1140, <8 x i32>* %1128, align 32
  store <8 x i32> %1144, <8 x i32>* %1133, align 32
  %1145 = add nuw nsw i64 %1106, 1
  %1146 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %1145
  %1147 = bitcast <4 x i64>* %1146 to <8 x i32>*
  %1148 = load <8 x i32>, <8 x i32>* %1147, align 32
  %1149 = and i64 %1145, 4294967280
  %1150 = or i64 %1149, 5
  %1151 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %1150
  %1152 = bitcast <4 x i64>* %1151 to <8 x i32>*
  %1153 = load <8 x i32>, <8 x i32>* %1152, align 32
  %1154 = add <8 x i32> %1153, %1148
  %1155 = sub <8 x i32> %1148, %1153
  %1156 = icmp sgt <8 x i32> %1154, %971
  %1157 = select <8 x i1> %1156, <8 x i32> %1154, <8 x i32> %971
  %1158 = icmp slt <8 x i32> %1157, %974
  %1159 = select <8 x i1> %1158, <8 x i32> %1157, <8 x i32> %974
  %1160 = icmp sgt <8 x i32> %1155, %971
  %1161 = select <8 x i1> %1160, <8 x i32> %1155, <8 x i32> %971
  %1162 = icmp slt <8 x i32> %1161, %974
  %1163 = select <8 x i1> %1162, <8 x i32> %1161, <8 x i32> %974
  store <8 x i32> %1159, <8 x i32>* %1147, align 32
  store <8 x i32> %1163, <8 x i32>* %1152, align 32
  %1164 = and i64 %1145, 4294967280
  %1165 = or i64 %1164, 13
  %1166 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %1165
  %1167 = bitcast <4 x i64>* %1166 to <8 x i32>*
  %1168 = load <8 x i32>, <8 x i32>* %1167, align 32
  %1169 = and i64 %1145, 4294967282
  %1170 = or i64 %1169, 8
  %1171 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %1170
  %1172 = bitcast <4 x i64>* %1171 to <8 x i32>*
  %1173 = load <8 x i32>, <8 x i32>* %1172, align 32
  %1174 = add <8 x i32> %1173, %1168
  %1175 = sub <8 x i32> %1168, %1173
  %1176 = icmp sgt <8 x i32> %1174, %971
  %1177 = select <8 x i1> %1176, <8 x i32> %1174, <8 x i32> %971
  %1178 = icmp slt <8 x i32> %1177, %974
  %1179 = select <8 x i1> %1178, <8 x i32> %1177, <8 x i32> %974
  %1180 = icmp sgt <8 x i32> %1175, %971
  %1181 = select <8 x i1> %1180, <8 x i32> %1175, <8 x i32> %971
  %1182 = icmp slt <8 x i32> %1181, %974
  %1183 = select <8 x i1> %1182, <8 x i32> %1181, <8 x i32> %974
  store <8 x i32> %1179, <8 x i32>* %1167, align 32
  store <8 x i32> %1183, <8 x i32>* %1172, align 32
  %1184 = or i64 %1066, 3
  %1185 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %1184
  %1186 = bitcast <4 x i64>* %1185 to <8 x i32>*
  %1187 = load <8 x i32>, <8 x i32>* %1186, align 32
  %1188 = and i64 %1066, 4294967280
  %1189 = or i64 %1188, 4
  %1190 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %1189
  %1191 = bitcast <4 x i64>* %1190 to <8 x i32>*
  %1192 = load <8 x i32>, <8 x i32>* %1191, align 32
  %1193 = add <8 x i32> %1192, %1187
  %1194 = sub <8 x i32> %1187, %1192
  %1195 = icmp sgt <8 x i32> %1193, %971
  %1196 = select <8 x i1> %1195, <8 x i32> %1193, <8 x i32> %971
  %1197 = icmp slt <8 x i32> %1196, %974
  %1198 = select <8 x i1> %1197, <8 x i32> %1196, <8 x i32> %974
  %1199 = icmp sgt <8 x i32> %1194, %971
  %1200 = select <8 x i1> %1199, <8 x i32> %1194, <8 x i32> %971
  %1201 = icmp slt <8 x i32> %1200, %974
  %1202 = select <8 x i1> %1201, <8 x i32> %1200, <8 x i32> %974
  store <8 x i32> %1198, <8 x i32>* %1186, align 32
  store <8 x i32> %1202, <8 x i32>* %1191, align 32
  %1203 = and i64 %1066, 4294967280
  %1204 = or i64 %1203, 12
  %1205 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %1204
  %1206 = bitcast <4 x i64>* %1205 to <8 x i32>*
  %1207 = load <8 x i32>, <8 x i32>* %1206, align 32
  %1208 = and i64 %1184, 4294967283
  %1209 = or i64 %1208, 8
  %1210 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %1209
  %1211 = bitcast <4 x i64>* %1210 to <8 x i32>*
  %1212 = load <8 x i32>, <8 x i32>* %1211, align 32
  %1213 = add <8 x i32> %1212, %1207
  %1214 = sub <8 x i32> %1207, %1212
  %1215 = icmp sgt <8 x i32> %1213, %971
  %1216 = select <8 x i1> %1215, <8 x i32> %1213, <8 x i32> %971
  %1217 = icmp slt <8 x i32> %1216, %974
  %1218 = select <8 x i1> %1217, <8 x i32> %1216, <8 x i32> %974
  %1219 = icmp sgt <8 x i32> %1214, %971
  %1220 = select <8 x i1> %1219, <8 x i32> %1214, <8 x i32> %971
  %1221 = icmp slt <8 x i32> %1220, %974
  %1222 = select <8 x i1> %1221, <8 x i32> %1220, <8 x i32> %974
  store <8 x i32> %1218, <8 x i32>* %1206, align 32
  store <8 x i32> %1222, <8 x i32>* %1211, align 32
  %1223 = add nuw nsw i32 %1067, 16
  %1224 = icmp ult i32 %1223, 64
  %1225 = add nuw nsw i64 %1066, 16
  br i1 %1224, label %1065, label %1226

1226:                                             ; preds = %1065
  %1227 = bitcast [64 x <4 x i64>]* %12 to <8 x i32>*
  %1228 = load <8 x i32>, <8 x i32>* %1227, align 32
  %1229 = load <8 x i32>, <8 x i32>* %579, align 32
  %1230 = add <8 x i32> %1229, %1228
  %1231 = sub <8 x i32> %1228, %1229
  %1232 = icmp sgt <8 x i32> %1230, %971
  %1233 = select <8 x i1> %1232, <8 x i32> %1230, <8 x i32> %971
  %1234 = icmp slt <8 x i32> %1233, %974
  %1235 = select <8 x i1> %1234, <8 x i32> %1233, <8 x i32> %974
  %1236 = icmp sgt <8 x i32> %1231, %971
  %1237 = select <8 x i1> %1236, <8 x i32> %1231, <8 x i32> %971
  %1238 = icmp slt <8 x i32> %1237, %974
  %1239 = select <8 x i1> %1238, <8 x i32> %1237, <8 x i32> %974
  store <8 x i32> %1235, <8 x i32>* %1227, align 32
  store <8 x i32> %1239, <8 x i32>* %579, align 32
  %1240 = load <8 x i32>, <8 x i32>* %728, align 32
  %1241 = load <8 x i32>, <8 x i32>* %958, align 32
  %1242 = add <8 x i32> %1241, %1240
  %1243 = sub <8 x i32> %1240, %1241
  %1244 = icmp sgt <8 x i32> %1242, %971
  %1245 = select <8 x i1> %1244, <8 x i32> %1242, <8 x i32> %971
  %1246 = icmp slt <8 x i32> %1245, %974
  %1247 = select <8 x i1> %1246, <8 x i32> %1245, <8 x i32> %974
  %1248 = icmp sgt <8 x i32> %1243, %971
  %1249 = select <8 x i1> %1248, <8 x i32> %1243, <8 x i32> %971
  %1250 = icmp slt <8 x i32> %1249, %974
  %1251 = select <8 x i1> %1250, <8 x i32> %1249, <8 x i32> %974
  store <8 x i32> %1247, <8 x i32>* %728, align 32
  store <8 x i32> %1251, <8 x i32>* %958, align 32
  %1252 = bitcast <4 x i64>* %953 to <8 x i32>*
  %1253 = load <8 x i32>, <8 x i32>* %1252, align 32
  %1254 = load <8 x i32>, <8 x i32>* %955, align 32
  %1255 = add <8 x i32> %1254, %1253
  %1256 = sub <8 x i32> %1253, %1254
  %1257 = icmp sgt <8 x i32> %1255, %971
  %1258 = select <8 x i1> %1257, <8 x i32> %1255, <8 x i32> %971
  %1259 = icmp slt <8 x i32> %1258, %974
  %1260 = select <8 x i1> %1259, <8 x i32> %1258, <8 x i32> %974
  %1261 = icmp sgt <8 x i32> %1256, %971
  %1262 = select <8 x i1> %1261, <8 x i32> %1256, <8 x i32> %971
  %1263 = icmp slt <8 x i32> %1262, %974
  %1264 = select <8 x i1> %1263, <8 x i32> %1262, <8 x i32> %974
  store <8 x i32> %1260, <8 x i32>* %1252, align 32
  store <8 x i32> %1264, <8 x i32>* %955, align 32
  %1265 = bitcast <4 x i64>* %951 to <8 x i32>*
  %1266 = load <8 x i32>, <8 x i32>* %1265, align 32
  %1267 = load <8 x i32>, <8 x i32>* %573, align 32
  %1268 = add <8 x i32> %1267, %1266
  %1269 = sub <8 x i32> %1266, %1267
  %1270 = icmp sgt <8 x i32> %1268, %971
  %1271 = select <8 x i1> %1270, <8 x i32> %1268, <8 x i32> %971
  %1272 = icmp slt <8 x i32> %1271, %974
  %1273 = select <8 x i1> %1272, <8 x i32> %1271, <8 x i32> %974
  %1274 = icmp sgt <8 x i32> %1269, %971
  %1275 = select <8 x i1> %1274, <8 x i32> %1269, <8 x i32> %971
  %1276 = icmp slt <8 x i32> %1275, %974
  %1277 = select <8 x i1> %1276, <8 x i32> %1275, <8 x i32> %974
  store <8 x i32> %1273, <8 x i32>* %1265, align 32
  store <8 x i32> %1277, <8 x i32>* %573, align 32
  %1278 = load <8 x i32>, <8 x i32>* %586, align 32
  %1279 = mul <8 x i32> %1278, %954
  %1280 = load <8 x i32>, <8 x i32>* %588, align 32
  %1281 = mul <8 x i32> %1280, %721
  %1282 = add <8 x i32> %1281, %858
  %1283 = add <8 x i32> %1282, %1279
  %1284 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1283, i32 %2) #8
  %1285 = mul <8 x i32> %1278, %721
  %1286 = add <8 x i32> %1282, %1285
  %1287 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1286, i32 %2) #8
  store <8 x i32> %1287, <8 x i32>* %588, align 32
  store <8 x i32> %1284, <8 x i32>* %586, align 32
  %1288 = load <8 x i32>, <8 x i32>* %464, align 32
  %1289 = mul <8 x i32> %1288, %954
  %1290 = load <8 x i32>, <8 x i32>* %458, align 32
  %1291 = mul <8 x i32> %1290, %721
  %1292 = add <8 x i32> %1291, %858
  %1293 = add <8 x i32> %1292, %1289
  %1294 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1293, i32 %2) #8
  %1295 = mul <8 x i32> %1288, %721
  %1296 = add <8 x i32> %1292, %1295
  %1297 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1296, i32 %2) #8
  store <8 x i32> %1297, <8 x i32>* %458, align 32
  store <8 x i32> %1294, <8 x i32>* %464, align 32
  %1298 = load <8 x i32>, <8 x i32>* %376, align 32
  %1299 = load <8 x i32>, <8 x i32>* %412, align 32
  %1300 = add <8 x i32> %1299, %1298
  %1301 = sub <8 x i32> %1298, %1299
  %1302 = icmp sgt <8 x i32> %1300, %971
  %1303 = select <8 x i1> %1302, <8 x i32> %1300, <8 x i32> %971
  %1304 = icmp slt <8 x i32> %1303, %974
  %1305 = select <8 x i1> %1304, <8 x i32> %1303, <8 x i32> %974
  %1306 = icmp sgt <8 x i32> %1301, %971
  %1307 = select <8 x i1> %1306, <8 x i32> %1301, <8 x i32> %971
  %1308 = icmp slt <8 x i32> %1307, %974
  %1309 = select <8 x i1> %1308, <8 x i32> %1307, <8 x i32> %974
  store <8 x i32> %1305, <8 x i32>* %376, align 32
  store <8 x i32> %1309, <8 x i32>* %412, align 32
  %1310 = load <8 x i32>, <8 x i32>* %382, align 32
  %1311 = load <8 x i32>, <8 x i32>* %406, align 32
  %1312 = add <8 x i32> %1311, %1310
  %1313 = sub <8 x i32> %1310, %1311
  %1314 = icmp sgt <8 x i32> %1312, %971
  %1315 = select <8 x i1> %1314, <8 x i32> %1312, <8 x i32> %971
  %1316 = icmp slt <8 x i32> %1315, %974
  %1317 = select <8 x i1> %1316, <8 x i32> %1315, <8 x i32> %974
  %1318 = icmp sgt <8 x i32> %1313, %971
  %1319 = select <8 x i1> %1318, <8 x i32> %1313, <8 x i32> %971
  %1320 = icmp slt <8 x i32> %1319, %974
  %1321 = select <8 x i1> %1320, <8 x i32> %1319, <8 x i32> %974
  store <8 x i32> %1317, <8 x i32>* %382, align 32
  store <8 x i32> %1321, <8 x i32>* %406, align 32
  %1322 = load <8 x i32>, <8 x i32>* %469, align 32
  %1323 = load <8 x i32>, <8 x i32>* %475, align 32
  %1324 = add <8 x i32> %1323, %1322
  %1325 = sub <8 x i32> %1322, %1323
  %1326 = icmp sgt <8 x i32> %1324, %971
  %1327 = select <8 x i1> %1326, <8 x i32> %1324, <8 x i32> %971
  %1328 = icmp slt <8 x i32> %1327, %974
  %1329 = select <8 x i1> %1328, <8 x i32> %1327, <8 x i32> %974
  %1330 = icmp sgt <8 x i32> %1325, %971
  %1331 = select <8 x i1> %1330, <8 x i32> %1325, <8 x i32> %971
  %1332 = icmp slt <8 x i32> %1331, %974
  %1333 = select <8 x i1> %1332, <8 x i32> %1331, <8 x i32> %974
  store <8 x i32> %1329, <8 x i32>* %469, align 32
  store <8 x i32> %1333, <8 x i32>* %475, align 32
  %1334 = load <8 x i32>, <8 x i32>* %483, align 32
  %1335 = load <8 x i32>, <8 x i32>* %477, align 32
  %1336 = add <8 x i32> %1335, %1334
  %1337 = sub <8 x i32> %1334, %1335
  %1338 = icmp sgt <8 x i32> %1336, %971
  %1339 = select <8 x i1> %1338, <8 x i32> %1336, <8 x i32> %971
  %1340 = icmp slt <8 x i32> %1339, %974
  %1341 = select <8 x i1> %1340, <8 x i32> %1339, <8 x i32> %974
  %1342 = icmp sgt <8 x i32> %1337, %971
  %1343 = select <8 x i1> %1342, <8 x i32> %1337, <8 x i32> %971
  %1344 = icmp slt <8 x i32> %1343, %974
  %1345 = select <8 x i1> %1344, <8 x i32> %1343, <8 x i32> %974
  store <8 x i32> %1341, <8 x i32>* %483, align 32
  store <8 x i32> %1345, <8 x i32>* %477, align 32
  %1346 = load <8 x i32>, <8 x i32>* %471, align 32
  %1347 = load <8 x i32>, <8 x i32>* %473, align 32
  %1348 = add <8 x i32> %1347, %1346
  %1349 = sub <8 x i32> %1346, %1347
  %1350 = icmp sgt <8 x i32> %1348, %971
  %1351 = select <8 x i1> %1350, <8 x i32> %1348, <8 x i32> %971
  %1352 = icmp slt <8 x i32> %1351, %974
  %1353 = select <8 x i1> %1352, <8 x i32> %1351, <8 x i32> %974
  %1354 = icmp sgt <8 x i32> %1349, %971
  %1355 = select <8 x i1> %1354, <8 x i32> %1349, <8 x i32> %971
  %1356 = icmp slt <8 x i32> %1355, %974
  %1357 = select <8 x i1> %1356, <8 x i32> %1355, <8 x i32> %974
  store <8 x i32> %1353, <8 x i32>* %471, align 32
  store <8 x i32> %1357, <8 x i32>* %473, align 32
  %1358 = load <8 x i32>, <8 x i32>* %481, align 32
  %1359 = load <8 x i32>, <8 x i32>* %479, align 32
  %1360 = add <8 x i32> %1359, %1358
  %1361 = sub <8 x i32> %1358, %1359
  %1362 = icmp sgt <8 x i32> %1360, %971
  %1363 = select <8 x i1> %1362, <8 x i32> %1360, <8 x i32> %971
  %1364 = icmp slt <8 x i32> %1363, %974
  %1365 = select <8 x i1> %1364, <8 x i32> %1363, <8 x i32> %974
  %1366 = icmp sgt <8 x i32> %1361, %971
  %1367 = select <8 x i1> %1366, <8 x i32> %1361, <8 x i32> %971
  %1368 = icmp slt <8 x i32> %1367, %974
  %1369 = select <8 x i1> %1368, <8 x i32> %1367, <8 x i32> %974
  store <8 x i32> %1365, <8 x i32>* %481, align 32
  store <8 x i32> %1369, <8 x i32>* %479, align 32
  %1370 = load <8 x i32>, <8 x i32>* %392, align 32
  %1371 = load <8 x i32>, <8 x i32>* %396, align 32
  %1372 = add <8 x i32> %1371, %1370
  %1373 = sub <8 x i32> %1370, %1371
  %1374 = icmp sgt <8 x i32> %1372, %971
  %1375 = select <8 x i1> %1374, <8 x i32> %1372, <8 x i32> %971
  %1376 = icmp slt <8 x i32> %1375, %974
  %1377 = select <8 x i1> %1376, <8 x i32> %1375, <8 x i32> %974
  %1378 = icmp sgt <8 x i32> %1373, %971
  %1379 = select <8 x i1> %1378, <8 x i32> %1373, <8 x i32> %971
  %1380 = icmp slt <8 x i32> %1379, %974
  %1381 = select <8 x i1> %1380, <8 x i32> %1379, <8 x i32> %974
  store <8 x i32> %1377, <8 x i32>* %392, align 32
  store <8 x i32> %1381, <8 x i32>* %396, align 32
  %1382 = load <8 x i32>, <8 x i32>* %386, align 32
  %1383 = load <8 x i32>, <8 x i32>* %402, align 32
  %1384 = add <8 x i32> %1383, %1382
  %1385 = sub <8 x i32> %1382, %1383
  %1386 = icmp sgt <8 x i32> %1384, %971
  %1387 = select <8 x i1> %1386, <8 x i32> %1384, <8 x i32> %971
  %1388 = icmp slt <8 x i32> %1387, %974
  %1389 = select <8 x i1> %1388, <8 x i32> %1387, <8 x i32> %974
  %1390 = icmp sgt <8 x i32> %1385, %971
  %1391 = select <8 x i1> %1390, <8 x i32> %1385, <8 x i32> %971
  %1392 = icmp slt <8 x i32> %1391, %974
  %1393 = select <8 x i1> %1392, <8 x i32> %1391, <8 x i32> %974
  store <8 x i32> %1389, <8 x i32>* %386, align 32
  store <8 x i32> %1393, <8 x i32>* %402, align 32
  %1394 = load <8 x i32>, <8 x i32>* %318, align 32
  %1395 = mul <8 x i32> %1394, %719
  %1396 = load <8 x i32>, <8 x i32>* %323, align 32
  %1397 = mul <8 x i32> %1396, %718
  %1398 = add <8 x i32> %1395, %858
  %1399 = add <8 x i32> %1398, %1397
  %1400 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1399, i32 %2) #8
  %1401 = load <8 x i32>, <8 x i32>* %421, align 32
  %1402 = mul <8 x i32> %1401, %719
  %1403 = load <8 x i32>, <8 x i32>* %443, align 32
  %1404 = mul <8 x i32> %1403, %718
  %1405 = add <8 x i32> %1402, %858
  %1406 = add <8 x i32> %1405, %1404
  %1407 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1406, i32 %2) #8
  %1408 = load <8 x i32>, <8 x i32>* %423, align 32
  %1409 = mul <8 x i32> %1408, %719
  %1410 = load <8 x i32>, <8 x i32>* %441, align 32
  %1411 = mul <8 x i32> %1410, %718
  %1412 = add <8 x i32> %1409, %858
  %1413 = add <8 x i32> %1412, %1411
  %1414 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1413, i32 %2) #8
  %1415 = load <8 x i32>, <8 x i32>* %332, align 32
  %1416 = mul <8 x i32> %1415, %719
  %1417 = load <8 x i32>, <8 x i32>* %327, align 32
  %1418 = mul <8 x i32> %1417, %718
  %1419 = add <8 x i32> %1416, %858
  %1420 = add <8 x i32> %1419, %1418
  %1421 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1420, i32 %2) #8
  %1422 = mul <8 x i32> %1415, %718
  %1423 = mul <8 x i32> %1417, %717
  %1424 = add <8 x i32> %1422, %858
  %1425 = add <8 x i32> %1424, %1423
  %1426 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1425, i32 %2) #8
  store <8 x i32> %1426, <8 x i32>* %327, align 32
  %1427 = mul <8 x i32> %1408, %718
  %1428 = mul <8 x i32> %1410, %717
  %1429 = add <8 x i32> %1427, %858
  %1430 = add <8 x i32> %1429, %1428
  %1431 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1430, i32 %2) #8
  store <8 x i32> %1431, <8 x i32>* %441, align 32
  %1432 = mul <8 x i32> %1401, %718
  %1433 = mul <8 x i32> %1403, %717
  %1434 = add <8 x i32> %1432, %858
  %1435 = add <8 x i32> %1434, %1433
  %1436 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1435, i32 %2) #8
  store <8 x i32> %1436, <8 x i32>* %443, align 32
  %1437 = mul <8 x i32> %1394, %718
  %1438 = mul <8 x i32> %1396, %717
  %1439 = add <8 x i32> %1437, %858
  %1440 = add <8 x i32> %1439, %1438
  %1441 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1440, i32 %2) #8
  store <8 x i32> %1441, <8 x i32>* %323, align 32
  store <8 x i32> %1400, <8 x i32>* %318, align 32
  store <8 x i32> %1407, <8 x i32>* %421, align 32
  store <8 x i32> %1414, <8 x i32>* %423, align 32
  store <8 x i32> %1421, <8 x i32>* %332, align 32
  %1442 = load <8 x i32>, <8 x i32>* %336, align 32
  %1443 = mul <8 x i32> %1442, %720
  %1444 = load <8 x i32>, <8 x i32>* %342, align 32
  %1445 = mul <8 x i32> %1444, %719
  %1446 = add <8 x i32> %1443, %858
  %1447 = add <8 x i32> %1446, %1445
  %1448 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1447, i32 %2) #8
  %1449 = load <8 x i32>, <8 x i32>* %425, align 32
  %1450 = mul <8 x i32> %1449, %720
  %1451 = load <8 x i32>, <8 x i32>* %439, align 32
  %1452 = mul <8 x i32> %1451, %719
  %1453 = add <8 x i32> %1450, %858
  %1454 = add <8 x i32> %1453, %1452
  %1455 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1454, i32 %2) #8
  %1456 = load <8 x i32>, <8 x i32>* %427, align 32
  %1457 = mul <8 x i32> %1456, %720
  %1458 = load <8 x i32>, <8 x i32>* %437, align 32
  %1459 = mul <8 x i32> %1458, %719
  %1460 = add <8 x i32> %1457, %858
  %1461 = add <8 x i32> %1460, %1459
  %1462 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1461, i32 %2) #8
  %1463 = load <8 x i32>, <8 x i32>* %352, align 32
  %1464 = mul <8 x i32> %1463, %720
  %1465 = load <8 x i32>, <8 x i32>* %346, align 32
  %1466 = mul <8 x i32> %1465, %719
  %1467 = add <8 x i32> %1464, %858
  %1468 = add <8 x i32> %1467, %1466
  %1469 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1468, i32 %2) #8
  %1470 = mul <8 x i32> %1463, %719
  %1471 = mul <8 x i32> %1465, %718
  %1472 = add <8 x i32> %1470, %858
  %1473 = add <8 x i32> %1472, %1471
  %1474 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1473, i32 %2) #8
  store <8 x i32> %1474, <8 x i32>* %346, align 32
  %1475 = mul <8 x i32> %1456, %719
  %1476 = mul <8 x i32> %1458, %718
  %1477 = add <8 x i32> %1475, %858
  %1478 = add <8 x i32> %1477, %1476
  %1479 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1478, i32 %2) #8
  store <8 x i32> %1479, <8 x i32>* %437, align 32
  %1480 = mul <8 x i32> %1449, %719
  %1481 = mul <8 x i32> %1451, %718
  %1482 = add <8 x i32> %1480, %858
  %1483 = add <8 x i32> %1482, %1481
  %1484 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1483, i32 %2) #8
  store <8 x i32> %1484, <8 x i32>* %439, align 32
  %1485 = mul <8 x i32> %1442, %719
  %1486 = mul <8 x i32> %1444, %718
  %1487 = add <8 x i32> %1485, %858
  %1488 = add <8 x i32> %1487, %1486
  %1489 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1488, i32 %2) #8
  store <8 x i32> %1489, <8 x i32>* %342, align 32
  store <8 x i32> %1448, <8 x i32>* %336, align 32
  store <8 x i32> %1455, <8 x i32>* %425, align 32
  store <8 x i32> %1462, <8 x i32>* %427, align 32
  store <8 x i32> %1469, <8 x i32>* %352, align 32
  call fastcc void @idct64_stage9_avx2(<4 x i64>* nonnull %249, <4 x i64>* nonnull %11, <4 x i64>* nonnull %10, <4 x i64>* nonnull %8, <4 x i64>* nonnull %9, <4 x i64>* nonnull %7, i32 %2)
  br label %1490

1490:                                             ; preds = %1490, %1226
  %1491 = phi i64 [ 0, %1226 ], [ %1509, %1490 ]
  %1492 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %1491
  %1493 = bitcast <4 x i64>* %1492 to <8 x i32>*
  %1494 = load <8 x i32>, <8 x i32>* %1493, align 32
  %1495 = sub nuw nsw i64 31, %1491
  %1496 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %12, i64 0, i64 %1495
  %1497 = bitcast <4 x i64>* %1496 to <8 x i32>*
  %1498 = load <8 x i32>, <8 x i32>* %1497, align 32
  %1499 = add <8 x i32> %1498, %1494
  %1500 = sub <8 x i32> %1494, %1498
  %1501 = icmp sgt <8 x i32> %1499, %971
  %1502 = select <8 x i1> %1501, <8 x i32> %1499, <8 x i32> %971
  %1503 = icmp slt <8 x i32> %1502, %974
  %1504 = select <8 x i1> %1503, <8 x i32> %1502, <8 x i32> %974
  %1505 = icmp sgt <8 x i32> %1500, %971
  %1506 = select <8 x i1> %1505, <8 x i32> %1500, <8 x i32> %971
  %1507 = icmp slt <8 x i32> %1506, %974
  %1508 = select <8 x i1> %1507, <8 x i32> %1506, <8 x i32> %974
  store <8 x i32> %1504, <8 x i32>* %1493, align 32
  store <8 x i32> %1508, <8 x i32>* %1497, align 32
  %1509 = add nuw nsw i64 %1491, 1
  %1510 = icmp eq i64 %1509, 16
  br i1 %1510, label %1511, label %1490

1511:                                             ; preds = %1490
  %1512 = load <8 x i32>, <8 x i32>* %336, align 32
  %1513 = mul <8 x i32> %1512, %954
  %1514 = load <8 x i32>, <8 x i32>* %342, align 32
  %1515 = mul <8 x i32> %1514, %721
  %1516 = add <8 x i32> %1513, %858
  %1517 = add <8 x i32> %1516, %1515
  %1518 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1517, i32 %2) #8
  %1519 = load <8 x i32>, <8 x i32>* %425, align 32
  %1520 = mul <8 x i32> %1519, %954
  %1521 = load <8 x i32>, <8 x i32>* %439, align 32
  %1522 = mul <8 x i32> %1521, %721
  %1523 = add <8 x i32> %1520, %858
  %1524 = add <8 x i32> %1523, %1522
  %1525 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1524, i32 %2) #8
  %1526 = load <8 x i32>, <8 x i32>* %427, align 32
  %1527 = mul <8 x i32> %1526, %954
  %1528 = load <8 x i32>, <8 x i32>* %437, align 32
  %1529 = mul <8 x i32> %1528, %721
  %1530 = add <8 x i32> %1527, %858
  %1531 = add <8 x i32> %1530, %1529
  %1532 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1531, i32 %2) #8
  %1533 = load <8 x i32>, <8 x i32>* %352, align 32
  %1534 = mul <8 x i32> %1533, %954
  %1535 = load <8 x i32>, <8 x i32>* %346, align 32
  %1536 = mul <8 x i32> %1535, %721
  %1537 = add <8 x i32> %1536, %858
  %1538 = add <8 x i32> %1537, %1534
  %1539 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1538, i32 %2) #8
  %1540 = mul <8 x i32> %1533, %721
  %1541 = add <8 x i32> %1537, %1540
  %1542 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1541, i32 %2) #8
  store <8 x i32> %1542, <8 x i32>* %346, align 32
  %1543 = load <8 x i32>, <8 x i32>* %118, align 32
  %1544 = add <8 x i32> %1528, %1526
  %1545 = mul <8 x i32> %1543, %1544
  %1546 = add <8 x i32> %1545, %858
  %1547 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1546, i32 %2) #8
  store <8 x i32> %1547, <8 x i32>* %437, align 32
  %1548 = add <8 x i32> %1521, %1519
  %1549 = mul <8 x i32> %1543, %1548
  %1550 = add <8 x i32> %1549, %858
  %1551 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1550, i32 %2) #8
  store <8 x i32> %1551, <8 x i32>* %439, align 32
  %1552 = add <8 x i32> %1514, %1512
  %1553 = mul <8 x i32> %1543, %1552
  %1554 = add <8 x i32> %1553, %858
  %1555 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1554, i32 %2) #8
  store <8 x i32> %1555, <8 x i32>* %342, align 32
  store <8 x i32> %1518, <8 x i32>* %336, align 32
  store <8 x i32> %1525, <8 x i32>* %425, align 32
  store <8 x i32> %1532, <8 x i32>* %427, align 32
  store <8 x i32> %1539, <8 x i32>* %352, align 32
  %1556 = load <8 x i32>, <8 x i32>* %366, align 32
  %1557 = mul <8 x i32> %1556, %954
  %1558 = load <8 x i32>, <8 x i32>* %372, align 32
  %1559 = mul <8 x i32> %1558, %1543
  %1560 = add <8 x i32> %1557, %858
  %1561 = add <8 x i32> %1560, %1559
  %1562 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1561, i32 %2) #8
  %1563 = load <8 x i32>, <8 x i32>* %429, align 32
  %1564 = mul <8 x i32> %1563, %954
  %1565 = load <8 x i32>, <8 x i32>* %435, align 32
  %1566 = mul <8 x i32> %1565, %1543
  %1567 = add <8 x i32> %1564, %858
  %1568 = add <8 x i32> %1567, %1566
  %1569 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1568, i32 %2) #8
  %1570 = load <8 x i32>, <8 x i32>* %431, align 32
  %1571 = mul <8 x i32> %1570, %954
  %1572 = load <8 x i32>, <8 x i32>* %433, align 32
  %1573 = mul <8 x i32> %1572, %1543
  %1574 = add <8 x i32> %1571, %858
  %1575 = add <8 x i32> %1574, %1573
  %1576 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1575, i32 %2) #8
  %1577 = load <8 x i32>, <8 x i32>* %362, align 32
  %1578 = mul <8 x i32> %1577, %954
  %1579 = load <8 x i32>, <8 x i32>* %356, align 32
  %1580 = mul <8 x i32> %1579, %1543
  %1581 = add <8 x i32> %1580, %858
  %1582 = add <8 x i32> %1581, %1578
  %1583 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1582, i32 %2) #8
  %1584 = mul <8 x i32> %1577, %1543
  %1585 = add <8 x i32> %1581, %1584
  %1586 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1585, i32 %2) #8
  store <8 x i32> %1586, <8 x i32>* %356, align 32
  %1587 = add <8 x i32> %1572, %1570
  %1588 = mul <8 x i32> %1587, %1543
  %1589 = add <8 x i32> %1588, %858
  %1590 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1589, i32 %2) #8
  store <8 x i32> %1590, <8 x i32>* %433, align 32
  %1591 = add <8 x i32> %1565, %1563
  %1592 = mul <8 x i32> %1591, %1543
  %1593 = add <8 x i32> %1592, %858
  %1594 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1593, i32 %2) #8
  store <8 x i32> %1594, <8 x i32>* %435, align 32
  %1595 = add <8 x i32> %1558, %1556
  %1596 = mul <8 x i32> %1595, %1543
  %1597 = add <8 x i32> %1596, %858
  %1598 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1597, i32 %2) #8
  store <8 x i32> %1598, <8 x i32>* %372, align 32
  store <8 x i32> %1562, <8 x i32>* %366, align 32
  store <8 x i32> %1569, <8 x i32>* %429, align 32
  store <8 x i32> %1576, <8 x i32>* %431, align 32
  store <8 x i32> %1583, <8 x i32>* %362, align 32
  call fastcc void @idct64_stage11_avx2(<4 x i64>* nonnull %249, <4 x i64>* %1, i32 %3, i32 %4, i32 %5, <4 x i64>* nonnull %8, <4 x i64>* nonnull %9)
  call void @llvm.lifetime.end.p0i8(i64 2048, i8* nonnull %245) #8
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %190) #8
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %113) #8
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %33) #8
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %26) #8
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %15) #8
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal void @idct64_avx2(<4 x i64>* readonly, <4 x i64>*, i32, i32, i32, i32) #0 {
  %7 = alloca [64 x <4 x i64>], align 32
  %8 = alloca [64 x <4 x i64>], align 32
  %9 = add nsw i32 %2, -10
  %10 = sext i32 %9 to i64
  %11 = add nsw i32 %2, -1
  %12 = shl i32 1, %11
  %13 = insertelement <8 x i32> undef, i32 %12, i32 0
  %14 = shufflevector <8 x i32> %13, <8 x i32> undef, <8 x i32> zeroinitializer
  %15 = icmp ne i32 %3, 0
  %16 = select i1 %15, i32 6, i32 8
  %17 = add nsw i32 %16, %4
  %18 = icmp sgt i32 %17, 16
  %19 = select i1 %18, i32 %17, i32 16
  %20 = add nsw i32 %19, -1
  %21 = shl i32 1, %20
  %22 = sub nsw i32 0, %21
  %23 = insertelement <8 x i32> undef, i32 %22, i32 0
  %24 = shufflevector <8 x i32> %23, <8 x i32> undef, <8 x i32> zeroinitializer
  %25 = add nsw i32 %21, -1
  %26 = insertelement <8 x i32> undef, i32 %25, i32 0
  %27 = shufflevector <8 x i32> %26, <8 x i32> undef, <8 x i32> zeroinitializer
  %28 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 1
  %29 = load i32, i32* %28, align 4
  %30 = insertelement <8 x i32> undef, i32 %29, i32 0
  %31 = shufflevector <8 x i32> %30, <8 x i32> undef, <8 x i32> zeroinitializer
  %32 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 2
  %33 = load i32, i32* %32, align 8
  %34 = insertelement <8 x i32> undef, i32 %33, i32 0
  %35 = shufflevector <8 x i32> %34, <8 x i32> undef, <8 x i32> zeroinitializer
  %36 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 3
  %37 = load i32, i32* %36, align 4
  %38 = insertelement <8 x i32> undef, i32 %37, i32 0
  %39 = shufflevector <8 x i32> %38, <8 x i32> undef, <8 x i32> zeroinitializer
  %40 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 4
  %41 = load i32, i32* %40, align 16
  %42 = insertelement <8 x i32> undef, i32 %41, i32 0
  %43 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 5
  %44 = load i32, i32* %43, align 4
  %45 = insertelement <8 x i32> undef, i32 %44, i32 0
  %46 = shufflevector <8 x i32> %45, <8 x i32> undef, <8 x i32> zeroinitializer
  %47 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 6
  %48 = load i32, i32* %47, align 8
  %49 = insertelement <8 x i32> undef, i32 %48, i32 0
  %50 = shufflevector <8 x i32> %49, <8 x i32> undef, <8 x i32> zeroinitializer
  %51 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 7
  %52 = load i32, i32* %51, align 4
  %53 = insertelement <8 x i32> undef, i32 %52, i32 0
  %54 = shufflevector <8 x i32> %53, <8 x i32> undef, <8 x i32> zeroinitializer
  %55 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 8
  %56 = load i32, i32* %55, align 16
  %57 = insertelement <8 x i32> undef, i32 %56, i32 0
  %58 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 9
  %59 = load i32, i32* %58, align 4
  %60 = insertelement <8 x i32> undef, i32 %59, i32 0
  %61 = shufflevector <8 x i32> %60, <8 x i32> undef, <8 x i32> zeroinitializer
  %62 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 10
  %63 = load i32, i32* %62, align 8
  %64 = insertelement <8 x i32> undef, i32 %63, i32 0
  %65 = shufflevector <8 x i32> %64, <8 x i32> undef, <8 x i32> zeroinitializer
  %66 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 11
  %67 = load i32, i32* %66, align 4
  %68 = insertelement <8 x i32> undef, i32 %67, i32 0
  %69 = shufflevector <8 x i32> %68, <8 x i32> undef, <8 x i32> zeroinitializer
  %70 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 12
  %71 = load i32, i32* %70, align 16
  %72 = insertelement <8 x i32> undef, i32 %71, i32 0
  %73 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 13
  %74 = load i32, i32* %73, align 4
  %75 = insertelement <8 x i32> undef, i32 %74, i32 0
  %76 = shufflevector <8 x i32> %75, <8 x i32> undef, <8 x i32> zeroinitializer
  %77 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 14
  %78 = load i32, i32* %77, align 8
  %79 = insertelement <8 x i32> undef, i32 %78, i32 0
  %80 = shufflevector <8 x i32> %79, <8 x i32> undef, <8 x i32> zeroinitializer
  %81 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 15
  %82 = load i32, i32* %81, align 4
  %83 = insertelement <8 x i32> undef, i32 %82, i32 0
  %84 = shufflevector <8 x i32> %83, <8 x i32> undef, <8 x i32> zeroinitializer
  %85 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 16
  %86 = load i32, i32* %85, align 16
  %87 = insertelement <8 x i32> undef, i32 %86, i32 0
  %88 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 17
  %89 = load i32, i32* %88, align 4
  %90 = insertelement <8 x i32> undef, i32 %89, i32 0
  %91 = shufflevector <8 x i32> %90, <8 x i32> undef, <8 x i32> zeroinitializer
  %92 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 18
  %93 = load i32, i32* %92, align 8
  %94 = insertelement <8 x i32> undef, i32 %93, i32 0
  %95 = shufflevector <8 x i32> %94, <8 x i32> undef, <8 x i32> zeroinitializer
  %96 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 19
  %97 = load i32, i32* %96, align 4
  %98 = insertelement <8 x i32> undef, i32 %97, i32 0
  %99 = shufflevector <8 x i32> %98, <8 x i32> undef, <8 x i32> zeroinitializer
  %100 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 20
  %101 = load i32, i32* %100, align 16
  %102 = insertelement <8 x i32> undef, i32 %101, i32 0
  %103 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 21
  %104 = load i32, i32* %103, align 4
  %105 = insertelement <8 x i32> undef, i32 %104, i32 0
  %106 = shufflevector <8 x i32> %105, <8 x i32> undef, <8 x i32> zeroinitializer
  %107 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 22
  %108 = load i32, i32* %107, align 8
  %109 = insertelement <8 x i32> undef, i32 %108, i32 0
  %110 = shufflevector <8 x i32> %109, <8 x i32> undef, <8 x i32> zeroinitializer
  %111 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 23
  %112 = load i32, i32* %111, align 4
  %113 = insertelement <8 x i32> undef, i32 %112, i32 0
  %114 = shufflevector <8 x i32> %113, <8 x i32> undef, <8 x i32> zeroinitializer
  %115 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 24
  %116 = load i32, i32* %115, align 16
  %117 = insertelement <8 x i32> undef, i32 %116, i32 0
  %118 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 25
  %119 = load i32, i32* %118, align 4
  %120 = insertelement <8 x i32> undef, i32 %119, i32 0
  %121 = shufflevector <8 x i32> %120, <8 x i32> undef, <8 x i32> zeroinitializer
  %122 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 26
  %123 = load i32, i32* %122, align 8
  %124 = insertelement <8 x i32> undef, i32 %123, i32 0
  %125 = shufflevector <8 x i32> %124, <8 x i32> undef, <8 x i32> zeroinitializer
  %126 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 27
  %127 = load i32, i32* %126, align 4
  %128 = insertelement <8 x i32> undef, i32 %127, i32 0
  %129 = shufflevector <8 x i32> %128, <8 x i32> undef, <8 x i32> zeroinitializer
  %130 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 28
  %131 = load i32, i32* %130, align 16
  %132 = insertelement <8 x i32> undef, i32 %131, i32 0
  %133 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 29
  %134 = load i32, i32* %133, align 4
  %135 = insertelement <8 x i32> undef, i32 %134, i32 0
  %136 = shufflevector <8 x i32> %135, <8 x i32> undef, <8 x i32> zeroinitializer
  %137 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 30
  %138 = load i32, i32* %137, align 8
  %139 = insertelement <8 x i32> undef, i32 %138, i32 0
  %140 = shufflevector <8 x i32> %139, <8 x i32> undef, <8 x i32> zeroinitializer
  %141 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 31
  %142 = load i32, i32* %141, align 4
  %143 = insertelement <8 x i32> undef, i32 %142, i32 0
  %144 = shufflevector <8 x i32> %143, <8 x i32> undef, <8 x i32> zeroinitializer
  %145 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 32
  %146 = load i32, i32* %145, align 16
  %147 = insertelement <8 x i32> undef, i32 %146, i32 0
  %148 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 35
  %149 = load i32, i32* %148, align 4
  %150 = insertelement <8 x i32> undef, i32 %149, i32 0
  %151 = shufflevector <8 x i32> %150, <8 x i32> undef, <8 x i32> zeroinitializer
  %152 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 36
  %153 = load i32, i32* %152, align 16
  %154 = insertelement <8 x i32> undef, i32 %153, i32 0
  %155 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 38
  %156 = load i32, i32* %155, align 8
  %157 = insertelement <8 x i32> undef, i32 %156, i32 0
  %158 = shufflevector <8 x i32> %157, <8 x i32> undef, <8 x i32> zeroinitializer
  %159 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 39
  %160 = load i32, i32* %159, align 4
  %161 = insertelement <8 x i32> undef, i32 %160, i32 0
  %162 = shufflevector <8 x i32> %161, <8 x i32> undef, <8 x i32> zeroinitializer
  %163 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 40
  %164 = load i32, i32* %163, align 16
  %165 = insertelement <8 x i32> undef, i32 %164, i32 0
  %166 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 43
  %167 = load i32, i32* %166, align 4
  %168 = insertelement <8 x i32> undef, i32 %167, i32 0
  %169 = shufflevector <8 x i32> %168, <8 x i32> undef, <8 x i32> zeroinitializer
  %170 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 44
  %171 = load i32, i32* %170, align 16
  %172 = insertelement <8 x i32> undef, i32 %171, i32 0
  %173 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 46
  %174 = load i32, i32* %173, align 8
  %175 = insertelement <8 x i32> undef, i32 %174, i32 0
  %176 = shufflevector <8 x i32> %175, <8 x i32> undef, <8 x i32> zeroinitializer
  %177 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 47
  %178 = load i32, i32* %177, align 4
  %179 = insertelement <8 x i32> undef, i32 %178, i32 0
  %180 = shufflevector <8 x i32> %179, <8 x i32> undef, <8 x i32> zeroinitializer
  %181 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 48
  %182 = load i32, i32* %181, align 16
  %183 = insertelement <8 x i32> undef, i32 %182, i32 0
  %184 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 51
  %185 = load i32, i32* %184, align 4
  %186 = insertelement <8 x i32> undef, i32 %185, i32 0
  %187 = shufflevector <8 x i32> %186, <8 x i32> undef, <8 x i32> zeroinitializer
  %188 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 52
  %189 = load i32, i32* %188, align 16
  %190 = insertelement <8 x i32> undef, i32 %189, i32 0
  %191 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 54
  %192 = load i32, i32* %191, align 8
  %193 = insertelement <8 x i32> undef, i32 %192, i32 0
  %194 = shufflevector <8 x i32> %193, <8 x i32> undef, <8 x i32> zeroinitializer
  %195 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 55
  %196 = load i32, i32* %195, align 4
  %197 = insertelement <8 x i32> undef, i32 %196, i32 0
  %198 = shufflevector <8 x i32> %197, <8 x i32> undef, <8 x i32> zeroinitializer
  %199 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 56
  %200 = load i32, i32* %199, align 16
  %201 = insertelement <8 x i32> undef, i32 %200, i32 0
  %202 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 59
  %203 = load i32, i32* %202, align 4
  %204 = insertelement <8 x i32> undef, i32 %203, i32 0
  %205 = shufflevector <8 x i32> %204, <8 x i32> undef, <8 x i32> zeroinitializer
  %206 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 60
  %207 = load i32, i32* %206, align 16
  %208 = insertelement <8 x i32> undef, i32 %207, i32 0
  %209 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 62
  %210 = load i32, i32* %209, align 8
  %211 = insertelement <8 x i32> undef, i32 %210, i32 0
  %212 = shufflevector <8 x i32> %211, <8 x i32> undef, <8 x i32> zeroinitializer
  %213 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 63
  %214 = load i32, i32* %213, align 4
  %215 = insertelement <8 x i32> undef, i32 %214, i32 0
  %216 = shufflevector <8 x i32> %215, <8 x i32> undef, <8 x i32> zeroinitializer
  %217 = sub nsw i32 0, %41
  %218 = insertelement <8 x i32> undef, i32 %217, i32 0
  %219 = sub nsw i32 0, %56
  %220 = insertelement <8 x i32> undef, i32 %219, i32 0
  %221 = sub nsw i32 0, %71
  %222 = insertelement <8 x i32> undef, i32 %221, i32 0
  %223 = sub nsw i32 0, %86
  %224 = insertelement <8 x i32> undef, i32 %223, i32 0
  %225 = sub nsw i32 0, %101
  %226 = insertelement <8 x i32> undef, i32 %225, i32 0
  %227 = sub nsw i32 0, %116
  %228 = insertelement <8 x i32> undef, i32 %227, i32 0
  %229 = sub nsw i32 0, %131
  %230 = insertelement <8 x i32> undef, i32 %229, i32 0
  %231 = sub nsw i32 0, %146
  %232 = insertelement <8 x i32> undef, i32 %231, i32 0
  %233 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 33
  %234 = load i32, i32* %233, align 4
  %235 = sub nsw i32 0, %234
  %236 = insertelement <8 x i32> undef, i32 %235, i32 0
  %237 = shufflevector <8 x i32> %236, <8 x i32> undef, <8 x i32> zeroinitializer
  %238 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 34
  %239 = load i32, i32* %238, align 8
  %240 = sub nsw i32 0, %239
  %241 = insertelement <8 x i32> undef, i32 %240, i32 0
  %242 = shufflevector <8 x i32> %241, <8 x i32> undef, <8 x i32> zeroinitializer
  %243 = sub nsw i32 0, %153
  %244 = insertelement <8 x i32> undef, i32 %243, i32 0
  %245 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 37
  %246 = load i32, i32* %245, align 4
  %247 = sub nsw i32 0, %246
  %248 = insertelement <8 x i32> undef, i32 %247, i32 0
  %249 = shufflevector <8 x i32> %248, <8 x i32> undef, <8 x i32> zeroinitializer
  %250 = sub nsw i32 0, %164
  %251 = insertelement <8 x i32> undef, i32 %250, i32 0
  %252 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 41
  %253 = load i32, i32* %252, align 4
  %254 = sub nsw i32 0, %253
  %255 = insertelement <8 x i32> undef, i32 %254, i32 0
  %256 = shufflevector <8 x i32> %255, <8 x i32> undef, <8 x i32> zeroinitializer
  %257 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 42
  %258 = load i32, i32* %257, align 8
  %259 = sub nsw i32 0, %258
  %260 = insertelement <8 x i32> undef, i32 %259, i32 0
  %261 = shufflevector <8 x i32> %260, <8 x i32> undef, <8 x i32> zeroinitializer
  %262 = sub nsw i32 0, %171
  %263 = insertelement <8 x i32> undef, i32 %262, i32 0
  %264 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 45
  %265 = load i32, i32* %264, align 4
  %266 = sub nsw i32 0, %265
  %267 = insertelement <8 x i32> undef, i32 %266, i32 0
  %268 = shufflevector <8 x i32> %267, <8 x i32> undef, <8 x i32> zeroinitializer
  %269 = sub nsw i32 0, %182
  %270 = insertelement <8 x i32> undef, i32 %269, i32 0
  %271 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 49
  %272 = load i32, i32* %271, align 4
  %273 = sub nsw i32 0, %272
  %274 = insertelement <8 x i32> undef, i32 %273, i32 0
  %275 = shufflevector <8 x i32> %274, <8 x i32> undef, <8 x i32> zeroinitializer
  %276 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 50
  %277 = load i32, i32* %276, align 8
  %278 = sub nsw i32 0, %277
  %279 = insertelement <8 x i32> undef, i32 %278, i32 0
  %280 = shufflevector <8 x i32> %279, <8 x i32> undef, <8 x i32> zeroinitializer
  %281 = sub nsw i32 0, %189
  %282 = insertelement <8 x i32> undef, i32 %281, i32 0
  %283 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 53
  %284 = load i32, i32* %283, align 4
  %285 = sub nsw i32 0, %284
  %286 = insertelement <8 x i32> undef, i32 %285, i32 0
  %287 = shufflevector <8 x i32> %286, <8 x i32> undef, <8 x i32> zeroinitializer
  %288 = sub nsw i32 0, %200
  %289 = insertelement <8 x i32> undef, i32 %288, i32 0
  %290 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 57
  %291 = load i32, i32* %290, align 4
  %292 = sub nsw i32 0, %291
  %293 = insertelement <8 x i32> undef, i32 %292, i32 0
  %294 = shufflevector <8 x i32> %293, <8 x i32> undef, <8 x i32> zeroinitializer
  %295 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 58
  %296 = load i32, i32* %295, align 8
  %297 = sub nsw i32 0, %296
  %298 = insertelement <8 x i32> undef, i32 %297, i32 0
  %299 = shufflevector <8 x i32> %298, <8 x i32> undef, <8 x i32> zeroinitializer
  %300 = sub nsw i32 0, %207
  %301 = insertelement <8 x i32> undef, i32 %300, i32 0
  %302 = getelementptr inbounds [7 x [64 x i32]], [7 x [64 x i32]]* @av1_cospi_arr_data, i64 0, i64 %10, i64 61
  %303 = load i32, i32* %302, align 4
  %304 = sub nsw i32 0, %303
  %305 = insertelement <8 x i32> undef, i32 %304, i32 0
  %306 = shufflevector <8 x i32> %305, <8 x i32> undef, <8 x i32> zeroinitializer
  %307 = bitcast [64 x <4 x i64>]* %7 to i8*
  call void @llvm.lifetime.start.p0i8(i64 2048, i8* nonnull %307) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %307, i8 -86, i64 2048, i1 false)
  %308 = bitcast [64 x <4 x i64>]* %8 to i8*
  call void @llvm.lifetime.start.p0i8(i64 2048, i8* nonnull %308) #8
  call void @llvm.memset.p0i8.i64(i8* nonnull align 32 %308, i8 -86, i64 2048, i1 false)
  %309 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 1
  %310 = load <4 x i64>, <4 x i64>* %309, align 32
  %311 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 32
  store <4 x i64> %310, <4 x i64>* %311, align 32
  %312 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 17
  %313 = load <4 x i64>, <4 x i64>* %312, align 32
  %314 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 34
  store <4 x i64> %313, <4 x i64>* %314, align 32
  %315 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 9
  %316 = load <4 x i64>, <4 x i64>* %315, align 32
  %317 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 36
  store <4 x i64> %316, <4 x i64>* %317, align 32
  %318 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 25
  %319 = load <4 x i64>, <4 x i64>* %318, align 32
  %320 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 38
  store <4 x i64> %319, <4 x i64>* %320, align 32
  %321 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 5
  %322 = load <4 x i64>, <4 x i64>* %321, align 32
  %323 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 40
  store <4 x i64> %322, <4 x i64>* %323, align 32
  %324 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 21
  %325 = load <4 x i64>, <4 x i64>* %324, align 32
  %326 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 42
  store <4 x i64> %325, <4 x i64>* %326, align 32
  %327 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 13
  %328 = load <4 x i64>, <4 x i64>* %327, align 32
  %329 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 44
  store <4 x i64> %328, <4 x i64>* %329, align 32
  %330 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 29
  %331 = load <4 x i64>, <4 x i64>* %330, align 32
  %332 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 46
  store <4 x i64> %331, <4 x i64>* %332, align 32
  %333 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 3
  %334 = load <4 x i64>, <4 x i64>* %333, align 32
  %335 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 48
  store <4 x i64> %334, <4 x i64>* %335, align 32
  %336 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 19
  %337 = load <4 x i64>, <4 x i64>* %336, align 32
  %338 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 50
  store <4 x i64> %337, <4 x i64>* %338, align 32
  %339 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 11
  %340 = load <4 x i64>, <4 x i64>* %339, align 32
  %341 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 52
  store <4 x i64> %340, <4 x i64>* %341, align 32
  %342 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 27
  %343 = load <4 x i64>, <4 x i64>* %342, align 32
  %344 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 54
  store <4 x i64> %343, <4 x i64>* %344, align 32
  %345 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 7
  %346 = load <4 x i64>, <4 x i64>* %345, align 32
  %347 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 56
  store <4 x i64> %346, <4 x i64>* %347, align 32
  %348 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 23
  %349 = load <4 x i64>, <4 x i64>* %348, align 32
  %350 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 58
  store <4 x i64> %349, <4 x i64>* %350, align 32
  %351 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 15
  %352 = load <4 x i64>, <4 x i64>* %351, align 32
  %353 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 60
  store <4 x i64> %352, <4 x i64>* %353, align 32
  %354 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 31
  %355 = load <4 x i64>, <4 x i64>* %354, align 32
  %356 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 62
  store <4 x i64> %355, <4 x i64>* %356, align 32
  %357 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 2
  %358 = load <4 x i64>, <4 x i64>* %357, align 32
  %359 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 16
  store <4 x i64> %358, <4 x i64>* %359, align 32
  %360 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 18
  %361 = load <4 x i64>, <4 x i64>* %360, align 32
  %362 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 18
  store <4 x i64> %361, <4 x i64>* %362, align 32
  %363 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 10
  %364 = load <4 x i64>, <4 x i64>* %363, align 32
  %365 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 20
  store <4 x i64> %364, <4 x i64>* %365, align 32
  %366 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 26
  %367 = load <4 x i64>, <4 x i64>* %366, align 32
  %368 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 22
  store <4 x i64> %367, <4 x i64>* %368, align 32
  %369 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 6
  %370 = load <4 x i64>, <4 x i64>* %369, align 32
  %371 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 24
  store <4 x i64> %370, <4 x i64>* %371, align 32
  %372 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 22
  %373 = load <4 x i64>, <4 x i64>* %372, align 32
  %374 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 26
  store <4 x i64> %373, <4 x i64>* %374, align 32
  %375 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 14
  %376 = load <4 x i64>, <4 x i64>* %375, align 32
  %377 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 28
  store <4 x i64> %376, <4 x i64>* %377, align 32
  %378 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 30
  %379 = load <4 x i64>, <4 x i64>* %378, align 32
  %380 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 30
  store <4 x i64> %379, <4 x i64>* %380, align 32
  %381 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 4
  %382 = load <4 x i64>, <4 x i64>* %381, align 32
  %383 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 8
  store <4 x i64> %382, <4 x i64>* %383, align 32
  %384 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 20
  %385 = load <4 x i64>, <4 x i64>* %384, align 32
  %386 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 10
  store <4 x i64> %385, <4 x i64>* %386, align 32
  %387 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 12
  %388 = load <4 x i64>, <4 x i64>* %387, align 32
  %389 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 12
  store <4 x i64> %388, <4 x i64>* %389, align 32
  %390 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 28
  %391 = load <4 x i64>, <4 x i64>* %390, align 32
  %392 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 14
  store <4 x i64> %391, <4 x i64>* %392, align 32
  %393 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 8
  %394 = load <4 x i64>, <4 x i64>* %393, align 32
  %395 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 4
  store <4 x i64> %394, <4 x i64>* %395, align 32
  %396 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 24
  %397 = load <4 x i64>, <4 x i64>* %396, align 32
  %398 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 6
  store <4 x i64> %397, <4 x i64>* %398, align 32
  %399 = load <4 x i64>, <4 x i64>* %0, align 32
  %400 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 0
  store <4 x i64> %399, <4 x i64>* %400, align 32
  %401 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 16
  %402 = load <4 x i64>, <4 x i64>* %401, align 32
  %403 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 2
  store <4 x i64> %402, <4 x i64>* %403, align 32
  %404 = bitcast <4 x i64>* %311 to <8 x i32>*
  %405 = load <8 x i32>, <8 x i32>* %404, align 32
  %406 = mul <8 x i32> %405, %216
  %407 = add <8 x i32> %406, %14
  %408 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %407, i32 %2) #8
  %409 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 32
  %410 = bitcast <4 x i64>* %409 to <8 x i32>*
  store <8 x i32> %408, <8 x i32>* %410, align 32
  %411 = bitcast <4 x i64>* %356 to <8 x i32>*
  %412 = bitcast <4 x i64> %355 to <8 x i32>
  %413 = mul <8 x i32> %237, %412
  %414 = add <8 x i32> %413, %14
  %415 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %414, i32 %2) #8
  %416 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 33
  %417 = bitcast <4 x i64>* %416 to <8 x i32>*
  store <8 x i32> %415, <8 x i32>* %417, align 32
  %418 = bitcast <4 x i64>* %314 to <8 x i32>*
  %419 = load <8 x i32>, <8 x i32>* %418, align 32
  %420 = mul <8 x i32> %419, %180
  %421 = add <8 x i32> %420, %14
  %422 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %421, i32 %2) #8
  %423 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 34
  %424 = bitcast <4 x i64>* %423 to <8 x i32>*
  store <8 x i32> %422, <8 x i32>* %424, align 32
  %425 = bitcast <4 x i64>* %353 to <8 x i32>*
  %426 = bitcast <4 x i64> %352 to <8 x i32>
  %427 = mul <8 x i32> %275, %426
  %428 = add <8 x i32> %427, %14
  %429 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %428, i32 %2) #8
  %430 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 35
  %431 = bitcast <4 x i64>* %430 to <8 x i32>*
  store <8 x i32> %429, <8 x i32>* %431, align 32
  %432 = bitcast <4 x i64>* %317 to <8 x i32>*
  %433 = load <8 x i32>, <8 x i32>* %432, align 32
  %434 = mul <8 x i32> %433, %198
  %435 = add <8 x i32> %434, %14
  %436 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %435, i32 %2) #8
  %437 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 36
  %438 = bitcast <4 x i64>* %437 to <8 x i32>*
  store <8 x i32> %436, <8 x i32>* %438, align 32
  %439 = bitcast <4 x i64>* %350 to <8 x i32>*
  %440 = load <8 x i32>, <8 x i32>* %439, align 32
  %441 = mul <8 x i32> %440, %256
  %442 = add <8 x i32> %441, %14
  %443 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %442, i32 %2) #8
  %444 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 37
  %445 = bitcast <4 x i64>* %444 to <8 x i32>*
  store <8 x i32> %443, <8 x i32>* %445, align 32
  %446 = bitcast <4 x i64>* %320 to <8 x i32>*
  %447 = load <8 x i32>, <8 x i32>* %446, align 32
  %448 = mul <8 x i32> %447, %162
  %449 = add <8 x i32> %448, %14
  %450 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %449, i32 %2) #8
  %451 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 38
  %452 = bitcast <4 x i64>* %451 to <8 x i32>*
  store <8 x i32> %450, <8 x i32>* %452, align 32
  %453 = bitcast <4 x i64>* %347 to <8 x i32>*
  %454 = load <8 x i32>, <8 x i32>* %453, align 32
  %455 = mul <8 x i32> %454, %294
  %456 = add <8 x i32> %455, %14
  %457 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %456, i32 %2) #8
  %458 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 39
  %459 = bitcast <4 x i64>* %458 to <8 x i32>*
  store <8 x i32> %457, <8 x i32>* %459, align 32
  %460 = bitcast <4 x i64>* %323 to <8 x i32>*
  %461 = load <8 x i32>, <8 x i32>* %460, align 32
  %462 = mul <8 x i32> %461, %205
  %463 = add <8 x i32> %462, %14
  %464 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %463, i32 %2) #8
  %465 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 40
  %466 = bitcast <4 x i64>* %465 to <8 x i32>*
  store <8 x i32> %464, <8 x i32>* %466, align 32
  %467 = bitcast <4 x i64>* %344 to <8 x i32>*
  %468 = load <8 x i32>, <8 x i32>* %467, align 32
  %469 = mul <8 x i32> %468, %249
  %470 = add <8 x i32> %469, %14
  %471 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %470, i32 %2) #8
  %472 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 41
  %473 = bitcast <4 x i64>* %472 to <8 x i32>*
  store <8 x i32> %471, <8 x i32>* %473, align 32
  %474 = bitcast <4 x i64>* %326 to <8 x i32>*
  %475 = load <8 x i32>, <8 x i32>* %474, align 32
  %476 = mul <8 x i32> %475, %169
  %477 = add <8 x i32> %476, %14
  %478 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %477, i32 %2) #8
  %479 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 42
  %480 = bitcast <4 x i64>* %479 to <8 x i32>*
  store <8 x i32> %478, <8 x i32>* %480, align 32
  %481 = bitcast <4 x i64>* %341 to <8 x i32>*
  %482 = load <8 x i32>, <8 x i32>* %481, align 32
  %483 = mul <8 x i32> %482, %287
  %484 = add <8 x i32> %483, %14
  %485 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %484, i32 %2) #8
  %486 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 43
  %487 = bitcast <4 x i64>* %486 to <8 x i32>*
  store <8 x i32> %485, <8 x i32>* %487, align 32
  %488 = bitcast <4 x i64>* %329 to <8 x i32>*
  %489 = load <8 x i32>, <8 x i32>* %488, align 32
  %490 = mul <8 x i32> %489, %187
  %491 = add <8 x i32> %490, %14
  %492 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %491, i32 %2) #8
  %493 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 44
  %494 = bitcast <4 x i64>* %493 to <8 x i32>*
  store <8 x i32> %492, <8 x i32>* %494, align 32
  %495 = bitcast <4 x i64>* %338 to <8 x i32>*
  %496 = load <8 x i32>, <8 x i32>* %495, align 32
  %497 = mul <8 x i32> %496, %268
  %498 = add <8 x i32> %497, %14
  %499 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %498, i32 %2) #8
  %500 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 45
  %501 = bitcast <4 x i64>* %500 to <8 x i32>*
  store <8 x i32> %499, <8 x i32>* %501, align 32
  %502 = bitcast <4 x i64>* %332 to <8 x i32>*
  %503 = load <8 x i32>, <8 x i32>* %502, align 32
  %504 = mul <8 x i32> %503, %151
  %505 = add <8 x i32> %504, %14
  %506 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %505, i32 %2) #8
  %507 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 46
  %508 = bitcast <4 x i64>* %507 to <8 x i32>*
  store <8 x i32> %506, <8 x i32>* %508, align 32
  %509 = bitcast <4 x i64>* %335 to <8 x i32>*
  %510 = load <8 x i32>, <8 x i32>* %509, align 32
  %511 = mul <8 x i32> %510, %306
  %512 = add <8 x i32> %511, %14
  %513 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %512, i32 %2) #8
  %514 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 47
  %515 = bitcast <4 x i64>* %514 to <8 x i32>*
  store <8 x i32> %513, <8 x i32>* %515, align 32
  %516 = mul <8 x i32> %510, %39
  %517 = add <8 x i32> %516, %14
  %518 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %517, i32 %2) #8
  %519 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 48
  %520 = bitcast <4 x i64>* %519 to <8 x i32>*
  store <8 x i32> %518, <8 x i32>* %520, align 32
  %521 = mul <8 x i32> %503, %136
  %522 = add <8 x i32> %521, %14
  %523 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %522, i32 %2) #8
  %524 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 49
  %525 = bitcast <4 x i64>* %524 to <8 x i32>*
  store <8 x i32> %523, <8 x i32>* %525, align 32
  %526 = mul <8 x i32> %496, %99
  %527 = add <8 x i32> %526, %14
  %528 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %527, i32 %2) #8
  %529 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 50
  %530 = bitcast <4 x i64>* %529 to <8 x i32>*
  store <8 x i32> %528, <8 x i32>* %530, align 32
  %531 = mul <8 x i32> %489, %76
  %532 = add <8 x i32> %531, %14
  %533 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %532, i32 %2) #8
  %534 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 51
  %535 = bitcast <4 x i64>* %534 to <8 x i32>*
  store <8 x i32> %533, <8 x i32>* %535, align 32
  %536 = mul <8 x i32> %482, %69
  %537 = add <8 x i32> %536, %14
  %538 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %537, i32 %2) #8
  %539 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 52
  %540 = bitcast <4 x i64>* %539 to <8 x i32>*
  store <8 x i32> %538, <8 x i32>* %540, align 32
  %541 = mul <8 x i32> %475, %106
  %542 = add <8 x i32> %541, %14
  %543 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %542, i32 %2) #8
  %544 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 53
  %545 = bitcast <4 x i64>* %544 to <8 x i32>*
  store <8 x i32> %543, <8 x i32>* %545, align 32
  %546 = mul <8 x i32> %468, %129
  %547 = add <8 x i32> %546, %14
  %548 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %547, i32 %2) #8
  %549 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 54
  %550 = bitcast <4 x i64>* %549 to <8 x i32>*
  store <8 x i32> %548, <8 x i32>* %550, align 32
  %551 = mul <8 x i32> %461, %46
  %552 = add <8 x i32> %551, %14
  %553 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %552, i32 %2) #8
  %554 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 55
  %555 = bitcast <4 x i64>* %554 to <8 x i32>*
  store <8 x i32> %553, <8 x i32>* %555, align 32
  %556 = mul <8 x i32> %454, %54
  %557 = add <8 x i32> %556, %14
  %558 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %557, i32 %2) #8
  %559 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 56
  %560 = bitcast <4 x i64>* %559 to <8 x i32>*
  store <8 x i32> %558, <8 x i32>* %560, align 32
  %561 = mul <8 x i32> %447, %121
  %562 = add <8 x i32> %561, %14
  %563 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %562, i32 %2) #8
  %564 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 57
  %565 = bitcast <4 x i64>* %564 to <8 x i32>*
  store <8 x i32> %563, <8 x i32>* %565, align 32
  %566 = mul <8 x i32> %440, %114
  %567 = add <8 x i32> %566, %14
  %568 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %567, i32 %2) #8
  %569 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 58
  %570 = bitcast <4 x i64>* %569 to <8 x i32>*
  store <8 x i32> %568, <8 x i32>* %570, align 32
  %571 = mul <8 x i32> %433, %61
  %572 = add <8 x i32> %571, %14
  %573 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %572, i32 %2) #8
  %574 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 59
  %575 = bitcast <4 x i64>* %574 to <8 x i32>*
  store <8 x i32> %573, <8 x i32>* %575, align 32
  %576 = mul <8 x i32> %84, %426
  %577 = add <8 x i32> %576, %14
  %578 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %577, i32 %2) #8
  %579 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 60
  %580 = bitcast <4 x i64>* %579 to <8 x i32>*
  store <8 x i32> %578, <8 x i32>* %580, align 32
  %581 = mul <8 x i32> %419, %91
  %582 = add <8 x i32> %581, %14
  %583 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %582, i32 %2) #8
  %584 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 61
  %585 = bitcast <4 x i64>* %584 to <8 x i32>*
  store <8 x i32> %583, <8 x i32>* %585, align 32
  %586 = mul <8 x i32> %144, %412
  %587 = add <8 x i32> %586, %14
  %588 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %587, i32 %2) #8
  %589 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 62
  %590 = bitcast <4 x i64>* %589 to <8 x i32>*
  store <8 x i32> %588, <8 x i32>* %590, align 32
  %591 = mul <8 x i32> %405, %31
  %592 = add <8 x i32> %591, %14
  %593 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %592, i32 %2) #8
  %594 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 63
  %595 = bitcast <4 x i64>* %594 to <8 x i32>*
  store <8 x i32> %593, <8 x i32>* %595, align 32
  %596 = bitcast <4 x i64>* %359 to <8 x i32>*
  %597 = load <8 x i32>, <8 x i32>* %596, align 32
  %598 = mul <8 x i32> %597, %212
  %599 = add <8 x i32> %598, %14
  %600 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %599, i32 %2) #8
  %601 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 16
  %602 = bitcast <4 x i64>* %601 to <8 x i32>*
  store <8 x i32> %600, <8 x i32>* %602, align 32
  %603 = bitcast <4 x i64>* %380 to <8 x i32>*
  %604 = load <8 x i32>, <8 x i32>* %603, align 32
  %605 = mul <8 x i32> %604, %242
  %606 = add <8 x i32> %605, %14
  %607 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %606, i32 %2) #8
  %608 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 17
  %609 = bitcast <4 x i64>* %608 to <8 x i32>*
  store <8 x i32> %607, <8 x i32>* %609, align 32
  %610 = bitcast <4 x i64>* %362 to <8 x i32>*
  %611 = load <8 x i32>, <8 x i32>* %610, align 32
  %612 = mul <8 x i32> %611, %176
  %613 = add <8 x i32> %612, %14
  %614 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %613, i32 %2) #8
  %615 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 18
  %616 = bitcast <4 x i64>* %615 to <8 x i32>*
  store <8 x i32> %614, <8 x i32>* %616, align 32
  %617 = bitcast <4 x i64>* %377 to <8 x i32>*
  %618 = load <8 x i32>, <8 x i32>* %617, align 32
  %619 = mul <8 x i32> %618, %280
  %620 = add <8 x i32> %619, %14
  %621 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %620, i32 %2) #8
  %622 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 19
  %623 = bitcast <4 x i64>* %622 to <8 x i32>*
  store <8 x i32> %621, <8 x i32>* %623, align 32
  %624 = bitcast <4 x i64>* %365 to <8 x i32>*
  %625 = load <8 x i32>, <8 x i32>* %624, align 32
  %626 = mul <8 x i32> %625, %194
  %627 = add <8 x i32> %626, %14
  %628 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %627, i32 %2) #8
  %629 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 20
  %630 = bitcast <4 x i64>* %629 to <8 x i32>*
  store <8 x i32> %628, <8 x i32>* %630, align 32
  %631 = bitcast <4 x i64>* %374 to <8 x i32>*
  %632 = load <8 x i32>, <8 x i32>* %631, align 32
  %633 = mul <8 x i32> %632, %261
  %634 = add <8 x i32> %633, %14
  %635 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %634, i32 %2) #8
  %636 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 21
  %637 = bitcast <4 x i64>* %636 to <8 x i32>*
  store <8 x i32> %635, <8 x i32>* %637, align 32
  %638 = bitcast <4 x i64>* %368 to <8 x i32>*
  %639 = load <8 x i32>, <8 x i32>* %638, align 32
  %640 = mul <8 x i32> %639, %158
  %641 = add <8 x i32> %640, %14
  %642 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %641, i32 %2) #8
  %643 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 22
  %644 = bitcast <4 x i64>* %643 to <8 x i32>*
  store <8 x i32> %642, <8 x i32>* %644, align 32
  %645 = bitcast <4 x i64>* %371 to <8 x i32>*
  %646 = load <8 x i32>, <8 x i32>* %645, align 32
  %647 = mul <8 x i32> %646, %299
  %648 = add <8 x i32> %647, %14
  %649 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %648, i32 %2) #8
  %650 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 23
  %651 = bitcast <4 x i64>* %650 to <8 x i32>*
  store <8 x i32> %649, <8 x i32>* %651, align 32
  %652 = mul <8 x i32> %646, %50
  %653 = add <8 x i32> %652, %14
  %654 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %653, i32 %2) #8
  %655 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 24
  %656 = bitcast <4 x i64>* %655 to <8 x i32>*
  store <8 x i32> %654, <8 x i32>* %656, align 32
  %657 = mul <8 x i32> %639, %125
  %658 = add <8 x i32> %657, %14
  %659 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %658, i32 %2) #8
  %660 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 25
  %661 = bitcast <4 x i64>* %660 to <8 x i32>*
  store <8 x i32> %659, <8 x i32>* %661, align 32
  %662 = mul <8 x i32> %632, %110
  %663 = add <8 x i32> %662, %14
  %664 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %663, i32 %2) #8
  %665 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 26
  %666 = bitcast <4 x i64>* %665 to <8 x i32>*
  store <8 x i32> %664, <8 x i32>* %666, align 32
  %667 = mul <8 x i32> %625, %65
  %668 = add <8 x i32> %667, %14
  %669 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %668, i32 %2) #8
  %670 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 27
  %671 = bitcast <4 x i64>* %670 to <8 x i32>*
  store <8 x i32> %669, <8 x i32>* %671, align 32
  %672 = mul <8 x i32> %618, %80
  %673 = add <8 x i32> %672, %14
  %674 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %673, i32 %2) #8
  %675 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 28
  %676 = bitcast <4 x i64>* %675 to <8 x i32>*
  store <8 x i32> %674, <8 x i32>* %676, align 32
  %677 = mul <8 x i32> %611, %95
  %678 = add <8 x i32> %677, %14
  %679 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %678, i32 %2) #8
  %680 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 29
  %681 = bitcast <4 x i64>* %680 to <8 x i32>*
  store <8 x i32> %679, <8 x i32>* %681, align 32
  %682 = mul <8 x i32> %604, %140
  %683 = add <8 x i32> %682, %14
  %684 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %683, i32 %2) #8
  %685 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 30
  %686 = bitcast <4 x i64>* %685 to <8 x i32>*
  store <8 x i32> %684, <8 x i32>* %686, align 32
  %687 = mul <8 x i32> %597, %35
  %688 = add <8 x i32> %687, %14
  %689 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %688, i32 %2) #8
  %690 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 31
  %691 = bitcast <4 x i64>* %690 to <8 x i32>*
  store <8 x i32> %689, <8 x i32>* %691, align 32
  br label %692

692:                                              ; preds = %6, %692
  %693 = phi i64 [ 32, %6 ], [ %737, %692 ]
  %694 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %693
  %695 = bitcast <4 x i64>* %694 to <8 x i32>*
  %696 = load <8 x i32>, <8 x i32>* %695, align 32
  %697 = or i64 %693, 1
  %698 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %697
  %699 = bitcast <4 x i64>* %698 to <8 x i32>*
  %700 = load <8 x i32>, <8 x i32>* %699, align 32
  %701 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %693
  %702 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %697
  %703 = add <8 x i32> %700, %696
  %704 = sub <8 x i32> %696, %700
  %705 = icmp sgt <8 x i32> %703, %24
  %706 = select <8 x i1> %705, <8 x i32> %703, <8 x i32> %24
  %707 = icmp slt <8 x i32> %706, %27
  %708 = select <8 x i1> %707, <8 x i32> %706, <8 x i32> %27
  %709 = icmp sgt <8 x i32> %704, %24
  %710 = select <8 x i1> %709, <8 x i32> %704, <8 x i32> %24
  %711 = icmp slt <8 x i32> %710, %27
  %712 = select <8 x i1> %711, <8 x i32> %710, <8 x i32> %27
  %713 = bitcast <4 x i64>* %701 to <8 x i32>*
  store <8 x i32> %708, <8 x i32>* %713, align 32
  %714 = bitcast <4 x i64>* %702 to <8 x i32>*
  store <8 x i32> %712, <8 x i32>* %714, align 32
  %715 = or i64 %693, 3
  %716 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %715
  %717 = bitcast <4 x i64>* %716 to <8 x i32>*
  %718 = load <8 x i32>, <8 x i32>* %717, align 32
  %719 = or i64 %693, 2
  %720 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %719
  %721 = bitcast <4 x i64>* %720 to <8 x i32>*
  %722 = load <8 x i32>, <8 x i32>* %721, align 32
  %723 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %715
  %724 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %719
  %725 = add <8 x i32> %722, %718
  %726 = sub <8 x i32> %718, %722
  %727 = icmp sgt <8 x i32> %725, %24
  %728 = select <8 x i1> %727, <8 x i32> %725, <8 x i32> %24
  %729 = icmp slt <8 x i32> %728, %27
  %730 = select <8 x i1> %729, <8 x i32> %728, <8 x i32> %27
  %731 = icmp sgt <8 x i32> %726, %24
  %732 = select <8 x i1> %731, <8 x i32> %726, <8 x i32> %24
  %733 = icmp slt <8 x i32> %732, %27
  %734 = select <8 x i1> %733, <8 x i32> %732, <8 x i32> %27
  %735 = bitcast <4 x i64>* %723 to <8 x i32>*
  store <8 x i32> %730, <8 x i32>* %735, align 32
  %736 = bitcast <4 x i64>* %724 to <8 x i32>*
  store <8 x i32> %734, <8 x i32>* %736, align 32
  %737 = add nuw nsw i64 %693, 4
  %738 = icmp ult i64 %737, 64
  br i1 %738, label %692, label %739

739:                                              ; preds = %692
  %740 = shufflevector <8 x i32> %42, <8 x i32> undef, <8 x i32> zeroinitializer
  %741 = shufflevector <8 x i32> %57, <8 x i32> undef, <8 x i32> zeroinitializer
  %742 = shufflevector <8 x i32> %72, <8 x i32> undef, <8 x i32> zeroinitializer
  %743 = shufflevector <8 x i32> %87, <8 x i32> undef, <8 x i32> zeroinitializer
  %744 = shufflevector <8 x i32> %102, <8 x i32> undef, <8 x i32> zeroinitializer
  %745 = shufflevector <8 x i32> %117, <8 x i32> undef, <8 x i32> zeroinitializer
  %746 = shufflevector <8 x i32> %132, <8 x i32> undef, <8 x i32> zeroinitializer
  %747 = shufflevector <8 x i32> %147, <8 x i32> undef, <8 x i32> zeroinitializer
  %748 = shufflevector <8 x i32> %154, <8 x i32> undef, <8 x i32> zeroinitializer
  %749 = shufflevector <8 x i32> %165, <8 x i32> undef, <8 x i32> zeroinitializer
  %750 = shufflevector <8 x i32> %172, <8 x i32> undef, <8 x i32> zeroinitializer
  %751 = shufflevector <8 x i32> %183, <8 x i32> undef, <8 x i32> zeroinitializer
  %752 = shufflevector <8 x i32> %190, <8 x i32> undef, <8 x i32> zeroinitializer
  %753 = shufflevector <8 x i32> %201, <8 x i32> undef, <8 x i32> zeroinitializer
  %754 = shufflevector <8 x i32> %208, <8 x i32> undef, <8 x i32> zeroinitializer
  %755 = shufflevector <8 x i32> %218, <8 x i32> undef, <8 x i32> zeroinitializer
  %756 = shufflevector <8 x i32> %220, <8 x i32> undef, <8 x i32> zeroinitializer
  %757 = shufflevector <8 x i32> %222, <8 x i32> undef, <8 x i32> zeroinitializer
  %758 = shufflevector <8 x i32> %224, <8 x i32> undef, <8 x i32> zeroinitializer
  %759 = shufflevector <8 x i32> %226, <8 x i32> undef, <8 x i32> zeroinitializer
  %760 = shufflevector <8 x i32> %228, <8 x i32> undef, <8 x i32> zeroinitializer
  %761 = shufflevector <8 x i32> %230, <8 x i32> undef, <8 x i32> zeroinitializer
  %762 = shufflevector <8 x i32> %232, <8 x i32> undef, <8 x i32> zeroinitializer
  %763 = shufflevector <8 x i32> %244, <8 x i32> undef, <8 x i32> zeroinitializer
  %764 = shufflevector <8 x i32> %251, <8 x i32> undef, <8 x i32> zeroinitializer
  %765 = shufflevector <8 x i32> %263, <8 x i32> undef, <8 x i32> zeroinitializer
  %766 = shufflevector <8 x i32> %270, <8 x i32> undef, <8 x i32> zeroinitializer
  %767 = shufflevector <8 x i32> %282, <8 x i32> undef, <8 x i32> zeroinitializer
  %768 = shufflevector <8 x i32> %289, <8 x i32> undef, <8 x i32> zeroinitializer
  %769 = shufflevector <8 x i32> %301, <8 x i32> undef, <8 x i32> zeroinitializer
  %770 = bitcast <4 x i64>* %383 to <8 x i32>*
  %771 = load <8 x i32>, <8 x i32>* %770, align 32
  %772 = mul <8 x i32> %771, %754
  %773 = add <8 x i32> %772, %14
  %774 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %773, i32 %2) #8
  %775 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 8
  %776 = bitcast <4 x i64>* %775 to <8 x i32>*
  store <8 x i32> %774, <8 x i32>* %776, align 32
  %777 = bitcast <4 x i64>* %392 to <8 x i32>*
  %778 = load <8 x i32>, <8 x i32>* %777, align 32
  %779 = mul <8 x i32> %778, %763
  %780 = add <8 x i32> %779, %14
  %781 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %780, i32 %2) #8
  %782 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 9
  %783 = bitcast <4 x i64>* %782 to <8 x i32>*
  store <8 x i32> %781, <8 x i32>* %783, align 32
  %784 = bitcast <4 x i64>* %386 to <8 x i32>*
  %785 = load <8 x i32>, <8 x i32>* %784, align 32
  %786 = mul <8 x i32> %785, %750
  %787 = add <8 x i32> %786, %14
  %788 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %787, i32 %2) #8
  %789 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 10
  %790 = bitcast <4 x i64>* %789 to <8 x i32>*
  store <8 x i32> %788, <8 x i32>* %790, align 32
  %791 = bitcast <4 x i64>* %389 to <8 x i32>*
  %792 = load <8 x i32>, <8 x i32>* %791, align 32
  %793 = mul <8 x i32> %792, %767
  %794 = add <8 x i32> %793, %14
  %795 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %794, i32 %2) #8
  %796 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 11
  %797 = bitcast <4 x i64>* %796 to <8 x i32>*
  store <8 x i32> %795, <8 x i32>* %797, align 32
  %798 = mul <8 x i32> %792, %742
  %799 = add <8 x i32> %798, %14
  %800 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %799, i32 %2) #8
  %801 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 12
  %802 = bitcast <4 x i64>* %801 to <8 x i32>*
  store <8 x i32> %800, <8 x i32>* %802, align 32
  %803 = mul <8 x i32> %785, %744
  %804 = add <8 x i32> %803, %14
  %805 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %804, i32 %2) #8
  %806 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 13
  %807 = bitcast <4 x i64>* %806 to <8 x i32>*
  store <8 x i32> %805, <8 x i32>* %807, align 32
  %808 = mul <8 x i32> %778, %746
  %809 = add <8 x i32> %808, %14
  %810 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %809, i32 %2) #8
  %811 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 14
  %812 = bitcast <4 x i64>* %811 to <8 x i32>*
  store <8 x i32> %810, <8 x i32>* %812, align 32
  %813 = mul <8 x i32> %771, %740
  %814 = add <8 x i32> %813, %14
  %815 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %814, i32 %2) #8
  %816 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 15
  %817 = bitcast <4 x i64>* %816 to <8 x i32>*
  store <8 x i32> %815, <8 x i32>* %817, align 32
  %818 = load <8 x i32>, <8 x i32>* %602, align 32
  %819 = load <8 x i32>, <8 x i32>* %609, align 32
  %820 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 17
  %821 = add <8 x i32> %819, %818
  %822 = sub <8 x i32> %818, %819
  %823 = icmp sgt <8 x i32> %821, %24
  %824 = select <8 x i1> %823, <8 x i32> %821, <8 x i32> %24
  %825 = icmp slt <8 x i32> %824, %27
  %826 = select <8 x i1> %825, <8 x i32> %824, <8 x i32> %27
  %827 = icmp sgt <8 x i32> %822, %24
  %828 = select <8 x i1> %827, <8 x i32> %822, <8 x i32> %24
  %829 = icmp slt <8 x i32> %828, %27
  %830 = select <8 x i1> %829, <8 x i32> %828, <8 x i32> %27
  store <8 x i32> %826, <8 x i32>* %596, align 32
  %831 = bitcast <4 x i64>* %820 to <8 x i32>*
  store <8 x i32> %830, <8 x i32>* %831, align 32
  %832 = load <8 x i32>, <8 x i32>* %623, align 32
  %833 = load <8 x i32>, <8 x i32>* %616, align 32
  %834 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 19
  %835 = add <8 x i32> %833, %832
  %836 = sub <8 x i32> %832, %833
  %837 = icmp sgt <8 x i32> %835, %24
  %838 = select <8 x i1> %837, <8 x i32> %835, <8 x i32> %24
  %839 = icmp slt <8 x i32> %838, %27
  %840 = select <8 x i1> %839, <8 x i32> %838, <8 x i32> %27
  %841 = icmp sgt <8 x i32> %836, %24
  %842 = select <8 x i1> %841, <8 x i32> %836, <8 x i32> %24
  %843 = icmp slt <8 x i32> %842, %27
  %844 = select <8 x i1> %843, <8 x i32> %842, <8 x i32> %27
  %845 = bitcast <4 x i64>* %834 to <8 x i32>*
  store <8 x i32> %840, <8 x i32>* %845, align 32
  store <8 x i32> %844, <8 x i32>* %610, align 32
  %846 = load <8 x i32>, <8 x i32>* %630, align 32
  %847 = load <8 x i32>, <8 x i32>* %637, align 32
  %848 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 21
  %849 = add <8 x i32> %847, %846
  %850 = sub <8 x i32> %846, %847
  %851 = icmp sgt <8 x i32> %849, %24
  %852 = select <8 x i1> %851, <8 x i32> %849, <8 x i32> %24
  %853 = icmp slt <8 x i32> %852, %27
  %854 = select <8 x i1> %853, <8 x i32> %852, <8 x i32> %27
  %855 = icmp sgt <8 x i32> %850, %24
  %856 = select <8 x i1> %855, <8 x i32> %850, <8 x i32> %24
  %857 = icmp slt <8 x i32> %856, %27
  %858 = select <8 x i1> %857, <8 x i32> %856, <8 x i32> %27
  store <8 x i32> %854, <8 x i32>* %624, align 32
  %859 = bitcast <4 x i64>* %848 to <8 x i32>*
  store <8 x i32> %858, <8 x i32>* %859, align 32
  %860 = load <8 x i32>, <8 x i32>* %651, align 32
  %861 = load <8 x i32>, <8 x i32>* %644, align 32
  %862 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 23
  %863 = add <8 x i32> %861, %860
  %864 = sub <8 x i32> %860, %861
  %865 = icmp sgt <8 x i32> %863, %24
  %866 = select <8 x i1> %865, <8 x i32> %863, <8 x i32> %24
  %867 = icmp slt <8 x i32> %866, %27
  %868 = select <8 x i1> %867, <8 x i32> %866, <8 x i32> %27
  %869 = icmp sgt <8 x i32> %864, %24
  %870 = select <8 x i1> %869, <8 x i32> %864, <8 x i32> %24
  %871 = icmp slt <8 x i32> %870, %27
  %872 = select <8 x i1> %871, <8 x i32> %870, <8 x i32> %27
  %873 = bitcast <4 x i64>* %862 to <8 x i32>*
  store <8 x i32> %868, <8 x i32>* %873, align 32
  store <8 x i32> %872, <8 x i32>* %638, align 32
  %874 = load <8 x i32>, <8 x i32>* %656, align 32
  %875 = load <8 x i32>, <8 x i32>* %661, align 32
  %876 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 25
  %877 = add <8 x i32> %875, %874
  %878 = sub <8 x i32> %874, %875
  %879 = icmp sgt <8 x i32> %877, %24
  %880 = select <8 x i1> %879, <8 x i32> %877, <8 x i32> %24
  %881 = icmp slt <8 x i32> %880, %27
  %882 = select <8 x i1> %881, <8 x i32> %880, <8 x i32> %27
  %883 = icmp sgt <8 x i32> %878, %24
  %884 = select <8 x i1> %883, <8 x i32> %878, <8 x i32> %24
  %885 = icmp slt <8 x i32> %884, %27
  %886 = select <8 x i1> %885, <8 x i32> %884, <8 x i32> %27
  store <8 x i32> %882, <8 x i32>* %645, align 32
  %887 = bitcast <4 x i64>* %876 to <8 x i32>*
  store <8 x i32> %886, <8 x i32>* %887, align 32
  %888 = load <8 x i32>, <8 x i32>* %671, align 32
  %889 = load <8 x i32>, <8 x i32>* %666, align 32
  %890 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 27
  %891 = add <8 x i32> %889, %888
  %892 = sub <8 x i32> %888, %889
  %893 = icmp sgt <8 x i32> %891, %24
  %894 = select <8 x i1> %893, <8 x i32> %891, <8 x i32> %24
  %895 = icmp slt <8 x i32> %894, %27
  %896 = select <8 x i1> %895, <8 x i32> %894, <8 x i32> %27
  %897 = icmp sgt <8 x i32> %892, %24
  %898 = select <8 x i1> %897, <8 x i32> %892, <8 x i32> %24
  %899 = icmp slt <8 x i32> %898, %27
  %900 = select <8 x i1> %899, <8 x i32> %898, <8 x i32> %27
  %901 = bitcast <4 x i64>* %890 to <8 x i32>*
  store <8 x i32> %896, <8 x i32>* %901, align 32
  store <8 x i32> %900, <8 x i32>* %631, align 32
  %902 = load <8 x i32>, <8 x i32>* %676, align 32
  %903 = load <8 x i32>, <8 x i32>* %681, align 32
  %904 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 29
  %905 = add <8 x i32> %903, %902
  %906 = sub <8 x i32> %902, %903
  %907 = icmp sgt <8 x i32> %905, %24
  %908 = select <8 x i1> %907, <8 x i32> %905, <8 x i32> %24
  %909 = icmp slt <8 x i32> %908, %27
  %910 = select <8 x i1> %909, <8 x i32> %908, <8 x i32> %27
  %911 = icmp sgt <8 x i32> %906, %24
  %912 = select <8 x i1> %911, <8 x i32> %906, <8 x i32> %24
  %913 = icmp slt <8 x i32> %912, %27
  %914 = select <8 x i1> %913, <8 x i32> %912, <8 x i32> %27
  store <8 x i32> %910, <8 x i32>* %617, align 32
  %915 = bitcast <4 x i64>* %904 to <8 x i32>*
  store <8 x i32> %914, <8 x i32>* %915, align 32
  %916 = load <8 x i32>, <8 x i32>* %691, align 32
  %917 = load <8 x i32>, <8 x i32>* %686, align 32
  %918 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 31
  %919 = add <8 x i32> %917, %916
  %920 = sub <8 x i32> %916, %917
  %921 = icmp sgt <8 x i32> %919, %24
  %922 = select <8 x i1> %921, <8 x i32> %919, <8 x i32> %24
  %923 = icmp slt <8 x i32> %922, %27
  %924 = select <8 x i1> %923, <8 x i32> %922, <8 x i32> %27
  %925 = icmp sgt <8 x i32> %920, %24
  %926 = select <8 x i1> %925, <8 x i32> %920, <8 x i32> %24
  %927 = icmp slt <8 x i32> %926, %27
  %928 = select <8 x i1> %927, <8 x i32> %926, <8 x i32> %27
  %929 = bitcast <4 x i64>* %918 to <8 x i32>*
  store <8 x i32> %924, <8 x i32>* %929, align 32
  store <8 x i32> %928, <8 x i32>* %603, align 32
  %930 = load <4 x i64>, <4 x i64>* %311, align 32
  store <4 x i64> %930, <4 x i64>* %409, align 32
  %931 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 35
  %932 = load <4 x i64>, <4 x i64>* %931, align 32
  store <4 x i64> %932, <4 x i64>* %430, align 32
  %933 = load <4 x i64>, <4 x i64>* %317, align 32
  store <4 x i64> %933, <4 x i64>* %437, align 32
  %934 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 39
  %935 = load <4 x i64>, <4 x i64>* %934, align 32
  store <4 x i64> %935, <4 x i64>* %458, align 32
  %936 = load <4 x i64>, <4 x i64>* %323, align 32
  store <4 x i64> %936, <4 x i64>* %465, align 32
  %937 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 43
  %938 = load <4 x i64>, <4 x i64>* %937, align 32
  store <4 x i64> %938, <4 x i64>* %486, align 32
  %939 = load <4 x i64>, <4 x i64>* %329, align 32
  store <4 x i64> %939, <4 x i64>* %493, align 32
  %940 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 47
  %941 = load <4 x i64>, <4 x i64>* %940, align 32
  store <4 x i64> %941, <4 x i64>* %514, align 32
  %942 = load <4 x i64>, <4 x i64>* %335, align 32
  store <4 x i64> %942, <4 x i64>* %519, align 32
  %943 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 51
  %944 = load <4 x i64>, <4 x i64>* %943, align 32
  store <4 x i64> %944, <4 x i64>* %534, align 32
  %945 = load <4 x i64>, <4 x i64>* %341, align 32
  store <4 x i64> %945, <4 x i64>* %539, align 32
  %946 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 55
  %947 = load <4 x i64>, <4 x i64>* %946, align 32
  store <4 x i64> %947, <4 x i64>* %554, align 32
  %948 = load <4 x i64>, <4 x i64>* %347, align 32
  store <4 x i64> %948, <4 x i64>* %559, align 32
  %949 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 59
  %950 = load <4 x i64>, <4 x i64>* %949, align 32
  store <4 x i64> %950, <4 x i64>* %574, align 32
  %951 = load <4 x i64>, <4 x i64>* %353, align 32
  store <4 x i64> %951, <4 x i64>* %579, align 32
  %952 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 63
  %953 = load <4 x i64>, <4 x i64>* %952, align 32
  store <4 x i64> %953, <4 x i64>* %594, align 32
  %954 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 33
  %955 = bitcast <4 x i64>* %954 to <8 x i32>*
  %956 = load <8 x i32>, <8 x i32>* %955, align 32
  %957 = mul <8 x i32> %956, %755
  %958 = load <8 x i32>, <8 x i32>* %411, align 32
  %959 = mul <8 x i32> %958, %754
  %960 = add <8 x i32> %957, %14
  %961 = add <8 x i32> %960, %959
  %962 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %961, i32 %2) #8
  store <8 x i32> %962, <8 x i32>* %417, align 32
  %963 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 61
  %964 = load <8 x i32>, <8 x i32>* %418, align 32
  %965 = mul <8 x i32> %964, %769
  %966 = bitcast <4 x i64>* %963 to <8 x i32>*
  %967 = load <8 x i32>, <8 x i32>* %966, align 32
  %968 = mul <8 x i32> %967, %755
  %969 = add <8 x i32> %965, %14
  %970 = add <8 x i32> %969, %968
  %971 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %970, i32 %2) #8
  store <8 x i32> %971, <8 x i32>* %424, align 32
  %972 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 37
  %973 = bitcast <4 x i64>* %972 to <8 x i32>*
  %974 = load <8 x i32>, <8 x i32>* %973, align 32
  %975 = mul <8 x i32> %974, %763
  %976 = load <8 x i32>, <8 x i32>* %439, align 32
  %977 = mul <8 x i32> %976, %746
  %978 = add <8 x i32> %975, %14
  %979 = add <8 x i32> %978, %977
  %980 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %979, i32 %2) #8
  store <8 x i32> %980, <8 x i32>* %445, align 32
  %981 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 57
  %982 = load <8 x i32>, <8 x i32>* %446, align 32
  %983 = mul <8 x i32> %982, %761
  %984 = bitcast <4 x i64>* %981 to <8 x i32>*
  %985 = load <8 x i32>, <8 x i32>* %984, align 32
  %986 = mul <8 x i32> %985, %763
  %987 = add <8 x i32> %983, %14
  %988 = add <8 x i32> %987, %986
  %989 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %988, i32 %2) #8
  store <8 x i32> %989, <8 x i32>* %452, align 32
  %990 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 41
  %991 = bitcast <4 x i64>* %990 to <8 x i32>*
  %992 = load <8 x i32>, <8 x i32>* %991, align 32
  %993 = mul <8 x i32> %992, %759
  %994 = load <8 x i32>, <8 x i32>* %467, align 32
  %995 = mul <8 x i32> %994, %750
  %996 = add <8 x i32> %993, %14
  %997 = add <8 x i32> %996, %995
  %998 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %997, i32 %2) #8
  store <8 x i32> %998, <8 x i32>* %473, align 32
  %999 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 53
  %1000 = load <8 x i32>, <8 x i32>* %474, align 32
  %1001 = mul <8 x i32> %1000, %765
  %1002 = bitcast <4 x i64>* %999 to <8 x i32>*
  %1003 = load <8 x i32>, <8 x i32>* %1002, align 32
  %1004 = mul <8 x i32> %1003, %759
  %1005 = add <8 x i32> %1001, %14
  %1006 = add <8 x i32> %1005, %1004
  %1007 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1006, i32 %2) #8
  store <8 x i32> %1007, <8 x i32>* %480, align 32
  %1008 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 45
  %1009 = bitcast <4 x i64>* %1008 to <8 x i32>*
  %1010 = load <8 x i32>, <8 x i32>* %1009, align 32
  %1011 = mul <8 x i32> %1010, %767
  %1012 = load <8 x i32>, <8 x i32>* %495, align 32
  %1013 = mul <8 x i32> %1012, %742
  %1014 = add <8 x i32> %1011, %14
  %1015 = add <8 x i32> %1014, %1013
  %1016 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1015, i32 %2) #8
  store <8 x i32> %1016, <8 x i32>* %501, align 32
  %1017 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 49
  %1018 = load <8 x i32>, <8 x i32>* %502, align 32
  %1019 = mul <8 x i32> %1018, %757
  %1020 = bitcast <4 x i64>* %1017 to <8 x i32>*
  %1021 = load <8 x i32>, <8 x i32>* %1020, align 32
  %1022 = mul <8 x i32> %1021, %767
  %1023 = add <8 x i32> %1019, %14
  %1024 = add <8 x i32> %1023, %1022
  %1025 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1024, i32 %2) #8
  store <8 x i32> %1025, <8 x i32>* %508, align 32
  %1026 = mul <8 x i32> %1018, %767
  %1027 = mul <8 x i32> %1021, %742
  %1028 = add <8 x i32> %1026, %14
  %1029 = add <8 x i32> %1028, %1027
  %1030 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1029, i32 %2) #8
  store <8 x i32> %1030, <8 x i32>* %525, align 32
  %1031 = mul <8 x i32> %1010, %742
  %1032 = mul <8 x i32> %1012, %752
  %1033 = add <8 x i32> %1031, %14
  %1034 = add <8 x i32> %1033, %1032
  %1035 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1034, i32 %2) #8
  store <8 x i32> %1035, <8 x i32>* %530, align 32
  %1036 = mul <8 x i32> %1000, %759
  %1037 = mul <8 x i32> %1003, %750
  %1038 = add <8 x i32> %1036, %14
  %1039 = add <8 x i32> %1038, %1037
  %1040 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1039, i32 %2) #8
  store <8 x i32> %1040, <8 x i32>* %545, align 32
  %1041 = mul <8 x i32> %992, %750
  %1042 = mul <8 x i32> %994, %744
  %1043 = add <8 x i32> %1041, %14
  %1044 = add <8 x i32> %1043, %1042
  %1045 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1044, i32 %2) #8
  store <8 x i32> %1045, <8 x i32>* %550, align 32
  %1046 = mul <8 x i32> %982, %763
  %1047 = mul <8 x i32> %985, %746
  %1048 = add <8 x i32> %1046, %14
  %1049 = add <8 x i32> %1048, %1047
  %1050 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1049, i32 %2) #8
  store <8 x i32> %1050, <8 x i32>* %565, align 32
  %1051 = mul <8 x i32> %974, %746
  %1052 = mul <8 x i32> %976, %748
  %1053 = add <8 x i32> %1051, %14
  %1054 = add <8 x i32> %1053, %1052
  %1055 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1054, i32 %2) #8
  store <8 x i32> %1055, <8 x i32>* %570, align 32
  %1056 = mul <8 x i32> %964, %755
  %1057 = mul <8 x i32> %967, %754
  %1058 = add <8 x i32> %1056, %14
  %1059 = add <8 x i32> %1058, %1057
  %1060 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1059, i32 %2) #8
  store <8 x i32> %1060, <8 x i32>* %585, align 32
  %1061 = mul <8 x i32> %956, %754
  %1062 = mul <8 x i32> %958, %740
  %1063 = add <8 x i32> %1061, %14
  %1064 = add <8 x i32> %1063, %1062
  %1065 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1064, i32 %2) #8
  store <8 x i32> %1065, <8 x i32>* %590, align 32
  %1066 = bitcast <4 x i64>* %395 to <8 x i32>*
  %1067 = load <8 x i32>, <8 x i32>* %1066, align 32
  %1068 = mul <8 x i32> %1067, %753
  %1069 = add <8 x i32> %1068, %14
  %1070 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1069, i32 %2) #8
  %1071 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 4
  %1072 = bitcast <4 x i64>* %1071 to <8 x i32>*
  store <8 x i32> %1070, <8 x i32>* %1072, align 32
  %1073 = bitcast <4 x i64>* %398 to <8 x i32>*
  %1074 = load <8 x i32>, <8 x i32>* %1073, align 32
  %1075 = mul <8 x i32> %1074, %764
  %1076 = add <8 x i32> %1075, %14
  %1077 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1076, i32 %2) #8
  %1078 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 5
  %1079 = bitcast <4 x i64>* %1078 to <8 x i32>*
  store <8 x i32> %1077, <8 x i32>* %1079, align 32
  %1080 = mul <8 x i32> %1074, %745
  %1081 = add <8 x i32> %1080, %14
  %1082 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1081, i32 %2) #8
  %1083 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 6
  %1084 = bitcast <4 x i64>* %1083 to <8 x i32>*
  store <8 x i32> %1082, <8 x i32>* %1084, align 32
  %1085 = mul <8 x i32> %1067, %741
  %1086 = add <8 x i32> %1085, %14
  %1087 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1086, i32 %2) #8
  %1088 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 7
  %1089 = bitcast <4 x i64>* %1088 to <8 x i32>*
  store <8 x i32> %1087, <8 x i32>* %1089, align 32
  %1090 = load <8 x i32>, <8 x i32>* %776, align 32
  %1091 = load <8 x i32>, <8 x i32>* %783, align 32
  %1092 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 9
  %1093 = add <8 x i32> %1091, %1090
  %1094 = sub <8 x i32> %1090, %1091
  %1095 = icmp sgt <8 x i32> %1093, %24
  %1096 = select <8 x i1> %1095, <8 x i32> %1093, <8 x i32> %24
  %1097 = icmp slt <8 x i32> %1096, %27
  %1098 = select <8 x i1> %1097, <8 x i32> %1096, <8 x i32> %27
  %1099 = icmp sgt <8 x i32> %1094, %24
  %1100 = select <8 x i1> %1099, <8 x i32> %1094, <8 x i32> %24
  %1101 = icmp slt <8 x i32> %1100, %27
  %1102 = select <8 x i1> %1101, <8 x i32> %1100, <8 x i32> %27
  store <8 x i32> %1098, <8 x i32>* %770, align 32
  %1103 = bitcast <4 x i64>* %1092 to <8 x i32>*
  store <8 x i32> %1102, <8 x i32>* %1103, align 32
  %1104 = load <8 x i32>, <8 x i32>* %797, align 32
  %1105 = load <8 x i32>, <8 x i32>* %790, align 32
  %1106 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 11
  %1107 = add <8 x i32> %1105, %1104
  %1108 = sub <8 x i32> %1104, %1105
  %1109 = icmp sgt <8 x i32> %1107, %24
  %1110 = select <8 x i1> %1109, <8 x i32> %1107, <8 x i32> %24
  %1111 = icmp slt <8 x i32> %1110, %27
  %1112 = select <8 x i1> %1111, <8 x i32> %1110, <8 x i32> %27
  %1113 = icmp sgt <8 x i32> %1108, %24
  %1114 = select <8 x i1> %1113, <8 x i32> %1108, <8 x i32> %24
  %1115 = icmp slt <8 x i32> %1114, %27
  %1116 = select <8 x i1> %1115, <8 x i32> %1114, <8 x i32> %27
  %1117 = bitcast <4 x i64>* %1106 to <8 x i32>*
  store <8 x i32> %1112, <8 x i32>* %1117, align 32
  store <8 x i32> %1116, <8 x i32>* %784, align 32
  %1118 = load <8 x i32>, <8 x i32>* %802, align 32
  %1119 = load <8 x i32>, <8 x i32>* %807, align 32
  %1120 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 13
  %1121 = add <8 x i32> %1119, %1118
  %1122 = sub <8 x i32> %1118, %1119
  %1123 = icmp sgt <8 x i32> %1121, %24
  %1124 = select <8 x i1> %1123, <8 x i32> %1121, <8 x i32> %24
  %1125 = icmp slt <8 x i32> %1124, %27
  %1126 = select <8 x i1> %1125, <8 x i32> %1124, <8 x i32> %27
  %1127 = icmp sgt <8 x i32> %1122, %24
  %1128 = select <8 x i1> %1127, <8 x i32> %1122, <8 x i32> %24
  %1129 = icmp slt <8 x i32> %1128, %27
  %1130 = select <8 x i1> %1129, <8 x i32> %1128, <8 x i32> %27
  store <8 x i32> %1126, <8 x i32>* %791, align 32
  %1131 = bitcast <4 x i64>* %1120 to <8 x i32>*
  store <8 x i32> %1130, <8 x i32>* %1131, align 32
  %1132 = load <8 x i32>, <8 x i32>* %817, align 32
  %1133 = load <8 x i32>, <8 x i32>* %812, align 32
  %1134 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 15
  %1135 = add <8 x i32> %1133, %1132
  %1136 = sub <8 x i32> %1132, %1133
  %1137 = icmp sgt <8 x i32> %1135, %24
  %1138 = select <8 x i1> %1137, <8 x i32> %1135, <8 x i32> %24
  %1139 = icmp slt <8 x i32> %1138, %27
  %1140 = select <8 x i1> %1139, <8 x i32> %1138, <8 x i32> %27
  %1141 = icmp sgt <8 x i32> %1136, %24
  %1142 = select <8 x i1> %1141, <8 x i32> %1136, <8 x i32> %24
  %1143 = icmp slt <8 x i32> %1142, %27
  %1144 = select <8 x i1> %1143, <8 x i32> %1142, <8 x i32> %27
  %1145 = bitcast <4 x i64>* %1134 to <8 x i32>*
  store <8 x i32> %1140, <8 x i32>* %1145, align 32
  store <8 x i32> %1144, <8 x i32>* %777, align 32
  %1146 = load <4 x i64>, <4 x i64>* %359, align 32
  store <4 x i64> %1146, <4 x i64>* %601, align 32
  %1147 = load <4 x i64>, <4 x i64>* %834, align 32
  store <4 x i64> %1147, <4 x i64>* %622, align 32
  %1148 = load <4 x i64>, <4 x i64>* %365, align 32
  store <4 x i64> %1148, <4 x i64>* %629, align 32
  %1149 = load <4 x i64>, <4 x i64>* %862, align 32
  store <4 x i64> %1149, <4 x i64>* %650, align 32
  %1150 = load <4 x i64>, <4 x i64>* %371, align 32
  store <4 x i64> %1150, <4 x i64>* %655, align 32
  %1151 = load <4 x i64>, <4 x i64>* %890, align 32
  store <4 x i64> %1151, <4 x i64>* %670, align 32
  %1152 = load <4 x i64>, <4 x i64>* %377, align 32
  store <4 x i64> %1152, <4 x i64>* %675, align 32
  %1153 = load <4 x i64>, <4 x i64>* %918, align 32
  store <4 x i64> %1153, <4 x i64>* %690, align 32
  %1154 = load <8 x i32>, <8 x i32>* %831, align 32
  %1155 = mul <8 x i32> %1154, %756
  %1156 = load <8 x i32>, <8 x i32>* %603, align 32
  %1157 = mul <8 x i32> %1156, %753
  %1158 = add <8 x i32> %1155, %14
  %1159 = add <8 x i32> %1158, %1157
  %1160 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1159, i32 %2) #8
  store <8 x i32> %1160, <8 x i32>* %609, align 32
  %1161 = load <8 x i32>, <8 x i32>* %610, align 32
  %1162 = mul <8 x i32> %1161, %768
  %1163 = load <8 x i32>, <8 x i32>* %915, align 32
  %1164 = mul <8 x i32> %1163, %756
  %1165 = add <8 x i32> %1162, %14
  %1166 = add <8 x i32> %1165, %1164
  %1167 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1166, i32 %2) #8
  store <8 x i32> %1167, <8 x i32>* %616, align 32
  %1168 = load <8 x i32>, <8 x i32>* %859, align 32
  %1169 = mul <8 x i32> %1168, %764
  %1170 = load <8 x i32>, <8 x i32>* %631, align 32
  %1171 = mul <8 x i32> %1170, %745
  %1172 = add <8 x i32> %1169, %14
  %1173 = add <8 x i32> %1172, %1171
  %1174 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1173, i32 %2) #8
  store <8 x i32> %1174, <8 x i32>* %637, align 32
  %1175 = load <8 x i32>, <8 x i32>* %638, align 32
  %1176 = mul <8 x i32> %1175, %760
  %1177 = load <8 x i32>, <8 x i32>* %887, align 32
  %1178 = mul <8 x i32> %1177, %764
  %1179 = add <8 x i32> %1176, %14
  %1180 = add <8 x i32> %1179, %1178
  %1181 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1180, i32 %2) #8
  store <8 x i32> %1181, <8 x i32>* %644, align 32
  %1182 = mul <8 x i32> %1175, %764
  %1183 = mul <8 x i32> %1177, %745
  %1184 = add <8 x i32> %1182, %14
  %1185 = add <8 x i32> %1184, %1183
  %1186 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1185, i32 %2) #8
  store <8 x i32> %1186, <8 x i32>* %661, align 32
  %1187 = mul <8 x i32> %1168, %745
  %1188 = mul <8 x i32> %1170, %749
  %1189 = add <8 x i32> %1187, %14
  %1190 = add <8 x i32> %1189, %1188
  %1191 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1190, i32 %2) #8
  store <8 x i32> %1191, <8 x i32>* %666, align 32
  %1192 = mul <8 x i32> %1161, %756
  %1193 = mul <8 x i32> %1163, %753
  %1194 = add <8 x i32> %1192, %14
  %1195 = add <8 x i32> %1194, %1193
  %1196 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1195, i32 %2) #8
  store <8 x i32> %1196, <8 x i32>* %681, align 32
  %1197 = mul <8 x i32> %1154, %753
  %1198 = mul <8 x i32> %1156, %741
  %1199 = add <8 x i32> %1197, %14
  %1200 = add <8 x i32> %1199, %1198
  %1201 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1200, i32 %2) #8
  store <8 x i32> %1201, <8 x i32>* %686, align 32
  br label %1202

1202:                                             ; preds = %739, %1202
  %1203 = phi i64 [ 32, %739 ], [ %1291, %1202 ]
  %1204 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %1203
  %1205 = bitcast <4 x i64>* %1204 to <8 x i32>*
  %1206 = load <8 x i32>, <8 x i32>* %1205, align 32
  %1207 = or i64 %1203, 3
  %1208 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %1207
  %1209 = bitcast <4 x i64>* %1208 to <8 x i32>*
  %1210 = load <8 x i32>, <8 x i32>* %1209, align 32
  %1211 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %1203
  %1212 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %1207
  %1213 = add <8 x i32> %1210, %1206
  %1214 = sub <8 x i32> %1206, %1210
  %1215 = icmp sgt <8 x i32> %1213, %24
  %1216 = select <8 x i1> %1215, <8 x i32> %1213, <8 x i32> %24
  %1217 = icmp slt <8 x i32> %1216, %27
  %1218 = select <8 x i1> %1217, <8 x i32> %1216, <8 x i32> %27
  %1219 = icmp sgt <8 x i32> %1214, %24
  %1220 = select <8 x i1> %1219, <8 x i32> %1214, <8 x i32> %24
  %1221 = icmp slt <8 x i32> %1220, %27
  %1222 = select <8 x i1> %1221, <8 x i32> %1220, <8 x i32> %27
  %1223 = bitcast <4 x i64>* %1211 to <8 x i32>*
  store <8 x i32> %1218, <8 x i32>* %1223, align 32
  %1224 = bitcast <4 x i64>* %1212 to <8 x i32>*
  store <8 x i32> %1222, <8 x i32>* %1224, align 32
  %1225 = or i64 %1203, 1
  %1226 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %1225
  %1227 = bitcast <4 x i64>* %1226 to <8 x i32>*
  %1228 = load <8 x i32>, <8 x i32>* %1227, align 32
  %1229 = or i64 %1203, 2
  %1230 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %1229
  %1231 = bitcast <4 x i64>* %1230 to <8 x i32>*
  %1232 = load <8 x i32>, <8 x i32>* %1231, align 32
  %1233 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %1225
  %1234 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %1229
  %1235 = add <8 x i32> %1232, %1228
  %1236 = sub <8 x i32> %1228, %1232
  %1237 = icmp sgt <8 x i32> %1235, %24
  %1238 = select <8 x i1> %1237, <8 x i32> %1235, <8 x i32> %24
  %1239 = icmp slt <8 x i32> %1238, %27
  %1240 = select <8 x i1> %1239, <8 x i32> %1238, <8 x i32> %27
  %1241 = icmp sgt <8 x i32> %1236, %24
  %1242 = select <8 x i1> %1241, <8 x i32> %1236, <8 x i32> %24
  %1243 = icmp slt <8 x i32> %1242, %27
  %1244 = select <8 x i1> %1243, <8 x i32> %1242, <8 x i32> %27
  %1245 = bitcast <4 x i64>* %1233 to <8 x i32>*
  store <8 x i32> %1240, <8 x i32>* %1245, align 32
  %1246 = bitcast <4 x i64>* %1234 to <8 x i32>*
  store <8 x i32> %1244, <8 x i32>* %1246, align 32
  %1247 = or i64 %1203, 7
  %1248 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %1247
  %1249 = bitcast <4 x i64>* %1248 to <8 x i32>*
  %1250 = load <8 x i32>, <8 x i32>* %1249, align 32
  %1251 = or i64 %1203, 4
  %1252 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %1251
  %1253 = bitcast <4 x i64>* %1252 to <8 x i32>*
  %1254 = load <8 x i32>, <8 x i32>* %1253, align 32
  %1255 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %1247
  %1256 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %1251
  %1257 = add <8 x i32> %1254, %1250
  %1258 = sub <8 x i32> %1250, %1254
  %1259 = icmp sgt <8 x i32> %1257, %24
  %1260 = select <8 x i1> %1259, <8 x i32> %1257, <8 x i32> %24
  %1261 = icmp slt <8 x i32> %1260, %27
  %1262 = select <8 x i1> %1261, <8 x i32> %1260, <8 x i32> %27
  %1263 = icmp sgt <8 x i32> %1258, %24
  %1264 = select <8 x i1> %1263, <8 x i32> %1258, <8 x i32> %24
  %1265 = icmp slt <8 x i32> %1264, %27
  %1266 = select <8 x i1> %1265, <8 x i32> %1264, <8 x i32> %27
  %1267 = bitcast <4 x i64>* %1255 to <8 x i32>*
  store <8 x i32> %1262, <8 x i32>* %1267, align 32
  %1268 = bitcast <4 x i64>* %1256 to <8 x i32>*
  store <8 x i32> %1266, <8 x i32>* %1268, align 32
  %1269 = or i64 %1203, 6
  %1270 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %1269
  %1271 = bitcast <4 x i64>* %1270 to <8 x i32>*
  %1272 = load <8 x i32>, <8 x i32>* %1271, align 32
  %1273 = or i64 %1203, 5
  %1274 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %1273
  %1275 = bitcast <4 x i64>* %1274 to <8 x i32>*
  %1276 = load <8 x i32>, <8 x i32>* %1275, align 32
  %1277 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %1269
  %1278 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %1273
  %1279 = add <8 x i32> %1276, %1272
  %1280 = sub <8 x i32> %1272, %1276
  %1281 = icmp sgt <8 x i32> %1279, %24
  %1282 = select <8 x i1> %1281, <8 x i32> %1279, <8 x i32> %24
  %1283 = icmp slt <8 x i32> %1282, %27
  %1284 = select <8 x i1> %1283, <8 x i32> %1282, <8 x i32> %27
  %1285 = icmp sgt <8 x i32> %1280, %24
  %1286 = select <8 x i1> %1285, <8 x i32> %1280, <8 x i32> %24
  %1287 = icmp slt <8 x i32> %1286, %27
  %1288 = select <8 x i1> %1287, <8 x i32> %1286, <8 x i32> %27
  %1289 = bitcast <4 x i64>* %1277 to <8 x i32>*
  store <8 x i32> %1284, <8 x i32>* %1289, align 32
  %1290 = bitcast <4 x i64>* %1278 to <8 x i32>*
  store <8 x i32> %1288, <8 x i32>* %1290, align 32
  %1291 = add nuw nsw i64 %1203, 8
  %1292 = icmp ult i64 %1291, 64
  br i1 %1292, label %1202, label %1293

1293:                                             ; preds = %1202
  %1294 = bitcast [64 x <4 x i64>]* %7 to <8 x i32>*
  %1295 = load <8 x i32>, <8 x i32>* %1294, align 32
  %1296 = mul <8 x i32> %1295, %747
  %1297 = add <8 x i32> %1296, %14
  %1298 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1297, i32 %2) #8
  %1299 = bitcast [64 x <4 x i64>]* %8 to <8 x i32>*
  store <8 x i32> %1298, <8 x i32>* %1299, align 32
  %1300 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 1
  %1301 = bitcast <4 x i64>* %1300 to <8 x i32>*
  store <8 x i32> %1298, <8 x i32>* %1301, align 32
  %1302 = bitcast <4 x i64>* %403 to <8 x i32>*
  %1303 = load <8 x i32>, <8 x i32>* %1302, align 32
  %1304 = mul <8 x i32> %1303, %751
  %1305 = add <8 x i32> %1304, %14
  %1306 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1305, i32 %2) #8
  %1307 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 2
  %1308 = bitcast <4 x i64>* %1307 to <8 x i32>*
  store <8 x i32> %1306, <8 x i32>* %1308, align 32
  %1309 = mul <8 x i32> %1303, %743
  %1310 = add <8 x i32> %1309, %14
  %1311 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1310, i32 %2) #8
  %1312 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 3
  %1313 = bitcast <4 x i64>* %1312 to <8 x i32>*
  store <8 x i32> %1311, <8 x i32>* %1313, align 32
  %1314 = load <8 x i32>, <8 x i32>* %1072, align 32
  %1315 = load <8 x i32>, <8 x i32>* %1079, align 32
  %1316 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 5
  %1317 = add <8 x i32> %1315, %1314
  %1318 = sub <8 x i32> %1314, %1315
  %1319 = icmp sgt <8 x i32> %1317, %24
  %1320 = select <8 x i1> %1319, <8 x i32> %1317, <8 x i32> %24
  %1321 = icmp slt <8 x i32> %1320, %27
  %1322 = select <8 x i1> %1321, <8 x i32> %1320, <8 x i32> %27
  %1323 = icmp sgt <8 x i32> %1318, %24
  %1324 = select <8 x i1> %1323, <8 x i32> %1318, <8 x i32> %24
  %1325 = icmp slt <8 x i32> %1324, %27
  %1326 = select <8 x i1> %1325, <8 x i32> %1324, <8 x i32> %27
  store <8 x i32> %1322, <8 x i32>* %1066, align 32
  %1327 = bitcast <4 x i64>* %1316 to <8 x i32>*
  store <8 x i32> %1326, <8 x i32>* %1327, align 32
  %1328 = load <8 x i32>, <8 x i32>* %1089, align 32
  %1329 = load <8 x i32>, <8 x i32>* %1084, align 32
  %1330 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 7
  %1331 = add <8 x i32> %1329, %1328
  %1332 = sub <8 x i32> %1328, %1329
  %1333 = icmp sgt <8 x i32> %1331, %24
  %1334 = select <8 x i1> %1333, <8 x i32> %1331, <8 x i32> %24
  %1335 = icmp slt <8 x i32> %1334, %27
  %1336 = select <8 x i1> %1335, <8 x i32> %1334, <8 x i32> %27
  %1337 = icmp sgt <8 x i32> %1332, %24
  %1338 = select <8 x i1> %1337, <8 x i32> %1332, <8 x i32> %24
  %1339 = icmp slt <8 x i32> %1338, %27
  %1340 = select <8 x i1> %1339, <8 x i32> %1338, <8 x i32> %27
  %1341 = bitcast <4 x i64>* %1330 to <8 x i32>*
  store <8 x i32> %1336, <8 x i32>* %1341, align 32
  store <8 x i32> %1340, <8 x i32>* %1073, align 32
  %1342 = load <4 x i64>, <4 x i64>* %383, align 32
  store <4 x i64> %1342, <4 x i64>* %775, align 32
  %1343 = load <4 x i64>, <4 x i64>* %1106, align 32
  store <4 x i64> %1343, <4 x i64>* %796, align 32
  %1344 = load <4 x i64>, <4 x i64>* %389, align 32
  store <4 x i64> %1344, <4 x i64>* %801, align 32
  %1345 = load <4 x i64>, <4 x i64>* %1134, align 32
  store <4 x i64> %1345, <4 x i64>* %816, align 32
  %1346 = load <8 x i32>, <8 x i32>* %1103, align 32
  %1347 = mul <8 x i32> %1346, %758
  %1348 = load <8 x i32>, <8 x i32>* %777, align 32
  %1349 = mul <8 x i32> %1348, %751
  %1350 = add <8 x i32> %1347, %14
  %1351 = add <8 x i32> %1350, %1349
  %1352 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1351, i32 %2) #8
  store <8 x i32> %1352, <8 x i32>* %783, align 32
  %1353 = load <8 x i32>, <8 x i32>* %784, align 32
  %1354 = mul <8 x i32> %1353, %766
  %1355 = load <8 x i32>, <8 x i32>* %1131, align 32
  %1356 = mul <8 x i32> %1355, %758
  %1357 = add <8 x i32> %1354, %14
  %1358 = add <8 x i32> %1357, %1356
  %1359 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1358, i32 %2) #8
  store <8 x i32> %1359, <8 x i32>* %790, align 32
  %1360 = mul <8 x i32> %1353, %758
  %1361 = mul <8 x i32> %1355, %751
  %1362 = add <8 x i32> %1360, %14
  %1363 = add <8 x i32> %1362, %1361
  %1364 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1363, i32 %2) #8
  store <8 x i32> %1364, <8 x i32>* %807, align 32
  %1365 = mul <8 x i32> %1346, %751
  %1366 = mul <8 x i32> %1348, %743
  %1367 = add <8 x i32> %1365, %14
  %1368 = add <8 x i32> %1367, %1366
  %1369 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1368, i32 %2) #8
  store <8 x i32> %1369, <8 x i32>* %812, align 32
  %1370 = load <8 x i32>, <8 x i32>* %602, align 32
  %1371 = load <8 x i32>, <8 x i32>* %623, align 32
  %1372 = add <8 x i32> %1371, %1370
  %1373 = sub <8 x i32> %1370, %1371
  %1374 = icmp sgt <8 x i32> %1372, %24
  %1375 = select <8 x i1> %1374, <8 x i32> %1372, <8 x i32> %24
  %1376 = icmp slt <8 x i32> %1375, %27
  %1377 = select <8 x i1> %1376, <8 x i32> %1375, <8 x i32> %27
  %1378 = icmp sgt <8 x i32> %1373, %24
  %1379 = select <8 x i1> %1378, <8 x i32> %1373, <8 x i32> %24
  %1380 = icmp slt <8 x i32> %1379, %27
  %1381 = select <8 x i1> %1380, <8 x i32> %1379, <8 x i32> %27
  store <8 x i32> %1377, <8 x i32>* %596, align 32
  store <8 x i32> %1381, <8 x i32>* %845, align 32
  %1382 = load <8 x i32>, <8 x i32>* %609, align 32
  %1383 = load <8 x i32>, <8 x i32>* %616, align 32
  %1384 = add <8 x i32> %1383, %1382
  %1385 = sub <8 x i32> %1382, %1383
  %1386 = icmp sgt <8 x i32> %1384, %24
  %1387 = select <8 x i1> %1386, <8 x i32> %1384, <8 x i32> %24
  %1388 = icmp slt <8 x i32> %1387, %27
  %1389 = select <8 x i1> %1388, <8 x i32> %1387, <8 x i32> %27
  %1390 = icmp sgt <8 x i32> %1385, %24
  %1391 = select <8 x i1> %1390, <8 x i32> %1385, <8 x i32> %24
  %1392 = icmp slt <8 x i32> %1391, %27
  %1393 = select <8 x i1> %1392, <8 x i32> %1391, <8 x i32> %27
  store <8 x i32> %1389, <8 x i32>* %831, align 32
  store <8 x i32> %1393, <8 x i32>* %610, align 32
  %1394 = load <8 x i32>, <8 x i32>* %651, align 32
  %1395 = load <8 x i32>, <8 x i32>* %630, align 32
  %1396 = add <8 x i32> %1395, %1394
  %1397 = sub <8 x i32> %1394, %1395
  %1398 = icmp sgt <8 x i32> %1396, %24
  %1399 = select <8 x i1> %1398, <8 x i32> %1396, <8 x i32> %24
  %1400 = icmp slt <8 x i32> %1399, %27
  %1401 = select <8 x i1> %1400, <8 x i32> %1399, <8 x i32> %27
  %1402 = icmp sgt <8 x i32> %1397, %24
  %1403 = select <8 x i1> %1402, <8 x i32> %1397, <8 x i32> %24
  %1404 = icmp slt <8 x i32> %1403, %27
  %1405 = select <8 x i1> %1404, <8 x i32> %1403, <8 x i32> %27
  store <8 x i32> %1401, <8 x i32>* %873, align 32
  store <8 x i32> %1405, <8 x i32>* %624, align 32
  %1406 = load <8 x i32>, <8 x i32>* %644, align 32
  %1407 = load <8 x i32>, <8 x i32>* %637, align 32
  %1408 = add <8 x i32> %1407, %1406
  %1409 = sub <8 x i32> %1406, %1407
  %1410 = icmp sgt <8 x i32> %1408, %24
  %1411 = select <8 x i1> %1410, <8 x i32> %1408, <8 x i32> %24
  %1412 = icmp slt <8 x i32> %1411, %27
  %1413 = select <8 x i1> %1412, <8 x i32> %1411, <8 x i32> %27
  %1414 = icmp sgt <8 x i32> %1409, %24
  %1415 = select <8 x i1> %1414, <8 x i32> %1409, <8 x i32> %24
  %1416 = icmp slt <8 x i32> %1415, %27
  %1417 = select <8 x i1> %1416, <8 x i32> %1415, <8 x i32> %27
  store <8 x i32> %1413, <8 x i32>* %638, align 32
  store <8 x i32> %1417, <8 x i32>* %859, align 32
  %1418 = load <8 x i32>, <8 x i32>* %656, align 32
  %1419 = load <8 x i32>, <8 x i32>* %671, align 32
  %1420 = add <8 x i32> %1419, %1418
  %1421 = sub <8 x i32> %1418, %1419
  %1422 = icmp sgt <8 x i32> %1420, %24
  %1423 = select <8 x i1> %1422, <8 x i32> %1420, <8 x i32> %24
  %1424 = icmp slt <8 x i32> %1423, %27
  %1425 = select <8 x i1> %1424, <8 x i32> %1423, <8 x i32> %27
  %1426 = icmp sgt <8 x i32> %1421, %24
  %1427 = select <8 x i1> %1426, <8 x i32> %1421, <8 x i32> %24
  %1428 = icmp slt <8 x i32> %1427, %27
  %1429 = select <8 x i1> %1428, <8 x i32> %1427, <8 x i32> %27
  store <8 x i32> %1425, <8 x i32>* %645, align 32
  store <8 x i32> %1429, <8 x i32>* %901, align 32
  %1430 = load <8 x i32>, <8 x i32>* %661, align 32
  %1431 = load <8 x i32>, <8 x i32>* %666, align 32
  %1432 = add <8 x i32> %1431, %1430
  %1433 = sub <8 x i32> %1430, %1431
  %1434 = icmp sgt <8 x i32> %1432, %24
  %1435 = select <8 x i1> %1434, <8 x i32> %1432, <8 x i32> %24
  %1436 = icmp slt <8 x i32> %1435, %27
  %1437 = select <8 x i1> %1436, <8 x i32> %1435, <8 x i32> %27
  %1438 = icmp sgt <8 x i32> %1433, %24
  %1439 = select <8 x i1> %1438, <8 x i32> %1433, <8 x i32> %24
  %1440 = icmp slt <8 x i32> %1439, %27
  %1441 = select <8 x i1> %1440, <8 x i32> %1439, <8 x i32> %27
  store <8 x i32> %1437, <8 x i32>* %887, align 32
  store <8 x i32> %1441, <8 x i32>* %631, align 32
  %1442 = load <8 x i32>, <8 x i32>* %691, align 32
  %1443 = load <8 x i32>, <8 x i32>* %676, align 32
  %1444 = add <8 x i32> %1443, %1442
  %1445 = sub <8 x i32> %1442, %1443
  %1446 = icmp sgt <8 x i32> %1444, %24
  %1447 = select <8 x i1> %1446, <8 x i32> %1444, <8 x i32> %24
  %1448 = icmp slt <8 x i32> %1447, %27
  %1449 = select <8 x i1> %1448, <8 x i32> %1447, <8 x i32> %27
  %1450 = icmp sgt <8 x i32> %1445, %24
  %1451 = select <8 x i1> %1450, <8 x i32> %1445, <8 x i32> %24
  %1452 = icmp slt <8 x i32> %1451, %27
  %1453 = select <8 x i1> %1452, <8 x i32> %1451, <8 x i32> %27
  store <8 x i32> %1449, <8 x i32>* %929, align 32
  store <8 x i32> %1453, <8 x i32>* %617, align 32
  %1454 = load <8 x i32>, <8 x i32>* %686, align 32
  %1455 = load <8 x i32>, <8 x i32>* %681, align 32
  %1456 = add <8 x i32> %1455, %1454
  %1457 = sub <8 x i32> %1454, %1455
  %1458 = icmp sgt <8 x i32> %1456, %24
  %1459 = select <8 x i1> %1458, <8 x i32> %1456, <8 x i32> %24
  %1460 = icmp slt <8 x i32> %1459, %27
  %1461 = select <8 x i1> %1460, <8 x i32> %1459, <8 x i32> %27
  %1462 = icmp sgt <8 x i32> %1457, %24
  %1463 = select <8 x i1> %1462, <8 x i32> %1457, <8 x i32> %24
  %1464 = icmp slt <8 x i32> %1463, %27
  %1465 = select <8 x i1> %1464, <8 x i32> %1463, <8 x i32> %27
  store <8 x i32> %1461, <8 x i32>* %603, align 32
  store <8 x i32> %1465, <8 x i32>* %915, align 32
  %1466 = load <4 x i64>, <4 x i64>* %311, align 32
  store <4 x i64> %1466, <4 x i64>* %409, align 32
  %1467 = load <4 x i64>, <4 x i64>* %954, align 32
  store <4 x i64> %1467, <4 x i64>* %416, align 32
  %1468 = load <4 x i64>, <4 x i64>* %320, align 32
  store <4 x i64> %1468, <4 x i64>* %451, align 32
  %1469 = load <4 x i64>, <4 x i64>* %934, align 32
  store <4 x i64> %1469, <4 x i64>* %458, align 32
  %1470 = load <4 x i64>, <4 x i64>* %323, align 32
  store <4 x i64> %1470, <4 x i64>* %465, align 32
  %1471 = load <4 x i64>, <4 x i64>* %990, align 32
  store <4 x i64> %1471, <4 x i64>* %472, align 32
  %1472 = load <4 x i64>, <4 x i64>* %332, align 32
  store <4 x i64> %1472, <4 x i64>* %507, align 32
  %1473 = load <4 x i64>, <4 x i64>* %940, align 32
  store <4 x i64> %1473, <4 x i64>* %514, align 32
  %1474 = load <4 x i64>, <4 x i64>* %335, align 32
  store <4 x i64> %1474, <4 x i64>* %519, align 32
  %1475 = load <4 x i64>, <4 x i64>* %1017, align 32
  store <4 x i64> %1475, <4 x i64>* %524, align 32
  %1476 = load <4 x i64>, <4 x i64>* %344, align 32
  store <4 x i64> %1476, <4 x i64>* %549, align 32
  %1477 = load <4 x i64>, <4 x i64>* %946, align 32
  store <4 x i64> %1477, <4 x i64>* %554, align 32
  %1478 = load <4 x i64>, <4 x i64>* %347, align 32
  store <4 x i64> %1478, <4 x i64>* %559, align 32
  %1479 = load <4 x i64>, <4 x i64>* %981, align 32
  store <4 x i64> %1479, <4 x i64>* %564, align 32
  %1480 = load <4 x i64>, <4 x i64>* %356, align 32
  store <4 x i64> %1480, <4 x i64>* %589, align 32
  %1481 = load <4 x i64>, <4 x i64>* %952, align 32
  store <4 x i64> %1481, <4 x i64>* %594, align 32
  %1482 = load <8 x i32>, <8 x i32>* %418, align 32
  %1483 = mul <8 x i32> %1482, %756
  %1484 = load <8 x i32>, <8 x i32>* %966, align 32
  %1485 = mul <8 x i32> %1484, %753
  %1486 = add <8 x i32> %1483, %14
  %1487 = add <8 x i32> %1486, %1485
  %1488 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1487, i32 %2) #8
  store <8 x i32> %1488, <8 x i32>* %424, align 32
  %1489 = bitcast <4 x i64>* %931 to <8 x i32>*
  %1490 = load <8 x i32>, <8 x i32>* %1489, align 32
  %1491 = mul <8 x i32> %1490, %756
  %1492 = load <8 x i32>, <8 x i32>* %425, align 32
  %1493 = mul <8 x i32> %1492, %753
  %1494 = add <8 x i32> %1491, %14
  %1495 = add <8 x i32> %1494, %1493
  %1496 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1495, i32 %2) #8
  store <8 x i32> %1496, <8 x i32>* %431, align 32
  %1497 = load <8 x i32>, <8 x i32>* %432, align 32
  %1498 = mul <8 x i32> %1497, %768
  %1499 = bitcast <4 x i64>* %949 to <8 x i32>*
  %1500 = load <8 x i32>, <8 x i32>* %1499, align 32
  %1501 = mul <8 x i32> %1500, %756
  %1502 = add <8 x i32> %1498, %14
  %1503 = add <8 x i32> %1502, %1501
  %1504 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1503, i32 %2) #8
  store <8 x i32> %1504, <8 x i32>* %438, align 32
  %1505 = load <8 x i32>, <8 x i32>* %973, align 32
  %1506 = mul <8 x i32> %1505, %768
  %1507 = load <8 x i32>, <8 x i32>* %439, align 32
  %1508 = mul <8 x i32> %1507, %756
  %1509 = add <8 x i32> %1506, %14
  %1510 = add <8 x i32> %1509, %1508
  %1511 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1510, i32 %2) #8
  store <8 x i32> %1511, <8 x i32>* %445, align 32
  %1512 = load <8 x i32>, <8 x i32>* %474, align 32
  %1513 = mul <8 x i32> %1512, %764
  %1514 = load <8 x i32>, <8 x i32>* %1002, align 32
  %1515 = mul <8 x i32> %1514, %745
  %1516 = add <8 x i32> %1513, %14
  %1517 = add <8 x i32> %1516, %1515
  %1518 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1517, i32 %2) #8
  store <8 x i32> %1518, <8 x i32>* %480, align 32
  %1519 = bitcast <4 x i64>* %937 to <8 x i32>*
  %1520 = load <8 x i32>, <8 x i32>* %1519, align 32
  %1521 = mul <8 x i32> %1520, %764
  %1522 = load <8 x i32>, <8 x i32>* %481, align 32
  %1523 = mul <8 x i32> %1522, %745
  %1524 = add <8 x i32> %1521, %14
  %1525 = add <8 x i32> %1524, %1523
  %1526 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1525, i32 %2) #8
  store <8 x i32> %1526, <8 x i32>* %487, align 32
  %1527 = load <8 x i32>, <8 x i32>* %488, align 32
  %1528 = mul <8 x i32> %1527, %760
  %1529 = bitcast <4 x i64>* %943 to <8 x i32>*
  %1530 = load <8 x i32>, <8 x i32>* %1529, align 32
  %1531 = mul <8 x i32> %1530, %764
  %1532 = add <8 x i32> %1528, %14
  %1533 = add <8 x i32> %1532, %1531
  %1534 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1533, i32 %2) #8
  store <8 x i32> %1534, <8 x i32>* %494, align 32
  %1535 = load <8 x i32>, <8 x i32>* %1009, align 32
  %1536 = mul <8 x i32> %1535, %760
  %1537 = load <8 x i32>, <8 x i32>* %495, align 32
  %1538 = mul <8 x i32> %1537, %764
  %1539 = add <8 x i32> %1536, %14
  %1540 = add <8 x i32> %1539, %1538
  %1541 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1540, i32 %2) #8
  store <8 x i32> %1541, <8 x i32>* %501, align 32
  %1542 = mul <8 x i32> %1535, %764
  %1543 = mul <8 x i32> %1537, %745
  %1544 = add <8 x i32> %1542, %14
  %1545 = add <8 x i32> %1544, %1543
  %1546 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1545, i32 %2) #8
  store <8 x i32> %1546, <8 x i32>* %530, align 32
  %1547 = mul <8 x i32> %1527, %764
  %1548 = mul <8 x i32> %1530, %745
  %1549 = add <8 x i32> %1547, %14
  %1550 = add <8 x i32> %1549, %1548
  %1551 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1550, i32 %2) #8
  store <8 x i32> %1551, <8 x i32>* %535, align 32
  %1552 = mul <8 x i32> %1520, %745
  %1553 = mul <8 x i32> %1522, %749
  %1554 = add <8 x i32> %1552, %14
  %1555 = add <8 x i32> %1554, %1553
  %1556 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1555, i32 %2) #8
  store <8 x i32> %1556, <8 x i32>* %540, align 32
  %1557 = mul <8 x i32> %1512, %745
  %1558 = mul <8 x i32> %1514, %749
  %1559 = add <8 x i32> %1557, %14
  %1560 = add <8 x i32> %1559, %1558
  %1561 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1560, i32 %2) #8
  store <8 x i32> %1561, <8 x i32>* %545, align 32
  %1562 = mul <8 x i32> %1505, %756
  %1563 = mul <8 x i32> %1507, %753
  %1564 = add <8 x i32> %1562, %14
  %1565 = add <8 x i32> %1564, %1563
  %1566 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1565, i32 %2) #8
  store <8 x i32> %1566, <8 x i32>* %570, align 32
  %1567 = mul <8 x i32> %1497, %756
  %1568 = mul <8 x i32> %1500, %753
  %1569 = add <8 x i32> %1567, %14
  %1570 = add <8 x i32> %1569, %1568
  %1571 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1570, i32 %2) #8
  store <8 x i32> %1571, <8 x i32>* %575, align 32
  %1572 = mul <8 x i32> %1490, %753
  %1573 = mul <8 x i32> %1492, %741
  %1574 = add <8 x i32> %1572, %14
  %1575 = add <8 x i32> %1574, %1573
  %1576 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1575, i32 %2) #8
  store <8 x i32> %1576, <8 x i32>* %580, align 32
  %1577 = mul <8 x i32> %1482, %753
  %1578 = mul <8 x i32> %1484, %741
  %1579 = add <8 x i32> %1577, %14
  %1580 = add <8 x i32> %1579, %1578
  %1581 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1580, i32 %2) #8
  store <8 x i32> %1581, <8 x i32>* %585, align 32
  %1582 = load <8 x i32>, <8 x i32>* %1299, align 32
  %1583 = load <8 x i32>, <8 x i32>* %1313, align 32
  %1584 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 3
  %1585 = add <8 x i32> %1583, %1582
  %1586 = sub <8 x i32> %1582, %1583
  %1587 = icmp sgt <8 x i32> %1585, %24
  %1588 = select <8 x i1> %1587, <8 x i32> %1585, <8 x i32> %24
  %1589 = icmp slt <8 x i32> %1588, %27
  %1590 = select <8 x i1> %1589, <8 x i32> %1588, <8 x i32> %27
  %1591 = icmp sgt <8 x i32> %1586, %24
  %1592 = select <8 x i1> %1591, <8 x i32> %1586, <8 x i32> %24
  %1593 = icmp slt <8 x i32> %1592, %27
  %1594 = select <8 x i1> %1593, <8 x i32> %1592, <8 x i32> %27
  store <8 x i32> %1590, <8 x i32>* %1294, align 32
  %1595 = bitcast <4 x i64>* %1584 to <8 x i32>*
  store <8 x i32> %1594, <8 x i32>* %1595, align 32
  %1596 = load <8 x i32>, <8 x i32>* %1301, align 32
  %1597 = load <8 x i32>, <8 x i32>* %1308, align 32
  %1598 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 1
  %1599 = add <8 x i32> %1597, %1596
  %1600 = sub <8 x i32> %1596, %1597
  %1601 = icmp sgt <8 x i32> %1599, %24
  %1602 = select <8 x i1> %1601, <8 x i32> %1599, <8 x i32> %24
  %1603 = icmp slt <8 x i32> %1602, %27
  %1604 = select <8 x i1> %1603, <8 x i32> %1602, <8 x i32> %27
  %1605 = icmp sgt <8 x i32> %1600, %24
  %1606 = select <8 x i1> %1605, <8 x i32> %1600, <8 x i32> %24
  %1607 = icmp slt <8 x i32> %1606, %27
  %1608 = select <8 x i1> %1607, <8 x i32> %1606, <8 x i32> %27
  %1609 = bitcast <4 x i64>* %1598 to <8 x i32>*
  store <8 x i32> %1604, <8 x i32>* %1609, align 32
  store <8 x i32> %1608, <8 x i32>* %1302, align 32
  %1610 = load <4 x i64>, <4 x i64>* %395, align 32
  store <4 x i64> %1610, <4 x i64>* %1071, align 32
  %1611 = load <4 x i64>, <4 x i64>* %1330, align 32
  store <4 x i64> %1611, <4 x i64>* %1088, align 32
  %1612 = load <8 x i32>, <8 x i32>* %1327, align 32
  %1613 = mul <8 x i32> %1612, %762
  %1614 = load <8 x i32>, <8 x i32>* %1073, align 32
  %1615 = mul <8 x i32> %1614, %747
  %1616 = add <8 x i32> %1615, %14
  %1617 = add <8 x i32> %1616, %1613
  %1618 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1617, i32 %2) #8
  store <8 x i32> %1618, <8 x i32>* %1079, align 32
  %1619 = mul <8 x i32> %1612, %747
  %1620 = add <8 x i32> %1616, %1619
  %1621 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1620, i32 %2) #8
  store <8 x i32> %1621, <8 x i32>* %1084, align 32
  %1622 = load <8 x i32>, <8 x i32>* %776, align 32
  %1623 = load <8 x i32>, <8 x i32>* %797, align 32
  %1624 = add <8 x i32> %1623, %1622
  %1625 = sub <8 x i32> %1622, %1623
  %1626 = icmp sgt <8 x i32> %1624, %24
  %1627 = select <8 x i1> %1626, <8 x i32> %1624, <8 x i32> %24
  %1628 = icmp slt <8 x i32> %1627, %27
  %1629 = select <8 x i1> %1628, <8 x i32> %1627, <8 x i32> %27
  %1630 = icmp sgt <8 x i32> %1625, %24
  %1631 = select <8 x i1> %1630, <8 x i32> %1625, <8 x i32> %24
  %1632 = icmp slt <8 x i32> %1631, %27
  %1633 = select <8 x i1> %1632, <8 x i32> %1631, <8 x i32> %27
  store <8 x i32> %1629, <8 x i32>* %770, align 32
  store <8 x i32> %1633, <8 x i32>* %1117, align 32
  %1634 = load <8 x i32>, <8 x i32>* %783, align 32
  %1635 = load <8 x i32>, <8 x i32>* %790, align 32
  %1636 = add <8 x i32> %1635, %1634
  %1637 = sub <8 x i32> %1634, %1635
  %1638 = icmp sgt <8 x i32> %1636, %24
  %1639 = select <8 x i1> %1638, <8 x i32> %1636, <8 x i32> %24
  %1640 = icmp slt <8 x i32> %1639, %27
  %1641 = select <8 x i1> %1640, <8 x i32> %1639, <8 x i32> %27
  %1642 = icmp sgt <8 x i32> %1637, %24
  %1643 = select <8 x i1> %1642, <8 x i32> %1637, <8 x i32> %24
  %1644 = icmp slt <8 x i32> %1643, %27
  %1645 = select <8 x i1> %1644, <8 x i32> %1643, <8 x i32> %27
  store <8 x i32> %1641, <8 x i32>* %1103, align 32
  store <8 x i32> %1645, <8 x i32>* %784, align 32
  %1646 = load <8 x i32>, <8 x i32>* %817, align 32
  %1647 = load <8 x i32>, <8 x i32>* %802, align 32
  %1648 = add <8 x i32> %1647, %1646
  %1649 = sub <8 x i32> %1646, %1647
  %1650 = icmp sgt <8 x i32> %1648, %24
  %1651 = select <8 x i1> %1650, <8 x i32> %1648, <8 x i32> %24
  %1652 = icmp slt <8 x i32> %1651, %27
  %1653 = select <8 x i1> %1652, <8 x i32> %1651, <8 x i32> %27
  %1654 = icmp sgt <8 x i32> %1649, %24
  %1655 = select <8 x i1> %1654, <8 x i32> %1649, <8 x i32> %24
  %1656 = icmp slt <8 x i32> %1655, %27
  %1657 = select <8 x i1> %1656, <8 x i32> %1655, <8 x i32> %27
  store <8 x i32> %1653, <8 x i32>* %1145, align 32
  store <8 x i32> %1657, <8 x i32>* %791, align 32
  %1658 = load <8 x i32>, <8 x i32>* %812, align 32
  %1659 = load <8 x i32>, <8 x i32>* %807, align 32
  %1660 = add <8 x i32> %1659, %1658
  %1661 = sub <8 x i32> %1658, %1659
  %1662 = icmp sgt <8 x i32> %1660, %24
  %1663 = select <8 x i1> %1662, <8 x i32> %1660, <8 x i32> %24
  %1664 = icmp slt <8 x i32> %1663, %27
  %1665 = select <8 x i1> %1664, <8 x i32> %1663, <8 x i32> %27
  %1666 = icmp sgt <8 x i32> %1661, %24
  %1667 = select <8 x i1> %1666, <8 x i32> %1661, <8 x i32> %24
  %1668 = icmp slt <8 x i32> %1667, %27
  %1669 = select <8 x i1> %1668, <8 x i32> %1667, <8 x i32> %27
  store <8 x i32> %1665, <8 x i32>* %777, align 32
  store <8 x i32> %1669, <8 x i32>* %1131, align 32
  %1670 = load <4 x i64>, <4 x i64>* %359, align 32
  store <4 x i64> %1670, <4 x i64>* %601, align 32
  %1671 = load <4 x i64>, <4 x i64>* %820, align 32
  store <4 x i64> %1671, <4 x i64>* %608, align 32
  %1672 = load <4 x i64>, <4 x i64>* %368, align 32
  store <4 x i64> %1672, <4 x i64>* %643, align 32
  %1673 = load <4 x i64>, <4 x i64>* %862, align 32
  store <4 x i64> %1673, <4 x i64>* %650, align 32
  %1674 = load <4 x i64>, <4 x i64>* %371, align 32
  store <4 x i64> %1674, <4 x i64>* %655, align 32
  %1675 = load <4 x i64>, <4 x i64>* %876, align 32
  store <4 x i64> %1675, <4 x i64>* %660, align 32
  %1676 = load <4 x i64>, <4 x i64>* %380, align 32
  store <4 x i64> %1676, <4 x i64>* %685, align 32
  %1677 = load <4 x i64>, <4 x i64>* %918, align 32
  store <4 x i64> %1677, <4 x i64>* %690, align 32
  %1678 = load <8 x i32>, <8 x i32>* %610, align 32
  %1679 = mul <8 x i32> %1678, %758
  %1680 = load <8 x i32>, <8 x i32>* %915, align 32
  %1681 = mul <8 x i32> %1680, %751
  %1682 = add <8 x i32> %1679, %14
  %1683 = add <8 x i32> %1682, %1681
  %1684 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1683, i32 %2) #8
  store <8 x i32> %1684, <8 x i32>* %616, align 32
  %1685 = load <8 x i32>, <8 x i32>* %845, align 32
  %1686 = mul <8 x i32> %1685, %758
  %1687 = load <8 x i32>, <8 x i32>* %617, align 32
  %1688 = mul <8 x i32> %1687, %751
  %1689 = add <8 x i32> %1686, %14
  %1690 = add <8 x i32> %1689, %1688
  %1691 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1690, i32 %2) #8
  store <8 x i32> %1691, <8 x i32>* %623, align 32
  %1692 = load <8 x i32>, <8 x i32>* %624, align 32
  %1693 = mul <8 x i32> %1692, %766
  %1694 = load <8 x i32>, <8 x i32>* %901, align 32
  %1695 = mul <8 x i32> %1694, %758
  %1696 = add <8 x i32> %1693, %14
  %1697 = add <8 x i32> %1696, %1695
  %1698 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1697, i32 %2) #8
  store <8 x i32> %1698, <8 x i32>* %630, align 32
  %1699 = load <8 x i32>, <8 x i32>* %859, align 32
  %1700 = mul <8 x i32> %1699, %766
  %1701 = load <8 x i32>, <8 x i32>* %631, align 32
  %1702 = mul <8 x i32> %1701, %758
  %1703 = add <8 x i32> %1700, %14
  %1704 = add <8 x i32> %1703, %1702
  %1705 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1704, i32 %2) #8
  store <8 x i32> %1705, <8 x i32>* %637, align 32
  %1706 = mul <8 x i32> %1699, %758
  %1707 = mul <8 x i32> %1701, %751
  %1708 = add <8 x i32> %1706, %14
  %1709 = add <8 x i32> %1708, %1707
  %1710 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1709, i32 %2) #8
  store <8 x i32> %1710, <8 x i32>* %666, align 32
  %1711 = mul <8 x i32> %1692, %758
  %1712 = mul <8 x i32> %1694, %751
  %1713 = add <8 x i32> %1711, %14
  %1714 = add <8 x i32> %1713, %1712
  %1715 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1714, i32 %2) #8
  store <8 x i32> %1715, <8 x i32>* %671, align 32
  %1716 = mul <8 x i32> %1685, %751
  %1717 = mul <8 x i32> %1687, %743
  %1718 = add <8 x i32> %1716, %14
  %1719 = add <8 x i32> %1718, %1717
  %1720 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1719, i32 %2) #8
  store <8 x i32> %1720, <8 x i32>* %676, align 32
  %1721 = mul <8 x i32> %1678, %751
  %1722 = mul <8 x i32> %1680, %743
  %1723 = add <8 x i32> %1721, %14
  %1724 = add <8 x i32> %1723, %1722
  %1725 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %1724, i32 %2) #8
  store <8 x i32> %1725, <8 x i32>* %681, align 32
  %1726 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 32
  %1727 = bitcast <4 x i64>* %1726 to <8 x i32>*
  %1728 = load <8 x i32>, <8 x i32>* %1727, align 32
  %1729 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 39
  %1730 = bitcast <4 x i64>* %1729 to <8 x i32>*
  %1731 = load <8 x i32>, <8 x i32>* %1730, align 32
  %1732 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 32
  %1733 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 39
  %1734 = add <8 x i32> %1731, %1728
  %1735 = sub <8 x i32> %1728, %1731
  %1736 = icmp sgt <8 x i32> %1734, %24
  %1737 = select <8 x i1> %1736, <8 x i32> %1734, <8 x i32> %24
  %1738 = icmp slt <8 x i32> %1737, %27
  %1739 = select <8 x i1> %1738, <8 x i32> %1737, <8 x i32> %27
  %1740 = icmp sgt <8 x i32> %1735, %24
  %1741 = select <8 x i1> %1740, <8 x i32> %1735, <8 x i32> %24
  %1742 = icmp slt <8 x i32> %1741, %27
  %1743 = select <8 x i1> %1742, <8 x i32> %1741, <8 x i32> %27
  %1744 = bitcast <4 x i64>* %1732 to <8 x i32>*
  store <8 x i32> %1739, <8 x i32>* %1744, align 32
  %1745 = bitcast <4 x i64>* %1733 to <8 x i32>*
  store <8 x i32> %1743, <8 x i32>* %1745, align 32
  %1746 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 47
  %1747 = bitcast <4 x i64>* %1746 to <8 x i32>*
  %1748 = load <8 x i32>, <8 x i32>* %1747, align 32
  %1749 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 40
  %1750 = bitcast <4 x i64>* %1749 to <8 x i32>*
  %1751 = load <8 x i32>, <8 x i32>* %1750, align 32
  %1752 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 47
  %1753 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 40
  %1754 = add <8 x i32> %1751, %1748
  %1755 = sub <8 x i32> %1748, %1751
  %1756 = icmp sgt <8 x i32> %1754, %24
  %1757 = select <8 x i1> %1756, <8 x i32> %1754, <8 x i32> %24
  %1758 = icmp slt <8 x i32> %1757, %27
  %1759 = select <8 x i1> %1758, <8 x i32> %1757, <8 x i32> %27
  %1760 = icmp sgt <8 x i32> %1755, %24
  %1761 = select <8 x i1> %1760, <8 x i32> %1755, <8 x i32> %24
  %1762 = icmp slt <8 x i32> %1761, %27
  %1763 = select <8 x i1> %1762, <8 x i32> %1761, <8 x i32> %27
  %1764 = bitcast <4 x i64>* %1752 to <8 x i32>*
  store <8 x i32> %1759, <8 x i32>* %1764, align 32
  %1765 = bitcast <4 x i64>* %1753 to <8 x i32>*
  store <8 x i32> %1763, <8 x i32>* %1765, align 32
  %1766 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 33
  %1767 = bitcast <4 x i64>* %1766 to <8 x i32>*
  %1768 = load <8 x i32>, <8 x i32>* %1767, align 32
  %1769 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 38
  %1770 = bitcast <4 x i64>* %1769 to <8 x i32>*
  %1771 = load <8 x i32>, <8 x i32>* %1770, align 32
  %1772 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 33
  %1773 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 38
  %1774 = add <8 x i32> %1771, %1768
  %1775 = sub <8 x i32> %1768, %1771
  %1776 = icmp sgt <8 x i32> %1774, %24
  %1777 = select <8 x i1> %1776, <8 x i32> %1774, <8 x i32> %24
  %1778 = icmp slt <8 x i32> %1777, %27
  %1779 = select <8 x i1> %1778, <8 x i32> %1777, <8 x i32> %27
  %1780 = icmp sgt <8 x i32> %1775, %24
  %1781 = select <8 x i1> %1780, <8 x i32> %1775, <8 x i32> %24
  %1782 = icmp slt <8 x i32> %1781, %27
  %1783 = select <8 x i1> %1782, <8 x i32> %1781, <8 x i32> %27
  %1784 = bitcast <4 x i64>* %1772 to <8 x i32>*
  store <8 x i32> %1779, <8 x i32>* %1784, align 32
  %1785 = bitcast <4 x i64>* %1773 to <8 x i32>*
  store <8 x i32> %1783, <8 x i32>* %1785, align 32
  %1786 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 46
  %1787 = bitcast <4 x i64>* %1786 to <8 x i32>*
  %1788 = load <8 x i32>, <8 x i32>* %1787, align 32
  %1789 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 41
  %1790 = bitcast <4 x i64>* %1789 to <8 x i32>*
  %1791 = load <8 x i32>, <8 x i32>* %1790, align 32
  %1792 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 46
  %1793 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 41
  %1794 = add <8 x i32> %1791, %1788
  %1795 = sub <8 x i32> %1788, %1791
  %1796 = icmp sgt <8 x i32> %1794, %24
  %1797 = select <8 x i1> %1796, <8 x i32> %1794, <8 x i32> %24
  %1798 = icmp slt <8 x i32> %1797, %27
  %1799 = select <8 x i1> %1798, <8 x i32> %1797, <8 x i32> %27
  %1800 = icmp sgt <8 x i32> %1795, %24
  %1801 = select <8 x i1> %1800, <8 x i32> %1795, <8 x i32> %24
  %1802 = icmp slt <8 x i32> %1801, %27
  %1803 = select <8 x i1> %1802, <8 x i32> %1801, <8 x i32> %27
  %1804 = bitcast <4 x i64>* %1792 to <8 x i32>*
  store <8 x i32> %1799, <8 x i32>* %1804, align 32
  %1805 = bitcast <4 x i64>* %1793 to <8 x i32>*
  store <8 x i32> %1803, <8 x i32>* %1805, align 32
  %1806 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 34
  %1807 = bitcast <4 x i64>* %1806 to <8 x i32>*
  %1808 = load <8 x i32>, <8 x i32>* %1807, align 32
  %1809 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 37
  %1810 = bitcast <4 x i64>* %1809 to <8 x i32>*
  %1811 = load <8 x i32>, <8 x i32>* %1810, align 32
  %1812 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 34
  %1813 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 37
  %1814 = add <8 x i32> %1811, %1808
  %1815 = sub <8 x i32> %1808, %1811
  %1816 = icmp sgt <8 x i32> %1814, %24
  %1817 = select <8 x i1> %1816, <8 x i32> %1814, <8 x i32> %24
  %1818 = icmp slt <8 x i32> %1817, %27
  %1819 = select <8 x i1> %1818, <8 x i32> %1817, <8 x i32> %27
  %1820 = icmp sgt <8 x i32> %1815, %24
  %1821 = select <8 x i1> %1820, <8 x i32> %1815, <8 x i32> %24
  %1822 = icmp slt <8 x i32> %1821, %27
  %1823 = select <8 x i1> %1822, <8 x i32> %1821, <8 x i32> %27
  %1824 = bitcast <4 x i64>* %1812 to <8 x i32>*
  store <8 x i32> %1819, <8 x i32>* %1824, align 32
  %1825 = bitcast <4 x i64>* %1813 to <8 x i32>*
  store <8 x i32> %1823, <8 x i32>* %1825, align 32
  %1826 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 45
  %1827 = bitcast <4 x i64>* %1826 to <8 x i32>*
  %1828 = load <8 x i32>, <8 x i32>* %1827, align 32
  %1829 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 42
  %1830 = bitcast <4 x i64>* %1829 to <8 x i32>*
  %1831 = load <8 x i32>, <8 x i32>* %1830, align 32
  %1832 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 45
  %1833 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 42
  %1834 = add <8 x i32> %1831, %1828
  %1835 = sub <8 x i32> %1828, %1831
  %1836 = icmp sgt <8 x i32> %1834, %24
  %1837 = select <8 x i1> %1836, <8 x i32> %1834, <8 x i32> %24
  %1838 = icmp slt <8 x i32> %1837, %27
  %1839 = select <8 x i1> %1838, <8 x i32> %1837, <8 x i32> %27
  %1840 = icmp sgt <8 x i32> %1835, %24
  %1841 = select <8 x i1> %1840, <8 x i32> %1835, <8 x i32> %24
  %1842 = icmp slt <8 x i32> %1841, %27
  %1843 = select <8 x i1> %1842, <8 x i32> %1841, <8 x i32> %27
  %1844 = bitcast <4 x i64>* %1832 to <8 x i32>*
  store <8 x i32> %1839, <8 x i32>* %1844, align 32
  %1845 = bitcast <4 x i64>* %1833 to <8 x i32>*
  store <8 x i32> %1843, <8 x i32>* %1845, align 32
  %1846 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 35
  %1847 = bitcast <4 x i64>* %1846 to <8 x i32>*
  %1848 = load <8 x i32>, <8 x i32>* %1847, align 32
  %1849 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 36
  %1850 = bitcast <4 x i64>* %1849 to <8 x i32>*
  %1851 = load <8 x i32>, <8 x i32>* %1850, align 32
  %1852 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 35
  %1853 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 36
  %1854 = add <8 x i32> %1851, %1848
  %1855 = sub <8 x i32> %1848, %1851
  %1856 = icmp sgt <8 x i32> %1854, %24
  %1857 = select <8 x i1> %1856, <8 x i32> %1854, <8 x i32> %24
  %1858 = icmp slt <8 x i32> %1857, %27
  %1859 = select <8 x i1> %1858, <8 x i32> %1857, <8 x i32> %27
  %1860 = icmp sgt <8 x i32> %1855, %24
  %1861 = select <8 x i1> %1860, <8 x i32> %1855, <8 x i32> %24
  %1862 = icmp slt <8 x i32> %1861, %27
  %1863 = select <8 x i1> %1862, <8 x i32> %1861, <8 x i32> %27
  %1864 = bitcast <4 x i64>* %1852 to <8 x i32>*
  store <8 x i32> %1859, <8 x i32>* %1864, align 32
  %1865 = bitcast <4 x i64>* %1853 to <8 x i32>*
  store <8 x i32> %1863, <8 x i32>* %1865, align 32
  %1866 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 44
  %1867 = bitcast <4 x i64>* %1866 to <8 x i32>*
  %1868 = load <8 x i32>, <8 x i32>* %1867, align 32
  %1869 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 43
  %1870 = bitcast <4 x i64>* %1869 to <8 x i32>*
  %1871 = load <8 x i32>, <8 x i32>* %1870, align 32
  %1872 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 44
  %1873 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 43
  %1874 = add <8 x i32> %1871, %1868
  %1875 = sub <8 x i32> %1868, %1871
  %1876 = icmp sgt <8 x i32> %1874, %24
  %1877 = select <8 x i1> %1876, <8 x i32> %1874, <8 x i32> %24
  %1878 = icmp slt <8 x i32> %1877, %27
  %1879 = select <8 x i1> %1878, <8 x i32> %1877, <8 x i32> %27
  %1880 = icmp sgt <8 x i32> %1875, %24
  %1881 = select <8 x i1> %1880, <8 x i32> %1875, <8 x i32> %24
  %1882 = icmp slt <8 x i32> %1881, %27
  %1883 = select <8 x i1> %1882, <8 x i32> %1881, <8 x i32> %27
  %1884 = bitcast <4 x i64>* %1872 to <8 x i32>*
  store <8 x i32> %1879, <8 x i32>* %1884, align 32
  %1885 = bitcast <4 x i64>* %1873 to <8 x i32>*
  store <8 x i32> %1883, <8 x i32>* %1885, align 32
  %1886 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 48
  %1887 = bitcast <4 x i64>* %1886 to <8 x i32>*
  %1888 = load <8 x i32>, <8 x i32>* %1887, align 32
  %1889 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 55
  %1890 = bitcast <4 x i64>* %1889 to <8 x i32>*
  %1891 = load <8 x i32>, <8 x i32>* %1890, align 32
  %1892 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 48
  %1893 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 55
  %1894 = add <8 x i32> %1891, %1888
  %1895 = sub <8 x i32> %1888, %1891
  %1896 = icmp sgt <8 x i32> %1894, %24
  %1897 = select <8 x i1> %1896, <8 x i32> %1894, <8 x i32> %24
  %1898 = icmp slt <8 x i32> %1897, %27
  %1899 = select <8 x i1> %1898, <8 x i32> %1897, <8 x i32> %27
  %1900 = icmp sgt <8 x i32> %1895, %24
  %1901 = select <8 x i1> %1900, <8 x i32> %1895, <8 x i32> %24
  %1902 = icmp slt <8 x i32> %1901, %27
  %1903 = select <8 x i1> %1902, <8 x i32> %1901, <8 x i32> %27
  %1904 = bitcast <4 x i64>* %1892 to <8 x i32>*
  store <8 x i32> %1899, <8 x i32>* %1904, align 32
  %1905 = bitcast <4 x i64>* %1893 to <8 x i32>*
  store <8 x i32> %1903, <8 x i32>* %1905, align 32
  %1906 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 63
  %1907 = bitcast <4 x i64>* %1906 to <8 x i32>*
  %1908 = load <8 x i32>, <8 x i32>* %1907, align 32
  %1909 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 56
  %1910 = bitcast <4 x i64>* %1909 to <8 x i32>*
  %1911 = load <8 x i32>, <8 x i32>* %1910, align 32
  %1912 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 63
  %1913 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 56
  %1914 = add <8 x i32> %1911, %1908
  %1915 = sub <8 x i32> %1908, %1911
  %1916 = icmp sgt <8 x i32> %1914, %24
  %1917 = select <8 x i1> %1916, <8 x i32> %1914, <8 x i32> %24
  %1918 = icmp slt <8 x i32> %1917, %27
  %1919 = select <8 x i1> %1918, <8 x i32> %1917, <8 x i32> %27
  %1920 = icmp sgt <8 x i32> %1915, %24
  %1921 = select <8 x i1> %1920, <8 x i32> %1915, <8 x i32> %24
  %1922 = icmp slt <8 x i32> %1921, %27
  %1923 = select <8 x i1> %1922, <8 x i32> %1921, <8 x i32> %27
  %1924 = bitcast <4 x i64>* %1912 to <8 x i32>*
  store <8 x i32> %1919, <8 x i32>* %1924, align 32
  %1925 = bitcast <4 x i64>* %1913 to <8 x i32>*
  store <8 x i32> %1923, <8 x i32>* %1925, align 32
  %1926 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 49
  %1927 = bitcast <4 x i64>* %1926 to <8 x i32>*
  %1928 = load <8 x i32>, <8 x i32>* %1927, align 32
  %1929 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 54
  %1930 = bitcast <4 x i64>* %1929 to <8 x i32>*
  %1931 = load <8 x i32>, <8 x i32>* %1930, align 32
  %1932 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 49
  %1933 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 54
  %1934 = add <8 x i32> %1931, %1928
  %1935 = sub <8 x i32> %1928, %1931
  %1936 = icmp sgt <8 x i32> %1934, %24
  %1937 = select <8 x i1> %1936, <8 x i32> %1934, <8 x i32> %24
  %1938 = icmp slt <8 x i32> %1937, %27
  %1939 = select <8 x i1> %1938, <8 x i32> %1937, <8 x i32> %27
  %1940 = icmp sgt <8 x i32> %1935, %24
  %1941 = select <8 x i1> %1940, <8 x i32> %1935, <8 x i32> %24
  %1942 = icmp slt <8 x i32> %1941, %27
  %1943 = select <8 x i1> %1942, <8 x i32> %1941, <8 x i32> %27
  %1944 = bitcast <4 x i64>* %1932 to <8 x i32>*
  store <8 x i32> %1939, <8 x i32>* %1944, align 32
  %1945 = bitcast <4 x i64>* %1933 to <8 x i32>*
  store <8 x i32> %1943, <8 x i32>* %1945, align 32
  %1946 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 62
  %1947 = bitcast <4 x i64>* %1946 to <8 x i32>*
  %1948 = load <8 x i32>, <8 x i32>* %1947, align 32
  %1949 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 57
  %1950 = bitcast <4 x i64>* %1949 to <8 x i32>*
  %1951 = load <8 x i32>, <8 x i32>* %1950, align 32
  %1952 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 62
  %1953 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 57
  %1954 = add <8 x i32> %1951, %1948
  %1955 = sub <8 x i32> %1948, %1951
  %1956 = icmp sgt <8 x i32> %1954, %24
  %1957 = select <8 x i1> %1956, <8 x i32> %1954, <8 x i32> %24
  %1958 = icmp slt <8 x i32> %1957, %27
  %1959 = select <8 x i1> %1958, <8 x i32> %1957, <8 x i32> %27
  %1960 = icmp sgt <8 x i32> %1955, %24
  %1961 = select <8 x i1> %1960, <8 x i32> %1955, <8 x i32> %24
  %1962 = icmp slt <8 x i32> %1961, %27
  %1963 = select <8 x i1> %1962, <8 x i32> %1961, <8 x i32> %27
  %1964 = bitcast <4 x i64>* %1952 to <8 x i32>*
  store <8 x i32> %1959, <8 x i32>* %1964, align 32
  %1965 = bitcast <4 x i64>* %1953 to <8 x i32>*
  store <8 x i32> %1963, <8 x i32>* %1965, align 32
  %1966 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 50
  %1967 = bitcast <4 x i64>* %1966 to <8 x i32>*
  %1968 = load <8 x i32>, <8 x i32>* %1967, align 32
  %1969 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 53
  %1970 = bitcast <4 x i64>* %1969 to <8 x i32>*
  %1971 = load <8 x i32>, <8 x i32>* %1970, align 32
  %1972 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 50
  %1973 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 53
  %1974 = add <8 x i32> %1971, %1968
  %1975 = sub <8 x i32> %1968, %1971
  %1976 = icmp sgt <8 x i32> %1974, %24
  %1977 = select <8 x i1> %1976, <8 x i32> %1974, <8 x i32> %24
  %1978 = icmp slt <8 x i32> %1977, %27
  %1979 = select <8 x i1> %1978, <8 x i32> %1977, <8 x i32> %27
  %1980 = icmp sgt <8 x i32> %1975, %24
  %1981 = select <8 x i1> %1980, <8 x i32> %1975, <8 x i32> %24
  %1982 = icmp slt <8 x i32> %1981, %27
  %1983 = select <8 x i1> %1982, <8 x i32> %1981, <8 x i32> %27
  %1984 = bitcast <4 x i64>* %1972 to <8 x i32>*
  store <8 x i32> %1979, <8 x i32>* %1984, align 32
  %1985 = bitcast <4 x i64>* %1973 to <8 x i32>*
  store <8 x i32> %1983, <8 x i32>* %1985, align 32
  %1986 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 61
  %1987 = bitcast <4 x i64>* %1986 to <8 x i32>*
  %1988 = load <8 x i32>, <8 x i32>* %1987, align 32
  %1989 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 58
  %1990 = bitcast <4 x i64>* %1989 to <8 x i32>*
  %1991 = load <8 x i32>, <8 x i32>* %1990, align 32
  %1992 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 61
  %1993 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 58
  %1994 = add <8 x i32> %1991, %1988
  %1995 = sub <8 x i32> %1988, %1991
  %1996 = icmp sgt <8 x i32> %1994, %24
  %1997 = select <8 x i1> %1996, <8 x i32> %1994, <8 x i32> %24
  %1998 = icmp slt <8 x i32> %1997, %27
  %1999 = select <8 x i1> %1998, <8 x i32> %1997, <8 x i32> %27
  %2000 = icmp sgt <8 x i32> %1995, %24
  %2001 = select <8 x i1> %2000, <8 x i32> %1995, <8 x i32> %24
  %2002 = icmp slt <8 x i32> %2001, %27
  %2003 = select <8 x i1> %2002, <8 x i32> %2001, <8 x i32> %27
  %2004 = bitcast <4 x i64>* %1992 to <8 x i32>*
  store <8 x i32> %1999, <8 x i32>* %2004, align 32
  %2005 = bitcast <4 x i64>* %1993 to <8 x i32>*
  store <8 x i32> %2003, <8 x i32>* %2005, align 32
  %2006 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 51
  %2007 = bitcast <4 x i64>* %2006 to <8 x i32>*
  %2008 = load <8 x i32>, <8 x i32>* %2007, align 32
  %2009 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 52
  %2010 = bitcast <4 x i64>* %2009 to <8 x i32>*
  %2011 = load <8 x i32>, <8 x i32>* %2010, align 32
  %2012 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 51
  %2013 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 52
  %2014 = add <8 x i32> %2011, %2008
  %2015 = sub <8 x i32> %2008, %2011
  %2016 = icmp sgt <8 x i32> %2014, %24
  %2017 = select <8 x i1> %2016, <8 x i32> %2014, <8 x i32> %24
  %2018 = icmp slt <8 x i32> %2017, %27
  %2019 = select <8 x i1> %2018, <8 x i32> %2017, <8 x i32> %27
  %2020 = icmp sgt <8 x i32> %2015, %24
  %2021 = select <8 x i1> %2020, <8 x i32> %2015, <8 x i32> %24
  %2022 = icmp slt <8 x i32> %2021, %27
  %2023 = select <8 x i1> %2022, <8 x i32> %2021, <8 x i32> %27
  %2024 = bitcast <4 x i64>* %2012 to <8 x i32>*
  store <8 x i32> %2019, <8 x i32>* %2024, align 32
  %2025 = bitcast <4 x i64>* %2013 to <8 x i32>*
  store <8 x i32> %2023, <8 x i32>* %2025, align 32
  %2026 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 60
  %2027 = bitcast <4 x i64>* %2026 to <8 x i32>*
  %2028 = load <8 x i32>, <8 x i32>* %2027, align 32
  %2029 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 59
  %2030 = bitcast <4 x i64>* %2029 to <8 x i32>*
  %2031 = load <8 x i32>, <8 x i32>* %2030, align 32
  %2032 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 60
  %2033 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 59
  %2034 = add <8 x i32> %2031, %2028
  %2035 = sub <8 x i32> %2028, %2031
  %2036 = icmp sgt <8 x i32> %2034, %24
  %2037 = select <8 x i1> %2036, <8 x i32> %2034, <8 x i32> %24
  %2038 = icmp slt <8 x i32> %2037, %27
  %2039 = select <8 x i1> %2038, <8 x i32> %2037, <8 x i32> %27
  %2040 = icmp sgt <8 x i32> %2035, %24
  %2041 = select <8 x i1> %2040, <8 x i32> %2035, <8 x i32> %24
  %2042 = icmp slt <8 x i32> %2041, %27
  %2043 = select <8 x i1> %2042, <8 x i32> %2041, <8 x i32> %27
  %2044 = bitcast <4 x i64>* %2032 to <8 x i32>*
  store <8 x i32> %2039, <8 x i32>* %2044, align 32
  %2045 = bitcast <4 x i64>* %2033 to <8 x i32>*
  store <8 x i32> %2043, <8 x i32>* %2045, align 32
  %2046 = bitcast [64 x <4 x i64>]* %7 to <8 x i32>*
  %2047 = load <8 x i32>, <8 x i32>* %2046, align 32
  %2048 = load <8 x i32>, <8 x i32>* %1089, align 32
  %2049 = add <8 x i32> %2048, %2047
  %2050 = sub <8 x i32> %2047, %2048
  %2051 = icmp sgt <8 x i32> %2049, %24
  %2052 = select <8 x i1> %2051, <8 x i32> %2049, <8 x i32> %24
  %2053 = icmp slt <8 x i32> %2052, %27
  %2054 = select <8 x i1> %2053, <8 x i32> %2052, <8 x i32> %27
  %2055 = icmp sgt <8 x i32> %2050, %24
  %2056 = select <8 x i1> %2055, <8 x i32> %2050, <8 x i32> %24
  %2057 = icmp slt <8 x i32> %2056, %27
  %2058 = select <8 x i1> %2057, <8 x i32> %2056, <8 x i32> %27
  %2059 = bitcast [64 x <4 x i64>]* %8 to <8 x i32>*
  store <8 x i32> %2054, <8 x i32>* %2059, align 32
  store <8 x i32> %2058, <8 x i32>* %1341, align 32
  %2060 = load <8 x i32>, <8 x i32>* %1609, align 32
  %2061 = load <8 x i32>, <8 x i32>* %1084, align 32
  %2062 = add <8 x i32> %2061, %2060
  %2063 = sub <8 x i32> %2060, %2061
  %2064 = icmp sgt <8 x i32> %2062, %24
  %2065 = select <8 x i1> %2064, <8 x i32> %2062, <8 x i32> %24
  %2066 = icmp slt <8 x i32> %2065, %27
  %2067 = select <8 x i1> %2066, <8 x i32> %2065, <8 x i32> %27
  %2068 = icmp sgt <8 x i32> %2063, %24
  %2069 = select <8 x i1> %2068, <8 x i32> %2063, <8 x i32> %24
  %2070 = icmp slt <8 x i32> %2069, %27
  %2071 = select <8 x i1> %2070, <8 x i32> %2069, <8 x i32> %27
  store <8 x i32> %2067, <8 x i32>* %1301, align 32
  store <8 x i32> %2071, <8 x i32>* %1073, align 32
  %2072 = load <8 x i32>, <8 x i32>* %1302, align 32
  %2073 = load <8 x i32>, <8 x i32>* %1079, align 32
  %2074 = add <8 x i32> %2073, %2072
  %2075 = sub <8 x i32> %2072, %2073
  %2076 = icmp sgt <8 x i32> %2074, %24
  %2077 = select <8 x i1> %2076, <8 x i32> %2074, <8 x i32> %24
  %2078 = icmp slt <8 x i32> %2077, %27
  %2079 = select <8 x i1> %2078, <8 x i32> %2077, <8 x i32> %27
  %2080 = icmp sgt <8 x i32> %2075, %24
  %2081 = select <8 x i1> %2080, <8 x i32> %2075, <8 x i32> %24
  %2082 = icmp slt <8 x i32> %2081, %27
  %2083 = select <8 x i1> %2082, <8 x i32> %2081, <8 x i32> %27
  store <8 x i32> %2079, <8 x i32>* %1308, align 32
  store <8 x i32> %2083, <8 x i32>* %1327, align 32
  %2084 = load <8 x i32>, <8 x i32>* %1595, align 32
  %2085 = load <8 x i32>, <8 x i32>* %1072, align 32
  %2086 = add <8 x i32> %2085, %2084
  %2087 = sub <8 x i32> %2084, %2085
  %2088 = icmp sgt <8 x i32> %2086, %24
  %2089 = select <8 x i1> %2088, <8 x i32> %2086, <8 x i32> %24
  %2090 = icmp slt <8 x i32> %2089, %27
  %2091 = select <8 x i1> %2090, <8 x i32> %2089, <8 x i32> %27
  %2092 = icmp sgt <8 x i32> %2087, %24
  %2093 = select <8 x i1> %2092, <8 x i32> %2087, <8 x i32> %24
  %2094 = icmp slt <8 x i32> %2093, %27
  %2095 = select <8 x i1> %2094, <8 x i32> %2093, <8 x i32> %27
  store <8 x i32> %2091, <8 x i32>* %1313, align 32
  store <8 x i32> %2095, <8 x i32>* %1066, align 32
  %2096 = load <4 x i64>, <4 x i64>* %383, align 32
  store <4 x i64> %2096, <4 x i64>* %775, align 32
  %2097 = load <4 x i64>, <4 x i64>* %1092, align 32
  store <4 x i64> %2097, <4 x i64>* %782, align 32
  %2098 = load <4 x i64>, <4 x i64>* %392, align 32
  store <4 x i64> %2098, <4 x i64>* %811, align 32
  %2099 = load <4 x i64>, <4 x i64>* %1134, align 32
  store <4 x i64> %2099, <4 x i64>* %816, align 32
  %2100 = load <8 x i32>, <8 x i32>* %784, align 32
  %2101 = mul <8 x i32> %2100, %762
  %2102 = load <8 x i32>, <8 x i32>* %1131, align 32
  %2103 = mul <8 x i32> %2102, %747
  %2104 = add <8 x i32> %2103, %14
  %2105 = add <8 x i32> %2104, %2101
  %2106 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2105, i32 %2) #8
  store <8 x i32> %2106, <8 x i32>* %790, align 32
  %2107 = load <8 x i32>, <8 x i32>* %1117, align 32
  %2108 = mul <8 x i32> %2107, %762
  %2109 = load <8 x i32>, <8 x i32>* %791, align 32
  %2110 = mul <8 x i32> %2109, %747
  %2111 = add <8 x i32> %2110, %14
  %2112 = add <8 x i32> %2111, %2108
  %2113 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2112, i32 %2) #8
  store <8 x i32> %2113, <8 x i32>* %797, align 32
  %2114 = mul <8 x i32> %2107, %747
  %2115 = add <8 x i32> %2111, %2114
  %2116 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2115, i32 %2) #8
  store <8 x i32> %2116, <8 x i32>* %802, align 32
  %2117 = mul <8 x i32> %2100, %747
  %2118 = add <8 x i32> %2104, %2117
  %2119 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2118, i32 %2) #8
  store <8 x i32> %2119, <8 x i32>* %807, align 32
  %2120 = load <8 x i32>, <8 x i32>* %602, align 32
  %2121 = load <8 x i32>, <8 x i32>* %651, align 32
  %2122 = add <8 x i32> %2121, %2120
  %2123 = sub <8 x i32> %2120, %2121
  %2124 = icmp sgt <8 x i32> %2122, %24
  %2125 = select <8 x i1> %2124, <8 x i32> %2122, <8 x i32> %24
  %2126 = icmp slt <8 x i32> %2125, %27
  %2127 = select <8 x i1> %2126, <8 x i32> %2125, <8 x i32> %27
  %2128 = icmp sgt <8 x i32> %2123, %24
  %2129 = select <8 x i1> %2128, <8 x i32> %2123, <8 x i32> %24
  %2130 = icmp slt <8 x i32> %2129, %27
  %2131 = select <8 x i1> %2130, <8 x i32> %2129, <8 x i32> %27
  store <8 x i32> %2127, <8 x i32>* %596, align 32
  store <8 x i32> %2131, <8 x i32>* %873, align 32
  %2132 = load <8 x i32>, <8 x i32>* %691, align 32
  %2133 = load <8 x i32>, <8 x i32>* %656, align 32
  %2134 = add <8 x i32> %2133, %2132
  %2135 = sub <8 x i32> %2132, %2133
  %2136 = icmp sgt <8 x i32> %2134, %24
  %2137 = select <8 x i1> %2136, <8 x i32> %2134, <8 x i32> %24
  %2138 = icmp slt <8 x i32> %2137, %27
  %2139 = select <8 x i1> %2138, <8 x i32> %2137, <8 x i32> %27
  %2140 = icmp sgt <8 x i32> %2135, %24
  %2141 = select <8 x i1> %2140, <8 x i32> %2135, <8 x i32> %24
  %2142 = icmp slt <8 x i32> %2141, %27
  %2143 = select <8 x i1> %2142, <8 x i32> %2141, <8 x i32> %27
  store <8 x i32> %2139, <8 x i32>* %929, align 32
  store <8 x i32> %2143, <8 x i32>* %645, align 32
  %2144 = load <8 x i32>, <8 x i32>* %609, align 32
  %2145 = load <8 x i32>, <8 x i32>* %644, align 32
  %2146 = add <8 x i32> %2145, %2144
  %2147 = sub <8 x i32> %2144, %2145
  %2148 = icmp sgt <8 x i32> %2146, %24
  %2149 = select <8 x i1> %2148, <8 x i32> %2146, <8 x i32> %24
  %2150 = icmp slt <8 x i32> %2149, %27
  %2151 = select <8 x i1> %2150, <8 x i32> %2149, <8 x i32> %27
  %2152 = icmp sgt <8 x i32> %2147, %24
  %2153 = select <8 x i1> %2152, <8 x i32> %2147, <8 x i32> %24
  %2154 = icmp slt <8 x i32> %2153, %27
  %2155 = select <8 x i1> %2154, <8 x i32> %2153, <8 x i32> %27
  store <8 x i32> %2151, <8 x i32>* %831, align 32
  store <8 x i32> %2155, <8 x i32>* %638, align 32
  %2156 = load <8 x i32>, <8 x i32>* %686, align 32
  %2157 = load <8 x i32>, <8 x i32>* %661, align 32
  %2158 = add <8 x i32> %2157, %2156
  %2159 = sub <8 x i32> %2156, %2157
  %2160 = icmp sgt <8 x i32> %2158, %24
  %2161 = select <8 x i1> %2160, <8 x i32> %2158, <8 x i32> %24
  %2162 = icmp slt <8 x i32> %2161, %27
  %2163 = select <8 x i1> %2162, <8 x i32> %2161, <8 x i32> %27
  %2164 = icmp sgt <8 x i32> %2159, %24
  %2165 = select <8 x i1> %2164, <8 x i32> %2159, <8 x i32> %24
  %2166 = icmp slt <8 x i32> %2165, %27
  %2167 = select <8 x i1> %2166, <8 x i32> %2165, <8 x i32> %27
  store <8 x i32> %2163, <8 x i32>* %603, align 32
  store <8 x i32> %2167, <8 x i32>* %887, align 32
  %2168 = load <8 x i32>, <8 x i32>* %616, align 32
  %2169 = load <8 x i32>, <8 x i32>* %637, align 32
  %2170 = add <8 x i32> %2169, %2168
  %2171 = sub <8 x i32> %2168, %2169
  %2172 = icmp sgt <8 x i32> %2170, %24
  %2173 = select <8 x i1> %2172, <8 x i32> %2170, <8 x i32> %24
  %2174 = icmp slt <8 x i32> %2173, %27
  %2175 = select <8 x i1> %2174, <8 x i32> %2173, <8 x i32> %27
  %2176 = icmp sgt <8 x i32> %2171, %24
  %2177 = select <8 x i1> %2176, <8 x i32> %2171, <8 x i32> %24
  %2178 = icmp slt <8 x i32> %2177, %27
  %2179 = select <8 x i1> %2178, <8 x i32> %2177, <8 x i32> %27
  store <8 x i32> %2175, <8 x i32>* %610, align 32
  store <8 x i32> %2179, <8 x i32>* %859, align 32
  %2180 = load <8 x i32>, <8 x i32>* %681, align 32
  %2181 = load <8 x i32>, <8 x i32>* %666, align 32
  %2182 = add <8 x i32> %2181, %2180
  %2183 = sub <8 x i32> %2180, %2181
  %2184 = icmp sgt <8 x i32> %2182, %24
  %2185 = select <8 x i1> %2184, <8 x i32> %2182, <8 x i32> %24
  %2186 = icmp slt <8 x i32> %2185, %27
  %2187 = select <8 x i1> %2186, <8 x i32> %2185, <8 x i32> %27
  %2188 = icmp sgt <8 x i32> %2183, %24
  %2189 = select <8 x i1> %2188, <8 x i32> %2183, <8 x i32> %24
  %2190 = icmp slt <8 x i32> %2189, %27
  %2191 = select <8 x i1> %2190, <8 x i32> %2189, <8 x i32> %27
  store <8 x i32> %2187, <8 x i32>* %915, align 32
  store <8 x i32> %2191, <8 x i32>* %631, align 32
  %2192 = load <8 x i32>, <8 x i32>* %623, align 32
  %2193 = load <8 x i32>, <8 x i32>* %630, align 32
  %2194 = add <8 x i32> %2193, %2192
  %2195 = sub <8 x i32> %2192, %2193
  %2196 = icmp sgt <8 x i32> %2194, %24
  %2197 = select <8 x i1> %2196, <8 x i32> %2194, <8 x i32> %24
  %2198 = icmp slt <8 x i32> %2197, %27
  %2199 = select <8 x i1> %2198, <8 x i32> %2197, <8 x i32> %27
  %2200 = icmp sgt <8 x i32> %2195, %24
  %2201 = select <8 x i1> %2200, <8 x i32> %2195, <8 x i32> %24
  %2202 = icmp slt <8 x i32> %2201, %27
  %2203 = select <8 x i1> %2202, <8 x i32> %2201, <8 x i32> %27
  store <8 x i32> %2199, <8 x i32>* %845, align 32
  store <8 x i32> %2203, <8 x i32>* %624, align 32
  %2204 = load <8 x i32>, <8 x i32>* %676, align 32
  %2205 = load <8 x i32>, <8 x i32>* %671, align 32
  %2206 = add <8 x i32> %2205, %2204
  %2207 = sub <8 x i32> %2204, %2205
  %2208 = icmp sgt <8 x i32> %2206, %24
  %2209 = select <8 x i1> %2208, <8 x i32> %2206, <8 x i32> %24
  %2210 = icmp slt <8 x i32> %2209, %27
  %2211 = select <8 x i1> %2210, <8 x i32> %2209, <8 x i32> %27
  %2212 = icmp sgt <8 x i32> %2207, %24
  %2213 = select <8 x i1> %2212, <8 x i32> %2207, <8 x i32> %24
  %2214 = icmp slt <8 x i32> %2213, %27
  %2215 = select <8 x i1> %2214, <8 x i32> %2213, <8 x i32> %27
  store <8 x i32> %2211, <8 x i32>* %617, align 32
  store <8 x i32> %2215, <8 x i32>* %901, align 32
  %2216 = load <4 x i64>, <4 x i64>* %311, align 32
  store <4 x i64> %2216, <4 x i64>* %409, align 32
  %2217 = load <4 x i64>, <4 x i64>* %329, align 32
  store <4 x i64> %2217, <4 x i64>* %493, align 32
  %2218 = load <4 x i64>, <4 x i64>* %335, align 32
  store <4 x i64> %2218, <4 x i64>* %519, align 32
  %2219 = load <4 x i64>, <4 x i64>* %353, align 32
  store <4 x i64> %2219, <4 x i64>* %579, align 32
  %2220 = load <4 x i64>, <4 x i64>* %954, align 32
  store <4 x i64> %2220, <4 x i64>* %416, align 32
  %2221 = load <4 x i64>, <4 x i64>* %1008, align 32
  store <4 x i64> %2221, <4 x i64>* %500, align 32
  %2222 = load <4 x i64>, <4 x i64>* %1017, align 32
  store <4 x i64> %2222, <4 x i64>* %524, align 32
  %2223 = load <4 x i64>, <4 x i64>* %963, align 32
  store <4 x i64> %2223, <4 x i64>* %584, align 32
  %2224 = load <4 x i64>, <4 x i64>* %314, align 32
  store <4 x i64> %2224, <4 x i64>* %423, align 32
  %2225 = load <4 x i64>, <4 x i64>* %332, align 32
  store <4 x i64> %2225, <4 x i64>* %507, align 32
  %2226 = load <4 x i64>, <4 x i64>* %338, align 32
  store <4 x i64> %2226, <4 x i64>* %529, align 32
  %2227 = load <4 x i64>, <4 x i64>* %356, align 32
  store <4 x i64> %2227, <4 x i64>* %589, align 32
  %2228 = load <4 x i64>, <4 x i64>* %931, align 32
  store <4 x i64> %2228, <4 x i64>* %430, align 32
  %2229 = load <4 x i64>, <4 x i64>* %940, align 32
  store <4 x i64> %2229, <4 x i64>* %514, align 32
  %2230 = load <4 x i64>, <4 x i64>* %943, align 32
  store <4 x i64> %2230, <4 x i64>* %534, align 32
  %2231 = load <4 x i64>, <4 x i64>* %952, align 32
  store <4 x i64> %2231, <4 x i64>* %594, align 32
  %2232 = load <8 x i32>, <8 x i32>* %432, align 32
  %2233 = mul <8 x i32> %2232, %758
  %2234 = load <8 x i32>, <8 x i32>* %1499, align 32
  %2235 = mul <8 x i32> %2234, %751
  %2236 = add <8 x i32> %2233, %14
  %2237 = add <8 x i32> %2236, %2235
  %2238 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2237, i32 %2) #8
  store <8 x i32> %2238, <8 x i32>* %438, align 32
  %2239 = load <8 x i32>, <8 x i32>* %973, align 32
  %2240 = mul <8 x i32> %2239, %758
  %2241 = load <8 x i32>, <8 x i32>* %439, align 32
  %2242 = mul <8 x i32> %2241, %751
  %2243 = add <8 x i32> %2240, %14
  %2244 = add <8 x i32> %2243, %2242
  %2245 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2244, i32 %2) #8
  store <8 x i32> %2245, <8 x i32>* %445, align 32
  %2246 = load <8 x i32>, <8 x i32>* %446, align 32
  %2247 = mul <8 x i32> %2246, %758
  %2248 = load <8 x i32>, <8 x i32>* %984, align 32
  %2249 = mul <8 x i32> %2248, %751
  %2250 = add <8 x i32> %2247, %14
  %2251 = add <8 x i32> %2250, %2249
  %2252 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2251, i32 %2) #8
  store <8 x i32> %2252, <8 x i32>* %452, align 32
  %2253 = bitcast <4 x i64>* %934 to <8 x i32>*
  %2254 = load <8 x i32>, <8 x i32>* %2253, align 32
  %2255 = mul <8 x i32> %2254, %758
  %2256 = load <8 x i32>, <8 x i32>* %453, align 32
  %2257 = mul <8 x i32> %2256, %751
  %2258 = add <8 x i32> %2255, %14
  %2259 = add <8 x i32> %2258, %2257
  %2260 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2259, i32 %2) #8
  store <8 x i32> %2260, <8 x i32>* %459, align 32
  %2261 = load <8 x i32>, <8 x i32>* %460, align 32
  %2262 = mul <8 x i32> %2261, %766
  %2263 = bitcast <4 x i64>* %946 to <8 x i32>*
  %2264 = load <8 x i32>, <8 x i32>* %2263, align 32
  %2265 = mul <8 x i32> %2264, %758
  %2266 = add <8 x i32> %2262, %14
  %2267 = add <8 x i32> %2266, %2265
  %2268 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2267, i32 %2) #8
  store <8 x i32> %2268, <8 x i32>* %466, align 32
  %2269 = load <8 x i32>, <8 x i32>* %991, align 32
  %2270 = mul <8 x i32> %2269, %766
  %2271 = load <8 x i32>, <8 x i32>* %467, align 32
  %2272 = mul <8 x i32> %2271, %758
  %2273 = add <8 x i32> %2270, %14
  %2274 = add <8 x i32> %2273, %2272
  %2275 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2274, i32 %2) #8
  store <8 x i32> %2275, <8 x i32>* %473, align 32
  %2276 = load <8 x i32>, <8 x i32>* %474, align 32
  %2277 = mul <8 x i32> %2276, %766
  %2278 = load <8 x i32>, <8 x i32>* %1002, align 32
  %2279 = mul <8 x i32> %2278, %758
  %2280 = add <8 x i32> %2277, %14
  %2281 = add <8 x i32> %2280, %2279
  %2282 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2281, i32 %2) #8
  store <8 x i32> %2282, <8 x i32>* %480, align 32
  %2283 = load <8 x i32>, <8 x i32>* %1519, align 32
  %2284 = mul <8 x i32> %2283, %766
  %2285 = load <8 x i32>, <8 x i32>* %481, align 32
  %2286 = mul <8 x i32> %2285, %758
  %2287 = add <8 x i32> %2284, %14
  %2288 = add <8 x i32> %2287, %2286
  %2289 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2288, i32 %2) #8
  store <8 x i32> %2289, <8 x i32>* %487, align 32
  %2290 = mul <8 x i32> %2283, %758
  %2291 = mul <8 x i32> %2285, %751
  %2292 = add <8 x i32> %2290, %14
  %2293 = add <8 x i32> %2292, %2291
  %2294 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2293, i32 %2) #8
  store <8 x i32> %2294, <8 x i32>* %540, align 32
  %2295 = mul <8 x i32> %2276, %758
  %2296 = mul <8 x i32> %2278, %751
  %2297 = add <8 x i32> %2295, %14
  %2298 = add <8 x i32> %2297, %2296
  %2299 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2298, i32 %2) #8
  store <8 x i32> %2299, <8 x i32>* %545, align 32
  %2300 = mul <8 x i32> %2269, %758
  %2301 = mul <8 x i32> %2271, %751
  %2302 = add <8 x i32> %2300, %14
  %2303 = add <8 x i32> %2302, %2301
  %2304 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2303, i32 %2) #8
  store <8 x i32> %2304, <8 x i32>* %550, align 32
  %2305 = mul <8 x i32> %2261, %758
  %2306 = mul <8 x i32> %2264, %751
  %2307 = add <8 x i32> %2305, %14
  %2308 = add <8 x i32> %2307, %2306
  %2309 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2308, i32 %2) #8
  store <8 x i32> %2309, <8 x i32>* %555, align 32
  %2310 = mul <8 x i32> %2254, %751
  %2311 = mul <8 x i32> %2256, %743
  %2312 = add <8 x i32> %2310, %14
  %2313 = add <8 x i32> %2312, %2311
  %2314 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2313, i32 %2) #8
  store <8 x i32> %2314, <8 x i32>* %560, align 32
  %2315 = mul <8 x i32> %2246, %751
  %2316 = mul <8 x i32> %2248, %743
  %2317 = add <8 x i32> %2315, %14
  %2318 = add <8 x i32> %2317, %2316
  %2319 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2318, i32 %2) #8
  store <8 x i32> %2319, <8 x i32>* %565, align 32
  %2320 = mul <8 x i32> %2239, %751
  %2321 = mul <8 x i32> %2241, %743
  %2322 = add <8 x i32> %2320, %14
  %2323 = add <8 x i32> %2322, %2321
  %2324 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2323, i32 %2) #8
  store <8 x i32> %2324, <8 x i32>* %570, align 32
  %2325 = mul <8 x i32> %2232, %751
  %2326 = mul <8 x i32> %2234, %743
  %2327 = add <8 x i32> %2325, %14
  %2328 = add <8 x i32> %2327, %2326
  %2329 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2328, i32 %2) #8
  store <8 x i32> %2329, <8 x i32>* %575, align 32
  %2330 = load <8 x i32>, <8 x i32>* %2059, align 32
  %2331 = load <8 x i32>, <8 x i32>* %817, align 32
  %2332 = add <8 x i32> %2331, %2330
  %2333 = sub <8 x i32> %2330, %2331
  %2334 = icmp sgt <8 x i32> %2332, %24
  %2335 = select <8 x i1> %2334, <8 x i32> %2332, <8 x i32> %24
  %2336 = icmp slt <8 x i32> %2335, %27
  %2337 = select <8 x i1> %2336, <8 x i32> %2335, <8 x i32> %27
  %2338 = icmp sgt <8 x i32> %2333, %24
  %2339 = select <8 x i1> %2338, <8 x i32> %2333, <8 x i32> %24
  %2340 = icmp slt <8 x i32> %2339, %27
  %2341 = select <8 x i1> %2340, <8 x i32> %2339, <8 x i32> %27
  store <8 x i32> %2337, <8 x i32>* %2046, align 32
  store <8 x i32> %2341, <8 x i32>* %1145, align 32
  %2342 = load <8 x i32>, <8 x i32>* %1301, align 32
  %2343 = load <8 x i32>, <8 x i32>* %812, align 32
  %2344 = add <8 x i32> %2343, %2342
  %2345 = sub <8 x i32> %2342, %2343
  %2346 = icmp sgt <8 x i32> %2344, %24
  %2347 = select <8 x i1> %2346, <8 x i32> %2344, <8 x i32> %24
  %2348 = icmp slt <8 x i32> %2347, %27
  %2349 = select <8 x i1> %2348, <8 x i32> %2347, <8 x i32> %27
  %2350 = icmp sgt <8 x i32> %2345, %24
  %2351 = select <8 x i1> %2350, <8 x i32> %2345, <8 x i32> %24
  %2352 = icmp slt <8 x i32> %2351, %27
  %2353 = select <8 x i1> %2352, <8 x i32> %2351, <8 x i32> %27
  store <8 x i32> %2349, <8 x i32>* %1609, align 32
  store <8 x i32> %2353, <8 x i32>* %777, align 32
  %2354 = load <8 x i32>, <8 x i32>* %1308, align 32
  %2355 = load <8 x i32>, <8 x i32>* %807, align 32
  %2356 = add <8 x i32> %2355, %2354
  %2357 = sub <8 x i32> %2354, %2355
  %2358 = icmp sgt <8 x i32> %2356, %24
  %2359 = select <8 x i1> %2358, <8 x i32> %2356, <8 x i32> %24
  %2360 = icmp slt <8 x i32> %2359, %27
  %2361 = select <8 x i1> %2360, <8 x i32> %2359, <8 x i32> %27
  %2362 = icmp sgt <8 x i32> %2357, %24
  %2363 = select <8 x i1> %2362, <8 x i32> %2357, <8 x i32> %24
  %2364 = icmp slt <8 x i32> %2363, %27
  %2365 = select <8 x i1> %2364, <8 x i32> %2363, <8 x i32> %27
  store <8 x i32> %2361, <8 x i32>* %1302, align 32
  store <8 x i32> %2365, <8 x i32>* %1131, align 32
  %2366 = load <8 x i32>, <8 x i32>* %1313, align 32
  %2367 = load <8 x i32>, <8 x i32>* %802, align 32
  %2368 = add <8 x i32> %2367, %2366
  %2369 = sub <8 x i32> %2366, %2367
  %2370 = icmp sgt <8 x i32> %2368, %24
  %2371 = select <8 x i1> %2370, <8 x i32> %2368, <8 x i32> %24
  %2372 = icmp slt <8 x i32> %2371, %27
  %2373 = select <8 x i1> %2372, <8 x i32> %2371, <8 x i32> %27
  %2374 = icmp sgt <8 x i32> %2369, %24
  %2375 = select <8 x i1> %2374, <8 x i32> %2369, <8 x i32> %24
  %2376 = icmp slt <8 x i32> %2375, %27
  %2377 = select <8 x i1> %2376, <8 x i32> %2375, <8 x i32> %27
  store <8 x i32> %2373, <8 x i32>* %1595, align 32
  store <8 x i32> %2377, <8 x i32>* %791, align 32
  %2378 = load <8 x i32>, <8 x i32>* %1066, align 32
  %2379 = load <8 x i32>, <8 x i32>* %797, align 32
  %2380 = add <8 x i32> %2379, %2378
  %2381 = sub <8 x i32> %2378, %2379
  %2382 = icmp sgt <8 x i32> %2380, %24
  %2383 = select <8 x i1> %2382, <8 x i32> %2380, <8 x i32> %24
  %2384 = icmp slt <8 x i32> %2383, %27
  %2385 = select <8 x i1> %2384, <8 x i32> %2383, <8 x i32> %27
  %2386 = icmp sgt <8 x i32> %2381, %24
  %2387 = select <8 x i1> %2386, <8 x i32> %2381, <8 x i32> %24
  %2388 = icmp slt <8 x i32> %2387, %27
  %2389 = select <8 x i1> %2388, <8 x i32> %2387, <8 x i32> %27
  store <8 x i32> %2385, <8 x i32>* %1072, align 32
  store <8 x i32> %2389, <8 x i32>* %1117, align 32
  %2390 = load <8 x i32>, <8 x i32>* %1327, align 32
  %2391 = load <8 x i32>, <8 x i32>* %790, align 32
  %2392 = add <8 x i32> %2391, %2390
  %2393 = sub <8 x i32> %2390, %2391
  %2394 = icmp sgt <8 x i32> %2392, %24
  %2395 = select <8 x i1> %2394, <8 x i32> %2392, <8 x i32> %24
  %2396 = icmp slt <8 x i32> %2395, %27
  %2397 = select <8 x i1> %2396, <8 x i32> %2395, <8 x i32> %27
  %2398 = icmp sgt <8 x i32> %2393, %24
  %2399 = select <8 x i1> %2398, <8 x i32> %2393, <8 x i32> %24
  %2400 = icmp slt <8 x i32> %2399, %27
  %2401 = select <8 x i1> %2400, <8 x i32> %2399, <8 x i32> %27
  store <8 x i32> %2397, <8 x i32>* %1079, align 32
  store <8 x i32> %2401, <8 x i32>* %784, align 32
  %2402 = load <8 x i32>, <8 x i32>* %1073, align 32
  %2403 = load <8 x i32>, <8 x i32>* %783, align 32
  %2404 = add <8 x i32> %2403, %2402
  %2405 = sub <8 x i32> %2402, %2403
  %2406 = icmp sgt <8 x i32> %2404, %24
  %2407 = select <8 x i1> %2406, <8 x i32> %2404, <8 x i32> %24
  %2408 = icmp slt <8 x i32> %2407, %27
  %2409 = select <8 x i1> %2408, <8 x i32> %2407, <8 x i32> %27
  %2410 = icmp sgt <8 x i32> %2405, %24
  %2411 = select <8 x i1> %2410, <8 x i32> %2405, <8 x i32> %24
  %2412 = icmp slt <8 x i32> %2411, %27
  %2413 = select <8 x i1> %2412, <8 x i32> %2411, <8 x i32> %27
  store <8 x i32> %2409, <8 x i32>* %1084, align 32
  store <8 x i32> %2413, <8 x i32>* %1103, align 32
  %2414 = load <8 x i32>, <8 x i32>* %1341, align 32
  %2415 = load <8 x i32>, <8 x i32>* %776, align 32
  %2416 = add <8 x i32> %2415, %2414
  %2417 = sub <8 x i32> %2414, %2415
  %2418 = icmp sgt <8 x i32> %2416, %24
  %2419 = select <8 x i1> %2418, <8 x i32> %2416, <8 x i32> %24
  %2420 = icmp slt <8 x i32> %2419, %27
  %2421 = select <8 x i1> %2420, <8 x i32> %2419, <8 x i32> %27
  %2422 = icmp sgt <8 x i32> %2417, %24
  %2423 = select <8 x i1> %2422, <8 x i32> %2417, <8 x i32> %24
  %2424 = icmp slt <8 x i32> %2423, %27
  %2425 = select <8 x i1> %2424, <8 x i32> %2423, <8 x i32> %27
  store <8 x i32> %2421, <8 x i32>* %1089, align 32
  store <8 x i32> %2425, <8 x i32>* %770, align 32
  %2426 = load <4 x i64>, <4 x i64>* %359, align 32
  store <4 x i64> %2426, <4 x i64>* %601, align 32
  %2427 = load <4 x i64>, <4 x i64>* %377, align 32
  store <4 x i64> %2427, <4 x i64>* %675, align 32
  %2428 = load <4 x i64>, <4 x i64>* %820, align 32
  store <4 x i64> %2428, <4 x i64>* %608, align 32
  %2429 = load <4 x i64>, <4 x i64>* %904, align 32
  store <4 x i64> %2429, <4 x i64>* %680, align 32
  %2430 = load <4 x i64>, <4 x i64>* %362, align 32
  store <4 x i64> %2430, <4 x i64>* %615, align 32
  %2431 = load <4 x i64>, <4 x i64>* %380, align 32
  store <4 x i64> %2431, <4 x i64>* %685, align 32
  %2432 = load <4 x i64>, <4 x i64>* %834, align 32
  store <4 x i64> %2432, <4 x i64>* %622, align 32
  %2433 = load <4 x i64>, <4 x i64>* %918, align 32
  store <4 x i64> %2433, <4 x i64>* %690, align 32
  %2434 = load <8 x i32>, <8 x i32>* %624, align 32
  %2435 = mul <8 x i32> %2434, %762
  %2436 = load <8 x i32>, <8 x i32>* %901, align 32
  %2437 = mul <8 x i32> %2436, %747
  %2438 = add <8 x i32> %2437, %14
  %2439 = add <8 x i32> %2438, %2435
  %2440 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2439, i32 %2) #8
  store <8 x i32> %2440, <8 x i32>* %630, align 32
  %2441 = load <8 x i32>, <8 x i32>* %859, align 32
  %2442 = mul <8 x i32> %2441, %762
  %2443 = load <8 x i32>, <8 x i32>* %631, align 32
  %2444 = mul <8 x i32> %2443, %747
  %2445 = add <8 x i32> %2444, %14
  %2446 = add <8 x i32> %2445, %2442
  %2447 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2446, i32 %2) #8
  store <8 x i32> %2447, <8 x i32>* %637, align 32
  %2448 = load <8 x i32>, <8 x i32>* %638, align 32
  %2449 = mul <8 x i32> %2448, %762
  %2450 = load <8 x i32>, <8 x i32>* %887, align 32
  %2451 = mul <8 x i32> %2450, %747
  %2452 = add <8 x i32> %2451, %14
  %2453 = add <8 x i32> %2452, %2449
  %2454 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2453, i32 %2) #8
  store <8 x i32> %2454, <8 x i32>* %644, align 32
  %2455 = load <8 x i32>, <8 x i32>* %873, align 32
  %2456 = mul <8 x i32> %2455, %762
  %2457 = load <8 x i32>, <8 x i32>* %645, align 32
  %2458 = mul <8 x i32> %2457, %747
  %2459 = add <8 x i32> %2458, %14
  %2460 = add <8 x i32> %2459, %2456
  %2461 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2460, i32 %2) #8
  store <8 x i32> %2461, <8 x i32>* %651, align 32
  %2462 = mul <8 x i32> %2455, %747
  %2463 = add <8 x i32> %2459, %2462
  %2464 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2463, i32 %2) #8
  store <8 x i32> %2464, <8 x i32>* %656, align 32
  %2465 = mul <8 x i32> %2448, %747
  %2466 = add <8 x i32> %2452, %2465
  %2467 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2466, i32 %2) #8
  store <8 x i32> %2467, <8 x i32>* %661, align 32
  %2468 = mul <8 x i32> %2441, %747
  %2469 = add <8 x i32> %2445, %2468
  %2470 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2469, i32 %2) #8
  store <8 x i32> %2470, <8 x i32>* %666, align 32
  %2471 = mul <8 x i32> %2434, %747
  %2472 = add <8 x i32> %2438, %2471
  %2473 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2472, i32 %2) #8
  store <8 x i32> %2473, <8 x i32>* %671, align 32
  %2474 = load <8 x i32>, <8 x i32>* %410, align 32
  %2475 = load <8 x i32>, <8 x i32>* %515, align 32
  %2476 = add <8 x i32> %2475, %2474
  %2477 = sub <8 x i32> %2474, %2475
  %2478 = icmp sgt <8 x i32> %2476, %24
  %2479 = select <8 x i1> %2478, <8 x i32> %2476, <8 x i32> %24
  %2480 = icmp slt <8 x i32> %2479, %27
  %2481 = select <8 x i1> %2480, <8 x i32> %2479, <8 x i32> %27
  %2482 = icmp sgt <8 x i32> %2477, %24
  %2483 = select <8 x i1> %2482, <8 x i32> %2477, <8 x i32> %24
  %2484 = icmp slt <8 x i32> %2483, %27
  %2485 = select <8 x i1> %2484, <8 x i32> %2483, <8 x i32> %27
  store <8 x i32> %2481, <8 x i32>* %404, align 32
  %2486 = bitcast <4 x i64>* %940 to <8 x i32>*
  store <8 x i32> %2485, <8 x i32>* %2486, align 32
  %2487 = load <8 x i32>, <8 x i32>* %417, align 32
  %2488 = load <8 x i32>, <8 x i32>* %508, align 32
  %2489 = add <8 x i32> %2488, %2487
  %2490 = sub <8 x i32> %2487, %2488
  %2491 = icmp sgt <8 x i32> %2489, %24
  %2492 = select <8 x i1> %2491, <8 x i32> %2489, <8 x i32> %24
  %2493 = icmp slt <8 x i32> %2492, %27
  %2494 = select <8 x i1> %2493, <8 x i32> %2492, <8 x i32> %27
  %2495 = icmp sgt <8 x i32> %2490, %24
  %2496 = select <8 x i1> %2495, <8 x i32> %2490, <8 x i32> %24
  %2497 = icmp slt <8 x i32> %2496, %27
  %2498 = select <8 x i1> %2497, <8 x i32> %2496, <8 x i32> %27
  store <8 x i32> %2494, <8 x i32>* %955, align 32
  store <8 x i32> %2498, <8 x i32>* %502, align 32
  %2499 = load <8 x i32>, <8 x i32>* %424, align 32
  %2500 = load <8 x i32>, <8 x i32>* %501, align 32
  %2501 = add <8 x i32> %2500, %2499
  %2502 = sub <8 x i32> %2499, %2500
  %2503 = icmp sgt <8 x i32> %2501, %24
  %2504 = select <8 x i1> %2503, <8 x i32> %2501, <8 x i32> %24
  %2505 = icmp slt <8 x i32> %2504, %27
  %2506 = select <8 x i1> %2505, <8 x i32> %2504, <8 x i32> %27
  %2507 = icmp sgt <8 x i32> %2502, %24
  %2508 = select <8 x i1> %2507, <8 x i32> %2502, <8 x i32> %24
  %2509 = icmp slt <8 x i32> %2508, %27
  %2510 = select <8 x i1> %2509, <8 x i32> %2508, <8 x i32> %27
  store <8 x i32> %2506, <8 x i32>* %418, align 32
  store <8 x i32> %2510, <8 x i32>* %1009, align 32
  %2511 = load <8 x i32>, <8 x i32>* %431, align 32
  %2512 = load <8 x i32>, <8 x i32>* %494, align 32
  %2513 = add <8 x i32> %2512, %2511
  %2514 = sub <8 x i32> %2511, %2512
  %2515 = icmp sgt <8 x i32> %2513, %24
  %2516 = select <8 x i1> %2515, <8 x i32> %2513, <8 x i32> %24
  %2517 = icmp slt <8 x i32> %2516, %27
  %2518 = select <8 x i1> %2517, <8 x i32> %2516, <8 x i32> %27
  %2519 = icmp sgt <8 x i32> %2514, %24
  %2520 = select <8 x i1> %2519, <8 x i32> %2514, <8 x i32> %24
  %2521 = icmp slt <8 x i32> %2520, %27
  %2522 = select <8 x i1> %2521, <8 x i32> %2520, <8 x i32> %27
  store <8 x i32> %2518, <8 x i32>* %1489, align 32
  store <8 x i32> %2522, <8 x i32>* %488, align 32
  %2523 = load <8 x i32>, <8 x i32>* %438, align 32
  %2524 = load <8 x i32>, <8 x i32>* %487, align 32
  %2525 = add <8 x i32> %2524, %2523
  %2526 = sub <8 x i32> %2523, %2524
  %2527 = icmp sgt <8 x i32> %2525, %24
  %2528 = select <8 x i1> %2527, <8 x i32> %2525, <8 x i32> %24
  %2529 = icmp slt <8 x i32> %2528, %27
  %2530 = select <8 x i1> %2529, <8 x i32> %2528, <8 x i32> %27
  %2531 = icmp sgt <8 x i32> %2526, %24
  %2532 = select <8 x i1> %2531, <8 x i32> %2526, <8 x i32> %24
  %2533 = icmp slt <8 x i32> %2532, %27
  %2534 = select <8 x i1> %2533, <8 x i32> %2532, <8 x i32> %27
  store <8 x i32> %2530, <8 x i32>* %432, align 32
  store <8 x i32> %2534, <8 x i32>* %1519, align 32
  %2535 = load <8 x i32>, <8 x i32>* %445, align 32
  %2536 = load <8 x i32>, <8 x i32>* %480, align 32
  %2537 = add <8 x i32> %2536, %2535
  %2538 = sub <8 x i32> %2535, %2536
  %2539 = icmp sgt <8 x i32> %2537, %24
  %2540 = select <8 x i1> %2539, <8 x i32> %2537, <8 x i32> %24
  %2541 = icmp slt <8 x i32> %2540, %27
  %2542 = select <8 x i1> %2541, <8 x i32> %2540, <8 x i32> %27
  %2543 = icmp sgt <8 x i32> %2538, %24
  %2544 = select <8 x i1> %2543, <8 x i32> %2538, <8 x i32> %24
  %2545 = icmp slt <8 x i32> %2544, %27
  %2546 = select <8 x i1> %2545, <8 x i32> %2544, <8 x i32> %27
  store <8 x i32> %2542, <8 x i32>* %973, align 32
  store <8 x i32> %2546, <8 x i32>* %474, align 32
  %2547 = load <8 x i32>, <8 x i32>* %452, align 32
  %2548 = load <8 x i32>, <8 x i32>* %473, align 32
  %2549 = add <8 x i32> %2548, %2547
  %2550 = sub <8 x i32> %2547, %2548
  %2551 = icmp sgt <8 x i32> %2549, %24
  %2552 = select <8 x i1> %2551, <8 x i32> %2549, <8 x i32> %24
  %2553 = icmp slt <8 x i32> %2552, %27
  %2554 = select <8 x i1> %2553, <8 x i32> %2552, <8 x i32> %27
  %2555 = icmp sgt <8 x i32> %2550, %24
  %2556 = select <8 x i1> %2555, <8 x i32> %2550, <8 x i32> %24
  %2557 = icmp slt <8 x i32> %2556, %27
  %2558 = select <8 x i1> %2557, <8 x i32> %2556, <8 x i32> %27
  store <8 x i32> %2554, <8 x i32>* %446, align 32
  store <8 x i32> %2558, <8 x i32>* %991, align 32
  %2559 = load <8 x i32>, <8 x i32>* %459, align 32
  %2560 = load <8 x i32>, <8 x i32>* %466, align 32
  %2561 = add <8 x i32> %2560, %2559
  %2562 = sub <8 x i32> %2559, %2560
  %2563 = icmp sgt <8 x i32> %2561, %24
  %2564 = select <8 x i1> %2563, <8 x i32> %2561, <8 x i32> %24
  %2565 = icmp slt <8 x i32> %2564, %27
  %2566 = select <8 x i1> %2565, <8 x i32> %2564, <8 x i32> %27
  %2567 = icmp sgt <8 x i32> %2562, %24
  %2568 = select <8 x i1> %2567, <8 x i32> %2562, <8 x i32> %24
  %2569 = icmp slt <8 x i32> %2568, %27
  %2570 = select <8 x i1> %2569, <8 x i32> %2568, <8 x i32> %27
  store <8 x i32> %2566, <8 x i32>* %2253, align 32
  store <8 x i32> %2570, <8 x i32>* %460, align 32
  %2571 = load <8 x i32>, <8 x i32>* %595, align 32
  %2572 = load <8 x i32>, <8 x i32>* %520, align 32
  %2573 = add <8 x i32> %2572, %2571
  %2574 = sub <8 x i32> %2571, %2572
  %2575 = icmp sgt <8 x i32> %2573, %24
  %2576 = select <8 x i1> %2575, <8 x i32> %2573, <8 x i32> %24
  %2577 = icmp slt <8 x i32> %2576, %27
  %2578 = select <8 x i1> %2577, <8 x i32> %2576, <8 x i32> %27
  %2579 = icmp sgt <8 x i32> %2574, %24
  %2580 = select <8 x i1> %2579, <8 x i32> %2574, <8 x i32> %24
  %2581 = icmp slt <8 x i32> %2580, %27
  %2582 = select <8 x i1> %2581, <8 x i32> %2580, <8 x i32> %27
  %2583 = bitcast <4 x i64>* %952 to <8 x i32>*
  store <8 x i32> %2578, <8 x i32>* %2583, align 32
  store <8 x i32> %2582, <8 x i32>* %509, align 32
  %2584 = load <8 x i32>, <8 x i32>* %590, align 32
  %2585 = load <8 x i32>, <8 x i32>* %525, align 32
  %2586 = add <8 x i32> %2585, %2584
  %2587 = sub <8 x i32> %2584, %2585
  %2588 = icmp sgt <8 x i32> %2586, %24
  %2589 = select <8 x i1> %2588, <8 x i32> %2586, <8 x i32> %24
  %2590 = icmp slt <8 x i32> %2589, %27
  %2591 = select <8 x i1> %2590, <8 x i32> %2589, <8 x i32> %27
  %2592 = icmp sgt <8 x i32> %2587, %24
  %2593 = select <8 x i1> %2592, <8 x i32> %2587, <8 x i32> %24
  %2594 = icmp slt <8 x i32> %2593, %27
  %2595 = select <8 x i1> %2594, <8 x i32> %2593, <8 x i32> %27
  store <8 x i32> %2591, <8 x i32>* %411, align 32
  store <8 x i32> %2595, <8 x i32>* %1020, align 32
  %2596 = load <8 x i32>, <8 x i32>* %585, align 32
  %2597 = load <8 x i32>, <8 x i32>* %530, align 32
  %2598 = add <8 x i32> %2597, %2596
  %2599 = sub <8 x i32> %2596, %2597
  %2600 = icmp sgt <8 x i32> %2598, %24
  %2601 = select <8 x i1> %2600, <8 x i32> %2598, <8 x i32> %24
  %2602 = icmp slt <8 x i32> %2601, %27
  %2603 = select <8 x i1> %2602, <8 x i32> %2601, <8 x i32> %27
  %2604 = icmp sgt <8 x i32> %2599, %24
  %2605 = select <8 x i1> %2604, <8 x i32> %2599, <8 x i32> %24
  %2606 = icmp slt <8 x i32> %2605, %27
  %2607 = select <8 x i1> %2606, <8 x i32> %2605, <8 x i32> %27
  store <8 x i32> %2603, <8 x i32>* %966, align 32
  store <8 x i32> %2607, <8 x i32>* %495, align 32
  %2608 = load <8 x i32>, <8 x i32>* %580, align 32
  %2609 = load <8 x i32>, <8 x i32>* %535, align 32
  %2610 = add <8 x i32> %2609, %2608
  %2611 = sub <8 x i32> %2608, %2609
  %2612 = icmp sgt <8 x i32> %2610, %24
  %2613 = select <8 x i1> %2612, <8 x i32> %2610, <8 x i32> %24
  %2614 = icmp slt <8 x i32> %2613, %27
  %2615 = select <8 x i1> %2614, <8 x i32> %2613, <8 x i32> %27
  %2616 = icmp sgt <8 x i32> %2611, %24
  %2617 = select <8 x i1> %2616, <8 x i32> %2611, <8 x i32> %24
  %2618 = icmp slt <8 x i32> %2617, %27
  %2619 = select <8 x i1> %2618, <8 x i32> %2617, <8 x i32> %27
  store <8 x i32> %2615, <8 x i32>* %425, align 32
  store <8 x i32> %2619, <8 x i32>* %1529, align 32
  %2620 = load <8 x i32>, <8 x i32>* %575, align 32
  %2621 = load <8 x i32>, <8 x i32>* %540, align 32
  %2622 = add <8 x i32> %2621, %2620
  %2623 = sub <8 x i32> %2620, %2621
  %2624 = icmp sgt <8 x i32> %2622, %24
  %2625 = select <8 x i1> %2624, <8 x i32> %2622, <8 x i32> %24
  %2626 = icmp slt <8 x i32> %2625, %27
  %2627 = select <8 x i1> %2626, <8 x i32> %2625, <8 x i32> %27
  %2628 = icmp sgt <8 x i32> %2623, %24
  %2629 = select <8 x i1> %2628, <8 x i32> %2623, <8 x i32> %24
  %2630 = icmp slt <8 x i32> %2629, %27
  %2631 = select <8 x i1> %2630, <8 x i32> %2629, <8 x i32> %27
  store <8 x i32> %2627, <8 x i32>* %1499, align 32
  store <8 x i32> %2631, <8 x i32>* %481, align 32
  %2632 = load <8 x i32>, <8 x i32>* %570, align 32
  %2633 = load <8 x i32>, <8 x i32>* %545, align 32
  %2634 = add <8 x i32> %2633, %2632
  %2635 = sub <8 x i32> %2632, %2633
  %2636 = icmp sgt <8 x i32> %2634, %24
  %2637 = select <8 x i1> %2636, <8 x i32> %2634, <8 x i32> %24
  %2638 = icmp slt <8 x i32> %2637, %27
  %2639 = select <8 x i1> %2638, <8 x i32> %2637, <8 x i32> %27
  %2640 = icmp sgt <8 x i32> %2635, %24
  %2641 = select <8 x i1> %2640, <8 x i32> %2635, <8 x i32> %24
  %2642 = icmp slt <8 x i32> %2641, %27
  %2643 = select <8 x i1> %2642, <8 x i32> %2641, <8 x i32> %27
  store <8 x i32> %2639, <8 x i32>* %439, align 32
  store <8 x i32> %2643, <8 x i32>* %1002, align 32
  %2644 = load <8 x i32>, <8 x i32>* %565, align 32
  %2645 = load <8 x i32>, <8 x i32>* %550, align 32
  %2646 = add <8 x i32> %2645, %2644
  %2647 = sub <8 x i32> %2644, %2645
  %2648 = icmp sgt <8 x i32> %2646, %24
  %2649 = select <8 x i1> %2648, <8 x i32> %2646, <8 x i32> %24
  %2650 = icmp slt <8 x i32> %2649, %27
  %2651 = select <8 x i1> %2650, <8 x i32> %2649, <8 x i32> %27
  %2652 = icmp sgt <8 x i32> %2647, %24
  %2653 = select <8 x i1> %2652, <8 x i32> %2647, <8 x i32> %24
  %2654 = icmp slt <8 x i32> %2653, %27
  %2655 = select <8 x i1> %2654, <8 x i32> %2653, <8 x i32> %27
  store <8 x i32> %2651, <8 x i32>* %984, align 32
  store <8 x i32> %2655, <8 x i32>* %467, align 32
  %2656 = load <8 x i32>, <8 x i32>* %560, align 32
  %2657 = load <8 x i32>, <8 x i32>* %555, align 32
  %2658 = add <8 x i32> %2657, %2656
  %2659 = sub <8 x i32> %2656, %2657
  %2660 = icmp sgt <8 x i32> %2658, %24
  %2661 = select <8 x i1> %2660, <8 x i32> %2658, <8 x i32> %24
  %2662 = icmp slt <8 x i32> %2661, %27
  %2663 = select <8 x i1> %2662, <8 x i32> %2661, <8 x i32> %27
  %2664 = icmp sgt <8 x i32> %2659, %24
  %2665 = select <8 x i1> %2664, <8 x i32> %2659, <8 x i32> %24
  %2666 = icmp slt <8 x i32> %2665, %27
  %2667 = select <8 x i1> %2666, <8 x i32> %2665, <8 x i32> %27
  store <8 x i32> %2663, <8 x i32>* %453, align 32
  store <8 x i32> %2667, <8 x i32>* %2263, align 32
  br label %2668

2668:                                             ; preds = %2668, %1293
  %2669 = phi i64 [ 0, %1293 ], [ %2691, %2668 ]
  %2670 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %2669
  %2671 = bitcast <4 x i64>* %2670 to <8 x i32>*
  %2672 = load <8 x i32>, <8 x i32>* %2671, align 32
  %2673 = sub nuw nsw i64 31, %2669
  %2674 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %7, i64 0, i64 %2673
  %2675 = bitcast <4 x i64>* %2674 to <8 x i32>*
  %2676 = load <8 x i32>, <8 x i32>* %2675, align 32
  %2677 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %2669
  %2678 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %2673
  %2679 = add <8 x i32> %2676, %2672
  %2680 = sub <8 x i32> %2672, %2676
  %2681 = icmp sgt <8 x i32> %2679, %24
  %2682 = select <8 x i1> %2681, <8 x i32> %2679, <8 x i32> %24
  %2683 = icmp slt <8 x i32> %2682, %27
  %2684 = select <8 x i1> %2683, <8 x i32> %2682, <8 x i32> %27
  %2685 = icmp sgt <8 x i32> %2680, %24
  %2686 = select <8 x i1> %2685, <8 x i32> %2680, <8 x i32> %24
  %2687 = icmp slt <8 x i32> %2686, %27
  %2688 = select <8 x i1> %2687, <8 x i32> %2686, <8 x i32> %27
  %2689 = bitcast <4 x i64>* %2677 to <8 x i32>*
  store <8 x i32> %2684, <8 x i32>* %2689, align 32
  %2690 = bitcast <4 x i64>* %2678 to <8 x i32>*
  store <8 x i32> %2688, <8 x i32>* %2690, align 32
  %2691 = add nuw nsw i64 %2669, 1
  %2692 = icmp eq i64 %2691, 16
  br i1 %2692, label %2693, label %2668

2693:                                             ; preds = %2668
  %2694 = bitcast <4 x i64>* %409 to i8*
  %2695 = bitcast <4 x i64>* %311 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 32 %2694, i8* align 32 %2695, i64 256, i1 false)
  %2696 = load <8 x i32>, <8 x i32>* %460, align 32
  %2697 = mul <8 x i32> %2696, %762
  %2698 = mul <8 x i32> %2667, %747
  %2699 = add <8 x i32> %2698, %14
  %2700 = add <8 x i32> %2699, %2697
  %2701 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2700, i32 %2) #8
  store <8 x i32> %2701, <8 x i32>* %466, align 32
  %2702 = load <8 x i32>, <8 x i32>* %991, align 32
  %2703 = mul <8 x i32> %2702, %762
  %2704 = mul <8 x i32> %2655, %747
  %2705 = add <8 x i32> %2704, %14
  %2706 = add <8 x i32> %2705, %2703
  %2707 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2706, i32 %2) #8
  store <8 x i32> %2707, <8 x i32>* %473, align 32
  %2708 = load <8 x i32>, <8 x i32>* %474, align 32
  %2709 = mul <8 x i32> %2708, %762
  %2710 = mul <8 x i32> %2643, %747
  %2711 = add <8 x i32> %2710, %14
  %2712 = add <8 x i32> %2711, %2709
  %2713 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2712, i32 %2) #8
  store <8 x i32> %2713, <8 x i32>* %480, align 32
  %2714 = load <8 x i32>, <8 x i32>* %1519, align 32
  %2715 = mul <8 x i32> %2714, %762
  %2716 = mul <8 x i32> %2631, %747
  %2717 = add <8 x i32> %2716, %14
  %2718 = add <8 x i32> %2717, %2715
  %2719 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2718, i32 %2) #8
  store <8 x i32> %2719, <8 x i32>* %487, align 32
  %2720 = load <8 x i32>, <8 x i32>* %488, align 32
  %2721 = mul <8 x i32> %2720, %762
  %2722 = mul <8 x i32> %2619, %747
  %2723 = add <8 x i32> %2722, %14
  %2724 = add <8 x i32> %2723, %2721
  %2725 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2724, i32 %2) #8
  store <8 x i32> %2725, <8 x i32>* %494, align 32
  %2726 = load <8 x i32>, <8 x i32>* %1009, align 32
  %2727 = mul <8 x i32> %2726, %762
  %2728 = mul <8 x i32> %2607, %747
  %2729 = add <8 x i32> %2728, %14
  %2730 = add <8 x i32> %2729, %2727
  %2731 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2730, i32 %2) #8
  store <8 x i32> %2731, <8 x i32>* %501, align 32
  %2732 = load <8 x i32>, <8 x i32>* %502, align 32
  %2733 = mul <8 x i32> %2732, %762
  %2734 = mul <8 x i32> %2595, %747
  %2735 = add <8 x i32> %2734, %14
  %2736 = add <8 x i32> %2735, %2733
  %2737 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2736, i32 %2) #8
  store <8 x i32> %2737, <8 x i32>* %508, align 32
  %2738 = load <8 x i32>, <8 x i32>* %2486, align 32
  %2739 = mul <8 x i32> %2738, %762
  %2740 = load <8 x i32>, <8 x i32>* %509, align 32
  %2741 = mul <8 x i32> %2740, %747
  %2742 = add <8 x i32> %2741, %14
  %2743 = add <8 x i32> %2742, %2739
  %2744 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2743, i32 %2) #8
  store <8 x i32> %2744, <8 x i32>* %515, align 32
  %2745 = mul <8 x i32> %2738, %747
  %2746 = add <8 x i32> %2742, %2745
  %2747 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2746, i32 %2) #8
  store <8 x i32> %2747, <8 x i32>* %520, align 32
  %2748 = mul <8 x i32> %2732, %747
  %2749 = add <8 x i32> %2735, %2748
  %2750 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2749, i32 %2) #8
  store <8 x i32> %2750, <8 x i32>* %525, align 32
  %2751 = mul <8 x i32> %2726, %747
  %2752 = add <8 x i32> %2729, %2751
  %2753 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2752, i32 %2) #8
  store <8 x i32> %2753, <8 x i32>* %530, align 32
  %2754 = mul <8 x i32> %2720, %747
  %2755 = add <8 x i32> %2723, %2754
  %2756 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2755, i32 %2) #8
  store <8 x i32> %2756, <8 x i32>* %535, align 32
  %2757 = mul <8 x i32> %2714, %747
  %2758 = add <8 x i32> %2717, %2757
  %2759 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2758, i32 %2) #8
  store <8 x i32> %2759, <8 x i32>* %540, align 32
  %2760 = mul <8 x i32> %2708, %747
  %2761 = add <8 x i32> %2711, %2760
  %2762 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2761, i32 %2) #8
  store <8 x i32> %2762, <8 x i32>* %545, align 32
  %2763 = mul <8 x i32> %2702, %747
  %2764 = add <8 x i32> %2705, %2763
  %2765 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2764, i32 %2) #8
  store <8 x i32> %2765, <8 x i32>* %550, align 32
  %2766 = mul <8 x i32> %2696, %747
  %2767 = add <8 x i32> %2699, %2766
  %2768 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2767, i32 %2) #8
  store <8 x i32> %2768, <8 x i32>* %555, align 32
  %2769 = bitcast <4 x i64>* %559 to i8*
  %2770 = bitcast <4 x i64>* %347 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 32 %2769, i8* align 32 %2770, i64 256, i1 false)
  br label %2771

2771:                                             ; preds = %2771, %2693
  %2772 = phi i64 [ 0, %2693 ], [ %2794, %2771 ]
  %2773 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %2772
  %2774 = bitcast <4 x i64>* %2773 to <8 x i32>*
  %2775 = load <8 x i32>, <8 x i32>* %2774, align 32
  %2776 = sub nuw nsw i64 63, %2772
  %2777 = getelementptr inbounds [64 x <4 x i64>], [64 x <4 x i64>]* %8, i64 0, i64 %2776
  %2778 = bitcast <4 x i64>* %2777 to <8 x i32>*
  %2779 = load <8 x i32>, <8 x i32>* %2778, align 32
  %2780 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 %2772
  %2781 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 %2776
  %2782 = add <8 x i32> %2779, %2775
  %2783 = sub <8 x i32> %2775, %2779
  %2784 = icmp sgt <8 x i32> %2782, %24
  %2785 = select <8 x i1> %2784, <8 x i32> %2782, <8 x i32> %24
  %2786 = icmp slt <8 x i32> %2785, %27
  %2787 = select <8 x i1> %2786, <8 x i32> %2785, <8 x i32> %27
  %2788 = icmp sgt <8 x i32> %2783, %24
  %2789 = select <8 x i1> %2788, <8 x i32> %2783, <8 x i32> %24
  %2790 = icmp slt <8 x i32> %2789, %27
  %2791 = select <8 x i1> %2790, <8 x i32> %2789, <8 x i32> %27
  %2792 = bitcast <4 x i64>* %2780 to <8 x i32>*
  store <8 x i32> %2787, <8 x i32>* %2792, align 32
  %2793 = bitcast <4 x i64>* %2781 to <8 x i32>*
  store <8 x i32> %2791, <8 x i32>* %2793, align 32
  %2794 = add nuw nsw i64 %2772, 1
  %2795 = icmp eq i64 %2794, 32
  br i1 %2795, label %2796, label %2771

2796:                                             ; preds = %2771
  br i1 %15, label %3168, label %2797

2797:                                             ; preds = %2796
  %2798 = icmp sgt i32 %4, 10
  %2799 = select i1 %2798, i32 %4, i32 10
  %2800 = shl i32 32, %2799
  %2801 = sub nsw i32 0, %2800
  %2802 = insertelement <8 x i32> undef, i32 %2801, i32 0
  %2803 = shufflevector <8 x i32> %2802, <8 x i32> undef, <8 x i32> zeroinitializer
  %2804 = add nsw i32 %2800, -1
  %2805 = insertelement <8 x i32> undef, i32 %2804, i32 0
  %2806 = shufflevector <8 x i32> %2805, <8 x i32> undef, <8 x i32> zeroinitializer
  %2807 = icmp eq i32 %5, 0
  br i1 %2807, label %3132, label %2808

2808:                                             ; preds = %2797
  %2809 = add nsw i32 %5, -1
  %2810 = shl i32 1, %2809
  %2811 = insertelement <8 x i32> undef, i32 %2810, i32 0
  %2812 = shufflevector <8 x i32> %2811, <8 x i32> undef, <8 x i32> zeroinitializer
  %2813 = bitcast <4 x i64>* %1 to <8 x i32>*
  %2814 = load <8 x i32>, <8 x i32>* %2813, align 32
  %2815 = add <8 x i32> %2814, %2812
  %2816 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %2817 = bitcast <4 x i64>* %2816 to <8 x i32>*
  %2818 = load <8 x i32>, <8 x i32>* %2817, align 32
  %2819 = add <8 x i32> %2818, %2812
  %2820 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %2821 = bitcast <4 x i64>* %2820 to <8 x i32>*
  %2822 = load <8 x i32>, <8 x i32>* %2821, align 32
  %2823 = add <8 x i32> %2822, %2812
  %2824 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %2825 = bitcast <4 x i64>* %2824 to <8 x i32>*
  %2826 = load <8 x i32>, <8 x i32>* %2825, align 32
  %2827 = add <8 x i32> %2826, %2812
  %2828 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2815, i32 %5) #8
  store <8 x i32> %2828, <8 x i32>* %2813, align 32
  %2829 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2819, i32 %5) #8
  store <8 x i32> %2829, <8 x i32>* %2817, align 32
  %2830 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2823, i32 %5) #8
  store <8 x i32> %2830, <8 x i32>* %2821, align 32
  %2831 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2827, i32 %5) #8
  store <8 x i32> %2831, <8 x i32>* %2825, align 32
  %2832 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %2833 = bitcast <4 x i64>* %2832 to <8 x i32>*
  %2834 = load <8 x i32>, <8 x i32>* %2833, align 32
  %2835 = add <8 x i32> %2834, %2812
  %2836 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %2837 = bitcast <4 x i64>* %2836 to <8 x i32>*
  %2838 = load <8 x i32>, <8 x i32>* %2837, align 32
  %2839 = add <8 x i32> %2838, %2812
  %2840 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %2841 = bitcast <4 x i64>* %2840 to <8 x i32>*
  %2842 = load <8 x i32>, <8 x i32>* %2841, align 32
  %2843 = add <8 x i32> %2842, %2812
  %2844 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %2845 = bitcast <4 x i64>* %2844 to <8 x i32>*
  %2846 = load <8 x i32>, <8 x i32>* %2845, align 32
  %2847 = add <8 x i32> %2846, %2812
  %2848 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2835, i32 %5) #8
  store <8 x i32> %2848, <8 x i32>* %2833, align 32
  %2849 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2839, i32 %5) #8
  store <8 x i32> %2849, <8 x i32>* %2837, align 32
  %2850 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2843, i32 %5) #8
  store <8 x i32> %2850, <8 x i32>* %2841, align 32
  %2851 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2847, i32 %5) #8
  store <8 x i32> %2851, <8 x i32>* %2845, align 32
  %2852 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %2853 = bitcast <4 x i64>* %2852 to <8 x i32>*
  %2854 = load <8 x i32>, <8 x i32>* %2853, align 32
  %2855 = add <8 x i32> %2854, %2812
  %2856 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %2857 = bitcast <4 x i64>* %2856 to <8 x i32>*
  %2858 = load <8 x i32>, <8 x i32>* %2857, align 32
  %2859 = add <8 x i32> %2858, %2812
  %2860 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %2861 = bitcast <4 x i64>* %2860 to <8 x i32>*
  %2862 = load <8 x i32>, <8 x i32>* %2861, align 32
  %2863 = add <8 x i32> %2862, %2812
  %2864 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %2865 = bitcast <4 x i64>* %2864 to <8 x i32>*
  %2866 = load <8 x i32>, <8 x i32>* %2865, align 32
  %2867 = add <8 x i32> %2866, %2812
  %2868 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2855, i32 %5) #8
  store <8 x i32> %2868, <8 x i32>* %2853, align 32
  %2869 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2859, i32 %5) #8
  store <8 x i32> %2869, <8 x i32>* %2857, align 32
  %2870 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2863, i32 %5) #8
  store <8 x i32> %2870, <8 x i32>* %2861, align 32
  %2871 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2867, i32 %5) #8
  store <8 x i32> %2871, <8 x i32>* %2865, align 32
  %2872 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %2873 = bitcast <4 x i64>* %2872 to <8 x i32>*
  %2874 = load <8 x i32>, <8 x i32>* %2873, align 32
  %2875 = add <8 x i32> %2874, %2812
  %2876 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %2877 = bitcast <4 x i64>* %2876 to <8 x i32>*
  %2878 = load <8 x i32>, <8 x i32>* %2877, align 32
  %2879 = add <8 x i32> %2878, %2812
  %2880 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %2881 = bitcast <4 x i64>* %2880 to <8 x i32>*
  %2882 = load <8 x i32>, <8 x i32>* %2881, align 32
  %2883 = add <8 x i32> %2882, %2812
  %2884 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 15
  %2885 = bitcast <4 x i64>* %2884 to <8 x i32>*
  %2886 = load <8 x i32>, <8 x i32>* %2885, align 32
  %2887 = add <8 x i32> %2886, %2812
  %2888 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2875, i32 %5) #8
  store <8 x i32> %2888, <8 x i32>* %2873, align 32
  %2889 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2879, i32 %5) #8
  store <8 x i32> %2889, <8 x i32>* %2877, align 32
  %2890 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2883, i32 %5) #8
  store <8 x i32> %2890, <8 x i32>* %2881, align 32
  %2891 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2887, i32 %5) #8
  store <8 x i32> %2891, <8 x i32>* %2885, align 32
  %2892 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 16
  %2893 = bitcast <4 x i64>* %2892 to <8 x i32>*
  %2894 = load <8 x i32>, <8 x i32>* %2893, align 32
  %2895 = add <8 x i32> %2894, %2812
  %2896 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 17
  %2897 = bitcast <4 x i64>* %2896 to <8 x i32>*
  %2898 = load <8 x i32>, <8 x i32>* %2897, align 32
  %2899 = add <8 x i32> %2898, %2812
  %2900 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 18
  %2901 = bitcast <4 x i64>* %2900 to <8 x i32>*
  %2902 = load <8 x i32>, <8 x i32>* %2901, align 32
  %2903 = add <8 x i32> %2902, %2812
  %2904 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 19
  %2905 = bitcast <4 x i64>* %2904 to <8 x i32>*
  %2906 = load <8 x i32>, <8 x i32>* %2905, align 32
  %2907 = add <8 x i32> %2906, %2812
  %2908 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2895, i32 %5) #8
  store <8 x i32> %2908, <8 x i32>* %2893, align 32
  %2909 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2899, i32 %5) #8
  store <8 x i32> %2909, <8 x i32>* %2897, align 32
  %2910 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2903, i32 %5) #8
  store <8 x i32> %2910, <8 x i32>* %2901, align 32
  %2911 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2907, i32 %5) #8
  store <8 x i32> %2911, <8 x i32>* %2905, align 32
  %2912 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 20
  %2913 = bitcast <4 x i64>* %2912 to <8 x i32>*
  %2914 = load <8 x i32>, <8 x i32>* %2913, align 32
  %2915 = add <8 x i32> %2914, %2812
  %2916 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 21
  %2917 = bitcast <4 x i64>* %2916 to <8 x i32>*
  %2918 = load <8 x i32>, <8 x i32>* %2917, align 32
  %2919 = add <8 x i32> %2918, %2812
  %2920 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 22
  %2921 = bitcast <4 x i64>* %2920 to <8 x i32>*
  %2922 = load <8 x i32>, <8 x i32>* %2921, align 32
  %2923 = add <8 x i32> %2922, %2812
  %2924 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 23
  %2925 = bitcast <4 x i64>* %2924 to <8 x i32>*
  %2926 = load <8 x i32>, <8 x i32>* %2925, align 32
  %2927 = add <8 x i32> %2926, %2812
  %2928 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2915, i32 %5) #8
  store <8 x i32> %2928, <8 x i32>* %2913, align 32
  %2929 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2919, i32 %5) #8
  store <8 x i32> %2929, <8 x i32>* %2917, align 32
  %2930 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2923, i32 %5) #8
  store <8 x i32> %2930, <8 x i32>* %2921, align 32
  %2931 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2927, i32 %5) #8
  store <8 x i32> %2931, <8 x i32>* %2925, align 32
  %2932 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 24
  %2933 = bitcast <4 x i64>* %2932 to <8 x i32>*
  %2934 = load <8 x i32>, <8 x i32>* %2933, align 32
  %2935 = add <8 x i32> %2934, %2812
  %2936 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 25
  %2937 = bitcast <4 x i64>* %2936 to <8 x i32>*
  %2938 = load <8 x i32>, <8 x i32>* %2937, align 32
  %2939 = add <8 x i32> %2938, %2812
  %2940 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 26
  %2941 = bitcast <4 x i64>* %2940 to <8 x i32>*
  %2942 = load <8 x i32>, <8 x i32>* %2941, align 32
  %2943 = add <8 x i32> %2942, %2812
  %2944 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 27
  %2945 = bitcast <4 x i64>* %2944 to <8 x i32>*
  %2946 = load <8 x i32>, <8 x i32>* %2945, align 32
  %2947 = add <8 x i32> %2946, %2812
  %2948 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2935, i32 %5) #8
  store <8 x i32> %2948, <8 x i32>* %2933, align 32
  %2949 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2939, i32 %5) #8
  store <8 x i32> %2949, <8 x i32>* %2937, align 32
  %2950 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2943, i32 %5) #8
  store <8 x i32> %2950, <8 x i32>* %2941, align 32
  %2951 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2947, i32 %5) #8
  store <8 x i32> %2951, <8 x i32>* %2945, align 32
  %2952 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 28
  %2953 = bitcast <4 x i64>* %2952 to <8 x i32>*
  %2954 = load <8 x i32>, <8 x i32>* %2953, align 32
  %2955 = add <8 x i32> %2954, %2812
  %2956 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 29
  %2957 = bitcast <4 x i64>* %2956 to <8 x i32>*
  %2958 = load <8 x i32>, <8 x i32>* %2957, align 32
  %2959 = add <8 x i32> %2958, %2812
  %2960 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 30
  %2961 = bitcast <4 x i64>* %2960 to <8 x i32>*
  %2962 = load <8 x i32>, <8 x i32>* %2961, align 32
  %2963 = add <8 x i32> %2962, %2812
  %2964 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 31
  %2965 = bitcast <4 x i64>* %2964 to <8 x i32>*
  %2966 = load <8 x i32>, <8 x i32>* %2965, align 32
  %2967 = add <8 x i32> %2966, %2812
  %2968 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2955, i32 %5) #8
  store <8 x i32> %2968, <8 x i32>* %2953, align 32
  %2969 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2959, i32 %5) #8
  store <8 x i32> %2969, <8 x i32>* %2957, align 32
  %2970 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2963, i32 %5) #8
  store <8 x i32> %2970, <8 x i32>* %2961, align 32
  %2971 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2967, i32 %5) #8
  store <8 x i32> %2971, <8 x i32>* %2965, align 32
  %2972 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 32
  %2973 = bitcast <4 x i64>* %2972 to <8 x i32>*
  %2974 = load <8 x i32>, <8 x i32>* %2973, align 32
  %2975 = add <8 x i32> %2974, %2812
  %2976 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 33
  %2977 = bitcast <4 x i64>* %2976 to <8 x i32>*
  %2978 = load <8 x i32>, <8 x i32>* %2977, align 32
  %2979 = add <8 x i32> %2978, %2812
  %2980 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 34
  %2981 = bitcast <4 x i64>* %2980 to <8 x i32>*
  %2982 = load <8 x i32>, <8 x i32>* %2981, align 32
  %2983 = add <8 x i32> %2982, %2812
  %2984 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 35
  %2985 = bitcast <4 x i64>* %2984 to <8 x i32>*
  %2986 = load <8 x i32>, <8 x i32>* %2985, align 32
  %2987 = add <8 x i32> %2986, %2812
  %2988 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2975, i32 %5) #8
  store <8 x i32> %2988, <8 x i32>* %2973, align 32
  %2989 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2979, i32 %5) #8
  store <8 x i32> %2989, <8 x i32>* %2977, align 32
  %2990 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2983, i32 %5) #8
  store <8 x i32> %2990, <8 x i32>* %2981, align 32
  %2991 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2987, i32 %5) #8
  store <8 x i32> %2991, <8 x i32>* %2985, align 32
  %2992 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 36
  %2993 = bitcast <4 x i64>* %2992 to <8 x i32>*
  %2994 = load <8 x i32>, <8 x i32>* %2993, align 32
  %2995 = add <8 x i32> %2994, %2812
  %2996 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 37
  %2997 = bitcast <4 x i64>* %2996 to <8 x i32>*
  %2998 = load <8 x i32>, <8 x i32>* %2997, align 32
  %2999 = add <8 x i32> %2998, %2812
  %3000 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 38
  %3001 = bitcast <4 x i64>* %3000 to <8 x i32>*
  %3002 = load <8 x i32>, <8 x i32>* %3001, align 32
  %3003 = add <8 x i32> %3002, %2812
  %3004 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 39
  %3005 = bitcast <4 x i64>* %3004 to <8 x i32>*
  %3006 = load <8 x i32>, <8 x i32>* %3005, align 32
  %3007 = add <8 x i32> %3006, %2812
  %3008 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2995, i32 %5) #8
  store <8 x i32> %3008, <8 x i32>* %2993, align 32
  %3009 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %2999, i32 %5) #8
  store <8 x i32> %3009, <8 x i32>* %2997, align 32
  %3010 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %3003, i32 %5) #8
  store <8 x i32> %3010, <8 x i32>* %3001, align 32
  %3011 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %3007, i32 %5) #8
  store <8 x i32> %3011, <8 x i32>* %3005, align 32
  %3012 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 40
  %3013 = bitcast <4 x i64>* %3012 to <8 x i32>*
  %3014 = load <8 x i32>, <8 x i32>* %3013, align 32
  %3015 = add <8 x i32> %3014, %2812
  %3016 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 41
  %3017 = bitcast <4 x i64>* %3016 to <8 x i32>*
  %3018 = load <8 x i32>, <8 x i32>* %3017, align 32
  %3019 = add <8 x i32> %3018, %2812
  %3020 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 42
  %3021 = bitcast <4 x i64>* %3020 to <8 x i32>*
  %3022 = load <8 x i32>, <8 x i32>* %3021, align 32
  %3023 = add <8 x i32> %3022, %2812
  %3024 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 43
  %3025 = bitcast <4 x i64>* %3024 to <8 x i32>*
  %3026 = load <8 x i32>, <8 x i32>* %3025, align 32
  %3027 = add <8 x i32> %3026, %2812
  %3028 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %3015, i32 %5) #8
  store <8 x i32> %3028, <8 x i32>* %3013, align 32
  %3029 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %3019, i32 %5) #8
  store <8 x i32> %3029, <8 x i32>* %3017, align 32
  %3030 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %3023, i32 %5) #8
  store <8 x i32> %3030, <8 x i32>* %3021, align 32
  %3031 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %3027, i32 %5) #8
  store <8 x i32> %3031, <8 x i32>* %3025, align 32
  %3032 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 44
  %3033 = bitcast <4 x i64>* %3032 to <8 x i32>*
  %3034 = load <8 x i32>, <8 x i32>* %3033, align 32
  %3035 = add <8 x i32> %3034, %2812
  %3036 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 45
  %3037 = bitcast <4 x i64>* %3036 to <8 x i32>*
  %3038 = load <8 x i32>, <8 x i32>* %3037, align 32
  %3039 = add <8 x i32> %3038, %2812
  %3040 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 46
  %3041 = bitcast <4 x i64>* %3040 to <8 x i32>*
  %3042 = load <8 x i32>, <8 x i32>* %3041, align 32
  %3043 = add <8 x i32> %3042, %2812
  %3044 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 47
  %3045 = bitcast <4 x i64>* %3044 to <8 x i32>*
  %3046 = load <8 x i32>, <8 x i32>* %3045, align 32
  %3047 = add <8 x i32> %3046, %2812
  %3048 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %3035, i32 %5) #8
  store <8 x i32> %3048, <8 x i32>* %3033, align 32
  %3049 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %3039, i32 %5) #8
  store <8 x i32> %3049, <8 x i32>* %3037, align 32
  %3050 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %3043, i32 %5) #8
  store <8 x i32> %3050, <8 x i32>* %3041, align 32
  %3051 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %3047, i32 %5) #8
  store <8 x i32> %3051, <8 x i32>* %3045, align 32
  %3052 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 48
  %3053 = bitcast <4 x i64>* %3052 to <8 x i32>*
  %3054 = load <8 x i32>, <8 x i32>* %3053, align 32
  %3055 = add <8 x i32> %3054, %2812
  %3056 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 49
  %3057 = bitcast <4 x i64>* %3056 to <8 x i32>*
  %3058 = load <8 x i32>, <8 x i32>* %3057, align 32
  %3059 = add <8 x i32> %3058, %2812
  %3060 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 50
  %3061 = bitcast <4 x i64>* %3060 to <8 x i32>*
  %3062 = load <8 x i32>, <8 x i32>* %3061, align 32
  %3063 = add <8 x i32> %3062, %2812
  %3064 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 51
  %3065 = bitcast <4 x i64>* %3064 to <8 x i32>*
  %3066 = load <8 x i32>, <8 x i32>* %3065, align 32
  %3067 = add <8 x i32> %3066, %2812
  %3068 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %3055, i32 %5) #8
  store <8 x i32> %3068, <8 x i32>* %3053, align 32
  %3069 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %3059, i32 %5) #8
  store <8 x i32> %3069, <8 x i32>* %3057, align 32
  %3070 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %3063, i32 %5) #8
  store <8 x i32> %3070, <8 x i32>* %3061, align 32
  %3071 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %3067, i32 %5) #8
  store <8 x i32> %3071, <8 x i32>* %3065, align 32
  %3072 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 52
  %3073 = bitcast <4 x i64>* %3072 to <8 x i32>*
  %3074 = load <8 x i32>, <8 x i32>* %3073, align 32
  %3075 = add <8 x i32> %3074, %2812
  %3076 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 53
  %3077 = bitcast <4 x i64>* %3076 to <8 x i32>*
  %3078 = load <8 x i32>, <8 x i32>* %3077, align 32
  %3079 = add <8 x i32> %3078, %2812
  %3080 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 54
  %3081 = bitcast <4 x i64>* %3080 to <8 x i32>*
  %3082 = load <8 x i32>, <8 x i32>* %3081, align 32
  %3083 = add <8 x i32> %3082, %2812
  %3084 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 55
  %3085 = bitcast <4 x i64>* %3084 to <8 x i32>*
  %3086 = load <8 x i32>, <8 x i32>* %3085, align 32
  %3087 = add <8 x i32> %3086, %2812
  %3088 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %3075, i32 %5) #8
  store <8 x i32> %3088, <8 x i32>* %3073, align 32
  %3089 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %3079, i32 %5) #8
  store <8 x i32> %3089, <8 x i32>* %3077, align 32
  %3090 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %3083, i32 %5) #8
  store <8 x i32> %3090, <8 x i32>* %3081, align 32
  %3091 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %3087, i32 %5) #8
  store <8 x i32> %3091, <8 x i32>* %3085, align 32
  %3092 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 56
  %3093 = bitcast <4 x i64>* %3092 to <8 x i32>*
  %3094 = load <8 x i32>, <8 x i32>* %3093, align 32
  %3095 = add <8 x i32> %3094, %2812
  %3096 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 57
  %3097 = bitcast <4 x i64>* %3096 to <8 x i32>*
  %3098 = load <8 x i32>, <8 x i32>* %3097, align 32
  %3099 = add <8 x i32> %3098, %2812
  %3100 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 58
  %3101 = bitcast <4 x i64>* %3100 to <8 x i32>*
  %3102 = load <8 x i32>, <8 x i32>* %3101, align 32
  %3103 = add <8 x i32> %3102, %2812
  %3104 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 59
  %3105 = bitcast <4 x i64>* %3104 to <8 x i32>*
  %3106 = load <8 x i32>, <8 x i32>* %3105, align 32
  %3107 = add <8 x i32> %3106, %2812
  %3108 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %3095, i32 %5) #8
  store <8 x i32> %3108, <8 x i32>* %3093, align 32
  %3109 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %3099, i32 %5) #8
  store <8 x i32> %3109, <8 x i32>* %3097, align 32
  %3110 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %3103, i32 %5) #8
  store <8 x i32> %3110, <8 x i32>* %3101, align 32
  %3111 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %3107, i32 %5) #8
  store <8 x i32> %3111, <8 x i32>* %3105, align 32
  %3112 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 60
  %3113 = bitcast <4 x i64>* %3112 to <8 x i32>*
  %3114 = load <8 x i32>, <8 x i32>* %3113, align 32
  %3115 = add <8 x i32> %3114, %2812
  %3116 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 61
  %3117 = bitcast <4 x i64>* %3116 to <8 x i32>*
  %3118 = load <8 x i32>, <8 x i32>* %3117, align 32
  %3119 = add <8 x i32> %3118, %2812
  %3120 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 62
  %3121 = bitcast <4 x i64>* %3120 to <8 x i32>*
  %3122 = load <8 x i32>, <8 x i32>* %3121, align 32
  %3123 = add <8 x i32> %3122, %2812
  %3124 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 63
  %3125 = bitcast <4 x i64>* %3124 to <8 x i32>*
  %3126 = load <8 x i32>, <8 x i32>* %3125, align 32
  %3127 = add <8 x i32> %3126, %2812
  %3128 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %3115, i32 %5) #8
  store <8 x i32> %3128, <8 x i32>* %3113, align 32
  %3129 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %3119, i32 %5) #8
  store <8 x i32> %3129, <8 x i32>* %3117, align 32
  %3130 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %3123, i32 %5) #8
  store <8 x i32> %3130, <8 x i32>* %3121, align 32
  %3131 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %3127, i32 %5) #8
  store <8 x i32> %3131, <8 x i32>* %3125, align 32
  br label %3132

3132:                                             ; preds = %2797, %2808
  br label %3133

3133:                                             ; preds = %3132, %3133
  %3134 = phi i64 [ %3166, %3133 ], [ 0, %3132 ]
  %3135 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 %3134
  %3136 = bitcast <4 x i64>* %3135 to <8 x i32>*
  %3137 = load <8 x i32>, <8 x i32>* %3136, align 32
  %3138 = icmp sgt <8 x i32> %3137, %2803
  %3139 = select <8 x i1> %3138, <8 x i32> %3137, <8 x i32> %2803
  %3140 = icmp slt <8 x i32> %3139, %2806
  %3141 = select <8 x i1> %3140, <8 x i32> %3139, <8 x i32> %2806
  store <8 x i32> %3141, <8 x i32>* %3136, align 32
  %3142 = or i64 %3134, 1
  %3143 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 %3142
  %3144 = bitcast <4 x i64>* %3143 to <8 x i32>*
  %3145 = load <8 x i32>, <8 x i32>* %3144, align 32
  %3146 = icmp sgt <8 x i32> %3145, %2803
  %3147 = select <8 x i1> %3146, <8 x i32> %3145, <8 x i32> %2803
  %3148 = icmp slt <8 x i32> %3147, %2806
  %3149 = select <8 x i1> %3148, <8 x i32> %3147, <8 x i32> %2806
  store <8 x i32> %3149, <8 x i32>* %3144, align 32
  %3150 = or i64 %3134, 2
  %3151 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 %3150
  %3152 = bitcast <4 x i64>* %3151 to <8 x i32>*
  %3153 = load <8 x i32>, <8 x i32>* %3152, align 32
  %3154 = icmp sgt <8 x i32> %3153, %2803
  %3155 = select <8 x i1> %3154, <8 x i32> %3153, <8 x i32> %2803
  %3156 = icmp slt <8 x i32> %3155, %2806
  %3157 = select <8 x i1> %3156, <8 x i32> %3155, <8 x i32> %2806
  store <8 x i32> %3157, <8 x i32>* %3152, align 32
  %3158 = or i64 %3134, 3
  %3159 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 %3158
  %3160 = bitcast <4 x i64>* %3159 to <8 x i32>*
  %3161 = load <8 x i32>, <8 x i32>* %3160, align 32
  %3162 = icmp sgt <8 x i32> %3161, %2803
  %3163 = select <8 x i1> %3162, <8 x i32> %3161, <8 x i32> %2803
  %3164 = icmp slt <8 x i32> %3163, %2806
  %3165 = select <8 x i1> %3164, <8 x i32> %3163, <8 x i32> %2806
  store <8 x i32> %3165, <8 x i32>* %3160, align 32
  %3166 = add nuw nsw i64 %3134, 4
  %3167 = icmp ult i64 %3166, 64
  br i1 %3167, label %3133, label %3168

3168:                                             ; preds = %3133, %2796
  call void @llvm.lifetime.end.p0i8(i64 2048, i8* nonnull %308) #8
  call void @llvm.lifetime.end.p0i8(i64 2048, i8* nonnull %307) #8
  ret void
}

; Function Attrs: nounwind readnone
declare <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32>, i32) #5

; Function Attrs: nounwind readnone
declare <8 x i32> @llvm.x86.avx2.psra.d(<8 x i32>, <4 x i32>) #5

; Function Attrs: inlinehint nounwind ssp uwtable
define internal fastcc void @idct32_stage9_avx2(<4 x i64>* readonly, <4 x i64>*, i32, i32, i32, <4 x i64>* nocapture readonly, <4 x i64>* nocapture readonly) unnamed_addr #6 {
  %8 = bitcast <4 x i64>* %0 to <8 x i32>*
  %9 = load <8 x i32>, <8 x i32>* %8, align 32
  %10 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 31
  %11 = bitcast <4 x i64>* %10 to <8 x i32>*
  %12 = load <8 x i32>, <8 x i32>* %11, align 32
  %13 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 31
  %14 = add <8 x i32> %12, %9
  %15 = sub <8 x i32> %9, %12
  %16 = bitcast <4 x i64>* %5 to <8 x i32>*
  %17 = load <8 x i32>, <8 x i32>* %16, align 32
  %18 = icmp sgt <8 x i32> %14, %17
  %19 = select <8 x i1> %18, <8 x i32> %14, <8 x i32> %17
  %20 = bitcast <4 x i64>* %6 to <8 x i32>*
  %21 = load <8 x i32>, <8 x i32>* %20, align 32
  %22 = icmp slt <8 x i32> %19, %21
  %23 = select <8 x i1> %22, <8 x i32> %19, <8 x i32> %21
  %24 = icmp sgt <8 x i32> %15, %17
  %25 = select <8 x i1> %24, <8 x i32> %15, <8 x i32> %17
  %26 = icmp slt <8 x i32> %25, %21
  %27 = select <8 x i1> %26, <8 x i32> %25, <8 x i32> %21
  %28 = bitcast <4 x i64>* %1 to <8 x i32>*
  store <8 x i32> %23, <8 x i32>* %28, align 32
  %29 = bitcast <4 x i64>* %13 to <8 x i32>*
  store <8 x i32> %27, <8 x i32>* %29, align 32
  %30 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 1
  %31 = bitcast <4 x i64>* %30 to <8 x i32>*
  %32 = load <8 x i32>, <8 x i32>* %31, align 32
  %33 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 30
  %34 = bitcast <4 x i64>* %33 to <8 x i32>*
  %35 = load <8 x i32>, <8 x i32>* %34, align 32
  %36 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %37 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 30
  %38 = add <8 x i32> %35, %32
  %39 = sub <8 x i32> %32, %35
  %40 = load <8 x i32>, <8 x i32>* %16, align 32
  %41 = icmp sgt <8 x i32> %38, %40
  %42 = select <8 x i1> %41, <8 x i32> %38, <8 x i32> %40
  %43 = load <8 x i32>, <8 x i32>* %20, align 32
  %44 = icmp slt <8 x i32> %42, %43
  %45 = select <8 x i1> %44, <8 x i32> %42, <8 x i32> %43
  %46 = icmp sgt <8 x i32> %39, %40
  %47 = select <8 x i1> %46, <8 x i32> %39, <8 x i32> %40
  %48 = icmp slt <8 x i32> %47, %43
  %49 = select <8 x i1> %48, <8 x i32> %47, <8 x i32> %43
  %50 = bitcast <4 x i64>* %36 to <8 x i32>*
  store <8 x i32> %45, <8 x i32>* %50, align 32
  %51 = bitcast <4 x i64>* %37 to <8 x i32>*
  store <8 x i32> %49, <8 x i32>* %51, align 32
  %52 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 2
  %53 = bitcast <4 x i64>* %52 to <8 x i32>*
  %54 = load <8 x i32>, <8 x i32>* %53, align 32
  %55 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 29
  %56 = bitcast <4 x i64>* %55 to <8 x i32>*
  %57 = load <8 x i32>, <8 x i32>* %56, align 32
  %58 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %59 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 29
  %60 = add <8 x i32> %57, %54
  %61 = sub <8 x i32> %54, %57
  %62 = load <8 x i32>, <8 x i32>* %16, align 32
  %63 = icmp sgt <8 x i32> %60, %62
  %64 = select <8 x i1> %63, <8 x i32> %60, <8 x i32> %62
  %65 = load <8 x i32>, <8 x i32>* %20, align 32
  %66 = icmp slt <8 x i32> %64, %65
  %67 = select <8 x i1> %66, <8 x i32> %64, <8 x i32> %65
  %68 = icmp sgt <8 x i32> %61, %62
  %69 = select <8 x i1> %68, <8 x i32> %61, <8 x i32> %62
  %70 = icmp slt <8 x i32> %69, %65
  %71 = select <8 x i1> %70, <8 x i32> %69, <8 x i32> %65
  %72 = bitcast <4 x i64>* %58 to <8 x i32>*
  store <8 x i32> %67, <8 x i32>* %72, align 32
  %73 = bitcast <4 x i64>* %59 to <8 x i32>*
  store <8 x i32> %71, <8 x i32>* %73, align 32
  %74 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 3
  %75 = bitcast <4 x i64>* %74 to <8 x i32>*
  %76 = load <8 x i32>, <8 x i32>* %75, align 32
  %77 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 28
  %78 = bitcast <4 x i64>* %77 to <8 x i32>*
  %79 = load <8 x i32>, <8 x i32>* %78, align 32
  %80 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %81 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 28
  %82 = add <8 x i32> %79, %76
  %83 = sub <8 x i32> %76, %79
  %84 = load <8 x i32>, <8 x i32>* %16, align 32
  %85 = icmp sgt <8 x i32> %82, %84
  %86 = select <8 x i1> %85, <8 x i32> %82, <8 x i32> %84
  %87 = load <8 x i32>, <8 x i32>* %20, align 32
  %88 = icmp slt <8 x i32> %86, %87
  %89 = select <8 x i1> %88, <8 x i32> %86, <8 x i32> %87
  %90 = icmp sgt <8 x i32> %83, %84
  %91 = select <8 x i1> %90, <8 x i32> %83, <8 x i32> %84
  %92 = icmp slt <8 x i32> %91, %87
  %93 = select <8 x i1> %92, <8 x i32> %91, <8 x i32> %87
  %94 = bitcast <4 x i64>* %80 to <8 x i32>*
  store <8 x i32> %89, <8 x i32>* %94, align 32
  %95 = bitcast <4 x i64>* %81 to <8 x i32>*
  store <8 x i32> %93, <8 x i32>* %95, align 32
  %96 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 4
  %97 = bitcast <4 x i64>* %96 to <8 x i32>*
  %98 = load <8 x i32>, <8 x i32>* %97, align 32
  %99 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 27
  %100 = bitcast <4 x i64>* %99 to <8 x i32>*
  %101 = load <8 x i32>, <8 x i32>* %100, align 32
  %102 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %103 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 27
  %104 = add <8 x i32> %101, %98
  %105 = sub <8 x i32> %98, %101
  %106 = load <8 x i32>, <8 x i32>* %16, align 32
  %107 = icmp sgt <8 x i32> %104, %106
  %108 = select <8 x i1> %107, <8 x i32> %104, <8 x i32> %106
  %109 = load <8 x i32>, <8 x i32>* %20, align 32
  %110 = icmp slt <8 x i32> %108, %109
  %111 = select <8 x i1> %110, <8 x i32> %108, <8 x i32> %109
  %112 = icmp sgt <8 x i32> %105, %106
  %113 = select <8 x i1> %112, <8 x i32> %105, <8 x i32> %106
  %114 = icmp slt <8 x i32> %113, %109
  %115 = select <8 x i1> %114, <8 x i32> %113, <8 x i32> %109
  %116 = bitcast <4 x i64>* %102 to <8 x i32>*
  store <8 x i32> %111, <8 x i32>* %116, align 32
  %117 = bitcast <4 x i64>* %103 to <8 x i32>*
  store <8 x i32> %115, <8 x i32>* %117, align 32
  %118 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 5
  %119 = bitcast <4 x i64>* %118 to <8 x i32>*
  %120 = load <8 x i32>, <8 x i32>* %119, align 32
  %121 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 26
  %122 = bitcast <4 x i64>* %121 to <8 x i32>*
  %123 = load <8 x i32>, <8 x i32>* %122, align 32
  %124 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %125 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 26
  %126 = add <8 x i32> %123, %120
  %127 = sub <8 x i32> %120, %123
  %128 = load <8 x i32>, <8 x i32>* %16, align 32
  %129 = icmp sgt <8 x i32> %126, %128
  %130 = select <8 x i1> %129, <8 x i32> %126, <8 x i32> %128
  %131 = load <8 x i32>, <8 x i32>* %20, align 32
  %132 = icmp slt <8 x i32> %130, %131
  %133 = select <8 x i1> %132, <8 x i32> %130, <8 x i32> %131
  %134 = icmp sgt <8 x i32> %127, %128
  %135 = select <8 x i1> %134, <8 x i32> %127, <8 x i32> %128
  %136 = icmp slt <8 x i32> %135, %131
  %137 = select <8 x i1> %136, <8 x i32> %135, <8 x i32> %131
  %138 = bitcast <4 x i64>* %124 to <8 x i32>*
  store <8 x i32> %133, <8 x i32>* %138, align 32
  %139 = bitcast <4 x i64>* %125 to <8 x i32>*
  store <8 x i32> %137, <8 x i32>* %139, align 32
  %140 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 6
  %141 = bitcast <4 x i64>* %140 to <8 x i32>*
  %142 = load <8 x i32>, <8 x i32>* %141, align 32
  %143 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 25
  %144 = bitcast <4 x i64>* %143 to <8 x i32>*
  %145 = load <8 x i32>, <8 x i32>* %144, align 32
  %146 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %147 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 25
  %148 = add <8 x i32> %145, %142
  %149 = sub <8 x i32> %142, %145
  %150 = load <8 x i32>, <8 x i32>* %16, align 32
  %151 = icmp sgt <8 x i32> %148, %150
  %152 = select <8 x i1> %151, <8 x i32> %148, <8 x i32> %150
  %153 = load <8 x i32>, <8 x i32>* %20, align 32
  %154 = icmp slt <8 x i32> %152, %153
  %155 = select <8 x i1> %154, <8 x i32> %152, <8 x i32> %153
  %156 = icmp sgt <8 x i32> %149, %150
  %157 = select <8 x i1> %156, <8 x i32> %149, <8 x i32> %150
  %158 = icmp slt <8 x i32> %157, %153
  %159 = select <8 x i1> %158, <8 x i32> %157, <8 x i32> %153
  %160 = bitcast <4 x i64>* %146 to <8 x i32>*
  store <8 x i32> %155, <8 x i32>* %160, align 32
  %161 = bitcast <4 x i64>* %147 to <8 x i32>*
  store <8 x i32> %159, <8 x i32>* %161, align 32
  %162 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 7
  %163 = bitcast <4 x i64>* %162 to <8 x i32>*
  %164 = load <8 x i32>, <8 x i32>* %163, align 32
  %165 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 24
  %166 = bitcast <4 x i64>* %165 to <8 x i32>*
  %167 = load <8 x i32>, <8 x i32>* %166, align 32
  %168 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %169 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 24
  %170 = add <8 x i32> %167, %164
  %171 = sub <8 x i32> %164, %167
  %172 = load <8 x i32>, <8 x i32>* %16, align 32
  %173 = icmp sgt <8 x i32> %170, %172
  %174 = select <8 x i1> %173, <8 x i32> %170, <8 x i32> %172
  %175 = load <8 x i32>, <8 x i32>* %20, align 32
  %176 = icmp slt <8 x i32> %174, %175
  %177 = select <8 x i1> %176, <8 x i32> %174, <8 x i32> %175
  %178 = icmp sgt <8 x i32> %171, %172
  %179 = select <8 x i1> %178, <8 x i32> %171, <8 x i32> %172
  %180 = icmp slt <8 x i32> %179, %175
  %181 = select <8 x i1> %180, <8 x i32> %179, <8 x i32> %175
  %182 = bitcast <4 x i64>* %168 to <8 x i32>*
  store <8 x i32> %177, <8 x i32>* %182, align 32
  %183 = bitcast <4 x i64>* %169 to <8 x i32>*
  store <8 x i32> %181, <8 x i32>* %183, align 32
  %184 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 8
  %185 = bitcast <4 x i64>* %184 to <8 x i32>*
  %186 = load <8 x i32>, <8 x i32>* %185, align 32
  %187 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 23
  %188 = bitcast <4 x i64>* %187 to <8 x i32>*
  %189 = load <8 x i32>, <8 x i32>* %188, align 32
  %190 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %191 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 23
  %192 = add <8 x i32> %189, %186
  %193 = sub <8 x i32> %186, %189
  %194 = load <8 x i32>, <8 x i32>* %16, align 32
  %195 = icmp sgt <8 x i32> %192, %194
  %196 = select <8 x i1> %195, <8 x i32> %192, <8 x i32> %194
  %197 = load <8 x i32>, <8 x i32>* %20, align 32
  %198 = icmp slt <8 x i32> %196, %197
  %199 = select <8 x i1> %198, <8 x i32> %196, <8 x i32> %197
  %200 = icmp sgt <8 x i32> %193, %194
  %201 = select <8 x i1> %200, <8 x i32> %193, <8 x i32> %194
  %202 = icmp slt <8 x i32> %201, %197
  %203 = select <8 x i1> %202, <8 x i32> %201, <8 x i32> %197
  %204 = bitcast <4 x i64>* %190 to <8 x i32>*
  store <8 x i32> %199, <8 x i32>* %204, align 32
  %205 = bitcast <4 x i64>* %191 to <8 x i32>*
  store <8 x i32> %203, <8 x i32>* %205, align 32
  %206 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 9
  %207 = bitcast <4 x i64>* %206 to <8 x i32>*
  %208 = load <8 x i32>, <8 x i32>* %207, align 32
  %209 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 22
  %210 = bitcast <4 x i64>* %209 to <8 x i32>*
  %211 = load <8 x i32>, <8 x i32>* %210, align 32
  %212 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %213 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 22
  %214 = add <8 x i32> %211, %208
  %215 = sub <8 x i32> %208, %211
  %216 = load <8 x i32>, <8 x i32>* %16, align 32
  %217 = icmp sgt <8 x i32> %214, %216
  %218 = select <8 x i1> %217, <8 x i32> %214, <8 x i32> %216
  %219 = load <8 x i32>, <8 x i32>* %20, align 32
  %220 = icmp slt <8 x i32> %218, %219
  %221 = select <8 x i1> %220, <8 x i32> %218, <8 x i32> %219
  %222 = icmp sgt <8 x i32> %215, %216
  %223 = select <8 x i1> %222, <8 x i32> %215, <8 x i32> %216
  %224 = icmp slt <8 x i32> %223, %219
  %225 = select <8 x i1> %224, <8 x i32> %223, <8 x i32> %219
  %226 = bitcast <4 x i64>* %212 to <8 x i32>*
  store <8 x i32> %221, <8 x i32>* %226, align 32
  %227 = bitcast <4 x i64>* %213 to <8 x i32>*
  store <8 x i32> %225, <8 x i32>* %227, align 32
  %228 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 10
  %229 = bitcast <4 x i64>* %228 to <8 x i32>*
  %230 = load <8 x i32>, <8 x i32>* %229, align 32
  %231 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 21
  %232 = bitcast <4 x i64>* %231 to <8 x i32>*
  %233 = load <8 x i32>, <8 x i32>* %232, align 32
  %234 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %235 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 21
  %236 = add <8 x i32> %233, %230
  %237 = sub <8 x i32> %230, %233
  %238 = load <8 x i32>, <8 x i32>* %16, align 32
  %239 = icmp sgt <8 x i32> %236, %238
  %240 = select <8 x i1> %239, <8 x i32> %236, <8 x i32> %238
  %241 = load <8 x i32>, <8 x i32>* %20, align 32
  %242 = icmp slt <8 x i32> %240, %241
  %243 = select <8 x i1> %242, <8 x i32> %240, <8 x i32> %241
  %244 = icmp sgt <8 x i32> %237, %238
  %245 = select <8 x i1> %244, <8 x i32> %237, <8 x i32> %238
  %246 = icmp slt <8 x i32> %245, %241
  %247 = select <8 x i1> %246, <8 x i32> %245, <8 x i32> %241
  %248 = bitcast <4 x i64>* %234 to <8 x i32>*
  store <8 x i32> %243, <8 x i32>* %248, align 32
  %249 = bitcast <4 x i64>* %235 to <8 x i32>*
  store <8 x i32> %247, <8 x i32>* %249, align 32
  %250 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 11
  %251 = bitcast <4 x i64>* %250 to <8 x i32>*
  %252 = load <8 x i32>, <8 x i32>* %251, align 32
  %253 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 20
  %254 = bitcast <4 x i64>* %253 to <8 x i32>*
  %255 = load <8 x i32>, <8 x i32>* %254, align 32
  %256 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %257 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 20
  %258 = add <8 x i32> %255, %252
  %259 = sub <8 x i32> %252, %255
  %260 = load <8 x i32>, <8 x i32>* %16, align 32
  %261 = icmp sgt <8 x i32> %258, %260
  %262 = select <8 x i1> %261, <8 x i32> %258, <8 x i32> %260
  %263 = load <8 x i32>, <8 x i32>* %20, align 32
  %264 = icmp slt <8 x i32> %262, %263
  %265 = select <8 x i1> %264, <8 x i32> %262, <8 x i32> %263
  %266 = icmp sgt <8 x i32> %259, %260
  %267 = select <8 x i1> %266, <8 x i32> %259, <8 x i32> %260
  %268 = icmp slt <8 x i32> %267, %263
  %269 = select <8 x i1> %268, <8 x i32> %267, <8 x i32> %263
  %270 = bitcast <4 x i64>* %256 to <8 x i32>*
  store <8 x i32> %265, <8 x i32>* %270, align 32
  %271 = bitcast <4 x i64>* %257 to <8 x i32>*
  store <8 x i32> %269, <8 x i32>* %271, align 32
  %272 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 12
  %273 = bitcast <4 x i64>* %272 to <8 x i32>*
  %274 = load <8 x i32>, <8 x i32>* %273, align 32
  %275 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 19
  %276 = bitcast <4 x i64>* %275 to <8 x i32>*
  %277 = load <8 x i32>, <8 x i32>* %276, align 32
  %278 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %279 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 19
  %280 = add <8 x i32> %277, %274
  %281 = sub <8 x i32> %274, %277
  %282 = load <8 x i32>, <8 x i32>* %16, align 32
  %283 = icmp sgt <8 x i32> %280, %282
  %284 = select <8 x i1> %283, <8 x i32> %280, <8 x i32> %282
  %285 = load <8 x i32>, <8 x i32>* %20, align 32
  %286 = icmp slt <8 x i32> %284, %285
  %287 = select <8 x i1> %286, <8 x i32> %284, <8 x i32> %285
  %288 = icmp sgt <8 x i32> %281, %282
  %289 = select <8 x i1> %288, <8 x i32> %281, <8 x i32> %282
  %290 = icmp slt <8 x i32> %289, %285
  %291 = select <8 x i1> %290, <8 x i32> %289, <8 x i32> %285
  %292 = bitcast <4 x i64>* %278 to <8 x i32>*
  store <8 x i32> %287, <8 x i32>* %292, align 32
  %293 = bitcast <4 x i64>* %279 to <8 x i32>*
  store <8 x i32> %291, <8 x i32>* %293, align 32
  %294 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 13
  %295 = bitcast <4 x i64>* %294 to <8 x i32>*
  %296 = load <8 x i32>, <8 x i32>* %295, align 32
  %297 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 18
  %298 = bitcast <4 x i64>* %297 to <8 x i32>*
  %299 = load <8 x i32>, <8 x i32>* %298, align 32
  %300 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %301 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 18
  %302 = add <8 x i32> %299, %296
  %303 = sub <8 x i32> %296, %299
  %304 = load <8 x i32>, <8 x i32>* %16, align 32
  %305 = icmp sgt <8 x i32> %302, %304
  %306 = select <8 x i1> %305, <8 x i32> %302, <8 x i32> %304
  %307 = load <8 x i32>, <8 x i32>* %20, align 32
  %308 = icmp slt <8 x i32> %306, %307
  %309 = select <8 x i1> %308, <8 x i32> %306, <8 x i32> %307
  %310 = icmp sgt <8 x i32> %303, %304
  %311 = select <8 x i1> %310, <8 x i32> %303, <8 x i32> %304
  %312 = icmp slt <8 x i32> %311, %307
  %313 = select <8 x i1> %312, <8 x i32> %311, <8 x i32> %307
  %314 = bitcast <4 x i64>* %300 to <8 x i32>*
  store <8 x i32> %309, <8 x i32>* %314, align 32
  %315 = bitcast <4 x i64>* %301 to <8 x i32>*
  store <8 x i32> %313, <8 x i32>* %315, align 32
  %316 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 14
  %317 = bitcast <4 x i64>* %316 to <8 x i32>*
  %318 = load <8 x i32>, <8 x i32>* %317, align 32
  %319 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 17
  %320 = bitcast <4 x i64>* %319 to <8 x i32>*
  %321 = load <8 x i32>, <8 x i32>* %320, align 32
  %322 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %323 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 17
  %324 = add <8 x i32> %321, %318
  %325 = sub <8 x i32> %318, %321
  %326 = load <8 x i32>, <8 x i32>* %16, align 32
  %327 = icmp sgt <8 x i32> %324, %326
  %328 = select <8 x i1> %327, <8 x i32> %324, <8 x i32> %326
  %329 = load <8 x i32>, <8 x i32>* %20, align 32
  %330 = icmp slt <8 x i32> %328, %329
  %331 = select <8 x i1> %330, <8 x i32> %328, <8 x i32> %329
  %332 = icmp sgt <8 x i32> %325, %326
  %333 = select <8 x i1> %332, <8 x i32> %325, <8 x i32> %326
  %334 = icmp slt <8 x i32> %333, %329
  %335 = select <8 x i1> %334, <8 x i32> %333, <8 x i32> %329
  %336 = bitcast <4 x i64>* %322 to <8 x i32>*
  store <8 x i32> %331, <8 x i32>* %336, align 32
  %337 = bitcast <4 x i64>* %323 to <8 x i32>*
  store <8 x i32> %335, <8 x i32>* %337, align 32
  %338 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 15
  %339 = bitcast <4 x i64>* %338 to <8 x i32>*
  %340 = load <8 x i32>, <8 x i32>* %339, align 32
  %341 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 16
  %342 = bitcast <4 x i64>* %341 to <8 x i32>*
  %343 = load <8 x i32>, <8 x i32>* %342, align 32
  %344 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 15
  %345 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 16
  %346 = add <8 x i32> %343, %340
  %347 = sub <8 x i32> %340, %343
  %348 = load <8 x i32>, <8 x i32>* %16, align 32
  %349 = icmp sgt <8 x i32> %346, %348
  %350 = select <8 x i1> %349, <8 x i32> %346, <8 x i32> %348
  %351 = load <8 x i32>, <8 x i32>* %20, align 32
  %352 = icmp slt <8 x i32> %350, %351
  %353 = select <8 x i1> %352, <8 x i32> %350, <8 x i32> %351
  %354 = icmp sgt <8 x i32> %347, %348
  %355 = select <8 x i1> %354, <8 x i32> %347, <8 x i32> %348
  %356 = icmp slt <8 x i32> %355, %351
  %357 = select <8 x i1> %356, <8 x i32> %355, <8 x i32> %351
  %358 = bitcast <4 x i64>* %344 to <8 x i32>*
  store <8 x i32> %353, <8 x i32>* %358, align 32
  %359 = bitcast <4 x i64>* %345 to <8 x i32>*
  store <8 x i32> %357, <8 x i32>* %359, align 32
  %360 = icmp eq i32 %2, 0
  br i1 %360, label %361, label %488

361:                                              ; preds = %7
  %362 = icmp sgt i32 %3, 10
  %363 = select i1 %362, i32 %3, i32 10
  %364 = shl i32 32, %363
  %365 = sub nsw i32 0, %364
  %366 = insertelement <8 x i32> undef, i32 %365, i32 0
  %367 = shufflevector <8 x i32> %366, <8 x i32> undef, <8 x i32> zeroinitializer
  %368 = add nsw i32 %364, -1
  %369 = insertelement <8 x i32> undef, i32 %368, i32 0
  %370 = shufflevector <8 x i32> %369, <8 x i32> undef, <8 x i32> zeroinitializer
  %371 = icmp eq i32 %4, 0
  br i1 %371, label %452, label %372

372:                                              ; preds = %361
  %373 = add nsw i32 %4, -1
  %374 = shl i32 1, %373
  %375 = insertelement <8 x i32> undef, i32 %374, i32 0
  %376 = shufflevector <8 x i32> %375, <8 x i32> undef, <8 x i32> zeroinitializer
  %377 = add <8 x i32> %23, %376
  %378 = add <8 x i32> %45, %376
  %379 = add <8 x i32> %67, %376
  %380 = add <8 x i32> %89, %376
  %381 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %377, i32 %4) #8
  store <8 x i32> %381, <8 x i32>* %28, align 32
  %382 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %378, i32 %4) #8
  store <8 x i32> %382, <8 x i32>* %50, align 32
  %383 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %379, i32 %4) #8
  store <8 x i32> %383, <8 x i32>* %72, align 32
  %384 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %380, i32 %4) #8
  store <8 x i32> %384, <8 x i32>* %94, align 32
  %385 = add <8 x i32> %111, %376
  %386 = add <8 x i32> %133, %376
  %387 = add <8 x i32> %155, %376
  %388 = add <8 x i32> %177, %376
  %389 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %385, i32 %4) #8
  store <8 x i32> %389, <8 x i32>* %116, align 32
  %390 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %386, i32 %4) #8
  store <8 x i32> %390, <8 x i32>* %138, align 32
  %391 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %387, i32 %4) #8
  store <8 x i32> %391, <8 x i32>* %160, align 32
  %392 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %388, i32 %4) #8
  store <8 x i32> %392, <8 x i32>* %182, align 32
  %393 = add <8 x i32> %199, %376
  %394 = add <8 x i32> %221, %376
  %395 = add <8 x i32> %243, %376
  %396 = add <8 x i32> %265, %376
  %397 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %393, i32 %4) #8
  store <8 x i32> %397, <8 x i32>* %204, align 32
  %398 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %394, i32 %4) #8
  store <8 x i32> %398, <8 x i32>* %226, align 32
  %399 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %395, i32 %4) #8
  store <8 x i32> %399, <8 x i32>* %248, align 32
  %400 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %396, i32 %4) #8
  store <8 x i32> %400, <8 x i32>* %270, align 32
  %401 = add <8 x i32> %287, %376
  %402 = add <8 x i32> %309, %376
  %403 = add <8 x i32> %331, %376
  %404 = add <8 x i32> %353, %376
  %405 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %401, i32 %4) #8
  store <8 x i32> %405, <8 x i32>* %292, align 32
  %406 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %402, i32 %4) #8
  store <8 x i32> %406, <8 x i32>* %314, align 32
  %407 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %403, i32 %4) #8
  store <8 x i32> %407, <8 x i32>* %336, align 32
  %408 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %404, i32 %4) #8
  store <8 x i32> %408, <8 x i32>* %358, align 32
  %409 = add <8 x i32> %357, %376
  %410 = add <8 x i32> %335, %376
  %411 = add <8 x i32> %313, %376
  %412 = add <8 x i32> %291, %376
  %413 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %409, i32 %4) #8
  store <8 x i32> %413, <8 x i32>* %359, align 32
  %414 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %410, i32 %4) #8
  store <8 x i32> %414, <8 x i32>* %337, align 32
  %415 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %411, i32 %4) #8
  store <8 x i32> %415, <8 x i32>* %315, align 32
  %416 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %412, i32 %4) #8
  store <8 x i32> %416, <8 x i32>* %293, align 32
  %417 = add <8 x i32> %269, %376
  %418 = load <8 x i32>, <8 x i32>* %249, align 32
  %419 = add <8 x i32> %418, %376
  %420 = load <8 x i32>, <8 x i32>* %227, align 32
  %421 = add <8 x i32> %420, %376
  %422 = load <8 x i32>, <8 x i32>* %205, align 32
  %423 = add <8 x i32> %422, %376
  %424 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %417, i32 %4) #8
  store <8 x i32> %424, <8 x i32>* %271, align 32
  %425 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %419, i32 %4) #8
  store <8 x i32> %425, <8 x i32>* %249, align 32
  %426 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %421, i32 %4) #8
  store <8 x i32> %426, <8 x i32>* %227, align 32
  %427 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %423, i32 %4) #8
  store <8 x i32> %427, <8 x i32>* %205, align 32
  %428 = load <8 x i32>, <8 x i32>* %183, align 32
  %429 = add <8 x i32> %428, %376
  %430 = load <8 x i32>, <8 x i32>* %161, align 32
  %431 = add <8 x i32> %430, %376
  %432 = load <8 x i32>, <8 x i32>* %139, align 32
  %433 = add <8 x i32> %432, %376
  %434 = load <8 x i32>, <8 x i32>* %117, align 32
  %435 = add <8 x i32> %434, %376
  %436 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %429, i32 %4) #8
  store <8 x i32> %436, <8 x i32>* %183, align 32
  %437 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %431, i32 %4) #8
  store <8 x i32> %437, <8 x i32>* %161, align 32
  %438 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %433, i32 %4) #8
  store <8 x i32> %438, <8 x i32>* %139, align 32
  %439 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %435, i32 %4) #8
  store <8 x i32> %439, <8 x i32>* %117, align 32
  %440 = load <8 x i32>, <8 x i32>* %95, align 32
  %441 = add <8 x i32> %440, %376
  %442 = load <8 x i32>, <8 x i32>* %73, align 32
  %443 = add <8 x i32> %442, %376
  %444 = load <8 x i32>, <8 x i32>* %51, align 32
  %445 = add <8 x i32> %444, %376
  %446 = load <8 x i32>, <8 x i32>* %29, align 32
  %447 = add <8 x i32> %446, %376
  %448 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %441, i32 %4) #8
  store <8 x i32> %448, <8 x i32>* %95, align 32
  %449 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %443, i32 %4) #8
  store <8 x i32> %449, <8 x i32>* %73, align 32
  %450 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %445, i32 %4) #8
  store <8 x i32> %450, <8 x i32>* %51, align 32
  %451 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %447, i32 %4) #8
  store <8 x i32> %451, <8 x i32>* %29, align 32
  br label %452

452:                                              ; preds = %361, %372
  br label %453

453:                                              ; preds = %452, %453
  %454 = phi i64 [ %486, %453 ], [ 0, %452 ]
  %455 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 %454
  %456 = bitcast <4 x i64>* %455 to <8 x i32>*
  %457 = load <8 x i32>, <8 x i32>* %456, align 32
  %458 = icmp sgt <8 x i32> %457, %367
  %459 = select <8 x i1> %458, <8 x i32> %457, <8 x i32> %367
  %460 = icmp slt <8 x i32> %459, %370
  %461 = select <8 x i1> %460, <8 x i32> %459, <8 x i32> %370
  store <8 x i32> %461, <8 x i32>* %456, align 32
  %462 = or i64 %454, 1
  %463 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 %462
  %464 = bitcast <4 x i64>* %463 to <8 x i32>*
  %465 = load <8 x i32>, <8 x i32>* %464, align 32
  %466 = icmp sgt <8 x i32> %465, %367
  %467 = select <8 x i1> %466, <8 x i32> %465, <8 x i32> %367
  %468 = icmp slt <8 x i32> %467, %370
  %469 = select <8 x i1> %468, <8 x i32> %467, <8 x i32> %370
  store <8 x i32> %469, <8 x i32>* %464, align 32
  %470 = or i64 %454, 2
  %471 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 %470
  %472 = bitcast <4 x i64>* %471 to <8 x i32>*
  %473 = load <8 x i32>, <8 x i32>* %472, align 32
  %474 = icmp sgt <8 x i32> %473, %367
  %475 = select <8 x i1> %474, <8 x i32> %473, <8 x i32> %367
  %476 = icmp slt <8 x i32> %475, %370
  %477 = select <8 x i1> %476, <8 x i32> %475, <8 x i32> %370
  store <8 x i32> %477, <8 x i32>* %472, align 32
  %478 = or i64 %454, 3
  %479 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 %478
  %480 = bitcast <4 x i64>* %479 to <8 x i32>*
  %481 = load <8 x i32>, <8 x i32>* %480, align 32
  %482 = icmp sgt <8 x i32> %481, %367
  %483 = select <8 x i1> %482, <8 x i32> %481, <8 x i32> %367
  %484 = icmp slt <8 x i32> %483, %370
  %485 = select <8 x i1> %484, <8 x i32> %483, <8 x i32> %370
  store <8 x i32> %485, <8 x i32>* %480, align 32
  %486 = add nuw nsw i64 %454, 4
  %487 = icmp ult i64 %486, 32
  br i1 %487, label %453, label %488

488:                                              ; preds = %453, %7
  ret void
}

; Function Attrs: inlinehint nofree nounwind ssp uwtable
define internal fastcc void @idct64_stage9_avx2(<4 x i64>* nocapture, <4 x i64>* nocapture readonly, <4 x i64>* nocapture readonly, <4 x i64>* nocapture readonly, <4 x i64>* nocapture readonly, <4 x i64>* nocapture readonly, i32) unnamed_addr #7 {
  %8 = bitcast <4 x i64>* %3 to <8 x i32>*
  %9 = bitcast <4 x i64>* %4 to <8 x i32>*
  %10 = bitcast <4 x i64>* %0 to <8 x i32>*
  %11 = load <8 x i32>, <8 x i32>* %10, align 32
  %12 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 15
  %13 = bitcast <4 x i64>* %12 to <8 x i32>*
  %14 = load <8 x i32>, <8 x i32>* %13, align 32
  %15 = add <8 x i32> %14, %11
  %16 = sub <8 x i32> %11, %14
  %17 = load <8 x i32>, <8 x i32>* %8, align 32
  %18 = icmp sgt <8 x i32> %15, %17
  %19 = select <8 x i1> %18, <8 x i32> %15, <8 x i32> %17
  %20 = load <8 x i32>, <8 x i32>* %9, align 32
  %21 = icmp slt <8 x i32> %19, %20
  %22 = select <8 x i1> %21, <8 x i32> %19, <8 x i32> %20
  %23 = icmp sgt <8 x i32> %16, %17
  %24 = select <8 x i1> %23, <8 x i32> %16, <8 x i32> %17
  %25 = icmp slt <8 x i32> %24, %20
  %26 = select <8 x i1> %25, <8 x i32> %24, <8 x i32> %20
  store <8 x i32> %22, <8 x i32>* %10, align 32
  store <8 x i32> %26, <8 x i32>* %13, align 32
  %27 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 1
  %28 = bitcast <4 x i64>* %27 to <8 x i32>*
  %29 = load <8 x i32>, <8 x i32>* %28, align 32
  %30 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 14
  %31 = bitcast <4 x i64>* %30 to <8 x i32>*
  %32 = load <8 x i32>, <8 x i32>* %31, align 32
  %33 = add <8 x i32> %32, %29
  %34 = sub <8 x i32> %29, %32
  %35 = load <8 x i32>, <8 x i32>* %8, align 32
  %36 = icmp sgt <8 x i32> %33, %35
  %37 = select <8 x i1> %36, <8 x i32> %33, <8 x i32> %35
  %38 = load <8 x i32>, <8 x i32>* %9, align 32
  %39 = icmp slt <8 x i32> %37, %38
  %40 = select <8 x i1> %39, <8 x i32> %37, <8 x i32> %38
  %41 = icmp sgt <8 x i32> %34, %35
  %42 = select <8 x i1> %41, <8 x i32> %34, <8 x i32> %35
  %43 = icmp slt <8 x i32> %42, %38
  %44 = select <8 x i1> %43, <8 x i32> %42, <8 x i32> %38
  store <8 x i32> %40, <8 x i32>* %28, align 32
  store <8 x i32> %44, <8 x i32>* %31, align 32
  %45 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 2
  %46 = bitcast <4 x i64>* %45 to <8 x i32>*
  %47 = load <8 x i32>, <8 x i32>* %46, align 32
  %48 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 13
  %49 = bitcast <4 x i64>* %48 to <8 x i32>*
  %50 = load <8 x i32>, <8 x i32>* %49, align 32
  %51 = add <8 x i32> %50, %47
  %52 = sub <8 x i32> %47, %50
  %53 = load <8 x i32>, <8 x i32>* %8, align 32
  %54 = icmp sgt <8 x i32> %51, %53
  %55 = select <8 x i1> %54, <8 x i32> %51, <8 x i32> %53
  %56 = load <8 x i32>, <8 x i32>* %9, align 32
  %57 = icmp slt <8 x i32> %55, %56
  %58 = select <8 x i1> %57, <8 x i32> %55, <8 x i32> %56
  %59 = icmp sgt <8 x i32> %52, %53
  %60 = select <8 x i1> %59, <8 x i32> %52, <8 x i32> %53
  %61 = icmp slt <8 x i32> %60, %56
  %62 = select <8 x i1> %61, <8 x i32> %60, <8 x i32> %56
  store <8 x i32> %58, <8 x i32>* %46, align 32
  store <8 x i32> %62, <8 x i32>* %49, align 32
  %63 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 3
  %64 = bitcast <4 x i64>* %63 to <8 x i32>*
  %65 = load <8 x i32>, <8 x i32>* %64, align 32
  %66 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 12
  %67 = bitcast <4 x i64>* %66 to <8 x i32>*
  %68 = load <8 x i32>, <8 x i32>* %67, align 32
  %69 = add <8 x i32> %68, %65
  %70 = sub <8 x i32> %65, %68
  %71 = load <8 x i32>, <8 x i32>* %8, align 32
  %72 = icmp sgt <8 x i32> %69, %71
  %73 = select <8 x i1> %72, <8 x i32> %69, <8 x i32> %71
  %74 = load <8 x i32>, <8 x i32>* %9, align 32
  %75 = icmp slt <8 x i32> %73, %74
  %76 = select <8 x i1> %75, <8 x i32> %73, <8 x i32> %74
  %77 = icmp sgt <8 x i32> %70, %71
  %78 = select <8 x i1> %77, <8 x i32> %70, <8 x i32> %71
  %79 = icmp slt <8 x i32> %78, %74
  %80 = select <8 x i1> %79, <8 x i32> %78, <8 x i32> %74
  store <8 x i32> %76, <8 x i32>* %64, align 32
  store <8 x i32> %80, <8 x i32>* %67, align 32
  %81 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 4
  %82 = bitcast <4 x i64>* %81 to <8 x i32>*
  %83 = load <8 x i32>, <8 x i32>* %82, align 32
  %84 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 11
  %85 = bitcast <4 x i64>* %84 to <8 x i32>*
  %86 = load <8 x i32>, <8 x i32>* %85, align 32
  %87 = add <8 x i32> %86, %83
  %88 = sub <8 x i32> %83, %86
  %89 = load <8 x i32>, <8 x i32>* %8, align 32
  %90 = icmp sgt <8 x i32> %87, %89
  %91 = select <8 x i1> %90, <8 x i32> %87, <8 x i32> %89
  %92 = load <8 x i32>, <8 x i32>* %9, align 32
  %93 = icmp slt <8 x i32> %91, %92
  %94 = select <8 x i1> %93, <8 x i32> %91, <8 x i32> %92
  %95 = icmp sgt <8 x i32> %88, %89
  %96 = select <8 x i1> %95, <8 x i32> %88, <8 x i32> %89
  %97 = icmp slt <8 x i32> %96, %92
  %98 = select <8 x i1> %97, <8 x i32> %96, <8 x i32> %92
  store <8 x i32> %94, <8 x i32>* %82, align 32
  store <8 x i32> %98, <8 x i32>* %85, align 32
  %99 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 5
  %100 = bitcast <4 x i64>* %99 to <8 x i32>*
  %101 = load <8 x i32>, <8 x i32>* %100, align 32
  %102 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 10
  %103 = bitcast <4 x i64>* %102 to <8 x i32>*
  %104 = load <8 x i32>, <8 x i32>* %103, align 32
  %105 = add <8 x i32> %104, %101
  %106 = sub <8 x i32> %101, %104
  %107 = load <8 x i32>, <8 x i32>* %8, align 32
  %108 = icmp sgt <8 x i32> %105, %107
  %109 = select <8 x i1> %108, <8 x i32> %105, <8 x i32> %107
  %110 = load <8 x i32>, <8 x i32>* %9, align 32
  %111 = icmp slt <8 x i32> %109, %110
  %112 = select <8 x i1> %111, <8 x i32> %109, <8 x i32> %110
  %113 = icmp sgt <8 x i32> %106, %107
  %114 = select <8 x i1> %113, <8 x i32> %106, <8 x i32> %107
  %115 = icmp slt <8 x i32> %114, %110
  %116 = select <8 x i1> %115, <8 x i32> %114, <8 x i32> %110
  store <8 x i32> %112, <8 x i32>* %100, align 32
  store <8 x i32> %116, <8 x i32>* %103, align 32
  %117 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 6
  %118 = bitcast <4 x i64>* %117 to <8 x i32>*
  %119 = load <8 x i32>, <8 x i32>* %118, align 32
  %120 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 9
  %121 = bitcast <4 x i64>* %120 to <8 x i32>*
  %122 = load <8 x i32>, <8 x i32>* %121, align 32
  %123 = add <8 x i32> %122, %119
  %124 = sub <8 x i32> %119, %122
  %125 = load <8 x i32>, <8 x i32>* %8, align 32
  %126 = icmp sgt <8 x i32> %123, %125
  %127 = select <8 x i1> %126, <8 x i32> %123, <8 x i32> %125
  %128 = load <8 x i32>, <8 x i32>* %9, align 32
  %129 = icmp slt <8 x i32> %127, %128
  %130 = select <8 x i1> %129, <8 x i32> %127, <8 x i32> %128
  %131 = icmp sgt <8 x i32> %124, %125
  %132 = select <8 x i1> %131, <8 x i32> %124, <8 x i32> %125
  %133 = icmp slt <8 x i32> %132, %128
  %134 = select <8 x i1> %133, <8 x i32> %132, <8 x i32> %128
  store <8 x i32> %130, <8 x i32>* %118, align 32
  store <8 x i32> %134, <8 x i32>* %121, align 32
  %135 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 7
  %136 = bitcast <4 x i64>* %135 to <8 x i32>*
  %137 = load <8 x i32>, <8 x i32>* %136, align 32
  %138 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 8
  %139 = bitcast <4 x i64>* %138 to <8 x i32>*
  %140 = load <8 x i32>, <8 x i32>* %139, align 32
  %141 = add <8 x i32> %140, %137
  %142 = sub <8 x i32> %137, %140
  %143 = load <8 x i32>, <8 x i32>* %8, align 32
  %144 = icmp sgt <8 x i32> %141, %143
  %145 = select <8 x i1> %144, <8 x i32> %141, <8 x i32> %143
  %146 = load <8 x i32>, <8 x i32>* %9, align 32
  %147 = icmp slt <8 x i32> %145, %146
  %148 = select <8 x i1> %147, <8 x i32> %145, <8 x i32> %146
  %149 = icmp sgt <8 x i32> %142, %143
  %150 = select <8 x i1> %149, <8 x i32> %142, <8 x i32> %143
  %151 = icmp slt <8 x i32> %150, %146
  %152 = select <8 x i1> %151, <8 x i32> %150, <8 x i32> %146
  store <8 x i32> %148, <8 x i32>* %136, align 32
  store <8 x i32> %152, <8 x i32>* %139, align 32
  %153 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 20
  %154 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 27
  %155 = bitcast <4 x i64>* %1 to <8 x i32>*
  %156 = load <8 x i32>, <8 x i32>* %155, align 32
  %157 = bitcast <4 x i64>* %153 to <8 x i32>*
  %158 = load <8 x i32>, <8 x i32>* %157, align 32
  %159 = mul <8 x i32> %158, %156
  %160 = bitcast <4 x i64>* %2 to <8 x i32>*
  %161 = load <8 x i32>, <8 x i32>* %160, align 32
  %162 = bitcast <4 x i64>* %154 to <8 x i32>*
  %163 = load <8 x i32>, <8 x i32>* %162, align 32
  %164 = mul <8 x i32> %163, %161
  %165 = add <8 x i32> %164, %159
  %166 = bitcast <4 x i64>* %5 to <8 x i32>*
  %167 = load <8 x i32>, <8 x i32>* %166, align 32
  %168 = add <8 x i32> %165, %167
  %169 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %168, i32 %6) #8
  %170 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 21
  %171 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 26
  %172 = bitcast <4 x i64>* %170 to <8 x i32>*
  %173 = load <8 x i32>, <8 x i32>* %172, align 32
  %174 = mul <8 x i32> %173, %156
  %175 = bitcast <4 x i64>* %171 to <8 x i32>*
  %176 = load <8 x i32>, <8 x i32>* %175, align 32
  %177 = mul <8 x i32> %176, %161
  %178 = add <8 x i32> %174, %167
  %179 = add <8 x i32> %178, %177
  %180 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %179, i32 %6) #8
  %181 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 22
  %182 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 25
  %183 = bitcast <4 x i64>* %181 to <8 x i32>*
  %184 = load <8 x i32>, <8 x i32>* %183, align 32
  %185 = mul <8 x i32> %184, %156
  %186 = bitcast <4 x i64>* %182 to <8 x i32>*
  %187 = load <8 x i32>, <8 x i32>* %186, align 32
  %188 = mul <8 x i32> %187, %161
  %189 = add <8 x i32> %185, %167
  %190 = add <8 x i32> %189, %188
  %191 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %190, i32 %6) #8
  %192 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 23
  %193 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 24
  %194 = bitcast <4 x i64>* %192 to <8 x i32>*
  %195 = load <8 x i32>, <8 x i32>* %194, align 32
  %196 = mul <8 x i32> %195, %156
  %197 = bitcast <4 x i64>* %193 to <8 x i32>*
  %198 = load <8 x i32>, <8 x i32>* %197, align 32
  %199 = mul <8 x i32> %198, %161
  %200 = add <8 x i32> %199, %167
  %201 = add <8 x i32> %200, %196
  %202 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %201, i32 %6) #8
  %203 = mul <8 x i32> %195, %161
  %204 = add <8 x i32> %200, %203
  %205 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %204, i32 %6) #8
  store <8 x i32> %205, <8 x i32>* %197, align 32
  %206 = load <8 x i32>, <8 x i32>* %160, align 32
  %207 = add <8 x i32> %187, %184
  %208 = mul <8 x i32> %206, %207
  %209 = load <8 x i32>, <8 x i32>* %166, align 32
  %210 = add <8 x i32> %208, %209
  %211 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %210, i32 %6) #8
  store <8 x i32> %211, <8 x i32>* %186, align 32
  %212 = load <8 x i32>, <8 x i32>* %160, align 32
  %213 = add <8 x i32> %176, %173
  %214 = mul <8 x i32> %212, %213
  %215 = load <8 x i32>, <8 x i32>* %166, align 32
  %216 = add <8 x i32> %214, %215
  %217 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %216, i32 %6) #8
  store <8 x i32> %217, <8 x i32>* %175, align 32
  %218 = load <8 x i32>, <8 x i32>* %160, align 32
  %219 = add <8 x i32> %163, %158
  %220 = mul <8 x i32> %218, %219
  %221 = load <8 x i32>, <8 x i32>* %166, align 32
  %222 = add <8 x i32> %220, %221
  %223 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %222, i32 %6) #8
  store <8 x i32> %223, <8 x i32>* %162, align 32
  store <8 x i32> %169, <8 x i32>* %157, align 32
  store <8 x i32> %180, <8 x i32>* %172, align 32
  store <8 x i32> %191, <8 x i32>* %183, align 32
  store <8 x i32> %202, <8 x i32>* %194, align 32
  %224 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 32
  %225 = bitcast <4 x i64>* %224 to <8 x i32>*
  %226 = load <8 x i32>, <8 x i32>* %225, align 32
  %227 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 47
  %228 = bitcast <4 x i64>* %227 to <8 x i32>*
  %229 = load <8 x i32>, <8 x i32>* %228, align 32
  %230 = add <8 x i32> %229, %226
  %231 = sub <8 x i32> %226, %229
  %232 = load <8 x i32>, <8 x i32>* %8, align 32
  %233 = icmp sgt <8 x i32> %230, %232
  %234 = select <8 x i1> %233, <8 x i32> %230, <8 x i32> %232
  %235 = load <8 x i32>, <8 x i32>* %9, align 32
  %236 = icmp slt <8 x i32> %234, %235
  %237 = select <8 x i1> %236, <8 x i32> %234, <8 x i32> %235
  %238 = icmp sgt <8 x i32> %231, %232
  %239 = select <8 x i1> %238, <8 x i32> %231, <8 x i32> %232
  %240 = icmp slt <8 x i32> %239, %235
  %241 = select <8 x i1> %240, <8 x i32> %239, <8 x i32> %235
  store <8 x i32> %237, <8 x i32>* %225, align 32
  store <8 x i32> %241, <8 x i32>* %228, align 32
  %242 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 33
  %243 = bitcast <4 x i64>* %242 to <8 x i32>*
  %244 = load <8 x i32>, <8 x i32>* %243, align 32
  %245 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 46
  %246 = bitcast <4 x i64>* %245 to <8 x i32>*
  %247 = load <8 x i32>, <8 x i32>* %246, align 32
  %248 = add <8 x i32> %247, %244
  %249 = sub <8 x i32> %244, %247
  %250 = load <8 x i32>, <8 x i32>* %8, align 32
  %251 = icmp sgt <8 x i32> %248, %250
  %252 = select <8 x i1> %251, <8 x i32> %248, <8 x i32> %250
  %253 = load <8 x i32>, <8 x i32>* %9, align 32
  %254 = icmp slt <8 x i32> %252, %253
  %255 = select <8 x i1> %254, <8 x i32> %252, <8 x i32> %253
  %256 = icmp sgt <8 x i32> %249, %250
  %257 = select <8 x i1> %256, <8 x i32> %249, <8 x i32> %250
  %258 = icmp slt <8 x i32> %257, %253
  %259 = select <8 x i1> %258, <8 x i32> %257, <8 x i32> %253
  store <8 x i32> %255, <8 x i32>* %243, align 32
  store <8 x i32> %259, <8 x i32>* %246, align 32
  %260 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 34
  %261 = bitcast <4 x i64>* %260 to <8 x i32>*
  %262 = load <8 x i32>, <8 x i32>* %261, align 32
  %263 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 45
  %264 = bitcast <4 x i64>* %263 to <8 x i32>*
  %265 = load <8 x i32>, <8 x i32>* %264, align 32
  %266 = add <8 x i32> %265, %262
  %267 = sub <8 x i32> %262, %265
  %268 = load <8 x i32>, <8 x i32>* %8, align 32
  %269 = icmp sgt <8 x i32> %266, %268
  %270 = select <8 x i1> %269, <8 x i32> %266, <8 x i32> %268
  %271 = load <8 x i32>, <8 x i32>* %9, align 32
  %272 = icmp slt <8 x i32> %270, %271
  %273 = select <8 x i1> %272, <8 x i32> %270, <8 x i32> %271
  %274 = icmp sgt <8 x i32> %267, %268
  %275 = select <8 x i1> %274, <8 x i32> %267, <8 x i32> %268
  %276 = icmp slt <8 x i32> %275, %271
  %277 = select <8 x i1> %276, <8 x i32> %275, <8 x i32> %271
  store <8 x i32> %273, <8 x i32>* %261, align 32
  store <8 x i32> %277, <8 x i32>* %264, align 32
  %278 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 35
  %279 = bitcast <4 x i64>* %278 to <8 x i32>*
  %280 = load <8 x i32>, <8 x i32>* %279, align 32
  %281 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 44
  %282 = bitcast <4 x i64>* %281 to <8 x i32>*
  %283 = load <8 x i32>, <8 x i32>* %282, align 32
  %284 = add <8 x i32> %283, %280
  %285 = sub <8 x i32> %280, %283
  %286 = load <8 x i32>, <8 x i32>* %8, align 32
  %287 = icmp sgt <8 x i32> %284, %286
  %288 = select <8 x i1> %287, <8 x i32> %284, <8 x i32> %286
  %289 = load <8 x i32>, <8 x i32>* %9, align 32
  %290 = icmp slt <8 x i32> %288, %289
  %291 = select <8 x i1> %290, <8 x i32> %288, <8 x i32> %289
  %292 = icmp sgt <8 x i32> %285, %286
  %293 = select <8 x i1> %292, <8 x i32> %285, <8 x i32> %286
  %294 = icmp slt <8 x i32> %293, %289
  %295 = select <8 x i1> %294, <8 x i32> %293, <8 x i32> %289
  store <8 x i32> %291, <8 x i32>* %279, align 32
  store <8 x i32> %295, <8 x i32>* %282, align 32
  %296 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 36
  %297 = bitcast <4 x i64>* %296 to <8 x i32>*
  %298 = load <8 x i32>, <8 x i32>* %297, align 32
  %299 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 43
  %300 = bitcast <4 x i64>* %299 to <8 x i32>*
  %301 = load <8 x i32>, <8 x i32>* %300, align 32
  %302 = add <8 x i32> %301, %298
  %303 = sub <8 x i32> %298, %301
  %304 = load <8 x i32>, <8 x i32>* %8, align 32
  %305 = icmp sgt <8 x i32> %302, %304
  %306 = select <8 x i1> %305, <8 x i32> %302, <8 x i32> %304
  %307 = load <8 x i32>, <8 x i32>* %9, align 32
  %308 = icmp slt <8 x i32> %306, %307
  %309 = select <8 x i1> %308, <8 x i32> %306, <8 x i32> %307
  %310 = icmp sgt <8 x i32> %303, %304
  %311 = select <8 x i1> %310, <8 x i32> %303, <8 x i32> %304
  %312 = icmp slt <8 x i32> %311, %307
  %313 = select <8 x i1> %312, <8 x i32> %311, <8 x i32> %307
  store <8 x i32> %309, <8 x i32>* %297, align 32
  store <8 x i32> %313, <8 x i32>* %300, align 32
  %314 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 37
  %315 = bitcast <4 x i64>* %314 to <8 x i32>*
  %316 = load <8 x i32>, <8 x i32>* %315, align 32
  %317 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 42
  %318 = bitcast <4 x i64>* %317 to <8 x i32>*
  %319 = load <8 x i32>, <8 x i32>* %318, align 32
  %320 = add <8 x i32> %319, %316
  %321 = sub <8 x i32> %316, %319
  %322 = load <8 x i32>, <8 x i32>* %8, align 32
  %323 = icmp sgt <8 x i32> %320, %322
  %324 = select <8 x i1> %323, <8 x i32> %320, <8 x i32> %322
  %325 = load <8 x i32>, <8 x i32>* %9, align 32
  %326 = icmp slt <8 x i32> %324, %325
  %327 = select <8 x i1> %326, <8 x i32> %324, <8 x i32> %325
  %328 = icmp sgt <8 x i32> %321, %322
  %329 = select <8 x i1> %328, <8 x i32> %321, <8 x i32> %322
  %330 = icmp slt <8 x i32> %329, %325
  %331 = select <8 x i1> %330, <8 x i32> %329, <8 x i32> %325
  store <8 x i32> %327, <8 x i32>* %315, align 32
  store <8 x i32> %331, <8 x i32>* %318, align 32
  %332 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 38
  %333 = bitcast <4 x i64>* %332 to <8 x i32>*
  %334 = load <8 x i32>, <8 x i32>* %333, align 32
  %335 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 41
  %336 = bitcast <4 x i64>* %335 to <8 x i32>*
  %337 = load <8 x i32>, <8 x i32>* %336, align 32
  %338 = add <8 x i32> %337, %334
  %339 = sub <8 x i32> %334, %337
  %340 = load <8 x i32>, <8 x i32>* %8, align 32
  %341 = icmp sgt <8 x i32> %338, %340
  %342 = select <8 x i1> %341, <8 x i32> %338, <8 x i32> %340
  %343 = load <8 x i32>, <8 x i32>* %9, align 32
  %344 = icmp slt <8 x i32> %342, %343
  %345 = select <8 x i1> %344, <8 x i32> %342, <8 x i32> %343
  %346 = icmp sgt <8 x i32> %339, %340
  %347 = select <8 x i1> %346, <8 x i32> %339, <8 x i32> %340
  %348 = icmp slt <8 x i32> %347, %343
  %349 = select <8 x i1> %348, <8 x i32> %347, <8 x i32> %343
  store <8 x i32> %345, <8 x i32>* %333, align 32
  store <8 x i32> %349, <8 x i32>* %336, align 32
  %350 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 39
  %351 = bitcast <4 x i64>* %350 to <8 x i32>*
  %352 = load <8 x i32>, <8 x i32>* %351, align 32
  %353 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 40
  %354 = bitcast <4 x i64>* %353 to <8 x i32>*
  %355 = load <8 x i32>, <8 x i32>* %354, align 32
  %356 = add <8 x i32> %355, %352
  %357 = sub <8 x i32> %352, %355
  %358 = load <8 x i32>, <8 x i32>* %8, align 32
  %359 = icmp sgt <8 x i32> %356, %358
  %360 = select <8 x i1> %359, <8 x i32> %356, <8 x i32> %358
  %361 = load <8 x i32>, <8 x i32>* %9, align 32
  %362 = icmp slt <8 x i32> %360, %361
  %363 = select <8 x i1> %362, <8 x i32> %360, <8 x i32> %361
  %364 = icmp sgt <8 x i32> %357, %358
  %365 = select <8 x i1> %364, <8 x i32> %357, <8 x i32> %358
  %366 = icmp slt <8 x i32> %365, %361
  %367 = select <8 x i1> %366, <8 x i32> %365, <8 x i32> %361
  store <8 x i32> %363, <8 x i32>* %351, align 32
  store <8 x i32> %367, <8 x i32>* %354, align 32
  %368 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 63
  %369 = bitcast <4 x i64>* %368 to <8 x i32>*
  %370 = load <8 x i32>, <8 x i32>* %369, align 32
  %371 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 48
  %372 = bitcast <4 x i64>* %371 to <8 x i32>*
  %373 = load <8 x i32>, <8 x i32>* %372, align 32
  %374 = add <8 x i32> %373, %370
  %375 = sub <8 x i32> %370, %373
  %376 = load <8 x i32>, <8 x i32>* %8, align 32
  %377 = icmp sgt <8 x i32> %374, %376
  %378 = select <8 x i1> %377, <8 x i32> %374, <8 x i32> %376
  %379 = load <8 x i32>, <8 x i32>* %9, align 32
  %380 = icmp slt <8 x i32> %378, %379
  %381 = select <8 x i1> %380, <8 x i32> %378, <8 x i32> %379
  %382 = icmp sgt <8 x i32> %375, %376
  %383 = select <8 x i1> %382, <8 x i32> %375, <8 x i32> %376
  %384 = icmp slt <8 x i32> %383, %379
  %385 = select <8 x i1> %384, <8 x i32> %383, <8 x i32> %379
  store <8 x i32> %381, <8 x i32>* %369, align 32
  store <8 x i32> %385, <8 x i32>* %372, align 32
  %386 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 62
  %387 = bitcast <4 x i64>* %386 to <8 x i32>*
  %388 = load <8 x i32>, <8 x i32>* %387, align 32
  %389 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 49
  %390 = bitcast <4 x i64>* %389 to <8 x i32>*
  %391 = load <8 x i32>, <8 x i32>* %390, align 32
  %392 = add <8 x i32> %391, %388
  %393 = sub <8 x i32> %388, %391
  %394 = load <8 x i32>, <8 x i32>* %8, align 32
  %395 = icmp sgt <8 x i32> %392, %394
  %396 = select <8 x i1> %395, <8 x i32> %392, <8 x i32> %394
  %397 = load <8 x i32>, <8 x i32>* %9, align 32
  %398 = icmp slt <8 x i32> %396, %397
  %399 = select <8 x i1> %398, <8 x i32> %396, <8 x i32> %397
  %400 = icmp sgt <8 x i32> %393, %394
  %401 = select <8 x i1> %400, <8 x i32> %393, <8 x i32> %394
  %402 = icmp slt <8 x i32> %401, %397
  %403 = select <8 x i1> %402, <8 x i32> %401, <8 x i32> %397
  store <8 x i32> %399, <8 x i32>* %387, align 32
  store <8 x i32> %403, <8 x i32>* %390, align 32
  %404 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 61
  %405 = bitcast <4 x i64>* %404 to <8 x i32>*
  %406 = load <8 x i32>, <8 x i32>* %405, align 32
  %407 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 50
  %408 = bitcast <4 x i64>* %407 to <8 x i32>*
  %409 = load <8 x i32>, <8 x i32>* %408, align 32
  %410 = add <8 x i32> %409, %406
  %411 = sub <8 x i32> %406, %409
  %412 = load <8 x i32>, <8 x i32>* %8, align 32
  %413 = icmp sgt <8 x i32> %410, %412
  %414 = select <8 x i1> %413, <8 x i32> %410, <8 x i32> %412
  %415 = load <8 x i32>, <8 x i32>* %9, align 32
  %416 = icmp slt <8 x i32> %414, %415
  %417 = select <8 x i1> %416, <8 x i32> %414, <8 x i32> %415
  %418 = icmp sgt <8 x i32> %411, %412
  %419 = select <8 x i1> %418, <8 x i32> %411, <8 x i32> %412
  %420 = icmp slt <8 x i32> %419, %415
  %421 = select <8 x i1> %420, <8 x i32> %419, <8 x i32> %415
  store <8 x i32> %417, <8 x i32>* %405, align 32
  store <8 x i32> %421, <8 x i32>* %408, align 32
  %422 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 60
  %423 = bitcast <4 x i64>* %422 to <8 x i32>*
  %424 = load <8 x i32>, <8 x i32>* %423, align 32
  %425 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 51
  %426 = bitcast <4 x i64>* %425 to <8 x i32>*
  %427 = load <8 x i32>, <8 x i32>* %426, align 32
  %428 = add <8 x i32> %427, %424
  %429 = sub <8 x i32> %424, %427
  %430 = load <8 x i32>, <8 x i32>* %8, align 32
  %431 = icmp sgt <8 x i32> %428, %430
  %432 = select <8 x i1> %431, <8 x i32> %428, <8 x i32> %430
  %433 = load <8 x i32>, <8 x i32>* %9, align 32
  %434 = icmp slt <8 x i32> %432, %433
  %435 = select <8 x i1> %434, <8 x i32> %432, <8 x i32> %433
  %436 = icmp sgt <8 x i32> %429, %430
  %437 = select <8 x i1> %436, <8 x i32> %429, <8 x i32> %430
  %438 = icmp slt <8 x i32> %437, %433
  %439 = select <8 x i1> %438, <8 x i32> %437, <8 x i32> %433
  store <8 x i32> %435, <8 x i32>* %423, align 32
  store <8 x i32> %439, <8 x i32>* %426, align 32
  %440 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 59
  %441 = bitcast <4 x i64>* %440 to <8 x i32>*
  %442 = load <8 x i32>, <8 x i32>* %441, align 32
  %443 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 52
  %444 = bitcast <4 x i64>* %443 to <8 x i32>*
  %445 = load <8 x i32>, <8 x i32>* %444, align 32
  %446 = add <8 x i32> %445, %442
  %447 = sub <8 x i32> %442, %445
  %448 = load <8 x i32>, <8 x i32>* %8, align 32
  %449 = icmp sgt <8 x i32> %446, %448
  %450 = select <8 x i1> %449, <8 x i32> %446, <8 x i32> %448
  %451 = load <8 x i32>, <8 x i32>* %9, align 32
  %452 = icmp slt <8 x i32> %450, %451
  %453 = select <8 x i1> %452, <8 x i32> %450, <8 x i32> %451
  %454 = icmp sgt <8 x i32> %447, %448
  %455 = select <8 x i1> %454, <8 x i32> %447, <8 x i32> %448
  %456 = icmp slt <8 x i32> %455, %451
  %457 = select <8 x i1> %456, <8 x i32> %455, <8 x i32> %451
  store <8 x i32> %453, <8 x i32>* %441, align 32
  store <8 x i32> %457, <8 x i32>* %444, align 32
  %458 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 58
  %459 = bitcast <4 x i64>* %458 to <8 x i32>*
  %460 = load <8 x i32>, <8 x i32>* %459, align 32
  %461 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 53
  %462 = bitcast <4 x i64>* %461 to <8 x i32>*
  %463 = load <8 x i32>, <8 x i32>* %462, align 32
  %464 = add <8 x i32> %463, %460
  %465 = sub <8 x i32> %460, %463
  %466 = load <8 x i32>, <8 x i32>* %8, align 32
  %467 = icmp sgt <8 x i32> %464, %466
  %468 = select <8 x i1> %467, <8 x i32> %464, <8 x i32> %466
  %469 = load <8 x i32>, <8 x i32>* %9, align 32
  %470 = icmp slt <8 x i32> %468, %469
  %471 = select <8 x i1> %470, <8 x i32> %468, <8 x i32> %469
  %472 = icmp sgt <8 x i32> %465, %466
  %473 = select <8 x i1> %472, <8 x i32> %465, <8 x i32> %466
  %474 = icmp slt <8 x i32> %473, %469
  %475 = select <8 x i1> %474, <8 x i32> %473, <8 x i32> %469
  store <8 x i32> %471, <8 x i32>* %459, align 32
  store <8 x i32> %475, <8 x i32>* %462, align 32
  %476 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 57
  %477 = bitcast <4 x i64>* %476 to <8 x i32>*
  %478 = load <8 x i32>, <8 x i32>* %477, align 32
  %479 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 54
  %480 = bitcast <4 x i64>* %479 to <8 x i32>*
  %481 = load <8 x i32>, <8 x i32>* %480, align 32
  %482 = add <8 x i32> %481, %478
  %483 = sub <8 x i32> %478, %481
  %484 = load <8 x i32>, <8 x i32>* %8, align 32
  %485 = icmp sgt <8 x i32> %482, %484
  %486 = select <8 x i1> %485, <8 x i32> %482, <8 x i32> %484
  %487 = load <8 x i32>, <8 x i32>* %9, align 32
  %488 = icmp slt <8 x i32> %486, %487
  %489 = select <8 x i1> %488, <8 x i32> %486, <8 x i32> %487
  %490 = icmp sgt <8 x i32> %483, %484
  %491 = select <8 x i1> %490, <8 x i32> %483, <8 x i32> %484
  %492 = icmp slt <8 x i32> %491, %487
  %493 = select <8 x i1> %492, <8 x i32> %491, <8 x i32> %487
  store <8 x i32> %489, <8 x i32>* %477, align 32
  store <8 x i32> %493, <8 x i32>* %480, align 32
  %494 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 56
  %495 = bitcast <4 x i64>* %494 to <8 x i32>*
  %496 = load <8 x i32>, <8 x i32>* %495, align 32
  %497 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 55
  %498 = bitcast <4 x i64>* %497 to <8 x i32>*
  %499 = load <8 x i32>, <8 x i32>* %498, align 32
  %500 = add <8 x i32> %499, %496
  %501 = sub <8 x i32> %496, %499
  %502 = load <8 x i32>, <8 x i32>* %8, align 32
  %503 = icmp sgt <8 x i32> %500, %502
  %504 = select <8 x i1> %503, <8 x i32> %500, <8 x i32> %502
  %505 = load <8 x i32>, <8 x i32>* %9, align 32
  %506 = icmp slt <8 x i32> %504, %505
  %507 = select <8 x i1> %506, <8 x i32> %504, <8 x i32> %505
  %508 = icmp sgt <8 x i32> %501, %502
  %509 = select <8 x i1> %508, <8 x i32> %501, <8 x i32> %502
  %510 = icmp slt <8 x i32> %509, %505
  %511 = select <8 x i1> %510, <8 x i32> %509, <8 x i32> %505
  store <8 x i32> %507, <8 x i32>* %495, align 32
  store <8 x i32> %511, <8 x i32>* %498, align 32
  ret void
}

; Function Attrs: inlinehint nounwind ssp uwtable
define internal fastcc void @idct64_stage11_avx2(<4 x i64>* nocapture readonly, <4 x i64>*, i32, i32, i32, <4 x i64>* nocapture readonly, <4 x i64>* nocapture readonly) unnamed_addr #6 {
  %8 = bitcast <4 x i64>* %5 to <8 x i32>*
  %9 = bitcast <4 x i64>* %6 to <8 x i32>*
  br label %12

10:                                               ; preds = %12
  %11 = icmp eq i32 %2, 0
  br i1 %11, label %39, label %410

12:                                               ; preds = %12, %7
  %13 = phi i64 [ 0, %7 ], [ %37, %12 ]
  %14 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 %13
  %15 = bitcast <4 x i64>* %14 to <8 x i32>*
  %16 = load <8 x i32>, <8 x i32>* %15, align 32
  %17 = sub nuw nsw i64 63, %13
  %18 = getelementptr inbounds <4 x i64>, <4 x i64>* %0, i64 %17
  %19 = bitcast <4 x i64>* %18 to <8 x i32>*
  %20 = load <8 x i32>, <8 x i32>* %19, align 32
  %21 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 %13
  %22 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 %17
  %23 = add <8 x i32> %20, %16
  %24 = sub <8 x i32> %16, %20
  %25 = load <8 x i32>, <8 x i32>* %8, align 32
  %26 = icmp sgt <8 x i32> %23, %25
  %27 = select <8 x i1> %26, <8 x i32> %23, <8 x i32> %25
  %28 = load <8 x i32>, <8 x i32>* %9, align 32
  %29 = icmp slt <8 x i32> %27, %28
  %30 = select <8 x i1> %29, <8 x i32> %27, <8 x i32> %28
  %31 = icmp sgt <8 x i32> %24, %25
  %32 = select <8 x i1> %31, <8 x i32> %24, <8 x i32> %25
  %33 = icmp slt <8 x i32> %32, %28
  %34 = select <8 x i1> %33, <8 x i32> %32, <8 x i32> %28
  %35 = bitcast <4 x i64>* %21 to <8 x i32>*
  store <8 x i32> %30, <8 x i32>* %35, align 32
  %36 = bitcast <4 x i64>* %22 to <8 x i32>*
  store <8 x i32> %34, <8 x i32>* %36, align 32
  %37 = add nuw nsw i64 %13, 1
  %38 = icmp eq i64 %37, 32
  br i1 %38, label %10, label %12

39:                                               ; preds = %10
  %40 = icmp sgt i32 %3, 10
  %41 = select i1 %40, i32 %3, i32 10
  %42 = shl i32 32, %41
  %43 = sub nsw i32 0, %42
  %44 = insertelement <8 x i32> undef, i32 %43, i32 0
  %45 = shufflevector <8 x i32> %44, <8 x i32> undef, <8 x i32> zeroinitializer
  %46 = add nsw i32 %42, -1
  %47 = insertelement <8 x i32> undef, i32 %46, i32 0
  %48 = shufflevector <8 x i32> %47, <8 x i32> undef, <8 x i32> zeroinitializer
  %49 = icmp eq i32 %4, 0
  br i1 %49, label %374, label %50

50:                                               ; preds = %39
  %51 = add nsw i32 %4, -1
  %52 = shl i32 1, %51
  %53 = insertelement <8 x i32> undef, i32 %52, i32 0
  %54 = shufflevector <8 x i32> %53, <8 x i32> undef, <8 x i32> zeroinitializer
  %55 = bitcast <4 x i64>* %1 to <8 x i32>*
  %56 = load <8 x i32>, <8 x i32>* %55, align 32
  %57 = add <8 x i32> %56, %54
  %58 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 1
  %59 = bitcast <4 x i64>* %58 to <8 x i32>*
  %60 = load <8 x i32>, <8 x i32>* %59, align 32
  %61 = add <8 x i32> %60, %54
  %62 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 2
  %63 = bitcast <4 x i64>* %62 to <8 x i32>*
  %64 = load <8 x i32>, <8 x i32>* %63, align 32
  %65 = add <8 x i32> %64, %54
  %66 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 3
  %67 = bitcast <4 x i64>* %66 to <8 x i32>*
  %68 = load <8 x i32>, <8 x i32>* %67, align 32
  %69 = add <8 x i32> %68, %54
  %70 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %57, i32 %4) #8
  store <8 x i32> %70, <8 x i32>* %55, align 32
  %71 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %61, i32 %4) #8
  store <8 x i32> %71, <8 x i32>* %59, align 32
  %72 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %65, i32 %4) #8
  store <8 x i32> %72, <8 x i32>* %63, align 32
  %73 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %69, i32 %4) #8
  store <8 x i32> %73, <8 x i32>* %67, align 32
  %74 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 4
  %75 = bitcast <4 x i64>* %74 to <8 x i32>*
  %76 = load <8 x i32>, <8 x i32>* %75, align 32
  %77 = add <8 x i32> %76, %54
  %78 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 5
  %79 = bitcast <4 x i64>* %78 to <8 x i32>*
  %80 = load <8 x i32>, <8 x i32>* %79, align 32
  %81 = add <8 x i32> %80, %54
  %82 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 6
  %83 = bitcast <4 x i64>* %82 to <8 x i32>*
  %84 = load <8 x i32>, <8 x i32>* %83, align 32
  %85 = add <8 x i32> %84, %54
  %86 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 7
  %87 = bitcast <4 x i64>* %86 to <8 x i32>*
  %88 = load <8 x i32>, <8 x i32>* %87, align 32
  %89 = add <8 x i32> %88, %54
  %90 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %77, i32 %4) #8
  store <8 x i32> %90, <8 x i32>* %75, align 32
  %91 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %81, i32 %4) #8
  store <8 x i32> %91, <8 x i32>* %79, align 32
  %92 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %85, i32 %4) #8
  store <8 x i32> %92, <8 x i32>* %83, align 32
  %93 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %89, i32 %4) #8
  store <8 x i32> %93, <8 x i32>* %87, align 32
  %94 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 8
  %95 = bitcast <4 x i64>* %94 to <8 x i32>*
  %96 = load <8 x i32>, <8 x i32>* %95, align 32
  %97 = add <8 x i32> %96, %54
  %98 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 9
  %99 = bitcast <4 x i64>* %98 to <8 x i32>*
  %100 = load <8 x i32>, <8 x i32>* %99, align 32
  %101 = add <8 x i32> %100, %54
  %102 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 10
  %103 = bitcast <4 x i64>* %102 to <8 x i32>*
  %104 = load <8 x i32>, <8 x i32>* %103, align 32
  %105 = add <8 x i32> %104, %54
  %106 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 11
  %107 = bitcast <4 x i64>* %106 to <8 x i32>*
  %108 = load <8 x i32>, <8 x i32>* %107, align 32
  %109 = add <8 x i32> %108, %54
  %110 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %97, i32 %4) #8
  store <8 x i32> %110, <8 x i32>* %95, align 32
  %111 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %101, i32 %4) #8
  store <8 x i32> %111, <8 x i32>* %99, align 32
  %112 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %105, i32 %4) #8
  store <8 x i32> %112, <8 x i32>* %103, align 32
  %113 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %109, i32 %4) #8
  store <8 x i32> %113, <8 x i32>* %107, align 32
  %114 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 12
  %115 = bitcast <4 x i64>* %114 to <8 x i32>*
  %116 = load <8 x i32>, <8 x i32>* %115, align 32
  %117 = add <8 x i32> %116, %54
  %118 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 13
  %119 = bitcast <4 x i64>* %118 to <8 x i32>*
  %120 = load <8 x i32>, <8 x i32>* %119, align 32
  %121 = add <8 x i32> %120, %54
  %122 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 14
  %123 = bitcast <4 x i64>* %122 to <8 x i32>*
  %124 = load <8 x i32>, <8 x i32>* %123, align 32
  %125 = add <8 x i32> %124, %54
  %126 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 15
  %127 = bitcast <4 x i64>* %126 to <8 x i32>*
  %128 = load <8 x i32>, <8 x i32>* %127, align 32
  %129 = add <8 x i32> %128, %54
  %130 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %117, i32 %4) #8
  store <8 x i32> %130, <8 x i32>* %115, align 32
  %131 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %121, i32 %4) #8
  store <8 x i32> %131, <8 x i32>* %119, align 32
  %132 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %125, i32 %4) #8
  store <8 x i32> %132, <8 x i32>* %123, align 32
  %133 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %129, i32 %4) #8
  store <8 x i32> %133, <8 x i32>* %127, align 32
  %134 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 16
  %135 = bitcast <4 x i64>* %134 to <8 x i32>*
  %136 = load <8 x i32>, <8 x i32>* %135, align 32
  %137 = add <8 x i32> %136, %54
  %138 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 17
  %139 = bitcast <4 x i64>* %138 to <8 x i32>*
  %140 = load <8 x i32>, <8 x i32>* %139, align 32
  %141 = add <8 x i32> %140, %54
  %142 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 18
  %143 = bitcast <4 x i64>* %142 to <8 x i32>*
  %144 = load <8 x i32>, <8 x i32>* %143, align 32
  %145 = add <8 x i32> %144, %54
  %146 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 19
  %147 = bitcast <4 x i64>* %146 to <8 x i32>*
  %148 = load <8 x i32>, <8 x i32>* %147, align 32
  %149 = add <8 x i32> %148, %54
  %150 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %137, i32 %4) #8
  store <8 x i32> %150, <8 x i32>* %135, align 32
  %151 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %141, i32 %4) #8
  store <8 x i32> %151, <8 x i32>* %139, align 32
  %152 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %145, i32 %4) #8
  store <8 x i32> %152, <8 x i32>* %143, align 32
  %153 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %149, i32 %4) #8
  store <8 x i32> %153, <8 x i32>* %147, align 32
  %154 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 20
  %155 = bitcast <4 x i64>* %154 to <8 x i32>*
  %156 = load <8 x i32>, <8 x i32>* %155, align 32
  %157 = add <8 x i32> %156, %54
  %158 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 21
  %159 = bitcast <4 x i64>* %158 to <8 x i32>*
  %160 = load <8 x i32>, <8 x i32>* %159, align 32
  %161 = add <8 x i32> %160, %54
  %162 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 22
  %163 = bitcast <4 x i64>* %162 to <8 x i32>*
  %164 = load <8 x i32>, <8 x i32>* %163, align 32
  %165 = add <8 x i32> %164, %54
  %166 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 23
  %167 = bitcast <4 x i64>* %166 to <8 x i32>*
  %168 = load <8 x i32>, <8 x i32>* %167, align 32
  %169 = add <8 x i32> %168, %54
  %170 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %157, i32 %4) #8
  store <8 x i32> %170, <8 x i32>* %155, align 32
  %171 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %161, i32 %4) #8
  store <8 x i32> %171, <8 x i32>* %159, align 32
  %172 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %165, i32 %4) #8
  store <8 x i32> %172, <8 x i32>* %163, align 32
  %173 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %169, i32 %4) #8
  store <8 x i32> %173, <8 x i32>* %167, align 32
  %174 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 24
  %175 = bitcast <4 x i64>* %174 to <8 x i32>*
  %176 = load <8 x i32>, <8 x i32>* %175, align 32
  %177 = add <8 x i32> %176, %54
  %178 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 25
  %179 = bitcast <4 x i64>* %178 to <8 x i32>*
  %180 = load <8 x i32>, <8 x i32>* %179, align 32
  %181 = add <8 x i32> %180, %54
  %182 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 26
  %183 = bitcast <4 x i64>* %182 to <8 x i32>*
  %184 = load <8 x i32>, <8 x i32>* %183, align 32
  %185 = add <8 x i32> %184, %54
  %186 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 27
  %187 = bitcast <4 x i64>* %186 to <8 x i32>*
  %188 = load <8 x i32>, <8 x i32>* %187, align 32
  %189 = add <8 x i32> %188, %54
  %190 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %177, i32 %4) #8
  store <8 x i32> %190, <8 x i32>* %175, align 32
  %191 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %181, i32 %4) #8
  store <8 x i32> %191, <8 x i32>* %179, align 32
  %192 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %185, i32 %4) #8
  store <8 x i32> %192, <8 x i32>* %183, align 32
  %193 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %189, i32 %4) #8
  store <8 x i32> %193, <8 x i32>* %187, align 32
  %194 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 28
  %195 = bitcast <4 x i64>* %194 to <8 x i32>*
  %196 = load <8 x i32>, <8 x i32>* %195, align 32
  %197 = add <8 x i32> %196, %54
  %198 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 29
  %199 = bitcast <4 x i64>* %198 to <8 x i32>*
  %200 = load <8 x i32>, <8 x i32>* %199, align 32
  %201 = add <8 x i32> %200, %54
  %202 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 30
  %203 = bitcast <4 x i64>* %202 to <8 x i32>*
  %204 = load <8 x i32>, <8 x i32>* %203, align 32
  %205 = add <8 x i32> %204, %54
  %206 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 31
  %207 = bitcast <4 x i64>* %206 to <8 x i32>*
  %208 = load <8 x i32>, <8 x i32>* %207, align 32
  %209 = add <8 x i32> %208, %54
  %210 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %197, i32 %4) #8
  store <8 x i32> %210, <8 x i32>* %195, align 32
  %211 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %201, i32 %4) #8
  store <8 x i32> %211, <8 x i32>* %199, align 32
  %212 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %205, i32 %4) #8
  store <8 x i32> %212, <8 x i32>* %203, align 32
  %213 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %209, i32 %4) #8
  store <8 x i32> %213, <8 x i32>* %207, align 32
  %214 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 32
  %215 = bitcast <4 x i64>* %214 to <8 x i32>*
  %216 = load <8 x i32>, <8 x i32>* %215, align 32
  %217 = add <8 x i32> %216, %54
  %218 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 33
  %219 = bitcast <4 x i64>* %218 to <8 x i32>*
  %220 = load <8 x i32>, <8 x i32>* %219, align 32
  %221 = add <8 x i32> %220, %54
  %222 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 34
  %223 = bitcast <4 x i64>* %222 to <8 x i32>*
  %224 = load <8 x i32>, <8 x i32>* %223, align 32
  %225 = add <8 x i32> %224, %54
  %226 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 35
  %227 = bitcast <4 x i64>* %226 to <8 x i32>*
  %228 = load <8 x i32>, <8 x i32>* %227, align 32
  %229 = add <8 x i32> %228, %54
  %230 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %217, i32 %4) #8
  store <8 x i32> %230, <8 x i32>* %215, align 32
  %231 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %221, i32 %4) #8
  store <8 x i32> %231, <8 x i32>* %219, align 32
  %232 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %225, i32 %4) #8
  store <8 x i32> %232, <8 x i32>* %223, align 32
  %233 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %229, i32 %4) #8
  store <8 x i32> %233, <8 x i32>* %227, align 32
  %234 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 36
  %235 = bitcast <4 x i64>* %234 to <8 x i32>*
  %236 = load <8 x i32>, <8 x i32>* %235, align 32
  %237 = add <8 x i32> %236, %54
  %238 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 37
  %239 = bitcast <4 x i64>* %238 to <8 x i32>*
  %240 = load <8 x i32>, <8 x i32>* %239, align 32
  %241 = add <8 x i32> %240, %54
  %242 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 38
  %243 = bitcast <4 x i64>* %242 to <8 x i32>*
  %244 = load <8 x i32>, <8 x i32>* %243, align 32
  %245 = add <8 x i32> %244, %54
  %246 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 39
  %247 = bitcast <4 x i64>* %246 to <8 x i32>*
  %248 = load <8 x i32>, <8 x i32>* %247, align 32
  %249 = add <8 x i32> %248, %54
  %250 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %237, i32 %4) #8
  store <8 x i32> %250, <8 x i32>* %235, align 32
  %251 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %241, i32 %4) #8
  store <8 x i32> %251, <8 x i32>* %239, align 32
  %252 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %245, i32 %4) #8
  store <8 x i32> %252, <8 x i32>* %243, align 32
  %253 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %249, i32 %4) #8
  store <8 x i32> %253, <8 x i32>* %247, align 32
  %254 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 40
  %255 = bitcast <4 x i64>* %254 to <8 x i32>*
  %256 = load <8 x i32>, <8 x i32>* %255, align 32
  %257 = add <8 x i32> %256, %54
  %258 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 41
  %259 = bitcast <4 x i64>* %258 to <8 x i32>*
  %260 = load <8 x i32>, <8 x i32>* %259, align 32
  %261 = add <8 x i32> %260, %54
  %262 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 42
  %263 = bitcast <4 x i64>* %262 to <8 x i32>*
  %264 = load <8 x i32>, <8 x i32>* %263, align 32
  %265 = add <8 x i32> %264, %54
  %266 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 43
  %267 = bitcast <4 x i64>* %266 to <8 x i32>*
  %268 = load <8 x i32>, <8 x i32>* %267, align 32
  %269 = add <8 x i32> %268, %54
  %270 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %257, i32 %4) #8
  store <8 x i32> %270, <8 x i32>* %255, align 32
  %271 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %261, i32 %4) #8
  store <8 x i32> %271, <8 x i32>* %259, align 32
  %272 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %265, i32 %4) #8
  store <8 x i32> %272, <8 x i32>* %263, align 32
  %273 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %269, i32 %4) #8
  store <8 x i32> %273, <8 x i32>* %267, align 32
  %274 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 44
  %275 = bitcast <4 x i64>* %274 to <8 x i32>*
  %276 = load <8 x i32>, <8 x i32>* %275, align 32
  %277 = add <8 x i32> %276, %54
  %278 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 45
  %279 = bitcast <4 x i64>* %278 to <8 x i32>*
  %280 = load <8 x i32>, <8 x i32>* %279, align 32
  %281 = add <8 x i32> %280, %54
  %282 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 46
  %283 = bitcast <4 x i64>* %282 to <8 x i32>*
  %284 = load <8 x i32>, <8 x i32>* %283, align 32
  %285 = add <8 x i32> %284, %54
  %286 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 47
  %287 = bitcast <4 x i64>* %286 to <8 x i32>*
  %288 = load <8 x i32>, <8 x i32>* %287, align 32
  %289 = add <8 x i32> %288, %54
  %290 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %277, i32 %4) #8
  store <8 x i32> %290, <8 x i32>* %275, align 32
  %291 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %281, i32 %4) #8
  store <8 x i32> %291, <8 x i32>* %279, align 32
  %292 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %285, i32 %4) #8
  store <8 x i32> %292, <8 x i32>* %283, align 32
  %293 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %289, i32 %4) #8
  store <8 x i32> %293, <8 x i32>* %287, align 32
  %294 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 48
  %295 = bitcast <4 x i64>* %294 to <8 x i32>*
  %296 = load <8 x i32>, <8 x i32>* %295, align 32
  %297 = add <8 x i32> %296, %54
  %298 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 49
  %299 = bitcast <4 x i64>* %298 to <8 x i32>*
  %300 = load <8 x i32>, <8 x i32>* %299, align 32
  %301 = add <8 x i32> %300, %54
  %302 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 50
  %303 = bitcast <4 x i64>* %302 to <8 x i32>*
  %304 = load <8 x i32>, <8 x i32>* %303, align 32
  %305 = add <8 x i32> %304, %54
  %306 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 51
  %307 = bitcast <4 x i64>* %306 to <8 x i32>*
  %308 = load <8 x i32>, <8 x i32>* %307, align 32
  %309 = add <8 x i32> %308, %54
  %310 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %297, i32 %4) #8
  store <8 x i32> %310, <8 x i32>* %295, align 32
  %311 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %301, i32 %4) #8
  store <8 x i32> %311, <8 x i32>* %299, align 32
  %312 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %305, i32 %4) #8
  store <8 x i32> %312, <8 x i32>* %303, align 32
  %313 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %309, i32 %4) #8
  store <8 x i32> %313, <8 x i32>* %307, align 32
  %314 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 52
  %315 = bitcast <4 x i64>* %314 to <8 x i32>*
  %316 = load <8 x i32>, <8 x i32>* %315, align 32
  %317 = add <8 x i32> %316, %54
  %318 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 53
  %319 = bitcast <4 x i64>* %318 to <8 x i32>*
  %320 = load <8 x i32>, <8 x i32>* %319, align 32
  %321 = add <8 x i32> %320, %54
  %322 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 54
  %323 = bitcast <4 x i64>* %322 to <8 x i32>*
  %324 = load <8 x i32>, <8 x i32>* %323, align 32
  %325 = add <8 x i32> %324, %54
  %326 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 55
  %327 = bitcast <4 x i64>* %326 to <8 x i32>*
  %328 = load <8 x i32>, <8 x i32>* %327, align 32
  %329 = add <8 x i32> %328, %54
  %330 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %317, i32 %4) #8
  store <8 x i32> %330, <8 x i32>* %315, align 32
  %331 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %321, i32 %4) #8
  store <8 x i32> %331, <8 x i32>* %319, align 32
  %332 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %325, i32 %4) #8
  store <8 x i32> %332, <8 x i32>* %323, align 32
  %333 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %329, i32 %4) #8
  store <8 x i32> %333, <8 x i32>* %327, align 32
  %334 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 56
  %335 = bitcast <4 x i64>* %334 to <8 x i32>*
  %336 = load <8 x i32>, <8 x i32>* %335, align 32
  %337 = add <8 x i32> %336, %54
  %338 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 57
  %339 = bitcast <4 x i64>* %338 to <8 x i32>*
  %340 = load <8 x i32>, <8 x i32>* %339, align 32
  %341 = add <8 x i32> %340, %54
  %342 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 58
  %343 = bitcast <4 x i64>* %342 to <8 x i32>*
  %344 = load <8 x i32>, <8 x i32>* %343, align 32
  %345 = add <8 x i32> %344, %54
  %346 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 59
  %347 = bitcast <4 x i64>* %346 to <8 x i32>*
  %348 = load <8 x i32>, <8 x i32>* %347, align 32
  %349 = add <8 x i32> %348, %54
  %350 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %337, i32 %4) #8
  store <8 x i32> %350, <8 x i32>* %335, align 32
  %351 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %341, i32 %4) #8
  store <8 x i32> %351, <8 x i32>* %339, align 32
  %352 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %345, i32 %4) #8
  store <8 x i32> %352, <8 x i32>* %343, align 32
  %353 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %349, i32 %4) #8
  store <8 x i32> %353, <8 x i32>* %347, align 32
  %354 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 60
  %355 = bitcast <4 x i64>* %354 to <8 x i32>*
  %356 = load <8 x i32>, <8 x i32>* %355, align 32
  %357 = add <8 x i32> %356, %54
  %358 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 61
  %359 = bitcast <4 x i64>* %358 to <8 x i32>*
  %360 = load <8 x i32>, <8 x i32>* %359, align 32
  %361 = add <8 x i32> %360, %54
  %362 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 62
  %363 = bitcast <4 x i64>* %362 to <8 x i32>*
  %364 = load <8 x i32>, <8 x i32>* %363, align 32
  %365 = add <8 x i32> %364, %54
  %366 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 63
  %367 = bitcast <4 x i64>* %366 to <8 x i32>*
  %368 = load <8 x i32>, <8 x i32>* %367, align 32
  %369 = add <8 x i32> %368, %54
  %370 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %357, i32 %4) #8
  store <8 x i32> %370, <8 x i32>* %355, align 32
  %371 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %361, i32 %4) #8
  store <8 x i32> %371, <8 x i32>* %359, align 32
  %372 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %365, i32 %4) #8
  store <8 x i32> %372, <8 x i32>* %363, align 32
  %373 = tail call <8 x i32> @llvm.x86.avx2.psrai.d(<8 x i32> %369, i32 %4) #8
  store <8 x i32> %373, <8 x i32>* %367, align 32
  br label %374

374:                                              ; preds = %39, %50
  br label %375

375:                                              ; preds = %374, %375
  %376 = phi i64 [ %408, %375 ], [ 0, %374 ]
  %377 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 %376
  %378 = bitcast <4 x i64>* %377 to <8 x i32>*
  %379 = load <8 x i32>, <8 x i32>* %378, align 32
  %380 = icmp sgt <8 x i32> %379, %45
  %381 = select <8 x i1> %380, <8 x i32> %379, <8 x i32> %45
  %382 = icmp slt <8 x i32> %381, %48
  %383 = select <8 x i1> %382, <8 x i32> %381, <8 x i32> %48
  store <8 x i32> %383, <8 x i32>* %378, align 32
  %384 = or i64 %376, 1
  %385 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 %384
  %386 = bitcast <4 x i64>* %385 to <8 x i32>*
  %387 = load <8 x i32>, <8 x i32>* %386, align 32
  %388 = icmp sgt <8 x i32> %387, %45
  %389 = select <8 x i1> %388, <8 x i32> %387, <8 x i32> %45
  %390 = icmp slt <8 x i32> %389, %48
  %391 = select <8 x i1> %390, <8 x i32> %389, <8 x i32> %48
  store <8 x i32> %391, <8 x i32>* %386, align 32
  %392 = or i64 %376, 2
  %393 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 %392
  %394 = bitcast <4 x i64>* %393 to <8 x i32>*
  %395 = load <8 x i32>, <8 x i32>* %394, align 32
  %396 = icmp sgt <8 x i32> %395, %45
  %397 = select <8 x i1> %396, <8 x i32> %395, <8 x i32> %45
  %398 = icmp slt <8 x i32> %397, %48
  %399 = select <8 x i1> %398, <8 x i32> %397, <8 x i32> %48
  store <8 x i32> %399, <8 x i32>* %394, align 32
  %400 = or i64 %376, 3
  %401 = getelementptr inbounds <4 x i64>, <4 x i64>* %1, i64 %400
  %402 = bitcast <4 x i64>* %401 to <8 x i32>*
  %403 = load <8 x i32>, <8 x i32>* %402, align 32
  %404 = icmp sgt <8 x i32> %403, %45
  %405 = select <8 x i1> %404, <8 x i32> %403, <8 x i32> %45
  %406 = icmp slt <8 x i32> %405, %48
  %407 = select <8 x i1> %406, <8 x i32> %405, <8 x i32> %48
  store <8 x i32> %407, <8 x i32>* %402, align 32
  %408 = add nuw nsw i64 %376, 4
  %409 = icmp ult i64 %408, 64
  br i1 %409, label %375, label %410

410:                                              ; preds = %375, %10
  ret void
}

; Function Attrs: nounwind readnone
declare <8 x i32> @llvm.x86.avx2.pslli.d(<8 x i32>, i32) #5

; Function Attrs: nounwind readnone
declare <16 x i16> @llvm.x86.avx2.packusdw(<8 x i32>, <8 x i32>) #5

; Function Attrs: nounwind readnone
declare <16 x i16> @llvm.x86.avx2.pslli.w(<16 x i16>, i32) #5

; Function Attrs: argmemonly nounwind
declare void @llvm.memcpy.p0i8.p0i8.i64(i8* nocapture writeonly, i8* nocapture readonly, i64, i1 immarg) #3

attributes #0 = { nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="256" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+avx,+avx2,+cx8,+fxsr,+mmx,+popcnt,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+avx,+avx2,+cx8,+fxsr,+mmx,+popcnt,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #2 = { nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+avx,+avx2,+cx8,+fxsr,+mmx,+popcnt,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #3 = { argmemonly nounwind }
attributes #4 = { nofree nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="256" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+avx,+avx2,+cx8,+fxsr,+mmx,+popcnt,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #5 = { nounwind readnone }
attributes #6 = { inlinehint nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="256" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+avx,+avx2,+cx8,+fxsr,+mmx,+popcnt,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #7 = { inlinehint nofree nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="256" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+avx,+avx2,+cx8,+fxsr,+mmx,+popcnt,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #8 = { nounwind }

!llvm.module.flags = !{!0, !1}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 2}
!2 = distinct !{!2, !3}
!3 = !{!"llvm.loop.unroll.disable"}
!4 = distinct !{!4, !3}
