; ModuleID = '../../third_party/libaom/source/libaom/av1/common/cdef_block_ssse3.c'
source_filename = "../../third_party/libaom/source/libaom/av1/common/cdef_block_ssse3.c"
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

module asm ".symver exp, exp@GLIBC_2.2.5"
module asm ".symver exp2, exp2@GLIBC_2.2.5"
module asm ".symver exp2f, exp2f@GLIBC_2.2.5"
module asm ".symver expf, expf@GLIBC_2.2.5"
module asm ".symver lgamma, lgamma@GLIBC_2.2.5"
module asm ".symver lgammaf, lgammaf@GLIBC_2.2.5"
module asm ".symver lgammal, lgammal@GLIBC_2.2.5"
module asm ".symver log, log@GLIBC_2.2.5"
module asm ".symver log2, log2@GLIBC_2.2.5"
module asm ".symver log2f, log2f@GLIBC_2.2.5"
module asm ".symver logf, logf@GLIBC_2.2.5"
module asm ".symver pow, pow@GLIBC_2.2.5"
module asm ".symver powf, powf@GLIBC_2.2.5"

@cdef_directions = external local_unnamed_addr constant [8 x [2 x i32]], align 16
@cdef_pri_taps = external local_unnamed_addr constant [2 x [2 x i32]], align 16
@cdef_sec_taps = external local_unnamed_addr constant [2 x i32], align 4

; Function Attrs: nounwind ssp uwtable
define hidden i32 @cdef_find_dir_ssse3(i16*, i32, i32* nocapture, i32) local_unnamed_addr #0 {
  %5 = alloca [8 x i32], align 16
  %6 = alloca [8 x <2 x i64>], align 16
  %7 = bitcast [8 x i32]* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %7) #8
  %8 = getelementptr inbounds [8 x i32], [8 x i32]* %5, i64 0, i64 4
  %9 = bitcast [8 x <2 x i64>]* %6 to i8*
  %10 = bitcast [8 x i32]* %5 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %10, i8 -86, i64 32, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 128, i8* nonnull %9) #8
  %11 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %3, i32 0
  %12 = bitcast <4 x i32> %11 to <8 x i16>
  %13 = sext i32 %1 to i64
  %14 = bitcast i16* %0 to i8*
  %15 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %14) #8
  %16 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 0
  %17 = bitcast <16 x i8> %15 to <8 x i16>
  %18 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %17, <8 x i16> %12) #8
  %19 = add <8 x i16> %18, <i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128>
  %20 = bitcast [8 x <2 x i64>]* %6 to <8 x i16>*
  store <8 x i16> %19, <8 x i16>* %20, align 16
  %21 = getelementptr inbounds i16, i16* %0, i64 %13
  %22 = bitcast i16* %21 to i8*
  %23 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %22) #8
  %24 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 1
  %25 = bitcast <16 x i8> %23 to <8 x i16>
  %26 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %25, <8 x i16> %12) #8
  %27 = add <8 x i16> %26, <i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128>
  %28 = bitcast <2 x i64>* %24 to <8 x i16>*
  store <8 x i16> %27, <8 x i16>* %28, align 16
  %29 = shl nsw i64 %13, 1
  %30 = getelementptr inbounds i16, i16* %0, i64 %29
  %31 = bitcast i16* %30 to i8*
  %32 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %31) #8
  %33 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 2
  %34 = bitcast <16 x i8> %32 to <8 x i16>
  %35 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %34, <8 x i16> %12) #8
  %36 = add <8 x i16> %35, <i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128>
  %37 = bitcast <2 x i64>* %33 to <8 x i16>*
  store <8 x i16> %36, <8 x i16>* %37, align 16
  %38 = mul nsw i64 %13, 3
  %39 = getelementptr inbounds i16, i16* %0, i64 %38
  %40 = bitcast i16* %39 to i8*
  %41 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %40) #8
  %42 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 3
  %43 = bitcast <16 x i8> %41 to <8 x i16>
  %44 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %43, <8 x i16> %12) #8
  %45 = add <8 x i16> %44, <i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128>
  %46 = bitcast <2 x i64>* %42 to <8 x i16>*
  store <8 x i16> %45, <8 x i16>* %46, align 16
  %47 = shl nsw i64 %13, 2
  %48 = getelementptr inbounds i16, i16* %0, i64 %47
  %49 = bitcast i16* %48 to i8*
  %50 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %49) #8
  %51 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 4
  %52 = bitcast <16 x i8> %50 to <8 x i16>
  %53 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %52, <8 x i16> %12) #8
  %54 = add <8 x i16> %53, <i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128>
  %55 = bitcast <2 x i64>* %51 to <8 x i16>*
  store <8 x i16> %54, <8 x i16>* %55, align 16
  %56 = mul nsw i64 %13, 5
  %57 = getelementptr inbounds i16, i16* %0, i64 %56
  %58 = bitcast i16* %57 to i8*
  %59 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %58) #8
  %60 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 5
  %61 = bitcast <16 x i8> %59 to <8 x i16>
  %62 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %61, <8 x i16> %12) #8
  %63 = add <8 x i16> %62, <i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128>
  %64 = bitcast <2 x i64>* %60 to <8 x i16>*
  store <8 x i16> %63, <8 x i16>* %64, align 16
  %65 = mul nsw i64 %13, 6
  %66 = getelementptr inbounds i16, i16* %0, i64 %65
  %67 = bitcast i16* %66 to i8*
  %68 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %67) #8
  %69 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 6
  %70 = bitcast <16 x i8> %68 to <8 x i16>
  %71 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %70, <8 x i16> %12) #8
  %72 = add <8 x i16> %71, <i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128>
  %73 = bitcast <2 x i64>* %69 to <8 x i16>*
  store <8 x i16> %72, <8 x i16>* %73, align 16
  %74 = mul nsw i64 %13, 7
  %75 = getelementptr inbounds i16, i16* %0, i64 %74
  %76 = bitcast i16* %75 to i8*
  %77 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %76) #8
  %78 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 7
  %79 = bitcast <16 x i8> %77 to <8 x i16>
  %80 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %79, <8 x i16> %12) #8
  %81 = add <8 x i16> %80, <i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128>
  %82 = bitcast <2 x i64>* %78 to <8 x i16>*
  store <8 x i16> %81, <8 x i16>* %82, align 16
  %83 = bitcast <8 x i16> %19 to <16 x i8>
  %84 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %83, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %85 = shufflevector <16 x i8> %83, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %86 = bitcast <8 x i16> %27 to <16 x i8>
  %87 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %86, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19>
  %88 = bitcast <16 x i8> %84 to <8 x i16>
  %89 = bitcast <16 x i8> %87 to <8 x i16>
  %90 = add <8 x i16> %89, %88
  %91 = shufflevector <16 x i8> %86, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19>
  %92 = bitcast <16 x i8> %85 to <8 x i16>
  %93 = bitcast <16 x i8> %91 to <8 x i16>
  %94 = add <8 x i16> %93, %92
  %95 = add <8 x i16> %27, %19
  %96 = bitcast <8 x i16> %95 to <16 x i8>
  %97 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %96, <16 x i32> <i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21>
  %98 = shufflevector <16 x i8> %96, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21>
  %99 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %96, <16 x i32> <i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27>
  %100 = shufflevector <16 x i8> %96, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27>
  %101 = bitcast <8 x i16> %36 to <16 x i8>
  %102 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %101, <16 x i32> <i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21>
  %103 = bitcast <16 x i8> %102 to <8 x i16>
  %104 = add <8 x i16> %90, %103
  %105 = shufflevector <16 x i8> %101, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21>
  %106 = bitcast <16 x i8> %105 to <8 x i16>
  %107 = add <8 x i16> %94, %106
  %108 = bitcast <8 x i16> %45 to <16 x i8>
  %109 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %108, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %110 = bitcast <16 x i8> %109 to <8 x i16>
  %111 = add <8 x i16> %104, %110
  %112 = shufflevector <16 x i8> %108, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %113 = bitcast <16 x i8> %112 to <8 x i16>
  %114 = add <8 x i16> %107, %113
  %115 = add <8 x i16> %45, %36
  %116 = bitcast <8 x i16> %115 to <16 x i8>
  %117 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %116, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %118 = bitcast <16 x i8> %97 to <8 x i16>
  %119 = bitcast <16 x i8> %117 to <8 x i16>
  %120 = add <8 x i16> %119, %118
  %121 = shufflevector <16 x i8> %116, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %122 = bitcast <16 x i8> %98 to <8 x i16>
  %123 = bitcast <16 x i8> %121 to <8 x i16>
  %124 = add <8 x i16> %123, %122
  %125 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %116, <16 x i32> <i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25>
  %126 = bitcast <16 x i8> %99 to <8 x i16>
  %127 = bitcast <16 x i8> %125 to <8 x i16>
  %128 = add <8 x i16> %127, %126
  %129 = shufflevector <16 x i8> %116, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25>
  %130 = bitcast <16 x i8> %100 to <8 x i16>
  %131 = bitcast <16 x i8> %129 to <8 x i16>
  %132 = add <8 x i16> %131, %130
  %133 = add <8 x i16> %115, %95
  %134 = bitcast <8 x i16> %54 to <16 x i8>
  %135 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %134, <16 x i32> <i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25>
  %136 = bitcast <16 x i8> %135 to <8 x i16>
  %137 = add <8 x i16> %111, %136
  %138 = shufflevector <16 x i8> %134, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25>
  %139 = bitcast <16 x i8> %138 to <8 x i16>
  %140 = add <8 x i16> %114, %139
  %141 = bitcast <8 x i16> %63 to <16 x i8>
  %142 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %141, <16 x i32> <i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27>
  %143 = bitcast <16 x i8> %142 to <8 x i16>
  %144 = add <8 x i16> %137, %143
  %145 = shufflevector <16 x i8> %141, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27>
  %146 = bitcast <16 x i8> %145 to <8 x i16>
  %147 = add <8 x i16> %140, %146
  %148 = add <8 x i16> %63, %54
  %149 = bitcast <8 x i16> %148 to <16 x i8>
  %150 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %149, <16 x i32> <i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25>
  %151 = bitcast <16 x i8> %150 to <8 x i16>
  %152 = add <8 x i16> %120, %151
  %153 = shufflevector <16 x i8> %149, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25>
  %154 = bitcast <16 x i8> %153 to <8 x i16>
  %155 = add <8 x i16> %124, %154
  %156 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %149, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %157 = bitcast <16 x i8> %156 to <8 x i16>
  %158 = add <8 x i16> %128, %157
  %159 = shufflevector <16 x i8> %149, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %160 = bitcast <16 x i8> %159 to <8 x i16>
  %161 = add <8 x i16> %132, %160
  %162 = add <8 x i16> %133, %148
  %163 = bitcast <2 x i64>* %69 to <16 x i8>*
  %164 = load <16 x i8>, <16 x i8>* %163, align 16
  %165 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0>, <16 x i8> %164, <16 x i32> <i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29>
  %166 = bitcast <16 x i8> %165 to <8 x i16>
  %167 = shufflevector <16 x i8> %164, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef>, <16 x i32> <i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29>
  %168 = bitcast <16 x i8> %167 to <8 x i16>
  %169 = add <8 x i16> %147, %168
  %170 = load <8 x i16>, <8 x i16>* %82, align 16
  %171 = add <8 x i16> %144, %170
  %172 = add <8 x i16> %171, %166
  %173 = bitcast <16 x i8> %164 to <8 x i16>
  %174 = add <8 x i16> %170, %173
  %175 = bitcast <8 x i16> %174 to <16 x i8>
  %176 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %175, <16 x i32> <i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27>
  %177 = bitcast <16 x i8> %176 to <8 x i16>
  %178 = add <8 x i16> %152, %177
  %179 = shufflevector <16 x i8> %175, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27>
  %180 = bitcast <16 x i8> %179 to <8 x i16>
  %181 = add <8 x i16> %155, %180
  %182 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %175, <16 x i32> <i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21>
  %183 = bitcast <16 x i8> %182 to <8 x i16>
  %184 = add <8 x i16> %158, %183
  %185 = shufflevector <16 x i8> %175, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21>
  %186 = bitcast <16 x i8> %185 to <8 x i16>
  %187 = add <8 x i16> %161, %186
  %188 = add <8 x i16> %162, %174
  %189 = bitcast <8 x i16> %169 to <16 x i8>
  %190 = shufflevector <16 x i8> %189, <16 x i8> undef, <16 x i32> <i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15>
  %191 = bitcast <16 x i8> %190 to <8 x i16>
  %192 = shufflevector <8 x i16> %172, <8 x i16> %191, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %193 = shufflevector <8 x i16> %172, <8 x i16> %191, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %194 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %192, <8 x i16> %192) #8
  %195 = bitcast <4 x i32> %194 to <2 x i64>
  %196 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %193, <8 x i16> %193) #8
  %197 = bitcast <4 x i32> %196 to <2 x i64>
  %198 = and <2 x i64> %195, <i64 4294967295, i64 4294967295>
  %199 = mul nuw nsw <2 x i64> %198, <i64 840, i64 280>
  %200 = bitcast <2 x i64> %199 to <4 x i32>
  %201 = shufflevector <4 x i32> %200, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %202 = bitcast <4 x i32> %194 to <16 x i8>
  %203 = shufflevector <16 x i8> %202, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19>
  %204 = bitcast <16 x i8> %203 to <2 x i64>
  %205 = and <2 x i64> %204, <i64 4294967295, i64 4294967295>
  %206 = mul nuw nsw <2 x i64> %205, <i64 420, i64 210>
  %207 = bitcast <2 x i64> %206 to <4 x i32>
  %208 = shufflevector <4 x i32> %207, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %209 = shufflevector <4 x i32> %201, <4 x i32> %208, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %210 = and <2 x i64> %197, <i64 4294967295, i64 4294967295>
  %211 = mul nuw nsw <2 x i64> %210, <i64 168, i64 120>
  %212 = bitcast <2 x i64> %211 to <4 x i32>
  %213 = shufflevector <4 x i32> %212, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %214 = bitcast <4 x i32> %196 to <16 x i8>
  %215 = shufflevector <16 x i8> %214, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19>
  %216 = bitcast <16 x i8> %215 to <2 x i64>
  %217 = and <2 x i64> %216, <i64 4294967295, i64 4294967295>
  %218 = mul nuw nsw <2 x i64> %217, <i64 140, i64 105>
  %219 = bitcast <2 x i64> %218 to <4 x i32>
  %220 = shufflevector <4 x i32> %219, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %221 = shufflevector <4 x i32> %213, <4 x i32> %220, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %222 = add <4 x i32> %221, %209
  %223 = bitcast <8 x i16> %187 to <16 x i8>
  %224 = shufflevector <16 x i8> %223, <16 x i8> undef, <16 x i32> <i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15>
  %225 = bitcast <16 x i8> %224 to <8 x i16>
  %226 = shufflevector <8 x i16> %184, <8 x i16> %225, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %227 = shufflevector <8 x i16> %184, <8 x i16> %225, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %228 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %226, <8 x i16> %226) #8
  %229 = bitcast <4 x i32> %228 to <2 x i64>
  %230 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %227, <8 x i16> %227) #8
  %231 = bitcast <4 x i32> %230 to <2 x i64>
  %232 = and <2 x i64> %229, <i64 4294967295, i64 4294967295>
  %233 = mul nuw nsw <2 x i64> %232, <i64 0, i64 420>
  %234 = bitcast <2 x i64> %233 to <4 x i32>
  %235 = shufflevector <4 x i32> %234, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %236 = bitcast <4 x i32> %228 to <16 x i8>
  %237 = shufflevector <16 x i8> %236, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19>
  %238 = bitcast <16 x i8> %237 to <2 x i64>
  %239 = and <2 x i64> %238, <i64 4294967295, i64 4294967295>
  %240 = mul nuw nsw <2 x i64> %239, <i64 0, i64 210>
  %241 = bitcast <2 x i64> %240 to <4 x i32>
  %242 = shufflevector <4 x i32> %241, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %243 = shufflevector <4 x i32> %235, <4 x i32> %242, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %244 = and <2 x i64> %231, <i64 4294967295, i64 4294967295>
  %245 = mul nuw nsw <2 x i64> %244, <i64 140, i64 105>
  %246 = bitcast <2 x i64> %245 to <4 x i32>
  %247 = shufflevector <4 x i32> %246, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %248 = bitcast <4 x i32> %230 to <16 x i8>
  %249 = shufflevector <16 x i8> %248, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19>
  %250 = bitcast <16 x i8> %249 to <2 x i64>
  %251 = and <2 x i64> %250, <i64 4294967295, i64 4294967295>
  %252 = mul nuw nsw <2 x i64> %251, <i64 105, i64 105>
  %253 = bitcast <2 x i64> %252 to <4 x i32>
  %254 = shufflevector <4 x i32> %253, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %255 = shufflevector <4 x i32> %247, <4 x i32> %254, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %256 = add <4 x i32> %255, %243
  %257 = bitcast <8 x i16> %181 to <16 x i8>
  %258 = shufflevector <16 x i8> %257, <16 x i8> undef, <16 x i32> <i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15>
  %259 = bitcast <16 x i8> %258 to <8 x i16>
  %260 = shufflevector <8 x i16> %178, <8 x i16> %259, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %261 = shufflevector <8 x i16> %178, <8 x i16> %259, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %262 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %260, <8 x i16> %260) #8
  %263 = bitcast <4 x i32> %262 to <2 x i64>
  %264 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %261, <8 x i16> %261) #8
  %265 = bitcast <4 x i32> %264 to <2 x i64>
  %266 = and <2 x i64> %263, <i64 4294967295, i64 4294967295>
  %267 = mul nuw nsw <2 x i64> %266, <i64 0, i64 420>
  %268 = bitcast <2 x i64> %267 to <4 x i32>
  %269 = shufflevector <4 x i32> %268, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %270 = bitcast <4 x i32> %262 to <16 x i8>
  %271 = shufflevector <16 x i8> %270, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19>
  %272 = bitcast <16 x i8> %271 to <2 x i64>
  %273 = and <2 x i64> %272, <i64 4294967295, i64 4294967295>
  %274 = mul nuw nsw <2 x i64> %273, <i64 0, i64 210>
  %275 = bitcast <2 x i64> %274 to <4 x i32>
  %276 = shufflevector <4 x i32> %275, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %277 = shufflevector <4 x i32> %269, <4 x i32> %276, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %278 = and <2 x i64> %265, <i64 4294967295, i64 4294967295>
  %279 = mul nuw nsw <2 x i64> %278, <i64 140, i64 105>
  %280 = bitcast <2 x i64> %279 to <4 x i32>
  %281 = shufflevector <4 x i32> %280, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %282 = bitcast <4 x i32> %264 to <16 x i8>
  %283 = shufflevector <16 x i8> %282, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19>
  %284 = bitcast <16 x i8> %283 to <2 x i64>
  %285 = and <2 x i64> %284, <i64 4294967295, i64 4294967295>
  %286 = mul nuw nsw <2 x i64> %285, <i64 105, i64 105>
  %287 = bitcast <2 x i64> %286 to <4 x i32>
  %288 = shufflevector <4 x i32> %287, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %289 = shufflevector <4 x i32> %281, <4 x i32> %288, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %290 = add <4 x i32> %289, %277
  %291 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %188, <8 x i16> %188) #8
  %292 = bitcast <4 x i32> %291 to <2 x i64>
  %293 = and <2 x i64> %292, <i64 4294967295, i64 4294967295>
  %294 = mul nuw nsw <2 x i64> %293, <i64 105, i64 105>
  %295 = bitcast <2 x i64> %294 to <4 x i32>
  %296 = shufflevector <4 x i32> %295, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %297 = bitcast <4 x i32> %291 to <16 x i8>
  %298 = shufflevector <16 x i8> %297, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19>
  %299 = bitcast <16 x i8> %298 to <2 x i64>
  %300 = and <2 x i64> %299, <i64 4294967295, i64 4294967295>
  %301 = mul nuw nsw <2 x i64> %300, <i64 105, i64 105>
  %302 = bitcast <2 x i64> %301 to <4 x i32>
  %303 = shufflevector <4 x i32> %302, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %304 = shufflevector <4 x i32> %296, <4 x i32> %303, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %305 = shufflevector <4 x i32> %222, <4 x i32> %290, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %306 = bitcast <4 x i32> %305 to <2 x i64>
  %307 = shufflevector <4 x i32> %304, <4 x i32> %256, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %308 = bitcast <4 x i32> %307 to <2 x i64>
  %309 = shufflevector <4 x i32> %222, <4 x i32> %290, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %310 = bitcast <4 x i32> %309 to <2 x i64>
  %311 = shufflevector <4 x i32> %304, <4 x i32> %256, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %312 = bitcast <4 x i32> %311 to <2 x i64>
  %313 = shufflevector <2 x i64> %306, <2 x i64> %308, <2 x i32> <i32 0, i32 2>
  %314 = shufflevector <2 x i64> %306, <2 x i64> %308, <2 x i32> <i32 1, i32 3>
  %315 = shufflevector <2 x i64> %310, <2 x i64> %312, <2 x i32> <i32 0, i32 2>
  %316 = shufflevector <2 x i64> %310, <2 x i64> %312, <2 x i32> <i32 1, i32 3>
  %317 = bitcast <2 x i64> %313 to <4 x i32>
  %318 = bitcast <2 x i64> %314 to <4 x i32>
  %319 = bitcast <2 x i64> %315 to <4 x i32>
  %320 = bitcast <2 x i64> %316 to <4 x i32>
  %321 = add <4 x i32> %320, %319
  %322 = add <4 x i32> %321, %317
  %323 = add <4 x i32> %322, %318
  %324 = bitcast <4 x i32> %323 to <2 x i64>
  %325 = bitcast i32* %8 to <4 x i32>*
  store <4 x i32> %323, <4 x i32>* %325, align 16
  %326 = load <8 x i16>, <8 x i16>* %28, align 16
  %327 = bitcast [8 x <2 x i64>]* %6 to <8 x i16>*
  %328 = load <8 x i16>, <8 x i16>* %327, align 16
  %329 = shufflevector <8 x i16> %328, <8 x i16> %326, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %330 = load <8 x i16>, <8 x i16>* %46, align 16
  %331 = load <8 x i16>, <8 x i16>* %37, align 16
  %332 = shufflevector <8 x i16> %331, <8 x i16> %330, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %333 = shufflevector <8 x i16> %328, <8 x i16> %326, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %334 = shufflevector <8 x i16> %331, <8 x i16> %330, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %335 = load <8 x i16>, <8 x i16>* %64, align 16
  %336 = load <8 x i16>, <8 x i16>* %55, align 16
  %337 = shufflevector <8 x i16> %336, <8 x i16> %335, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %338 = load <8 x i16>, <8 x i16>* %73, align 16
  %339 = shufflevector <8 x i16> %338, <8 x i16> %170, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %340 = shufflevector <8 x i16> %336, <8 x i16> %335, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %341 = shufflevector <8 x i16> %338, <8 x i16> %170, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %342 = bitcast <8 x i16> %329 to <4 x i32>
  %343 = bitcast <8 x i16> %332 to <4 x i32>
  %344 = shufflevector <4 x i32> %342, <4 x i32> %343, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %345 = bitcast <4 x i32> %344 to <2 x i64>
  %346 = bitcast <8 x i16> %337 to <4 x i32>
  %347 = bitcast <8 x i16> %339 to <4 x i32>
  %348 = shufflevector <4 x i32> %346, <4 x i32> %347, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %349 = bitcast <4 x i32> %348 to <2 x i64>
  %350 = shufflevector <4 x i32> %342, <4 x i32> %343, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %351 = bitcast <4 x i32> %350 to <2 x i64>
  %352 = shufflevector <4 x i32> %346, <4 x i32> %347, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %353 = bitcast <4 x i32> %352 to <2 x i64>
  %354 = bitcast <8 x i16> %333 to <4 x i32>
  %355 = bitcast <8 x i16> %334 to <4 x i32>
  %356 = shufflevector <4 x i32> %354, <4 x i32> %355, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %357 = bitcast <4 x i32> %356 to <2 x i64>
  %358 = bitcast <8 x i16> %340 to <4 x i32>
  %359 = bitcast <8 x i16> %341 to <4 x i32>
  %360 = shufflevector <4 x i32> %358, <4 x i32> %359, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %361 = bitcast <4 x i32> %360 to <2 x i64>
  %362 = shufflevector <4 x i32> %354, <4 x i32> %355, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %363 = bitcast <4 x i32> %362 to <2 x i64>
  %364 = shufflevector <4 x i32> %358, <4 x i32> %359, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %365 = bitcast <4 x i32> %364 to <2 x i64>
  %366 = shufflevector <2 x i64> %345, <2 x i64> %349, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %366, <2 x i64>* %78, align 16
  %367 = shufflevector <2 x i64> %345, <2 x i64> %349, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %367, <2 x i64>* %69, align 16
  %368 = shufflevector <2 x i64> %351, <2 x i64> %353, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %368, <2 x i64>* %60, align 16
  %369 = shufflevector <2 x i64> %351, <2 x i64> %353, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %369, <2 x i64>* %51, align 16
  %370 = shufflevector <2 x i64> %357, <2 x i64> %361, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %370, <2 x i64>* %42, align 16
  %371 = shufflevector <2 x i64> %357, <2 x i64> %361, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %371, <2 x i64>* %33, align 16
  %372 = shufflevector <2 x i64> %363, <2 x i64> %365, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %372, <2 x i64>* %24, align 16
  %373 = shufflevector <2 x i64> %363, <2 x i64> %365, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %373, <2 x i64>* %16, align 16
  %374 = bitcast <2 x i64> %373 to <16 x i8>
  %375 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %374, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %376 = shufflevector <16 x i8> %374, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %377 = bitcast <2 x i64> %372 to <16 x i8>
  %378 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %377, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19>
  %379 = bitcast <16 x i8> %375 to <8 x i16>
  %380 = bitcast <16 x i8> %378 to <8 x i16>
  %381 = add <8 x i16> %380, %379
  %382 = shufflevector <16 x i8> %377, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19>
  %383 = bitcast <16 x i8> %376 to <8 x i16>
  %384 = bitcast <16 x i8> %382 to <8 x i16>
  %385 = add <8 x i16> %384, %383
  %386 = bitcast <2 x i64> %373 to <8 x i16>
  %387 = bitcast <2 x i64> %372 to <8 x i16>
  %388 = add <8 x i16> %387, %386
  %389 = bitcast <8 x i16> %388 to <16 x i8>
  %390 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %389, <16 x i32> <i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21>
  %391 = shufflevector <16 x i8> %389, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21>
  %392 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %389, <16 x i32> <i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27>
  %393 = shufflevector <16 x i8> %389, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27>
  %394 = bitcast <2 x i64> %371 to <16 x i8>
  %395 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %394, <16 x i32> <i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21>
  %396 = bitcast <16 x i8> %395 to <8 x i16>
  %397 = add <8 x i16> %381, %396
  %398 = shufflevector <16 x i8> %394, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21>
  %399 = bitcast <16 x i8> %398 to <8 x i16>
  %400 = add <8 x i16> %385, %399
  %401 = bitcast <2 x i64> %370 to <16 x i8>
  %402 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %401, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %403 = bitcast <16 x i8> %402 to <8 x i16>
  %404 = add <8 x i16> %397, %403
  %405 = shufflevector <16 x i8> %401, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %406 = bitcast <16 x i8> %405 to <8 x i16>
  %407 = add <8 x i16> %400, %406
  %408 = bitcast <2 x i64> %371 to <8 x i16>
  %409 = bitcast <2 x i64> %370 to <8 x i16>
  %410 = add <8 x i16> %409, %408
  %411 = bitcast <8 x i16> %410 to <16 x i8>
  %412 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %411, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %413 = bitcast <16 x i8> %390 to <8 x i16>
  %414 = bitcast <16 x i8> %412 to <8 x i16>
  %415 = add <8 x i16> %414, %413
  %416 = shufflevector <16 x i8> %411, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %417 = bitcast <16 x i8> %391 to <8 x i16>
  %418 = bitcast <16 x i8> %416 to <8 x i16>
  %419 = add <8 x i16> %418, %417
  %420 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %411, <16 x i32> <i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25>
  %421 = bitcast <16 x i8> %392 to <8 x i16>
  %422 = bitcast <16 x i8> %420 to <8 x i16>
  %423 = add <8 x i16> %422, %421
  %424 = shufflevector <16 x i8> %411, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25>
  %425 = bitcast <16 x i8> %393 to <8 x i16>
  %426 = bitcast <16 x i8> %424 to <8 x i16>
  %427 = add <8 x i16> %426, %425
  %428 = add <8 x i16> %410, %388
  %429 = bitcast <2 x i64> %369 to <16 x i8>
  %430 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %429, <16 x i32> <i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25>
  %431 = bitcast <16 x i8> %430 to <8 x i16>
  %432 = add <8 x i16> %404, %431
  %433 = shufflevector <16 x i8> %429, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25>
  %434 = bitcast <16 x i8> %433 to <8 x i16>
  %435 = add <8 x i16> %407, %434
  %436 = bitcast <2 x i64> %368 to <16 x i8>
  %437 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %436, <16 x i32> <i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27>
  %438 = bitcast <16 x i8> %437 to <8 x i16>
  %439 = add <8 x i16> %432, %438
  %440 = shufflevector <16 x i8> %436, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27>
  %441 = bitcast <16 x i8> %440 to <8 x i16>
  %442 = add <8 x i16> %435, %441
  %443 = bitcast <2 x i64> %369 to <8 x i16>
  %444 = bitcast <2 x i64> %368 to <8 x i16>
  %445 = add <8 x i16> %444, %443
  %446 = bitcast <8 x i16> %445 to <16 x i8>
  %447 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %446, <16 x i32> <i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25>
  %448 = bitcast <16 x i8> %447 to <8 x i16>
  %449 = add <8 x i16> %415, %448
  %450 = shufflevector <16 x i8> %446, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25>
  %451 = bitcast <16 x i8> %450 to <8 x i16>
  %452 = add <8 x i16> %419, %451
  %453 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %446, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %454 = bitcast <16 x i8> %453 to <8 x i16>
  %455 = add <8 x i16> %423, %454
  %456 = shufflevector <16 x i8> %446, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %457 = bitcast <16 x i8> %456 to <8 x i16>
  %458 = add <8 x i16> %427, %457
  %459 = add <8 x i16> %428, %445
  %460 = load <16 x i8>, <16 x i8>* %163, align 16
  %461 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0>, <16 x i8> %460, <16 x i32> <i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29>
  %462 = bitcast <16 x i8> %461 to <8 x i16>
  %463 = shufflevector <16 x i8> %460, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef>, <16 x i32> <i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29>
  %464 = bitcast <16 x i8> %463 to <8 x i16>
  %465 = add <8 x i16> %442, %464
  %466 = load <8 x i16>, <8 x i16>* %82, align 16
  %467 = add <8 x i16> %439, %466
  %468 = add <8 x i16> %467, %462
  %469 = bitcast <16 x i8> %460 to <8 x i16>
  %470 = add <8 x i16> %466, %469
  %471 = bitcast <8 x i16> %470 to <16 x i8>
  %472 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %471, <16 x i32> <i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27>
  %473 = bitcast <16 x i8> %472 to <8 x i16>
  %474 = add <8 x i16> %449, %473
  %475 = shufflevector <16 x i8> %471, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27>
  %476 = bitcast <16 x i8> %475 to <8 x i16>
  %477 = add <8 x i16> %452, %476
  %478 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %471, <16 x i32> <i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21>
  %479 = bitcast <16 x i8> %478 to <8 x i16>
  %480 = add <8 x i16> %455, %479
  %481 = shufflevector <16 x i8> %471, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21>
  %482 = bitcast <16 x i8> %481 to <8 x i16>
  %483 = add <8 x i16> %458, %482
  %484 = add <8 x i16> %459, %470
  %485 = bitcast <8 x i16> %465 to <16 x i8>
  %486 = shufflevector <16 x i8> %485, <16 x i8> undef, <16 x i32> <i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15>
  %487 = bitcast <16 x i8> %486 to <8 x i16>
  %488 = shufflevector <8 x i16> %468, <8 x i16> %487, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %489 = shufflevector <8 x i16> %468, <8 x i16> %487, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %490 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %488, <8 x i16> %488) #8
  %491 = bitcast <4 x i32> %490 to <2 x i64>
  %492 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %489, <8 x i16> %489) #8
  %493 = bitcast <4 x i32> %492 to <2 x i64>
  %494 = and <2 x i64> %491, <i64 4294967295, i64 4294967295>
  %495 = mul nuw nsw <2 x i64> %494, <i64 840, i64 280>
  %496 = bitcast <2 x i64> %495 to <4 x i32>
  %497 = shufflevector <4 x i32> %496, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %498 = bitcast <4 x i32> %490 to <16 x i8>
  %499 = shufflevector <16 x i8> %498, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19>
  %500 = bitcast <16 x i8> %499 to <2 x i64>
  %501 = and <2 x i64> %500, <i64 4294967295, i64 4294967295>
  %502 = mul nuw nsw <2 x i64> %501, <i64 420, i64 210>
  %503 = bitcast <2 x i64> %502 to <4 x i32>
  %504 = shufflevector <4 x i32> %503, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %505 = shufflevector <4 x i32> %497, <4 x i32> %504, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %506 = and <2 x i64> %493, <i64 4294967295, i64 4294967295>
  %507 = mul nuw nsw <2 x i64> %506, <i64 168, i64 120>
  %508 = bitcast <2 x i64> %507 to <4 x i32>
  %509 = shufflevector <4 x i32> %508, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %510 = bitcast <4 x i32> %492 to <16 x i8>
  %511 = shufflevector <16 x i8> %510, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19>
  %512 = bitcast <16 x i8> %511 to <2 x i64>
  %513 = and <2 x i64> %512, <i64 4294967295, i64 4294967295>
  %514 = mul nuw nsw <2 x i64> %513, <i64 140, i64 105>
  %515 = bitcast <2 x i64> %514 to <4 x i32>
  %516 = shufflevector <4 x i32> %515, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %517 = shufflevector <4 x i32> %509, <4 x i32> %516, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %518 = add <4 x i32> %517, %505
  %519 = bitcast <8 x i16> %483 to <16 x i8>
  %520 = shufflevector <16 x i8> %519, <16 x i8> undef, <16 x i32> <i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15>
  %521 = bitcast <16 x i8> %520 to <8 x i16>
  %522 = shufflevector <8 x i16> %480, <8 x i16> %521, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %523 = shufflevector <8 x i16> %480, <8 x i16> %521, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %524 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %522, <8 x i16> %522) #8
  %525 = bitcast <4 x i32> %524 to <2 x i64>
  %526 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %523, <8 x i16> %523) #8
  %527 = bitcast <4 x i32> %526 to <2 x i64>
  %528 = and <2 x i64> %525, <i64 4294967295, i64 4294967295>
  %529 = mul nuw nsw <2 x i64> %528, <i64 0, i64 420>
  %530 = bitcast <2 x i64> %529 to <4 x i32>
  %531 = shufflevector <4 x i32> %530, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %532 = bitcast <4 x i32> %524 to <16 x i8>
  %533 = shufflevector <16 x i8> %532, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19>
  %534 = bitcast <16 x i8> %533 to <2 x i64>
  %535 = and <2 x i64> %534, <i64 4294967295, i64 4294967295>
  %536 = mul nuw nsw <2 x i64> %535, <i64 0, i64 210>
  %537 = bitcast <2 x i64> %536 to <4 x i32>
  %538 = shufflevector <4 x i32> %537, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %539 = shufflevector <4 x i32> %531, <4 x i32> %538, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %540 = and <2 x i64> %527, <i64 4294967295, i64 4294967295>
  %541 = mul nuw nsw <2 x i64> %540, <i64 140, i64 105>
  %542 = bitcast <2 x i64> %541 to <4 x i32>
  %543 = shufflevector <4 x i32> %542, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %544 = bitcast <4 x i32> %526 to <16 x i8>
  %545 = shufflevector <16 x i8> %544, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19>
  %546 = bitcast <16 x i8> %545 to <2 x i64>
  %547 = and <2 x i64> %546, <i64 4294967295, i64 4294967295>
  %548 = mul nuw nsw <2 x i64> %547, <i64 105, i64 105>
  %549 = bitcast <2 x i64> %548 to <4 x i32>
  %550 = shufflevector <4 x i32> %549, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %551 = shufflevector <4 x i32> %543, <4 x i32> %550, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %552 = add <4 x i32> %551, %539
  %553 = bitcast <8 x i16> %477 to <16 x i8>
  %554 = shufflevector <16 x i8> %553, <16 x i8> undef, <16 x i32> <i32 12, i32 13, i32 10, i32 11, i32 8, i32 9, i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1, i32 14, i32 15>
  %555 = bitcast <16 x i8> %554 to <8 x i16>
  %556 = shufflevector <8 x i16> %474, <8 x i16> %555, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %557 = shufflevector <8 x i16> %474, <8 x i16> %555, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %558 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %556, <8 x i16> %556) #8
  %559 = bitcast <4 x i32> %558 to <2 x i64>
  %560 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %557, <8 x i16> %557) #8
  %561 = bitcast <4 x i32> %560 to <2 x i64>
  %562 = and <2 x i64> %559, <i64 4294967295, i64 4294967295>
  %563 = mul nuw nsw <2 x i64> %562, <i64 0, i64 420>
  %564 = bitcast <2 x i64> %563 to <4 x i32>
  %565 = shufflevector <4 x i32> %564, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %566 = bitcast <4 x i32> %558 to <16 x i8>
  %567 = shufflevector <16 x i8> %566, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19>
  %568 = bitcast <16 x i8> %567 to <2 x i64>
  %569 = and <2 x i64> %568, <i64 4294967295, i64 4294967295>
  %570 = mul nuw nsw <2 x i64> %569, <i64 0, i64 210>
  %571 = bitcast <2 x i64> %570 to <4 x i32>
  %572 = shufflevector <4 x i32> %571, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %573 = shufflevector <4 x i32> %565, <4 x i32> %572, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %574 = and <2 x i64> %561, <i64 4294967295, i64 4294967295>
  %575 = mul nuw nsw <2 x i64> %574, <i64 140, i64 105>
  %576 = bitcast <2 x i64> %575 to <4 x i32>
  %577 = shufflevector <4 x i32> %576, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %578 = bitcast <4 x i32> %560 to <16 x i8>
  %579 = shufflevector <16 x i8> %578, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19>
  %580 = bitcast <16 x i8> %579 to <2 x i64>
  %581 = and <2 x i64> %580, <i64 4294967295, i64 4294967295>
  %582 = mul nuw nsw <2 x i64> %581, <i64 105, i64 105>
  %583 = bitcast <2 x i64> %582 to <4 x i32>
  %584 = shufflevector <4 x i32> %583, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %585 = shufflevector <4 x i32> %577, <4 x i32> %584, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %586 = add <4 x i32> %585, %573
  %587 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %484, <8 x i16> %484) #8
  %588 = bitcast <4 x i32> %587 to <2 x i64>
  %589 = and <2 x i64> %588, <i64 4294967295, i64 4294967295>
  %590 = mul nuw nsw <2 x i64> %589, <i64 105, i64 105>
  %591 = bitcast <2 x i64> %590 to <4 x i32>
  %592 = shufflevector <4 x i32> %591, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %593 = bitcast <4 x i32> %587 to <16 x i8>
  %594 = shufflevector <16 x i8> %593, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19>
  %595 = bitcast <16 x i8> %594 to <2 x i64>
  %596 = and <2 x i64> %595, <i64 4294967295, i64 4294967295>
  %597 = mul nuw nsw <2 x i64> %596, <i64 105, i64 105>
  %598 = bitcast <2 x i64> %597 to <4 x i32>
  %599 = shufflevector <4 x i32> %598, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %600 = shufflevector <4 x i32> %592, <4 x i32> %599, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %601 = shufflevector <4 x i32> %518, <4 x i32> %586, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %602 = bitcast <4 x i32> %601 to <2 x i64>
  %603 = shufflevector <4 x i32> %600, <4 x i32> %552, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %604 = bitcast <4 x i32> %603 to <2 x i64>
  %605 = shufflevector <4 x i32> %518, <4 x i32> %586, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %606 = bitcast <4 x i32> %605 to <2 x i64>
  %607 = shufflevector <4 x i32> %600, <4 x i32> %552, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %608 = bitcast <4 x i32> %607 to <2 x i64>
  %609 = shufflevector <2 x i64> %602, <2 x i64> %604, <2 x i32> <i32 0, i32 2>
  %610 = shufflevector <2 x i64> %602, <2 x i64> %604, <2 x i32> <i32 1, i32 3>
  %611 = shufflevector <2 x i64> %606, <2 x i64> %608, <2 x i32> <i32 0, i32 2>
  %612 = shufflevector <2 x i64> %606, <2 x i64> %608, <2 x i32> <i32 1, i32 3>
  %613 = bitcast <2 x i64> %609 to <4 x i32>
  %614 = bitcast <2 x i64> %610 to <4 x i32>
  %615 = bitcast <2 x i64> %611 to <4 x i32>
  %616 = bitcast <2 x i64> %612 to <4 x i32>
  %617 = add <4 x i32> %616, %615
  %618 = add <4 x i32> %617, %613
  %619 = add <4 x i32> %618, %614
  %620 = bitcast [8 x i32]* %5 to <4 x i32>*
  store <4 x i32> %619, <4 x i32>* %620, align 16
  %621 = icmp sgt <4 x i32> %619, %323
  %622 = sext <4 x i1> %621 to <4 x i32>
  %623 = bitcast <4 x i32> %622 to <2 x i64>
  %624 = xor <2 x i64> %623, <i64 -1, i64 -1>
  %625 = and <2 x i64> %624, %324
  %626 = and <4 x i32> %619, %622
  %627 = bitcast <4 x i32> %626 to <2 x i64>
  %628 = or <2 x i64> %625, %627
  %629 = bitcast <2 x i64> %628 to <16 x i8>
  %630 = shufflevector <16 x i8> %629, <16 x i8> undef, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %631 = bitcast <16 x i8> %630 to <2 x i64>
  %632 = bitcast <2 x i64> %628 to <4 x i32>
  %633 = bitcast <16 x i8> %630 to <4 x i32>
  %634 = icmp sgt <4 x i32> %632, %633
  %635 = sext <4 x i1> %634 to <4 x i32>
  %636 = bitcast <4 x i32> %635 to <2 x i64>
  %637 = xor <2 x i64> %636, <i64 -1, i64 -1>
  %638 = and <2 x i64> %637, %631
  %639 = and <2 x i64> %628, %636
  %640 = or <2 x i64> %639, %638
  %641 = bitcast <2 x i64> %640 to <16 x i8>
  %642 = shufflevector <16 x i8> %641, <16 x i8> undef, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 0, i32 1, i32 2, i32 3>
  %643 = bitcast <16 x i8> %642 to <2 x i64>
  %644 = bitcast <2 x i64> %640 to <4 x i32>
  %645 = bitcast <16 x i8> %642 to <4 x i32>
  %646 = icmp sgt <4 x i32> %644, %645
  %647 = sext <4 x i1> %646 to <4 x i32>
  %648 = bitcast <4 x i32> %647 to <2 x i64>
  %649 = xor <2 x i64> %648, <i64 -1, i64 -1>
  %650 = and <2 x i64> %649, %643
  %651 = and <2 x i64> %640, %648
  %652 = or <2 x i64> %651, %650
  %653 = bitcast <2 x i64> %652 to <4 x i32>
  %654 = extractelement <4 x i32> %653, i32 0
  %655 = icmp eq <4 x i32> %323, %653
  %656 = sext <4 x i1> %655 to <4 x i32>
  %657 = icmp eq <4 x i32> %619, %653
  %658 = sext <4 x i1> %657 to <4 x i32>
  %659 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %658, <4 x i32> %656) #8
  %660 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %659, <8 x i16> %659) #8
  %661 = icmp slt <16 x i8> %660, zeroinitializer
  %662 = bitcast <16 x i1> %661 to i16
  %663 = zext i16 %662 to i32
  %664 = add nsw i32 %663, -1
  %665 = xor i32 %664, %663
  %666 = tail call i32 @llvm.ctlz.i32(i32 %665, i1 true) #8, !range !2
  %667 = xor i32 %666, 31
  %668 = add nuw nsw i32 %667, 4
  %669 = and i32 %668, 7
  %670 = zext i32 %669 to i64
  %671 = getelementptr inbounds [8 x i32], [8 x i32]* %5, i64 0, i64 %670
  %672 = load i32, i32* %671, align 4
  %673 = sub nsw i32 %654, %672
  %674 = ashr i32 %673, 10
  store i32 %674, i32* %2, align 4
  call void @llvm.lifetime.end.p0i8(i64 128, i8* nonnull %9) #8
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %7) #8
  ret i32 %667
}

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #1

; Function Attrs: argmemonly nounwind
declare void @llvm.memset.p0i8.i64(i8* nocapture writeonly, i8, i64, i1 immarg) #1

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #1

; Function Attrs: nounwind ssp uwtable
define hidden void @cdef_filter_block_4x4_8_ssse3(i8* nocapture, i32, i16* readonly, i32, i32, i32, i32, i32, i32) local_unnamed_addr #0 {
  %10 = sext i32 %5 to i64
  %11 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %10, i64 0
  %12 = load i32, i32* %11, align 8
  %13 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %10, i64 1
  %14 = load i32, i32* %13, align 4
  %15 = add nsw i32 %5, 2
  %16 = and i32 %15, 7
  %17 = zext i32 %16 to i64
  %18 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %17, i64 0
  %19 = load i32, i32* %18, align 8
  %20 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %17, i64 1
  %21 = load i32, i32* %20, align 4
  %22 = add nsw i32 %5, 6
  %23 = and i32 %22, 7
  %24 = zext i32 %23 to i64
  %25 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %24, i64 0
  %26 = load i32, i32* %25, align 8
  %27 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %24, i64 1
  %28 = load i32, i32* %27, align 4
  %29 = lshr i32 %3, %8
  %30 = and i32 %29, 1
  %31 = zext i32 %30 to i64
  %32 = getelementptr inbounds [2 x [2 x i32]], [2 x [2 x i32]]* @cdef_pri_taps, i64 0, i64 %31, i64 0
  %33 = icmp ne i32 %3, 0
  br i1 %33, label %34, label %40

34:                                               ; preds = %9
  %35 = tail call i32 @llvm.ctlz.i32(i32 %3, i1 true) #8, !range !2
  %36 = xor i32 %35, 31
  %37 = icmp sgt i32 %36, %6
  %38 = sub nsw i32 %6, %36
  %39 = select i1 %37, i32 0, i32 %38
  br label %40

40:                                               ; preds = %34, %9
  %41 = phi i32 [ %6, %9 ], [ %39, %34 ]
  %42 = icmp ne i32 %4, 0
  br i1 %42, label %43, label %49

43:                                               ; preds = %40
  %44 = tail call i32 @llvm.ctlz.i32(i32 %4, i1 true) #8, !range !2
  %45 = xor i32 %44, 31
  %46 = icmp sgt i32 %45, %7
  %47 = sub nsw i32 %7, %45
  %48 = select i1 %46, i32 0, i32 %47
  br label %49

49:                                               ; preds = %43, %40
  %50 = phi i32 [ %7, %40 ], [ %48, %43 ]
  %51 = bitcast i16* %2 to i64*
  %52 = load i64, i64* %51, align 1
  %53 = getelementptr inbounds i16, i16* %2, i64 144
  %54 = bitcast i16* %53 to i64*
  %55 = load i64, i64* %54, align 1
  %56 = insertelement <2 x i64> undef, i64 %55, i32 0
  %57 = getelementptr inbounds i16, i16* %2, i64 288
  %58 = bitcast i16* %57 to i64*
  %59 = load i64, i64* %58, align 1
  %60 = getelementptr inbounds i16, i16* %2, i64 432
  %61 = bitcast i16* %60 to i64*
  %62 = load i64, i64* %61, align 1
  %63 = insertelement <2 x i64> undef, i64 %62, i32 0
  %64 = insertelement <2 x i64> %56, i64 %52, i32 1
  %65 = insertelement <2 x i64> %63, i64 %59, i32 1
  br i1 %33, label %66, label %347

66:                                               ; preds = %49
  %67 = sext i32 %12 to i64
  %68 = getelementptr inbounds i16, i16* %2, i64 %67
  %69 = bitcast i16* %68 to i64*
  %70 = load i64, i64* %69, align 1
  %71 = add nsw i32 %12, 144
  %72 = sext i32 %71 to i64
  %73 = getelementptr inbounds i16, i16* %2, i64 %72
  %74 = bitcast i16* %73 to i64*
  %75 = load i64, i64* %74, align 1
  %76 = insertelement <2 x i64> undef, i64 %75, i32 0
  %77 = add nsw i32 %12, 288
  %78 = sext i32 %77 to i64
  %79 = getelementptr inbounds i16, i16* %2, i64 %78
  %80 = bitcast i16* %79 to i64*
  %81 = load i64, i64* %80, align 1
  %82 = add nsw i32 %12, 432
  %83 = sext i32 %82 to i64
  %84 = getelementptr inbounds i16, i16* %2, i64 %83
  %85 = bitcast i16* %84 to i64*
  %86 = load i64, i64* %85, align 1
  %87 = insertelement <2 x i64> undef, i64 %86, i32 0
  %88 = insertelement <2 x i64> %76, i64 %70, i32 1
  %89 = insertelement <2 x i64> %87, i64 %81, i32 1
  %90 = bitcast <2 x i64> %88 to <8 x i16>
  %91 = icmp eq <8 x i16> %90, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %92 = sext <8 x i1> %91 to <8 x i16>
  %93 = bitcast <2 x i64> %89 to <8 x i16>
  %94 = icmp eq <8 x i16> %93, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %95 = sext <8 x i1> %94 to <8 x i16>
  %96 = bitcast <8 x i16> %95 to <2 x i64>
  %97 = bitcast <8 x i16> %92 to <2 x i64>
  %98 = xor <2 x i64> %97, <i64 -1, i64 -1>
  %99 = and <2 x i64> %88, %98
  %100 = xor <2 x i64> %96, <i64 -1, i64 -1>
  %101 = and <2 x i64> %89, %100
  %102 = bitcast <2 x i64> %64 to <8 x i16>
  %103 = bitcast <2 x i64> %99 to <8 x i16>
  %104 = icmp sgt <8 x i16> %102, %103
  %105 = select <8 x i1> %104, <8 x i16> %102, <8 x i16> %103
  %106 = bitcast <2 x i64> %65 to <8 x i16>
  %107 = bitcast <2 x i64> %101 to <8 x i16>
  %108 = icmp sgt <8 x i16> %106, %107
  %109 = select <8 x i1> %108, <8 x i16> %106, <8 x i16> %107
  %110 = icmp sgt <8 x i16> %90, %102
  %111 = select <8 x i1> %110, <8 x i16> %102, <8 x i16> %90
  %112 = icmp sgt <8 x i16> %93, %106
  %113 = select <8 x i1> %112, <8 x i16> %106, <8 x i16> %93
  %114 = sub <8 x i16> %90, %102
  %115 = sub <8 x i16> %93, %106
  %116 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %115, <8 x i16> %114) #8
  %117 = ashr <16 x i8> %116, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %118 = sub <16 x i8> zeroinitializer, %116
  %119 = icmp slt <16 x i8> %116, zeroinitializer
  %120 = select <16 x i1> %119, <16 x i8> %118, <16 x i8> %116
  %121 = trunc i32 %3 to i8
  %122 = insertelement <16 x i8> undef, i8 %121, i32 0
  %123 = shufflevector <16 x i8> %122, <16 x i8> undef, <16 x i32> zeroinitializer
  %124 = lshr i32 255, %41
  %125 = trunc i32 %124 to i8
  %126 = insertelement <16 x i8> undef, i8 %125, i32 0
  %127 = shufflevector <16 x i8> %126, <16 x i8> undef, <16 x i32> zeroinitializer
  %128 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %41, i32 0
  %129 = bitcast <16 x i8> %120 to <8 x i16>
  %130 = bitcast <4 x i32> %128 to <8 x i16>
  %131 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %129, <8 x i16> %130) #8
  %132 = bitcast <8 x i16> %131 to <16 x i8>
  %133 = and <16 x i8> %127, %132
  %134 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %123, <16 x i8> %133) #8
  %135 = icmp ult <16 x i8> %120, %134
  %136 = select <16 x i1> %135, <16 x i8> %120, <16 x i8> %134
  %137 = add <16 x i8> %136, %117
  %138 = xor <16 x i8> %137, %117
  %139 = sub nsw i32 0, %12
  %140 = sext i32 %139 to i64
  %141 = getelementptr inbounds i16, i16* %2, i64 %140
  %142 = bitcast i16* %141 to i64*
  %143 = load i64, i64* %142, align 1
  %144 = sub nsw i32 144, %12
  %145 = sext i32 %144 to i64
  %146 = getelementptr inbounds i16, i16* %2, i64 %145
  %147 = bitcast i16* %146 to i64*
  %148 = load i64, i64* %147, align 1
  %149 = insertelement <2 x i64> undef, i64 %148, i32 0
  %150 = sub nsw i32 288, %12
  %151 = sext i32 %150 to i64
  %152 = getelementptr inbounds i16, i16* %2, i64 %151
  %153 = bitcast i16* %152 to i64*
  %154 = load i64, i64* %153, align 1
  %155 = sub nsw i32 432, %12
  %156 = sext i32 %155 to i64
  %157 = getelementptr inbounds i16, i16* %2, i64 %156
  %158 = bitcast i16* %157 to i64*
  %159 = load i64, i64* %158, align 1
  %160 = insertelement <2 x i64> undef, i64 %159, i32 0
  %161 = insertelement <2 x i64> %149, i64 %143, i32 1
  %162 = insertelement <2 x i64> %160, i64 %154, i32 1
  %163 = bitcast <2 x i64> %161 to <8 x i16>
  %164 = icmp eq <8 x i16> %163, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %165 = sext <8 x i1> %164 to <8 x i16>
  %166 = bitcast <2 x i64> %162 to <8 x i16>
  %167 = icmp eq <8 x i16> %166, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %168 = sext <8 x i1> %167 to <8 x i16>
  %169 = bitcast <8 x i16> %168 to <2 x i64>
  %170 = bitcast <8 x i16> %165 to <2 x i64>
  %171 = xor <2 x i64> %170, <i64 -1, i64 -1>
  %172 = and <2 x i64> %161, %171
  %173 = xor <2 x i64> %169, <i64 -1, i64 -1>
  %174 = and <2 x i64> %162, %173
  %175 = bitcast <2 x i64> %172 to <8 x i16>
  %176 = icmp sgt <8 x i16> %105, %175
  %177 = select <8 x i1> %176, <8 x i16> %105, <8 x i16> %175
  %178 = bitcast <2 x i64> %174 to <8 x i16>
  %179 = icmp sgt <8 x i16> %109, %178
  %180 = select <8 x i1> %179, <8 x i16> %109, <8 x i16> %178
  %181 = icmp slt <8 x i16> %111, %163
  %182 = select <8 x i1> %181, <8 x i16> %111, <8 x i16> %163
  %183 = icmp slt <8 x i16> %113, %166
  %184 = select <8 x i1> %183, <8 x i16> %113, <8 x i16> %166
  %185 = sub <8 x i16> %163, %102
  %186 = sub <8 x i16> %166, %106
  %187 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %186, <8 x i16> %185) #8
  %188 = ashr <16 x i8> %187, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %189 = sub <16 x i8> zeroinitializer, %187
  %190 = icmp slt <16 x i8> %187, zeroinitializer
  %191 = select <16 x i1> %190, <16 x i8> %189, <16 x i8> %187
  %192 = bitcast <16 x i8> %191 to <8 x i16>
  %193 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %192, <8 x i16> %130) #8
  %194 = bitcast <8 x i16> %193 to <16 x i8>
  %195 = and <16 x i8> %127, %194
  %196 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %123, <16 x i8> %195) #8
  %197 = icmp ult <16 x i8> %191, %196
  %198 = select <16 x i1> %197, <16 x i8> %191, <16 x i8> %196
  %199 = add <16 x i8> %198, %188
  %200 = xor <16 x i8> %199, %188
  %201 = load i32, i32* %32, align 8
  %202 = trunc i32 %201 to i8
  %203 = insertelement <16 x i8> undef, i8 %202, i32 0
  %204 = shufflevector <16 x i8> %203, <16 x i8> undef, <16 x i32> zeroinitializer
  %205 = shufflevector <16 x i8> %200, <16 x i8> %138, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %206 = shufflevector <16 x i8> %200, <16 x i8> %138, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %207 = tail call <8 x i16> @llvm.x86.ssse3.pmadd.ub.sw.128(<16 x i8> %204, <16 x i8> %205) #8
  %208 = tail call <8 x i16> @llvm.x86.ssse3.pmadd.ub.sw.128(<16 x i8> %204, <16 x i8> %206) #8
  %209 = sext i32 %14 to i64
  %210 = getelementptr inbounds i16, i16* %2, i64 %209
  %211 = bitcast i16* %210 to i64*
  %212 = load i64, i64* %211, align 1
  %213 = add nsw i32 %14, 144
  %214 = sext i32 %213 to i64
  %215 = getelementptr inbounds i16, i16* %2, i64 %214
  %216 = bitcast i16* %215 to i64*
  %217 = load i64, i64* %216, align 1
  %218 = insertelement <2 x i64> undef, i64 %217, i32 0
  %219 = add nsw i32 %14, 288
  %220 = sext i32 %219 to i64
  %221 = getelementptr inbounds i16, i16* %2, i64 %220
  %222 = bitcast i16* %221 to i64*
  %223 = load i64, i64* %222, align 1
  %224 = add nsw i32 %14, 432
  %225 = sext i32 %224 to i64
  %226 = getelementptr inbounds i16, i16* %2, i64 %225
  %227 = bitcast i16* %226 to i64*
  %228 = load i64, i64* %227, align 1
  %229 = insertelement <2 x i64> undef, i64 %228, i32 0
  %230 = insertelement <2 x i64> %218, i64 %212, i32 1
  %231 = insertelement <2 x i64> %229, i64 %223, i32 1
  %232 = bitcast <2 x i64> %230 to <8 x i16>
  %233 = icmp eq <8 x i16> %232, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %234 = sext <8 x i1> %233 to <8 x i16>
  %235 = bitcast <2 x i64> %231 to <8 x i16>
  %236 = icmp eq <8 x i16> %235, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %237 = sext <8 x i1> %236 to <8 x i16>
  %238 = bitcast <8 x i16> %237 to <2 x i64>
  %239 = bitcast <8 x i16> %234 to <2 x i64>
  %240 = xor <2 x i64> %239, <i64 -1, i64 -1>
  %241 = and <2 x i64> %230, %240
  %242 = xor <2 x i64> %238, <i64 -1, i64 -1>
  %243 = and <2 x i64> %231, %242
  %244 = bitcast <2 x i64> %241 to <8 x i16>
  %245 = icmp sgt <8 x i16> %177, %244
  %246 = select <8 x i1> %245, <8 x i16> %177, <8 x i16> %244
  %247 = bitcast <2 x i64> %243 to <8 x i16>
  %248 = icmp sgt <8 x i16> %180, %247
  %249 = select <8 x i1> %248, <8 x i16> %180, <8 x i16> %247
  %250 = icmp slt <8 x i16> %182, %232
  %251 = select <8 x i1> %250, <8 x i16> %182, <8 x i16> %232
  %252 = icmp slt <8 x i16> %184, %235
  %253 = select <8 x i1> %252, <8 x i16> %184, <8 x i16> %235
  %254 = sub <8 x i16> %232, %102
  %255 = sub <8 x i16> %235, %106
  %256 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %255, <8 x i16> %254) #8
  %257 = ashr <16 x i8> %256, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %258 = sub <16 x i8> zeroinitializer, %256
  %259 = icmp slt <16 x i8> %256, zeroinitializer
  %260 = select <16 x i1> %259, <16 x i8> %258, <16 x i8> %256
  %261 = bitcast <16 x i8> %260 to <8 x i16>
  %262 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %261, <8 x i16> %130) #8
  %263 = bitcast <8 x i16> %262 to <16 x i8>
  %264 = and <16 x i8> %127, %263
  %265 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %123, <16 x i8> %264) #8
  %266 = icmp ult <16 x i8> %260, %265
  %267 = select <16 x i1> %266, <16 x i8> %260, <16 x i8> %265
  %268 = add <16 x i8> %267, %257
  %269 = xor <16 x i8> %268, %257
  %270 = sub nsw i32 0, %14
  %271 = sext i32 %270 to i64
  %272 = getelementptr inbounds i16, i16* %2, i64 %271
  %273 = bitcast i16* %272 to i64*
  %274 = load i64, i64* %273, align 1
  %275 = sub nsw i32 144, %14
  %276 = sext i32 %275 to i64
  %277 = getelementptr inbounds i16, i16* %2, i64 %276
  %278 = bitcast i16* %277 to i64*
  %279 = load i64, i64* %278, align 1
  %280 = insertelement <2 x i64> undef, i64 %279, i32 0
  %281 = sub nsw i32 288, %14
  %282 = sext i32 %281 to i64
  %283 = getelementptr inbounds i16, i16* %2, i64 %282
  %284 = bitcast i16* %283 to i64*
  %285 = load i64, i64* %284, align 1
  %286 = sub nsw i32 432, %14
  %287 = sext i32 %286 to i64
  %288 = getelementptr inbounds i16, i16* %2, i64 %287
  %289 = bitcast i16* %288 to i64*
  %290 = load i64, i64* %289, align 1
  %291 = insertelement <2 x i64> undef, i64 %290, i32 0
  %292 = insertelement <2 x i64> %280, i64 %274, i32 1
  %293 = insertelement <2 x i64> %291, i64 %285, i32 1
  %294 = bitcast <2 x i64> %292 to <8 x i16>
  %295 = icmp eq <8 x i16> %294, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %296 = sext <8 x i1> %295 to <8 x i16>
  %297 = bitcast <2 x i64> %293 to <8 x i16>
  %298 = icmp eq <8 x i16> %297, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %299 = sext <8 x i1> %298 to <8 x i16>
  %300 = bitcast <8 x i16> %299 to <2 x i64>
  %301 = bitcast <8 x i16> %296 to <2 x i64>
  %302 = xor <2 x i64> %301, <i64 -1, i64 -1>
  %303 = and <2 x i64> %292, %302
  %304 = xor <2 x i64> %300, <i64 -1, i64 -1>
  %305 = and <2 x i64> %293, %304
  %306 = bitcast <2 x i64> %303 to <8 x i16>
  %307 = icmp sgt <8 x i16> %246, %306
  %308 = select <8 x i1> %307, <8 x i16> %246, <8 x i16> %306
  %309 = bitcast <2 x i64> %305 to <8 x i16>
  %310 = icmp sgt <8 x i16> %249, %309
  %311 = select <8 x i1> %310, <8 x i16> %249, <8 x i16> %309
  %312 = bitcast <8 x i16> %311 to <2 x i64>
  %313 = bitcast <8 x i16> %308 to <2 x i64>
  %314 = icmp slt <8 x i16> %251, %294
  %315 = select <8 x i1> %314, <8 x i16> %251, <8 x i16> %294
  %316 = icmp slt <8 x i16> %253, %297
  %317 = select <8 x i1> %316, <8 x i16> %253, <8 x i16> %297
  %318 = bitcast <8 x i16> %317 to <2 x i64>
  %319 = bitcast <8 x i16> %315 to <2 x i64>
  %320 = sub <8 x i16> %294, %102
  %321 = sub <8 x i16> %297, %106
  %322 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %321, <8 x i16> %320) #8
  %323 = ashr <16 x i8> %322, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %324 = sub <16 x i8> zeroinitializer, %322
  %325 = icmp slt <16 x i8> %322, zeroinitializer
  %326 = select <16 x i1> %325, <16 x i8> %324, <16 x i8> %322
  %327 = bitcast <16 x i8> %326 to <8 x i16>
  %328 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %327, <8 x i16> %130) #8
  %329 = bitcast <8 x i16> %328 to <16 x i8>
  %330 = and <16 x i8> %127, %329
  %331 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %123, <16 x i8> %330) #8
  %332 = icmp ult <16 x i8> %326, %331
  %333 = select <16 x i1> %332, <16 x i8> %326, <16 x i8> %331
  %334 = add <16 x i8> %333, %323
  %335 = xor <16 x i8> %334, %323
  %336 = getelementptr inbounds [2 x [2 x i32]], [2 x [2 x i32]]* @cdef_pri_taps, i64 0, i64 %31, i64 1
  %337 = load i32, i32* %336, align 4
  %338 = trunc i32 %337 to i8
  %339 = insertelement <16 x i8> undef, i8 %338, i32 0
  %340 = shufflevector <16 x i8> %339, <16 x i8> undef, <16 x i32> zeroinitializer
  %341 = shufflevector <16 x i8> %335, <16 x i8> %269, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %342 = shufflevector <16 x i8> %335, <16 x i8> %269, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %343 = tail call <8 x i16> @llvm.x86.ssse3.pmadd.ub.sw.128(<16 x i8> %340, <16 x i8> %341) #8
  %344 = tail call <8 x i16> @llvm.x86.ssse3.pmadd.ub.sw.128(<16 x i8> %340, <16 x i8> %342) #8
  %345 = add <8 x i16> %343, %207
  %346 = add <8 x i16> %344, %208
  br label %347

347:                                              ; preds = %66, %49
  %348 = phi <2 x i64> [ %318, %66 ], [ %65, %49 ]
  %349 = phi <2 x i64> [ %319, %66 ], [ %64, %49 ]
  %350 = phi <2 x i64> [ %312, %66 ], [ %65, %49 ]
  %351 = phi <2 x i64> [ %313, %66 ], [ %64, %49 ]
  %352 = phi <8 x i16> [ %346, %66 ], [ zeroinitializer, %49 ]
  %353 = phi <8 x i16> [ %345, %66 ], [ zeroinitializer, %49 ]
  br i1 %42, label %357, label %354

354:                                              ; preds = %347
  %355 = bitcast <2 x i64> %64 to <8 x i16>
  %356 = bitcast <2 x i64> %65 to <8 x i16>
  br label %893

357:                                              ; preds = %347
  %358 = sext i32 %19 to i64
  %359 = getelementptr inbounds i16, i16* %2, i64 %358
  %360 = bitcast i16* %359 to i64*
  %361 = load i64, i64* %360, align 1
  %362 = add nsw i32 %19, 144
  %363 = sext i32 %362 to i64
  %364 = getelementptr inbounds i16, i16* %2, i64 %363
  %365 = bitcast i16* %364 to i64*
  %366 = load i64, i64* %365, align 1
  %367 = insertelement <2 x i64> undef, i64 %366, i32 0
  %368 = add nsw i32 %19, 288
  %369 = sext i32 %368 to i64
  %370 = getelementptr inbounds i16, i16* %2, i64 %369
  %371 = bitcast i16* %370 to i64*
  %372 = load i64, i64* %371, align 1
  %373 = add nsw i32 %19, 432
  %374 = sext i32 %373 to i64
  %375 = getelementptr inbounds i16, i16* %2, i64 %374
  %376 = bitcast i16* %375 to i64*
  %377 = load i64, i64* %376, align 1
  %378 = insertelement <2 x i64> undef, i64 %377, i32 0
  %379 = insertelement <2 x i64> %367, i64 %361, i32 1
  %380 = insertelement <2 x i64> %378, i64 %372, i32 1
  %381 = bitcast <2 x i64> %379 to <8 x i16>
  %382 = icmp eq <8 x i16> %381, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %383 = sext <8 x i1> %382 to <8 x i16>
  %384 = bitcast <2 x i64> %380 to <8 x i16>
  %385 = icmp eq <8 x i16> %384, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %386 = sext <8 x i1> %385 to <8 x i16>
  %387 = bitcast <8 x i16> %386 to <2 x i64>
  %388 = bitcast <8 x i16> %383 to <2 x i64>
  %389 = xor <2 x i64> %388, <i64 -1, i64 -1>
  %390 = and <2 x i64> %379, %389
  %391 = xor <2 x i64> %387, <i64 -1, i64 -1>
  %392 = and <2 x i64> %380, %391
  %393 = bitcast <2 x i64> %351 to <8 x i16>
  %394 = bitcast <2 x i64> %390 to <8 x i16>
  %395 = icmp sgt <8 x i16> %393, %394
  %396 = select <8 x i1> %395, <8 x i16> %393, <8 x i16> %394
  %397 = bitcast <2 x i64> %350 to <8 x i16>
  %398 = bitcast <2 x i64> %392 to <8 x i16>
  %399 = icmp sgt <8 x i16> %397, %398
  %400 = select <8 x i1> %399, <8 x i16> %397, <8 x i16> %398
  %401 = bitcast <2 x i64> %349 to <8 x i16>
  %402 = icmp slt <8 x i16> %401, %381
  %403 = select <8 x i1> %402, <8 x i16> %401, <8 x i16> %381
  %404 = bitcast <2 x i64> %348 to <8 x i16>
  %405 = icmp slt <8 x i16> %404, %384
  %406 = select <8 x i1> %405, <8 x i16> %404, <8 x i16> %384
  %407 = bitcast <2 x i64> %65 to <8 x i16>
  %408 = bitcast <2 x i64> %64 to <8 x i16>
  %409 = sub <8 x i16> %381, %408
  %410 = sub <8 x i16> %384, %407
  %411 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %410, <8 x i16> %409) #8
  %412 = ashr <16 x i8> %411, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %413 = sub <16 x i8> zeroinitializer, %411
  %414 = icmp slt <16 x i8> %411, zeroinitializer
  %415 = select <16 x i1> %414, <16 x i8> %413, <16 x i8> %411
  %416 = trunc i32 %4 to i8
  %417 = insertelement <16 x i8> undef, i8 %416, i32 0
  %418 = shufflevector <16 x i8> %417, <16 x i8> undef, <16 x i32> zeroinitializer
  %419 = lshr i32 255, %50
  %420 = trunc i32 %419 to i8
  %421 = insertelement <16 x i8> undef, i8 %420, i32 0
  %422 = shufflevector <16 x i8> %421, <16 x i8> undef, <16 x i32> zeroinitializer
  %423 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %50, i32 0
  %424 = bitcast <16 x i8> %415 to <8 x i16>
  %425 = bitcast <4 x i32> %423 to <8 x i16>
  %426 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %424, <8 x i16> %425) #8
  %427 = bitcast <8 x i16> %426 to <16 x i8>
  %428 = and <16 x i8> %422, %427
  %429 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %418, <16 x i8> %428) #8
  %430 = icmp ult <16 x i8> %415, %429
  %431 = select <16 x i1> %430, <16 x i8> %415, <16 x i8> %429
  %432 = add <16 x i8> %431, %412
  %433 = xor <16 x i8> %432, %412
  %434 = sub nsw i32 0, %19
  %435 = sext i32 %434 to i64
  %436 = getelementptr inbounds i16, i16* %2, i64 %435
  %437 = bitcast i16* %436 to i64*
  %438 = load i64, i64* %437, align 1
  %439 = sub nsw i32 144, %19
  %440 = sext i32 %439 to i64
  %441 = getelementptr inbounds i16, i16* %2, i64 %440
  %442 = bitcast i16* %441 to i64*
  %443 = load i64, i64* %442, align 1
  %444 = insertelement <2 x i64> undef, i64 %443, i32 0
  %445 = sub nsw i32 288, %19
  %446 = sext i32 %445 to i64
  %447 = getelementptr inbounds i16, i16* %2, i64 %446
  %448 = bitcast i16* %447 to i64*
  %449 = load i64, i64* %448, align 1
  %450 = sub nsw i32 432, %19
  %451 = sext i32 %450 to i64
  %452 = getelementptr inbounds i16, i16* %2, i64 %451
  %453 = bitcast i16* %452 to i64*
  %454 = load i64, i64* %453, align 1
  %455 = insertelement <2 x i64> undef, i64 %454, i32 0
  %456 = insertelement <2 x i64> %444, i64 %438, i32 1
  %457 = insertelement <2 x i64> %455, i64 %449, i32 1
  %458 = bitcast <2 x i64> %456 to <8 x i16>
  %459 = icmp eq <8 x i16> %458, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %460 = sext <8 x i1> %459 to <8 x i16>
  %461 = bitcast <2 x i64> %457 to <8 x i16>
  %462 = icmp eq <8 x i16> %461, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %463 = sext <8 x i1> %462 to <8 x i16>
  %464 = bitcast <8 x i16> %463 to <2 x i64>
  %465 = bitcast <8 x i16> %460 to <2 x i64>
  %466 = xor <2 x i64> %465, <i64 -1, i64 -1>
  %467 = and <2 x i64> %456, %466
  %468 = xor <2 x i64> %464, <i64 -1, i64 -1>
  %469 = and <2 x i64> %457, %468
  %470 = bitcast <2 x i64> %467 to <8 x i16>
  %471 = icmp sgt <8 x i16> %396, %470
  %472 = select <8 x i1> %471, <8 x i16> %396, <8 x i16> %470
  %473 = bitcast <2 x i64> %469 to <8 x i16>
  %474 = icmp sgt <8 x i16> %400, %473
  %475 = select <8 x i1> %474, <8 x i16> %400, <8 x i16> %473
  %476 = icmp slt <8 x i16> %403, %458
  %477 = select <8 x i1> %476, <8 x i16> %403, <8 x i16> %458
  %478 = icmp slt <8 x i16> %406, %461
  %479 = select <8 x i1> %478, <8 x i16> %406, <8 x i16> %461
  %480 = sub <8 x i16> %458, %408
  %481 = sub <8 x i16> %461, %407
  %482 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %481, <8 x i16> %480) #8
  %483 = ashr <16 x i8> %482, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %484 = sub <16 x i8> zeroinitializer, %482
  %485 = icmp slt <16 x i8> %482, zeroinitializer
  %486 = select <16 x i1> %485, <16 x i8> %484, <16 x i8> %482
  %487 = bitcast <16 x i8> %486 to <8 x i16>
  %488 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %487, <8 x i16> %425) #8
  %489 = bitcast <8 x i16> %488 to <16 x i8>
  %490 = and <16 x i8> %422, %489
  %491 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %418, <16 x i8> %490) #8
  %492 = icmp ult <16 x i8> %486, %491
  %493 = select <16 x i1> %492, <16 x i8> %486, <16 x i8> %491
  %494 = add <16 x i8> %493, %483
  %495 = xor <16 x i8> %494, %483
  %496 = sext i32 %26 to i64
  %497 = getelementptr inbounds i16, i16* %2, i64 %496
  %498 = bitcast i16* %497 to i64*
  %499 = load i64, i64* %498, align 1
  %500 = add nsw i32 %26, 144
  %501 = sext i32 %500 to i64
  %502 = getelementptr inbounds i16, i16* %2, i64 %501
  %503 = bitcast i16* %502 to i64*
  %504 = load i64, i64* %503, align 1
  %505 = insertelement <2 x i64> undef, i64 %504, i32 0
  %506 = add nsw i32 %26, 288
  %507 = sext i32 %506 to i64
  %508 = getelementptr inbounds i16, i16* %2, i64 %507
  %509 = bitcast i16* %508 to i64*
  %510 = load i64, i64* %509, align 1
  %511 = add nsw i32 %26, 432
  %512 = sext i32 %511 to i64
  %513 = getelementptr inbounds i16, i16* %2, i64 %512
  %514 = bitcast i16* %513 to i64*
  %515 = load i64, i64* %514, align 1
  %516 = insertelement <2 x i64> undef, i64 %515, i32 0
  %517 = insertelement <2 x i64> %505, i64 %499, i32 1
  %518 = insertelement <2 x i64> %516, i64 %510, i32 1
  %519 = bitcast <2 x i64> %517 to <8 x i16>
  %520 = icmp eq <8 x i16> %519, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %521 = sext <8 x i1> %520 to <8 x i16>
  %522 = bitcast <2 x i64> %518 to <8 x i16>
  %523 = icmp eq <8 x i16> %522, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %524 = sext <8 x i1> %523 to <8 x i16>
  %525 = bitcast <8 x i16> %524 to <2 x i64>
  %526 = bitcast <8 x i16> %521 to <2 x i64>
  %527 = xor <2 x i64> %526, <i64 -1, i64 -1>
  %528 = and <2 x i64> %517, %527
  %529 = xor <2 x i64> %525, <i64 -1, i64 -1>
  %530 = and <2 x i64> %518, %529
  %531 = bitcast <2 x i64> %528 to <8 x i16>
  %532 = icmp sgt <8 x i16> %472, %531
  %533 = select <8 x i1> %532, <8 x i16> %472, <8 x i16> %531
  %534 = bitcast <2 x i64> %530 to <8 x i16>
  %535 = icmp sgt <8 x i16> %475, %534
  %536 = select <8 x i1> %535, <8 x i16> %475, <8 x i16> %534
  %537 = icmp slt <8 x i16> %477, %519
  %538 = select <8 x i1> %537, <8 x i16> %477, <8 x i16> %519
  %539 = icmp slt <8 x i16> %479, %522
  %540 = select <8 x i1> %539, <8 x i16> %479, <8 x i16> %522
  %541 = sub <8 x i16> %519, %408
  %542 = sub <8 x i16> %522, %407
  %543 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %542, <8 x i16> %541) #8
  %544 = ashr <16 x i8> %543, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %545 = sub <16 x i8> zeroinitializer, %543
  %546 = icmp slt <16 x i8> %543, zeroinitializer
  %547 = select <16 x i1> %546, <16 x i8> %545, <16 x i8> %543
  %548 = bitcast <16 x i8> %547 to <8 x i16>
  %549 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %548, <8 x i16> %425) #8
  %550 = bitcast <8 x i16> %549 to <16 x i8>
  %551 = and <16 x i8> %422, %550
  %552 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %418, <16 x i8> %551) #8
  %553 = icmp ult <16 x i8> %547, %552
  %554 = select <16 x i1> %553, <16 x i8> %547, <16 x i8> %552
  %555 = add <16 x i8> %554, %544
  %556 = xor <16 x i8> %555, %544
  %557 = sub nsw i32 0, %26
  %558 = sext i32 %557 to i64
  %559 = getelementptr inbounds i16, i16* %2, i64 %558
  %560 = bitcast i16* %559 to i64*
  %561 = load i64, i64* %560, align 1
  %562 = sub nsw i32 144, %26
  %563 = sext i32 %562 to i64
  %564 = getelementptr inbounds i16, i16* %2, i64 %563
  %565 = bitcast i16* %564 to i64*
  %566 = load i64, i64* %565, align 1
  %567 = insertelement <2 x i64> undef, i64 %566, i32 0
  %568 = sub nsw i32 288, %26
  %569 = sext i32 %568 to i64
  %570 = getelementptr inbounds i16, i16* %2, i64 %569
  %571 = bitcast i16* %570 to i64*
  %572 = load i64, i64* %571, align 1
  %573 = sub nsw i32 432, %26
  %574 = sext i32 %573 to i64
  %575 = getelementptr inbounds i16, i16* %2, i64 %574
  %576 = bitcast i16* %575 to i64*
  %577 = load i64, i64* %576, align 1
  %578 = insertelement <2 x i64> undef, i64 %577, i32 0
  %579 = insertelement <2 x i64> %567, i64 %561, i32 1
  %580 = insertelement <2 x i64> %578, i64 %572, i32 1
  %581 = bitcast <2 x i64> %579 to <8 x i16>
  %582 = icmp eq <8 x i16> %581, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %583 = sext <8 x i1> %582 to <8 x i16>
  %584 = bitcast <2 x i64> %580 to <8 x i16>
  %585 = icmp eq <8 x i16> %584, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %586 = sext <8 x i1> %585 to <8 x i16>
  %587 = bitcast <8 x i16> %586 to <2 x i64>
  %588 = bitcast <8 x i16> %583 to <2 x i64>
  %589 = xor <2 x i64> %588, <i64 -1, i64 -1>
  %590 = and <2 x i64> %579, %589
  %591 = xor <2 x i64> %587, <i64 -1, i64 -1>
  %592 = and <2 x i64> %580, %591
  %593 = bitcast <2 x i64> %590 to <8 x i16>
  %594 = icmp sgt <8 x i16> %533, %593
  %595 = select <8 x i1> %594, <8 x i16> %533, <8 x i16> %593
  %596 = bitcast <2 x i64> %592 to <8 x i16>
  %597 = icmp sgt <8 x i16> %536, %596
  %598 = select <8 x i1> %597, <8 x i16> %536, <8 x i16> %596
  %599 = icmp slt <8 x i16> %538, %581
  %600 = select <8 x i1> %599, <8 x i16> %538, <8 x i16> %581
  %601 = icmp slt <8 x i16> %540, %584
  %602 = select <8 x i1> %601, <8 x i16> %540, <8 x i16> %584
  %603 = sub <8 x i16> %581, %408
  %604 = sub <8 x i16> %584, %407
  %605 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %604, <8 x i16> %603) #8
  %606 = ashr <16 x i8> %605, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %607 = sub <16 x i8> zeroinitializer, %605
  %608 = icmp slt <16 x i8> %605, zeroinitializer
  %609 = select <16 x i1> %608, <16 x i8> %607, <16 x i8> %605
  %610 = bitcast <16 x i8> %609 to <8 x i16>
  %611 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %610, <8 x i16> %425) #8
  %612 = bitcast <8 x i16> %611 to <16 x i8>
  %613 = and <16 x i8> %422, %612
  %614 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %418, <16 x i8> %613) #8
  %615 = icmp ult <16 x i8> %609, %614
  %616 = select <16 x i1> %615, <16 x i8> %609, <16 x i8> %614
  %617 = add <16 x i8> %616, %606
  %618 = xor <16 x i8> %617, %606
  %619 = add <16 x i8> %495, %433
  %620 = add <16 x i8> %618, %556
  %621 = load i32, i32* getelementptr inbounds ([2 x i32], [2 x i32]* @cdef_sec_taps, i64 0, i64 0), align 4
  %622 = trunc i32 %621 to i8
  %623 = insertelement <16 x i8> undef, i8 %622, i32 0
  %624 = shufflevector <16 x i8> %623, <16 x i8> undef, <16 x i32> zeroinitializer
  %625 = shufflevector <16 x i8> %620, <16 x i8> %619, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %626 = shufflevector <16 x i8> %620, <16 x i8> %619, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %627 = tail call <8 x i16> @llvm.x86.ssse3.pmadd.ub.sw.128(<16 x i8> %624, <16 x i8> %625) #8
  %628 = tail call <8 x i16> @llvm.x86.ssse3.pmadd.ub.sw.128(<16 x i8> %624, <16 x i8> %626) #8
  %629 = add <8 x i16> %627, %353
  %630 = add <8 x i16> %628, %352
  %631 = sext i32 %21 to i64
  %632 = getelementptr inbounds i16, i16* %2, i64 %631
  %633 = bitcast i16* %632 to i64*
  %634 = load i64, i64* %633, align 1
  %635 = add nsw i32 %21, 144
  %636 = sext i32 %635 to i64
  %637 = getelementptr inbounds i16, i16* %2, i64 %636
  %638 = bitcast i16* %637 to i64*
  %639 = load i64, i64* %638, align 1
  %640 = insertelement <2 x i64> undef, i64 %639, i32 0
  %641 = add nsw i32 %21, 288
  %642 = sext i32 %641 to i64
  %643 = getelementptr inbounds i16, i16* %2, i64 %642
  %644 = bitcast i16* %643 to i64*
  %645 = load i64, i64* %644, align 1
  %646 = add nsw i32 %21, 432
  %647 = sext i32 %646 to i64
  %648 = getelementptr inbounds i16, i16* %2, i64 %647
  %649 = bitcast i16* %648 to i64*
  %650 = load i64, i64* %649, align 1
  %651 = insertelement <2 x i64> undef, i64 %650, i32 0
  %652 = insertelement <2 x i64> %640, i64 %634, i32 1
  %653 = insertelement <2 x i64> %651, i64 %645, i32 1
  %654 = bitcast <2 x i64> %652 to <8 x i16>
  %655 = icmp eq <8 x i16> %654, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %656 = sext <8 x i1> %655 to <8 x i16>
  %657 = bitcast <2 x i64> %653 to <8 x i16>
  %658 = icmp eq <8 x i16> %657, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %659 = sext <8 x i1> %658 to <8 x i16>
  %660 = bitcast <8 x i16> %659 to <2 x i64>
  %661 = bitcast <8 x i16> %656 to <2 x i64>
  %662 = xor <2 x i64> %661, <i64 -1, i64 -1>
  %663 = and <2 x i64> %652, %662
  %664 = xor <2 x i64> %660, <i64 -1, i64 -1>
  %665 = and <2 x i64> %653, %664
  %666 = bitcast <2 x i64> %663 to <8 x i16>
  %667 = icmp sgt <8 x i16> %595, %666
  %668 = select <8 x i1> %667, <8 x i16> %595, <8 x i16> %666
  %669 = bitcast <2 x i64> %665 to <8 x i16>
  %670 = icmp sgt <8 x i16> %598, %669
  %671 = select <8 x i1> %670, <8 x i16> %598, <8 x i16> %669
  %672 = icmp slt <8 x i16> %600, %654
  %673 = select <8 x i1> %672, <8 x i16> %600, <8 x i16> %654
  %674 = icmp slt <8 x i16> %602, %657
  %675 = select <8 x i1> %674, <8 x i16> %602, <8 x i16> %657
  %676 = sub <8 x i16> %654, %408
  %677 = sub <8 x i16> %657, %407
  %678 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %677, <8 x i16> %676) #8
  %679 = ashr <16 x i8> %678, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %680 = sub <16 x i8> zeroinitializer, %678
  %681 = icmp slt <16 x i8> %678, zeroinitializer
  %682 = select <16 x i1> %681, <16 x i8> %680, <16 x i8> %678
  %683 = bitcast <16 x i8> %682 to <8 x i16>
  %684 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %683, <8 x i16> %425) #8
  %685 = bitcast <8 x i16> %684 to <16 x i8>
  %686 = and <16 x i8> %422, %685
  %687 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %418, <16 x i8> %686) #8
  %688 = icmp ult <16 x i8> %682, %687
  %689 = select <16 x i1> %688, <16 x i8> %682, <16 x i8> %687
  %690 = add <16 x i8> %689, %679
  %691 = xor <16 x i8> %690, %679
  %692 = sub nsw i32 0, %21
  %693 = sext i32 %692 to i64
  %694 = getelementptr inbounds i16, i16* %2, i64 %693
  %695 = bitcast i16* %694 to i64*
  %696 = load i64, i64* %695, align 1
  %697 = sub nsw i32 144, %21
  %698 = sext i32 %697 to i64
  %699 = getelementptr inbounds i16, i16* %2, i64 %698
  %700 = bitcast i16* %699 to i64*
  %701 = load i64, i64* %700, align 1
  %702 = insertelement <2 x i64> undef, i64 %701, i32 0
  %703 = sub nsw i32 288, %21
  %704 = sext i32 %703 to i64
  %705 = getelementptr inbounds i16, i16* %2, i64 %704
  %706 = bitcast i16* %705 to i64*
  %707 = load i64, i64* %706, align 1
  %708 = sub nsw i32 432, %21
  %709 = sext i32 %708 to i64
  %710 = getelementptr inbounds i16, i16* %2, i64 %709
  %711 = bitcast i16* %710 to i64*
  %712 = load i64, i64* %711, align 1
  %713 = insertelement <2 x i64> undef, i64 %712, i32 0
  %714 = insertelement <2 x i64> %702, i64 %696, i32 1
  %715 = insertelement <2 x i64> %713, i64 %707, i32 1
  %716 = bitcast <2 x i64> %714 to <8 x i16>
  %717 = icmp eq <8 x i16> %716, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %718 = sext <8 x i1> %717 to <8 x i16>
  %719 = bitcast <2 x i64> %715 to <8 x i16>
  %720 = icmp eq <8 x i16> %719, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %721 = sext <8 x i1> %720 to <8 x i16>
  %722 = bitcast <8 x i16> %721 to <2 x i64>
  %723 = bitcast <8 x i16> %718 to <2 x i64>
  %724 = xor <2 x i64> %723, <i64 -1, i64 -1>
  %725 = and <2 x i64> %714, %724
  %726 = xor <2 x i64> %722, <i64 -1, i64 -1>
  %727 = and <2 x i64> %715, %726
  %728 = bitcast <2 x i64> %725 to <8 x i16>
  %729 = icmp sgt <8 x i16> %668, %728
  %730 = select <8 x i1> %729, <8 x i16> %668, <8 x i16> %728
  %731 = bitcast <2 x i64> %727 to <8 x i16>
  %732 = icmp sgt <8 x i16> %671, %731
  %733 = select <8 x i1> %732, <8 x i16> %671, <8 x i16> %731
  %734 = icmp slt <8 x i16> %673, %716
  %735 = select <8 x i1> %734, <8 x i16> %673, <8 x i16> %716
  %736 = icmp slt <8 x i16> %675, %719
  %737 = select <8 x i1> %736, <8 x i16> %675, <8 x i16> %719
  %738 = sub <8 x i16> %716, %408
  %739 = sub <8 x i16> %719, %407
  %740 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %739, <8 x i16> %738) #8
  %741 = ashr <16 x i8> %740, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %742 = sub <16 x i8> zeroinitializer, %740
  %743 = icmp slt <16 x i8> %740, zeroinitializer
  %744 = select <16 x i1> %743, <16 x i8> %742, <16 x i8> %740
  %745 = bitcast <16 x i8> %744 to <8 x i16>
  %746 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %745, <8 x i16> %425) #8
  %747 = bitcast <8 x i16> %746 to <16 x i8>
  %748 = and <16 x i8> %422, %747
  %749 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %418, <16 x i8> %748) #8
  %750 = icmp ult <16 x i8> %744, %749
  %751 = select <16 x i1> %750, <16 x i8> %744, <16 x i8> %749
  %752 = add <16 x i8> %751, %741
  %753 = xor <16 x i8> %752, %741
  %754 = sext i32 %28 to i64
  %755 = getelementptr inbounds i16, i16* %2, i64 %754
  %756 = bitcast i16* %755 to i64*
  %757 = load i64, i64* %756, align 1
  %758 = add nsw i32 %28, 144
  %759 = sext i32 %758 to i64
  %760 = getelementptr inbounds i16, i16* %2, i64 %759
  %761 = bitcast i16* %760 to i64*
  %762 = load i64, i64* %761, align 1
  %763 = insertelement <2 x i64> undef, i64 %762, i32 0
  %764 = add nsw i32 %28, 288
  %765 = sext i32 %764 to i64
  %766 = getelementptr inbounds i16, i16* %2, i64 %765
  %767 = bitcast i16* %766 to i64*
  %768 = load i64, i64* %767, align 1
  %769 = add nsw i32 %28, 432
  %770 = sext i32 %769 to i64
  %771 = getelementptr inbounds i16, i16* %2, i64 %770
  %772 = bitcast i16* %771 to i64*
  %773 = load i64, i64* %772, align 1
  %774 = insertelement <2 x i64> undef, i64 %773, i32 0
  %775 = insertelement <2 x i64> %763, i64 %757, i32 1
  %776 = insertelement <2 x i64> %774, i64 %768, i32 1
  %777 = bitcast <2 x i64> %775 to <8 x i16>
  %778 = icmp eq <8 x i16> %777, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %779 = sext <8 x i1> %778 to <8 x i16>
  %780 = bitcast <2 x i64> %776 to <8 x i16>
  %781 = icmp eq <8 x i16> %780, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %782 = sext <8 x i1> %781 to <8 x i16>
  %783 = bitcast <8 x i16> %782 to <2 x i64>
  %784 = bitcast <8 x i16> %779 to <2 x i64>
  %785 = xor <2 x i64> %784, <i64 -1, i64 -1>
  %786 = and <2 x i64> %775, %785
  %787 = xor <2 x i64> %783, <i64 -1, i64 -1>
  %788 = and <2 x i64> %776, %787
  %789 = bitcast <2 x i64> %786 to <8 x i16>
  %790 = icmp sgt <8 x i16> %730, %789
  %791 = select <8 x i1> %790, <8 x i16> %730, <8 x i16> %789
  %792 = bitcast <2 x i64> %788 to <8 x i16>
  %793 = icmp sgt <8 x i16> %733, %792
  %794 = select <8 x i1> %793, <8 x i16> %733, <8 x i16> %792
  %795 = icmp slt <8 x i16> %735, %777
  %796 = select <8 x i1> %795, <8 x i16> %735, <8 x i16> %777
  %797 = icmp slt <8 x i16> %737, %780
  %798 = select <8 x i1> %797, <8 x i16> %737, <8 x i16> %780
  %799 = sub <8 x i16> %777, %408
  %800 = sub <8 x i16> %780, %407
  %801 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %800, <8 x i16> %799) #8
  %802 = ashr <16 x i8> %801, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %803 = sub <16 x i8> zeroinitializer, %801
  %804 = icmp slt <16 x i8> %801, zeroinitializer
  %805 = select <16 x i1> %804, <16 x i8> %803, <16 x i8> %801
  %806 = bitcast <16 x i8> %805 to <8 x i16>
  %807 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %806, <8 x i16> %425) #8
  %808 = bitcast <8 x i16> %807 to <16 x i8>
  %809 = and <16 x i8> %422, %808
  %810 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %418, <16 x i8> %809) #8
  %811 = icmp ult <16 x i8> %805, %810
  %812 = select <16 x i1> %811, <16 x i8> %805, <16 x i8> %810
  %813 = add <16 x i8> %812, %802
  %814 = xor <16 x i8> %813, %802
  %815 = sub nsw i32 0, %28
  %816 = sext i32 %815 to i64
  %817 = getelementptr inbounds i16, i16* %2, i64 %816
  %818 = bitcast i16* %817 to i64*
  %819 = load i64, i64* %818, align 1
  %820 = sub nsw i32 144, %28
  %821 = sext i32 %820 to i64
  %822 = getelementptr inbounds i16, i16* %2, i64 %821
  %823 = bitcast i16* %822 to i64*
  %824 = load i64, i64* %823, align 1
  %825 = insertelement <2 x i64> undef, i64 %824, i32 0
  %826 = sub nsw i32 288, %28
  %827 = sext i32 %826 to i64
  %828 = getelementptr inbounds i16, i16* %2, i64 %827
  %829 = bitcast i16* %828 to i64*
  %830 = load i64, i64* %829, align 1
  %831 = sub nsw i32 432, %28
  %832 = sext i32 %831 to i64
  %833 = getelementptr inbounds i16, i16* %2, i64 %832
  %834 = bitcast i16* %833 to i64*
  %835 = load i64, i64* %834, align 1
  %836 = insertelement <2 x i64> undef, i64 %835, i32 0
  %837 = insertelement <2 x i64> %825, i64 %819, i32 1
  %838 = insertelement <2 x i64> %836, i64 %830, i32 1
  %839 = bitcast <2 x i64> %837 to <8 x i16>
  %840 = icmp eq <8 x i16> %839, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %841 = sext <8 x i1> %840 to <8 x i16>
  %842 = bitcast <2 x i64> %838 to <8 x i16>
  %843 = icmp eq <8 x i16> %842, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %844 = sext <8 x i1> %843 to <8 x i16>
  %845 = bitcast <8 x i16> %844 to <2 x i64>
  %846 = bitcast <8 x i16> %841 to <2 x i64>
  %847 = xor <2 x i64> %846, <i64 -1, i64 -1>
  %848 = and <2 x i64> %837, %847
  %849 = xor <2 x i64> %845, <i64 -1, i64 -1>
  %850 = and <2 x i64> %838, %849
  %851 = bitcast <2 x i64> %848 to <8 x i16>
  %852 = icmp sgt <8 x i16> %791, %851
  %853 = select <8 x i1> %852, <8 x i16> %791, <8 x i16> %851
  %854 = bitcast <2 x i64> %850 to <8 x i16>
  %855 = icmp sgt <8 x i16> %794, %854
  %856 = select <8 x i1> %855, <8 x i16> %794, <8 x i16> %854
  %857 = bitcast <8 x i16> %856 to <2 x i64>
  %858 = bitcast <8 x i16> %853 to <2 x i64>
  %859 = icmp slt <8 x i16> %796, %839
  %860 = select <8 x i1> %859, <8 x i16> %796, <8 x i16> %839
  %861 = icmp slt <8 x i16> %798, %842
  %862 = select <8 x i1> %861, <8 x i16> %798, <8 x i16> %842
  %863 = bitcast <8 x i16> %862 to <2 x i64>
  %864 = bitcast <8 x i16> %860 to <2 x i64>
  %865 = sub <8 x i16> %839, %408
  %866 = sub <8 x i16> %842, %407
  %867 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %866, <8 x i16> %865) #8
  %868 = ashr <16 x i8> %867, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %869 = sub <16 x i8> zeroinitializer, %867
  %870 = icmp slt <16 x i8> %867, zeroinitializer
  %871 = select <16 x i1> %870, <16 x i8> %869, <16 x i8> %867
  %872 = bitcast <16 x i8> %871 to <8 x i16>
  %873 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %872, <8 x i16> %425) #8
  %874 = bitcast <8 x i16> %873 to <16 x i8>
  %875 = and <16 x i8> %422, %874
  %876 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %418, <16 x i8> %875) #8
  %877 = icmp ult <16 x i8> %871, %876
  %878 = select <16 x i1> %877, <16 x i8> %871, <16 x i8> %876
  %879 = add <16 x i8> %878, %868
  %880 = xor <16 x i8> %879, %868
  %881 = add <16 x i8> %753, %691
  %882 = add <16 x i8> %880, %814
  %883 = load i32, i32* getelementptr inbounds ([2 x i32], [2 x i32]* @cdef_sec_taps, i64 0, i64 1), align 4
  %884 = trunc i32 %883 to i8
  %885 = insertelement <16 x i8> undef, i8 %884, i32 0
  %886 = shufflevector <16 x i8> %885, <16 x i8> undef, <16 x i32> zeroinitializer
  %887 = shufflevector <16 x i8> %882, <16 x i8> %881, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %888 = shufflevector <16 x i8> %882, <16 x i8> %881, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %889 = tail call <8 x i16> @llvm.x86.ssse3.pmadd.ub.sw.128(<16 x i8> %886, <16 x i8> %887) #8
  %890 = tail call <8 x i16> @llvm.x86.ssse3.pmadd.ub.sw.128(<16 x i8> %886, <16 x i8> %888) #8
  %891 = add <8 x i16> %629, %889
  %892 = add <8 x i16> %630, %890
  br label %893

893:                                              ; preds = %354, %357
  %894 = phi <8 x i16> [ %356, %354 ], [ %407, %357 ]
  %895 = phi <8 x i16> [ %355, %354 ], [ %408, %357 ]
  %896 = phi <2 x i64> [ %348, %354 ], [ %863, %357 ]
  %897 = phi <2 x i64> [ %349, %354 ], [ %864, %357 ]
  %898 = phi <2 x i64> [ %350, %354 ], [ %857, %357 ]
  %899 = phi <2 x i64> [ %351, %354 ], [ %858, %357 ]
  %900 = phi <8 x i16> [ %352, %354 ], [ %892, %357 ]
  %901 = phi <8 x i16> [ %353, %354 ], [ %891, %357 ]
  %902 = ashr <8 x i16> %901, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %903 = ashr <8 x i16> %900, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %904 = add <8 x i16> %901, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %905 = add <8 x i16> %904, %902
  %906 = add <8 x i16> %900, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %907 = add <8 x i16> %906, %903
  %908 = ashr <8 x i16> %905, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %909 = ashr <8 x i16> %907, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %910 = add <8 x i16> %908, %895
  %911 = add <8 x i16> %909, %894
  %912 = bitcast <2 x i64> %897 to <8 x i16>
  %913 = icmp sgt <8 x i16> %910, %912
  %914 = select <8 x i1> %913, <8 x i16> %910, <8 x i16> %912
  %915 = bitcast <2 x i64> %896 to <8 x i16>
  %916 = icmp sgt <8 x i16> %911, %915
  %917 = select <8 x i1> %916, <8 x i16> %911, <8 x i16> %915
  %918 = bitcast <2 x i64> %899 to <8 x i16>
  %919 = icmp slt <8 x i16> %914, %918
  %920 = select <8 x i1> %919, <8 x i16> %914, <8 x i16> %918
  %921 = bitcast <2 x i64> %898 to <8 x i16>
  %922 = icmp slt <8 x i16> %917, %921
  %923 = select <8 x i1> %922, <8 x i16> %917, <8 x i16> %921
  %924 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %923, <8 x i16> %920) #8
  %925 = bitcast <16 x i8> %924 to <2 x i64>
  %926 = shufflevector <16 x i8> %924, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %927 = shufflevector <16 x i8> %926, <16 x i8> undef, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %928 = bitcast <16 x i8> %927 to <4 x i32>
  %929 = extractelement <4 x i32> %928, i32 0
  %930 = bitcast i8* %0 to i32*
  store i32 %929, i32* %930, align 4
  %931 = sext i32 %1 to i64
  %932 = getelementptr inbounds i8, i8* %0, i64 %931
  %933 = bitcast <16 x i8> %926 to <4 x i32>
  %934 = extractelement <4 x i32> %933, i32 0
  %935 = bitcast i8* %932 to i32*
  store i32 %934, i32* %935, align 4
  %936 = shl nsw i32 %1, 1
  %937 = sext i32 %936 to i64
  %938 = getelementptr inbounds i8, i8* %0, i64 %937
  %939 = insertelement <2 x i64> %925, i64 0, i32 1
  %940 = bitcast <2 x i64> %939 to <16 x i8>
  %941 = shufflevector <16 x i8> %940, <16 x i8> undef, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %942 = bitcast <16 x i8> %941 to <4 x i32>
  %943 = extractelement <4 x i32> %942, i32 0
  %944 = bitcast i8* %938 to i32*
  store i32 %943, i32* %944, align 4
  %945 = mul nsw i32 %1, 3
  %946 = sext i32 %945 to i64
  %947 = getelementptr inbounds i8, i8* %0, i64 %946
  %948 = bitcast <2 x i64> %939 to <4 x i32>
  %949 = extractelement <4 x i32> %948, i32 0
  %950 = bitcast i8* %947 to i32*
  store i32 %949, i32* %950, align 4
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @cdef_filter_block_8x8_8_ssse3(i8* nocapture, i32, i16*, i32, i32, i32, i32, i32, i32) local_unnamed_addr #0 {
  %10 = sext i32 %5 to i64
  %11 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %10, i64 0
  %12 = load i32, i32* %11, align 8
  %13 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %10, i64 1
  %14 = load i32, i32* %13, align 4
  %15 = add nsw i32 %5, 2
  %16 = and i32 %15, 7
  %17 = zext i32 %16 to i64
  %18 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %17, i64 0
  %19 = load i32, i32* %18, align 8
  %20 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %17, i64 1
  %21 = load i32, i32* %20, align 4
  %22 = add nsw i32 %5, 6
  %23 = and i32 %22, 7
  %24 = zext i32 %23 to i64
  %25 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %24, i64 0
  %26 = load i32, i32* %25, align 8
  %27 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %24, i64 1
  %28 = load i32, i32* %27, align 4
  %29 = lshr i32 %3, %8
  %30 = and i32 %29, 1
  %31 = zext i32 %30 to i64
  %32 = getelementptr inbounds [2 x [2 x i32]], [2 x [2 x i32]]* @cdef_pri_taps, i64 0, i64 %31, i64 0
  %33 = icmp eq i32 %3, 0
  br i1 %33, label %40, label %34

34:                                               ; preds = %9
  %35 = tail call i32 @llvm.ctlz.i32(i32 %3, i1 true) #8, !range !2
  %36 = xor i32 %35, 31
  %37 = icmp sgt i32 %36, %6
  %38 = sub nsw i32 %6, %36
  %39 = select i1 %37, i32 0, i32 %38
  br label %40

40:                                               ; preds = %34, %9
  %41 = phi i32 [ %6, %9 ], [ %39, %34 ]
  %42 = icmp eq i32 %4, 0
  br i1 %42, label %49, label %43

43:                                               ; preds = %40
  %44 = tail call i32 @llvm.ctlz.i32(i32 %4, i1 true) #8, !range !2
  %45 = xor i32 %44, 31
  %46 = icmp sgt i32 %45, %7
  %47 = sub nsw i32 %7, %45
  %48 = select i1 %46, i32 0, i32 %47
  br label %49

49:                                               ; preds = %43, %40
  %50 = phi i32 [ %7, %40 ], [ %48, %43 ]
  %51 = trunc i32 %3 to i8
  %52 = insertelement <16 x i8> undef, i8 %51, i32 0
  %53 = shufflevector <16 x i8> %52, <16 x i8> undef, <16 x i32> zeroinitializer
  %54 = lshr i32 255, %41
  %55 = trunc i32 %54 to i8
  %56 = insertelement <16 x i8> undef, i8 %55, i32 0
  %57 = shufflevector <16 x i8> %56, <16 x i8> undef, <16 x i32> zeroinitializer
  %58 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %41, i32 0
  %59 = bitcast <4 x i32> %58 to <8 x i16>
  %60 = load i32, i32* %32, align 8
  %61 = trunc i32 %60 to i8
  %62 = insertelement <16 x i8> undef, i8 %61, i32 0
  %63 = shufflevector <16 x i8> %62, <16 x i8> undef, <16 x i32> zeroinitializer
  %64 = getelementptr inbounds [2 x [2 x i32]], [2 x [2 x i32]]* @cdef_pri_taps, i64 0, i64 %31, i64 1
  %65 = load i32, i32* %64, align 4
  %66 = trunc i32 %65 to i8
  %67 = insertelement <16 x i8> undef, i8 %66, i32 0
  %68 = shufflevector <16 x i8> %67, <16 x i8> undef, <16 x i32> zeroinitializer
  %69 = trunc i32 %4 to i8
  %70 = insertelement <16 x i8> undef, i8 %69, i32 0
  %71 = shufflevector <16 x i8> %70, <16 x i8> undef, <16 x i32> zeroinitializer
  %72 = lshr i32 255, %50
  %73 = trunc i32 %72 to i8
  %74 = insertelement <16 x i8> undef, i8 %73, i32 0
  %75 = shufflevector <16 x i8> %74, <16 x i8> undef, <16 x i32> zeroinitializer
  %76 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %50, i32 0
  %77 = bitcast <4 x i32> %76 to <8 x i16>
  %78 = load i32, i32* getelementptr inbounds ([2 x i32], [2 x i32]* @cdef_sec_taps, i64 0, i64 0), align 4
  %79 = trunc i32 %78 to i8
  %80 = insertelement <16 x i8> undef, i8 %79, i32 0
  %81 = shufflevector <16 x i8> %80, <16 x i8> undef, <16 x i32> zeroinitializer
  %82 = load i32, i32* getelementptr inbounds ([2 x i32], [2 x i32]* @cdef_sec_taps, i64 0, i64 1), align 4
  %83 = trunc i32 %82 to i8
  %84 = insertelement <16 x i8> undef, i8 %83, i32 0
  %85 = shufflevector <16 x i8> %84, <16 x i8> undef, <16 x i32> zeroinitializer
  %86 = sext i32 %1 to i64
  %87 = sext i32 %12 to i64
  %88 = sext i32 %14 to i64
  %89 = sext i32 %19 to i64
  %90 = sext i32 %26 to i64
  %91 = sext i32 %21 to i64
  %92 = sext i32 %28 to i64
  br label %93

93:                                               ; preds = %49, %93
  %94 = phi i64 [ 0, %49 ], [ %736, %93 ]
  %95 = mul nuw nsw i64 %94, 144
  %96 = getelementptr inbounds i16, i16* %2, i64 %95
  %97 = bitcast i16* %96 to <8 x i16>*
  %98 = load <8 x i16>, <8 x i16>* %97, align 16
  %99 = or i64 %94, 1
  %100 = mul nuw nsw i64 %99, 144
  %101 = getelementptr inbounds i16, i16* %2, i64 %100
  %102 = bitcast i16* %101 to <8 x i16>*
  %103 = load <8 x i16>, <8 x i16>* %102, align 16
  %104 = add nsw i64 %95, %87
  %105 = getelementptr inbounds i16, i16* %2, i64 %104
  %106 = bitcast i16* %105 to i8*
  %107 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %106) #8
  %108 = bitcast <16 x i8> %107 to <2 x i64>
  %109 = add nsw i64 %100, %87
  %110 = getelementptr inbounds i16, i16* %2, i64 %109
  %111 = bitcast i16* %110 to i8*
  %112 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %111) #8
  %113 = bitcast <16 x i8> %112 to <2 x i64>
  %114 = bitcast <16 x i8> %107 to <8 x i16>
  %115 = icmp eq <8 x i16> %114, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %116 = sext <8 x i1> %115 to <8 x i16>
  %117 = bitcast <16 x i8> %112 to <8 x i16>
  %118 = icmp eq <8 x i16> %117, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %119 = sext <8 x i1> %118 to <8 x i16>
  %120 = bitcast <8 x i16> %119 to <2 x i64>
  %121 = bitcast <8 x i16> %116 to <2 x i64>
  %122 = xor <2 x i64> %121, <i64 -1, i64 -1>
  %123 = and <2 x i64> %122, %108
  %124 = xor <2 x i64> %120, <i64 -1, i64 -1>
  %125 = and <2 x i64> %124, %113
  %126 = bitcast <2 x i64> %123 to <8 x i16>
  %127 = icmp sgt <8 x i16> %98, %126
  %128 = select <8 x i1> %127, <8 x i16> %98, <8 x i16> %126
  %129 = bitcast <2 x i64> %125 to <8 x i16>
  %130 = icmp sgt <8 x i16> %103, %129
  %131 = select <8 x i1> %130, <8 x i16> %103, <8 x i16> %129
  %132 = icmp slt <8 x i16> %98, %114
  %133 = select <8 x i1> %132, <8 x i16> %98, <8 x i16> %114
  %134 = icmp slt <8 x i16> %103, %117
  %135 = select <8 x i1> %134, <8 x i16> %103, <8 x i16> %117
  %136 = sub <8 x i16> %114, %98
  %137 = sub <8 x i16> %117, %103
  %138 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %137, <8 x i16> %136) #8
  %139 = ashr <16 x i8> %138, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %140 = sub <16 x i8> zeroinitializer, %138
  %141 = icmp slt <16 x i8> %138, zeroinitializer
  %142 = select <16 x i1> %141, <16 x i8> %140, <16 x i8> %138
  %143 = bitcast <16 x i8> %142 to <8 x i16>
  %144 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %143, <8 x i16> %59) #8
  %145 = bitcast <8 x i16> %144 to <16 x i8>
  %146 = and <16 x i8> %57, %145
  %147 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %53, <16 x i8> %146) #8
  %148 = icmp ult <16 x i8> %142, %147
  %149 = select <16 x i1> %148, <16 x i8> %142, <16 x i8> %147
  %150 = add <16 x i8> %149, %139
  %151 = xor <16 x i8> %150, %139
  %152 = sub nsw i64 %95, %87
  %153 = getelementptr inbounds i16, i16* %2, i64 %152
  %154 = bitcast i16* %153 to i8*
  %155 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %154) #8
  %156 = bitcast <16 x i8> %155 to <2 x i64>
  %157 = sub nsw i64 %100, %87
  %158 = getelementptr inbounds i16, i16* %2, i64 %157
  %159 = bitcast i16* %158 to i8*
  %160 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %159) #8
  %161 = bitcast <16 x i8> %160 to <2 x i64>
  %162 = bitcast <16 x i8> %155 to <8 x i16>
  %163 = icmp eq <8 x i16> %162, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %164 = sext <8 x i1> %163 to <8 x i16>
  %165 = bitcast <16 x i8> %160 to <8 x i16>
  %166 = icmp eq <8 x i16> %165, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %167 = sext <8 x i1> %166 to <8 x i16>
  %168 = bitcast <8 x i16> %167 to <2 x i64>
  %169 = bitcast <8 x i16> %164 to <2 x i64>
  %170 = xor <2 x i64> %169, <i64 -1, i64 -1>
  %171 = and <2 x i64> %170, %156
  %172 = xor <2 x i64> %168, <i64 -1, i64 -1>
  %173 = and <2 x i64> %172, %161
  %174 = bitcast <2 x i64> %171 to <8 x i16>
  %175 = icmp sgt <8 x i16> %128, %174
  %176 = select <8 x i1> %175, <8 x i16> %128, <8 x i16> %174
  %177 = bitcast <2 x i64> %173 to <8 x i16>
  %178 = icmp sgt <8 x i16> %131, %177
  %179 = select <8 x i1> %178, <8 x i16> %131, <8 x i16> %177
  %180 = icmp slt <8 x i16> %133, %162
  %181 = select <8 x i1> %180, <8 x i16> %133, <8 x i16> %162
  %182 = icmp slt <8 x i16> %135, %165
  %183 = select <8 x i1> %182, <8 x i16> %135, <8 x i16> %165
  %184 = sub <8 x i16> %162, %98
  %185 = sub <8 x i16> %165, %103
  %186 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %185, <8 x i16> %184) #8
  %187 = ashr <16 x i8> %186, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %188 = sub <16 x i8> zeroinitializer, %186
  %189 = icmp slt <16 x i8> %186, zeroinitializer
  %190 = select <16 x i1> %189, <16 x i8> %188, <16 x i8> %186
  %191 = bitcast <16 x i8> %190 to <8 x i16>
  %192 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %191, <8 x i16> %59) #8
  %193 = bitcast <8 x i16> %192 to <16 x i8>
  %194 = and <16 x i8> %57, %193
  %195 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %53, <16 x i8> %194) #8
  %196 = icmp ult <16 x i8> %190, %195
  %197 = select <16 x i1> %196, <16 x i8> %190, <16 x i8> %195
  %198 = add <16 x i8> %197, %187
  %199 = xor <16 x i8> %198, %187
  %200 = shufflevector <16 x i8> %199, <16 x i8> %151, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %201 = shufflevector <16 x i8> %199, <16 x i8> %151, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %202 = tail call <8 x i16> @llvm.x86.ssse3.pmadd.ub.sw.128(<16 x i8> %63, <16 x i8> %200) #8
  %203 = tail call <8 x i16> @llvm.x86.ssse3.pmadd.ub.sw.128(<16 x i8> %63, <16 x i8> %201) #8
  %204 = add nsw i64 %95, %88
  %205 = getelementptr inbounds i16, i16* %2, i64 %204
  %206 = bitcast i16* %205 to i8*
  %207 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %206) #8
  %208 = bitcast <16 x i8> %207 to <2 x i64>
  %209 = add nsw i64 %100, %88
  %210 = getelementptr inbounds i16, i16* %2, i64 %209
  %211 = bitcast i16* %210 to i8*
  %212 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %211) #8
  %213 = bitcast <16 x i8> %212 to <2 x i64>
  %214 = bitcast <16 x i8> %207 to <8 x i16>
  %215 = icmp eq <8 x i16> %214, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %216 = sext <8 x i1> %215 to <8 x i16>
  %217 = bitcast <16 x i8> %212 to <8 x i16>
  %218 = icmp eq <8 x i16> %217, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %219 = sext <8 x i1> %218 to <8 x i16>
  %220 = bitcast <8 x i16> %219 to <2 x i64>
  %221 = bitcast <8 x i16> %216 to <2 x i64>
  %222 = xor <2 x i64> %221, <i64 -1, i64 -1>
  %223 = and <2 x i64> %222, %208
  %224 = xor <2 x i64> %220, <i64 -1, i64 -1>
  %225 = and <2 x i64> %224, %213
  %226 = bitcast <2 x i64> %223 to <8 x i16>
  %227 = icmp sgt <8 x i16> %176, %226
  %228 = select <8 x i1> %227, <8 x i16> %176, <8 x i16> %226
  %229 = bitcast <2 x i64> %225 to <8 x i16>
  %230 = icmp sgt <8 x i16> %179, %229
  %231 = select <8 x i1> %230, <8 x i16> %179, <8 x i16> %229
  %232 = icmp slt <8 x i16> %181, %214
  %233 = select <8 x i1> %232, <8 x i16> %181, <8 x i16> %214
  %234 = icmp slt <8 x i16> %183, %217
  %235 = select <8 x i1> %234, <8 x i16> %183, <8 x i16> %217
  %236 = sub <8 x i16> %214, %98
  %237 = sub <8 x i16> %217, %103
  %238 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %237, <8 x i16> %236) #8
  %239 = ashr <16 x i8> %238, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %240 = sub <16 x i8> zeroinitializer, %238
  %241 = icmp slt <16 x i8> %238, zeroinitializer
  %242 = select <16 x i1> %241, <16 x i8> %240, <16 x i8> %238
  %243 = bitcast <16 x i8> %242 to <8 x i16>
  %244 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %243, <8 x i16> %59) #8
  %245 = bitcast <8 x i16> %244 to <16 x i8>
  %246 = and <16 x i8> %57, %245
  %247 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %53, <16 x i8> %246) #8
  %248 = icmp ult <16 x i8> %242, %247
  %249 = select <16 x i1> %248, <16 x i8> %242, <16 x i8> %247
  %250 = add <16 x i8> %249, %239
  %251 = xor <16 x i8> %250, %239
  %252 = sub nsw i64 %95, %88
  %253 = getelementptr inbounds i16, i16* %2, i64 %252
  %254 = bitcast i16* %253 to i8*
  %255 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %254) #8
  %256 = bitcast <16 x i8> %255 to <2 x i64>
  %257 = sub nsw i64 %100, %88
  %258 = getelementptr inbounds i16, i16* %2, i64 %257
  %259 = bitcast i16* %258 to i8*
  %260 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %259) #8
  %261 = bitcast <16 x i8> %260 to <2 x i64>
  %262 = bitcast <16 x i8> %255 to <8 x i16>
  %263 = icmp eq <8 x i16> %262, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %264 = sext <8 x i1> %263 to <8 x i16>
  %265 = bitcast <16 x i8> %260 to <8 x i16>
  %266 = icmp eq <8 x i16> %265, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %267 = sext <8 x i1> %266 to <8 x i16>
  %268 = bitcast <8 x i16> %267 to <2 x i64>
  %269 = bitcast <8 x i16> %264 to <2 x i64>
  %270 = xor <2 x i64> %269, <i64 -1, i64 -1>
  %271 = and <2 x i64> %270, %256
  %272 = xor <2 x i64> %268, <i64 -1, i64 -1>
  %273 = and <2 x i64> %272, %261
  %274 = bitcast <2 x i64> %271 to <8 x i16>
  %275 = icmp sgt <8 x i16> %228, %274
  %276 = select <8 x i1> %275, <8 x i16> %228, <8 x i16> %274
  %277 = bitcast <2 x i64> %273 to <8 x i16>
  %278 = icmp sgt <8 x i16> %231, %277
  %279 = select <8 x i1> %278, <8 x i16> %231, <8 x i16> %277
  %280 = icmp slt <8 x i16> %233, %262
  %281 = select <8 x i1> %280, <8 x i16> %233, <8 x i16> %262
  %282 = icmp slt <8 x i16> %235, %265
  %283 = select <8 x i1> %282, <8 x i16> %235, <8 x i16> %265
  %284 = sub <8 x i16> %262, %98
  %285 = sub <8 x i16> %265, %103
  %286 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %285, <8 x i16> %284) #8
  %287 = ashr <16 x i8> %286, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %288 = sub <16 x i8> zeroinitializer, %286
  %289 = icmp slt <16 x i8> %286, zeroinitializer
  %290 = select <16 x i1> %289, <16 x i8> %288, <16 x i8> %286
  %291 = bitcast <16 x i8> %290 to <8 x i16>
  %292 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %291, <8 x i16> %59) #8
  %293 = bitcast <8 x i16> %292 to <16 x i8>
  %294 = and <16 x i8> %57, %293
  %295 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %53, <16 x i8> %294) #8
  %296 = icmp ult <16 x i8> %290, %295
  %297 = select <16 x i1> %296, <16 x i8> %290, <16 x i8> %295
  %298 = add <16 x i8> %297, %287
  %299 = xor <16 x i8> %298, %287
  %300 = shufflevector <16 x i8> %299, <16 x i8> %251, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %301 = shufflevector <16 x i8> %299, <16 x i8> %251, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %302 = tail call <8 x i16> @llvm.x86.ssse3.pmadd.ub.sw.128(<16 x i8> %68, <16 x i8> %300) #8
  %303 = tail call <8 x i16> @llvm.x86.ssse3.pmadd.ub.sw.128(<16 x i8> %68, <16 x i8> %301) #8
  %304 = add <8 x i16> %302, %202
  %305 = add <8 x i16> %303, %203
  %306 = add nsw i64 %95, %89
  %307 = getelementptr inbounds i16, i16* %2, i64 %306
  %308 = bitcast i16* %307 to i8*
  %309 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %308) #8
  %310 = bitcast <16 x i8> %309 to <2 x i64>
  %311 = add nsw i64 %100, %89
  %312 = getelementptr inbounds i16, i16* %2, i64 %311
  %313 = bitcast i16* %312 to i8*
  %314 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %313) #8
  %315 = bitcast <16 x i8> %314 to <2 x i64>
  %316 = bitcast <16 x i8> %309 to <8 x i16>
  %317 = icmp eq <8 x i16> %316, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %318 = sext <8 x i1> %317 to <8 x i16>
  %319 = bitcast <16 x i8> %314 to <8 x i16>
  %320 = icmp eq <8 x i16> %319, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %321 = sext <8 x i1> %320 to <8 x i16>
  %322 = bitcast <8 x i16> %321 to <2 x i64>
  %323 = bitcast <8 x i16> %318 to <2 x i64>
  %324 = xor <2 x i64> %323, <i64 -1, i64 -1>
  %325 = and <2 x i64> %324, %310
  %326 = xor <2 x i64> %322, <i64 -1, i64 -1>
  %327 = and <2 x i64> %326, %315
  %328 = bitcast <2 x i64> %325 to <8 x i16>
  %329 = icmp sgt <8 x i16> %276, %328
  %330 = select <8 x i1> %329, <8 x i16> %276, <8 x i16> %328
  %331 = bitcast <2 x i64> %327 to <8 x i16>
  %332 = icmp sgt <8 x i16> %279, %331
  %333 = select <8 x i1> %332, <8 x i16> %279, <8 x i16> %331
  %334 = icmp slt <8 x i16> %281, %316
  %335 = select <8 x i1> %334, <8 x i16> %281, <8 x i16> %316
  %336 = icmp slt <8 x i16> %283, %319
  %337 = select <8 x i1> %336, <8 x i16> %283, <8 x i16> %319
  %338 = sub <8 x i16> %316, %98
  %339 = sub <8 x i16> %319, %103
  %340 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %339, <8 x i16> %338) #8
  %341 = ashr <16 x i8> %340, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %342 = sub <16 x i8> zeroinitializer, %340
  %343 = icmp slt <16 x i8> %340, zeroinitializer
  %344 = select <16 x i1> %343, <16 x i8> %342, <16 x i8> %340
  %345 = bitcast <16 x i8> %344 to <8 x i16>
  %346 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %345, <8 x i16> %77) #8
  %347 = bitcast <8 x i16> %346 to <16 x i8>
  %348 = and <16 x i8> %75, %347
  %349 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %71, <16 x i8> %348) #8
  %350 = icmp ult <16 x i8> %344, %349
  %351 = select <16 x i1> %350, <16 x i8> %344, <16 x i8> %349
  %352 = add <16 x i8> %351, %341
  %353 = xor <16 x i8> %352, %341
  %354 = sub nsw i64 %95, %89
  %355 = getelementptr inbounds i16, i16* %2, i64 %354
  %356 = bitcast i16* %355 to i8*
  %357 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %356) #8
  %358 = bitcast <16 x i8> %357 to <2 x i64>
  %359 = sub nsw i64 %100, %89
  %360 = getelementptr inbounds i16, i16* %2, i64 %359
  %361 = bitcast i16* %360 to i8*
  %362 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %361) #8
  %363 = bitcast <16 x i8> %362 to <2 x i64>
  %364 = bitcast <16 x i8> %357 to <8 x i16>
  %365 = icmp eq <8 x i16> %364, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %366 = sext <8 x i1> %365 to <8 x i16>
  %367 = bitcast <16 x i8> %362 to <8 x i16>
  %368 = icmp eq <8 x i16> %367, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %369 = sext <8 x i1> %368 to <8 x i16>
  %370 = bitcast <8 x i16> %369 to <2 x i64>
  %371 = bitcast <8 x i16> %366 to <2 x i64>
  %372 = xor <2 x i64> %371, <i64 -1, i64 -1>
  %373 = and <2 x i64> %372, %358
  %374 = xor <2 x i64> %370, <i64 -1, i64 -1>
  %375 = and <2 x i64> %374, %363
  %376 = bitcast <2 x i64> %373 to <8 x i16>
  %377 = icmp sgt <8 x i16> %330, %376
  %378 = select <8 x i1> %377, <8 x i16> %330, <8 x i16> %376
  %379 = bitcast <2 x i64> %375 to <8 x i16>
  %380 = icmp sgt <8 x i16> %333, %379
  %381 = select <8 x i1> %380, <8 x i16> %333, <8 x i16> %379
  %382 = icmp slt <8 x i16> %335, %364
  %383 = select <8 x i1> %382, <8 x i16> %335, <8 x i16> %364
  %384 = icmp slt <8 x i16> %337, %367
  %385 = select <8 x i1> %384, <8 x i16> %337, <8 x i16> %367
  %386 = sub <8 x i16> %364, %98
  %387 = sub <8 x i16> %367, %103
  %388 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %387, <8 x i16> %386) #8
  %389 = ashr <16 x i8> %388, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %390 = sub <16 x i8> zeroinitializer, %388
  %391 = icmp slt <16 x i8> %388, zeroinitializer
  %392 = select <16 x i1> %391, <16 x i8> %390, <16 x i8> %388
  %393 = bitcast <16 x i8> %392 to <8 x i16>
  %394 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %393, <8 x i16> %77) #8
  %395 = bitcast <8 x i16> %394 to <16 x i8>
  %396 = and <16 x i8> %75, %395
  %397 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %71, <16 x i8> %396) #8
  %398 = icmp ult <16 x i8> %392, %397
  %399 = select <16 x i1> %398, <16 x i8> %392, <16 x i8> %397
  %400 = add <16 x i8> %399, %389
  %401 = xor <16 x i8> %400, %389
  %402 = add nsw i64 %95, %90
  %403 = getelementptr inbounds i16, i16* %2, i64 %402
  %404 = bitcast i16* %403 to i8*
  %405 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %404) #8
  %406 = bitcast <16 x i8> %405 to <2 x i64>
  %407 = add nsw i64 %100, %90
  %408 = getelementptr inbounds i16, i16* %2, i64 %407
  %409 = bitcast i16* %408 to i8*
  %410 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %409) #8
  %411 = bitcast <16 x i8> %410 to <2 x i64>
  %412 = bitcast <16 x i8> %405 to <8 x i16>
  %413 = icmp eq <8 x i16> %412, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %414 = sext <8 x i1> %413 to <8 x i16>
  %415 = bitcast <16 x i8> %410 to <8 x i16>
  %416 = icmp eq <8 x i16> %415, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %417 = sext <8 x i1> %416 to <8 x i16>
  %418 = bitcast <8 x i16> %417 to <2 x i64>
  %419 = bitcast <8 x i16> %414 to <2 x i64>
  %420 = xor <2 x i64> %419, <i64 -1, i64 -1>
  %421 = and <2 x i64> %420, %406
  %422 = xor <2 x i64> %418, <i64 -1, i64 -1>
  %423 = and <2 x i64> %422, %411
  %424 = bitcast <2 x i64> %421 to <8 x i16>
  %425 = icmp sgt <8 x i16> %378, %424
  %426 = select <8 x i1> %425, <8 x i16> %378, <8 x i16> %424
  %427 = bitcast <2 x i64> %423 to <8 x i16>
  %428 = icmp sgt <8 x i16> %381, %427
  %429 = select <8 x i1> %428, <8 x i16> %381, <8 x i16> %427
  %430 = icmp slt <8 x i16> %383, %412
  %431 = select <8 x i1> %430, <8 x i16> %383, <8 x i16> %412
  %432 = icmp slt <8 x i16> %385, %415
  %433 = select <8 x i1> %432, <8 x i16> %385, <8 x i16> %415
  %434 = sub <8 x i16> %412, %98
  %435 = sub <8 x i16> %415, %103
  %436 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %435, <8 x i16> %434) #8
  %437 = ashr <16 x i8> %436, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %438 = sub <16 x i8> zeroinitializer, %436
  %439 = icmp slt <16 x i8> %436, zeroinitializer
  %440 = select <16 x i1> %439, <16 x i8> %438, <16 x i8> %436
  %441 = bitcast <16 x i8> %440 to <8 x i16>
  %442 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %441, <8 x i16> %77) #8
  %443 = bitcast <8 x i16> %442 to <16 x i8>
  %444 = and <16 x i8> %75, %443
  %445 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %71, <16 x i8> %444) #8
  %446 = icmp ult <16 x i8> %440, %445
  %447 = select <16 x i1> %446, <16 x i8> %440, <16 x i8> %445
  %448 = add <16 x i8> %447, %437
  %449 = xor <16 x i8> %448, %437
  %450 = sub nsw i64 %95, %90
  %451 = getelementptr inbounds i16, i16* %2, i64 %450
  %452 = bitcast i16* %451 to i8*
  %453 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %452) #8
  %454 = bitcast <16 x i8> %453 to <2 x i64>
  %455 = sub nsw i64 %100, %90
  %456 = getelementptr inbounds i16, i16* %2, i64 %455
  %457 = bitcast i16* %456 to i8*
  %458 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %457) #8
  %459 = bitcast <16 x i8> %458 to <2 x i64>
  %460 = bitcast <16 x i8> %453 to <8 x i16>
  %461 = icmp eq <8 x i16> %460, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %462 = sext <8 x i1> %461 to <8 x i16>
  %463 = bitcast <16 x i8> %458 to <8 x i16>
  %464 = icmp eq <8 x i16> %463, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %465 = sext <8 x i1> %464 to <8 x i16>
  %466 = bitcast <8 x i16> %465 to <2 x i64>
  %467 = bitcast <8 x i16> %462 to <2 x i64>
  %468 = xor <2 x i64> %467, <i64 -1, i64 -1>
  %469 = and <2 x i64> %468, %454
  %470 = xor <2 x i64> %466, <i64 -1, i64 -1>
  %471 = and <2 x i64> %470, %459
  %472 = bitcast <2 x i64> %469 to <8 x i16>
  %473 = icmp sgt <8 x i16> %426, %472
  %474 = select <8 x i1> %473, <8 x i16> %426, <8 x i16> %472
  %475 = bitcast <2 x i64> %471 to <8 x i16>
  %476 = icmp sgt <8 x i16> %429, %475
  %477 = select <8 x i1> %476, <8 x i16> %429, <8 x i16> %475
  %478 = icmp slt <8 x i16> %431, %460
  %479 = select <8 x i1> %478, <8 x i16> %431, <8 x i16> %460
  %480 = icmp slt <8 x i16> %433, %463
  %481 = select <8 x i1> %480, <8 x i16> %433, <8 x i16> %463
  %482 = sub <8 x i16> %460, %98
  %483 = sub <8 x i16> %463, %103
  %484 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %483, <8 x i16> %482) #8
  %485 = ashr <16 x i8> %484, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %486 = sub <16 x i8> zeroinitializer, %484
  %487 = icmp slt <16 x i8> %484, zeroinitializer
  %488 = select <16 x i1> %487, <16 x i8> %486, <16 x i8> %484
  %489 = bitcast <16 x i8> %488 to <8 x i16>
  %490 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %489, <8 x i16> %77) #8
  %491 = bitcast <8 x i16> %490 to <16 x i8>
  %492 = and <16 x i8> %75, %491
  %493 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %71, <16 x i8> %492) #8
  %494 = icmp ult <16 x i8> %488, %493
  %495 = select <16 x i1> %494, <16 x i8> %488, <16 x i8> %493
  %496 = add <16 x i8> %495, %485
  %497 = xor <16 x i8> %496, %485
  %498 = add <16 x i8> %401, %353
  %499 = add <16 x i8> %497, %449
  %500 = shufflevector <16 x i8> %499, <16 x i8> %498, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %501 = shufflevector <16 x i8> %499, <16 x i8> %498, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %502 = tail call <8 x i16> @llvm.x86.ssse3.pmadd.ub.sw.128(<16 x i8> %81, <16 x i8> %500) #8
  %503 = tail call <8 x i16> @llvm.x86.ssse3.pmadd.ub.sw.128(<16 x i8> %81, <16 x i8> %501) #8
  %504 = add <8 x i16> %304, %502
  %505 = add <8 x i16> %305, %503
  %506 = add nsw i64 %95, %91
  %507 = getelementptr inbounds i16, i16* %2, i64 %506
  %508 = bitcast i16* %507 to i8*
  %509 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %508) #8
  %510 = bitcast <16 x i8> %509 to <2 x i64>
  %511 = add nsw i64 %100, %91
  %512 = getelementptr inbounds i16, i16* %2, i64 %511
  %513 = bitcast i16* %512 to i8*
  %514 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %513) #8
  %515 = bitcast <16 x i8> %514 to <2 x i64>
  %516 = bitcast <16 x i8> %509 to <8 x i16>
  %517 = icmp eq <8 x i16> %516, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %518 = sext <8 x i1> %517 to <8 x i16>
  %519 = bitcast <16 x i8> %514 to <8 x i16>
  %520 = icmp eq <8 x i16> %519, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %521 = sext <8 x i1> %520 to <8 x i16>
  %522 = bitcast <8 x i16> %521 to <2 x i64>
  %523 = bitcast <8 x i16> %518 to <2 x i64>
  %524 = xor <2 x i64> %523, <i64 -1, i64 -1>
  %525 = and <2 x i64> %524, %510
  %526 = xor <2 x i64> %522, <i64 -1, i64 -1>
  %527 = and <2 x i64> %526, %515
  %528 = bitcast <2 x i64> %525 to <8 x i16>
  %529 = icmp sgt <8 x i16> %474, %528
  %530 = select <8 x i1> %529, <8 x i16> %474, <8 x i16> %528
  %531 = bitcast <2 x i64> %527 to <8 x i16>
  %532 = icmp sgt <8 x i16> %477, %531
  %533 = select <8 x i1> %532, <8 x i16> %477, <8 x i16> %531
  %534 = icmp slt <8 x i16> %479, %516
  %535 = select <8 x i1> %534, <8 x i16> %479, <8 x i16> %516
  %536 = icmp slt <8 x i16> %481, %519
  %537 = select <8 x i1> %536, <8 x i16> %481, <8 x i16> %519
  %538 = sub <8 x i16> %516, %98
  %539 = sub <8 x i16> %519, %103
  %540 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %539, <8 x i16> %538) #8
  %541 = ashr <16 x i8> %540, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %542 = sub <16 x i8> zeroinitializer, %540
  %543 = icmp slt <16 x i8> %540, zeroinitializer
  %544 = select <16 x i1> %543, <16 x i8> %542, <16 x i8> %540
  %545 = bitcast <16 x i8> %544 to <8 x i16>
  %546 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %545, <8 x i16> %77) #8
  %547 = bitcast <8 x i16> %546 to <16 x i8>
  %548 = and <16 x i8> %75, %547
  %549 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %71, <16 x i8> %548) #8
  %550 = icmp ult <16 x i8> %544, %549
  %551 = select <16 x i1> %550, <16 x i8> %544, <16 x i8> %549
  %552 = add <16 x i8> %551, %541
  %553 = xor <16 x i8> %552, %541
  %554 = sub nsw i64 %95, %91
  %555 = getelementptr inbounds i16, i16* %2, i64 %554
  %556 = bitcast i16* %555 to i8*
  %557 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %556) #8
  %558 = bitcast <16 x i8> %557 to <2 x i64>
  %559 = sub nsw i64 %100, %91
  %560 = getelementptr inbounds i16, i16* %2, i64 %559
  %561 = bitcast i16* %560 to i8*
  %562 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %561) #8
  %563 = bitcast <16 x i8> %562 to <2 x i64>
  %564 = bitcast <16 x i8> %557 to <8 x i16>
  %565 = icmp eq <8 x i16> %564, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %566 = sext <8 x i1> %565 to <8 x i16>
  %567 = bitcast <16 x i8> %562 to <8 x i16>
  %568 = icmp eq <8 x i16> %567, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %569 = sext <8 x i1> %568 to <8 x i16>
  %570 = bitcast <8 x i16> %569 to <2 x i64>
  %571 = bitcast <8 x i16> %566 to <2 x i64>
  %572 = xor <2 x i64> %571, <i64 -1, i64 -1>
  %573 = and <2 x i64> %572, %558
  %574 = xor <2 x i64> %570, <i64 -1, i64 -1>
  %575 = and <2 x i64> %574, %563
  %576 = bitcast <2 x i64> %573 to <8 x i16>
  %577 = icmp sgt <8 x i16> %530, %576
  %578 = select <8 x i1> %577, <8 x i16> %530, <8 x i16> %576
  %579 = bitcast <2 x i64> %575 to <8 x i16>
  %580 = icmp sgt <8 x i16> %533, %579
  %581 = select <8 x i1> %580, <8 x i16> %533, <8 x i16> %579
  %582 = icmp slt <8 x i16> %535, %564
  %583 = select <8 x i1> %582, <8 x i16> %535, <8 x i16> %564
  %584 = icmp slt <8 x i16> %537, %567
  %585 = select <8 x i1> %584, <8 x i16> %537, <8 x i16> %567
  %586 = sub <8 x i16> %564, %98
  %587 = sub <8 x i16> %567, %103
  %588 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %587, <8 x i16> %586) #8
  %589 = ashr <16 x i8> %588, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %590 = sub <16 x i8> zeroinitializer, %588
  %591 = icmp slt <16 x i8> %588, zeroinitializer
  %592 = select <16 x i1> %591, <16 x i8> %590, <16 x i8> %588
  %593 = bitcast <16 x i8> %592 to <8 x i16>
  %594 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %593, <8 x i16> %77) #8
  %595 = bitcast <8 x i16> %594 to <16 x i8>
  %596 = and <16 x i8> %75, %595
  %597 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %71, <16 x i8> %596) #8
  %598 = icmp ult <16 x i8> %592, %597
  %599 = select <16 x i1> %598, <16 x i8> %592, <16 x i8> %597
  %600 = add <16 x i8> %599, %589
  %601 = xor <16 x i8> %600, %589
  %602 = add nsw i64 %95, %92
  %603 = getelementptr inbounds i16, i16* %2, i64 %602
  %604 = bitcast i16* %603 to i8*
  %605 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %604) #8
  %606 = bitcast <16 x i8> %605 to <2 x i64>
  %607 = add nsw i64 %100, %92
  %608 = getelementptr inbounds i16, i16* %2, i64 %607
  %609 = bitcast i16* %608 to i8*
  %610 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %609) #8
  %611 = bitcast <16 x i8> %610 to <2 x i64>
  %612 = bitcast <16 x i8> %605 to <8 x i16>
  %613 = icmp eq <8 x i16> %612, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %614 = sext <8 x i1> %613 to <8 x i16>
  %615 = bitcast <16 x i8> %610 to <8 x i16>
  %616 = icmp eq <8 x i16> %615, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %617 = sext <8 x i1> %616 to <8 x i16>
  %618 = bitcast <8 x i16> %617 to <2 x i64>
  %619 = bitcast <8 x i16> %614 to <2 x i64>
  %620 = xor <2 x i64> %619, <i64 -1, i64 -1>
  %621 = and <2 x i64> %620, %606
  %622 = xor <2 x i64> %618, <i64 -1, i64 -1>
  %623 = and <2 x i64> %622, %611
  %624 = bitcast <2 x i64> %621 to <8 x i16>
  %625 = icmp sgt <8 x i16> %578, %624
  %626 = select <8 x i1> %625, <8 x i16> %578, <8 x i16> %624
  %627 = bitcast <2 x i64> %623 to <8 x i16>
  %628 = icmp sgt <8 x i16> %581, %627
  %629 = select <8 x i1> %628, <8 x i16> %581, <8 x i16> %627
  %630 = icmp slt <8 x i16> %583, %612
  %631 = select <8 x i1> %630, <8 x i16> %583, <8 x i16> %612
  %632 = icmp slt <8 x i16> %585, %615
  %633 = select <8 x i1> %632, <8 x i16> %585, <8 x i16> %615
  %634 = sub <8 x i16> %612, %98
  %635 = sub <8 x i16> %615, %103
  %636 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %635, <8 x i16> %634) #8
  %637 = ashr <16 x i8> %636, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %638 = sub <16 x i8> zeroinitializer, %636
  %639 = icmp slt <16 x i8> %636, zeroinitializer
  %640 = select <16 x i1> %639, <16 x i8> %638, <16 x i8> %636
  %641 = bitcast <16 x i8> %640 to <8 x i16>
  %642 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %641, <8 x i16> %77) #8
  %643 = bitcast <8 x i16> %642 to <16 x i8>
  %644 = and <16 x i8> %75, %643
  %645 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %71, <16 x i8> %644) #8
  %646 = icmp ult <16 x i8> %640, %645
  %647 = select <16 x i1> %646, <16 x i8> %640, <16 x i8> %645
  %648 = add <16 x i8> %647, %637
  %649 = xor <16 x i8> %648, %637
  %650 = sub nsw i64 %95, %92
  %651 = getelementptr inbounds i16, i16* %2, i64 %650
  %652 = bitcast i16* %651 to i8*
  %653 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %652) #8
  %654 = bitcast <16 x i8> %653 to <2 x i64>
  %655 = sub nsw i64 %100, %92
  %656 = getelementptr inbounds i16, i16* %2, i64 %655
  %657 = bitcast i16* %656 to i8*
  %658 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %657) #8
  %659 = bitcast <16 x i8> %658 to <2 x i64>
  %660 = bitcast <16 x i8> %653 to <8 x i16>
  %661 = icmp eq <8 x i16> %660, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %662 = sext <8 x i1> %661 to <8 x i16>
  %663 = bitcast <16 x i8> %658 to <8 x i16>
  %664 = icmp eq <8 x i16> %663, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %665 = sext <8 x i1> %664 to <8 x i16>
  %666 = bitcast <8 x i16> %665 to <2 x i64>
  %667 = bitcast <8 x i16> %662 to <2 x i64>
  %668 = xor <2 x i64> %667, <i64 -1, i64 -1>
  %669 = and <2 x i64> %668, %654
  %670 = xor <2 x i64> %666, <i64 -1, i64 -1>
  %671 = and <2 x i64> %670, %659
  %672 = bitcast <2 x i64> %669 to <8 x i16>
  %673 = icmp sgt <8 x i16> %626, %672
  %674 = select <8 x i1> %673, <8 x i16> %626, <8 x i16> %672
  %675 = bitcast <2 x i64> %671 to <8 x i16>
  %676 = icmp sgt <8 x i16> %629, %675
  %677 = select <8 x i1> %676, <8 x i16> %629, <8 x i16> %675
  %678 = icmp slt <8 x i16> %631, %660
  %679 = select <8 x i1> %678, <8 x i16> %631, <8 x i16> %660
  %680 = icmp slt <8 x i16> %633, %663
  %681 = select <8 x i1> %680, <8 x i16> %633, <8 x i16> %663
  %682 = sub <8 x i16> %660, %98
  %683 = sub <8 x i16> %663, %103
  %684 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %683, <8 x i16> %682) #8
  %685 = ashr <16 x i8> %684, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %686 = sub <16 x i8> zeroinitializer, %684
  %687 = icmp slt <16 x i8> %684, zeroinitializer
  %688 = select <16 x i1> %687, <16 x i8> %686, <16 x i8> %684
  %689 = bitcast <16 x i8> %688 to <8 x i16>
  %690 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %689, <8 x i16> %77) #8
  %691 = bitcast <8 x i16> %690 to <16 x i8>
  %692 = and <16 x i8> %75, %691
  %693 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %71, <16 x i8> %692) #8
  %694 = icmp ult <16 x i8> %688, %693
  %695 = select <16 x i1> %694, <16 x i8> %688, <16 x i8> %693
  %696 = add <16 x i8> %695, %685
  %697 = xor <16 x i8> %696, %685
  %698 = add <16 x i8> %601, %553
  %699 = add <16 x i8> %697, %649
  %700 = shufflevector <16 x i8> %699, <16 x i8> %698, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %701 = shufflevector <16 x i8> %699, <16 x i8> %698, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %702 = tail call <8 x i16> @llvm.x86.ssse3.pmadd.ub.sw.128(<16 x i8> %85, <16 x i8> %700) #8
  %703 = tail call <8 x i16> @llvm.x86.ssse3.pmadd.ub.sw.128(<16 x i8> %85, <16 x i8> %701) #8
  %704 = add <8 x i16> %504, %702
  %705 = add <8 x i16> %505, %703
  %706 = ashr <8 x i16> %704, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %707 = ashr <8 x i16> %705, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %708 = add <8 x i16> %704, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %709 = add <8 x i16> %708, %706
  %710 = add <8 x i16> %705, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %711 = add <8 x i16> %710, %707
  %712 = ashr <8 x i16> %709, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %713 = ashr <8 x i16> %711, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %714 = add <8 x i16> %712, %98
  %715 = add <8 x i16> %713, %103
  %716 = icmp sgt <8 x i16> %714, %679
  %717 = select <8 x i1> %716, <8 x i16> %714, <8 x i16> %679
  %718 = icmp sgt <8 x i16> %715, %681
  %719 = select <8 x i1> %718, <8 x i16> %715, <8 x i16> %681
  %720 = icmp slt <8 x i16> %717, %674
  %721 = select <8 x i1> %720, <8 x i16> %717, <8 x i16> %674
  %722 = icmp slt <8 x i16> %719, %677
  %723 = select <8 x i1> %722, <8 x i16> %719, <8 x i16> %677
  %724 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %723, <8 x i16> %721) #8
  %725 = bitcast <16 x i8> %724 to <2 x i64>
  %726 = mul nsw i64 %94, %86
  %727 = getelementptr inbounds i8, i8* %0, i64 %726
  %728 = shufflevector <16 x i8> %724, <16 x i8> undef, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %729 = bitcast <16 x i8> %728 to <2 x i64>
  %730 = extractelement <2 x i64> %729, i32 0
  %731 = bitcast i8* %727 to i64*
  store i64 %730, i64* %731, align 1
  %732 = mul nsw i64 %99, %86
  %733 = getelementptr inbounds i8, i8* %0, i64 %732
  %734 = extractelement <2 x i64> %725, i32 0
  %735 = bitcast i8* %733 to i64*
  store i64 %734, i64* %735, align 1
  %736 = add nuw nsw i64 %94, 2
  %737 = icmp ult i64 %736, 8
  br i1 %737, label %93, label %738

738:                                              ; preds = %93
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @cdef_filter_block_4x4_16_ssse3(i16* nocapture, i32, i16* readonly, i32, i32, i32, i32, i32, i32) local_unnamed_addr #0 {
  %10 = sext i32 %5 to i64
  %11 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %10, i64 0
  %12 = load i32, i32* %11, align 8
  %13 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %10, i64 1
  %14 = load i32, i32* %13, align 4
  %15 = add nsw i32 %5, 2
  %16 = and i32 %15, 7
  %17 = zext i32 %16 to i64
  %18 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %17, i64 0
  %19 = load i32, i32* %18, align 8
  %20 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %17, i64 1
  %21 = load i32, i32* %20, align 4
  %22 = add nsw i32 %5, 6
  %23 = and i32 %22, 7
  %24 = zext i32 %23 to i64
  %25 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %24, i64 0
  %26 = load i32, i32* %25, align 8
  %27 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %24, i64 1
  %28 = load i32, i32* %27, align 4
  %29 = lshr i32 %3, %8
  %30 = and i32 %29, 1
  %31 = zext i32 %30 to i64
  %32 = getelementptr inbounds [2 x [2 x i32]], [2 x [2 x i32]]* @cdef_pri_taps, i64 0, i64 %31, i64 0
  %33 = icmp eq i32 %3, 0
  br i1 %33, label %40, label %34

34:                                               ; preds = %9
  %35 = tail call i32 @llvm.ctlz.i32(i32 %3, i1 true) #8, !range !2
  %36 = xor i32 %35, 31
  %37 = icmp sgt i32 %36, %6
  %38 = sub nsw i32 %6, %36
  %39 = select i1 %37, i32 0, i32 %38
  br label %40

40:                                               ; preds = %34, %9
  %41 = phi i32 [ %6, %9 ], [ %39, %34 ]
  %42 = icmp eq i32 %4, 0
  br i1 %42, label %49, label %43

43:                                               ; preds = %40
  %44 = tail call i32 @llvm.ctlz.i32(i32 %4, i1 true) #8, !range !2
  %45 = xor i32 %44, 31
  %46 = icmp sgt i32 %45, %7
  %47 = sub nsw i32 %7, %45
  %48 = select i1 %46, i32 0, i32 %47
  br label %49

49:                                               ; preds = %43, %40
  %50 = phi i32 [ %7, %40 ], [ %48, %43 ]
  %51 = trunc i32 %3 to i16
  %52 = insertelement <8 x i16> undef, i16 %51, i32 0
  %53 = shufflevector <8 x i16> %52, <8 x i16> undef, <8 x i32> zeroinitializer
  %54 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %41, i32 0
  %55 = bitcast <4 x i32> %54 to <8 x i16>
  %56 = load i32, i32* %32, align 8
  %57 = trunc i32 %56 to i16
  %58 = insertelement <8 x i16> undef, i16 %57, i32 0
  %59 = shufflevector <8 x i16> %58, <8 x i16> undef, <8 x i32> zeroinitializer
  %60 = getelementptr inbounds [2 x [2 x i32]], [2 x [2 x i32]]* @cdef_pri_taps, i64 0, i64 %31, i64 1
  %61 = load i32, i32* %60, align 4
  %62 = trunc i32 %61 to i16
  %63 = insertelement <8 x i16> undef, i16 %62, i32 0
  %64 = shufflevector <8 x i16> %63, <8 x i16> undef, <8 x i32> zeroinitializer
  %65 = trunc i32 %4 to i16
  %66 = insertelement <8 x i16> undef, i16 %65, i32 0
  %67 = shufflevector <8 x i16> %66, <8 x i16> undef, <8 x i32> zeroinitializer
  %68 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %50, i32 0
  %69 = bitcast <4 x i32> %68 to <8 x i16>
  %70 = load i32, i32* getelementptr inbounds ([2 x i32], [2 x i32]* @cdef_sec_taps, i64 0, i64 0), align 4
  %71 = trunc i32 %70 to i16
  %72 = insertelement <8 x i16> undef, i16 %71, i32 0
  %73 = shufflevector <8 x i16> %72, <8 x i16> undef, <8 x i32> zeroinitializer
  %74 = load i32, i32* getelementptr inbounds ([2 x i32], [2 x i32]* @cdef_sec_taps, i64 0, i64 1), align 4
  %75 = trunc i32 %74 to i16
  %76 = insertelement <8 x i16> undef, i16 %75, i32 0
  %77 = shufflevector <8 x i16> %76, <8 x i16> undef, <8 x i32> zeroinitializer
  %78 = bitcast i16* %2 to i64*
  %79 = load i64, i64* %78, align 1
  %80 = getelementptr inbounds i16, i16* %2, i64 144
  %81 = bitcast i16* %80 to i64*
  %82 = load i64, i64* %81, align 1
  %83 = insertelement <2 x i64> undef, i64 %82, i32 0
  %84 = getelementptr inbounds i16, i16* %2, i64 288
  %85 = bitcast i16* %84 to i64*
  %86 = load i64, i64* %85, align 1
  %87 = getelementptr inbounds i16, i16* %2, i64 432
  %88 = bitcast i16* %87 to i64*
  %89 = load i64, i64* %88, align 1
  %90 = insertelement <2 x i64> undef, i64 %89, i32 0
  %91 = insertelement <2 x i64> %83, i64 %79, i32 1
  %92 = insertelement <2 x i64> %90, i64 %86, i32 1
  %93 = sext i32 %12 to i64
  %94 = getelementptr inbounds i16, i16* %2, i64 %93
  %95 = bitcast i16* %94 to i64*
  %96 = load i64, i64* %95, align 1
  %97 = add nsw i32 %12, 144
  %98 = sext i32 %97 to i64
  %99 = getelementptr inbounds i16, i16* %2, i64 %98
  %100 = bitcast i16* %99 to i64*
  %101 = load i64, i64* %100, align 1
  %102 = insertelement <2 x i64> undef, i64 %101, i32 0
  %103 = add nsw i32 %12, 288
  %104 = sext i32 %103 to i64
  %105 = getelementptr inbounds i16, i16* %2, i64 %104
  %106 = bitcast i16* %105 to i64*
  %107 = load i64, i64* %106, align 1
  %108 = add nsw i32 %12, 432
  %109 = sext i32 %108 to i64
  %110 = getelementptr inbounds i16, i16* %2, i64 %109
  %111 = bitcast i16* %110 to i64*
  %112 = load i64, i64* %111, align 1
  %113 = insertelement <2 x i64> undef, i64 %112, i32 0
  %114 = insertelement <2 x i64> %102, i64 %96, i32 1
  %115 = insertelement <2 x i64> %113, i64 %107, i32 1
  %116 = sub nsw i32 0, %12
  %117 = sext i32 %116 to i64
  %118 = getelementptr inbounds i16, i16* %2, i64 %117
  %119 = bitcast i16* %118 to i64*
  %120 = load i64, i64* %119, align 1
  %121 = sub nsw i32 144, %12
  %122 = sext i32 %121 to i64
  %123 = getelementptr inbounds i16, i16* %2, i64 %122
  %124 = bitcast i16* %123 to i64*
  %125 = load i64, i64* %124, align 1
  %126 = insertelement <2 x i64> undef, i64 %125, i32 0
  %127 = sub nsw i32 288, %12
  %128 = sext i32 %127 to i64
  %129 = getelementptr inbounds i16, i16* %2, i64 %128
  %130 = bitcast i16* %129 to i64*
  %131 = load i64, i64* %130, align 1
  %132 = sub nsw i32 432, %12
  %133 = sext i32 %132 to i64
  %134 = getelementptr inbounds i16, i16* %2, i64 %133
  %135 = bitcast i16* %134 to i64*
  %136 = load i64, i64* %135, align 1
  %137 = insertelement <2 x i64> undef, i64 %136, i32 0
  %138 = insertelement <2 x i64> %126, i64 %120, i32 1
  %139 = insertelement <2 x i64> %137, i64 %131, i32 1
  %140 = bitcast <2 x i64> %114 to <8 x i16>
  %141 = icmp eq <8 x i16> %140, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %142 = sext <8 x i1> %141 to <8 x i16>
  %143 = bitcast <2 x i64> %115 to <8 x i16>
  %144 = icmp eq <8 x i16> %143, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %145 = sext <8 x i1> %144 to <8 x i16>
  %146 = bitcast <8 x i16> %145 to <2 x i64>
  %147 = bitcast <8 x i16> %142 to <2 x i64>
  %148 = xor <2 x i64> %147, <i64 -1, i64 -1>
  %149 = and <2 x i64> %114, %148
  %150 = xor <2 x i64> %146, <i64 -1, i64 -1>
  %151 = and <2 x i64> %115, %150
  %152 = bitcast <2 x i64> %91 to <8 x i16>
  %153 = bitcast <2 x i64> %149 to <8 x i16>
  %154 = icmp sgt <8 x i16> %152, %153
  %155 = select <8 x i1> %154, <8 x i16> %152, <8 x i16> %153
  %156 = bitcast <2 x i64> %92 to <8 x i16>
  %157 = bitcast <2 x i64> %151 to <8 x i16>
  %158 = icmp sgt <8 x i16> %156, %157
  %159 = select <8 x i1> %158, <8 x i16> %156, <8 x i16> %157
  %160 = bitcast <2 x i64> %138 to <8 x i16>
  %161 = icmp eq <8 x i16> %160, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %162 = sext <8 x i1> %161 to <8 x i16>
  %163 = bitcast <2 x i64> %139 to <8 x i16>
  %164 = icmp eq <8 x i16> %163, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %165 = sext <8 x i1> %164 to <8 x i16>
  %166 = bitcast <8 x i16> %165 to <2 x i64>
  %167 = bitcast <8 x i16> %162 to <2 x i64>
  %168 = xor <2 x i64> %167, <i64 -1, i64 -1>
  %169 = and <2 x i64> %138, %168
  %170 = xor <2 x i64> %166, <i64 -1, i64 -1>
  %171 = and <2 x i64> %139, %170
  %172 = bitcast <2 x i64> %169 to <8 x i16>
  %173 = icmp sgt <8 x i16> %155, %172
  %174 = select <8 x i1> %173, <8 x i16> %155, <8 x i16> %172
  %175 = bitcast <2 x i64> %171 to <8 x i16>
  %176 = icmp sgt <8 x i16> %159, %175
  %177 = select <8 x i1> %176, <8 x i16> %159, <8 x i16> %175
  %178 = icmp sgt <8 x i16> %140, %152
  %179 = select <8 x i1> %178, <8 x i16> %152, <8 x i16> %140
  %180 = icmp sgt <8 x i16> %143, %156
  %181 = select <8 x i1> %180, <8 x i16> %156, <8 x i16> %143
  %182 = icmp slt <8 x i16> %179, %160
  %183 = select <8 x i1> %182, <8 x i16> %179, <8 x i16> %160
  %184 = icmp slt <8 x i16> %181, %163
  %185 = select <8 x i1> %184, <8 x i16> %181, <8 x i16> %163
  %186 = sub <8 x i16> %140, %152
  %187 = sub <8 x i16> %143, %156
  %188 = ashr <8 x i16> %186, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %189 = ashr <8 x i16> %187, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %190 = sub <8 x i16> zeroinitializer, %186
  %191 = icmp slt <8 x i16> %186, zeroinitializer
  %192 = select <8 x i1> %191, <8 x i16> %190, <8 x i16> %186
  %193 = sub <8 x i16> zeroinitializer, %187
  %194 = icmp slt <8 x i16> %187, zeroinitializer
  %195 = select <8 x i1> %194, <8 x i16> %193, <8 x i16> %187
  %196 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %192, <8 x i16> %55) #8
  %197 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %195, <8 x i16> %55) #8
  %198 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %53, <8 x i16> %196) #8
  %199 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %53, <8 x i16> %197) #8
  %200 = icmp slt <8 x i16> %192, %198
  %201 = select <8 x i1> %200, <8 x i16> %192, <8 x i16> %198
  %202 = icmp slt <8 x i16> %195, %199
  %203 = select <8 x i1> %202, <8 x i16> %195, <8 x i16> %199
  %204 = add <8 x i16> %201, %188
  %205 = add <8 x i16> %203, %189
  %206 = xor <8 x i16> %204, %188
  %207 = xor <8 x i16> %205, %189
  %208 = sub <8 x i16> %160, %152
  %209 = sub <8 x i16> %163, %156
  %210 = ashr <8 x i16> %208, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %211 = ashr <8 x i16> %209, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %212 = sub <8 x i16> zeroinitializer, %208
  %213 = icmp slt <8 x i16> %208, zeroinitializer
  %214 = select <8 x i1> %213, <8 x i16> %212, <8 x i16> %208
  %215 = sub <8 x i16> zeroinitializer, %209
  %216 = icmp slt <8 x i16> %209, zeroinitializer
  %217 = select <8 x i1> %216, <8 x i16> %215, <8 x i16> %209
  %218 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %214, <8 x i16> %55) #8
  %219 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %217, <8 x i16> %55) #8
  %220 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %53, <8 x i16> %218) #8
  %221 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %53, <8 x i16> %219) #8
  %222 = icmp slt <8 x i16> %214, %220
  %223 = select <8 x i1> %222, <8 x i16> %214, <8 x i16> %220
  %224 = icmp slt <8 x i16> %217, %221
  %225 = select <8 x i1> %224, <8 x i16> %217, <8 x i16> %221
  %226 = add <8 x i16> %223, %210
  %227 = add <8 x i16> %225, %211
  %228 = xor <8 x i16> %226, %210
  %229 = xor <8 x i16> %227, %211
  %230 = add <8 x i16> %228, %206
  %231 = add <8 x i16> %229, %207
  %232 = mul <8 x i16> %230, %59
  %233 = mul <8 x i16> %231, %59
  %234 = sext i32 %14 to i64
  %235 = getelementptr inbounds i16, i16* %2, i64 %234
  %236 = bitcast i16* %235 to i64*
  %237 = load i64, i64* %236, align 1
  %238 = add nsw i32 %14, 144
  %239 = sext i32 %238 to i64
  %240 = getelementptr inbounds i16, i16* %2, i64 %239
  %241 = bitcast i16* %240 to i64*
  %242 = load i64, i64* %241, align 1
  %243 = insertelement <2 x i64> undef, i64 %242, i32 0
  %244 = add nsw i32 %14, 288
  %245 = sext i32 %244 to i64
  %246 = getelementptr inbounds i16, i16* %2, i64 %245
  %247 = bitcast i16* %246 to i64*
  %248 = load i64, i64* %247, align 1
  %249 = add nsw i32 %14, 432
  %250 = sext i32 %249 to i64
  %251 = getelementptr inbounds i16, i16* %2, i64 %250
  %252 = bitcast i16* %251 to i64*
  %253 = load i64, i64* %252, align 1
  %254 = insertelement <2 x i64> undef, i64 %253, i32 0
  %255 = insertelement <2 x i64> %243, i64 %237, i32 1
  %256 = insertelement <2 x i64> %254, i64 %248, i32 1
  %257 = sub nsw i32 0, %14
  %258 = sext i32 %257 to i64
  %259 = getelementptr inbounds i16, i16* %2, i64 %258
  %260 = bitcast i16* %259 to i64*
  %261 = load i64, i64* %260, align 1
  %262 = sub nsw i32 144, %14
  %263 = sext i32 %262 to i64
  %264 = getelementptr inbounds i16, i16* %2, i64 %263
  %265 = bitcast i16* %264 to i64*
  %266 = load i64, i64* %265, align 1
  %267 = insertelement <2 x i64> undef, i64 %266, i32 0
  %268 = sub nsw i32 288, %14
  %269 = sext i32 %268 to i64
  %270 = getelementptr inbounds i16, i16* %2, i64 %269
  %271 = bitcast i16* %270 to i64*
  %272 = load i64, i64* %271, align 1
  %273 = sub nsw i32 432, %14
  %274 = sext i32 %273 to i64
  %275 = getelementptr inbounds i16, i16* %2, i64 %274
  %276 = bitcast i16* %275 to i64*
  %277 = load i64, i64* %276, align 1
  %278 = insertelement <2 x i64> undef, i64 %277, i32 0
  %279 = insertelement <2 x i64> %267, i64 %261, i32 1
  %280 = insertelement <2 x i64> %278, i64 %272, i32 1
  %281 = bitcast <2 x i64> %255 to <8 x i16>
  %282 = icmp eq <8 x i16> %281, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %283 = sext <8 x i1> %282 to <8 x i16>
  %284 = bitcast <2 x i64> %256 to <8 x i16>
  %285 = icmp eq <8 x i16> %284, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %286 = sext <8 x i1> %285 to <8 x i16>
  %287 = bitcast <8 x i16> %286 to <2 x i64>
  %288 = bitcast <8 x i16> %283 to <2 x i64>
  %289 = xor <2 x i64> %288, <i64 -1, i64 -1>
  %290 = and <2 x i64> %255, %289
  %291 = xor <2 x i64> %287, <i64 -1, i64 -1>
  %292 = and <2 x i64> %256, %291
  %293 = bitcast <2 x i64> %290 to <8 x i16>
  %294 = icmp sgt <8 x i16> %174, %293
  %295 = select <8 x i1> %294, <8 x i16> %174, <8 x i16> %293
  %296 = bitcast <2 x i64> %292 to <8 x i16>
  %297 = icmp sgt <8 x i16> %177, %296
  %298 = select <8 x i1> %297, <8 x i16> %177, <8 x i16> %296
  %299 = bitcast <2 x i64> %279 to <8 x i16>
  %300 = icmp eq <8 x i16> %299, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %301 = sext <8 x i1> %300 to <8 x i16>
  %302 = bitcast <2 x i64> %280 to <8 x i16>
  %303 = icmp eq <8 x i16> %302, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %304 = sext <8 x i1> %303 to <8 x i16>
  %305 = bitcast <8 x i16> %304 to <2 x i64>
  %306 = bitcast <8 x i16> %301 to <2 x i64>
  %307 = xor <2 x i64> %306, <i64 -1, i64 -1>
  %308 = and <2 x i64> %279, %307
  %309 = xor <2 x i64> %305, <i64 -1, i64 -1>
  %310 = and <2 x i64> %280, %309
  %311 = bitcast <2 x i64> %308 to <8 x i16>
  %312 = icmp sgt <8 x i16> %295, %311
  %313 = select <8 x i1> %312, <8 x i16> %295, <8 x i16> %311
  %314 = bitcast <2 x i64> %310 to <8 x i16>
  %315 = icmp sgt <8 x i16> %298, %314
  %316 = select <8 x i1> %315, <8 x i16> %298, <8 x i16> %314
  %317 = icmp slt <8 x i16> %183, %281
  %318 = select <8 x i1> %317, <8 x i16> %183, <8 x i16> %281
  %319 = icmp slt <8 x i16> %185, %284
  %320 = select <8 x i1> %319, <8 x i16> %185, <8 x i16> %284
  %321 = icmp slt <8 x i16> %318, %299
  %322 = select <8 x i1> %321, <8 x i16> %318, <8 x i16> %299
  %323 = icmp slt <8 x i16> %320, %302
  %324 = select <8 x i1> %323, <8 x i16> %320, <8 x i16> %302
  %325 = sub <8 x i16> %281, %152
  %326 = sub <8 x i16> %284, %156
  %327 = ashr <8 x i16> %325, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %328 = ashr <8 x i16> %326, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %329 = sub <8 x i16> zeroinitializer, %325
  %330 = icmp slt <8 x i16> %325, zeroinitializer
  %331 = select <8 x i1> %330, <8 x i16> %329, <8 x i16> %325
  %332 = sub <8 x i16> zeroinitializer, %326
  %333 = icmp slt <8 x i16> %326, zeroinitializer
  %334 = select <8 x i1> %333, <8 x i16> %332, <8 x i16> %326
  %335 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %331, <8 x i16> %55) #8
  %336 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %334, <8 x i16> %55) #8
  %337 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %53, <8 x i16> %335) #8
  %338 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %53, <8 x i16> %336) #8
  %339 = icmp slt <8 x i16> %331, %337
  %340 = select <8 x i1> %339, <8 x i16> %331, <8 x i16> %337
  %341 = icmp slt <8 x i16> %334, %338
  %342 = select <8 x i1> %341, <8 x i16> %334, <8 x i16> %338
  %343 = add <8 x i16> %340, %327
  %344 = add <8 x i16> %342, %328
  %345 = xor <8 x i16> %343, %327
  %346 = xor <8 x i16> %344, %328
  %347 = sub <8 x i16> %299, %152
  %348 = sub <8 x i16> %302, %156
  %349 = ashr <8 x i16> %347, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %350 = ashr <8 x i16> %348, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %351 = sub <8 x i16> zeroinitializer, %347
  %352 = icmp slt <8 x i16> %347, zeroinitializer
  %353 = select <8 x i1> %352, <8 x i16> %351, <8 x i16> %347
  %354 = sub <8 x i16> zeroinitializer, %348
  %355 = icmp slt <8 x i16> %348, zeroinitializer
  %356 = select <8 x i1> %355, <8 x i16> %354, <8 x i16> %348
  %357 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %353, <8 x i16> %55) #8
  %358 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %356, <8 x i16> %55) #8
  %359 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %53, <8 x i16> %357) #8
  %360 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %53, <8 x i16> %358) #8
  %361 = icmp slt <8 x i16> %353, %359
  %362 = select <8 x i1> %361, <8 x i16> %353, <8 x i16> %359
  %363 = icmp slt <8 x i16> %356, %360
  %364 = select <8 x i1> %363, <8 x i16> %356, <8 x i16> %360
  %365 = add <8 x i16> %362, %349
  %366 = add <8 x i16> %364, %350
  %367 = xor <8 x i16> %365, %349
  %368 = xor <8 x i16> %366, %350
  %369 = add <8 x i16> %367, %345
  %370 = add <8 x i16> %368, %346
  %371 = mul <8 x i16> %369, %64
  %372 = mul <8 x i16> %370, %64
  %373 = add <8 x i16> %371, %232
  %374 = add <8 x i16> %372, %233
  %375 = sext i32 %19 to i64
  %376 = getelementptr inbounds i16, i16* %2, i64 %375
  %377 = bitcast i16* %376 to i64*
  %378 = load i64, i64* %377, align 1
  %379 = add nsw i32 %19, 144
  %380 = sext i32 %379 to i64
  %381 = getelementptr inbounds i16, i16* %2, i64 %380
  %382 = bitcast i16* %381 to i64*
  %383 = load i64, i64* %382, align 1
  %384 = insertelement <2 x i64> undef, i64 %383, i32 0
  %385 = add nsw i32 %19, 288
  %386 = sext i32 %385 to i64
  %387 = getelementptr inbounds i16, i16* %2, i64 %386
  %388 = bitcast i16* %387 to i64*
  %389 = load i64, i64* %388, align 1
  %390 = add nsw i32 %19, 432
  %391 = sext i32 %390 to i64
  %392 = getelementptr inbounds i16, i16* %2, i64 %391
  %393 = bitcast i16* %392 to i64*
  %394 = load i64, i64* %393, align 1
  %395 = insertelement <2 x i64> undef, i64 %394, i32 0
  %396 = insertelement <2 x i64> %384, i64 %378, i32 1
  %397 = insertelement <2 x i64> %395, i64 %389, i32 1
  %398 = sub nsw i32 0, %19
  %399 = sext i32 %398 to i64
  %400 = getelementptr inbounds i16, i16* %2, i64 %399
  %401 = bitcast i16* %400 to i64*
  %402 = load i64, i64* %401, align 1
  %403 = sub nsw i32 144, %19
  %404 = sext i32 %403 to i64
  %405 = getelementptr inbounds i16, i16* %2, i64 %404
  %406 = bitcast i16* %405 to i64*
  %407 = load i64, i64* %406, align 1
  %408 = insertelement <2 x i64> undef, i64 %407, i32 0
  %409 = sub nsw i32 288, %19
  %410 = sext i32 %409 to i64
  %411 = getelementptr inbounds i16, i16* %2, i64 %410
  %412 = bitcast i16* %411 to i64*
  %413 = load i64, i64* %412, align 1
  %414 = sub nsw i32 432, %19
  %415 = sext i32 %414 to i64
  %416 = getelementptr inbounds i16, i16* %2, i64 %415
  %417 = bitcast i16* %416 to i64*
  %418 = load i64, i64* %417, align 1
  %419 = insertelement <2 x i64> undef, i64 %418, i32 0
  %420 = insertelement <2 x i64> %408, i64 %402, i32 1
  %421 = insertelement <2 x i64> %419, i64 %413, i32 1
  %422 = sext i32 %26 to i64
  %423 = getelementptr inbounds i16, i16* %2, i64 %422
  %424 = bitcast i16* %423 to i64*
  %425 = load i64, i64* %424, align 1
  %426 = add nsw i32 %26, 144
  %427 = sext i32 %426 to i64
  %428 = getelementptr inbounds i16, i16* %2, i64 %427
  %429 = bitcast i16* %428 to i64*
  %430 = load i64, i64* %429, align 1
  %431 = insertelement <2 x i64> undef, i64 %430, i32 0
  %432 = add nsw i32 %26, 288
  %433 = sext i32 %432 to i64
  %434 = getelementptr inbounds i16, i16* %2, i64 %433
  %435 = bitcast i16* %434 to i64*
  %436 = load i64, i64* %435, align 1
  %437 = add nsw i32 %26, 432
  %438 = sext i32 %437 to i64
  %439 = getelementptr inbounds i16, i16* %2, i64 %438
  %440 = bitcast i16* %439 to i64*
  %441 = load i64, i64* %440, align 1
  %442 = insertelement <2 x i64> undef, i64 %441, i32 0
  %443 = insertelement <2 x i64> %431, i64 %425, i32 1
  %444 = insertelement <2 x i64> %442, i64 %436, i32 1
  %445 = sub nsw i32 0, %26
  %446 = sext i32 %445 to i64
  %447 = getelementptr inbounds i16, i16* %2, i64 %446
  %448 = bitcast i16* %447 to i64*
  %449 = load i64, i64* %448, align 1
  %450 = sub nsw i32 144, %26
  %451 = sext i32 %450 to i64
  %452 = getelementptr inbounds i16, i16* %2, i64 %451
  %453 = bitcast i16* %452 to i64*
  %454 = load i64, i64* %453, align 1
  %455 = insertelement <2 x i64> undef, i64 %454, i32 0
  %456 = sub nsw i32 288, %26
  %457 = sext i32 %456 to i64
  %458 = getelementptr inbounds i16, i16* %2, i64 %457
  %459 = bitcast i16* %458 to i64*
  %460 = load i64, i64* %459, align 1
  %461 = sub nsw i32 432, %26
  %462 = sext i32 %461 to i64
  %463 = getelementptr inbounds i16, i16* %2, i64 %462
  %464 = bitcast i16* %463 to i64*
  %465 = load i64, i64* %464, align 1
  %466 = insertelement <2 x i64> undef, i64 %465, i32 0
  %467 = insertelement <2 x i64> %455, i64 %449, i32 1
  %468 = insertelement <2 x i64> %466, i64 %460, i32 1
  %469 = bitcast <2 x i64> %396 to <8 x i16>
  %470 = icmp eq <8 x i16> %469, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %471 = sext <8 x i1> %470 to <8 x i16>
  %472 = bitcast <2 x i64> %397 to <8 x i16>
  %473 = icmp eq <8 x i16> %472, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %474 = sext <8 x i1> %473 to <8 x i16>
  %475 = bitcast <8 x i16> %474 to <2 x i64>
  %476 = bitcast <8 x i16> %471 to <2 x i64>
  %477 = xor <2 x i64> %476, <i64 -1, i64 -1>
  %478 = and <2 x i64> %396, %477
  %479 = xor <2 x i64> %475, <i64 -1, i64 -1>
  %480 = and <2 x i64> %397, %479
  %481 = bitcast <2 x i64> %478 to <8 x i16>
  %482 = icmp sgt <8 x i16> %313, %481
  %483 = select <8 x i1> %482, <8 x i16> %313, <8 x i16> %481
  %484 = bitcast <2 x i64> %480 to <8 x i16>
  %485 = icmp sgt <8 x i16> %316, %484
  %486 = select <8 x i1> %485, <8 x i16> %316, <8 x i16> %484
  %487 = bitcast <2 x i64> %420 to <8 x i16>
  %488 = icmp eq <8 x i16> %487, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %489 = sext <8 x i1> %488 to <8 x i16>
  %490 = bitcast <2 x i64> %421 to <8 x i16>
  %491 = icmp eq <8 x i16> %490, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %492 = sext <8 x i1> %491 to <8 x i16>
  %493 = bitcast <8 x i16> %492 to <2 x i64>
  %494 = bitcast <8 x i16> %489 to <2 x i64>
  %495 = xor <2 x i64> %494, <i64 -1, i64 -1>
  %496 = and <2 x i64> %420, %495
  %497 = xor <2 x i64> %493, <i64 -1, i64 -1>
  %498 = and <2 x i64> %421, %497
  %499 = bitcast <2 x i64> %496 to <8 x i16>
  %500 = icmp sgt <8 x i16> %483, %499
  %501 = select <8 x i1> %500, <8 x i16> %483, <8 x i16> %499
  %502 = bitcast <2 x i64> %498 to <8 x i16>
  %503 = icmp sgt <8 x i16> %486, %502
  %504 = select <8 x i1> %503, <8 x i16> %486, <8 x i16> %502
  %505 = bitcast <2 x i64> %443 to <8 x i16>
  %506 = icmp eq <8 x i16> %505, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %507 = sext <8 x i1> %506 to <8 x i16>
  %508 = bitcast <2 x i64> %444 to <8 x i16>
  %509 = icmp eq <8 x i16> %508, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %510 = sext <8 x i1> %509 to <8 x i16>
  %511 = bitcast <8 x i16> %510 to <2 x i64>
  %512 = bitcast <8 x i16> %507 to <2 x i64>
  %513 = xor <2 x i64> %512, <i64 -1, i64 -1>
  %514 = and <2 x i64> %443, %513
  %515 = xor <2 x i64> %511, <i64 -1, i64 -1>
  %516 = and <2 x i64> %444, %515
  %517 = bitcast <2 x i64> %514 to <8 x i16>
  %518 = icmp sgt <8 x i16> %501, %517
  %519 = select <8 x i1> %518, <8 x i16> %501, <8 x i16> %517
  %520 = bitcast <2 x i64> %516 to <8 x i16>
  %521 = icmp sgt <8 x i16> %504, %520
  %522 = select <8 x i1> %521, <8 x i16> %504, <8 x i16> %520
  %523 = bitcast <2 x i64> %467 to <8 x i16>
  %524 = icmp eq <8 x i16> %523, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %525 = sext <8 x i1> %524 to <8 x i16>
  %526 = bitcast <2 x i64> %468 to <8 x i16>
  %527 = icmp eq <8 x i16> %526, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %528 = sext <8 x i1> %527 to <8 x i16>
  %529 = bitcast <8 x i16> %528 to <2 x i64>
  %530 = bitcast <8 x i16> %525 to <2 x i64>
  %531 = xor <2 x i64> %530, <i64 -1, i64 -1>
  %532 = and <2 x i64> %467, %531
  %533 = xor <2 x i64> %529, <i64 -1, i64 -1>
  %534 = and <2 x i64> %468, %533
  %535 = bitcast <2 x i64> %532 to <8 x i16>
  %536 = icmp sgt <8 x i16> %519, %535
  %537 = select <8 x i1> %536, <8 x i16> %519, <8 x i16> %535
  %538 = bitcast <2 x i64> %534 to <8 x i16>
  %539 = icmp sgt <8 x i16> %522, %538
  %540 = select <8 x i1> %539, <8 x i16> %522, <8 x i16> %538
  %541 = icmp slt <8 x i16> %322, %469
  %542 = select <8 x i1> %541, <8 x i16> %322, <8 x i16> %469
  %543 = icmp slt <8 x i16> %324, %472
  %544 = select <8 x i1> %543, <8 x i16> %324, <8 x i16> %472
  %545 = icmp slt <8 x i16> %542, %487
  %546 = select <8 x i1> %545, <8 x i16> %542, <8 x i16> %487
  %547 = icmp slt <8 x i16> %544, %490
  %548 = select <8 x i1> %547, <8 x i16> %544, <8 x i16> %490
  %549 = icmp slt <8 x i16> %546, %505
  %550 = select <8 x i1> %549, <8 x i16> %546, <8 x i16> %505
  %551 = icmp slt <8 x i16> %548, %508
  %552 = select <8 x i1> %551, <8 x i16> %548, <8 x i16> %508
  %553 = icmp slt <8 x i16> %550, %523
  %554 = select <8 x i1> %553, <8 x i16> %550, <8 x i16> %523
  %555 = icmp slt <8 x i16> %552, %526
  %556 = select <8 x i1> %555, <8 x i16> %552, <8 x i16> %526
  %557 = sub <8 x i16> %469, %152
  %558 = sub <8 x i16> %472, %156
  %559 = ashr <8 x i16> %557, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %560 = ashr <8 x i16> %558, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %561 = sub <8 x i16> zeroinitializer, %557
  %562 = icmp slt <8 x i16> %557, zeroinitializer
  %563 = select <8 x i1> %562, <8 x i16> %561, <8 x i16> %557
  %564 = sub <8 x i16> zeroinitializer, %558
  %565 = icmp slt <8 x i16> %558, zeroinitializer
  %566 = select <8 x i1> %565, <8 x i16> %564, <8 x i16> %558
  %567 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %563, <8 x i16> %69) #8
  %568 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %566, <8 x i16> %69) #8
  %569 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %567) #8
  %570 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %568) #8
  %571 = icmp slt <8 x i16> %563, %569
  %572 = select <8 x i1> %571, <8 x i16> %563, <8 x i16> %569
  %573 = icmp slt <8 x i16> %566, %570
  %574 = select <8 x i1> %573, <8 x i16> %566, <8 x i16> %570
  %575 = add <8 x i16> %572, %559
  %576 = add <8 x i16> %574, %560
  %577 = xor <8 x i16> %575, %559
  %578 = xor <8 x i16> %576, %560
  %579 = sub <8 x i16> %487, %152
  %580 = sub <8 x i16> %490, %156
  %581 = ashr <8 x i16> %579, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %582 = ashr <8 x i16> %580, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %583 = sub <8 x i16> zeroinitializer, %579
  %584 = icmp slt <8 x i16> %579, zeroinitializer
  %585 = select <8 x i1> %584, <8 x i16> %583, <8 x i16> %579
  %586 = sub <8 x i16> zeroinitializer, %580
  %587 = icmp slt <8 x i16> %580, zeroinitializer
  %588 = select <8 x i1> %587, <8 x i16> %586, <8 x i16> %580
  %589 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %585, <8 x i16> %69) #8
  %590 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %588, <8 x i16> %69) #8
  %591 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %589) #8
  %592 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %590) #8
  %593 = icmp slt <8 x i16> %585, %591
  %594 = select <8 x i1> %593, <8 x i16> %585, <8 x i16> %591
  %595 = icmp slt <8 x i16> %588, %592
  %596 = select <8 x i1> %595, <8 x i16> %588, <8 x i16> %592
  %597 = add <8 x i16> %594, %581
  %598 = add <8 x i16> %596, %582
  %599 = xor <8 x i16> %597, %581
  %600 = xor <8 x i16> %598, %582
  %601 = sub <8 x i16> %505, %152
  %602 = sub <8 x i16> %508, %156
  %603 = ashr <8 x i16> %601, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %604 = ashr <8 x i16> %602, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %605 = sub <8 x i16> zeroinitializer, %601
  %606 = icmp slt <8 x i16> %601, zeroinitializer
  %607 = select <8 x i1> %606, <8 x i16> %605, <8 x i16> %601
  %608 = sub <8 x i16> zeroinitializer, %602
  %609 = icmp slt <8 x i16> %602, zeroinitializer
  %610 = select <8 x i1> %609, <8 x i16> %608, <8 x i16> %602
  %611 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %607, <8 x i16> %69) #8
  %612 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %610, <8 x i16> %69) #8
  %613 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %611) #8
  %614 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %612) #8
  %615 = icmp slt <8 x i16> %607, %613
  %616 = select <8 x i1> %615, <8 x i16> %607, <8 x i16> %613
  %617 = icmp slt <8 x i16> %610, %614
  %618 = select <8 x i1> %617, <8 x i16> %610, <8 x i16> %614
  %619 = add <8 x i16> %616, %603
  %620 = add <8 x i16> %618, %604
  %621 = xor <8 x i16> %619, %603
  %622 = xor <8 x i16> %620, %604
  %623 = sub <8 x i16> %523, %152
  %624 = sub <8 x i16> %526, %156
  %625 = ashr <8 x i16> %623, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %626 = ashr <8 x i16> %624, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %627 = sub <8 x i16> zeroinitializer, %623
  %628 = icmp slt <8 x i16> %623, zeroinitializer
  %629 = select <8 x i1> %628, <8 x i16> %627, <8 x i16> %623
  %630 = sub <8 x i16> zeroinitializer, %624
  %631 = icmp slt <8 x i16> %624, zeroinitializer
  %632 = select <8 x i1> %631, <8 x i16> %630, <8 x i16> %624
  %633 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %629, <8 x i16> %69) #8
  %634 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %632, <8 x i16> %69) #8
  %635 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %633) #8
  %636 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %634) #8
  %637 = icmp slt <8 x i16> %629, %635
  %638 = select <8 x i1> %637, <8 x i16> %629, <8 x i16> %635
  %639 = icmp slt <8 x i16> %632, %636
  %640 = select <8 x i1> %639, <8 x i16> %632, <8 x i16> %636
  %641 = add <8 x i16> %638, %625
  %642 = add <8 x i16> %640, %626
  %643 = xor <8 x i16> %641, %625
  %644 = xor <8 x i16> %642, %626
  %645 = add <8 x i16> %599, %577
  %646 = add <8 x i16> %645, %621
  %647 = add <8 x i16> %646, %643
  %648 = add <8 x i16> %600, %578
  %649 = add <8 x i16> %648, %622
  %650 = add <8 x i16> %649, %644
  %651 = mul <8 x i16> %647, %73
  %652 = mul <8 x i16> %650, %73
  %653 = add <8 x i16> %373, %651
  %654 = add <8 x i16> %374, %652
  %655 = sext i32 %21 to i64
  %656 = getelementptr inbounds i16, i16* %2, i64 %655
  %657 = bitcast i16* %656 to i64*
  %658 = load i64, i64* %657, align 1
  %659 = add nsw i32 %21, 144
  %660 = sext i32 %659 to i64
  %661 = getelementptr inbounds i16, i16* %2, i64 %660
  %662 = bitcast i16* %661 to i64*
  %663 = load i64, i64* %662, align 1
  %664 = insertelement <2 x i64> undef, i64 %663, i32 0
  %665 = add nsw i32 %21, 288
  %666 = sext i32 %665 to i64
  %667 = getelementptr inbounds i16, i16* %2, i64 %666
  %668 = bitcast i16* %667 to i64*
  %669 = load i64, i64* %668, align 1
  %670 = add nsw i32 %21, 432
  %671 = sext i32 %670 to i64
  %672 = getelementptr inbounds i16, i16* %2, i64 %671
  %673 = bitcast i16* %672 to i64*
  %674 = load i64, i64* %673, align 1
  %675 = insertelement <2 x i64> undef, i64 %674, i32 0
  %676 = insertelement <2 x i64> %664, i64 %658, i32 1
  %677 = insertelement <2 x i64> %675, i64 %669, i32 1
  %678 = sub nsw i32 0, %21
  %679 = sext i32 %678 to i64
  %680 = getelementptr inbounds i16, i16* %2, i64 %679
  %681 = bitcast i16* %680 to i64*
  %682 = load i64, i64* %681, align 1
  %683 = sub nsw i32 144, %21
  %684 = sext i32 %683 to i64
  %685 = getelementptr inbounds i16, i16* %2, i64 %684
  %686 = bitcast i16* %685 to i64*
  %687 = load i64, i64* %686, align 1
  %688 = insertelement <2 x i64> undef, i64 %687, i32 0
  %689 = sub nsw i32 288, %21
  %690 = sext i32 %689 to i64
  %691 = getelementptr inbounds i16, i16* %2, i64 %690
  %692 = bitcast i16* %691 to i64*
  %693 = load i64, i64* %692, align 1
  %694 = sub nsw i32 432, %21
  %695 = sext i32 %694 to i64
  %696 = getelementptr inbounds i16, i16* %2, i64 %695
  %697 = bitcast i16* %696 to i64*
  %698 = load i64, i64* %697, align 1
  %699 = insertelement <2 x i64> undef, i64 %698, i32 0
  %700 = insertelement <2 x i64> %688, i64 %682, i32 1
  %701 = insertelement <2 x i64> %699, i64 %693, i32 1
  %702 = sext i32 %28 to i64
  %703 = getelementptr inbounds i16, i16* %2, i64 %702
  %704 = bitcast i16* %703 to i64*
  %705 = load i64, i64* %704, align 1
  %706 = add nsw i32 %28, 144
  %707 = sext i32 %706 to i64
  %708 = getelementptr inbounds i16, i16* %2, i64 %707
  %709 = bitcast i16* %708 to i64*
  %710 = load i64, i64* %709, align 1
  %711 = insertelement <2 x i64> undef, i64 %710, i32 0
  %712 = add nsw i32 %28, 288
  %713 = sext i32 %712 to i64
  %714 = getelementptr inbounds i16, i16* %2, i64 %713
  %715 = bitcast i16* %714 to i64*
  %716 = load i64, i64* %715, align 1
  %717 = add nsw i32 %28, 432
  %718 = sext i32 %717 to i64
  %719 = getelementptr inbounds i16, i16* %2, i64 %718
  %720 = bitcast i16* %719 to i64*
  %721 = load i64, i64* %720, align 1
  %722 = insertelement <2 x i64> undef, i64 %721, i32 0
  %723 = insertelement <2 x i64> %711, i64 %705, i32 1
  %724 = insertelement <2 x i64> %722, i64 %716, i32 1
  %725 = sub nsw i32 0, %28
  %726 = sext i32 %725 to i64
  %727 = getelementptr inbounds i16, i16* %2, i64 %726
  %728 = bitcast i16* %727 to i64*
  %729 = load i64, i64* %728, align 1
  %730 = sub nsw i32 144, %28
  %731 = sext i32 %730 to i64
  %732 = getelementptr inbounds i16, i16* %2, i64 %731
  %733 = bitcast i16* %732 to i64*
  %734 = load i64, i64* %733, align 1
  %735 = insertelement <2 x i64> undef, i64 %734, i32 0
  %736 = sub nsw i32 288, %28
  %737 = sext i32 %736 to i64
  %738 = getelementptr inbounds i16, i16* %2, i64 %737
  %739 = bitcast i16* %738 to i64*
  %740 = load i64, i64* %739, align 1
  %741 = sub nsw i32 432, %28
  %742 = sext i32 %741 to i64
  %743 = getelementptr inbounds i16, i16* %2, i64 %742
  %744 = bitcast i16* %743 to i64*
  %745 = load i64, i64* %744, align 1
  %746 = insertelement <2 x i64> undef, i64 %745, i32 0
  %747 = insertelement <2 x i64> %735, i64 %729, i32 1
  %748 = insertelement <2 x i64> %746, i64 %740, i32 1
  %749 = bitcast <2 x i64> %676 to <8 x i16>
  %750 = icmp eq <8 x i16> %749, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %751 = sext <8 x i1> %750 to <8 x i16>
  %752 = bitcast <2 x i64> %677 to <8 x i16>
  %753 = icmp eq <8 x i16> %752, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %754 = sext <8 x i1> %753 to <8 x i16>
  %755 = bitcast <8 x i16> %754 to <2 x i64>
  %756 = bitcast <8 x i16> %751 to <2 x i64>
  %757 = xor <2 x i64> %756, <i64 -1, i64 -1>
  %758 = and <2 x i64> %676, %757
  %759 = xor <2 x i64> %755, <i64 -1, i64 -1>
  %760 = and <2 x i64> %677, %759
  %761 = bitcast <2 x i64> %758 to <8 x i16>
  %762 = icmp sgt <8 x i16> %537, %761
  %763 = select <8 x i1> %762, <8 x i16> %537, <8 x i16> %761
  %764 = bitcast <2 x i64> %760 to <8 x i16>
  %765 = icmp sgt <8 x i16> %540, %764
  %766 = select <8 x i1> %765, <8 x i16> %540, <8 x i16> %764
  %767 = bitcast <2 x i64> %700 to <8 x i16>
  %768 = icmp eq <8 x i16> %767, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %769 = sext <8 x i1> %768 to <8 x i16>
  %770 = bitcast <2 x i64> %701 to <8 x i16>
  %771 = icmp eq <8 x i16> %770, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %772 = sext <8 x i1> %771 to <8 x i16>
  %773 = bitcast <8 x i16> %772 to <2 x i64>
  %774 = bitcast <8 x i16> %769 to <2 x i64>
  %775 = xor <2 x i64> %774, <i64 -1, i64 -1>
  %776 = and <2 x i64> %700, %775
  %777 = xor <2 x i64> %773, <i64 -1, i64 -1>
  %778 = and <2 x i64> %701, %777
  %779 = bitcast <2 x i64> %776 to <8 x i16>
  %780 = icmp sgt <8 x i16> %763, %779
  %781 = select <8 x i1> %780, <8 x i16> %763, <8 x i16> %779
  %782 = bitcast <2 x i64> %778 to <8 x i16>
  %783 = icmp sgt <8 x i16> %766, %782
  %784 = select <8 x i1> %783, <8 x i16> %766, <8 x i16> %782
  %785 = bitcast <2 x i64> %723 to <8 x i16>
  %786 = icmp eq <8 x i16> %785, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %787 = sext <8 x i1> %786 to <8 x i16>
  %788 = bitcast <2 x i64> %724 to <8 x i16>
  %789 = icmp eq <8 x i16> %788, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %790 = sext <8 x i1> %789 to <8 x i16>
  %791 = bitcast <8 x i16> %790 to <2 x i64>
  %792 = bitcast <8 x i16> %787 to <2 x i64>
  %793 = xor <2 x i64> %792, <i64 -1, i64 -1>
  %794 = and <2 x i64> %723, %793
  %795 = xor <2 x i64> %791, <i64 -1, i64 -1>
  %796 = and <2 x i64> %724, %795
  %797 = bitcast <2 x i64> %794 to <8 x i16>
  %798 = icmp sgt <8 x i16> %781, %797
  %799 = select <8 x i1> %798, <8 x i16> %781, <8 x i16> %797
  %800 = bitcast <2 x i64> %796 to <8 x i16>
  %801 = icmp sgt <8 x i16> %784, %800
  %802 = select <8 x i1> %801, <8 x i16> %784, <8 x i16> %800
  %803 = bitcast <2 x i64> %747 to <8 x i16>
  %804 = icmp eq <8 x i16> %803, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %805 = sext <8 x i1> %804 to <8 x i16>
  %806 = bitcast <2 x i64> %748 to <8 x i16>
  %807 = icmp eq <8 x i16> %806, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %808 = sext <8 x i1> %807 to <8 x i16>
  %809 = bitcast <8 x i16> %808 to <2 x i64>
  %810 = bitcast <8 x i16> %805 to <2 x i64>
  %811 = xor <2 x i64> %810, <i64 -1, i64 -1>
  %812 = and <2 x i64> %747, %811
  %813 = xor <2 x i64> %809, <i64 -1, i64 -1>
  %814 = and <2 x i64> %748, %813
  %815 = bitcast <2 x i64> %812 to <8 x i16>
  %816 = icmp sgt <8 x i16> %799, %815
  %817 = select <8 x i1> %816, <8 x i16> %799, <8 x i16> %815
  %818 = bitcast <2 x i64> %814 to <8 x i16>
  %819 = icmp sgt <8 x i16> %802, %818
  %820 = select <8 x i1> %819, <8 x i16> %802, <8 x i16> %818
  %821 = icmp slt <8 x i16> %554, %749
  %822 = select <8 x i1> %821, <8 x i16> %554, <8 x i16> %749
  %823 = icmp slt <8 x i16> %556, %752
  %824 = select <8 x i1> %823, <8 x i16> %556, <8 x i16> %752
  %825 = icmp slt <8 x i16> %822, %767
  %826 = select <8 x i1> %825, <8 x i16> %822, <8 x i16> %767
  %827 = icmp slt <8 x i16> %824, %770
  %828 = select <8 x i1> %827, <8 x i16> %824, <8 x i16> %770
  %829 = icmp slt <8 x i16> %826, %785
  %830 = select <8 x i1> %829, <8 x i16> %826, <8 x i16> %785
  %831 = icmp slt <8 x i16> %828, %788
  %832 = select <8 x i1> %831, <8 x i16> %828, <8 x i16> %788
  %833 = icmp slt <8 x i16> %830, %803
  %834 = select <8 x i1> %833, <8 x i16> %830, <8 x i16> %803
  %835 = icmp slt <8 x i16> %832, %806
  %836 = select <8 x i1> %835, <8 x i16> %832, <8 x i16> %806
  %837 = sub <8 x i16> %749, %152
  %838 = sub <8 x i16> %752, %156
  %839 = ashr <8 x i16> %837, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %840 = ashr <8 x i16> %838, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %841 = sub <8 x i16> zeroinitializer, %837
  %842 = icmp slt <8 x i16> %837, zeroinitializer
  %843 = select <8 x i1> %842, <8 x i16> %841, <8 x i16> %837
  %844 = sub <8 x i16> zeroinitializer, %838
  %845 = icmp slt <8 x i16> %838, zeroinitializer
  %846 = select <8 x i1> %845, <8 x i16> %844, <8 x i16> %838
  %847 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %843, <8 x i16> %69) #8
  %848 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %846, <8 x i16> %69) #8
  %849 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %847) #8
  %850 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %848) #8
  %851 = icmp slt <8 x i16> %843, %849
  %852 = select <8 x i1> %851, <8 x i16> %843, <8 x i16> %849
  %853 = icmp slt <8 x i16> %846, %850
  %854 = select <8 x i1> %853, <8 x i16> %846, <8 x i16> %850
  %855 = add <8 x i16> %852, %839
  %856 = add <8 x i16> %854, %840
  %857 = xor <8 x i16> %855, %839
  %858 = xor <8 x i16> %856, %840
  %859 = sub <8 x i16> %767, %152
  %860 = sub <8 x i16> %770, %156
  %861 = ashr <8 x i16> %859, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %862 = ashr <8 x i16> %860, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %863 = sub <8 x i16> zeroinitializer, %859
  %864 = icmp slt <8 x i16> %859, zeroinitializer
  %865 = select <8 x i1> %864, <8 x i16> %863, <8 x i16> %859
  %866 = sub <8 x i16> zeroinitializer, %860
  %867 = icmp slt <8 x i16> %860, zeroinitializer
  %868 = select <8 x i1> %867, <8 x i16> %866, <8 x i16> %860
  %869 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %865, <8 x i16> %69) #8
  %870 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %868, <8 x i16> %69) #8
  %871 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %869) #8
  %872 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %870) #8
  %873 = icmp slt <8 x i16> %865, %871
  %874 = select <8 x i1> %873, <8 x i16> %865, <8 x i16> %871
  %875 = icmp slt <8 x i16> %868, %872
  %876 = select <8 x i1> %875, <8 x i16> %868, <8 x i16> %872
  %877 = add <8 x i16> %874, %861
  %878 = add <8 x i16> %876, %862
  %879 = xor <8 x i16> %877, %861
  %880 = xor <8 x i16> %878, %862
  %881 = sub <8 x i16> %785, %152
  %882 = sub <8 x i16> %788, %156
  %883 = ashr <8 x i16> %881, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %884 = ashr <8 x i16> %882, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %885 = sub <8 x i16> zeroinitializer, %881
  %886 = icmp slt <8 x i16> %881, zeroinitializer
  %887 = select <8 x i1> %886, <8 x i16> %885, <8 x i16> %881
  %888 = sub <8 x i16> zeroinitializer, %882
  %889 = icmp slt <8 x i16> %882, zeroinitializer
  %890 = select <8 x i1> %889, <8 x i16> %888, <8 x i16> %882
  %891 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %887, <8 x i16> %69) #8
  %892 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %890, <8 x i16> %69) #8
  %893 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %891) #8
  %894 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %892) #8
  %895 = icmp slt <8 x i16> %887, %893
  %896 = select <8 x i1> %895, <8 x i16> %887, <8 x i16> %893
  %897 = icmp slt <8 x i16> %890, %894
  %898 = select <8 x i1> %897, <8 x i16> %890, <8 x i16> %894
  %899 = add <8 x i16> %896, %883
  %900 = add <8 x i16> %898, %884
  %901 = xor <8 x i16> %899, %883
  %902 = xor <8 x i16> %900, %884
  %903 = sub <8 x i16> %803, %152
  %904 = sub <8 x i16> %806, %156
  %905 = ashr <8 x i16> %903, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %906 = ashr <8 x i16> %904, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %907 = sub <8 x i16> zeroinitializer, %903
  %908 = icmp slt <8 x i16> %903, zeroinitializer
  %909 = select <8 x i1> %908, <8 x i16> %907, <8 x i16> %903
  %910 = sub <8 x i16> zeroinitializer, %904
  %911 = icmp slt <8 x i16> %904, zeroinitializer
  %912 = select <8 x i1> %911, <8 x i16> %910, <8 x i16> %904
  %913 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %909, <8 x i16> %69) #8
  %914 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %912, <8 x i16> %69) #8
  %915 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %913) #8
  %916 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %914) #8
  %917 = icmp slt <8 x i16> %909, %915
  %918 = select <8 x i1> %917, <8 x i16> %909, <8 x i16> %915
  %919 = icmp slt <8 x i16> %912, %916
  %920 = select <8 x i1> %919, <8 x i16> %912, <8 x i16> %916
  %921 = add <8 x i16> %918, %905
  %922 = add <8 x i16> %920, %906
  %923 = xor <8 x i16> %921, %905
  %924 = xor <8 x i16> %922, %906
  %925 = add <8 x i16> %879, %857
  %926 = add <8 x i16> %925, %901
  %927 = add <8 x i16> %926, %923
  %928 = add <8 x i16> %880, %858
  %929 = add <8 x i16> %928, %902
  %930 = add <8 x i16> %929, %924
  %931 = mul <8 x i16> %927, %77
  %932 = mul <8 x i16> %930, %77
  %933 = add <8 x i16> %653, %931
  %934 = add <8 x i16> %654, %932
  %935 = ashr <8 x i16> %933, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %936 = ashr <8 x i16> %934, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %937 = add <8 x i16> %933, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %938 = add <8 x i16> %937, %935
  %939 = add <8 x i16> %934, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %940 = add <8 x i16> %939, %936
  %941 = ashr <8 x i16> %938, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %942 = ashr <8 x i16> %940, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %943 = add <8 x i16> %941, %152
  %944 = add <8 x i16> %942, %156
  %945 = icmp sgt <8 x i16> %943, %834
  %946 = select <8 x i1> %945, <8 x i16> %943, <8 x i16> %834
  %947 = icmp sgt <8 x i16> %944, %836
  %948 = select <8 x i1> %947, <8 x i16> %944, <8 x i16> %836
  %949 = icmp slt <8 x i16> %946, %817
  %950 = select <8 x i1> %949, <8 x i16> %946, <8 x i16> %817
  %951 = icmp slt <8 x i16> %948, %820
  %952 = select <8 x i1> %951, <8 x i16> %948, <8 x i16> %820
  %953 = bitcast <8 x i16> %952 to <2 x i64>
  %954 = bitcast <8 x i16> %950 to <2 x i64>
  %955 = bitcast <8 x i16> %950 to <16 x i8>
  %956 = shufflevector <16 x i8> %955, <16 x i8> undef, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %957 = bitcast <16 x i8> %956 to <2 x i64>
  %958 = extractelement <2 x i64> %957, i32 0
  %959 = bitcast i16* %0 to i64*
  store i64 %958, i64* %959, align 1
  %960 = sext i32 %1 to i64
  %961 = getelementptr inbounds i16, i16* %0, i64 %960
  %962 = extractelement <2 x i64> %954, i32 0
  %963 = bitcast i16* %961 to i64*
  store i64 %962, i64* %963, align 1
  %964 = shl nsw i32 %1, 1
  %965 = sext i32 %964 to i64
  %966 = getelementptr inbounds i16, i16* %0, i64 %965
  %967 = bitcast <8 x i16> %952 to <16 x i8>
  %968 = shufflevector <16 x i8> %967, <16 x i8> undef, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %969 = bitcast <16 x i8> %968 to <2 x i64>
  %970 = extractelement <2 x i64> %969, i32 0
  %971 = bitcast i16* %966 to i64*
  store i64 %970, i64* %971, align 1
  %972 = mul nsw i32 %1, 3
  %973 = sext i32 %972 to i64
  %974 = getelementptr inbounds i16, i16* %0, i64 %973
  %975 = extractelement <2 x i64> %953, i32 0
  %976 = bitcast i16* %974 to i64*
  store i64 %975, i64* %976, align 1
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @cdef_filter_block_8x8_16_ssse3(i16* nocapture, i32, i16*, i32, i32, i32, i32, i32, i32) local_unnamed_addr #0 {
  %10 = sext i32 %5 to i64
  %11 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %10, i64 0
  %12 = load i32, i32* %11, align 8
  %13 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %10, i64 1
  %14 = load i32, i32* %13, align 4
  %15 = add nsw i32 %5, 2
  %16 = and i32 %15, 7
  %17 = zext i32 %16 to i64
  %18 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %17, i64 0
  %19 = load i32, i32* %18, align 8
  %20 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %17, i64 1
  %21 = load i32, i32* %20, align 4
  %22 = add nsw i32 %5, 6
  %23 = and i32 %22, 7
  %24 = zext i32 %23 to i64
  %25 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %24, i64 0
  %26 = load i32, i32* %25, align 8
  %27 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %24, i64 1
  %28 = load i32, i32* %27, align 4
  %29 = lshr i32 %3, %8
  %30 = and i32 %29, 1
  %31 = zext i32 %30 to i64
  %32 = getelementptr inbounds [2 x [2 x i32]], [2 x [2 x i32]]* @cdef_pri_taps, i64 0, i64 %31, i64 0
  %33 = icmp eq i32 %3, 0
  br i1 %33, label %40, label %34

34:                                               ; preds = %9
  %35 = tail call i32 @llvm.ctlz.i32(i32 %3, i1 true) #8, !range !2
  %36 = xor i32 %35, 31
  %37 = icmp sgt i32 %36, %6
  %38 = sub nsw i32 %6, %36
  %39 = select i1 %37, i32 0, i32 %38
  br label %40

40:                                               ; preds = %34, %9
  %41 = phi i32 [ %6, %9 ], [ %39, %34 ]
  %42 = icmp eq i32 %4, 0
  br i1 %42, label %49, label %43

43:                                               ; preds = %40
  %44 = tail call i32 @llvm.ctlz.i32(i32 %4, i1 true) #8, !range !2
  %45 = xor i32 %44, 31
  %46 = icmp sgt i32 %45, %7
  %47 = sub nsw i32 %7, %45
  %48 = select i1 %46, i32 0, i32 %47
  br label %49

49:                                               ; preds = %43, %40
  %50 = phi i32 [ %7, %40 ], [ %48, %43 ]
  %51 = trunc i32 %3 to i16
  %52 = insertelement <8 x i16> undef, i16 %51, i32 0
  %53 = shufflevector <8 x i16> %52, <8 x i16> undef, <8 x i32> zeroinitializer
  %54 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %41, i32 0
  %55 = bitcast <4 x i32> %54 to <8 x i16>
  %56 = load i32, i32* %32, align 8
  %57 = trunc i32 %56 to i16
  %58 = insertelement <8 x i16> undef, i16 %57, i32 0
  %59 = shufflevector <8 x i16> %58, <8 x i16> undef, <8 x i32> zeroinitializer
  %60 = getelementptr inbounds [2 x [2 x i32]], [2 x [2 x i32]]* @cdef_pri_taps, i64 0, i64 %31, i64 1
  %61 = load i32, i32* %60, align 4
  %62 = trunc i32 %61 to i16
  %63 = insertelement <8 x i16> undef, i16 %62, i32 0
  %64 = shufflevector <8 x i16> %63, <8 x i16> undef, <8 x i32> zeroinitializer
  %65 = trunc i32 %4 to i16
  %66 = insertelement <8 x i16> undef, i16 %65, i32 0
  %67 = shufflevector <8 x i16> %66, <8 x i16> undef, <8 x i32> zeroinitializer
  %68 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %50, i32 0
  %69 = bitcast <4 x i32> %68 to <8 x i16>
  %70 = load i32, i32* getelementptr inbounds ([2 x i32], [2 x i32]* @cdef_sec_taps, i64 0, i64 0), align 4
  %71 = trunc i32 %70 to i16
  %72 = insertelement <8 x i16> undef, i16 %71, i32 0
  %73 = shufflevector <8 x i16> %72, <8 x i16> undef, <8 x i32> zeroinitializer
  %74 = load i32, i32* getelementptr inbounds ([2 x i32], [2 x i32]* @cdef_sec_taps, i64 0, i64 1), align 4
  %75 = trunc i32 %74 to i16
  %76 = insertelement <8 x i16> undef, i16 %75, i32 0
  %77 = shufflevector <8 x i16> %76, <8 x i16> undef, <8 x i32> zeroinitializer
  %78 = sext i32 %1 to i64
  %79 = sext i32 %12 to i64
  %80 = sext i32 %14 to i64
  %81 = sext i32 %19 to i64
  %82 = sext i32 %26 to i64
  %83 = sext i32 %21 to i64
  %84 = sext i32 %28 to i64
  br label %85

85:                                               ; preds = %49, %85
  %86 = phi i64 [ 0, %49 ], [ %798, %85 ]
  %87 = mul nuw nsw i64 %86, 144
  %88 = getelementptr inbounds i16, i16* %2, i64 %87
  %89 = bitcast i16* %88 to <8 x i16>*
  %90 = load <8 x i16>, <8 x i16>* %89, align 16
  %91 = or i64 %86, 1
  %92 = mul nuw nsw i64 %91, 144
  %93 = getelementptr inbounds i16, i16* %2, i64 %92
  %94 = bitcast i16* %93 to <8 x i16>*
  %95 = load <8 x i16>, <8 x i16>* %94, align 16
  %96 = add nsw i64 %87, %79
  %97 = getelementptr inbounds i16, i16* %2, i64 %96
  %98 = bitcast i16* %97 to i8*
  %99 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %98) #8
  %100 = bitcast <16 x i8> %99 to <2 x i64>
  %101 = add nsw i64 %92, %79
  %102 = getelementptr inbounds i16, i16* %2, i64 %101
  %103 = bitcast i16* %102 to i8*
  %104 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %103) #8
  %105 = bitcast <16 x i8> %104 to <2 x i64>
  %106 = sub nsw i64 %87, %79
  %107 = getelementptr inbounds i16, i16* %2, i64 %106
  %108 = bitcast i16* %107 to i8*
  %109 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %108) #8
  %110 = bitcast <16 x i8> %109 to <2 x i64>
  %111 = sub nsw i64 %92, %79
  %112 = getelementptr inbounds i16, i16* %2, i64 %111
  %113 = bitcast i16* %112 to i8*
  %114 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %113) #8
  %115 = bitcast <16 x i8> %114 to <2 x i64>
  %116 = bitcast <16 x i8> %99 to <8 x i16>
  %117 = icmp eq <8 x i16> %116, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %118 = sext <8 x i1> %117 to <8 x i16>
  %119 = bitcast <16 x i8> %104 to <8 x i16>
  %120 = icmp eq <8 x i16> %119, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %121 = sext <8 x i1> %120 to <8 x i16>
  %122 = bitcast <8 x i16> %121 to <2 x i64>
  %123 = bitcast <8 x i16> %118 to <2 x i64>
  %124 = xor <2 x i64> %123, <i64 -1, i64 -1>
  %125 = and <2 x i64> %124, %100
  %126 = xor <2 x i64> %122, <i64 -1, i64 -1>
  %127 = and <2 x i64> %126, %105
  %128 = bitcast <2 x i64> %125 to <8 x i16>
  %129 = icmp sgt <8 x i16> %90, %128
  %130 = select <8 x i1> %129, <8 x i16> %90, <8 x i16> %128
  %131 = bitcast <2 x i64> %127 to <8 x i16>
  %132 = icmp sgt <8 x i16> %95, %131
  %133 = select <8 x i1> %132, <8 x i16> %95, <8 x i16> %131
  %134 = bitcast <16 x i8> %109 to <8 x i16>
  %135 = icmp eq <8 x i16> %134, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %136 = sext <8 x i1> %135 to <8 x i16>
  %137 = bitcast <16 x i8> %114 to <8 x i16>
  %138 = icmp eq <8 x i16> %137, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %139 = sext <8 x i1> %138 to <8 x i16>
  %140 = bitcast <8 x i16> %139 to <2 x i64>
  %141 = bitcast <8 x i16> %136 to <2 x i64>
  %142 = xor <2 x i64> %141, <i64 -1, i64 -1>
  %143 = and <2 x i64> %142, %110
  %144 = xor <2 x i64> %140, <i64 -1, i64 -1>
  %145 = and <2 x i64> %144, %115
  %146 = bitcast <2 x i64> %143 to <8 x i16>
  %147 = icmp sgt <8 x i16> %130, %146
  %148 = select <8 x i1> %147, <8 x i16> %130, <8 x i16> %146
  %149 = bitcast <2 x i64> %145 to <8 x i16>
  %150 = icmp sgt <8 x i16> %133, %149
  %151 = select <8 x i1> %150, <8 x i16> %133, <8 x i16> %149
  %152 = icmp slt <8 x i16> %90, %116
  %153 = select <8 x i1> %152, <8 x i16> %90, <8 x i16> %116
  %154 = icmp slt <8 x i16> %95, %119
  %155 = select <8 x i1> %154, <8 x i16> %95, <8 x i16> %119
  %156 = icmp slt <8 x i16> %153, %134
  %157 = select <8 x i1> %156, <8 x i16> %153, <8 x i16> %134
  %158 = icmp slt <8 x i16> %155, %137
  %159 = select <8 x i1> %158, <8 x i16> %155, <8 x i16> %137
  %160 = sub <8 x i16> %116, %90
  %161 = sub <8 x i16> %119, %95
  %162 = ashr <8 x i16> %160, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %163 = ashr <8 x i16> %161, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %164 = sub <8 x i16> zeroinitializer, %160
  %165 = icmp slt <8 x i16> %160, zeroinitializer
  %166 = select <8 x i1> %165, <8 x i16> %164, <8 x i16> %160
  %167 = sub <8 x i16> zeroinitializer, %161
  %168 = icmp slt <8 x i16> %161, zeroinitializer
  %169 = select <8 x i1> %168, <8 x i16> %167, <8 x i16> %161
  %170 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %166, <8 x i16> %55) #8
  %171 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %169, <8 x i16> %55) #8
  %172 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %53, <8 x i16> %170) #8
  %173 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %53, <8 x i16> %171) #8
  %174 = icmp slt <8 x i16> %166, %172
  %175 = select <8 x i1> %174, <8 x i16> %166, <8 x i16> %172
  %176 = icmp slt <8 x i16> %169, %173
  %177 = select <8 x i1> %176, <8 x i16> %169, <8 x i16> %173
  %178 = add <8 x i16> %175, %162
  %179 = add <8 x i16> %177, %163
  %180 = xor <8 x i16> %178, %162
  %181 = xor <8 x i16> %179, %163
  %182 = sub <8 x i16> %134, %90
  %183 = sub <8 x i16> %137, %95
  %184 = ashr <8 x i16> %182, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %185 = ashr <8 x i16> %183, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %186 = sub <8 x i16> zeroinitializer, %182
  %187 = icmp slt <8 x i16> %182, zeroinitializer
  %188 = select <8 x i1> %187, <8 x i16> %186, <8 x i16> %182
  %189 = sub <8 x i16> zeroinitializer, %183
  %190 = icmp slt <8 x i16> %183, zeroinitializer
  %191 = select <8 x i1> %190, <8 x i16> %189, <8 x i16> %183
  %192 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %188, <8 x i16> %55) #8
  %193 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %191, <8 x i16> %55) #8
  %194 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %53, <8 x i16> %192) #8
  %195 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %53, <8 x i16> %193) #8
  %196 = icmp slt <8 x i16> %188, %194
  %197 = select <8 x i1> %196, <8 x i16> %188, <8 x i16> %194
  %198 = icmp slt <8 x i16> %191, %195
  %199 = select <8 x i1> %198, <8 x i16> %191, <8 x i16> %195
  %200 = add <8 x i16> %197, %184
  %201 = add <8 x i16> %199, %185
  %202 = xor <8 x i16> %200, %184
  %203 = xor <8 x i16> %201, %185
  %204 = add <8 x i16> %202, %180
  %205 = add <8 x i16> %203, %181
  %206 = mul <8 x i16> %204, %59
  %207 = mul <8 x i16> %205, %59
  %208 = add nsw i64 %87, %80
  %209 = getelementptr inbounds i16, i16* %2, i64 %208
  %210 = bitcast i16* %209 to i8*
  %211 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %210) #8
  %212 = bitcast <16 x i8> %211 to <2 x i64>
  %213 = add nsw i64 %92, %80
  %214 = getelementptr inbounds i16, i16* %2, i64 %213
  %215 = bitcast i16* %214 to i8*
  %216 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %215) #8
  %217 = bitcast <16 x i8> %216 to <2 x i64>
  %218 = sub nsw i64 %87, %80
  %219 = getelementptr inbounds i16, i16* %2, i64 %218
  %220 = bitcast i16* %219 to i8*
  %221 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %220) #8
  %222 = bitcast <16 x i8> %221 to <2 x i64>
  %223 = sub nsw i64 %92, %80
  %224 = getelementptr inbounds i16, i16* %2, i64 %223
  %225 = bitcast i16* %224 to i8*
  %226 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %225) #8
  %227 = bitcast <16 x i8> %226 to <2 x i64>
  %228 = bitcast <16 x i8> %211 to <8 x i16>
  %229 = icmp eq <8 x i16> %228, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %230 = sext <8 x i1> %229 to <8 x i16>
  %231 = bitcast <16 x i8> %216 to <8 x i16>
  %232 = icmp eq <8 x i16> %231, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %233 = sext <8 x i1> %232 to <8 x i16>
  %234 = bitcast <8 x i16> %233 to <2 x i64>
  %235 = bitcast <8 x i16> %230 to <2 x i64>
  %236 = xor <2 x i64> %235, <i64 -1, i64 -1>
  %237 = and <2 x i64> %236, %212
  %238 = xor <2 x i64> %234, <i64 -1, i64 -1>
  %239 = and <2 x i64> %238, %217
  %240 = bitcast <2 x i64> %237 to <8 x i16>
  %241 = icmp sgt <8 x i16> %148, %240
  %242 = select <8 x i1> %241, <8 x i16> %148, <8 x i16> %240
  %243 = bitcast <2 x i64> %239 to <8 x i16>
  %244 = icmp sgt <8 x i16> %151, %243
  %245 = select <8 x i1> %244, <8 x i16> %151, <8 x i16> %243
  %246 = bitcast <16 x i8> %221 to <8 x i16>
  %247 = icmp eq <8 x i16> %246, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %248 = sext <8 x i1> %247 to <8 x i16>
  %249 = bitcast <16 x i8> %226 to <8 x i16>
  %250 = icmp eq <8 x i16> %249, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %251 = sext <8 x i1> %250 to <8 x i16>
  %252 = bitcast <8 x i16> %251 to <2 x i64>
  %253 = bitcast <8 x i16> %248 to <2 x i64>
  %254 = xor <2 x i64> %253, <i64 -1, i64 -1>
  %255 = and <2 x i64> %254, %222
  %256 = xor <2 x i64> %252, <i64 -1, i64 -1>
  %257 = and <2 x i64> %256, %227
  %258 = bitcast <2 x i64> %255 to <8 x i16>
  %259 = icmp sgt <8 x i16> %242, %258
  %260 = select <8 x i1> %259, <8 x i16> %242, <8 x i16> %258
  %261 = bitcast <2 x i64> %257 to <8 x i16>
  %262 = icmp sgt <8 x i16> %245, %261
  %263 = select <8 x i1> %262, <8 x i16> %245, <8 x i16> %261
  %264 = icmp slt <8 x i16> %157, %228
  %265 = select <8 x i1> %264, <8 x i16> %157, <8 x i16> %228
  %266 = icmp slt <8 x i16> %159, %231
  %267 = select <8 x i1> %266, <8 x i16> %159, <8 x i16> %231
  %268 = icmp slt <8 x i16> %265, %246
  %269 = select <8 x i1> %268, <8 x i16> %265, <8 x i16> %246
  %270 = icmp slt <8 x i16> %267, %249
  %271 = select <8 x i1> %270, <8 x i16> %267, <8 x i16> %249
  %272 = sub <8 x i16> %228, %90
  %273 = sub <8 x i16> %231, %95
  %274 = ashr <8 x i16> %272, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %275 = ashr <8 x i16> %273, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %276 = sub <8 x i16> zeroinitializer, %272
  %277 = icmp slt <8 x i16> %272, zeroinitializer
  %278 = select <8 x i1> %277, <8 x i16> %276, <8 x i16> %272
  %279 = sub <8 x i16> zeroinitializer, %273
  %280 = icmp slt <8 x i16> %273, zeroinitializer
  %281 = select <8 x i1> %280, <8 x i16> %279, <8 x i16> %273
  %282 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %278, <8 x i16> %55) #8
  %283 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %281, <8 x i16> %55) #8
  %284 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %53, <8 x i16> %282) #8
  %285 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %53, <8 x i16> %283) #8
  %286 = icmp slt <8 x i16> %278, %284
  %287 = select <8 x i1> %286, <8 x i16> %278, <8 x i16> %284
  %288 = icmp slt <8 x i16> %281, %285
  %289 = select <8 x i1> %288, <8 x i16> %281, <8 x i16> %285
  %290 = add <8 x i16> %287, %274
  %291 = add <8 x i16> %289, %275
  %292 = xor <8 x i16> %290, %274
  %293 = xor <8 x i16> %291, %275
  %294 = sub <8 x i16> %246, %90
  %295 = sub <8 x i16> %249, %95
  %296 = ashr <8 x i16> %294, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %297 = ashr <8 x i16> %295, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %298 = sub <8 x i16> zeroinitializer, %294
  %299 = icmp slt <8 x i16> %294, zeroinitializer
  %300 = select <8 x i1> %299, <8 x i16> %298, <8 x i16> %294
  %301 = sub <8 x i16> zeroinitializer, %295
  %302 = icmp slt <8 x i16> %295, zeroinitializer
  %303 = select <8 x i1> %302, <8 x i16> %301, <8 x i16> %295
  %304 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %300, <8 x i16> %55) #8
  %305 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %303, <8 x i16> %55) #8
  %306 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %53, <8 x i16> %304) #8
  %307 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %53, <8 x i16> %305) #8
  %308 = icmp slt <8 x i16> %300, %306
  %309 = select <8 x i1> %308, <8 x i16> %300, <8 x i16> %306
  %310 = icmp slt <8 x i16> %303, %307
  %311 = select <8 x i1> %310, <8 x i16> %303, <8 x i16> %307
  %312 = add <8 x i16> %309, %296
  %313 = add <8 x i16> %311, %297
  %314 = xor <8 x i16> %312, %296
  %315 = xor <8 x i16> %313, %297
  %316 = add <8 x i16> %314, %292
  %317 = add <8 x i16> %315, %293
  %318 = mul <8 x i16> %316, %64
  %319 = mul <8 x i16> %317, %64
  %320 = add <8 x i16> %318, %206
  %321 = add <8 x i16> %319, %207
  %322 = add nsw i64 %87, %81
  %323 = getelementptr inbounds i16, i16* %2, i64 %322
  %324 = bitcast i16* %323 to i8*
  %325 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %324) #8
  %326 = bitcast <16 x i8> %325 to <2 x i64>
  %327 = add nsw i64 %92, %81
  %328 = getelementptr inbounds i16, i16* %2, i64 %327
  %329 = bitcast i16* %328 to i8*
  %330 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %329) #8
  %331 = bitcast <16 x i8> %330 to <2 x i64>
  %332 = sub nsw i64 %87, %81
  %333 = getelementptr inbounds i16, i16* %2, i64 %332
  %334 = bitcast i16* %333 to i8*
  %335 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %334) #8
  %336 = bitcast <16 x i8> %335 to <2 x i64>
  %337 = sub nsw i64 %92, %81
  %338 = getelementptr inbounds i16, i16* %2, i64 %337
  %339 = bitcast i16* %338 to i8*
  %340 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %339) #8
  %341 = bitcast <16 x i8> %340 to <2 x i64>
  %342 = add nsw i64 %87, %82
  %343 = getelementptr inbounds i16, i16* %2, i64 %342
  %344 = bitcast i16* %343 to i8*
  %345 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %344) #8
  %346 = bitcast <16 x i8> %345 to <2 x i64>
  %347 = add nsw i64 %92, %82
  %348 = getelementptr inbounds i16, i16* %2, i64 %347
  %349 = bitcast i16* %348 to i8*
  %350 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %349) #8
  %351 = bitcast <16 x i8> %350 to <2 x i64>
  %352 = sub nsw i64 %87, %82
  %353 = getelementptr inbounds i16, i16* %2, i64 %352
  %354 = bitcast i16* %353 to i8*
  %355 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %354) #8
  %356 = bitcast <16 x i8> %355 to <2 x i64>
  %357 = sub nsw i64 %92, %82
  %358 = getelementptr inbounds i16, i16* %2, i64 %357
  %359 = bitcast i16* %358 to i8*
  %360 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %359) #8
  %361 = bitcast <16 x i8> %360 to <2 x i64>
  %362 = bitcast <16 x i8> %325 to <8 x i16>
  %363 = icmp eq <8 x i16> %362, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %364 = sext <8 x i1> %363 to <8 x i16>
  %365 = bitcast <16 x i8> %330 to <8 x i16>
  %366 = icmp eq <8 x i16> %365, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %367 = sext <8 x i1> %366 to <8 x i16>
  %368 = bitcast <8 x i16> %367 to <2 x i64>
  %369 = bitcast <8 x i16> %364 to <2 x i64>
  %370 = xor <2 x i64> %369, <i64 -1, i64 -1>
  %371 = and <2 x i64> %370, %326
  %372 = xor <2 x i64> %368, <i64 -1, i64 -1>
  %373 = and <2 x i64> %372, %331
  %374 = bitcast <2 x i64> %371 to <8 x i16>
  %375 = icmp sgt <8 x i16> %260, %374
  %376 = select <8 x i1> %375, <8 x i16> %260, <8 x i16> %374
  %377 = bitcast <2 x i64> %373 to <8 x i16>
  %378 = icmp sgt <8 x i16> %263, %377
  %379 = select <8 x i1> %378, <8 x i16> %263, <8 x i16> %377
  %380 = bitcast <16 x i8> %335 to <8 x i16>
  %381 = icmp eq <8 x i16> %380, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %382 = sext <8 x i1> %381 to <8 x i16>
  %383 = bitcast <16 x i8> %340 to <8 x i16>
  %384 = icmp eq <8 x i16> %383, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %385 = sext <8 x i1> %384 to <8 x i16>
  %386 = bitcast <8 x i16> %385 to <2 x i64>
  %387 = bitcast <8 x i16> %382 to <2 x i64>
  %388 = xor <2 x i64> %387, <i64 -1, i64 -1>
  %389 = and <2 x i64> %388, %336
  %390 = xor <2 x i64> %386, <i64 -1, i64 -1>
  %391 = and <2 x i64> %390, %341
  %392 = bitcast <2 x i64> %389 to <8 x i16>
  %393 = icmp sgt <8 x i16> %376, %392
  %394 = select <8 x i1> %393, <8 x i16> %376, <8 x i16> %392
  %395 = bitcast <2 x i64> %391 to <8 x i16>
  %396 = icmp sgt <8 x i16> %379, %395
  %397 = select <8 x i1> %396, <8 x i16> %379, <8 x i16> %395
  %398 = bitcast <16 x i8> %345 to <8 x i16>
  %399 = icmp eq <8 x i16> %398, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %400 = sext <8 x i1> %399 to <8 x i16>
  %401 = bitcast <16 x i8> %350 to <8 x i16>
  %402 = icmp eq <8 x i16> %401, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %403 = sext <8 x i1> %402 to <8 x i16>
  %404 = bitcast <8 x i16> %403 to <2 x i64>
  %405 = bitcast <8 x i16> %400 to <2 x i64>
  %406 = xor <2 x i64> %405, <i64 -1, i64 -1>
  %407 = and <2 x i64> %406, %346
  %408 = xor <2 x i64> %404, <i64 -1, i64 -1>
  %409 = and <2 x i64> %408, %351
  %410 = bitcast <2 x i64> %407 to <8 x i16>
  %411 = icmp sgt <8 x i16> %394, %410
  %412 = select <8 x i1> %411, <8 x i16> %394, <8 x i16> %410
  %413 = bitcast <2 x i64> %409 to <8 x i16>
  %414 = icmp sgt <8 x i16> %397, %413
  %415 = select <8 x i1> %414, <8 x i16> %397, <8 x i16> %413
  %416 = bitcast <16 x i8> %355 to <8 x i16>
  %417 = icmp eq <8 x i16> %416, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %418 = sext <8 x i1> %417 to <8 x i16>
  %419 = bitcast <16 x i8> %360 to <8 x i16>
  %420 = icmp eq <8 x i16> %419, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %421 = sext <8 x i1> %420 to <8 x i16>
  %422 = bitcast <8 x i16> %421 to <2 x i64>
  %423 = bitcast <8 x i16> %418 to <2 x i64>
  %424 = xor <2 x i64> %423, <i64 -1, i64 -1>
  %425 = and <2 x i64> %424, %356
  %426 = xor <2 x i64> %422, <i64 -1, i64 -1>
  %427 = and <2 x i64> %426, %361
  %428 = bitcast <2 x i64> %425 to <8 x i16>
  %429 = icmp sgt <8 x i16> %412, %428
  %430 = select <8 x i1> %429, <8 x i16> %412, <8 x i16> %428
  %431 = bitcast <2 x i64> %427 to <8 x i16>
  %432 = icmp sgt <8 x i16> %415, %431
  %433 = select <8 x i1> %432, <8 x i16> %415, <8 x i16> %431
  %434 = icmp slt <8 x i16> %269, %362
  %435 = select <8 x i1> %434, <8 x i16> %269, <8 x i16> %362
  %436 = icmp slt <8 x i16> %271, %365
  %437 = select <8 x i1> %436, <8 x i16> %271, <8 x i16> %365
  %438 = icmp slt <8 x i16> %435, %380
  %439 = select <8 x i1> %438, <8 x i16> %435, <8 x i16> %380
  %440 = icmp slt <8 x i16> %437, %383
  %441 = select <8 x i1> %440, <8 x i16> %437, <8 x i16> %383
  %442 = icmp slt <8 x i16> %439, %398
  %443 = select <8 x i1> %442, <8 x i16> %439, <8 x i16> %398
  %444 = icmp slt <8 x i16> %441, %401
  %445 = select <8 x i1> %444, <8 x i16> %441, <8 x i16> %401
  %446 = icmp slt <8 x i16> %443, %416
  %447 = select <8 x i1> %446, <8 x i16> %443, <8 x i16> %416
  %448 = icmp slt <8 x i16> %445, %419
  %449 = select <8 x i1> %448, <8 x i16> %445, <8 x i16> %419
  %450 = sub <8 x i16> %362, %90
  %451 = sub <8 x i16> %365, %95
  %452 = ashr <8 x i16> %450, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %453 = ashr <8 x i16> %451, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %454 = sub <8 x i16> zeroinitializer, %450
  %455 = icmp slt <8 x i16> %450, zeroinitializer
  %456 = select <8 x i1> %455, <8 x i16> %454, <8 x i16> %450
  %457 = sub <8 x i16> zeroinitializer, %451
  %458 = icmp slt <8 x i16> %451, zeroinitializer
  %459 = select <8 x i1> %458, <8 x i16> %457, <8 x i16> %451
  %460 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %456, <8 x i16> %69) #8
  %461 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %459, <8 x i16> %69) #8
  %462 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %460) #8
  %463 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %461) #8
  %464 = icmp slt <8 x i16> %456, %462
  %465 = select <8 x i1> %464, <8 x i16> %456, <8 x i16> %462
  %466 = icmp slt <8 x i16> %459, %463
  %467 = select <8 x i1> %466, <8 x i16> %459, <8 x i16> %463
  %468 = add <8 x i16> %465, %452
  %469 = add <8 x i16> %467, %453
  %470 = xor <8 x i16> %468, %452
  %471 = xor <8 x i16> %469, %453
  %472 = sub <8 x i16> %380, %90
  %473 = sub <8 x i16> %383, %95
  %474 = ashr <8 x i16> %472, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %475 = ashr <8 x i16> %473, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %476 = sub <8 x i16> zeroinitializer, %472
  %477 = icmp slt <8 x i16> %472, zeroinitializer
  %478 = select <8 x i1> %477, <8 x i16> %476, <8 x i16> %472
  %479 = sub <8 x i16> zeroinitializer, %473
  %480 = icmp slt <8 x i16> %473, zeroinitializer
  %481 = select <8 x i1> %480, <8 x i16> %479, <8 x i16> %473
  %482 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %478, <8 x i16> %69) #8
  %483 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %481, <8 x i16> %69) #8
  %484 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %482) #8
  %485 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %483) #8
  %486 = icmp slt <8 x i16> %478, %484
  %487 = select <8 x i1> %486, <8 x i16> %478, <8 x i16> %484
  %488 = icmp slt <8 x i16> %481, %485
  %489 = select <8 x i1> %488, <8 x i16> %481, <8 x i16> %485
  %490 = add <8 x i16> %487, %474
  %491 = add <8 x i16> %489, %475
  %492 = xor <8 x i16> %490, %474
  %493 = xor <8 x i16> %491, %475
  %494 = sub <8 x i16> %398, %90
  %495 = sub <8 x i16> %401, %95
  %496 = ashr <8 x i16> %494, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %497 = ashr <8 x i16> %495, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %498 = sub <8 x i16> zeroinitializer, %494
  %499 = icmp slt <8 x i16> %494, zeroinitializer
  %500 = select <8 x i1> %499, <8 x i16> %498, <8 x i16> %494
  %501 = sub <8 x i16> zeroinitializer, %495
  %502 = icmp slt <8 x i16> %495, zeroinitializer
  %503 = select <8 x i1> %502, <8 x i16> %501, <8 x i16> %495
  %504 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %500, <8 x i16> %69) #8
  %505 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %503, <8 x i16> %69) #8
  %506 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %504) #8
  %507 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %505) #8
  %508 = icmp slt <8 x i16> %500, %506
  %509 = select <8 x i1> %508, <8 x i16> %500, <8 x i16> %506
  %510 = icmp slt <8 x i16> %503, %507
  %511 = select <8 x i1> %510, <8 x i16> %503, <8 x i16> %507
  %512 = add <8 x i16> %509, %496
  %513 = add <8 x i16> %511, %497
  %514 = xor <8 x i16> %512, %496
  %515 = xor <8 x i16> %513, %497
  %516 = sub <8 x i16> %416, %90
  %517 = sub <8 x i16> %419, %95
  %518 = ashr <8 x i16> %516, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %519 = ashr <8 x i16> %517, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %520 = sub <8 x i16> zeroinitializer, %516
  %521 = icmp slt <8 x i16> %516, zeroinitializer
  %522 = select <8 x i1> %521, <8 x i16> %520, <8 x i16> %516
  %523 = sub <8 x i16> zeroinitializer, %517
  %524 = icmp slt <8 x i16> %517, zeroinitializer
  %525 = select <8 x i1> %524, <8 x i16> %523, <8 x i16> %517
  %526 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %522, <8 x i16> %69) #8
  %527 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %525, <8 x i16> %69) #8
  %528 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %526) #8
  %529 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %527) #8
  %530 = icmp slt <8 x i16> %522, %528
  %531 = select <8 x i1> %530, <8 x i16> %522, <8 x i16> %528
  %532 = icmp slt <8 x i16> %525, %529
  %533 = select <8 x i1> %532, <8 x i16> %525, <8 x i16> %529
  %534 = add <8 x i16> %531, %518
  %535 = add <8 x i16> %533, %519
  %536 = xor <8 x i16> %534, %518
  %537 = xor <8 x i16> %535, %519
  %538 = add <8 x i16> %492, %470
  %539 = add <8 x i16> %538, %514
  %540 = add <8 x i16> %539, %536
  %541 = add <8 x i16> %493, %471
  %542 = add <8 x i16> %541, %515
  %543 = add <8 x i16> %542, %537
  %544 = mul <8 x i16> %540, %73
  %545 = mul <8 x i16> %543, %73
  %546 = add <8 x i16> %320, %544
  %547 = add <8 x i16> %321, %545
  %548 = add nsw i64 %87, %83
  %549 = getelementptr inbounds i16, i16* %2, i64 %548
  %550 = bitcast i16* %549 to i8*
  %551 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %550) #8
  %552 = bitcast <16 x i8> %551 to <2 x i64>
  %553 = add nsw i64 %92, %83
  %554 = getelementptr inbounds i16, i16* %2, i64 %553
  %555 = bitcast i16* %554 to i8*
  %556 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %555) #8
  %557 = bitcast <16 x i8> %556 to <2 x i64>
  %558 = sub nsw i64 %87, %83
  %559 = getelementptr inbounds i16, i16* %2, i64 %558
  %560 = bitcast i16* %559 to i8*
  %561 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %560) #8
  %562 = bitcast <16 x i8> %561 to <2 x i64>
  %563 = sub nsw i64 %92, %83
  %564 = getelementptr inbounds i16, i16* %2, i64 %563
  %565 = bitcast i16* %564 to i8*
  %566 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %565) #8
  %567 = bitcast <16 x i8> %566 to <2 x i64>
  %568 = add nsw i64 %87, %84
  %569 = getelementptr inbounds i16, i16* %2, i64 %568
  %570 = bitcast i16* %569 to i8*
  %571 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %570) #8
  %572 = bitcast <16 x i8> %571 to <2 x i64>
  %573 = add nsw i64 %92, %84
  %574 = getelementptr inbounds i16, i16* %2, i64 %573
  %575 = bitcast i16* %574 to i8*
  %576 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %575) #8
  %577 = bitcast <16 x i8> %576 to <2 x i64>
  %578 = sub nsw i64 %87, %84
  %579 = getelementptr inbounds i16, i16* %2, i64 %578
  %580 = bitcast i16* %579 to i8*
  %581 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %580) #8
  %582 = bitcast <16 x i8> %581 to <2 x i64>
  %583 = sub nsw i64 %92, %84
  %584 = getelementptr inbounds i16, i16* %2, i64 %583
  %585 = bitcast i16* %584 to i8*
  %586 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %585) #8
  %587 = bitcast <16 x i8> %586 to <2 x i64>
  %588 = bitcast <16 x i8> %551 to <8 x i16>
  %589 = icmp eq <8 x i16> %588, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %590 = sext <8 x i1> %589 to <8 x i16>
  %591 = bitcast <16 x i8> %556 to <8 x i16>
  %592 = icmp eq <8 x i16> %591, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %593 = sext <8 x i1> %592 to <8 x i16>
  %594 = bitcast <8 x i16> %593 to <2 x i64>
  %595 = bitcast <8 x i16> %590 to <2 x i64>
  %596 = xor <2 x i64> %595, <i64 -1, i64 -1>
  %597 = and <2 x i64> %596, %552
  %598 = xor <2 x i64> %594, <i64 -1, i64 -1>
  %599 = and <2 x i64> %598, %557
  %600 = bitcast <2 x i64> %597 to <8 x i16>
  %601 = icmp sgt <8 x i16> %430, %600
  %602 = select <8 x i1> %601, <8 x i16> %430, <8 x i16> %600
  %603 = bitcast <2 x i64> %599 to <8 x i16>
  %604 = icmp sgt <8 x i16> %433, %603
  %605 = select <8 x i1> %604, <8 x i16> %433, <8 x i16> %603
  %606 = bitcast <16 x i8> %561 to <8 x i16>
  %607 = icmp eq <8 x i16> %606, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %608 = sext <8 x i1> %607 to <8 x i16>
  %609 = bitcast <16 x i8> %566 to <8 x i16>
  %610 = icmp eq <8 x i16> %609, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %611 = sext <8 x i1> %610 to <8 x i16>
  %612 = bitcast <8 x i16> %611 to <2 x i64>
  %613 = bitcast <8 x i16> %608 to <2 x i64>
  %614 = xor <2 x i64> %613, <i64 -1, i64 -1>
  %615 = and <2 x i64> %614, %562
  %616 = xor <2 x i64> %612, <i64 -1, i64 -1>
  %617 = and <2 x i64> %616, %567
  %618 = bitcast <2 x i64> %615 to <8 x i16>
  %619 = icmp sgt <8 x i16> %602, %618
  %620 = select <8 x i1> %619, <8 x i16> %602, <8 x i16> %618
  %621 = bitcast <2 x i64> %617 to <8 x i16>
  %622 = icmp sgt <8 x i16> %605, %621
  %623 = select <8 x i1> %622, <8 x i16> %605, <8 x i16> %621
  %624 = bitcast <16 x i8> %571 to <8 x i16>
  %625 = icmp eq <8 x i16> %624, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %626 = sext <8 x i1> %625 to <8 x i16>
  %627 = bitcast <16 x i8> %576 to <8 x i16>
  %628 = icmp eq <8 x i16> %627, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %629 = sext <8 x i1> %628 to <8 x i16>
  %630 = bitcast <8 x i16> %629 to <2 x i64>
  %631 = bitcast <8 x i16> %626 to <2 x i64>
  %632 = xor <2 x i64> %631, <i64 -1, i64 -1>
  %633 = and <2 x i64> %632, %572
  %634 = xor <2 x i64> %630, <i64 -1, i64 -1>
  %635 = and <2 x i64> %634, %577
  %636 = bitcast <2 x i64> %633 to <8 x i16>
  %637 = icmp sgt <8 x i16> %620, %636
  %638 = select <8 x i1> %637, <8 x i16> %620, <8 x i16> %636
  %639 = bitcast <2 x i64> %635 to <8 x i16>
  %640 = icmp sgt <8 x i16> %623, %639
  %641 = select <8 x i1> %640, <8 x i16> %623, <8 x i16> %639
  %642 = bitcast <16 x i8> %581 to <8 x i16>
  %643 = icmp eq <8 x i16> %642, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %644 = sext <8 x i1> %643 to <8 x i16>
  %645 = bitcast <16 x i8> %586 to <8 x i16>
  %646 = icmp eq <8 x i16> %645, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %647 = sext <8 x i1> %646 to <8 x i16>
  %648 = bitcast <8 x i16> %647 to <2 x i64>
  %649 = bitcast <8 x i16> %644 to <2 x i64>
  %650 = xor <2 x i64> %649, <i64 -1, i64 -1>
  %651 = and <2 x i64> %650, %582
  %652 = xor <2 x i64> %648, <i64 -1, i64 -1>
  %653 = and <2 x i64> %652, %587
  %654 = bitcast <2 x i64> %651 to <8 x i16>
  %655 = icmp sgt <8 x i16> %638, %654
  %656 = select <8 x i1> %655, <8 x i16> %638, <8 x i16> %654
  %657 = bitcast <2 x i64> %653 to <8 x i16>
  %658 = icmp sgt <8 x i16> %641, %657
  %659 = select <8 x i1> %658, <8 x i16> %641, <8 x i16> %657
  %660 = icmp slt <8 x i16> %447, %588
  %661 = select <8 x i1> %660, <8 x i16> %447, <8 x i16> %588
  %662 = icmp slt <8 x i16> %449, %591
  %663 = select <8 x i1> %662, <8 x i16> %449, <8 x i16> %591
  %664 = icmp slt <8 x i16> %661, %606
  %665 = select <8 x i1> %664, <8 x i16> %661, <8 x i16> %606
  %666 = icmp slt <8 x i16> %663, %609
  %667 = select <8 x i1> %666, <8 x i16> %663, <8 x i16> %609
  %668 = icmp slt <8 x i16> %665, %624
  %669 = select <8 x i1> %668, <8 x i16> %665, <8 x i16> %624
  %670 = icmp slt <8 x i16> %667, %627
  %671 = select <8 x i1> %670, <8 x i16> %667, <8 x i16> %627
  %672 = icmp slt <8 x i16> %669, %642
  %673 = select <8 x i1> %672, <8 x i16> %669, <8 x i16> %642
  %674 = icmp slt <8 x i16> %671, %645
  %675 = select <8 x i1> %674, <8 x i16> %671, <8 x i16> %645
  %676 = sub <8 x i16> %588, %90
  %677 = sub <8 x i16> %591, %95
  %678 = ashr <8 x i16> %676, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %679 = ashr <8 x i16> %677, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %680 = sub <8 x i16> zeroinitializer, %676
  %681 = icmp slt <8 x i16> %676, zeroinitializer
  %682 = select <8 x i1> %681, <8 x i16> %680, <8 x i16> %676
  %683 = sub <8 x i16> zeroinitializer, %677
  %684 = icmp slt <8 x i16> %677, zeroinitializer
  %685 = select <8 x i1> %684, <8 x i16> %683, <8 x i16> %677
  %686 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %682, <8 x i16> %69) #8
  %687 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %685, <8 x i16> %69) #8
  %688 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %686) #8
  %689 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %687) #8
  %690 = icmp slt <8 x i16> %682, %688
  %691 = select <8 x i1> %690, <8 x i16> %682, <8 x i16> %688
  %692 = icmp slt <8 x i16> %685, %689
  %693 = select <8 x i1> %692, <8 x i16> %685, <8 x i16> %689
  %694 = add <8 x i16> %691, %678
  %695 = add <8 x i16> %693, %679
  %696 = xor <8 x i16> %694, %678
  %697 = xor <8 x i16> %695, %679
  %698 = sub <8 x i16> %606, %90
  %699 = sub <8 x i16> %609, %95
  %700 = ashr <8 x i16> %698, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %701 = ashr <8 x i16> %699, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %702 = sub <8 x i16> zeroinitializer, %698
  %703 = icmp slt <8 x i16> %698, zeroinitializer
  %704 = select <8 x i1> %703, <8 x i16> %702, <8 x i16> %698
  %705 = sub <8 x i16> zeroinitializer, %699
  %706 = icmp slt <8 x i16> %699, zeroinitializer
  %707 = select <8 x i1> %706, <8 x i16> %705, <8 x i16> %699
  %708 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %704, <8 x i16> %69) #8
  %709 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %707, <8 x i16> %69) #8
  %710 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %708) #8
  %711 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %709) #8
  %712 = icmp slt <8 x i16> %704, %710
  %713 = select <8 x i1> %712, <8 x i16> %704, <8 x i16> %710
  %714 = icmp slt <8 x i16> %707, %711
  %715 = select <8 x i1> %714, <8 x i16> %707, <8 x i16> %711
  %716 = add <8 x i16> %713, %700
  %717 = add <8 x i16> %715, %701
  %718 = xor <8 x i16> %716, %700
  %719 = xor <8 x i16> %717, %701
  %720 = sub <8 x i16> %624, %90
  %721 = sub <8 x i16> %627, %95
  %722 = ashr <8 x i16> %720, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %723 = ashr <8 x i16> %721, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %724 = sub <8 x i16> zeroinitializer, %720
  %725 = icmp slt <8 x i16> %720, zeroinitializer
  %726 = select <8 x i1> %725, <8 x i16> %724, <8 x i16> %720
  %727 = sub <8 x i16> zeroinitializer, %721
  %728 = icmp slt <8 x i16> %721, zeroinitializer
  %729 = select <8 x i1> %728, <8 x i16> %727, <8 x i16> %721
  %730 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %726, <8 x i16> %69) #8
  %731 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %729, <8 x i16> %69) #8
  %732 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %730) #8
  %733 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %731) #8
  %734 = icmp slt <8 x i16> %726, %732
  %735 = select <8 x i1> %734, <8 x i16> %726, <8 x i16> %732
  %736 = icmp slt <8 x i16> %729, %733
  %737 = select <8 x i1> %736, <8 x i16> %729, <8 x i16> %733
  %738 = add <8 x i16> %735, %722
  %739 = add <8 x i16> %737, %723
  %740 = xor <8 x i16> %738, %722
  %741 = xor <8 x i16> %739, %723
  %742 = sub <8 x i16> %642, %90
  %743 = sub <8 x i16> %645, %95
  %744 = ashr <8 x i16> %742, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %745 = ashr <8 x i16> %743, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %746 = sub <8 x i16> zeroinitializer, %742
  %747 = icmp slt <8 x i16> %742, zeroinitializer
  %748 = select <8 x i1> %747, <8 x i16> %746, <8 x i16> %742
  %749 = sub <8 x i16> zeroinitializer, %743
  %750 = icmp slt <8 x i16> %743, zeroinitializer
  %751 = select <8 x i1> %750, <8 x i16> %749, <8 x i16> %743
  %752 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %748, <8 x i16> %69) #8
  %753 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %751, <8 x i16> %69) #8
  %754 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %752) #8
  %755 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %753) #8
  %756 = icmp slt <8 x i16> %748, %754
  %757 = select <8 x i1> %756, <8 x i16> %748, <8 x i16> %754
  %758 = icmp slt <8 x i16> %751, %755
  %759 = select <8 x i1> %758, <8 x i16> %751, <8 x i16> %755
  %760 = add <8 x i16> %757, %744
  %761 = add <8 x i16> %759, %745
  %762 = xor <8 x i16> %760, %744
  %763 = xor <8 x i16> %761, %745
  %764 = add <8 x i16> %718, %696
  %765 = add <8 x i16> %764, %740
  %766 = add <8 x i16> %765, %762
  %767 = add <8 x i16> %719, %697
  %768 = add <8 x i16> %767, %741
  %769 = add <8 x i16> %768, %763
  %770 = mul <8 x i16> %766, %77
  %771 = mul <8 x i16> %769, %77
  %772 = add <8 x i16> %546, %770
  %773 = add <8 x i16> %547, %771
  %774 = ashr <8 x i16> %772, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %775 = ashr <8 x i16> %773, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %776 = add <8 x i16> %772, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %777 = add <8 x i16> %776, %774
  %778 = add <8 x i16> %773, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %779 = add <8 x i16> %778, %775
  %780 = ashr <8 x i16> %777, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %781 = ashr <8 x i16> %779, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %782 = add <8 x i16> %780, %90
  %783 = add <8 x i16> %781, %95
  %784 = icmp sgt <8 x i16> %782, %673
  %785 = select <8 x i1> %784, <8 x i16> %782, <8 x i16> %673
  %786 = icmp sgt <8 x i16> %783, %675
  %787 = select <8 x i1> %786, <8 x i16> %783, <8 x i16> %675
  %788 = icmp slt <8 x i16> %785, %656
  %789 = select <8 x i1> %788, <8 x i16> %785, <8 x i16> %656
  %790 = icmp slt <8 x i16> %787, %659
  %791 = select <8 x i1> %790, <8 x i16> %787, <8 x i16> %659
  %792 = mul nsw i64 %86, %78
  %793 = getelementptr inbounds i16, i16* %0, i64 %792
  %794 = bitcast i16* %793 to <8 x i16>*
  store <8 x i16> %789, <8 x i16>* %794, align 1
  %795 = mul nsw i64 %91, %78
  %796 = getelementptr inbounds i16, i16* %0, i64 %795
  %797 = bitcast i16* %796 to <8 x i16>*
  store <8 x i16> %791, <8 x i16>* %797, align 1
  %798 = add nuw nsw i64 %86, 2
  %799 = icmp ult i64 %798, 8
  br i1 %799, label %85, label %800

800:                                              ; preds = %85
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @cdef_filter_block_ssse3(i8*, i16* nocapture, i32, i16*, i32, i32, i32, i32, i32, i32, i32) local_unnamed_addr #2 {
  %12 = icmp eq i8* %0, null
  %13 = icmp eq i32 %9, 3
  br i1 %12, label %28, label %14

14:                                               ; preds = %11
  br i1 %13, label %15, label %16

15:                                               ; preds = %14
  tail call void @cdef_filter_block_8x8_8_ssse3(i8* nonnull %0, i32 %2, i16* %3, i32 %4, i32 %5, i32 %6, i32 %7, i32 %8, i32 %10)
  br label %42

16:                                               ; preds = %14
  %17 = icmp eq i32 %9, 1
  br i1 %17, label %18, label %23

18:                                               ; preds = %16
  tail call void @cdef_filter_block_4x4_8_ssse3(i8* nonnull %0, i32 %2, i16* %3, i32 %4, i32 %5, i32 %6, i32 %7, i32 %8, i32 %10)
  %19 = shl nsw i32 %2, 2
  %20 = sext i32 %19 to i64
  %21 = getelementptr inbounds i8, i8* %0, i64 %20
  %22 = getelementptr inbounds i16, i16* %3, i64 576
  tail call void @cdef_filter_block_4x4_8_ssse3(i8* %21, i32 %2, i16* %22, i32 %4, i32 %5, i32 %6, i32 %7, i32 %8, i32 %10)
  br label %42

23:                                               ; preds = %16
  %24 = icmp eq i32 %9, 2
  tail call void @cdef_filter_block_4x4_8_ssse3(i8* nonnull %0, i32 %2, i16* %3, i32 %4, i32 %5, i32 %6, i32 %7, i32 %8, i32 %10)
  br i1 %24, label %25, label %42

25:                                               ; preds = %23
  %26 = getelementptr inbounds i8, i8* %0, i64 4
  %27 = getelementptr inbounds i16, i16* %3, i64 4
  tail call void @cdef_filter_block_4x4_8_ssse3(i8* %26, i32 %2, i16* %27, i32 %4, i32 %5, i32 %6, i32 %7, i32 %8, i32 %10)
  br label %42

28:                                               ; preds = %11
  br i1 %13, label %29, label %30

29:                                               ; preds = %28
  tail call void @cdef_filter_block_8x8_16_ssse3(i16* %1, i32 %2, i16* %3, i32 %4, i32 %5, i32 %6, i32 %7, i32 %8, i32 %10)
  br label %42

30:                                               ; preds = %28
  %31 = icmp eq i32 %9, 1
  br i1 %31, label %32, label %37

32:                                               ; preds = %30
  tail call void @cdef_filter_block_4x4_16_ssse3(i16* %1, i32 %2, i16* %3, i32 %4, i32 %5, i32 %6, i32 %7, i32 %8, i32 %10)
  %33 = shl nsw i32 %2, 2
  %34 = sext i32 %33 to i64
  %35 = getelementptr inbounds i16, i16* %1, i64 %34
  %36 = getelementptr inbounds i16, i16* %3, i64 576
  tail call void @cdef_filter_block_4x4_16_ssse3(i16* %35, i32 %2, i16* %36, i32 %4, i32 %5, i32 %6, i32 %7, i32 %8, i32 %10)
  br label %42

37:                                               ; preds = %30
  %38 = icmp eq i32 %9, 2
  tail call void @cdef_filter_block_4x4_16_ssse3(i16* %1, i32 %2, i16* %3, i32 %4, i32 %5, i32 %6, i32 %7, i32 %8, i32 %10)
  br i1 %38, label %39, label %42

39:                                               ; preds = %37
  %40 = getelementptr inbounds i16, i16* %1, i64 4
  %41 = getelementptr inbounds i16, i16* %3, i64 4
  tail call void @cdef_filter_block_4x4_16_ssse3(i16* %40, i32 %2, i16* %41, i32 %4, i32 %5, i32 %6, i32 %7, i32 %8, i32 %10)
  br label %42

42:                                               ; preds = %29, %39, %37, %32, %15, %25, %23, %18
  ret void
}

; Function Attrs: nofree norecurse nounwind ssp uwtable
define hidden void @cdef_copy_rect8_8bit_to_16bit_ssse3(i16* nocapture, i32, i8* nocapture readonly, i32, i32, i32) local_unnamed_addr #3 {
  %7 = icmp sgt i32 %4, 0
  br i1 %7, label %8, label %222

8:                                                ; preds = %6
  %9 = and i32 %5, -8
  %10 = icmp sgt i32 %9, 0
  %11 = sext i32 %9 to i64
  %12 = sext i32 %1 to i64
  %13 = sext i32 %3 to i64
  %14 = zext i32 %4 to i64
  %15 = zext i32 %5 to i64
  %16 = add nsw i64 %11, -1
  %17 = lshr i64 %16, 3
  %18 = add nuw nsw i64 %17, 1
  %19 = and i64 %18, 1
  %20 = icmp eq i64 %17, 0
  %21 = sub nuw nsw i64 %18, %19
  %22 = icmp eq i64 %19, 0
  br label %23

23:                                               ; preds = %219, %8
  %24 = phi i64 [ 0, %8 ], [ %220, %219 ]
  %25 = mul i64 %24, %12
  %26 = add i64 %25, %15
  %27 = getelementptr i16, i16* %0, i64 %26
  %28 = bitcast i16* %27 to i8*
  %29 = mul i64 %24, %13
  %30 = getelementptr i8, i8* %2, i64 %29
  %31 = add i64 %29, %15
  %32 = getelementptr i8, i8* %2, i64 %31
  br i1 %10, label %33, label %54

33:                                               ; preds = %23
  %34 = mul nsw i64 %24, %13
  %35 = mul nsw i64 %24, %12
  br i1 %20, label %36, label %161

36:                                               ; preds = %161, %33
  %37 = phi i64 [ undef, %33 ], [ %185, %161 ]
  %38 = phi i64 [ 0, %33 ], [ %185, %161 ]
  br i1 %22, label %51, label %39

39:                                               ; preds = %36
  %40 = add nsw i64 %38, %34
  %41 = getelementptr inbounds i8, i8* %2, i64 %40
  %42 = bitcast i8* %41 to i64*
  %43 = load i64, i64* %42, align 1
  %44 = insertelement <2 x i64> undef, i64 %43, i32 0
  %45 = add nsw i64 %38, %35
  %46 = getelementptr inbounds i16, i16* %0, i64 %45
  %47 = bitcast <2 x i64> %44 to <16 x i8>
  %48 = shufflevector <16 x i8> %47, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %49 = bitcast i16* %46 to <16 x i8>*
  store <16 x i8> %48, <16 x i8>* %49, align 1
  %50 = add nuw nsw i64 %38, 8
  br label %51

51:                                               ; preds = %36, %39
  %52 = phi i64 [ %37, %36 ], [ %50, %39 ]
  %53 = trunc i64 %52 to i32
  br label %54

54:                                               ; preds = %51, %23
  %55 = phi i32 [ 0, %23 ], [ %53, %51 ]
  %56 = icmp slt i32 %55, %5
  br i1 %56, label %57, label %219

57:                                               ; preds = %54
  %58 = mul nsw i64 %24, %13
  %59 = mul nsw i64 %24, %12
  %60 = zext i32 %55 to i64
  %61 = sub nsw i64 %15, %60
  %62 = icmp ult i64 %61, 16
  br i1 %62, label %63, label %85

63:                                               ; preds = %159, %85, %57
  %64 = phi i64 [ %60, %85 ], [ %60, %57 ], [ %95, %159 ]
  %65 = sub nsw i64 %15, %64
  %66 = xor i64 %64, -1
  %67 = add nsw i64 %66, %15
  %68 = and i64 %65, 3
  %69 = icmp eq i64 %68, 0
  br i1 %69, label %82, label %70

70:                                               ; preds = %63, %70
  %71 = phi i64 [ %79, %70 ], [ %64, %63 ]
  %72 = phi i64 [ %80, %70 ], [ %68, %63 ]
  %73 = add nsw i64 %71, %58
  %74 = getelementptr inbounds i8, i8* %2, i64 %73
  %75 = load i8, i8* %74, align 1
  %76 = zext i8 %75 to i16
  %77 = add nsw i64 %71, %59
  %78 = getelementptr inbounds i16, i16* %0, i64 %77
  store i16 %76, i16* %78, align 2
  %79 = add nuw nsw i64 %71, 1
  %80 = add i64 %72, -1
  %81 = icmp eq i64 %80, 0
  br i1 %81, label %82, label %70, !llvm.loop !3

82:                                               ; preds = %70, %63
  %83 = phi i64 [ %64, %63 ], [ %79, %70 ]
  %84 = icmp ult i64 %67, 3
  br i1 %84, label %219, label %188

85:                                               ; preds = %57
  %86 = add nsw i64 %25, %60
  %87 = getelementptr i16, i16* %0, i64 %86
  %88 = bitcast i16* %87 to i8*
  %89 = getelementptr i8, i8* %30, i64 %60
  %90 = icmp ugt i8* %32, %88
  %91 = icmp ult i8* %89, %28
  %92 = and i1 %90, %91
  br i1 %92, label %63, label %93

93:                                               ; preds = %85
  %94 = and i64 %61, -16
  %95 = add nsw i64 %94, %60
  %96 = add nsw i64 %94, -16
  %97 = lshr exact i64 %96, 4
  %98 = add nuw nsw i64 %97, 1
  %99 = and i64 %98, 1
  %100 = icmp eq i64 %96, 0
  br i1 %100, label %140, label %101

101:                                              ; preds = %93
  %102 = sub nuw nsw i64 %98, %99
  br label %103

103:                                              ; preds = %103, %101
  %104 = phi i64 [ 0, %101 ], [ %137, %103 ]
  %105 = phi i64 [ %102, %101 ], [ %138, %103 ]
  %106 = add i64 %104, %60
  %107 = add nsw i64 %106, %58
  %108 = getelementptr inbounds i8, i8* %2, i64 %107
  %109 = bitcast i8* %108 to <8 x i8>*
  %110 = load <8 x i8>, <8 x i8>* %109, align 1, !alias.scope !5
  %111 = getelementptr inbounds i8, i8* %108, i64 8
  %112 = bitcast i8* %111 to <8 x i8>*
  %113 = load <8 x i8>, <8 x i8>* %112, align 1, !alias.scope !5
  %114 = zext <8 x i8> %110 to <8 x i16>
  %115 = zext <8 x i8> %113 to <8 x i16>
  %116 = add nsw i64 %106, %59
  %117 = getelementptr inbounds i16, i16* %0, i64 %116
  %118 = bitcast i16* %117 to <8 x i16>*
  store <8 x i16> %114, <8 x i16>* %118, align 2, !alias.scope !8, !noalias !5
  %119 = getelementptr inbounds i16, i16* %117, i64 8
  %120 = bitcast i16* %119 to <8 x i16>*
  store <8 x i16> %115, <8 x i16>* %120, align 2, !alias.scope !8, !noalias !5
  %121 = or i64 %104, 16
  %122 = add i64 %121, %60
  %123 = add nsw i64 %122, %58
  %124 = getelementptr inbounds i8, i8* %2, i64 %123
  %125 = bitcast i8* %124 to <8 x i8>*
  %126 = load <8 x i8>, <8 x i8>* %125, align 1, !alias.scope !5
  %127 = getelementptr inbounds i8, i8* %124, i64 8
  %128 = bitcast i8* %127 to <8 x i8>*
  %129 = load <8 x i8>, <8 x i8>* %128, align 1, !alias.scope !5
  %130 = zext <8 x i8> %126 to <8 x i16>
  %131 = zext <8 x i8> %129 to <8 x i16>
  %132 = add nsw i64 %122, %59
  %133 = getelementptr inbounds i16, i16* %0, i64 %132
  %134 = bitcast i16* %133 to <8 x i16>*
  store <8 x i16> %130, <8 x i16>* %134, align 2, !alias.scope !8, !noalias !5
  %135 = getelementptr inbounds i16, i16* %133, i64 8
  %136 = bitcast i16* %135 to <8 x i16>*
  store <8 x i16> %131, <8 x i16>* %136, align 2, !alias.scope !8, !noalias !5
  %137 = add i64 %104, 32
  %138 = add i64 %105, -2
  %139 = icmp eq i64 %138, 0
  br i1 %139, label %140, label %103, !llvm.loop !10

140:                                              ; preds = %103, %93
  %141 = phi i64 [ 0, %93 ], [ %137, %103 ]
  %142 = icmp eq i64 %99, 0
  br i1 %142, label %159, label %143

143:                                              ; preds = %140
  %144 = add i64 %141, %60
  %145 = add nsw i64 %144, %58
  %146 = getelementptr inbounds i8, i8* %2, i64 %145
  %147 = bitcast i8* %146 to <8 x i8>*
  %148 = load <8 x i8>, <8 x i8>* %147, align 1, !alias.scope !5
  %149 = getelementptr inbounds i8, i8* %146, i64 8
  %150 = bitcast i8* %149 to <8 x i8>*
  %151 = load <8 x i8>, <8 x i8>* %150, align 1, !alias.scope !5
  %152 = zext <8 x i8> %148 to <8 x i16>
  %153 = zext <8 x i8> %151 to <8 x i16>
  %154 = add nsw i64 %144, %59
  %155 = getelementptr inbounds i16, i16* %0, i64 %154
  %156 = bitcast i16* %155 to <8 x i16>*
  store <8 x i16> %152, <8 x i16>* %156, align 2, !alias.scope !8, !noalias !5
  %157 = getelementptr inbounds i16, i16* %155, i64 8
  %158 = bitcast i16* %157 to <8 x i16>*
  store <8 x i16> %153, <8 x i16>* %158, align 2, !alias.scope !8, !noalias !5
  br label %159

159:                                              ; preds = %140, %143
  %160 = icmp eq i64 %61, %94
  br i1 %160, label %219, label %63

161:                                              ; preds = %33, %161
  %162 = phi i64 [ %185, %161 ], [ 0, %33 ]
  %163 = phi i64 [ %186, %161 ], [ %21, %33 ]
  %164 = add nsw i64 %162, %34
  %165 = getelementptr inbounds i8, i8* %2, i64 %164
  %166 = bitcast i8* %165 to i64*
  %167 = load i64, i64* %166, align 1
  %168 = insertelement <2 x i64> undef, i64 %167, i32 0
  %169 = add nsw i64 %162, %35
  %170 = getelementptr inbounds i16, i16* %0, i64 %169
  %171 = bitcast <2 x i64> %168 to <16 x i8>
  %172 = shufflevector <16 x i8> %171, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %173 = bitcast i16* %170 to <16 x i8>*
  store <16 x i8> %172, <16 x i8>* %173, align 1
  %174 = or i64 %162, 8
  %175 = add nsw i64 %174, %34
  %176 = getelementptr inbounds i8, i8* %2, i64 %175
  %177 = bitcast i8* %176 to i64*
  %178 = load i64, i64* %177, align 1
  %179 = insertelement <2 x i64> undef, i64 %178, i32 0
  %180 = add nsw i64 %174, %35
  %181 = getelementptr inbounds i16, i16* %0, i64 %180
  %182 = bitcast <2 x i64> %179 to <16 x i8>
  %183 = shufflevector <16 x i8> %182, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %184 = bitcast i16* %181 to <16 x i8>*
  store <16 x i8> %183, <16 x i8>* %184, align 1
  %185 = add nuw nsw i64 %162, 16
  %186 = add i64 %163, -2
  %187 = icmp eq i64 %186, 0
  br i1 %187, label %36, label %161

188:                                              ; preds = %82, %188
  %189 = phi i64 [ %217, %188 ], [ %83, %82 ]
  %190 = add nsw i64 %189, %58
  %191 = getelementptr inbounds i8, i8* %2, i64 %190
  %192 = load i8, i8* %191, align 1
  %193 = zext i8 %192 to i16
  %194 = add nsw i64 %189, %59
  %195 = getelementptr inbounds i16, i16* %0, i64 %194
  store i16 %193, i16* %195, align 2
  %196 = add nuw nsw i64 %189, 1
  %197 = add nsw i64 %196, %58
  %198 = getelementptr inbounds i8, i8* %2, i64 %197
  %199 = load i8, i8* %198, align 1
  %200 = zext i8 %199 to i16
  %201 = add nsw i64 %196, %59
  %202 = getelementptr inbounds i16, i16* %0, i64 %201
  store i16 %200, i16* %202, align 2
  %203 = add nuw nsw i64 %189, 2
  %204 = add nsw i64 %203, %58
  %205 = getelementptr inbounds i8, i8* %2, i64 %204
  %206 = load i8, i8* %205, align 1
  %207 = zext i8 %206 to i16
  %208 = add nsw i64 %203, %59
  %209 = getelementptr inbounds i16, i16* %0, i64 %208
  store i16 %207, i16* %209, align 2
  %210 = add nuw nsw i64 %189, 3
  %211 = add nsw i64 %210, %58
  %212 = getelementptr inbounds i8, i8* %2, i64 %211
  %213 = load i8, i8* %212, align 1
  %214 = zext i8 %213 to i16
  %215 = add nsw i64 %210, %59
  %216 = getelementptr inbounds i16, i16* %0, i64 %215
  store i16 %214, i16* %216, align 2
  %217 = add nuw nsw i64 %189, 4
  %218 = icmp eq i64 %217, %15
  br i1 %218, label %219, label %188, !llvm.loop !12

219:                                              ; preds = %82, %188, %159, %54
  %220 = add nuw nsw i64 %24, 1
  %221 = icmp eq i64 %220, %14
  br i1 %221, label %222, label %23

222:                                              ; preds = %219, %6
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define hidden void @cdef_copy_rect8_16bit_to_16bit_ssse3(i16* nocapture, i32, i16*, i32, i32, i32) local_unnamed_addr #4 {
  %7 = icmp sgt i32 %4, 0
  br i1 %7, label %8, label %220

8:                                                ; preds = %6
  %9 = and i32 %5, -8
  %10 = icmp sgt i32 %9, 0
  %11 = sext i32 %9 to i64
  %12 = sext i32 %1 to i64
  %13 = sext i32 %3 to i64
  %14 = zext i32 %4 to i64
  %15 = zext i32 %5 to i64
  %16 = add nsw i64 %11, -1
  %17 = lshr i64 %16, 3
  %18 = add nuw nsw i64 %17, 1
  %19 = and i64 %18, 3
  %20 = icmp ult i64 %16, 24
  %21 = sub nsw i64 %18, %19
  %22 = icmp eq i64 %19, 0
  br label %23

23:                                               ; preds = %217, %8
  %24 = phi i64 [ 0, %8 ], [ %218, %217 ]
  %25 = mul i64 %24, %12
  %26 = add i64 %25, %15
  %27 = getelementptr i16, i16* %0, i64 %26
  %28 = mul i64 %24, %13
  %29 = add i64 %28, %15
  %30 = getelementptr i16, i16* %2, i64 %29
  br i1 %10, label %31, label %53

31:                                               ; preds = %23
  %32 = mul nsw i64 %24, %13
  %33 = mul nsw i64 %24, %12
  br i1 %20, label %34, label %153

34:                                               ; preds = %153, %31
  %35 = phi i64 [ undef, %31 ], [ %187, %153 ]
  %36 = phi i64 [ 0, %31 ], [ %187, %153 ]
  br i1 %22, label %50, label %37

37:                                               ; preds = %34, %37
  %38 = phi i64 [ %47, %37 ], [ %36, %34 ]
  %39 = phi i64 [ %48, %37 ], [ %19, %34 ]
  %40 = add nsw i64 %38, %32
  %41 = getelementptr inbounds i16, i16* %2, i64 %40
  %42 = bitcast i16* %41 to i8*
  %43 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %42) #8
  %44 = add nsw i64 %38, %33
  %45 = getelementptr inbounds i16, i16* %0, i64 %44
  %46 = bitcast i16* %45 to <16 x i8>*
  store <16 x i8> %43, <16 x i8>* %46, align 1
  %47 = add nuw nsw i64 %38, 8
  %48 = add i64 %39, -1
  %49 = icmp eq i64 %48, 0
  br i1 %49, label %50, label %37, !llvm.loop !13

50:                                               ; preds = %37, %34
  %51 = phi i64 [ %35, %34 ], [ %47, %37 ]
  %52 = trunc i64 %51 to i32
  br label %53

53:                                               ; preds = %50, %23
  %54 = phi i32 [ 0, %23 ], [ %52, %50 ]
  %55 = icmp slt i32 %54, %5
  br i1 %55, label %56, label %217

56:                                               ; preds = %53
  %57 = mul nsw i64 %24, %13
  %58 = mul nsw i64 %24, %12
  %59 = zext i32 %54 to i64
  %60 = sub nsw i64 %15, %59
  %61 = icmp ult i64 %60, 16
  br i1 %61, label %62, label %83

62:                                               ; preds = %151, %83, %56
  %63 = phi i64 [ %59, %83 ], [ %59, %56 ], [ %93, %151 ]
  %64 = sub nsw i64 %15, %63
  %65 = xor i64 %63, -1
  %66 = add nsw i64 %65, %15
  %67 = and i64 %64, 3
  %68 = icmp eq i64 %67, 0
  br i1 %68, label %80, label %69

69:                                               ; preds = %62, %69
  %70 = phi i64 [ %77, %69 ], [ %63, %62 ]
  %71 = phi i64 [ %78, %69 ], [ %67, %62 ]
  %72 = add nsw i64 %70, %57
  %73 = getelementptr inbounds i16, i16* %2, i64 %72
  %74 = load i16, i16* %73, align 2
  %75 = add nsw i64 %70, %58
  %76 = getelementptr inbounds i16, i16* %0, i64 %75
  store i16 %74, i16* %76, align 2
  %77 = add nuw nsw i64 %70, 1
  %78 = add i64 %71, -1
  %79 = icmp eq i64 %78, 0
  br i1 %79, label %80, label %69, !llvm.loop !14

80:                                               ; preds = %69, %62
  %81 = phi i64 [ %63, %62 ], [ %77, %69 ]
  %82 = icmp ult i64 %66, 3
  br i1 %82, label %217, label %190

83:                                               ; preds = %56
  %84 = add nsw i64 %25, %59
  %85 = getelementptr i16, i16* %0, i64 %84
  %86 = add nsw i64 %28, %59
  %87 = getelementptr i16, i16* %2, i64 %86
  %88 = icmp ult i16* %85, %30
  %89 = icmp ult i16* %87, %27
  %90 = and i1 %88, %89
  br i1 %90, label %62, label %91

91:                                               ; preds = %83
  %92 = and i64 %60, -16
  %93 = add nsw i64 %92, %59
  %94 = add nsw i64 %92, -16
  %95 = lshr exact i64 %94, 4
  %96 = add nuw nsw i64 %95, 1
  %97 = and i64 %96, 1
  %98 = icmp eq i64 %94, 0
  br i1 %98, label %134, label %99

99:                                               ; preds = %91
  %100 = sub nuw nsw i64 %96, %97
  br label %101

101:                                              ; preds = %101, %99
  %102 = phi i64 [ 0, %99 ], [ %131, %101 ]
  %103 = phi i64 [ %100, %99 ], [ %132, %101 ]
  %104 = add i64 %102, %59
  %105 = add nsw i64 %104, %57
  %106 = getelementptr inbounds i16, i16* %2, i64 %105
  %107 = bitcast i16* %106 to <8 x i16>*
  %108 = load <8 x i16>, <8 x i16>* %107, align 2, !alias.scope !15
  %109 = getelementptr inbounds i16, i16* %106, i64 8
  %110 = bitcast i16* %109 to <8 x i16>*
  %111 = load <8 x i16>, <8 x i16>* %110, align 2, !alias.scope !15
  %112 = add nsw i64 %104, %58
  %113 = getelementptr inbounds i16, i16* %0, i64 %112
  %114 = bitcast i16* %113 to <8 x i16>*
  store <8 x i16> %108, <8 x i16>* %114, align 2, !alias.scope !18, !noalias !15
  %115 = getelementptr inbounds i16, i16* %113, i64 8
  %116 = bitcast i16* %115 to <8 x i16>*
  store <8 x i16> %111, <8 x i16>* %116, align 2, !alias.scope !18, !noalias !15
  %117 = or i64 %102, 16
  %118 = add i64 %117, %59
  %119 = add nsw i64 %118, %57
  %120 = getelementptr inbounds i16, i16* %2, i64 %119
  %121 = bitcast i16* %120 to <8 x i16>*
  %122 = load <8 x i16>, <8 x i16>* %121, align 2, !alias.scope !15
  %123 = getelementptr inbounds i16, i16* %120, i64 8
  %124 = bitcast i16* %123 to <8 x i16>*
  %125 = load <8 x i16>, <8 x i16>* %124, align 2, !alias.scope !15
  %126 = add nsw i64 %118, %58
  %127 = getelementptr inbounds i16, i16* %0, i64 %126
  %128 = bitcast i16* %127 to <8 x i16>*
  store <8 x i16> %122, <8 x i16>* %128, align 2, !alias.scope !18, !noalias !15
  %129 = getelementptr inbounds i16, i16* %127, i64 8
  %130 = bitcast i16* %129 to <8 x i16>*
  store <8 x i16> %125, <8 x i16>* %130, align 2, !alias.scope !18, !noalias !15
  %131 = add i64 %102, 32
  %132 = add i64 %103, -2
  %133 = icmp eq i64 %132, 0
  br i1 %133, label %134, label %101, !llvm.loop !20

134:                                              ; preds = %101, %91
  %135 = phi i64 [ 0, %91 ], [ %131, %101 ]
  %136 = icmp eq i64 %97, 0
  br i1 %136, label %151, label %137

137:                                              ; preds = %134
  %138 = add i64 %135, %59
  %139 = add nsw i64 %138, %57
  %140 = getelementptr inbounds i16, i16* %2, i64 %139
  %141 = bitcast i16* %140 to <8 x i16>*
  %142 = load <8 x i16>, <8 x i16>* %141, align 2, !alias.scope !15
  %143 = getelementptr inbounds i16, i16* %140, i64 8
  %144 = bitcast i16* %143 to <8 x i16>*
  %145 = load <8 x i16>, <8 x i16>* %144, align 2, !alias.scope !15
  %146 = add nsw i64 %138, %58
  %147 = getelementptr inbounds i16, i16* %0, i64 %146
  %148 = bitcast i16* %147 to <8 x i16>*
  store <8 x i16> %142, <8 x i16>* %148, align 2, !alias.scope !18, !noalias !15
  %149 = getelementptr inbounds i16, i16* %147, i64 8
  %150 = bitcast i16* %149 to <8 x i16>*
  store <8 x i16> %145, <8 x i16>* %150, align 2, !alias.scope !18, !noalias !15
  br label %151

151:                                              ; preds = %134, %137
  %152 = icmp eq i64 %60, %92
  br i1 %152, label %217, label %62

153:                                              ; preds = %31, %153
  %154 = phi i64 [ %187, %153 ], [ 0, %31 ]
  %155 = phi i64 [ %188, %153 ], [ %21, %31 ]
  %156 = add nsw i64 %154, %32
  %157 = getelementptr inbounds i16, i16* %2, i64 %156
  %158 = bitcast i16* %157 to i8*
  %159 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %158) #8
  %160 = add nsw i64 %154, %33
  %161 = getelementptr inbounds i16, i16* %0, i64 %160
  %162 = bitcast i16* %161 to <16 x i8>*
  store <16 x i8> %159, <16 x i8>* %162, align 1
  %163 = or i64 %154, 8
  %164 = add nsw i64 %163, %32
  %165 = getelementptr inbounds i16, i16* %2, i64 %164
  %166 = bitcast i16* %165 to i8*
  %167 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %166) #8
  %168 = add nsw i64 %163, %33
  %169 = getelementptr inbounds i16, i16* %0, i64 %168
  %170 = bitcast i16* %169 to <16 x i8>*
  store <16 x i8> %167, <16 x i8>* %170, align 1
  %171 = or i64 %154, 16
  %172 = add nsw i64 %171, %32
  %173 = getelementptr inbounds i16, i16* %2, i64 %172
  %174 = bitcast i16* %173 to i8*
  %175 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %174) #8
  %176 = add nsw i64 %171, %33
  %177 = getelementptr inbounds i16, i16* %0, i64 %176
  %178 = bitcast i16* %177 to <16 x i8>*
  store <16 x i8> %175, <16 x i8>* %178, align 1
  %179 = or i64 %154, 24
  %180 = add nsw i64 %179, %32
  %181 = getelementptr inbounds i16, i16* %2, i64 %180
  %182 = bitcast i16* %181 to i8*
  %183 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %182) #8
  %184 = add nsw i64 %179, %33
  %185 = getelementptr inbounds i16, i16* %0, i64 %184
  %186 = bitcast i16* %185 to <16 x i8>*
  store <16 x i8> %183, <16 x i8>* %186, align 1
  %187 = add nuw nsw i64 %154, 32
  %188 = add i64 %155, -4
  %189 = icmp eq i64 %188, 0
  br i1 %189, label %34, label %153

190:                                              ; preds = %80, %190
  %191 = phi i64 [ %215, %190 ], [ %81, %80 ]
  %192 = add nsw i64 %191, %57
  %193 = getelementptr inbounds i16, i16* %2, i64 %192
  %194 = load i16, i16* %193, align 2
  %195 = add nsw i64 %191, %58
  %196 = getelementptr inbounds i16, i16* %0, i64 %195
  store i16 %194, i16* %196, align 2
  %197 = add nuw nsw i64 %191, 1
  %198 = add nsw i64 %197, %57
  %199 = getelementptr inbounds i16, i16* %2, i64 %198
  %200 = load i16, i16* %199, align 2
  %201 = add nsw i64 %197, %58
  %202 = getelementptr inbounds i16, i16* %0, i64 %201
  store i16 %200, i16* %202, align 2
  %203 = add nuw nsw i64 %191, 2
  %204 = add nsw i64 %203, %57
  %205 = getelementptr inbounds i16, i16* %2, i64 %204
  %206 = load i16, i16* %205, align 2
  %207 = add nsw i64 %203, %58
  %208 = getelementptr inbounds i16, i16* %0, i64 %207
  store i16 %206, i16* %208, align 2
  %209 = add nuw nsw i64 %191, 3
  %210 = add nsw i64 %209, %57
  %211 = getelementptr inbounds i16, i16* %2, i64 %210
  %212 = load i16, i16* %211, align 2
  %213 = add nsw i64 %209, %58
  %214 = getelementptr inbounds i16, i16* %0, i64 %213
  store i16 %212, i16* %214, align 2
  %215 = add nuw nsw i64 %191, 4
  %216 = icmp eq i64 %215, %15
  br i1 %216, label %217, label %190, !llvm.loop !21

217:                                              ; preds = %80, %190, %151, %53
  %218 = add nuw nsw i64 %24, 1
  %219 = icmp eq i64 %218, %14
  br i1 %219, label %220, label %23

220:                                              ; preds = %217, %6
  ret void
}

; Function Attrs: nounwind readonly
declare <16 x i8> @llvm.x86.sse3.ldu.dq(i8*) #5

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16>, <8 x i16>) #6

; Function Attrs: nounwind readnone
declare <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16>, <8 x i16>) #6

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32>, <4 x i32>) #6

; Function Attrs: nounwind readnone
declare <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16>, <8 x i16>) #6

; Function Attrs: nounwind readnone speculatable
declare i32 @llvm.ctlz.i32(i32, i1 immarg) #7

; Function Attrs: nounwind readnone speculatable
declare <16 x i8> @llvm.usub.sat.v16i8(<16 x i8>, <16 x i8>) #7

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16>, <8 x i16>) #6

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.ssse3.pmadd.ub.sw.128(<16 x i8>, <16 x i8>) #6

; Function Attrs: nounwind readnone
declare <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16>, <8 x i16>) #6

; Function Attrs: nounwind readnone speculatable
declare <8 x i16> @llvm.usub.sat.v8i16(<8 x i16>, <8 x i16>) #7

attributes #0 = { nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="128" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+ssse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { argmemonly nounwind }
attributes #2 = { nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+ssse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #3 = { nofree norecurse nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="128" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+ssse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #4 = { nofree nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="128" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+ssse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #5 = { nounwind readonly }
attributes #6 = { nounwind readnone }
attributes #7 = { nounwind readnone speculatable }
attributes #8 = { nounwind }

!llvm.module.flags = !{!0, !1}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 2}
!2 = !{i32 0, i32 33}
!3 = distinct !{!3, !4}
!4 = !{!"llvm.loop.unroll.disable"}
!5 = !{!6}
!6 = distinct !{!6, !7}
!7 = distinct !{!7, !"LVerDomain"}
!8 = !{!9}
!9 = distinct !{!9, !7}
!10 = distinct !{!10, !11}
!11 = !{!"llvm.loop.isvectorized", i32 1}
!12 = distinct !{!12, !11}
!13 = distinct !{!13, !4}
!14 = distinct !{!14, !4}
!15 = !{!16}
!16 = distinct !{!16, !17}
!17 = distinct !{!17, !"LVerDomain"}
!18 = !{!19}
!19 = distinct !{!19, !17}
!20 = distinct !{!20, !11}
!21 = distinct !{!21, !11}
