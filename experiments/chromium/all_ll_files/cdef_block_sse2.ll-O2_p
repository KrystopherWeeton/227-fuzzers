; ModuleID = '../../third_party/libaom/source/libaom/av1/common/cdef_block_sse2.c'
source_filename = "../../third_party/libaom/source/libaom/av1/common/cdef_block_sse2.c"
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

module asm ".symver exp, exp@GLIBC_2.2.5"
module asm ".symver exp2, exp2@GLIBC_2.2.5"
module asm ".symver exp2f, exp2f@GLIBC_2.2.5"
module asm ".symver expf, expf@GLIBC_2.2.5"
module asm ".symver lgamma, lgamma@GLIBC_2.2.5"
module asm ".symver lgammaf, lgammaf@GLIBC_2.2.5"
module asm ".symver lgammal, lgammal@GLIBC_2.2.5"
module asm ".symver log, log@GLIBC_2.2.5"
module asm ".symver log2, log2@GLIBC_2.2.5"
module asm ".symver log2f, log2f@GLIBC_2.2.5"
module asm ".symver logf, logf@GLIBC_2.2.5"
module asm ".symver pow, pow@GLIBC_2.2.5"
module asm ".symver powf, powf@GLIBC_2.2.5"

@cdef_directions = external local_unnamed_addr constant [8 x [2 x i32]], align 16
@cdef_pri_taps = external local_unnamed_addr constant [2 x [2 x i32]], align 16
@cdef_sec_taps = external local_unnamed_addr constant [2 x i32], align 4

; Function Attrs: nounwind ssp uwtable
define hidden i32 @cdef_find_dir_sse2(i16* nocapture readonly, i32, i32* nocapture, i32) local_unnamed_addr #0 {
  %5 = alloca [8 x i32], align 16
  %6 = alloca [8 x <2 x i64>], align 16
  %7 = bitcast [8 x i32]* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 32, i8* nonnull %7) #7
  %8 = getelementptr inbounds [8 x i32], [8 x i32]* %5, i64 0, i64 0
  %9 = getelementptr inbounds [8 x i32], [8 x i32]* %5, i64 0, i64 4
  %10 = bitcast [8 x <2 x i64>]* %6 to i8*
  %11 = bitcast [8 x i32]* %5 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %11, i8 -86, i64 32, i1 false)
  call void @llvm.lifetime.start.p0i8(i64 128, i8* nonnull %10) #7
  %12 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %3, i32 0
  %13 = bitcast <4 x i32> %12 to <8 x i16>
  %14 = sext i32 %1 to i64
  %15 = bitcast i16* %0 to <8 x i16>*
  %16 = load <8 x i16>, <8 x i16>* %15, align 1
  %17 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 0
  %18 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %16, <8 x i16> %13) #7
  %19 = add <8 x i16> %18, <i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128>
  %20 = bitcast [8 x <2 x i64>]* %6 to <8 x i16>*
  store <8 x i16> %19, <8 x i16>* %20, align 16
  %21 = getelementptr inbounds i16, i16* %0, i64 %14
  %22 = bitcast i16* %21 to <8 x i16>*
  %23 = load <8 x i16>, <8 x i16>* %22, align 1
  %24 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 1
  %25 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %23, <8 x i16> %13) #7
  %26 = add <8 x i16> %25, <i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128>
  %27 = bitcast <2 x i64>* %24 to <8 x i16>*
  store <8 x i16> %26, <8 x i16>* %27, align 16
  %28 = shl nsw i64 %14, 1
  %29 = getelementptr inbounds i16, i16* %0, i64 %28
  %30 = bitcast i16* %29 to <8 x i16>*
  %31 = load <8 x i16>, <8 x i16>* %30, align 1
  %32 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 2
  %33 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %31, <8 x i16> %13) #7
  %34 = add <8 x i16> %33, <i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128>
  %35 = bitcast <2 x i64>* %32 to <8 x i16>*
  store <8 x i16> %34, <8 x i16>* %35, align 16
  %36 = mul nsw i64 %14, 3
  %37 = getelementptr inbounds i16, i16* %0, i64 %36
  %38 = bitcast i16* %37 to <8 x i16>*
  %39 = load <8 x i16>, <8 x i16>* %38, align 1
  %40 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 3
  %41 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %39, <8 x i16> %13) #7
  %42 = add <8 x i16> %41, <i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128>
  %43 = bitcast <2 x i64>* %40 to <8 x i16>*
  store <8 x i16> %42, <8 x i16>* %43, align 16
  %44 = shl nsw i64 %14, 2
  %45 = getelementptr inbounds i16, i16* %0, i64 %44
  %46 = bitcast i16* %45 to <8 x i16>*
  %47 = load <8 x i16>, <8 x i16>* %46, align 1
  %48 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 4
  %49 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %47, <8 x i16> %13) #7
  %50 = add <8 x i16> %49, <i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128>
  %51 = bitcast <2 x i64>* %48 to <8 x i16>*
  store <8 x i16> %50, <8 x i16>* %51, align 16
  %52 = mul nsw i64 %14, 5
  %53 = getelementptr inbounds i16, i16* %0, i64 %52
  %54 = bitcast i16* %53 to <8 x i16>*
  %55 = load <8 x i16>, <8 x i16>* %54, align 1
  %56 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 5
  %57 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %55, <8 x i16> %13) #7
  %58 = add <8 x i16> %57, <i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128>
  %59 = bitcast <2 x i64>* %56 to <8 x i16>*
  store <8 x i16> %58, <8 x i16>* %59, align 16
  %60 = mul nsw i64 %14, 6
  %61 = getelementptr inbounds i16, i16* %0, i64 %60
  %62 = bitcast i16* %61 to <8 x i16>*
  %63 = load <8 x i16>, <8 x i16>* %62, align 1
  %64 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 6
  %65 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %63, <8 x i16> %13) #7
  %66 = add <8 x i16> %65, <i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128>
  %67 = bitcast <2 x i64>* %64 to <8 x i16>*
  store <8 x i16> %66, <8 x i16>* %67, align 16
  %68 = mul nsw i64 %14, 7
  %69 = getelementptr inbounds i16, i16* %0, i64 %68
  %70 = bitcast i16* %69 to <8 x i16>*
  %71 = load <8 x i16>, <8 x i16>* %70, align 1
  %72 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 7
  %73 = tail call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %71, <8 x i16> %13) #7
  %74 = add <8 x i16> %73, <i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128, i16 -128>
  %75 = bitcast <2 x i64>* %72 to <8 x i16>*
  store <8 x i16> %74, <8 x i16>* %75, align 16
  %76 = call fastcc <2 x i64> @compute_directions(<2 x i64>* nonnull %17, i32* %9)
  %77 = shufflevector <8 x i16> %19, <8 x i16> %26, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %78 = shufflevector <8 x i16> %34, <8 x i16> %42, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %79 = shufflevector <8 x i16> %19, <8 x i16> %26, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %80 = shufflevector <8 x i16> %34, <8 x i16> %42, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %81 = shufflevector <8 x i16> %50, <8 x i16> %58, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %82 = shufflevector <8 x i16> %66, <8 x i16> %74, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %83 = shufflevector <8 x i16> %50, <8 x i16> %58, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %84 = shufflevector <8 x i16> %66, <8 x i16> %74, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %85 = bitcast <8 x i16> %77 to <4 x i32>
  %86 = bitcast <8 x i16> %78 to <4 x i32>
  %87 = shufflevector <4 x i32> %85, <4 x i32> %86, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %88 = bitcast <4 x i32> %87 to <2 x i64>
  %89 = bitcast <8 x i16> %81 to <4 x i32>
  %90 = bitcast <8 x i16> %82 to <4 x i32>
  %91 = shufflevector <4 x i32> %89, <4 x i32> %90, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %92 = bitcast <4 x i32> %91 to <2 x i64>
  %93 = shufflevector <4 x i32> %85, <4 x i32> %86, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %94 = bitcast <4 x i32> %93 to <2 x i64>
  %95 = shufflevector <4 x i32> %89, <4 x i32> %90, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %96 = bitcast <4 x i32> %95 to <2 x i64>
  %97 = bitcast <8 x i16> %79 to <4 x i32>
  %98 = bitcast <8 x i16> %80 to <4 x i32>
  %99 = shufflevector <4 x i32> %97, <4 x i32> %98, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %100 = bitcast <4 x i32> %99 to <2 x i64>
  %101 = bitcast <8 x i16> %83 to <4 x i32>
  %102 = bitcast <8 x i16> %84 to <4 x i32>
  %103 = shufflevector <4 x i32> %101, <4 x i32> %102, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %104 = bitcast <4 x i32> %103 to <2 x i64>
  %105 = shufflevector <4 x i32> %97, <4 x i32> %98, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %106 = bitcast <4 x i32> %105 to <2 x i64>
  %107 = shufflevector <4 x i32> %101, <4 x i32> %102, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %108 = bitcast <4 x i32> %107 to <2 x i64>
  %109 = shufflevector <2 x i64> %88, <2 x i64> %92, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %109, <2 x i64>* %72, align 16
  %110 = shufflevector <2 x i64> %88, <2 x i64> %92, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %110, <2 x i64>* %64, align 16
  %111 = shufflevector <2 x i64> %94, <2 x i64> %96, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %111, <2 x i64>* %56, align 16
  %112 = shufflevector <2 x i64> %94, <2 x i64> %96, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %112, <2 x i64>* %48, align 16
  %113 = shufflevector <2 x i64> %100, <2 x i64> %104, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %113, <2 x i64>* %40, align 16
  %114 = shufflevector <2 x i64> %100, <2 x i64> %104, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %114, <2 x i64>* %32, align 16
  %115 = shufflevector <2 x i64> %106, <2 x i64> %108, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %115, <2 x i64>* %24, align 16
  %116 = shufflevector <2 x i64> %106, <2 x i64> %108, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %116, <2 x i64>* %17, align 16
  %117 = call fastcc <2 x i64> @compute_directions(<2 x i64>* nonnull %17, i32* nonnull %8)
  %118 = bitcast <2 x i64> %117 to <4 x i32>
  %119 = bitcast <2 x i64> %76 to <4 x i32>
  %120 = icmp sgt <4 x i32> %118, %119
  %121 = sext <4 x i1> %120 to <4 x i32>
  %122 = bitcast <4 x i32> %121 to <2 x i64>
  %123 = xor <2 x i64> %122, <i64 -1, i64 -1>
  %124 = and <2 x i64> %76, %123
  %125 = and <2 x i64> %117, %122
  %126 = or <2 x i64> %125, %124
  %127 = bitcast <2 x i64> %126 to <16 x i8>
  %128 = shufflevector <16 x i8> %127, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %129 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %127, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %130 = or <16 x i8> %129, %128
  %131 = bitcast <16 x i8> %130 to <2 x i64>
  %132 = bitcast <2 x i64> %126 to <4 x i32>
  %133 = bitcast <16 x i8> %130 to <4 x i32>
  %134 = icmp sgt <4 x i32> %132, %133
  %135 = sext <4 x i1> %134 to <4 x i32>
  %136 = bitcast <4 x i32> %135 to <2 x i64>
  %137 = xor <2 x i64> %136, <i64 -1, i64 -1>
  %138 = and <2 x i64> %137, %131
  %139 = and <2 x i64> %126, %136
  %140 = or <2 x i64> %139, %138
  %141 = bitcast <2 x i64> %140 to <16 x i8>
  %142 = shufflevector <16 x i8> %141, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19>
  %143 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %141, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19>
  %144 = or <16 x i8> %143, %142
  %145 = bitcast <16 x i8> %144 to <2 x i64>
  %146 = bitcast <2 x i64> %140 to <4 x i32>
  %147 = bitcast <16 x i8> %144 to <4 x i32>
  %148 = icmp sgt <4 x i32> %146, %147
  %149 = sext <4 x i1> %148 to <4 x i32>
  %150 = bitcast <4 x i32> %149 to <2 x i64>
  %151 = xor <2 x i64> %150, <i64 -1, i64 -1>
  %152 = and <2 x i64> %151, %145
  %153 = and <2 x i64> %140, %150
  %154 = or <2 x i64> %153, %152
  %155 = bitcast <2 x i64> %154 to <4 x i32>
  %156 = extractelement <4 x i32> %155, i32 0
  %157 = icmp eq <4 x i32> %155, %119
  %158 = sext <4 x i1> %157 to <4 x i32>
  %159 = icmp eq <4 x i32> %155, %118
  %160 = sext <4 x i1> %159 to <4 x i32>
  %161 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %160, <4 x i32> %158) #7
  %162 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %161, <8 x i16> %161) #7
  %163 = icmp slt <16 x i8> %162, zeroinitializer
  %164 = bitcast <16 x i1> %163 to i16
  %165 = zext i16 %164 to i32
  %166 = add nsw i32 %165, -1
  %167 = xor i32 %166, %165
  %168 = tail call i32 @llvm.ctlz.i32(i32 %167, i1 true) #7, !range !2
  %169 = xor i32 %168, 31
  %170 = add nuw nsw i32 %169, 4
  %171 = and i32 %170, 7
  %172 = zext i32 %171 to i64
  %173 = getelementptr inbounds [8 x i32], [8 x i32]* %5, i64 0, i64 %172
  %174 = load i32, i32* %173, align 4
  %175 = sub nsw i32 %156, %174
  %176 = ashr i32 %175, 10
  store i32 %176, i32* %2, align 4
  call void @llvm.lifetime.end.p0i8(i64 128, i8* nonnull %10) #7
  call void @llvm.lifetime.end.p0i8(i64 32, i8* nonnull %7) #7
  ret i32 %169
}

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #1

; Function Attrs: argmemonly nounwind
declare void @llvm.memset.p0i8.i64(i8* nocapture writeonly, i8, i64, i1 immarg) #1

; Function Attrs: inlinehint nounwind ssp uwtable
define internal fastcc <2 x i64> @compute_directions(<2 x i64>* nocapture readonly, i32* nocapture) unnamed_addr #2 {
  %3 = alloca <8 x i16>, align 16
  %4 = alloca <8 x i16>, align 16
  %5 = alloca <8 x i16>, align 16
  %6 = bitcast <2 x i64>* %0 to <16 x i8>*
  %7 = load <16 x i8>, <16 x i8>* %6, align 16
  %8 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %7, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %9 = shufflevector <16 x i8> %7, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %10 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %11 = bitcast <2 x i64>* %10 to <16 x i8>*
  %12 = load <16 x i8>, <16 x i8>* %11, align 16
  %13 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %12, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19>
  %14 = bitcast <16 x i8> %8 to <8 x i16>
  %15 = bitcast <16 x i8> %13 to <8 x i16>
  %16 = add <8 x i16> %15, %14
  %17 = shufflevector <16 x i8> %12, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19>
  %18 = bitcast <16 x i8> %9 to <8 x i16>
  %19 = bitcast <16 x i8> %17 to <8 x i16>
  %20 = add <8 x i16> %19, %18
  %21 = bitcast <16 x i8> %7 to <8 x i16>
  %22 = bitcast <16 x i8> %12 to <8 x i16>
  %23 = add <8 x i16> %22, %21
  %24 = bitcast <8 x i16> %23 to <16 x i8>
  %25 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %24, <16 x i32> <i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21>
  %26 = shufflevector <16 x i8> %24, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21>
  %27 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %24, <16 x i32> <i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27>
  %28 = shufflevector <16 x i8> %24, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27>
  %29 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %30 = bitcast <2 x i64>* %29 to <16 x i8>*
  %31 = load <16 x i8>, <16 x i8>* %30, align 16
  %32 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %31, <16 x i32> <i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21>
  %33 = bitcast <16 x i8> %32 to <8 x i16>
  %34 = add <8 x i16> %16, %33
  %35 = shufflevector <16 x i8> %31, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21>
  %36 = bitcast <16 x i8> %35 to <8 x i16>
  %37 = add <8 x i16> %20, %36
  %38 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %39 = bitcast <2 x i64>* %38 to <16 x i8>*
  %40 = load <16 x i8>, <16 x i8>* %39, align 16
  %41 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %40, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %42 = bitcast <16 x i8> %41 to <8 x i16>
  %43 = add <8 x i16> %34, %42
  %44 = shufflevector <16 x i8> %40, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %45 = bitcast <16 x i8> %44 to <8 x i16>
  %46 = add <8 x i16> %37, %45
  %47 = bitcast <16 x i8> %31 to <8 x i16>
  %48 = bitcast <16 x i8> %40 to <8 x i16>
  %49 = add <8 x i16> %48, %47
  %50 = bitcast <8 x i16> %49 to <16 x i8>
  %51 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %50, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %52 = bitcast <16 x i8> %25 to <8 x i16>
  %53 = bitcast <16 x i8> %51 to <8 x i16>
  %54 = add <8 x i16> %53, %52
  %55 = shufflevector <16 x i8> %50, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %56 = bitcast <16 x i8> %26 to <8 x i16>
  %57 = bitcast <16 x i8> %55 to <8 x i16>
  %58 = add <8 x i16> %57, %56
  %59 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %50, <16 x i32> <i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25>
  %60 = bitcast <16 x i8> %27 to <8 x i16>
  %61 = bitcast <16 x i8> %59 to <8 x i16>
  %62 = add <8 x i16> %61, %60
  %63 = shufflevector <16 x i8> %50, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25>
  %64 = bitcast <16 x i8> %28 to <8 x i16>
  %65 = bitcast <16 x i8> %63 to <8 x i16>
  %66 = add <8 x i16> %65, %64
  %67 = add <8 x i16> %49, %23
  %68 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 4
  %69 = bitcast <2 x i64>* %68 to <16 x i8>*
  %70 = load <16 x i8>, <16 x i8>* %69, align 16
  %71 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %70, <16 x i32> <i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25>
  %72 = bitcast <16 x i8> %71 to <8 x i16>
  %73 = add <8 x i16> %43, %72
  %74 = shufflevector <16 x i8> %70, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25>
  %75 = bitcast <16 x i8> %74 to <8 x i16>
  %76 = add <8 x i16> %46, %75
  %77 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 5
  %78 = bitcast <2 x i64>* %77 to <16 x i8>*
  %79 = load <16 x i8>, <16 x i8>* %78, align 16
  %80 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %79, <16 x i32> <i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27>
  %81 = bitcast <16 x i8> %80 to <8 x i16>
  %82 = add <8 x i16> %73, %81
  %83 = shufflevector <16 x i8> %79, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27>
  %84 = bitcast <16 x i8> %83 to <8 x i16>
  %85 = add <8 x i16> %76, %84
  %86 = bitcast <16 x i8> %70 to <8 x i16>
  %87 = bitcast <16 x i8> %79 to <8 x i16>
  %88 = add <8 x i16> %87, %86
  %89 = bitcast <8 x i16> %88 to <16 x i8>
  %90 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %89, <16 x i32> <i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25>
  %91 = bitcast <16 x i8> %90 to <8 x i16>
  %92 = add <8 x i16> %54, %91
  %93 = shufflevector <16 x i8> %89, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25>
  %94 = bitcast <16 x i8> %93 to <8 x i16>
  %95 = add <8 x i16> %58, %94
  %96 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %89, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %97 = bitcast <16 x i8> %96 to <8 x i16>
  %98 = add <8 x i16> %62, %97
  %99 = shufflevector <16 x i8> %89, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %100 = bitcast <16 x i8> %99 to <8 x i16>
  %101 = add <8 x i16> %66, %100
  %102 = add <8 x i16> %67, %88
  %103 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 6
  %104 = bitcast <2 x i64>* %103 to <16 x i8>*
  %105 = load <16 x i8>, <16 x i8>* %104, align 16
  %106 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0>, <16 x i8> %105, <16 x i32> <i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29>
  %107 = bitcast <16 x i8> %106 to <8 x i16>
  %108 = shufflevector <16 x i8> %105, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef>, <16 x i32> <i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29>
  %109 = bitcast <16 x i8> %108 to <8 x i16>
  %110 = add <8 x i16> %85, %109
  %111 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 7
  %112 = bitcast <2 x i64>* %111 to <8 x i16>*
  %113 = load <8 x i16>, <8 x i16>* %112, align 16
  %114 = add <8 x i16> %82, %113
  %115 = add <8 x i16> %114, %107
  %116 = bitcast <16 x i8> %105 to <8 x i16>
  %117 = add <8 x i16> %113, %116
  %118 = bitcast <8 x i16> %117 to <16 x i8>
  %119 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %118, <16 x i32> <i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27>
  %120 = bitcast <16 x i8> %119 to <8 x i16>
  %121 = add <8 x i16> %92, %120
  %122 = shufflevector <16 x i8> %118, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27>
  %123 = bitcast <16 x i8> %122 to <8 x i16>
  %124 = add <8 x i16> %95, %123
  %125 = shufflevector <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i8> %118, <16 x i32> <i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21>
  %126 = bitcast <16 x i8> %125 to <8 x i16>
  %127 = add <8 x i16> %98, %126
  %128 = shufflevector <16 x i8> %118, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21>
  %129 = bitcast <16 x i8> %128 to <8 x i16>
  %130 = add <8 x i16> %101, %129
  %131 = add <8 x i16> %102, %117
  %132 = bitcast <8 x i16>* %3 to i8*
  call void @llvm.lifetime.start.p0i8(i64 16, i8* nonnull %132)
  %133 = bitcast <8 x i16> %110 to <16 x i8>
  %134 = extractelement <16 x i8> %133, i32 12
  store i8 %134, i8* %132, align 16
  %135 = extractelement <16 x i8> %133, i32 13
  %136 = getelementptr inbounds i8, i8* %132, i64 1
  store i8 %135, i8* %136, align 1
  %137 = extractelement <16 x i8> %133, i32 10
  %138 = getelementptr inbounds <8 x i16>, <8 x i16>* %3, i64 0, i64 1
  %139 = bitcast i16* %138 to i8*
  store i8 %137, i8* %139, align 2
  %140 = extractelement <16 x i8> %133, i32 11
  %141 = getelementptr inbounds i8, i8* %132, i64 3
  store i8 %140, i8* %141, align 1
  %142 = extractelement <16 x i8> %133, i32 8
  %143 = getelementptr inbounds <8 x i16>, <8 x i16>* %3, i64 0, i64 2
  %144 = bitcast i16* %143 to i8*
  store i8 %142, i8* %144, align 4
  %145 = extractelement <16 x i8> %133, i32 9
  %146 = getelementptr inbounds i8, i8* %132, i64 5
  store i8 %145, i8* %146, align 1
  %147 = extractelement <16 x i8> %133, i32 6
  %148 = getelementptr inbounds <8 x i16>, <8 x i16>* %3, i64 0, i64 3
  %149 = bitcast i16* %148 to i8*
  store i8 %147, i8* %149, align 2
  %150 = extractelement <16 x i8> %133, i32 7
  %151 = getelementptr inbounds i8, i8* %132, i64 7
  store i8 %150, i8* %151, align 1
  %152 = extractelement <16 x i8> %133, i32 4
  %153 = getelementptr inbounds <8 x i16>, <8 x i16>* %3, i64 0, i64 4
  %154 = bitcast i16* %153 to i8*
  store i8 %152, i8* %154, align 8
  %155 = extractelement <16 x i8> %133, i32 5
  %156 = getelementptr inbounds i8, i8* %132, i64 9
  store i8 %155, i8* %156, align 1
  %157 = extractelement <16 x i8> %133, i32 2
  %158 = getelementptr inbounds <8 x i16>, <8 x i16>* %3, i64 0, i64 5
  %159 = bitcast i16* %158 to i8*
  store i8 %157, i8* %159, align 2
  %160 = extractelement <16 x i8> %133, i32 3
  %161 = getelementptr inbounds i8, i8* %132, i64 11
  store i8 %160, i8* %161, align 1
  %162 = extractelement <16 x i8> %133, i32 0
  %163 = getelementptr inbounds <8 x i16>, <8 x i16>* %3, i64 0, i64 6
  %164 = bitcast i16* %163 to i8*
  store i8 %162, i8* %164, align 4
  %165 = extractelement <16 x i8> %133, i32 1
  %166 = getelementptr inbounds i8, i8* %132, i64 13
  store i8 %165, i8* %166, align 1
  %167 = extractelement <16 x i8> %133, i32 14
  %168 = getelementptr inbounds <8 x i16>, <8 x i16>* %3, i64 0, i64 7
  %169 = bitcast i16* %168 to i8*
  store i8 %167, i8* %169, align 2
  %170 = bitcast <8 x i16> %110 to <16 x i8>
  %171 = extractelement <16 x i8> %170, i32 15
  %172 = getelementptr inbounds i8, i8* %132, i64 15
  store i8 %171, i8* %172, align 1
  %173 = load <8 x i16>, <8 x i16>* %3, align 16
  call void @llvm.lifetime.end.p0i8(i64 16, i8* nonnull %132)
  %174 = shufflevector <8 x i16> %115, <8 x i16> %173, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %175 = shufflevector <8 x i16> %115, <8 x i16> %173, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %176 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %174, <8 x i16> %174) #7
  %177 = bitcast <4 x i32> %176 to <2 x i64>
  %178 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %175, <8 x i16> %175) #7
  %179 = bitcast <4 x i32> %178 to <2 x i64>
  %180 = and <2 x i64> %177, <i64 4294967295, i64 4294967295>
  %181 = mul nuw nsw <2 x i64> %180, <i64 840, i64 280>
  %182 = bitcast <2 x i64> %181 to <4 x i32>
  %183 = shufflevector <4 x i32> %182, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %184 = bitcast <4 x i32> %176 to <16 x i8>
  %185 = shufflevector <16 x i8> %184, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19>
  %186 = bitcast <16 x i8> %185 to <2 x i64>
  %187 = and <2 x i64> %186, <i64 4294967295, i64 4294967295>
  %188 = mul nuw nsw <2 x i64> %187, <i64 420, i64 210>
  %189 = bitcast <2 x i64> %188 to <4 x i32>
  %190 = shufflevector <4 x i32> %189, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %191 = shufflevector <4 x i32> %183, <4 x i32> %190, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %192 = and <2 x i64> %179, <i64 4294967295, i64 4294967295>
  %193 = mul nuw nsw <2 x i64> %192, <i64 168, i64 120>
  %194 = bitcast <2 x i64> %193 to <4 x i32>
  %195 = shufflevector <4 x i32> %194, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %196 = bitcast <4 x i32> %178 to <16 x i8>
  %197 = shufflevector <16 x i8> %196, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19>
  %198 = bitcast <16 x i8> %197 to <2 x i64>
  %199 = and <2 x i64> %198, <i64 4294967295, i64 4294967295>
  %200 = mul nuw nsw <2 x i64> %199, <i64 140, i64 105>
  %201 = bitcast <2 x i64> %200 to <4 x i32>
  %202 = shufflevector <4 x i32> %201, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %203 = shufflevector <4 x i32> %195, <4 x i32> %202, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %204 = add <4 x i32> %203, %191
  %205 = bitcast <8 x i16>* %4 to i8*
  call void @llvm.lifetime.start.p0i8(i64 16, i8* nonnull %205)
  %206 = bitcast <8 x i16> %130 to <16 x i8>
  %207 = extractelement <16 x i8> %206, i32 12
  store i8 %207, i8* %205, align 16
  %208 = extractelement <16 x i8> %206, i32 13
  %209 = getelementptr inbounds i8, i8* %205, i64 1
  store i8 %208, i8* %209, align 1
  %210 = extractelement <16 x i8> %206, i32 10
  %211 = getelementptr inbounds <8 x i16>, <8 x i16>* %4, i64 0, i64 1
  %212 = bitcast i16* %211 to i8*
  store i8 %210, i8* %212, align 2
  %213 = extractelement <16 x i8> %206, i32 11
  %214 = getelementptr inbounds i8, i8* %205, i64 3
  store i8 %213, i8* %214, align 1
  %215 = extractelement <16 x i8> %206, i32 8
  %216 = getelementptr inbounds <8 x i16>, <8 x i16>* %4, i64 0, i64 2
  %217 = bitcast i16* %216 to i8*
  store i8 %215, i8* %217, align 4
  %218 = extractelement <16 x i8> %206, i32 9
  %219 = getelementptr inbounds i8, i8* %205, i64 5
  store i8 %218, i8* %219, align 1
  %220 = extractelement <16 x i8> %206, i32 6
  %221 = getelementptr inbounds <8 x i16>, <8 x i16>* %4, i64 0, i64 3
  %222 = bitcast i16* %221 to i8*
  store i8 %220, i8* %222, align 2
  %223 = extractelement <16 x i8> %206, i32 7
  %224 = getelementptr inbounds i8, i8* %205, i64 7
  store i8 %223, i8* %224, align 1
  %225 = extractelement <16 x i8> %206, i32 4
  %226 = getelementptr inbounds <8 x i16>, <8 x i16>* %4, i64 0, i64 4
  %227 = bitcast i16* %226 to i8*
  store i8 %225, i8* %227, align 8
  %228 = extractelement <16 x i8> %206, i32 5
  %229 = getelementptr inbounds i8, i8* %205, i64 9
  store i8 %228, i8* %229, align 1
  %230 = extractelement <16 x i8> %206, i32 2
  %231 = getelementptr inbounds <8 x i16>, <8 x i16>* %4, i64 0, i64 5
  %232 = bitcast i16* %231 to i8*
  store i8 %230, i8* %232, align 2
  %233 = extractelement <16 x i8> %206, i32 3
  %234 = getelementptr inbounds i8, i8* %205, i64 11
  store i8 %233, i8* %234, align 1
  %235 = extractelement <16 x i8> %206, i32 0
  %236 = getelementptr inbounds <8 x i16>, <8 x i16>* %4, i64 0, i64 6
  %237 = bitcast i16* %236 to i8*
  store i8 %235, i8* %237, align 4
  %238 = extractelement <16 x i8> %206, i32 1
  %239 = getelementptr inbounds i8, i8* %205, i64 13
  store i8 %238, i8* %239, align 1
  %240 = extractelement <16 x i8> %206, i32 14
  %241 = getelementptr inbounds <8 x i16>, <8 x i16>* %4, i64 0, i64 7
  %242 = bitcast i16* %241 to i8*
  store i8 %240, i8* %242, align 2
  %243 = bitcast <8 x i16> %130 to <16 x i8>
  %244 = extractelement <16 x i8> %243, i32 15
  %245 = getelementptr inbounds i8, i8* %205, i64 15
  store i8 %244, i8* %245, align 1
  %246 = load <8 x i16>, <8 x i16>* %4, align 16
  call void @llvm.lifetime.end.p0i8(i64 16, i8* nonnull %205)
  %247 = shufflevector <8 x i16> %127, <8 x i16> %246, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %248 = shufflevector <8 x i16> %127, <8 x i16> %246, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %249 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %247, <8 x i16> %247) #7
  %250 = bitcast <4 x i32> %249 to <2 x i64>
  %251 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %248, <8 x i16> %248) #7
  %252 = bitcast <4 x i32> %251 to <2 x i64>
  %253 = and <2 x i64> %250, <i64 4294967295, i64 4294967295>
  %254 = mul nuw nsw <2 x i64> %253, <i64 0, i64 420>
  %255 = bitcast <2 x i64> %254 to <4 x i32>
  %256 = shufflevector <4 x i32> %255, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %257 = bitcast <4 x i32> %249 to <16 x i8>
  %258 = shufflevector <16 x i8> %257, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19>
  %259 = bitcast <16 x i8> %258 to <2 x i64>
  %260 = and <2 x i64> %259, <i64 4294967295, i64 4294967295>
  %261 = mul nuw nsw <2 x i64> %260, <i64 0, i64 210>
  %262 = bitcast <2 x i64> %261 to <4 x i32>
  %263 = shufflevector <4 x i32> %262, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %264 = shufflevector <4 x i32> %256, <4 x i32> %263, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %265 = and <2 x i64> %252, <i64 4294967295, i64 4294967295>
  %266 = mul nuw nsw <2 x i64> %265, <i64 140, i64 105>
  %267 = bitcast <2 x i64> %266 to <4 x i32>
  %268 = shufflevector <4 x i32> %267, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %269 = bitcast <4 x i32> %251 to <16 x i8>
  %270 = shufflevector <16 x i8> %269, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19>
  %271 = bitcast <16 x i8> %270 to <2 x i64>
  %272 = and <2 x i64> %271, <i64 4294967295, i64 4294967295>
  %273 = mul nuw nsw <2 x i64> %272, <i64 105, i64 105>
  %274 = bitcast <2 x i64> %273 to <4 x i32>
  %275 = shufflevector <4 x i32> %274, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %276 = shufflevector <4 x i32> %268, <4 x i32> %275, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %277 = add <4 x i32> %276, %264
  %278 = bitcast <8 x i16>* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 16, i8* nonnull %278)
  %279 = bitcast <8 x i16> %124 to <16 x i8>
  %280 = extractelement <16 x i8> %279, i32 12
  store i8 %280, i8* %278, align 16
  %281 = extractelement <16 x i8> %279, i32 13
  %282 = getelementptr inbounds i8, i8* %278, i64 1
  store i8 %281, i8* %282, align 1
  %283 = extractelement <16 x i8> %279, i32 10
  %284 = getelementptr inbounds <8 x i16>, <8 x i16>* %5, i64 0, i64 1
  %285 = bitcast i16* %284 to i8*
  store i8 %283, i8* %285, align 2
  %286 = extractelement <16 x i8> %279, i32 11
  %287 = getelementptr inbounds i8, i8* %278, i64 3
  store i8 %286, i8* %287, align 1
  %288 = extractelement <16 x i8> %279, i32 8
  %289 = getelementptr inbounds <8 x i16>, <8 x i16>* %5, i64 0, i64 2
  %290 = bitcast i16* %289 to i8*
  store i8 %288, i8* %290, align 4
  %291 = extractelement <16 x i8> %279, i32 9
  %292 = getelementptr inbounds i8, i8* %278, i64 5
  store i8 %291, i8* %292, align 1
  %293 = extractelement <16 x i8> %279, i32 6
  %294 = getelementptr inbounds <8 x i16>, <8 x i16>* %5, i64 0, i64 3
  %295 = bitcast i16* %294 to i8*
  store i8 %293, i8* %295, align 2
  %296 = extractelement <16 x i8> %279, i32 7
  %297 = getelementptr inbounds i8, i8* %278, i64 7
  store i8 %296, i8* %297, align 1
  %298 = extractelement <16 x i8> %279, i32 4
  %299 = getelementptr inbounds <8 x i16>, <8 x i16>* %5, i64 0, i64 4
  %300 = bitcast i16* %299 to i8*
  store i8 %298, i8* %300, align 8
  %301 = extractelement <16 x i8> %279, i32 5
  %302 = getelementptr inbounds i8, i8* %278, i64 9
  store i8 %301, i8* %302, align 1
  %303 = extractelement <16 x i8> %279, i32 2
  %304 = getelementptr inbounds <8 x i16>, <8 x i16>* %5, i64 0, i64 5
  %305 = bitcast i16* %304 to i8*
  store i8 %303, i8* %305, align 2
  %306 = extractelement <16 x i8> %279, i32 3
  %307 = getelementptr inbounds i8, i8* %278, i64 11
  store i8 %306, i8* %307, align 1
  %308 = extractelement <16 x i8> %279, i32 0
  %309 = getelementptr inbounds <8 x i16>, <8 x i16>* %5, i64 0, i64 6
  %310 = bitcast i16* %309 to i8*
  store i8 %308, i8* %310, align 4
  %311 = extractelement <16 x i8> %279, i32 1
  %312 = getelementptr inbounds i8, i8* %278, i64 13
  store i8 %311, i8* %312, align 1
  %313 = extractelement <16 x i8> %279, i32 14
  %314 = getelementptr inbounds <8 x i16>, <8 x i16>* %5, i64 0, i64 7
  %315 = bitcast i16* %314 to i8*
  store i8 %313, i8* %315, align 2
  %316 = bitcast <8 x i16> %124 to <16 x i8>
  %317 = extractelement <16 x i8> %316, i32 15
  %318 = getelementptr inbounds i8, i8* %278, i64 15
  store i8 %317, i8* %318, align 1
  %319 = load <8 x i16>, <8 x i16>* %5, align 16
  call void @llvm.lifetime.end.p0i8(i64 16, i8* nonnull %278)
  %320 = shufflevector <8 x i16> %121, <8 x i16> %319, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %321 = shufflevector <8 x i16> %121, <8 x i16> %319, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %322 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %320, <8 x i16> %320) #7
  %323 = bitcast <4 x i32> %322 to <2 x i64>
  %324 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %321, <8 x i16> %321) #7
  %325 = bitcast <4 x i32> %324 to <2 x i64>
  %326 = and <2 x i64> %323, <i64 4294967295, i64 4294967295>
  %327 = mul nuw nsw <2 x i64> %326, <i64 0, i64 420>
  %328 = bitcast <2 x i64> %327 to <4 x i32>
  %329 = shufflevector <4 x i32> %328, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %330 = bitcast <4 x i32> %322 to <16 x i8>
  %331 = shufflevector <16 x i8> %330, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19>
  %332 = bitcast <16 x i8> %331 to <2 x i64>
  %333 = and <2 x i64> %332, <i64 4294967295, i64 4294967295>
  %334 = mul nuw nsw <2 x i64> %333, <i64 0, i64 210>
  %335 = bitcast <2 x i64> %334 to <4 x i32>
  %336 = shufflevector <4 x i32> %335, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %337 = shufflevector <4 x i32> %329, <4 x i32> %336, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %338 = and <2 x i64> %325, <i64 4294967295, i64 4294967295>
  %339 = mul nuw nsw <2 x i64> %338, <i64 140, i64 105>
  %340 = bitcast <2 x i64> %339 to <4 x i32>
  %341 = shufflevector <4 x i32> %340, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %342 = bitcast <4 x i32> %324 to <16 x i8>
  %343 = shufflevector <16 x i8> %342, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19>
  %344 = bitcast <16 x i8> %343 to <2 x i64>
  %345 = and <2 x i64> %344, <i64 4294967295, i64 4294967295>
  %346 = mul nuw nsw <2 x i64> %345, <i64 105, i64 105>
  %347 = bitcast <2 x i64> %346 to <4 x i32>
  %348 = shufflevector <4 x i32> %347, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %349 = shufflevector <4 x i32> %341, <4 x i32> %348, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %350 = add <4 x i32> %349, %337
  %351 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %131, <8 x i16> %131) #7
  %352 = bitcast <4 x i32> %351 to <2 x i64>
  %353 = and <2 x i64> %352, <i64 4294967295, i64 4294967295>
  %354 = mul nuw nsw <2 x i64> %353, <i64 105, i64 105>
  %355 = bitcast <2 x i64> %354 to <4 x i32>
  %356 = shufflevector <4 x i32> %355, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %357 = bitcast <4 x i32> %351 to <16 x i8>
  %358 = shufflevector <16 x i8> %357, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19>
  %359 = bitcast <16 x i8> %358 to <2 x i64>
  %360 = and <2 x i64> %359, <i64 4294967295, i64 4294967295>
  %361 = mul nuw nsw <2 x i64> %360, <i64 105, i64 105>
  %362 = bitcast <2 x i64> %361 to <4 x i32>
  %363 = shufflevector <4 x i32> %362, <4 x i32> undef, <4 x i32> <i32 0, i32 2, i32 undef, i32 undef>
  %364 = shufflevector <4 x i32> %356, <4 x i32> %363, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %365 = shufflevector <4 x i32> %204, <4 x i32> %350, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %366 = bitcast <4 x i32> %365 to <2 x i64>
  %367 = shufflevector <4 x i32> %364, <4 x i32> %277, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %368 = bitcast <4 x i32> %367 to <2 x i64>
  %369 = shufflevector <4 x i32> %204, <4 x i32> %350, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %370 = bitcast <4 x i32> %369 to <2 x i64>
  %371 = shufflevector <4 x i32> %364, <4 x i32> %277, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %372 = bitcast <4 x i32> %371 to <2 x i64>
  %373 = shufflevector <2 x i64> %366, <2 x i64> %368, <2 x i32> <i32 0, i32 2>
  %374 = shufflevector <2 x i64> %366, <2 x i64> %368, <2 x i32> <i32 1, i32 3>
  %375 = shufflevector <2 x i64> %370, <2 x i64> %372, <2 x i32> <i32 0, i32 2>
  %376 = shufflevector <2 x i64> %370, <2 x i64> %372, <2 x i32> <i32 1, i32 3>
  %377 = bitcast <2 x i64> %373 to <4 x i32>
  %378 = bitcast <2 x i64> %374 to <4 x i32>
  %379 = bitcast <2 x i64> %375 to <4 x i32>
  %380 = bitcast <2 x i64> %376 to <4 x i32>
  %381 = add <4 x i32> %380, %379
  %382 = add <4 x i32> %381, %377
  %383 = add <4 x i32> %382, %378
  %384 = bitcast <4 x i32> %383 to <2 x i64>
  %385 = bitcast i32* %1 to <4 x i32>*
  store <4 x i32> %383, <4 x i32>* %385, align 1
  ret <2 x i64> %384
}

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #1

; Function Attrs: nounwind ssp uwtable
define hidden void @cdef_filter_block_4x4_8_sse2(i8* nocapture, i32, i16* readonly, i32, i32, i32, i32, i32, i32) local_unnamed_addr #0 {
  %10 = sext i32 %5 to i64
  %11 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %10, i64 0
  %12 = load i32, i32* %11, align 8
  %13 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %10, i64 1
  %14 = load i32, i32* %13, align 4
  %15 = add nsw i32 %5, 2
  %16 = and i32 %15, 7
  %17 = zext i32 %16 to i64
  %18 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %17, i64 0
  %19 = load i32, i32* %18, align 8
  %20 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %17, i64 1
  %21 = load i32, i32* %20, align 4
  %22 = add nsw i32 %5, 6
  %23 = and i32 %22, 7
  %24 = zext i32 %23 to i64
  %25 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %24, i64 0
  %26 = load i32, i32* %25, align 8
  %27 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %24, i64 1
  %28 = load i32, i32* %27, align 4
  %29 = lshr i32 %3, %8
  %30 = and i32 %29, 1
  %31 = zext i32 %30 to i64
  %32 = getelementptr inbounds [2 x [2 x i32]], [2 x [2 x i32]]* @cdef_pri_taps, i64 0, i64 %31, i64 0
  %33 = icmp ne i32 %3, 0
  br i1 %33, label %34, label %40

34:                                               ; preds = %9
  %35 = tail call i32 @llvm.ctlz.i32(i32 %3, i1 true) #7, !range !2
  %36 = xor i32 %35, 31
  %37 = icmp sgt i32 %36, %6
  %38 = sub nsw i32 %6, %36
  %39 = select i1 %37, i32 0, i32 %38
  br label %40

40:                                               ; preds = %34, %9
  %41 = phi i32 [ %6, %9 ], [ %39, %34 ]
  %42 = icmp ne i32 %4, 0
  br i1 %42, label %43, label %49

43:                                               ; preds = %40
  %44 = tail call i32 @llvm.ctlz.i32(i32 %4, i1 true) #7, !range !2
  %45 = xor i32 %44, 31
  %46 = icmp sgt i32 %45, %7
  %47 = sub nsw i32 %7, %45
  %48 = select i1 %46, i32 0, i32 %47
  br label %49

49:                                               ; preds = %43, %40
  %50 = phi i32 [ %7, %40 ], [ %48, %43 ]
  %51 = bitcast i16* %2 to i64*
  %52 = load i64, i64* %51, align 1
  %53 = getelementptr inbounds i16, i16* %2, i64 144
  %54 = bitcast i16* %53 to i64*
  %55 = load i64, i64* %54, align 1
  %56 = insertelement <2 x i64> undef, i64 %55, i32 0
  %57 = getelementptr inbounds i16, i16* %2, i64 288
  %58 = bitcast i16* %57 to i64*
  %59 = load i64, i64* %58, align 1
  %60 = getelementptr inbounds i16, i16* %2, i64 432
  %61 = bitcast i16* %60 to i64*
  %62 = load i64, i64* %61, align 1
  %63 = insertelement <2 x i64> undef, i64 %62, i32 0
  %64 = insertelement <2 x i64> %56, i64 %52, i32 1
  %65 = insertelement <2 x i64> %63, i64 %59, i32 1
  br i1 %33, label %66, label %387

66:                                               ; preds = %49
  %67 = sext i32 %12 to i64
  %68 = getelementptr inbounds i16, i16* %2, i64 %67
  %69 = bitcast i16* %68 to i64*
  %70 = load i64, i64* %69, align 1
  %71 = add nsw i32 %12, 144
  %72 = sext i32 %71 to i64
  %73 = getelementptr inbounds i16, i16* %2, i64 %72
  %74 = bitcast i16* %73 to i64*
  %75 = load i64, i64* %74, align 1
  %76 = insertelement <2 x i64> undef, i64 %75, i32 0
  %77 = add nsw i32 %12, 288
  %78 = sext i32 %77 to i64
  %79 = getelementptr inbounds i16, i16* %2, i64 %78
  %80 = bitcast i16* %79 to i64*
  %81 = load i64, i64* %80, align 1
  %82 = add nsw i32 %12, 432
  %83 = sext i32 %82 to i64
  %84 = getelementptr inbounds i16, i16* %2, i64 %83
  %85 = bitcast i16* %84 to i64*
  %86 = load i64, i64* %85, align 1
  %87 = insertelement <2 x i64> undef, i64 %86, i32 0
  %88 = insertelement <2 x i64> %76, i64 %70, i32 1
  %89 = insertelement <2 x i64> %87, i64 %81, i32 1
  %90 = bitcast <2 x i64> %88 to <8 x i16>
  %91 = icmp eq <8 x i16> %90, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %92 = sext <8 x i1> %91 to <8 x i16>
  %93 = bitcast <2 x i64> %89 to <8 x i16>
  %94 = icmp eq <8 x i16> %93, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %95 = sext <8 x i1> %94 to <8 x i16>
  %96 = bitcast <8 x i16> %95 to <2 x i64>
  %97 = bitcast <8 x i16> %92 to <2 x i64>
  %98 = xor <2 x i64> %97, <i64 -1, i64 -1>
  %99 = and <2 x i64> %88, %98
  %100 = xor <2 x i64> %96, <i64 -1, i64 -1>
  %101 = and <2 x i64> %89, %100
  %102 = bitcast <2 x i64> %64 to <8 x i16>
  %103 = bitcast <2 x i64> %99 to <8 x i16>
  %104 = icmp sgt <8 x i16> %102, %103
  %105 = select <8 x i1> %104, <8 x i16> %102, <8 x i16> %103
  %106 = bitcast <2 x i64> %65 to <8 x i16>
  %107 = bitcast <2 x i64> %101 to <8 x i16>
  %108 = icmp sgt <8 x i16> %106, %107
  %109 = select <8 x i1> %108, <8 x i16> %106, <8 x i16> %107
  %110 = icmp sgt <8 x i16> %90, %102
  %111 = select <8 x i1> %110, <8 x i16> %102, <8 x i16> %90
  %112 = icmp sgt <8 x i16> %93, %106
  %113 = select <8 x i1> %112, <8 x i16> %106, <8 x i16> %93
  %114 = sub <8 x i16> %90, %102
  %115 = sub <8 x i16> %93, %106
  %116 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %115, <8 x i16> %114) #7
  %117 = ashr <16 x i8> %116, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %118 = icmp slt <16 x i8> %116, zeroinitializer
  %119 = sub <16 x i8> zeroinitializer, %116
  %120 = select <16 x i1> %118, <16 x i8> %119, <16 x i8> %116
  %121 = trunc i32 %3 to i8
  %122 = insertelement <16 x i8> undef, i8 %121, i32 0
  %123 = shufflevector <16 x i8> %122, <16 x i8> undef, <16 x i32> zeroinitializer
  %124 = lshr i32 255, %41
  %125 = trunc i32 %124 to i8
  %126 = insertelement <16 x i8> undef, i8 %125, i32 0
  %127 = shufflevector <16 x i8> %126, <16 x i8> undef, <16 x i32> zeroinitializer
  %128 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %41, i32 0
  %129 = bitcast <16 x i8> %120 to <8 x i16>
  %130 = bitcast <4 x i32> %128 to <8 x i16>
  %131 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %129, <8 x i16> %130) #7
  %132 = bitcast <8 x i16> %131 to <16 x i8>
  %133 = and <16 x i8> %127, %132
  %134 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %123, <16 x i8> %133) #7
  %135 = icmp ult <16 x i8> %120, %134
  %136 = select <16 x i1> %135, <16 x i8> %120, <16 x i8> %134
  %137 = add <16 x i8> %136, %117
  %138 = xor <16 x i8> %137, %117
  %139 = sub nsw i32 0, %12
  %140 = sext i32 %139 to i64
  %141 = getelementptr inbounds i16, i16* %2, i64 %140
  %142 = bitcast i16* %141 to i64*
  %143 = load i64, i64* %142, align 1
  %144 = sub nsw i32 144, %12
  %145 = sext i32 %144 to i64
  %146 = getelementptr inbounds i16, i16* %2, i64 %145
  %147 = bitcast i16* %146 to i64*
  %148 = load i64, i64* %147, align 1
  %149 = insertelement <2 x i64> undef, i64 %148, i32 0
  %150 = sub nsw i32 288, %12
  %151 = sext i32 %150 to i64
  %152 = getelementptr inbounds i16, i16* %2, i64 %151
  %153 = bitcast i16* %152 to i64*
  %154 = load i64, i64* %153, align 1
  %155 = sub nsw i32 432, %12
  %156 = sext i32 %155 to i64
  %157 = getelementptr inbounds i16, i16* %2, i64 %156
  %158 = bitcast i16* %157 to i64*
  %159 = load i64, i64* %158, align 1
  %160 = insertelement <2 x i64> undef, i64 %159, i32 0
  %161 = insertelement <2 x i64> %149, i64 %143, i32 1
  %162 = insertelement <2 x i64> %160, i64 %154, i32 1
  %163 = bitcast <2 x i64> %161 to <8 x i16>
  %164 = icmp eq <8 x i16> %163, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %165 = sext <8 x i1> %164 to <8 x i16>
  %166 = bitcast <2 x i64> %162 to <8 x i16>
  %167 = icmp eq <8 x i16> %166, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %168 = sext <8 x i1> %167 to <8 x i16>
  %169 = bitcast <8 x i16> %168 to <2 x i64>
  %170 = bitcast <8 x i16> %165 to <2 x i64>
  %171 = xor <2 x i64> %170, <i64 -1, i64 -1>
  %172 = and <2 x i64> %161, %171
  %173 = xor <2 x i64> %169, <i64 -1, i64 -1>
  %174 = and <2 x i64> %162, %173
  %175 = bitcast <2 x i64> %172 to <8 x i16>
  %176 = icmp sgt <8 x i16> %105, %175
  %177 = select <8 x i1> %176, <8 x i16> %105, <8 x i16> %175
  %178 = bitcast <2 x i64> %174 to <8 x i16>
  %179 = icmp sgt <8 x i16> %109, %178
  %180 = select <8 x i1> %179, <8 x i16> %109, <8 x i16> %178
  %181 = icmp slt <8 x i16> %111, %163
  %182 = select <8 x i1> %181, <8 x i16> %111, <8 x i16> %163
  %183 = icmp slt <8 x i16> %113, %166
  %184 = select <8 x i1> %183, <8 x i16> %113, <8 x i16> %166
  %185 = sub <8 x i16> %163, %102
  %186 = sub <8 x i16> %166, %106
  %187 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %186, <8 x i16> %185) #7
  %188 = ashr <16 x i8> %187, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %189 = icmp slt <16 x i8> %187, zeroinitializer
  %190 = sub <16 x i8> zeroinitializer, %187
  %191 = select <16 x i1> %189, <16 x i8> %190, <16 x i8> %187
  %192 = bitcast <16 x i8> %191 to <8 x i16>
  %193 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %192, <8 x i16> %130) #7
  %194 = bitcast <8 x i16> %193 to <16 x i8>
  %195 = and <16 x i8> %127, %194
  %196 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %123, <16 x i8> %195) #7
  %197 = icmp ult <16 x i8> %191, %196
  %198 = select <16 x i1> %197, <16 x i8> %191, <16 x i8> %196
  %199 = add <16 x i8> %198, %188
  %200 = xor <16 x i8> %199, %188
  %201 = load i32, i32* %32, align 8
  %202 = trunc i32 %201 to i8
  %203 = insertelement <16 x i8> undef, i8 %202, i32 0
  %204 = shufflevector <16 x i8> %203, <16 x i8> undef, <16 x i32> zeroinitializer
  %205 = shufflevector <16 x i8> %200, <16 x i8> %138, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %206 = shufflevector <16 x i8> %200, <16 x i8> %138, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %207 = shufflevector <16 x i8> %204, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %208 = shufflevector <16 x i8> %205, <16 x i8> undef, <16 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3, i32 4, i32 4, i32 5, i32 5, i32 6, i32 6, i32 7, i32 7>
  %209 = bitcast <16 x i8> %208 to <8 x i16>
  %210 = ashr <8 x i16> %209, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %211 = bitcast <16 x i8> %207 to <8 x i16>
  %212 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %211, <8 x i16> %210) #7
  %213 = shufflevector <16 x i8> %204, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %214 = shufflevector <16 x i8> %205, <16 x i8> undef, <16 x i32> <i32 8, i32 8, i32 9, i32 9, i32 10, i32 10, i32 11, i32 11, i32 12, i32 12, i32 13, i32 13, i32 14, i32 14, i32 15, i32 15>
  %215 = bitcast <16 x i8> %214 to <8 x i16>
  %216 = ashr <8 x i16> %215, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %217 = bitcast <16 x i8> %213 to <8 x i16>
  %218 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %217, <8 x i16> %216) #7
  %219 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %212, <4 x i32> %218) #7
  %220 = shufflevector <16 x i8> %206, <16 x i8> undef, <16 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3, i32 4, i32 4, i32 5, i32 5, i32 6, i32 6, i32 7, i32 7>
  %221 = bitcast <16 x i8> %220 to <8 x i16>
  %222 = ashr <8 x i16> %221, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %223 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %211, <8 x i16> %222) #7
  %224 = shufflevector <16 x i8> %206, <16 x i8> undef, <16 x i32> <i32 8, i32 8, i32 9, i32 9, i32 10, i32 10, i32 11, i32 11, i32 12, i32 12, i32 13, i32 13, i32 14, i32 14, i32 15, i32 15>
  %225 = bitcast <16 x i8> %224 to <8 x i16>
  %226 = ashr <8 x i16> %225, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %227 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %217, <8 x i16> %226) #7
  %228 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %223, <4 x i32> %227) #7
  %229 = sext i32 %14 to i64
  %230 = getelementptr inbounds i16, i16* %2, i64 %229
  %231 = bitcast i16* %230 to i64*
  %232 = load i64, i64* %231, align 1
  %233 = add nsw i32 %14, 144
  %234 = sext i32 %233 to i64
  %235 = getelementptr inbounds i16, i16* %2, i64 %234
  %236 = bitcast i16* %235 to i64*
  %237 = load i64, i64* %236, align 1
  %238 = insertelement <2 x i64> undef, i64 %237, i32 0
  %239 = add nsw i32 %14, 288
  %240 = sext i32 %239 to i64
  %241 = getelementptr inbounds i16, i16* %2, i64 %240
  %242 = bitcast i16* %241 to i64*
  %243 = load i64, i64* %242, align 1
  %244 = add nsw i32 %14, 432
  %245 = sext i32 %244 to i64
  %246 = getelementptr inbounds i16, i16* %2, i64 %245
  %247 = bitcast i16* %246 to i64*
  %248 = load i64, i64* %247, align 1
  %249 = insertelement <2 x i64> undef, i64 %248, i32 0
  %250 = insertelement <2 x i64> %238, i64 %232, i32 1
  %251 = insertelement <2 x i64> %249, i64 %243, i32 1
  %252 = bitcast <2 x i64> %250 to <8 x i16>
  %253 = icmp eq <8 x i16> %252, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %254 = sext <8 x i1> %253 to <8 x i16>
  %255 = bitcast <2 x i64> %251 to <8 x i16>
  %256 = icmp eq <8 x i16> %255, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %257 = sext <8 x i1> %256 to <8 x i16>
  %258 = bitcast <8 x i16> %257 to <2 x i64>
  %259 = bitcast <8 x i16> %254 to <2 x i64>
  %260 = xor <2 x i64> %259, <i64 -1, i64 -1>
  %261 = and <2 x i64> %250, %260
  %262 = xor <2 x i64> %258, <i64 -1, i64 -1>
  %263 = and <2 x i64> %251, %262
  %264 = bitcast <2 x i64> %261 to <8 x i16>
  %265 = icmp sgt <8 x i16> %177, %264
  %266 = select <8 x i1> %265, <8 x i16> %177, <8 x i16> %264
  %267 = bitcast <2 x i64> %263 to <8 x i16>
  %268 = icmp sgt <8 x i16> %180, %267
  %269 = select <8 x i1> %268, <8 x i16> %180, <8 x i16> %267
  %270 = icmp slt <8 x i16> %182, %252
  %271 = select <8 x i1> %270, <8 x i16> %182, <8 x i16> %252
  %272 = icmp slt <8 x i16> %184, %255
  %273 = select <8 x i1> %272, <8 x i16> %184, <8 x i16> %255
  %274 = sub <8 x i16> %252, %102
  %275 = sub <8 x i16> %255, %106
  %276 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %275, <8 x i16> %274) #7
  %277 = ashr <16 x i8> %276, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %278 = icmp slt <16 x i8> %276, zeroinitializer
  %279 = sub <16 x i8> zeroinitializer, %276
  %280 = select <16 x i1> %278, <16 x i8> %279, <16 x i8> %276
  %281 = bitcast <16 x i8> %280 to <8 x i16>
  %282 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %281, <8 x i16> %130) #7
  %283 = bitcast <8 x i16> %282 to <16 x i8>
  %284 = and <16 x i8> %127, %283
  %285 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %123, <16 x i8> %284) #7
  %286 = icmp ult <16 x i8> %280, %285
  %287 = select <16 x i1> %286, <16 x i8> %280, <16 x i8> %285
  %288 = add <16 x i8> %287, %277
  %289 = xor <16 x i8> %288, %277
  %290 = sub nsw i32 0, %14
  %291 = sext i32 %290 to i64
  %292 = getelementptr inbounds i16, i16* %2, i64 %291
  %293 = bitcast i16* %292 to i64*
  %294 = load i64, i64* %293, align 1
  %295 = sub nsw i32 144, %14
  %296 = sext i32 %295 to i64
  %297 = getelementptr inbounds i16, i16* %2, i64 %296
  %298 = bitcast i16* %297 to i64*
  %299 = load i64, i64* %298, align 1
  %300 = insertelement <2 x i64> undef, i64 %299, i32 0
  %301 = sub nsw i32 288, %14
  %302 = sext i32 %301 to i64
  %303 = getelementptr inbounds i16, i16* %2, i64 %302
  %304 = bitcast i16* %303 to i64*
  %305 = load i64, i64* %304, align 1
  %306 = sub nsw i32 432, %14
  %307 = sext i32 %306 to i64
  %308 = getelementptr inbounds i16, i16* %2, i64 %307
  %309 = bitcast i16* %308 to i64*
  %310 = load i64, i64* %309, align 1
  %311 = insertelement <2 x i64> undef, i64 %310, i32 0
  %312 = insertelement <2 x i64> %300, i64 %294, i32 1
  %313 = insertelement <2 x i64> %311, i64 %305, i32 1
  %314 = bitcast <2 x i64> %312 to <8 x i16>
  %315 = icmp eq <8 x i16> %314, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %316 = sext <8 x i1> %315 to <8 x i16>
  %317 = bitcast <2 x i64> %313 to <8 x i16>
  %318 = icmp eq <8 x i16> %317, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %319 = sext <8 x i1> %318 to <8 x i16>
  %320 = bitcast <8 x i16> %319 to <2 x i64>
  %321 = bitcast <8 x i16> %316 to <2 x i64>
  %322 = xor <2 x i64> %321, <i64 -1, i64 -1>
  %323 = and <2 x i64> %312, %322
  %324 = xor <2 x i64> %320, <i64 -1, i64 -1>
  %325 = and <2 x i64> %313, %324
  %326 = bitcast <2 x i64> %323 to <8 x i16>
  %327 = icmp sgt <8 x i16> %266, %326
  %328 = select <8 x i1> %327, <8 x i16> %266, <8 x i16> %326
  %329 = bitcast <2 x i64> %325 to <8 x i16>
  %330 = icmp sgt <8 x i16> %269, %329
  %331 = select <8 x i1> %330, <8 x i16> %269, <8 x i16> %329
  %332 = bitcast <8 x i16> %331 to <2 x i64>
  %333 = bitcast <8 x i16> %328 to <2 x i64>
  %334 = icmp slt <8 x i16> %271, %314
  %335 = select <8 x i1> %334, <8 x i16> %271, <8 x i16> %314
  %336 = icmp slt <8 x i16> %273, %317
  %337 = select <8 x i1> %336, <8 x i16> %273, <8 x i16> %317
  %338 = bitcast <8 x i16> %337 to <2 x i64>
  %339 = bitcast <8 x i16> %335 to <2 x i64>
  %340 = sub <8 x i16> %314, %102
  %341 = sub <8 x i16> %317, %106
  %342 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %341, <8 x i16> %340) #7
  %343 = ashr <16 x i8> %342, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %344 = icmp slt <16 x i8> %342, zeroinitializer
  %345 = sub <16 x i8> zeroinitializer, %342
  %346 = select <16 x i1> %344, <16 x i8> %345, <16 x i8> %342
  %347 = bitcast <16 x i8> %346 to <8 x i16>
  %348 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %347, <8 x i16> %130) #7
  %349 = bitcast <8 x i16> %348 to <16 x i8>
  %350 = and <16 x i8> %127, %349
  %351 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %123, <16 x i8> %350) #7
  %352 = icmp ult <16 x i8> %346, %351
  %353 = select <16 x i1> %352, <16 x i8> %346, <16 x i8> %351
  %354 = add <16 x i8> %353, %343
  %355 = xor <16 x i8> %354, %343
  %356 = getelementptr inbounds [2 x [2 x i32]], [2 x [2 x i32]]* @cdef_pri_taps, i64 0, i64 %31, i64 1
  %357 = load i32, i32* %356, align 4
  %358 = trunc i32 %357 to i8
  %359 = insertelement <16 x i8> undef, i8 %358, i32 0
  %360 = shufflevector <16 x i8> %359, <16 x i8> undef, <16 x i32> zeroinitializer
  %361 = shufflevector <16 x i8> %355, <16 x i8> %289, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %362 = shufflevector <16 x i8> %355, <16 x i8> %289, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %363 = shufflevector <16 x i8> %360, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %364 = shufflevector <16 x i8> %361, <16 x i8> undef, <16 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3, i32 4, i32 4, i32 5, i32 5, i32 6, i32 6, i32 7, i32 7>
  %365 = bitcast <16 x i8> %364 to <8 x i16>
  %366 = ashr <8 x i16> %365, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %367 = bitcast <16 x i8> %363 to <8 x i16>
  %368 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %367, <8 x i16> %366) #7
  %369 = shufflevector <16 x i8> %360, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %370 = shufflevector <16 x i8> %361, <16 x i8> undef, <16 x i32> <i32 8, i32 8, i32 9, i32 9, i32 10, i32 10, i32 11, i32 11, i32 12, i32 12, i32 13, i32 13, i32 14, i32 14, i32 15, i32 15>
  %371 = bitcast <16 x i8> %370 to <8 x i16>
  %372 = ashr <8 x i16> %371, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %373 = bitcast <16 x i8> %369 to <8 x i16>
  %374 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %373, <8 x i16> %372) #7
  %375 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %368, <4 x i32> %374) #7
  %376 = shufflevector <16 x i8> %362, <16 x i8> undef, <16 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3, i32 4, i32 4, i32 5, i32 5, i32 6, i32 6, i32 7, i32 7>
  %377 = bitcast <16 x i8> %376 to <8 x i16>
  %378 = ashr <8 x i16> %377, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %379 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %367, <8 x i16> %378) #7
  %380 = shufflevector <16 x i8> %362, <16 x i8> undef, <16 x i32> <i32 8, i32 8, i32 9, i32 9, i32 10, i32 10, i32 11, i32 11, i32 12, i32 12, i32 13, i32 13, i32 14, i32 14, i32 15, i32 15>
  %381 = bitcast <16 x i8> %380 to <8 x i16>
  %382 = ashr <8 x i16> %381, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %383 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %373, <8 x i16> %382) #7
  %384 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %379, <4 x i32> %383) #7
  %385 = add <8 x i16> %375, %219
  %386 = add <8 x i16> %384, %228
  br label %387

387:                                              ; preds = %66, %49
  %388 = phi <2 x i64> [ %338, %66 ], [ %65, %49 ]
  %389 = phi <2 x i64> [ %339, %66 ], [ %64, %49 ]
  %390 = phi <2 x i64> [ %332, %66 ], [ %65, %49 ]
  %391 = phi <2 x i64> [ %333, %66 ], [ %64, %49 ]
  %392 = phi <8 x i16> [ %386, %66 ], [ zeroinitializer, %49 ]
  %393 = phi <8 x i16> [ %385, %66 ], [ zeroinitializer, %49 ]
  br i1 %42, label %397, label %394

394:                                              ; preds = %387
  %395 = bitcast <2 x i64> %64 to <8 x i16>
  %396 = bitcast <2 x i64> %65 to <8 x i16>
  br label %973

397:                                              ; preds = %387
  %398 = sext i32 %19 to i64
  %399 = getelementptr inbounds i16, i16* %2, i64 %398
  %400 = bitcast i16* %399 to i64*
  %401 = load i64, i64* %400, align 1
  %402 = add nsw i32 %19, 144
  %403 = sext i32 %402 to i64
  %404 = getelementptr inbounds i16, i16* %2, i64 %403
  %405 = bitcast i16* %404 to i64*
  %406 = load i64, i64* %405, align 1
  %407 = insertelement <2 x i64> undef, i64 %406, i32 0
  %408 = add nsw i32 %19, 288
  %409 = sext i32 %408 to i64
  %410 = getelementptr inbounds i16, i16* %2, i64 %409
  %411 = bitcast i16* %410 to i64*
  %412 = load i64, i64* %411, align 1
  %413 = add nsw i32 %19, 432
  %414 = sext i32 %413 to i64
  %415 = getelementptr inbounds i16, i16* %2, i64 %414
  %416 = bitcast i16* %415 to i64*
  %417 = load i64, i64* %416, align 1
  %418 = insertelement <2 x i64> undef, i64 %417, i32 0
  %419 = insertelement <2 x i64> %407, i64 %401, i32 1
  %420 = insertelement <2 x i64> %418, i64 %412, i32 1
  %421 = bitcast <2 x i64> %419 to <8 x i16>
  %422 = icmp eq <8 x i16> %421, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %423 = sext <8 x i1> %422 to <8 x i16>
  %424 = bitcast <2 x i64> %420 to <8 x i16>
  %425 = icmp eq <8 x i16> %424, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %426 = sext <8 x i1> %425 to <8 x i16>
  %427 = bitcast <8 x i16> %426 to <2 x i64>
  %428 = bitcast <8 x i16> %423 to <2 x i64>
  %429 = xor <2 x i64> %428, <i64 -1, i64 -1>
  %430 = and <2 x i64> %419, %429
  %431 = xor <2 x i64> %427, <i64 -1, i64 -1>
  %432 = and <2 x i64> %420, %431
  %433 = bitcast <2 x i64> %391 to <8 x i16>
  %434 = bitcast <2 x i64> %430 to <8 x i16>
  %435 = icmp sgt <8 x i16> %433, %434
  %436 = select <8 x i1> %435, <8 x i16> %433, <8 x i16> %434
  %437 = bitcast <2 x i64> %390 to <8 x i16>
  %438 = bitcast <2 x i64> %432 to <8 x i16>
  %439 = icmp sgt <8 x i16> %437, %438
  %440 = select <8 x i1> %439, <8 x i16> %437, <8 x i16> %438
  %441 = bitcast <2 x i64> %389 to <8 x i16>
  %442 = icmp slt <8 x i16> %441, %421
  %443 = select <8 x i1> %442, <8 x i16> %441, <8 x i16> %421
  %444 = bitcast <2 x i64> %388 to <8 x i16>
  %445 = icmp slt <8 x i16> %444, %424
  %446 = select <8 x i1> %445, <8 x i16> %444, <8 x i16> %424
  %447 = bitcast <2 x i64> %65 to <8 x i16>
  %448 = bitcast <2 x i64> %64 to <8 x i16>
  %449 = sub <8 x i16> %421, %448
  %450 = sub <8 x i16> %424, %447
  %451 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %450, <8 x i16> %449) #7
  %452 = ashr <16 x i8> %451, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %453 = icmp slt <16 x i8> %451, zeroinitializer
  %454 = sub <16 x i8> zeroinitializer, %451
  %455 = select <16 x i1> %453, <16 x i8> %454, <16 x i8> %451
  %456 = trunc i32 %4 to i8
  %457 = insertelement <16 x i8> undef, i8 %456, i32 0
  %458 = shufflevector <16 x i8> %457, <16 x i8> undef, <16 x i32> zeroinitializer
  %459 = lshr i32 255, %50
  %460 = trunc i32 %459 to i8
  %461 = insertelement <16 x i8> undef, i8 %460, i32 0
  %462 = shufflevector <16 x i8> %461, <16 x i8> undef, <16 x i32> zeroinitializer
  %463 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %50, i32 0
  %464 = bitcast <16 x i8> %455 to <8 x i16>
  %465 = bitcast <4 x i32> %463 to <8 x i16>
  %466 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %464, <8 x i16> %465) #7
  %467 = bitcast <8 x i16> %466 to <16 x i8>
  %468 = and <16 x i8> %462, %467
  %469 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %458, <16 x i8> %468) #7
  %470 = icmp ult <16 x i8> %455, %469
  %471 = select <16 x i1> %470, <16 x i8> %455, <16 x i8> %469
  %472 = add <16 x i8> %471, %452
  %473 = xor <16 x i8> %472, %452
  %474 = sub nsw i32 0, %19
  %475 = sext i32 %474 to i64
  %476 = getelementptr inbounds i16, i16* %2, i64 %475
  %477 = bitcast i16* %476 to i64*
  %478 = load i64, i64* %477, align 1
  %479 = sub nsw i32 144, %19
  %480 = sext i32 %479 to i64
  %481 = getelementptr inbounds i16, i16* %2, i64 %480
  %482 = bitcast i16* %481 to i64*
  %483 = load i64, i64* %482, align 1
  %484 = insertelement <2 x i64> undef, i64 %483, i32 0
  %485 = sub nsw i32 288, %19
  %486 = sext i32 %485 to i64
  %487 = getelementptr inbounds i16, i16* %2, i64 %486
  %488 = bitcast i16* %487 to i64*
  %489 = load i64, i64* %488, align 1
  %490 = sub nsw i32 432, %19
  %491 = sext i32 %490 to i64
  %492 = getelementptr inbounds i16, i16* %2, i64 %491
  %493 = bitcast i16* %492 to i64*
  %494 = load i64, i64* %493, align 1
  %495 = insertelement <2 x i64> undef, i64 %494, i32 0
  %496 = insertelement <2 x i64> %484, i64 %478, i32 1
  %497 = insertelement <2 x i64> %495, i64 %489, i32 1
  %498 = bitcast <2 x i64> %496 to <8 x i16>
  %499 = icmp eq <8 x i16> %498, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %500 = sext <8 x i1> %499 to <8 x i16>
  %501 = bitcast <2 x i64> %497 to <8 x i16>
  %502 = icmp eq <8 x i16> %501, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %503 = sext <8 x i1> %502 to <8 x i16>
  %504 = bitcast <8 x i16> %503 to <2 x i64>
  %505 = bitcast <8 x i16> %500 to <2 x i64>
  %506 = xor <2 x i64> %505, <i64 -1, i64 -1>
  %507 = and <2 x i64> %496, %506
  %508 = xor <2 x i64> %504, <i64 -1, i64 -1>
  %509 = and <2 x i64> %497, %508
  %510 = bitcast <2 x i64> %507 to <8 x i16>
  %511 = icmp sgt <8 x i16> %436, %510
  %512 = select <8 x i1> %511, <8 x i16> %436, <8 x i16> %510
  %513 = bitcast <2 x i64> %509 to <8 x i16>
  %514 = icmp sgt <8 x i16> %440, %513
  %515 = select <8 x i1> %514, <8 x i16> %440, <8 x i16> %513
  %516 = icmp slt <8 x i16> %443, %498
  %517 = select <8 x i1> %516, <8 x i16> %443, <8 x i16> %498
  %518 = icmp slt <8 x i16> %446, %501
  %519 = select <8 x i1> %518, <8 x i16> %446, <8 x i16> %501
  %520 = sub <8 x i16> %498, %448
  %521 = sub <8 x i16> %501, %447
  %522 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %521, <8 x i16> %520) #7
  %523 = ashr <16 x i8> %522, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %524 = icmp slt <16 x i8> %522, zeroinitializer
  %525 = sub <16 x i8> zeroinitializer, %522
  %526 = select <16 x i1> %524, <16 x i8> %525, <16 x i8> %522
  %527 = bitcast <16 x i8> %526 to <8 x i16>
  %528 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %527, <8 x i16> %465) #7
  %529 = bitcast <8 x i16> %528 to <16 x i8>
  %530 = and <16 x i8> %462, %529
  %531 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %458, <16 x i8> %530) #7
  %532 = icmp ult <16 x i8> %526, %531
  %533 = select <16 x i1> %532, <16 x i8> %526, <16 x i8> %531
  %534 = add <16 x i8> %533, %523
  %535 = xor <16 x i8> %534, %523
  %536 = sext i32 %26 to i64
  %537 = getelementptr inbounds i16, i16* %2, i64 %536
  %538 = bitcast i16* %537 to i64*
  %539 = load i64, i64* %538, align 1
  %540 = add nsw i32 %26, 144
  %541 = sext i32 %540 to i64
  %542 = getelementptr inbounds i16, i16* %2, i64 %541
  %543 = bitcast i16* %542 to i64*
  %544 = load i64, i64* %543, align 1
  %545 = insertelement <2 x i64> undef, i64 %544, i32 0
  %546 = add nsw i32 %26, 288
  %547 = sext i32 %546 to i64
  %548 = getelementptr inbounds i16, i16* %2, i64 %547
  %549 = bitcast i16* %548 to i64*
  %550 = load i64, i64* %549, align 1
  %551 = add nsw i32 %26, 432
  %552 = sext i32 %551 to i64
  %553 = getelementptr inbounds i16, i16* %2, i64 %552
  %554 = bitcast i16* %553 to i64*
  %555 = load i64, i64* %554, align 1
  %556 = insertelement <2 x i64> undef, i64 %555, i32 0
  %557 = insertelement <2 x i64> %545, i64 %539, i32 1
  %558 = insertelement <2 x i64> %556, i64 %550, i32 1
  %559 = bitcast <2 x i64> %557 to <8 x i16>
  %560 = icmp eq <8 x i16> %559, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %561 = sext <8 x i1> %560 to <8 x i16>
  %562 = bitcast <2 x i64> %558 to <8 x i16>
  %563 = icmp eq <8 x i16> %562, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %564 = sext <8 x i1> %563 to <8 x i16>
  %565 = bitcast <8 x i16> %564 to <2 x i64>
  %566 = bitcast <8 x i16> %561 to <2 x i64>
  %567 = xor <2 x i64> %566, <i64 -1, i64 -1>
  %568 = and <2 x i64> %557, %567
  %569 = xor <2 x i64> %565, <i64 -1, i64 -1>
  %570 = and <2 x i64> %558, %569
  %571 = bitcast <2 x i64> %568 to <8 x i16>
  %572 = icmp sgt <8 x i16> %512, %571
  %573 = select <8 x i1> %572, <8 x i16> %512, <8 x i16> %571
  %574 = bitcast <2 x i64> %570 to <8 x i16>
  %575 = icmp sgt <8 x i16> %515, %574
  %576 = select <8 x i1> %575, <8 x i16> %515, <8 x i16> %574
  %577 = icmp slt <8 x i16> %517, %559
  %578 = select <8 x i1> %577, <8 x i16> %517, <8 x i16> %559
  %579 = icmp slt <8 x i16> %519, %562
  %580 = select <8 x i1> %579, <8 x i16> %519, <8 x i16> %562
  %581 = sub <8 x i16> %559, %448
  %582 = sub <8 x i16> %562, %447
  %583 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %582, <8 x i16> %581) #7
  %584 = ashr <16 x i8> %583, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %585 = icmp slt <16 x i8> %583, zeroinitializer
  %586 = sub <16 x i8> zeroinitializer, %583
  %587 = select <16 x i1> %585, <16 x i8> %586, <16 x i8> %583
  %588 = bitcast <16 x i8> %587 to <8 x i16>
  %589 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %588, <8 x i16> %465) #7
  %590 = bitcast <8 x i16> %589 to <16 x i8>
  %591 = and <16 x i8> %462, %590
  %592 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %458, <16 x i8> %591) #7
  %593 = icmp ult <16 x i8> %587, %592
  %594 = select <16 x i1> %593, <16 x i8> %587, <16 x i8> %592
  %595 = add <16 x i8> %594, %584
  %596 = xor <16 x i8> %595, %584
  %597 = sub nsw i32 0, %26
  %598 = sext i32 %597 to i64
  %599 = getelementptr inbounds i16, i16* %2, i64 %598
  %600 = bitcast i16* %599 to i64*
  %601 = load i64, i64* %600, align 1
  %602 = sub nsw i32 144, %26
  %603 = sext i32 %602 to i64
  %604 = getelementptr inbounds i16, i16* %2, i64 %603
  %605 = bitcast i16* %604 to i64*
  %606 = load i64, i64* %605, align 1
  %607 = insertelement <2 x i64> undef, i64 %606, i32 0
  %608 = sub nsw i32 288, %26
  %609 = sext i32 %608 to i64
  %610 = getelementptr inbounds i16, i16* %2, i64 %609
  %611 = bitcast i16* %610 to i64*
  %612 = load i64, i64* %611, align 1
  %613 = sub nsw i32 432, %26
  %614 = sext i32 %613 to i64
  %615 = getelementptr inbounds i16, i16* %2, i64 %614
  %616 = bitcast i16* %615 to i64*
  %617 = load i64, i64* %616, align 1
  %618 = insertelement <2 x i64> undef, i64 %617, i32 0
  %619 = insertelement <2 x i64> %607, i64 %601, i32 1
  %620 = insertelement <2 x i64> %618, i64 %612, i32 1
  %621 = bitcast <2 x i64> %619 to <8 x i16>
  %622 = icmp eq <8 x i16> %621, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %623 = sext <8 x i1> %622 to <8 x i16>
  %624 = bitcast <2 x i64> %620 to <8 x i16>
  %625 = icmp eq <8 x i16> %624, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %626 = sext <8 x i1> %625 to <8 x i16>
  %627 = bitcast <8 x i16> %626 to <2 x i64>
  %628 = bitcast <8 x i16> %623 to <2 x i64>
  %629 = xor <2 x i64> %628, <i64 -1, i64 -1>
  %630 = and <2 x i64> %619, %629
  %631 = xor <2 x i64> %627, <i64 -1, i64 -1>
  %632 = and <2 x i64> %620, %631
  %633 = bitcast <2 x i64> %630 to <8 x i16>
  %634 = icmp sgt <8 x i16> %573, %633
  %635 = select <8 x i1> %634, <8 x i16> %573, <8 x i16> %633
  %636 = bitcast <2 x i64> %632 to <8 x i16>
  %637 = icmp sgt <8 x i16> %576, %636
  %638 = select <8 x i1> %637, <8 x i16> %576, <8 x i16> %636
  %639 = icmp slt <8 x i16> %578, %621
  %640 = select <8 x i1> %639, <8 x i16> %578, <8 x i16> %621
  %641 = icmp slt <8 x i16> %580, %624
  %642 = select <8 x i1> %641, <8 x i16> %580, <8 x i16> %624
  %643 = sub <8 x i16> %621, %448
  %644 = sub <8 x i16> %624, %447
  %645 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %644, <8 x i16> %643) #7
  %646 = ashr <16 x i8> %645, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %647 = icmp slt <16 x i8> %645, zeroinitializer
  %648 = sub <16 x i8> zeroinitializer, %645
  %649 = select <16 x i1> %647, <16 x i8> %648, <16 x i8> %645
  %650 = bitcast <16 x i8> %649 to <8 x i16>
  %651 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %650, <8 x i16> %465) #7
  %652 = bitcast <8 x i16> %651 to <16 x i8>
  %653 = and <16 x i8> %462, %652
  %654 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %458, <16 x i8> %653) #7
  %655 = icmp ult <16 x i8> %649, %654
  %656 = select <16 x i1> %655, <16 x i8> %649, <16 x i8> %654
  %657 = add <16 x i8> %656, %646
  %658 = xor <16 x i8> %657, %646
  %659 = add <16 x i8> %535, %473
  %660 = add <16 x i8> %658, %596
  %661 = load i32, i32* getelementptr inbounds ([2 x i32], [2 x i32]* @cdef_sec_taps, i64 0, i64 0), align 4
  %662 = trunc i32 %661 to i8
  %663 = insertelement <16 x i8> undef, i8 %662, i32 0
  %664 = shufflevector <16 x i8> %663, <16 x i8> undef, <16 x i32> zeroinitializer
  %665 = shufflevector <16 x i8> %660, <16 x i8> %659, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %666 = shufflevector <16 x i8> %660, <16 x i8> %659, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %667 = shufflevector <16 x i8> %664, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %668 = shufflevector <16 x i8> %665, <16 x i8> undef, <16 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3, i32 4, i32 4, i32 5, i32 5, i32 6, i32 6, i32 7, i32 7>
  %669 = bitcast <16 x i8> %668 to <8 x i16>
  %670 = ashr <8 x i16> %669, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %671 = bitcast <16 x i8> %667 to <8 x i16>
  %672 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %671, <8 x i16> %670) #7
  %673 = shufflevector <16 x i8> %664, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %674 = shufflevector <16 x i8> %665, <16 x i8> undef, <16 x i32> <i32 8, i32 8, i32 9, i32 9, i32 10, i32 10, i32 11, i32 11, i32 12, i32 12, i32 13, i32 13, i32 14, i32 14, i32 15, i32 15>
  %675 = bitcast <16 x i8> %674 to <8 x i16>
  %676 = ashr <8 x i16> %675, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %677 = bitcast <16 x i8> %673 to <8 x i16>
  %678 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %677, <8 x i16> %676) #7
  %679 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %672, <4 x i32> %678) #7
  %680 = shufflevector <16 x i8> %666, <16 x i8> undef, <16 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3, i32 4, i32 4, i32 5, i32 5, i32 6, i32 6, i32 7, i32 7>
  %681 = bitcast <16 x i8> %680 to <8 x i16>
  %682 = ashr <8 x i16> %681, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %683 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %671, <8 x i16> %682) #7
  %684 = shufflevector <16 x i8> %666, <16 x i8> undef, <16 x i32> <i32 8, i32 8, i32 9, i32 9, i32 10, i32 10, i32 11, i32 11, i32 12, i32 12, i32 13, i32 13, i32 14, i32 14, i32 15, i32 15>
  %685 = bitcast <16 x i8> %684 to <8 x i16>
  %686 = ashr <8 x i16> %685, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %687 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %677, <8 x i16> %686) #7
  %688 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %683, <4 x i32> %687) #7
  %689 = add <8 x i16> %679, %393
  %690 = add <8 x i16> %688, %392
  %691 = sext i32 %21 to i64
  %692 = getelementptr inbounds i16, i16* %2, i64 %691
  %693 = bitcast i16* %692 to i64*
  %694 = load i64, i64* %693, align 1
  %695 = add nsw i32 %21, 144
  %696 = sext i32 %695 to i64
  %697 = getelementptr inbounds i16, i16* %2, i64 %696
  %698 = bitcast i16* %697 to i64*
  %699 = load i64, i64* %698, align 1
  %700 = insertelement <2 x i64> undef, i64 %699, i32 0
  %701 = add nsw i32 %21, 288
  %702 = sext i32 %701 to i64
  %703 = getelementptr inbounds i16, i16* %2, i64 %702
  %704 = bitcast i16* %703 to i64*
  %705 = load i64, i64* %704, align 1
  %706 = add nsw i32 %21, 432
  %707 = sext i32 %706 to i64
  %708 = getelementptr inbounds i16, i16* %2, i64 %707
  %709 = bitcast i16* %708 to i64*
  %710 = load i64, i64* %709, align 1
  %711 = insertelement <2 x i64> undef, i64 %710, i32 0
  %712 = insertelement <2 x i64> %700, i64 %694, i32 1
  %713 = insertelement <2 x i64> %711, i64 %705, i32 1
  %714 = bitcast <2 x i64> %712 to <8 x i16>
  %715 = icmp eq <8 x i16> %714, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %716 = sext <8 x i1> %715 to <8 x i16>
  %717 = bitcast <2 x i64> %713 to <8 x i16>
  %718 = icmp eq <8 x i16> %717, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %719 = sext <8 x i1> %718 to <8 x i16>
  %720 = bitcast <8 x i16> %719 to <2 x i64>
  %721 = bitcast <8 x i16> %716 to <2 x i64>
  %722 = xor <2 x i64> %721, <i64 -1, i64 -1>
  %723 = and <2 x i64> %712, %722
  %724 = xor <2 x i64> %720, <i64 -1, i64 -1>
  %725 = and <2 x i64> %713, %724
  %726 = bitcast <2 x i64> %723 to <8 x i16>
  %727 = icmp sgt <8 x i16> %635, %726
  %728 = select <8 x i1> %727, <8 x i16> %635, <8 x i16> %726
  %729 = bitcast <2 x i64> %725 to <8 x i16>
  %730 = icmp sgt <8 x i16> %638, %729
  %731 = select <8 x i1> %730, <8 x i16> %638, <8 x i16> %729
  %732 = icmp slt <8 x i16> %640, %714
  %733 = select <8 x i1> %732, <8 x i16> %640, <8 x i16> %714
  %734 = icmp slt <8 x i16> %642, %717
  %735 = select <8 x i1> %734, <8 x i16> %642, <8 x i16> %717
  %736 = sub <8 x i16> %714, %448
  %737 = sub <8 x i16> %717, %447
  %738 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %737, <8 x i16> %736) #7
  %739 = ashr <16 x i8> %738, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %740 = icmp slt <16 x i8> %738, zeroinitializer
  %741 = sub <16 x i8> zeroinitializer, %738
  %742 = select <16 x i1> %740, <16 x i8> %741, <16 x i8> %738
  %743 = bitcast <16 x i8> %742 to <8 x i16>
  %744 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %743, <8 x i16> %465) #7
  %745 = bitcast <8 x i16> %744 to <16 x i8>
  %746 = and <16 x i8> %462, %745
  %747 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %458, <16 x i8> %746) #7
  %748 = icmp ult <16 x i8> %742, %747
  %749 = select <16 x i1> %748, <16 x i8> %742, <16 x i8> %747
  %750 = add <16 x i8> %749, %739
  %751 = xor <16 x i8> %750, %739
  %752 = sub nsw i32 0, %21
  %753 = sext i32 %752 to i64
  %754 = getelementptr inbounds i16, i16* %2, i64 %753
  %755 = bitcast i16* %754 to i64*
  %756 = load i64, i64* %755, align 1
  %757 = sub nsw i32 144, %21
  %758 = sext i32 %757 to i64
  %759 = getelementptr inbounds i16, i16* %2, i64 %758
  %760 = bitcast i16* %759 to i64*
  %761 = load i64, i64* %760, align 1
  %762 = insertelement <2 x i64> undef, i64 %761, i32 0
  %763 = sub nsw i32 288, %21
  %764 = sext i32 %763 to i64
  %765 = getelementptr inbounds i16, i16* %2, i64 %764
  %766 = bitcast i16* %765 to i64*
  %767 = load i64, i64* %766, align 1
  %768 = sub nsw i32 432, %21
  %769 = sext i32 %768 to i64
  %770 = getelementptr inbounds i16, i16* %2, i64 %769
  %771 = bitcast i16* %770 to i64*
  %772 = load i64, i64* %771, align 1
  %773 = insertelement <2 x i64> undef, i64 %772, i32 0
  %774 = insertelement <2 x i64> %762, i64 %756, i32 1
  %775 = insertelement <2 x i64> %773, i64 %767, i32 1
  %776 = bitcast <2 x i64> %774 to <8 x i16>
  %777 = icmp eq <8 x i16> %776, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %778 = sext <8 x i1> %777 to <8 x i16>
  %779 = bitcast <2 x i64> %775 to <8 x i16>
  %780 = icmp eq <8 x i16> %779, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %781 = sext <8 x i1> %780 to <8 x i16>
  %782 = bitcast <8 x i16> %781 to <2 x i64>
  %783 = bitcast <8 x i16> %778 to <2 x i64>
  %784 = xor <2 x i64> %783, <i64 -1, i64 -1>
  %785 = and <2 x i64> %774, %784
  %786 = xor <2 x i64> %782, <i64 -1, i64 -1>
  %787 = and <2 x i64> %775, %786
  %788 = bitcast <2 x i64> %785 to <8 x i16>
  %789 = icmp sgt <8 x i16> %728, %788
  %790 = select <8 x i1> %789, <8 x i16> %728, <8 x i16> %788
  %791 = bitcast <2 x i64> %787 to <8 x i16>
  %792 = icmp sgt <8 x i16> %731, %791
  %793 = select <8 x i1> %792, <8 x i16> %731, <8 x i16> %791
  %794 = icmp slt <8 x i16> %733, %776
  %795 = select <8 x i1> %794, <8 x i16> %733, <8 x i16> %776
  %796 = icmp slt <8 x i16> %735, %779
  %797 = select <8 x i1> %796, <8 x i16> %735, <8 x i16> %779
  %798 = sub <8 x i16> %776, %448
  %799 = sub <8 x i16> %779, %447
  %800 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %799, <8 x i16> %798) #7
  %801 = ashr <16 x i8> %800, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %802 = icmp slt <16 x i8> %800, zeroinitializer
  %803 = sub <16 x i8> zeroinitializer, %800
  %804 = select <16 x i1> %802, <16 x i8> %803, <16 x i8> %800
  %805 = bitcast <16 x i8> %804 to <8 x i16>
  %806 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %805, <8 x i16> %465) #7
  %807 = bitcast <8 x i16> %806 to <16 x i8>
  %808 = and <16 x i8> %462, %807
  %809 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %458, <16 x i8> %808) #7
  %810 = icmp ult <16 x i8> %804, %809
  %811 = select <16 x i1> %810, <16 x i8> %804, <16 x i8> %809
  %812 = add <16 x i8> %811, %801
  %813 = xor <16 x i8> %812, %801
  %814 = sext i32 %28 to i64
  %815 = getelementptr inbounds i16, i16* %2, i64 %814
  %816 = bitcast i16* %815 to i64*
  %817 = load i64, i64* %816, align 1
  %818 = add nsw i32 %28, 144
  %819 = sext i32 %818 to i64
  %820 = getelementptr inbounds i16, i16* %2, i64 %819
  %821 = bitcast i16* %820 to i64*
  %822 = load i64, i64* %821, align 1
  %823 = insertelement <2 x i64> undef, i64 %822, i32 0
  %824 = add nsw i32 %28, 288
  %825 = sext i32 %824 to i64
  %826 = getelementptr inbounds i16, i16* %2, i64 %825
  %827 = bitcast i16* %826 to i64*
  %828 = load i64, i64* %827, align 1
  %829 = add nsw i32 %28, 432
  %830 = sext i32 %829 to i64
  %831 = getelementptr inbounds i16, i16* %2, i64 %830
  %832 = bitcast i16* %831 to i64*
  %833 = load i64, i64* %832, align 1
  %834 = insertelement <2 x i64> undef, i64 %833, i32 0
  %835 = insertelement <2 x i64> %823, i64 %817, i32 1
  %836 = insertelement <2 x i64> %834, i64 %828, i32 1
  %837 = bitcast <2 x i64> %835 to <8 x i16>
  %838 = icmp eq <8 x i16> %837, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %839 = sext <8 x i1> %838 to <8 x i16>
  %840 = bitcast <2 x i64> %836 to <8 x i16>
  %841 = icmp eq <8 x i16> %840, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %842 = sext <8 x i1> %841 to <8 x i16>
  %843 = bitcast <8 x i16> %842 to <2 x i64>
  %844 = bitcast <8 x i16> %839 to <2 x i64>
  %845 = xor <2 x i64> %844, <i64 -1, i64 -1>
  %846 = and <2 x i64> %835, %845
  %847 = xor <2 x i64> %843, <i64 -1, i64 -1>
  %848 = and <2 x i64> %836, %847
  %849 = bitcast <2 x i64> %846 to <8 x i16>
  %850 = icmp sgt <8 x i16> %790, %849
  %851 = select <8 x i1> %850, <8 x i16> %790, <8 x i16> %849
  %852 = bitcast <2 x i64> %848 to <8 x i16>
  %853 = icmp sgt <8 x i16> %793, %852
  %854 = select <8 x i1> %853, <8 x i16> %793, <8 x i16> %852
  %855 = icmp slt <8 x i16> %795, %837
  %856 = select <8 x i1> %855, <8 x i16> %795, <8 x i16> %837
  %857 = icmp slt <8 x i16> %797, %840
  %858 = select <8 x i1> %857, <8 x i16> %797, <8 x i16> %840
  %859 = sub <8 x i16> %837, %448
  %860 = sub <8 x i16> %840, %447
  %861 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %860, <8 x i16> %859) #7
  %862 = ashr <16 x i8> %861, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %863 = icmp slt <16 x i8> %861, zeroinitializer
  %864 = sub <16 x i8> zeroinitializer, %861
  %865 = select <16 x i1> %863, <16 x i8> %864, <16 x i8> %861
  %866 = bitcast <16 x i8> %865 to <8 x i16>
  %867 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %866, <8 x i16> %465) #7
  %868 = bitcast <8 x i16> %867 to <16 x i8>
  %869 = and <16 x i8> %462, %868
  %870 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %458, <16 x i8> %869) #7
  %871 = icmp ult <16 x i8> %865, %870
  %872 = select <16 x i1> %871, <16 x i8> %865, <16 x i8> %870
  %873 = add <16 x i8> %872, %862
  %874 = xor <16 x i8> %873, %862
  %875 = sub nsw i32 0, %28
  %876 = sext i32 %875 to i64
  %877 = getelementptr inbounds i16, i16* %2, i64 %876
  %878 = bitcast i16* %877 to i64*
  %879 = load i64, i64* %878, align 1
  %880 = sub nsw i32 144, %28
  %881 = sext i32 %880 to i64
  %882 = getelementptr inbounds i16, i16* %2, i64 %881
  %883 = bitcast i16* %882 to i64*
  %884 = load i64, i64* %883, align 1
  %885 = insertelement <2 x i64> undef, i64 %884, i32 0
  %886 = sub nsw i32 288, %28
  %887 = sext i32 %886 to i64
  %888 = getelementptr inbounds i16, i16* %2, i64 %887
  %889 = bitcast i16* %888 to i64*
  %890 = load i64, i64* %889, align 1
  %891 = sub nsw i32 432, %28
  %892 = sext i32 %891 to i64
  %893 = getelementptr inbounds i16, i16* %2, i64 %892
  %894 = bitcast i16* %893 to i64*
  %895 = load i64, i64* %894, align 1
  %896 = insertelement <2 x i64> undef, i64 %895, i32 0
  %897 = insertelement <2 x i64> %885, i64 %879, i32 1
  %898 = insertelement <2 x i64> %896, i64 %890, i32 1
  %899 = bitcast <2 x i64> %897 to <8 x i16>
  %900 = icmp eq <8 x i16> %899, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %901 = sext <8 x i1> %900 to <8 x i16>
  %902 = bitcast <2 x i64> %898 to <8 x i16>
  %903 = icmp eq <8 x i16> %902, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %904 = sext <8 x i1> %903 to <8 x i16>
  %905 = bitcast <8 x i16> %904 to <2 x i64>
  %906 = bitcast <8 x i16> %901 to <2 x i64>
  %907 = xor <2 x i64> %906, <i64 -1, i64 -1>
  %908 = and <2 x i64> %897, %907
  %909 = xor <2 x i64> %905, <i64 -1, i64 -1>
  %910 = and <2 x i64> %898, %909
  %911 = bitcast <2 x i64> %908 to <8 x i16>
  %912 = icmp sgt <8 x i16> %851, %911
  %913 = select <8 x i1> %912, <8 x i16> %851, <8 x i16> %911
  %914 = bitcast <2 x i64> %910 to <8 x i16>
  %915 = icmp sgt <8 x i16> %854, %914
  %916 = select <8 x i1> %915, <8 x i16> %854, <8 x i16> %914
  %917 = bitcast <8 x i16> %916 to <2 x i64>
  %918 = bitcast <8 x i16> %913 to <2 x i64>
  %919 = icmp slt <8 x i16> %856, %899
  %920 = select <8 x i1> %919, <8 x i16> %856, <8 x i16> %899
  %921 = icmp slt <8 x i16> %858, %902
  %922 = select <8 x i1> %921, <8 x i16> %858, <8 x i16> %902
  %923 = bitcast <8 x i16> %922 to <2 x i64>
  %924 = bitcast <8 x i16> %920 to <2 x i64>
  %925 = sub <8 x i16> %899, %448
  %926 = sub <8 x i16> %902, %447
  %927 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %926, <8 x i16> %925) #7
  %928 = ashr <16 x i8> %927, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %929 = icmp slt <16 x i8> %927, zeroinitializer
  %930 = sub <16 x i8> zeroinitializer, %927
  %931 = select <16 x i1> %929, <16 x i8> %930, <16 x i8> %927
  %932 = bitcast <16 x i8> %931 to <8 x i16>
  %933 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %932, <8 x i16> %465) #7
  %934 = bitcast <8 x i16> %933 to <16 x i8>
  %935 = and <16 x i8> %462, %934
  %936 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %458, <16 x i8> %935) #7
  %937 = icmp ult <16 x i8> %931, %936
  %938 = select <16 x i1> %937, <16 x i8> %931, <16 x i8> %936
  %939 = add <16 x i8> %938, %928
  %940 = xor <16 x i8> %939, %928
  %941 = add <16 x i8> %813, %751
  %942 = add <16 x i8> %940, %874
  %943 = load i32, i32* getelementptr inbounds ([2 x i32], [2 x i32]* @cdef_sec_taps, i64 0, i64 1), align 4
  %944 = trunc i32 %943 to i8
  %945 = insertelement <16 x i8> undef, i8 %944, i32 0
  %946 = shufflevector <16 x i8> %945, <16 x i8> undef, <16 x i32> zeroinitializer
  %947 = shufflevector <16 x i8> %942, <16 x i8> %941, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %948 = shufflevector <16 x i8> %942, <16 x i8> %941, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %949 = shufflevector <16 x i8> %946, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %950 = shufflevector <16 x i8> %947, <16 x i8> undef, <16 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3, i32 4, i32 4, i32 5, i32 5, i32 6, i32 6, i32 7, i32 7>
  %951 = bitcast <16 x i8> %950 to <8 x i16>
  %952 = ashr <8 x i16> %951, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %953 = bitcast <16 x i8> %949 to <8 x i16>
  %954 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %953, <8 x i16> %952) #7
  %955 = shufflevector <16 x i8> %946, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %956 = shufflevector <16 x i8> %947, <16 x i8> undef, <16 x i32> <i32 8, i32 8, i32 9, i32 9, i32 10, i32 10, i32 11, i32 11, i32 12, i32 12, i32 13, i32 13, i32 14, i32 14, i32 15, i32 15>
  %957 = bitcast <16 x i8> %956 to <8 x i16>
  %958 = ashr <8 x i16> %957, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %959 = bitcast <16 x i8> %955 to <8 x i16>
  %960 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %959, <8 x i16> %958) #7
  %961 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %954, <4 x i32> %960) #7
  %962 = shufflevector <16 x i8> %948, <16 x i8> undef, <16 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3, i32 4, i32 4, i32 5, i32 5, i32 6, i32 6, i32 7, i32 7>
  %963 = bitcast <16 x i8> %962 to <8 x i16>
  %964 = ashr <8 x i16> %963, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %965 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %953, <8 x i16> %964) #7
  %966 = shufflevector <16 x i8> %948, <16 x i8> undef, <16 x i32> <i32 8, i32 8, i32 9, i32 9, i32 10, i32 10, i32 11, i32 11, i32 12, i32 12, i32 13, i32 13, i32 14, i32 14, i32 15, i32 15>
  %967 = bitcast <16 x i8> %966 to <8 x i16>
  %968 = ashr <8 x i16> %967, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %969 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %959, <8 x i16> %968) #7
  %970 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %965, <4 x i32> %969) #7
  %971 = add <8 x i16> %689, %961
  %972 = add <8 x i16> %690, %970
  br label %973

973:                                              ; preds = %394, %397
  %974 = phi <8 x i16> [ %396, %394 ], [ %447, %397 ]
  %975 = phi <8 x i16> [ %395, %394 ], [ %448, %397 ]
  %976 = phi <2 x i64> [ %388, %394 ], [ %923, %397 ]
  %977 = phi <2 x i64> [ %389, %394 ], [ %924, %397 ]
  %978 = phi <2 x i64> [ %390, %394 ], [ %917, %397 ]
  %979 = phi <2 x i64> [ %391, %394 ], [ %918, %397 ]
  %980 = phi <8 x i16> [ %392, %394 ], [ %972, %397 ]
  %981 = phi <8 x i16> [ %393, %394 ], [ %971, %397 ]
  %982 = ashr <8 x i16> %981, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %983 = ashr <8 x i16> %980, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %984 = add <8 x i16> %981, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %985 = add <8 x i16> %984, %982
  %986 = add <8 x i16> %980, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %987 = add <8 x i16> %986, %983
  %988 = ashr <8 x i16> %985, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %989 = ashr <8 x i16> %987, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %990 = add <8 x i16> %988, %975
  %991 = add <8 x i16> %989, %974
  %992 = bitcast <2 x i64> %977 to <8 x i16>
  %993 = icmp sgt <8 x i16> %990, %992
  %994 = select <8 x i1> %993, <8 x i16> %990, <8 x i16> %992
  %995 = bitcast <2 x i64> %976 to <8 x i16>
  %996 = icmp sgt <8 x i16> %991, %995
  %997 = select <8 x i1> %996, <8 x i16> %991, <8 x i16> %995
  %998 = bitcast <2 x i64> %979 to <8 x i16>
  %999 = icmp slt <8 x i16> %994, %998
  %1000 = select <8 x i1> %999, <8 x i16> %994, <8 x i16> %998
  %1001 = bitcast <2 x i64> %978 to <8 x i16>
  %1002 = icmp slt <8 x i16> %997, %1001
  %1003 = select <8 x i1> %1002, <8 x i16> %997, <8 x i16> %1001
  %1004 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %1003, <8 x i16> %1000) #7
  %1005 = bitcast <16 x i8> %1004 to <2 x i64>
  %1006 = shufflevector <16 x i8> %1004, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23>
  %1007 = shufflevector <16 x i8> %1006, <16 x i8> undef, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %1008 = bitcast <16 x i8> %1007 to <4 x i32>
  %1009 = extractelement <4 x i32> %1008, i32 0
  %1010 = bitcast i8* %0 to i32*
  store i32 %1009, i32* %1010, align 4
  %1011 = sext i32 %1 to i64
  %1012 = getelementptr inbounds i8, i8* %0, i64 %1011
  %1013 = bitcast <16 x i8> %1006 to <4 x i32>
  %1014 = extractelement <4 x i32> %1013, i32 0
  %1015 = bitcast i8* %1012 to i32*
  store i32 %1014, i32* %1015, align 4
  %1016 = shl nsw i32 %1, 1
  %1017 = sext i32 %1016 to i64
  %1018 = getelementptr inbounds i8, i8* %0, i64 %1017
  %1019 = insertelement <2 x i64> %1005, i64 0, i32 1
  %1020 = bitcast <2 x i64> %1019 to <16 x i8>
  %1021 = shufflevector <16 x i8> %1020, <16 x i8> undef, <16 x i32> <i32 4, i32 5, i32 6, i32 7, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %1022 = bitcast <16 x i8> %1021 to <4 x i32>
  %1023 = extractelement <4 x i32> %1022, i32 0
  %1024 = bitcast i8* %1018 to i32*
  store i32 %1023, i32* %1024, align 4
  %1025 = mul nsw i32 %1, 3
  %1026 = sext i32 %1025 to i64
  %1027 = getelementptr inbounds i8, i8* %0, i64 %1026
  %1028 = bitcast <2 x i64> %1019 to <4 x i32>
  %1029 = extractelement <4 x i32> %1028, i32 0
  %1030 = bitcast i8* %1027 to i32*
  store i32 %1029, i32* %1030, align 4
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @cdef_filter_block_8x8_8_sse2(i8* nocapture, i32, i16* readonly, i32, i32, i32, i32, i32, i32) local_unnamed_addr #0 {
  %10 = sext i32 %5 to i64
  %11 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %10, i64 0
  %12 = load i32, i32* %11, align 8
  %13 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %10, i64 1
  %14 = load i32, i32* %13, align 4
  %15 = add nsw i32 %5, 2
  %16 = and i32 %15, 7
  %17 = zext i32 %16 to i64
  %18 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %17, i64 0
  %19 = load i32, i32* %18, align 8
  %20 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %17, i64 1
  %21 = load i32, i32* %20, align 4
  %22 = add nsw i32 %5, 6
  %23 = and i32 %22, 7
  %24 = zext i32 %23 to i64
  %25 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %24, i64 0
  %26 = load i32, i32* %25, align 8
  %27 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %24, i64 1
  %28 = load i32, i32* %27, align 4
  %29 = lshr i32 %3, %8
  %30 = and i32 %29, 1
  %31 = zext i32 %30 to i64
  %32 = getelementptr inbounds [2 x [2 x i32]], [2 x [2 x i32]]* @cdef_pri_taps, i64 0, i64 %31, i64 0
  %33 = icmp eq i32 %3, 0
  br i1 %33, label %40, label %34

34:                                               ; preds = %9
  %35 = tail call i32 @llvm.ctlz.i32(i32 %3, i1 true) #7, !range !2
  %36 = xor i32 %35, 31
  %37 = icmp sgt i32 %36, %6
  %38 = sub nsw i32 %6, %36
  %39 = select i1 %37, i32 0, i32 %38
  br label %40

40:                                               ; preds = %34, %9
  %41 = phi i32 [ %6, %9 ], [ %39, %34 ]
  %42 = icmp eq i32 %4, 0
  br i1 %42, label %49, label %43

43:                                               ; preds = %40
  %44 = tail call i32 @llvm.ctlz.i32(i32 %4, i1 true) #7, !range !2
  %45 = xor i32 %44, 31
  %46 = icmp sgt i32 %45, %7
  %47 = sub nsw i32 %7, %45
  %48 = select i1 %46, i32 0, i32 %47
  br label %49

49:                                               ; preds = %43, %40
  %50 = phi i32 [ %7, %40 ], [ %48, %43 ]
  %51 = trunc i32 %3 to i8
  %52 = insertelement <16 x i8> undef, i8 %51, i32 0
  %53 = shufflevector <16 x i8> %52, <16 x i8> undef, <16 x i32> zeroinitializer
  %54 = lshr i32 255, %41
  %55 = trunc i32 %54 to i8
  %56 = insertelement <16 x i8> undef, i8 %55, i32 0
  %57 = shufflevector <16 x i8> %56, <16 x i8> undef, <16 x i32> zeroinitializer
  %58 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %41, i32 0
  %59 = bitcast <4 x i32> %58 to <8 x i16>
  %60 = load i32, i32* %32, align 8
  %61 = trunc i32 %60 to i8
  %62 = insertelement <16 x i8> undef, i8 %61, i32 0
  %63 = shufflevector <16 x i8> %62, <16 x i8> undef, <16 x i32> zeroinitializer
  %64 = shufflevector <16 x i8> %63, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %65 = bitcast <16 x i8> %64 to <8 x i16>
  %66 = shufflevector <16 x i8> %63, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %67 = bitcast <16 x i8> %66 to <8 x i16>
  %68 = getelementptr inbounds [2 x [2 x i32]], [2 x [2 x i32]]* @cdef_pri_taps, i64 0, i64 %31, i64 1
  %69 = load i32, i32* %68, align 4
  %70 = trunc i32 %69 to i8
  %71 = insertelement <16 x i8> undef, i8 %70, i32 0
  %72 = shufflevector <16 x i8> %71, <16 x i8> undef, <16 x i32> zeroinitializer
  %73 = shufflevector <16 x i8> %72, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %74 = bitcast <16 x i8> %73 to <8 x i16>
  %75 = shufflevector <16 x i8> %72, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %76 = bitcast <16 x i8> %75 to <8 x i16>
  %77 = trunc i32 %4 to i8
  %78 = insertelement <16 x i8> undef, i8 %77, i32 0
  %79 = shufflevector <16 x i8> %78, <16 x i8> undef, <16 x i32> zeroinitializer
  %80 = lshr i32 255, %50
  %81 = trunc i32 %80 to i8
  %82 = insertelement <16 x i8> undef, i8 %81, i32 0
  %83 = shufflevector <16 x i8> %82, <16 x i8> undef, <16 x i32> zeroinitializer
  %84 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %50, i32 0
  %85 = bitcast <4 x i32> %84 to <8 x i16>
  %86 = load i32, i32* getelementptr inbounds ([2 x i32], [2 x i32]* @cdef_sec_taps, i64 0, i64 0), align 4
  %87 = trunc i32 %86 to i8
  %88 = insertelement <16 x i8> undef, i8 %87, i32 0
  %89 = shufflevector <16 x i8> %88, <16 x i8> undef, <16 x i32> zeroinitializer
  %90 = shufflevector <16 x i8> %89, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %91 = bitcast <16 x i8> %90 to <8 x i16>
  %92 = shufflevector <16 x i8> %89, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %93 = bitcast <16 x i8> %92 to <8 x i16>
  %94 = load i32, i32* getelementptr inbounds ([2 x i32], [2 x i32]* @cdef_sec_taps, i64 0, i64 1), align 4
  %95 = trunc i32 %94 to i8
  %96 = insertelement <16 x i8> undef, i8 %95, i32 0
  %97 = shufflevector <16 x i8> %96, <16 x i8> undef, <16 x i32> zeroinitializer
  %98 = shufflevector <16 x i8> %97, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %99 = bitcast <16 x i8> %98 to <8 x i16>
  %100 = shufflevector <16 x i8> %97, <16 x i8> <i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0>, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %101 = bitcast <16 x i8> %100 to <8 x i16>
  %102 = sext i32 %1 to i64
  %103 = sext i32 %12 to i64
  %104 = sext i32 %14 to i64
  %105 = sext i32 %19 to i64
  %106 = sext i32 %26 to i64
  %107 = sext i32 %21 to i64
  %108 = sext i32 %28 to i64
  br label %109

109:                                              ; preds = %49, %109
  %110 = phi i64 [ 0, %49 ], [ %792, %109 ]
  %111 = mul nuw nsw i64 %110, 144
  %112 = getelementptr inbounds i16, i16* %2, i64 %111
  %113 = bitcast i16* %112 to <8 x i16>*
  %114 = load <8 x i16>, <8 x i16>* %113, align 16
  %115 = or i64 %110, 1
  %116 = mul nuw nsw i64 %115, 144
  %117 = getelementptr inbounds i16, i16* %2, i64 %116
  %118 = bitcast i16* %117 to <8 x i16>*
  %119 = load <8 x i16>, <8 x i16>* %118, align 16
  %120 = add nsw i64 %111, %103
  %121 = getelementptr inbounds i16, i16* %2, i64 %120
  %122 = bitcast i16* %121 to <2 x i64>*
  %123 = load <2 x i64>, <2 x i64>* %122, align 1
  %124 = add nsw i64 %116, %103
  %125 = getelementptr inbounds i16, i16* %2, i64 %124
  %126 = bitcast i16* %125 to <2 x i64>*
  %127 = load <2 x i64>, <2 x i64>* %126, align 1
  %128 = bitcast <2 x i64> %123 to <8 x i16>
  %129 = icmp eq <8 x i16> %128, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %130 = sext <8 x i1> %129 to <8 x i16>
  %131 = bitcast <2 x i64> %127 to <8 x i16>
  %132 = icmp eq <8 x i16> %131, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %133 = sext <8 x i1> %132 to <8 x i16>
  %134 = bitcast <8 x i16> %133 to <2 x i64>
  %135 = bitcast <8 x i16> %130 to <2 x i64>
  %136 = xor <2 x i64> %135, <i64 -1, i64 -1>
  %137 = and <2 x i64> %123, %136
  %138 = xor <2 x i64> %134, <i64 -1, i64 -1>
  %139 = and <2 x i64> %127, %138
  %140 = bitcast <2 x i64> %137 to <8 x i16>
  %141 = icmp sgt <8 x i16> %114, %140
  %142 = select <8 x i1> %141, <8 x i16> %114, <8 x i16> %140
  %143 = bitcast <2 x i64> %139 to <8 x i16>
  %144 = icmp sgt <8 x i16> %119, %143
  %145 = select <8 x i1> %144, <8 x i16> %119, <8 x i16> %143
  %146 = icmp slt <8 x i16> %114, %128
  %147 = select <8 x i1> %146, <8 x i16> %114, <8 x i16> %128
  %148 = icmp slt <8 x i16> %119, %131
  %149 = select <8 x i1> %148, <8 x i16> %119, <8 x i16> %131
  %150 = sub <8 x i16> %128, %114
  %151 = sub <8 x i16> %131, %119
  %152 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %151, <8 x i16> %150) #7
  %153 = ashr <16 x i8> %152, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %154 = icmp slt <16 x i8> %152, zeroinitializer
  %155 = sub <16 x i8> zeroinitializer, %152
  %156 = select <16 x i1> %154, <16 x i8> %155, <16 x i8> %152
  %157 = bitcast <16 x i8> %156 to <8 x i16>
  %158 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %157, <8 x i16> %59) #7
  %159 = bitcast <8 x i16> %158 to <16 x i8>
  %160 = and <16 x i8> %57, %159
  %161 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %53, <16 x i8> %160) #7
  %162 = icmp ult <16 x i8> %156, %161
  %163 = select <16 x i1> %162, <16 x i8> %156, <16 x i8> %161
  %164 = add <16 x i8> %163, %153
  %165 = xor <16 x i8> %164, %153
  %166 = sub nsw i64 %111, %103
  %167 = getelementptr inbounds i16, i16* %2, i64 %166
  %168 = bitcast i16* %167 to <2 x i64>*
  %169 = load <2 x i64>, <2 x i64>* %168, align 1
  %170 = sub nsw i64 %116, %103
  %171 = getelementptr inbounds i16, i16* %2, i64 %170
  %172 = bitcast i16* %171 to <2 x i64>*
  %173 = load <2 x i64>, <2 x i64>* %172, align 1
  %174 = bitcast <2 x i64> %169 to <8 x i16>
  %175 = icmp eq <8 x i16> %174, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %176 = sext <8 x i1> %175 to <8 x i16>
  %177 = bitcast <2 x i64> %173 to <8 x i16>
  %178 = icmp eq <8 x i16> %177, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %179 = sext <8 x i1> %178 to <8 x i16>
  %180 = bitcast <8 x i16> %179 to <2 x i64>
  %181 = bitcast <8 x i16> %176 to <2 x i64>
  %182 = xor <2 x i64> %181, <i64 -1, i64 -1>
  %183 = and <2 x i64> %169, %182
  %184 = xor <2 x i64> %180, <i64 -1, i64 -1>
  %185 = and <2 x i64> %173, %184
  %186 = bitcast <2 x i64> %183 to <8 x i16>
  %187 = icmp sgt <8 x i16> %142, %186
  %188 = select <8 x i1> %187, <8 x i16> %142, <8 x i16> %186
  %189 = bitcast <2 x i64> %185 to <8 x i16>
  %190 = icmp sgt <8 x i16> %145, %189
  %191 = select <8 x i1> %190, <8 x i16> %145, <8 x i16> %189
  %192 = icmp slt <8 x i16> %147, %174
  %193 = select <8 x i1> %192, <8 x i16> %147, <8 x i16> %174
  %194 = icmp slt <8 x i16> %149, %177
  %195 = select <8 x i1> %194, <8 x i16> %149, <8 x i16> %177
  %196 = sub <8 x i16> %174, %114
  %197 = sub <8 x i16> %177, %119
  %198 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %197, <8 x i16> %196) #7
  %199 = ashr <16 x i8> %198, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %200 = icmp slt <16 x i8> %198, zeroinitializer
  %201 = sub <16 x i8> zeroinitializer, %198
  %202 = select <16 x i1> %200, <16 x i8> %201, <16 x i8> %198
  %203 = bitcast <16 x i8> %202 to <8 x i16>
  %204 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %203, <8 x i16> %59) #7
  %205 = bitcast <8 x i16> %204 to <16 x i8>
  %206 = and <16 x i8> %57, %205
  %207 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %53, <16 x i8> %206) #7
  %208 = icmp ult <16 x i8> %202, %207
  %209 = select <16 x i1> %208, <16 x i8> %202, <16 x i8> %207
  %210 = add <16 x i8> %209, %199
  %211 = xor <16 x i8> %210, %199
  %212 = shufflevector <16 x i8> %211, <16 x i8> %165, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %213 = shufflevector <16 x i8> %211, <16 x i8> %165, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %214 = shufflevector <16 x i8> %212, <16 x i8> undef, <16 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3, i32 4, i32 4, i32 5, i32 5, i32 6, i32 6, i32 7, i32 7>
  %215 = bitcast <16 x i8> %214 to <8 x i16>
  %216 = ashr <8 x i16> %215, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %217 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %65, <8 x i16> %216) #7
  %218 = shufflevector <16 x i8> %212, <16 x i8> undef, <16 x i32> <i32 8, i32 8, i32 9, i32 9, i32 10, i32 10, i32 11, i32 11, i32 12, i32 12, i32 13, i32 13, i32 14, i32 14, i32 15, i32 15>
  %219 = bitcast <16 x i8> %218 to <8 x i16>
  %220 = ashr <8 x i16> %219, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %221 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %67, <8 x i16> %220) #7
  %222 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %217, <4 x i32> %221) #7
  %223 = shufflevector <16 x i8> %213, <16 x i8> undef, <16 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3, i32 4, i32 4, i32 5, i32 5, i32 6, i32 6, i32 7, i32 7>
  %224 = bitcast <16 x i8> %223 to <8 x i16>
  %225 = ashr <8 x i16> %224, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %226 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %65, <8 x i16> %225) #7
  %227 = shufflevector <16 x i8> %213, <16 x i8> undef, <16 x i32> <i32 8, i32 8, i32 9, i32 9, i32 10, i32 10, i32 11, i32 11, i32 12, i32 12, i32 13, i32 13, i32 14, i32 14, i32 15, i32 15>
  %228 = bitcast <16 x i8> %227 to <8 x i16>
  %229 = ashr <8 x i16> %228, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %230 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %67, <8 x i16> %229) #7
  %231 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %226, <4 x i32> %230) #7
  %232 = add nsw i64 %111, %104
  %233 = getelementptr inbounds i16, i16* %2, i64 %232
  %234 = bitcast i16* %233 to <2 x i64>*
  %235 = load <2 x i64>, <2 x i64>* %234, align 1
  %236 = add nsw i64 %116, %104
  %237 = getelementptr inbounds i16, i16* %2, i64 %236
  %238 = bitcast i16* %237 to <2 x i64>*
  %239 = load <2 x i64>, <2 x i64>* %238, align 1
  %240 = bitcast <2 x i64> %235 to <8 x i16>
  %241 = icmp eq <8 x i16> %240, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %242 = sext <8 x i1> %241 to <8 x i16>
  %243 = bitcast <2 x i64> %239 to <8 x i16>
  %244 = icmp eq <8 x i16> %243, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %245 = sext <8 x i1> %244 to <8 x i16>
  %246 = bitcast <8 x i16> %245 to <2 x i64>
  %247 = bitcast <8 x i16> %242 to <2 x i64>
  %248 = xor <2 x i64> %247, <i64 -1, i64 -1>
  %249 = and <2 x i64> %235, %248
  %250 = xor <2 x i64> %246, <i64 -1, i64 -1>
  %251 = and <2 x i64> %239, %250
  %252 = bitcast <2 x i64> %249 to <8 x i16>
  %253 = icmp sgt <8 x i16> %188, %252
  %254 = select <8 x i1> %253, <8 x i16> %188, <8 x i16> %252
  %255 = bitcast <2 x i64> %251 to <8 x i16>
  %256 = icmp sgt <8 x i16> %191, %255
  %257 = select <8 x i1> %256, <8 x i16> %191, <8 x i16> %255
  %258 = icmp slt <8 x i16> %193, %240
  %259 = select <8 x i1> %258, <8 x i16> %193, <8 x i16> %240
  %260 = icmp slt <8 x i16> %195, %243
  %261 = select <8 x i1> %260, <8 x i16> %195, <8 x i16> %243
  %262 = sub <8 x i16> %240, %114
  %263 = sub <8 x i16> %243, %119
  %264 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %263, <8 x i16> %262) #7
  %265 = ashr <16 x i8> %264, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %266 = icmp slt <16 x i8> %264, zeroinitializer
  %267 = sub <16 x i8> zeroinitializer, %264
  %268 = select <16 x i1> %266, <16 x i8> %267, <16 x i8> %264
  %269 = bitcast <16 x i8> %268 to <8 x i16>
  %270 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %269, <8 x i16> %59) #7
  %271 = bitcast <8 x i16> %270 to <16 x i8>
  %272 = and <16 x i8> %57, %271
  %273 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %53, <16 x i8> %272) #7
  %274 = icmp ult <16 x i8> %268, %273
  %275 = select <16 x i1> %274, <16 x i8> %268, <16 x i8> %273
  %276 = add <16 x i8> %275, %265
  %277 = xor <16 x i8> %276, %265
  %278 = sub nsw i64 %111, %104
  %279 = getelementptr inbounds i16, i16* %2, i64 %278
  %280 = bitcast i16* %279 to <2 x i64>*
  %281 = load <2 x i64>, <2 x i64>* %280, align 1
  %282 = sub nsw i64 %116, %104
  %283 = getelementptr inbounds i16, i16* %2, i64 %282
  %284 = bitcast i16* %283 to <2 x i64>*
  %285 = load <2 x i64>, <2 x i64>* %284, align 1
  %286 = bitcast <2 x i64> %281 to <8 x i16>
  %287 = icmp eq <8 x i16> %286, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %288 = sext <8 x i1> %287 to <8 x i16>
  %289 = bitcast <2 x i64> %285 to <8 x i16>
  %290 = icmp eq <8 x i16> %289, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %291 = sext <8 x i1> %290 to <8 x i16>
  %292 = bitcast <8 x i16> %291 to <2 x i64>
  %293 = bitcast <8 x i16> %288 to <2 x i64>
  %294 = xor <2 x i64> %293, <i64 -1, i64 -1>
  %295 = and <2 x i64> %281, %294
  %296 = xor <2 x i64> %292, <i64 -1, i64 -1>
  %297 = and <2 x i64> %285, %296
  %298 = bitcast <2 x i64> %295 to <8 x i16>
  %299 = icmp sgt <8 x i16> %254, %298
  %300 = select <8 x i1> %299, <8 x i16> %254, <8 x i16> %298
  %301 = bitcast <2 x i64> %297 to <8 x i16>
  %302 = icmp sgt <8 x i16> %257, %301
  %303 = select <8 x i1> %302, <8 x i16> %257, <8 x i16> %301
  %304 = icmp slt <8 x i16> %259, %286
  %305 = select <8 x i1> %304, <8 x i16> %259, <8 x i16> %286
  %306 = icmp slt <8 x i16> %261, %289
  %307 = select <8 x i1> %306, <8 x i16> %261, <8 x i16> %289
  %308 = sub <8 x i16> %286, %114
  %309 = sub <8 x i16> %289, %119
  %310 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %309, <8 x i16> %308) #7
  %311 = ashr <16 x i8> %310, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %312 = icmp slt <16 x i8> %310, zeroinitializer
  %313 = sub <16 x i8> zeroinitializer, %310
  %314 = select <16 x i1> %312, <16 x i8> %313, <16 x i8> %310
  %315 = bitcast <16 x i8> %314 to <8 x i16>
  %316 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %315, <8 x i16> %59) #7
  %317 = bitcast <8 x i16> %316 to <16 x i8>
  %318 = and <16 x i8> %57, %317
  %319 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %53, <16 x i8> %318) #7
  %320 = icmp ult <16 x i8> %314, %319
  %321 = select <16 x i1> %320, <16 x i8> %314, <16 x i8> %319
  %322 = add <16 x i8> %321, %311
  %323 = xor <16 x i8> %322, %311
  %324 = shufflevector <16 x i8> %323, <16 x i8> %277, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %325 = shufflevector <16 x i8> %323, <16 x i8> %277, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %326 = shufflevector <16 x i8> %324, <16 x i8> undef, <16 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3, i32 4, i32 4, i32 5, i32 5, i32 6, i32 6, i32 7, i32 7>
  %327 = bitcast <16 x i8> %326 to <8 x i16>
  %328 = ashr <8 x i16> %327, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %329 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %74, <8 x i16> %328) #7
  %330 = shufflevector <16 x i8> %324, <16 x i8> undef, <16 x i32> <i32 8, i32 8, i32 9, i32 9, i32 10, i32 10, i32 11, i32 11, i32 12, i32 12, i32 13, i32 13, i32 14, i32 14, i32 15, i32 15>
  %331 = bitcast <16 x i8> %330 to <8 x i16>
  %332 = ashr <8 x i16> %331, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %333 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %76, <8 x i16> %332) #7
  %334 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %329, <4 x i32> %333) #7
  %335 = shufflevector <16 x i8> %325, <16 x i8> undef, <16 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3, i32 4, i32 4, i32 5, i32 5, i32 6, i32 6, i32 7, i32 7>
  %336 = bitcast <16 x i8> %335 to <8 x i16>
  %337 = ashr <8 x i16> %336, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %338 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %74, <8 x i16> %337) #7
  %339 = shufflevector <16 x i8> %325, <16 x i8> undef, <16 x i32> <i32 8, i32 8, i32 9, i32 9, i32 10, i32 10, i32 11, i32 11, i32 12, i32 12, i32 13, i32 13, i32 14, i32 14, i32 15, i32 15>
  %340 = bitcast <16 x i8> %339 to <8 x i16>
  %341 = ashr <8 x i16> %340, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %342 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %76, <8 x i16> %341) #7
  %343 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %338, <4 x i32> %342) #7
  %344 = add <8 x i16> %334, %222
  %345 = add <8 x i16> %343, %231
  %346 = add nsw i64 %111, %105
  %347 = getelementptr inbounds i16, i16* %2, i64 %346
  %348 = bitcast i16* %347 to <2 x i64>*
  %349 = load <2 x i64>, <2 x i64>* %348, align 1
  %350 = add nsw i64 %116, %105
  %351 = getelementptr inbounds i16, i16* %2, i64 %350
  %352 = bitcast i16* %351 to <2 x i64>*
  %353 = load <2 x i64>, <2 x i64>* %352, align 1
  %354 = bitcast <2 x i64> %349 to <8 x i16>
  %355 = icmp eq <8 x i16> %354, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %356 = sext <8 x i1> %355 to <8 x i16>
  %357 = bitcast <2 x i64> %353 to <8 x i16>
  %358 = icmp eq <8 x i16> %357, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %359 = sext <8 x i1> %358 to <8 x i16>
  %360 = bitcast <8 x i16> %359 to <2 x i64>
  %361 = bitcast <8 x i16> %356 to <2 x i64>
  %362 = xor <2 x i64> %361, <i64 -1, i64 -1>
  %363 = and <2 x i64> %349, %362
  %364 = xor <2 x i64> %360, <i64 -1, i64 -1>
  %365 = and <2 x i64> %353, %364
  %366 = bitcast <2 x i64> %363 to <8 x i16>
  %367 = icmp sgt <8 x i16> %300, %366
  %368 = select <8 x i1> %367, <8 x i16> %300, <8 x i16> %366
  %369 = bitcast <2 x i64> %365 to <8 x i16>
  %370 = icmp sgt <8 x i16> %303, %369
  %371 = select <8 x i1> %370, <8 x i16> %303, <8 x i16> %369
  %372 = icmp slt <8 x i16> %305, %354
  %373 = select <8 x i1> %372, <8 x i16> %305, <8 x i16> %354
  %374 = icmp slt <8 x i16> %307, %357
  %375 = select <8 x i1> %374, <8 x i16> %307, <8 x i16> %357
  %376 = sub <8 x i16> %354, %114
  %377 = sub <8 x i16> %357, %119
  %378 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %377, <8 x i16> %376) #7
  %379 = ashr <16 x i8> %378, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %380 = icmp slt <16 x i8> %378, zeroinitializer
  %381 = sub <16 x i8> zeroinitializer, %378
  %382 = select <16 x i1> %380, <16 x i8> %381, <16 x i8> %378
  %383 = bitcast <16 x i8> %382 to <8 x i16>
  %384 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %383, <8 x i16> %85) #7
  %385 = bitcast <8 x i16> %384 to <16 x i8>
  %386 = and <16 x i8> %83, %385
  %387 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %79, <16 x i8> %386) #7
  %388 = icmp ult <16 x i8> %382, %387
  %389 = select <16 x i1> %388, <16 x i8> %382, <16 x i8> %387
  %390 = add <16 x i8> %389, %379
  %391 = xor <16 x i8> %390, %379
  %392 = sub nsw i64 %111, %105
  %393 = getelementptr inbounds i16, i16* %2, i64 %392
  %394 = bitcast i16* %393 to <2 x i64>*
  %395 = load <2 x i64>, <2 x i64>* %394, align 1
  %396 = sub nsw i64 %116, %105
  %397 = getelementptr inbounds i16, i16* %2, i64 %396
  %398 = bitcast i16* %397 to <2 x i64>*
  %399 = load <2 x i64>, <2 x i64>* %398, align 1
  %400 = bitcast <2 x i64> %395 to <8 x i16>
  %401 = icmp eq <8 x i16> %400, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %402 = sext <8 x i1> %401 to <8 x i16>
  %403 = bitcast <2 x i64> %399 to <8 x i16>
  %404 = icmp eq <8 x i16> %403, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %405 = sext <8 x i1> %404 to <8 x i16>
  %406 = bitcast <8 x i16> %405 to <2 x i64>
  %407 = bitcast <8 x i16> %402 to <2 x i64>
  %408 = xor <2 x i64> %407, <i64 -1, i64 -1>
  %409 = and <2 x i64> %395, %408
  %410 = xor <2 x i64> %406, <i64 -1, i64 -1>
  %411 = and <2 x i64> %399, %410
  %412 = bitcast <2 x i64> %409 to <8 x i16>
  %413 = icmp sgt <8 x i16> %368, %412
  %414 = select <8 x i1> %413, <8 x i16> %368, <8 x i16> %412
  %415 = bitcast <2 x i64> %411 to <8 x i16>
  %416 = icmp sgt <8 x i16> %371, %415
  %417 = select <8 x i1> %416, <8 x i16> %371, <8 x i16> %415
  %418 = icmp slt <8 x i16> %373, %400
  %419 = select <8 x i1> %418, <8 x i16> %373, <8 x i16> %400
  %420 = icmp slt <8 x i16> %375, %403
  %421 = select <8 x i1> %420, <8 x i16> %375, <8 x i16> %403
  %422 = sub <8 x i16> %400, %114
  %423 = sub <8 x i16> %403, %119
  %424 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %423, <8 x i16> %422) #7
  %425 = ashr <16 x i8> %424, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %426 = icmp slt <16 x i8> %424, zeroinitializer
  %427 = sub <16 x i8> zeroinitializer, %424
  %428 = select <16 x i1> %426, <16 x i8> %427, <16 x i8> %424
  %429 = bitcast <16 x i8> %428 to <8 x i16>
  %430 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %429, <8 x i16> %85) #7
  %431 = bitcast <8 x i16> %430 to <16 x i8>
  %432 = and <16 x i8> %83, %431
  %433 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %79, <16 x i8> %432) #7
  %434 = icmp ult <16 x i8> %428, %433
  %435 = select <16 x i1> %434, <16 x i8> %428, <16 x i8> %433
  %436 = add <16 x i8> %435, %425
  %437 = xor <16 x i8> %436, %425
  %438 = add nsw i64 %111, %106
  %439 = getelementptr inbounds i16, i16* %2, i64 %438
  %440 = bitcast i16* %439 to <2 x i64>*
  %441 = load <2 x i64>, <2 x i64>* %440, align 1
  %442 = add nsw i64 %116, %106
  %443 = getelementptr inbounds i16, i16* %2, i64 %442
  %444 = bitcast i16* %443 to <2 x i64>*
  %445 = load <2 x i64>, <2 x i64>* %444, align 1
  %446 = bitcast <2 x i64> %441 to <8 x i16>
  %447 = icmp eq <8 x i16> %446, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %448 = sext <8 x i1> %447 to <8 x i16>
  %449 = bitcast <2 x i64> %445 to <8 x i16>
  %450 = icmp eq <8 x i16> %449, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %451 = sext <8 x i1> %450 to <8 x i16>
  %452 = bitcast <8 x i16> %451 to <2 x i64>
  %453 = bitcast <8 x i16> %448 to <2 x i64>
  %454 = xor <2 x i64> %453, <i64 -1, i64 -1>
  %455 = and <2 x i64> %441, %454
  %456 = xor <2 x i64> %452, <i64 -1, i64 -1>
  %457 = and <2 x i64> %445, %456
  %458 = bitcast <2 x i64> %455 to <8 x i16>
  %459 = icmp sgt <8 x i16> %414, %458
  %460 = select <8 x i1> %459, <8 x i16> %414, <8 x i16> %458
  %461 = bitcast <2 x i64> %457 to <8 x i16>
  %462 = icmp sgt <8 x i16> %417, %461
  %463 = select <8 x i1> %462, <8 x i16> %417, <8 x i16> %461
  %464 = icmp slt <8 x i16> %419, %446
  %465 = select <8 x i1> %464, <8 x i16> %419, <8 x i16> %446
  %466 = icmp slt <8 x i16> %421, %449
  %467 = select <8 x i1> %466, <8 x i16> %421, <8 x i16> %449
  %468 = sub <8 x i16> %446, %114
  %469 = sub <8 x i16> %449, %119
  %470 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %469, <8 x i16> %468) #7
  %471 = ashr <16 x i8> %470, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %472 = icmp slt <16 x i8> %470, zeroinitializer
  %473 = sub <16 x i8> zeroinitializer, %470
  %474 = select <16 x i1> %472, <16 x i8> %473, <16 x i8> %470
  %475 = bitcast <16 x i8> %474 to <8 x i16>
  %476 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %475, <8 x i16> %85) #7
  %477 = bitcast <8 x i16> %476 to <16 x i8>
  %478 = and <16 x i8> %83, %477
  %479 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %79, <16 x i8> %478) #7
  %480 = icmp ult <16 x i8> %474, %479
  %481 = select <16 x i1> %480, <16 x i8> %474, <16 x i8> %479
  %482 = add <16 x i8> %481, %471
  %483 = xor <16 x i8> %482, %471
  %484 = sub nsw i64 %111, %106
  %485 = getelementptr inbounds i16, i16* %2, i64 %484
  %486 = bitcast i16* %485 to <2 x i64>*
  %487 = load <2 x i64>, <2 x i64>* %486, align 1
  %488 = sub nsw i64 %116, %106
  %489 = getelementptr inbounds i16, i16* %2, i64 %488
  %490 = bitcast i16* %489 to <2 x i64>*
  %491 = load <2 x i64>, <2 x i64>* %490, align 1
  %492 = bitcast <2 x i64> %487 to <8 x i16>
  %493 = icmp eq <8 x i16> %492, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %494 = sext <8 x i1> %493 to <8 x i16>
  %495 = bitcast <2 x i64> %491 to <8 x i16>
  %496 = icmp eq <8 x i16> %495, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %497 = sext <8 x i1> %496 to <8 x i16>
  %498 = bitcast <8 x i16> %497 to <2 x i64>
  %499 = bitcast <8 x i16> %494 to <2 x i64>
  %500 = xor <2 x i64> %499, <i64 -1, i64 -1>
  %501 = and <2 x i64> %487, %500
  %502 = xor <2 x i64> %498, <i64 -1, i64 -1>
  %503 = and <2 x i64> %491, %502
  %504 = bitcast <2 x i64> %501 to <8 x i16>
  %505 = icmp sgt <8 x i16> %460, %504
  %506 = select <8 x i1> %505, <8 x i16> %460, <8 x i16> %504
  %507 = bitcast <2 x i64> %503 to <8 x i16>
  %508 = icmp sgt <8 x i16> %463, %507
  %509 = select <8 x i1> %508, <8 x i16> %463, <8 x i16> %507
  %510 = icmp slt <8 x i16> %465, %492
  %511 = select <8 x i1> %510, <8 x i16> %465, <8 x i16> %492
  %512 = icmp slt <8 x i16> %467, %495
  %513 = select <8 x i1> %512, <8 x i16> %467, <8 x i16> %495
  %514 = sub <8 x i16> %492, %114
  %515 = sub <8 x i16> %495, %119
  %516 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %515, <8 x i16> %514) #7
  %517 = ashr <16 x i8> %516, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %518 = icmp slt <16 x i8> %516, zeroinitializer
  %519 = sub <16 x i8> zeroinitializer, %516
  %520 = select <16 x i1> %518, <16 x i8> %519, <16 x i8> %516
  %521 = bitcast <16 x i8> %520 to <8 x i16>
  %522 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %521, <8 x i16> %85) #7
  %523 = bitcast <8 x i16> %522 to <16 x i8>
  %524 = and <16 x i8> %83, %523
  %525 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %79, <16 x i8> %524) #7
  %526 = icmp ult <16 x i8> %520, %525
  %527 = select <16 x i1> %526, <16 x i8> %520, <16 x i8> %525
  %528 = add <16 x i8> %527, %517
  %529 = xor <16 x i8> %528, %517
  %530 = add <16 x i8> %437, %391
  %531 = add <16 x i8> %529, %483
  %532 = shufflevector <16 x i8> %531, <16 x i8> %530, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %533 = shufflevector <16 x i8> %531, <16 x i8> %530, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %534 = shufflevector <16 x i8> %532, <16 x i8> undef, <16 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3, i32 4, i32 4, i32 5, i32 5, i32 6, i32 6, i32 7, i32 7>
  %535 = bitcast <16 x i8> %534 to <8 x i16>
  %536 = ashr <8 x i16> %535, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %537 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %91, <8 x i16> %536) #7
  %538 = shufflevector <16 x i8> %532, <16 x i8> undef, <16 x i32> <i32 8, i32 8, i32 9, i32 9, i32 10, i32 10, i32 11, i32 11, i32 12, i32 12, i32 13, i32 13, i32 14, i32 14, i32 15, i32 15>
  %539 = bitcast <16 x i8> %538 to <8 x i16>
  %540 = ashr <8 x i16> %539, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %541 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %93, <8 x i16> %540) #7
  %542 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %537, <4 x i32> %541) #7
  %543 = shufflevector <16 x i8> %533, <16 x i8> undef, <16 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3, i32 4, i32 4, i32 5, i32 5, i32 6, i32 6, i32 7, i32 7>
  %544 = bitcast <16 x i8> %543 to <8 x i16>
  %545 = ashr <8 x i16> %544, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %546 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %91, <8 x i16> %545) #7
  %547 = shufflevector <16 x i8> %533, <16 x i8> undef, <16 x i32> <i32 8, i32 8, i32 9, i32 9, i32 10, i32 10, i32 11, i32 11, i32 12, i32 12, i32 13, i32 13, i32 14, i32 14, i32 15, i32 15>
  %548 = bitcast <16 x i8> %547 to <8 x i16>
  %549 = ashr <8 x i16> %548, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %550 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %93, <8 x i16> %549) #7
  %551 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %546, <4 x i32> %550) #7
  %552 = add <8 x i16> %344, %542
  %553 = add <8 x i16> %345, %551
  %554 = add nsw i64 %111, %107
  %555 = getelementptr inbounds i16, i16* %2, i64 %554
  %556 = bitcast i16* %555 to <2 x i64>*
  %557 = load <2 x i64>, <2 x i64>* %556, align 1
  %558 = add nsw i64 %116, %107
  %559 = getelementptr inbounds i16, i16* %2, i64 %558
  %560 = bitcast i16* %559 to <2 x i64>*
  %561 = load <2 x i64>, <2 x i64>* %560, align 1
  %562 = bitcast <2 x i64> %557 to <8 x i16>
  %563 = icmp eq <8 x i16> %562, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %564 = sext <8 x i1> %563 to <8 x i16>
  %565 = bitcast <2 x i64> %561 to <8 x i16>
  %566 = icmp eq <8 x i16> %565, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %567 = sext <8 x i1> %566 to <8 x i16>
  %568 = bitcast <8 x i16> %567 to <2 x i64>
  %569 = bitcast <8 x i16> %564 to <2 x i64>
  %570 = xor <2 x i64> %569, <i64 -1, i64 -1>
  %571 = and <2 x i64> %557, %570
  %572 = xor <2 x i64> %568, <i64 -1, i64 -1>
  %573 = and <2 x i64> %561, %572
  %574 = bitcast <2 x i64> %571 to <8 x i16>
  %575 = icmp sgt <8 x i16> %506, %574
  %576 = select <8 x i1> %575, <8 x i16> %506, <8 x i16> %574
  %577 = bitcast <2 x i64> %573 to <8 x i16>
  %578 = icmp sgt <8 x i16> %509, %577
  %579 = select <8 x i1> %578, <8 x i16> %509, <8 x i16> %577
  %580 = icmp slt <8 x i16> %511, %562
  %581 = select <8 x i1> %580, <8 x i16> %511, <8 x i16> %562
  %582 = icmp slt <8 x i16> %513, %565
  %583 = select <8 x i1> %582, <8 x i16> %513, <8 x i16> %565
  %584 = sub <8 x i16> %562, %114
  %585 = sub <8 x i16> %565, %119
  %586 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %585, <8 x i16> %584) #7
  %587 = ashr <16 x i8> %586, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %588 = icmp slt <16 x i8> %586, zeroinitializer
  %589 = sub <16 x i8> zeroinitializer, %586
  %590 = select <16 x i1> %588, <16 x i8> %589, <16 x i8> %586
  %591 = bitcast <16 x i8> %590 to <8 x i16>
  %592 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %591, <8 x i16> %85) #7
  %593 = bitcast <8 x i16> %592 to <16 x i8>
  %594 = and <16 x i8> %83, %593
  %595 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %79, <16 x i8> %594) #7
  %596 = icmp ult <16 x i8> %590, %595
  %597 = select <16 x i1> %596, <16 x i8> %590, <16 x i8> %595
  %598 = add <16 x i8> %597, %587
  %599 = xor <16 x i8> %598, %587
  %600 = sub nsw i64 %111, %107
  %601 = getelementptr inbounds i16, i16* %2, i64 %600
  %602 = bitcast i16* %601 to <2 x i64>*
  %603 = load <2 x i64>, <2 x i64>* %602, align 1
  %604 = sub nsw i64 %116, %107
  %605 = getelementptr inbounds i16, i16* %2, i64 %604
  %606 = bitcast i16* %605 to <2 x i64>*
  %607 = load <2 x i64>, <2 x i64>* %606, align 1
  %608 = bitcast <2 x i64> %603 to <8 x i16>
  %609 = icmp eq <8 x i16> %608, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %610 = sext <8 x i1> %609 to <8 x i16>
  %611 = bitcast <2 x i64> %607 to <8 x i16>
  %612 = icmp eq <8 x i16> %611, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %613 = sext <8 x i1> %612 to <8 x i16>
  %614 = bitcast <8 x i16> %613 to <2 x i64>
  %615 = bitcast <8 x i16> %610 to <2 x i64>
  %616 = xor <2 x i64> %615, <i64 -1, i64 -1>
  %617 = and <2 x i64> %603, %616
  %618 = xor <2 x i64> %614, <i64 -1, i64 -1>
  %619 = and <2 x i64> %607, %618
  %620 = bitcast <2 x i64> %617 to <8 x i16>
  %621 = icmp sgt <8 x i16> %576, %620
  %622 = select <8 x i1> %621, <8 x i16> %576, <8 x i16> %620
  %623 = bitcast <2 x i64> %619 to <8 x i16>
  %624 = icmp sgt <8 x i16> %579, %623
  %625 = select <8 x i1> %624, <8 x i16> %579, <8 x i16> %623
  %626 = icmp slt <8 x i16> %581, %608
  %627 = select <8 x i1> %626, <8 x i16> %581, <8 x i16> %608
  %628 = icmp slt <8 x i16> %583, %611
  %629 = select <8 x i1> %628, <8 x i16> %583, <8 x i16> %611
  %630 = sub <8 x i16> %608, %114
  %631 = sub <8 x i16> %611, %119
  %632 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %631, <8 x i16> %630) #7
  %633 = ashr <16 x i8> %632, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %634 = icmp slt <16 x i8> %632, zeroinitializer
  %635 = sub <16 x i8> zeroinitializer, %632
  %636 = select <16 x i1> %634, <16 x i8> %635, <16 x i8> %632
  %637 = bitcast <16 x i8> %636 to <8 x i16>
  %638 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %637, <8 x i16> %85) #7
  %639 = bitcast <8 x i16> %638 to <16 x i8>
  %640 = and <16 x i8> %83, %639
  %641 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %79, <16 x i8> %640) #7
  %642 = icmp ult <16 x i8> %636, %641
  %643 = select <16 x i1> %642, <16 x i8> %636, <16 x i8> %641
  %644 = add <16 x i8> %643, %633
  %645 = xor <16 x i8> %644, %633
  %646 = add nsw i64 %111, %108
  %647 = getelementptr inbounds i16, i16* %2, i64 %646
  %648 = bitcast i16* %647 to <2 x i64>*
  %649 = load <2 x i64>, <2 x i64>* %648, align 1
  %650 = add nsw i64 %116, %108
  %651 = getelementptr inbounds i16, i16* %2, i64 %650
  %652 = bitcast i16* %651 to <2 x i64>*
  %653 = load <2 x i64>, <2 x i64>* %652, align 1
  %654 = bitcast <2 x i64> %649 to <8 x i16>
  %655 = icmp eq <8 x i16> %654, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %656 = sext <8 x i1> %655 to <8 x i16>
  %657 = bitcast <2 x i64> %653 to <8 x i16>
  %658 = icmp eq <8 x i16> %657, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %659 = sext <8 x i1> %658 to <8 x i16>
  %660 = bitcast <8 x i16> %659 to <2 x i64>
  %661 = bitcast <8 x i16> %656 to <2 x i64>
  %662 = xor <2 x i64> %661, <i64 -1, i64 -1>
  %663 = and <2 x i64> %649, %662
  %664 = xor <2 x i64> %660, <i64 -1, i64 -1>
  %665 = and <2 x i64> %653, %664
  %666 = bitcast <2 x i64> %663 to <8 x i16>
  %667 = icmp sgt <8 x i16> %622, %666
  %668 = select <8 x i1> %667, <8 x i16> %622, <8 x i16> %666
  %669 = bitcast <2 x i64> %665 to <8 x i16>
  %670 = icmp sgt <8 x i16> %625, %669
  %671 = select <8 x i1> %670, <8 x i16> %625, <8 x i16> %669
  %672 = icmp slt <8 x i16> %627, %654
  %673 = select <8 x i1> %672, <8 x i16> %627, <8 x i16> %654
  %674 = icmp slt <8 x i16> %629, %657
  %675 = select <8 x i1> %674, <8 x i16> %629, <8 x i16> %657
  %676 = sub <8 x i16> %654, %114
  %677 = sub <8 x i16> %657, %119
  %678 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %677, <8 x i16> %676) #7
  %679 = ashr <16 x i8> %678, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %680 = icmp slt <16 x i8> %678, zeroinitializer
  %681 = sub <16 x i8> zeroinitializer, %678
  %682 = select <16 x i1> %680, <16 x i8> %681, <16 x i8> %678
  %683 = bitcast <16 x i8> %682 to <8 x i16>
  %684 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %683, <8 x i16> %85) #7
  %685 = bitcast <8 x i16> %684 to <16 x i8>
  %686 = and <16 x i8> %83, %685
  %687 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %79, <16 x i8> %686) #7
  %688 = icmp ult <16 x i8> %682, %687
  %689 = select <16 x i1> %688, <16 x i8> %682, <16 x i8> %687
  %690 = add <16 x i8> %689, %679
  %691 = xor <16 x i8> %690, %679
  %692 = sub nsw i64 %111, %108
  %693 = getelementptr inbounds i16, i16* %2, i64 %692
  %694 = bitcast i16* %693 to <2 x i64>*
  %695 = load <2 x i64>, <2 x i64>* %694, align 1
  %696 = sub nsw i64 %116, %108
  %697 = getelementptr inbounds i16, i16* %2, i64 %696
  %698 = bitcast i16* %697 to <2 x i64>*
  %699 = load <2 x i64>, <2 x i64>* %698, align 1
  %700 = bitcast <2 x i64> %695 to <8 x i16>
  %701 = icmp eq <8 x i16> %700, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %702 = sext <8 x i1> %701 to <8 x i16>
  %703 = bitcast <2 x i64> %699 to <8 x i16>
  %704 = icmp eq <8 x i16> %703, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %705 = sext <8 x i1> %704 to <8 x i16>
  %706 = bitcast <8 x i16> %705 to <2 x i64>
  %707 = bitcast <8 x i16> %702 to <2 x i64>
  %708 = xor <2 x i64> %707, <i64 -1, i64 -1>
  %709 = and <2 x i64> %695, %708
  %710 = xor <2 x i64> %706, <i64 -1, i64 -1>
  %711 = and <2 x i64> %699, %710
  %712 = bitcast <2 x i64> %709 to <8 x i16>
  %713 = icmp sgt <8 x i16> %668, %712
  %714 = select <8 x i1> %713, <8 x i16> %668, <8 x i16> %712
  %715 = bitcast <2 x i64> %711 to <8 x i16>
  %716 = icmp sgt <8 x i16> %671, %715
  %717 = select <8 x i1> %716, <8 x i16> %671, <8 x i16> %715
  %718 = icmp slt <8 x i16> %673, %700
  %719 = select <8 x i1> %718, <8 x i16> %673, <8 x i16> %700
  %720 = icmp slt <8 x i16> %675, %703
  %721 = select <8 x i1> %720, <8 x i16> %675, <8 x i16> %703
  %722 = sub <8 x i16> %700, %114
  %723 = sub <8 x i16> %703, %119
  %724 = tail call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %723, <8 x i16> %722) #7
  %725 = ashr <16 x i8> %724, <i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7, i8 7>
  %726 = icmp slt <16 x i8> %724, zeroinitializer
  %727 = sub <16 x i8> zeroinitializer, %724
  %728 = select <16 x i1> %726, <16 x i8> %727, <16 x i8> %724
  %729 = bitcast <16 x i8> %728 to <8 x i16>
  %730 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %729, <8 x i16> %85) #7
  %731 = bitcast <8 x i16> %730 to <16 x i8>
  %732 = and <16 x i8> %83, %731
  %733 = tail call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> %79, <16 x i8> %732) #7
  %734 = icmp ult <16 x i8> %728, %733
  %735 = select <16 x i1> %734, <16 x i8> %728, <16 x i8> %733
  %736 = add <16 x i8> %735, %725
  %737 = xor <16 x i8> %736, %725
  %738 = add <16 x i8> %645, %599
  %739 = add <16 x i8> %737, %691
  %740 = shufflevector <16 x i8> %739, <16 x i8> %738, <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  %741 = shufflevector <16 x i8> %739, <16 x i8> %738, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %742 = shufflevector <16 x i8> %740, <16 x i8> undef, <16 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3, i32 4, i32 4, i32 5, i32 5, i32 6, i32 6, i32 7, i32 7>
  %743 = bitcast <16 x i8> %742 to <8 x i16>
  %744 = ashr <8 x i16> %743, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %745 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %99, <8 x i16> %744) #7
  %746 = shufflevector <16 x i8> %740, <16 x i8> undef, <16 x i32> <i32 8, i32 8, i32 9, i32 9, i32 10, i32 10, i32 11, i32 11, i32 12, i32 12, i32 13, i32 13, i32 14, i32 14, i32 15, i32 15>
  %747 = bitcast <16 x i8> %746 to <8 x i16>
  %748 = ashr <8 x i16> %747, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %749 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %101, <8 x i16> %748) #7
  %750 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %745, <4 x i32> %749) #7
  %751 = shufflevector <16 x i8> %741, <16 x i8> undef, <16 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3, i32 4, i32 4, i32 5, i32 5, i32 6, i32 6, i32 7, i32 7>
  %752 = bitcast <16 x i8> %751 to <8 x i16>
  %753 = ashr <8 x i16> %752, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %754 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %99, <8 x i16> %753) #7
  %755 = shufflevector <16 x i8> %741, <16 x i8> undef, <16 x i32> <i32 8, i32 8, i32 9, i32 9, i32 10, i32 10, i32 11, i32 11, i32 12, i32 12, i32 13, i32 13, i32 14, i32 14, i32 15, i32 15>
  %756 = bitcast <16 x i8> %755 to <8 x i16>
  %757 = ashr <8 x i16> %756, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %758 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %101, <8 x i16> %757) #7
  %759 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %754, <4 x i32> %758) #7
  %760 = add <8 x i16> %552, %750
  %761 = add <8 x i16> %553, %759
  %762 = ashr <8 x i16> %760, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %763 = ashr <8 x i16> %761, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %764 = add <8 x i16> %760, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %765 = add <8 x i16> %764, %762
  %766 = add <8 x i16> %761, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %767 = add <8 x i16> %766, %763
  %768 = ashr <8 x i16> %765, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %769 = ashr <8 x i16> %767, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %770 = add <8 x i16> %768, %114
  %771 = add <8 x i16> %769, %119
  %772 = icmp sgt <8 x i16> %770, %719
  %773 = select <8 x i1> %772, <8 x i16> %770, <8 x i16> %719
  %774 = icmp sgt <8 x i16> %771, %721
  %775 = select <8 x i1> %774, <8 x i16> %771, <8 x i16> %721
  %776 = icmp slt <8 x i16> %773, %714
  %777 = select <8 x i1> %776, <8 x i16> %773, <8 x i16> %714
  %778 = icmp slt <8 x i16> %775, %717
  %779 = select <8 x i1> %778, <8 x i16> %775, <8 x i16> %717
  %780 = tail call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %779, <8 x i16> %777) #7
  %781 = bitcast <16 x i8> %780 to <2 x i64>
  %782 = mul nsw i64 %110, %102
  %783 = getelementptr inbounds i8, i8* %0, i64 %782
  %784 = shufflevector <16 x i8> %780, <16 x i8> undef, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %785 = bitcast <16 x i8> %784 to <2 x i64>
  %786 = extractelement <2 x i64> %785, i32 0
  %787 = bitcast i8* %783 to i64*
  store i64 %786, i64* %787, align 1
  %788 = mul nsw i64 %115, %102
  %789 = getelementptr inbounds i8, i8* %0, i64 %788
  %790 = extractelement <2 x i64> %781, i32 0
  %791 = bitcast i8* %789 to i64*
  store i64 %790, i64* %791, align 1
  %792 = add nuw nsw i64 %110, 2
  %793 = icmp ult i64 %792, 8
  br i1 %793, label %109, label %794

794:                                              ; preds = %109
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @cdef_filter_block_4x4_16_sse2(i16* nocapture, i32, i16* readonly, i32, i32, i32, i32, i32, i32) local_unnamed_addr #0 {
  %10 = sext i32 %5 to i64
  %11 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %10, i64 0
  %12 = load i32, i32* %11, align 8
  %13 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %10, i64 1
  %14 = load i32, i32* %13, align 4
  %15 = add nsw i32 %5, 2
  %16 = and i32 %15, 7
  %17 = zext i32 %16 to i64
  %18 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %17, i64 0
  %19 = load i32, i32* %18, align 8
  %20 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %17, i64 1
  %21 = load i32, i32* %20, align 4
  %22 = add nsw i32 %5, 6
  %23 = and i32 %22, 7
  %24 = zext i32 %23 to i64
  %25 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %24, i64 0
  %26 = load i32, i32* %25, align 8
  %27 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %24, i64 1
  %28 = load i32, i32* %27, align 4
  %29 = lshr i32 %3, %8
  %30 = and i32 %29, 1
  %31 = zext i32 %30 to i64
  %32 = getelementptr inbounds [2 x [2 x i32]], [2 x [2 x i32]]* @cdef_pri_taps, i64 0, i64 %31, i64 0
  %33 = icmp eq i32 %3, 0
  br i1 %33, label %40, label %34

34:                                               ; preds = %9
  %35 = tail call i32 @llvm.ctlz.i32(i32 %3, i1 true) #7, !range !2
  %36 = xor i32 %35, 31
  %37 = icmp sgt i32 %36, %6
  %38 = sub nsw i32 %6, %36
  %39 = select i1 %37, i32 0, i32 %38
  br label %40

40:                                               ; preds = %34, %9
  %41 = phi i32 [ %6, %9 ], [ %39, %34 ]
  %42 = icmp eq i32 %4, 0
  br i1 %42, label %49, label %43

43:                                               ; preds = %40
  %44 = tail call i32 @llvm.ctlz.i32(i32 %4, i1 true) #7, !range !2
  %45 = xor i32 %44, 31
  %46 = icmp sgt i32 %45, %7
  %47 = sub nsw i32 %7, %45
  %48 = select i1 %46, i32 0, i32 %47
  br label %49

49:                                               ; preds = %43, %40
  %50 = phi i32 [ %7, %40 ], [ %48, %43 ]
  %51 = trunc i32 %3 to i16
  %52 = insertelement <8 x i16> undef, i16 %51, i32 0
  %53 = shufflevector <8 x i16> %52, <8 x i16> undef, <8 x i32> zeroinitializer
  %54 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %41, i32 0
  %55 = bitcast <4 x i32> %54 to <8 x i16>
  %56 = load i32, i32* %32, align 8
  %57 = trunc i32 %56 to i16
  %58 = insertelement <8 x i16> undef, i16 %57, i32 0
  %59 = shufflevector <8 x i16> %58, <8 x i16> undef, <8 x i32> zeroinitializer
  %60 = getelementptr inbounds [2 x [2 x i32]], [2 x [2 x i32]]* @cdef_pri_taps, i64 0, i64 %31, i64 1
  %61 = load i32, i32* %60, align 4
  %62 = trunc i32 %61 to i16
  %63 = insertelement <8 x i16> undef, i16 %62, i32 0
  %64 = shufflevector <8 x i16> %63, <8 x i16> undef, <8 x i32> zeroinitializer
  %65 = trunc i32 %4 to i16
  %66 = insertelement <8 x i16> undef, i16 %65, i32 0
  %67 = shufflevector <8 x i16> %66, <8 x i16> undef, <8 x i32> zeroinitializer
  %68 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %50, i32 0
  %69 = bitcast <4 x i32> %68 to <8 x i16>
  %70 = load i32, i32* getelementptr inbounds ([2 x i32], [2 x i32]* @cdef_sec_taps, i64 0, i64 0), align 4
  %71 = trunc i32 %70 to i16
  %72 = insertelement <8 x i16> undef, i16 %71, i32 0
  %73 = shufflevector <8 x i16> %72, <8 x i16> undef, <8 x i32> zeroinitializer
  %74 = load i32, i32* getelementptr inbounds ([2 x i32], [2 x i32]* @cdef_sec_taps, i64 0, i64 1), align 4
  %75 = trunc i32 %74 to i16
  %76 = insertelement <8 x i16> undef, i16 %75, i32 0
  %77 = shufflevector <8 x i16> %76, <8 x i16> undef, <8 x i32> zeroinitializer
  %78 = bitcast i16* %2 to i64*
  %79 = load i64, i64* %78, align 1
  %80 = getelementptr inbounds i16, i16* %2, i64 144
  %81 = bitcast i16* %80 to i64*
  %82 = load i64, i64* %81, align 1
  %83 = insertelement <2 x i64> undef, i64 %82, i32 0
  %84 = getelementptr inbounds i16, i16* %2, i64 288
  %85 = bitcast i16* %84 to i64*
  %86 = load i64, i64* %85, align 1
  %87 = getelementptr inbounds i16, i16* %2, i64 432
  %88 = bitcast i16* %87 to i64*
  %89 = load i64, i64* %88, align 1
  %90 = insertelement <2 x i64> undef, i64 %89, i32 0
  %91 = insertelement <2 x i64> %83, i64 %79, i32 1
  %92 = insertelement <2 x i64> %90, i64 %86, i32 1
  %93 = sext i32 %12 to i64
  %94 = getelementptr inbounds i16, i16* %2, i64 %93
  %95 = bitcast i16* %94 to i64*
  %96 = load i64, i64* %95, align 1
  %97 = add nsw i32 %12, 144
  %98 = sext i32 %97 to i64
  %99 = getelementptr inbounds i16, i16* %2, i64 %98
  %100 = bitcast i16* %99 to i64*
  %101 = load i64, i64* %100, align 1
  %102 = insertelement <2 x i64> undef, i64 %101, i32 0
  %103 = add nsw i32 %12, 288
  %104 = sext i32 %103 to i64
  %105 = getelementptr inbounds i16, i16* %2, i64 %104
  %106 = bitcast i16* %105 to i64*
  %107 = load i64, i64* %106, align 1
  %108 = add nsw i32 %12, 432
  %109 = sext i32 %108 to i64
  %110 = getelementptr inbounds i16, i16* %2, i64 %109
  %111 = bitcast i16* %110 to i64*
  %112 = load i64, i64* %111, align 1
  %113 = insertelement <2 x i64> undef, i64 %112, i32 0
  %114 = insertelement <2 x i64> %102, i64 %96, i32 1
  %115 = insertelement <2 x i64> %113, i64 %107, i32 1
  %116 = sub nsw i32 0, %12
  %117 = sext i32 %116 to i64
  %118 = getelementptr inbounds i16, i16* %2, i64 %117
  %119 = bitcast i16* %118 to i64*
  %120 = load i64, i64* %119, align 1
  %121 = sub nsw i32 144, %12
  %122 = sext i32 %121 to i64
  %123 = getelementptr inbounds i16, i16* %2, i64 %122
  %124 = bitcast i16* %123 to i64*
  %125 = load i64, i64* %124, align 1
  %126 = insertelement <2 x i64> undef, i64 %125, i32 0
  %127 = sub nsw i32 288, %12
  %128 = sext i32 %127 to i64
  %129 = getelementptr inbounds i16, i16* %2, i64 %128
  %130 = bitcast i16* %129 to i64*
  %131 = load i64, i64* %130, align 1
  %132 = sub nsw i32 432, %12
  %133 = sext i32 %132 to i64
  %134 = getelementptr inbounds i16, i16* %2, i64 %133
  %135 = bitcast i16* %134 to i64*
  %136 = load i64, i64* %135, align 1
  %137 = insertelement <2 x i64> undef, i64 %136, i32 0
  %138 = insertelement <2 x i64> %126, i64 %120, i32 1
  %139 = insertelement <2 x i64> %137, i64 %131, i32 1
  %140 = bitcast <2 x i64> %114 to <8 x i16>
  %141 = icmp eq <8 x i16> %140, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %142 = sext <8 x i1> %141 to <8 x i16>
  %143 = bitcast <2 x i64> %115 to <8 x i16>
  %144 = icmp eq <8 x i16> %143, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %145 = sext <8 x i1> %144 to <8 x i16>
  %146 = bitcast <8 x i16> %145 to <2 x i64>
  %147 = bitcast <8 x i16> %142 to <2 x i64>
  %148 = xor <2 x i64> %147, <i64 -1, i64 -1>
  %149 = and <2 x i64> %114, %148
  %150 = xor <2 x i64> %146, <i64 -1, i64 -1>
  %151 = and <2 x i64> %115, %150
  %152 = bitcast <2 x i64> %91 to <8 x i16>
  %153 = bitcast <2 x i64> %149 to <8 x i16>
  %154 = icmp sgt <8 x i16> %152, %153
  %155 = select <8 x i1> %154, <8 x i16> %152, <8 x i16> %153
  %156 = bitcast <2 x i64> %92 to <8 x i16>
  %157 = bitcast <2 x i64> %151 to <8 x i16>
  %158 = icmp sgt <8 x i16> %156, %157
  %159 = select <8 x i1> %158, <8 x i16> %156, <8 x i16> %157
  %160 = bitcast <2 x i64> %138 to <8 x i16>
  %161 = icmp eq <8 x i16> %160, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %162 = sext <8 x i1> %161 to <8 x i16>
  %163 = bitcast <2 x i64> %139 to <8 x i16>
  %164 = icmp eq <8 x i16> %163, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %165 = sext <8 x i1> %164 to <8 x i16>
  %166 = bitcast <8 x i16> %165 to <2 x i64>
  %167 = bitcast <8 x i16> %162 to <2 x i64>
  %168 = xor <2 x i64> %167, <i64 -1, i64 -1>
  %169 = and <2 x i64> %138, %168
  %170 = xor <2 x i64> %166, <i64 -1, i64 -1>
  %171 = and <2 x i64> %139, %170
  %172 = bitcast <2 x i64> %169 to <8 x i16>
  %173 = icmp sgt <8 x i16> %155, %172
  %174 = select <8 x i1> %173, <8 x i16> %155, <8 x i16> %172
  %175 = bitcast <2 x i64> %171 to <8 x i16>
  %176 = icmp sgt <8 x i16> %159, %175
  %177 = select <8 x i1> %176, <8 x i16> %159, <8 x i16> %175
  %178 = icmp sgt <8 x i16> %140, %152
  %179 = select <8 x i1> %178, <8 x i16> %152, <8 x i16> %140
  %180 = icmp sgt <8 x i16> %143, %156
  %181 = select <8 x i1> %180, <8 x i16> %156, <8 x i16> %143
  %182 = icmp slt <8 x i16> %179, %160
  %183 = select <8 x i1> %182, <8 x i16> %179, <8 x i16> %160
  %184 = icmp slt <8 x i16> %181, %163
  %185 = select <8 x i1> %184, <8 x i16> %181, <8 x i16> %163
  %186 = sub <8 x i16> %140, %152
  %187 = sub <8 x i16> %143, %156
  %188 = ashr <8 x i16> %186, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %189 = ashr <8 x i16> %187, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %190 = sub <8 x i16> zeroinitializer, %186
  %191 = icmp sgt <8 x i16> %186, %190
  %192 = select <8 x i1> %191, <8 x i16> %186, <8 x i16> %190
  %193 = sub <8 x i16> zeroinitializer, %187
  %194 = icmp sgt <8 x i16> %187, %193
  %195 = select <8 x i1> %194, <8 x i16> %187, <8 x i16> %193
  %196 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %192, <8 x i16> %55) #7
  %197 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %195, <8 x i16> %55) #7
  %198 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %53, <8 x i16> %196) #7
  %199 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %53, <8 x i16> %197) #7
  %200 = icmp slt <8 x i16> %192, %198
  %201 = select <8 x i1> %200, <8 x i16> %192, <8 x i16> %198
  %202 = icmp slt <8 x i16> %195, %199
  %203 = select <8 x i1> %202, <8 x i16> %195, <8 x i16> %199
  %204 = add <8 x i16> %201, %188
  %205 = add <8 x i16> %203, %189
  %206 = xor <8 x i16> %204, %188
  %207 = xor <8 x i16> %205, %189
  %208 = sub <8 x i16> %160, %152
  %209 = sub <8 x i16> %163, %156
  %210 = ashr <8 x i16> %208, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %211 = ashr <8 x i16> %209, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %212 = sub <8 x i16> zeroinitializer, %208
  %213 = icmp sgt <8 x i16> %208, %212
  %214 = select <8 x i1> %213, <8 x i16> %208, <8 x i16> %212
  %215 = sub <8 x i16> zeroinitializer, %209
  %216 = icmp sgt <8 x i16> %209, %215
  %217 = select <8 x i1> %216, <8 x i16> %209, <8 x i16> %215
  %218 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %214, <8 x i16> %55) #7
  %219 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %217, <8 x i16> %55) #7
  %220 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %53, <8 x i16> %218) #7
  %221 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %53, <8 x i16> %219) #7
  %222 = icmp slt <8 x i16> %214, %220
  %223 = select <8 x i1> %222, <8 x i16> %214, <8 x i16> %220
  %224 = icmp slt <8 x i16> %217, %221
  %225 = select <8 x i1> %224, <8 x i16> %217, <8 x i16> %221
  %226 = add <8 x i16> %223, %210
  %227 = add <8 x i16> %225, %211
  %228 = xor <8 x i16> %226, %210
  %229 = xor <8 x i16> %227, %211
  %230 = add <8 x i16> %228, %206
  %231 = add <8 x i16> %229, %207
  %232 = mul <8 x i16> %230, %59
  %233 = mul <8 x i16> %231, %59
  %234 = sext i32 %14 to i64
  %235 = getelementptr inbounds i16, i16* %2, i64 %234
  %236 = bitcast i16* %235 to i64*
  %237 = load i64, i64* %236, align 1
  %238 = add nsw i32 %14, 144
  %239 = sext i32 %238 to i64
  %240 = getelementptr inbounds i16, i16* %2, i64 %239
  %241 = bitcast i16* %240 to i64*
  %242 = load i64, i64* %241, align 1
  %243 = insertelement <2 x i64> undef, i64 %242, i32 0
  %244 = add nsw i32 %14, 288
  %245 = sext i32 %244 to i64
  %246 = getelementptr inbounds i16, i16* %2, i64 %245
  %247 = bitcast i16* %246 to i64*
  %248 = load i64, i64* %247, align 1
  %249 = add nsw i32 %14, 432
  %250 = sext i32 %249 to i64
  %251 = getelementptr inbounds i16, i16* %2, i64 %250
  %252 = bitcast i16* %251 to i64*
  %253 = load i64, i64* %252, align 1
  %254 = insertelement <2 x i64> undef, i64 %253, i32 0
  %255 = insertelement <2 x i64> %243, i64 %237, i32 1
  %256 = insertelement <2 x i64> %254, i64 %248, i32 1
  %257 = sub nsw i32 0, %14
  %258 = sext i32 %257 to i64
  %259 = getelementptr inbounds i16, i16* %2, i64 %258
  %260 = bitcast i16* %259 to i64*
  %261 = load i64, i64* %260, align 1
  %262 = sub nsw i32 144, %14
  %263 = sext i32 %262 to i64
  %264 = getelementptr inbounds i16, i16* %2, i64 %263
  %265 = bitcast i16* %264 to i64*
  %266 = load i64, i64* %265, align 1
  %267 = insertelement <2 x i64> undef, i64 %266, i32 0
  %268 = sub nsw i32 288, %14
  %269 = sext i32 %268 to i64
  %270 = getelementptr inbounds i16, i16* %2, i64 %269
  %271 = bitcast i16* %270 to i64*
  %272 = load i64, i64* %271, align 1
  %273 = sub nsw i32 432, %14
  %274 = sext i32 %273 to i64
  %275 = getelementptr inbounds i16, i16* %2, i64 %274
  %276 = bitcast i16* %275 to i64*
  %277 = load i64, i64* %276, align 1
  %278 = insertelement <2 x i64> undef, i64 %277, i32 0
  %279 = insertelement <2 x i64> %267, i64 %261, i32 1
  %280 = insertelement <2 x i64> %278, i64 %272, i32 1
  %281 = bitcast <2 x i64> %255 to <8 x i16>
  %282 = icmp eq <8 x i16> %281, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %283 = sext <8 x i1> %282 to <8 x i16>
  %284 = bitcast <2 x i64> %256 to <8 x i16>
  %285 = icmp eq <8 x i16> %284, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %286 = sext <8 x i1> %285 to <8 x i16>
  %287 = bitcast <8 x i16> %286 to <2 x i64>
  %288 = bitcast <8 x i16> %283 to <2 x i64>
  %289 = xor <2 x i64> %288, <i64 -1, i64 -1>
  %290 = and <2 x i64> %255, %289
  %291 = xor <2 x i64> %287, <i64 -1, i64 -1>
  %292 = and <2 x i64> %256, %291
  %293 = bitcast <2 x i64> %290 to <8 x i16>
  %294 = icmp sgt <8 x i16> %174, %293
  %295 = select <8 x i1> %294, <8 x i16> %174, <8 x i16> %293
  %296 = bitcast <2 x i64> %292 to <8 x i16>
  %297 = icmp sgt <8 x i16> %177, %296
  %298 = select <8 x i1> %297, <8 x i16> %177, <8 x i16> %296
  %299 = bitcast <2 x i64> %279 to <8 x i16>
  %300 = icmp eq <8 x i16> %299, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %301 = sext <8 x i1> %300 to <8 x i16>
  %302 = bitcast <2 x i64> %280 to <8 x i16>
  %303 = icmp eq <8 x i16> %302, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %304 = sext <8 x i1> %303 to <8 x i16>
  %305 = bitcast <8 x i16> %304 to <2 x i64>
  %306 = bitcast <8 x i16> %301 to <2 x i64>
  %307 = xor <2 x i64> %306, <i64 -1, i64 -1>
  %308 = and <2 x i64> %279, %307
  %309 = xor <2 x i64> %305, <i64 -1, i64 -1>
  %310 = and <2 x i64> %280, %309
  %311 = bitcast <2 x i64> %308 to <8 x i16>
  %312 = icmp sgt <8 x i16> %295, %311
  %313 = select <8 x i1> %312, <8 x i16> %295, <8 x i16> %311
  %314 = bitcast <2 x i64> %310 to <8 x i16>
  %315 = icmp sgt <8 x i16> %298, %314
  %316 = select <8 x i1> %315, <8 x i16> %298, <8 x i16> %314
  %317 = icmp slt <8 x i16> %183, %281
  %318 = select <8 x i1> %317, <8 x i16> %183, <8 x i16> %281
  %319 = icmp slt <8 x i16> %185, %284
  %320 = select <8 x i1> %319, <8 x i16> %185, <8 x i16> %284
  %321 = icmp slt <8 x i16> %318, %299
  %322 = select <8 x i1> %321, <8 x i16> %318, <8 x i16> %299
  %323 = icmp slt <8 x i16> %320, %302
  %324 = select <8 x i1> %323, <8 x i16> %320, <8 x i16> %302
  %325 = sub <8 x i16> %281, %152
  %326 = sub <8 x i16> %284, %156
  %327 = ashr <8 x i16> %325, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %328 = ashr <8 x i16> %326, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %329 = sub <8 x i16> zeroinitializer, %325
  %330 = icmp sgt <8 x i16> %325, %329
  %331 = select <8 x i1> %330, <8 x i16> %325, <8 x i16> %329
  %332 = sub <8 x i16> zeroinitializer, %326
  %333 = icmp sgt <8 x i16> %326, %332
  %334 = select <8 x i1> %333, <8 x i16> %326, <8 x i16> %332
  %335 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %331, <8 x i16> %55) #7
  %336 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %334, <8 x i16> %55) #7
  %337 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %53, <8 x i16> %335) #7
  %338 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %53, <8 x i16> %336) #7
  %339 = icmp slt <8 x i16> %331, %337
  %340 = select <8 x i1> %339, <8 x i16> %331, <8 x i16> %337
  %341 = icmp slt <8 x i16> %334, %338
  %342 = select <8 x i1> %341, <8 x i16> %334, <8 x i16> %338
  %343 = add <8 x i16> %340, %327
  %344 = add <8 x i16> %342, %328
  %345 = xor <8 x i16> %343, %327
  %346 = xor <8 x i16> %344, %328
  %347 = sub <8 x i16> %299, %152
  %348 = sub <8 x i16> %302, %156
  %349 = ashr <8 x i16> %347, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %350 = ashr <8 x i16> %348, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %351 = sub <8 x i16> zeroinitializer, %347
  %352 = icmp sgt <8 x i16> %347, %351
  %353 = select <8 x i1> %352, <8 x i16> %347, <8 x i16> %351
  %354 = sub <8 x i16> zeroinitializer, %348
  %355 = icmp sgt <8 x i16> %348, %354
  %356 = select <8 x i1> %355, <8 x i16> %348, <8 x i16> %354
  %357 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %353, <8 x i16> %55) #7
  %358 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %356, <8 x i16> %55) #7
  %359 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %53, <8 x i16> %357) #7
  %360 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %53, <8 x i16> %358) #7
  %361 = icmp slt <8 x i16> %353, %359
  %362 = select <8 x i1> %361, <8 x i16> %353, <8 x i16> %359
  %363 = icmp slt <8 x i16> %356, %360
  %364 = select <8 x i1> %363, <8 x i16> %356, <8 x i16> %360
  %365 = add <8 x i16> %362, %349
  %366 = add <8 x i16> %364, %350
  %367 = xor <8 x i16> %365, %349
  %368 = xor <8 x i16> %366, %350
  %369 = add <8 x i16> %367, %345
  %370 = add <8 x i16> %368, %346
  %371 = mul <8 x i16> %369, %64
  %372 = mul <8 x i16> %370, %64
  %373 = add <8 x i16> %371, %232
  %374 = add <8 x i16> %372, %233
  %375 = sext i32 %19 to i64
  %376 = getelementptr inbounds i16, i16* %2, i64 %375
  %377 = bitcast i16* %376 to i64*
  %378 = load i64, i64* %377, align 1
  %379 = add nsw i32 %19, 144
  %380 = sext i32 %379 to i64
  %381 = getelementptr inbounds i16, i16* %2, i64 %380
  %382 = bitcast i16* %381 to i64*
  %383 = load i64, i64* %382, align 1
  %384 = insertelement <2 x i64> undef, i64 %383, i32 0
  %385 = add nsw i32 %19, 288
  %386 = sext i32 %385 to i64
  %387 = getelementptr inbounds i16, i16* %2, i64 %386
  %388 = bitcast i16* %387 to i64*
  %389 = load i64, i64* %388, align 1
  %390 = add nsw i32 %19, 432
  %391 = sext i32 %390 to i64
  %392 = getelementptr inbounds i16, i16* %2, i64 %391
  %393 = bitcast i16* %392 to i64*
  %394 = load i64, i64* %393, align 1
  %395 = insertelement <2 x i64> undef, i64 %394, i32 0
  %396 = insertelement <2 x i64> %384, i64 %378, i32 1
  %397 = insertelement <2 x i64> %395, i64 %389, i32 1
  %398 = sub nsw i32 0, %19
  %399 = sext i32 %398 to i64
  %400 = getelementptr inbounds i16, i16* %2, i64 %399
  %401 = bitcast i16* %400 to i64*
  %402 = load i64, i64* %401, align 1
  %403 = sub nsw i32 144, %19
  %404 = sext i32 %403 to i64
  %405 = getelementptr inbounds i16, i16* %2, i64 %404
  %406 = bitcast i16* %405 to i64*
  %407 = load i64, i64* %406, align 1
  %408 = insertelement <2 x i64> undef, i64 %407, i32 0
  %409 = sub nsw i32 288, %19
  %410 = sext i32 %409 to i64
  %411 = getelementptr inbounds i16, i16* %2, i64 %410
  %412 = bitcast i16* %411 to i64*
  %413 = load i64, i64* %412, align 1
  %414 = sub nsw i32 432, %19
  %415 = sext i32 %414 to i64
  %416 = getelementptr inbounds i16, i16* %2, i64 %415
  %417 = bitcast i16* %416 to i64*
  %418 = load i64, i64* %417, align 1
  %419 = insertelement <2 x i64> undef, i64 %418, i32 0
  %420 = insertelement <2 x i64> %408, i64 %402, i32 1
  %421 = insertelement <2 x i64> %419, i64 %413, i32 1
  %422 = sext i32 %26 to i64
  %423 = getelementptr inbounds i16, i16* %2, i64 %422
  %424 = bitcast i16* %423 to i64*
  %425 = load i64, i64* %424, align 1
  %426 = add nsw i32 %26, 144
  %427 = sext i32 %426 to i64
  %428 = getelementptr inbounds i16, i16* %2, i64 %427
  %429 = bitcast i16* %428 to i64*
  %430 = load i64, i64* %429, align 1
  %431 = insertelement <2 x i64> undef, i64 %430, i32 0
  %432 = add nsw i32 %26, 288
  %433 = sext i32 %432 to i64
  %434 = getelementptr inbounds i16, i16* %2, i64 %433
  %435 = bitcast i16* %434 to i64*
  %436 = load i64, i64* %435, align 1
  %437 = add nsw i32 %26, 432
  %438 = sext i32 %437 to i64
  %439 = getelementptr inbounds i16, i16* %2, i64 %438
  %440 = bitcast i16* %439 to i64*
  %441 = load i64, i64* %440, align 1
  %442 = insertelement <2 x i64> undef, i64 %441, i32 0
  %443 = insertelement <2 x i64> %431, i64 %425, i32 1
  %444 = insertelement <2 x i64> %442, i64 %436, i32 1
  %445 = sub nsw i32 0, %26
  %446 = sext i32 %445 to i64
  %447 = getelementptr inbounds i16, i16* %2, i64 %446
  %448 = bitcast i16* %447 to i64*
  %449 = load i64, i64* %448, align 1
  %450 = sub nsw i32 144, %26
  %451 = sext i32 %450 to i64
  %452 = getelementptr inbounds i16, i16* %2, i64 %451
  %453 = bitcast i16* %452 to i64*
  %454 = load i64, i64* %453, align 1
  %455 = insertelement <2 x i64> undef, i64 %454, i32 0
  %456 = sub nsw i32 288, %26
  %457 = sext i32 %456 to i64
  %458 = getelementptr inbounds i16, i16* %2, i64 %457
  %459 = bitcast i16* %458 to i64*
  %460 = load i64, i64* %459, align 1
  %461 = sub nsw i32 432, %26
  %462 = sext i32 %461 to i64
  %463 = getelementptr inbounds i16, i16* %2, i64 %462
  %464 = bitcast i16* %463 to i64*
  %465 = load i64, i64* %464, align 1
  %466 = insertelement <2 x i64> undef, i64 %465, i32 0
  %467 = insertelement <2 x i64> %455, i64 %449, i32 1
  %468 = insertelement <2 x i64> %466, i64 %460, i32 1
  %469 = bitcast <2 x i64> %396 to <8 x i16>
  %470 = icmp eq <8 x i16> %469, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %471 = sext <8 x i1> %470 to <8 x i16>
  %472 = bitcast <2 x i64> %397 to <8 x i16>
  %473 = icmp eq <8 x i16> %472, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %474 = sext <8 x i1> %473 to <8 x i16>
  %475 = bitcast <8 x i16> %474 to <2 x i64>
  %476 = bitcast <8 x i16> %471 to <2 x i64>
  %477 = xor <2 x i64> %476, <i64 -1, i64 -1>
  %478 = and <2 x i64> %396, %477
  %479 = xor <2 x i64> %475, <i64 -1, i64 -1>
  %480 = and <2 x i64> %397, %479
  %481 = bitcast <2 x i64> %478 to <8 x i16>
  %482 = icmp sgt <8 x i16> %313, %481
  %483 = select <8 x i1> %482, <8 x i16> %313, <8 x i16> %481
  %484 = bitcast <2 x i64> %480 to <8 x i16>
  %485 = icmp sgt <8 x i16> %316, %484
  %486 = select <8 x i1> %485, <8 x i16> %316, <8 x i16> %484
  %487 = bitcast <2 x i64> %420 to <8 x i16>
  %488 = icmp eq <8 x i16> %487, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %489 = sext <8 x i1> %488 to <8 x i16>
  %490 = bitcast <2 x i64> %421 to <8 x i16>
  %491 = icmp eq <8 x i16> %490, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %492 = sext <8 x i1> %491 to <8 x i16>
  %493 = bitcast <8 x i16> %492 to <2 x i64>
  %494 = bitcast <8 x i16> %489 to <2 x i64>
  %495 = xor <2 x i64> %494, <i64 -1, i64 -1>
  %496 = and <2 x i64> %420, %495
  %497 = xor <2 x i64> %493, <i64 -1, i64 -1>
  %498 = and <2 x i64> %421, %497
  %499 = bitcast <2 x i64> %496 to <8 x i16>
  %500 = icmp sgt <8 x i16> %483, %499
  %501 = select <8 x i1> %500, <8 x i16> %483, <8 x i16> %499
  %502 = bitcast <2 x i64> %498 to <8 x i16>
  %503 = icmp sgt <8 x i16> %486, %502
  %504 = select <8 x i1> %503, <8 x i16> %486, <8 x i16> %502
  %505 = bitcast <2 x i64> %443 to <8 x i16>
  %506 = icmp eq <8 x i16> %505, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %507 = sext <8 x i1> %506 to <8 x i16>
  %508 = bitcast <2 x i64> %444 to <8 x i16>
  %509 = icmp eq <8 x i16> %508, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %510 = sext <8 x i1> %509 to <8 x i16>
  %511 = bitcast <8 x i16> %510 to <2 x i64>
  %512 = bitcast <8 x i16> %507 to <2 x i64>
  %513 = xor <2 x i64> %512, <i64 -1, i64 -1>
  %514 = and <2 x i64> %443, %513
  %515 = xor <2 x i64> %511, <i64 -1, i64 -1>
  %516 = and <2 x i64> %444, %515
  %517 = bitcast <2 x i64> %514 to <8 x i16>
  %518 = icmp sgt <8 x i16> %501, %517
  %519 = select <8 x i1> %518, <8 x i16> %501, <8 x i16> %517
  %520 = bitcast <2 x i64> %516 to <8 x i16>
  %521 = icmp sgt <8 x i16> %504, %520
  %522 = select <8 x i1> %521, <8 x i16> %504, <8 x i16> %520
  %523 = bitcast <2 x i64> %467 to <8 x i16>
  %524 = icmp eq <8 x i16> %523, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %525 = sext <8 x i1> %524 to <8 x i16>
  %526 = bitcast <2 x i64> %468 to <8 x i16>
  %527 = icmp eq <8 x i16> %526, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %528 = sext <8 x i1> %527 to <8 x i16>
  %529 = bitcast <8 x i16> %528 to <2 x i64>
  %530 = bitcast <8 x i16> %525 to <2 x i64>
  %531 = xor <2 x i64> %530, <i64 -1, i64 -1>
  %532 = and <2 x i64> %467, %531
  %533 = xor <2 x i64> %529, <i64 -1, i64 -1>
  %534 = and <2 x i64> %468, %533
  %535 = bitcast <2 x i64> %532 to <8 x i16>
  %536 = icmp sgt <8 x i16> %519, %535
  %537 = select <8 x i1> %536, <8 x i16> %519, <8 x i16> %535
  %538 = bitcast <2 x i64> %534 to <8 x i16>
  %539 = icmp sgt <8 x i16> %522, %538
  %540 = select <8 x i1> %539, <8 x i16> %522, <8 x i16> %538
  %541 = icmp slt <8 x i16> %322, %469
  %542 = select <8 x i1> %541, <8 x i16> %322, <8 x i16> %469
  %543 = icmp slt <8 x i16> %324, %472
  %544 = select <8 x i1> %543, <8 x i16> %324, <8 x i16> %472
  %545 = icmp slt <8 x i16> %542, %487
  %546 = select <8 x i1> %545, <8 x i16> %542, <8 x i16> %487
  %547 = icmp slt <8 x i16> %544, %490
  %548 = select <8 x i1> %547, <8 x i16> %544, <8 x i16> %490
  %549 = icmp slt <8 x i16> %546, %505
  %550 = select <8 x i1> %549, <8 x i16> %546, <8 x i16> %505
  %551 = icmp slt <8 x i16> %548, %508
  %552 = select <8 x i1> %551, <8 x i16> %548, <8 x i16> %508
  %553 = icmp slt <8 x i16> %550, %523
  %554 = select <8 x i1> %553, <8 x i16> %550, <8 x i16> %523
  %555 = icmp slt <8 x i16> %552, %526
  %556 = select <8 x i1> %555, <8 x i16> %552, <8 x i16> %526
  %557 = sub <8 x i16> %469, %152
  %558 = sub <8 x i16> %472, %156
  %559 = ashr <8 x i16> %557, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %560 = ashr <8 x i16> %558, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %561 = sub <8 x i16> zeroinitializer, %557
  %562 = icmp sgt <8 x i16> %557, %561
  %563 = select <8 x i1> %562, <8 x i16> %557, <8 x i16> %561
  %564 = sub <8 x i16> zeroinitializer, %558
  %565 = icmp sgt <8 x i16> %558, %564
  %566 = select <8 x i1> %565, <8 x i16> %558, <8 x i16> %564
  %567 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %563, <8 x i16> %69) #7
  %568 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %566, <8 x i16> %69) #7
  %569 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %567) #7
  %570 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %568) #7
  %571 = icmp slt <8 x i16> %563, %569
  %572 = select <8 x i1> %571, <8 x i16> %563, <8 x i16> %569
  %573 = icmp slt <8 x i16> %566, %570
  %574 = select <8 x i1> %573, <8 x i16> %566, <8 x i16> %570
  %575 = add <8 x i16> %572, %559
  %576 = add <8 x i16> %574, %560
  %577 = xor <8 x i16> %575, %559
  %578 = xor <8 x i16> %576, %560
  %579 = sub <8 x i16> %487, %152
  %580 = sub <8 x i16> %490, %156
  %581 = ashr <8 x i16> %579, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %582 = ashr <8 x i16> %580, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %583 = sub <8 x i16> zeroinitializer, %579
  %584 = icmp sgt <8 x i16> %579, %583
  %585 = select <8 x i1> %584, <8 x i16> %579, <8 x i16> %583
  %586 = sub <8 x i16> zeroinitializer, %580
  %587 = icmp sgt <8 x i16> %580, %586
  %588 = select <8 x i1> %587, <8 x i16> %580, <8 x i16> %586
  %589 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %585, <8 x i16> %69) #7
  %590 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %588, <8 x i16> %69) #7
  %591 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %589) #7
  %592 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %590) #7
  %593 = icmp slt <8 x i16> %585, %591
  %594 = select <8 x i1> %593, <8 x i16> %585, <8 x i16> %591
  %595 = icmp slt <8 x i16> %588, %592
  %596 = select <8 x i1> %595, <8 x i16> %588, <8 x i16> %592
  %597 = add <8 x i16> %594, %581
  %598 = add <8 x i16> %596, %582
  %599 = xor <8 x i16> %597, %581
  %600 = xor <8 x i16> %598, %582
  %601 = sub <8 x i16> %505, %152
  %602 = sub <8 x i16> %508, %156
  %603 = ashr <8 x i16> %601, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %604 = ashr <8 x i16> %602, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %605 = sub <8 x i16> zeroinitializer, %601
  %606 = icmp sgt <8 x i16> %601, %605
  %607 = select <8 x i1> %606, <8 x i16> %601, <8 x i16> %605
  %608 = sub <8 x i16> zeroinitializer, %602
  %609 = icmp sgt <8 x i16> %602, %608
  %610 = select <8 x i1> %609, <8 x i16> %602, <8 x i16> %608
  %611 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %607, <8 x i16> %69) #7
  %612 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %610, <8 x i16> %69) #7
  %613 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %611) #7
  %614 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %612) #7
  %615 = icmp slt <8 x i16> %607, %613
  %616 = select <8 x i1> %615, <8 x i16> %607, <8 x i16> %613
  %617 = icmp slt <8 x i16> %610, %614
  %618 = select <8 x i1> %617, <8 x i16> %610, <8 x i16> %614
  %619 = add <8 x i16> %616, %603
  %620 = add <8 x i16> %618, %604
  %621 = xor <8 x i16> %619, %603
  %622 = xor <8 x i16> %620, %604
  %623 = sub <8 x i16> %523, %152
  %624 = sub <8 x i16> %526, %156
  %625 = ashr <8 x i16> %623, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %626 = ashr <8 x i16> %624, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %627 = sub <8 x i16> zeroinitializer, %623
  %628 = icmp sgt <8 x i16> %623, %627
  %629 = select <8 x i1> %628, <8 x i16> %623, <8 x i16> %627
  %630 = sub <8 x i16> zeroinitializer, %624
  %631 = icmp sgt <8 x i16> %624, %630
  %632 = select <8 x i1> %631, <8 x i16> %624, <8 x i16> %630
  %633 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %629, <8 x i16> %69) #7
  %634 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %632, <8 x i16> %69) #7
  %635 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %633) #7
  %636 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %634) #7
  %637 = icmp slt <8 x i16> %629, %635
  %638 = select <8 x i1> %637, <8 x i16> %629, <8 x i16> %635
  %639 = icmp slt <8 x i16> %632, %636
  %640 = select <8 x i1> %639, <8 x i16> %632, <8 x i16> %636
  %641 = add <8 x i16> %638, %625
  %642 = add <8 x i16> %640, %626
  %643 = xor <8 x i16> %641, %625
  %644 = xor <8 x i16> %642, %626
  %645 = add <8 x i16> %599, %577
  %646 = add <8 x i16> %645, %621
  %647 = add <8 x i16> %646, %643
  %648 = add <8 x i16> %600, %578
  %649 = add <8 x i16> %648, %622
  %650 = add <8 x i16> %649, %644
  %651 = mul <8 x i16> %647, %73
  %652 = mul <8 x i16> %650, %73
  %653 = add <8 x i16> %373, %651
  %654 = add <8 x i16> %374, %652
  %655 = sext i32 %21 to i64
  %656 = getelementptr inbounds i16, i16* %2, i64 %655
  %657 = bitcast i16* %656 to i64*
  %658 = load i64, i64* %657, align 1
  %659 = add nsw i32 %21, 144
  %660 = sext i32 %659 to i64
  %661 = getelementptr inbounds i16, i16* %2, i64 %660
  %662 = bitcast i16* %661 to i64*
  %663 = load i64, i64* %662, align 1
  %664 = insertelement <2 x i64> undef, i64 %663, i32 0
  %665 = add nsw i32 %21, 288
  %666 = sext i32 %665 to i64
  %667 = getelementptr inbounds i16, i16* %2, i64 %666
  %668 = bitcast i16* %667 to i64*
  %669 = load i64, i64* %668, align 1
  %670 = add nsw i32 %21, 432
  %671 = sext i32 %670 to i64
  %672 = getelementptr inbounds i16, i16* %2, i64 %671
  %673 = bitcast i16* %672 to i64*
  %674 = load i64, i64* %673, align 1
  %675 = insertelement <2 x i64> undef, i64 %674, i32 0
  %676 = insertelement <2 x i64> %664, i64 %658, i32 1
  %677 = insertelement <2 x i64> %675, i64 %669, i32 1
  %678 = sub nsw i32 0, %21
  %679 = sext i32 %678 to i64
  %680 = getelementptr inbounds i16, i16* %2, i64 %679
  %681 = bitcast i16* %680 to i64*
  %682 = load i64, i64* %681, align 1
  %683 = sub nsw i32 144, %21
  %684 = sext i32 %683 to i64
  %685 = getelementptr inbounds i16, i16* %2, i64 %684
  %686 = bitcast i16* %685 to i64*
  %687 = load i64, i64* %686, align 1
  %688 = insertelement <2 x i64> undef, i64 %687, i32 0
  %689 = sub nsw i32 288, %21
  %690 = sext i32 %689 to i64
  %691 = getelementptr inbounds i16, i16* %2, i64 %690
  %692 = bitcast i16* %691 to i64*
  %693 = load i64, i64* %692, align 1
  %694 = sub nsw i32 432, %21
  %695 = sext i32 %694 to i64
  %696 = getelementptr inbounds i16, i16* %2, i64 %695
  %697 = bitcast i16* %696 to i64*
  %698 = load i64, i64* %697, align 1
  %699 = insertelement <2 x i64> undef, i64 %698, i32 0
  %700 = insertelement <2 x i64> %688, i64 %682, i32 1
  %701 = insertelement <2 x i64> %699, i64 %693, i32 1
  %702 = sext i32 %28 to i64
  %703 = getelementptr inbounds i16, i16* %2, i64 %702
  %704 = bitcast i16* %703 to i64*
  %705 = load i64, i64* %704, align 1
  %706 = add nsw i32 %28, 144
  %707 = sext i32 %706 to i64
  %708 = getelementptr inbounds i16, i16* %2, i64 %707
  %709 = bitcast i16* %708 to i64*
  %710 = load i64, i64* %709, align 1
  %711 = insertelement <2 x i64> undef, i64 %710, i32 0
  %712 = add nsw i32 %28, 288
  %713 = sext i32 %712 to i64
  %714 = getelementptr inbounds i16, i16* %2, i64 %713
  %715 = bitcast i16* %714 to i64*
  %716 = load i64, i64* %715, align 1
  %717 = add nsw i32 %28, 432
  %718 = sext i32 %717 to i64
  %719 = getelementptr inbounds i16, i16* %2, i64 %718
  %720 = bitcast i16* %719 to i64*
  %721 = load i64, i64* %720, align 1
  %722 = insertelement <2 x i64> undef, i64 %721, i32 0
  %723 = insertelement <2 x i64> %711, i64 %705, i32 1
  %724 = insertelement <2 x i64> %722, i64 %716, i32 1
  %725 = sub nsw i32 0, %28
  %726 = sext i32 %725 to i64
  %727 = getelementptr inbounds i16, i16* %2, i64 %726
  %728 = bitcast i16* %727 to i64*
  %729 = load i64, i64* %728, align 1
  %730 = sub nsw i32 144, %28
  %731 = sext i32 %730 to i64
  %732 = getelementptr inbounds i16, i16* %2, i64 %731
  %733 = bitcast i16* %732 to i64*
  %734 = load i64, i64* %733, align 1
  %735 = insertelement <2 x i64> undef, i64 %734, i32 0
  %736 = sub nsw i32 288, %28
  %737 = sext i32 %736 to i64
  %738 = getelementptr inbounds i16, i16* %2, i64 %737
  %739 = bitcast i16* %738 to i64*
  %740 = load i64, i64* %739, align 1
  %741 = sub nsw i32 432, %28
  %742 = sext i32 %741 to i64
  %743 = getelementptr inbounds i16, i16* %2, i64 %742
  %744 = bitcast i16* %743 to i64*
  %745 = load i64, i64* %744, align 1
  %746 = insertelement <2 x i64> undef, i64 %745, i32 0
  %747 = insertelement <2 x i64> %735, i64 %729, i32 1
  %748 = insertelement <2 x i64> %746, i64 %740, i32 1
  %749 = bitcast <2 x i64> %676 to <8 x i16>
  %750 = icmp eq <8 x i16> %749, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %751 = sext <8 x i1> %750 to <8 x i16>
  %752 = bitcast <2 x i64> %677 to <8 x i16>
  %753 = icmp eq <8 x i16> %752, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %754 = sext <8 x i1> %753 to <8 x i16>
  %755 = bitcast <8 x i16> %754 to <2 x i64>
  %756 = bitcast <8 x i16> %751 to <2 x i64>
  %757 = xor <2 x i64> %756, <i64 -1, i64 -1>
  %758 = and <2 x i64> %676, %757
  %759 = xor <2 x i64> %755, <i64 -1, i64 -1>
  %760 = and <2 x i64> %677, %759
  %761 = bitcast <2 x i64> %758 to <8 x i16>
  %762 = icmp sgt <8 x i16> %537, %761
  %763 = select <8 x i1> %762, <8 x i16> %537, <8 x i16> %761
  %764 = bitcast <2 x i64> %760 to <8 x i16>
  %765 = icmp sgt <8 x i16> %540, %764
  %766 = select <8 x i1> %765, <8 x i16> %540, <8 x i16> %764
  %767 = bitcast <2 x i64> %700 to <8 x i16>
  %768 = icmp eq <8 x i16> %767, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %769 = sext <8 x i1> %768 to <8 x i16>
  %770 = bitcast <2 x i64> %701 to <8 x i16>
  %771 = icmp eq <8 x i16> %770, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %772 = sext <8 x i1> %771 to <8 x i16>
  %773 = bitcast <8 x i16> %772 to <2 x i64>
  %774 = bitcast <8 x i16> %769 to <2 x i64>
  %775 = xor <2 x i64> %774, <i64 -1, i64 -1>
  %776 = and <2 x i64> %700, %775
  %777 = xor <2 x i64> %773, <i64 -1, i64 -1>
  %778 = and <2 x i64> %701, %777
  %779 = bitcast <2 x i64> %776 to <8 x i16>
  %780 = icmp sgt <8 x i16> %763, %779
  %781 = select <8 x i1> %780, <8 x i16> %763, <8 x i16> %779
  %782 = bitcast <2 x i64> %778 to <8 x i16>
  %783 = icmp sgt <8 x i16> %766, %782
  %784 = select <8 x i1> %783, <8 x i16> %766, <8 x i16> %782
  %785 = bitcast <2 x i64> %723 to <8 x i16>
  %786 = icmp eq <8 x i16> %785, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %787 = sext <8 x i1> %786 to <8 x i16>
  %788 = bitcast <2 x i64> %724 to <8 x i16>
  %789 = icmp eq <8 x i16> %788, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %790 = sext <8 x i1> %789 to <8 x i16>
  %791 = bitcast <8 x i16> %790 to <2 x i64>
  %792 = bitcast <8 x i16> %787 to <2 x i64>
  %793 = xor <2 x i64> %792, <i64 -1, i64 -1>
  %794 = and <2 x i64> %723, %793
  %795 = xor <2 x i64> %791, <i64 -1, i64 -1>
  %796 = and <2 x i64> %724, %795
  %797 = bitcast <2 x i64> %794 to <8 x i16>
  %798 = icmp sgt <8 x i16> %781, %797
  %799 = select <8 x i1> %798, <8 x i16> %781, <8 x i16> %797
  %800 = bitcast <2 x i64> %796 to <8 x i16>
  %801 = icmp sgt <8 x i16> %784, %800
  %802 = select <8 x i1> %801, <8 x i16> %784, <8 x i16> %800
  %803 = bitcast <2 x i64> %747 to <8 x i16>
  %804 = icmp eq <8 x i16> %803, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %805 = sext <8 x i1> %804 to <8 x i16>
  %806 = bitcast <2 x i64> %748 to <8 x i16>
  %807 = icmp eq <8 x i16> %806, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %808 = sext <8 x i1> %807 to <8 x i16>
  %809 = bitcast <8 x i16> %808 to <2 x i64>
  %810 = bitcast <8 x i16> %805 to <2 x i64>
  %811 = xor <2 x i64> %810, <i64 -1, i64 -1>
  %812 = and <2 x i64> %747, %811
  %813 = xor <2 x i64> %809, <i64 -1, i64 -1>
  %814 = and <2 x i64> %748, %813
  %815 = bitcast <2 x i64> %812 to <8 x i16>
  %816 = icmp sgt <8 x i16> %799, %815
  %817 = select <8 x i1> %816, <8 x i16> %799, <8 x i16> %815
  %818 = bitcast <2 x i64> %814 to <8 x i16>
  %819 = icmp sgt <8 x i16> %802, %818
  %820 = select <8 x i1> %819, <8 x i16> %802, <8 x i16> %818
  %821 = icmp slt <8 x i16> %554, %749
  %822 = select <8 x i1> %821, <8 x i16> %554, <8 x i16> %749
  %823 = icmp slt <8 x i16> %556, %752
  %824 = select <8 x i1> %823, <8 x i16> %556, <8 x i16> %752
  %825 = icmp slt <8 x i16> %822, %767
  %826 = select <8 x i1> %825, <8 x i16> %822, <8 x i16> %767
  %827 = icmp slt <8 x i16> %824, %770
  %828 = select <8 x i1> %827, <8 x i16> %824, <8 x i16> %770
  %829 = icmp slt <8 x i16> %826, %785
  %830 = select <8 x i1> %829, <8 x i16> %826, <8 x i16> %785
  %831 = icmp slt <8 x i16> %828, %788
  %832 = select <8 x i1> %831, <8 x i16> %828, <8 x i16> %788
  %833 = icmp slt <8 x i16> %830, %803
  %834 = select <8 x i1> %833, <8 x i16> %830, <8 x i16> %803
  %835 = icmp slt <8 x i16> %832, %806
  %836 = select <8 x i1> %835, <8 x i16> %832, <8 x i16> %806
  %837 = sub <8 x i16> %749, %152
  %838 = sub <8 x i16> %752, %156
  %839 = ashr <8 x i16> %837, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %840 = ashr <8 x i16> %838, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %841 = sub <8 x i16> zeroinitializer, %837
  %842 = icmp sgt <8 x i16> %837, %841
  %843 = select <8 x i1> %842, <8 x i16> %837, <8 x i16> %841
  %844 = sub <8 x i16> zeroinitializer, %838
  %845 = icmp sgt <8 x i16> %838, %844
  %846 = select <8 x i1> %845, <8 x i16> %838, <8 x i16> %844
  %847 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %843, <8 x i16> %69) #7
  %848 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %846, <8 x i16> %69) #7
  %849 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %847) #7
  %850 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %848) #7
  %851 = icmp slt <8 x i16> %843, %849
  %852 = select <8 x i1> %851, <8 x i16> %843, <8 x i16> %849
  %853 = icmp slt <8 x i16> %846, %850
  %854 = select <8 x i1> %853, <8 x i16> %846, <8 x i16> %850
  %855 = add <8 x i16> %852, %839
  %856 = add <8 x i16> %854, %840
  %857 = xor <8 x i16> %855, %839
  %858 = xor <8 x i16> %856, %840
  %859 = sub <8 x i16> %767, %152
  %860 = sub <8 x i16> %770, %156
  %861 = ashr <8 x i16> %859, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %862 = ashr <8 x i16> %860, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %863 = sub <8 x i16> zeroinitializer, %859
  %864 = icmp sgt <8 x i16> %859, %863
  %865 = select <8 x i1> %864, <8 x i16> %859, <8 x i16> %863
  %866 = sub <8 x i16> zeroinitializer, %860
  %867 = icmp sgt <8 x i16> %860, %866
  %868 = select <8 x i1> %867, <8 x i16> %860, <8 x i16> %866
  %869 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %865, <8 x i16> %69) #7
  %870 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %868, <8 x i16> %69) #7
  %871 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %869) #7
  %872 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %870) #7
  %873 = icmp slt <8 x i16> %865, %871
  %874 = select <8 x i1> %873, <8 x i16> %865, <8 x i16> %871
  %875 = icmp slt <8 x i16> %868, %872
  %876 = select <8 x i1> %875, <8 x i16> %868, <8 x i16> %872
  %877 = add <8 x i16> %874, %861
  %878 = add <8 x i16> %876, %862
  %879 = xor <8 x i16> %877, %861
  %880 = xor <8 x i16> %878, %862
  %881 = sub <8 x i16> %785, %152
  %882 = sub <8 x i16> %788, %156
  %883 = ashr <8 x i16> %881, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %884 = ashr <8 x i16> %882, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %885 = sub <8 x i16> zeroinitializer, %881
  %886 = icmp sgt <8 x i16> %881, %885
  %887 = select <8 x i1> %886, <8 x i16> %881, <8 x i16> %885
  %888 = sub <8 x i16> zeroinitializer, %882
  %889 = icmp sgt <8 x i16> %882, %888
  %890 = select <8 x i1> %889, <8 x i16> %882, <8 x i16> %888
  %891 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %887, <8 x i16> %69) #7
  %892 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %890, <8 x i16> %69) #7
  %893 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %891) #7
  %894 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %892) #7
  %895 = icmp slt <8 x i16> %887, %893
  %896 = select <8 x i1> %895, <8 x i16> %887, <8 x i16> %893
  %897 = icmp slt <8 x i16> %890, %894
  %898 = select <8 x i1> %897, <8 x i16> %890, <8 x i16> %894
  %899 = add <8 x i16> %896, %883
  %900 = add <8 x i16> %898, %884
  %901 = xor <8 x i16> %899, %883
  %902 = xor <8 x i16> %900, %884
  %903 = sub <8 x i16> %803, %152
  %904 = sub <8 x i16> %806, %156
  %905 = ashr <8 x i16> %903, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %906 = ashr <8 x i16> %904, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %907 = sub <8 x i16> zeroinitializer, %903
  %908 = icmp sgt <8 x i16> %903, %907
  %909 = select <8 x i1> %908, <8 x i16> %903, <8 x i16> %907
  %910 = sub <8 x i16> zeroinitializer, %904
  %911 = icmp sgt <8 x i16> %904, %910
  %912 = select <8 x i1> %911, <8 x i16> %904, <8 x i16> %910
  %913 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %909, <8 x i16> %69) #7
  %914 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %912, <8 x i16> %69) #7
  %915 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %913) #7
  %916 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %914) #7
  %917 = icmp slt <8 x i16> %909, %915
  %918 = select <8 x i1> %917, <8 x i16> %909, <8 x i16> %915
  %919 = icmp slt <8 x i16> %912, %916
  %920 = select <8 x i1> %919, <8 x i16> %912, <8 x i16> %916
  %921 = add <8 x i16> %918, %905
  %922 = add <8 x i16> %920, %906
  %923 = xor <8 x i16> %921, %905
  %924 = xor <8 x i16> %922, %906
  %925 = add <8 x i16> %879, %857
  %926 = add <8 x i16> %925, %901
  %927 = add <8 x i16> %926, %923
  %928 = add <8 x i16> %880, %858
  %929 = add <8 x i16> %928, %902
  %930 = add <8 x i16> %929, %924
  %931 = mul <8 x i16> %927, %77
  %932 = mul <8 x i16> %930, %77
  %933 = add <8 x i16> %653, %931
  %934 = add <8 x i16> %654, %932
  %935 = ashr <8 x i16> %933, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %936 = ashr <8 x i16> %934, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %937 = add <8 x i16> %933, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %938 = add <8 x i16> %937, %935
  %939 = add <8 x i16> %934, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %940 = add <8 x i16> %939, %936
  %941 = ashr <8 x i16> %938, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %942 = ashr <8 x i16> %940, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %943 = add <8 x i16> %941, %152
  %944 = add <8 x i16> %942, %156
  %945 = icmp sgt <8 x i16> %943, %834
  %946 = select <8 x i1> %945, <8 x i16> %943, <8 x i16> %834
  %947 = icmp sgt <8 x i16> %944, %836
  %948 = select <8 x i1> %947, <8 x i16> %944, <8 x i16> %836
  %949 = icmp slt <8 x i16> %946, %817
  %950 = select <8 x i1> %949, <8 x i16> %946, <8 x i16> %817
  %951 = icmp slt <8 x i16> %948, %820
  %952 = select <8 x i1> %951, <8 x i16> %948, <8 x i16> %820
  %953 = bitcast <8 x i16> %952 to <2 x i64>
  %954 = bitcast <8 x i16> %950 to <2 x i64>
  %955 = bitcast <8 x i16> %950 to <16 x i8>
  %956 = shufflevector <16 x i8> %955, <16 x i8> undef, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %957 = bitcast <16 x i8> %956 to <2 x i64>
  %958 = extractelement <2 x i64> %957, i32 0
  %959 = bitcast i16* %0 to i64*
  store i64 %958, i64* %959, align 1
  %960 = sext i32 %1 to i64
  %961 = getelementptr inbounds i16, i16* %0, i64 %960
  %962 = extractelement <2 x i64> %954, i32 0
  %963 = bitcast i16* %961 to i64*
  store i64 %962, i64* %963, align 1
  %964 = shl nsw i32 %1, 1
  %965 = sext i32 %964 to i64
  %966 = getelementptr inbounds i16, i16* %0, i64 %965
  %967 = bitcast <8 x i16> %952 to <16 x i8>
  %968 = shufflevector <16 x i8> %967, <16 x i8> undef, <16 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
  %969 = bitcast <16 x i8> %968 to <2 x i64>
  %970 = extractelement <2 x i64> %969, i32 0
  %971 = bitcast i16* %966 to i64*
  store i64 %970, i64* %971, align 1
  %972 = mul nsw i32 %1, 3
  %973 = sext i32 %972 to i64
  %974 = getelementptr inbounds i16, i16* %0, i64 %973
  %975 = extractelement <2 x i64> %953, i32 0
  %976 = bitcast i16* %974 to i64*
  store i64 %975, i64* %976, align 1
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @cdef_filter_block_8x8_16_sse2(i16* nocapture, i32, i16* readonly, i32, i32, i32, i32, i32, i32) local_unnamed_addr #0 {
  %10 = sext i32 %5 to i64
  %11 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %10, i64 0
  %12 = load i32, i32* %11, align 8
  %13 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %10, i64 1
  %14 = load i32, i32* %13, align 4
  %15 = add nsw i32 %5, 2
  %16 = and i32 %15, 7
  %17 = zext i32 %16 to i64
  %18 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %17, i64 0
  %19 = load i32, i32* %18, align 8
  %20 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %17, i64 1
  %21 = load i32, i32* %20, align 4
  %22 = add nsw i32 %5, 6
  %23 = and i32 %22, 7
  %24 = zext i32 %23 to i64
  %25 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %24, i64 0
  %26 = load i32, i32* %25, align 8
  %27 = getelementptr inbounds [8 x [2 x i32]], [8 x [2 x i32]]* @cdef_directions, i64 0, i64 %24, i64 1
  %28 = load i32, i32* %27, align 4
  %29 = lshr i32 %3, %8
  %30 = and i32 %29, 1
  %31 = zext i32 %30 to i64
  %32 = getelementptr inbounds [2 x [2 x i32]], [2 x [2 x i32]]* @cdef_pri_taps, i64 0, i64 %31, i64 0
  %33 = icmp eq i32 %3, 0
  br i1 %33, label %40, label %34

34:                                               ; preds = %9
  %35 = tail call i32 @llvm.ctlz.i32(i32 %3, i1 true) #7, !range !2
  %36 = xor i32 %35, 31
  %37 = icmp sgt i32 %36, %6
  %38 = sub nsw i32 %6, %36
  %39 = select i1 %37, i32 0, i32 %38
  br label %40

40:                                               ; preds = %34, %9
  %41 = phi i32 [ %6, %9 ], [ %39, %34 ]
  %42 = icmp eq i32 %4, 0
  br i1 %42, label %49, label %43

43:                                               ; preds = %40
  %44 = tail call i32 @llvm.ctlz.i32(i32 %4, i1 true) #7, !range !2
  %45 = xor i32 %44, 31
  %46 = icmp sgt i32 %45, %7
  %47 = sub nsw i32 %7, %45
  %48 = select i1 %46, i32 0, i32 %47
  br label %49

49:                                               ; preds = %43, %40
  %50 = phi i32 [ %7, %40 ], [ %48, %43 ]
  %51 = trunc i32 %3 to i16
  %52 = insertelement <8 x i16> undef, i16 %51, i32 0
  %53 = shufflevector <8 x i16> %52, <8 x i16> undef, <8 x i32> zeroinitializer
  %54 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %41, i32 0
  %55 = bitcast <4 x i32> %54 to <8 x i16>
  %56 = load i32, i32* %32, align 8
  %57 = trunc i32 %56 to i16
  %58 = insertelement <8 x i16> undef, i16 %57, i32 0
  %59 = shufflevector <8 x i16> %58, <8 x i16> undef, <8 x i32> zeroinitializer
  %60 = getelementptr inbounds [2 x [2 x i32]], [2 x [2 x i32]]* @cdef_pri_taps, i64 0, i64 %31, i64 1
  %61 = load i32, i32* %60, align 4
  %62 = trunc i32 %61 to i16
  %63 = insertelement <8 x i16> undef, i16 %62, i32 0
  %64 = shufflevector <8 x i16> %63, <8 x i16> undef, <8 x i32> zeroinitializer
  %65 = trunc i32 %4 to i16
  %66 = insertelement <8 x i16> undef, i16 %65, i32 0
  %67 = shufflevector <8 x i16> %66, <8 x i16> undef, <8 x i32> zeroinitializer
  %68 = insertelement <4 x i32> <i32 undef, i32 0, i32 undef, i32 undef>, i32 %50, i32 0
  %69 = bitcast <4 x i32> %68 to <8 x i16>
  %70 = load i32, i32* getelementptr inbounds ([2 x i32], [2 x i32]* @cdef_sec_taps, i64 0, i64 0), align 4
  %71 = trunc i32 %70 to i16
  %72 = insertelement <8 x i16> undef, i16 %71, i32 0
  %73 = shufflevector <8 x i16> %72, <8 x i16> undef, <8 x i32> zeroinitializer
  %74 = load i32, i32* getelementptr inbounds ([2 x i32], [2 x i32]* @cdef_sec_taps, i64 0, i64 1), align 4
  %75 = trunc i32 %74 to i16
  %76 = insertelement <8 x i16> undef, i16 %75, i32 0
  %77 = shufflevector <8 x i16> %76, <8 x i16> undef, <8 x i32> zeroinitializer
  %78 = sext i32 %1 to i64
  %79 = sext i32 %12 to i64
  %80 = sext i32 %14 to i64
  %81 = sext i32 %19 to i64
  %82 = sext i32 %26 to i64
  %83 = sext i32 %21 to i64
  %84 = sext i32 %28 to i64
  br label %85

85:                                               ; preds = %49, %85
  %86 = phi i64 [ 0, %49 ], [ %774, %85 ]
  %87 = mul nuw nsw i64 %86, 144
  %88 = getelementptr inbounds i16, i16* %2, i64 %87
  %89 = bitcast i16* %88 to <8 x i16>*
  %90 = load <8 x i16>, <8 x i16>* %89, align 16
  %91 = or i64 %86, 1
  %92 = mul nuw nsw i64 %91, 144
  %93 = getelementptr inbounds i16, i16* %2, i64 %92
  %94 = bitcast i16* %93 to <8 x i16>*
  %95 = load <8 x i16>, <8 x i16>* %94, align 16
  %96 = add nsw i64 %87, %79
  %97 = getelementptr inbounds i16, i16* %2, i64 %96
  %98 = bitcast i16* %97 to <2 x i64>*
  %99 = load <2 x i64>, <2 x i64>* %98, align 1
  %100 = add nsw i64 %92, %79
  %101 = getelementptr inbounds i16, i16* %2, i64 %100
  %102 = bitcast i16* %101 to <2 x i64>*
  %103 = load <2 x i64>, <2 x i64>* %102, align 1
  %104 = sub nsw i64 %87, %79
  %105 = getelementptr inbounds i16, i16* %2, i64 %104
  %106 = bitcast i16* %105 to <2 x i64>*
  %107 = load <2 x i64>, <2 x i64>* %106, align 1
  %108 = sub nsw i64 %92, %79
  %109 = getelementptr inbounds i16, i16* %2, i64 %108
  %110 = bitcast i16* %109 to <2 x i64>*
  %111 = load <2 x i64>, <2 x i64>* %110, align 1
  %112 = bitcast <2 x i64> %99 to <8 x i16>
  %113 = icmp eq <8 x i16> %112, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %114 = sext <8 x i1> %113 to <8 x i16>
  %115 = bitcast <2 x i64> %103 to <8 x i16>
  %116 = icmp eq <8 x i16> %115, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %117 = sext <8 x i1> %116 to <8 x i16>
  %118 = bitcast <8 x i16> %117 to <2 x i64>
  %119 = bitcast <8 x i16> %114 to <2 x i64>
  %120 = xor <2 x i64> %119, <i64 -1, i64 -1>
  %121 = and <2 x i64> %99, %120
  %122 = xor <2 x i64> %118, <i64 -1, i64 -1>
  %123 = and <2 x i64> %103, %122
  %124 = bitcast <2 x i64> %121 to <8 x i16>
  %125 = icmp sgt <8 x i16> %90, %124
  %126 = select <8 x i1> %125, <8 x i16> %90, <8 x i16> %124
  %127 = bitcast <2 x i64> %123 to <8 x i16>
  %128 = icmp sgt <8 x i16> %95, %127
  %129 = select <8 x i1> %128, <8 x i16> %95, <8 x i16> %127
  %130 = bitcast <2 x i64> %107 to <8 x i16>
  %131 = icmp eq <8 x i16> %130, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %132 = sext <8 x i1> %131 to <8 x i16>
  %133 = bitcast <2 x i64> %111 to <8 x i16>
  %134 = icmp eq <8 x i16> %133, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %135 = sext <8 x i1> %134 to <8 x i16>
  %136 = bitcast <8 x i16> %135 to <2 x i64>
  %137 = bitcast <8 x i16> %132 to <2 x i64>
  %138 = xor <2 x i64> %137, <i64 -1, i64 -1>
  %139 = and <2 x i64> %107, %138
  %140 = xor <2 x i64> %136, <i64 -1, i64 -1>
  %141 = and <2 x i64> %111, %140
  %142 = bitcast <2 x i64> %139 to <8 x i16>
  %143 = icmp sgt <8 x i16> %126, %142
  %144 = select <8 x i1> %143, <8 x i16> %126, <8 x i16> %142
  %145 = bitcast <2 x i64> %141 to <8 x i16>
  %146 = icmp sgt <8 x i16> %129, %145
  %147 = select <8 x i1> %146, <8 x i16> %129, <8 x i16> %145
  %148 = icmp slt <8 x i16> %90, %112
  %149 = select <8 x i1> %148, <8 x i16> %90, <8 x i16> %112
  %150 = icmp slt <8 x i16> %95, %115
  %151 = select <8 x i1> %150, <8 x i16> %95, <8 x i16> %115
  %152 = icmp slt <8 x i16> %149, %130
  %153 = select <8 x i1> %152, <8 x i16> %149, <8 x i16> %130
  %154 = icmp slt <8 x i16> %151, %133
  %155 = select <8 x i1> %154, <8 x i16> %151, <8 x i16> %133
  %156 = sub <8 x i16> %112, %90
  %157 = sub <8 x i16> %115, %95
  %158 = ashr <8 x i16> %156, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %159 = ashr <8 x i16> %157, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %160 = sub <8 x i16> zeroinitializer, %156
  %161 = icmp sgt <8 x i16> %156, %160
  %162 = select <8 x i1> %161, <8 x i16> %156, <8 x i16> %160
  %163 = sub <8 x i16> zeroinitializer, %157
  %164 = icmp sgt <8 x i16> %157, %163
  %165 = select <8 x i1> %164, <8 x i16> %157, <8 x i16> %163
  %166 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %162, <8 x i16> %55) #7
  %167 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %165, <8 x i16> %55) #7
  %168 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %53, <8 x i16> %166) #7
  %169 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %53, <8 x i16> %167) #7
  %170 = icmp slt <8 x i16> %162, %168
  %171 = select <8 x i1> %170, <8 x i16> %162, <8 x i16> %168
  %172 = icmp slt <8 x i16> %165, %169
  %173 = select <8 x i1> %172, <8 x i16> %165, <8 x i16> %169
  %174 = add <8 x i16> %171, %158
  %175 = add <8 x i16> %173, %159
  %176 = xor <8 x i16> %174, %158
  %177 = xor <8 x i16> %175, %159
  %178 = sub <8 x i16> %130, %90
  %179 = sub <8 x i16> %133, %95
  %180 = ashr <8 x i16> %178, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %181 = ashr <8 x i16> %179, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %182 = sub <8 x i16> zeroinitializer, %178
  %183 = icmp sgt <8 x i16> %178, %182
  %184 = select <8 x i1> %183, <8 x i16> %178, <8 x i16> %182
  %185 = sub <8 x i16> zeroinitializer, %179
  %186 = icmp sgt <8 x i16> %179, %185
  %187 = select <8 x i1> %186, <8 x i16> %179, <8 x i16> %185
  %188 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %184, <8 x i16> %55) #7
  %189 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %187, <8 x i16> %55) #7
  %190 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %53, <8 x i16> %188) #7
  %191 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %53, <8 x i16> %189) #7
  %192 = icmp slt <8 x i16> %184, %190
  %193 = select <8 x i1> %192, <8 x i16> %184, <8 x i16> %190
  %194 = icmp slt <8 x i16> %187, %191
  %195 = select <8 x i1> %194, <8 x i16> %187, <8 x i16> %191
  %196 = add <8 x i16> %193, %180
  %197 = add <8 x i16> %195, %181
  %198 = xor <8 x i16> %196, %180
  %199 = xor <8 x i16> %197, %181
  %200 = add <8 x i16> %198, %176
  %201 = add <8 x i16> %199, %177
  %202 = mul <8 x i16> %200, %59
  %203 = mul <8 x i16> %201, %59
  %204 = add nsw i64 %87, %80
  %205 = getelementptr inbounds i16, i16* %2, i64 %204
  %206 = bitcast i16* %205 to <2 x i64>*
  %207 = load <2 x i64>, <2 x i64>* %206, align 1
  %208 = add nsw i64 %92, %80
  %209 = getelementptr inbounds i16, i16* %2, i64 %208
  %210 = bitcast i16* %209 to <2 x i64>*
  %211 = load <2 x i64>, <2 x i64>* %210, align 1
  %212 = sub nsw i64 %87, %80
  %213 = getelementptr inbounds i16, i16* %2, i64 %212
  %214 = bitcast i16* %213 to <2 x i64>*
  %215 = load <2 x i64>, <2 x i64>* %214, align 1
  %216 = sub nsw i64 %92, %80
  %217 = getelementptr inbounds i16, i16* %2, i64 %216
  %218 = bitcast i16* %217 to <2 x i64>*
  %219 = load <2 x i64>, <2 x i64>* %218, align 1
  %220 = bitcast <2 x i64> %207 to <8 x i16>
  %221 = icmp eq <8 x i16> %220, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %222 = sext <8 x i1> %221 to <8 x i16>
  %223 = bitcast <2 x i64> %211 to <8 x i16>
  %224 = icmp eq <8 x i16> %223, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %225 = sext <8 x i1> %224 to <8 x i16>
  %226 = bitcast <8 x i16> %225 to <2 x i64>
  %227 = bitcast <8 x i16> %222 to <2 x i64>
  %228 = xor <2 x i64> %227, <i64 -1, i64 -1>
  %229 = and <2 x i64> %207, %228
  %230 = xor <2 x i64> %226, <i64 -1, i64 -1>
  %231 = and <2 x i64> %211, %230
  %232 = bitcast <2 x i64> %229 to <8 x i16>
  %233 = icmp sgt <8 x i16> %144, %232
  %234 = select <8 x i1> %233, <8 x i16> %144, <8 x i16> %232
  %235 = bitcast <2 x i64> %231 to <8 x i16>
  %236 = icmp sgt <8 x i16> %147, %235
  %237 = select <8 x i1> %236, <8 x i16> %147, <8 x i16> %235
  %238 = bitcast <2 x i64> %215 to <8 x i16>
  %239 = icmp eq <8 x i16> %238, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %240 = sext <8 x i1> %239 to <8 x i16>
  %241 = bitcast <2 x i64> %219 to <8 x i16>
  %242 = icmp eq <8 x i16> %241, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %243 = sext <8 x i1> %242 to <8 x i16>
  %244 = bitcast <8 x i16> %243 to <2 x i64>
  %245 = bitcast <8 x i16> %240 to <2 x i64>
  %246 = xor <2 x i64> %245, <i64 -1, i64 -1>
  %247 = and <2 x i64> %215, %246
  %248 = xor <2 x i64> %244, <i64 -1, i64 -1>
  %249 = and <2 x i64> %219, %248
  %250 = bitcast <2 x i64> %247 to <8 x i16>
  %251 = icmp sgt <8 x i16> %234, %250
  %252 = select <8 x i1> %251, <8 x i16> %234, <8 x i16> %250
  %253 = bitcast <2 x i64> %249 to <8 x i16>
  %254 = icmp sgt <8 x i16> %237, %253
  %255 = select <8 x i1> %254, <8 x i16> %237, <8 x i16> %253
  %256 = icmp slt <8 x i16> %153, %220
  %257 = select <8 x i1> %256, <8 x i16> %153, <8 x i16> %220
  %258 = icmp slt <8 x i16> %155, %223
  %259 = select <8 x i1> %258, <8 x i16> %155, <8 x i16> %223
  %260 = icmp slt <8 x i16> %257, %238
  %261 = select <8 x i1> %260, <8 x i16> %257, <8 x i16> %238
  %262 = icmp slt <8 x i16> %259, %241
  %263 = select <8 x i1> %262, <8 x i16> %259, <8 x i16> %241
  %264 = sub <8 x i16> %220, %90
  %265 = sub <8 x i16> %223, %95
  %266 = ashr <8 x i16> %264, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %267 = ashr <8 x i16> %265, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %268 = sub <8 x i16> zeroinitializer, %264
  %269 = icmp sgt <8 x i16> %264, %268
  %270 = select <8 x i1> %269, <8 x i16> %264, <8 x i16> %268
  %271 = sub <8 x i16> zeroinitializer, %265
  %272 = icmp sgt <8 x i16> %265, %271
  %273 = select <8 x i1> %272, <8 x i16> %265, <8 x i16> %271
  %274 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %270, <8 x i16> %55) #7
  %275 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %273, <8 x i16> %55) #7
  %276 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %53, <8 x i16> %274) #7
  %277 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %53, <8 x i16> %275) #7
  %278 = icmp slt <8 x i16> %270, %276
  %279 = select <8 x i1> %278, <8 x i16> %270, <8 x i16> %276
  %280 = icmp slt <8 x i16> %273, %277
  %281 = select <8 x i1> %280, <8 x i16> %273, <8 x i16> %277
  %282 = add <8 x i16> %279, %266
  %283 = add <8 x i16> %281, %267
  %284 = xor <8 x i16> %282, %266
  %285 = xor <8 x i16> %283, %267
  %286 = sub <8 x i16> %238, %90
  %287 = sub <8 x i16> %241, %95
  %288 = ashr <8 x i16> %286, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %289 = ashr <8 x i16> %287, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %290 = sub <8 x i16> zeroinitializer, %286
  %291 = icmp sgt <8 x i16> %286, %290
  %292 = select <8 x i1> %291, <8 x i16> %286, <8 x i16> %290
  %293 = sub <8 x i16> zeroinitializer, %287
  %294 = icmp sgt <8 x i16> %287, %293
  %295 = select <8 x i1> %294, <8 x i16> %287, <8 x i16> %293
  %296 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %292, <8 x i16> %55) #7
  %297 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %295, <8 x i16> %55) #7
  %298 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %53, <8 x i16> %296) #7
  %299 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %53, <8 x i16> %297) #7
  %300 = icmp slt <8 x i16> %292, %298
  %301 = select <8 x i1> %300, <8 x i16> %292, <8 x i16> %298
  %302 = icmp slt <8 x i16> %295, %299
  %303 = select <8 x i1> %302, <8 x i16> %295, <8 x i16> %299
  %304 = add <8 x i16> %301, %288
  %305 = add <8 x i16> %303, %289
  %306 = xor <8 x i16> %304, %288
  %307 = xor <8 x i16> %305, %289
  %308 = add <8 x i16> %306, %284
  %309 = add <8 x i16> %307, %285
  %310 = mul <8 x i16> %308, %64
  %311 = mul <8 x i16> %309, %64
  %312 = add <8 x i16> %310, %202
  %313 = add <8 x i16> %311, %203
  %314 = add nsw i64 %87, %81
  %315 = getelementptr inbounds i16, i16* %2, i64 %314
  %316 = bitcast i16* %315 to <2 x i64>*
  %317 = load <2 x i64>, <2 x i64>* %316, align 1
  %318 = add nsw i64 %92, %81
  %319 = getelementptr inbounds i16, i16* %2, i64 %318
  %320 = bitcast i16* %319 to <2 x i64>*
  %321 = load <2 x i64>, <2 x i64>* %320, align 1
  %322 = sub nsw i64 %87, %81
  %323 = getelementptr inbounds i16, i16* %2, i64 %322
  %324 = bitcast i16* %323 to <2 x i64>*
  %325 = load <2 x i64>, <2 x i64>* %324, align 1
  %326 = sub nsw i64 %92, %81
  %327 = getelementptr inbounds i16, i16* %2, i64 %326
  %328 = bitcast i16* %327 to <2 x i64>*
  %329 = load <2 x i64>, <2 x i64>* %328, align 1
  %330 = add nsw i64 %87, %82
  %331 = getelementptr inbounds i16, i16* %2, i64 %330
  %332 = bitcast i16* %331 to <2 x i64>*
  %333 = load <2 x i64>, <2 x i64>* %332, align 1
  %334 = add nsw i64 %92, %82
  %335 = getelementptr inbounds i16, i16* %2, i64 %334
  %336 = bitcast i16* %335 to <2 x i64>*
  %337 = load <2 x i64>, <2 x i64>* %336, align 1
  %338 = sub nsw i64 %87, %82
  %339 = getelementptr inbounds i16, i16* %2, i64 %338
  %340 = bitcast i16* %339 to <2 x i64>*
  %341 = load <2 x i64>, <2 x i64>* %340, align 1
  %342 = sub nsw i64 %92, %82
  %343 = getelementptr inbounds i16, i16* %2, i64 %342
  %344 = bitcast i16* %343 to <2 x i64>*
  %345 = load <2 x i64>, <2 x i64>* %344, align 1
  %346 = bitcast <2 x i64> %317 to <8 x i16>
  %347 = icmp eq <8 x i16> %346, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %348 = sext <8 x i1> %347 to <8 x i16>
  %349 = bitcast <2 x i64> %321 to <8 x i16>
  %350 = icmp eq <8 x i16> %349, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %351 = sext <8 x i1> %350 to <8 x i16>
  %352 = bitcast <8 x i16> %351 to <2 x i64>
  %353 = bitcast <8 x i16> %348 to <2 x i64>
  %354 = xor <2 x i64> %353, <i64 -1, i64 -1>
  %355 = and <2 x i64> %317, %354
  %356 = xor <2 x i64> %352, <i64 -1, i64 -1>
  %357 = and <2 x i64> %321, %356
  %358 = bitcast <2 x i64> %355 to <8 x i16>
  %359 = icmp sgt <8 x i16> %252, %358
  %360 = select <8 x i1> %359, <8 x i16> %252, <8 x i16> %358
  %361 = bitcast <2 x i64> %357 to <8 x i16>
  %362 = icmp sgt <8 x i16> %255, %361
  %363 = select <8 x i1> %362, <8 x i16> %255, <8 x i16> %361
  %364 = bitcast <2 x i64> %325 to <8 x i16>
  %365 = icmp eq <8 x i16> %364, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %366 = sext <8 x i1> %365 to <8 x i16>
  %367 = bitcast <2 x i64> %329 to <8 x i16>
  %368 = icmp eq <8 x i16> %367, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %369 = sext <8 x i1> %368 to <8 x i16>
  %370 = bitcast <8 x i16> %369 to <2 x i64>
  %371 = bitcast <8 x i16> %366 to <2 x i64>
  %372 = xor <2 x i64> %371, <i64 -1, i64 -1>
  %373 = and <2 x i64> %325, %372
  %374 = xor <2 x i64> %370, <i64 -1, i64 -1>
  %375 = and <2 x i64> %329, %374
  %376 = bitcast <2 x i64> %373 to <8 x i16>
  %377 = icmp sgt <8 x i16> %360, %376
  %378 = select <8 x i1> %377, <8 x i16> %360, <8 x i16> %376
  %379 = bitcast <2 x i64> %375 to <8 x i16>
  %380 = icmp sgt <8 x i16> %363, %379
  %381 = select <8 x i1> %380, <8 x i16> %363, <8 x i16> %379
  %382 = bitcast <2 x i64> %333 to <8 x i16>
  %383 = icmp eq <8 x i16> %382, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %384 = sext <8 x i1> %383 to <8 x i16>
  %385 = bitcast <2 x i64> %337 to <8 x i16>
  %386 = icmp eq <8 x i16> %385, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %387 = sext <8 x i1> %386 to <8 x i16>
  %388 = bitcast <8 x i16> %387 to <2 x i64>
  %389 = bitcast <8 x i16> %384 to <2 x i64>
  %390 = xor <2 x i64> %389, <i64 -1, i64 -1>
  %391 = and <2 x i64> %333, %390
  %392 = xor <2 x i64> %388, <i64 -1, i64 -1>
  %393 = and <2 x i64> %337, %392
  %394 = bitcast <2 x i64> %391 to <8 x i16>
  %395 = icmp sgt <8 x i16> %378, %394
  %396 = select <8 x i1> %395, <8 x i16> %378, <8 x i16> %394
  %397 = bitcast <2 x i64> %393 to <8 x i16>
  %398 = icmp sgt <8 x i16> %381, %397
  %399 = select <8 x i1> %398, <8 x i16> %381, <8 x i16> %397
  %400 = bitcast <2 x i64> %341 to <8 x i16>
  %401 = icmp eq <8 x i16> %400, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %402 = sext <8 x i1> %401 to <8 x i16>
  %403 = bitcast <2 x i64> %345 to <8 x i16>
  %404 = icmp eq <8 x i16> %403, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %405 = sext <8 x i1> %404 to <8 x i16>
  %406 = bitcast <8 x i16> %405 to <2 x i64>
  %407 = bitcast <8 x i16> %402 to <2 x i64>
  %408 = xor <2 x i64> %407, <i64 -1, i64 -1>
  %409 = and <2 x i64> %341, %408
  %410 = xor <2 x i64> %406, <i64 -1, i64 -1>
  %411 = and <2 x i64> %345, %410
  %412 = bitcast <2 x i64> %409 to <8 x i16>
  %413 = icmp sgt <8 x i16> %396, %412
  %414 = select <8 x i1> %413, <8 x i16> %396, <8 x i16> %412
  %415 = bitcast <2 x i64> %411 to <8 x i16>
  %416 = icmp sgt <8 x i16> %399, %415
  %417 = select <8 x i1> %416, <8 x i16> %399, <8 x i16> %415
  %418 = icmp slt <8 x i16> %261, %346
  %419 = select <8 x i1> %418, <8 x i16> %261, <8 x i16> %346
  %420 = icmp slt <8 x i16> %263, %349
  %421 = select <8 x i1> %420, <8 x i16> %263, <8 x i16> %349
  %422 = icmp slt <8 x i16> %419, %364
  %423 = select <8 x i1> %422, <8 x i16> %419, <8 x i16> %364
  %424 = icmp slt <8 x i16> %421, %367
  %425 = select <8 x i1> %424, <8 x i16> %421, <8 x i16> %367
  %426 = icmp slt <8 x i16> %423, %382
  %427 = select <8 x i1> %426, <8 x i16> %423, <8 x i16> %382
  %428 = icmp slt <8 x i16> %425, %385
  %429 = select <8 x i1> %428, <8 x i16> %425, <8 x i16> %385
  %430 = icmp slt <8 x i16> %427, %400
  %431 = select <8 x i1> %430, <8 x i16> %427, <8 x i16> %400
  %432 = icmp slt <8 x i16> %429, %403
  %433 = select <8 x i1> %432, <8 x i16> %429, <8 x i16> %403
  %434 = sub <8 x i16> %346, %90
  %435 = sub <8 x i16> %349, %95
  %436 = ashr <8 x i16> %434, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %437 = ashr <8 x i16> %435, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %438 = sub <8 x i16> zeroinitializer, %434
  %439 = icmp sgt <8 x i16> %434, %438
  %440 = select <8 x i1> %439, <8 x i16> %434, <8 x i16> %438
  %441 = sub <8 x i16> zeroinitializer, %435
  %442 = icmp sgt <8 x i16> %435, %441
  %443 = select <8 x i1> %442, <8 x i16> %435, <8 x i16> %441
  %444 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %440, <8 x i16> %69) #7
  %445 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %443, <8 x i16> %69) #7
  %446 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %444) #7
  %447 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %445) #7
  %448 = icmp slt <8 x i16> %440, %446
  %449 = select <8 x i1> %448, <8 x i16> %440, <8 x i16> %446
  %450 = icmp slt <8 x i16> %443, %447
  %451 = select <8 x i1> %450, <8 x i16> %443, <8 x i16> %447
  %452 = add <8 x i16> %449, %436
  %453 = add <8 x i16> %451, %437
  %454 = xor <8 x i16> %452, %436
  %455 = xor <8 x i16> %453, %437
  %456 = sub <8 x i16> %364, %90
  %457 = sub <8 x i16> %367, %95
  %458 = ashr <8 x i16> %456, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %459 = ashr <8 x i16> %457, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %460 = sub <8 x i16> zeroinitializer, %456
  %461 = icmp sgt <8 x i16> %456, %460
  %462 = select <8 x i1> %461, <8 x i16> %456, <8 x i16> %460
  %463 = sub <8 x i16> zeroinitializer, %457
  %464 = icmp sgt <8 x i16> %457, %463
  %465 = select <8 x i1> %464, <8 x i16> %457, <8 x i16> %463
  %466 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %462, <8 x i16> %69) #7
  %467 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %465, <8 x i16> %69) #7
  %468 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %466) #7
  %469 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %467) #7
  %470 = icmp slt <8 x i16> %462, %468
  %471 = select <8 x i1> %470, <8 x i16> %462, <8 x i16> %468
  %472 = icmp slt <8 x i16> %465, %469
  %473 = select <8 x i1> %472, <8 x i16> %465, <8 x i16> %469
  %474 = add <8 x i16> %471, %458
  %475 = add <8 x i16> %473, %459
  %476 = xor <8 x i16> %474, %458
  %477 = xor <8 x i16> %475, %459
  %478 = sub <8 x i16> %382, %90
  %479 = sub <8 x i16> %385, %95
  %480 = ashr <8 x i16> %478, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %481 = ashr <8 x i16> %479, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %482 = sub <8 x i16> zeroinitializer, %478
  %483 = icmp sgt <8 x i16> %478, %482
  %484 = select <8 x i1> %483, <8 x i16> %478, <8 x i16> %482
  %485 = sub <8 x i16> zeroinitializer, %479
  %486 = icmp sgt <8 x i16> %479, %485
  %487 = select <8 x i1> %486, <8 x i16> %479, <8 x i16> %485
  %488 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %484, <8 x i16> %69) #7
  %489 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %487, <8 x i16> %69) #7
  %490 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %488) #7
  %491 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %489) #7
  %492 = icmp slt <8 x i16> %484, %490
  %493 = select <8 x i1> %492, <8 x i16> %484, <8 x i16> %490
  %494 = icmp slt <8 x i16> %487, %491
  %495 = select <8 x i1> %494, <8 x i16> %487, <8 x i16> %491
  %496 = add <8 x i16> %493, %480
  %497 = add <8 x i16> %495, %481
  %498 = xor <8 x i16> %496, %480
  %499 = xor <8 x i16> %497, %481
  %500 = sub <8 x i16> %400, %90
  %501 = sub <8 x i16> %403, %95
  %502 = ashr <8 x i16> %500, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %503 = ashr <8 x i16> %501, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %504 = sub <8 x i16> zeroinitializer, %500
  %505 = icmp sgt <8 x i16> %500, %504
  %506 = select <8 x i1> %505, <8 x i16> %500, <8 x i16> %504
  %507 = sub <8 x i16> zeroinitializer, %501
  %508 = icmp sgt <8 x i16> %501, %507
  %509 = select <8 x i1> %508, <8 x i16> %501, <8 x i16> %507
  %510 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %506, <8 x i16> %69) #7
  %511 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %509, <8 x i16> %69) #7
  %512 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %510) #7
  %513 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %511) #7
  %514 = icmp slt <8 x i16> %506, %512
  %515 = select <8 x i1> %514, <8 x i16> %506, <8 x i16> %512
  %516 = icmp slt <8 x i16> %509, %513
  %517 = select <8 x i1> %516, <8 x i16> %509, <8 x i16> %513
  %518 = add <8 x i16> %515, %502
  %519 = add <8 x i16> %517, %503
  %520 = xor <8 x i16> %518, %502
  %521 = xor <8 x i16> %519, %503
  %522 = add <8 x i16> %476, %454
  %523 = add <8 x i16> %522, %498
  %524 = add <8 x i16> %523, %520
  %525 = add <8 x i16> %477, %455
  %526 = add <8 x i16> %525, %499
  %527 = add <8 x i16> %526, %521
  %528 = mul <8 x i16> %524, %73
  %529 = mul <8 x i16> %527, %73
  %530 = add <8 x i16> %312, %528
  %531 = add <8 x i16> %313, %529
  %532 = add nsw i64 %87, %83
  %533 = getelementptr inbounds i16, i16* %2, i64 %532
  %534 = bitcast i16* %533 to <2 x i64>*
  %535 = load <2 x i64>, <2 x i64>* %534, align 1
  %536 = add nsw i64 %92, %83
  %537 = getelementptr inbounds i16, i16* %2, i64 %536
  %538 = bitcast i16* %537 to <2 x i64>*
  %539 = load <2 x i64>, <2 x i64>* %538, align 1
  %540 = sub nsw i64 %87, %83
  %541 = getelementptr inbounds i16, i16* %2, i64 %540
  %542 = bitcast i16* %541 to <2 x i64>*
  %543 = load <2 x i64>, <2 x i64>* %542, align 1
  %544 = sub nsw i64 %92, %83
  %545 = getelementptr inbounds i16, i16* %2, i64 %544
  %546 = bitcast i16* %545 to <2 x i64>*
  %547 = load <2 x i64>, <2 x i64>* %546, align 1
  %548 = add nsw i64 %87, %84
  %549 = getelementptr inbounds i16, i16* %2, i64 %548
  %550 = bitcast i16* %549 to <2 x i64>*
  %551 = load <2 x i64>, <2 x i64>* %550, align 1
  %552 = add nsw i64 %92, %84
  %553 = getelementptr inbounds i16, i16* %2, i64 %552
  %554 = bitcast i16* %553 to <2 x i64>*
  %555 = load <2 x i64>, <2 x i64>* %554, align 1
  %556 = sub nsw i64 %87, %84
  %557 = getelementptr inbounds i16, i16* %2, i64 %556
  %558 = bitcast i16* %557 to <2 x i64>*
  %559 = load <2 x i64>, <2 x i64>* %558, align 1
  %560 = sub nsw i64 %92, %84
  %561 = getelementptr inbounds i16, i16* %2, i64 %560
  %562 = bitcast i16* %561 to <2 x i64>*
  %563 = load <2 x i64>, <2 x i64>* %562, align 1
  %564 = bitcast <2 x i64> %535 to <8 x i16>
  %565 = icmp eq <8 x i16> %564, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %566 = sext <8 x i1> %565 to <8 x i16>
  %567 = bitcast <2 x i64> %539 to <8 x i16>
  %568 = icmp eq <8 x i16> %567, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %569 = sext <8 x i1> %568 to <8 x i16>
  %570 = bitcast <8 x i16> %569 to <2 x i64>
  %571 = bitcast <8 x i16> %566 to <2 x i64>
  %572 = xor <2 x i64> %571, <i64 -1, i64 -1>
  %573 = and <2 x i64> %535, %572
  %574 = xor <2 x i64> %570, <i64 -1, i64 -1>
  %575 = and <2 x i64> %539, %574
  %576 = bitcast <2 x i64> %573 to <8 x i16>
  %577 = icmp sgt <8 x i16> %414, %576
  %578 = select <8 x i1> %577, <8 x i16> %414, <8 x i16> %576
  %579 = bitcast <2 x i64> %575 to <8 x i16>
  %580 = icmp sgt <8 x i16> %417, %579
  %581 = select <8 x i1> %580, <8 x i16> %417, <8 x i16> %579
  %582 = bitcast <2 x i64> %543 to <8 x i16>
  %583 = icmp eq <8 x i16> %582, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %584 = sext <8 x i1> %583 to <8 x i16>
  %585 = bitcast <2 x i64> %547 to <8 x i16>
  %586 = icmp eq <8 x i16> %585, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %587 = sext <8 x i1> %586 to <8 x i16>
  %588 = bitcast <8 x i16> %587 to <2 x i64>
  %589 = bitcast <8 x i16> %584 to <2 x i64>
  %590 = xor <2 x i64> %589, <i64 -1, i64 -1>
  %591 = and <2 x i64> %543, %590
  %592 = xor <2 x i64> %588, <i64 -1, i64 -1>
  %593 = and <2 x i64> %547, %592
  %594 = bitcast <2 x i64> %591 to <8 x i16>
  %595 = icmp sgt <8 x i16> %578, %594
  %596 = select <8 x i1> %595, <8 x i16> %578, <8 x i16> %594
  %597 = bitcast <2 x i64> %593 to <8 x i16>
  %598 = icmp sgt <8 x i16> %581, %597
  %599 = select <8 x i1> %598, <8 x i16> %581, <8 x i16> %597
  %600 = bitcast <2 x i64> %551 to <8 x i16>
  %601 = icmp eq <8 x i16> %600, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %602 = sext <8 x i1> %601 to <8 x i16>
  %603 = bitcast <2 x i64> %555 to <8 x i16>
  %604 = icmp eq <8 x i16> %603, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %605 = sext <8 x i1> %604 to <8 x i16>
  %606 = bitcast <8 x i16> %605 to <2 x i64>
  %607 = bitcast <8 x i16> %602 to <2 x i64>
  %608 = xor <2 x i64> %607, <i64 -1, i64 -1>
  %609 = and <2 x i64> %551, %608
  %610 = xor <2 x i64> %606, <i64 -1, i64 -1>
  %611 = and <2 x i64> %555, %610
  %612 = bitcast <2 x i64> %609 to <8 x i16>
  %613 = icmp sgt <8 x i16> %596, %612
  %614 = select <8 x i1> %613, <8 x i16> %596, <8 x i16> %612
  %615 = bitcast <2 x i64> %611 to <8 x i16>
  %616 = icmp sgt <8 x i16> %599, %615
  %617 = select <8 x i1> %616, <8 x i16> %599, <8 x i16> %615
  %618 = bitcast <2 x i64> %559 to <8 x i16>
  %619 = icmp eq <8 x i16> %618, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %620 = sext <8 x i1> %619 to <8 x i16>
  %621 = bitcast <2 x i64> %563 to <8 x i16>
  %622 = icmp eq <8 x i16> %621, <i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000, i16 30000>
  %623 = sext <8 x i1> %622 to <8 x i16>
  %624 = bitcast <8 x i16> %623 to <2 x i64>
  %625 = bitcast <8 x i16> %620 to <2 x i64>
  %626 = xor <2 x i64> %625, <i64 -1, i64 -1>
  %627 = and <2 x i64> %559, %626
  %628 = xor <2 x i64> %624, <i64 -1, i64 -1>
  %629 = and <2 x i64> %563, %628
  %630 = bitcast <2 x i64> %627 to <8 x i16>
  %631 = icmp sgt <8 x i16> %614, %630
  %632 = select <8 x i1> %631, <8 x i16> %614, <8 x i16> %630
  %633 = bitcast <2 x i64> %629 to <8 x i16>
  %634 = icmp sgt <8 x i16> %617, %633
  %635 = select <8 x i1> %634, <8 x i16> %617, <8 x i16> %633
  %636 = icmp slt <8 x i16> %431, %564
  %637 = select <8 x i1> %636, <8 x i16> %431, <8 x i16> %564
  %638 = icmp slt <8 x i16> %433, %567
  %639 = select <8 x i1> %638, <8 x i16> %433, <8 x i16> %567
  %640 = icmp slt <8 x i16> %637, %582
  %641 = select <8 x i1> %640, <8 x i16> %637, <8 x i16> %582
  %642 = icmp slt <8 x i16> %639, %585
  %643 = select <8 x i1> %642, <8 x i16> %639, <8 x i16> %585
  %644 = icmp slt <8 x i16> %641, %600
  %645 = select <8 x i1> %644, <8 x i16> %641, <8 x i16> %600
  %646 = icmp slt <8 x i16> %643, %603
  %647 = select <8 x i1> %646, <8 x i16> %643, <8 x i16> %603
  %648 = icmp slt <8 x i16> %645, %618
  %649 = select <8 x i1> %648, <8 x i16> %645, <8 x i16> %618
  %650 = icmp slt <8 x i16> %647, %621
  %651 = select <8 x i1> %650, <8 x i16> %647, <8 x i16> %621
  %652 = sub <8 x i16> %564, %90
  %653 = sub <8 x i16> %567, %95
  %654 = ashr <8 x i16> %652, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %655 = ashr <8 x i16> %653, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %656 = sub <8 x i16> zeroinitializer, %652
  %657 = icmp sgt <8 x i16> %652, %656
  %658 = select <8 x i1> %657, <8 x i16> %652, <8 x i16> %656
  %659 = sub <8 x i16> zeroinitializer, %653
  %660 = icmp sgt <8 x i16> %653, %659
  %661 = select <8 x i1> %660, <8 x i16> %653, <8 x i16> %659
  %662 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %658, <8 x i16> %69) #7
  %663 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %661, <8 x i16> %69) #7
  %664 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %662) #7
  %665 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %663) #7
  %666 = icmp slt <8 x i16> %658, %664
  %667 = select <8 x i1> %666, <8 x i16> %658, <8 x i16> %664
  %668 = icmp slt <8 x i16> %661, %665
  %669 = select <8 x i1> %668, <8 x i16> %661, <8 x i16> %665
  %670 = add <8 x i16> %667, %654
  %671 = add <8 x i16> %669, %655
  %672 = xor <8 x i16> %670, %654
  %673 = xor <8 x i16> %671, %655
  %674 = sub <8 x i16> %582, %90
  %675 = sub <8 x i16> %585, %95
  %676 = ashr <8 x i16> %674, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %677 = ashr <8 x i16> %675, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %678 = sub <8 x i16> zeroinitializer, %674
  %679 = icmp sgt <8 x i16> %674, %678
  %680 = select <8 x i1> %679, <8 x i16> %674, <8 x i16> %678
  %681 = sub <8 x i16> zeroinitializer, %675
  %682 = icmp sgt <8 x i16> %675, %681
  %683 = select <8 x i1> %682, <8 x i16> %675, <8 x i16> %681
  %684 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %680, <8 x i16> %69) #7
  %685 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %683, <8 x i16> %69) #7
  %686 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %684) #7
  %687 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %685) #7
  %688 = icmp slt <8 x i16> %680, %686
  %689 = select <8 x i1> %688, <8 x i16> %680, <8 x i16> %686
  %690 = icmp slt <8 x i16> %683, %687
  %691 = select <8 x i1> %690, <8 x i16> %683, <8 x i16> %687
  %692 = add <8 x i16> %689, %676
  %693 = add <8 x i16> %691, %677
  %694 = xor <8 x i16> %692, %676
  %695 = xor <8 x i16> %693, %677
  %696 = sub <8 x i16> %600, %90
  %697 = sub <8 x i16> %603, %95
  %698 = ashr <8 x i16> %696, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %699 = ashr <8 x i16> %697, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %700 = sub <8 x i16> zeroinitializer, %696
  %701 = icmp sgt <8 x i16> %696, %700
  %702 = select <8 x i1> %701, <8 x i16> %696, <8 x i16> %700
  %703 = sub <8 x i16> zeroinitializer, %697
  %704 = icmp sgt <8 x i16> %697, %703
  %705 = select <8 x i1> %704, <8 x i16> %697, <8 x i16> %703
  %706 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %702, <8 x i16> %69) #7
  %707 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %705, <8 x i16> %69) #7
  %708 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %706) #7
  %709 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %707) #7
  %710 = icmp slt <8 x i16> %702, %708
  %711 = select <8 x i1> %710, <8 x i16> %702, <8 x i16> %708
  %712 = icmp slt <8 x i16> %705, %709
  %713 = select <8 x i1> %712, <8 x i16> %705, <8 x i16> %709
  %714 = add <8 x i16> %711, %698
  %715 = add <8 x i16> %713, %699
  %716 = xor <8 x i16> %714, %698
  %717 = xor <8 x i16> %715, %699
  %718 = sub <8 x i16> %618, %90
  %719 = sub <8 x i16> %621, %95
  %720 = ashr <8 x i16> %718, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %721 = ashr <8 x i16> %719, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %722 = sub <8 x i16> zeroinitializer, %718
  %723 = icmp sgt <8 x i16> %718, %722
  %724 = select <8 x i1> %723, <8 x i16> %718, <8 x i16> %722
  %725 = sub <8 x i16> zeroinitializer, %719
  %726 = icmp sgt <8 x i16> %719, %725
  %727 = select <8 x i1> %726, <8 x i16> %719, <8 x i16> %725
  %728 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %724, <8 x i16> %69) #7
  %729 = tail call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %727, <8 x i16> %69) #7
  %730 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %728) #7
  %731 = tail call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> %67, <8 x i16> %729) #7
  %732 = icmp slt <8 x i16> %724, %730
  %733 = select <8 x i1> %732, <8 x i16> %724, <8 x i16> %730
  %734 = icmp slt <8 x i16> %727, %731
  %735 = select <8 x i1> %734, <8 x i16> %727, <8 x i16> %731
  %736 = add <8 x i16> %733, %720
  %737 = add <8 x i16> %735, %721
  %738 = xor <8 x i16> %736, %720
  %739 = xor <8 x i16> %737, %721
  %740 = add <8 x i16> %694, %672
  %741 = add <8 x i16> %740, %716
  %742 = add <8 x i16> %741, %738
  %743 = add <8 x i16> %695, %673
  %744 = add <8 x i16> %743, %717
  %745 = add <8 x i16> %744, %739
  %746 = mul <8 x i16> %742, %77
  %747 = mul <8 x i16> %745, %77
  %748 = add <8 x i16> %530, %746
  %749 = add <8 x i16> %531, %747
  %750 = ashr <8 x i16> %748, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %751 = ashr <8 x i16> %749, <i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15, i16 15>
  %752 = add <8 x i16> %748, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %753 = add <8 x i16> %752, %750
  %754 = add <8 x i16> %749, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %755 = add <8 x i16> %754, %751
  %756 = ashr <8 x i16> %753, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %757 = ashr <8 x i16> %755, <i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4, i16 4>
  %758 = add <8 x i16> %756, %90
  %759 = add <8 x i16> %757, %95
  %760 = icmp sgt <8 x i16> %758, %649
  %761 = select <8 x i1> %760, <8 x i16> %758, <8 x i16> %649
  %762 = icmp sgt <8 x i16> %759, %651
  %763 = select <8 x i1> %762, <8 x i16> %759, <8 x i16> %651
  %764 = icmp slt <8 x i16> %761, %632
  %765 = select <8 x i1> %764, <8 x i16> %761, <8 x i16> %632
  %766 = icmp slt <8 x i16> %763, %635
  %767 = select <8 x i1> %766, <8 x i16> %763, <8 x i16> %635
  %768 = mul nsw i64 %86, %78
  %769 = getelementptr inbounds i16, i16* %0, i64 %768
  %770 = bitcast i16* %769 to <8 x i16>*
  store <8 x i16> %765, <8 x i16>* %770, align 1
  %771 = mul nsw i64 %91, %78
  %772 = getelementptr inbounds i16, i16* %0, i64 %771
  %773 = bitcast i16* %772 to <8 x i16>*
  store <8 x i16> %767, <8 x i16>* %773, align 1
  %774 = add nuw nsw i64 %86, 2
  %775 = icmp ult i64 %774, 8
  br i1 %775, label %85, label %776

776:                                              ; preds = %85
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @cdef_filter_block_sse2(i8*, i16* nocapture, i32, i16* readonly, i32, i32, i32, i32, i32, i32, i32) local_unnamed_addr #3 {
  %12 = icmp eq i8* %0, null
  %13 = icmp eq i32 %9, 3
  br i1 %12, label %28, label %14

14:                                               ; preds = %11
  br i1 %13, label %15, label %16

15:                                               ; preds = %14
  tail call void @cdef_filter_block_8x8_8_sse2(i8* nonnull %0, i32 %2, i16* %3, i32 %4, i32 %5, i32 %6, i32 %7, i32 %8, i32 %10)
  br label %42

16:                                               ; preds = %14
  %17 = icmp eq i32 %9, 1
  br i1 %17, label %18, label %23

18:                                               ; preds = %16
  tail call void @cdef_filter_block_4x4_8_sse2(i8* nonnull %0, i32 %2, i16* %3, i32 %4, i32 %5, i32 %6, i32 %7, i32 %8, i32 %10)
  %19 = shl nsw i32 %2, 2
  %20 = sext i32 %19 to i64
  %21 = getelementptr inbounds i8, i8* %0, i64 %20
  %22 = getelementptr inbounds i16, i16* %3, i64 576
  tail call void @cdef_filter_block_4x4_8_sse2(i8* %21, i32 %2, i16* %22, i32 %4, i32 %5, i32 %6, i32 %7, i32 %8, i32 %10)
  br label %42

23:                                               ; preds = %16
  %24 = icmp eq i32 %9, 2
  tail call void @cdef_filter_block_4x4_8_sse2(i8* nonnull %0, i32 %2, i16* %3, i32 %4, i32 %5, i32 %6, i32 %7, i32 %8, i32 %10)
  br i1 %24, label %25, label %42

25:                                               ; preds = %23
  %26 = getelementptr inbounds i8, i8* %0, i64 4
  %27 = getelementptr inbounds i16, i16* %3, i64 4
  tail call void @cdef_filter_block_4x4_8_sse2(i8* %26, i32 %2, i16* %27, i32 %4, i32 %5, i32 %6, i32 %7, i32 %8, i32 %10)
  br label %42

28:                                               ; preds = %11
  br i1 %13, label %29, label %30

29:                                               ; preds = %28
  tail call void @cdef_filter_block_8x8_16_sse2(i16* %1, i32 %2, i16* %3, i32 %4, i32 %5, i32 %6, i32 %7, i32 %8, i32 %10)
  br label %42

30:                                               ; preds = %28
  %31 = icmp eq i32 %9, 1
  br i1 %31, label %32, label %37

32:                                               ; preds = %30
  tail call void @cdef_filter_block_4x4_16_sse2(i16* %1, i32 %2, i16* %3, i32 %4, i32 %5, i32 %6, i32 %7, i32 %8, i32 %10)
  %33 = shl nsw i32 %2, 2
  %34 = sext i32 %33 to i64
  %35 = getelementptr inbounds i16, i16* %1, i64 %34
  %36 = getelementptr inbounds i16, i16* %3, i64 576
  tail call void @cdef_filter_block_4x4_16_sse2(i16* %35, i32 %2, i16* %36, i32 %4, i32 %5, i32 %6, i32 %7, i32 %8, i32 %10)
  br label %42

37:                                               ; preds = %30
  %38 = icmp eq i32 %9, 2
  tail call void @cdef_filter_block_4x4_16_sse2(i16* %1, i32 %2, i16* %3, i32 %4, i32 %5, i32 %6, i32 %7, i32 %8, i32 %10)
  br i1 %38, label %39, label %42

39:                                               ; preds = %37
  %40 = getelementptr inbounds i16, i16* %1, i64 4
  %41 = getelementptr inbounds i16, i16* %3, i64 4
  tail call void @cdef_filter_block_4x4_16_sse2(i16* %40, i32 %2, i16* %41, i32 %4, i32 %5, i32 %6, i32 %7, i32 %8, i32 %10)
  br label %42

42:                                               ; preds = %29, %39, %37, %32, %15, %25, %23, %18
  ret void
}

; Function Attrs: nofree norecurse nounwind ssp uwtable
define hidden void @cdef_copy_rect8_8bit_to_16bit_sse2(i16* nocapture, i32, i8* nocapture readonly, i32, i32, i32) local_unnamed_addr #4 {
  %7 = icmp sgt i32 %4, 0
  br i1 %7, label %8, label %222

8:                                                ; preds = %6
  %9 = and i32 %5, -8
  %10 = icmp sgt i32 %9, 0
  %11 = sext i32 %9 to i64
  %12 = sext i32 %1 to i64
  %13 = sext i32 %3 to i64
  %14 = zext i32 %4 to i64
  %15 = zext i32 %5 to i64
  %16 = add nsw i64 %11, -1
  %17 = lshr i64 %16, 3
  %18 = add nuw nsw i64 %17, 1
  %19 = and i64 %18, 1
  %20 = icmp eq i64 %17, 0
  %21 = sub nuw nsw i64 %18, %19
  %22 = icmp eq i64 %19, 0
  br label %23

23:                                               ; preds = %219, %8
  %24 = phi i64 [ 0, %8 ], [ %220, %219 ]
  %25 = mul i64 %24, %12
  %26 = add i64 %25, %15
  %27 = getelementptr i16, i16* %0, i64 %26
  %28 = bitcast i16* %27 to i8*
  %29 = mul i64 %24, %13
  %30 = getelementptr i8, i8* %2, i64 %29
  %31 = add i64 %29, %15
  %32 = getelementptr i8, i8* %2, i64 %31
  br i1 %10, label %33, label %54

33:                                               ; preds = %23
  %34 = mul nsw i64 %24, %13
  %35 = mul nsw i64 %24, %12
  br i1 %20, label %36, label %161

36:                                               ; preds = %161, %33
  %37 = phi i64 [ undef, %33 ], [ %185, %161 ]
  %38 = phi i64 [ 0, %33 ], [ %185, %161 ]
  br i1 %22, label %51, label %39

39:                                               ; preds = %36
  %40 = add nsw i64 %38, %34
  %41 = getelementptr inbounds i8, i8* %2, i64 %40
  %42 = bitcast i8* %41 to i64*
  %43 = load i64, i64* %42, align 1
  %44 = insertelement <2 x i64> undef, i64 %43, i32 0
  %45 = add nsw i64 %38, %35
  %46 = getelementptr inbounds i16, i16* %0, i64 %45
  %47 = bitcast <2 x i64> %44 to <16 x i8>
  %48 = shufflevector <16 x i8> %47, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %49 = bitcast i16* %46 to <16 x i8>*
  store <16 x i8> %48, <16 x i8>* %49, align 1
  %50 = add nuw nsw i64 %38, 8
  br label %51

51:                                               ; preds = %36, %39
  %52 = phi i64 [ %37, %36 ], [ %50, %39 ]
  %53 = trunc i64 %52 to i32
  br label %54

54:                                               ; preds = %51, %23
  %55 = phi i32 [ 0, %23 ], [ %53, %51 ]
  %56 = icmp slt i32 %55, %5
  br i1 %56, label %57, label %219

57:                                               ; preds = %54
  %58 = mul nsw i64 %24, %13
  %59 = mul nsw i64 %24, %12
  %60 = zext i32 %55 to i64
  %61 = sub nsw i64 %15, %60
  %62 = icmp ult i64 %61, 16
  br i1 %62, label %63, label %85

63:                                               ; preds = %159, %85, %57
  %64 = phi i64 [ %60, %85 ], [ %60, %57 ], [ %95, %159 ]
  %65 = sub nsw i64 %15, %64
  %66 = xor i64 %64, -1
  %67 = add nsw i64 %66, %15
  %68 = and i64 %65, 3
  %69 = icmp eq i64 %68, 0
  br i1 %69, label %82, label %70

70:                                               ; preds = %63, %70
  %71 = phi i64 [ %79, %70 ], [ %64, %63 ]
  %72 = phi i64 [ %80, %70 ], [ %68, %63 ]
  %73 = add nsw i64 %71, %58
  %74 = getelementptr inbounds i8, i8* %2, i64 %73
  %75 = load i8, i8* %74, align 1
  %76 = zext i8 %75 to i16
  %77 = add nsw i64 %71, %59
  %78 = getelementptr inbounds i16, i16* %0, i64 %77
  store i16 %76, i16* %78, align 2
  %79 = add nuw nsw i64 %71, 1
  %80 = add i64 %72, -1
  %81 = icmp eq i64 %80, 0
  br i1 %81, label %82, label %70, !llvm.loop !3

82:                                               ; preds = %70, %63
  %83 = phi i64 [ %64, %63 ], [ %79, %70 ]
  %84 = icmp ult i64 %67, 3
  br i1 %84, label %219, label %188

85:                                               ; preds = %57
  %86 = add nsw i64 %25, %60
  %87 = getelementptr i16, i16* %0, i64 %86
  %88 = bitcast i16* %87 to i8*
  %89 = getelementptr i8, i8* %30, i64 %60
  %90 = icmp ugt i8* %32, %88
  %91 = icmp ult i8* %89, %28
  %92 = and i1 %90, %91
  br i1 %92, label %63, label %93

93:                                               ; preds = %85
  %94 = and i64 %61, -16
  %95 = add nsw i64 %94, %60
  %96 = add nsw i64 %94, -16
  %97 = lshr exact i64 %96, 4
  %98 = add nuw nsw i64 %97, 1
  %99 = and i64 %98, 1
  %100 = icmp eq i64 %96, 0
  br i1 %100, label %140, label %101

101:                                              ; preds = %93
  %102 = sub nuw nsw i64 %98, %99
  br label %103

103:                                              ; preds = %103, %101
  %104 = phi i64 [ 0, %101 ], [ %137, %103 ]
  %105 = phi i64 [ %102, %101 ], [ %138, %103 ]
  %106 = add i64 %104, %60
  %107 = add nsw i64 %106, %58
  %108 = getelementptr inbounds i8, i8* %2, i64 %107
  %109 = bitcast i8* %108 to <8 x i8>*
  %110 = load <8 x i8>, <8 x i8>* %109, align 1, !alias.scope !5
  %111 = getelementptr inbounds i8, i8* %108, i64 8
  %112 = bitcast i8* %111 to <8 x i8>*
  %113 = load <8 x i8>, <8 x i8>* %112, align 1, !alias.scope !5
  %114 = zext <8 x i8> %110 to <8 x i16>
  %115 = zext <8 x i8> %113 to <8 x i16>
  %116 = add nsw i64 %106, %59
  %117 = getelementptr inbounds i16, i16* %0, i64 %116
  %118 = bitcast i16* %117 to <8 x i16>*
  store <8 x i16> %114, <8 x i16>* %118, align 2, !alias.scope !8, !noalias !5
  %119 = getelementptr inbounds i16, i16* %117, i64 8
  %120 = bitcast i16* %119 to <8 x i16>*
  store <8 x i16> %115, <8 x i16>* %120, align 2, !alias.scope !8, !noalias !5
  %121 = or i64 %104, 16
  %122 = add i64 %121, %60
  %123 = add nsw i64 %122, %58
  %124 = getelementptr inbounds i8, i8* %2, i64 %123
  %125 = bitcast i8* %124 to <8 x i8>*
  %126 = load <8 x i8>, <8 x i8>* %125, align 1, !alias.scope !5
  %127 = getelementptr inbounds i8, i8* %124, i64 8
  %128 = bitcast i8* %127 to <8 x i8>*
  %129 = load <8 x i8>, <8 x i8>* %128, align 1, !alias.scope !5
  %130 = zext <8 x i8> %126 to <8 x i16>
  %131 = zext <8 x i8> %129 to <8 x i16>
  %132 = add nsw i64 %122, %59
  %133 = getelementptr inbounds i16, i16* %0, i64 %132
  %134 = bitcast i16* %133 to <8 x i16>*
  store <8 x i16> %130, <8 x i16>* %134, align 2, !alias.scope !8, !noalias !5
  %135 = getelementptr inbounds i16, i16* %133, i64 8
  %136 = bitcast i16* %135 to <8 x i16>*
  store <8 x i16> %131, <8 x i16>* %136, align 2, !alias.scope !8, !noalias !5
  %137 = add i64 %104, 32
  %138 = add i64 %105, -2
  %139 = icmp eq i64 %138, 0
  br i1 %139, label %140, label %103, !llvm.loop !10

140:                                              ; preds = %103, %93
  %141 = phi i64 [ 0, %93 ], [ %137, %103 ]
  %142 = icmp eq i64 %99, 0
  br i1 %142, label %159, label %143

143:                                              ; preds = %140
  %144 = add i64 %141, %60
  %145 = add nsw i64 %144, %58
  %146 = getelementptr inbounds i8, i8* %2, i64 %145
  %147 = bitcast i8* %146 to <8 x i8>*
  %148 = load <8 x i8>, <8 x i8>* %147, align 1, !alias.scope !5
  %149 = getelementptr inbounds i8, i8* %146, i64 8
  %150 = bitcast i8* %149 to <8 x i8>*
  %151 = load <8 x i8>, <8 x i8>* %150, align 1, !alias.scope !5
  %152 = zext <8 x i8> %148 to <8 x i16>
  %153 = zext <8 x i8> %151 to <8 x i16>
  %154 = add nsw i64 %144, %59
  %155 = getelementptr inbounds i16, i16* %0, i64 %154
  %156 = bitcast i16* %155 to <8 x i16>*
  store <8 x i16> %152, <8 x i16>* %156, align 2, !alias.scope !8, !noalias !5
  %157 = getelementptr inbounds i16, i16* %155, i64 8
  %158 = bitcast i16* %157 to <8 x i16>*
  store <8 x i16> %153, <8 x i16>* %158, align 2, !alias.scope !8, !noalias !5
  br label %159

159:                                              ; preds = %140, %143
  %160 = icmp eq i64 %61, %94
  br i1 %160, label %219, label %63

161:                                              ; preds = %33, %161
  %162 = phi i64 [ %185, %161 ], [ 0, %33 ]
  %163 = phi i64 [ %186, %161 ], [ %21, %33 ]
  %164 = add nsw i64 %162, %34
  %165 = getelementptr inbounds i8, i8* %2, i64 %164
  %166 = bitcast i8* %165 to i64*
  %167 = load i64, i64* %166, align 1
  %168 = insertelement <2 x i64> undef, i64 %167, i32 0
  %169 = add nsw i64 %162, %35
  %170 = getelementptr inbounds i16, i16* %0, i64 %169
  %171 = bitcast <2 x i64> %168 to <16 x i8>
  %172 = shufflevector <16 x i8> %171, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %173 = bitcast i16* %170 to <16 x i8>*
  store <16 x i8> %172, <16 x i8>* %173, align 1
  %174 = or i64 %162, 8
  %175 = add nsw i64 %174, %34
  %176 = getelementptr inbounds i8, i8* %2, i64 %175
  %177 = bitcast i8* %176 to i64*
  %178 = load i64, i64* %177, align 1
  %179 = insertelement <2 x i64> undef, i64 %178, i32 0
  %180 = add nsw i64 %174, %35
  %181 = getelementptr inbounds i16, i16* %0, i64 %180
  %182 = bitcast <2 x i64> %179 to <16 x i8>
  %183 = shufflevector <16 x i8> %182, <16 x i8> <i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
  %184 = bitcast i16* %181 to <16 x i8>*
  store <16 x i8> %183, <16 x i8>* %184, align 1
  %185 = add nuw nsw i64 %162, 16
  %186 = add i64 %163, -2
  %187 = icmp eq i64 %186, 0
  br i1 %187, label %36, label %161

188:                                              ; preds = %82, %188
  %189 = phi i64 [ %217, %188 ], [ %83, %82 ]
  %190 = add nsw i64 %189, %58
  %191 = getelementptr inbounds i8, i8* %2, i64 %190
  %192 = load i8, i8* %191, align 1
  %193 = zext i8 %192 to i16
  %194 = add nsw i64 %189, %59
  %195 = getelementptr inbounds i16, i16* %0, i64 %194
  store i16 %193, i16* %195, align 2
  %196 = add nuw nsw i64 %189, 1
  %197 = add nsw i64 %196, %58
  %198 = getelementptr inbounds i8, i8* %2, i64 %197
  %199 = load i8, i8* %198, align 1
  %200 = zext i8 %199 to i16
  %201 = add nsw i64 %196, %59
  %202 = getelementptr inbounds i16, i16* %0, i64 %201
  store i16 %200, i16* %202, align 2
  %203 = add nuw nsw i64 %189, 2
  %204 = add nsw i64 %203, %58
  %205 = getelementptr inbounds i8, i8* %2, i64 %204
  %206 = load i8, i8* %205, align 1
  %207 = zext i8 %206 to i16
  %208 = add nsw i64 %203, %59
  %209 = getelementptr inbounds i16, i16* %0, i64 %208
  store i16 %207, i16* %209, align 2
  %210 = add nuw nsw i64 %189, 3
  %211 = add nsw i64 %210, %58
  %212 = getelementptr inbounds i8, i8* %2, i64 %211
  %213 = load i8, i8* %212, align 1
  %214 = zext i8 %213 to i16
  %215 = add nsw i64 %210, %59
  %216 = getelementptr inbounds i16, i16* %0, i64 %215
  store i16 %214, i16* %216, align 2
  %217 = add nuw nsw i64 %189, 4
  %218 = icmp eq i64 %217, %15
  br i1 %218, label %219, label %188, !llvm.loop !12

219:                                              ; preds = %82, %188, %159, %54
  %220 = add nuw nsw i64 %24, 1
  %221 = icmp eq i64 %220, %14
  br i1 %221, label %222, label %23

222:                                              ; preds = %219, %6
  ret void
}

; Function Attrs: nofree norecurse nounwind ssp uwtable
define hidden void @cdef_copy_rect8_16bit_to_16bit_sse2(i16* nocapture, i32, i16* nocapture readonly, i32, i32, i32) local_unnamed_addr #4 {
  %7 = icmp sgt i32 %4, 0
  br i1 %7, label %8, label %220

8:                                                ; preds = %6
  %9 = and i32 %5, -8
  %10 = icmp sgt i32 %9, 0
  %11 = sext i32 %9 to i64
  %12 = sext i32 %1 to i64
  %13 = sext i32 %3 to i64
  %14 = zext i32 %4 to i64
  %15 = zext i32 %5 to i64
  %16 = add nsw i64 %11, -1
  %17 = lshr i64 %16, 3
  %18 = add nuw nsw i64 %17, 1
  %19 = and i64 %18, 3
  %20 = icmp ult i64 %16, 24
  %21 = sub nsw i64 %18, %19
  %22 = icmp eq i64 %19, 0
  br label %23

23:                                               ; preds = %217, %8
  %24 = phi i64 [ 0, %8 ], [ %218, %217 ]
  %25 = mul i64 %24, %12
  %26 = add i64 %25, %15
  %27 = getelementptr i16, i16* %0, i64 %26
  %28 = mul i64 %24, %13
  %29 = add i64 %28, %15
  %30 = getelementptr i16, i16* %2, i64 %29
  br i1 %10, label %31, label %53

31:                                               ; preds = %23
  %32 = mul nsw i64 %24, %13
  %33 = mul nsw i64 %24, %12
  br i1 %20, label %34, label %153

34:                                               ; preds = %153, %31
  %35 = phi i64 [ undef, %31 ], [ %187, %153 ]
  %36 = phi i64 [ 0, %31 ], [ %187, %153 ]
  br i1 %22, label %50, label %37

37:                                               ; preds = %34, %37
  %38 = phi i64 [ %47, %37 ], [ %36, %34 ]
  %39 = phi i64 [ %48, %37 ], [ %19, %34 ]
  %40 = add nsw i64 %38, %32
  %41 = getelementptr inbounds i16, i16* %2, i64 %40
  %42 = bitcast i16* %41 to <2 x i64>*
  %43 = load <2 x i64>, <2 x i64>* %42, align 1
  %44 = add nsw i64 %38, %33
  %45 = getelementptr inbounds i16, i16* %0, i64 %44
  %46 = bitcast i16* %45 to <2 x i64>*
  store <2 x i64> %43, <2 x i64>* %46, align 1
  %47 = add nuw nsw i64 %38, 8
  %48 = add i64 %39, -1
  %49 = icmp eq i64 %48, 0
  br i1 %49, label %50, label %37, !llvm.loop !13

50:                                               ; preds = %37, %34
  %51 = phi i64 [ %35, %34 ], [ %47, %37 ]
  %52 = trunc i64 %51 to i32
  br label %53

53:                                               ; preds = %50, %23
  %54 = phi i32 [ 0, %23 ], [ %52, %50 ]
  %55 = icmp slt i32 %54, %5
  br i1 %55, label %56, label %217

56:                                               ; preds = %53
  %57 = mul nsw i64 %24, %13
  %58 = mul nsw i64 %24, %12
  %59 = zext i32 %54 to i64
  %60 = sub nsw i64 %15, %59
  %61 = icmp ult i64 %60, 16
  br i1 %61, label %62, label %83

62:                                               ; preds = %151, %83, %56
  %63 = phi i64 [ %59, %83 ], [ %59, %56 ], [ %93, %151 ]
  %64 = sub nsw i64 %15, %63
  %65 = xor i64 %63, -1
  %66 = add nsw i64 %65, %15
  %67 = and i64 %64, 3
  %68 = icmp eq i64 %67, 0
  br i1 %68, label %80, label %69

69:                                               ; preds = %62, %69
  %70 = phi i64 [ %77, %69 ], [ %63, %62 ]
  %71 = phi i64 [ %78, %69 ], [ %67, %62 ]
  %72 = add nsw i64 %70, %57
  %73 = getelementptr inbounds i16, i16* %2, i64 %72
  %74 = load i16, i16* %73, align 2
  %75 = add nsw i64 %70, %58
  %76 = getelementptr inbounds i16, i16* %0, i64 %75
  store i16 %74, i16* %76, align 2
  %77 = add nuw nsw i64 %70, 1
  %78 = add i64 %71, -1
  %79 = icmp eq i64 %78, 0
  br i1 %79, label %80, label %69, !llvm.loop !14

80:                                               ; preds = %69, %62
  %81 = phi i64 [ %63, %62 ], [ %77, %69 ]
  %82 = icmp ult i64 %66, 3
  br i1 %82, label %217, label %190

83:                                               ; preds = %56
  %84 = add nsw i64 %25, %59
  %85 = getelementptr i16, i16* %0, i64 %84
  %86 = add nsw i64 %28, %59
  %87 = getelementptr i16, i16* %2, i64 %86
  %88 = icmp ult i16* %85, %30
  %89 = icmp ult i16* %87, %27
  %90 = and i1 %88, %89
  br i1 %90, label %62, label %91

91:                                               ; preds = %83
  %92 = and i64 %60, -16
  %93 = add nsw i64 %92, %59
  %94 = add nsw i64 %92, -16
  %95 = lshr exact i64 %94, 4
  %96 = add nuw nsw i64 %95, 1
  %97 = and i64 %96, 1
  %98 = icmp eq i64 %94, 0
  br i1 %98, label %134, label %99

99:                                               ; preds = %91
  %100 = sub nuw nsw i64 %96, %97
  br label %101

101:                                              ; preds = %101, %99
  %102 = phi i64 [ 0, %99 ], [ %131, %101 ]
  %103 = phi i64 [ %100, %99 ], [ %132, %101 ]
  %104 = add i64 %102, %59
  %105 = add nsw i64 %104, %57
  %106 = getelementptr inbounds i16, i16* %2, i64 %105
  %107 = bitcast i16* %106 to <8 x i16>*
  %108 = load <8 x i16>, <8 x i16>* %107, align 2, !alias.scope !15
  %109 = getelementptr inbounds i16, i16* %106, i64 8
  %110 = bitcast i16* %109 to <8 x i16>*
  %111 = load <8 x i16>, <8 x i16>* %110, align 2, !alias.scope !15
  %112 = add nsw i64 %104, %58
  %113 = getelementptr inbounds i16, i16* %0, i64 %112
  %114 = bitcast i16* %113 to <8 x i16>*
  store <8 x i16> %108, <8 x i16>* %114, align 2, !alias.scope !18, !noalias !15
  %115 = getelementptr inbounds i16, i16* %113, i64 8
  %116 = bitcast i16* %115 to <8 x i16>*
  store <8 x i16> %111, <8 x i16>* %116, align 2, !alias.scope !18, !noalias !15
  %117 = or i64 %102, 16
  %118 = add i64 %117, %59
  %119 = add nsw i64 %118, %57
  %120 = getelementptr inbounds i16, i16* %2, i64 %119
  %121 = bitcast i16* %120 to <8 x i16>*
  %122 = load <8 x i16>, <8 x i16>* %121, align 2, !alias.scope !15
  %123 = getelementptr inbounds i16, i16* %120, i64 8
  %124 = bitcast i16* %123 to <8 x i16>*
  %125 = load <8 x i16>, <8 x i16>* %124, align 2, !alias.scope !15
  %126 = add nsw i64 %118, %58
  %127 = getelementptr inbounds i16, i16* %0, i64 %126
  %128 = bitcast i16* %127 to <8 x i16>*
  store <8 x i16> %122, <8 x i16>* %128, align 2, !alias.scope !18, !noalias !15
  %129 = getelementptr inbounds i16, i16* %127, i64 8
  %130 = bitcast i16* %129 to <8 x i16>*
  store <8 x i16> %125, <8 x i16>* %130, align 2, !alias.scope !18, !noalias !15
  %131 = add i64 %102, 32
  %132 = add i64 %103, -2
  %133 = icmp eq i64 %132, 0
  br i1 %133, label %134, label %101, !llvm.loop !20

134:                                              ; preds = %101, %91
  %135 = phi i64 [ 0, %91 ], [ %131, %101 ]
  %136 = icmp eq i64 %97, 0
  br i1 %136, label %151, label %137

137:                                              ; preds = %134
  %138 = add i64 %135, %59
  %139 = add nsw i64 %138, %57
  %140 = getelementptr inbounds i16, i16* %2, i64 %139
  %141 = bitcast i16* %140 to <8 x i16>*
  %142 = load <8 x i16>, <8 x i16>* %141, align 2, !alias.scope !15
  %143 = getelementptr inbounds i16, i16* %140, i64 8
  %144 = bitcast i16* %143 to <8 x i16>*
  %145 = load <8 x i16>, <8 x i16>* %144, align 2, !alias.scope !15
  %146 = add nsw i64 %138, %58
  %147 = getelementptr inbounds i16, i16* %0, i64 %146
  %148 = bitcast i16* %147 to <8 x i16>*
  store <8 x i16> %142, <8 x i16>* %148, align 2, !alias.scope !18, !noalias !15
  %149 = getelementptr inbounds i16, i16* %147, i64 8
  %150 = bitcast i16* %149 to <8 x i16>*
  store <8 x i16> %145, <8 x i16>* %150, align 2, !alias.scope !18, !noalias !15
  br label %151

151:                                              ; preds = %134, %137
  %152 = icmp eq i64 %60, %92
  br i1 %152, label %217, label %62

153:                                              ; preds = %31, %153
  %154 = phi i64 [ %187, %153 ], [ 0, %31 ]
  %155 = phi i64 [ %188, %153 ], [ %21, %31 ]
  %156 = add nsw i64 %154, %32
  %157 = getelementptr inbounds i16, i16* %2, i64 %156
  %158 = bitcast i16* %157 to <2 x i64>*
  %159 = load <2 x i64>, <2 x i64>* %158, align 1
  %160 = add nsw i64 %154, %33
  %161 = getelementptr inbounds i16, i16* %0, i64 %160
  %162 = bitcast i16* %161 to <2 x i64>*
  store <2 x i64> %159, <2 x i64>* %162, align 1
  %163 = or i64 %154, 8
  %164 = add nsw i64 %163, %32
  %165 = getelementptr inbounds i16, i16* %2, i64 %164
  %166 = bitcast i16* %165 to <2 x i64>*
  %167 = load <2 x i64>, <2 x i64>* %166, align 1
  %168 = add nsw i64 %163, %33
  %169 = getelementptr inbounds i16, i16* %0, i64 %168
  %170 = bitcast i16* %169 to <2 x i64>*
  store <2 x i64> %167, <2 x i64>* %170, align 1
  %171 = or i64 %154, 16
  %172 = add nsw i64 %171, %32
  %173 = getelementptr inbounds i16, i16* %2, i64 %172
  %174 = bitcast i16* %173 to <2 x i64>*
  %175 = load <2 x i64>, <2 x i64>* %174, align 1
  %176 = add nsw i64 %171, %33
  %177 = getelementptr inbounds i16, i16* %0, i64 %176
  %178 = bitcast i16* %177 to <2 x i64>*
  store <2 x i64> %175, <2 x i64>* %178, align 1
  %179 = or i64 %154, 24
  %180 = add nsw i64 %179, %32
  %181 = getelementptr inbounds i16, i16* %2, i64 %180
  %182 = bitcast i16* %181 to <2 x i64>*
  %183 = load <2 x i64>, <2 x i64>* %182, align 1
  %184 = add nsw i64 %179, %33
  %185 = getelementptr inbounds i16, i16* %0, i64 %184
  %186 = bitcast i16* %185 to <2 x i64>*
  store <2 x i64> %183, <2 x i64>* %186, align 1
  %187 = add nuw nsw i64 %154, 32
  %188 = add i64 %155, -4
  %189 = icmp eq i64 %188, 0
  br i1 %189, label %34, label %153

190:                                              ; preds = %80, %190
  %191 = phi i64 [ %215, %190 ], [ %81, %80 ]
  %192 = add nsw i64 %191, %57
  %193 = getelementptr inbounds i16, i16* %2, i64 %192
  %194 = load i16, i16* %193, align 2
  %195 = add nsw i64 %191, %58
  %196 = getelementptr inbounds i16, i16* %0, i64 %195
  store i16 %194, i16* %196, align 2
  %197 = add nuw nsw i64 %191, 1
  %198 = add nsw i64 %197, %57
  %199 = getelementptr inbounds i16, i16* %2, i64 %198
  %200 = load i16, i16* %199, align 2
  %201 = add nsw i64 %197, %58
  %202 = getelementptr inbounds i16, i16* %0, i64 %201
  store i16 %200, i16* %202, align 2
  %203 = add nuw nsw i64 %191, 2
  %204 = add nsw i64 %203, %57
  %205 = getelementptr inbounds i16, i16* %2, i64 %204
  %206 = load i16, i16* %205, align 2
  %207 = add nsw i64 %203, %58
  %208 = getelementptr inbounds i16, i16* %0, i64 %207
  store i16 %206, i16* %208, align 2
  %209 = add nuw nsw i64 %191, 3
  %210 = add nsw i64 %209, %57
  %211 = getelementptr inbounds i16, i16* %2, i64 %210
  %212 = load i16, i16* %211, align 2
  %213 = add nsw i64 %209, %58
  %214 = getelementptr inbounds i16, i16* %0, i64 %213
  store i16 %212, i16* %214, align 2
  %215 = add nuw nsw i64 %191, 4
  %216 = icmp eq i64 %215, %15
  br i1 %216, label %217, label %190, !llvm.loop !21

217:                                              ; preds = %80, %190, %151, %53
  %218 = add nuw nsw i64 %24, 1
  %219 = icmp eq i64 %218, %14
  br i1 %219, label %220, label %23

220:                                              ; preds = %217, %6
  ret void
}

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16>, <8 x i16>) #5

; Function Attrs: nounwind readnone
declare <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16>, <8 x i16>) #5

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32>, <4 x i32>) #5

; Function Attrs: nounwind readnone
declare <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16>, <8 x i16>) #5

; Function Attrs: nounwind readnone speculatable
declare i32 @llvm.ctlz.i32(i32, i1 immarg) #6

; Function Attrs: nounwind readnone speculatable
declare <16 x i8> @llvm.usub.sat.v16i8(<16 x i8>, <16 x i8>) #6

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16>, <8 x i16>) #5

; Function Attrs: nounwind readnone
declare <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16>, <8 x i16>) #5

; Function Attrs: nounwind readnone speculatable
declare <8 x i16> @llvm.usub.sat.v8i16(<8 x i16>, <8 x i16>) #6

attributes #0 = { nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="128" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { argmemonly nounwind }
attributes #2 = { inlinehint nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="128" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #3 = { nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="0" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #4 = { nofree norecurse nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="128" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #5 = { nounwind readnone }
attributes #6 = { nounwind readnone speculatable }
attributes #7 = { nounwind }

!llvm.module.flags = !{!0, !1}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 2}
!2 = !{i32 0, i32 33}
!3 = distinct !{!3, !4}
!4 = !{!"llvm.loop.unroll.disable"}
!5 = !{!6}
!6 = distinct !{!6, !7}
!7 = distinct !{!7, !"LVerDomain"}
!8 = !{!9}
!9 = distinct !{!9, !7}
!10 = distinct !{!10, !11}
!11 = !{!"llvm.loop.isvectorized", i32 1}
!12 = distinct !{!12, !11}
!13 = distinct !{!13, !4}
!14 = distinct !{!14, !4}
!15 = !{!16}
!16 = distinct !{!16, !17}
!17 = distinct !{!17, !"LVerDomain"}
!18 = !{!19}
!19 = distinct !{!19, !17}
!20 = distinct !{!20, !11}
!21 = distinct !{!21, !11}
