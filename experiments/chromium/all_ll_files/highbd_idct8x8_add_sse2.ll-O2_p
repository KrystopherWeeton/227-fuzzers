; ModuleID = '../../third_party/libvpx/source/libvpx/vpx_dsp/x86/highbd_idct8x8_add_sse2.c'
source_filename = "../../third_party/libvpx/source/libvpx/vpx_dsp/x86/highbd_idct8x8_add_sse2.c"
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

; Function Attrs: nounwind ssp uwtable
define hidden void @vpx_highbd_idct8x8_64_add_sse2(i32* readonly, i16* nocapture, i32, i32) local_unnamed_addr #0 {
  %5 = alloca [16 x <2 x i64>], align 16
  %6 = alloca [8 x <2 x i64>], align 16
  %7 = bitcast [16 x <2 x i64>]* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %7) #6
  %8 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 8
  %9 = bitcast <2 x i64>* %8 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %9, i8 -86, i64 128, i1 false)
  %10 = bitcast i32* %0 to <2 x i64>*
  %11 = load <2 x i64>, <2 x i64>* %10, align 16
  %12 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 0
  store <2 x i64> %11, <2 x i64>* %12, align 16
  %13 = getelementptr inbounds i32, i32* %0, i64 4
  %14 = bitcast i32* %13 to <2 x i64>*
  %15 = load <2 x i64>, <2 x i64>* %14, align 16
  %16 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 4
  store <2 x i64> %15, <2 x i64>* %16, align 16
  %17 = getelementptr inbounds i32, i32* %0, i64 8
  %18 = bitcast i32* %17 to <2 x i64>*
  %19 = load <2 x i64>, <2 x i64>* %18, align 16
  %20 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 1
  store <2 x i64> %19, <2 x i64>* %20, align 16
  %21 = getelementptr inbounds i32, i32* %0, i64 12
  %22 = bitcast i32* %21 to <2 x i64>*
  %23 = load <2 x i64>, <2 x i64>* %22, align 16
  %24 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 5
  store <2 x i64> %23, <2 x i64>* %24, align 16
  %25 = getelementptr inbounds i32, i32* %0, i64 16
  %26 = bitcast i32* %25 to <2 x i64>*
  %27 = load <2 x i64>, <2 x i64>* %26, align 16
  %28 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 2
  store <2 x i64> %27, <2 x i64>* %28, align 16
  %29 = getelementptr inbounds i32, i32* %0, i64 20
  %30 = bitcast i32* %29 to <2 x i64>*
  %31 = load <2 x i64>, <2 x i64>* %30, align 16
  %32 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 6
  store <2 x i64> %31, <2 x i64>* %32, align 16
  %33 = getelementptr inbounds i32, i32* %0, i64 24
  %34 = bitcast i32* %33 to <2 x i64>*
  %35 = load <2 x i64>, <2 x i64>* %34, align 16
  %36 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 3
  store <2 x i64> %35, <2 x i64>* %36, align 16
  %37 = getelementptr inbounds i32, i32* %0, i64 28
  %38 = bitcast i32* %37 to <2 x i64>*
  %39 = load <2 x i64>, <2 x i64>* %38, align 16
  %40 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 7
  store <2 x i64> %39, <2 x i64>* %40, align 16
  %41 = icmp eq i32 %3, 8
  br i1 %41, label %42, label %148

42:                                               ; preds = %4
  %43 = bitcast [8 x <2 x i64>]* %6 to i8*
  call void @llvm.lifetime.start.p0i8(i64 128, i8* nonnull %43) #6
  %44 = bitcast <2 x i64> %11 to <4 x i32>
  %45 = bitcast <2 x i64> %15 to <4 x i32>
  %46 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %44, <4 x i32> %45) #6
  %47 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 0
  %48 = bitcast [8 x <2 x i64>]* %6 to <8 x i16>*
  store <8 x i16> %46, <8 x i16>* %48, align 16
  %49 = bitcast <2 x i64> %19 to <4 x i32>
  %50 = bitcast <2 x i64> %23 to <4 x i32>
  %51 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %49, <4 x i32> %50) #6
  %52 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 1
  %53 = bitcast <2 x i64>* %52 to <8 x i16>*
  store <8 x i16> %51, <8 x i16>* %53, align 16
  %54 = bitcast <2 x i64> %27 to <4 x i32>
  %55 = bitcast <2 x i64> %31 to <4 x i32>
  %56 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %54, <4 x i32> %55) #6
  %57 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 2
  %58 = bitcast <2 x i64>* %57 to <8 x i16>*
  store <8 x i16> %56, <8 x i16>* %58, align 16
  %59 = bitcast <2 x i64> %35 to <4 x i32>
  %60 = bitcast <2 x i64> %39 to <4 x i32>
  %61 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %59, <4 x i32> %60) #6
  %62 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 3
  %63 = bitcast <2 x i64>* %62 to <8 x i16>*
  store <8 x i16> %61, <8 x i16>* %63, align 16
  %64 = getelementptr inbounds i32, i32* %0, i64 32
  %65 = bitcast i32* %64 to <2 x i64>*
  %66 = load <2 x i64>, <2 x i64>* %65, align 16
  %67 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 8
  store <2 x i64> %66, <2 x i64>* %67, align 16
  %68 = getelementptr inbounds i32, i32* %0, i64 36
  %69 = bitcast i32* %68 to <2 x i64>*
  %70 = load <2 x i64>, <2 x i64>* %69, align 16
  %71 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 12
  store <2 x i64> %70, <2 x i64>* %71, align 16
  %72 = getelementptr inbounds i32, i32* %0, i64 40
  %73 = bitcast i32* %72 to <2 x i64>*
  %74 = load <2 x i64>, <2 x i64>* %73, align 16
  %75 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 9
  store <2 x i64> %74, <2 x i64>* %75, align 16
  %76 = getelementptr inbounds i32, i32* %0, i64 44
  %77 = bitcast i32* %76 to <2 x i64>*
  %78 = load <2 x i64>, <2 x i64>* %77, align 16
  %79 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 13
  store <2 x i64> %78, <2 x i64>* %79, align 16
  %80 = getelementptr inbounds i32, i32* %0, i64 48
  %81 = bitcast i32* %80 to <2 x i64>*
  %82 = load <2 x i64>, <2 x i64>* %81, align 16
  %83 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 10
  store <2 x i64> %82, <2 x i64>* %83, align 16
  %84 = getelementptr inbounds i32, i32* %0, i64 52
  %85 = bitcast i32* %84 to <2 x i64>*
  %86 = load <2 x i64>, <2 x i64>* %85, align 16
  %87 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 14
  store <2 x i64> %86, <2 x i64>* %87, align 16
  %88 = getelementptr inbounds i32, i32* %0, i64 56
  %89 = bitcast i32* %88 to <2 x i64>*
  %90 = load <2 x i64>, <2 x i64>* %89, align 16
  %91 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 11
  store <2 x i64> %90, <2 x i64>* %91, align 16
  %92 = getelementptr inbounds i32, i32* %0, i64 60
  %93 = bitcast i32* %92 to <2 x i64>*
  %94 = load <2 x i64>, <2 x i64>* %93, align 16
  %95 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 15
  store <2 x i64> %94, <2 x i64>* %95, align 16
  %96 = bitcast <2 x i64> %66 to <4 x i32>
  %97 = bitcast <2 x i64> %70 to <4 x i32>
  %98 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %96, <4 x i32> %97) #6
  %99 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 4
  %100 = bitcast <2 x i64>* %99 to <8 x i16>*
  store <8 x i16> %98, <8 x i16>* %100, align 16
  %101 = bitcast <2 x i64> %74 to <4 x i32>
  %102 = bitcast <2 x i64> %78 to <4 x i32>
  %103 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %101, <4 x i32> %102) #6
  %104 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 5
  %105 = bitcast <2 x i64>* %104 to <8 x i16>*
  store <8 x i16> %103, <8 x i16>* %105, align 16
  %106 = bitcast <2 x i64> %82 to <4 x i32>
  %107 = bitcast <2 x i64> %86 to <4 x i32>
  %108 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %106, <4 x i32> %107) #6
  %109 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 6
  %110 = bitcast <2 x i64>* %109 to <8 x i16>*
  store <8 x i16> %108, <8 x i16>* %110, align 16
  %111 = bitcast <2 x i64> %90 to <4 x i32>
  %112 = bitcast <2 x i64> %94 to <4 x i32>
  %113 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %111, <4 x i32> %112) #6
  %114 = getelementptr inbounds [8 x <2 x i64>], [8 x <2 x i64>]* %6, i64 0, i64 7
  %115 = bitcast <2 x i64>* %114 to <8 x i16>*
  store <8 x i16> %113, <8 x i16>* %115, align 16
  call void @vpx_idct8_sse2(<2 x i64>* nonnull %47) #6
  call void @vpx_idct8_sse2(<2 x i64>* nonnull %47) #6
  %116 = load <8 x i16>, <8 x i16>* %48, align 16
  %117 = add <8 x i16> %116, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %118 = bitcast [16 x <2 x i64>]* %5 to <8 x i16>*
  %119 = load <8 x i16>, <8 x i16>* %53, align 16
  %120 = add <8 x i16> %119, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %121 = bitcast <2 x i64>* %20 to <8 x i16>*
  %122 = load <8 x i16>, <8 x i16>* %58, align 16
  %123 = add <8 x i16> %122, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %124 = bitcast <2 x i64>* %28 to <8 x i16>*
  %125 = load <8 x i16>, <8 x i16>* %63, align 16
  %126 = add <8 x i16> %125, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %127 = bitcast <2 x i64>* %36 to <8 x i16>*
  %128 = load <8 x i16>, <8 x i16>* %100, align 16
  %129 = add <8 x i16> %128, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %130 = bitcast <2 x i64>* %16 to <8 x i16>*
  %131 = load <8 x i16>, <8 x i16>* %105, align 16
  %132 = add <8 x i16> %131, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %133 = bitcast <2 x i64>* %24 to <8 x i16>*
  %134 = load <8 x i16>, <8 x i16>* %110, align 16
  %135 = add <8 x i16> %134, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %136 = bitcast <2 x i64>* %32 to <8 x i16>*
  %137 = load <8 x i16>, <8 x i16>* %115, align 16
  %138 = add <8 x i16> %137, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %139 = bitcast <2 x i64>* %40 to <8 x i16>*
  %140 = ashr <8 x i16> %117, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %140, <8 x i16>* %118, align 16
  %141 = ashr <8 x i16> %120, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %141, <8 x i16>* %121, align 16
  %142 = ashr <8 x i16> %123, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %142, <8 x i16>* %124, align 16
  %143 = ashr <8 x i16> %126, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %143, <8 x i16>* %127, align 16
  %144 = ashr <8 x i16> %129, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %144, <8 x i16>* %130, align 16
  %145 = ashr <8 x i16> %132, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %145, <8 x i16>* %133, align 16
  %146 = ashr <8 x i16> %135, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %146, <8 x i16>* %136, align 16
  %147 = ashr <8 x i16> %138, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %147, <8 x i16>* %139, align 16
  call void @llvm.lifetime.end.p0i8(i64 128, i8* nonnull %43) #6
  br label %269

148:                                              ; preds = %4
  call fastcc void @highbd_idct8x8_half1d(<2 x i64>* nonnull %12)
  %149 = getelementptr inbounds i32, i32* %0, i64 32
  %150 = bitcast i32* %149 to <2 x i64>*
  %151 = load <2 x i64>, <2 x i64>* %150, align 16
  %152 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 8
  store <2 x i64> %151, <2 x i64>* %152, align 16
  %153 = getelementptr inbounds i32, i32* %0, i64 36
  %154 = bitcast i32* %153 to <2 x i64>*
  %155 = load <2 x i64>, <2 x i64>* %154, align 16
  %156 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 12
  store <2 x i64> %155, <2 x i64>* %156, align 16
  %157 = getelementptr inbounds i32, i32* %0, i64 40
  %158 = bitcast i32* %157 to <2 x i64>*
  %159 = load <2 x i64>, <2 x i64>* %158, align 16
  %160 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 9
  store <2 x i64> %159, <2 x i64>* %160, align 16
  %161 = getelementptr inbounds i32, i32* %0, i64 44
  %162 = bitcast i32* %161 to <2 x i64>*
  %163 = load <2 x i64>, <2 x i64>* %162, align 16
  %164 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 13
  store <2 x i64> %163, <2 x i64>* %164, align 16
  %165 = getelementptr inbounds i32, i32* %0, i64 48
  %166 = bitcast i32* %165 to <2 x i64>*
  %167 = load <2 x i64>, <2 x i64>* %166, align 16
  %168 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 10
  store <2 x i64> %167, <2 x i64>* %168, align 16
  %169 = getelementptr inbounds i32, i32* %0, i64 52
  %170 = bitcast i32* %169 to <2 x i64>*
  %171 = load <2 x i64>, <2 x i64>* %170, align 16
  %172 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 14
  store <2 x i64> %171, <2 x i64>* %172, align 16
  %173 = getelementptr inbounds i32, i32* %0, i64 56
  %174 = bitcast i32* %173 to <2 x i64>*
  %175 = load <2 x i64>, <2 x i64>* %174, align 16
  %176 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 11
  store <2 x i64> %175, <2 x i64>* %176, align 16
  %177 = getelementptr inbounds i32, i32* %0, i64 60
  %178 = bitcast i32* %177 to <2 x i64>*
  %179 = load <2 x i64>, <2 x i64>* %178, align 16
  %180 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 15
  store <2 x i64> %179, <2 x i64>* %180, align 16
  call fastcc void @highbd_idct8x8_half1d(<2 x i64>* %152)
  %181 = load <2 x i64>, <2 x i64>* %16, align 16
  %182 = load <2 x i64>, <2 x i64>* %24, align 16
  %183 = load <2 x i64>, <2 x i64>* %32, align 16
  %184 = load <2 x i64>, <2 x i64>* %40, align 16
  %185 = load <2 x i64>, <2 x i64>* %152, align 16
  store <2 x i64> %185, <2 x i64>* %16, align 16
  %186 = load <2 x i64>, <2 x i64>* %160, align 16
  store <2 x i64> %186, <2 x i64>* %24, align 16
  %187 = load <2 x i64>, <2 x i64>* %168, align 16
  store <2 x i64> %187, <2 x i64>* %32, align 16
  %188 = load <2 x i64>, <2 x i64>* %176, align 16
  store <2 x i64> %188, <2 x i64>* %40, align 16
  call fastcc void @highbd_idct8x8_half1d(<2 x i64>* nonnull %12)
  store <2 x i64> %181, <2 x i64>* %152, align 16
  store <2 x i64> %182, <2 x i64>* %160, align 16
  store <2 x i64> %183, <2 x i64>* %168, align 16
  store <2 x i64> %184, <2 x i64>* %176, align 16
  call fastcc void @highbd_idct8x8_half1d(<2 x i64>* %152)
  %189 = bitcast [16 x <2 x i64>]* %5 to <4 x i32>*
  %190 = load <4 x i32>, <4 x i32>* %189, align 16
  %191 = bitcast <2 x i64>* %152 to <4 x i32>*
  %192 = load <4 x i32>, <4 x i32>* %191, align 16
  %193 = add <4 x i32> %190, <i32 16, i32 16, i32 16, i32 16>
  %194 = add <4 x i32> %192, <i32 16, i32 16, i32 16, i32 16>
  %195 = ashr <4 x i32> %193, <i32 5, i32 5, i32 5, i32 5>
  %196 = ashr <4 x i32> %194, <i32 5, i32 5, i32 5, i32 5>
  %197 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %195, <4 x i32> %196) #6
  %198 = bitcast [16 x <2 x i64>]* %5 to <8 x i16>*
  store <8 x i16> %197, <8 x i16>* %198, align 16
  %199 = bitcast <2 x i64>* %20 to <4 x i32>*
  %200 = load <4 x i32>, <4 x i32>* %199, align 16
  %201 = bitcast <2 x i64>* %160 to <4 x i32>*
  %202 = load <4 x i32>, <4 x i32>* %201, align 16
  %203 = add <4 x i32> %200, <i32 16, i32 16, i32 16, i32 16>
  %204 = add <4 x i32> %202, <i32 16, i32 16, i32 16, i32 16>
  %205 = ashr <4 x i32> %203, <i32 5, i32 5, i32 5, i32 5>
  %206 = ashr <4 x i32> %204, <i32 5, i32 5, i32 5, i32 5>
  %207 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %205, <4 x i32> %206) #6
  %208 = bitcast <2 x i64>* %20 to <8 x i16>*
  store <8 x i16> %207, <8 x i16>* %208, align 16
  %209 = bitcast <2 x i64>* %28 to <4 x i32>*
  %210 = load <4 x i32>, <4 x i32>* %209, align 16
  %211 = bitcast <2 x i64>* %168 to <4 x i32>*
  %212 = load <4 x i32>, <4 x i32>* %211, align 16
  %213 = add <4 x i32> %210, <i32 16, i32 16, i32 16, i32 16>
  %214 = add <4 x i32> %212, <i32 16, i32 16, i32 16, i32 16>
  %215 = ashr <4 x i32> %213, <i32 5, i32 5, i32 5, i32 5>
  %216 = ashr <4 x i32> %214, <i32 5, i32 5, i32 5, i32 5>
  %217 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %215, <4 x i32> %216) #6
  %218 = bitcast <2 x i64>* %28 to <8 x i16>*
  store <8 x i16> %217, <8 x i16>* %218, align 16
  %219 = bitcast <2 x i64>* %36 to <4 x i32>*
  %220 = load <4 x i32>, <4 x i32>* %219, align 16
  %221 = bitcast <2 x i64>* %176 to <4 x i32>*
  %222 = load <4 x i32>, <4 x i32>* %221, align 16
  %223 = add <4 x i32> %220, <i32 16, i32 16, i32 16, i32 16>
  %224 = add <4 x i32> %222, <i32 16, i32 16, i32 16, i32 16>
  %225 = ashr <4 x i32> %223, <i32 5, i32 5, i32 5, i32 5>
  %226 = ashr <4 x i32> %224, <i32 5, i32 5, i32 5, i32 5>
  %227 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %225, <4 x i32> %226) #6
  %228 = bitcast <2 x i64>* %36 to <8 x i16>*
  store <8 x i16> %227, <8 x i16>* %228, align 16
  %229 = bitcast <2 x i64>* %16 to <4 x i32>*
  %230 = load <4 x i32>, <4 x i32>* %229, align 16
  %231 = bitcast <2 x i64>* %156 to <4 x i32>*
  %232 = load <4 x i32>, <4 x i32>* %231, align 16
  %233 = add <4 x i32> %230, <i32 16, i32 16, i32 16, i32 16>
  %234 = add <4 x i32> %232, <i32 16, i32 16, i32 16, i32 16>
  %235 = ashr <4 x i32> %233, <i32 5, i32 5, i32 5, i32 5>
  %236 = ashr <4 x i32> %234, <i32 5, i32 5, i32 5, i32 5>
  %237 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %235, <4 x i32> %236) #6
  %238 = bitcast <2 x i64>* %16 to <8 x i16>*
  store <8 x i16> %237, <8 x i16>* %238, align 16
  %239 = bitcast <2 x i64>* %24 to <4 x i32>*
  %240 = load <4 x i32>, <4 x i32>* %239, align 16
  %241 = bitcast <2 x i64>* %164 to <4 x i32>*
  %242 = load <4 x i32>, <4 x i32>* %241, align 16
  %243 = add <4 x i32> %240, <i32 16, i32 16, i32 16, i32 16>
  %244 = add <4 x i32> %242, <i32 16, i32 16, i32 16, i32 16>
  %245 = ashr <4 x i32> %243, <i32 5, i32 5, i32 5, i32 5>
  %246 = ashr <4 x i32> %244, <i32 5, i32 5, i32 5, i32 5>
  %247 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %245, <4 x i32> %246) #6
  %248 = bitcast <2 x i64>* %24 to <8 x i16>*
  store <8 x i16> %247, <8 x i16>* %248, align 16
  %249 = bitcast <2 x i64>* %32 to <4 x i32>*
  %250 = load <4 x i32>, <4 x i32>* %249, align 16
  %251 = bitcast <2 x i64>* %172 to <4 x i32>*
  %252 = load <4 x i32>, <4 x i32>* %251, align 16
  %253 = add <4 x i32> %250, <i32 16, i32 16, i32 16, i32 16>
  %254 = add <4 x i32> %252, <i32 16, i32 16, i32 16, i32 16>
  %255 = ashr <4 x i32> %253, <i32 5, i32 5, i32 5, i32 5>
  %256 = ashr <4 x i32> %254, <i32 5, i32 5, i32 5, i32 5>
  %257 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %255, <4 x i32> %256) #6
  %258 = bitcast <2 x i64>* %32 to <8 x i16>*
  store <8 x i16> %257, <8 x i16>* %258, align 16
  %259 = bitcast <2 x i64>* %40 to <4 x i32>*
  %260 = load <4 x i32>, <4 x i32>* %259, align 16
  %261 = bitcast <2 x i64>* %180 to <4 x i32>*
  %262 = load <4 x i32>, <4 x i32>* %261, align 16
  %263 = add <4 x i32> %260, <i32 16, i32 16, i32 16, i32 16>
  %264 = add <4 x i32> %262, <i32 16, i32 16, i32 16, i32 16>
  %265 = ashr <4 x i32> %263, <i32 5, i32 5, i32 5, i32 5>
  %266 = ashr <4 x i32> %264, <i32 5, i32 5, i32 5, i32 5>
  %267 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %265, <4 x i32> %266) #6
  %268 = bitcast <2 x i64>* %40 to <8 x i16>*
  store <8 x i16> %267, <8 x i16>* %268, align 16
  br label %269

269:                                              ; preds = %148, %42
  %270 = phi <8 x i16> [ %267, %148 ], [ %147, %42 ]
  %271 = phi <8 x i16> [ %257, %148 ], [ %146, %42 ]
  %272 = phi <8 x i16> [ %247, %148 ], [ %145, %42 ]
  %273 = phi <8 x i16> [ %237, %148 ], [ %144, %42 ]
  %274 = phi <8 x i16> [ %227, %148 ], [ %143, %42 ]
  %275 = phi <8 x i16> [ %217, %148 ], [ %142, %42 ]
  %276 = phi <8 x i16> [ %207, %148 ], [ %141, %42 ]
  %277 = phi <8 x i16> [ %197, %148 ], [ %140, %42 ]
  %278 = bitcast i16* %1 to <8 x i16>*
  %279 = load <8 x i16>, <8 x i16>* %278, align 16
  %280 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>, i32 %3) #6
  %281 = add <8 x i16> %280, <i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1>
  %282 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %279, <8 x i16> %277) #6
  %283 = icmp sgt <8 x i16> %282, zeroinitializer
  %284 = select <8 x i1> %283, <8 x i16> %282, <8 x i16> zeroinitializer
  %285 = icmp slt <8 x i16> %284, %281
  %286 = select <8 x i1> %285, <8 x i16> %284, <8 x i16> %281
  store <8 x i16> %286, <8 x i16>* %278, align 16
  %287 = sext i32 %2 to i64
  %288 = getelementptr inbounds i16, i16* %1, i64 %287
  %289 = bitcast i16* %288 to <8 x i16>*
  %290 = load <8 x i16>, <8 x i16>* %289, align 16
  %291 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %290, <8 x i16> %276) #6
  %292 = icmp sgt <8 x i16> %291, zeroinitializer
  %293 = select <8 x i1> %292, <8 x i16> %291, <8 x i16> zeroinitializer
  %294 = icmp slt <8 x i16> %293, %281
  %295 = select <8 x i1> %294, <8 x i16> %293, <8 x i16> %281
  store <8 x i16> %295, <8 x i16>* %289, align 16
  %296 = getelementptr inbounds i16, i16* %288, i64 %287
  %297 = bitcast i16* %296 to <8 x i16>*
  %298 = load <8 x i16>, <8 x i16>* %297, align 16
  %299 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %298, <8 x i16> %275) #6
  %300 = icmp sgt <8 x i16> %299, zeroinitializer
  %301 = select <8 x i1> %300, <8 x i16> %299, <8 x i16> zeroinitializer
  %302 = icmp slt <8 x i16> %301, %281
  %303 = select <8 x i1> %302, <8 x i16> %301, <8 x i16> %281
  store <8 x i16> %303, <8 x i16>* %297, align 16
  %304 = getelementptr inbounds i16, i16* %296, i64 %287
  %305 = bitcast i16* %304 to <8 x i16>*
  %306 = load <8 x i16>, <8 x i16>* %305, align 16
  %307 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %306, <8 x i16> %274) #6
  %308 = icmp sgt <8 x i16> %307, zeroinitializer
  %309 = select <8 x i1> %308, <8 x i16> %307, <8 x i16> zeroinitializer
  %310 = icmp slt <8 x i16> %309, %281
  %311 = select <8 x i1> %310, <8 x i16> %309, <8 x i16> %281
  store <8 x i16> %311, <8 x i16>* %305, align 16
  %312 = getelementptr inbounds i16, i16* %304, i64 %287
  %313 = bitcast i16* %312 to <8 x i16>*
  %314 = load <8 x i16>, <8 x i16>* %313, align 16
  %315 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %314, <8 x i16> %273) #6
  %316 = icmp sgt <8 x i16> %315, zeroinitializer
  %317 = select <8 x i1> %316, <8 x i16> %315, <8 x i16> zeroinitializer
  %318 = icmp slt <8 x i16> %317, %281
  %319 = select <8 x i1> %318, <8 x i16> %317, <8 x i16> %281
  store <8 x i16> %319, <8 x i16>* %313, align 16
  %320 = getelementptr inbounds i16, i16* %312, i64 %287
  %321 = bitcast i16* %320 to <8 x i16>*
  %322 = load <8 x i16>, <8 x i16>* %321, align 16
  %323 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %322, <8 x i16> %272) #6
  %324 = icmp sgt <8 x i16> %323, zeroinitializer
  %325 = select <8 x i1> %324, <8 x i16> %323, <8 x i16> zeroinitializer
  %326 = icmp slt <8 x i16> %325, %281
  %327 = select <8 x i1> %326, <8 x i16> %325, <8 x i16> %281
  store <8 x i16> %327, <8 x i16>* %321, align 16
  %328 = getelementptr inbounds i16, i16* %320, i64 %287
  %329 = bitcast i16* %328 to <8 x i16>*
  %330 = load <8 x i16>, <8 x i16>* %329, align 16
  %331 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %330, <8 x i16> %271) #6
  %332 = icmp sgt <8 x i16> %331, zeroinitializer
  %333 = select <8 x i1> %332, <8 x i16> %331, <8 x i16> zeroinitializer
  %334 = icmp slt <8 x i16> %333, %281
  %335 = select <8 x i1> %334, <8 x i16> %333, <8 x i16> %281
  store <8 x i16> %335, <8 x i16>* %329, align 16
  %336 = getelementptr inbounds i16, i16* %328, i64 %287
  %337 = bitcast i16* %336 to <8 x i16>*
  %338 = load <8 x i16>, <8 x i16>* %337, align 16
  %339 = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %338, <8 x i16> %270) #6
  %340 = icmp sgt <8 x i16> %339, zeroinitializer
  %341 = select <8 x i1> %340, <8 x i16> %339, <8 x i16> zeroinitializer
  %342 = icmp slt <8 x i16> %341, %281
  %343 = select <8 x i1> %342, <8 x i16> %341, <8 x i16> %281
  store <8 x i16> %343, <8 x i16>* %337, align 16
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %7) #6
  ret void
}

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #1

; Function Attrs: argmemonly nounwind
declare void @llvm.memset.p0i8.i64(i8* nocapture writeonly, i8, i64, i1 immarg) #1

declare void @vpx_idct8_sse2(<2 x i64>*) local_unnamed_addr #2

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #1

; Function Attrs: nounwind ssp uwtable
define internal fastcc void @highbd_idct8x8_half1d(<2 x i64>*) unnamed_addr #0 {
  %2 = bitcast <2 x i64>* %0 to <4 x i32>*
  %3 = load <4 x i32>, <4 x i32>* %2, align 16
  %4 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %5 = bitcast <2 x i64>* %4 to <4 x i32>*
  %6 = load <4 x i32>, <4 x i32>* %5, align 16
  %7 = shufflevector <4 x i32> %3, <4 x i32> %6, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %8 = bitcast <4 x i32> %7 to <2 x i64>
  %9 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %10 = bitcast <2 x i64>* %9 to <4 x i32>*
  %11 = load <4 x i32>, <4 x i32>* %10, align 16
  %12 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %13 = bitcast <2 x i64>* %12 to <4 x i32>*
  %14 = load <4 x i32>, <4 x i32>* %13, align 16
  %15 = shufflevector <4 x i32> %11, <4 x i32> %14, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %16 = bitcast <4 x i32> %15 to <2 x i64>
  %17 = shufflevector <4 x i32> %3, <4 x i32> %6, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %18 = bitcast <4 x i32> %17 to <2 x i64>
  %19 = shufflevector <4 x i32> %11, <4 x i32> %14, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %20 = bitcast <4 x i32> %19 to <2 x i64>
  %21 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 4
  %22 = bitcast <2 x i64>* %21 to <4 x i32>*
  %23 = load <4 x i32>, <4 x i32>* %22, align 16
  %24 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 5
  %25 = bitcast <2 x i64>* %24 to <4 x i32>*
  %26 = load <4 x i32>, <4 x i32>* %25, align 16
  %27 = shufflevector <4 x i32> %23, <4 x i32> %26, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %28 = bitcast <4 x i32> %27 to <2 x i64>
  %29 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 6
  %30 = bitcast <2 x i64>* %29 to <4 x i32>*
  %31 = load <4 x i32>, <4 x i32>* %30, align 16
  %32 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 7
  %33 = bitcast <2 x i64>* %32 to <4 x i32>*
  %34 = load <4 x i32>, <4 x i32>* %33, align 16
  %35 = shufflevector <4 x i32> %31, <4 x i32> %34, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %36 = bitcast <4 x i32> %35 to <2 x i64>
  %37 = shufflevector <4 x i32> %23, <4 x i32> %26, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %38 = bitcast <4 x i32> %37 to <2 x i64>
  %39 = shufflevector <4 x i32> %31, <4 x i32> %34, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %40 = bitcast <4 x i32> %39 to <2 x i64>
  %41 = shufflevector <2 x i64> %8, <2 x i64> %16, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %41, <2 x i64>* %0, align 16
  %42 = shufflevector <2 x i64> %8, <2 x i64> %16, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %42, <2 x i64>* %4, align 16
  %43 = shufflevector <2 x i64> %18, <2 x i64> %20, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %43, <2 x i64>* %9, align 16
  %44 = shufflevector <2 x i64> %18, <2 x i64> %20, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %44, <2 x i64>* %12, align 16
  %45 = shufflevector <2 x i64> %28, <2 x i64> %36, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %45, <2 x i64>* %21, align 16
  %46 = shufflevector <2 x i64> %28, <2 x i64> %36, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %46, <2 x i64>* %24, align 16
  %47 = shufflevector <2 x i64> %38, <2 x i64> %40, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %47, <2 x i64>* %29, align 16
  %48 = shufflevector <2 x i64> %38, <2 x i64> %40, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %48, <2 x i64>* %32, align 16
  %49 = bitcast <2 x i64> %42 to <4 x i32>
  %50 = ashr <4 x i32> %49, <i32 31, i32 31, i32 31, i32 31>
  %51 = xor <4 x i32> %50, %49
  %52 = sub <4 x i32> %51, %50
  %53 = shufflevector <4 x i32> %50, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %54 = bitcast <4 x i32> %53 to <2 x i64>
  %55 = shufflevector <4 x i32> %50, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %56 = bitcast <4 x i32> %55 to <2 x i64>
  %57 = shufflevector <4 x i32> %52, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %58 = bitcast <4 x i32> %57 to <2 x i64>
  %59 = shufflevector <4 x i32> %52, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %60 = bitcast <4 x i32> %59 to <2 x i64>
  %61 = bitcast <2 x i64> %48 to <4 x i32>
  %62 = ashr <4 x i32> %61, <i32 31, i32 31, i32 31, i32 31>
  %63 = xor <4 x i32> %62, %61
  %64 = sub <4 x i32> %63, %62
  %65 = shufflevector <4 x i32> %62, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %66 = bitcast <4 x i32> %65 to <2 x i64>
  %67 = shufflevector <4 x i32> %62, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %68 = bitcast <4 x i32> %67 to <2 x i64>
  %69 = shufflevector <4 x i32> %64, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %70 = bitcast <4 x i32> %69 to <2 x i64>
  %71 = shufflevector <4 x i32> %64, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %72 = bitcast <4 x i32> %71 to <2 x i64>
  %73 = and <2 x i64> %60, <i64 4294967295, i64 4294967295>
  %74 = mul nuw nsw <2 x i64> %73, <i64 64276, i64 64276>
  %75 = xor <2 x i64> %74, %56
  %76 = and <2 x i64> %58, <i64 4294967295, i64 4294967295>
  %77 = mul nuw nsw <2 x i64> %76, <i64 64276, i64 64276>
  %78 = xor <2 x i64> %77, %54
  %79 = mul nuw nsw <2 x i64> %73, <i64 12784, i64 12784>
  %80 = xor <2 x i64> %79, %56
  %81 = mul nuw nsw <2 x i64> %76, <i64 12784, i64 12784>
  %82 = xor <2 x i64> %81, %54
  %83 = and <2 x i64> %72, <i64 4294967295, i64 4294967295>
  %84 = mul nuw nsw <2 x i64> %83, <i64 12784, i64 12784>
  %85 = xor <2 x i64> %84, %68
  %86 = and <2 x i64> %70, <i64 4294967295, i64 4294967295>
  %87 = mul nuw nsw <2 x i64> %86, <i64 12784, i64 12784>
  %88 = xor <2 x i64> %87, %66
  %89 = mul nuw nsw <2 x i64> %83, <i64 64276, i64 64276>
  %90 = xor <2 x i64> %89, %68
  %91 = mul nuw nsw <2 x i64> %86, <i64 64276, i64 64276>
  %92 = xor <2 x i64> %91, %66
  %93 = sub <2 x i64> <i64 32768, i64 32768>, %56
  %94 = add <2 x i64> %93, %68
  %95 = add <2 x i64> %94, %80
  %96 = sub <2 x i64> %95, %90
  %97 = bitcast <2 x i64> %96 to <16 x i8>
  %98 = shufflevector <16 x i8> %97, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %99 = sub <2 x i64> <i64 32768, i64 32768>, %54
  %100 = add <2 x i64> %99, %66
  %101 = add <2 x i64> %100, %82
  %102 = sub <2 x i64> %101, %92
  %103 = bitcast <2 x i64> %102 to <16 x i8>
  %104 = shufflevector <16 x i8> %103, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %105 = sub <2 x i64> %93, %68
  %106 = add <2 x i64> %105, %75
  %107 = add <2 x i64> %106, %85
  %108 = bitcast <2 x i64> %107 to <16 x i8>
  %109 = shufflevector <16 x i8> %108, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %110 = sub <2 x i64> %99, %66
  %111 = add <2 x i64> %110, %78
  %112 = add <2 x i64> %111, %88
  %113 = bitcast <2 x i64> %112 to <16 x i8>
  %114 = shufflevector <16 x i8> %113, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %115 = bitcast <16 x i8> %98 to <4 x i32>
  %116 = bitcast <16 x i8> %104 to <4 x i32>
  %117 = shufflevector <4 x i32> %115, <4 x i32> %116, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %118 = shufflevector <4 x i32> %115, <4 x i32> %116, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %119 = shufflevector <4 x i32> %117, <4 x i32> %118, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %120 = bitcast <16 x i8> %109 to <4 x i32>
  %121 = bitcast <16 x i8> %114 to <4 x i32>
  %122 = shufflevector <4 x i32> %120, <4 x i32> %121, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %123 = shufflevector <4 x i32> %120, <4 x i32> %121, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %124 = shufflevector <4 x i32> %122, <4 x i32> %123, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %125 = bitcast <2 x i64> %46 to <4 x i32>
  %126 = ashr <4 x i32> %125, <i32 31, i32 31, i32 31, i32 31>
  %127 = xor <4 x i32> %126, %125
  %128 = sub <4 x i32> %127, %126
  %129 = shufflevector <4 x i32> %126, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %130 = bitcast <4 x i32> %129 to <2 x i64>
  %131 = shufflevector <4 x i32> %126, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %132 = bitcast <4 x i32> %131 to <2 x i64>
  %133 = shufflevector <4 x i32> %128, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %134 = bitcast <4 x i32> %133 to <2 x i64>
  %135 = shufflevector <4 x i32> %128, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %136 = bitcast <4 x i32> %135 to <2 x i64>
  %137 = bitcast <2 x i64> %44 to <4 x i32>
  %138 = ashr <4 x i32> %137, <i32 31, i32 31, i32 31, i32 31>
  %139 = xor <4 x i32> %138, %137
  %140 = sub <4 x i32> %139, %138
  %141 = shufflevector <4 x i32> %138, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %142 = bitcast <4 x i32> %141 to <2 x i64>
  %143 = shufflevector <4 x i32> %138, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %144 = bitcast <4 x i32> %143 to <2 x i64>
  %145 = shufflevector <4 x i32> %140, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %146 = bitcast <4 x i32> %145 to <2 x i64>
  %147 = shufflevector <4 x i32> %140, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %148 = bitcast <4 x i32> %147 to <2 x i64>
  %149 = and <2 x i64> %136, <i64 4294967295, i64 4294967295>
  %150 = mul nuw nsw <2 x i64> %149, <i64 36408, i64 36408>
  %151 = xor <2 x i64> %150, %132
  %152 = and <2 x i64> %134, <i64 4294967295, i64 4294967295>
  %153 = mul nuw nsw <2 x i64> %152, <i64 36408, i64 36408>
  %154 = xor <2 x i64> %153, %130
  %155 = mul nuw nsw <2 x i64> %149, <i64 54492, i64 54492>
  %156 = xor <2 x i64> %155, %132
  %157 = mul nuw nsw <2 x i64> %152, <i64 54492, i64 54492>
  %158 = xor <2 x i64> %157, %130
  %159 = and <2 x i64> %148, <i64 4294967295, i64 4294967295>
  %160 = mul nuw nsw <2 x i64> %159, <i64 54492, i64 54492>
  %161 = xor <2 x i64> %160, %144
  %162 = and <2 x i64> %146, <i64 4294967295, i64 4294967295>
  %163 = mul nuw nsw <2 x i64> %162, <i64 54492, i64 54492>
  %164 = xor <2 x i64> %163, %142
  %165 = mul nuw nsw <2 x i64> %159, <i64 36408, i64 36408>
  %166 = xor <2 x i64> %165, %144
  %167 = mul nuw nsw <2 x i64> %162, <i64 36408, i64 36408>
  %168 = xor <2 x i64> %167, %142
  %169 = sub <2 x i64> <i64 32768, i64 32768>, %132
  %170 = add <2 x i64> %169, %144
  %171 = sub <2 x i64> %170, %166
  %172 = add <2 x i64> %171, %156
  %173 = bitcast <2 x i64> %172 to <16 x i8>
  %174 = shufflevector <16 x i8> %173, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %175 = sub <2 x i64> <i64 32768, i64 32768>, %130
  %176 = add <2 x i64> %175, %142
  %177 = sub <2 x i64> %176, %168
  %178 = add <2 x i64> %177, %158
  %179 = bitcast <2 x i64> %178 to <16 x i8>
  %180 = shufflevector <16 x i8> %179, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %181 = sub <2 x i64> %169, %144
  %182 = add <2 x i64> %181, %161
  %183 = add <2 x i64> %182, %151
  %184 = bitcast <2 x i64> %183 to <16 x i8>
  %185 = shufflevector <16 x i8> %184, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %186 = sub <2 x i64> %175, %142
  %187 = add <2 x i64> %186, %164
  %188 = add <2 x i64> %187, %154
  %189 = bitcast <2 x i64> %188 to <16 x i8>
  %190 = shufflevector <16 x i8> %189, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %191 = bitcast <16 x i8> %174 to <4 x i32>
  %192 = bitcast <16 x i8> %180 to <4 x i32>
  %193 = shufflevector <4 x i32> %191, <4 x i32> %192, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %194 = shufflevector <4 x i32> %191, <4 x i32> %192, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %195 = shufflevector <4 x i32> %193, <4 x i32> %194, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %196 = bitcast <16 x i8> %185 to <4 x i32>
  %197 = bitcast <16 x i8> %190 to <4 x i32>
  %198 = shufflevector <4 x i32> %196, <4 x i32> %197, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %199 = shufflevector <4 x i32> %196, <4 x i32> %197, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %200 = shufflevector <4 x i32> %198, <4 x i32> %199, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %201 = bitcast <2 x i64> %41 to <4 x i32>
  %202 = bitcast <2 x i64> %45 to <4 x i32>
  %203 = add <4 x i32> %202, %201
  %204 = ashr <4 x i32> %203, <i32 31, i32 31, i32 31, i32 31>
  %205 = xor <4 x i32> %204, %203
  %206 = sub <4 x i32> %205, %204
  %207 = shufflevector <4 x i32> %204, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %208 = bitcast <4 x i32> %207 to <2 x i64>
  %209 = shufflevector <4 x i32> %204, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %210 = bitcast <4 x i32> %209 to <2 x i64>
  %211 = shufflevector <4 x i32> %206, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %212 = bitcast <4 x i32> %211 to <2 x i64>
  %213 = shufflevector <4 x i32> %206, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %214 = bitcast <4 x i32> %213 to <2 x i64>
  %215 = and <2 x i64> %214, <i64 4294967295, i64 4294967295>
  %216 = mul nuw nsw <2 x i64> %215, <i64 46340, i64 46340>
  %217 = xor <2 x i64> %216, %210
  %218 = and <2 x i64> %212, <i64 4294967295, i64 4294967295>
  %219 = mul nuw nsw <2 x i64> %218, <i64 46340, i64 46340>
  %220 = xor <2 x i64> %219, %208
  %221 = sub <2 x i64> <i64 32768, i64 32768>, %210
  %222 = add <2 x i64> %221, %217
  %223 = bitcast <2 x i64> %222 to <16 x i8>
  %224 = shufflevector <16 x i8> %223, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %225 = sub <2 x i64> <i64 32768, i64 32768>, %208
  %226 = add <2 x i64> %225, %220
  %227 = bitcast <2 x i64> %226 to <16 x i8>
  %228 = shufflevector <16 x i8> %227, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %229 = bitcast <16 x i8> %224 to <4 x i32>
  %230 = bitcast <16 x i8> %228 to <4 x i32>
  %231 = shufflevector <4 x i32> %229, <4 x i32> %230, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %232 = shufflevector <4 x i32> %229, <4 x i32> %230, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %233 = shufflevector <4 x i32> %231, <4 x i32> %232, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %234 = sub <4 x i32> %201, %202
  %235 = ashr <4 x i32> %234, <i32 31, i32 31, i32 31, i32 31>
  %236 = xor <4 x i32> %235, %234
  %237 = sub <4 x i32> %236, %235
  %238 = shufflevector <4 x i32> %235, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %239 = bitcast <4 x i32> %238 to <2 x i64>
  %240 = shufflevector <4 x i32> %235, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %241 = bitcast <4 x i32> %240 to <2 x i64>
  %242 = shufflevector <4 x i32> %237, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %243 = bitcast <4 x i32> %242 to <2 x i64>
  %244 = shufflevector <4 x i32> %237, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %245 = bitcast <4 x i32> %244 to <2 x i64>
  %246 = and <2 x i64> %245, <i64 4294967295, i64 4294967295>
  %247 = mul nuw nsw <2 x i64> %246, <i64 46340, i64 46340>
  %248 = xor <2 x i64> %247, %241
  %249 = and <2 x i64> %243, <i64 4294967295, i64 4294967295>
  %250 = mul nuw nsw <2 x i64> %249, <i64 46340, i64 46340>
  %251 = xor <2 x i64> %250, %239
  %252 = sub <2 x i64> <i64 32768, i64 32768>, %241
  %253 = add <2 x i64> %252, %248
  %254 = bitcast <2 x i64> %253 to <16 x i8>
  %255 = shufflevector <16 x i8> %254, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %256 = sub <2 x i64> <i64 32768, i64 32768>, %239
  %257 = add <2 x i64> %256, %251
  %258 = bitcast <2 x i64> %257 to <16 x i8>
  %259 = shufflevector <16 x i8> %258, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %260 = bitcast <16 x i8> %255 to <4 x i32>
  %261 = bitcast <16 x i8> %259 to <4 x i32>
  %262 = shufflevector <4 x i32> %260, <4 x i32> %261, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %263 = shufflevector <4 x i32> %260, <4 x i32> %261, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %264 = shufflevector <4 x i32> %262, <4 x i32> %263, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %265 = bitcast <2 x i64> %43 to <4 x i32>
  %266 = ashr <4 x i32> %265, <i32 31, i32 31, i32 31, i32 31>
  %267 = xor <4 x i32> %266, %265
  %268 = sub <4 x i32> %267, %266
  %269 = shufflevector <4 x i32> %266, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %270 = bitcast <4 x i32> %269 to <2 x i64>
  %271 = shufflevector <4 x i32> %266, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %272 = bitcast <4 x i32> %271 to <2 x i64>
  %273 = shufflevector <4 x i32> %268, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %274 = bitcast <4 x i32> %273 to <2 x i64>
  %275 = shufflevector <4 x i32> %268, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %276 = bitcast <4 x i32> %275 to <2 x i64>
  %277 = bitcast <2 x i64> %47 to <4 x i32>
  %278 = ashr <4 x i32> %277, <i32 31, i32 31, i32 31, i32 31>
  %279 = xor <4 x i32> %278, %277
  %280 = sub <4 x i32> %279, %278
  %281 = shufflevector <4 x i32> %278, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %282 = bitcast <4 x i32> %281 to <2 x i64>
  %283 = shufflevector <4 x i32> %278, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %284 = bitcast <4 x i32> %283 to <2 x i64>
  %285 = shufflevector <4 x i32> %280, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %286 = bitcast <4 x i32> %285 to <2 x i64>
  %287 = shufflevector <4 x i32> %280, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %288 = bitcast <4 x i32> %287 to <2 x i64>
  %289 = and <2 x i64> %276, <i64 4294967295, i64 4294967295>
  %290 = mul nuw nsw <2 x i64> %289, <i64 60548, i64 60548>
  %291 = xor <2 x i64> %290, %272
  %292 = and <2 x i64> %274, <i64 4294967295, i64 4294967295>
  %293 = mul nuw nsw <2 x i64> %292, <i64 60548, i64 60548>
  %294 = xor <2 x i64> %293, %270
  %295 = mul nuw nsw <2 x i64> %289, <i64 25080, i64 25080>
  %296 = xor <2 x i64> %295, %272
  %297 = mul nuw nsw <2 x i64> %292, <i64 25080, i64 25080>
  %298 = xor <2 x i64> %297, %270
  %299 = and <2 x i64> %288, <i64 4294967295, i64 4294967295>
  %300 = mul nuw nsw <2 x i64> %299, <i64 25080, i64 25080>
  %301 = xor <2 x i64> %300, %284
  %302 = and <2 x i64> %286, <i64 4294967295, i64 4294967295>
  %303 = mul nuw nsw <2 x i64> %302, <i64 25080, i64 25080>
  %304 = xor <2 x i64> %303, %282
  %305 = mul nuw nsw <2 x i64> %299, <i64 60548, i64 60548>
  %306 = xor <2 x i64> %305, %284
  %307 = mul nuw nsw <2 x i64> %302, <i64 60548, i64 60548>
  %308 = xor <2 x i64> %307, %282
  %309 = sub <2 x i64> <i64 32768, i64 32768>, %272
  %310 = add <2 x i64> %309, %284
  %311 = add <2 x i64> %310, %296
  %312 = sub <2 x i64> %311, %306
  %313 = bitcast <2 x i64> %312 to <16 x i8>
  %314 = shufflevector <16 x i8> %313, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %315 = sub <2 x i64> <i64 32768, i64 32768>, %270
  %316 = add <2 x i64> %315, %282
  %317 = add <2 x i64> %316, %298
  %318 = sub <2 x i64> %317, %308
  %319 = bitcast <2 x i64> %318 to <16 x i8>
  %320 = shufflevector <16 x i8> %319, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %321 = sub <2 x i64> %309, %284
  %322 = add <2 x i64> %321, %291
  %323 = add <2 x i64> %322, %301
  %324 = bitcast <2 x i64> %323 to <16 x i8>
  %325 = shufflevector <16 x i8> %324, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %326 = sub <2 x i64> %315, %282
  %327 = add <2 x i64> %326, %294
  %328 = add <2 x i64> %327, %304
  %329 = bitcast <2 x i64> %328 to <16 x i8>
  %330 = shufflevector <16 x i8> %329, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %331 = bitcast <16 x i8> %314 to <4 x i32>
  %332 = bitcast <16 x i8> %320 to <4 x i32>
  %333 = shufflevector <4 x i32> %331, <4 x i32> %332, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %334 = shufflevector <4 x i32> %331, <4 x i32> %332, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %335 = shufflevector <4 x i32> %333, <4 x i32> %334, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %336 = bitcast <16 x i8> %325 to <4 x i32>
  %337 = bitcast <16 x i8> %330 to <4 x i32>
  %338 = shufflevector <4 x i32> %336, <4 x i32> %337, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %339 = shufflevector <4 x i32> %336, <4 x i32> %337, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %340 = shufflevector <4 x i32> %338, <4 x i32> %339, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %341 = add <4 x i32> %195, %119
  %342 = sub <4 x i32> %119, %195
  %343 = sub <4 x i32> %124, %200
  %344 = add <4 x i32> %124, %200
  %345 = add <4 x i32> %340, %233
  %346 = add <4 x i32> %264, %335
  %347 = sub <4 x i32> %264, %335
  %348 = sub <4 x i32> %233, %340
  %349 = add <4 x i32> %343, %342
  %350 = ashr <4 x i32> %349, <i32 31, i32 31, i32 31, i32 31>
  %351 = xor <4 x i32> %350, %349
  %352 = sub <4 x i32> %351, %350
  %353 = shufflevector <4 x i32> %350, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %354 = bitcast <4 x i32> %353 to <2 x i64>
  %355 = shufflevector <4 x i32> %350, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %356 = bitcast <4 x i32> %355 to <2 x i64>
  %357 = shufflevector <4 x i32> %352, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %358 = bitcast <4 x i32> %357 to <2 x i64>
  %359 = shufflevector <4 x i32> %352, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %360 = bitcast <4 x i32> %359 to <2 x i64>
  %361 = and <2 x i64> %360, <i64 4294967295, i64 4294967295>
  %362 = mul nuw nsw <2 x i64> %361, <i64 46340, i64 46340>
  %363 = xor <2 x i64> %362, %356
  %364 = and <2 x i64> %358, <i64 4294967295, i64 4294967295>
  %365 = mul nuw nsw <2 x i64> %364, <i64 46340, i64 46340>
  %366 = xor <2 x i64> %365, %354
  %367 = sub <2 x i64> <i64 32768, i64 32768>, %356
  %368 = add <2 x i64> %367, %363
  %369 = bitcast <2 x i64> %368 to <16 x i8>
  %370 = shufflevector <16 x i8> %369, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %371 = sub <2 x i64> <i64 32768, i64 32768>, %354
  %372 = add <2 x i64> %371, %366
  %373 = bitcast <2 x i64> %372 to <16 x i8>
  %374 = shufflevector <16 x i8> %373, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %375 = bitcast <16 x i8> %370 to <4 x i32>
  %376 = bitcast <16 x i8> %374 to <4 x i32>
  %377 = shufflevector <4 x i32> %375, <4 x i32> %376, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %378 = shufflevector <4 x i32> %375, <4 x i32> %376, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %379 = shufflevector <4 x i32> %377, <4 x i32> %378, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %380 = sub <4 x i32> %343, %342
  %381 = ashr <4 x i32> %380, <i32 31, i32 31, i32 31, i32 31>
  %382 = xor <4 x i32> %381, %380
  %383 = sub <4 x i32> %382, %381
  %384 = shufflevector <4 x i32> %381, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %385 = bitcast <4 x i32> %384 to <2 x i64>
  %386 = shufflevector <4 x i32> %381, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %387 = bitcast <4 x i32> %386 to <2 x i64>
  %388 = shufflevector <4 x i32> %383, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %389 = bitcast <4 x i32> %388 to <2 x i64>
  %390 = shufflevector <4 x i32> %383, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %391 = bitcast <4 x i32> %390 to <2 x i64>
  %392 = and <2 x i64> %391, <i64 4294967295, i64 4294967295>
  %393 = mul nuw nsw <2 x i64> %392, <i64 46340, i64 46340>
  %394 = xor <2 x i64> %393, %387
  %395 = and <2 x i64> %389, <i64 4294967295, i64 4294967295>
  %396 = mul nuw nsw <2 x i64> %395, <i64 46340, i64 46340>
  %397 = xor <2 x i64> %396, %385
  %398 = sub <2 x i64> <i64 32768, i64 32768>, %387
  %399 = add <2 x i64> %398, %394
  %400 = bitcast <2 x i64> %399 to <16 x i8>
  %401 = shufflevector <16 x i8> %400, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %402 = sub <2 x i64> <i64 32768, i64 32768>, %385
  %403 = add <2 x i64> %402, %397
  %404 = bitcast <2 x i64> %403 to <16 x i8>
  %405 = shufflevector <16 x i8> %404, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %406 = bitcast <16 x i8> %401 to <4 x i32>
  %407 = bitcast <16 x i8> %405 to <4 x i32>
  %408 = shufflevector <4 x i32> %406, <4 x i32> %407, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %409 = shufflevector <4 x i32> %406, <4 x i32> %407, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %410 = shufflevector <4 x i32> %408, <4 x i32> %409, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %411 = add <4 x i32> %344, %345
  store <4 x i32> %411, <4 x i32>* %2, align 16
  %412 = add <4 x i32> %379, %346
  store <4 x i32> %412, <4 x i32>* %5, align 16
  %413 = add <4 x i32> %410, %347
  store <4 x i32> %413, <4 x i32>* %10, align 16
  %414 = add <4 x i32> %348, %341
  store <4 x i32> %414, <4 x i32>* %13, align 16
  %415 = sub <4 x i32> %348, %341
  store <4 x i32> %415, <4 x i32>* %22, align 16
  %416 = sub <4 x i32> %347, %410
  store <4 x i32> %416, <4 x i32>* %25, align 16
  %417 = sub <4 x i32> %346, %379
  store <4 x i32> %417, <4 x i32>* %30, align 16
  %418 = sub <4 x i32> %345, %344
  store <4 x i32> %418, <4 x i32>* %33, align 16
  ret void
}

; Function Attrs: nounwind ssp uwtable
define hidden void @vpx_highbd_idct8x8_12_add_sse2(i32* nocapture readonly, i16* nocapture, i32, i32) local_unnamed_addr #0 {
  %5 = alloca [16 x <2 x i64>], align 16
  %6 = bitcast [16 x <2 x i64>]* %5 to i8*
  call void @llvm.lifetime.start.p0i8(i64 256, i8* nonnull %6) #6
  %7 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 4
  %8 = bitcast <2 x i64>* %7 to i8*
  call void @llvm.memset.p0i8.i64(i8* nonnull align 16 %8, i8 -86, i64 192, i1 false)
  %9 = bitcast i32* %0 to <2 x i64>*
  %10 = load <2 x i64>, <2 x i64>* %9, align 16
  %11 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 0
  store <2 x i64> %10, <2 x i64>* %11, align 16
  %12 = getelementptr inbounds i32, i32* %0, i64 8
  %13 = bitcast i32* %12 to <2 x i64>*
  %14 = load <2 x i64>, <2 x i64>* %13, align 16
  %15 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 1
  store <2 x i64> %14, <2 x i64>* %15, align 16
  %16 = getelementptr inbounds i32, i32* %0, i64 16
  %17 = bitcast i32* %16 to <2 x i64>*
  %18 = load <2 x i64>, <2 x i64>* %17, align 16
  %19 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 2
  store <2 x i64> %18, <2 x i64>* %19, align 16
  %20 = getelementptr inbounds i32, i32* %0, i64 24
  %21 = bitcast i32* %20 to <2 x i64>*
  %22 = load <2 x i64>, <2 x i64>* %21, align 16
  %23 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 3
  store <2 x i64> %22, <2 x i64>* %23, align 16
  %24 = icmp eq i32 %3, 8
  br i1 %24, label %25, label %244

25:                                               ; preds = %4
  %26 = bitcast <2 x i64> %10 to <4 x i32>
  %27 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %26, <4 x i32> undef) #6
  %28 = bitcast <2 x i64> %14 to <4 x i32>
  %29 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %28, <4 x i32> undef) #6
  %30 = bitcast <2 x i64> %18 to <4 x i32>
  %31 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %30, <4 x i32> undef) #6
  %32 = bitcast <2 x i64> %22 to <4 x i32>
  %33 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %32, <4 x i32> undef) #6
  %34 = shufflevector <8 x i16> %27, <8 x i16> %29, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %35 = shufflevector <8 x i16> %31, <8 x i16> %33, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %36 = bitcast <8 x i16> %34 to <4 x i32>
  %37 = bitcast <8 x i16> %35 to <4 x i32>
  %38 = shufflevector <4 x i32> %36, <4 x i32> %37, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %39 = shufflevector <4 x i32> %36, <4 x i32> %37, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %40 = bitcast <4 x i32> %38 to <8 x i16>
  %41 = shufflevector <8 x i16> %40, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 0, i16 0, i16 0, i16 0>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %42 = bitcast <4 x i32> %39 to <8 x i16>
  %43 = shufflevector <8 x i16> %42, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 0, i16 0, i16 0, i16 0>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %44 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> <i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069>, <8 x i16> %41) #6
  %45 = add <4 x i32> %44, <i32 8192, i32 8192, i32 8192, i32 8192>
  %46 = ashr <4 x i32> %45, <i32 14, i32 14, i32 14, i32 14>
  %47 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> <i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196>, <8 x i16> %41) #6
  %48 = add <4 x i32> %47, <i32 8192, i32 8192, i32 8192, i32 8192>
  %49 = ashr <4 x i32> %48, <i32 14, i32 14, i32 14, i32 14>
  %50 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %46, <4 x i32> %49) #6
  %51 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> <i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623>, <8 x i16> %43) #6
  %52 = add <4 x i32> %51, <i32 8192, i32 8192, i32 8192, i32 8192>
  %53 = ashr <4 x i32> %52, <i32 14, i32 14, i32 14, i32 14>
  %54 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> <i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102>, <8 x i16> %43) #6
  %55 = add <4 x i32> %54, <i32 8192, i32 8192, i32 8192, i32 8192>
  %56 = ashr <4 x i32> %55, <i32 14, i32 14, i32 14, i32 14>
  %57 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %53, <4 x i32> %56) #6
  %58 = shufflevector <8 x i16> %40, <8 x i16> <i16 0, i16 0, i16 0, i16 0, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %59 = shufflevector <8 x i16> %42, <8 x i16> <i16 0, i16 0, i16 0, i16 0, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %60 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>, <8 x i16> %58) #6
  %61 = add <4 x i32> %60, <i32 8192, i32 8192, i32 8192, i32 8192>
  %62 = ashr <4 x i32> %61, <i32 14, i32 14, i32 14, i32 14>
  %63 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %62, <4 x i32> %62) #6
  %64 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> <i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270>, <8 x i16> %59) #6
  %65 = add <4 x i32> %64, <i32 8192, i32 8192, i32 8192, i32 8192>
  %66 = ashr <4 x i32> %65, <i32 14, i32 14, i32 14, i32 14>
  %67 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> <i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137>, <8 x i16> %59) #6
  %68 = add <4 x i32> %67, <i32 8192, i32 8192, i32 8192, i32 8192>
  %69 = ashr <4 x i32> %68, <i32 14, i32 14, i32 14, i32 14>
  %70 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %66, <4 x i32> %69) #6
  %71 = add <8 x i16> %57, %50
  %72 = sub <8 x i16> %50, %57
  %73 = bitcast <8 x i16> %72 to <2 x i64>
  %74 = shufflevector <2 x i64> %73, <2 x i64> undef, <2 x i32> <i32 1, i32 undef>
  %75 = bitcast <2 x i64> %74 to <8 x i16>
  %76 = shufflevector <8 x i16> %75, <8 x i16> %72, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %77 = add <8 x i16> %70, %63
  %78 = bitcast <8 x i16> %77 to <2 x i64>
  %79 = sub <8 x i16> %63, %70
  %80 = bitcast <8 x i16> %79 to <2 x i64>
  %81 = shufflevector <2 x i64> %80, <2 x i64> %78, <2 x i32> <i32 1, i32 3>
  %82 = shufflevector <2 x i64> %80, <2 x i64> %78, <2 x i32> <i32 0, i32 2>
  %83 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>, <8 x i16> %76) #6
  %84 = add <4 x i32> %83, <i32 8192, i32 8192, i32 8192, i32 8192>
  %85 = ashr <4 x i32> %84, <i32 14, i32 14, i32 14, i32 14>
  %86 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>, <8 x i16> %76) #6
  %87 = add <4 x i32> %86, <i32 8192, i32 8192, i32 8192, i32 8192>
  %88 = ashr <4 x i32> %87, <i32 14, i32 14, i32 14, i32 14>
  %89 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %85, <4 x i32> %88) #6
  %90 = bitcast <2 x i64> %82 to <8 x i16>
  %91 = add <8 x i16> %71, %90
  %92 = bitcast <2 x i64> %81 to <8 x i16>
  %93 = add <8 x i16> %89, %92
  %94 = sub <8 x i16> %90, %71
  %95 = sub <8 x i16> %92, %89
  %96 = shufflevector <8 x i16> %91, <8 x i16> %93, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %97 = shufflevector <8 x i16> %93, <8 x i16> %91, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %98 = shufflevector <8 x i16> %94, <8 x i16> %95, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %99 = shufflevector <8 x i16> %95, <8 x i16> %94, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %100 = bitcast <8 x i16> %96 to <4 x i32>
  %101 = bitcast <8 x i16> %97 to <4 x i32>
  %102 = shufflevector <4 x i32> %100, <4 x i32> %101, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %103 = bitcast <4 x i32> %102 to <2 x i64>
  %104 = bitcast <8 x i16> %98 to <4 x i32>
  %105 = bitcast <8 x i16> %99 to <4 x i32>
  %106 = shufflevector <4 x i32> %104, <4 x i32> %105, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %107 = bitcast <4 x i32> %106 to <2 x i64>
  %108 = shufflevector <4 x i32> %100, <4 x i32> %101, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %109 = bitcast <4 x i32> %108 to <2 x i64>
  %110 = shufflevector <4 x i32> %104, <4 x i32> %105, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %111 = bitcast <4 x i32> %110 to <2 x i64>
  %112 = shufflevector <2 x i64> %103, <2 x i64> %107, <2 x i32> <i32 0, i32 2>
  %113 = shufflevector <2 x i64> %103, <2 x i64> %107, <2 x i32> <i32 1, i32 3>
  %114 = shufflevector <2 x i64> %109, <2 x i64> %111, <2 x i32> <i32 0, i32 2>
  %115 = shufflevector <2 x i64> %109, <2 x i64> %111, <2 x i32> <i32 1, i32 3>
  %116 = bitcast <2 x i64> %113 to <8 x i16>
  %117 = shufflevector <8 x i16> %116, <8 x i16> <i16 0, i16 0, i16 0, i16 0, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %118 = shufflevector <8 x i16> %116, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 0, i16 0, i16 0, i16 0>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %119 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %117, <8 x i16> <i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069>) #6
  %120 = add <4 x i32> %119, <i32 8192, i32 8192, i32 8192, i32 8192>
  %121 = ashr <4 x i32> %120, <i32 14, i32 14, i32 14, i32 14>
  %122 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %118, <8 x i16> <i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069, i16 3196, i16 -16069>) #6
  %123 = add <4 x i32> %122, <i32 8192, i32 8192, i32 8192, i32 8192>
  %124 = ashr <4 x i32> %123, <i32 14, i32 14, i32 14, i32 14>
  %125 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %121, <4 x i32> %124) #6
  %126 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %117, <8 x i16> <i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196>) #6
  %127 = add <4 x i32> %126, <i32 8192, i32 8192, i32 8192, i32 8192>
  %128 = ashr <4 x i32> %127, <i32 14, i32 14, i32 14, i32 14>
  %129 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %118, <8 x i16> <i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196, i16 16069, i16 3196>) #6
  %130 = add <4 x i32> %129, <i32 8192, i32 8192, i32 8192, i32 8192>
  %131 = ashr <4 x i32> %130, <i32 14, i32 14, i32 14, i32 14>
  %132 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %128, <4 x i32> %131) #6
  %133 = bitcast <2 x i64> %115 to <8 x i16>
  %134 = shufflevector <8 x i16> <i16 0, i16 0, i16 0, i16 0, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i16> %133, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %135 = shufflevector <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 0, i16 0, i16 0, i16 0>, <8 x i16> %133, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %136 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %134, <8 x i16> <i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102>) #6
  %137 = add <4 x i32> %136, <i32 8192, i32 8192, i32 8192, i32 8192>
  %138 = ashr <4 x i32> %137, <i32 14, i32 14, i32 14, i32 14>
  %139 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %135, <8 x i16> <i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102, i16 13623, i16 -9102>) #6
  %140 = add <4 x i32> %139, <i32 8192, i32 8192, i32 8192, i32 8192>
  %141 = ashr <4 x i32> %140, <i32 14, i32 14, i32 14, i32 14>
  %142 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %138, <4 x i32> %141) #6
  %143 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %134, <8 x i16> <i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623>) #6
  %144 = add <4 x i32> %143, <i32 8192, i32 8192, i32 8192, i32 8192>
  %145 = ashr <4 x i32> %144, <i32 14, i32 14, i32 14, i32 14>
  %146 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %135, <8 x i16> <i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623, i16 9102, i16 13623>) #6
  %147 = add <4 x i32> %146, <i32 8192, i32 8192, i32 8192, i32 8192>
  %148 = ashr <4 x i32> %147, <i32 14, i32 14, i32 14, i32 14>
  %149 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %145, <4 x i32> %148) #6
  %150 = bitcast <2 x i64> %112 to <8 x i16>
  %151 = shufflevector <8 x i16> %150, <8 x i16> <i16 0, i16 0, i16 0, i16 0, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %152 = shufflevector <8 x i16> %150, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 0, i16 0, i16 0, i16 0>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %153 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %151, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %154 = add <4 x i32> %153, <i32 8192, i32 8192, i32 8192, i32 8192>
  %155 = ashr <4 x i32> %154, <i32 14, i32 14, i32 14, i32 14>
  %156 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %152, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %157 = add <4 x i32> %156, <i32 8192, i32 8192, i32 8192, i32 8192>
  %158 = ashr <4 x i32> %157, <i32 14, i32 14, i32 14, i32 14>
  %159 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %155, <4 x i32> %158) #6
  %160 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %151, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %161 = add <4 x i32> %160, <i32 8192, i32 8192, i32 8192, i32 8192>
  %162 = ashr <4 x i32> %161, <i32 14, i32 14, i32 14, i32 14>
  %163 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %152, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %164 = add <4 x i32> %163, <i32 8192, i32 8192, i32 8192, i32 8192>
  %165 = ashr <4 x i32> %164, <i32 14, i32 14, i32 14, i32 14>
  %166 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %162, <4 x i32> %165) #6
  %167 = bitcast <2 x i64> %114 to <8 x i16>
  %168 = shufflevector <8 x i16> %167, <8 x i16> <i16 0, i16 0, i16 0, i16 0, i16 undef, i16 undef, i16 undef, i16 undef>, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %169 = shufflevector <8 x i16> %167, <8 x i16> <i16 undef, i16 undef, i16 undef, i16 undef, i16 0, i16 0, i16 0, i16 0>, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %170 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %168, <8 x i16> <i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137>) #6
  %171 = add <4 x i32> %170, <i32 8192, i32 8192, i32 8192, i32 8192>
  %172 = ashr <4 x i32> %171, <i32 14, i32 14, i32 14, i32 14>
  %173 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %169, <8 x i16> <i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137, i16 6270, i16 -15137>) #6
  %174 = add <4 x i32> %173, <i32 8192, i32 8192, i32 8192, i32 8192>
  %175 = ashr <4 x i32> %174, <i32 14, i32 14, i32 14, i32 14>
  %176 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %172, <4 x i32> %175) #6
  %177 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %168, <8 x i16> <i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270>) #6
  %178 = add <4 x i32> %177, <i32 8192, i32 8192, i32 8192, i32 8192>
  %179 = ashr <4 x i32> %178, <i32 14, i32 14, i32 14, i32 14>
  %180 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %169, <8 x i16> <i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270, i16 15137, i16 6270>) #6
  %181 = add <4 x i32> %180, <i32 8192, i32 8192, i32 8192, i32 8192>
  %182 = ashr <4 x i32> %181, <i32 14, i32 14, i32 14, i32 14>
  %183 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %179, <4 x i32> %182) #6
  %184 = add <8 x i16> %142, %125
  %185 = sub <8 x i16> %125, %142
  %186 = sub <8 x i16> %132, %149
  %187 = add <8 x i16> %149, %132
  %188 = add <8 x i16> %183, %166
  %189 = add <8 x i16> %176, %159
  %190 = sub <8 x i16> %159, %176
  %191 = sub <8 x i16> %166, %183
  %192 = shufflevector <8 x i16> %186, <8 x i16> %185, <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
  %193 = shufflevector <8 x i16> %186, <8 x i16> %185, <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  %194 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %192, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %195 = add <4 x i32> %194, <i32 8192, i32 8192, i32 8192, i32 8192>
  %196 = ashr <4 x i32> %195, <i32 14, i32 14, i32 14, i32 14>
  %197 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %193, <8 x i16> <i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585, i16 11585, i16 -11585>) #6
  %198 = add <4 x i32> %197, <i32 8192, i32 8192, i32 8192, i32 8192>
  %199 = ashr <4 x i32> %198, <i32 14, i32 14, i32 14, i32 14>
  %200 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %196, <4 x i32> %199) #6
  %201 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %192, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %202 = add <4 x i32> %201, <i32 8192, i32 8192, i32 8192, i32 8192>
  %203 = ashr <4 x i32> %202, <i32 14, i32 14, i32 14, i32 14>
  %204 = tail call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %193, <8 x i16> <i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585, i16 11585>) #6
  %205 = add <4 x i32> %204, <i32 8192, i32 8192, i32 8192, i32 8192>
  %206 = ashr <4 x i32> %205, <i32 14, i32 14, i32 14, i32 14>
  %207 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %203, <4 x i32> %206) #6
  %208 = sub <8 x i16> %191, %184
  %209 = sub <8 x i16> %190, %200
  %210 = sub <8 x i16> %189, %207
  %211 = add <8 x i16> %187, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %212 = add <8 x i16> %211, %188
  %213 = bitcast [16 x <2 x i64>]* %5 to <8 x i16>*
  %214 = add <8 x i16> %189, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %215 = add <8 x i16> %214, %207
  %216 = bitcast <2 x i64>* %15 to <8 x i16>*
  %217 = add <8 x i16> %190, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %218 = add <8 x i16> %217, %200
  %219 = bitcast <2 x i64>* %19 to <8 x i16>*
  %220 = add <8 x i16> %184, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %221 = add <8 x i16> %220, %191
  %222 = bitcast <2 x i64>* %23 to <8 x i16>*
  %223 = add <8 x i16> %208, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %224 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 4
  %225 = bitcast <2 x i64>* %224 to <8 x i16>*
  %226 = add <8 x i16> %209, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %227 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 5
  %228 = bitcast <2 x i64>* %227 to <8 x i16>*
  %229 = add <8 x i16> %210, <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>
  %230 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 6
  %231 = bitcast <2 x i64>* %230 to <8 x i16>*
  %232 = sub <8 x i16> <i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16, i16 16>, %187
  %233 = add <8 x i16> %232, %188
  %234 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 7
  %235 = bitcast <2 x i64>* %234 to <8 x i16>*
  %236 = ashr <8 x i16> %212, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %236, <8 x i16>* %213, align 16
  %237 = ashr <8 x i16> %215, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %237, <8 x i16>* %216, align 16
  %238 = ashr <8 x i16> %218, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %238, <8 x i16>* %219, align 16
  %239 = ashr <8 x i16> %221, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %239, <8 x i16>* %222, align 16
  %240 = ashr <8 x i16> %223, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %240, <8 x i16>* %225, align 16
  %241 = ashr <8 x i16> %226, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %241, <8 x i16>* %228, align 16
  %242 = ashr <8 x i16> %229, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %242, <8 x i16>* %231, align 16
  %243 = ashr <8 x i16> %233, <i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5, i16 5>
  store <8 x i16> %243, <8 x i16>* %235, align 16
  br label %341

244:                                              ; preds = %4
  call fastcc void @highbd_idct8x8_12_half1d(<2 x i64>* nonnull %11)
  %245 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 4
  %246 = load <2 x i64>, <2 x i64>* %245, align 16
  %247 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 5
  %248 = load <2 x i64>, <2 x i64>* %247, align 16
  %249 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 6
  %250 = load <2 x i64>, <2 x i64>* %249, align 16
  %251 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 7
  %252 = load <2 x i64>, <2 x i64>* %251, align 16
  call fastcc void @highbd_idct8x8_12_half1d(<2 x i64>* nonnull %11)
  %253 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 8
  store <2 x i64> %246, <2 x i64>* %253, align 16
  %254 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 9
  store <2 x i64> %248, <2 x i64>* %254, align 16
  %255 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 10
  store <2 x i64> %250, <2 x i64>* %255, align 16
  %256 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 11
  store <2 x i64> %252, <2 x i64>* %256, align 16
  call fastcc void @highbd_idct8x8_12_half1d(<2 x i64>* %253)
  %257 = bitcast [16 x <2 x i64>]* %5 to <4 x i32>*
  %258 = load <4 x i32>, <4 x i32>* %257, align 16
  %259 = bitcast <2 x i64>* %253 to <4 x i32>*
  %260 = load <4 x i32>, <4 x i32>* %259, align 16
  %261 = add <4 x i32> %258, <i32 16, i32 16, i32 16, i32 16>
  %262 = add <4 x i32> %260, <i32 16, i32 16, i32 16, i32 16>
  %263 = ashr <4 x i32> %261, <i32 5, i32 5, i32 5, i32 5>
  %264 = ashr <4 x i32> %262, <i32 5, i32 5, i32 5, i32 5>
  %265 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %263, <4 x i32> %264) #6
  %266 = bitcast [16 x <2 x i64>]* %5 to <8 x i16>*
  store <8 x i16> %265, <8 x i16>* %266, align 16
  %267 = bitcast <2 x i64>* %15 to <4 x i32>*
  %268 = load <4 x i32>, <4 x i32>* %267, align 16
  %269 = bitcast <2 x i64>* %254 to <4 x i32>*
  %270 = load <4 x i32>, <4 x i32>* %269, align 16
  %271 = add <4 x i32> %268, <i32 16, i32 16, i32 16, i32 16>
  %272 = add <4 x i32> %270, <i32 16, i32 16, i32 16, i32 16>
  %273 = ashr <4 x i32> %271, <i32 5, i32 5, i32 5, i32 5>
  %274 = ashr <4 x i32> %272, <i32 5, i32 5, i32 5, i32 5>
  %275 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %273, <4 x i32> %274) #6
  %276 = bitcast <2 x i64>* %15 to <8 x i16>*
  store <8 x i16> %275, <8 x i16>* %276, align 16
  %277 = bitcast <2 x i64>* %19 to <4 x i32>*
  %278 = load <4 x i32>, <4 x i32>* %277, align 16
  %279 = bitcast <2 x i64>* %255 to <4 x i32>*
  %280 = load <4 x i32>, <4 x i32>* %279, align 16
  %281 = add <4 x i32> %278, <i32 16, i32 16, i32 16, i32 16>
  %282 = add <4 x i32> %280, <i32 16, i32 16, i32 16, i32 16>
  %283 = ashr <4 x i32> %281, <i32 5, i32 5, i32 5, i32 5>
  %284 = ashr <4 x i32> %282, <i32 5, i32 5, i32 5, i32 5>
  %285 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %283, <4 x i32> %284) #6
  %286 = bitcast <2 x i64>* %19 to <8 x i16>*
  store <8 x i16> %285, <8 x i16>* %286, align 16
  %287 = bitcast <2 x i64>* %23 to <4 x i32>*
  %288 = load <4 x i32>, <4 x i32>* %287, align 16
  %289 = bitcast <2 x i64>* %256 to <4 x i32>*
  %290 = load <4 x i32>, <4 x i32>* %289, align 16
  %291 = add <4 x i32> %288, <i32 16, i32 16, i32 16, i32 16>
  %292 = add <4 x i32> %290, <i32 16, i32 16, i32 16, i32 16>
  %293 = ashr <4 x i32> %291, <i32 5, i32 5, i32 5, i32 5>
  %294 = ashr <4 x i32> %292, <i32 5, i32 5, i32 5, i32 5>
  %295 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %293, <4 x i32> %294) #6
  %296 = bitcast <2 x i64>* %23 to <8 x i16>*
  store <8 x i16> %295, <8 x i16>* %296, align 16
  %297 = bitcast <2 x i64>* %245 to <4 x i32>*
  %298 = load <4 x i32>, <4 x i32>* %297, align 16
  %299 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 12
  %300 = bitcast <2 x i64>* %299 to <4 x i32>*
  %301 = load <4 x i32>, <4 x i32>* %300, align 16
  %302 = add <4 x i32> %298, <i32 16, i32 16, i32 16, i32 16>
  %303 = add <4 x i32> %301, <i32 16, i32 16, i32 16, i32 16>
  %304 = ashr <4 x i32> %302, <i32 5, i32 5, i32 5, i32 5>
  %305 = ashr <4 x i32> %303, <i32 5, i32 5, i32 5, i32 5>
  %306 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %304, <4 x i32> %305) #6
  %307 = bitcast <2 x i64>* %245 to <8 x i16>*
  store <8 x i16> %306, <8 x i16>* %307, align 16
  %308 = bitcast <2 x i64>* %247 to <4 x i32>*
  %309 = load <4 x i32>, <4 x i32>* %308, align 16
  %310 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 13
  %311 = bitcast <2 x i64>* %310 to <4 x i32>*
  %312 = load <4 x i32>, <4 x i32>* %311, align 16
  %313 = add <4 x i32> %309, <i32 16, i32 16, i32 16, i32 16>
  %314 = add <4 x i32> %312, <i32 16, i32 16, i32 16, i32 16>
  %315 = ashr <4 x i32> %313, <i32 5, i32 5, i32 5, i32 5>
  %316 = ashr <4 x i32> %314, <i32 5, i32 5, i32 5, i32 5>
  %317 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %315, <4 x i32> %316) #6
  %318 = bitcast <2 x i64>* %247 to <8 x i16>*
  store <8 x i16> %317, <8 x i16>* %318, align 16
  %319 = bitcast <2 x i64>* %249 to <4 x i32>*
  %320 = load <4 x i32>, <4 x i32>* %319, align 16
  %321 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 14
  %322 = bitcast <2 x i64>* %321 to <4 x i32>*
  %323 = load <4 x i32>, <4 x i32>* %322, align 16
  %324 = add <4 x i32> %320, <i32 16, i32 16, i32 16, i32 16>
  %325 = add <4 x i32> %323, <i32 16, i32 16, i32 16, i32 16>
  %326 = ashr <4 x i32> %324, <i32 5, i32 5, i32 5, i32 5>
  %327 = ashr <4 x i32> %325, <i32 5, i32 5, i32 5, i32 5>
  %328 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %326, <4 x i32> %327) #6
  %329 = bitcast <2 x i64>* %249 to <8 x i16>*
  store <8 x i16> %328, <8 x i16>* %329, align 16
  %330 = bitcast <2 x i64>* %251 to <4 x i32>*
  %331 = load <4 x i32>, <4 x i32>* %330, align 16
  %332 = getelementptr inbounds [16 x <2 x i64>], [16 x <2 x i64>]* %5, i64 0, i64 15
  %333 = bitcast <2 x i64>* %332 to <4 x i32>*
  %334 = load <4 x i32>, <4 x i32>* %333, align 16
  %335 = add <4 x i32> %331, <i32 16, i32 16, i32 16, i32 16>
  %336 = add <4 x i32> %334, <i32 16, i32 16, i32 16, i32 16>
  %337 = ashr <4 x i32> %335, <i32 5, i32 5, i32 5, i32 5>
  %338 = ashr <4 x i32> %336, <i32 5, i32 5, i32 5, i32 5>
  %339 = tail call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %337, <4 x i32> %338) #6
  %340 = bitcast <2 x i64>* %251 to <8 x i16>*
  store <8 x i16> %339, <8 x i16>* %340, align 16
  br label %341

341:                                              ; preds = %244, %25
  %342 = phi <8 x i16> [ %339, %244 ], [ %243, %25 ]
  %343 = phi <8 x i16> [ %328, %244 ], [ %242, %25 ]
  %344 = phi <8 x i16> [ %317, %244 ], [ %241, %25 ]
  %345 = phi <8 x i16> [ %306, %244 ], [ %240, %25 ]
  %346 = phi <8 x i16> [ %295, %244 ], [ %239, %25 ]
  %347 = phi <8 x i16> [ %285, %244 ], [ %238, %25 ]
  %348 = phi <8 x i16> [ %275, %244 ], [ %237, %25 ]
  %349 = phi <8 x i16> [ %265, %244 ], [ %236, %25 ]
  %350 = bitcast i16* %1 to <8 x i16>*
  %351 = load <8 x i16>, <8 x i16>* %350, align 16
  %352 = tail call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>, i32 %3) #6
  %353 = add <8 x i16> %352, <i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1>
  %354 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %351, <8 x i16> %349) #6
  %355 = icmp sgt <8 x i16> %354, zeroinitializer
  %356 = select <8 x i1> %355, <8 x i16> %354, <8 x i16> zeroinitializer
  %357 = icmp slt <8 x i16> %356, %353
  %358 = select <8 x i1> %357, <8 x i16> %356, <8 x i16> %353
  store <8 x i16> %358, <8 x i16>* %350, align 16
  %359 = sext i32 %2 to i64
  %360 = getelementptr inbounds i16, i16* %1, i64 %359
  %361 = bitcast i16* %360 to <8 x i16>*
  %362 = load <8 x i16>, <8 x i16>* %361, align 16
  %363 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %362, <8 x i16> %348) #6
  %364 = icmp sgt <8 x i16> %363, zeroinitializer
  %365 = select <8 x i1> %364, <8 x i16> %363, <8 x i16> zeroinitializer
  %366 = icmp slt <8 x i16> %365, %353
  %367 = select <8 x i1> %366, <8 x i16> %365, <8 x i16> %353
  store <8 x i16> %367, <8 x i16>* %361, align 16
  %368 = getelementptr inbounds i16, i16* %360, i64 %359
  %369 = bitcast i16* %368 to <8 x i16>*
  %370 = load <8 x i16>, <8 x i16>* %369, align 16
  %371 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %370, <8 x i16> %347) #6
  %372 = icmp sgt <8 x i16> %371, zeroinitializer
  %373 = select <8 x i1> %372, <8 x i16> %371, <8 x i16> zeroinitializer
  %374 = icmp slt <8 x i16> %373, %353
  %375 = select <8 x i1> %374, <8 x i16> %373, <8 x i16> %353
  store <8 x i16> %375, <8 x i16>* %369, align 16
  %376 = getelementptr inbounds i16, i16* %368, i64 %359
  %377 = bitcast i16* %376 to <8 x i16>*
  %378 = load <8 x i16>, <8 x i16>* %377, align 16
  %379 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %378, <8 x i16> %346) #6
  %380 = icmp sgt <8 x i16> %379, zeroinitializer
  %381 = select <8 x i1> %380, <8 x i16> %379, <8 x i16> zeroinitializer
  %382 = icmp slt <8 x i16> %381, %353
  %383 = select <8 x i1> %382, <8 x i16> %381, <8 x i16> %353
  store <8 x i16> %383, <8 x i16>* %377, align 16
  %384 = getelementptr inbounds i16, i16* %376, i64 %359
  %385 = bitcast i16* %384 to <8 x i16>*
  %386 = load <8 x i16>, <8 x i16>* %385, align 16
  %387 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %386, <8 x i16> %345) #6
  %388 = icmp sgt <8 x i16> %387, zeroinitializer
  %389 = select <8 x i1> %388, <8 x i16> %387, <8 x i16> zeroinitializer
  %390 = icmp slt <8 x i16> %389, %353
  %391 = select <8 x i1> %390, <8 x i16> %389, <8 x i16> %353
  store <8 x i16> %391, <8 x i16>* %385, align 16
  %392 = getelementptr inbounds i16, i16* %384, i64 %359
  %393 = bitcast i16* %392 to <8 x i16>*
  %394 = load <8 x i16>, <8 x i16>* %393, align 16
  %395 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %394, <8 x i16> %344) #6
  %396 = icmp sgt <8 x i16> %395, zeroinitializer
  %397 = select <8 x i1> %396, <8 x i16> %395, <8 x i16> zeroinitializer
  %398 = icmp slt <8 x i16> %397, %353
  %399 = select <8 x i1> %398, <8 x i16> %397, <8 x i16> %353
  store <8 x i16> %399, <8 x i16>* %393, align 16
  %400 = getelementptr inbounds i16, i16* %392, i64 %359
  %401 = bitcast i16* %400 to <8 x i16>*
  %402 = load <8 x i16>, <8 x i16>* %401, align 16
  %403 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %402, <8 x i16> %343) #6
  %404 = icmp sgt <8 x i16> %403, zeroinitializer
  %405 = select <8 x i1> %404, <8 x i16> %403, <8 x i16> zeroinitializer
  %406 = icmp slt <8 x i16> %405, %353
  %407 = select <8 x i1> %406, <8 x i16> %405, <8 x i16> %353
  store <8 x i16> %407, <8 x i16>* %401, align 16
  %408 = getelementptr inbounds i16, i16* %400, i64 %359
  %409 = bitcast i16* %408 to <8 x i16>*
  %410 = load <8 x i16>, <8 x i16>* %409, align 16
  %411 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %410, <8 x i16> %342) #6
  %412 = icmp sgt <8 x i16> %411, zeroinitializer
  %413 = select <8 x i1> %412, <8 x i16> %411, <8 x i16> zeroinitializer
  %414 = icmp slt <8 x i16> %413, %353
  %415 = select <8 x i1> %414, <8 x i16> %413, <8 x i16> %353
  store <8 x i16> %415, <8 x i16>* %409, align 16
  call void @llvm.lifetime.end.p0i8(i64 256, i8* nonnull %6) #6
  ret void
}

; Function Attrs: nounwind ssp uwtable
define internal fastcc void @highbd_idct8x8_12_half1d(<2 x i64>* nocapture) unnamed_addr #0 {
  %2 = bitcast <2 x i64>* %0 to <4 x i32>*
  %3 = load <4 x i32>, <4 x i32>* %2, align 16
  %4 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 1
  %5 = bitcast <2 x i64>* %4 to <4 x i32>*
  %6 = load <4 x i32>, <4 x i32>* %5, align 16
  %7 = shufflevector <4 x i32> %3, <4 x i32> %6, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %8 = bitcast <4 x i32> %7 to <2 x i64>
  %9 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 2
  %10 = bitcast <2 x i64>* %9 to <4 x i32>*
  %11 = load <4 x i32>, <4 x i32>* %10, align 16
  %12 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 3
  %13 = bitcast <2 x i64>* %12 to <4 x i32>*
  %14 = load <4 x i32>, <4 x i32>* %13, align 16
  %15 = shufflevector <4 x i32> %11, <4 x i32> %14, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %16 = bitcast <4 x i32> %15 to <2 x i64>
  %17 = shufflevector <4 x i32> %3, <4 x i32> %6, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %18 = bitcast <4 x i32> %17 to <2 x i64>
  %19 = shufflevector <4 x i32> %11, <4 x i32> %14, <4 x i32> <i32 2, i32 6, i32 3, i32 7>
  %20 = bitcast <4 x i32> %19 to <2 x i64>
  %21 = shufflevector <2 x i64> %8, <2 x i64> %16, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %21, <2 x i64>* %0, align 16
  %22 = shufflevector <2 x i64> %8, <2 x i64> %16, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %22, <2 x i64>* %4, align 16
  %23 = shufflevector <2 x i64> %18, <2 x i64> %20, <2 x i32> <i32 0, i32 2>
  store <2 x i64> %23, <2 x i64>* %9, align 16
  %24 = shufflevector <2 x i64> %18, <2 x i64> %20, <2 x i32> <i32 1, i32 3>
  store <2 x i64> %24, <2 x i64>* %12, align 16
  %25 = bitcast <2 x i64> %22 to <4 x i32>
  %26 = ashr <4 x i32> %25, <i32 31, i32 31, i32 31, i32 31>
  %27 = xor <4 x i32> %26, %25
  %28 = sub <4 x i32> %27, %26
  %29 = shufflevector <4 x i32> %26, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %30 = bitcast <4 x i32> %29 to <2 x i64>
  %31 = shufflevector <4 x i32> %26, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %32 = bitcast <4 x i32> %31 to <2 x i64>
  %33 = shufflevector <4 x i32> %28, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %34 = bitcast <4 x i32> %33 to <2 x i64>
  %35 = shufflevector <4 x i32> %28, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %36 = bitcast <4 x i32> %35 to <2 x i64>
  %37 = and <2 x i64> %36, <i64 4294967295, i64 4294967295>
  %38 = mul nuw nsw <2 x i64> %37, <i64 12784, i64 12784>
  %39 = xor <2 x i64> %38, %32
  %40 = and <2 x i64> %34, <i64 4294967295, i64 4294967295>
  %41 = mul nuw nsw <2 x i64> %40, <i64 12784, i64 12784>
  %42 = xor <2 x i64> %41, %30
  %43 = sub <2 x i64> <i64 32768, i64 32768>, %32
  %44 = add <2 x i64> %39, %43
  %45 = bitcast <2 x i64> %44 to <16 x i8>
  %46 = shufflevector <16 x i8> %45, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %47 = sub <2 x i64> <i64 32768, i64 32768>, %30
  %48 = add <2 x i64> %42, %47
  %49 = bitcast <2 x i64> %48 to <16 x i8>
  %50 = shufflevector <16 x i8> %49, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %51 = bitcast <16 x i8> %46 to <4 x i32>
  %52 = bitcast <16 x i8> %50 to <4 x i32>
  %53 = shufflevector <4 x i32> %51, <4 x i32> %52, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %54 = shufflevector <4 x i32> %51, <4 x i32> %52, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %55 = shufflevector <4 x i32> %53, <4 x i32> %54, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %56 = mul nuw nsw <2 x i64> %37, <i64 64276, i64 64276>
  %57 = xor <2 x i64> %56, %32
  %58 = mul nuw nsw <2 x i64> %40, <i64 64276, i64 64276>
  %59 = xor <2 x i64> %58, %30
  %60 = add <2 x i64> %57, %43
  %61 = bitcast <2 x i64> %60 to <16 x i8>
  %62 = shufflevector <16 x i8> %61, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %63 = add <2 x i64> %59, %47
  %64 = bitcast <2 x i64> %63 to <16 x i8>
  %65 = shufflevector <16 x i8> %64, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %66 = bitcast <16 x i8> %62 to <4 x i32>
  %67 = bitcast <16 x i8> %65 to <4 x i32>
  %68 = shufflevector <4 x i32> %66, <4 x i32> %67, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %69 = shufflevector <4 x i32> %66, <4 x i32> %67, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %70 = shufflevector <4 x i32> %68, <4 x i32> %69, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %71 = bitcast <2 x i64> %24 to <4 x i32>
  %72 = ashr <4 x i32> %71, <i32 31, i32 31, i32 31, i32 31>
  %73 = xor <4 x i32> %72, %71
  %74 = sub <4 x i32> %73, %72
  %75 = shufflevector <4 x i32> %72, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %76 = bitcast <4 x i32> %75 to <2 x i64>
  %77 = shufflevector <4 x i32> %72, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %78 = bitcast <4 x i32> %77 to <2 x i64>
  %79 = shufflevector <4 x i32> %74, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %80 = bitcast <4 x i32> %79 to <2 x i64>
  %81 = shufflevector <4 x i32> %74, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %82 = bitcast <4 x i32> %81 to <2 x i64>
  %83 = and <2 x i64> %82, <i64 4294967295, i64 4294967295>
  %84 = mul nuw nsw <2 x i64> %83, <i64 36408, i64 36408>
  %85 = xor <2 x i64> %84, %78
  %86 = and <2 x i64> %80, <i64 4294967295, i64 4294967295>
  %87 = mul nuw nsw <2 x i64> %86, <i64 36408, i64 36408>
  %88 = xor <2 x i64> %87, %76
  %89 = add <2 x i64> %78, <i64 32768, i64 32768>
  %90 = sub <2 x i64> %89, %85
  %91 = bitcast <2 x i64> %90 to <16 x i8>
  %92 = shufflevector <16 x i8> %91, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %93 = add <2 x i64> %76, <i64 32768, i64 32768>
  %94 = sub <2 x i64> %93, %88
  %95 = bitcast <2 x i64> %94 to <16 x i8>
  %96 = shufflevector <16 x i8> %95, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %97 = bitcast <16 x i8> %92 to <4 x i32>
  %98 = bitcast <16 x i8> %96 to <4 x i32>
  %99 = shufflevector <4 x i32> %97, <4 x i32> %98, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %100 = shufflevector <4 x i32> %97, <4 x i32> %98, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %101 = shufflevector <4 x i32> %99, <4 x i32> %100, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %102 = mul nuw nsw <2 x i64> %83, <i64 54492, i64 54492>
  %103 = xor <2 x i64> %102, %78
  %104 = mul nuw nsw <2 x i64> %86, <i64 54492, i64 54492>
  %105 = xor <2 x i64> %104, %76
  %106 = sub <2 x i64> <i64 32768, i64 32768>, %78
  %107 = add <2 x i64> %106, %103
  %108 = bitcast <2 x i64> %107 to <16 x i8>
  %109 = shufflevector <16 x i8> %108, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %110 = sub <2 x i64> <i64 32768, i64 32768>, %76
  %111 = add <2 x i64> %110, %105
  %112 = bitcast <2 x i64> %111 to <16 x i8>
  %113 = shufflevector <16 x i8> %112, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %114 = bitcast <16 x i8> %109 to <4 x i32>
  %115 = bitcast <16 x i8> %113 to <4 x i32>
  %116 = shufflevector <4 x i32> %114, <4 x i32> %115, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %117 = shufflevector <4 x i32> %114, <4 x i32> %115, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %118 = shufflevector <4 x i32> %116, <4 x i32> %117, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %119 = bitcast <2 x i64> %21 to <4 x i32>
  %120 = ashr <4 x i32> %119, <i32 31, i32 31, i32 31, i32 31>
  %121 = xor <4 x i32> %120, %119
  %122 = sub <4 x i32> %121, %120
  %123 = shufflevector <4 x i32> %120, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %124 = bitcast <4 x i32> %123 to <2 x i64>
  %125 = shufflevector <4 x i32> %120, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %126 = bitcast <4 x i32> %125 to <2 x i64>
  %127 = shufflevector <4 x i32> %122, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %128 = bitcast <4 x i32> %127 to <2 x i64>
  %129 = shufflevector <4 x i32> %122, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %130 = bitcast <4 x i32> %129 to <2 x i64>
  %131 = and <2 x i64> %130, <i64 4294967295, i64 4294967295>
  %132 = mul nuw nsw <2 x i64> %131, <i64 46340, i64 46340>
  %133 = xor <2 x i64> %132, %126
  %134 = and <2 x i64> %128, <i64 4294967295, i64 4294967295>
  %135 = mul nuw nsw <2 x i64> %134, <i64 46340, i64 46340>
  %136 = xor <2 x i64> %135, %124
  %137 = sub <2 x i64> <i64 32768, i64 32768>, %126
  %138 = add <2 x i64> %137, %133
  %139 = bitcast <2 x i64> %138 to <16 x i8>
  %140 = shufflevector <16 x i8> %139, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %141 = sub <2 x i64> <i64 32768, i64 32768>, %124
  %142 = add <2 x i64> %141, %136
  %143 = bitcast <2 x i64> %142 to <16 x i8>
  %144 = shufflevector <16 x i8> %143, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %145 = bitcast <16 x i8> %140 to <4 x i32>
  %146 = bitcast <16 x i8> %144 to <4 x i32>
  %147 = shufflevector <4 x i32> %145, <4 x i32> %146, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %148 = shufflevector <4 x i32> %145, <4 x i32> %146, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %149 = shufflevector <4 x i32> %147, <4 x i32> %148, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %150 = bitcast <2 x i64> %23 to <4 x i32>
  %151 = ashr <4 x i32> %150, <i32 31, i32 31, i32 31, i32 31>
  %152 = xor <4 x i32> %151, %150
  %153 = sub <4 x i32> %152, %151
  %154 = shufflevector <4 x i32> %151, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %155 = bitcast <4 x i32> %154 to <2 x i64>
  %156 = shufflevector <4 x i32> %151, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %157 = bitcast <4 x i32> %156 to <2 x i64>
  %158 = shufflevector <4 x i32> %153, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %159 = bitcast <4 x i32> %158 to <2 x i64>
  %160 = shufflevector <4 x i32> %153, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %161 = bitcast <4 x i32> %160 to <2 x i64>
  %162 = and <2 x i64> %161, <i64 4294967295, i64 4294967295>
  %163 = mul nuw nsw <2 x i64> %162, <i64 25080, i64 25080>
  %164 = xor <2 x i64> %163, %157
  %165 = and <2 x i64> %159, <i64 4294967295, i64 4294967295>
  %166 = mul nuw nsw <2 x i64> %165, <i64 25080, i64 25080>
  %167 = xor <2 x i64> %166, %155
  %168 = sub <2 x i64> <i64 32768, i64 32768>, %157
  %169 = add <2 x i64> %164, %168
  %170 = bitcast <2 x i64> %169 to <16 x i8>
  %171 = shufflevector <16 x i8> %170, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %172 = sub <2 x i64> <i64 32768, i64 32768>, %155
  %173 = add <2 x i64> %167, %172
  %174 = bitcast <2 x i64> %173 to <16 x i8>
  %175 = shufflevector <16 x i8> %174, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %176 = bitcast <16 x i8> %171 to <4 x i32>
  %177 = bitcast <16 x i8> %175 to <4 x i32>
  %178 = shufflevector <4 x i32> %176, <4 x i32> %177, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %179 = shufflevector <4 x i32> %176, <4 x i32> %177, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %180 = shufflevector <4 x i32> %178, <4 x i32> %179, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %181 = mul nuw nsw <2 x i64> %162, <i64 60548, i64 60548>
  %182 = xor <2 x i64> %181, %157
  %183 = mul nuw nsw <2 x i64> %165, <i64 60548, i64 60548>
  %184 = xor <2 x i64> %183, %155
  %185 = add <2 x i64> %182, %168
  %186 = bitcast <2 x i64> %185 to <16 x i8>
  %187 = shufflevector <16 x i8> %186, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %188 = add <2 x i64> %184, %172
  %189 = bitcast <2 x i64> %188 to <16 x i8>
  %190 = shufflevector <16 x i8> %189, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %191 = bitcast <16 x i8> %187 to <4 x i32>
  %192 = bitcast <16 x i8> %190 to <4 x i32>
  %193 = shufflevector <4 x i32> %191, <4 x i32> %192, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %194 = shufflevector <4 x i32> %191, <4 x i32> %192, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %195 = shufflevector <4 x i32> %193, <4 x i32> %194, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %196 = add <4 x i32> %101, %55
  %197 = sub <4 x i32> %55, %101
  %198 = sub <4 x i32> %70, %118
  %199 = add <4 x i32> %118, %70
  %200 = add <4 x i32> %195, %149
  %201 = add <4 x i32> %180, %149
  %202 = sub <4 x i32> %149, %180
  %203 = sub <4 x i32> %149, %195
  %204 = add <4 x i32> %197, %198
  %205 = ashr <4 x i32> %204, <i32 31, i32 31, i32 31, i32 31>
  %206 = xor <4 x i32> %205, %204
  %207 = sub <4 x i32> %206, %205
  %208 = shufflevector <4 x i32> %205, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %209 = bitcast <4 x i32> %208 to <2 x i64>
  %210 = shufflevector <4 x i32> %205, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %211 = bitcast <4 x i32> %210 to <2 x i64>
  %212 = shufflevector <4 x i32> %207, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %213 = bitcast <4 x i32> %212 to <2 x i64>
  %214 = shufflevector <4 x i32> %207, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %215 = bitcast <4 x i32> %214 to <2 x i64>
  %216 = and <2 x i64> %215, <i64 4294967295, i64 4294967295>
  %217 = mul nuw nsw <2 x i64> %216, <i64 46340, i64 46340>
  %218 = xor <2 x i64> %217, %211
  %219 = and <2 x i64> %213, <i64 4294967295, i64 4294967295>
  %220 = mul nuw nsw <2 x i64> %219, <i64 46340, i64 46340>
  %221 = xor <2 x i64> %220, %209
  %222 = sub <2 x i64> <i64 32768, i64 32768>, %211
  %223 = add <2 x i64> %222, %218
  %224 = bitcast <2 x i64> %223 to <16 x i8>
  %225 = shufflevector <16 x i8> %224, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %226 = sub <2 x i64> <i64 32768, i64 32768>, %209
  %227 = add <2 x i64> %226, %221
  %228 = bitcast <2 x i64> %227 to <16 x i8>
  %229 = shufflevector <16 x i8> %228, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %230 = bitcast <16 x i8> %225 to <4 x i32>
  %231 = bitcast <16 x i8> %229 to <4 x i32>
  %232 = shufflevector <4 x i32> %230, <4 x i32> %231, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %233 = shufflevector <4 x i32> %230, <4 x i32> %231, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %234 = shufflevector <4 x i32> %232, <4 x i32> %233, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %235 = sub <4 x i32> %198, %197
  %236 = ashr <4 x i32> %235, <i32 31, i32 31, i32 31, i32 31>
  %237 = xor <4 x i32> %236, %235
  %238 = sub <4 x i32> %237, %236
  %239 = shufflevector <4 x i32> %236, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %240 = bitcast <4 x i32> %239 to <2 x i64>
  %241 = shufflevector <4 x i32> %236, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %242 = bitcast <4 x i32> %241 to <2 x i64>
  %243 = shufflevector <4 x i32> %238, <4 x i32> undef, <4 x i32> <i32 2, i32 2, i32 3, i32 3>
  %244 = bitcast <4 x i32> %243 to <2 x i64>
  %245 = shufflevector <4 x i32> %238, <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 1, i32 1>
  %246 = bitcast <4 x i32> %245 to <2 x i64>
  %247 = and <2 x i64> %246, <i64 4294967295, i64 4294967295>
  %248 = mul nuw nsw <2 x i64> %247, <i64 46340, i64 46340>
  %249 = xor <2 x i64> %248, %242
  %250 = and <2 x i64> %244, <i64 4294967295, i64 4294967295>
  %251 = mul nuw nsw <2 x i64> %250, <i64 46340, i64 46340>
  %252 = xor <2 x i64> %251, %240
  %253 = sub <2 x i64> <i64 32768, i64 32768>, %242
  %254 = add <2 x i64> %253, %249
  %255 = bitcast <2 x i64> %254 to <16 x i8>
  %256 = shufflevector <16 x i8> %255, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %257 = sub <2 x i64> <i64 32768, i64 32768>, %240
  %258 = add <2 x i64> %257, %252
  %259 = bitcast <2 x i64> %258 to <16 x i8>
  %260 = shufflevector <16 x i8> %259, <16 x i8> <i8 0, i8 0, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef, i8 undef>, <16 x i32> <i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17>
  %261 = bitcast <16 x i8> %256 to <4 x i32>
  %262 = bitcast <16 x i8> %260 to <4 x i32>
  %263 = shufflevector <4 x i32> %261, <4 x i32> %262, <4 x i32> <i32 0, i32 4, i32 undef, i32 undef>
  %264 = shufflevector <4 x i32> %261, <4 x i32> %262, <4 x i32> <i32 2, i32 6, i32 undef, i32 undef>
  %265 = shufflevector <4 x i32> %263, <4 x i32> %264, <4 x i32> <i32 0, i32 4, i32 1, i32 5>
  %266 = add <4 x i32> %199, %200
  store <4 x i32> %266, <4 x i32>* %2, align 16
  %267 = add <4 x i32> %234, %201
  store <4 x i32> %267, <4 x i32>* %5, align 16
  %268 = add <4 x i32> %265, %202
  store <4 x i32> %268, <4 x i32>* %10, align 16
  %269 = add <4 x i32> %196, %203
  store <4 x i32> %269, <4 x i32>* %13, align 16
  %270 = sub <4 x i32> %203, %196
  %271 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 4
  %272 = bitcast <2 x i64>* %271 to <4 x i32>*
  store <4 x i32> %270, <4 x i32>* %272, align 16
  %273 = sub <4 x i32> %202, %265
  %274 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 5
  %275 = bitcast <2 x i64>* %274 to <4 x i32>*
  store <4 x i32> %273, <4 x i32>* %275, align 16
  %276 = sub <4 x i32> %201, %234
  %277 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 6
  %278 = bitcast <2 x i64>* %277 to <4 x i32>*
  store <4 x i32> %276, <4 x i32>* %278, align 16
  %279 = sub <4 x i32> %200, %199
  %280 = getelementptr inbounds <2 x i64>, <2 x i64>* %0, i64 7
  %281 = bitcast <2 x i64>* %280 to <4 x i32>*
  store <4 x i32> %279, <4 x i32>* %281, align 16
  ret void
}

; Function Attrs: nofree nounwind ssp uwtable
define hidden void @vpx_highbd_idct8x8_1_add_sse2(i32* nocapture readonly, i16* nocapture, i32, i32) local_unnamed_addr #3 {
  %5 = load i32, i32* %0, align 4
  %6 = sext i32 %5 to i64
  %7 = mul nsw i64 %6, 11585
  %8 = add nsw i64 %7, 8192
  %9 = lshr i64 %8, 14
  %10 = shl i64 %9, 32
  %11 = ashr exact i64 %10, 32
  %12 = mul nsw i64 %11, 11585
  %13 = add nsw i64 %12, 8192
  %14 = lshr i64 %13, 14
  %15 = trunc i64 %14 to i32
  %16 = add nsw i32 %15, 16
  %17 = lshr i32 %16, 5
  %18 = trunc i32 %17 to i16
  %19 = insertelement <8 x i16> undef, i16 %18, i32 0
  %20 = shufflevector <8 x i16> %19, <8 x i16> undef, <8 x i32> zeroinitializer
  %21 = tail call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>, i32 %3) #6
  %22 = add <8 x i16> %21, <i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1>
  %23 = sext i32 %2 to i64
  %24 = bitcast i16* %1 to <8 x i16>*
  %25 = load <8 x i16>, <8 x i16>* %24, align 16
  %26 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %25, <8 x i16> %20) #6
  %27 = icmp sgt <8 x i16> %26, zeroinitializer
  %28 = select <8 x i1> %27, <8 x i16> %26, <8 x i16> zeroinitializer
  %29 = icmp slt <8 x i16> %28, %22
  %30 = select <8 x i1> %29, <8 x i16> %28, <8 x i16> %22
  store <8 x i16> %30, <8 x i16>* %24, align 16
  %31 = getelementptr inbounds i16, i16* %1, i64 %23
  %32 = bitcast i16* %31 to <8 x i16>*
  %33 = load <8 x i16>, <8 x i16>* %32, align 16
  %34 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %33, <8 x i16> %20) #6
  %35 = icmp sgt <8 x i16> %34, zeroinitializer
  %36 = select <8 x i1> %35, <8 x i16> %34, <8 x i16> zeroinitializer
  %37 = icmp slt <8 x i16> %36, %22
  %38 = select <8 x i1> %37, <8 x i16> %36, <8 x i16> %22
  store <8 x i16> %38, <8 x i16>* %32, align 16
  %39 = getelementptr inbounds i16, i16* %31, i64 %23
  %40 = bitcast i16* %39 to <8 x i16>*
  %41 = load <8 x i16>, <8 x i16>* %40, align 16
  %42 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %41, <8 x i16> %20) #6
  %43 = icmp sgt <8 x i16> %42, zeroinitializer
  %44 = select <8 x i1> %43, <8 x i16> %42, <8 x i16> zeroinitializer
  %45 = icmp slt <8 x i16> %44, %22
  %46 = select <8 x i1> %45, <8 x i16> %44, <8 x i16> %22
  store <8 x i16> %46, <8 x i16>* %40, align 16
  %47 = getelementptr inbounds i16, i16* %39, i64 %23
  %48 = bitcast i16* %47 to <8 x i16>*
  %49 = load <8 x i16>, <8 x i16>* %48, align 16
  %50 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %49, <8 x i16> %20) #6
  %51 = icmp sgt <8 x i16> %50, zeroinitializer
  %52 = select <8 x i1> %51, <8 x i16> %50, <8 x i16> zeroinitializer
  %53 = icmp slt <8 x i16> %52, %22
  %54 = select <8 x i1> %53, <8 x i16> %52, <8 x i16> %22
  store <8 x i16> %54, <8 x i16>* %48, align 16
  %55 = getelementptr inbounds i16, i16* %47, i64 %23
  %56 = bitcast i16* %55 to <8 x i16>*
  %57 = load <8 x i16>, <8 x i16>* %56, align 16
  %58 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %57, <8 x i16> %20) #6
  %59 = icmp sgt <8 x i16> %58, zeroinitializer
  %60 = select <8 x i1> %59, <8 x i16> %58, <8 x i16> zeroinitializer
  %61 = icmp slt <8 x i16> %60, %22
  %62 = select <8 x i1> %61, <8 x i16> %60, <8 x i16> %22
  store <8 x i16> %62, <8 x i16>* %56, align 16
  %63 = getelementptr inbounds i16, i16* %55, i64 %23
  %64 = bitcast i16* %63 to <8 x i16>*
  %65 = load <8 x i16>, <8 x i16>* %64, align 16
  %66 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %65, <8 x i16> %20) #6
  %67 = icmp sgt <8 x i16> %66, zeroinitializer
  %68 = select <8 x i1> %67, <8 x i16> %66, <8 x i16> zeroinitializer
  %69 = icmp slt <8 x i16> %68, %22
  %70 = select <8 x i1> %69, <8 x i16> %68, <8 x i16> %22
  store <8 x i16> %70, <8 x i16>* %64, align 16
  %71 = getelementptr inbounds i16, i16* %63, i64 %23
  %72 = bitcast i16* %71 to <8 x i16>*
  %73 = load <8 x i16>, <8 x i16>* %72, align 16
  %74 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %73, <8 x i16> %20) #6
  %75 = icmp sgt <8 x i16> %74, zeroinitializer
  %76 = select <8 x i1> %75, <8 x i16> %74, <8 x i16> zeroinitializer
  %77 = icmp slt <8 x i16> %76, %22
  %78 = select <8 x i1> %77, <8 x i16> %76, <8 x i16> %22
  store <8 x i16> %78, <8 x i16>* %72, align 16
  %79 = getelementptr inbounds i16, i16* %71, i64 %23
  %80 = bitcast i16* %79 to <8 x i16>*
  %81 = load <8 x i16>, <8 x i16>* %80, align 16
  %82 = tail call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> %81, <8 x i16> %20) #6
  %83 = icmp sgt <8 x i16> %82, zeroinitializer
  %84 = select <8 x i1> %83, <8 x i16> %82, <8 x i16> zeroinitializer
  %85 = icmp slt <8 x i16> %84, %22
  %86 = select <8 x i1> %85, <8 x i16> %84, <8 x i16> %22
  store <8 x i16> %86, <8 x i16>* %80, align 16
  ret void
}

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32>, <4 x i32>) #4

; Function Attrs: nounwind readnone
declare <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16>, i32) #4

; Function Attrs: nounwind readnone speculatable
declare <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16>, <8 x i16>) #5

; Function Attrs: nounwind readnone
declare <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16>, <8 x i16>) #4

attributes #0 = { nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="128" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { argmemonly nounwind }
attributes #2 = { "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #3 = { nofree nounwind ssp uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "min-legal-vector-width"="128" "no-frame-pointer-elim"="false" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "null-pointer-is-valid"="true" "stack-protector-buffer-size"="4" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+sse3,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #4 = { nounwind readnone }
attributes #5 = { nounwind readnone speculatable }
attributes #6 = { nounwind }

!llvm.module.flags = !{!0, !1}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"PIC Level", i32 2}
